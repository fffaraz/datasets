That’s horrible advice. Might be practical if you aren’t sharing your work and want a quick fix, but if this is for a project or something reproducible you can’t just say “step 3. Open excel and transpose the data.”
It's already an Excel file so not a strange idea. 
Its the original data. OP would do this before doing anything in R. To its dumb to input data that its wrongly organized in the first place. Organize your data, then load it in R. 
Maybe a rolling mean? Or don't just look at the change in yaw but the actual angle it's at
What do you mean by rolling mean? And is has to be the a change in raw otherwise participants who held the steering wheel above an angle would automatically be above threshold even though they haven't actually turned the wheel... thus a change in yaw rate specifically shows a wheel turn
Sorry for my delayed response. I think the problem lies in the `facet_wrap()`. Try removing that and see if that gives you what you want. The reason this is a problem is because `facet_wrap()` creates a new plot for each unique level of party in this case. It sort of subsets the data before plotting, but when you do this and then go to plot party on the y-axis there is no longer any variation to plot so you get straight lines. Also, a boxplot seems a little weird to me for plotting categorical data on the y-axis. I'm not sure what live1 is, but if that is more of a continuous measure I would consider plotting that variable on the y-axis.
Why not just use an if_else? df %&gt;% mutate(result = if_else(avg_expression1 &gt;= avg_expression2, avg_expression1/avg_expression2, avg_expression2/avg_expression1))
Here's a snippet of code that might do what you want. thresh &lt;- 40 set.seed(0) YawRateChange &lt;- rnorm(100, mean = 45, sd = 5) look_ahead &lt;- 5 conseq_over_thresh &lt;- rep(NA, length(YawRateChange)) for (i in seq_along(YawRateChange)) { if (i + look_ahead - 1 &gt; length(YawRateChange)) next # stop early if we run out of look ahead space chunk &lt;- YawRateChange[i:(i + look_ahead - 1)] if (all(chunk &gt; thresh)) { conseq_over_thresh[i + look_ahead - 1] &lt;- 1 } else { conseq_over_thresh[i + look_ahead - 1] &lt;- 0 } } data.frame( YawRateChange, conseq_over_thresh ) It looks at a "look_ahead" window of 5 elements in the YawRateChange vector. It sets up a vector called conseq_over_thresh that is 1 if the current row and the 4 previous had values over threshold, a 0 if not, and NA if there are not 4 previous rows. This doesn't fit neatly into your `transmute()`, but I think it at least is clear in what it is doing.
I was trying to over-complicate things; this worked beautifully. Thanks!
It just takes the average over a given window, like say 4 measurements. It just smooths out the data bit. You can use your threshold on the average. The following will calculate the average of the following 3 observations library(zoo) window &lt;- 4 #number of observation to average over workingdata &lt;- data.frame(YawRateChange = rnorm(20, 0, 0.1), RMEAN = NA) workingdata[1:(nrow(workingdata) - window + 1),]$RMEAN &lt;- rollmean(workingdata$YawRateChange, k=window) The function is from the zoo package. It just uses `rollmean` to calculate the average of the "window" you set, in this case an average over 4 observations (the current line and the 3 ahead). 
Have you read https://cran.r-project.org/web/views/Spatial.html ?
That's okay no need to apologize! I appreciate your help! I'm sorry I keep badgering you with questions. I've made significant progress but I'm still having a some issues. If you're able to could you help me use glm to make a linear model? This is the code I have so far: dataset &lt;- mutate_if(dataset, is.numeric, as.integer) logitmodelfull &lt;- glm(clickonsponsored ~ . -race3m2 -race3m3 -race3m4, family = binomial, data = dataset) and I keep getting this error: Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : contrasts can be applied only to factors with 2 or more levels Thanks for all of your help so far I literally couldn't have done any of this without you. 
I'm less familiar with `glm()` than `ggplot2`, but I suspect one of your variables only has one level. There is no variation in the variable so it won't provide any information to a model so you can drop it from the formula. You could check the number of unique levels with `sapply(dataset, function(col) length(unique(col)))` and look for any 1's.
Also, I welcome PM's, this thread is getting a little crazy.
&gt; sapply(dataset, function(col) length(unique(col))) Thank you! I thought that was the issue too and I tried removing those variables but to no avail. I also ran the code you sent me and there weren't any 1s. Are you familiar with corrplot by any chance? I'm trying to create one and it keeps telling me that it can't find the object even though it is there in the dataset and it even shows up in the autofill thing(not sure what it's called). I'm using rstudio if that helps at all. Thank you!!
By the way next time try to make a reproducible example and show your data so its easier for others to help. I'll use mtcars to demonstrate how to change the labels. &amp;#x200B; &amp;#x200B; \## Old Fashioned mtcars. Let's do a boxplot using cal and mpg. &amp;#x200B; library(tidyverse) head(mtcars) mpg cyl disp hp drat wt qsec vs am gear carb Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 \##Boxplot ## Let's do a box plot with with x = cyl and y = mpg, facet by cyl mtcars %&gt;% ggplot(aes(x = cyl, y = mpg, group = cyl)) + geom_boxplot() + facet_grid(~cyl) \##Boxplot with new labels Values are originally the same as the cyl variable. We can change the labels using a lookup table. Notice the variables should be on left side of = sign, and new values on the right. cyl_labels &lt;- c("4" = "haha", "6" = "hoho","8" = "hehe") ##Now lets try the plot again mtcars %&gt;% ggplot(aes(x = cyl, y = mpg, group = cyl)) + geom_boxplot() + facet_grid(~cyl, labeller = labeller(cyl = cyl_labels)) 
&gt; To me its dumb to input data that its wrongly organized in the first place. Unless, as u/MacaroniGold suggested, this is part of a workflow. Maybe that's just the standard way the data are provided. Being in a different shape than you want does not make something *wrong* in general. Or maybe the data are stored somewhere that you can read but not write.
Is your goal to turn the first table into the second table? When you say `merge`, most people will understand that as a `join`
 df &lt;- mtcars # variable you want name1 &lt;- 'mpg' # pull df[[name1]] df[,name1]
Oh God it was that so simple! Thank you stranger, after a long week of programming someone can get really dumb
Ah sorry, I should have clarified that the second table is my desired output. What's the correct term to use in this case?
So your dataframe has multiple columns with the same name? "Wide to long format" is a better way to state this.
Np bro. We all get stuck. Especially after long long sessions simple solutions never come to mind until I have a good break.
Yes, it has multiple columns with the same name.
I see! This might just do the trick. I have thought of another way to complete this which I could post over the weekend, but this solution might be more simple... thank you!
I'm a little confused here. Do you have multiple columns in the same dataframe with the same name? Are you trying the make a new dataframe with columns that are not all the same length? Both of these are big no-no's for dataframes and I'm not even sure if they are possible. You should probably use nested lists for this
Just to confirm, your column names in r show as A, A.1, B, B.1, C, C.1, C.2,... right?
Thank you! This is neat little trick... I've thought of a slightly different way to solve my problem which I might post if I need help with over the weekend. I'll try to see if I can still utilise my `transmute()` function with this loop!
Yep.
I had to transpose the data into this format because of a software requirement.
Not the prettiest code but how is this? # your data df &lt;- data.frame( A = 1, A = 2, B = 1, B = 2, C = 1, C = 2, C = 3, C = 4, D = 1, D = 2 ) # from long to wide df %&gt;% gather(var, val) %&gt;% mutate(var = gsub('\\..*', '', var)) %&gt;% group_by(var) %&gt;% mutate(seq = 1:n()) %&gt;% spread(var, val) %&gt;% select(-seq)
Hi, so I have used the split() function plp=split(v, sample(rep(1:5,14912))) to split the data frame "v" into 5 equal chunks. But how can I recall each chunk?
I'm not sure how your data is set up, but it's going to be a lot easier to do this if it's in a format like |letter|number| |------|------| |A|1| |A|2| |B|1| |B|2| etc. From there you could use the [melt](https://www.rdocumentation.org/packages/reshape2/versions/1.4.3/topics/melt) function from the reshape2 package
That's actually the original format before I transposed it. Let me try it with the melt function then.
Thank you for your help and taking the time to write it, I'll go try this.
I'm a fan us using python within R (via the R package called reticulate) for certain things. Python is better at web-scraping and building tools. R is better for stats.
You can also try the `spread` function from [tidyr](https://tidyr.tidyverse.org/). 
Hum, reticulate. Never heard about... Will look it up.
try this. it runs with the library data.table, which i use extensively. it might work with data.frames too. &amp;#x200B; library (data.table) merge.by &lt;- c( 'A', 'B', 'C', 'D') c &lt;- merge (a , b , by = merge.by , all = T ) 
I use R + Node because I also wish R was better at scraping. What sort of tools do you build out with Python?
RVest doesn't cut it for you? I know node has some dope scraping packages but I've been happy with RVest.
rvest is great for pulling in tables, and when there aren't hundreds/thousands of pages or authentication. I found rvest a little wonky with xml too, but that it probably my fault. I also like to use Node for scraping tweets in an RStudio Server terminal tab, and then I can run/rerun what I have gathered currently from ndjson. 
Using `dplyr`, you can use `group_by()` and `row_number()`: set.seed(42) df &lt;- data.frame( x = sample( letters[1:4], 20, replace=TRUE ) ) df %&gt;% group_by(x) %&gt;% mutate( num = row_number() ) %&gt;% ungroup() %&gt;% arrange( x ) 
Yeah that makes sense. For large scrapes I would guess node is way, way faster since it's non blocking and fast IO is kind of it's thing.
Yeah, Node is great for when you want to fetch 20 other pages while the first one is fetched. I will probably do a speed comparison once I finish my [lastfm music match](https://static.appup.io/plotly/artists/The_Mountain_Goats/Explosions_in_the_Sky/) tool. 
That's counting the number of rows for the group. There is a time point variable which makes this innacurate. I want to count the number of unique values of B, for each A. 
&gt; I want to count the number of unique values of B, for each A. Then can you give a bit more clear or representative example? I'm not quite certain what you're trying to get at.
Yeah, let's say A is a list of each student. B is a list of their classes. C is a time point (midterm, final) So a few rows could be: Smith, English, midterm Smith, math, midterm Smith, math, final Johnson, English, final Etc. Ideal result replaces names and class with numbers 1, 1, midterm 1, 2, midterm, 1, 2, final 2, 1, final
Can you write code in python with reticulate and build a shiny app?
I'm still not 100% certain. Are you saying that the data will be something like: Person | Subject | Time -------|-------|------- Smith | Math | Midterm Smith | Math | Midterm Smith | Math | Final Smith | English | Midterm Smith | English | Final Johnson | Math | Midterm Johnson | Math | Final Johnson | Math | Final Johnson | Math | Final Johnson | Math | Final Johnson | English | Midterm Johnson | English | Midterm Johnson | English | Midterm Johnson | English | Final And the output will look something like: Person | Subject | Time | Attempts -------|-------|-------|------- Smith | Math | Midterm | 2 Smith | Math | Final | 1 Smith | English | Midterm | 1 Smith | English | Final | 1 Johnson | Math | Midterm | 1 Johnson | Math | Final | 4 Johnson | English | Midterm | 3 Johnson | English | Final | 1 Or are you trying to count something else? If it's the combination like what I showed, then you can use multiple variables in the `group_by()` and use `summarize()` with the function `n()` to count the number of rows in each combination. set.seed(42) nn &lt;- 10 df &lt;- data.frame( var1 = sample( letters[1:3], nn, replace=TRUE ), var2 = sample( letters[4:6], nn, replace=TRUE ), var3 = sample( letters[25:26], nn, replace=TRUE ) ) df %&gt;% group_by( var1, var2, var3 ) %&gt;% summarize( Count = n() ) %&gt;% ungroup() 
Is `plp[[1]]` what you are after?
No, sorry. I'll try to use the real variables as much as I can Var1 is personal identifier. Var2 is a unique for medical procedures Var3 is lab results at specific time points following the procedure So, each person may have multiple procedures, and each procedure may have multiple lab results I'd like a variable that counts each unique procedure, per patient. New column names could be personID, procedure_number, timepoint
Hmm ... so is this along the lines you're thinking? Person | Proc | LabResult -------|-------|------- Smith | A | 6 Smith | A | 8 Smith | B | 7 Johnson | A | 6 Johnson | B | 7 Johnson | C | 8 Johnson | C | 9 And then you want to say that Smith had 2 procedures, and Johnson had 3? Or are you thinking that the first two rows could represent Smith having procedure A twice (e.g., Flu shot), and this represents two different procedures?
I tried to learn R: but I don’t know much stats. So learning R felt like trying to learn French using a french textbook on nuclear physics: if I don’t understand the material it is manipulating, then I’m going to struggle to learn the language. I had a much easier time with Python because I could keep to projects and examples I could more easily grasp. So the programming element could be isolated and conquered. 
If you're ever trying to pick up R again it helps to just pick apart the basic data structures, like a list is closer to python dict and a vector is like a python list, but immutable and with optional element names. Tough to really work with dataframes and functional programming until you get past those differences. Also, no such thing as a lambda function etc
You need RTVS to run shiny apps in VS.
python as data frames and functional programming constructs (albeit complete). In R there is no lambda function because anonymous functions are truly just unnamed functions - same syntax as named functions without the assignment. 
I'm unclear on what to loop. This looks like the correct way to go about it - you are taking advantage of the vectorized nature of R. The only improvement I could is getting fancy with named vectors, but it's definitely not necessary: Model_Dict = c( '17412F45' = 'Accent LE' , '28402F45' = 'Sonata SE' , '284H2F4P' = 'Sonata SE' , '284J2F4P' = 'Sonata SEL' ) Model &lt;- sample(names(Model_Dict), 10, replace = TRUE) Model Model_Dict[Model] 
yes you can-- https://rviews.rstudio.com/2018/04/17/reticulated-shiny/ 
I also had this problem with R. The material I read when I was first learning it felt like it was written by someone who had discovered R and taught themselves how to use it without knowing any of the right words to describe programming language constructs. I think there are just some bad guides out there written for stats people by stats people - this was 7 or 8 years ago. Now there are better resources - particularly the stuff written by Hadley Wickam
 df %&gt;% group_by(Var1) %&gt;% mutate(count = n_distinct(Var2))
I suggest learning how to deploy your Shiny app through Docker. Your IT guy will know how to deploy if you can create self contained Docker image. 
Or... ModelNames = read.table( text= "Model ModelName 17412F45 'Accent LE' 28402F45 'Sonata SE' 284H2F4P 'Sonata SE' 284J2F4P 'Sonata SEL' ", header=TRUE, as.is=TRUE) DF1 &lt;- DF %&gt;% left_join( ModelNames, by= "Model" )
It might be more correct to say that all functions in R are lambda functions because they always pick up the environment in which they are defined. https://lukesingham.com/anonymous-functions-in-r-python/
Divide col1 by col2 store in result1. Divide col2 by col1 store in result2. Create a ratio column with the max of each using mapply. df["ratio"] &lt;- mapply(result1,result2,FUN=function (i,j){max(i,j)}) Just passing `max` as the FUN arg should work as well.
Going from Python to R was a bit of a headache. My research involves quite a bit of geospatial analysis, which R is great for, but it's syntax can be quite odd at times when compared to Python's. I've spent a week trying to figure out a syntax error I'm getting and I can't get it to work for the life of me.
I used to write Python scripts and call them from R via `system()`. Reticulate changed everything. My favorite is taking a tool that returns lists (like Python `usaddress`) and parse tons of results to a perfect data.frame with ldply. So hot. All I use Python directly for is building APIs with Flask... but I just learned about Plumber and it’s like __stupid__ easy to build an API backed in R. 
Hey! I tried that solution, but got the same data frame as the original. I think the entries works fine, it has 0 and 1’s, but it last part doesn’t seem to be working. (it doesn’t delete the undersides row)
I feel you. I teach a course on spatial analysis in R (very stats/ML heavy, but technically a GIS course) - my students hated R at first for the same reasons, but once you get the hang of it enough it just RIPS. It is odd going from something like geopandas, fiona, or arcpy and into the way R “sees” data. I really love how easily integrated R is with PostgreSQL/PostGIS. At the end of the day: all fantastic tools, but definitely geared for different audiences / research questions / desired outputs. I really can’t recommend DataCamp enough for learning R, they even have some solid spatial courses.
I swear I don’t work for DataCamp... but it’s a great way to learn. They come at it from a data analysis point of view, and the practical explanations really make it stick. But Python’s also badass so whatever floats your boat :)
Is this a homework problem?
I do signal and image processing for engineering work in Python, and I do statistical analysis for an unrelated field in R. It's to the point where if I tried to do something in one that I usually did on the other, I might be stymied because my approach to the problem would be so different. I came from C++, so picking up Python felt natural. Learning R required a bit more mind bending. I used to program in a form of Lisp in high school, so R's ideas weren't completely foreign. But I did have to read and tinker a lot to get used to R's data structures. 
pt(q = 2.56, df=14, lower.tail = FALSE)
I would replace with dplyr’s mutate and an ifelse. dataframe %&gt;% mutate(worth_column = ifelse(id_column == id, worth_column - worth, worth_column)) The above basically says take the dataframe, alter the worth_column anywhere id_column is equal to the id you want to adjust, else just keep the value currently in the worth column.
Thanks. I'm getting this error now though on an earlier smart bind: Error in !sapply(data, function(l) is.null(l) | (ncol(l) == 0) | (nrow(l) == : invalid argument type where the code is data_frame = smartbind(data_frame, input_checked[i,]) Any thoughts?
Hard to tell from the error. Post the code and give an idea of what your data looks like
Here: https://pastebin.com/LVAynwfQ
case_when from dplyr should do it for you
Are you trying to set you bid\_book and order\_book to empty data frames? If so the correct syntax would be `bid_book = data.frame()`
Ah, thanks, that solved the error. I'm getting this error now though: Error in bid_book$order - id : non-numeric argument to binary operator Not sure wht could be causing this one either.
You're trying to say 'order-id' but R is interpreting this as order (subtract) id. Put quotes or backticks around it: `bid_book$\`order-id\`` 
The subtractions sign/ dashes are best to avoid in variable names. Stick to underscores. Standardizing your variable names is important. Make sure they are descriptive. I like to use all lower case and place underscores where there would intuitively be a space. 
Thanks mate. My program seems to be processing 'size\_column' fine but all of a sudden I'm getting object 'order\_id\_column' not found for some reason. (Note: I changed all order-id to order\_id)
I'm going to assume yes. OP: try using a matrix, since this is all one type. Then evaluate your conditions. Your results will be logical, which you can use to subset and filter as needed.
This is also good practice since, if your output is a table or chart for presentation, it's easy to write your final column names to a character vector and gsub "_" for " ", or "_pct_ " for " % ", etc. 
Are you sure that the correct answer is 0.011? If it's a two-tailed t-test, the p-value is **2 \*** pt(-2.56, df = 14) = 0.023.
id\_vec = c( '17412F45' , '28402F45' , ...) str\_vec = c( "Accent LE", "Sonata SE", ...) &amp;#x200B; data7$Model\[ data7$Model == id\_vec\] = str\_vec &amp;#x200B; &amp;#x200B;
Not super obvious to me what your data looks like from your code- looks like something squirrely going on aggregating data from a bunch of different data frames named df?. But once you've got a single happy data frame, I'd shave down to just the columns I want, melt into long format, do any processing stuff I need, then ggplot the thing. Something like this, fudging the initial data: dates &lt;- c(1999, 2000,2001) congress &lt;- c(1,2,3) president &lt;- c(2,4,6) house &lt;- c(2,5,7) govtrust &lt;- c(5,7,8) df &lt;- data.frame(dates, congress, president, house, govtrust) forplot &lt;- df %&gt;% dplyr::select(dates, congress, president, house, govtrust) %&gt;% reshape2::melt(variable.name = 'group', value.name = 'approval', id = 'dates') %&gt;% dplyr::mutate(fix_approval = case_when(approval %in% 1:5~ -approval+6 )) plot &lt;- ggplot2::ggplot(forplot, aes(x = dates, y = fix_approval, fill = group)) + ggplot2::geom_bar(stat = 'identity', position = 'dodge') plot
Hi, Thanks for your response. I tried out and it did publish... but it is slightly different from what I have expected. Stargazer returned something like this: \&gt; stargazer(annual.retreg,type = "text") &amp;#x200B; =============================================== Dependent variable: \--------------------------- annual.vwRtp1 \----------------------------------------------- annual.vwR 0.008 (0.112) Constant 1.105\*\*\* (0.127) \----------------------------------------------- Observations 82 R2 0.0001 Adjusted R2 -0.012 Residual Std. Error 0.209 (df = 80) F Statistic 0.005 (df = 1; 80) =============================================== Note: \*p&lt;0.1; \*\*p&lt;0.05; \*\*\*p&lt;0.01 &amp;#x200B; I was hoping it to format it like this as it appears in summary table of the console: &amp;#x200B; \&gt; summary(annual.retreg) &amp;#x200B; Call: lm(formula = annual.vwRtp1 \~ annual.vwR) &amp;#x200B; Residuals: Min 1Q Median 3Q Max \-0.5542 -0.1388 0.0282 0.1554 0.4629 &amp;#x200B; Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.105026 0.126514 8.734 2.92e-13 \*\*\* annual.vwR 0.007797 0.111677 0.070 0.945 \--- Signif. codes: 0 ‘\*\*\*’ 0.001 ‘\*\*’ 0.01 ‘\*’ 0.05 ‘.’ 0.1 ‘ ’ 1 &amp;#x200B; Residual standard error: 0.2085 on 80 degrees of freedom Multiple R-squared: 6.093e-05, Adjusted R-squared: -0.01244 F-statistic: 0.004875 on 1 and 80 DF, p-value: 0.9445
Interested in this as well. I have just started learning R myself. !remindme 2 hours
I will be messaging you on [**2019-03-10 00:58:39 UTC**](http://www.wolframalpha.com/input/?i=2019-03-10 00:58:39 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/az8qu3/plotly_vs_highcharts_vs_other_d3_libraries/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/az8qu3/plotly_vs_highcharts_vs_other_d3_libraries/]%0A%0ARemindMe! 2 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I too would love a well grounded oppinion on this. I have experimented with plotly, it looks most developed of the options out there. But highcharts look better. 
I'm a huge fan of plotly, mostly due to the ggplotly function. You just need to call ggplotly on a ggplot2 object to convert it to an interactive. There is next to no additional work necessary, only adding label/text aesthetics in ggplot2 for additional information to be displayed.
I use Google Charts, Highcharts and Plotly interchangeably in shiny applications. Google Charts via the googleVis package is my current favorite. It takes a little extra data transformation than Plotly or Highcharts to utilize effectively (you typically need to transform your data from long to wide format and occasionally will need to use specific naming) but the charts it generates have fantastic, clean default aesthetics/labels/breaks etc. and I find that it handles datetimes natively much better than Highcharts. If I build something a little more bespoke in ggplot then it’s Plotly all the way (ggplotly() function is magic) since it’s difficult to build outside the box with Highcharts and Google Charts without in depth knowledge of the API’s and in many cases additional javascript. 
I've looked into this recently and landed on plotly as my got to for interactive plots. The integration in R is very good and, like others have mentioned, you can also bring ggplot2 visuals directly into it using ggplotly. I haven't spent any time with highcharter mainly due to it being free only for non-commercial use. If that's not a concern for you then I think it's worth looking at, and it definitely produces some beautiful plots from what I've seen.
You can use get() and assign() to deal with using string names for data frames. name &lt;- paste0("sub", j) df &lt;- get(name) # do stuff to df assign(name, df, envir = .GlobalEnv) Generally, loops are the slowest way of doing something. What are you trying to accomplish with the loop?
Gotcha !! Thanks! I can actually do this manually, but just too lazy Lol.
Plotly is definitely is the most flexible, but I've found the rendering can be quite slow and I prefer how Highcharter looks so I use it the most currently
What have you tried?
Tried use cut function for variable c then glm(y~a+b+cut.c, dataset, family=poisson) glm.nb(y~a+b+cut.c, dataset) But both models are insignificant It seems strange....
That's a legitimate question. Are you asking help doing it, or are you asking people to do it for you?
This question isn't really related to the R language... Seems more like a stats/econometrics question. Might wanna try some other subs.
Did you try Google dataset search? [https://toolbox.google.com/datasetsearch/search?query=March%20Madness&amp;docid=Ht0HxLYjxCn0rMsjAAAAAA%3D%3D](https://toolbox.google.com/datasetsearch/search?query=March%20Madness&amp;docid=Ht0HxLYjxCn0rMsjAAAAAA%3D%3D)
Try to [uninstall](https://support.rstudio.com/hc/en-us/articles/200554736-How-To-Uninstall-RStudio-Desktop), and follow [this guide](http://web.cs.ucla.edu/~gulzar/rstudio/) to install R and RStudio.
Already tried several times but not working :(
Probably the most comprehensive and free data set you'll find in one place: https://www.kaggle.com/c/mens-machine-learning-competition-2019/data
Have you installed R in addition to RStudio?
yeah i did, when i tried to reset rstudio desktop's state from terminal, i got a warning like this " mv: rename /Users/keba/.rstudio-desktop to /Users/keba/backup-rstudio-desktop: No such file or directory "
To make sure I understand: for each group where YawRateChange &gt; some upper threshold value AND &lt; some lower threshold value, identify the row with the YawRateChange that is the next lowest versus the upper threshold. If so, are duplicates possible, for YawRateChange? And are the values fixed precision?
Not quite. I want go *down* the YawRateChange column until a hit a value at the upper threshold. Then, I want to go back up the column and then find the first value that is lower than my lower threshold and select the first row. Lets say my `upper threshold` is `0.1` and my `lower threshold` is `0.035` for this example: The code would go down the YawRateChange column, find the first value above `0.1` (row 14), it would then go back up the column and find the first value below `0.035` (row 7) and then it would select row 7's data. This would happened for every ppid\_trialn and save them all to a new dataframe. &amp;#x200B;
Not quite. I want go down the YawRateChange column until a hit a value at the upper threshold. Then, I want to go back up the column and then find the first value that is lower than my lower threshold and select the first row. Lets say my upper threshold is 0.1 and my lower threshold is 0.035 for the example above: The code would go down the YawRateChange column, find the first value above 0.1 (row 14), it would then go back up the column and find the first value below 0.035 (row 7) and then it would select row 7's data. This would happened for every ppid\_trialn and save them all to a new dataframe. &amp;#x200B;
Not quite. I want go down the YawRateChange column until a hit a value at the upper threshold. Then, I want to go back up the column and then find the first value that is lower than my lower threshold and select the first row. Lets say my upper threshold is 0.1 and my lower threshold is 0.035 for the example above: The code would go down the YawRateChange column, find the first value above 0.1 (row 14), it would then go back up the column and find the first value below 0.035 (row 7) and then it would select row 7's data. This would happened for every ppid\_trialn and save them all to a new dataframe.
Your code worked for me in RStudio. How are you running it? Also, in the raster documentation it says that "Not all graphics devices are capable of rendering raster images". Could that be the reason? Finally, you can try saving the plot as a file. Like this: library(grid) png(filename="C:/tmp/img.png") x &lt;- array(runif(50*50*3, 0, 1), dim=c(50, 50, 3)) grid.newpage() grid.raster(x) dev.off()
&gt;OMG. I just closed and reopened RStudio and it shows up now wtf. thanks for the suggestion though. I didn't know how to save to image file before anyway.
Have you tried making the directory? I figure that you have /Users/Keba, so just cd into that and mkdir backup-rstudio-desktop
Let me know if this is roughly what you are looking for. I don't use dplyr syntax much these days, so the data.table based example is intentionally verbose to make interpretation easier. library(data.table) # First make two examples to test logic across ppid groups ---------------- `1_13` &lt;- data.table( heading = -1L, YawRateChange = c( 0.03499, 0.07, 0.03499, # below, should not be selected 0.03501, 0.03499, # below, should be selected 0.03501, 0.07, 0.03501, 0.03501, 0.07, 0.03501, 0.10495, # above threshold 0.03501 ), anchored_timestamp = 100:112 ) `1_14` &lt;- data.table( heading = -1L, YawRateChange = c( 0.03499, 0, # below, should not be selected 0.03499, # below, should not be selected 0.03501, 0.03498, # below, should not be selected 0, # below, should be selected 0.07, 0.03501, 0.03501, 0.07, 0.03501, 0.10495, # above threshold 0.03501 ), anchored_timestamp = 1000:1012 ) # Test table -------------------------------------------------------------- dt &lt;- rbindlist(list(`1_13`, `1_14`), use.names = TRUE, idcol = "ppid_train") # set a group index dt[, grpidx := .I, by = ppid_train] # vars -------------------------------------------------------------------- ut = 0.1 lt = 0.035 # make subset that has only valid groups by upper ------------------------- valid_ppid &lt;- dt[YawRateChange &gt; ut, unique(ppid_train)] dt_sub &lt;- dt[ppid_train %in% valid_ppid] # identify ---------------------------------------------------------------- filt &lt;- dt_sub[YawRateChange &lt; lt, .(grpidx = max(grpidx)), by = ppid_train] dt_sub[filt, on = c("ppid_train", "grpidx"), nomatch = 0L] 
Sounds like a permissions problem. I haven't installed R Studio on Mac OS X in over 2 years, but I do recall having a problem like this when I did. Off the top of my head, try reinstalling with *sudo*, then, if you hit more errors during install or while trying to run, post back here.
Same problem here. I had to install R directly from CRAN for the Mac rather than via brew or any other package manager.
I think this code nearly does what I want, however the rest of my code revolves around manipulating data frames using dplyr, thus using this technique might be inefficient. However I might use some of these ideas and work on them - thank you!
Thats one of the reasons I haven’t used highcharts yet, the licence
The reason I'm not sold based on ggplotly is that I've read that the function will often times change the position of chart attributes like the legend in the migration, especially when highly customizing the chart, which I need to do for work because our firm has a very specific format. For that reason, I'd like to work natively in whichever package I choose. Have you had that experience?
Yeah, the licensing is a big reason I'm hesitant on it.
Thanks for your comment. Do you have a public Shiny dashboard I could take a look at? The data transformation's not a big deal. I input data into ggplot in both format, depending on which ends up making it easier all the time. I'm mostly interested in how customizability, how easy and costly it is to be able to post charts to our company's website and responsiveness on websites. Which do you think would best fit the bill? It sounds like you would suggest plotly in my case since I have no desire to learn Javascript and need to be able to customize.
I have had it center the chart title when I haven't specified a position in ggplot, but nothing else that I can think of. I have mostly used it for school projects and EDA, so stringent formatting standards haven't been a concern for me. If you know ggplot already, it really is fantastically easy. I'd recommend trying it out and seeing if it works for you. I think there is a bit of formatting stuff that can be specified in the ggplotly function. you can also use the style function after using ggplotly to adjust formatting.
Cool, I'll give it a try and let you know.
I'm not an expert in it by any means, but it has been pretty easy to use. Hopefully it'll work for you. Good Luck.
Performance - wise data.table is almost always faster, but I understand the importance of style consistency. The concepts should be readily transferable to dplyr. 
Step 1: throw your Mac away Step 2: never purchased an apple product again 
Had the same problem when I tried to open the newest version through Anaconda. In my case choosing the lower version (or installing the previous one) helped. I installed the vanilla RStudio though, less problems on Mac.
Plop this code under slin30's '''lt = 0.035 statement. I agree with his approach in that an explicit loop is unnecessary. ''' dt%&gt;% group_by(ppid_train)%&gt;% mutate(row_number())%&gt;% filter(max(YawRateChange) &gt; ut, min(YawRateChange) &lt; lt)%&gt;% slice(1:min(which(YawRateChange &gt; ut)))%&gt;% slice(max(which(YawRateChange &lt; lt)))
Second this, try a different version and then move up. Alternatively just install rstudio-server and run from there. 
I don't think I can help you and I'd bet that most other people will be in the same boat. You should have a look at [this article about creating reproducible examples when asking for help in R](http://adv-r.had.co.nz/Reproducibility.html).
It is weird to see anti-Apple sentiment in this subreddit. It seems like most people furthering the language use Apple products, and the big talks are all on Macbooks. Not only that, but this is definitely not a hardware issue, and Macs can run Linux, Windows, and some other Oses. 
I don't know a single coder that uses a Mac in the biological sciences. It's pretty frowned upon. 
Once you start meeting programmers instead of coders you will probably see them used more. ggplot2, dplyr, etc. were written on a Macbook. The people behind Microsoft's Open R were mostly on Macbooks as well. Seems like college classes that include LabVIEW expect people to use Windows, not sure why. rJava was also build by a Macbook user and is a dependency for many libraries. There is no need to be tribal in regard to Windows, Linux, or OSX. I prefer Macbooks, and my girlfriend prefers Windows. I know a guy who just joined Amazon and loves Linux on his Dell laptops. If you decide to be tribal, then at least include some supporting arguments. 
Yea ecologists aren't really programmers we mostly do stats and population modeling and most of the software for instruments we use run best or only on PC or Linux.
Oooo, thanks !
This is great! I haven't come across the which() function before, that's very handy. Thank you for this! 
You're doing logistic regression. You need to learn what logistic regression is before any of this will make sense.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/bioinformatics] [Can anyone help offer guidance with preparing my correlation data for network analysis (generating Nodes and Edges) ? \[x-posted from Rlanguage\]](https://www.reddit.com/r/bioinformatics/comments/aztqaz/can_anyone_help_offer_guidance_with_preparing_my/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Looks like the titanic data set? That regression gives you the log of odds of survival (your prediction variable). glm is good for categorical classification regressions like this (survived/did not survived) as opposed to OLS or other models that you would use for a numeric variable.
Have you already removed duplicate correlations from your data? By default, most correlation packages will display both the *upper* and *lower* panels for a correlation matrix. For this reason, each pair is duplicated. For example, | gene1 | gene2 | cor | pval | |:------:|:------:|:----:|:---:| |ACOT1|ACOT2|0.906|1.00E-5| |ACOT2|ACOT1|0.906|1.00E-5| The rest of my answer depends on your answer.
igraph can handle all your graphing needs. [https://igraph.org](https://igraph.org) It is a package I am very familiar with so feel free to ask a follow up question if needed. &amp;#x200B; This should work &amp;#x200B; `library(igraph)` `library(visNetwork)` &amp;#x200B; `# Example Data` &amp;#x200B; `nodea &lt;- sample(LETTERS[1:10], 100, replace = T)` `nodeb &lt;- sample(LETTERS[11:20], 100, replace = T)` `corr &lt;- sample(seq(from = 0.5, to = 1, by = 0.05), 100, replace = T)` `pval &lt;- sample(seq(from = 0, to = 0.3, by = 0.05), 100, replace = T)` &amp;#x200B; `example_net_data &lt;- as.data.frame(matrix(c(nodea, nodeb, corr, pval), ncol = 4))` `example_net_data[,3] &lt;- as.numeric(as.character(example_net_data[,3]))` `example_net_data[,4] &lt;- as.numeric(as.character(example_net_data[,4]))` &amp;#x200B; `# Filter` `sig_cor &lt;- which(example_net_data$V3 &gt; 0.7)` `sig_pval &lt;- which(example_net_data$V4 &lt; 0.1)` &amp;#x200B; `sig_matrix &lt;- intersect(sig_cor, sig_pval)` `sig_matrix &lt;- example_net_data[sig_matrix,]` &amp;#x200B; `# Build` `network &lt;- graph_from_data_frame(sig_matrix)` `E(network)$weight = sig_matrix$V3*5` &amp;#x200B; `network &lt;- as.undirected(network)` &amp;#x200B; `linMap &lt;- function(x, from, to){` `(x - min(x)) / max(x - min(x)) * (to - from) + from` `}` &amp;#x200B; `E(network)$weight &lt;- linMap(E(network)$weight, 1, 5)` &amp;#x200B; `plot(network,` `edge.width = E(network)$weight)` &amp;#x200B; &amp;#x200B; `# visNetwork` `nodes &lt;- igraph::as_data_frame(network, what = "vertices")` `colnames(nodes)[1] &lt;- "id"` &amp;#x200B; `links &lt;- igraph::as_data_frame(network, what = "edges")` `links$arrows &lt;- "to"` &amp;#x200B; `nodes$label &lt;- V(network)$name` `links$value &lt;- E(network)$weight` &amp;#x200B; &amp;#x200B; `visNetwork::visNetwork(nodes, links, width="100%", height="1000px")` `visnet &lt;- visNetwork::visNetwork(nodes, links, width="100%", height="1000px")` &amp;#x200B; `visNetwork::visSave(visnet, file=paste0("Example.html"))` &amp;#x200B;
Yes sorry I realize duplicates are displayed here. Yes i have removed duplicates and the diagonal from my matrix to get a list like the one in this post (except without duplicates). Thank you
Your example works beautifully. I'm going to spend some time applying my data to this....I'm super appreciative for this - thanks!
Feel to ask more questions. Some relevant parameters that may be of interest for you. Node size Node color Node boarder color Edge color You may also be interested in running community analysis on this graph. Visit here, http://kateto.net/networks-r-igraph That tutorial alone was my savior when I started working on networks.
You pretty much have it. The details are significantly more complicated and you'll need years of statistics training to really grasp what's going on, but at a fundamental level that's basically what's happening. 
Awesome! I’m reading more into it. It’s very fascinating to me. But I want to learn more about the details. Thanks for the help. 
Oh man! This is perfect! Thank you for the explanation. 
I'd suggest a 300 level econometrics class then. Glm is one of the workhorses of multivariate statistics. It's essentially a unified model of dozens of related models developed over the years. So any courses that cover multivariate statistics and regression methods for observational data will be up your alley.
There's probably a cleaner way to do it, but it's pretty straightforward if you convert to a matrix and use rowSums() and colSums(): `\&gt; S &lt;- data.frame(matrix(S, nrow = 6, ncol = 5))` `\&gt; S\[,6\] &lt;- rowSums(S)` `\&gt; S\[6,\] &lt;- colSums(S)` `\&gt; as.matrix(S)`
On my phone, so I'll make this short. I used it for a Market Mix Model to show how TV add strength and internet advertising strength. We would then use the results to determine how much we would spend on advertising.
Probably you want to use `colMeans` and `rowMeans` instead. Also, your third line is replacing the 6th row, rather than adding a row. It's a bit more tedious, but using something like `S[,ncol(s)+1]` would guard against that.
Whoops, I was thinking that it was a square matrix, and only noticed the "sums" part of his question. Spellcaster, listen to Statman, he read your question more thoroughly than I did
&gt; S\[,6\] &lt;- rowSums(S) Hey, it took me a while, also because you made a couple of mistakes, and the mistakes actually helped me understand much more what was I doing. So this was extreamly helpful. Thanks a lot!
[tidyverse](https://www.tidyverse.org/) provides some neat solution but it requires you reformat your data in a tidy fashion: Assuming in your data columns and rows are two variables, you can tidy it like this: ```{r} S = seq(1,90, by = 3) S_df = data.frame(matrix(S, nrow = 6, ncol = 5), row.names = seq(1,6, by = 1)) S_tidy = gather(rownames_to_column(S_df), col, value, -rowname) ``` This will give you a tidy table (or a list of key-value pairs): &amp;#x200B; |rowname|col|value| |:-|:-|:-| |1|X1|1| |1|X2|4| |...|...|...| |4|X2|28| |...|...|...| |6|X5|88| Note the `rowname`would be your "Row variable" and `col` would be your "Column variable" (you can name them however you want, just change the code accordingly) and `value` is the value and corresponding row and column. `group_by()` allows you to group data by either `rowname` or `col` and `summarise()` apply a simple function to the group: summarise(group_by(S_tidy, rowname), n=n()) &amp;#x200B; |rowname|n| |:-|:-| |1|5| |2|5| |3|5| |4|5| |5|5| |6|5| This tells you that for each `rowname` you have 5 values (or 5 columns per row in your original matrix). Same can be done to `col` variable as well. To calculate col means and row sums: summarise(group_by(S_tidy, col), means = mean(value)) |col|means| |:-|:-| |X1|8.5| |...|...| summarise(group_by(S_tidy, rowname), sum = sum(value)) |rowname|means| |:-|:-| |1|185| |...|...| &amp;#x200B; If you want these stats next to your original data, `mutate()`allows you to add a new variable to your data frame: S_rowSum = mutate(group_by(S_tidy, rowname), sum = sum(value)) S_colMean = mutate(group_by(S_rowSum, col), means = mean(value)) S_colMean |rowname|col|value|sum|means| |:-|:-|:-|:-|:-| |1|X1|1|185|8.5| |...|...|...|...|...| |3|X1|7|215|8.5| |...|...|...|...|...| |1|X4|55|185|62.5| |...|...|...|...|...| &amp;#x200B; If you want it in more familiar matrix format: #convert S_tidy back to "wide" format: spread(S_tidy, col, value) %&gt;% #Note you can simply use "df %&gt;%" instead #paste row sums left_join(summarise(group_by(S_tidy, rowname), sum = sum(value)), by = "rowname") %&gt;% #Add col means bind_rows(spread(summarise(group_by(S_tidy, col), means = mean(value)), col, means)) &amp;#x200B; |rowname|X1|X2|X3|X4|X5|sum| |:-|:-|:-|:-|:-|:-|:-| |1|1|19|37|55|73|185| |...|...|...|...|...|...|...| |*NA*|8.5|26.5|44.5|62.5|80.5|*NA*| &amp;#x200B; Granted, this is way more complicated than u/statisnaught's answer and definitely overkill for what you're trying to achieve, but it's a good habit to always tidy your data as it is incredibly powerful with some more complicated data.
wow man. thanks I'll definitely study this. It is certainly an overkill but you provided good and useful information that will help me a lot as I venture in the mysterious ways that are of this dumb genius that is R.
What is the best way to deploy shiny apps on own server without paying for shinyserver pro? We have some confidential data that needs to go on our website , so all data has to be on our own server. Thanks ! Will take a look at that. Is the open source shiny a good solution? What is the limitatioj os using open source shiny server instead of shiny server pro? or this? - https://qualityandinnovation.com/2015/12/09/deploying-your-very-own-shiny-server/
Thanks ! Will take a look at that. We have some confidential data that needs to go on our website , so all data has to be on our own server. Is the open source shiny a good solution? What is the limitations of using open source shiny server instead of shiny server pro? or this? - https://qualityandinnovation.com/2015/12/09/deploying-your-very-own-shiny-server/
The first error: not enough points, isn't an error with the code part. I would assume there's not enough threshold points to fit a smoothened ROC curve. Tools tend do that, so as to not give a misleading oversimplified jagged curve. See if you can add more thresholds/levels/bins to your dataset, for IQ (I assume?). You don't need to go into editing the tool. Most cases, the problem is much simpler to solve. Also, it is acceptable to display the ROC curve, without smoothening? Unsmoothened ROC curves are acceptable in Biology field, iirc.
Also keep in mind that it is providing you with log odds. Not odds. So to make it a little more interpretable - convert log odds to odds. And to take it 1 step further, convert odds to probability. 
&gt; is acceptable to display the ROC curve, without smoothening? Yeah, this is just a personal project. The smoothing code is built into the command I was trying to edit earlier. I'm guessing at this point I need to build my own code? &amp;#x200B;
Your ROC curve will be more informative without smoothening. Consider using the unsmoothened curve. Or try using some other library for smoothening ? Smooth from pROC library seems to do what you want. Give it a shot and see if it works ? https://www.google.com/amp/s/rdrr.io/a/cran/pROC/man/smooth.html
Non Google Amp link 1: [here](https://rdrr.io/cran/pROC/man/smooth.html) --- ^^I ^^am ^^a ^^bot. ^^Please ^^send ^^me ^^a ^^message ^^if ^^I ^^am ^^acting ^^up. ^^Click ^^[here](https://medium.com/@danbuben/why-amp-is-bad-for-your-site-and-for-the-web-e4d060a4ff31) ^^to ^^read ^^more ^^about ^^why ^^this ^^bot ^^exists.
I'm not too sure what the differences are between shiny server pro and the free version, there are some options in the config that won't work for the free version but how extensive those limitations are I'm not sure. &amp;#x200B; This is the tutorial I used to get it all up. If you are needing to set this all up on your internal server and have an IT guy on hand to set it all up for you then it should be reasonably straightforward. [https://www.charlesbordet.com/en/shiny-aws-1/](https://www.charlesbordet.com/en/shiny-aws-1/)
Will do. I’ll google how to do that. Thank you
Redfin allows you to export a search via an api (though you have to “fake” act like a browser.) That is what I used to get a bunch of data on house info and prices. 
Came in expecting to see old stupid "Mine is superior" argument. Got a dozen good packages instead. Loving this subreddit so far.
It's probably predicting the negative outcome. Label your positive as 1 and negative as 0 and run it again. Either this, or the larger issue could be that there'd issues with your fit - multicollineariy between predictors, class imbalance? &amp;#x200B; Also, just because a univariate relationship is positive, when controlling for other factors this could change. Read about simpson's paradox. &amp;#x200B;
Are you using full rank encoding? If you are including all of your binary categories in the model, then weird things can happen. The model will be unbiased, however (for example) you could get an intercept of 100 with coefficients Gender.Male = 1.5, Gender.Female = 0.5, it looks like both of the coefficients have a positive impact on the model however only Gender.Male actually does. If you took Gender.Female out you would get a more intuitive result, that Gender.Male has an additional effect of exactly &lt;whatever&gt;.
Not so much a library but I think you're looking for a data set? 
Actut I was going to plug into Zillow to get data, but if you have a source to suggest I’m all ears!
Sounds like a good data source, I personally wouldn't necessarily feel competent enough to construct a model which would outperform their Zestimate, unless I had supplemental information about the area which I could use to enhance it. Sounds like a fun project though. 
I first started working with R like ten years ago or so. I don;t think proper IDEs for R were a thing yet. Things like stack overflow were not what they are now. It was pretty bad to say the least. 
Just a tip that two days of being stuck is not too bad, don't stress out about it. 
Maybe try `ftable`? It produces something similar to what you're asking for. &gt; ftable(dat, row.vars = c("Center", "Group")) Response Success Failure Center Group 1 Drug 6 4 Placebo 2 8 2 Drug 4 3 Placebo 1 5 3 Drug 5 3 Placebo 3 6 &amp;#x200B;
Thanks!
You may consider unwrapping two dimensions into one, and view the result as a 4xk heatmap, e.g.
Rstudio's changelog shows 2011 as the initial public release. I wonder if there was anything decent before that? Unlikely I think
Where are you getting the slash? It's R.
I'd be tempted to do bang bang again Following
I recommend the use of [ggrepel](https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html). You can hide the labels that you don't want and they will also not overlap woth each other.
Not exactly what you are requesting but perhaps you could create a biplot (example below using library factoextra) and select the n individuals that are most contributing. It also has a repel function like /u/cannotgameinair mentioned. plot_pca &lt;- function(data, groups, num_contributors = 10) { pca.data &lt;- log2(data) pca.data &lt;- t(pca.data) p.pca &lt;- prcomp(pca.data, center = TRUE, scale. = TRUE) pcaplot &lt;- factoextra::fviz_pca_biplot(p.pca, title = 'Biplot', label = "var", habillage = groups, addEllipses = TRUE, ellipse.level = .7, select.var = list(contrib = num_contributors), repel = TRUE) return (pcaplot) } library(factoextra) plot_pca(data, groups) 
What have you tried? What doesn’t work? Why do you think it doesn’t work?
This is impossible to do in /R. Please try R instead and let us know where you are getting stuck. Alternatively, post this on StackOverflow and provide the link so we can follow along. 
https://old.reddit.com/r/HomeworkHelp/ 
For this you would probably want to make use of `rlang::ensym()` to capture the second argument as an expression, check to see if it is either symbol or a string, and then convert to a symbol, which you can evaluate later: my_freq &lt;- function(df, group_var) { group_var &lt;- ensym(group_var) df %&gt;% count(!!group_var) %&gt;% mutate(pct_n = n / nrow(df)) } See [Advanced R 19.3.2](https://adv-r.hadley.nz/quasiquotation.html#capturing-symbols) for more info.
Thank you for the very clean and simple solution!
Thanks for the package! It's my fault for not being more clear, but my primary concern is why certain functions aren't found when I rewrite the program in the global environment. The adjust\_model function does a lot more than smoothing the curve. Is there a way to fix this?
There is no such thing in R as a "cell". The closest analog to an Excel "cell" is an element of a vector or matrix. Since there are no cells, they cannot be "empty". If the vector is numeric then each element can be finite (e.g. 27.9) or infinite (`Inf`) or unknown value (`NA`). I think you want to mark some values as unknown. You seem to have a data frame `my_dataset` with a column `Time`. Timestamp columns can be imported in a variety of formats but you have not mentioned anything about that yet. When referring to part of a data frame by indexing, there are two index dimensions separated by a comma. Your example `DF[i,j]` put an expression in the `i` place (`WSData[,3] == "A010A"`) that evaluates to a logical vector that indicates the positions of the (zero or more) rows containing `"A010A"`, but the fact that there was nothing **after** the comma means that entire rows were involved. If you also want to be specific about which column to affect then you need to not leave the `j` part empty. Something like... my_dataset[ "3/13/19"==my_dataset$Time, "sensor_reading" ] &lt;- NA though this probably won't work until you clarify how your timestamp column is defined.
There might be a trivial solution. .lilikoi\_create\_the\_model is called before it's defined in the current environment, because it is nested under lilikoi.adjust\_the\_model function. Just take out the function block for .lilikoi\_create\_the\_model and put it as a separate function, before the function block for lilikoi.adjust\_the\_model function. 
You never called staplr::staple_pdf. You need to actually call the function, I am not sure what you are doing with the other code.
What do you want to merge?
I think what you want is a TermDocumentMatrix
I did some quick fiddling with [`purrr`](), and was able to map over the teams and extract their data. library(rvest) library(purrr) library(dplyr) library(janitor) premier_url &lt;- "https://www.transfermarkt.com/premier-league/transfers/wettbewerb/GB1/plus/?saison_id=2012&amp;s_w=&amp;leihe=0&amp;leihe=1&amp;intern=0" premier_league_divs &lt;- premier_url %&gt;% read_html() %&gt;% html_nodes("div.box") %&gt;% .[5:24] map_df(premier_league_divs, function(team){ team_name &lt;- team %&gt;% html_node(".table-header") %&gt;% html_nodes("a") %&gt;% .[2] %&gt;% html_text() team_in &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[1] %&gt;% html_node("table") %&gt;% html_table() %&gt;% .[[1]] %&gt;% clean_names() %&gt;% select(-left) %&gt;% rename(player = "in", team_other = left_2) %&gt;% as_tibble() %&gt;% mutate(player = as.character(player), age = as.integer(age), nat = as.character(nat), position = as.character(position), pos = as.character(pos), market_value = as.character(market_value), team_other = as.character(team_other), fee = as.character(fee)) team_out &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[2] %&gt;% html_node("table") %&gt;% html_table() %&gt;% .[[1]] %&gt;% clean_names() %&gt;% select(-joined) %&gt;% rename(player = out, team_other = joined_2) %&gt;% as_tibble() %&gt;% mutate(player = as.character(player), age = as.integer(age), nat = as.character(nat), position = as.character(position), pos = as.character(pos), market_value = as.character(market_value), team_other = as.character(team_other), fee = as.character(fee)) team_data &lt;- bind_rows(team_in, team_out) %&gt;% mutate(team = team_name) }) It's not quite finished, as the nationalities are stored as images that you'll need to hop into the code to see if you can extract the nationality name from the metadata. Hopefully this helps!
Here's an example of somebody doing just that -- [https://www.r-bloggers.com/generating-your-own-normal-distribution-table/](https://www.r-bloggers.com/generating-your-own-normal-distribution-table/) &amp;#x200B; &amp;#x200B;
What have you tried so far?
Thank you very much for your detailed response. &amp;#x200B; My timestamp is defined as follows: &amp;#x200B; Time &lt;- as.POSIXct(strptime(sensor\_time$Time, format = "%m/%d/%Y %H:%M:%S")) &amp;#x200B; I will try working with the info that you've kindly provided so far..
This helps a lot, I just did it somehow but waaay more lines, I'll go over this and see where I was wrong. Thank you very much!
No problem. A good rule of thumb with R - if you're trying to do it, somebody has probably done it before. Half of being a good programmer in R is just knowing how to phrase your question the right way in a search engine. And I doubt you did it "wrong." There's a million ways to do everything in R. If you did it and it works then you did it right. But there's no harm in finding cleaner solutions! 
This is a massive help -- I was struggling with something that looked like this...and didn't work require(rvest) page = "https://www.transfermarkt.com/premier-league/transfers/wettbewerb/GB1/plus/?saison_id=2012&amp;s_w=&amp;leihe=0&amp;leihe=1&amp;intern=0" scraped_page &lt;- read_html(page) Team_html = html_nodes(scraped_page, '.table-header .tooltipstered:nth-child(2)') Team = html_text(Team_html) Name_html = html_nodes(scrapped_page, '//*[contains(concat( " ", @class, " " ), concat( " ", "spielprofil_tooltip", " " ))]') Name = html_text(Name_html) Age_html = html_notes(scrapped_page, '//td[contains(concat( " ", @class, " " ), concat( " ", "alter-transfer-cell", " " ))]') Age = html_text(Age_html) df &lt;- data.frame(Name, Age) head(df) I'm not very good with R yet so I may misunderstand some bits...I generally understand your code you get 2 distinct tables that is the ins and the outs and then merge them for each team. I tried to make your team_data into a dataframe df so i could see what was going on and try to finish whatever was unfinished or at least see what had happened but I got a couple weird errors around premier_league_divs I didn't understand including the code not returning any sort of dataframe with my minor addition. &gt; library(rvest) &gt; library(purrr) &gt; library(dplyr) &gt; library(janitor) &gt; &gt; premier_url &lt;- "https://www.transfermarkt.com/premier-league/transfers/wettbewerb/GB1/plus/?saison_id=2012&amp;s_w=&amp;leihe=0&amp;leihe=1&amp;intern=0" &gt; &gt; premier_league_divs &lt;- premier_url %&gt;% + read_html() %&gt;% + html_nodes("div.box") %&gt;% + .[5:24] Error in TokenStream$new(tokenize(css)) : attempt to apply non-function &gt; &gt; map_df(premier_league_divs, function(team){ + team_name &lt;- team %&gt;% + html_node(".table-header") %&gt;% + html_nodes("a") %&gt;% + .[2] %&gt;% + html_text() + + team_in &lt;- team %&gt;% + html_nodes("div.responsive-table") %&gt;% + .[1] %&gt;% + html_node("table") %&gt;% + html_table() %&gt;% + .[[1]] %&gt;% + clean_names() %&gt;% + select(-left) %&gt;% + rename(player = "in", + team_other = left_2) %&gt;% + as_tibble() %&gt;% + mutate(player = as.character(player), + age = as.integer(age), + nat = as.character(nat), + position = as.character(position), + pos = as.character(pos), + market_value = as.character(market_value), + team_other = as.character(team_other), + fee = as.character(fee)) + + + team_out &lt;- team %&gt;% + html_nodes("div.responsive-table") %&gt;% + .[2] %&gt;% + html_node("table") %&gt;% + html_table() %&gt;% + .[[1]] %&gt;% + clean_names() %&gt;% + select(-joined) %&gt;% + rename(player = out, + team_other = joined_2) %&gt;% + as_tibble() %&gt;% + mutate(player = as.character(player), + age = as.integer(age), + nat = as.character(nat), + position = as.character(position), + pos = as.character(pos), + market_value = as.character(market_value), + team_other = as.character(team_other), + fee = as.character(fee)) + + team_data &lt;- + bind_rows(team_in, team_out) %&gt;% + mutate(team = team_name) + + df = data.frame(team_data) + head(df) + }) Error in map(.x, .f, ...) : object 'premier_league_divs' not found
Did the example forget an end-parenthesis in its matrix call??
Hmm. I'm not quite sure what that error message means either. Have you installed all the packages I loaded in the header? install.packages("rvest") install.packages("purrr") install.packages("dplyr") install.packages("janitor") 
That could have been it -- though I had made sure to do so -- I installed and loaded them all again. This time it seems to have worked better I see the output of the top of the table but I don't see a dataframe appearing in my data section under global environment. Though there is a dataframe premier_league_divs that is described as a list of 20. Also for other leagues if I'm trying to modify the text beyond the link I would have to change... html_nodes("div.box") %&gt;% .[5:24] and so on to match the number of teams in the league starting at 5 correct? For the nationality images I used inspect element (don't know if that's what you meant) and each flag has some sort of code I think can be used to translate the image into a given country name like this is the first player &lt;img src="https://tmssl.akamaized.net//images/flagge/tiny/157.png?lm=1520611569" title="Spain" alt="Spain" class="flaggenrahmen"&gt; 
if you are allowed to use other functions besides seq and pnorm, the following should work as well `col_ &lt;- seq(0, 0.09, by = 0.01)` `row_ &lt;- seq(0, 3.2, by = 0.1)` `z &lt;- outer(X = row_, Y = col_, function(X,Y) round(pnorm(X+Y)-0.5,4))` `colnames(z) &lt;- col_` `rownames(z) &lt;- row_` `print(z)`
You're on the right path! `premier_league_divs` is a `list` and not a dataframe, though. It's the list of divs on the page that contain team in/out table data. The `map_df` function from the purrr package maps over a list, executes code on each list element, and returns a data_frame created in the code block. You're right about the result not being stored to the global environment, that was on me. If you change the map_df line to something like premier_teams &lt;- map_df(premier_league_divs, function(team){ you should see the premier_teams object in your global environment. Getting the nationalities from the images is a bit of a trickier problem, unfortunately. You were right that the nationalities in the table are represented as &lt;img&gt; elements (html images), but there's a slight hitch with players having multiple nationalities. Give me a minute to dive into this (I'm pretty interested in how to solve this problem now!) and I'll get back to you.
&gt; premier_teams &lt;- map_df(premier_league_divs, function(team){ That fix did work -- I was unfortunately assigning the dataframe inside the actual function instead of outside of it which is a rookie mistake. Thanks for explaining the list I need a lot of work with R. The nationalities are particularly weird because you'd have to isolate whatever changes to only the player nationality and not the column listing the team they left because there are flags present as well. Also should I want to further automate this to grab a massive number of seasons I assume I would so something with paste to alter the url inside a for loop, and to differentiate the years add a new variable like year = whatever is being used from the range at the top of a for loop..don't think that sounded very clear but I mean something to this exent for (season in 1991:2019) 
Yes it did
So I did some tinkering and figured out how to extract player nationalities from the site, using the `xml2` package: nats_in &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[1] %&gt;% html_node("table") %&gt;% html_nodes("td.nat-transfer-cell") %&gt;% map_df(function(r){ nats &lt;- r %&gt;% html_nodes("img") %&gt;% xml_attr("title") %&gt;% as_tibble() %&gt;% t() %&gt;% as_tibble() %&gt;% rename_(.dots = setNames(names(.), str_replace(names(.), "V", "nat_"))) }) You should be able to replicate that with the out table by changing the code, and place it within the function that does the other table parsing. As for running this in batches over each year, if I were you I'd wrap the map_df function from my code with another map_df, something like this: library(rvest) library(purrr) library(dplyr) library(janitor) library(xml2) library(tidyr) library(stringr) base_url &lt;- "https://www.transfermarkt.com/premier-league/transfers/wettbewerb/GB1/plus/?saison_id=" end_url &lt;- "&amp;s_w=&amp;leihe=0&amp;leihe=1&amp;intern=0" premier_teams &lt;- map_df(1991:2019, function(y) { year_url &lt;- paste0(base_url, y, end_url) year_divs &lt;- year_url %&gt;% read_html() %&gt;% html_nodes("div.box") %&gt;% .[5:24] map_df(year_divs, function(team){ team_name &lt;- team %&gt;% html_node(".table-header") %&gt;% html_nodes("a") %&gt;% .[2] %&gt;% html_text() team_in &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[1] %&gt;% html_node("table") %&gt;% html_table() %&gt;% .[[1]] %&gt;% clean_names() %&gt;% select(-left) %&gt;% rename(player = "in", team_other = left_2) %&gt;% as_tibble() %&gt;% mutate(player = as.character(player), age = as.integer(age), nat = as.character(nat), position = as.character(position), pos = as.character(pos), market_value = as.character(market_value), team_other = as.character(team_other), fee = as.character(fee)) team_out &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[2] %&gt;% html_node("table") %&gt;% html_table() %&gt;% .[[1]] %&gt;% clean_names() %&gt;% select(-joined) %&gt;% rename(player = out, team_other = joined_2) %&gt;% as_tibble() %&gt;% mutate(player = as.character(player), age = as.integer(age), nat = as.character(nat), position = as.character(position), pos = as.character(pos), market_value = as.character(market_value), team_other = as.character(team_other), fee = as.character(fee)) # Nationality scraping nats_in &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[1] %&gt;% html_node("table") %&gt;% html_nodes("td.nat-transfer-cell") %&gt;% map_df(function(r){ nats &lt;- r %&gt;% html_nodes("img") %&gt;% xml_attr("title") %&gt;% as_tibble() %&gt;% t() %&gt;% as_tibble() %&gt;% rename_(.dots = setNames(names(.), str_replace(names(.), "V", "nat_"))) }) nats_out &lt;- team %&gt;% html_nodes("div.responsive-table") %&gt;% .[2] %&gt;% html_node("table") %&gt;% html_nodes("td.nat-transfer-cell") %&gt;% map_df(function(r){ nats &lt;- r %&gt;% html_nodes("img") %&gt;% xml_attr("title") %&gt;% as_tibble() %&gt;% t() %&gt;% as_tibble() %&gt;% rename_(.dots = setNames(names(.), str_replace(names(.), "V", "nat_"))) }) team_data &lt;- bind_rows(team_in %&gt;% bind_cols(nats_in), team_out %&gt;% bind_cols(nats_out)) %&gt;% select(-nat) %&gt;% mutate(team = team_name, year = y) %&gt;% select(year, team, player, everything()) }) })
My god you've been an absolute massive help. I'm gonna run it across all the leagues I need (I think I know where changes have to be made in each case) and aside from this instance do you mind me asking how you personally learned R? I don't have much of a programming background, I learned some C a while back, and have been working through a book called Analyzing Baseball Data with R that has taught me everything I know so far and then working on separate projects to pick up what I need in regards to the work that interests me. 
Glad to help! A quick note, I tried running the code I sent you and it seems like the team tables aren't perfectly consistent across years: #&gt; Error: Table has inconsistent number of columns. Do you want fill = TRUE? I might keep looking into this, but I can't make any promises. As for the selector you're asking about: html_node(".table-header") %&gt;% html_nodes("a") %&gt;% I do some web design as a hobby, which has been really helpful in web scraping with R. The `.table-header` selector pulls the html node from the [DOM](https://developer.mozilla.org/en-US/docs/Web/API/Document_Object_Model/Introduction) in the current scope with the class `table-header`, and the `a` selector finds the hyperlink tag (i.e., `a`) in the table header to extract the team name. As far as how I learned R, I've been using it for about 4 years now in various capacities, but have been really diving into it in the past couple years or so. I started learning R in grad school where I studied biostatistics and loved it (especially as an alternative/pair to SAS). I had developed a bit of a foundation in programming from undergrad (I took a couple CS courses), which were hugely helpful. I did a bit of my learning through this sub and others, but a huge resource was [R for Data Science](https://r4ds.had.co.nz/), a book by Hadley Wickham and Garrett Grolemund. If you haven't checked it out yet, I'd definitely recommend it. It has a bias towards the `tidyverse` way of doing things (after all, Hadley is the original author of a bulk of the tidyverse) but in my opinion, it shallows out the learning curve to R pretty nicely. Other than that, all I can recommend is keep doing what you're doing! Keep finding projects you'd like to tackle in R, and in a sort of "do as I say, not as I do", I'd really recommend putting together a blog or website when you have some projects you're happy with. It'll help you both boost your exposure/portfolio of experience, and could even help out people trying to solve a similar problem.
Thanks I'll take a look for sure at that book. I reran the same code for Serie A a different league that has the same number of leagues and has everything seemingly structured the same changing only the url details to base_url &lt;- "https://www.transfermarkt.com/serie-a/transfers/wettbewerb/IT1/plus/?saison_id=" end_url &lt;- "&amp;s_w=&amp;leihe=0&amp;leihe=1&amp;intern=0" but get this error returned Error in nodes_duplicated(nodes) : Expecting an external pointer: [type=NULL]. the information, and structure seem to be the same across leagues . In regards to your issue with an inconsistent number of columns that error did not pop up for me when getting the premier league data. 
Training the package "Quanteda".
x = data[data$Group == “T”]$After
Put quotations around the T. It is looking for the non-existent variable T instead of the name T
T is an alias for TRUE.
 A few things: 1. You need quotations around the string "T". In R, T and TRUE both refer too true in the logical sense. So, your filter is looking for values that are TRUE, or it would select 1 if the vector were made up of 1's and 0's. If you put some arbitrary letter or word in there without quotations E.g., data$After[data$Group==fizz] it will give you a warning that "fizz" is not found. But, because it is looking for TRUE, you get no warning, just nothing. 2. The equal sign can be used for assignment (i.e., x = data$After.....), but it is not recommended in R style guides. (https://style.tidyverse.org) (https://google.github.io/styleguide/Rguide.xml). It is better practice to use &lt;- for assignment. 3. Generally, it is easier to read code that has space around operators. E.g., (Group == T) in this case. See style guide for more info. 4. You can do it several ways. You were very close though. You can just add quotations around the T and you are good to go. Enjoy learning R. It is frustrating at first but is a powerful tool once you get to know it better. Feel free to ask more questions like this one. x &lt;- data$After[data$Group == "T"]
Good point.
i want merge this pdfs [https://drive.google.com/open?id=1rkse0qfkKHS-o4VyqUIR4Ey7J95OmKRS](https://drive.google.com/open?id=1rkse0qfkKHS-o4VyqUIR4Ey7J95OmKRS) ,so i can mine the repeated words is it possible
If you are running gnu/linux, there is this nice bash tool that allows you to merge pdf outside R: pdftk. then it's only a matter of putting all the pdf in a directory. &gt; cd dir &gt; pdftk *.pdf output merged.pdf I don't really know the libraries you use but it seems that you already know how to load text from your files. So in R you can do &gt; setwd("/path/of/pdf") &gt; filenames &lt;- c("pdf1filename","pdf2..") &gt; for( pdf in filename){ &gt; 
But maybe your question was to have all text from a pdf in an unique cell?
Check out shiny. You can embed shiny elements in Rmarkdown which would let you do this with plotly
Can it be done without the need of using R? As far as I get, any interaction with a shiny plot require actual R code to be run somewhere (in background or on a server) and for what I'm developing I need to deliver a standalone file for an ideal guy that has not even R installed on a pc. Also, I don't have the resources/skills/help/will to run the code on an R server, that we don't have.
pull(After) instead of select(After) since he said he wants a vector 
Install R/RStudio, right click -&gt; open with. 
It's looking like you probably can do it without hosting. I'm not sure, and less confident now that I've done a bit more research. The source code from [this](https://bookdown.org/csgillespie/shiny_components/#htmlwidget-and-value-boxes) example suggests that you can run shiny elements from a single Rmarkdown file. It's hosted, however, which may defeat the purpose for you. If you have the time, I'd prowl through [this site](https://rmarkdown.rstudio.com/gallery.html). Additionally, you might want to look into setting up a free [shinyapps.io](https://www.shinyapps.io/) account. I've used mine to host more complex shiny apps for work. The limits on the free account are that you can't have more than ~5 hours of "up time" (time that the app is being accessed on any computer) over 5 different (active) apps. That is to say, if two people view your app for 5 minutes, that counts as 10 mins of up time (5 mins x 2 users). If you have a ton of people viewing the document every day, it may not be the best, but if you don't have a whole bunch of demand, it's a great free way to do it.
I've read many things regarding shiny and, despite being amazing, I cannot allow the report to have hosted elements. Basically it will be even fine to have like 7-8 pre-made plots that you browse trough a button in an html 
There is prob a better way but here is I could quickly think of: for each unique ppid_trialn: subset = bindrows(subset,filter(data, ppid_tialn == "xx")[1:266,]) 
Hmm, I have 150 unique ppid\_trialn's, so I might need something a little more automated... 
How large is the data? Would a for loop be heavy on the machine? 
Turns out tidyverse already has a solution in place (of course they do!) Check it out: https://dplyr.tidyverse.org/reference/top_n.html
I'm not sure this is exactly what i'm looking for... I'm looking to select n rows ***of*** each ppid\_trialn "value", not ***by*** a specific value. This function only allows integers as arguments to select by, and not my ppid\_trialn
They *are* text files. You can actually use them to make output files using `rmarkdown::render()` in R without having RStudio at all. But RStudio is more convenient.
&gt; n number of rows to return. If x is grouped, this is the number of rows per group. So something like: Group_by(data, ppid_trialn) %&gt;% top_n(266) 
Yeah tried this, however I had no luck. I tried this in the console and got a comment saying R was selecting by a variable of mine: `&gt; b &lt;- group_by(workingdata, ppid_trialn) %&gt;% top_n(-266)` `Selecting by StraightVisible` Then when checking each ppid\_trialn for the number of rows, they still had their original rather than 266.
I just tested on my computer and it should work as intended. Was your data grouped before? Can you try ungroup() it first? Alternatively, you can try something like this: group_by(data, ppid_trialn) %&gt;% arrange(someVariable) %&gt;% slice(1:266) &amp;#x200B;
I think the alternative will work actually. Although it's clipping rows from the end and not the start, my frame rate was 60 frames per second. So losing less than 10 frames from the end, when I'm interested in the beginning of trials probably isn't a bad idea... Thank you so much for your help! Appreciate it!
Then you can arrange rows in descending order and slice. That'll give you last 266 rows.
Yes, this should work: cleandata &lt;- olddata %&gt;% group_by(ppid_train) %&gt;% mutate(n = n()) %&gt;% ungroup() %&gt;% filter(n == 273)
 set.seed(42) library(dplyr) # make data for reprex tibble(x = 1:(10*273), ppid_trialn = rep(paste0(1:10, "_0"), each = 273), heading = -2, cameraoffset = -2, occlusion = 0, timestamp = runif(10*273)) %&gt;% sample_frac(0.999) %&gt;% arrange(x) -&gt; olddata # bit you are interested in olddata %&gt;% group_by(ppid_trialn) %&gt;% mutate(n = n()) %&gt;% filter(n == 273) %&gt;% select(-n) -&gt; cleandata 
Perfect! Thank you so much!
Thank you!
You should post your code and the errors that you're getting. If your problems go deeper than syntax, you might also need to explain a little more about what you're trying to do. Full disclosure: I don't know these packages, and I probably can't solve your problem. But I can guarantee that nobody else will be able to solve it without more information.
Have you tried htmlwidgets? http://rstudio.github.io/crosstalk/ 
I can't quite picture what you mean. Could you produce an example of the output that you're looking for?
Sorry I wrote the question incorrectly. It should have been a one-sided test. &amp;#x200B; Thank you everyone for you help!! It's been very useful. &amp;#x200B;
You can skip the mutate as well. olddata %&gt;% group_by(ppid_train) %&gt;% filter(n() == 273) %&gt;% ungroup()
Heyo. Here is [a page](https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise) of a fabulous book that will help you get going. If you're at all serious about learning r, I would just start reading it. You can skip around a bit to whatever seems useful or interesting
Check out dplyr. It's part of the tidyverse and will make R very easy for you to learn. Try something like: summary_diet &lt;- diet.dat %&gt;% group_by(diet) %&gt;% summarise(mean_gain = mean(gain), sd_gain = sd(gain)) summary_diet
What about this? library(tidyverse) N = 25 d = data.frame( v0 = rnorm(N), v1 = rnorm(N, 1, 0.1), v2 = rnorm(N, 2, 0.5), v3 = rlnorm(N, log(5), 0.2) ) getQuantileTable = function(d, vars, probs) { if(any(sapply(vars, function(s) !(s %in% names(d))))) { stop('Not all of the variables are in the data frame') } result = d %&gt;% dplyr::select(vars) %&gt;% gather(var, value, vars) %&gt;% group_by(var) %&gt;% nest() %&gt;% mutate(res = map(data, function(d) { data.frame(quantile = quantile(d$value, probs), probability = sprintf('quant_%i', floor(probs*100))) %&gt;% spread(probability, quantile) })) %&gt;% dplyr::select(-data) %&gt;% unnest() return(result) } vars = c('v1', 'v2', 'v3') probs = c(0.1, 0.5, 0.9) getQuantileTable(d, vars, probs) The output is: # A tibble: 3 x 4 var quant_10 quant_50 quant_90 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 v1 0.919 1.00 1.13 2 v2 1.39 2.04 2.67 3 v3 3.54 4.96 6.07 It is possible that there was a cleaner way, but this seems pretty functional...
If you are mostly interesting in the summary statistics you can use the `*apply` family: ?tapply # tapply takes in the data vector, the grouping vector, and a function to apply tapply(diet.dat$gain, diet.dat$diet, summary) tapply(diet.dat$gain, diet.dat$diet, mean) tapply(diet.dat$gain, diet.dat$diet, sd)
Thank you!
You could put the plots behind a {.tabset}. You could even generate the repot automatically using knitr in a script. 
&gt; You could even generate the repot automatically using knitr in a script. I'm doing exactly that. &gt; You could put the plots behind a {.tabset}. Can you elaborate a bit?
Sorry, don't have enough time to explain here at the moment, but this SO question explains the concept of tabs. https://stackoverflow.com/questions/38062706/rmarkdown-tabbed-and-untabbed-headings
no i want to load in r like this [https://drive.google.com/open?id=1O7JROOQo2yRSPLpd2b421TMlzEqjUSVW](https://drive.google.com/open?id=1O7JROOQo2yRSPLpd2b421TMlzEqjUSVW)
I like to load in R like this [ https://drive.google.com/open?id=1O9DKjkIIoOOUWS](https://www.youtube.com/watch?v=dQw4w9WgXcQ)
#MultiMachine Response - u/amigo0621 ###Rick Roll Detected &gt;---- &gt;EMERGENCY ALERT RICK ROLL DETECTED. LIE DOWN ON YOUR HANDS AWAY FROM THE DEVICE. You are under arrest. In the future remember SICKO MODE is the new rick roll. Punishment will be lifted ifyou subscribe to [PewDiePie](https://www.youtube.com/user/PewDiePie) &gt;---- Don't be a worm rate the bot! Contact the creator at /u/Rogocraft
If you have slack. Sign in to the r4datascience slack channel. I am sure someone can point you in the right direction.
&gt; {.tabset} &amp;#x200B; Dude, from the bottom of my heart, THANKS. You made my day, that's exactly what I wanted!!!
Manual error bands can be drawn with a geom_ribbon.
No problem, glad I could help! 
Your best bet is probably recreating the plot in either [vegalite](https://github.com/hrbrmstr/vegalite) or moving to python and using Altair. Altair has an API that's fairly close to ggplot, and outputs vega-lite.
I don't know anything about `plumber` other than the README example on GitHub, but it looks like your function simply needs to return an object and `plumber` will deliver it in JSON to the requestor. The `print` function output would be a side effect on your server console.
yeah, I'm think the same. But what do u mean by "recreating"? Given a ggplot object, how can I generate the corresponding vegalite or altair code? That's what confuses me.
Atomic is usually used to denote something that is not further indivisible (though it is not technically correct). In R, it denotes the simplest 6 vectors types - logical, integer, real, complex, string (or character) and raw.
Doc clarify a little further, an atomic vector is a vector that is the same type (e.g., a vector with only characters is considered an atomic vector, as is a vector with only logical elements). A list is a vector that is "recursive", it can contains different types (e.g., integer and character), or it can contain other lists (i.e., it is recursive). 
It is common to need to create new columns with the data you need to complete a plot. You have enough information to calculate your upper and lower limits, so just augment your data frame before you use ggplot. Then use the `mapping` argument to `geom_ribbon` to map those columns to `ymin` and `ymax`.
You're correct except for a small typo which might cause confusion. indivisible = not further divisible It seems like most of everybody understood. &gt; sheepishly corrects spelling (sorry)
take a look at ggplot: http://www.cookbook-r.com/Graphs/
You are too deep into domain-specific jargon and packages for me to follow, but if you understand that there are three graphing systems in R, and that the `portfolioFrontier` function appears to use base R graphics, then you need to look for a parameter that allows you to plot another graph on top of a previously-plotted graph (like `add=TRUE`). (Adding to a base R graph is like adding ink to a page... which also means you need to get your scaling right from the beginning.) If such an argument does not exist then you need to build your own plotting function. Depending on which of the three graphics systems floats your boat, you may choose to copy the existing function and creating your own modified version, or build a completely new version using `lattice` or `ggplot`. Try Googling for the terms `portfolioFrontier ggplot` for more ideas.
Try, par(mfrow=c(2,2)) For a 4 panel plot par(mfrow=c(3,3)) For a 9 panel plot Etc... 
To add to this, Use this code first, it splits your panel into the specified number, Then plot your graphs and they will appear in the order you plot them in each panel Hope this helps
&gt; par(mfrow=c(2,2)) where should I add this in my code^
Just execute it before you plot the graphs
#for a vector v #step 1: have 3 check counts for v Check1 = sum(v&gt;= 1 &amp; v&lt;= 30) Check2 = sum(v&gt;=31 &amp; v&lt;= 60) Check3 = sum(v&gt;=61 &amp; v&lt;= 70) #step2 check if 3 counts are same as required CorrectSum = (Check1 == 8) &amp; (Check2 ==2) &amp; (Check3 ==3) 
I don't use XML that often but when I do I've used the [XML package](https://cran.r-project.org/web/packages/XML/index.html). Several GB should be manageable (depends on your hardware I guess) - since R loads everything into memory first, it's typically not the most efficient, though would be the same with most high level languages. 
I may be able to speak on this a little. To my knowledge, the way works is it uses the ram available to temporarily hold the data before it actually stores it. A student in in our class attempted to extract 6gb worth of data and I believe he ran into issues and said that he may have been able to extract it if his computer had around 16gb of ram. So to answer your question, it would depend on how much ram you have available to you. So it becomes more of a resource issue instead of a “can R do it” type issue. People who know more. Pls correct me. 
I've used R and the XML package to parse 1gb XMLs into dataframes. Each took 20-30 minutes on a 2015 MacBook Pro (i5, 8gb). It didn't feel efficient at all. I also tried with xml2 but that took even longer.
As others have already said, R can do what you're asking about but really isn't the best tool for this kind of job.
What do you want to know? They are functions with special names and a convenient alternate syntax. https://cran.r-project.org/doc/manuals/r-release/R-lang.html#Special-operators https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html
That is what exactly I was looking for. Thank you.
I used to R for this with 3GB files on a machine with 32GB of RAM. As other's have said it is probably not the best tool for the job overall. However in my case it had the benefit of keeping my entire tool chain in one language which simplified maintenance and mental overhead. It was part of a much longer ETL tool chain that already took hours to run, so the extra 30 or so min from using R rather than a faster language was not a major problem compared to the initial investment of time. That being said we eventually moved to a custom solution based on a modified version of tiny-xml2. Our main bottleneck is now the disk IO speed. However this was a major project that took multiple developers about a month to develop and integrate and it is still much more brittle. The initial R script I wrote in an afternoon and it worked for years. &amp;#x200B; Questions to ask yourself: 1) Do I need to do anything else to this data in R? 2) How often do I need to do this? 3) What other tools am I familiar with can I do this with? 4) Do other people need to be able to use and understand my process? &amp;#x200B; Tl:dr It is slow but if it is part of a larger ecosystem it might still be worth it.
The p values are listed in the summary output. Any p value less than 0.2 would be significant at that alpha level.
I see! Thanks.
Library Lubridate should help. Check out the formats suported. You might have to use strreplace on the ”T”. 
R isn't great at this. Like others have said you could probably get your server or local laptop to work ok on a few Gb of data using R, but I tried to solve this problem using R on like 6Gb file and couldn't do it with 32Gb using the xml2 package. After looking at some python xml libraries I got a ~25 line python to iterate through each node in the xml and extract each piece I needed and do the thing I needed to do (insert as a row in a database table). The benefit here was that I didn't have to read in the entire XML file into RAM, I could just iterate through the XML file just like you can read in line by line of a file.
Portfoliofrontier(...) Par(...) Plot(...)
An iterator that conditionally reads specific lines can be written in r as well. Here's a few similar solutions: https://stackoverflow.com/q/37923041/7124477
Some example data might help if you have it available
Good point, i've edited and included a picture of my simplified dataframe
Or from the base package strftime and strptime
Read `?strptime`. You can try as.POSIXct( x, format="%Y-%m%dT%H:%M:%S%z" ) though the standard format for the time zone offset (`"%z"`) does not include the colon so that might not work. You might need to remove that colon by sub( ":(\\d\\d)$", "\\1", x ) Be aware that R has no native support for separate time zones for each element of a time vector... at best it will recognize what the offsets are in the character data and convert them all to your specified (via `tz=` argument) or default (via `TZ` environment variable accessible through `Sys.setenv` and `Sys.getenv`) time zone.
That's cool but I was referring to XML nodes. That seems like a bit more work to write yourself.
Not going to run all that - when you go through line-by-line, where do you get the error?
The traceback shows &amp;#x200B; `Error in \`[.default\`(P, a, ) : incorrect number of dimensions` `4.` `NextMethod("[")` `3.` `\`[.factor\`(P, a, )` `2.` `P[a, ]` `1.` `ProbTable(Attack_Def_strength, "Arsenal", "Wolves")` 
I presume you're already looked and made sure `Attack_Def_strength` is what you expected it to be. &gt; teams &lt;- rownames(Attack_Def_strength$Club) &gt; &gt; P &lt;- Attack_Def_strength$Club &gt; &gt; a &lt;- which(teams == Home.Team) &gt; &gt; b &lt;- which(teams == Away.Team) &gt; &gt; x &lt;- exp(P[a,]$'Home Attacking Strength' - P[b,]$'Home Defending Strength') This looks like where it borks up. I'd recommend going through and verifying that `teams` is what you expect, `P` is what you expect, `a` is what you expect, and `P[a,]$'Home Attacking Strength'` is what you expect. To debug, make a stripped down function that just goes through until `P[a,]$'Home Attacking Strength'` is what you expect it to be. My first guess is that your problem is the first line in the quote above. You probably just want `teams &lt;- Attack_Def_strength$Club`, but, again, I haven't downloaded your data and run your code.
Please could you just give it a try for us, the code takes about 5 seconds to run, and ill will look at it too
R handles paths like anything else: `read_csv("./a_folder/a_file.csv")`
Good lord that’s so easy. Thank you. We haven’t learned any of this. 
Just to clarify, the `./` at the beginning is utterly unnecessary. It's literally the same as leaving it off: read_csv("a_folder/a_file.csv") Also, this is the operating system's own path handling, R doesn't do anything in this case, it just passes the path on to the underlying filesystem.
Ok. But here is pain in the butt got-ya. If you are using a Windows machine, you need to specify the path separations with a "/" instead of a "\\". When you pass a string literal (the filename in the first function argument as an example), R will interpret each "\\" as an escape character and not as a path separator. So there are two options: 1. Replace every "\\" with a "/" (This is what I do) 2. Replace every "\\" with a "\\\\" &amp;#x200B;
This definitely works on data.frames, not sure about tables. In base R: `df &lt;- df[which(df$Count == 888),]` This returns the rows for which the count == 888, then subsets the dataframe for those rows and all columns. Then it redefines the df as the subsetted df. 
Ah, yes. The r iterator is definitely less efficient then.
It is true `./` is not needed but rather a stylistic carryover from Unix command lines. If you do pathing in both directions (e.g. `../` for calls to the parent directory; common in .Rmds inside projects) I feel like its nice to maintain symmetry even if not required in R or many other modern platforms.
That should work with data.table also, but is unnecessarily verbose: df[Count == 888, ] 
ok. that explains a lot, didn't know that. thanks
df [ df$Count == 888, ] You can select rows from a dataframe by providing any of: 1. An integer vector indicating row numbers 2. A logical vector indicating which to include 3. A character vector, if the rows have names Try all 3, you'll be much more comfortable after. Dont use data.table or dplyer until you've at least stepped thru vanilla syntax
Not trying to be lazy (who am I kidding) but if you just boil it down to like 5 rows and then what you expect to be returned for those rows, you might realize the solution yourself on the spot. Otherwise there's just a whole lot to read here for what I assume is a few aggregate's and joins
Just to piggyback off this a little, you can usually just copy path to save yourself some headache. On Windows you will still have to do the replacement stuff here though :(
The best longterm solution is to learn dplyr (I love it). In vanilla R you can use ave() and swap the function it takes as argument. A bit awkward.
Really... who chooses the directory separator to be the same as the escape character... Ohh yea.... never mind.
Really... who chooses the directory separator to be the same as the escape character... Ohh yea.... never mind.
Thank you so much
A little off topic but in R for Data Science (or some other Hadley text) it is strongly suggested to not change your working directory within your code as this limits the reproducibility of that code. Just something to consider. 
Huh. That’s interesting. I always use forward slashes to separate files anyway, but why would that be something specific to Windows machines?
Yeah that was a big problem I had with that option. If you run the code on any other machine you’re almost guaranteed not to have the same folders for a working directory. 
Sorry I don’t understand what this means. Would you be able to explain what you mean by “pathing in both directions”? I don’t see where the symmetry comes in with the dot and double dot. 
Some options could be to use list.files() and then paste to start with a high level dir and get specific file paths that way. Without knowing anything about what you are doing- look at tibbles and lists- my guess is many intermediate steps can be limited via utilizing those variable types. If you haven’t read R for DataScience and will be using R more, I highly encourage it. It’s free lot available and has a lot of great information. 
I actually do use list.files() already but thanks. Laerphon above mentioned that you can just enter the whole file-path to get to the file you want to read from the working directory. Mostly I’m just trying to read .txt files into a data frame. But when you don’t specify the file-path R apparently only checks the top layer of the directory for the file. Haha yeah I’m slowly getting through R for Data Science. I’m usually pretty crunched for time though so it’s been a process to get as much as I want out of it. 
Sounds good. I agree with the idea to enter the full path into the file- that’s what I do personally. 
Surprising that I’ve only ever seen examples with just the file name in the function argument. You would think that’s pretty important. Really makes me appreciate communities like this. 
\`../folder\_next\_to\_working\_directory\` &amp;#x200B; \`..\` goes up one folder. &amp;#x200B; \`../../..\` goes up 3 folders.
this works. thank you!!
Ahhhh ok I think I get it. Thank you!
Ahhhh ok I think I get it. Thank you!
Using Rstudio projects and the here packages solves this problem! Your working directory is automatically set to where the .rproj file is, so if you or someone else opens the project on another computer, the working directory is automatically set. The here package can also help with specifying paths to files that work on any operating system.
Check out the psych package. Lots methods and resources.
[removed]
"when you don’t specify the file-path R apparently only checks the top layer of the directory for the file" is false. The real secret to predictable results accessing files is to set your working directory before you run R. One easy way to do this is to always use an RStudio project. There is no benefit to using absolute paths that start from the root directory... only drawbacks.
I see what you mean, but what if I want to read from or write to a file outside of my working directory? Right now I’m mostly just reading from .txt and .csv files. I’m not sure what an RStudio project is. Would you mind explaining what that means? Is it just a folder where you keep all of the files relevant to a project? Sorry I’m coming from a pretty basic, naive programming background and a lot of what people are saying here has been new to me.
Someone else just mentioned this and I remembered your comment. What is an RStudio project? Just a special folder where everything relevant to something you’re doing gets saved? Thanks for letting me know about the here packages. I’ll look into those when I have some time. 
I try to keep my relevant data close to my analysis code... either in a subdirectory of my working directory, or in a directory "parallel" to my working directory. See the discussion of the ".." directory elsewhere in this thread. Say I have a directory for a job, or a class I am taking, called MyJob. In it I might have a TimeSeriesAnalysis directory where my R code and TimeSeriesAnalysis.Rproj files are. In another directory under MyJob I might have a "data" directory with a file called "SomeData.csv". I will run R with MyJob/TimeSeriesAnalysis as the working directory... say, by double-clicking the TimeSeriesAnalysis.Rproj file. Then my R file can simply use dta &lt;- read.csv( "../data/SomeData.csv" ) or perhaps dtadir &lt;- "../data" dta &lt;- read.csv( file.path( dtadir, "SomeData.csv" ) ) Read about RStudio projects here: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects You can even put R files in subdirectories of your working directory without changing directories. Just always work with relative file paths from your working directory no matter where your R files are and finding files should be easy.
The "official" directory separator in Windows is a backslash character (`\`), but in many places a forward slash ('/') is also accepted by Windows... but not all. R is pretty good about allowing you to use forward slashes throughout in your scripts... but the platform-independent way to build paths is to use the `file.path` function.
Thank you very much for the response! This is very helpful. I think I’ll probably have to try that a few times, but I’m starting to get a good idea of how file paths work. Again much appreciated!
Ahhh I see. I’m actually getting familiar with file.path now. At the risk of being a bit annoying for asking so many questions, what’s a good example of where a forward slash is not accepted?
I don't have the energy to ferret through all of your actors tests, but there is a useful trick for finding when something happens after something else. Find a starting condition (I didn't look closely enough to be sure this is what you actually wanted your first event to be, but you can tweak it) T1 &lt;- abs(YawRateChange) &gt; upperthreshold Note that this may happen many times during the record of data. Now accumulate occurrences X1 &lt;- cumsum( T1 ) Now back to logical, obtaining `TRUE` forever after the first occurrence T2 &lt;- 0 &lt; T1 Now you can use `T2` as a conditioning variable on your next test for whatever you want to look for after the first event T3 &lt;- T2 &amp; abs(YawRateChange) &lt; lowerthreshold and then use the `cumsum` trick again to get `T4`. Then you can use `T2 &amp; !T4` to identify records between those two events. Rinse, lather and repeat. 
Command line (`system()`).
If you are into the whole brevity thing, it will work without the leading `df$` and the comma per the data.table introduction vignette: [[link]](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html). df[Count == 888] 
You likely would want to do what patricklyngrutz suggest - aggregate the table. ```top_n()``` would be your best approach followed by a ```left_join()``` to what you already have. But, to continue down that path your on, I've modified the code (with sample data!!). It's less than satisfying because you would probably want the time of the peak_magnitude as well. library(tidyverse) ##Generate Sample Data --------- simulations &lt;- 1000 ut&lt;-qlnorm(0.99) lt&lt;-qlnorm(0.05) constants &lt;-tibble(ppid_train = letters[1:5], meanlog = c(0,0,0,0,0.1)) sample_data &lt;- constants%&gt;% group_by(ppid_train)%&gt;% do(YawRateChange = rlnorm(simulations,.$meanlog))%&gt;% unnest() ##Do the calculation -------------- sample_data%&gt;% group_by(ppid_train)%&gt;% filter(max(abs(YawRateChange)) &gt; ut, min(abs(YawRateChange)) &lt; ut)%&gt;% mutate(Peak_Magnitude = max(abs(YawRateChange)))%&gt;% slice(1:head(which(YawRateChange &gt; ut),1))%&gt;% slice(tail(which(YawRateChange &lt; lt),1))%&gt;% ungroup()%&gt;% mutate(Genuine_Response = if_else(Peak_Magnitude == YawRateChange, TRUE, FALSE)) 
This is a good point, thank you for this tip
Ah okay yes, I implemented something similar to this but not using dplyr - this is much smoother. Thank you for the tips!
This is a great tip for me to think about, thank you for this!
You don't need a which here. 
Glad you brought this up! I try to share the [here](https://github.com/jennybc/here_here) package with everyone when this topic shows up. In short, when you load the library the directory that script inhabits becomes the reference directory. Everything else is an extension off of that. Check out Jenny Bryant's readme for more. tagging u/OneMeterWonder so they see this as well. Good luck! In my experience the *here* package has always solved the problem you describe in this comment: &gt;Yeah that was a big problem I had with that option. If you run the code on any other machine you’re almost guaranteed not to have the same folders for a working directory.
Fun idea! One option would be to create a shiny app. A disadvantage of this would of course be that users would not be familiarized with the R/RStudio interface. But in terms of structuring a storyline with text output and visualizing it, Shiny is great. All 'backend' code still runs R and you have more options on how to present it to the users. If you need help with anything you can send me a pm.
That sounds great! I’ve been looking into shiny. Would a shiny app have space for users to write their own code? My idea is for a non-linear narrative where users are presented with data and have to explore it through R to come to a conclusion (a bit like the game Her Story).
maybe take a look at how "swirl" courses work and build your story through that? kind of like a choose-your-own-adventure through the R console? https://github.com/swirldev/swirl 
No. Sounds like you missed some syntax elements in your UI.
Yep! A bit hacky but try this library(shiny) # A predefined list of valid R functions stat_functions &lt;- lsf.str("package:stats") # base_functions &lt;- lsf.str("package:base") ui &lt;- fluidPage( # Application title titlePanel("L.O.G.I.C. Database"), helpText("You are logged in, AUTH_GUEST"), helpText("Enter query to access archived footage."), # This is where the user inputs their code sidebarLayout( sidebarPanel( textInput("user_code", label = "Terminal", value = "rnorm(10, 0, 1)"), actionButton("run_code", "Run code") ), # Function output as text mainPanel( helpText("Calculated output:"), textOutput("user_function") ) ) ) # Define server logic required to draw a histogram server &lt;- function(input, output) { observeEvent(input$run_code, { # Get the user input code code &lt;- input$user_code # Extract the function (assuming it's expressed as function_name(arg1, arg2...)) function_name &lt;- gsub('\\(.*', '', code) # Can user function be found in predefined list of accepted functions? if (function_name %in% stat_functions) { # Execute 'code' string as a function output$user_function &lt;- renderText({ eval(parse(text=code)) }) } else { "Not a valid function." } }) } # Run the application shinyApp(ui = ui, server = server) 
Why didn't you try something like a treasure hunt, with a lot of cryptic clues on the way.
well, I’d like to so something like that; I’m trying to figure out how to present these clues to my users
Thanks. Here is what I found. &amp;#x200B; '''{r} chunk ''' ========================== &amp;#x200B; Will not result in a new page. An extra space must be added between the end of the chunk and the equals signs if you want a new page.
An example output would be nice.
Show your code
Should be pretty simple, could you post your code as text rather than an imagine so I/we can play around with it?
edited post. should be there.
Brill, thanks! I personally would go with a solution using the purrr package, as that comes with the map_dfr function which will post all the results of each individual page scrape into a single data frame, but I’m sure someone else will come up with another solution. I’m on mobile atm as it’s approaching midnight for me, but happy to post a working solution along the lines of the above in the morning. Been playing with scraping a bit at work so this is some good practice for me if nothing else!
 &gt; mlb &lt;- read.csv(file="MLB.csv",head=TRUE,sep=",")
What's your working directory for the R session? Unless your working directory contains the file, you're going to need to specify the file location when using read.csv
Let me know what you think. I know almost nothing about sports. Is this data supposed to be mainly pitchers? Also, are pitchers the highest paid players? **Source Code** --- library(tibble) library(readr) library(ggplot2) library(RCurl) library(gridExtra) raw_csv &lt;- RCurl::getURLContent( url = "https://www.draftkings.com/lineup/getavailableplayerscsv?contestTypeId=28&amp;draftGroupId=25456" ,binary = FALSE ) dat &lt;- readr::read_csv(raw_csv) p1 &lt;- ggplot(dat) + geom_boxplot(aes(x = `Roster Position`, y = Salary, fill = `Roster Position`), show.legend = FALSE) p1 p2 &lt;- ggplot(dat) + geom_histogram(aes(x = Salary, fill = `Roster Position`), bins = 20, show.legend = FALSE) p2 p3 &lt;- p2 + facet_wrap(`Roster Position` ~ .) p3 gridExtra::grid.arrange( p1, p2, p3, layout_matrix = matrix(c(1, 1, 1, 1, 2, 2, 3, 3), nrow = 2, ncol = 4, byrow = TRUE)) --- **Figure**: [Example Composite Plot](https://i.imgur.com/Fmmg7zm.png)
that would be awesome, I'll take a look at that package and see what I can do as well. thanks man!
Absolutely incredible, that's perfect. So let me see, I can substitute the url in the script with a new one each day and it'll update the display with the new information? Where do I get that specific url? I understand the rest of the script, that seems fairly self-explanatory. What does the RCurl package do exactly? I mean yeah, pitchers are going to be a large portion of the data. Maybe it's best to bin them in their own category? It seems as if they're setting a scale that's a little inflated for the rest of the data. The issue with the site is that they take the entire roster of a given team and that includes the "bullpen," the entire troop of pitchers available. The unique thing I'm noticing is that the RP's (relief pitchers, the guys who come in later in the game as support) are dragging the mean down, thus leaving the SP's (starting pitchers, the guys pitching the majority of the game) well above the final quartile. Quick and dirty about salary—the site has a minimum salary for players ($2500) and thus players who are unlikely to play on any given day get delegated down to minimum salary. Potentially, you can almost say "minimum price? Probably a healthy scratch" and then go from there. Would I do something like Salary &gt; 2500 at the end of the graphing scripts if I wanted to clean out the "inactives"? 
I just created this plot of [Salary vs Avg Points per Game](https://i.imgur.com/cmYcTCg.png), and it was very enlightening. Below a certain threshold, there is little correlation between salary and points, but above the threshold, the correlation is highly linear. &gt; Absolutely incredible, that's perfect. Thank you. It was fun to work up. &gt; So let me see, I can substitute the url in the script with a new one each day and it'll update the display with the new information? Yes, that's correct. &gt; Where do I get that specific url? I'm not really sure. I just clicked through to the link that you posted, then grabbed the URL. &gt; I understand the rest of the script, that seems fairly self-explanatory. What does the RCurl package do exactly? The RCurl package is used to actually download the data, which occurs in this code segment. raw_csv &lt;- RCurl::getURLContent( url = "https://www.draftkings.com/lineup/getavailableplayerscsv?contestTypeId=28&amp;draftGroupId=25456" ,binary = FALSE ) RCurl is incredibly powerful. In this case, other than the `binary = FALSE` option, there are no other special options because this URL does not require authentication. In cases where you have to authenticate with an API Key as part of the HTTP header and not the URL line, the call can get a little more complicated, but it's still not bad. RCurl is a wonderful package. &gt; I mean yeah, pitchers are going to be a large portion of the data. Maybe it's best to bin them in their own category? It seems as if they're setting a scale that's a little inflated for the rest of the data. Yeah, but the pitcher OF (is that outfielder?) and C (is that centerfield?) data is 76% of the data. position | count | cumsum | cumsum_percent ----| ----| ---- | ---- | P |526 | 526 | 0.4915888 | |OF |206 | 732 | 0.6841121 | |C |83 | 815 | 0.7616822 | |SS | 54 | 869 | 0.8121495 | |1B | 41 | 910 | 0.8504673 | |2B | 32 | 942 | 0.8803738 | |3B |32 | 974 | 0.9102804 | &gt; The issue with the site is that they take the entire roster of a given team and that includes the "bullpen," the entire troop of pitchers available. The unique thing I'm noticing is that the RP's (relief pitchers, the guys who come in later in the game as support) are dragging the mean down, thus leaving the SP's (starting pitchers, the guys pitching the majority of the game) well above the final quartile. Interesting. &gt; Quick and dirty about salary—the site has a minimum salary for players ($2500) and thus players who are unlikely to play on any given day get delegated down to minimum salary. Potentially, you can almost say "minimum price? Probably a healthy scratch" and then go from there. Would I do something like Salary &gt; 2500 at the end of the graphing scripts if I wanted to clean out the "inactives"? That's very clear in the plot above.
With this data set, I didn't even bother saving the CSV locally. I used RCurl to download it directly and readr::read_csv to convert it to a tibble. My source code is shown in a comment below.
&gt; Would I do something like Salary &gt; 2500 at the end of the graphing scripts if I wanted to clean out the "inactives"? I realize that I didn't answer your question. You can use `dplyr` to slice and dice your data set. For example, I used the following code to extract the records for 4 players who appear to be outliers in terms of being paid at rates that are far above their performance relative to others. I'm curious if you had any insights as to why these players might be so highly paid? dat3 &lt;- dat %&gt;% filter(Salary &gt; 7500, AvgPointsPerGame &lt; 10) Position| Name + ID | Name | ID | Roster Position | Salary | Game Info|TeamAbbrev|AvgPointsPerGame ---| ---- | --- | --- | --- | --- | --- | --- | --- | 1|SP|Kenta Maeda (12259073)|Kenta Maeda|12259073|P|9300|ARI@LAD 03/28/2019 04:10PM ET|LAD|8.94 2|RP|Tyler Glasnow (12259510)|Tyler Glasnow|12259510|P|8700|HOU@TB 03/28/2019 04:00PM ET|TB|7.5 3|RP|Collin McHugh (12258999)|Collin McHugh|12258999|P|8400|HOU@TB 03/28/2019 04:00PM ET|HOU|5.19 4|SP|Danny Salazar (12258558)|Danny Salazar|12258558|P|8100|CLE@MIN 03/28/2019 04:10PM ET|CLE|0 
Hey! Writing you a DM currently, didn't mean to leave you hanging. But those are pitchers, which are typically higher-salaried players due to more opportunities to do baseball actions that produce fantasy points. While batters get 3-4 "at bats" aka opportunities, pitchers can have nearly 100 pitches to strike out batters and do other valuable things. I think the points per game as of now are calculated based off of preseason stats, which are fairly misleading because pitchers won't play nearly as much as they would in a typical game. I'll be contacting you shortly!
This sounds pretty cool, would love to try it out!
Man, this would have been so easy if you only had 1185 entries... you are so out of luck. Actually, you are painting an abstract image with your words and it is totally lost on me... give up on the 1184 entries and make a reasonably-sized reproducible example in a suitably concrete language like R... specifically, give us the sample input data and output data in R syntax and someone may try to put code in between them.
Attributes?
I believe ks.test is only compatible with continuous distributions which poisson is not. So normal dist should work: Fitting.Lognormal &lt;- fitdistr(Event.Intensities, densfun = "lognormal") KS.Lognormal &lt;- ks.test(Event.Intensities, "pnorm", mean = Fitting.Lognormal$estimate[1], sd = Fitting.Lognormal$estimate[2]) As for logistic, could you try: Fitting.Logistic &lt;- fitdistr(Event.Intensities, densfun = "logistic") Fitting.Logistic.Location = Fitting.Logistic$estimate[1] Fitting.Logistic.Scale = Fitting.Logistic$estimate[2] KS.Logistic &lt;- ks.test(Event.Intensities, "plogis", location = Fitting.Logistic.Location, scale = Fitting.Logistic.Scale) 
This mostly works, but on the third iteration (I.E. when start position is 40) we get back an error because the "content" contains fewer records than "date" and "review". Also note that I have had to make some helper variables (clean\_date and clean\_review) as you were running into an issue at the end of the loop where the length of the initial sequence of the loop was now longer than the number of character strings to be looped through after the bad dates had been removed. library(tidyverse) library(rvest) #list all the combinatios of start position, either by writing them out or through a loop of some kind STARTS &lt;- c(0, 20, 40) #extend as needed yelp_scrape &lt;- function(STARTS) { yelp_html &lt;- read_html(paste0("https://www.yelp.com/biz/madtree-brewing-cincinnati-3?start=", STARTS),"div.review-list") yelp_html_2 &lt;- html_nodes(yelp_html,"div.review.review--with-sidebar") # Reviewer locations location &lt;- html_text(html_nodes(yelp_html_2,"li.user-location.responsive-hidden-small")) str_replace_all(str_replace_all(location,"\n","")," ","") # reveiw content content &lt;- html_text(html_nodes(yelp_html_2,"div.review-content &gt; p")) # Review dates date &lt;- html_text(html_nodes(yelp_html_2,"div.biz-rating.biz-rating-large.clearfix &gt; span")) date &lt;- str_replace_all(str_replace_all(date,"\n","")," ","") # Review Rating review &lt;- html_nodes(yelp_html_2,"div.i-stars")%&gt;% html_attr("title") # Identify and indicate if a rewview is an update to a previous review clean_date &lt;- date clean_review &lt;- review for(i in seq_along(date)) { if (str_detect(date[i],"Previous review") == TRUE) { bad_date &lt;- i clean_date &lt;- date[-bad_date] clean_review &lt;- review[-bad_date] } else{"do nothing"} if (str_detect(date[i],"Updated review") == TRUE) { clean_date[i] &lt;- str_replace(date[i],"Updated review","") }else{"do nothing"} } return(data.frame(clean_date, location, clean_review, content)) } yelp_output &lt;- map_dfr(STARTS, yelp_scrape) &amp;#x200B;
Found the answer:" On mobile, forgive formatting. Assuming the vectors are of equal length: mat = matrix(all_entries, ncol = N) df = as.data.frame(mat) Unless I’m misunderstanding your question. Let me know." Anyhow thanks for the help!
I've tried to replicate your Salary v. Avg Point chart with `ggplot(dat, aes(AvgPointsPerGame, Salary, color = \`Roster Position\`)) +` `geom_point() +` `geom_smooth(method = lm)` but came away many smooths could you tell me what I'm doing wrong? Also is there any way to do pop ups in ggplot2 without plotly?
&gt; *...but came away many smooths could you tell me what I'm doing wrong?* I can't test it right now, but I think the problem is with you adding the parameter `method = lm` for `geom_smooth`. I used the default, which is a *loess* smooth. &gt; *is there any way to do pop ups in ggplot2 without plotly?* Do you mean that you want to create an interactive plot? If so, the only two options that are easy is to use either *Shiny* or *Plotly*.
If you are using data.table: table[ColumnA == ColumnB,] If you are not: table[table$ColumnA == table$ColumnB,]
Could also look into dplyr::setdiff()
I use something like table %&gt;% filter(ColumnA %in% table$ColumnB) But it’s probably not as fast as the base or data.table methods.
 intersect(unique(table$ColumnA), unique(table$ColumnB))
I am thinking that this would only show you the rows where A and B matched exactly. I think the OP was curious about what are the overlapping values that could be found in A and B.
I think you're right. This is why I don't like to answer questions that don't have a reproduceable example.
I think you're looking for tidyr::separate()
 separate(mydf, Roster Position, c(“1B”, “2B”, “3B”, “C”, “OF”, “SS”, “P”) , sep = " ", remove = TRUE, convert = FALSE) Is this what I'm looking for? It seems like that isn't working 
How about... library(tidyverse) library(readxl) filename = 'c:/Users/---------/Desktop/Example.xlsx' d = read_excel(filename, sheet = 'Original Dataframe') positions = c('1B', '2B', '3B', 'C', 'OF', 'SS', 'P') d_new = d %&gt;% dplyr::select(Name, Salary, TeamAbbreviation, `Game Info`, AvgPointsPerGame, `Roster Position`) %&gt;% mutate(positions = map(`Roster Position`, function(pos) { df = data.frame(position_name = positions, value=0) tmp_positions = str_split(pos, pattern = '/')[[1]] for(p in tmp_positions) { df$value[which(positions == p)] = 1 } return(df) })) %&gt;% dplyr::select(-`Roster Position`) %&gt;% unnest(positions) %&gt;% spread(key = position_name, value = value) &amp;#x200B;
This is my current script library(tibble) library(readr) library(ggplot2) library(RCurl) library(gridExtra) library (tidyverse) raw_csv &lt;- RCurl::getURLContent( url = "https://www.draftkings.com/lineup/getavailableplayerscsv?contestTypeId=28&amp;draftGroupId=25456" ,binary = FALSE ) dat &lt;- readr::read_csv(raw_csv) p1 &lt;- ggplot(dat) + geom_boxplot(aes(x = `Roster Position`, y = Salary, fill = `Roster Position`), show.legend = FALSE) p1 p2 &lt;- ggplot(dat) + geom_histogram(aes(x = Salary, fill = `Roster Position`), bins = 20, show.legend = FALSE) p2 p3 &lt;- p2 + facet_wrap(`Roster Position` ~ .) p3 gridExtra::grid.arrange( p1, p2, p3, layout_matrix = matrix(c(1, 1, 1, 1, 2, 2, 3, 3), nrow = 2, ncol = 4, byrow = TRUE)) dat3 &lt;- dat %&gt;% filter(Salary &gt; 2500) Would I want to just copy paste from "positions" and down and then change d to dat?
Sure. That sounds like it might work &amp;#x200B;
It didn't error out, that's a good start haha. What do I call to see the new dataframe? I did df, d_new, dat and none of em worked. I did view, header, data.frame, etc. 
I am not sure about your plotting code. It might be good to just sketch out what you would like the plots to look like and show that here. &amp;#x200B;
https://i.imgur.com/Fmmg7zm.png This is what I had going on. Essentially what I'm trying to do is reclassify the data in the classes with "/" in them and put that data in the separate components. I effectively only want 1B, 2B, 3B, C, OF, SS, and P (7 classes). 
I mean you could create the new colums from scratch: df$`1B` &lt;- ifelse(grepl(pattern = "1B", x = df$classes), 1,0) Something like that
Along with my code I posted? And would I individually do each position in that case, substituting 1B with the other classes I want each time?
This problem is easier to solve with vectors, as opposed to dataframes. &gt; col_A &lt;- c(1, 2, 3, 4, 7) &gt; col_B &lt;- c(1, 2, 5, 6) &gt; &gt; intersect(col_A, col_B) [1] 1 2 &gt; setdiff(col_A, col_B) [1] 3 4 7 
Your last one missed the 6...
This came in handy! Thank you!
 Error: unexpected numeric constant in "df$1"
This will add the columns for each position to your data frame. just change df to whatever the name you are using for the data frame in your current code. positions&lt;-c("1B","2B","3B","SS","C","P","OF") for(pos in positions) df[pos] &lt;- as.numeric(stringr::str_dectect(df["Roster Position"], pos))
&gt; for(pos in positions) Error: 'str_dectect' is not an exported object from 'namespace:stringr' What happened? My active libraries: library(tibble) library(readr) library(ggplot2) library(RCurl) library(gridExtra) library(stringr)
Oops. That should be str_detect 
Ooooh Warning messages: 1: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 2: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 3: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 4: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 5: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 6: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing 7: In stri_detect_regex(string, pattern, negate = negate, opts_regex = opts(pattern)) : argument is not an atomic vector; coercing
I thought str_detect was vectorized, but I guess not. This should work, I tested it with your code and it does what I think you want to do. for(pos in positions) dat[pos]&lt;-sapply(dat['Roster Position'], function(x)as.numeric(str_detect(x, pos)))
/u/TheCreamsman has a perfectly fine answer... but here's another for no particular reason: A &lt;- c(1, 2, 3, 4, 7) B &lt;- c(1, 2, 5, 6) # Get matches A[which(A %in% B)] # Get non-matches A[which(!A %in% B)] # Get all non-matches, named by their original column non_match &lt;- c(A = A[which(!A %in% B)], B = B[which(!B %in% A)]) # If you don't want index numbers in the names: names(non_match) &lt;- substr(names(non_match),1,1)
That's because the backticks are missing. If the column name isn't in a standard form (doesn't start with a letter or uses special symbols) you can still use it, but enclosing it in ` `
 # Create data frame df &lt;- data.frame("col1" = runif(100, 10, 20), "col2" = runif(100, 50, 60), "canopy_area" = runif(100), "canopy_group" = c(rep("A", 50), rep("B", 50))) # Create two additional columns with values from canopy area depending on its group df$canopy_areaA &lt;- ifelse(df$canopy_group == "A", df$canopy_area, NA) df$canopy_areaB &lt;- ifelse(df$canopy_group == "B", df$canopy_area, NA) library(dplyr) library(tidyr) # Drop the original canopy area and group columns df &lt;- dplyr::select(df, -c(canopy_area, canopy_group)) # Reshape dataframe df &lt;- gather(df, factor_key=TRUE, na.rm = T) # Summarize averages and SD for each column/group df &lt;- df %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value))
 # Using the excellent data.table package library(data.table) # Example using the built-in iris data set data(iris) dt &lt;- data.table(iris) # Reshape the data into long format. Replace Species with your Canopy variable dt &lt;- melt(dt, id.vars = c("Species")) dt[, .(mean = mean(value), sd = sd(value)), by = .(variable, Species)]
Just to make sure, did you do mydata$columnname&lt;-as.character(mydata$columnname) ? If you already tried that and it didn't work, would you mind sharing the codes that you wrote so I could get a better sense of it? 
No, I did not. Thanks mate, thanks a ton. No clue why I couldn't find that simple line anywhere. Saved me so much trouble &lt;3
Another option: lst &lt;- list( col_a = c(1, 2, 3, 4, 7), col_b = c(1, 2, 5, 6) ) olap &lt;- Reduce(intersect, lst) delta &lt;- Map(setdiff, lst, list(olap)) &amp;#x200B;
Try `as.character(mydata)[,x]` The \[,x\] would specify the xth column whereas \[x\] would specify the xth element of a vector
I think that it would be best to avoid this syntax. This would first convert all of the columns to character and then would subset that result for the correct column. The initial cost of converting the entire table might be expensive. I would go with the result that @hknd23 provided as that only converts what is needed.
Look at the `sep` argument of `separate()`.
Happy to help if you share the code
Huh good to know. I'm typically guilty of ignoring the computational costs of different methods... 
Perhaps this would help? https://rdrr.io/cran/RAdwords/
Posting the errors would be the first step
Try lubridate::mdy() to convert to datetime. The package requires some learning but it pays dividends if you need to work with time data
u/hknd23 already gave the right answer, but you say "all available options", so... &amp;#x200B; If you have a column or vector of class numeric, converting only one element to character will cause all the column elements will be coerced to character. An example: &amp;#x200B; `# Create dataframe` `mydf &lt;- data.frame(` `col1 = c(1, 2, 3, 4, 5),` `col2 = c(6, 7, 8, 9, 10)` `)` &amp;#x200B; `# Check column classes` `str(mydf)` `'data.frame': 5 obs. of 2 variables:` `$ col1: num 1 2 3 4 5` `$ col2: num 6 7 8 9 10` &amp;#x200B; `# Change first observation of first column to character` `mydf[1,1] &lt;- "1"` &amp;#x200B; `# Check column classes again` `str(mydf)` `'data.frame': 5 obs. of 2 variables:` `$ col1: chr "1" "2" "3" "4" ...` `$ col2: num 6 7 8 9 10`
Daaamn, thank you! :D
Posting the code and the errors would be the first step
We run R scripts in bash using a variety of automation tools, like Airflow and Jenkins. Basically whenever we need a data pipeline, R might be the right tool for the job. But Python and SQL and bash utilities are sometimes better, among many others.
Well it's not very simple in a way, you'll have to keep behind the new versions in the servers and change only once a year for example. You'll be now working on 3.3.3 Maintaining package/dependecy is also complicated, you need to have sepreated package directory by users, systems, and you go back very offen to source to install older versions. You can't give permission to everyone to install system package. R&amp;D should be done separatly and Everytime you want to upgrade, you need to test it to evaluate impact on all your process before pushing. Other than this, it's quite the usual stuff. 
I get data from SQL or elsewhere and export it into a .txt or .csv. then read it into R, perform analysis. Repeat. &amp;#x200B; For data pipelining or inserting into SQL I find Python to be better and faster. R is great for the statistical packages.
&gt; maintain all of their scripts Version control systems. Typically git or mercurial. &gt; automated processes Depends on how automated you need it to be. A small start-up might do everything in terms of crontabs but a large enterprise will probably have some sort of scheduling software available internally. 
Use `case_when()` for this. mutate(df, type = case_when( count==20 ~ "tiny", count==21 ~"small", count&gt;=22 &amp; count &lt;=24 ~ "medium", TRUE ~ "large" )
&gt;mutate(df, type = case\_when( count==20 \~ "tiny", count==21 \~"small", count&gt;=22 &amp; count &lt;=24 \~ "medium", TRUE \~ "large" ) Hey thanks for the reply &amp;#x200B; I got "Error in mutate\_impl(.data, dots) : Evaluation error: comparison (4) is possible only for atomic and list types." &amp;#x200B; What does that mean, sorry i am a noob?
&gt;mutate(df,type = case\_when(count==20 \~ "tiny",count==21 \~"small",count&gt;=22 &amp; count &lt;=24 \~ "medium",TRUE \~ "large") Hey thanks for the reply I got "Error in mutate\_impl(.data, dots) : Evaluation error: comparison (4) is possible only for atomic and list types." What does that mean in this context?, sorry i am a noob
Also as an extension to this - I have having another, separate issue, how would i fix this? &amp;#x200B; mutate(df, price = ifelse(&gt;=0){ "OP" } else \[ "UP" \]) &amp;#x200B; &amp;#x200B;
Make sure you have a variable named `count` inside `df`. If you do not, it will fail to find `count` inside `df` then look in a higher level environment, where it will find the `count()` function and try to test against it. It will fail. This is only one possible error, however. Do a `head(df)` and show me the results.
What do you want to test is greater than or equal to 0? `ifelse()` has different syntax than `if`. Say you want this to work testing against the variable `x`, it would be like this: mutate(df, price = ifelse(x &gt;= 0, "OP", "UP") This says, in dataframe named `df`, make a new variable called `price` equal to the result of thise `ifelse()` function. `ifelse()` checks to see if `x` is greater than or equal to zero. If this statement is `TRUE`, it returns `OP` if not, it returns "UP", if the statement is `NA` it returns `NA`. Whatever is returned is assigned to `price`.
Out of curiosity what Python packages do you use for pipelining with SQL? We know Python should be better at it but we’ve actually found R to be quicker and more reliable for interacting with SQL Server. 
I can't recall atm as it is on my work computer and I don't use it extensively. I don't think the data at my company is robust enough to really call it true pipelining so for what I have to handle Python/R would both suffice pretty well. True pipelining, if you will, is in the works for me to get into soon... &amp;#x200B; I just find Python more versatile for working with a computer as a whole rather than R so I generally prefer it for more data collection tasks. We have a lot of data still in Excel at my company so I have to grab that stuff and push it to SQL or other places quite often... &amp;#x200B; What packages do you use in R for your data pipeline tasks?
We use the odbc/dbi packages for importing to SQL Server and the RODBC for exporting. We’ve had best luck doing it that way. 
Perfect!
Perfect found the issue!
There is a lot that CAN go into maintaining multiple dashboards and reporting systems. I will go into a little bit of what I have set up for clients. * The first step is probably setting up a project, and using git with that project. * Next up is what is connecting to the data source. MySQL/FTP/API/etc. * Are these running in a cron job to save static reports, or pulled on demand? If they are serving static reports, then you can just have it save to an nginx directory, or even plumbr endpoint. If it is dynamic you can use Shiny or implement your own system. I implement my own system using R as a websocket server for that realtime goodness and constant communication on what task it is on. Clients seem to love knowing what step it is on (pulling AdWords data, aggregating, joining, etc). * Are there multiple projects on the same server? If so, use Docker. I use Portainer to manage docker containers. 
Why isn't it letting me view() dat or dat[pos]? 
The R gurus here might provide a more elegant solution but I'd do something like the following: for (position in unique(df['Roster Position'])) { df[position] &lt;- ifelse(df['Roster Position'] == position, 1, 0) } You'll probably wanna do some error handling, in case there's a value in `Roster Position` that doesn't necessarily have a corresponding column in your dataframe.
Hmm, didn't error out, but it did do 1's across the columns in the first row, but every other value in those columns are 0
try this: for (position in unique(as.character(df['Roster Position']))) { df[position] &lt;- ifelse(as.character(df['Roster Position']) == position, 1, 0) } your `Roster Position` column might have been read in as a Factor instead of a string. Also I just noticed you have some values like "1B/3B," which I assume means you want both of those columns to be 1 in that row. You'll have to tweak what I gave you a little bit in that case. You'll have to separate `position` by "/" using something like [strsplit](https://www.dummies.com/programming/r/how-to-split-strings-in-r/).
[This is weird.](https://imgur.com/a/phMKiIj) The first is the first code, the second is the second. I'm gonna send you my script to see if there's a bad no-no somewhere
Do you use R server or R Desktop in your local machine? 
There's no need to use which() in this scenario. The logical vector be used can pull the values without having to use which to get the numeric indices. A[A %in% B]
I use pyodbc and Cx_oracle a lot, to pipe data from various DB’s into other production environments. I do use R too, but purely for EDA and modelling - what do you use in R for pipe-lining? Would be keen to give it a go!
ETL - pipe data from various sources, clean and transform into beautiful data frames EDA - R’s greatest strength, exploring data, creating visuals and modelling - includes statistical testing, machine learning and outputs for presenting your work Now - at this point in my workflow I generally build the production model with Python, because I can write pipelines, model and web UX all with Python; which is good for collaborating with other teams in the process. 
See my response below. I meant to reply to this comment, but posted it as it’s own instead. 
dat %&gt;% mutate(\`1B\` = ifelse(grepl("1B", Position), 1, 0), \`2B\` = ifelse(grepl("2B", Position), 1, 0), \`3B\` = ifelse(grepl("3B", Position), 1, 0))
``` dat %&gt;% mutate(`1B` = ifelse(grepl("1B", Position), 1, 0), `2B` = ifelse(grepl("2B", Position), 1, 0), `3B` = ifelse(grepl("3B", Position), 1, 0)) ```
Or try this too see if this works for what you need: ``` dat %&gt;% separate(Position, c("pos_1", "pos_2"), sep = "/") %&gt;% mutate(pos_1_val = 1, pos_2_val = 1) %&gt;% spread(pos_1, pos_1_val, fill = 0) %&gt;% spread(pos_2, pos_2_val, fill = 0) %&gt;% select(-`&lt;NA&gt;`) ```
Got it! Thank you! Did you see my most recent post? Maybe you have an idea for that
char_cols = supply(df, is.character) fill_na = function (x){x[is.na(x)]="" x} df[char_cols] = lapply(df[char_cols], fill_na)
I'm looking at it. &amp;#x200B; I would highly recommend to you to struggle through it and really try to figure it out on your own. The only reason I know how to do anything in R is I have spent countless hours totally confused just trying stuff over and over and over. It sucks at first, but eventually it makes sense.
``` dat %&gt;% mutate(Position = ifelse(grepl("RP|SP", Position), "P", Position)) %&gt;% separate(Position, c("pos_1", "pos_2"), sep = "/") %&gt;% mutate(pos_1_val = Salary, pos_2_val = Salary) %&gt;% spread(pos_1, pos_1_val) %&gt;% spread(pos_2, pos_2_val) %&gt;% select(-`&lt;NA&gt;`) %&gt;% select(9:15) %&gt;% gather(position, salary) %&gt;% ggplot(aes(x = position, y = salary)) + geom_boxplot() ```
Try opening a 100GB cvs in Excel
Many statisticians/data scientists don't know SQL and eschew Excel, but do know how to use R and Pandas. In R the [tidyverse](https://www.tidyverse.org/) combined with [RMarkdown](https://bookdown.org/yihui/rmarkdown/) allow non-developers to make reproducible reports or even [Shiny](https://shiny.rstudio.com/) websites, really easily to make their work reproducible as you note and easy to reach a wide audience. There are some equivalent tools in Python for reproducible reports such as [pweave](http://mpastell.com/pweave/) and the more recent [codebraid](https://github.com/gpoore/codebraid). But I don't find them as rounded as RMarkdown (obviously you can integrate with Django in Python as an alternative to Shiny). Then there are the statistical routines for modelling in both R and Python that you'd be hard pushed to do in SQL or Excel. These are the real appeal, being able to do all your data preparation in the same software is a bonus. As a developer you can do things using the tools you know, but if you end up collaborating with anyone who doesn't know how to use the tools you use it will to some extent hinder progress. You are probably going to pick up R/Pandas faster than someone else can learn SQL. Thats not to say they wouldn't benefit from doing so, as it is very useful to know too, just you're probably going to be quicker.
Also, if I would have known originally you wanted to do boxplots, I would have not done the same stuff above. &amp;#x200B; I actually used the salary instead of 0 or 1 because it made more sense to me with the final step of making a box plot. The code is in the other post and it is different than what I put here.
Okay that's true But once opened - what do you do with it that you can't do in SQL? I mean, what kind of analysis and business conclusions?
Thanks! Language and reports aside, what kind of analysis do data scientists do that will give business value to my boss? (That I can't do with SQL/Python/Excel) I mean, i'm on datacamp learning how to read/write to a dataframe, and filter by column. When does it actually provide more business value that Excel/SQL can provide? 
A predictive model? 
Can SQL run machine learning algorithms on the data?
Basically any complex analysis is not really possible in Excel. I would suggest you go to Kaggle and pick out a competition you find interesting, e.g. predicting sales given info about weather, customers, etc. Look at the notebooks explaining what people did to create their predictions, and try to replicate that in Excel. You won’t be able to.
My bad, I didn't update my script. "dat" is now "xxx" and it combined RP and SP already. I cut out mutate(Position = ifelse(grepl("RP|SP", Position), "P", Position)) %&gt;% and it gave an error of Error: `var` must evaluate to a single number or a column name, not an environment Call `rlang::last_error()` to see a backtrace If I'm right, what I'm looking at is mutate to combine RP and SP into P, separate to split players with two positions into both of their roles, the pos_1 and pos_2 are the products, and spread puts them in the table, right? It seems like most of that is accomplished by my updated script, but when I reduce what seems redundant, it errors out. I'm trying to work my way through this by identifying the roles of each function
Okay fine - but how do I explain to my boss the value of predicting the weather, or more seriously - predicting future revenue? Jan - $100,000 Feb - $110,000 Mar - $125,000 April - ??? "Boss - with data science and predictive analysis, i'm predicting next month we'll generate $132,019.12 - it only took me half a day to do this, and I did it all with a language common in Data Science called "R" &amp;#x200B; What value is that to my boss? &amp;#x200B;
What is the value of predicting future revenue better? I don’t understand the question really. 
Why is why I told you if I knew you were making box plots to begin with I would have done it differently. Go back to the "dat" right after you read it in as a csv. I normally don't overwrite the name of my object like that either. I would have done something like transformed\_dat or something to show that it had been changed from the raw input. That way you still have the raw input to mess with if you need to go back.
Every month we gain 100 customers and lose 10 customers Jan - 1200 customers, win: 100, lose 10 Feb - 1290 customers, win: 100, lose 10 Mar - 1380 customers, win: 100, lose 10 Apri - ?? &amp;#x200B; "Boss, I calculated with Data Science and R that in April, using a predictive formula, that our churn rate is 6.51848194% and our expected customers in April is going to be 1470.1938. I didn't even use Excel - I used data science, and it took me just 3 hours to work this out, after extracting and downloading all our sales data from our SQL database, so all up it only took me a day" Boss: WTF? &amp;#x200B;
Can't you estimate it without "data science"? Draw a line through some dots. What difference does it make if the value is $132,000 or $145,000 or $135,000 or if i'm off by 2-3%
No, ask yourself why no one is winning Kaggle competitions with Excel. There’s no rule against it. And yes, often accuracy does matter. Surely you can imagine situations where being accurate is important.
I'm not sure what your intention is here, but if you think there is no value in more advanced modeling techniques, I'm not sure what I can say to convince you. Obviously, there are times when very simple is better. There are times though that you need to do more complex things. If the simple things work for YOU and YOUR business case, then use them. I don't care. I'm not sure why you are trying to attack using more advanced methods. If you can do everything you need to in SQL and Excel then by all means keep doing it. You don't have to learn anything else.
Guess what -- drawing a line through some dots IS "data science".
I do mostly BI dev, and my daily tools are SQL and R; you could just as easily substitute Python for R. I use each for their respective strengths: SQL for interacting with DBs and select/filter/group-by, and R for the more ad-hoc-type analysis, which includes EDA and viz. &amp;#x200B; I most often start with R when I have a new dataset, which could be one or more local flat files, or one or more tables in a DB. I may need to figure out an optimal ETL strategy that will ultimately be written as an e.g. SQL procedure to be called by an ETL pipelining tool. Getting to the core transformations needed is often significantly faster with R. I can take a sample of data (load from local or pull down a subset from DB directly within R) and determine what is needed, then translate the approach into SQL. Over time, you accumulate various functions that you could e.g. package up for sharing/re-use. 
It might take you half a day. 
Well - the weather? You always get that wrong - Revenue forecasts? You'll never accurately predict that. I guess I don't see the benefit in hiring (for a lot of money) to predict things more accurately than the accountant can already "predict" with Excel It kind of feels like playing with a Raspberry Pi - it's "cool" but where is the business value? I'm making you $500 per day - how are you making me more than $500 per day? That I can't just get my SQL/Excel guy to estimate
So you think the whole industry of data scientists is just a group of dum dums who don’t really understand business and if they were smarter they would just use excel instead?
Every model is wrong. The goal is to make the most useful model. If YOU can do it for what you need using Excel then do it. Why would anyone care? Why do you care? Just get your "SQL/Excel guy" to estimate it then. You don't have to use R or Python. But if you think that the best models are using only SQL and Excel then I don't know what to tell you. Just because you don't see a need for other things, that doesn't mean there is value in them for more complex predictive models.
Lets say company A decides that "data analysts" are the hot thing right now. We put an ad up that says "We need a data analyst paying $xxx,xxx" The data analyst visits office for the interview. We say, "we heard data driven decisions are the hot thing right now, can you help us make better decisions with data?" Data analyst says "Yes sure - of course" We say, "We want to increase revenue and reduce costs, I mean - that's basically why we want to employ someone, to do those things" Data analyst says "okay" We say, "Okay so how do we do that?" (Surely this is a common scenario) &amp;#x200B; Obviously the data scientist isn't going to say "I can predict things", or "I can do things that Excel can not", or "Unlike Excel - I can create code that can easily reviewed and analysed" or "My graphing libraries are far superior" &amp;#x200B; 
Consider that this same argument can be applied to Excel. Why do we need to use the fancy dancy general computing devices? I can do the same thing with a TI-89, a pencil, and a notebook. But why don’t I? The answers to that question are in a general sense the same as why “data scientists” don’t use Excel.
So you're importing data into data frames, and filtering and grouping by. You know "optimal ETL strategies" and you know SQL proceedures. You know how to pull data from flat files and from SQL &amp;#x200B; Is this a data engineer? What is the purpose of R here, when you could just use SQL? I guess you need R to do the parsing of the data? and cleaning the data? &amp;#x200B; That's fine - but what does the business do with all this cleaned data, do they just run reports? That sum and average and subtract A from B? and Multiply C by 12 months? 
Well I don't even know what a 'data model' is .. so far in my Datacamp course i'm just pulling in data into a data frame and sorting it, or showing only a single column, or aggregating data, or joining two dataframes" &amp;#x200B; I mean, I could already do this with SQL 12 years ago ..
Are you talking about data analysts or data scientists? Typically a data analyst is someone who does more descriptive work and works in SQL/Excel. The terms are fuzzy though and there is some overlap depending on the company. I'm still not sure what your intention is of trying to attack using more rigorous methods to make better models is. If you have no use for them, then don't use them. It's as simple as that.
You guys are hot property right now, are they paying you to draw lines through dots? I mean surely they want some kind of return for the new hire?
The computer science term for such an operation is a join and pandas does indeed have a join function so you can probably use that!
I'm going to keep repeating my comments from above because you just keep repeating the same things over and over and over. If you have no need for R or Python, then don't use them! If you like SQL then just use it. 
If what you are doing works, then keep doing it. Don't hire a data scientist if you don't need one.
thanks for your detailed reply, i'm suprised you think Excel is that much harder/slower than Pandas/R (it takes me three seconds to sort something, or add a table filter, or add a pivot table, or duplicate a worksheet) &amp;#x200B; Why would a business want a model? Is it basically a assumption of what is going to happen? Is that what a model is? Is it drawing a curve between dots to create new data? Or extend the data point to create a prediction?
Then keep doing what you're doing if you see no need for it. It's not my job to convince you. If you don't see a need for "advanced methods" then don't use them. It sounds like you're very unwilling to even accept the possibility that there are things that you don't know that could be useful to you. 
I'll give you a concrete example from a recent project. We need to build a Tableau dashboard for sales support. There are over a dozen inputs coming from three disparate sources. Most of the inputs will be S3 feeds, but some will come from web analytics queries that automatically get dropped in some location, and say one of them is manual, out of necessity. We basically need to build a DW to support this. Figuring out the initial requirements can be expedited by taking manually exported representations of the expected feeds (because of course, the feeds have to be spec'd and set up by devs in other teams). In the course of loading these file, we realize that we need to handle certain details-- one file has trailing whitespace because the VARCHAR(255) field exported as fixed-length, another has embedded escapes that need to be stripped out, headers are inconsistent in a few, encoding is mixed and should not be, etc. I could easily write SQL to load these files and interrogate them, but it's more efficient to load them as a list of data.frames in R, check the structure, perform intersections of headers names, check line terminators, perform any string manipulation required, etc-- and then create schemas and write some simple functions that will load these into our DB, so we can do additional checks. For each iteration, I make the necessary tweaks to my R code, and then drop and re-load. Rinse and repeat until we are good-- and once we are, I can output the requirements as markdown for documentation in e.g. Jira. Of couse, all of this could be done in SQL. Technically, all of it could also be done in R. But I don't want to write SQL to do all of this ad-hoc wrangling, nor do I want to make R my production ETL tool. Again, substitute any number of tools for R-- Python, bash, Perl, etc. 
I'm trying to figure it out
I just don't know how advanced methods provide business value
So I must be a data analyst, yep I do "descriptive work" - yeh I basically make reports, and my boss asks me a question, and I reply with "A" is greater than "B" and "looks like C is going to be even higher than B"
A model could, for example, tell you you won’t lose many sales by increasing the price by 50%. Specific examples kind of miss the point though. A model is just a way of describing the world, and a business won’t get very far without understanding the word they operate in. Their customers, the market for their products, the market for their supplies, these are all complex things and humans are only capable of understanding simplifications, since they can’t possibly remember the entirety of all the facts it’s possible to know about these things, and they wouldn’t want to anyway, i.e. how many hairs does bob who bought my muffler have on his head? So given that you have to operate based on simplified models of the world, you might as well use a good one, and in many cases R and Python can help you do that.
They do exactly the same things - but Excel does it faster. And you can save it for later. But they both give exactly the same results. I'm guessing data scientists produce different results to a calculator, pencil or Excel - we've had those things for years. Now everyone wants a data science, surely they're not just outputting the same outputs as Excel?
Is it actually? Data models are mathematical representations of data points? You're trying to fit those dots with a curve? 
I should do some more research, my path on datacamp.com makes it appear that i'm doing the same work i've always been doing, only now doing it by typing the commands in a language i'm not familiar with (R or Python) I have no idea what a model is, because datacamp doesn't introduce it early in the course, it just shows you how to turn on the powersaw
So where I work we have TONS of different things we know about our customers (how much time they spend doing X or Y or whatever), where they live, what gender they are, etc. There is a desire from the business to know the things that are most likely to impact customer retention. If you have over 100 different variables to choose from, it's going to be hard to use straight-up descriptive techniques to try to even begin to figure this out. Yes -- doing the exploratory and descriptive data analysis is critical, but it gets really complex when you start trying to figure out how all these different customer characteristics interact with each other to ultimately have an impact on whether or not they remain customers. There's not an exact "science" to it either. Basically, you're trying to use some other techniques to extract meaningful information from the data. If you have simple data and simple needs, then trying to shoehorn things in to more advanced methods will seem unnecessary. So ultimately, you take what the business people know (domain knowledge), what the descriptive picture looks like, and then try to extract even more information using some other techniques to try to eke out every last drop of information/value that you can. Are there times when you don't get much value? Sure. Are there times when you can uncover something you never would have thought of just by looking at the simple data? Yep.
It sounds like you should use spread or pivot_wide() from tidyr. A better question is what will you do after you pivot it?
Well, Excel is turing-complete, which means it’s technically possible to produce the same results given unlimited amounts of time. I don’t want to mislead you though, in many cases doing the same in Excel would take years or lifetimes instead of minutes and no one does it anyway. The difference is that if you’re working in Excel, it wouldn’t have even occurred to you to do that, and even if it did, the theoretical fact of it being possible doesn’t stop it from being 100% impractical. Any set of predictions a computer can produce, you can also do with pencil and paper given enough time, it’s not like computers are magic. The difference is all in how easy it is.
This?: merge(DF1, DF2, by.x = c("ID","[Request.Date](https://Request.Date)"), by.y = c("ID", "[Received.Date](https://Received.Date)"), all = TRUE)
If you work with huge amounts of data, you can't use Excel. If you want to predict not only how many clients will go away, but which, so you can target them: you probably want to use a model that you can't create in Excel. If you're tired of repeating the same steps on Excel: sure you could use VBA but that language is hot garbage, and it's not even reliable or consistent. And let's just say that SQL is very limited in what transformations you can do to data. Python and R libraries make it much more convenient and reproductible. If your business predicting with 5% more accuracy things like sales per store, overall revenue, or clients that will buy a certain offer, represents a sizable figure compared to 1 or 2 days of pay for a data scientist, then it's valuable. If your 5% is pennies, forget about it, use something simpler. Read about Data Science use cases and what amazing models exist for certain kinds of problems, and you'll see doing those on Excel goes from unpractical to humanly impossible. And yes, data scientists use SQL. A lot. 
\&gt;Again, substitute any number of tools for R-- Python, bash, Perl, etc. **Thanks for your reply!** I understood everything up to here - i'm kind of suprised that i've come to know what you're talking about over the past 12 months - you're basically building a data warehouse (which i'm assuming is just a collection of tables that store your data in a well organised way), i'm doing this type of stuff at work (when i've finished creating reports in excel or tableau) **Here's what I understood** 1. you're figuring out what to do with the data that exceeds 255 characters 2. you're fixing embedded escapes - so I think you mean \\' and \\" and \\n and \\r - so you're stripping those and replacing them with the actual character 3. headers inconsistent - so you're just renaming them to match 4. encoding is mixed - i guessing some data sets you're importing have different character maps such as utf-8 vs utf-8 **next you're talking about R - which i'm guessing is like pandas, but using the R language** It seems like you've already got the data in SQL - but now you're downloading it into an R data frame, and performing more manipulations on the data, and writing some functions in R to send this back to your tables in your DW - as you pointed out this could have been done in any language - but obviously not in Excel (which would have sucked, you don't want to be messing with find-replace or anything like across multiple worksheets **I can write a base query in R, create a list of variants based on test cases, and send those queries to the DB to benchmark-- after which I can spit out pre-configured plots for evaluation** You kind of lost me here, but I think you're mostly saying that you mostly used R to hold and execute ETL procedures, and as described many times by many people, the code is ready to go and modify when necessary In the scenario you described - yes - R or Pandas or Python is supremely superior, doing this in Excel would kind of be a joke, and you probably have at least 200mb of data in your DW - so it would be really painful to do this in Excel \*and\* it would be very difficult to do this on a regular basis and reupload to Tableau &amp;#x200B; **At this point of the scenario, we've only used R as our "language of choice" to perform ETL duties and pull data from various source, inspect and clean the data (as described - fixing white space, escaped characters, concatenating columns and fixing header names)** This seems to be more enterprise data collection, manipulation, and storage in a data warehouse. PHP or .NET could have done this work, and we're basically sending it all back to the DW so that Tableau guys and girls can do some "descriptive analysis" &amp;#x200B; **Can I make "decisions based on data"?** Yes - with clean data, and predetermined functions i've written and revised I can make decisions based on data &amp;#x200B; **Is this data science?** I still don't know what data science is, or what a model is. It feels like i'm doing the work of a programmer, not a "scientist". Or providing any insight - i'm just creating nice routines to collect data and send it to the data warehouse &amp;#x200B; **Could I have done this with SQL** Not as easily as I could with Python or R &amp;#x200B; **Could I have done this with Excel?** No it would be hell - especially if this was a daily ETL operating &amp;#x200B;
\&gt;try to extract even more information using some other techniques to try to eke out every last drop of information/value that you can. **Thanks for replying!** Okay so i'm understanding simple descriptive analysis "eg. car uses 25 mpg vs 20mpg therefore it's better", which I guess anyone can do, and i've been providing this "wisdom" to my boss for years, but as you said - sometimes that's really all that's needed for many questions **100 different variables to choose from, it's going to be hard to use straight-up descriptive techniques to try to even begin to figure this out.** This sounds really common - at this point - I would join a bunch of tables together to create a really wide table - and see if I can start using filters to cut the data, and just muscle out some information, such as 1. it seems that Males over 35 stick around the longest - "Boss, we should spend more money advertising to males over 35" 2. it seems that Females living in these postcodes are the biggest spenders - "Boss - females in these zip codes spend the most money, lets spend more money there" I don't really have an educated way to make these conclusions, I just know I need "plenty" of data make the insight accurate and not just random But I still don't feel like an advanced "data science" guy. **I could pull this information from a SQL, a dataframe, or from Tableau.** What's so special about that? I could have joined a bunch of tables in SQL, connected Tableau (via some web connector) or R (via some library) and just added some simple filters "select cost/revenue group by gender order by revenue descending" or "select gender,revenue group by gender &amp;#x200B; **So - is this what data science jobs are?** They look a lot more mathematical, but on the other hand, a lot of DS people write that most of their day is just ETL? Where would 'predictive analysis' or 'data models' come into this scenario? &amp;#x200B;
Okay got it - i'm getting that message a lot in this post? They're just tools designed for the job and execute that tool automatically, rather that retyping or reclicking or recreating a different pivot table in Excel, or redownloading a CSV and reimporting it into Excel &amp;#x200B; At this point it still feels like "awesome tools for ETL and descriptive analysis"
What was your reshape command?
I've found the tidyverse package easy to work with for this. See the 'long to wide, in one step' section https://rstudio-pubs-static.s3.amazonaws.com/282405_e280f5f0073544d7be417cde893d78d0.html
&gt; what kind of analysis do data scientists do that will give business value to my boss? That very much depends on what business you are working in and you've not stated that. Personally I've used R as... * Statistical Geneticist - working on human genomic data to identify genes associated with diseases via various linkage and association analyses. * Medical Statistics - working on clinical trials and analysing the results of survey and time-series data. Also analysing routine data from ambulance services. * Telematics (current) - analysing data from mobile phones on people's driving behaviour to quantify "good" and "bad" drivers for insurance premiums. A more general use might be that by not using Excel you will be able to automate reports via time-scheduled jobs (using for example [cron]() under GNU/Linux systems to schedule scripts to be run). Thus if youre BA's require the same graphs/reports regularly then why waste time importing them when you could have a script that pulls the data, tidies it, summarises it and outputs to PDF which can then be automatically emailed (or a link sent to its location on a server). 
Is it like a sequence of events you think your data will follow? And describing it in code? Eg. Say I have a table with 100,000 transactions and the dates of transaction, and another table with the customers and their age group. Would a model describe their frequency of purchase? Such as Age group 20-30 buys every 30 days, age group 30-40 buys ever 45 days?
The general theme is rapid, iteritive prototyping with an end goal of figuring out production requirements. In many cases, I don't have the data in our (or any) DB, especially early in the development cycle. The feeds may not exist yet. However, the clock is still ticking down against our delivery expectations. To expedite this process, I will either figure out how to connect to a data source and pull down subsets that represent the desired, or if possible, expected "real" data. In a lot of cases, the simplest way to do this is to connect to the various sources and pull data down, then output as delimited text, and do a LOAD LOCAL. That's three steps, at a high level. I can handle the LOAD LOCAL step using only SQL, but the first two require some other mechanism. That's where some kind of language comes into play-- and why I keep saying that R should be taken as a proxy for a general language that you are fluent in and allows you to streamline the connect-pull-munge-output-repeat workflow. I just happen to know R quite well. It is beneficial if said language is tailored towards data analysis, or has libraries that enable this, because some kind of munging/transformation is often needed as an intermediate step. So, in R, I will configure the connections, make the calls, pull the data down, and write out-- and since I'm already connected to our DB/DW, I also push (load local) the datasets using R. That way, when requirements change, it's all handled in a single language/set of scripts. Since this is all initial investigation/requirements gathering/prototyping, using a high-level language is efficient-- this is not production code, so quick and efficient is the goal. Once everything is pushed up, I often go back to R and write some unit tests, that are basically queries against the DW to check expectations-- and once I've figured out the most useful tests, I'll (also) translate that into SQL for production purposes. &amp;#x200B; This is not "data science" in the sense most think of. It's definitely more data engineering, as you mentioned. My work does not currently require machine learning, although I will use some basic regression/classification for profiling work, when it might be helpful to take a look at distributions and correlations, as these might inspire some additional useful views in Tableau. But that's a side benefit to the main goal (for me). The bottom line is data-centric language + SQL is a powerful combination for data-related tasks.
Thanks for that. I am still having some trouble applying it. &amp;#x200B; How would that come out on dateframe = df long column (the coulmn that we want to spread) = sim other columns = d o h l c v a sim x x x x x x x &amp;#x200B;
Hey that's my bad, I didn't realize your table was a tibble, which I don't have much experience with. So when I was calling `unique()` on that column, it wasn't acting the way I was expecting. Try this: for (position in unique(dat[['Roster Position']])) { positions = strsplit(position, "/") for(pos in positions) { dat[pos] &lt;- ifelse(dat['Roster Position'] == pos, 1, 0) } } I just ran this on my end it looks like it works. I also added the part that separates the positions by "/". As others have said, I would strongly recommend you take a look at what's going on and understand it.
The link you sent explains it perfectly ggplot(mydata, aes(x = month, y = product, fill = Product)) + geom_col()
You could try to build a model that explains the associations, so you could try to say things like Females living in this postcode tend to be 5% more likely to do whatever. &amp;#x200B; If you are just slicing and dicing data trying to "find" things, this can be dangerous as well. More advanced techniques try to take in to account whether something is an actual signal or just noise. You can find descriptive things all day long, but the real question is will that same relationship occur in the future -- can you infer to a bigger population that the same thing is true? &amp;#x200B; Again, I'm not sure why your attitude is that there's nothing special about it. I feel that you don't really know what you're talking about, so you just make it sound like you already do things in a better, more efficient manner. I can't understand this mentality. Why wouldn't you want to be open to the possibility that you are missing something?
Sorry i'm not familiar with the Windows environment Could this mean you're running RStudio on any environment and just using RODBC and ODBC/DBI libraries to connect to MSSQL server?
100% think i'm missing something - it was probably the simple examples when learning dataframes, I started to think "What's the point in writing this code just to filter a table, I could have finished this task 10 minutes ago with Excel - and it seems kind of weird learning how to draw graphs with code, when I could have just clicked on the graph button" this thread is helping a lot though
&gt; If you are just slicing and dicing data trying to "find" things, this can be dangerous as well. More advanced techniques try to take in to account whether something is an actual signal or just noise Yes this is exactly where i'm at - sometimes i'll explain an insight in my data to someone, but in the back of my mind - knowing that there is a chance that i'm describing the conclusion without full confidence of accuracy
Hey! I was suprised you mentioned Adwords - how are you pulling the data - Supermetrics or Adwords API or bigquery?
It just depends on what level of control you want. Making a "graph with code" offers you more fine-tuned control. There is also a specific set of steps documented in code of how the graph was made. Do you write down every time you point and click when you're working in Excel? I can hand off my script to anyone and they can re-run it and reproduce the same exact graph. 
I would recommend reviewing some basic statistical inference especially confidence intervals. I think adding some "margins of error" around your insights helps decision makers. If you say "I think sales will go up by 15% next year" versus "I think sales will go up somewhere between 1% and 29%" that shows that the 15% isn't really that certain.
https://tidyr.tidyverse.org/reference/spread.html
There is also the new pivot functions from the tidyverse. I've not used them as yet but may be of use for you. https://tidyr.tidyverse.org/dev/articles/pivot.html
See below. If we're on the same page, where do you want to go from here, and why? What are you ultimately trying to do? You're going to have a pretty funky looking structure if you want to put the symbol into columns, and date as the dimension. library(tidyquant) tickers &lt;- c("AAPL", "AMZN", "GOOG", "XOM", "TMO") dat &lt;- tq_get( x = tickers, get = "stock.prices", from = "2016-06-06" ) str(dat) Which looks like: Classes ‘tbl_df’, ‘tbl’ and 'data.frame': 3515 obs. of 8 variables: $ symbol : chr "AAPL" "AAPL" "AAPL" "AAPL" ... $ date : Date, format: "2016-06-06" "2016-06-07" "2016-06-08" "2016-06-09" ... $ open : num 98 99.2 99 98.5 98.5 ... $ high : num 101.9 99.9 99.6 100 99.3 ... $ low : num 97.6 99 98.7 98.5 98.5 ... $ close : num 98.6 99 98.9 99.7 98.8 ... $ volume : num 23292500 22409500 20848100 26601400 31712900 ... $ adjusted: num 94.2 94.6 94.5 95.2 94.4 ...
I’m trying to get the sim variable to be across the top and other variables (currently columns) to change to rows 
https://github.com/jburkhardt/RAdwords
While I am still not sure why you want this structure, here's one approach you could try, using a matrix. The main challenge is that you have two dimensions (symbol, date) in the return. If you reshape it, you effectively create a third dimension, which comprise the variable names (open, high, low, ...). If you take advantage of the fact that your dates basically a number, you could make a numeric matrix as such, then simply call transpose on it: library(tidyquant) tickers &lt;- c("AAPL", "AMZN", "GOOG", "XOM", "TMO") dat &lt;- tq_get( x = tickers, get = "stock.prices", from = "2016-06-06" ) str(dat) # make a copy dat_cpy &lt;- dat # transform date to numeric (can get back using as.Date()) # this lets us make a numeric matrix dat_cpy$date &lt;- as.numeric(dat_cpy$date) # make a matrix, without the symbol variable mat &lt;- as.matrix(dat_cpy[, !names(dat_cpy) %in% "symbol"]) rownames(mat) &lt;- dat_cpy$symbol # now transpose mat_t &lt;- t(mat) `mat_t` is a 7-row, 3515-column matrix. The first 10 columns: &gt; mat_t[, 1:10] AAPL AAPL AAPL AAPL AAPL AAPL AAPL AAPL AAPL AAPL date 1.69580e+04 1.695900e+04 1.696000e+04 1.696100e+04 1.69620e+04 1.696500e+04 1.696600e+04 1.696700e+04 1.696800e+04 1.69690e+04 open 9.79900e+01 9.925000e+01 9.902000e+01 9.850000e+01 9.85300e+01 9.869000e+01 9.732000e+01 9.782000e+01 9.645000e+01 9.66200e+01 high 1.01890e+02 9.987000e+01 9.956000e+01 9.999000e+01 9.93500e+01 9.912000e+01 9.848000e+01 9.841000e+01 9.775000e+01 9.66500e+01 low 9.75500e+01 9.896000e+01 9.868000e+01 9.846000e+01 9.84800e+01 9.710000e+01 9.675000e+01 9.703000e+01 9.607000e+01 9.53000e+01 close 9.86300e+01 9.903000e+01 9.894000e+01 9.965000e+01 9.88300e+01 9.734000e+01 9.746000e+01 9.714000e+01 9.755000e+01 9.53300e+01 volume 2.32925e+07 2.240950e+07 2.084810e+07 2.660140e+07 3.17129e+07 3.802050e+07 3.193190e+07 2.944520e+07 3.132680e+07 6.10082e+07 adjusted 9.42280e+01 9.461016e+01 9.452419e+01 9.520249e+01 9.44191e+01 9.299558e+01 9.311023e+01 9.280452e+01 9.319623e+01 9.10753e+01 &amp;#x200B;
It's ordering by factor levels, this article will help you [https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2/](https://www.r-graph-gallery.com/267-reorder-a-variable-in-ggplot2/)
Yea sorry this comment was actually supposed to be a reply to someone else’s so it makes more sense in that context. To answer your question, the simple answer is yes. In a windows environment it’s really simple actually. You can automate simply by using the Windows scheduler (or even the RStudio scheduleR add-on but I don’t recommend that). So as long as you have windows running on any type of machine (server, laptop, VM, etc) you can schedule your script to run whenever you want. The packages I listed are how you can read and write to your sql database.
Unfortunately, Month can't be modified because it is a grouping variable. &amp;#x200B;
Hmm. But how do I get it to cycle through all my variables and put each name, each mean, and each sd in? Dataframe looks like this: Nest | CanopyArea | FoliageEstimate | NumberTrees ----|----------|---------------|----------- Yes | 10 | 1 | 5 No | 20 | 2 | 10 Yes | 30 | 3 | 15 No | 40 | 4 | 20 This is a truncated version, I actually have 100 columns that I don't want to analyze by hand and I want the averages/sd to be for nests and non-nests separately. So the resulting matrix should look something like this: Variable | Average | SD --------|-------|-- CanopyArea[nest] | 20 | 2 Canopy Area[non-nest] | 30 | 3 FoliageEstimate[nest] | 2 | 3 FoliageEstimate[non-nest] | 3 | 4 NumberTrees[nest] | 10 | 5 NumberTrees[non-nest] | 15 | 6 Maybe there can be another column for nest/non-nest, but this is the gist. 
Thank you, see comment above
Doesn't the code work?
ah never mind solved it! factor(df$Month, ordered=TRUE, levels=c(1,2,3,4,5,6,7,8,9,10,11,12) &amp;#x200B; Thanks friend! 
Ah sorry I mistunderstood how your dataset was structured. Try this # Read the dataframe you posted canopy &lt;- read.csv("canopy.tsv", sep = '\t', header = T) # Split it up into two dataframes based on nest canopy_wNest &lt;- canopy[canopy$Nest == "Yes", 2:ncol(canopy)] canopy_woNest &lt;- canopy[canopy$Nest == "No", 2:ncol(canopy)] library(tidyr) library(dplyr) # Reshape dataframe canopy_wNest &lt;- gather(canopy_wNest, factor_key=T) canopy_woNest &lt;- gather(canopy_woNest, factor_key=T) # Summarize averages and SD for each column/group canopy_wNest &lt;- canopy_wNest %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value)) canopy_woNest &lt;- canopy_woNest %&gt;% group_by(key) %&gt;% summarise(mean = mean(value), sd = sd(value)) # Fix names canopy_wNest$key &lt;- sapply(canopy_wNest$key, FUN = function(x) paste(x,"[nest]", sep = '')) canopy_woNest$key &lt;- sapply(canopy_woNest$key, FUN = function(x) paste(x,"[non-nest]", sep = '')) # Combine canopy_combined &lt;- as.data.frame(rbind(canopy_wNest, canopy_woNest)) 
You can use message() and warning() to spice up your console output
Desktop. IT is way too stingy about giving us access to our own linux VM.
Sorry - what exactly is piping? I only know of the bash pipe | As in: cat file.txt | tr -d '\\n' | sed -r "s/test/tester/g"
It is annoying work to be sure. It is fairly simple to write one that will just use `readLines` `grep` and `gsub` to replace all `source` calls with the file that is being called but for it to work on all cases you need to consider more complex cases such as flop = source flop("file.R") or print("hey");source('file.R');print("hello") or print("this is no source('file.R'). you should ignore this") Stuff like this will probably force you to use `parse`
Sorry - what exactly is piping? I only know of the bash pipe | As in: cat file.txt | tr -d '\\n' | sed -r "s/test/tester/g"
In this context, I mean transferring data. 
This isn't really an R question is it? You can download the data with [https://rdrr.io/cran/RAdwords/](https://rdrr.io/cran/RAdwords/) but it won't help you to understand your data any more than the Adwords interface, will it?
Looks like it’s already been solved, but I think that converting the month variable to be numeric variable would also fix the issue. I know that when I have variables coded as ‘factor’ I see weird things happen. 
Can you supply an example of your code so we can try to reproduce the problem?
yes certainly Value &lt;- c(3,9,7,1,2,7,6,5,7,5,11,10,5,11,1,3,4,7,3,3,6,12,8,12,4,9,10,12,5,13,6,11,8,8,6,2,1,4,11,13,7,11,12,4,13,8,6,8,4,1) Suit &lt;- c("H","H","C","S","D","C","S","D","S","S","H","H","D","D","S","S","C","C","C","D","C","H","S","S","C","D","D","S","D","D","C","D","C","D","D","H","S","C","S","C","H","S","S","H","D","H","C","D","S","S") MyData &lt;- data.frame(Value,Suit) I am trying to make a histogram of values vs frequency.
dplyr DF %&gt;% group_by(ID) %&gt;% arrange(ID,Date) data.table DT[order(ID,Date)] However... if that's the actual format of your dates, you'll likely want to first convert to a Date with [as.Date](https://as.Date)(yourdatevar, "%m/%d/%Y"). &amp;#x200B;
&gt;DF %&gt;% group\_by(ID) %&gt;% arrange(ID,Date) Converted thank you so much stranger! &amp;#x200B;
Why do you need the group_by() in this scenario? Sholdn't it work without? 
Can I ask why you're doing this?
```DF$Letter_Grade &lt;- dplyr::case_when(DF$Numerical_Grade &gt;= 90 ~ "A", DF$Numerical_Grade &lt; 90 &amp; DF$Numerical_Grade &gt;= 80 ~ "B", etc.......```
try: df$month &lt;- as.numeric(as.character(df$month)) &amp;#x200B; convert the month to character and then to numeric and then reassign the variable name.
There is a function called cut which will help you cut an interval of numbers and assign other values to them. I would first write a function that takes a number grade and gives its letter grade. Then I would call that function on my column of number grades. Try out this function: grader &lt;- function(num) { as.character(cut(num, c(50, 60, 70, 80, 90, 100), c("F", "D", "C", "B", "A"))) }
Here's my approach. Let me know what you think: # Calculate the critical value to which you want to plot critical_value &lt;- qt(p = .95, df = 3) # Generate some values on the x-axis from -5 to the critical value xs &lt;- seq(from = -5, to = critical_value, length.out = 1000) # Calculate the assocated density of the x-axis values ys &lt;- dt(x = xs, df = 3) # Plot the curve curve(dt(x, df = 3), from = -5, to = 5, main = "T Distribution With 3 Degrees of Freedom") # Plot the shading polygon(c(-5, xs, critical_value), c(0, ys, 0), col = 'grey')
Thanks! I've used this before, it's pretty awesome - how often do you have to reauthenticate?
Thanks - sounds like I need to learn some statistics, otherwise i'm just writing reports
I just call the doauth function every time I run a report. Keeps it in there for quite some time, months I think. 
Cool thanks! Are you guys mostly doing this in RStudio and just sending the code to each other, which they can re-run on their machine?
Is it on a remote server to which you have access? If so you might be looking for Rstudio server
Yes - the data is on a VPS I created Is this the whole point of RStudio and Jupyter? Can R handle a 2gb table? Or do you instead run a command such as "random sort the table, and only show me the first 10% of rows"?
Awesome thank you! Is R sitting on your local machine or are you running it as a cron job?
Rstudio Server is browser based ide that runs on the remote server, which means you'll have access to the server's computing resources as well as data stored on the server. R can handle as much as your RAM allows it to. How well it scales up depends on specific package/approach you use.
Thanks, that definitely achieves what I'm looking for as an example. However, I'm hoping for a solution that works with the sample code I provided above so I can get it to work for actual sample data that I acquire.
Okay great! So a 1gb text file will be about 1gb of data frame - that doesn't sound too bad at all
seemed like a good idea last night, and one of those "maybe there's a quick way" kind of thing. I stated that it wasn't worth much effort , as I don't want to waste anyone's time on it. Just something that I was curious about. 
I certainly hope my question didn't seem judgemental - this is an interesting question and genuinely something I haven't tried to do in R. Almost seems like minifying JavaScript.
No problem, i just wanted to be clear that this is just a curiosity for me and that I wouldn't want anyone to go out of their way. so if someone knew something off the top of their head then that'd be alright, otherwise it's fine. as someone else noted, i think one could script this just checking lines for `source(` then reading `&lt;file&gt;` and printing out. That kind of thing, I thought there might be a quicker solution though. cheers
I have my own data from my website. But the problem is that I'm fairly new in data analysis with AdWords so I don't know where to begin other than reading the documentation on AdWords
What are you trying to discover? Most Adwords people compare the cost of the campaign with the revenue that it's creating - not much more to it than that
Sounds like a job for SSIS and a scheduled job in SQL management studio.
&gt; SSIS Thanks for introducing that term, I guess it's in the same family as SSAS which I now understand as belonging to Microsoft 
Do you not have a direct connection to the Data Warehouse via a SQL editor? Writing the SQL needed to limit the dataset and explore its contents would execute it on the local server it's being stored, using those resources and not having to download all of it without knowing what you need. Whenever I setup my Data Architectures, I always account for Data Analysts/Data Scientists needing direct access and provide them logins to at least read the data from the server in a safe manner. 
&gt; Do you not have a direct connection to the Data Warehouse via a SQL editor? Yes I do - however i'm sometimes not sure what data I need and think i'll need to download the whole thing to ensure things are "accurate" - i'm not really confident in using smaller samples of data, since i'm often looking for data that is in one table, but not in the other - but as you said - i could just do that with SQL &gt; Whenever I setup my Data Architectures, I always account for Data Analysts/Data Scientists needing direct access and provide them logins to at least read the data from the server in a safe manner. Thanks for bringing this up - what's a good way to give the sys admin a bit of confidence in my SQL queries? They often have concern that a long query will lock up the database. Is there something I can do, or they can do, to minimise or remove this concern/risk for them?
Begin working with your DBAs to peer review your SQL and give you insight into the data you're looking for. More often than not, they know all the little, but highly important, details about the data and can probably guide you in what data you'd actually want to pull. I've always worked hand-in-hand with the Data Analysts/Data Scientists within an organization. They tell me what question they're trying to answer and that gives me a baseline to guide them to all the data in the system they may be looking for. I can tell them that this particular date column automatically defaults to '1960-01-01' or is only populated when some event is triggered from the application. Your DBAs are going to be your best supplemental resource to helping you make sure that the data you're trying to pull is as accurate as possible and accounts for any irregularities that you may not be privy to. (Well, a good DBA anyways). 
Thanks! All that sounds very professional .. In these kind of environments, is it unusual to query a production environment? To ease his concerns, wouldn't he have a read only back up copy of the DB I could query instead?
Well, if you have a Data Warehouse, that's technically what it's there for. A Data Warehouse is an OLAP (Online Analytical Processing Database) - it's specifically designed to query against, and do Data Analytics on. And yes, no one should be querying the OLTP (Online Transaction Processing Database) - that's being written to - in a perfect environment. The database team should be securing and restricting access to this database as much as possible. If you don't have a Data Warehouse proper, then the next best thing would be to do some near-real-time replication of the production database to a read-only OLTP where you can query the system until your heart's content. 
Thank you for explaining OLAP/OLTP - did you learn all this at university? Sounds like this is your main gig
I don't have a degree, unfortunately - I dropped out of uni when I was younger. I'm a Data Architect now professionally and just studied this stuff on my own or learned it on the job. 
All good - i'm in the same place you were probably in 5-10 years ago - did you have to learn statistics to get to data architect level? 
Yeah, I've taught myself enough to keep up with the Data Analytics and BI departments and got my Six Sigma certification. I actually have a call with an enrollment officer at a local uni today at 4:30PM to see what options are available to get my B.S. in Stats. Not having a degree has been something that's been a weight on my shoulders for awhile and now that my children are older and I have the money I'm going to get it :) Stats has always been something that's interested me but I've never had any formal teaching in it past an Intro to Stats class I took before I dropped out. 
Congrats on your progress! I'll google Six Sigma 
It's specific to process improvement techniques in businesses who use anything akin to a manufacturing or production process. It's a nice cert to have if you work in product-based industries. 
&gt;DF$Letter\_Grade &lt;- dplyr::case\_when(DF$Numerical\_Grade &gt;= 90 \~ "A", DF$Numerical\_Grade &gt;= 80 \~ "B", I did that but when I print out the dataframe it list out for every single row for LetterGrade as A. 
&gt;grader &lt;- function(num) { as.character(cut(num, c(50, 60, 70, 80, 90, 100), c("F", "D", "C", "B", "A"), include.lowest = TRUE)) } Cool. Let me try it. I am considering using this function for other purposes as well. How do I use this function if I have one interval that is an exception: Ex: 18 and 29 would be classified as "18-29". But everything else would be 30-39, 40-49, etc. 
Thought so. By the way - is ETL really 80% of the job your team has to do? Right now it's my favourite part - and it doesn't seem that hard - just scheduled jobs and cleaning junk characters from fields, and a bit of string trimming
Just combine what you had with what clamiam45 has said: x&lt;-rnorm(1000, mean=0, sd=1) percentile&lt;-quantile(x, .95) d&lt;-density(x) # Plot the line plot(d, main="Standard Normal Distribution") # Compute the index of the 95th percentile in the density vectors p95=which.max(cumsum(d$y/sum(d$y)) &gt;= 0.95) # Plot the shading polygon(c(-5, d$x[1:p95], d$x[p95]), c(0, d$y[1:p95], 0), col = 'grey')
It's difficult to help without a reproducible example.
R doesn't compile; it's an interpreted language. You'd need to write an implementation that transpiles R down to something else (but, definitely don't do this). I think what you might actually want is [Plumber](https://www.rplumber.io/). Write your web stack however you want, and just have Plumber endpoints running to request your R functionality from. 
Not at all. A majority of the work I do is refactoring. My specialty is working with companies who have under-performing or rubbish Data Architectures (or perhaps really none to begin with) - and refactoring the architecture to meet the current demands of the business as well as make sure it has room for scale. In the current role I feel, I lead a team responsible for any aspects of Data or Databases. Here are some of those responsibilities: * All Data Governance, including meeting with different business units to continue to flesh out and improve on our Data Governance policies we implement. * Included with the Data Governance, but making sure we meet auditing and country/state specific data standards. (GDPR, J-SOX, etc.) * Build/Maintain/Improve the Data Catalog. * Maintain the health of all databases (including upgrading/patching, performance monitoring, backups and recovery, DR, etc.) * Database Design for any new databases that are being stood up, or any new data that is needing to be added to our Data Warehouse (or if a new Data Mart is needed). * Database Development/Refactoring of any existing code that needs some attention or the data API/Data Abstraction Layer (DAL) when the Software Development team is needing to create a new application. * Any Scheduled Reporting creation/maintenance. We use SSRS, and it's what I'm most comfortable with. Any new periodic/scheduled reports that are needed are designed and developed by my team. * PowerBI/Dashboard development. If any persistent PowerBI development is needed, we handle that as well. * Electronic Data Integration (EDI) development (machines, scientific instruments, financial institutions, etc) is all done in SQL via Microsoft's Broker Services and we manage, develop and maintain all that. So if our R&amp;D lab gets a new instrument, we will write the code needed to pull the instrument data from the instrument and correctly store the data for future use. * ETL pipelines (like you mentioned) to the Data Warehouse - although I prefer and have implemented ELT pipelines instead as I find them easier to maintain and debug. 
Plumber is a great suggestion. An alternative is to use nodeJS with the module [r-script] (https://github.com/joshkatz/r-script#r-script). The last time tried this, the master branch was broken. If they havent fixed it yet, the fix is in this [pull request] (https://github.com/joshkatz/r-script/pull/8).
&gt; R doesn't compile; it's an interpreted language R *can be* compiled, and as of a few versions ago, installed packages are compiled by default (I don't know if this can even be disabled). It's not really a defining characteristic of the language.
I test locally, and my clients use on demand reports (they select date ranges and features), which runs on Digital Ocean. 
Sure thing. If you head down that path, ZappySys does wonders.
As another reply hinted at, compilation and interpretation are not features of a language but rather features of their different implementations. So in theory, R could be compiled to WASM. But I don't think there is any package to do this currently and I don't think anyone would be terribly interested. This would require the ability to manipulate the DOM in R, and that sounds awful to me personally. It also would likely result in pretty big WASM executables since, if you're like me at least, most of your R code is other people's R code, and R packages are often quite big. The recommendations to expose your R code as an API is a good one. If your main interest is writing feature rich dashboards or other web apps in R, I would recommend looking into Shiny as well.
Thanks, this helps tie the two together and works great. &amp;#x200B;
Check out the R command `?cut`. It will show you the documentation for the function. You will specify 18 and 29 in the `breaks` and `labels` arguments. If you need help post your attempt and I'll help you debug.
The biggest problem with compiling the R interpreter to wasm is that a large part of it is written in Fortran. I'm not sure if Fortran =&gt; wasm is possible.
You can put a calculation of y directly in a ggplot aes() call. You can create your own function elsewhere and call it, or use a case or ifelse statement directly there. In https://ggplot2.tidyverse.org/reference/stat_function.html they show a way to plot that function on top of other x and y values.
Thanks! How do you do that? With Shiny or something?
Create your piecewise function as an R function (assuming "sen" means "sin"): myfun &lt;- function(x) { ifelse(x &lt; 0, sin(x), ifelse( x &lt; 2, x^2, 4*exp(x-2))) } Pick some x values to plot some_xvals &lt;- seq(-2, 4, by=0.1) And create your plot plot(x=some_xvals, y=myfun(some_xvals)) &amp;#x200B;
Ok so Installed R packages are compiled to bytecode. This is absolutely not the same as compiling R code.
This should be all you need: data$pp &lt;- fitted(fit1) &amp;#x200B;
You can use the odbc/DBI to connect directly to SQL server. If this is a table that is updated daily and you are worried about pulling massive amounts of data accross the network every day, you can write a stored procedure to pull new transactions (or whatever) into a table to append to a data.table RDS you store on your local machine. That way, you're updating an RDS on your local machine instead of creating a new one every time.
It's compilation. Native code was never the question. WASM isn't native code either. Native execution is a complete red herring.
Not only possible but trivial, since there's a WASM backend for LLVM (and obviously an LLVM frontend for Fortran, "flang").
Maybe I don’t know enough about WASM but it seems that he wants R to run “natively” in a virtual machine. R can compile to bytecode that R can execute, but that compilation strikes me as quite different than compiling to something that can run in some virtual machine.
What can I say? You're simply wrong and, as you suspect, you might not know enough about WASM to judge.
Lots of great comment already posted. I would like to add that don't discount subsampling. You say as if it's a bad thing. In my experience there have been very few instances when I have seen a difference between the conclusions I draw from sample data and full data once it gets beyond a certain size. It depends on your application. Move your computation to where the data is. Is the right tool for the job - get as much data manipulation done in SQL as possible. If you are doing data exploration start with a sample and explore, find your path and optimize before scaling. 
So you have Shiny and the data sitting on the digital ocean instance, and they query your Shiny web server?
Yeah this is good, /u/ArturKlauser. My only question t oyou, /u/dfposition1, is why if you know you're sampling from the standard normal distribution don't you just use my prior approach substituting qnorm and dnorm for qt and dt? For example: critical_value &lt;- qnorm(p = .95) xs &lt;- seq(from = -5, to = critical_value, length.out = 1000) ys &lt;- dnorm(x = xs) curve(dnorm(x), from = -5, to = 5, main = "Standard Normal Distribution") polygon(c(-5, xs, critical_value), c(0, ys, 0), col = 'grey') 
Here's my technique and some commentary. I hope it's helpful. # Example data set dat &lt;- mtcars[, c("mpg", "wt", "disp")] # Mark some values missing, those disp values less than 200 dat$disp2 &lt;- ifelse(dat$disp &lt; 200, NA_real_, dat$disp) # Fit a couple plausible models mod1 &lt;- lm(mpg ~ wt + disp, data = dat) mod2 &lt;- lm(mpg ~ wt + disp2, data = dat) # Notice that the nonrandom missingness has hidden some relationships in the regression! summary(mod1) summary(mod2) # Use the complete.cases function to add the fitted values to the data frame dat[complete.cases(dat), "mpg_predicted"] &lt;- fitted(mod2) &gt; The issues is that because in the new model around half the variables are missing, my arguments have differing number of rows. This is because some people refused to answer certain questions, or they responded with 'don't know', so they are marked as missing. I have a couple thoughts on this point. In the case of missing data, many R functions do what's called "complete case analysis." This means that every row in your data frame which is not a "complete case" with all values not missing is dropped. Inferring from a model trained by complete case analysis is fraught because the complete cases are usually not representative of the population. That is to say that missingness itself is causally linked to other variables. If you would like to learn more about a way to do better than complete case analysis, check out Richard McElreath's Statistical Rethinking. Richard has some great lectures on YouTube. Here is one on analyzing data sets with missing values: https://www.youtube.com/watch?v=UgLF0aLk85s.
This sounds more like a statistics question than an R question. I would definitely look into poisson or negative binomial regression. Consider the zero inflated versions of these models if you thinm there are alternate pathways to a zero such as a species literally not being present a site
Alright, thanks, I'll look into them :) 
grader &lt;- function(num) { as.character(cut(num, breaks=c(17,29,39,49,59,69,79,89,99),labels=labs)) } &amp;#x200B; got it man. Thanks! :D &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; I b
If you’re measuring species capture in terms of the number caught, you might want to try specifying a Poisson distribution. 
&gt; Sampling I'm a beginner analyst - i'm mostly using the data to make comparisons to see if there are any outliers or missing rows. I don't think I can use sampling for this. As a beginner, i'm also often thinking, "the more data I have, the less chance i'm going to be wrong"
That's a pretty good idea! what's the advantage of using an SP to do this? 
 df &lt;- data.frame( "A" = 1:10, "B" = 1:10, "C" = 1:10, "D" = 1:10 ) df &lt;- rbind(df, c(11,NA,11,NA)) df apply(df, 2, function(x) any(is.na(x))) #T/F vector corresponding to which columns which(apply(df, 2, function(x) any(is.na(x)))) #Numerical vector denoting which columns 
sum(df1$id == df2$id &amp; df1$date == df2$date2) 
`complete.cases()` will tell you which rows of your data frame have no `NA` values. Therefore `!complete.cases()` will tell you which rows do have `NA` values.
Actually when you look at the data frame it looks right but once I did class(Letter Grade), I got it as a list. 
The output type of lapply is a list. You can remember that because lapply is short for "list apply." There's fortunately no need to use lapply here. You can create this field with the typical base R pattern. If you really want to do it with `dplyr::mutate` method, you can do that too. set.seed(20190324) grader &lt;- function(num) { as.character(cut(num, c(50, 60, 70, 80, 90, 100), c("F", "D", "C", "B", "A"), include.lowest = TRUE)) } df &lt;- data.frame(number_grade = runif(n = 20, min = 30, max = 110)) df$letter_grade &lt;- grader(df$number_grade) dplyr::mutate(df, letter_grade_dplyr = grader(number_grade)) df 
Use [window functions](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html). Group by person, sort by date then compare every age with its ```lag```ged value.
Unless I am missing something, you could just subset your first vector three times. Something like this: v &lt;- rnorm(20) v1 &lt;- v\[1:10\] v2 &lt;- v\[11:15\] v3 &lt;- v\[16:20\] would be a functional solution.
Thank you for your response, but that is not what I wanted :( When I do v &lt;- rnorm(20), I want the numbers to print out as [1] [2] [3] That way, when I sort them into bins, it would print as the number of values. For example bin1 printing out to be 10.
I'm not entirely sure what you mean by your vector being 5 lines now... your vector is just a vector, length 20 if you just used var &lt;- rnorm(20). Your vector is "one line". It may print out in 3 or 4 or 5 lines on your screen, though unless your screen is really small it's probably printing one or two, but no matter how it prints, this vector now is just "one line". &amp;#x200B; You said you want to sort the 3 lines into bins... but it doesn't really matter how many numbers are in each? I don't completely understand your problem, but I'll try to help with a basic example that might help: &amp;#x200B; vec &lt;- rnorm(20) cutidx &lt;- as.integer(cut(vec, breaks = 3)) So here we create your vector... then cut() splits your vector into 3 parts. Look at ?cut to see how it does the splitting and how you can specify the breaks to get what you want. We use as.integer to convert the result of cut (in this case, all the results will be 1, 2, or 3. If you run this without as.integer(), you'll see how it has split each of the values in vec into a bin. &amp;#x200B; So, now we have vec and cutidx, and want to create 3 "lines" as you say. Since you said these will not be equal bins, my first thought is to just use a list. You'll end up with separate vectors in a list. binlist &lt;- vector("list", length = 3) for (i in 1:length(binlist)) { binlist[[i]] &lt;- vec[cutidx == i] } #Instead of length = 3 in binlist&lt;-, you might use length(unique(cutidx)) to so the only place #you need to adjust manually is in the cut function. The result of binlist is 3 "lines" you can access with binlist\[\[1\]\], binlist\[\[2\]\], binlist\[\[3\]\]. The for loop above does something like this, but keeps everything in one object: bina &lt;- vec[cutidx == 1] binb &lt;- vec[cutidx == 2] binc &lt;- vec[cutidx == 3] The benefit of the for loop is that if you want to split this 100 times, you can still use it while only changing cut() (and the length of your starting vector). &amp;#x200B; Hope I didn't misunderstand your issue too badly, and this gets you somewhere in the right direction.
Thank you for your response. I am quite a noob to R, I didn't even know you could loop in R. I don't think I explained the issue properly at all. I'm sorry. This is for my online course and this is what my teacher wants us to do: https://imgur.com/A2ekTYw With the vectors and it printing out lines, it's dependent on the size of the screen? I feel like a complete idiot. https://imgur.com/BpO9esL &lt; This is what I mean when I said I want it to print out 3 lines. It is printing out 7 here. binlist &lt;- vector("list", length = 3) 
No, in general there will be differing numbers of rows and pairs can occur in different orders. Further, there could be duplications. nrow( merge( unique( df1[ , c( "ID", "date" ) ] ) , unique( df2[ , c( "ID", "date.2" ) ] ) , by.x = c( "ID", "date" ) , by.y = c( "ID", "date.2" ) )) 
You desperately need to study @coleeagland 's reply... This is not a free do-my-homework-for-me service.
The advantage is that the actual SQL code is stored in SQL server and the admin can keep you from altering it, so they know exactly what you are running every time. This would make our MIS team happy, I know. You can even get them to write it for you. How you design this depends on what everyone is comfortable with.
R4ds.had.co.nz
Not Shiny, but websockets. The SQL server is is on another droplet, but you could put them on the same one. They are networked so only they can talk to each other, and the SQL sever runs on the same droplet whatever app I built runs on. The reports can be slower, but I usually need the apps to run faster. 
You're misunderstanding the output completely (no shame in that). The "lines" you are seeing are really only one line folded or wrapped around because of the space available on the screen. R will tell you index of the first element in each line as it wraps them. The number of "lines" depends on available space or (for some GUIs) how your settings are. If you use rstudio I think it depends on available space. In the raw R GUI there is a setting for how many characters you get in you output.
thanks!
You can treat your 01-99 values as categorical variables and then find the [Goodman and Kruskal's Lambda](https://en.wikipedia.org/wiki/Goodman_and_Kruskal%27s_lambda). I assume you could re-encode your data to look like this: &amp;#x200B; |01|02|...|09| |:-|:-|:-|:-| |1|0|...|1| |1|0|...|1| |0|0|...|1| &amp;#x200B; And then throw it into the Lambda function in the DescTools package.
Have you tried to use as.numeric() function? 
I think you should approach this problem with simple logic. I agree with @suspicious\_gardener that you could break them down in to separate columns in a data frame. You also might want to address this problem by using a for loop. Iterate over each vector finding the percentage of the time X can predict Y.
This kind of problem is called [association rule learning.](https://en.wikipedia.org/wiki/Association_rule_learning) I can't vouch for this particular tutorial from Datacamp on doing this in R using the `arules` package, but it seems like a good start: https://www.datacamp.com/community/tutorials/market-basket-analysis-r
**Association rule learning** Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. This rule-based approach also generates new rules as it analyzes more data. The ultimate goal, assuming a large enough dataset, is to help a machine mimic the human brain’s feature extraction and abstract association capabilities from new uncategorized data.Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
This should work. Note that this would also keep all products that are tied for 3rd place. df &gt;%&gt; group_by(month) &gt;%&gt; filter(min_rank(Prop) &lt; 4)&gt;%&gt; ungroup()
It does not work. The values for the prop aren't the same anymore. The values in prop for DF1 are retained. 
I was thinking something along the lines of TrueONah&lt;-**function**(num){**if** (num\[3,3\]**&gt;**num\[4,3\]) num\[**-**4,\] **else** }
Ummm... this is a great idea! But I am not quite sure how to re-encode the data (newbie to R). Thanks tho!
Wow!
A few things to check - typically tidyverse functions do not use quotations. Try just `yearID &gt; 2010`. Second, check that column `yearID` is a numeric. Third, if you’re not saving the filtered dataframe, it’s just going to print to the console/output. Fourth, check that there are actually any rows where `yearID` is greater than 2010. 
Not tested and not fired up R in about ten months but you probably don't need the quotes around `yearID` as your test is in effect saying... "If the string "yearID" is greater than 2010" ...instead you refer to the variable name directly... filter(df, yearID &gt; 2010) 
Thank you! The issue ended up being that the yearID values weren't stored as numeric.
Hah. I did it on mobile and not used to the pipe on mobile keyboard. If I had done %&gt;%, it should work. glad that you got a solution 
Yep, totally. I've been 99% data.table for a long time now and something about ID made my brain think "group by" when I started typing. Thanks!
I was curious as how to do it in base R vs. dplyr. I've never used ```by``` so that was cool to learn. #dplyr way library(dplyr) DF%&gt;% group_by(Month)%&gt;% filter(min_rank(desc(Prop)) &lt; 4)%&gt;% ungroup() #base r way do.call(rbind, by(DF, DF$Month, function(x) { x[rank(desc(x$Prop), ties.method = 'min') &lt; 4,] } ) )
I’m having trouble writing the following equation in r. This is what I tried. part_1 &lt;- -(1/theta) part_2&lt;- -log(1-(.05(1-1^(-theta)))/1^(-theta(z)) + .05(1-1^(-theta(x)))) alpha1 &lt;- 0.05 quantileLow &lt;- sapply(imgX1, function(z) (part_1*part_2))
You probably don’t. You look for someone else who did. 
I tried to look to see if it’s out there. Google didn’t help much. Any ideas where I can look?
Maybe https://datascienceplus.com/modelling-dependence-with-copulas/ and particularly the copula package
I saw this before. But didn’t give me what I needed. I’m gonna keep looking into it 
Yeah, kind of! Each rstudio project will have it's own folder on your computer where all the files for it live. [https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects](https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects)
Ahhh ok that’s really helpful. Thank you!
Use the exp() function to do exponentiation. You keep writing 1^(-theta) which is pointless because 1^t is always 1
If you want to display code, indent each line by 4 spaces to avoid having Reddit misinterpret what you type..
At this point you have several answers to choose from, but I don't think any of them are particularly clean or elegant. I'm on my phone right now so I apologize for any formatting issues... `select_top_n &lt;- function(df, n = 3, top_cols = seq_len(length(df)), groups = NULL){` `if(is.null(groups)){` `grouping &lt;- rep(x = 1, nrow(df))` `} else {` `grouping &lt;- df[,groups]` `}` `df[Reduce("c", by(df,` `grouping,` `function(q,top_cols, n){` `Reduce("|", lapply(q[,top_cols, drop = FALSE],` `function(y,n){` `y &gt;= y[order(y, decreasing = TRUE)[min(n,length(y))]]` `}, n = n))` `},` `top_cols = top_cols,` `n = n)),]` `}` `select_top_n(df = df, n = 3, top_cols = "prop", groups = "month")` &amp;#x200B; This should do what you need it to and more, with the advantage of being written in Base R, it's also flexible. The idea is you can choose some minimum number of values to select from each column you want to filter on and you can choose how granular to group the data.
Still wrong, you need to call `desc()` on Prop so it will select the *top* 3. 
Yea I think you can do this. Let me provide steps. 1. Import both csv files separately. Label them dk.df and lah.df 2. Then I would create a draft king playerID column. Where you manufacture matching IDs to the Lahman playerID column. 3. First Load in the stringr package. You could use native R or tidyverse but stringr is easier to work with. 4. Then I would string split dk.df$Name. str\_split(dk.df$Name, " ") 5. Then use the paste0 function to paste the newly created columns together to form a new player ID. 6. From there you can paste 01 or 02 based on a conditional of your choosing. &amp;#x200B; Hope that helps! Let me know if you need better instructions.
will do. how do you write ln() --, i know log is base 10, and ln is base e... but cant figure out how to write ln in r
The `log()` function in R defaults to base e; the `log10()` function is base 10. You can pass any base to `log()`, though.
Here's a potential solution. I assume that the relationship of 09 to 01 should be 67% because there are 3 instances of 09 with matches for 01 in only 2. library(tidyverse) x1= c(01, 34, 67, 09) x2= c(01, 22, 09, 78) x3= c(09, 83, 45, 82) x4= c(23, 89, 04, 44) x5= c(04, 44, 97, 56) x50=c(78, 90, 88, 00) #see https://stackoverflow.com/questions/15162197/combine-rbind-data-frames-and-create-column-with-name-of-original-data-frames ##for some background on how to combine vectors df &lt;- mget(ls(pattern = "x\\d+"))%&gt;% as.data.frame()%&gt;% tidyr::gather(key = 'grp') linked_df &lt;- df%&gt;% add_count(value)%&gt;% inner_join(df, by = c('grp'))%&gt;% filter(value.x != value.y)%&gt;% group_by(value.x, value.y, n)%&gt;% summarize(n_y = n())%&gt;% ungroup()%&gt;% mutate(percent_relationship = n_y / n) linked_df%&gt;% filter(value.x == 9) linked_df%&gt;% select(x = value.x, y = value.y, percent_relationship)%&gt;% spread(y, percent_relationship)
The only complication I see is that with any given dk.df dataset, there will be fewer data points than the lah.df will have and thus there won't ever be a situation where there's an easy copy-paste from one set to the other. The way that I've thought about it that makes the most sense to me is that it's almost like an Excel VLOOKUP function. First I probably need a reference column that utilizes some script to take the first 5, first 2, and then number (maybe an easier way to not worry about numbers? Maybe look for 5/2/team?) and put it for each row, then I'd essentially "VLOOKUP" that reference name from the dk.df table to the lah.df data. Know what I mean? Like lemme do a quick example in the sheet. So in the Example tab, you have an example of a row of dk.df data and then Column J is the reference row where I'd want a formula to come up with basically what lah.df already has as their convention. So then with that J2 value, I would search that in lah.df and for example, pull the value in Column 7 to the dk.df table. Lemme know if I need to reword anything
&gt;do.call(rbind,by(DF,DF$Month,function(x) {x\[rank(desc(x$Prop), ties.method = 'min') &lt; 4,\]})) For the record, the `desc()` function is not Base R, it comes from the `plyr` package.
 frank_copula &lt;- function(theta, alpha, u) { return((-1/theta)*log(1-(alpha*(1-exp(-theta)))/(exp(-theta*u)+alpha*(1-exp(-theta*u))))) }
Thank you, this worked perfectly! =)
Tidyverse is everywhere! Replacing ```desc(...)``` with ```-sort(..., descending = TRUE)``` seems to do it. Would you have any other suggestions? do.call(rbind, by(DF,DF$Month, function(x) { x[rank(-sort(x$Prop, decreasing = TRUE), ties.method = 'min') &lt; 4,] } ) ) 
Yea for this you should look at the merge function in R. You can use left joining or center joining based on your needs.
Assuming you have the data in a matrix, you can filter it like this: a=matrix(c(01, 34, 67, 09, 01, 03, 23, 78, 10, 35, 95, 26), nrow=3, byrow=T) a[!(apply(a==1, 1, any) &amp; apply(a==3, 1, any)), ] * Is any of the numbers in a row equal to 1: `apply(a==1, 1, any)` * Is any of the numbers in a row equal to 3: `apply(a==3, 1, any)` * If both are true: `&amp;` * Deselect the row: `!`
Thanks!
I interpreted it as 'x contains some arbitrary samples' of which you don't necessarily know the distribution a priori.
As I understand it, R is designed for ease of use by non-programmers and it trades some speed and memory performance to that end. This makes it easy to write R code that ends up duplicating entire matrices of data rather than modifying them in-place, for example. So if you can find your same algorithm implemented in C or python/numpy etc, you may see lower memory usage that way.
`complete.cases()` works great but you can also try using `which` like `which(`[`is.na`](https://is.na)`(df), arr.ind = T)` to return locations of your `NA` values. 
You need to add show.legend = FALSE to your geom_text call. [See this.](https://stackoverflow.com/questions/18337653/remove-a-from-legend-when-using-aesthetics-and-geom-text)
Man you're amazing!!!!! Thank you so much!!
If you do that, then your samples aren’t random 
I agree, but it could be fine for plotting and exploring.
I understand that, but the electrofishing array that I am using is 2 meters long, so I am concerned if random number generator selects lets say, meter 12 and meter 13, I am effectively sampling the same area twice. 
sort() on the results? And then a for loop through the results that looks at each item in the results +/- 2?
If that's true can you use rcpp to interface with c++ and keep things within an R script? I don't know anything about it, just genuinely curious
but maybe I'll just forget about the 2 m distance as location across the stream from right bank to left bank will be different between locations. Thank you! Editting original post to remove 2 m stipulation.
like revgizmo said, these aren't random, but here's some code to do it. There's probably a more efficient way, but this should work fine. samp = sample(1:27358, 400) samp = sort(samp) d = diff(samp) while(length(samp[c(d &gt; 2,TRUE)]) &lt; 400){ samp = samp[c(d &gt; 2,TRUE)] samp = c(samp, sample(1:27358, 400 - length(samp))) samp = sort(samp) d = diff(samp) }
Imagine you have 27358-800=26558 one-meter planks and 400 two-meter planks. You want a random arrangement of these planks so that they create a 27358-meter-long route. So you have 27358-400 planks total, and 400 of them are two-meter planks at random locations. Start from sampling where to put them, and order them from the closest one to the farthest: indices_of_longer_planks &lt;- sort(sample(1:(27358-400), 400)) All planks before the first one are one-meter long, so the position of the first plank is the same number as distance in meters. The second starts one meter further, because there's one 2-meter plank before. The third one starts two meters further, etc. start_positions_of_longer_planks &lt;- indices_of_longer_planks + 0:399 And that's it. There's an issue at the beginning/end (do you allow a point at zeroth meter? at 27358th meter?), but it should be easy to resolve now.
Thank you so much! This is greatly appreciated. 
I think this is what you want: sort(sample(27358/3,5, replace=FALSE)*3)
There's more to randomness than a uniform distribution.
Can I add another wrinkle to this problem. There is an eagle nesting closure from 3,701m to 6,920 m which will prevent stopping/sampling within a few kilometer reach. Is there a way to exclude any random numbers within that range?
thank you for the help!
Factors in R are useful, but there are a lot of gotchas. To create a factor use the factor function convert anything to a factor (numbers are treated as text). In the background a, b, c would now be treated as 1, 2, 3. But b,c,d would be treated as 1,2,3 as well. One way around this is to make sure you convert all your possible factor levels at the same time..., in this case a,b,c,d. 
samples &lt;- 400 sample_range &lt;- 27358 baby_eagles_begin &lt;- 3701 baby_eagles_end &lt;- 6920 eagles_are_assholes &lt;- baby_eagles_end - baby_eagles_begin real_sample_range &lt;- sample_range - eagles_are_assholes a &lt;- sort(sample(real_sample_range/3,5, replace=FALSE)*3) print(a) a[a&gt; baby_eagles_begin] = a[a&gt; baby_eagles_begin] + eagles_are_assholes print(a)
Very nice. Do you have any examples of the resulting SDK you can show?
Also depending on the program, your choice of clang/llvm or gcc can impact memory use up to 17% I think. *was just reading about that today :) 
Here are 2 examples: &amp;#x200B; NamSor R SDK: [https://github.com/namsor/namsor-R-sdk2](https://github.com/namsor/namsor-R-sdk2) (NamSor software classifies personal names accurately by gender, country of origin, or ethnicity) &amp;#x200B; Petstore R SDK: [https://github.com/OpenAPITools/openapi-generator/tree/master/samples/client/petstore/R](https://github.com/OpenAPITools/openapi-generator/tree/master/samples/client/petstore/R)
Neato! Thanks!
/u/clamiam45 This was what I was thinking; I just needed some arbitrary data. Future data I may apply this to may or may not necessarily be normally distributed, but using it on a normally distributed sample helps me see if it roughly looks correct as well. &amp;#x200B;
Switching languages is not the preferred way as it is could be quite a long one, but, yes the dilemma between using R and Python comes to our mind every other day. u/Octopus_Kitten could you elaborate more? I don't understand the abbreviations. If you mean garbage collection, then it is done automatically in R. u/Just_pull_harder Rcpp could be the way. I guess it should be easier than switching to C and it could achieve the same results. Thanks!
Very much depends on your audience, personally I'd construct confidence intervals around the point estimates using the standard error. Are you writing up work for submission as a student, presenting a talk to others with the same level of technical understanding of model fitting, writing for submission to a peer reviewed journal where there will be text and some of the numbers can be added. Tip : Indent your output by four lines and it appears as code, e.g..... Zero-inflation model coefficients (binomial with logit link): Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -1.66434 10.39435 -0.160 0.873 TEMP -0.06170 0.51945 -0.119 0.905 HABITATSIMPLE 19.16397 23.86655 0.803 0.422 RAIN -0.04852 0.26160 -0.185 0.853 TEMP:HABITATSIMPLE -1.00921 1.28568 -0.785 0.432 --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Theta = 8.8769 Number of iterations in BFGS optimization: 49 Log-likelihood: -97.61 on 11 Df
Currently just preparing this as a student but hoping to prepare for a proper manuscript in future. Given I ran similar analyses for the abundance of multiple species, I was thinking of putting it all in a table so I don't have a bulked-out Results section with just sentences encompassing streams of values. But I've never used this sort of distribution in R before so was unsure about the two sections of output and whether both are necessarily relevant ? 
I'm with you on using tables to cut down on repetitive prose, I used to get sick of writing such crap when I worked in academia. I'd be inclined to report both, for an explanation of the output see the worked example [here](https://stats.idre.ucla.edu/r/dae/zinb/)
haha I don't think I could ever tough through working in academia, honestly! And thank you very much for the help :) 
Negative binomial models are tough to interpret by the coefficients alone, especially when you have zero-inflation parameters as well, mainly because the marginal effects typically are functions of all independent variables and coefficients. This makes it very difficult for a human to get more than the sign and relative magnitudes of the effects from the raw output. Even then, the statistical significance of marginal effects is typically neither guaranteed by the significance of the parameters nor constant over all possible values of the covariates, so you really have to be cautious about interpreting these results directly. When I'm working with non-linear models, I like use the parameters to simulate the dependent variable over an interesting range of an independent variable, with others held at their means, medians, or interesting values. Then I'll compute bootstrap or delta-method standard errors to build confidence intervals and plot it all together. This will present the results in a way that anyone who can read a chart can interpret, because the axis are all in meaningful units. Uncertainty is easy to communicate as well, as error bands are intuitive. You can even plot the simulated outcomes separately at different values of a third variable. For example, you can make a figure with predicted population on the y-axis, temperature along the x-axis, and a curve for each habitat type you measure. The resulting figure will cleanly show whether or not habitat complexity has a mediating effect on temperature, or whatever your hypothesis might be. You'll definitely want to include the full results in a table somewhere, and an appendix may be appropriate. If you're running multiple models for several species, presenting the results in a single table with columns for each species is not only appropriate, but very useful for emphasizing differences between species. You can also produce a grid of figures for all your species, or include several species on one figure. 
Did it for just shy of 20 years in the UK. The decline in working conditions of about eight years of pay rises that didn't track inflation resulting in real-term pay cuts and the attempts to "restructure" pensions, which were previously generous and off-set the relatively low pay combined with the feeling that area of work (I ended up working as a medical statistician on clinical trials) was becoming more of a conveyor belt of grant application, and obtaining funding than about the actual science and its utility led me to leave. Enjoying new job, big shift, but challenging and interesting and a refreshing change from the staid academic environment I'd found myself in.
Different OSs compile in different ways, or you could say, they translate the code into different machine languages, so in Linux, which has a very varied ecosystem, most packages don't mantain binaries. 
What about Ubuntu - I can just run apt get install "x" - does that only work because it's pulling the exact binaries for my exact operating system?
With the limited information provided, I'm going to guess your vps is running out of memory. Is this a $5 Digitalocean / Linode / etc vps with 1GB of memory?
Yes! But it's actually 2gb. Do I actually need 4gb?
Maybe, it depends on the package you're installing. I think something linke `install.packages(tidyverse)` is big and resource intensive. If you're on Digitalocean, you can temporarily resize &amp; install the packages. Afterwards go back to the smaller size. But R keeps nearly everything in memory, so your use of R will be constrained by memory size. Go ahead and keep a bigger one, a guy making 6 figures can afford it!
I prefer `ggplot2` so here is what I tried and some of my thoughts. A `geom_tile` on the raw data doesn't work very well because the "tiles" aren't well defined. So the first plot, the `geom_point`, might be your best option for plotting the raw data. Otherwise, you could bin the x and y variables (I use `cut` here) and then summarise your fill variable (I took the mean). This type of data lends itself better for plotting with `geom_tile`. library(tidyverse) d &lt;- openxlsx::read.xlsx(file_path) colnames(d) &lt;- c("WT_WT1", "WT_KO1", "KO_KO1") # rename for ease d %&gt;% ggplot() + geom_point(aes(WT_WT1, WT_KO1, color = KO_KO1, size = KO_KO1)) d %&gt;% mutate( WT_WT1 = cut(WT_WT1, breaks = 5), WT_KO1 = cut(WT_KO1, breaks = 5) ) %&gt;% group_by(WT_WT1, WT_KO1) %&gt;% summarise( n = n(), KO_KO1 = mean(KO_KO1) ) %&gt;% ggplot() + geom_tile(aes(WT_WT1, WT_KO1, fill = KO_KO1)) + scale_fill_gradient2(low = "white", mid = "red", high = "blue", midpoint = 1.5) + # your color scaling here labs(fill = "avg KO_KO1") I use `dplyr` verbs in this so if you are unfamiliar with how they work, let me know and I can provide some explanation.
As a follow up to my own comment you could also skip the `group_by` and `summarise` steps and just change the x and y axes variables to factors. This would plot the raw data as a heat map and then you would just need to clean up the labels on the axes. d %&gt;% mutate( WT_WT1 = factor(WT_WT1), WT_KO1 = factor(WT_KO1) ) %&gt;% ggplot() + geom_tile(aes(WT_WT1, WT_KO1, fill = KO_KO1)) + scale_fill_gradient2(low = "white", mid = "red", high = "blue", midpoint = 1.5) 
I've been trying out a couple GIS's lately, thank you so much for posting this!
Looks like a problem.
If you are getting out of memory errors, try adding swap space first but that really shouldn't happen in a 2 GB digital ocean machine. I am on a 1 GB one with 1 GB swap space, never had an issue like that.
Thanks for the heads up on swap drives. I'll try to figure out how to do that. What is your install proceedure on a vps? Just apt g3t install?
Depends on what you are trying to install but for third party dependencies that'll probably do the trick. If you mean swap space just google "Ubuntu set swap" and you'll find instructions. 
thanks! it worked very straightfoward https://linuxize.com/post/how-to-add-swap-space-on-ubuntu-18-04/
Hey there, &amp;#x200B; Thanks for the quick reply. I tried your approach, and have received mixed [results](https://drive.google.com/file/d/1vw-UDLu3aRO5XPbGooIVjXWNDbqIMdxz/view?usp=drivesdk). Of course, this is not optimized, but since heatmaps in ggplot2 seem more like a workaround (and therefore, for a newbie like me, more complicated to modify). With `superheat`, I get a much more heatmap-y looking [image](https://drive.google.com/file/d/1XdW0_UbYqzZWosYUC9h5_DQiVB-lW9wi/view?usp=drivesdk). I believe that there may be something wrong with the way that I prepare my data for these functions. Could you maybe provide some info on how to modify the dataframe to make it more "readable" for R? Thank you so much for help!
That example image is very helpful! When I think of a heatmap I think of what \`geom\_tile\` does where you color (x, y) squares based on a value corresponding to them. Sort of like a low resolution \[topographical map\]([http://www.floodmap.net/Elevation/CountryElevationMap/?ct=US](http://www.floodmap.net/Elevation/CountryElevationMap/?ct=US)). &amp;#x200B; I don't think the problem is with your data frame being "readable". One issue you're having with \`superheat\` is that \`heat.pal.values\` needs to be between 0 and 1, like percentiles. For example, in the below code, I set it so that the transition to the red color doesn't happen until data is above 75% of the range. &amp;#x200B; library(superheat) superheat(d, heat.pal = c("white", "red", "blue"), heat.pal.values = c(0, 0.75, 1)) &amp;#x200B;
`row.names = FALSE` is the default setting in the latest version of openxlsx, 4.1.0. Are you using the latest version? Running `packageVersion("openxlsx")` will tell you what version you have.
write.xlsx() is also the name of a function in the xlsx package, so OP may be using that version of it. In my experience, the xlsx::write.xlsx() function is far less reliable than the openxlsx version.
if you're willing to use tidyr (or the tidyverse in general) you can do this with the `spread` function. &gt; tmp &lt;- data.frame(Year=c(2000, 2001, 2001, 2002, 2002, 2002), Name=c("Jane", "Jane", "John", "Jane", "John", "Rob"), Age=c(10,11,8,12, 9,14)) &gt; new_df &lt;- spread(tmp, Year, Age) &gt; new_df Name 2000 2001 2002 1 Jane 10 11 12 2 John NA 8 9 3 Rob NA NA 14
You didn't even change the misspelled title ^and that makes it even better
Damn I love that book.
`identical(x, y)` &gt; The safe and reliable way to test two objects for being exactly equal. It returns TRUE in this case, FALSE in every other case.
I wonder if it’s faster to do some version of a shell script? Ie system(“cmp -s file1 file2”) (if you don’t actually have to read in the csv’s, just compare them to see if they’re identical)
Probably, and [stackoverflow](https://stackoverflow.com/a/12900693/7547327) agrees. R isn't known for speed. Though it might be safer to compare objects after they've been read into R if that's where the analysis is being done.
Agreed. 
Thanks, worked like a charm. &gt; identical(newcalc,oldcalc) [1] TRUE
If only it was possible to order the book with a specific theme (Solarized Dark) for the appearance.
All the tidyverse-based packages eat memory like crazy. Just use data.table. If you still want to do parallel processing, use the multicore package (parallel and collect). 
Ok, I looked at the data, and there is nothing to plot. It is like the y column is the number of seconds between the two adjacent time periods. &amp;#x200B; To get help with questions like this, I would suggest that you provide a minimal example of the code you have generated. In this specific case, it would be the minimum code to generate your current result. Then tell us why what you have done is not meeting your needs. 
I do use data.table. The problem is that when you use %dopar%, any data that your process uses is multiplied by the number of back-ends you have registered. As far as I know, there is no way to break the data up into chunks to send to the back-ends.
If comparing the files would be better before loading them into memory in R, there's a command line solution too: https://stackoverflow.com/questions/12900538/fastest-way-to-tell-if-two-files-are-the-same-in-unix-linux/12900693#12900693
Here, I just threw together an example. Obviously, it uses one of my custom functions to operate, but that's just setting up the environment for a simulation. library(data.table) library(broom) build.nsim &lt;- function(nsim,nvec){ simdx &lt;- c() for(i in 1:length(nvec)) simdx &lt;- c(simdx,rep(1:nsim,each=nvec[i])+(i-1)*nsim) dt &lt;- data.table(sim=simdx) bigN &lt;- nrow(dt) dt$n &lt;- rep(rep(nvec,nvec),each=nsim) dt$one &lt;- 1 dt$simc &lt;- dt[,cumsum(one),by=sim]$V1 dt$one &lt;- NULL return(dt) } dt &lt;- build.nsim(1000,c(25,50,100,250,500)) n &lt;- nrow(dt) dt$x &lt;- rnorm(n) dt$y &lt;- rnorm(n)+dt$x system.time(dt[,tidy(lm(y~x))[2,],by=sim]) system.time({ for(i in unique(dt$n)){ parallel::mcparallel(dt[n==i,tidy(lm(y~x))[2,],by=sim]) } parallel::mccollect() }) 
link for the lazy https://adv-r.hadley.nz/ 
Aww man mcparallel isn't available on Windows. I can't run this, but it looks like that for loop would be the holdup here, as it is not run in parallel, however I don't know what mcparallel does. Does mcparallel recognize that it is inside a for-loop in this case, and run in parallel? I found this on stack overflow, which I think answers my question: [https://stackoverflow.com/questions/17350867/split-data-set-and-pass-the-subsets-in-parallel-to-function-then-recombine-the-r](https://stackoverflow.com/questions/17350867/split-data-set-and-pass-the-subsets-in-parallel-to-function-then-recombine-the-r)
For the record, plotting gaps is plotting nothing. It took a lot of piecing together different google results, but I've gotten the end result I wanted: [https://pastebin.com/raw/W28jtcGF](code) and [https://i.imgur.com/idMUNNk.png](plot)
Bash, diff
Oh crap. I forget about windows. Um... hmm... Yeah, you might have to use dopar. Yeah, the mcparallel just forks a process for the job and keeps going. Once you hit an mccollect it waits for the processes to finish before giving the result. 
How many do you have like that? If it's a few, I think it's always easier to pop into Excel and do some cleaning there to help R along. If it's more you can probably just start with getting a lay of land first. I have not used the console for this but it should give you a decent starting point. # you can use dplyr to create a new column library(dplyr) # the new column column called 'datecheck' uses the 'datecolumn' to create an integer df &lt;- df %&gt;% mutate(datecheck = len(datecolumn)) From here you can filter the data frame and see where you're outlier date formats are. Editing after that will be up to you on how you want to handle these exceptions. Does 199 automatically turn to 1999 or 1990? &amp;#x200B; Overall, there's several approaches to handling data exceptions. The easiest would be just to exclude the data you couldn't format and explicitly refer to that in any findings. Another would be to use neighbors as a template. So if you have '199' as a year and the neighbor above is 2001. You would just select the neighbor year in place of the faulty one. Or you can also get the median date and use that as a pesudo filler. Lots of options for ways to compensate but each comes with a cost and effect on analysis. 
Yeah if you convert string to date using as.posixct() or lubridate::mdy() I believe the junk dates will default to. NA, and maybe that's alright.
This is a perfect use case for tidyr::spread()
If the dates are imported as character you can split the column by '/' and get three columns one for month, day and year. Then you can convert them to number and check each of them to see if they fall outside the expected range. Then find the right fix (or discard that data point) and combine it back into a date field.
Out of 6500 rows, there's only about a 100 or so that have this incorrect format/typos. Ended up cleaning the ones that were easy to fix, such as the 05/09/19833 one. Appreciate the suggestion for the code!
great, thanks for the suggestion; will read on the documentation for lubridate!
oh, that's an interesting method; I hadn't considered that. Thank you!
God that sounds so useful. I've been meaning to set up Ubuntu on a separate partition on my computer, this may be the push I need to do it lol.
There is also \`reshape\` in base R, if you want to avoid external dependencies. 
&gt; Can data frames hold different types of data at once? For example, one particular data frame with both character and integer data types. Yes ​ &gt; Also, what is the major difference with tables as opposed to matrices and data frames? Are tables in R considered to be contingency? [this](http://adv-r.had.co.nz/Data-structures.html) should help clarify, specifically the portions that discuss matrices and data frames ​
Data frames are basically lists. So you can have different data types in each column if you like. Matrices must all be the same type (numeric). You can use the `apply` family of functions on matrices and data frames, lists and vectors.
If you want to get a good start in R then read [R for Data Science](https://r4ds.had.co.nz)
&gt; And to confirm, I can apply matrices operations to data frames? You can, but this causes the data.frame to be converted to a matrix first, which causes all columns to be coerced to a common type. This can be OK but (especially in conjunction with factor columns) it can also mess up your data in subtle ways, so beware of this implicit conversion.
Precisely. CRAN essentially doesn’t provide precompiled binaries (except for Windows, I think?) for R packages. It provides them as source, and they are compiled on your system.
Since your question is “why”? The reason is that ggplot2 intentionally by default includes a legend for *every* geometry in your plot. This is in contrast with other plotting software, but it’s a very sensible choice because it means that by default everything in the plot has an explanation. The downside is, as you see, that legends are thus sometimes redundant and this may lead to ugly graphical artefacts.
This is really helpful!! Thanks for taking the time
You really don't need that much to import an R script from GitHub. If you save that function as an R script and post that script to a GitHub repository, then you can load it with the devtools::source_url() function. I'm not familiar with what's required with the devtools::install_github() function.
Technically, an 'R Package' as devtools sees it is any directory that contains a Description field. Could you get away with a package without any actual R code? I don't know, you could test it and find out lol. Here is a good walkthrough of the minimum it takes to create a buildable R package: [https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/](https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/)
Have a look at the package kitten package by Dirk eddelbüttel. It basically creates (a working) sekelton for a package. You can then just add the file with the function to the 'R' folder. If that is necessarily the minimum in terms of lines of code or files/folders necessary I don't know or quote sure it is not. It is certainly the minimum in terms of effort. 
https://cran.r-project.org/web/packages/pkgKitten/index.html
Yes, provide the dummy data... This will be pretty easy with the dataset.
There I have provided the first 8 rows :)
Sorry I don’t have time to go to my computer but I think you need to see the way in which R evaluates vectors. You could take a slight longer route and filter each vector where the value is equal to 2. Tidyverse and dplyr are both great packages for manipulating and could perform that quite easily 
??
I'm probably understanding this wrong. If all you're trying to do is see if the elements of `list_test` each contain *at least* one value from `vector_test`, this can be done very simply using base R: occurrences &lt;- vector(length = length(list_test)) for (i in 1:length(list_test)) occurrences[i] &lt;- any(list_test[[i]] %in% test) occurrences
T
That's a good point, I hadn't thought about it like that!
E
&gt;Data frames are basically lists. So you can have different data types in each column if you like. With the caveat that all lists (columns) have the same length, i.e. it is rectangular like a matrix. &amp;#x200B;
Why are pirates called pirates?
Not at my PC, but this should work. date_fmt = "%d-%m-%Y %H:%M" start_dt = lubridate::parse_date_time(start_str,date_fmt) end_dt = lubridate::parse_date_time(end_str,date_fmt) dt_seq = seq(start_dt, end_dt, by="15 mins")
How about this: x = seq(-5,5,0.01) plot(x,dnorm(x),type='l') 
If I were to be honest, the code you provided makes no sense. It does not work. For helpful answers please provide functional code. Let us assume you mean, &amp;#x200B; `plot(dnorm(seq(-5,5,0.01)), type="l")` &amp;#x200B; Even with the above example, you are getting exactly what you ask for. &amp;#x200B; `seq(-5,5,0.01)` &amp;#x200B; This returns a vector of length 1000, representing the values between -5, and +5, incremented by .01. &amp;#x200B; `dnorm(seq(-5,5,0.01))` &amp;#x200B; This returns the density of these values, nothing odd going on here. &amp;#x200B; `plot(dnorm(seq(-5,5,0.01)), type="l")` &amp;#x200B; Now we plot the x values, which are an index from 1 to 1000 (the range of your seq and resulting dnorm call) and the corresponding y values, those returned by dnorm. &amp;#x200B; I think the real problem here, is that you are trying to plot something OTHER than what you are asking for. &amp;#x200B; &amp;#x200B; &amp;#x200B;
So I believe the issue is with how your using 'list'. Instead you should convert your numbers to characters (if they aren't already), add a header row to your data (So your column names appear in the forest plot), and add a column with a blank header followed by "Male" then "Female" (so your row names appear on the plot) then pass that as a matrix to the labeltext option of forestplot() #Your male/female, group/beta/95L/95H data df =MY DATA #Apply as.character to each column of df df =apply(df,2,as.character) #Add a header row df = rbind(c("Group","beta", "95L", "95H"),df) #Add a row names column with a blank placeholder df = cbind(c("","Male","Female"),df) #may need stringsAsFactors=F here, just check to make sure the results are still characters, not numbers #Now hopefully passing this to forestplot () should work (I'm not near my PC at the moment so cannot test it, and have no experience with the function or plot) forestplot(labeltext=df, mean= c(348, 376), lower= c(338, 365), upper= c(358, 387))
ok
Cheers, so what I originally had was wrapped in plot() I'm not sure how I've managed to write it as it is in the OP. But I had : plot(dnorm(seq(-5,5,0.01)), type="l") And the problem is that on the plot I would like to have the x display -5 to 5 rather than all the way up to 1000, that's just the level of accuracy that I wanted within that range. I've not *used* pnorm(800) at any point, for example, 800 would correspond with 3 Does that make sense? I can't see how to do this from the plot help, it's probably got a simple name but I'm not sure of it. Thanks
Is this what you mean? plot(....., xlim=c(-5,5) 
Does this help: # make a fake dataframe with some fake data df &lt;- data.frame(matrix(1:256, nrow = 16)) vec &lt;- df [, c(12,13,15)] colnames(vec) &lt;- c('kdata','fdata','jdata') vec kdata fdata jdata 1 177 193 225 2 178 194 226 3 179 195 227 4 180 196 228 5 181 197 229 6 182 198 230 7 183 199 231 ... 
The unexpected symbol is usually a typo from say, forgetting to close a bracket. 
Thanks for posting, this is pretty cool. I'm thinking it will be useful to show gaps in some of my data to help with validation
Please use the crosspost feature.
no original plot : https://i.imgur.com/5U9h9YK.png plot with those limits https://i.imgur.com/iuQbd17.png in the original plot the values on the x axis don't represent the actual value that was given to pnorm, and that is what i would like
Try the interact\_plot function in the sjPlot package
No, parsing to Posix or Date class in R will always use a single parsing format for all rows. If you're looking to work with dates in R I can recommend the readr and lubridate packages. Lubridate makes parsing dates easy, while readr will let you specify column times a import time.
Are you looking for the `xlim=c(-5,5)` argument to plot to ignore outliers in your data?
Well it depends on the class of the column. If the column is a certain class then all elements will be of that class (this is how all cab readers I know of import dates). But, nothing is stopping you from adding a list column where one element is a date, another is a complex number, and another is a list of dataframes.
If the Posix or Date column is truely one of those classes, then it will be all the same format. There are a couple qualifiers worth noting though: 1. If you have a character class that has not been coerced to a date-type class. It can appear to have different date formats (it is just a column of strings) 2. A list is a vector that can have different data types. Although, it would be unusual to have a list column in a dataframe with different date classes. I.e., this would not be common occurrence and could be checked using str or class. I know very little about python but someone told me lists in R are similar to dictionaries in Python (someone else please confirm) Also, I will second @Tarqon’s suggestion to use lubridate. It makes working with things like leap years much easier
Hey, denzelswashington, just a quick heads-up: **truely** is actually spelled **truly**. You can remember it by **no e**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Hey BooCMB, just a quick heads up: I learnt quite a lot from the bot. Though it's mnemonics are useless, and 'one lot' is it's most useful one, it's just here to help. This is like screaming at someone for trying to rescue kittens, because they annoyed you while doing that. (But really CMB get some quiality mnemonics) I do agree with your idea of holding reddit for hostage by spambots though, while it might be a bit ineffective. Have a nice day!
This is more fundamental than rows... each column is a vector and all elements of a vector must have the same type and all data in R is in vectors. For `POSIXt` datetime vectors every element must also convert between character and datetime using the same time zone, and there is no such thing as a time-zone-naive `POSIXt` value. (... which leads to some liberal use of GMT in surprising places like the `lubridate` package.) Note that the OP describes different cells having different presentation formats in Python (as distinguished from whatever the underlying internal representation is)... R datetime data has only an implied ISO presentation format and only uses floating point seconds since an epoch as the underlying representation. you don't get to specify the 'look' of the datetime values until you convert them to character values for output. This makes working with such data very predictable, but only once it has been imported. Converting character timestamps from files into datetime data when it is formatted various ways can be tedious in R (which is where `lubridate` can help) but it won't misinterpret formats in an effort to make life easier for the analyst the way Excel will (and apparently Python will if left to its own devices).
Is lubridate way easier? I often have csv files with different posix formats. And I got used to working with as.posixct and giving the tryformats argument a vector of possible formats.
&gt; all elements of a vector must have the same type and all data in R is in vectors. The code below returns `TRUE` is.vector(list(as.Date('2019-01-01'), 55, 'zebra')) Here's an example with a data frame df &lt;- data.frame(a = 1:3) df$b &lt;- list(as.Date('2019-01-01'), 55, 'zebra') class(df$b) # [1] "list" is.vector(df$b) # [1] TRUE class(df$b[[1]]) # [1] "Date" class(df$b[[2]]) # [1] "numeric" class(df$b[[3]]) # [1] "character" Now, you have to go out of your way to create a column whose elements are not all of the same class, but it's certainly not true that all data in R is vectors, or that all elements of a list (which is a type of vector) have to be the same type.
no, please [see this response](https://www.reddit.com/r/Rlanguage/comments/b725yh/mapping_x_label_to_something_sensible/ejq1030/)
Wtf the bots are at war 
The original code I posted should do that. plot(x=x,y=pnorm(x)) Note that I have added the x=x argument before you pass the pnorm call to y
ah, thank you . Yes that's right, I thought it should be something simple, that works :) 
My current loops grab a column from each original csv file and then transpose the column into the new data frame, so I can't use the same vector for each pull. I'm just looking for a way to combine the loops, not the vectors.
I believe the slow part of your operation is the repeated file access. So if you can get the 3 columns in one file operation, you can then do the work of transposing those into the new dataframes. Wrap the code I wrote above in a single loop, then process the `vec` that comes out to make your 3 separate dataframes. 
This response is academic and for all intents and purposes irrelevant to the thread. - Both lists and matrices are vectors, regardless of the behavior of `is.vector` for the user experience. This can have significant performance advantages for matrices, and drawbacks for lists. (That is lists *are not* implemented as linked lists in R. There is a reason why `c()` is used for both atomic vectors and lists.) - The performance penalty for using lists as compared to a vectorized `POSIXct` class is significant. The concept of importing a column of Excel cells or equivalent variety of types in one column quickly becomes impractical when real-life quantities of data are being processed. The `tidyr` practice of `nest`ing data frames has practical algorithmic value primarily because it does not normally get used for wrapping single atomic values... and as the sub-data frames become smaller the processing speed penalty becomes dramatic. - The OP complained about default behavior in Python, not customized behavior. Thus arguing that you can adapt R to handle special cases is an unfair argument. There is no intrinsic support for I/O of list columns in base R or tidyverse for single files.
What? You replied to a perfectly good answer with some pedantry about how this is really about fundamental properties of the language, like all data in R is vectors and all vectors' elements are the same class. Then, when given a simple example showing none of that is even true, you accuse me of being "academic"?
Yes. Lubricate is awesome, and works with the pipe
Still expecting for BooBBCMB answer
Sounds like something that could happen in the Tidyverse, I mean be supported in the Tidyverse. 
To clarify what I mean (ie the code in the stack overflow link), I imported a csv with the column MyDate (as below) into Python's pandas. I told pandas the column was a date, but didn't specify a date format. The file was loaded with no error message. I then used `pandas.DatetimeIndex(column_name).day` to extract the day from that column - just like `datepart()` in SQL. The result is the column DayExtracted shown below. &amp;#x200B; The column imported contained days from the 1st to the 30th, so no ambiguity was possible: with a column where days are all &lt;= 12 there can be ambiguity (is 1-2 Jan 2nd or Feb 1st?) but not in a file like mine. &amp;#x200B; Yet... look at the output: pandas defaults to the American convention of month first, and only realises the first element must be the day when it's &gt;= 13. This also means that different rows of the same date column can have (as they do in this case) a different date format. To be absolutely clear: * I import the file with pandas' read\_csv function * I tell pandas the column is a date * I get no error nor warning * checking the data type of the column, it is datetime64\[ns\] , ie a date - not a string or something else &amp;#x200B; **I just wanted to be sure that something like this cannot happen in R, before deciding whether to spend some time learning it (R).** &amp;#x200B; PS In the github discussion, the pandas team doesn't seem to appreciate how big an issue this can be. What does this look like from the perspective of an R suer? Because anyone with a SQL background would probably have a stroke! &amp;#x200B; |MyDate|DayExtracted| |:-|:-| |1/1/2017|1| |2/1/2017|1| |...|| |12/1/2017|1| |13/1/2017|13| |14/1/2017|14| &amp;#x200B; &amp;#x200B;
R is bytecode compiled, not natively compiled at the moment.
I have tried the following in R: `mydf &lt;- read.table("mydates.csv" , colClasses = c("factor","factor","factor","Date"))` &amp;#x200B; but if I then `str(mydf)` it tells me the last column is a factor, nor a date. I also tried with read.csv() - same thing. **How shall I tell R that the last column is a date?** Is weekdays() the way to extract the day from a date? I have tried `weekdays(mydf$date)` and I get &gt;Error in UseMethod("weekdays") : no applicable method for 'weekdays' applied to an object of class "factor" Well, unsurprising since str() tells me the column is a factor. &amp;#x200B; A few more newbie questions / comments: &amp;#x200B; * Looking for the official syntax for read.table I found this: [https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html) which is a text-only document that seems to come straight from the Commodore 64 times! Is that the official documentation? * I looked at the [R data import export](https://cran.r-project.org/doc/manuals/r-release/R-data.html#Variations-on-read_002etable) manual on CRAN but that, too, seems written in the '80s * Finally, I found this: [https://www.rdocumentation.org/packages/utils/versions/3.5.3/topics/read.table](https://www.rdocumentation.org/packages/utils/versions/3.5.3/topics/read.table) which has a bit of formatting and is easier to read; so, what is the official documentation? It all seems quite confusing! &amp;#x200B; I have always complained about the poor documentation in pandas and other python libraries, but it doesn't seem to me that R is that much better! By contrast, the clarity of the official documentation for Matlab is striking: [https://uk.mathworks.com/help/matlab/ref/readmatrix.html](https://uk.mathworks.com/help/matlab/ref/readmatrix.html) &amp;#x200B;
There's a lot that's far from ideal going on here. You're fighting the language pretty hard to get this done. I have a few questions but here are a few general points. 1. read.csv() sucks. Use read_csv from the readr package. It's significantly faster. 2. Creating dataframes row by row should be avoided at all costs. R data frames are structured columnwise and rowwise creation can be extremely slow. 3. You're doing a lot of the same work over and over. Now on to the questions. How many csv's and what shape/size are they? Are the names that you're pulling out of the first column the same or mostly the same from csv to csv?
With lubridate you can do `which(wday(df$day)&gt;=13)`, or you can do somethin like `df$day &lt;- wday(df$date)`
Don't use base R read.csv, read.table, etc. Use the read_csv, read_delim equivalents from readr. They're much better and faster. The readr package is part of the tidyverse, which is an excellent set of well supported packages mainly for working with dataframes
The [documentation for pandas read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv) is pretty clear that dayfirst=False by default. If you aren't overriding the default, you are telling it that ambiguous dates should be handled as month first. The documentation should be updated to reflect that it is non strict [to match the to_datetime documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas.to_datetime) &gt;dayfirst : boolean, default False &gt;Specify a date parse order if arg is str or its list-likes. If True, parses dates with the day first, eg 10/11/12 is parsed as 2012-11-10. Warning: dayfirst=True is not strict, but will prefer to parse with day first (this is a known bug, based on dateutil behavior).
Yes, but the fact remains that pandas' default parser can parse each row of the column differently - this is a huge bug and something that should be shouted from the rooftops. When I pointed it out on Github, noone seemed to care. the answer was: update the docs yourself. Well, I'd love to, but that requires learning how to use Git, which isn't exactly easy nor user friendly. I
&gt;Don't Thanks. I'll try. Still, why does that not work?
I have tried tidyverse, but how do I specify the date format? This doesn't work because it doesn't recognise the dates `df2 &lt;- read_csv("mydates.csv", col_types=cols('date'=col_date()))` I tried: `df2 &lt;- read_csv("mydates.csv", col_types=cols('date'=col_date(), format="%d/%m/%Y" ) )` The syntax is wrong, but what is the right one? &amp;#x200B; By the way, first impressions are that the documentation for R and Python both suck biiiig time! And R's date import functionality doesn't seem that much better, since R, too, fails to infer the date format, even though no ambiguity is possible.
This whole part is preceeded by a computation of coordinates from a dissimilarity matrix, and then the transformation of those coordinates into distances. This gives d(X). So we start with the first one, and compute the first x value (.50). My algorithm works until here. It computes X matrix, D matrix and x correctly. Then what I do is: 1. I use Guttman transformation to compute a new X 2. I compute a new D(X) 3. I use my new D(X) and the original delta to compute my new x value. Is it low enough? No, so I continue. 4. I compute a new B, that looks at the new X for zeros, but takes the numbers of the first X. 5. Guttman transformation... Now somehow, the first time I do this, the number decreases from .50 to .22, so I assume it’s working. Then, after the first computation of xt, the number starts increasing, until it stops at 157 iterations with the number 22. My x should approach zero, instead my matrix X approaches zero and x becomes bigger and bigger, so maybe I’m missing something in the process.
&gt;u/0neverafrown0 lmao Frank isn't gonna be happy about this.
This is my "ask internet" phase. Aren't we entitled to it? Haha
Hahaha, I'm just messing with you. Seems like you're doing a lot better than me so far. Good luck on your assignment.
It only seems like it... Thanks though :) You too!
I don't think many people will be able to help you unless you show your code. (I'm also not sure how much help you will get, given this is homework)
I was practicing loops and realized my answer was wrong. Mainly, I was evaluating the Peak_Magnitude to the YawRateChange that was below the lower threshold. Obviously, that always evaluated to false. The ```mutate``` sandwiched between the ```slice()``` takes the first YawRateChange that was above the upper threshold. ```last``` is used because the last row of this subset was filtered based on being above the threshold. For the Geniune_Response, I used ```near()``` in case there was a floating point issue, but I assume that's probably unnecessary. sample_data%&gt;% group_by(ppid_train)%&gt;% filter(max(abs(YawRateChange)) &gt; ut, min(abs(YawRateChange)) &lt; ut)%&gt;% mutate(Peak_Magnitude = max(abs(YawRateChange)))%&gt;% slice(1:head(which(abs(YawRateChange) &gt; ut),1))%&gt;% mutate(subset_Peak = last(YawRateChange))%&gt;% slice(tail(which(YawRateChange &lt; lt),1))%&gt;% ungroup()%&gt;% mutate(Genuine_Response = if_else(near(Peak_Magnitude, subset_Peak), T, F)) Finally, provided as reference, here's the the loop function. I was curious about performance so I fiddled around with different solutions. Not that performance matters because even with a million rows simulated, all solutions were still less than a second. do.call(rbind, by(df[['YawRateChange']],df$ppid_train, looping_f) ) looping_f &lt;- function(v) { last_below_lt &lt;- NA_integer_ last_above_ut &lt;- NA_integer_ Actual_Response &lt;- NA wmax &lt;- which.max(v) n &lt;- length(v) #Loops until first value above threshold for (i in 1:n) { if (abs(v[i]) &gt; ut) { last_above_ut = i; Actual_Response = (wmax == i) break } if (abs(v[i]) &lt; lt) { last_below_lt = i } } #Returns information only if the thresholds were exceeded if (!is.na(last_above_ut) &amp; !is.na(last_below_lt)) { return(data.frame(Genuine_Response = Actual_Response, Peak_Magnitude = v[wmax], YawRateChange_before_Response = v[last_below_lt]) ) } } 
Your code is probably wrong somewhere. Check that your B matrix is what you think it is, your V matrix is what you think it is, and that the appropriate products are what you think they are.
OP can also simplify to remove the other function and call ```read.csv``` directly. I think you can directly add which columns to skip but I didn't teat it which is why it's commented out. setwd("H:/csv/") filenames = list.files(pattern="*.csv") All_data&lt;-do.call(rbind, lapply(filenames, read.csv $, colClasses = c(NA, $ rep("NULL",10), $ NA, NA, "NULL", NA) ) 
Loop through `list_test`. Compare each vector in `list_test` to `test` via `match()`. result &lt;- vector(length=length(list_test)) for(i in seq_along(list_test)) { result[i] &lt;- any(!is.na(match(test, list_test[[i]]))) } &amp;#x200B;
[Here's the documentation](https://readr.tidyverse.org/reference/cols.html) col_date(format = "") A better solution would be to specify the date format in the locale and let the readr auto-detection do its thing. read_csv("test.csv",locale = locale(date_format = "%d/%m/%Y")) I just played around a bit with readr and it behaves better than pandas when the format is not provided and the default is incorrect(or the provided format is incorrect). It will either give warnings that the parsing had problems or will not auto-detect the column as a date and pull it in as a string. [bookdown.org](https://bookdown.org) has some very good resources for R, [R for Data Science](https://r4ds.had.co.nz/) and [R Programming for Data Science](https://bookdown.org/rdpeng/rprogdatascience/) will be particularly useful when learning the basics. [The swirl package](https://swirlstats.com/) provides some good exercises to learn how various things work. Are you using RStudio? If so you can type the name of any function into the search bar in the help window and it will bring up the documentation for that function. I won't speak for the rest of R, but the tidyverse is one of the most well documented and well supported sets of packages in any programming language. Neither base R, tidyverse, nor Pandas([infer_datetime_format=False by default](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv)) infer the date format. They use the format that you have chosen and if you haven't provided a format, you have chosen to use the default. 
This should do the trick, and be quite a bit faster. There's more room to speed this up, but based on the available info, this is pretty good. library(dplyr) library(readr) library(tidyr) setwd("H:/csv/") filenames &lt;- list.files(pattern = "*.csv") full_table &lt;- data.frame() for(file_name in filenames){ temp_df &lt;- read_delim(file_name, delim = ";")[,c(1,12,13,15)] names(temp_df) &lt;- c("col_id", "k_val", "f_val", "j_val") temp_df$source_name &lt;- file_name full_table &lt;- bind_rows(full_table, temp_df) } kdata &lt;- full_table %&gt;% select(source_name, col_id, k_val) %&gt;% spread(key = col_id, value = k_val) fdata &lt;- full_table %&gt;% select(source_name, col_id, f_val) %&gt;% spread(key = col_id, value = f_val) jdata &lt;- full_table %&gt;% select(source_name, col_id, j_val) %&gt;% spread(key = col_id, value = j_val)
The data they used is Default from the library ISLR. The left plot is a line from the linear model between default and balance. The right plot is a result from logistic regression, using the glm() function and 'binomial' as the family. There will be lots of StackExchange posts on plotting logistic regression in R.
Basically, what these graphs compare is linear regression and logistic regression to classify the probability of default `y` based on the balance `x`. There are tons of ways in which you could generate these values. One option is to create two vectors sampled from a normal distribution, using a different mean: x_0 &lt;- rnorm(n = 1000, mean = 1000, sd = 500) x_1 &lt;- rnorm(n = 300, mean = 2800, sd = 500) You can then create a dataframe by combining these two vectors, and assigning the value 0 to the first vector, and 1 to the other: df &lt;- data.frame(x = c(x_0, x_1), y = c(rep(0, 1000), rep(1, 300))) Then, for the first plot, you can plot x and y and add the regression line obtained from a linear model, using `lm`: plot(df$x, df$y) abline(lm(y ~ x, df)) And for the second plot, you can do the same with a logistic regression, using `glm`: m &lt;- glm(y ~ x, family = binomial, df) plot(df$x, df$y) curve(predict(m, data.frame(x = x), type = "resp"), add = TRUE) I kept this all in base R because I thought it'd make the syntax a bit clearer, but the plots could be made much prettier using `ggplot2` for instance. Let me know if any of this is unclear! 
&gt; The data they used is Default from the library ISLR. &gt; &gt; yeah thanks, i should have mentioned that in the post. I was curious about simulating it though. Everything you say makes sense, but wasn't what i was referring to. Sorry I wasn't clear enough
I'm well aware. I don't know how this is relevant, though: WebAssembly is likewise bytecode for a virtual machine.
Let me know what you think. library(tibble) library(dplyr) library(ggplot2) balance_threshold &lt;- 1200 damping_factor &lt;- 150 GetProbabilityOfDefault &lt;- function(pBalance, pBalance_threshold = balance_threshold, pDamping_factor = damping_factor) { return( 1 / (1 + exp((pBalance - pBalance_threshold) / pDamping_factor)) ) } dat01 &lt;- tibble( balances = seq(0, 2000, 50), probabilities_of_default = GetProbabilityOfDefault(balances) ) ggplot(dat01, aes(x = balances, y = probabilities_of_default)) + geom_point() + geom_line() + labs( title = "Probability of Default vs Balance", x = "Balance", y = "Probability of Default" ) num_defaults &lt;- 100 num_non_defaults &lt;- 50 default_mean &lt;- 500 default_sd &lt;- default_mean * 0.5 nondefault_mean &lt;- 1100 nondefault_sd &lt;- nondefault_mean * 0.5 dat_with_noise &lt;- tibble( bal_theoretical = c( rnorm(num_defaults, mean = default_mean, sd = default_sd), rnorm(num_non_defaults, mean = nondefault_mean, sd = nondefault_sd)), bal_empirical = sapply(bal_theoretical, function(x) rnorm(n = 1, mean = x, sd = 50)), outcome = factor(c(rep("default", num_defaults), rep("non-default", num_non_defaults)), ordered = FALSE) ) # Randomize rows set.seed(9123) dat_with_noise &lt;- dat_with_noise[order(rnorm(num_defaults + num_non_defaults)), ] ggplot(dat_with_noise, aes(x = bal_theoretical, y = bal_empirical, color = outcome)) + geom_point() + labs( title = "Empirical Balance vs Theoretical Balance", x = "Theoretical Balance", y = "Empirical Balance", color = "Outcome" ) model1 &lt;- glm(outcome ~ bal_empirical, family = binomial(), data = dat_with_noise) dat_with_noise$probability &lt;- predict(model1, dat_with_noise, type = "response") dat_with_noise$outcome_predicted &lt;- as.factor(ifelse( dat_with_noise$probability &lt; 0.5, "default", "non-default")) ggplot(dat_with_noise) + geom_point(aes(x = bal_empirical, y = 1 - probability, color = outcome), size = 3) + geom_line(aes(x = bal_empirical, y = 1 - probability)) + labs( x = "Balance", y = "Logistic Regression Value", title = "Logistic Regression Output vs Balance" ) --- [Plot 1](https://i.imgur.com/ZpCaOwV.png) [Plot 2](https://i.imgur.com/av6zNhy.png) [Plot 3](https://i.imgur.com/Fuvx0Wq.png)
that's a great base , thank you. i didn't mean for it to be so involved, i was mainly asking about the generation of the values and stuff, but that's a big help. thanks
that's great, i can port that over to ggplot if needed, what you did makes sense though. Thanks for using base, i don't know all the dplyr stuff :/ cheers!
Great, glad I could help!
Why do you think you need to use `for each`? That is a parallel programming construct that seems completely out of place here... are you a JavaScript programmer? This looks like a job for the `ifelse` function... if you are using `xpos` for azimuth and `ypos` for elevation... dta &lt;- data.frame(xpos=runif(n,0,360),ypos=runif(n,-90,90)) dta$hemi &lt;- ifelse( dta$ypos &lt; 0, "bottom", "top" )
Thank you so much for this comment! I had noticed the previous error and managed to edit my code a different way. Before I filter the YawRateChange for lower and upperthresholds, I perform this: `workingdata &lt;- workingdata %&gt;%` `group_by(ppid_trialn) %&gt;%` `mutate(PeakMagnitude = max(abs(YawRateChange))) %&gt;%` `mutate(Genuine_Response = if_else(PeakMagnitude &gt;= magnitudethreshold, TRUE, FALSE))` So basically, I calculate a peak magnitude from the YawRateChange, and then compare this to a magnitude threshold that I have chosen a magnitude threshold and assign TRUE for genuine responses, and FALSE of responses below it. &amp;#x200B; Thanks again for the advice, really appreciate it!
RemindMe! 2 days May the odds be ever in your favour
I think you've outlined a lot of the main arguments for/against R/python. I don't expect that you will not get a convincing answer. What language people use a lot of times (when choice is permitted) is a matter of personal connection, familiarity, and also community (e.g., do you colleagues use it). I will add two points: - For pure data analysis, R's syntax is typically much shorter. You need to load less libraries, don't refer to namespaces explicitly (with the assumption that the number available is far less than python). That's one of the advantages of a DSL. - I think "easy to write bad code" in R likely originates from a preference for language syntax. On the other hand with little effort it is easy to write something like &gt; f &lt;- function(x) function(y) x + 2*y &gt; g &lt;- f(3) &gt; g(2) [1] 7 &gt; assign("x", 4, environment(g)) &gt; g(2) [1] 8 You can modify almost any variable in any scope any time, which can make it possible to write dangerous ("bad") code with little effort in R.
I will be messaging you on [**2019-04-03 11:13:17 UTC**](http://www.wolframalpha.com/input/?i=2019-04-03 11:13:17 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/b80qx7/r_vs_python_for_lightweight_data_analysis_what/ejv7mqb/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/b80qx7/r_vs_python_for_lightweight_data_analysis_what/ejv7mqb/]%0A%0ARemindMe! 2 days ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
PS This discussion [https://www.reddit.com/r/statistics/comments/8de54s/is\_r\_better\_than\_python\_at\_anything\_i\_started/](https://www.reddit.com/r/statistics/comments/8de54s/is_r_better_than_python_at_anything_i_started/) shows some flaws in how some statistical methods were implemented in the Python libraries. Very interesting, but largely irrelevant for my workflow. &amp;#x200B; I have found stuff in pandas that makes me cringe, like the fact that [a date column can have multiple date formats](https://stackoverflow.com/questions/55309199/pandas-read-csv-can-apply-different-date-formats-within-the-same-column-is-it) (one row can be dd-mm-yyyy, another mm-dd-yyyy) but I have so far not found something that I can only do in one language and not in the other.
As others have said, it largely depends on personal preferences. I use both languages, both are good, but I find R more "agile" for data analysis, since the syntax is shorter. Also, RStudio integrates well with SQL dbs, and you get used to it, the tidy ecosystem is nice, stable and relatively fast (I prefer base R, but that's personal preference). I think R and tidy verse fits well in your workflow TBH, so give it a try.
Thank you. Actually I hate the fact that in R you don't refer to namespaces explicitly (you can use library:;function but not everyone seems to do it) - but this alone is no reason to ditch R. &amp;#x200B; So do you pretty much agree that, for the kind of workflow I have outlined, R and Python are probably equally good? &amp;#x200B; How about scalability, debugging and testing in Python vs R? Do you have experience / much of an opinion in that regard? &amp;#x200B; **And how about OOP?** I understand that R was not initially OOP, but is the current implementation so bad? Many Python programmers seem to complain it's an afterthought, but I don't care much that R didn't have OOP 20 years ago - I only care if it can do what I need now. For example, in many of my analyses I use custom classes in Python. Often simple stuff; say you are comparing countries, you define the class Country, define what attributes it contains, define some methods to run basic calculation on those attributes (eg GDP per head if you have total GDP and population), etc. **I can do all of this in R, too, can't I?**
With ggplot, or with other plotting libraries, can you interactively zoom into a chart, expand the window of the chart, and interactively track the x and y coordinates as you move the mouse? None of this affects how the final chart is produced and shared, but I find it very very useful when I am deciding which charts to keep, how to format them, etc.
 By OOP, do you mean the S3, S4, or R5 system? R5 is most similar to Python but its syntax is not pretty (IMHO). S3 has been part of R even more than 20 years ago and is quite integral to it, but it takes some getting used to (kind of like CLOS in Lisp, though single-dispatch. S4 is multiple dispatch). I can't speak much to the testing facilities in R, but you can find that this is quite within its scope of capabilities. Scalability is another issue - Python is touted as being more scalable, typically; prehaps because it integrates with web frameworks and a broader range of production-related tools. Handling data though, they both store data in memory to do computation so they are fundamentally not so different. Python is more efficient in that lists and data frames are copy by reference and not by value and object overhead is probably less. Again for R, since you can modify any variable in any scope it's difficult to be efficient with garbage collection. There are some memory mapping facilities in R and Python - don't know who they stack up against each other. In essence, you can do everything in R or Python. If you want to switch, you can give it a try and you'll find things you like and things you don't like. If you don't want to switch, you will also be fine. 
Echoing other comments, but where the two clearly overlap, it's down to personal preference. For interactive or exploratory analytics I really like RStudio. I think notebooks in general are overrated, but others disagree.
Could plotly as it is interactive chart not be able to zoom and expand off? I haven't done much with plotly but I believe it could do that. Plotly works with both Python and R though. 
Maybe setting up a GUI is easier in Python? Say I have, I don't know, 6 parameters I need to set before running my analysis, and I want to set them with a dropdown box from a GUI?
&gt; Actually I hate the fact that in R you don't refer to namespaces explicitly I have used namespaces in C++, but I don't understand how they are used in Python. However, R has the concept of *environments*, which are every bit as powerful as the namespaces in C++, if not even more so.
The most common way in Python is to assign an alias to the packages you import, like you assign an alias to a table in a SQL query. `import pandas as pd` `df=pd.DataFrame()` You can also do `from sqlalchemy import create_engine, MetaData, Table, select` Or to compare if two functions of two packages, but with the same name (this can also be a newer version of the same package) produce the same output: `import my_amazing_package as old` `import my_new_even_better_package as new` `x=old.function(my_input)` `y=new.function(my_input)` I think in R it's easier to mess things up by overwriting some function names without realising
I have no comparison or you to python, but I can throw out some info for reading data into R, especially as it relates to guessing column type. As far as inferring proper data type you should check out read_csv from the readr package and fread from the data.table package in R. They use different methods for guessing type (sounds like fread may be better for your how your data is set up), but read_csv can be altered to consider the whole column (or, e.g., 80%) when guessing type. **From readr' page on reading column types (e.g., read_csv):** &gt; One of NULL, a cols() specification, or a string. See vignette("readr") for more details. &gt; &gt; If NULL, all column types will be imputed from the first 1000 rows on the input. This is convenient (and fast), but not robust. If the imputation fails, you'll need to supply the correct types yourself. &gt; &gt; If a column specification created by cols(), it must contain one column specification for each column. If you only want to read a subset of the columns, use cols_only(). &gt; Alternatively, you can use a compact string representation where each character represents one column: c = character, i = integer, n = number, d = double, l = logical, f = factor, D = date, T = date time, t = time, ? = guess, or _/- to skip the column. You could also change the guess_max argument of readr to make it better at guessing type. Beware that I think the default is to convert a column with only blanks as logical and not string (at least, that is what is used to be). So, if you files start with thousands of blank for some columns, you will definitely want to explicitly specify type of have it guess more rows. **From fread's documentation:** &gt; colClasses: A character vector of classes (named or unnamed), as read.csv. Or a named list of vectors of column names or numbers, see examples. colClasses in fread is intended for rare overrides, not for routine use. fread will only promote a column to a higher type if colClasses requests it. It won't downgrade a column to a lower type since NAs would result. You have to coerce such columns afterwards yourself, if you really require data loss. Also from fread's documentation: &gt; A sample of 10,000 rows is used for a very good estimate of column types. 100 contiguous rows are read from 100 equally spaced points throughout the file including the beginning, middle and the very end. This results in a better guess when a column changes type later in the file (e.g. blank at the beginning/only populated near the end, or 001 at the start but 0A0 later on). This very good type guess enables a single allocation of the correct type up front once for speed, memory efficiency and convenience of avoiding the need to set colClasses after an error. Even though the sample is large and jumping over the file, it is almost instant regardless of the size of the file because a lazy on-demand memory map is used. If a jump lands inside a quoted field containing newlines, each newline is tested until 5 lines are found following it with the expected number of fields. The lowest type for each column is chosen from the ordered list: logical, integer, integer64, double, character. Rarely, the file may contain data of a higher type in rows outside the sample (referred to as an out-of-sample type exception). In this event fread will automatically reread just those columns from the beginning so that you don’t have the inconvenience of having to set colClasses yourself; particularly helpful if you have a lot of columns. Such columns must be read from the beginning to correctly distinguish "00" from "000" when those have both been interpreted as integer 0 due to the sample but 00A occurs out of sample. Set verbose=TRUE to see a detailed report of the logic deployed to read your file. Also, it sounds like you may like to set verbose=TRUE when using fread. It has been very helpful for me! Also, fread is very fast (not that will notice too much of a difference when work with files 5-100mb). **For importing dates:** I can be pretty neurotic, so I do not necessarily trust anything to guess my date format. The pandas issue would drive me crazy! In readr, you can specify it in the syntax but fread will read them in as characters. For fread they recommend using the package fasttime. I personally just prefer to convert the character vector to date using base functions or lubridate. Also, unrelated to dates. Rmarkdown may be more flexible than you think. But if you expert data into excel it is very easy in R. For example is you use the writexl package you just put a named list as the first argument and it will create different sheets in excel. Something like this: sheets &lt;- list( "sheet1" = Rdataframe1, "sheet2" = rdataframe2, "sheet3" = rdataframe3, ) write_xlsx(sheets, "C:/Users/denzelswashington/example_excel.xlsx", na = ") 
Maybe it’s just me but I feel like both languages have overcome a lot of the differences they had 5 years ago. The reality is that most of us that aren’t doing intense ML/DL work can probably use either language pretty seamlessly. 
Modern R and modern python are virtually as feature parity now. Many prominent package developers in both languages now work together to ensure that python and r have the same or similar features. The most prominent techniques use python or r as a wrapper to call the same c++ or java code underneath (spark, h2o, etc). I honestly don’t think that one should spend any time learning the other language if you already know one of them. However, it may be much more useful to improve your Scala, Sql, c++ or another complementary language instead. 
My lab has found Python really useful for text parsing tasks and as a general "glue" language. R isn't really so great at text parsing, imo. We also have tons of bash scripts we use fairly frequently whose outputs are then fed into R, and using Python as a "glue" language for automation or fully replacing the bash script with a Python script has been a side task for a couple of us.
The convention in R seems to be to refer to the namespace in the function call. dplyr::filter() stats::filter() You get the idea... :)
Your SQL example is really helpful to me. I agree with your point about R being "*easier to mess things up by overwriting some function names without realising*". If you're working interactively, when loading a new package, R will warn you when a function name is being masked. R's *environment* functionality is really powerful, but it's not intuitive. Hadley Wickham has [a great overview of environments](http://adv-r.had.co.nz/Environments.html) in his [Advanced R](http://adv-r.had.co.nz/Introduction.html) book.
I have used both python and r extensively for text parsing tasks and greatly prefer r. I would never counsel someone to switch from python to r or vice versa, but I certainly don’t agree that python has any advantage. As you know, python vs bash for general scripting is also a subjective choice. If your comment is just a general comment, thanks. If it is intended as a reason to use python over r, I don’t agree. 
You might be interested in [my {modules} package](https://github.com/klmr/modules), which makes such module namespacing available in R^(1). I’m currently in the process of finalising a new version of this package, which drastically improves on the current version (in particular, [it vastly simplifies its usage](https://github.com/klmr/modules/issues/129)). --- ^1 (not to be confused with a CRAN package of the same name. My package’s name will change soon.
&gt; it would be easy to write bad code in R; how so? And why should this be less so in Python? Because Python’s type system is better developed. Both R and Python are late bound but Python’s type system is considerably stronger than R’s; meaning, (much) fewer implicit conversions, and it’s more encouraged to extend the type system for your own use (though R totally makes this possible). In practice, people tend to cram everything into lists and data.frames in R even where, in Python, you’d use dedicated types. Furthermore, Python’s type annotations actually allow static type checking even though this is (still?) very much in the minority. &gt; Python is more scalable, more production-ready and better-suited for bigger analyses rather than small scripts; again: why / how? The main reason (apart from types, mentioned above) is the fact that you’ve alluded to in another comment: R’s lack of modern modularisation, including proper namespacing. R’s packages, and CRAN, were groundbreaking for its time but by modern standards they lack important features (ease of deployment, nesting, handling of name collisions, to name just a few).
This seems incredibly useful - thanks!
&gt;But it falls short because Python doesn’t allow metaprogramming with the same ease as R. As a consequence, embedding domain-specific languages for data analysis in R (via NSE and formulas) is fundamentally a lot more seamless than is possible in Python. Could you please elaborate a bit on this? I am very keen on understanding this more, but it's not very clear to me right now. I admit I am not familiar with metaprogramming, and even less with how it would apply in the context of data analysis in Python or R
Just a note, your R example of code that modifies bindings in different environments can be directly translated to Python: def f(x): return lambda y: x + 2 * y g = f(3) g(2) # 7 g.__closure__[0].cell_contents = 4 g(2) # 8
In my opinion for the tasks you are describing R is superior to Python. The main reason is the code compactness that R offers, pipes are a godsend and make it much more readable. On the same foot the functional side of R combines with he previous point brilliantly and composing functions using pipes feels just right. SQL, Excel are all fine in R ecosystem as is defining the data yu are importing and the speed of the said imports. I suggest giving R a good honest try and seeing the superiority of compact for ad hoc/small analysis.
&gt; If your comment is just a general comment, thanks Just a general comment! I think the general feel of this thread is more correct: Use whichever your feel more comfortable with/your colleagues use the most.
I'd strongly recommend checking out the "Radiant" package in R for lightweight data analysis. --&gt; https://vnijs.github.io/radiant/ "Radiant is an open-source platform-independent browser-based interface for business analytics in R. The application is based on the Shiny package and can be run locally or on a server. " 
&gt; Furthermore, Python’s type annotations actually allow static type checking even though this is (still?) very much in the minority. Is any kind of static typing / type safety checking possible in R at all? In Python it was added with Python 3.6 and it seems an afterthought, but it's better than nothing. This alone is a huge difference for me. Maybe a workflow could be to import the data, cleanse it, export it to SQL and run some first exploratory analyses with R, then do more complex scripts/programs with Python reading the data from SQL? The key would be that shorter analyses might be faster with R and tidyverse, and dynamic typing is less of an issue with shorter scripts. But, in bigger programs with tens of classes and functions calling each other, then static type checking can help prevent bugs - biiiig time. Does this make sense?
Metaprogramming just means that you can treat pieces of code as unevaluated expressions instead of directly (“eagerly”) evaluating it. This way you can transform expressions at runtime, and then optionally evaluate them. dplyr uses this to perform scoped computations, e.g. dataset %&gt;% select(- some_column) %&gt;% filter(foo %% 2 == 0) %&gt;% mutate(bar = foo + (1 : n())) %&gt;% summarize(foo = complex_function(bar * 2)) Here, the parameters to the functions are unevaluated expressions. Even the expression chaining (“pipe”) operator `%&gt;%` is implemented that way. The closest that Pandas can get to this syntax is tmp = dataset \ .drop('some_column', axis=1) \ .query('foo % 2 == 0') tmp = tmp.assign(bar=tmp.foo + range(len(tmp.index))) tmp.foo = tmp[['bar']].agg(lambda x: complex_function(x * 2)) The number of lines is the same but the R code is a lot more streamlined, in particular since all functions in the pipeline work by similar principles and receive the same kind of parameter (namely, names and expressions bound to these names). By contrast, the Python code needs to introduce a temporary variable, each method takes arguments in a different way even if they do different things, we need to repeat the name of the variable we’re operating on, and some expressions are [stringly typed](http://wiki.c2.com/?StringlyTyped) (which is an anti-pattern), rather than being naked names/expression (especially the `query`). Python *can’t* replicate the clean R API because it has no way of accepting unevaluated expressions. The closest it could get would be to accept each argument as a string and parse that: dataset \ .select('- some_column') \ .filter('foo %% 2 == 0') \ .mutate('bar = foo + (1 : n())') \ .summarize('foo = complex_function(bar * 2)') … I hope it’s obvious that this would be atrocious: we’re writing strings here, not Python. We have no type safety, no static checks, no syntax highlighting and no IDE support. And yet Pandas’ `query` method already does exactly that.
Agreed. And to answer ops direct question... no need to switch from one to the other. 
&gt;Is any kind of static typing / type safety checking possible in R at all? Not currently. There’s a branch of the R source that adds syntax for it, but it’s experimental and there are, as far as I know, no plans to ever merge it into master. Thanks to R metaprogramming you *can* actually cobble together an implementation even today but (a) there are no tools to take advantage of this, and (b) R is inherently more dynamic than Python so it’s less useful. It would for instance not work with dplyr or similar code. RStudio has attempted to add static checks for R code and they fail horribly for exactly that reason.
Thank you, that's very interesting, and very useful in understanding the differences between the languages. Most of the resources I have found online are too generic to be of any use (yes, I know R was developed for statistics and Python can be used to make games, but all of that's irrelevant for me!). I wish it were easier to find detailed comparisons like this!
Just go and check out the "fitdistrplus" package for R. I use it all the time.
That's actually pretty easy in R by using Shiny. Similar things in Python would be Dash or Flask.
Yes, you are correct. Plotly generates interactive charts and works in both worlds.
Cool, maybe I should have expected that from Python - I think there are probably more complex, unadvisable cases where it's possible to modify objects in R that isn't permitted in Python...
thank you!
And you can use ggplotly to translate ggplot objects into plotly objects (almost) seamlessly!
Also, visualisation in R is so much quicker, prettier, and less fussy.
Be warned, (and I say this as a huge fan of plotly), plotly can be a resource hog if you have too many data points on a single plot.
I run out of RAM when I try to use read_csv instead of read.csv. The current loops transpose the data. The original files (over 35,000) are hourly, collected data (rows are locations, columns are data types). The loops grab specific data (a column) from each hourly file and transpose them into a new table (rows = hour of collection time, columns = locations). Each hourly file is about 25 col x 8000 row.
Thank you. I'll try this out when I have time and edit my reply. I've had to use my existing method again for now. It isn't something I have to run very often; I was re-running it just to make sure the scripts didn't need updating (a few bits did because of package changes).
This is the killer package that keeps me tied to R. I can build a complex dashboard with no webskills.
Can you explain how R handles SQL better than python? I'm not arguing, I'm genuinely curious as I've only recently started using SQL (Teradata) in R.
I'm on the fence. I love not needing namespaces but every once in a while it causes something to break. Though most of the breakage is when I call variables something that overlaps with something. Ironically I end up getting more breakage from silly things. like recently I was applying summarise_all with the base R mean function and everything was honky dory. Then I added a sum function and apparently it doesn't know how to handle/ignore dates and drove me nuts figuring that out. I even did a summarise_if is.double and the dates still kept breaking things. I don't understand how base r mean() has a provision for dates but base r sum() doesn't.
The real answer is Non Standard Evaluation. R has it, Python doesn't: http://adv-r.had.co.nz/Computing-on-the-language.html
Normally there are help pages on each package for each data set. Have you looked?
Yeah, I got it figured out. 
He said R Studio. The IDE allows you to make a dB connection and see the tables and columns in the data explorer, will auto complete column names, etc.
yeah，it's may be conprehensive. However, logistic distribution, log-logistic distribution, log-normal distribution, exponential distribution are difficult to understand. as show in the question, we can estimate intercept parameter and log scale parameter for each parameter distribution, even with those imformation i still continue my work. and really need your help and many thanks.
yeah I would assume that. Plus it isn't as easy to make nice plots with it that I have discovered as of yet. Still learning though
* First you need to decide if you need to only look ahead, or if you need to look behind as well. * Give ids to the MRI scan * Order MRI scans by date (they can be reordered after since they now have ids) * Use lapply/sapply/for loop on the PET scans PET$MRI_ID &lt;- lapply(PET$Date,function(x) { # return first MRI_ID return(MRI$MRI_ID[which(MRI$Date&gt;x)][1] }) %&gt;% unlist * Use `data.table ` to merge the two dataframes df &lt;- merge(PET,MRI,by="MRI_ID",all.x=T) That appends MRI info to the PET dataframe. You could do it in reverse as well. 
not sure
You can use a for loop to do it. I’d also suggest the lubridate package for calculating days that are closest. 
I had to distinguish (North/South and East/West) of the points, so in my head it made sense. Thanks for the response, though. 
It’s a good starting point for sure. Throughout the course it could be helpful to work on a side project with data you are interested in to implement what you have learned without the guide rails that datacamp provides. Many people like to shit on datacamp in here, and while I don’t think it is sufficient if used alone, it is definitely a step in the right direction. Good luck!
Thank you! Would you recommend some other website to learn? 
Hadley Wickham’s book R for data science is free online and is a great resource for teaching yourself R: https://r4ds.had.co.nz
THANK YOU :D
I really think the main issue with learning through datacamp is that you do not build a library of code that you can refer back to, so you will find yourself often going back to the same videos, where as if you had the code saved off somewhere you could just go back and look. In some cases, where they use default data sets in R like the Iris or Cars data set, you can get around this by copying your finished exercises and saving them into an R script. As you can imagine, this can get tedious. Additionally, I have run into some courses that are using proprietary data, which is not easily found online. Once you have built a decent foundation, I highly recommend the data science specialization from Johns Hopkins [on coursera](https://www.coursera.org/specializations/jhu-data-science). You can audit the class for free, or if you want the certificate you can pay, but that course really opened my eyes as to all the possibilities in R.
it's a good way to get the basics. For my, though, I didn't really get comfortable with R until I threw myself into a project and just googled and learned as I went along.
I still have a few months before my project starts so I’m not sure what I should do to prepare till then. How comfortable are you now? Do you have a cheat sheet of the stuff you googled? Does that help?
Pretty much this, except I'd do ```which.min(abs(MRI$Date - x))```
I have a couple of bigger projects that I refer back to fairly frequently. One thing that has really helped me is to create Rmarkdown docs rather than just r sripts. To me it's an easier way to compile code and output in a comprehensible (and searchable) way. I'm fairly comfortable now (I took an intro to R class about a year ago and have been using it fairly steadily since then) in that I get the basic structure, but I still google my issues like a fiend. &amp;#x200B; In terms of having some time before your project starts, I'd just find a dataset that interests you in the meantime. Maybe it's something ridiculous like data from an NFL season or metrics from some subreddit, just anything that is vaguely interesting to you, and just fuck around with it. As you go along you'll have moments of "I wonder how I would do xyz..." then you google it, figure it out, and implement it. To me that process makes the lesson stick 100 times more than just an exercise on datacamp or wherever. 
Well dang, that is a solid way. I didn't know about which.min, but adding it to my toolbox now. 
Thank you :) I will do that 
This is all great advice. I went through the R and Python tracks through Data Camp and am now interviewing for Data Science jobs, and this is pretty much the same advice I’d give - DataCamp helps get your feet wet, but you really learn volumes by jumping into the deep end with a more substantial project. I also take notes on every DataCamp course I took in google docs not only for future reference but also to help with the learning process. Use Kaggle, data.gov, or scrape a website of your choosing for good data to work on. Tidy Tuesday is also great for data sets and seeing example code on GitHub to overcome plateaus. But most coding is done via google searches and stack overflow answers to get through tricky problems, and data science/stats with R isn’t much different in that regard. Getting comfortable in RStudio and/or Jupyter Notebooks is a also big help. 
I would switch to the data science python track. R is good modeling tool but your limited in deploying your model to production. Plus you'll want to enter kaggle competitions and most of them are in Python. I completed the R track and regret not switching.
I was planning on doing both, I have to do R because of the project I’m being assigned. I thought I’d spend a 1-2 months on R then do python so that I have at least a foundation for both. What’re these kaggle competitions? Would you do python now? Do you think focusing on one is a better option? While my supervisor wanted me to learn R, a colleague said that I should also do python Thanks for answering! :)
Kaggle.com hosts machine learning competitions and are great way to get some real world experience and collaborate with others. Most samples and competitions are done in Python. Get good at one language such as python. Plus you can do so much more such as build a website, ml modeling, or server side scripting. I'll probably still learn Python, since I haven't found a decent way to deploy an R model.
Just to piggyback here - I really found DataCamp plus this book to be a fantastic 1-2 punch. I'd say within 2 months I was pretty comfortable in the language and about 2 years later I am coding in R professionally.
I found that taking notes while going through the courses was very helpful for me. &amp;#x200B; Additionally, DataCamp has projects, where you can download the completed Jupyter notebooks with the code after you complete them.
Thanks for the input :) can I ask what your job is?
Thank you so much! I’m taking notes as well but not sure how much this will help me. I will look into all these 
I will definitely look into the coursera! Thank you 
Ooo I didn’t know that, thanks!
I used datacamp for R, SQL and Python for a year and I've found it useful as introduction to certain topics. If you wanna develop your skills you should start playing with some datasets from Kaggle and seeing the kernels there 
Thanks, soo on a familiarity scale of 1-10, I’d be like a 2-3 after finishing up with DataCamp? Should I not spend a lot of time on DataCamp and max out my time practicing with data sets? I have a few months to study
Take it and then set up models after models using open source data such as the ACS or even some municipalities offer extensive data libraries for the taking. At the very least, google and sign up for "data is plural", it's a news letter that comes out periodically with some awesome free datasets from across the web. Then, if you really want to start sweating, head over to Kaggle and see what R challenges are up. Don't worry about competing just yet, have a look at the challenges and see if you can do them in R... Maybe even throw python in there.
Np! I am a data scientist.
&gt; Plus you'll want to enter kaggle competitions and most of them are in Python. This is incorrect. &gt; Most samples and competitions are done in Python. Again, where did you get this impression from? The majority of users on kaggle use python. It's extremely rare for there to be any restrictions on what languages to use in competitions. #2 guy on kaggle uses R extensively.
You can always download the datasets from the shell in datacamp.
Oh cool, I didn’t know that, thanks fo the info.
I would say 3 in a scale of 1 to 10, kaggle has a lot of kernels with different topics which can help you to get to 7 really fast 
It’s kind of disheartening to spend so much time on DataCamp and only get to a 3 lol. But I guess it’s an investment 
Thank you! I’ll do that :) have a good night 
I started from scratch and did that same track. It worked great for me, but I also was using R often for all of my data analysis at work. Like everyone said, the more you can practice what you learn in the courses on your own project, the better. Also, I found that I needed to alter how I used datacamp to get more out of it. Make it hard. Watch the lecture, take notes, don't do the exercises. Switch to another course and do the exercises, or watch that course's lecture and take notes, but don't immediately do the exercises. This way when you do do the exercises you won't be just relying on short term memory, but forcing yourself to relearn or really remember it. I usually take 3-4 courses at a time. Switching between them also really helps keep it fresh. Also, when you do the exercises, delete everything but the comments. Filling in the dashes in a pre written function isn't a real life activity, but writing it from scratch is.
Very cool! 
Hey! This was helpful. When you say 3-4 courses that’s about 12-14 hours of their lecture right? So after that you do the exercises? Can I delete everything in the exercises? I didn’t see the option. What I really hated about the exercises was that there were no numbers or no way to keep track of which questions you’ve done. I had to wait till they started repeated to realize that I had done them all. But you’re right, I will make it harder and incorporate some extra projects. Thanks!
Cool, thanks for the great work! I’ll definitely try it.
I learned R that way, but I went out of my way to learn more than what is being taught at DataCamp, so that it would stick better. The problem with learning through DataCamp is if you stop writing in R, you're going to forget a lot of it.
I see! What other resources did you use?
I try to make sure I understand the etymology of both the concept and its vocabulary. That is, _why_ it came to be, _what_ the world was like before it and the _need_ that caused its invention or discovery. This understanding usually explains the thought process of the dev who put the concept into the language allowing me to understand and empathize with their decisions as well as understand the way the tool is intended to be used. This makes me an expert at the subject matter while simultaneously not forgetting it. Our minds have evolved to remember interesting stories, and seeing the timeline of a concept utilizes that part of the mind that rarely forgets things. It's easy for me to remember a concept, but harder for me to remember the story for its name. Thankfully most concepts in R come from mathematics (often statistics) and there are wikipedia articles about it, which explains both the history of the concept and the history of its name. For vocabulary I then map the math history a step further back using https://www.etymonline.com/ or https://www.dictionary.com/, because I struggle remembering vocabulary more than I do remembering the concept.
I didn’t think of doing this! Thank you, I’ll try it out :) hope you have a good day
Thanks; glad to hear it!
Thanks; glad you like!
Just go to any Kaggle competition kernels page, you'll see that the vast majority of them are done in Python.
Awesome job. I've been working on a shinyproxy implementation at my company as well. I might see if they would spin me up a new ubuntu server to test this out on. I really enjoy the ease of adding content that you describe here (rather than creating a new image for each app).
Thanks. Yeah, that was the kicker for me. This has definitely made life easier for me; hope it works out for you.
Amazing work 
R is really broad in terms of its capabilities. Running through 94 hours of DataCamp exclusively will give you a shallow understanding of a lot of different topics. You would learn more and it would stick better if you go deep on a narrow range of topics. I would suggest doing the first few intro courses in DataCamp (Intro to R, Intermediate R, maybe even the Intermediate Practice) and then pick an area of study that looks interesting to focus on from there. Are you interested in making graphs? Manipulating tables of data? Statistical modeling? Extracting useful data from messy strings of text? I'm new to data science as well so I know these are just a fraction of the use cases for R. My outline for success would be: 1. Pick a subject that interests you 2. Complete the DataCamp course for that subject 3. Find a data set online 4. Ask yourself "wouldn't it be cool if I could do \_\_\_\_\_?" 5. Figure out how to make it happen! Applying the basic lessons you learn from DataCamp on your own open-ended project will cement the lessons you learned way more than just taking more and more lessons. 
Thank you! From what I know, the project I’m being given has a lot of extractions from a sequencing data set, perhaps some modelling. I’m not sure what exactly I’ll be doing with it so I guess in the mean time I should focus on things that interest me :) What “subject” have you chosen for yourself?
My subjects of study have been driven by the projects that come up at work. Joining tables together with data frames has been the foundation of a lot of our work. Extracting useful data from long strings with Stringr/Rebus/Regex has been a challenging but powerful subject to learn. We've done some basic graphing and data visualization with packages like ggvis and ggplot2 but I'd love to do more of that in my spare time. 
Thanks for your response :) I hope I get a good grasp of the basics so that I can try out the more fun stuff. Good luck with work!
The way you phrased your comment implied the competitions themselves were restricted to Python. I suggest you instead say "Most people competing use Python" or "Most submitted solutions/kernels use Python". The sentence &gt; you'll want to enter kaggle competitions and most of them are in Python suggests the competitions themselves are restricted to a particular language, which is not true. My comment was just to clarify this for others who might be reading your comment.
I would like to try to get this working on Windows. I couldn't figure out what you're setting with the `{SITEPORT}` and the `{DESTSITE}` variables in the docker compose file.
You might want to check out the [MatchIt](https://cran.r-project.org/web/packages/MatchIt/MatchIt.pdf) package.
Yeah, I haven't looked into Windows support yet. If you make any progress, feel free to submit a PR. `SITEPORT` is the port for the site to be hosted on. `DESTSITE` is the port number of an existing site to share content with (default is SITEPORT). See: https://github.com/dm3ll3n/ShinyStudio#multiple-sites
neither tbh
Just out of interest: who gave you the task? It's a nice paper but I am not sure if their data is available (they used lastfm as far as I know)
It’s an assignment for class. They used Spotify data
Maybe you've seen this said elsewhere already but plotly syntax is clunky..ggplot syntax on the other hand is not. And you can easily construct pretty ggplot visuals and call ggplotly() on the ggplot object to convert it to a plotly object for instant plotly goodness (e.g. interactivity, point selection, hover tool tips, etc). This also allows you to selectively convert visuals so plotly doesn' tmake your computer sad.
I would say that you should give RStudio a shot, I've used VSCode with R in production - mainly for the git capabilities that are poor in RStudio. But RStudio was my main IDE even though I love VSCode.
For OOP in R, there's also R6 ([github](https://github.com/r-lib/R6), [another intro](https://adv-r.hadley.nz/r6.html)), which is not a standard part of R, but maybe closer to Python classes than any of the other OOP systems in R. For testing R packages, [testthat](https://testthat.r-lib.org). I'm not very familiar with Python, but my sense is that Python has a better set of tools for unit tests, etc. I find debugging to be easier and error messages to be clearer in R (when doing data-related work) than in Python, but then again, I mostly use R. 
Does it matter if it's on a flat plane or not? Does this have to be represented visually or can it be done in code? Coding it would be easier for dissecting information (i.e. declaring which octant of the globe it's on so you can draw inferences within that space). What exactly are you looking for? I'm at a concert, so I'll try and answer during intermission if I can assuming nobody else has.
Also, I just realized you subdivided a region into specific polygons. My bad, not reading well under distractions.
Sorry if this is off topic and not helpful, but how about using QGIS? r/QGIS. I am just learning about R so for me I don't know how to do this in R
Cool! Now there are 4 object systems... Yes for the common use cases of R I haven't seen many unit tests, though it's not a bad idea.
I'd look into the GIS tools available in R, such as: http://www.nickeubank.com/gis-in-r/
st_within in the [sf package](https://github.com/r-spatial/sf). If you Ctrl+f "Simple Features" on [this page](https://www.rstudio.com/resources/cheatsheets/), you'll find a cheat sheet for the sf package. You want to convert the lat/lng coordinates to a POINT geometry column in one data frame, and the region polygons into a POLYGON column in another data frame. Then do st_within(coords_dat, polygon_dat) and it should return the answer to "for each of these points, which polygon is it in?" Do you know what projection the region polygons are in? If they're WGS84 or NAD83 (if they come from the government they're probably NAD83), then you'll have an approximation within a few meters to true 3D shape of the earth if you assume a flat plane. You want to convert your coordinates to POINT objects in a geometry tibble (
R is a more than capable GIS. No need to rely on QGIS for this.
Can you provide a reproducible example of your point and polygon data?
Haven't used Matlab in over a decade but I think your assessment of the second part is correct. The first part subsets the matrix getting all the rows (hence the : without any numbers around it) and only columns 2 to end (2:end). End is an in build variable that is equivalent to ncol(df) or nrow(df) in R.
Thanks a lot!! Yea I was trying to figure out if : means all rows. 
You're more or less shit out of luck. The server needs to be running R and a service that will listen for incoming HTTP endpoint, run some R code, and reply with the relevant HTTP response. RStudio have written that application for you, and they called it Shiny Server, and they only support Linux. Since Linux, even in a VM or a container, isn't an option for you, you're essentially fucked. You could write a Windows version of that application yourself, but you'd be making Shiny Server again and I don't think you want to sign up to do that. You could in theory just use shiny's `runApp` function. If you specify a host (or `0.0.0.0`) then outside clients can connect. However, there will be no authentication or resource management, the security will be terrible, and you'll have a bunch of other problems. &amp;#x200B;
I’d go with `sf::st_join(your_points, your_regions)$region_column`. 
&gt; However, there will be no authentication or resource management, the security will be terrible, and you'll have a bunch of other problems. yep. this was my experience too. It is a fine option for a lightweight application, just not very robust.
Well, thanks for the downvote!
Anytime!
I don't see that much of a disadvantage on using `runApp` especially over the basic version. Does the basic shiny server has additional security features? There is some load balancing issues but if you aren't worried about resources, an always running session could actually be more responsive.
It's kind of hard to explain without showing you, but what I mean is that I don't take one course at a time and blaze through it in a four hour sitting. For me, I'm not going to remember everything from it if I do that. Instead I have three or four courses in my "courses in progress" section and I work on each just a little at a time. I watch their 3-4 minute video in one course, then switch to another course and do an exercise or two, then switch to another course, and so on. It might take me a few weeks to get through a batch of courses this way, but I remember them better. There's no option to delete everything in the exercise, you have to manually do that. If you forget what's there, use ls() in the console to see what objects are available, then str() on each of them to jog your memory. Use their directions on the side to do what they want, and use your notes to get the syntax right on your own. I haven't had trouble keeping track of the exercises I've done. When I click on course outline at the top of exercise page, the completed ones usually get greyed out with a checkmark.
It doesn't, but if the server is fully within the company's firewall, then I'm not sure I'm seeing what the problem is with that.
I know Linux can play with Active Directory because I use a Linux cluster in my company intranet that does just that. I don't know how it is done, but the existence proof is there.
Have you heard about [R shinyproxy](https://www.shinyproxy.io/)? I think this can run on Windows. With this you can bundle any shiny app into a docker image and spin up a container in response to a request for your app. It also allows for free authentication instead of paying for like R shiny server pro. Highly recommend!
The `sp` and `maptools` packages can do this kind of thing, and there is an `sf` package now that is tidyverse friendly but I haven't used it. The `PROJ4` coordinate system translation system is supported, though I just use the simple WGS84 longitude/latitude system which Google Earth uses. Check the Spatial CRAN Task View for more resources.
Don't play this game over formal request channels. Sit down with whoever has the power to make this happen in IT and ask them what an acceptable solution would be, while impressing on them how much the business values your work.
I'd recommend pushing the docker route as well. 
Thanks for your input. Is RStudio intellisense good enough as well?
It's clunky if you ask me, I don't tend to use it much.
I'm gonna leave this up in case I'm wrong, but I think I resolved it-- what I called x4 is temperature, called "T" in the dataset, which I think is a native R function that was screwing things up. I renamed the variable "T" to "t", and now it seems that I can write a function for f(t), use that in the formula and it gives the same results as typing out the linear model long form.
For the first one you list a shortcut is just x2*x3. You can try this in your function since.
I’ll try doing that! Hmm that’s odd that hasn’t been happening for me but oh well I’ll figure it out :) thanks
nevermind. after reading http://brucehardie.com/notes/022/ i found out that the transactions after the calibration period should not be included. so its (29.73 +14.96) / 2 = 22.345
My first thought is to programmatically generate the combinations you need as character strings. Paste them together with “+” as separator, tack on “y ~” at the beginning, and then use as.formula() to coerce the string to a formula. expand.grid() may help you generate combinations. You’ll get a data.frame and can then select the rows with the combinations you need, and paste each row together. Presumably there is a pattern to the interactions you need to include or exclude in each of the 400 cases; that may make it easier to write a function to select the rows with the combinations you need. Not knowing more about the form of the models, I can’t suggest a more elegant answer.
Just FYI, you can get it to run on windows but the stability is questionable. My IT was trying to get me to run it on a windows server first because they didn't want to support linux either. Once I finally was able to get it up and running, docker kept stopping overnight and I would have to restart it each morning. I'm sure there are probably ways to get it more stable, but I finally was able to convince them to let me use ubuntu and it has been running for weeks with no issues.
Just a heads up, this is a horrible practice statistically. Google p-hacking for more info. Basically it boils down to you're creating a bunch of independent regression equations, when they are actually all related. You can also take a look at "Tukeys honest significant difference" test to interpret these models correctly
what i described is copying recent literature in the field with the best predictive results, its not my design.
Like others have said, you're out of luck. I was in a similar position to you, large company, unhelpful IT department. I finally told a few C-level people "you can have this if you'll make IT support it." Amazing how quickly IT's attitude changed. 
its pretty ugly but update.formula() is doing what i want. I like the idea of using strings and converting them to formulae after the fact though, I think I'll try that next time I have to do something like this
Validation of linear models involves a statistical test (presumably based on the model r squared and p value). Your question made it seem like you are evaluating multiple models which have the same general terms (eg sharing x1). If that is the case then you need to account for the increase in random probability of success. if that's not the case, I totally misunderstood your question and I apologize
yeah maybe im misunderstanding something here but i dont see where the multiple comparisons are if you take model a = x + y, and model b = x + y + z, and you train and validate them with the same data and calculate some kind of loss function for each. I cant find anything about bonferroni considerations on loss functions on a quick google search. but i never took a class on regression so i could be wrong
`expand.grid` will make a data frame...
`install.packages(permute)` `m &lt;- allPerms(n = 4)`
Thanks! I went with another solution after fumbling around for awhile: library(gtools) a &lt;- 0:9 matrix &lt;- permutations(10,4,a)
Recall that the != operator is "not equal". Therefore, your filter will include the rows where the id is not equal to the *entire vector* of excluded ids. What you need is the %in% operator, which is "in" and returns TRUE when a scalar (i.e. value) is *in* a vector. Replace your filter line with ! (id %in% exclCodes) for the negated version. 
I don't live in the Bay Area but you may want to check out [meetup.com](https://meetup.com) for R Users groups in your area. That might be a way to meet others and find out about local workshops and classes. 
Thanks for your quick answer! I'll try it tomorrow (this was the last thing I did before packing my things and going home) and report back if for whatever reason it didn't work. 
No problem! Good luck in R! It's different, but a language more consistent stats theory. I hope my explanation as to why your solution didn't work was helpful. As for the execution of the actual code, the below Stack answers should be useful: https://stackoverflow.com/questions/5831794/opposite-of-in
Yea, it makes sense. I'm just not used to operators like %in% compared to similar function from other languages. I discovered the RStudio Cheatsheets just now and it includes a list of the available operators as well as a lot of other useful references. That should help me be more self-sufficient!
No.
Yeah %in% implementation in R is actually very hacky. If you open up the documentation for the operator, you'll find it's actually a pre-built function, hacked together from matching functions, that has been included in base R for years. But it works pretty well so 🤷‍♂️
this is beyond the scope of 'statistical analysis' - ideally a network or sys admin would be doing the background work of installing and configuring a shiny server and you simply deploy your model when finished.
It would be a good idea in the future when you ask questions that you present an example that actually works to demonstrate the problem. That would make it much easier to help, but I'll give it a try: In this part of the `ifelse` statement as.character(x$TreeSpecies[x$NestTree ==1 | x$NestTree==3]) the index of this x$TreeSpecies[] inside the square brackets: x$NestTree ==1 | x$NestTree==3 will always be `FALSE` / `TRUE` which translates to 0 / 1. So if `sum()` is different from 0 the `ifelse` returns either the zeroth or the first element of `x$TreeSpecies`. However, there is no zero-indexed element in R. This could be your problem - or it could be something else.
If you're already using tidyverse the syntax can be very elegant for this. my_tibble %&gt;% filter(!id %in% test_vector)
Check out the bay area R user group on the Meetup web site. It's active and a great way to meet other R users.
`!id %in% test_vector` resolves to the same as `!(id %in% test_vector)`? TIL!
Without a fully working code sample, I can only guess what your problem might be. Just looking at this statement, I think you should look into the difference between dplyr's `if_else()` function vs base R's `ifelse()` function. This [Stack Overflow post](https://stackoverflow.com/questions/50646133/dplyr-if-else-vs-base-r-ifelse) might be useful to you.
The best way to get help is by providing a minimal working solution so others can recreate the problem. I tried to make my own, and it looks like it's working for me? require(plyr) treedata = data.frame( PlotNum = c(1,2,3), NestRand = c(1,2,3), TreeSpecies = c("beech", "pine", "pine"), NestTree = c(1,3,3) ) treestats &lt;- ddply(treedata, c("PlotNum", "NestRand"), function(x) data.frame( SpeciesNT = ifelse(sum(x$NestTree, na.rm = T) == 0, NA, as.character(x$TreeSpecies[x$NestTree ==1 | x$NestTree==3])))) Here is the result I got: &gt; treestats PlotNum NestRand SpeciesNT 1 1 1 beech 2 2 2 pine 3 3 3 pine
Alternatively there's ```anti_join```.