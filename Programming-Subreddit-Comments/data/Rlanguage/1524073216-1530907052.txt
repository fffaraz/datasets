Unless you have some sort of distribution where values are very often close to zero and very rarely much larger, you might have to check all 10x10 pieces. Add the 100 different 991x991 dimension subsets of the original 1000x1000 matrix together and then find the maximum value of that resulting matrix.
This should do it, and it is set up to be turned into a function pretty easily. set.seed(44) mat &lt;- matrix(data = sample(1:100, size = 10000, replace = T), nrow = 100, byrow = T) square_size &lt;- 10 res &lt;- matrix(data = NA, nrow = nrow(mat)-(square_size-1), ncol = ncol(mat) - (square_size-1)) for(i in 1:(nrow(mat)-(square_size-1))) { for(j in 1:(ncol(mat)-(square_size-1))) { res[i,j] &lt;- sum(mat[i:(i+(square_size-1)),j:(j+(square_size-1))]) } } Make your data a matrix if it is all numbers, they tend to be faster in R. There's probably a way to do it without nested for loops, but I don't know how. The 100x100 matrix ran quickly on an old laptop. 
Not specific to R users, but I would highly recommend [Python Crash Course](Python Crash Course: A Hands-On, Project-Based Introduction to Programming https://www.amazon.com/dp/1593276036/ref=cm_sw_r_cp_apa_-y51AbV9YY7DA). It doesn't teach you everything, but it teaches you the basics and gives you plenty of exercises to do some hands-on learning. I would also recommend [snakify.org](snakify.org) it'll reinforce the stuff you learned from Crash Course, and it'll introduce you to sets. Be careful though. Some of the lessons are poorly worded (I think the guy is Russian originally). I had to look up videos on YouTube to get better explanations of the concepts before I could complete the exercises. Also, some of his code examples are pretty dense, which can get confusing for someone new to the language.
Thank you very much, I will give this a go for sure when I get back home tomorrow. I‚Äôm completely new to R so I‚Äôll need to figure out setting it up and importing / converting the csv. Again, thank you for your help!
Conditional on the frailty the model is a PH model with a group specific baseline hazard. Marginally it is no longer a PH model. This makes interpreting the coefficients a little tricky, and why some prefer marginal models.
You're welcome. If you need more help, just ask. Good luck. 
One potential option is to look at books written for Python in your field. For example, I started learning Python from [Python for Biologists](https://pythonforbiologists.com/python-books/) by Martin Jones. It clued me in to the important aspect of my field, and gave me a diving off point for future learning. 
I'm an R user and I'm doing the coursera course on data science in python. Mixed results. The videos are good but all I REALLY want is a crash course in the list/dataframe syntax. 
Besides the fact that electron software pack an entire friggin web browser with it? Besides the fact that is is a web app at the core wih all the disadvantages that come with it without any of the advantages? I want my computer RAM to be used by my script and data, not for it fo be completely gobbled up by this electron node shit. The code itself is also under several layers of interpretation so most of the cpu power to run electron apps is basically wasted. Electron is used by lazy devs who don't know anything better deluding themselves that it's good. And the proprietary license? Don't get me started.
I started by working through [Python for Data Analysis](https://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1491957662/ref=sr_1_3?ie=UTF8&amp;qid=1524093313&amp;sr=8-3&amp;keywords=python+for+data+analysis). There are also some free things online, including (but not limited to): [Think Python](http://greenteapress.com/wp/think-python-2e/) [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) [Computational and Inferential Thinking](https://www.inferentialthinking.com/) [How to Learn Pandas](https://medium.com/dunder-data/how-to-learn-pandas-108905ab4955) [Advanced NumPy Techniques](https://nbviewer.jupyter.org/github/vlad17/np-learn/blob/master/presentation.ipynb)
I wonder if people who hate on Electron apps make sure to never have an extra tab open in their browser - they're just so concerned about the extra RAM.
You should add labels to the axis ticks. Here's an example: x &lt;- c(1,5,3,8,4,1,4,9) y &lt;- c(.11,.23,.12,.04,.33,.06,.15,.27) plot(y~x, yaxt = "n") ticks &lt;- seq(0,.3,.1) axis(2, at=ticks, labels=paste0(100*ticks,"%")) 
Python is so so much more than lists, dataframes, or a platform to do data science. I recommend learn python, at least in part, for the sake of learning computer science. You can write some beautiful software with it.
I haven't done much in Python, but the stuff I have done was relatively easy to convert if you know exactly what you want to do with R. Just use the same terminology and search on stackoverflow for Python/Pandas/Numpy. Aside from that, the Jose Portilla courses on udemy are cheap and he is a very good instructor IMO. Example here: https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/ 
Never learn a programming language by yourself
Base R plot function. But I'm planning to use ggplot too. So it will be very helpful to know both. 
Memory leaks will slow down my computer since it will need to use the disk as ram for some of my programs. If electron uses a GB of ram more than an alternative, having 2GB ram sitting free when developing vs having 3GB free makes no difference. 
Ok, hold on, what do you think is difference between a memory leak which eats a gig of your ram which you could have been using for anything else (according to your words) and a electron app that also unnecessary uses a gig of your ram that could be used for anything else?
The most important thing is consistency. Most other things are negotiable. That said, your rules don‚Äôt improve readability as much as you think because they make arbitrary distinctions: &gt; Because functions are capitalized (at least if you follow style conventions), it's easier to see what the tables and variables are at a glance. This is, in effect, a variation of [system Hungarian typing](https://en.wikipedia.org/wiki/Hungarian_notation#Systems_vs._Apps_Hungarian). Something similar used to be widely used. But a consensus emerged that this is not only useless but actively harmful because (1) it emphasises low-level features of the type but not the much more relevant semantic meaning, and (2) it isn‚Äôt enforced by the language and can therefore be misleading if the user made a mistake. Check the [disadvantages](https://en.wikipedia.org/wiki/Hungarian_notation#Disadvantages) section on that page. Not all of these apply to R, but enough do. But much more directly: if you feel that need to see *at a glance* whether a name refers to a function or a table, without understanding the context of the code, then there‚Äôs a problem somewhere: you should be able to understand the context, it‚Äôs much more important than concrete types of names. So now you know that some name refers to a function ‚Äî and now what? You still know nothing about that function. It‚Äôs nothing more than a band-aid for bad names. If, by contrast, context and name together explain the usefulness of that variable, only then do you have a proper understanding of your code.
So I used the guy's example above. Note that ggplot2 is designed to work on data frames, so you need to make x and y into a data frame. Using the 'scales' package you can add percent format, dollar format, etc. library(ggplot2) library(scales) x &lt;- c(1,5,3,8,4,1,4,9) y &lt;- c(.11,.23,.12,.04,.33,.06,.15,.27) x_y &lt;- data.frame(x,y) ggplot(x_y, aes(x = x, y = y)) + geom_point() + scale_y_continuous(labels=percent_format()) Overall ggplot2 requires more initial typing, but is much easier to make more complicated graphs bc of it. Toss in the 'themes' package and your visualizations will look much better, too.
I do try to follow the styleguide by Google/Wickham. Mostly because of readability (the more people follow the same style guide, the easier sharing code becomes). I started working on an R package and thought that was a good project to work with some guidelines. In the end it boils down to preference. I was a little annoyed by using underscores in variables. Such as my_variable. But I got used to it. Why don't you just pick the parts from the guide that make sense to you / you like and do other parts the way you prefer them?
 group_by(DMHID) %&gt;% arrange(DMHID, DateOfService) %&gt;% mutate(days_between = as.numeric(DateOfService - lag(DateOfService, default = DateOfService[1]))) %&gt;% mutate(eoc_45dco = 1 + cumsum(days_between &gt;= 45)) %&gt;% mutate(id_eoc = as.integer(paste0(DMHID, eoc_45dco))) %&gt;% ... Something like this.
For my first year or two I bitched and bitched that I couldn't edit variables directly in RStudio. I eventually understood why this was the case and came to toit the forced replicability. I'm not sure how I feel about a program that violates this principle. The capitalist in me says Hooray, the scientist says Nay!
About the RAM whose needs RCode, for example on Windows 10, if you launch RCode you can check it in the Task Manager. It takes about 200 MB of RAM. RStudio also takes about 200 MB of RAM.
Slightly off topic, but check out the new reticulate package that allows easy access to Python from R.
It‚Äôs not bad! I like it because I‚Äôm still facing classic programming problems such as: the code is working and I have no idea why it the code isn‚Äôt working and I have no idea why! To be honest I kinda missed it.
Absolutely agree with you. My team is currently dedicated to cleaning up lots of legacy data. It's already hard enough to document what the changes were and what code was used to drive that. Direct variable editing with no traceability? Recipe for a nightmare. 
Solution was to first create formatted DT using DT::datatable, then I could pass to DT::renderDataTable SMH...
glad you figured it out! üòÅ
If you read the documentation in the link you provided, you will find out that you need to use the `estimate` function from the same package to get the dfa trendline. `estimate(dfa(timeseries))`
You can't send ```names(book_list)``` as an argument to your function, it will just receive the entire list of names instead of iterating through the elements of it. I think this would be easier if your data was in long format, so let's try that: book_list &lt;- data_frame(analects = c("xue-er", "wei-zheng", "ba-yi", "li-ren", "gong-ye-chang", "yong-ye"), mengzi = c("liang-hui-wang-i", "liang-hui-wang-ii", "gong-sun-chou-i", "gong-sun-chou-ii", "teng-wen-gong-i", "teng-wen-gong-ii"), liji = c("qu-li-i", "qu-li-ii", "tan-gong-i", "tan-gong-ii", "wang-zhi", "yue-ling"), mozi = c("befriending-the -learned", "self-cultivation", "on-dyeing", "on-the-necessity-of-standards", "seven-causes-of-anxiety", "indulgence-in-excess"), hanfeizi = c("chu-jian-qin", "cun-han", "nan-yan", "ai-chen", "zhu-dao", "you-du")) book_list &lt;- book_list %&gt;% gather(Book, Chapter) map2(book_list$Book, book_list$Chapter, get_text) As far as I can tell this returns the correct result.
 Very nice! This is indeed an improvement to what I had. Although I am a bit disappointed, gathering into a data frame almost seems to be a work around to working in a list which is what Purr seems to be specifically made for. But results matter more than methods, so many thanks. Now, slight problem - the book list was just a sample of 5 chapters from each book. So converting into a data frame is easy considering the columns have the same lengths. In reality, these books all have different lengths of chapters. So they would probably have to be a nested data frame. How can I nest my books so they can fit into a clean data frame, which I could then apply your same method?
If we take your original book list it's not too hard to convert to something workable: Books &lt;- unlist(map2(names(book_list), map(book_list, length), rep)) Chapters &lt;- as.vector(unlist(book_list)) book_list &lt;- list("Books" = Books, "Chapters" = Chapters) 
Ah, got it. https://stackoverflow.com/questions/49957914/mapping-different-args-to-a-list-of-data-frames/49957966#49957966
map2 loops over two list simultaneuously. In other words, first it will call the function with the first element of both lists, then the second element etc. What you need is a nested map (or loop), so that for each book, you loop over the chapters. Something like this should work: map(names(book_list), function(book) { map(book_list[book], function(chapter) { get_text(book, chapter) }) }) 
Nice answer!
You could simply make a character vector of the csv names and loop over them as for(i in csvNames){ dataFrame &lt;- rbind(dataFrame, read.csv(i)) } You will need to preallocate the dataframe which might be cumbersome with a lot of variables, but you can simply load in the first csv out of the loop and use it as a base. This will produce a difficult to use mess if the variable categories don't line up though.
You can initialize "dataFrame" to NULL and it will work without needing any complicated initialization.
I do this regularly by getting all the file names I want and appending them together with dplyr.
this chart was terrible https://www.reddit.com/r/badstats/comments/6wihjr/poor_pie_chart/ how in the world did you come across it?
I have a bunch of time-generated (hourly) files on which I use [this type of loop](https://pastebin.com/raw/uKTWiyeU). In that specific example, I only extract column 12 from each file to stitch together.
Underscore is the only way to account for a space in the name of a variable in R as far as I know?
Instead of gather, you can use [list-columns](http://r4ds.had.co.nz/many-models.html#list-columns-1): data_frame(book = names(book_list), chapter = book_list) %&gt;% unnest() 
Pretty much, yes. But since capitalization should be avoided (says the styleguide), that is how variables are easier to read. In VBA you see a lot of variables in this style: strMyFile. In comparison strmyfile is difficult to read. str_my_file is easier but took me a little to get used to.
Thank you so much. It worked. I just got back to my code. :)
Almost all of my R programming is for my own amusement unfortunately. Currently I‚Äôm fantasizing about using the chess package with a shiny app in some way. 
Nope, too inexperienced to know that was a thing, but I will consider it next semester.
I learned &lt;- first and by God I'll stick to it.
Is it appropriate to say "FUCKING GIVE ME A SIMPLER ERROR IF THE ISSUE IS COMMAS!" in an interview?
newb, same opinion
&lt;_&lt; But he's very attractive. *mutters something about tesco sausage*
What version of R dies glmnet say it needs? What version of R are you using? Which repo do you use, and what version does it have? If you need a higher version of R than your repo has, just install a higher version. 
That sounds good.
Not really programming, but I used the youtuber package to analyze stuff from videogamedunkey's channel, and made this word cloud from the comments of all his videos: https://i.imgur.com/8omtHJb.png Meant to do much more with it but haven't had the time. 
I had data from a chemical engineering experiment from before I knew much R or statistics. The model for the data involved a system of nonlinear DEs, and when I was doing the experiment I did everything in excel, then wrestled with solver until it found *a* set of point estimates that we're halfway decent. Fast forward to this semester, I wroke rk4 into a JAGS model (and posted it in this subreddit) and had my computer spend like 20 hours doing MCMC. It was really cool to see some of the results correspond to problems I had using solver in excel, like there being a lump of credibility for some parameters being zero, which was not realistic.
I wrote a script that makes it look like im working. It has lots of progress bars with mad libs style messages. It also scrapes random Wikipedia aricles and R stack overflow questions and prints them to the console. I like to leave it running when I go to lunch
I run Linux Mint MATE, so I have the Linux Mint &amp; Synaptic Package Manager repos. glmnet doesn't give a specific version error, but doing some googling it sounds like that is the core issue. Bleh, I hate tarballs.
When you say voice control, what do you mean by that? that might be useful for my app.
Pass the text captured from spoken voice as a shiny input. Can use it like any other input, filter data, navigate menu, fill search box etc. 
Well either capture the text and process that in R or use voice triggers for changing other reactives/observers. Both possible 
Link to your git please? :)
I did come across a javascript package that could be made to do that. I will dig it out later for you. It's licensing would mean no use in a commercial app though which is why I haven't bothered with it so much. 
Scripts to download all my chess games from chess.com and then visualise them using the rchess package I also started pulling tweets for a user (eg Donald Trump and Obama) and do some basic word analysis, unique words, most words used etc, and sort by which device it was posted from/time. All unfinished projects unfortunately. I'm great at starting stuff.
https://stackoverflow.com/questions/15653145/using-google-text-to-speech-in-javascript has some suggestions. 
I have been doing passive allocation of my investments for several years, last year I decided I should have it automated. The result is a [CLI](https://github.com/lf-araujo/allocator) program.
[Yeah, that's definitely Dunkey's comment section](https://i.imgur.com/NPKFzL9.png)
I imagine this could easily done for Facebook messenger data, right?
I wrote a small example of this here, hope it helps [http://jbrookes.com/code\-examples/curve\_fitting/](http://jbrookes.com/code-examples/curve_fitting/)
The plots window is purely a feature of RStudio to make development easier. You shouldn't rely on it to store a history of plots to then save. You could achieve your goal by storing a list of plots, then looping over the list (or `lapply`, etc) and running your `jpeg() ...` or `ggsave()`, etc.
It generates network information in the form of an edge list. I can make a ring network, scale free networks, networks with a given connectivity ratio, etc. Puts them in random 3D space and then applies some novel sorting algorithms to create pleasing network displays that don‚Äôt use spring tensioning. I‚Äôm now moving to hyper graphic networks using hidden nodes for n-tuples. If you‚Äôre into this, let me know! Let‚Äôs collaborste!
github.com/svenhalvorson
If you are using `ggplot`, its easy to add `+ ggsave("filename.png")` to each plot
I did not know that I always used jpg(). Does it rewrite the file if I use the same filename or does it create multiple versions ? Thanks a lot !
Overwrites http://ggplot2.tidyverse.org/reference/ggsave.html
The plot window also exists in plain R, not just in RStudio.
Thanks!
Is there a particular reason why you are importing as a table first? Phyloseq accepts BIOM, mothur, and Qiime formatted outputs.
Yes, it's because the .biom file I have is not in the right format. When I try and import_biom it says it needs to be in a .json (BIOM-v1) or hdrf (something like that?) Format. I've tried converting the biom I have to a .json in our server and it still doesn't work. If you know a way around that though that would also help !
Hmm, yes. I believe Phyloseq uses an older .biom version of the .biom format. There may be a way you can convert, but I'm not sure. I mostly use Mothur's default output, and it works like a charm with Phyloseq. From your .png image, it looks like your OTU#'s are not the row.names of the matrix. Have you tried doing that (row.names=1, when doing read.csv)?
Separate window. It would be cool if you could somehow get it to keep re-using the same window for new plots.
Oh! I'm not looking for someone to help me write the program. it's just a project I'm using to try and challenge myself, while I work through the udemy course I got. I realize I'm attempting something above me right now. But I figure it's a great way for me to try to encourage myself to look things up and try the things I'm learning about. But, if you're saying I should put that on the back burner until I get the basics down, then that's what I'll do. 
 install.packages("packagename") 
There is a button that stays installs on one of the 4 panes where you just click install and then type in the name of whatever package you‚Äôre looking for. After it downloads dont forget to make sure there is a check mark next to the package in the the same pane. 
That works only if OP has RStudio installed
Your URL and the GET request seem to work. Status 200, which seems to be the indication of a successful query. GET(paste0(url.coin,convert,base.currency,middle,base.currency)) Response [https://min-api.cryptocompare.com/data/price?fsym=USD&amp;tsyms=USD] Date: 2018-04-24 04:35 Status: 200 Content-Type: application/json; charset=UTF-8 Size: 9 B
https://github.com/trestletech/plumber
Note that to actually use the package you need &gt; library(packagename)
lmao
Unless you call `dev.new()` (explicitly or indirectly), the same window is reuse for new plots. Depending on your system, that window allows you to cycle between the plots (works on Windows and X systems ‚Äî Linux and macOS).
Well plain R is a console application and doesn‚Äôt have a ‚Äúmain window‚Äù so panes don‚Äôt exist. That said, plots are opened in a plots window which is fairly similar to RStudio‚Äôs ‚ÄúPlots‚Äù pane. Notably, it supports most of the same commands (cycling between previous plots, saving a plot).
When you pass ```map2``` ```book_list``` it's going to iterate over the elements of ```book_list```. Those elements are two full columns, not the individual rows.
Ok, two things. First, yes list based tools are cool and good to learn, but don't forget: *data frames are also lists*. A data frame is literally as list of lists with some code to make them easy to work with. Second, look at the data you're feeding to your functions. test_data is a (named) list of data frames. So your function will see 1. a data frame with a single column, and 2. the name of the book. If you want to work with your data in this format, you'll have to change your function to this in order to pass the character vector to paste rather than a data frame. my_function &lt;- function(book, chapter) { paste("www.fakewebsite.com/", book, "/", chapter, sep = "") } my_function_2 &lt;- function(book, chapter) { paste("www.fakewebsite.com/", book, "/", chapter$chapter, sep = "") } map2(names(book_list), book_list, my_function) map2(names(test_data), test_data, my_function_2) 
My only reservation about plumber is that you can't actually control what the payloads look like. But it's definitely the best REST API library I've found so far in R. Thanks for the reaffirmation.
[`require` is discouraged](http://r-pkgs.had.co.nz/namespace.html#search-path) because if it fails it merely shows a warning (which might be disabled), rather than stopping (as `library`) does.
yep
Does this work with ggplot too? Maybe I'm just anal and always close windows right away and never noticed!
ggplot2 changes the plotting API but the underlying devices are the same. So yes: this also works with ggplot2.
I don't know for sure, but my first thought is that you probably need a "row.names = 1" in your read.csv command. It seems to be treating them like a column of data instead.
The most typically R way is actually to use a dot, as in "read.table" or "install.packages". To access slots/attributes of an object you either use "$" (S3) or "@" (S4). The Hadleyverse (ggplot, tidyr, dplyr, ...) does tend to use underscores more, though, which is kind of annoying.
An alternative suggestion: make your script into an [Rmarkdown](https://rmarkdown.rstudio.com/) file. When you compile it, it will produce an .html (or .docx, or .pdf, or...) report with embedded plots.
Here's a super quick [reference guide](http://mathesaurus.sourceforge.net/r-numpy.html) that relates numpy commands to R commands.
Clever. Then I can drag and drop the plots to my report !
This looks like json data to me. If it is, you can parse it with `rjson`. &gt; library(rjson) &gt; test = '{"@odata.type":"#Microsoft.Azure.Connectors.SharePoint.SPListExpandedUser","Claims":"i:0#.f|membership|agent@email.com","DisplayName":"First Last","Email":"agent1@email.com"}' &gt; fromJSON(test)$DisplayName [1] "First Last"
`filter()` from `dplyr`/`tidyverse`
Check out the dplyr package
To add to this, `mutate()` coupled with `case_when()` is fantastic when you have a bunch of conditions to combine into groups.
I tried data %&gt;% filter (first==aaa &amp; second==bbb) but its still slower than what im using. Is that what you were thinking of or is there another way to do it?
Thank you. Can you give me an example of how I would do this? 
It looks like dplyr is the fastest at subsetting data.frames. Taken from [stackoverflow](https://stackoverflow.com/a/27304993/7547327) and adjusted slightly. set.seed(1) # for reproducible example # 1 million rows - big enough? df &lt;- data.frame(age=sample(1:65,1e6,replace=TRUE), y=sample(c('red', 'orange'), size = 1e6, replace = T)) library(microbenchmark) microbenchmark(result&lt;-df[which(df$age&gt;5 &amp; df$y == 'orange'),], result&lt;-df[df$age&gt;5 &amp; df$y == 'orange',], result&lt;-filter(df, age &gt; 5 &amp; y == 'orange'), times=10) Unit: milliseconds expr min lq mean median uq max result &lt;- df[which(df$age &gt; 5 &amp; df$y == "orange"), ] 54.50809 55.65279 66.25441 58.64587 68.17149 96.30066 result &lt;- df[df$age &gt; 5 &amp; df$y == "orange", ] 66.30712 70.36249 108.49919 89.02430 116.09330 286.44413 result &lt;- filter(df, age &gt; 5 &amp; y == "orange") 37.22462 37.78490 44.95071 40.12050 46.66179 79.70909
Subsetting with `which` is pretty fast as is. However it may depend on your data. The below simulated 10,000,000 rows `filter` is outperforming `which`. library(microbenchmark) library(dplyr) set.seed(123) df &lt;- data.frame( first = sample(LETTERS, 10000000, replace = TRUE), second = sample(LETTERS, 10000000, replace = TRUE), third = sample(0:10, 10000000, replace = TRUE), fourth = sample(0:10, 10000000, replace = TRUE), fifth = sample(0:10, 10000000, replace = TRUE), sixth = sample(0:10, 10000000, replace = TRUE) ) microbenchmark( df_which &lt;- df[which(df$first == 'A' &amp; df$second == 'B' &amp; df$third == 1 &amp; df$fourth == 2 &amp; df$fifth == 3 &amp; df$sixth == 4),], df_filter &lt;- df %&gt;% filter(first == 'A' &amp; second == 'B' &amp; third == 1 &amp; fourth == 2 &amp; fifth == 3 &amp; sixth == 4), times = 25) My mean for `which` method is 928.3479. Mean for `filter` is 918.1025
You use `case_when` when you're mutating which is for creating or altering a variable.
If you are constantly sunsetting the same data over and over again you could try saving the results to a temporary cache an just reference the cache.
something like `data %&gt;% mutate( case_when( first == "a" &amp; second == "b" ~ "AB", first == "a" &amp; second == "c" ~ "AC" ) )` sorry, gave up on nice formatting. 
Its subsetting the same data using different variables so its slightly different every time 
You could also try data.table, which for some applications can be faster but here on my machine it did not matter much. Downside is that it involves putting something into a different data type (data.table instead of data.frame). For this example you'd do something like: dft &lt;- data.table(df) result&lt;-dft[age&gt;5 &amp; y == 'orange']
Especially if just adding rows at the end.
8% difference is probably about expected, likely noise. You're not going to get tons of speedup on filtering. You might be interested in using a profiler to figure out where all the bottlenecks in your code are.
Okay ive never done this like what you are saying so im a little confused. Do I have to write down every variable variation when doing this? I have a ton of them so it would take forever. Going off what I think you are trying to do I made a new column in data that I pasted together all the variables that I was looking at then I made a new data frame that has all the variables I need. Then I just subsetted using that one column and the new data frame. When using mircobenchmark on this is came out to be using only 13.5% of the time I was originally using which is amazing. So it looks something like this data$extra=paste(data$First,data$Second,data$Third,data$Fourth, data$Fifth,data$Sixth) New=paste(aaa,bbb,ccc,ddd,eee,fff) subset[which(data$extra==New),] or data %&gt;% filter(extra==New) The 13.5% I got is just the second paste and subset since I only have to make a new column once. Also I was using which for this and when using filter it came out to about 29%.
Is the data loaded fully in memory and not accessing the hard drive or a remote server? If not your not going to get much faster, maybe try the dplyr::filter functions as others have suggested or moving to a computer with more memory bandwidth other than that you aren't going to get much faster than that.
Look into look behinds and lookaheads. The string cheat sheets from RStudio will also point you in the right direction
Can you post what you're trying with example data? Are you using ggplot2 or base R for plotting?
Yes it‚Äôs fully into memory. It‚Äôs not like it‚Äôs super slow or anything like that but I‚Äôm trying to run it a lot of times and really just learn how to get better at R. Right now I can usually do what I want but it‚Äôs probably not in the most efficient way. 
Well sorry to break the news to you but it's probably not going to get much faster. I switching to dplyr might help but I doubt even that will do much. Maybe getting a computer with more memory bandwidth?
Thank you for saving me a lot of time trying everything I can. My computer is pretty good but not super amazing. I‚Äôm going to run it on my brothers comp and see what happens. I did end up getting it a lot faster by pasting all the variables I was looking for into one spot and subset only one thing. I posted about it another reply. 
You don't *need* to, but any cases not covered in your `case_when` logical catches will have `NA` values for that variable. Here's an illustration from /u/Lareine's example above, library(tidyverse) data &lt;- data.frame( first = rep("a", 3), second = letters[2:4] ) data # first second # 1 a b # 2 a c # 3 a d data %&gt;% mutate( new_var = case_when( first == "a" &amp; second == "b" ~ "AB", first == "a" &amp; second == "c" ~ "AC" ) ) # first second new_var # 1 a b AB # 2 a c AC # 3 a d &lt;NA&gt; You can get around this issue by having a catch-all category where the logical check is `TRUE` (always true). data %&gt;% mutate( new_var = case_when( first == "a" &amp; second == "b" ~ "AB", first == "a" &amp; second == "c" ~ "AC", TRUE ~ "Uncategorized" ) ) # first second new_var # 1 a b AB # 2 a c AC # 3 a d Uncategorized 
R Studio is an Integrated Development Environment. When you're using vanilla R, it's just a command line interface where you input code line-by-line. In an IDE the text edit window let's you edit large chunks of code and execute it all at once, which is why R Studio numbers lines.
I use it for aggregating and merging for a reporting dashboard. Super short code which is nice for when things need to change. 
Okay I got it, thank you. I now notice the &gt; sign in the Console. 
Can you zip them and have your app unzip them? Do you need all of the files? Can you remotely download the files on deploy?
&gt; I was wondering if I can do anything to improve this Yes, trivially, by removing the entirely superfluous `which` call: data[data$first==aaa &amp; data$second==bbb, ] That said, performance of all these methods is going to be very similar. Be wary of benchmarks (like the one posted in another comment) that claim that methods that do more work are somehow faster. Case in point: that benchmark yields completely different results on my machine, and shows (unsurprisingly) that leaving out `which` is faster.
To expand, rstudio is not the only IDE for R out there. I personally don't like rstudio and much more prefer RKWard - you might want to try that out.
Sorry for the late reply. `filter` won't work on matrices so yes, use a data frame. 
Another option: any way to combine files?
Deploy your own shiny server somewhere perhaps.
Look in to the dbplyr package. 
 product &lt;- c("toy", "toy", "car") consumer &lt;- c(12, 3, 12) attribute_A &lt;- c("smart", "dumb", "smart") b &lt;- data.frame(product, consumer, attribute_A) a &lt;- aggregate(product ~ consumer + attribute_A, data = b, FUN=paste, collapse="") # print(a) |consumer|attribute\_A|product| |:-|:-|:-| |3|dumb|toy| |12|smart|toycar| That already doesn't quite look like what you want, but just for funsies I tried a group by: library(sqldf) c &lt;- sqldf("select * from b group by consumer") # print c |product|consumer|attribute\_A| |:-|:-|:-| |toy|3|dumb| |car|12|smart| So yeah, that didn't quite work. [This should, though.](https://stackoverflow.com/questions/12668528/sql-server-group-by-clause-to-get-comma-separated-values) Can't run it in sqldf, but see what this does: SELECT consumer, attribute_A, product = STUFF((SELECT ', ' + product FROM b b WHERE b.consumer = a.consumer FOR XML PATH(' ')), 1, 2, '') FROM b a GROUP BY consumer;
that actually might only work in sql server /shrug
LOL DERP thank you u/theDaninDanger, this is the correct solution u/ChubbyC312. You can run this in R: product &lt;- c("toy", "toy", "car") consumer &lt;- c(12, 3, 12) attribute_A &lt;- c("smart", "dumb", "smart") b &lt;- data.frame(product, consumer, attribute_A) # a &lt;- aggregate(product ~ consumer + attribute_A, data = b, FUN=paste, collapse="") # print(a) library(sqldf) c &lt;- sqldf("select consumer, GROUP_CONCAT(product) as product, attribute_A from b group by consumer") print(c) |consumer|product|attribute\_A| |:-|:-|:-| |3|toy|dumb| |12|toy,car|smart|
some use of strsplit()
Install.packages complete. Didn‚Äôt use it for this task but really interesting
Thanks for the follow up
You solved it for me, +1. I had never heard of this function before, but I will need To apply the code across sql environments and didn‚Äôt think about the fact that it likely won‚Äôt translate to some..
I am trying to compare two different group means using a GGplot2 and comparing them via a barplot. Thank you:\)
Totally!
Hmm interesting. I'll take a look at RKWard. R Studio seems to be the dominant interface for R it feels like. 
This is way too proprietary to answer properly. You're going to have to work with your IT team to resolve it.
You will probably need either continue using rvest with the same session, or extract the relevant cookies that you received when you logged in, and send then along when you download the file through curl.
I understand that the proprietary nature of this question makes this a frustrating question to try to answer. I was hoping (Given that I have successfully logged into the site using rvest) that someone had experience with using the session cookie from an rvest login, to follow a download link, along the lines of what /u/questionquality is suggesting. Although I know my particular use-case is undoubtedly very proprietary, I am certain that this could apply to other situations. Thanks for your input! 
That worked, you are a legend! I have been pulling my hair out with this for the past 2 days. Thank you so much!
Not a problem, happy to help. By the way, since it looks like you're using different databases, the type of code you want is an "ORM" (Object Relational Mapping) framework. ORM decouples the database connection from query operations, and uses a standard query syntax to give reusable code across multiple databases. As /u/dash_44 mentioned, dplyr, dbplyr are the current best in class for R ORM. Here is a stack overflow post which should give you the code you need: https://stackoverflow.com/questions/38160499/group-concat-with-dplyr-or-r As well as some docs on connecting to a db with dbplyr: https://db.rstudio.com/dplyr/ Now you only need to update the connection object with the different database info to make the code work, so it's much more portable and maintainable. Hope that helps!
I don't think it's the worst thing you can do. I use both for my work (genomics/transcriptomics). They are very similar, but there are a lot of differences. Just be prepared to mess up a lot when you go back and forth.
imo the cleanest way is: ogdata &lt;- ogdata %&gt;% dplyr::select(variablecare, variablecare2, etc, everything()) newdata &lt;- ogdata[,1:20] if you selected twenty variables. This way you move the variables you care about to the front of the dataset first. So, you're cleaning your original. Then, you know how many you wanted because not only are they all at the front, but you formatted select nicely so every row was 2-3 names of variables. Count the rows and boom, got it all. Also - never do this: &gt;I was thinking this would allow me to more easily find correlation/significance without wading through a bunch of unnecessary stuff. You don't find significance. You test things that should have significance due to theory. Don't go phunting or phacking.
Yea, I may just spend another month getting a solid foundation on R. I'm registered for the MIT Introduction to computer science and programming using python (on edx.org), which closes August 30, 2018 so I don't want to save that for the last minute. 
&gt;not generally recommended to start learning both at the same time Personally I think learning multiple languages simultaneously is a great approach. It teaches you the fundamental differences between languages, and how languages work. Post programmers work in multiple languages so its good practice. 
I teach a class that uses both or either. Some students choose one or another, others dabble in both. They're really not that different and it can be useful to see the differences. In fact, I just made a lecture showing the implications of R's 1-based indexes vs. Python's 0-based indexes. Also how Python has the `+=` increment while R doesn't. Lots of little differences that don't amount to much. Doing both doesn't seem to cause to many mental problems for me. Now, dealing with D2L...
I agree with everything that MrLegilimens posted above. You mention that your dataset is quite a few columns wide. Out of curiosity, is each column a particular, unique variable? If you‚Äôre new to R, I recommend taking a look at [how to tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) and have it in a longform rather a ‚Äòwide‚Äô form - it makes everything much easier! Apologies if this is already familiar to you
I have learned that it is seriously not practical to just focus entirely on one language at a time. You need to swap around out of necessity. This is for R and Python. If you never programmed before and simultaneously learned C++ and R then ya it would feel really bad probably. 
I like python for data management and R for modeling and visualization. So there is definitely a role for both. 
I'd definitely be interested in seeing your indexing examples. I think starting at 1 is more natural for humans but I understand that the majority of programming languages consciously choose 0.
data.table let's you read only the relevant columns, which will be much faster than reading them all and filtering later.
It's a matter of personal style. For me I can't do two languages at once so I opted for R first and it has paid off for me. I have the basics of python down but I don't have a current use case that R doesn't satisfy. (ecologist BTW) also could I ask what company you work for if you can't say here could you pm me? I'm mostly just wondering for future career outlook reasons. 
What data are you collecting...?
0 works well when you're having to index raw memory. If you have an array of 10 4-byte integers that starts at address 100, then the first one is at 100 + 0 * 4, the next at 100 + 1 *4, and so on. Here is an example of what I showed my class. This is not the most efficient way to estimate the binomial distribution - but this code example is to demonstrate how to get the result from a function and use that as a basis for changing values in a vector/array. As far as I can tell, these pieces of code are functionally equivalent. R: # estimate binomial distribution for 10 coin flips get_num_heads &lt;- function(n) { #function to return number of heads in n flips heads &lt;- sum(sample(x = 0:1, size = n, replace = TRUE)) return(heads) } num_flips &lt;- 10 # vector to accumulate counts heads_count &lt;- rep(0, num_flips + 1) for (i in 1:1000) { num_heads &lt;- get_num_heads(num_flips) # num_heads is an index into our vector, but we need to add 1. # e.g. if num_heads is 3, then we need to increment 4th position heads_count[num_heads + 1] &lt;- heads_count[num_heads + 1] + 1 } # put results in a dataframe and display df &lt;- data.frame(Heads=0:10, Probability=heads_count/sum(heads_count)) print(df) Heads Prob 1 0 0.002 2 1 0.011 3 2 0.044 4 3 0.100 5 4 0.207 6 5 0.238 7 6 0.229 8 7 0.111 9 8 0.048 10 9 0.008 11 10 0.002 Python: # estimate binomial distribution for 10 coin flips import numpy as np import pandas as pd def get_num_heads (n): #function to return number of heads in n flips heads = sum(np.random.randint(low=0, high=2, size=10)) return(heads) num_flips = 10 # array to accumulate counts, note we have to add 1 to get from 0 to 10 heads_count = np.zeros(num_flips + 1) for i in range(1000): num_heads = get_num_heads(num_flips) # num_heads is an index into our array - we don't have to add 1 like R # also note Python has an increment operator, +=, simplifying the code heads_count[num_heads] += 1 # put results in a dataframe and display df = pd.DataFrame({'Heads':np.array(range(num_flips + 1)), 'Prob':heads_count / sum(heads_count)}) print(df) Heads Prob 0 0 0.000 1 1 0.014 2 2 0.043 3 3 0.115 4 4 0.225 5 5 0.233 6 6 0.192 7 7 0.125 8 8 0.043 9 9 0.009 10 10 0.001 The output from each also reinforces the index idea, since the R code shows the row numbers of the dataframe as 1-11, while the Python/Pandas dataframe shows 0-10.
Here's the code I am using install.packages('ElemStatLearn') library(ElemStatLearn) set = test_set X1 = seq(min(set[, 1]) -1, max(set[, 1]) + 1, by = 0.01) X2 = seq(min(set[, 1]) -1, max(set[, 1]) + 1, by = 0.01) grid_set = expand.grid(X1, X2) colnames(grid_set) = c('Age', 'EstimatedSalary') y_grid = predict(classifier, newdata = grid_set) plot(set[, 3], main = 'Random Forest (Test Set)', xlab = 'Age', ylab = 'Estimated Salary', xlim = range(X1), ylim = range(X2)) contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato')) points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) Damn, I am not able to figure out how to make the code readable, Sorry. But anyway, this the plot I got. https://imgur.com/a/n1V97fi 
Use ggplot instead of deafult ploting. For me it is hard to read the deafult one. 
Thanks alot, freind.
No problem!
R is an interpreted language. There is not concept of a compiler. There is no compiling. However a lot of R packages are written in low level compiled language like Fortran, C or C++ for performance reasons and R has ways to communicate with them. It should be noted that the packages are calibrated to use precise build options and messing with some compile-time optimizations may result in severe crashes, so these should be left alone. However this is not your case and the first paragraph I've written applies. `ident` function simply uses features of R language that are computationally intensive.
Histogram would be good. Here's code for an overlapping histogram: https://www.r-bloggers.com/overlapping-histogram-in-r/ Box plots are fine. 
Box plots can be good, histograms as well. I typically show density plots on data that I use t-tests on. 
Overlapping density plots are the way to go IMO, super intuitive for people to look at and very easy to whip up in ggplot2. Some sample code: fake_data &lt;- data.frame( treatment = c(rep("control", 10000), rep("test", 10000)), value = c(rnorm(10000), rnorm(10000) + .5) ) fake_data %&gt;% ggplot(aes(value, fill = treatment))+ geom_density(alpha = .5)
Ok, using word "compiler" I mean the byte code compiler for R that apparently works when I execute the codes. It can be verified by compiler::enableJIT(0) # disabling the optimization and then microbenchmark::microbenchmark(f0(1e6)) results with Unit: milliseconds expr min lq mean median uq max neval f0(1e+06) 172.6005 203.6039 205.2085 205.3237 206.8338 238.1511 100 When I use the default optimization or explicitly compiler::enableJIT(3) I get Unit: milliseconds expr min lq mean median uq max neval f0(1e+06) 27.40175 27.5102 27.76135 27.57244 27.78196 31.28689 100 The difference in speed is significant: 205.3 / 27.6 =&gt; 7.438406. Tight loops in the body of f0 and f1 are for the purpose here - we really gain speed and this may not happen in other scenarios. I really want to find out why the block wrapped within effectively costless ident does not undergo optimization. The problem looks even more complicated as with JIT set to 3: microbenchmark::microbenchmark( f1(1e6), ident({ value &lt;- 0 for (i in seq_len(1e6)) value &lt;- value + i value }) ) Unit: milliseconds min lq mean median uq max neval 159.1249 165.68130 170.74032 167.20638 171.0127 217.35693 100 28.6507 29.00079 30.18074 29.61584 30.6858 44.60623 100 It seems like the byte-code compiler does not optimize function()ident({ ... }), but does it for ident({ ... }) outside of the function.
Thanks! One of my problems is that my data is very binary. I am comparing two races and their likelihood to face prison sentences. So I have Race1 with either 0/1 and Race2 with 0/1 being compared in PrisonSentences 0/1. All the graphs I've tried to make so far mess up because there isn't really a "range" per se. Is there a way around this?
Anyway, I cannot reproduce your issue: &gt; ident &lt;- function(x) x + f0 &lt;- function(n) { + value &lt;- 0 + for (i in seq_len(n)) value &lt;- value + i + value + } + f1 &lt;- function(n) ident({ + value &lt;- 0 + for (i in seq_len(n)) value &lt;- value + i + value + }) + microbenchmark::microbenchmark( + f0(1e5), + f1(1e5) + ) Unit: milliseconds expr min lq mean median uq max neval f0(1e+05) 6.177671 6.264519 6.378212 6.310929 6.400115 10.49288 100 f1(1e+05) 6.482109 6.811132 6.924119 6.835262 6.929093 11.20073 100 + compiler::enableJIT(0) # disabling the optimization [1] 3 + microbenchmark::microbenchmark( + f0(1e5), + f1(1e5) + ) Unit: milliseconds expr min lq mean median uq max neval f0(1e+05) 6.177252 6.256696 6.397524 6.299858 6.383249 9.009596 100 f1(1e+05) 6.470306 6.790493 6.838402 6.827475 6.850766 7.982929 100 + compiler::enableJIT(3) [1] 0 + microbenchmark::microbenchmark( + f0(1e5), + f1(1e5) + ) Unit: milliseconds expr min lq mean median uq max neval f0(1e+05) 6.176764 6.247443 6.390593 6.294935 6.369421 10.80207 100 f1(1e+05) 6.647145 6.797932 7.010917 6.826288 6.902031 10.72678 100
Ah, well, my app is predictive text. The user types some words, and the app predicts the ending of the current word they're on (or the next word if they've just typed a space), based on up to 3 of the most recently typed words. I've taken a large training text and extracted all the 1, 2, 3 and 4-word sequences it contains, and counted how many times each unique one appears. Then I've organised them alphabetically into many thousands of files. For the library of individual words, I've organised them according to their first 2 letters (i.e. "aa.txt", "ab.txt", ac.txt", ..., "zz.txt") and for the 2, 3, and 4-word sequences, I have a file for each 3-letter prefix ("aaa.txt", "aab.txt", etc.). So, for example, say the user wants to have "this is a sentence" and they've so-far typed "this is a ", my app would go into the 4-word-sequence directory and open the file "thi.txt" (i.e. the first 3 letters of the word "this"), which contains all 4-word sequences for which the first 3 characters are "thi". Then all the lines in this file that start with the sequence "this is a " are subsetted out, and a few of the most frequent sequences "this is a ___" are displayed. Thus I have many files. There are 26 letters in the alphabet, and word sequences with more than 1 word can contain spaces (ie. an additional character), so the number of files I initially have is 26*26 + 3*(26*(26+1)*(26+1)) = 57,538. Most files occupy just a few KB, although the largest is about 5 MB. For a total of about 350 MB. Many of these files are for obscure sequences of letters that no words start with ("fqz" and so on), so those files can be thrown away with no negative effects, but those files are only about a third of the total. So... does zipping up my files help? 
Maybe. You'd have to test how fast it is. There's some overhead involved, to be sure. https://unix.stackexchange.com/questions/14120/extract-only-a-specific-file-from-a-zipped-archive-to-a-given-directory 
It sounds like you could just show a contingency table?
If you can give me a minimum reproducible example of what your dataframe looks like, I can help you out. dplyr::bind_rows() and tidyr::unnest() are both super helpful functions here that are vectorized so they probably do not need to be used in a for loop. 
try &gt;dev.off() it's like the "unplug for ten seconds" of R plots. it closes the graphical device, and when you order plot again it will open anew. 
Answer is quick and dirty, since I am on mobile as well, but I usually do this: plyr::ldply(list, data.frame, .id="putnamehere")
I went to another machine, with Microsoft R Open this time. compiler::enableJIT(0) microbenchmark::microbenchmark( f0(1e5), f1(1e5)) Unit: milliseconds expr min lq mean median uq max neval f0(1e+05) 35.94107 37.12688 38.97551 37.73854 38.66981 48.62819 100 f1(1e+05) 35.87087 37.18715 40.52514 37.92832 42.88924 87.38360 100 and then compiler::enableJIT(3) Unit: milliseconds expr min lq mean median uq max neval f0(1e+05) 5.364841 5.434727 5.648472 5.525611 5.679808 9.468857 100 f1(1e+05) 33.638052 35.461329 37.473159 35.921515 36.922034 79.832750 100 The problem looks like sthing systematic. sessionInfo() R version 3.4.4 (2018-03-15) Platform: x86_64-w64-mingw32/x64 (64-bit) Running under: Windows 7 x64 (build 7601) Service Pack 1 Think I need more study on this. Best regards. Konrad
You might want to take a look at lapply in conjuction with either `[` or `[[` (those are backticks not single quotes.) (take a look at ?`[[` or search stackoverflow) 
You need to install the package data.table, and you should reinstall the car package as well. When you install the packages, make sure you use the option ‚Äúdependencies = TRUE‚Äù which will download all the packages that are needed i.e. install.packages(‚Äúdata.table‚Äù, dependencies = TRUE). I recommend doing this everytime you install a package, otherwise you will run into these types of issues.
I read an article about coders who didn't know how to write a basic fizzbuzz sequence, so I wrote one to prove to myself that I could at least do the bare minimum.
Please provide a reproducible example in the future. If I I understand your question correctly, this should work: data.table::rbindlist( lapply(fake_data, `[[`, 2) )
I notice these errors a lot, and often it‚Äôs because a package tried to install but failed. Try installing data.table and if you get an error, try fixing that error.
I assume you are running R 3.5. Lib data.table does yet work in R 3.5. They have a soultion that has worked for some, but not me, on their gitHub. I had to go back to R 3.4.4. In R Studio you do that in Global Settings. 
... That's a lot of unformatted and hard-to-read code. What's it do? Prepend each line of code with four spaces so it becomes formatted as code in yr post.
Sorry I didnt know how to do it and thank you for telling me. I hope that its better now. Its just a simple for loop but each line is dependent on some line before it. 
Do you have any data you can include so others can run yr code?
Okay I put in the original data as best I could since this is only about a quarter of the function and its already really long. 
Well, it's not necessarily true that "loops are garbage". Some things can only be done with loops. Native loops in R aren't necessarily bad, but they can be the slowest if there is an optimized piece of code that can do what you need (often written in C or Fortran and served as a library) - but even deep down, they're doing loops... probably with the equivalent of `GOTO`. It's weird to me that your code is using `i` in some places and `i + 1` in others... does this code as a loop function properly and generate correct results? It looks like you're doing something that is step-wise/numerically solving a differential equation. It might be worth seeing if something like deSolve could help you - since if you can set up the problem and the function for the integration step, it can do the work pretty fast. Take a look at this: https://journal.r-project.org/archive/2010/RJ-2010-013/RJ-2010-013.pdf, and this book is quite good: https://www.springer.com/us/book/9783642280696. If you consider this approach, I'd recommend starting simple (maybe ignoring drag, etc.) and get it working, then add the complexity of other variables and interactions. And generally, instead of doing all those `rbind` on separate variables, it might make more sense to use a dataframe to accumulate results, then have each variable only keep track of its state in the current iteration of the loop. Also, you can sometimes get out of looping by rows if you can calculate entire columns of some variables at a time, but that might not be the case here. Lastly, if you want to use `apply`, you'll need to make a function that gets used in the `apply`. I'd set it up so it takes the current state of each variable as an input (e.g. values at beginning `i = 1`) and returns the updated values at the end of that time-step (probably as a list, or one-row data-frame). Then you could use apply to do that 1126 times... but that function shouldn't be doing all those `rbind` operations. --- You might find this partial solution to a sky-diver problem from **Introduction to Scientific Programming and Simulation Using R** (Chapter 13, Excerise 6). interesting: chute &lt;- function(Ti, h = 0.1, tol = 1e-4, plot.traj = F) { # Missing code is denoted ?? # # Trajectory of a skydiver # Ti is time to open parachute # state y = (velocity, height) g &lt;- 9.81 # acceleration due to gravity m/s^2 m &lt;- 100 # mass of skydiver kg d1 &lt;- 0.31 # drag without chute kg/m d2 &lt;- 56 # drag with chute kg/m ht0 &lt;- 2000 # jump height m # d1 is obtained from terminal velocity for a 100kg person of 200 km/h # d2 is obtained from terminal velocity for a 100kg parachute of 15 km/h # derivatives of y at time x dydx &lt;- function(x, y) { ?? } # 4-th order Runge-Kutta with adaptive step size x &lt;- 0 y &lt;- ?? # initial conditions if (plot.traj) y.trace &lt;- c(x, y) while (??) { # stopping criteria # readline(paste(x, y[1], y[2])) # for diagnostics # RK4 using step size h k1 &lt;- h*dydx(x, y) k2 &lt;- h*dydx(x + h/2, y + k1/2) k3 &lt;- h*dydx(x + h/2, y + k2/2) k4 &lt;- h*dydx(x + h, y + k3) y1 &lt;- y + k1/6 + k2/3 + k3/3 + k4/6 # RK4 using two steps size h/2 k1a &lt;- k1/2 k2a &lt;- h/2*dydx(x + h/4, y + k1a/2) k3a &lt;- h/2*dydx(x + h/4, y + k2a/2) k4a &lt;- h/2*dydx(x + h/2, y + k3a) y2 &lt;- y + k1a/6 + k2a/3 + k3a/3 + k4a/6 k1b &lt;- h/2*dydx(x + h/2, y2) k2b &lt;- h/2*dydx(x + 3*h/4, y2 + k1b/2) k3b &lt;- h/2*dydx(x + 3*h/4, y2 + k2b/2) k4b &lt;- h/2*dydx(x + h, y2 + k3b) y3 &lt;- y2 + k1b/6 + k2b/3 + k3b/3 + k4b/6 y.new &lt;- y3 + (y3 - y1)/15 # update h if (max(abs(y3 - y1)) &gt; tol || y.new[2] &lt; -tol) { h &lt;- h/2 } else { # update x and y x &lt;- x + h y &lt;- y.new # update h if (max(abs(y3 - y1)) &lt; tol/2) h &lt;- 3*h/2 # record keeping if (plot.traj) y.trace &lt;- rbind(y.trace, c(x, y)) } } # output if (plot.traj) { opar &lt;- par(mfrow=c(2,1), mar=c(4,4,1,1)) plot(y.trace[,1], y.trace[,2], type='o', xlab='t', ylab='velocity') plot(y.trace[,1], y.trace[,3], type='o', xlab='t', ylab='height') par &lt;- opar } return(c(x, y)) } # safe landing speed is 20 km/h = 5.6 m/s This solution doesn't use `deSolve` but explicitly codes a Runge-Kutta solver for differential equations. That said, it's still essentially doing a loop. The book's website is: http://researchers.ms.unimelb.edu.au/~apro@unimelb/spuRs/ 
First of all thank you so much for your reply. I read it but have not had the chance to go through it in complete detail and read all the links you gave but I will do so. As the the i and i+1 I need it to be like that because sometimes Im using the previous row for the equation and sometimes im using the row that im in. It also has to go in an exact order for the loop to work. So yeah I know it looks weird but yes it was intentional and the loop works perfectly right now. As for the cbinds I did take them out but I was using numbers for the columns and just figured it would be easier to look at. I also wrote that code when I first started using R about a year ago so I didnt know what the hell I was doing although Im not sure im that much better now. The loop is actually pretty simple when it comes down to it but I probably made it look harder than it needs to be. Here is an easier one that is pretty much the same thing although I just made up random numbers and used i to where I need it to go in the loop. try=matrix(0:0,nrow=5,ncol=4) try[1,1]=1 try[1,2]=2 try[1,3]=try[1,1]+try[1,2] try[1,4]=try[1,2]+try[1,3] i=1 try[i+1,1]=try[i,3] + 1 try[i+1,2]=try[i,4] + 2 try[i+1,3]=try[i+1,1]+try[i+1,2] try[i+1,4]=try[i+1,2]+try[i+1,3] i =2 try[i+1,1]=try[i,3] + 1 try[i+1,2]=try[i,4] + 2 try[i+1,3]=try[i+1,1]+easy[i+1,2] try[i+1,4]=try[i+1,2]+try[i+1,3] So after I input the first two I have the data required to get the 3rd and 4th of the first row. For the 2nd row first column I need the first row and third column and so fourth. I just need everything to go in the right order so the right things can be added together. Here is what I had without the cbinds traj=as.matrix(seq(from=0, to = 11.26,by= 0.01)) x=0 vx=0 vxw=0 adragx=0 ax=0 traj=cbind(traj,x,vx,vxw,adragx,ax) colnames(traj)[1]="t" together=matrix(0:0,nrow=1127,ncol=20) together[1,1]=2 together[1,2]=3 together[1,3]=2 together[1,4]=0 together[1,5]=136.873 together[1,6]=37.96875 together[1,7]=142.0417 together[1,8]=136.3998 together[1,9]=96.82458 together[1,10]=0.07397359 together[1,11]=0.4109531 together[1,12]=0.1294107 together[1,13]=-40.25684 together[1,14]=5.864425 together[1,15]=-11.66719 together[1,16]=796.1411 together[1,17]=-3.549376 together[1,18]=12.24687 together[1,19]=-43.80622 together[1,20]=-31.59432 together[i+1,1]=together[i,1]+together[i,5]*dtsec+0.5*together[i,19]*dtsec*dtsec; together[i+1,2]=together[i,2]+together[i,6]*dtsec+0.5*together[i,20]*dtsec*dtsec; together[i+1,3]=sqrt(traj[i+1,"x"]^2+together[(i+1),1]^2); together[i+1,5]=together[i,5]+together[i,19]*dtsec; together[i+1,6]=together[i,6]+together[i,20]*dtsec; together[i+1,7]=sqrt(traj[i+1,"vx"]^2+together[i+1,5]^2+together[i+1,6]^2); together[i+1,8]=ifelse(together[i+1,2]&gt;=hwind,sqrt((traj[i+1,"vx"]-vxw)^2+(together[i+1,5]-vyw)^2+together[i+1,6]^2),together[i+1,7]); together[i+1,9]=together[i+1,7]/1.467; together[i+1,10]=(romega/together[i+1,8])*exp(-traj[i+1,'t']/(tausec*146.7/together[i+1,7])); together[i+1,11]=cd*(1+cdspin*together[i+1,10]^2); together[i+1,12]=1/(2.32+0.4/together[i+1,10]); together[i+1,13]=ifelse(together[i+1,2]&gt;=hwind,vyw,0); together[i+1,14]=-const*together[i+1,11]*together[i+1,8]*(together[i+1,5]-together[i+1,13]); together[i+1,15]=-const*together[i+1,11]*together[i+1,8]*together[i+1,6]; together[i+1,16]=omega*exp(-traj[i+1,"t"]/tausec)*30/pi; together[i+1,17]=const*(CI[i+1,]/omega)*vw[i+1,]*(wz*(traj$vx[i+1]-traj$vxw[i+1])-wx*vz[i+1,]); together[i+1,18]=const*(together[i+1,12]/omega)*together[i+1,8]*(wx*(together[i+1,5]-together[i+1,13])-wy*(traj[i+1,'vx']-traj[i+1,'vxw'])); together[i+1,19]=together[i+1,14]+together[i+1,17]; together[i+1,20]=together[i+1,15]+together[i+1,18]-32.174
This looks really helpful, thanks.
I would worry more about something not working when mixing old and new packages and R. You certainly might make a backup of your local R library and copy it over, but I'd save it as a last resort. I'd update everything to a new version from scratch (meaning R and packages).
Isn't this the purpose of the packrat package? 
I don't know the exact code that's do the trick for you,but look into the dplyr package, as it reads very similarly to SQL (I think it can even send SQL code to servers too...), and should have 2hat you need.
So if I wanted to analyze things like genre, male/female author, publication year, etc., should I repeat that info every time I record pages read? From what I understand, tidy data means each row is an observation, so in this case it would be a single day‚Äôs reading of a single book. But it seems that there would be a lot of repeated data. I know you said to worry about format later, but I just want to make sure I don‚Äôt make it more tedious than it needs to be.
I mean things like that you can just select and drag down in excel to copy.
Thanks, got it working R Tools package was the last piece of the puzzle.
Tedium at the expense of complexity. The repeated data thing you mention would generally be taken care of by creating multiple tables, and linking them with primary and foreign keys of some sort (e.g., a relational database); but, there‚Äôs no reason to add this complexity to your situation. I would just use a single spreadsheet, and repeat some things that identify the book. It‚Äôs going to keep things simple (tho a bit redundant), and straightforward. If I were doing what you are right now, I‚Äôd just use a single spreadsheet.
Awesome, thanks!
You can make an empty vector then have the output of each loop concatenated to it. x2 &lt;- vector() for (i in 1:nrow(x)) {x2 &lt;- c(x2, length(which(!is.na(x[i,]))))} Sorry if there are typos, doing this on my phone
Happy data-ing!
Cool idea. I don‚Äôt know how to do it purely in R. But if I had to get it done, I‚Äôd export the barplot as .ppt using the ReportERs package and edit it in PowerPoint. 
Not a bad idea. I need it for a powerpoint presentation anyways, may just do that. Thanks!
How do you sort by two variables simultaneously?
This is exactly what I need. Thanks a lot!
Do you mean why doesn't `mean( UQS(args) )` work as given? That's because you're asking R to "unquote" something that hasn't been "quoted". Since `args` isn't a quoted expression, it can't be unquoted. If you think you're struggling to understand the "why bother, then", [give this a watch](https://www.youtube.com/watch?v=nERXS3ssntw), and [give this a read](https://adv-r.hadley.nz/meta.html) (but read the whole chapter).
You have made many great contributions about R and statistics here on Reddit. Thank you! 
You rock, thanks for the tips!
But also, to clarify, would that result in one column being grouped and the other being sorted within those groups?
What I'm trying to do is to not repeat my self. My question is, if I had hundreds of rows instead of the last five rows I have now, would it be possible to just add the used variables into a vector (like in the first two rows) and somehow loop through them.
In addition to /u/ryapric‚Äôs excellent answer, it‚Äôs important to realise that `!!` and `!!!` (and `UQ`, `UQS`) by themselves don‚Äôt do anything. They are only meaningful *inside a ‚Äúquasiquotation‚Äù*. And since `mean` doesn‚Äôt treat its arguments as a quasiquotation, `mean(!!!args)` (or `mean(UQS(args))`) is meaningless. In fact, outside a quasiquotation, `!!x` is the same as `!(!(x))` ‚Äî that is, it simply negates the value of `x` twice: ‚å™ x = TRUE ‚å™ !!x [1] TRUE That‚Äôs why you need to use something like `quo`: this function creates a quasiquotation, inside which you can unquote/unquote-splice values.
You can certainly do both. I write python code and then call out python functions in R using the reticulate package. https://rstudio.github.io/reticulate/index.html
Take the content of your files and put them into a database? No restrictions on calls to database as far as I know. I use PostgreSQL to store many gigabytes of data for our Shiny application at my job.
This is kind of a hot mess - you should look into optimizing your code for R before you get into parallelizing it. Your 'for' loop, for example, doesn't need to add 'i' - R does this automatically. That loop could also be replaced with an 'apply' family function to really speed things up. Do you have any other coding experience? R is often painfully slow just by nature, and it's often easier to just rewrite code in C and import the compiled function into the R environment - R does this natively. Sorry if that doesn't answer your question - hope it helps!
Wow, no, stupidly enough I have not. Considering I use it for all my other graphing in R, I can't believe I missed this. Thanks for this advice. It seems to work well. I will go back and redo my graphs now.
Link to Stack Overflow with multiple implementations: https://stackoverflow.com/questions/6967664/ggplot2-histogram-with-normal-curve?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa
I don¬¥t have any code experience. This code was made by readding some pdfs and a lot of mistakes. Was a lot of help. The problem is the filter that i made, its dependent of the 'i' ins the for loop. arranjo &lt;- "AEBFCGDH" cond &lt;- which(ordem == arranjo) resultados &lt;- campeao[cond] table(resultados) tab_result &lt;- table(resultados) total &lt;- sum(tab_result) barplot(table(resultados)/total, main = arranjo, ylim = c(0,0.5)) text(3, 0.45, total)
As written, it's not dependent on the 'i' - maybe you left code out? R also automatically cycles through the 'i' - you don't need to advance the counter manually like in other languages. Does that make sense? What's your objective with this code? Can you add an example of what your data looks like before the processing?
Okay - why don't you put your script on Github, or pastebin? You're just going to need to get better at R to optimize this code, but I can give you some suggestions in the meantime.
Arriving at home I gonna put in GitHub. In the meantime, become clear what I need and wanna with this code? If a can substitute de 'for' function for lapply the algorithm is gonna be a lot faster.
Yeah, could you put in comments in English also? Even with 8 million runs the function isn't complex enough to take that long, if you structure it right. 
I'm gonna translate everything and post tonight.
If you want to stick with base graphics, you can specify the bins for the data with the `breaks` argument. For example: b = seq(floor(min(myData)), ceiling(max(myData)), by=10) hist(myData, breaks=b) Then you can use `lines()` to add a normal curve.
Yeah i love it
&gt; Github His is the link man [https://github.com/jaumjl/Playoff\-bracket/tree/master](https://github.com/jaumjl/Playoff-bracket/tree/master)
Okay I'll give it a look after lunch and see if I can help you out 
Thanks a lot man. 
I think the problem is that you have a bunch of things on the boundary perhaps - the histogram break function does not assign these symmetrically! I don't know why you think mirroring this would get a "normal distribution", however. 
This has been asked x&lt;-Inf times. Please use the search. 
K. Thanks for being a dick. 
\+
I'm having trouble buying things I don't have money for - I always want to have them. Can someone setup a regular direct deposit of just a mil/mo into my account? I lack the motivation, since I am earning $$k/mo. Routing number is: 555-12345.
I asked for help with a learninng data set so I can learn how to do something in R that I already know how to do in SQL. You guys are hilarious 
The other comments are needlessly hostile, I'll give you that. However, you basically asked "someone do this for me because I'm lazy," which is... less than smiled upon in the R community. If you want a more detailed, thorough response, you're going to want to make an honest try yourself and post your code if it doesn't work. Dplyr is used extensively, so simply googling something like "how to select rows based on column values in r" will likely yield a dplyr solution as the top result. You can Google similar things for whatever operation it is you're trying to do.
And thanks - didn't even catch the nuance of the pipeline operator
You'd not be the first to say that, my incredibly lazy and entitled fellow R user.
This looks amazing! Are there any travel scholarships? I'm a marine scientist, specialising in data science but i live in Australia so I won't be able to make it. Looks great though
Thank you! We do not offer travel scholarships but we are working on ways for people to participate remotely. Sign up for the mailing list at https://hackforthesea.tech to stay in touch.
not going to say this is impossible but I've been trying to do something similar with no luck finding a solution. If you find one, please respond back, I'd like to hear how you did it.
Where did you learn about it? It's not a base R function, must be from a package somewhere that you didn't load. I just searched through (`??summaryStats`) and it's not in any of the packages i have installed.
http://lmgtfy.com/?q=summarystats
I'd suggest posting it at https://community.rstudio.com/ or maybe at Stack Overflow. It's actually a good question.
How different are you expecting it to be from 'summary()'?
If you're using shinyapps.io, it doesn't get upgraded to new R versions until some time after the release of that version. Past releases its been a couple weeks if I recall.
[Funny I'm "so entitled" yet I did the exact same thing I'm asking for \(in the reverse direction\) for this community a week ago.](https://www.reddit.com/r/Rlanguage/comments/8f3ioc/quick_logic_question_converting_small_snippet_of/dy0jdqp/?utm_content=permalink&amp;utm_medium=user&amp;utm_source=reddit&amp;utm_name=frontpage) Must be hard to walk with that stick so far up your keister, bro. 
Dude, go away.
not until you write my dissertation for me, fam. 
I don't have input right now; however, when writing code in reddit, it's best to include 5 spaces before typing. Example without spaces: x &lt;- somedata y &lt;- moredata z &lt;- cbind(somedata, moredata) This then becomes: x &lt;- somedata y &lt;- moredata z &lt;- cbind(somedata, moredata)
thanks for the tip! 
So you have three variables: IV, dv1, and dv2. Line 1: Fitting the regression model. Line 2: Defining some colors for plotting. Presumably dv1 is binary with values 0 and 1 (or 1 and 2), while dv2 is some numeric variable. Line 3: Plot the data with different colors for the two levels of dv1. Line 4: This is where you are confused. Let's break it down a bit more. The first argument to the `curve()` function need to be en expression or function which has `x` as an argument. So `cbind(1,1,x) %*% coef(model)` is defining this particular function. What function is it? Let's look at the pieces: `coef(model)` gets the coefficients from your fitted model (in line 1). If you run this, you'll get the y-intercept, and the slope for both dv1 and dv2. Now, look at `model.matrix( model )`, you should see a column of 1's, a columns of 1's and 0's, and a column of numbers. The three values in `cbind(1,1,x)` are defining what gets multiplied by each column of that model matrix. So `cbind(1,1,x)` is saying to create a function: (Œ≤0√ó1 + Œ≤1√ó1 + Œ≤2√óX). The first `1` just means we include the intercept. The second `1` means that we set dv1=1, and the `x` means that `curve()` will plug in a number of values for `x` and plot the result. So what this whole line is doing is calculating the predicted value from the model, where dv1 is equal to 1. The next line is doing the same thing, but now `cbind(1,0,x)` says that we're going to set dv1=0. The result of the `plot()` and the two `curve()` commands is that we plot the data *IV vs. dv2*, with different colors for each value of *dv1*, and then we add the fitted regression line for the two possible values of *dv1*. If you don't have a dataset, I can whip together some code that will create some pretend data to run this.
Hey check my edit, I got it working just as I needed it to!
I just edited my original post to show that I got it working!
Thanks for the suggestion but I just edited my post to include how I got this working.
Nice job! You consider making a write up with a minimal example to publish online somewhere (like the rstudio forums or Stack Overflow like /u/AllezCannes mentioned, or on Rpubs, shiny.io, a github.io page, etc.). I'm sure there are other people out there that would be interested in implementing this. 
https://www.r-bloggers.com/careful-with-trycatch/ This looks like the most approachable page for error handling in R of the few I saw in a quick search. The initial example with `sqrt("a")` is pretty straightforward. Do you have a specific situation where you want to handle errors? 
Thanks! so much easier to understand
I tried that it was a generic function with a description that I didn't understand, thanks though :)
I'm just learning about R for a data analysis module at university. It's a function we had been shown how to use in a demonstration, so I really don't know anything about it. I was wondering if anyone had an insight on what the error I was getting was about. It seems like it may be in a library that failed to load properly. I'm using Microsoft's Azure Notebook online environment
Just four spaces are needed actually.
Love you
I love you both
thanks 
use code formatting (indent your code 4 spaces so it's readable) 
Can you show what you've tried so far?
Start by not processing in igraph. Load into a data frame and use dplyr to produce a new set of rows with ‚Äúfrom, ‚Äúto‚Äù, and ‚Äúcount‚Äù columns. The count is the number of rows with (from, to) in the original file. Write that to disk as a csv, no header. Use igraph‚Äôs function to read_ncol() format files and you should get exactly the network structure you asked for (sans nodes with no edges). 
Thank you for the reply, yes I have done that which got me down to 167760 rows which is still too much and hard to visualize I think.. The aim for me is to be able to visualize a network structure which reveals groups that have communicated with each other.
As seen in the previous post I have created a new data frame which looks like this: from|to|count :--:--:-- 45|15|24 This yields about 170 000 rows which is still too large I think. The goal is to visualize a network structure which reveals groups that have communicated with each other, like a cluster.
See the [labelled](https://cran.r-project.org/web/packages/labelled/vignettes/intro_labelled.html) package, notably `var_label()`.
If this is from an SPSS file, use the haven package.
The haven package is mostly to import SPSS files in to R, but it relies on the labelled package to handle the meta data after the import.
If you know CSS shiny allows you to add CSS code to shiny apps.
Create the command with a paste0() call. my.command &lt;- paste0("unzip -p zipped_dir.zip path_to_zipped_file/", file_to_open, " &gt; file_to_open.txt" system(command) 
Ahh excellent, works great thanks.
In addition to what /u/jeremymiles gave (which is basically identical to the following): If you start making more complicated `system()` or `system2()` calls, you might want to consider using `sprintf()` instead of `paste0()` calls, since it can be a bit more readable. Also don't be afraid to put _really_ complicated calls in a different shell script instead, and just call the script itself with `system()`. Unlikely you'll find yourself in such a hole, though (speaking from experience haha).
SCS Engineers. Currently the people who do statistics in my region either use excel or statgraphics. I'm trying to see what role python and/or R can have in improving what is currently done, and by extension carve out a role as a statistics person and get some practical statistics experience. 
You can also write and register your own controls/widgets. It might seem hard at first glance, but it's actually fairly simple if you know some JS ‚Äì and RStudio has a good introductory article, google "shiny custom widgets".
The shinyWidgets package offers greater customization with your inputs: https://github.com/dreamRs/shinyWidgets
Good point, thanks.
I'll look into these, thank you!
Depends on the audience, but generally you cannot go wrong with 1. [Data visualizations](http://ggplot.yhathq.com/) 2. [Dashboards](https://shiny.rstudio.com/) 3. [Generate reports](https://yihui.name/knitr/) 4. [Write your own r package](http://stat545.com/packages06_foofactors-package.html) If 4 seems a little daunting, try making a contribution to an existing package (like a new sort of script for ggplot or Shiny which integrates / alters existing features).
this is great thanks
Shiny is butt easy, beautiful, and super impressive to people who haven't used it before
Use the Twitter API and gather all tweets for particular hashtag or an event (soccer game, rugby, TV show). Then analyse the tweets perhaps for frequency, audience engagement, sentiment analysis. Present the results in shiny. Make your code reusable so that you can easily amend the keywords and date ranges. Next time you want to show off or have an interview, you can dust it down and make it current. 
I think you meant "easy but," but I think butt easy is funny too. 
Ah i just saw that it didn't sent everything i wrote. i need to write this pseudocode into R, and i don't even know how to start! can someone please help me with it? 
Use the `breaks` argument. Let's say you wanted the bins (i.e., x axis values) to range from -20 to 20, and you wanted 50 bins. Then you could add the following to your code: b = seq(from=-20, to=20, length.out=50) hist (dfWine$resid, breaks=b, ...) Here, the `...` just indicates the other arguments you fed to `hist()`. Note that you need to be careful to make sure the limits of your breaks span the data, or you'll get an error saying: `some 'x' not counted; maybe 'breaks' do not span range of 'x'`
thank you so much! I tried many things like converting each vector into a factor before I finally found this. Looks like I just need to improve my proofreading. 
You can also try clearing your workspace of all objects that are generated during the running of your code when looking for errors. I found it by trying to run your example and getting "Error: object 'positions' not found" because I didn't have an old version of it in my workspace.
this is brilliant
No prob. I did something similar for an Irish General Election a few years back. Happy to share the code (http://ge16tweets.com)
Have you tried just restarting R from the drop down? I use that when shit gets whack and I need to nuke things while mostly preserving my working environment 
Make plotting structured equation models more pretty and user friendly than [this](https://drsimonj.svbtle.com/ggsem-plot-sem-models-with-ggplot2) (no critics on the developer but there is some room for improvements)
So it didn‚Äôt work when you first hit the stop and then run stopCluster? There is usually a delay after running stopCluster, it‚Äôll close one process at a time usually and it can take 10+ sec depending on how much memory was allocated. At least this has been my experience.
Well now I see something I've been doing wrong with foreach. You're not the biggest newb here! Just as a side note, mcapply is another great package for parallel computing, and pbmcapply is a nice wrapper for it that gives you an ETA and a progress bar
Nope, hasn't historically! Though if it works for you then it may be that those workers are just executing external library calls and can't be stopped from the console until those finish? I haven't explicitly tested this actually, and usually give up waiting and kill the processes manually due to impatience (I don't look at the source code for most of the functions I use, so they might be written in C/C++/Fortran/Perl/etc.; at most I've waited a minute but that's probably not enough for plenty of functions). Should be easy to test though!
What were you missing? I've used mclapply before but never mcapply or pbmcapply, will check it out!
mclapply is in the mcapply package - I've only used the "l" variant, too, since I'm typically working with vectors and dataframes more than matrices. I didn't realize foreach required you to list the packages you'd be using. Makes sense since it's running your code on multiple instances of R.
From the session tab in the RStudio toolbar? Nope, but occasionally the console process won't stop and I'll begrudgingly confirm my desire to restart R through the popup, and iirc that's never killed any of the other workers. Can try tomorrow though!
ah gotcha -- did you originally mean mclapply, then? I googled mcapply and [this](https://www.rdocumentation.org/packages/mc2d/versions/0.1-17/topics/mcapply) popped up, which was a bit opaque and I didn't dig any deeper into the documentation. I've used mclapply but have often found it more straightforward to parallelize my scripts with foreach loops, since they'd each usually be at least a few hundred loc each and reading in and writing to file large-ish amounts of data (incl. e.g. pictures, graphs, etc.) which seemed trickier to use with mclapply (since I couldn't write the original with lapply, anyway). Sometimes I'd mclapply-erize smaller bits of code but usually the overhead wouldn't give me much speedup.
Let me know how it goes, I haven't tried parallels with R yet but it's in my near future!
Your version works too, but I meant butt easy
[This](https://rpubs.com/FaiHas/197581) might help you start.
Non-Markdown version of the links In this comment: **Link Text:** This **Link URL:** https://rpubs.com/FaiHas/197581 ^(Preventing misleading links on reddit by providing the links behind the markdown. **Why?** u/reallinkbot/comments/8igale/why_do_i_exist/)
The most efficient approach is to install the ncdf4 library and read the .nc file as a brick (from raster package)
Figure out an R code template for making movie narrative charts (e.g. https://xkcd.com/657/ )
As I'm guessing blah$d.avg.ci is a vector with two elements, just do two assignments; one for blah$d.avg.ci[1] and one for blah$d.avg.ci[2]
The "raster" package is comprehensive in its coverage of all things GIS related
ncdf4 was built to deal with netCDF files
Also ```raster::stack()```, which can be far quicker depending on the size of the .nc and your available RAM. 
All things spatial raster related... definitely not all things GIS related. 
Good read, thanks. 
great summary, thank you! i prefer base plot to gg plot for full control of axes and such and to be able to easily comment them out..never knew about or used lattice
why not base?
Hey what you like is what you like. I was just curious what you meant by ‚Äúfull control of axes‚Äù. You can comment out lines in both base plot and ggplot, so not sure what you meant there either.
I do, because I've used it forever. 
So I'm going to add some code for a graph I've made, but I think some of the stuff you were referring to is contained in scale\_x\_continuous\(\). full_df %&gt;% ggplot(aes(x = Month, y = avg_stuff, color = factor(Year))) + geom_point(size = 2) + geom_line(size = 1) + scale_x_continuous(breaks = seq(1, 12, 1), labels = month_labels) + expand_limits(y = c(0.025, 0.035)) + labs(title = "Stuff", x = "Month", y = "Stuff Rate", color = "Year") + theme_Publication() You can change limits inside scale\_x\_continuous\(\), or you can use expand\_limits\(\). The month\_labels variable I used was just the names of the months.
what if you want to move the spacing of the labels? i usually make an empty plot, then add points to the body, axes, labels, and legends separately so i can control the spacing and add special characters \(eg, greek letters\)..is it possible to do this in ggplot?
It obviously changes the list name `a`, which is indexed at 1st, as `foo`.
Try `dplyr::group_by(...)` and `summarise(...)`
https://medium.freecodecamp.org/understanding-by-reference-vs-by-value-d49139beb1c4 https://courses.washington.edu/css342/zander/css332/passby.html https://stackoverflow.com/questions/373419/whats-the-difference-between-passing-by-reference-vs-passing-by-value
To add to this - names(z) isn't an object, it's a function that returns the attributes of object z as another object. I'm on mobile and forget the code for assigning attributes, but it doesn't look the same as object assignment.
If you want to get technical (as the R documentation does) then yes, but I think that's more likely to confuse here. My guess (which could be wrong) is that this is being misunderstood because OP thinks more like how Python treats a list. It's separate from the issue of what names(z) does. In R: &gt; a &lt;- c(1,2,3) &gt; b &lt;- a &gt; b[2] = 5 &gt; a [1] 1 2 3 &gt; b [1] 1 5 3 Whereas in Python: &gt;&gt;&gt; a = [1,2,3] &gt;&gt;&gt; b = a &gt;&gt;&gt; b[1] = 5 &gt;&gt;&gt; a [1, 5, 3] &gt;&gt;&gt; b [1, 5, 3] 
Thanks, this is really helpful! I'm learning R from a mostly clean slate, but this code is actually based on something by someone else on my team who is coming from a C background, so good call! I'm starting to understand how R works between columns, and within rows, the thing that was tripping me up was trying to figure out how to work between rows (if that makes any sense). It sounds like the lag function is what I'm looking for, or group_by / summarise. Thanks so much for your help!
That's... definitely two different behaviors! It sounded more to me like he didn't understand that names were attributes, but I could have misunderstood. Hopefully his answer is somewhere in here lol
&gt; Users on Quora have commented that Base plots are good for exploratory data analysis. The idea is to plot quickly without thinking about neatness. But if you need to create plots for publications, ggplot2 is preferred. The opposite is true. ggplot2 is good for exploratory analysis, but with base R it's easier to make the plot "publication-ready". &gt; Jeef Leek has echoed a similar sentiment that he prefers using Base for exploratory data analysis. Defaults available in ggplot2 can produce great plots with minimal code Misrepresentation of what Jeff Leek said. His opinion is: 1. For exploratory graphs - base is good enough and has lot's of useful default functions (like heatmap). 2. For publication graphs - equal amount of work is required with both ggplot and base. 
I do prefer base plotting. Base R for the win.
so basically because the most valuable asset is the developer time?
Would you please elaborate on these packages? What are they used for and if there are any benchmarks specifically for these?
Sure.
This DPLYR must be a good package to learn. it is also mention in [this reply](https://www.reddit.com/r/Rlanguage/comments/8ison9/any_benchmarks_showing_the_benefits_of_r_over/dyucjeu).
The point of R isn't speed, it's ease-of-use, mod-ability, and portability. It was invented as a high-level interface for Fortran code, by the same organization that invented C, because trying to do complex tasks in Fortran is hot bullshit. Most of the computationally intensive math that R is doing isn't programmed in R - it's just imported by R and you interact with t through a more intuitive package structure. Don't think of R as the guy doing the heavy lifting - think of R as the desk jockey that makes himself useful by knowing all the heavy lifters and how to get around their shit social skills. 
So I geuss there are packages similar to numpy and numba for R?
There's packages for everything in R. Do you already know Python though? There's not a ton of utility in learning R if you know Python, which is a better general-purpose language. R wins at statistics but that's pretty much it. Maybe also graphics?
I do use python/matplotlib/numpy/scipy/sympy on daily basis but I'm not pro. I'm curious about R as I have seen too many job ads mentioning. so I decided to do some research before investing time.
data.table adds a new variable type. Like numpy adds the numpy array. Here's some benchmarks against pandas: https://github.com/Rdatatable/data.table/wiki/Benchmarks-:-Grouping One of the big advantages of data.table is that a lot of the methods are implemented in c, which gives you a good performance boost. 
great. thanks a lot. I wonder why Rers do not replicate the most famous benchmarks above with these nice packages. it could help a lot.
I wouldn't say there's much benefit to learning R if you're great with python. I think any company would be happy with a candidate knowing python while packing knowledge in an R shop. Really, the biggest benefit to knowing R is when your team works in R.
And reliability... you want to use proven solutions and iterate quickly. Compute is cheap, good people that can think abstractly and solve data problems that help management make informed decisions aren't. Additionally, you use good packages like dplyr and ranger R is actually very fast, so in the few instances where you really need speed you just use do it in C/C\+\+, Rccp makes this easy.
You always have to be a little careful with benchmarks like these. There are many ways to write code to do the job, and have they done it the most efficient way for each language? R in particular can have orders of magnitude differences depending on how you do iterative tasks (loops, apply, vectorisation etc). As others have mention R isn‚Äôt really about speed. But, outside of writing your code in a speed orientated way, it‚Äôs relatively trivial to make it much much faster for many computation intensive tasks (ones involving matrix manipulations) by using an optimised linear algebra library like Open BLAS (apart from on Windows). Microsoft have done the job for you if you install Microsoft Open R - but you have to be ok with the different license. For these types of operations, it makes R as fast as C. There‚Äôs also the option of linking to other languages like Python, C, or Fortran from within R for tasks that a new linear algebra library won‚Äôt help. Packages like Rcpp will make this a lot easier. 
I agree to the point, apart from your last sentence. If you‚Äôre heavily steeped in statistics, R still has the wider and deeper availability of packages to save you having to write bespoke code. That‚Äôs a huge benefit. 
Obviously. That‚Äôs why that‚Äôs not the question. The question is HOW!?
I understand they are attributes. I don‚Äôt understand what magic assigns those attributes.
In R code is also data, and it's possible to access the code you're running while executing it. This is extremely advanced if you're just beginning in R. But a basic example is the + operator: 1 + 1 Is interpreted as `+`(1, 1) If you'd like to learn more I suggest checking out *advanced R* by Hadley Wickham. It's available for free online.
TL;DR I The list of packages suggested for high performance computing in R: * DPLYR * Rcpp * data.table
You‚Äôre welcome. 1. R‚Äôs reason for existence is not really speed so it‚Äôs not something commonly done. I‚Äôve seen some, usually in response to claims by other languages - but not bookmarked them - e.g. there was a criticism of the benchmarking code used on the Julia website. Try googling Python, Julia, R speed or something. You can also try stackexchange as there are some comparisons about code in R using base, Rcpp, REigen, RArmadillo etc. I made a comment once as I found out that Microsoft Open R was faster than all - I‚Äôm on my phone now so tricky to find right now sorry. 2. I don‚Äôt actually think you need OpenBLAS for macOS. IIRC Apple‚Äôs Accelerate framework is an optimised library so you just need to tell base R to use that instead. I‚Äôve not done it but I remember googling and there was plenty of examples. It might depend what processor you have as to what is faster. Microsoft Open R uses the Intel MKL (math kernal library) - which is obviously heavily optimised for Intel CPUs and the fastest solution (though not by much over OpenBLAS). They claim it‚Äôs also optimised for AMD, but I‚Äôve not seen comparison vs OpenBLAS for those. 3. Check their website for license information. 4. Again, yes. Loads. Google will help for general tutorials. Plus prefixing any search with ‚ÄúCRAN‚Äù will help you find the package documentation specifically - there will be manuals/vignettes there (but sometimes external tutorials are better). RBloggers is also a good place to look. 
R is a fast war to turn ideas into code. If the solution I come up with in R cannot be as fast as I need, I‚Äôll try to work it out in Java, and maybe wrap that in R so I can get the speed I need. R, nowadays, is so concise and readable and there are so many libraries for the most obscure statistical concepts. It‚Äôs strength is that it has been developed by statisticians, and it‚Äôs weakness is that it‚Äôs been developed by statisticians.
That I know, thanx. I don‚Äôt see how it applies though. Even considering that &lt;- is really a function and writing &lt;-(names(z)[1], ‚Äûfoo‚Äú) doesnt really eplain it
```names&lt;-``` Is actually the function that sets names. ```&lt;-``` is a primitive function and dispatches to ```names&lt;-``` in this special case. So ```names()``` is the accessor for the names attribute and ```names&lt;-``` is a setter. You can look at the help for ```names&lt;-``` for more details.
OoooKkkk.... R can be really weird. so basically names&lt;- translates to do_namesgets. and "names&lt;-"(z,c("foo","bar")) would actually set names on z to foo and bar. So the next enigma is, how is that string actually evaluated. Which is the same as "print"("foo") printing "foo". any clue why??
foreach, doSNOW, FlashR, sparklyr
&gt;If a text is sent by her type="1" or me type="2" &gt;sentiment analysis You could just ask her out :D
See the answer on this SO: https://stackoverflow.com/questions/27546901/how-to-set-attributes-for-a-variable-in-r The generic way to do it is with 'attr' function. 'names(x)' is just shorthand for 'attr(x, "names")'. If you want more magic than that, you'll have to delve into the S3 class system, which is... messy. 
What's more romantic than quantifying affection?
Not sure if this will work for you, but I needed to parse some xml, whose structure I did not know, into a tidy data frame, and this solution helped me: https://gist.github.com/nacnudus/55e601ed7d466b6a22bb36002f23aa64
Unfortunately purrr's `split_by` function seems to be deprecated, but this is still progress, thanks!
&gt;R wins at statistics but that's pretty much it. Maybe also graphics? Is there a version of Shiny for Python? Also, I'd say that the tidyverse method of programming is an improvement for those who consider themselves more statisticians than programmers.
If you're doing data manipulation and analysis in R, Tidyverse packages is where it's at. Don't know what I'd do without the pipe `%&gt;%`. The only thing that makes them annoying in my opinion is NSE. 
Side note for ANOVA in R for anyone new to it: 'aov' is a generic 1 way ANOVA 'anova' is generally for comparing fitted model objects I've seen people have issues with this before, so I thought I'd point it out.
You could provide a link that is not broken and use text posts that can give your question a context?
Double it, round to nearest whole number, then half it. `BMI_rounded &lt;- round(BMI * 2) / 2`
I often use the form `rounded_x &lt;- round(x / t) * t` where `t` is the rounding value I want (e.g., 5 for rounding to nearest 5 or 1/2 for rounding to nearest half). This comes in handy a lot. 
Works in Excel. Also works on paper if you're weird.
&gt; Double it, round to nearest whole number, then half it. &gt; BMI_rounded &lt;- round(BMI * 2) / 2 Thanks for your efforts That is still rounding it to either the nearest whole number or .5 I want it only rounded to the closest .5 decimal place. no whole number rounding. So 152.9363 would round to 152.5 151.0308 would be 151.5
So here's what you do: add .5, round it, subtract .5. 
JSON will store these as a string so you need to convert from string to date. Look up character to date here https://www.statmethods.net/input/dates.html 
If you just need to drop the T and Z you can do this &gt; trimws(gsub('[A-Z]',' ','2018-01-17T10:17:34Z')) [1] "2018-01-17 10:17:34"
I feel like lubridate would handle this as-is, if not it should be easy enough to get rid of the T and Z
This date time format is Javascript ISO time notation for UTC time. \(see: [https://www.w3schools.com/js/js\_date\_formats.asp](https://www.w3schools.com/js/js_date_formats.asp)\) ISO Date Time format separates the date from the time portion with a "T" and signifies UTC \(a.k.a. GMT\) with a "Z" at the end. So all you need to do is use strptime and define the format of your input accordingly. This will create a posix format object. Seems like RFiddle is gone, so I a put a small example on my site: [http://puttingthedanindanger.com/?q=content/r\-fiddle\-testing\-page\-1](http://puttingthedanindanger.com/?q=content/r-fiddle-testing-page-1) just click "Run" and it will open the console. By the way, let me know if that works, I'm interested to see how well that functions. If it doesn't work the code is: `# Setting timestamp` `timestampISO = "2018-01-17T10:17:34Z"` `timestampR = strptime(timestampISO,format = "%Y-%m-%dT%H:%M:%SZ")` `print(timestampR)` `#Now in POSIX format, able to access date objects` `class(timestampR)` `timestampR$mday` `#add 1900 to get proper year` `timestampR$year` `timestampR$year + 1900`
Yes. Lubridate handles it automatically 
I would create a function that does your work for you and then parallelize it. See pastebin link: https://pastebin.com/XQ97FrJ5
 I have used rvest with success in the past. It's a Hadley Wickham package, so if you've used dplyr or some of his other packages, the syntax will be familiar (magritter pipes and all). I never used RSelenium so I can't speak to whether it's better or not, but rvest had everything I needed when it came to controlling a browser.
Depends what you need to do. Rvest is great until you need to interact with javascript and other more dynamic elements. That's where RSelenium comes in strong. 
Right? It's weird cause the entire book up to this point is very clear on everything. This one just jumps in acting like it's already there. I may just skip all the parts pertaining to this. I've just been typing it in and commenting it out in the meantime.
Here's a link to the part OP is talking about. http://r4ds.had.co.nz/pipes.html#introduction-11 Seems like it's just an example, not meant to be run.
Thanks for posting this. Yea, maybe. It's just weird I haven't seen him do that at all in this book. Everything he typed was also ran. But he also doesn't show the output, so maybe you are right!
try "\+ xlim\(\-1, 2\)"
Sadly no, I have an older version of r on another computer so can publish from there, a short term solution...
 Thanks it worked, although now the limites goes from -1 to 2 on the x-axis, and theese are not values that the dummy can take, so looks a bit wierd. ill try to remove those labels. 
Hey, blaesevejr, just a quick heads-up: **wierd** is actually spelled **weird**. You can remember it by **e before i**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Try use="pairwise.complete.obs" in the cor() function. This will let you keep more rows instead of omitting rows with any NAs even if the NA isn't one of the two variables being compared.
Here's a snippet for you, using `tidyverse`: library(tidyverse) # Load the data rpt_data &lt;- read_table2("~/Downloads/tmp.rpt", col_names = c("Variable", "x", "y")) # Simple scatterplot ggplot(rpt_data, aes(x = x, y = y)) + geom_point() # Scattered text ggplot(rpt_data, aes(x = x, y = y, label = Variable)) + geom_text() # Interactive scatterpplot library(ggiraph) { ggplot(rpt_data, aes(x = x, y = y, tooltip = Variable, data_id = Variable)) + geom_point_interactive() } %&gt;% ggiraph(ggobj = .)
What Unix/Linux? Is R installed? If so it's pretty easy. `Data&lt;-reas.table("file.path", header=F, sep=" ") `plot(Data$2,Data$3) Yes, you can use locator. ?locator
Unfortunate - I might reinstall the older version of R to make it work. I'm having an issue with running the googlesheets library that I can only assume is caused by a similar problem.
This is just a bar chart, not a Pareto chart. If you plan to use ggplot2, [here's the cheat sheet](http://www.rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf). You'd want to look under the coordinate systems section for how to flip the chart, etc.
It gives me a Error: unexpected numeric constant in "plot\(Data$2" error. what gives
First of all thank you for your answer,I accidentally linked the wrong picture, im actually trying to get a Pareto chart like this one [https://imgur.com/a/qHBJyXI](https://imgur.com/a/qHBJyXI). If you have any idea how I can do it that will be much appreciated.
Oh man, that looks... not so good. A Pareto chart is expected to look a certain way, so that it conveys the same info regardless of the actual plot contents. This orientation makes the reader have to think an extra step (or more) just to get the same information than if it were oriented as usual. However, you can still use the ggplot2 suggestion: make a Pareto chart as normal, then just add `coord_fip()` to it.
That's because I was being silly $2 and $3 should have been $V2 and $V3. I've corrected it. 
Hrnmm, interesting question. I don't know the answer to that. If you know which row that should have applied to you can do `Data[6,]` Where 6 is the row number. 
Thanks for the help. Maybe one last question. The locator command merely gives me the the coordinates. Is there a way to view the actual column 1 data \(Name, height\) corresponding to a point in the scatter plot by clicking on it? Also, is there a way to draw a diagonal line so that I can view the outliers better?
Just a few things: * Google the syllabi for your degree program. Is a programming language necessary? * Do you have an interest in statistics? Will you learn statistics independently? * R is a language, not a program.
I like it, and it will help let you make some pretty nice charts at the very least. 
Absolutely! Regardless of whether you will do much statistics or not, you'll still have a ton of fun!
I don't havw R access at the moment. But if I remember how locator works you should be able to do something like `Data[locator(1),]`
I think Python would be a better choice at this point. It's a general purpose language, so there are more scenarios in which it might be useful. Another option is matlab/octave. Years ago matlab used to be hugely important. Nowadays, its usage is declining (although it's far from being dead). R with R Studio is a great combo for data analysis (more convenient than python + pandas in my opinion), but I'd save it for later.
Python is better option for generalist engineer usage. 
I'm no expert but you should consider that R is a domain specific language for statistics, machine learning and data mining. As others suggested Python is a better language for general use.
Nitpick: R is a language, not a program. Thinking of it aa a program will lead to you misunderstanding what it can do for you you üòã
Try to learn matlab before you start school. The open source version is called octav. Professionally I use R, but learning matlab now will make school work significantly easier. Once you understand the maths and logic, picking up another language will be easy peasy. But for now, focus on optimizing your grades.
Do you need it? Engineering programs in my experience love Matlab. Try to Google what the courses require. If you want to get ahead of the game, start with required coursework first. I wouldn't worry about picking up some coding when you have tons of tough math classes ahead. Are you any good at linear algebra? Have you taken calculus yet? Maybe start looking into those first. 
&gt; R is a language, not a program. While true, I work with a lot of people who see R and R studio as more of a program they interact with. 
tell them to check the name of the subreddit
Python.
Or you can use this: \+ scale\_x\_continuous \(limits = c \(\-1,2\), breaks = seq\(\-1,2, 1\)\)\) please see this documentation: [http://ggplot.yhathq.com/docs/scale\_x\_continuous.html](http://ggplot.yhathq.com/docs/scale_x_continuous.html) it's been a while since the last time I used ggplot
http://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html
Hell yeah! Welcome to R, you'll love it. R has a pretty cool approach to this: [the Rprofile](https://www.statmethods.net/interface/customizing.html). Essentially, you can edit an .Rprofile script in your home directory, and that file will be [`source`](https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/source)d whenever you start R. It's a pretty common practice, and many use cases involve loading frequently-used libraries and setting global print/format/plotting options. I also like to assign aliases (`len &lt;- length` is a nice convenience) and some helper functions (e.g., a function for my reports that formats p-values to a few decimal places and adds significance-level *'s for co-investigators).
Dude, thank you! Perfect answer, and that is a very cool approach. I‚Äôm sure my coworkers and I can figure out a way to swap out .Rprofiles to create a sort of custom user setup, too - thanks for the jumping off point!
If you ever plan on sharing code with other people it's better to write an actual script and ```source``` it manually at the start of every project. You do not want to have any hidden differences between your workspace and other people's.
One easy way to do this is: install.packages(‚Äúusethis‚Äù) usethis::edit_r_profile() This will open up your r profile file and you can copy and paste your working start up code into it. 
Op wants that function in his or her environment automatically after every startup rather than having to rewrite it every time. 
Not sure why I was downvoted for asking for clarification of the question.
If you wanted to remove R from a PC, would you go to 'uninstall languages' on the control panel? 
The 'base R' way to do this is through row indexes, as in ```df[i-1,]$B``` inside a for loop. But really, do yourself a favor and get the dplyr package. R isn't like Stata, the ecosystem is actually a huge part of the value proposition. I wouldn't use R if all I got was the standard library.
Thanks, I installed dplyr, and it seems to have overwritten the lag\(\) function so that it actually works now? I wonder if i can add you an additional question: Basically i am trying to code : ADL = Previous ADL \+ Current MFV For the first row i need ADL = Current MFV though. `CalcADL &lt;- function(df){` `df$MFM &lt;- ((df$Close - df$Low)-(df$High - df$Close))/(df$High - df$Low)` `df$MFV &lt;- df$MFM * round(df$Volume/1000,digits=0)` `df$ADL[1] &lt;- df$MFV[1] # I want the first row of ADL to = 1st row of MFV` `df$ADL[2:nrow(df$ADL)] &lt;- lag(df$ADL) + df$MFV # ADL_n-1 + MFV_n` `df` `}` That's my code , and it does not work ... Also any other suggestions of improving my code is welcome. 
Maybe something like this. I haven't actually any of this, I'm not sure if it works exactly, but hopefully it can give you a place to start. My advise would be to use dplyr: library(dplyr) library(lubridate) customers &lt;- tbl(my_sql_connection, "Customers") tbl(my_sql_connection, "Purchases")%&gt;% inner_join(customers, by = c('CustomerId')%&gt;% mutate(year = year(as.POSIXct(DateCreated, origin='1970-01-01'))) group_by(customer_id, hour)%&gt;% summarise(customer_count = n_distinct(CustomerId), payment_total = sum(PaymentAmount))%&gt;% filter(hour == '2018' &amp; payment_total &gt; 1) 
Teradata connected directly to R?
i just use sqldf
If you're writing a SQL statement and loading that into R, you can read the sql file and run your analysis. If you have a .sql file then you can use [this](https://stackoverflow.com/questions/44853322/how-to-read-the-contents-of-an-sql-file-into-an-r-script-to-run-a-query?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa). This way your components can be separate and your project more "tidy".
This is a really great point, and 100% something to keep in mind
same error here folks..... I was wondering if on next week should be all fine....
R is the interpreter, it's the thing that takes your code and does work with it. Rstudio is a development environment, it helps you write and manage your code, but it cannot execute it without R.
So will I never be using R? All of my work would be on RStudio?
Rstudio connects to R for you, and you can run your code from it. You can also take an R script and call R directly from your operating system, but that is less common.
Thank you. :\)
Rstudio is simply an integrated development environment (IDE) that you will use to write code that executes within R. It will auto-fill functions for you, provide help, and allow you to more easily debug your code/functions. There are many IDEs out there - VS Code, Spyder, PyCharm, etc. The list is almost endless. Many of them work with R, but Rstudio is so closely integrated with R that the others tend to fall short in comparison.
It makes a vector containing 3 numbers: 1, 2, and 3. No offense, but if you don't know that your time will really be better spent going through a basic R tutorial than asking these questions 1 by 1. The thing I'm explaining, for instance, (copy vs. reference) is a concept that is way down the road from making vectors of number/strings. The vector is pretty much the most basic data type in R, although at first they will just look like scalars (just a single number or a single string), but even then they will actually be vectors with a length of 1. 
OP, just in case you didn't know, the subreddit /r/rstats is quite active.
This isn't a perfect analogy, but think of downloading the 'R' language as subscribing to the internet through AOL. R Studio is like then downloading your favorite browser eg. Chrome. R Studio / Chrome still relies on you having R langauge installed/an internet connection and if you don't have it then R Studio/Chrome is pretty useless. However, R Studio/Chrome provides a much better experience to users than the default R editor/AOL browser, allowing you to get on with your life and work more effectively and more easily. 
Thank you :)
My suggestion, since so many offices have Excel sheets with calculations laying around. Import an Excel sheet into R then perform a few simple calculations on the data and export that data back out. This is something that is relatively simple and there are quite a few libraries that will import/export Excel files for you.
This is a good suggestion. A big chunk of my team's work right now is hunting down people's bullshit Excel stuff that takes them a long time, and using R or Python to automate it out for them.
Pretty much this, this is how I learned.
I have always liked the ggplot graphing system. It is pretty simple to learn and lets you do a lot with graphics for presentations.
Like keynote or power note? Video or Tableau on roids
Are you trying to sample days independently of failure? Or do you need the observed pair of failure and day? If the former, make 2 calls to sample(). If the latter, make a dummy variable like and index and sample the index
&gt; boot.sample = sample(HardDisks$failed &amp; HardDisks$days&lt;365, replace=TRUE) A few problems here. First, what the "&amp;" operator does is it turns two variables into one variable. The first argument to "sample" here will evaluate to a vector that's T if the disk failed in less than 365 days, otherwise F. Sample is not going to keep track of multiple vectors at once in the fashion of "boot.sample=="TRUE" &amp; boot.sample&lt;365". Finally, since the question asks for the proportion of drives that fail within a year, you need to sample from all drives, not just the ones that fail. I would solve this by sampling rows from the data frame: boot.sample = HardDisks[sample(nrow(HardDisks), replace=TRUE), ] bootDist[a]= mean(boot.sample$failed=="TRUE" &amp; boot.sample$days&lt;365)
While batch generation of charts en masse is a really nice thing to be able to do. Ultimately, the power of Shiny and Shinydashboard is that these charts can be generated, browsed, and manipulated all in real-time.
Agreed. It‚Äôs definitely a more advanced solution / topic to explore. 
`lines()` or `points()` will add to the existing graph, but will not create a new window. So you'd need to do plot(x,y1,type="l",col="red") lines(x,y2,col="green") Credits: https://stackoverflow.com/a/2564276/9602484
&gt; set.seed(123) &gt; plot( x = 1:100, &gt; y = rnorm(100)/1:100, &gt; type = "l" &gt; ) Not with your code. If you use something alike set.seed(123) x = 1:100 y = rnorm(100)/x line = qnorm(0.95)/x plot(x,y,type="l",col="red") lines(x,line,col="green") lines(x,-line,col="green") This shows the 95% confidence interval for each calculation of y.
During the function execution, you are creating a *new* environment every time the function executes (the ‚Äústack frame‚Äù). That environment ‚Äúdies‚Äù when the function exits (unless you store it somewhere else!), so all changes to it are transient. That‚Äôs why your approach doesn‚Äôt work. Rather than embedding an environment inside the function, you need to do it the other way round: In fact, every function is associated with an environment in which it is embedded. That‚Äôs the environment where to store the counter. You can access that environment via `parent.env(environment())` (where `environment()`, itself, is the function‚Äôs stack frame): counter = function (fun) { env = parent.env(environment()) env$i = 0 function (...) { env$i = env$i + 1 fun(...) } } And `get_calls`: get_calls = function () { parent.env(environment())$i } This works beautifully, especially when used in a package. But if you define these functions in the global environment, then `parent.env(environment())` is just a very convoluted way of writing `.GlobalEnv`. And that‚Äôs probably not what you want, because now you have a variable `i` in the global environment (easily verified by running `ls()`). A common approach is to create the `counter` and `get_calls` function inside a local environment ‚Äî in fact, that‚Äôs probably what you intended. The problem is that, in your approach, `get_calls` cannot access the local environment of `counter`, since the two are unrelated functions. You need to ensure that they are using *the same* environment. You could, for example, use the `local` function to define a function in a given environment: counter_env = new.env() counter = local( function (fun) { env = counter_env # or: `parent.env(environment())` ‚Äî still works! env$i = 0 function (...) { env$i = env$i + 1 fun(...) } }, envir = counter_env ) get_calls = local( function () i, # implicitly finds `i` in the parent environment! envir = counter_env ) An alternative is to define the functions normally, and then assign a new environment: ‚Ä¶ define the functions and `counter_env`, then: environment(counter) = counter_env environment(get_calls) = counter_env
&gt; It's a pretty common practice, and many use cases involve loading frequently-used libraries and setting global print/format/plotting options. That‚Äôs also a phenomenally bad idea, because it makes code written by you completely unreproducible by others (including you in the future/on another machine). There are strict precautions that need to be taken to keep R clean. It‚Äôs generally a good idea to define very little custom functionality in `.Rprofile` (same goes for loading packages!), and only do it in interactive sessions, to start with.
I think your code looks correct. The issue may be the interpretation for the scatterplot. Not one single hard drive with average temp over 40 had a life above 500 days. This is a negative correlation. It's not a typical negative correlation because it's not a smooth relationship - there is a step function once temperatures go above 40. More widely it's difficult to see what is going on in the range 0-40 because of overplotting of the data. You could try either plotting a random subsample of the data, or doing a [2d density plot](https://www.r-graph-gallery.com/2d-density-plot-with-ggplot2/). Another thing to try would be to fit a non-linear line (e.g. loess) to the relationship, this will show you where the negative correlation arises. You can do this with `stat_smooth()` in ggplot2, or using a [loess model in base R](http://geog.uoregon.edu/bartlein/old_courses/geog414f03/lectures/lec05.htm). Alternatively the `car` package has a scatterplot function which includes both the linear and loess fit lines by default ([see here for an example](http://www.sthda.com/english/wiki/scatter-plots-r-base-graphs)).
Thanks mate! Will try that tomorrow. Gotta go to bed now 
Add ```&amp; !is.na(tdoc$PROBATION)``` into your subscripts. Because ```tdoc$PROBATION == FALSE``` is neither true nor false for missing values, R cannot determine whether they should be in- or outside of the subscript.
What do you mean by the step function? 
If f(x) plotted against x is constant then abruptly switches to a different constant then this would be called a step function probably because it looks like a step in a staircase. For this data I mean that instead of each additional degree of mean temperature decreasing the time to failure (a linear relationship over the range of temperatures) it appears that the time to failure is relatively constant between 0-35 degrees and then a different (lower) constant over 35 degrees. If you plot a smoothed line it might look like a [step function](https://upload.wikimedia.org/wikipedia/commons/d/d9/Dirac_distribution_CDF.svg). This may be an oversimplification of the data though. You'll see better when you plot it.
You are right, I added &amp; !is.na(tdoc$PROBATION) to the end, but it did not work. Do you think I should split these up?
What do you expect the output of your code to be?
Are you downsampling for the training process?
No, the downsampling is purely to increase modeling speed. The model has already been trained its just need to be ran 5000 times with different economic inputs. 
Do you have any sense of the complexity of your models? For instance, if it's roughly quadratic, then, say, taking 10 random samples of 1/10 of the data set and running the model on each ends up being ~10 faster. It gets even better if the complexity is worse than quadratic. The statistical behavior of the resulting estimators that result would need to be worked out. You would want to figure out what proportion preserves the characteristics you are interested in capturing in your model. You would do that empirically - find some statistics that characterize what you want, then see the distribution of those statistics when you take samples from your data set of given sizes. You may wish to do this by stratified sampling - that could, depending on what kind of characteristics you're interested in preserving - provide some guarantees that they are preserved.
You want to find the purest, most complete data which you can learn the most from. After cleaning the data should reduce. Use this pure subset. You can randomly sample it and lose information - but why? Learn to work with the large data. The real answer is **doing things in parallel across distributed systems**. Look into stuff like data.table, and FlashR.. ask your boss if he would like you use spark and look into sparklyr. Don't run from big data. You won't win that race. 
I'm assuming you are processing the model as a function and using apply to run it through your data? This may be an oversimplification but you could try...(example based on 10 subsamples) n_subsamples &lt;- 10 1. Create a random assignment vector (as @_cabron) stated: data$ran_num &lt;- sample(1:n_subsamples, nrow(data), replace = TRUE) 2. Split large data.frame based on random assignment vector: data_list &lt;- split(data, data$ran_num) 3. apply modeling fuction to list of smaller data.frames: result &lt;- lapply(data_list, function(x) model_function(x)) 4. convert results back to data.frame (optional) result &lt;- do.call("rbind", result) There are other performance improvements that you can look into. Not reassigning objects in for loops. Using matrices when you can. Using apply instead of for loops. I'm a little green when it comes to the programming side of R, but I hope I've inspired some insight. 
What do you expect it to plot?
Wouldn‚Äôt the sample() function work like this without needing to create a new column?
You could always do a faux carousel. Just keep the rest of the slide static and update it with a new image. There is also [revealjs](https://github.com/rstudio/revealjs). 
That will reduce the data size, but doesn't address the requirement of maintaining the characteristics of the data\-\-depending on how the rows are ordered, you could be introducing some bias by filtering that way.
Thank you üòâ
In full screen, all of the check boxes after 'SD' are masked by the plots.
tidyquant 
Thanks, what browser are you using? I have CSS scripts in it, but am not familiar with CSS at all. Works okay in Chrome for me but I have had issues with the checkboxes being masked. 
`mlr` is quite the outlier there, with lot's of commits proportionally to a small cadre of contributors. They are really active.
An ordered factor is categorical variable where there is a natural order to the levels in the context of the questions you are investigating e.g. low, medium, high. An unordered factor has levels which can't be put in such an order e.g. colours, types of treatments{Drug A, Drug B, Physiotherapy}. R has a special class (or [data type](https://www.statmethods.net/input/datatypes.html)) for ordered (ordinal) factors. When you make a factor you can add an ordered = T option to specify that it is an ordered factor `a &lt;- factor(1:3, ordered = T)`, or you can use the ordered function which does the same thing: `a &lt;- ordered(1:3)`. You can see whether you have an ordered factor by typing `class(a)` or `class df$a` if your factor is in a dataframe. Ordered factors have a class with two parts `"ordered" "factor"` this means it is class *ordered* inheriting from class *factor*. There is a shortcut to test if something is an ordered factor. This is the `is.ordered` function. `is.ordered(df$a)` will return TRUE if the argument is an ordered factor and FALSE otherwise. You can test all your variables in a dataframe by running is.ordered on them. This could be done all in one go with sapply. `sapply(df, is.ordered)` You can then count with sum: `sum(sapply(df, is.ordered))` What can be confusing is that a factor can be unordered even when the levels have a clear order to them. Unless you set the factor to be ordered, R will make it unordered. Just eyeballing the data for `factor(1:3)`, it might seem like an ordered factor with levels 1, 2 and 3, but unless R has been told it is ordered then the levels may as well be green, grey and orange. 
&gt;`mlr` is quite the outlier there, with lot's of commits proportionally to a small cadre of contributors. They are really active. Umm... No? They have fewer commits/contributor than 'plot.ly', 'data.table', or 'jsonlite'. If anything is an outlier here it would be the two packages with 0 contributors, but that's 10% of the packages listed, so probably not them either.
I checked in Chrome and Firefox earlier but it looks like it's working now. There's a bunch of small design things that need tweaking but it's pretty good first try. For instance, I would suggest you put the check boxes in alphabetical order by state name rather than by abbreviation. 
[Shiny](https://shiny.rstudio.com/)
hmm. I was more concerned with the alignment of the checkboxes but I've been having issues with it. I also got a select all button working but it would move the checkboxes all over the screen after it was clicked..
[Here's](https://stackoverflow.com/questions/29738975/how-to-align-a-group-of-checkboxgroupinput-in-r-shiny) an answer from StackOverflow about aligning the boxes.
Thank you. This is a lot more information than anyone in the forums of the class could offer and I appreciate it.
Because I love avoiding loops. Just make sure you have \`\`data.table installed. x \&lt;\- c\(252, 455, 1364, 68, 2125, 2400, \-1514, 252, 2775, 2064, 575, 242, 2376 , \-390, 2847, 976, \-726, 2099, 233, 270, \-1346, 1584, \-1961, 496, 1516, 2233\) mat \&lt;\- matrix\(x, nrow = 2\) SummarizeStreaks \&lt;\- function\(x\) { streaksSplit \&lt;\- unname\(split\(x, data.table::rleid\(x \&gt;= 0\)\)\) lapply\(streaksSplit, function\(streak\) list\(N = length\(streak\), sum = sum\(streak\)\)\) } apply\(mat, MARGIN = 1, SummarizeStreaks\)
Hi all Statistics Lovers :) _on a udacity project , I completed an EDA(Exploratory Data Analysis) on Youtube Trending Videos_ You can view the full project on Kaggle : https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda ___ ##Main findings: I found some interesting facts about Youtube trending videos; these are:- 84% or more trending videos are using one of its tag on the video title for at least once. Other than 604 trending videos,all trending videos are appeared in the trending list for more than 1 once. Maximum number of Youtube videos are listed on trending, within 0 to 14 days of the video publishing date. More users engaged in conversation when they were disliking a trending video rather than liking a trending video. If difference between first trending date &amp; publish date is less than 4 days,then there is a big chance,that video would not be re-trended for more than 3 times. There is a impact on Youtube trending videos views count over tag_appeared_in_title or not. Trending videos those have listed for more than 5 times got the highest number of views. Videos belongs to categories where number of subscriber is/are most ;those videos are using at least one of its tag on the trending video title. ##Surprise! findings: Many of Youube trending videos get listed on trending list for more than 1 time(or day), but they did not get higher number of traffics. Another point I already discussed,many of the trending videos have lower number of subscriber(some of them have 0) &amp; yet they managed to get greater number of viewers than top subscriber channels present in the Youtube. Also I saw there are many trending videos managed to get higher number of views counts,but they have very few likes(many of them have 0). ___ Don't forget to visit below Kaggle Kernel #URL: https://www.kaggle.com/sgonkaggle/exploring-youtube-trending-videos-insights-eda 
Disclaimer: I don't think that's a good idea, but here you go: df_list = c("A", "B", "C") for(i in df_list){ df_i = mutate(get(i), FUN) } 
In your code sample you're quoting "A", "B" and "C", so even if A, B, and C are dataframes in the proper environment, R is going to interpret them as characters. Or is that not what you mean?
If you're trying to apply the same function to several data frames, then this is a job for `lapply()`: ``` df_list &lt;- list(A, B, C) # not c(), and no quotes df_list_lapply(df_list, function(x) mutate(x, FUN)) ``` Though, you're better off not calling `mutate()` like that, and making it part of whatever `FUN` will be. I agree with /u/soft-error that this doesn't look/feel right, what you might be trying to do.
what does get() do? And why is this a bad idea?
Thanks. Mutate may not be the correct code here, but I'm just trying to learn a more general R programming principle. Essentially, for various tasks (not just this one), I always run into this looping problem where R treats the list as characters with quotes. In State however, I can just write -foreach x in A B C- and Stata will literally input A, B, and C as if I had typed it into the command line. How do i have R just literally put in words without quotations, and to treat it as if I had just written it in the command line? Whether it be looping through data frames or variable names or anything else. This quotation issue is driving me crazy
I really recommend digging into *apply family and spending time in wrapping your head around it - you and your code will benefit greatly. How to do the task without lapply you have been already shown. Another thing - if you want to learn what a function in R does just prepend the function with ? ?get Will give you a description about the get function. Function descriptions in R are reasonably well written and a good source of information on how things work. 
In most languages, if you put quotes around a word, you are saying 'this is a string'. So `"A"` is a string with the character A in it while just `A` is the variable. To understand what's going on, you should know the difference between a `list` and a `vector` in R. A `vector` can hold multiple values all of the same, simple type. `c("A", "B", "C")` creates a vector where each thing in the vector is a string. If you want to encapsulate more complicated types, like a dataframe, inside the same object, then you need a `list`. So if you do my_dataframes &lt;- list(A, B, C) Then you can access the dataframes like this: my_dataframes[[1]] # This gives you dataframe A Note the use of the double brackets to retrieve the item in the list. You can also iterate over the items in a list using a for loop. 
 A = data.frame(a = c(1, 2, 3), b = c("x", "y", "z")) B = data.frame(c = c(10, 11, 12), d = c("l", "m", "n")) df_vector = c(A, B) for (i in 1:length(df_vector)) { df_vector[i] = mutate(df_vector[i], FUN) }
get() get's the variable that has the name of the string it is given. It is bad practice, because you are only weakly coupling to the variables you want to get. On the other hand, if you use/pass a round a list that contains the dataframes, you know where something can happen to the elements in the list (only at the spots where the list or its elements are reassigned!) what you want to do is actually put the dataframes A,B and C in a list (and possibly name the list elements that way) and then access them via the list
If I loop through a list, itll be read not as characters? 
What does a single bracket retrieve?
A single bracket is used to subset the list. So if you have a list with 5 items, then: my_list[3:5] Gives you a new list with items 3, 4, and 5. This also applies for a single index and so: my_list[3] Will actually return a **list** with only a single item in it (item 3). It's kind of odd, but I believe the reasoning in R is that all numbers are really vectors of length one, and so the `3` in the index above is actually the same thing you would get with an expression like `c(3)`. 
it depends on how you create the list. list_of_names &lt;- list("A") A &lt;- data.frame(...) list_of_df_objects &lt;- list(A) 
If your data frames are A, B, and C, why are you making your list as "A", "B", and "C"?
So it willput in A
What if I want to loop through a name? For instance, let's say I have df_A, df_B, and df_C. In State, I would use &gt; foreach x in A B C{ &gt; gen new_`x' = df_`x' + 3 &gt;} What would I do in R? For instance, suppose the new loop is &gt; for (i in A B C){ &gt; df_i &lt;- mutate(df_i, FUN) &gt;} That code won't work because it will just treat i as the letter i. 
This doesn't seem to work. 
You told it to assign all the variables to ‚Äúdf_i‚Äù ‚Äúdf_i‚Äù is one string, and completely unrelated to i in the language. You need: assign(paste0(‚Äúdf_‚Äù, i), mutate(i, FUN))
That's annoying. I mean it's simple here, but sometimes I will have a large loop with multiple things I want to refer to the same macro word. 
Stop assigning them to different variables then
Please post a line of code that doesn't execute for you
First of all, you're using underscores in a very confusing way. Underscores are not used for indexing in R. df_i isn't the i^th df, it is its own thing. And it won't change with the loop because the only thing that's changing is just plain `i`. My apologies if you weren't using the underscores literally. Anyway, if you'd like to index by name, here's a way you might do it: A = data.frame(a = 1:4, b = c(TRUE, FALSE, TRUE, FALSE)) B = data.frame(a = 5:8, b = c(FALSE, TRUE, TRUE, TRUE)) C = data.frame(a = 9:12, b = c(TRUE, FALSE, FALSE, TRUE)) df_list = list("A" = A, "B" = B, "C" = C) for (n in names(df_list)) { df_list[[n]] = mutate(df_list[[n]], d = !b) } (I actually took the time to run this, so I know for sure it works.) There's a couple things going on here: * You're trying to change an object in the for loop. If you just wanted to, say, print something out or create new objects, then you really could just easily do something like this: df_list = list(A, B, C) df_list2 = list() for (df in df_list) { df_list2[[length(df_list2) + 1]] = mutate(df, !b) } * What's going on here is that if you just use `for (df in df_list)` then at each step `df` is a copy of that element of `df_list`. Modifying `df` in a `df$e = 0` kind of statement will just be thrown away at the end of each iteration. It doesn't error or anything, it just doesn't change anything that's kept around. * So if you want to change what you're iterating over, you simultaneously need a copy of that thing *and* a way to reference it directly. Hence me using a numeric index in my first example or a named list in this comment. Does that help?
I'm not sure how things work in Stata, but if you have a large loop you don't actually want to use that loop to create a few thousand new separate dataframes with names like `df_1`, `df_2`, `df_3`. Much better to put everything in a list. Then you can use strings to name each one if you want, but much better to have them all in one place.
What's the shell interpreting the command? Is it Windows cmd, Powershell? Or is it bash, et al? I'm speaking off the cuff here, but the shell should be interpreting the path before it's passed to `Rscript`. I'd first try what /u/usb_mouse suggested, which is to escape the space based on how your shell needs to have spaces escaped. If it's a UNIX-alike shell, wrap the path in single quotes instead, so it's interpreted literally (not sure if that works the same for Windows shells).
Some more information would be useful, how did you install keras? Do you have conda installed? What operating system are you using?
Windows 10 64bit, i installed it through development tools and the command in R
Keras uses tensorflow as a backend. tensorflow "in R" is just a wrapper around the python library.
Not too clued up on Windows, so maybe someone else can help, but it looks like you need to install Anaconda first. What does this return? library(reticulate) conda_version() 
You can reinstall it and uncheck the 32 bit files 
now im having this issue, and im running conda version 2.6.1. I'm trying to install tensorflow using: install_tensorflow(version="gpu") and i get the following error: Error: Error 2 occurred creating conda environment r-tensorflow In addition: Warning message: running command '"C:\Users\Admin\ANACON~1\Scripts\conda.exe" "create" "--yes" "--name" "r-tensorflow" "python=3.6"' had status 2 
im having trouble installing tensorflow in R for some reason, the GPU version. im running conda version 2.6.1. I'm trying to install tensorflow using: install_tensorflow(version="gpu") and i get the following error: Error: Error 2 occurred creating conda environment r-tensorflow In addition: Warning message: running command '"C:\Users\Admin\ANACON~1\Scripts\conda.exe" "create" "--yes" "--name" "r-tensorflow" "python=3.6"' had status 2
Ahh I didn't realize this. Learned something new today 
&gt; You could always do a faux carousel. Just keep the rest of the slide static and update it with a new image. I think I possibly won't get away with this, I'm supposed to have a maximum of 5 slides. &gt; There is also revealjs. Do you know of any examples?
Ah. That's interesting, I could possibly use that. But can it be made to cycle through images rather than generate plots? I've already generated plots and stored them as .png files so it'd be great (i.e. much easier) if I could use them.
I would have liked to see something showing the relationship between the exponential distribution and the poisson distribution. Given the same rate parameter/lambda, Poisson can be used to simulate how many arrivals will occur in a given time. The exponential will tell you when the next one will arrive. Given that, you can sum up "next arrival" times to approximate drawing from the Poisson distribution. arrivals_per_hour &lt;- 2.5 # average # arrivals per hour # how many show up in each of 8 hours of the day: rpois(n = 8, lambda = arrivals_per_hour) [1] 2 3 8 2 3 0 3 1 # when will the next arrival happen, given the same rate rexp(n = 1, rate = arrivals_per_hour) [1] 1.025447 # you can estimate rpois using rexp: get_num_events &lt;- function(lambda) { # keep adding the next arrival until the next one would exceed 1 t &lt;- 0 n &lt;- 0 per &lt;- 1 while (t &lt;= per) { next_arrival &lt;- rexp(n = 1, rate = lambda) t &lt;- t + next_arrival if (t &lt;= per) { n &lt;- n + 1} } return(n) } # how many show up in each of 8 hours of the day (will be different but similar to above) n = 8; replicate(n = n, expr = get_num_events(arrivals_per_hour)) [1] 1 1 2 1 0 1 7 4 # we can evaluate this approximation using monte carlo simulation... # if we do it "a bunch" of times, the mean number of arrivals should be close to 2.5 # 100 runs n = 100; sum(replicate(n = n, expr = get_num_events(arrivals_per_hour))) / n [1] 2.42 # 1000 runs n = 1000; sum(replicate(n = n, expr = get_num_events(arrivals_per_hour))) / n [1] 2.507 # 10,000 runs n = 10000; sum(replicate(n = n, expr = get_num_events(arrivals_per_hour))) / n [1] 2.502 # 100,000 runs n = 100000; sum(replicate(n = n, expr = get_num_events(arrivals_per_hour))) / n [1] 2.50185
see my comment.
look into forcats for factors - don't rely on as.factor https://blog.rstudio.com/2016/08/31/forcats-0-1-0/
Maybe this webinar will prove to be useful? https://www.rstudio.com/resources/webinars/thinking-inside-the-box-you-can-do-that-inside-a-data-frame/
You could try rewriting the page by injecting new JavaScript to change the behavior of the page.
That's why it's data[[colname]]. 
That's why it's data [[colname]]
Modified it a little bit: % books %&gt;% inner_join(books,'isbn') %&gt;% filter(year == '2015') %&gt;% group_by(publisher) %&gt;% summarize(sales2 = sum(sales))%&gt;% ungroup() %&gt;% arrange(desc(sales)) %&gt;% top_n(10, publisher) % Managed to make it too % books %&gt;% inner_join(sales) %&gt;% group_by(publisher) %&gt;% filter(year== '2015') %&gt;% top_n(10, publisher) %&gt;% summarise(sales2= sum(sales)) %&gt;% arrange(desc(sales2)) My problem is that, I need a column that counts how many times the books had been published.
jesus christ, it was that simple... i made it books %&gt;% inner_join(sales) %&gt;% group_by(publisher) %&gt;% filter(year == '2015') %&gt;% top_n(10, publisher) %&gt;% summarise(sales2= sum(sales), sales3= n()) %&gt;% arrange(desc(sales2)) Thank you for helping :). Not so many people around here willing to help :'(
You do want to avoid for loops in R if you can. If you go with this route, import and convert to data.table and loop through. With the data frame - Op wants to specificy a subset object of columns. Specificty an object with level values. Specifiy and object with labels lapply factor to the subset and pass in the levels and maybe factor recode with forcats and specify the labels. ANother option is to just use a the subset you want to recode and use 'mutate_each' from dplyr and specify facor and levels. I am really not sure of op's, but if discretizing he wants to do then apply an unsupervised discretize function to the data and make sure you look at the default parameter values and ensure its correct (breaks = 3) I believe.
You're welcome! I learned something too from your code: joins apparently don't have to explicitly state the by parameter if there are matching columns on both sides.
Thanks for everyone who responded. I ended up solving it. Here is the R code inner_join(Customer, Purchases, by = c('CustomerID')) %&gt;% filter(year(DateCreated.x) == 2018) %&gt;% group_by(CustomerID, year(DateCreated.x)) %&gt;% summarise(total = sum(PaymentAmount)) %&gt;% filter(total &gt; 1) %&gt;% View() 
I solved it, this was a good starting point, thanks!
+1 for data.table or parallel processing, though just switching to data.table usually fixes all my ram issues. I almost exclusively use data.table now. That and just vectorizing my code in general.
I do
Agreed. Every graph I‚Äôve used for publications had been in base, never ggplot. Every publisher tends to have specific requirements regarding graphs that is just to hard to meet with ggplot. Ggplot was meant to be a convenient way to graph, not necessarily a precise way.
Do you have a standing presentation that's given on a regular cadence? If it's graph-heavy, automate the presentation generation and deliver it as a PDF (see Beamer for LaTeX). That's full-credit. Partial credit can be given for just automating the graph generation for Power Point. Another thing that might be a good place to start--simulating some of your business planning (e.g. sales trend analysis or budget planning) or business process changes (I just found [simmer](https://r-simmer.org/) this weekend and it's brilliant with their vignettes being the clearest problem examples I think I've ever seen. 
&gt; Not so many people around here willing to help :'( More like not so many *able* to. You're asking for help from people who know both SQL an the less commonly used `%&gt;%` syntax in R. I've done a lot of R and a lot of SQL and couldn't help you with this without investing a lot of "figuring it out" time. Especially with no example data provided.
Nah, it.s okay. It was my last assignment for this semester. It was something introductory to R language. Not gonna hit with it until next 2 years. ( That if i continue with the master here- highly unlikely) Thanks anyway :)
This is great! Thank you! 
Is conda version 2.6.1 a typo? I think you have to have at least Anaconda 3. Other than that it sounds like an issue with the package and its absolutely insane way of managing dependencies. Might be worth posting an issue on github or the RStudio forums.
So I get this when I try to install keras again: &gt; library(keras) &gt; install_keras() Creating r-tensorflow conda environment for TensorFlow installation... Usage: conda [options] [INPUTFILE] (STDIN is assumed if no INPUTFILE is given) conda: error: no such option: --yes Error: Error 2 occurred creating conda environment r-tensorflow In addition: Warning message: running command '"C:\Users\Admin\ANACON~1\Scripts\conda.exe" "create" "--yes" "--name" "r-tensorflow" "python=3.6"' had status 2 
you could replace your `==` with an `%in%` and your singluar `'bad_value01'` with a vector of bad values c('bad_value01', 'bad_value02', etc). Then it would replace any bad value with good_value01. That would look something like bad_values &lt;- c(bad_value01, bad_value02) levels(data_frame$column) [levels(data_frame$column) %in% bad_values] &lt;- 'good_value01' 
The LOCATION column has an address, followed by a new line and the lat/long surrounded by parenthesis. To keep the regex simple, I used str_replace() to remove the address and the parentheses and then separated the column by the comma. df &lt;- structure(list(ROWNUM = c(1, 2, 3, 4, 5, 6), CASEID = c(1099487L, 1117507L, 985415L, 986019L, 996883L, 967855L), CRIMEID = c(1321797L, 1344185L, 1181882L, 1182632L, 1195867L, 1160270L), CRNO = c("0910020373.1", "0911060289.1", "0902190512.1", "0902200294.1", "0903170149.1", "0901080218.1"), CATEGORY = c("MISCELLANEOUS", "MISCELLANEOUS", "MISCELLANEOUS", "MISCELLANEOUS", "LARCENY", "LARCENY"), OFFENSEDESCRIPTION = c("MISCELLANEOUS - GENERAL NON-CRIMINAL", "MISCELLANEOUS - GENERAL NON-CRIMINAL", "MISCELLANEOUS - ABANDONED VEHICLE", "MISCELLANEOUS - GENERAL NON-CRIMINAL", "LARCENY - FROM BUILDING (INCLUDES LIBRARY, OFFICE USED BY PUBLIC, ETC)", "LARCENY - FROM BUILDING (INCLUDES LIBRARY, OFFICE USED BY PUBLIC, ETC)" ), STATEOFFENSEFILECLASS = c(99009L, 99009L, 99009L, 99009L, 23003L, 23003L), INCIDENTDATE = c("01/01/2009 12:00:00 AM", "01/01/2009 12:00:00 AM", "01/01/2009 12:00:00 AM", "01/01/2009 12:00:00 AM", "01/01/2009 12:00:00 AM", "01/01/2009 12:00:00 AM"), HOUR = c(0L, 0L, 0L, 0L, 0L, 0L), SCA = c(1107, NA, 1005, 414, 908, 312), PRECINCT = c(11L, NA, 10L, 4L, 9L, 3L), COUNCIL = c("City Council District 3", NA, "City Council District 5", "City Council District 6", "City Council District 3", "City Council District 6"), NEIGHBORHOOD = c("CONANT GARDENS", NA, "PECK", "HUBBARD-RICHARD", "BURBANK", "NECKLACE DISTRICT" ), CENSUSTRACT = c(5070L, 9999999L, 5313L, 5211L, 5052L, 5172L), LOCATION = c("18000 WEXFORD\n(42.4261, -83.0649)", "00 UNKNOWN\n(999999, 999999.0001)", "02000 CALVERT\n(42.3821, -83.1058)", "00 W GRAND BLVD AND W FORT\n(42.3145, -83.083)", "12500 CONNER\n(42.4134, -83.008)", "01500 WOODWARD\n(42.3358, -83.0494)")), .Names = c("ROWNUM", "CASEID", "CRIMEID", "CRNO", "CATEGORY", "OFFENSEDESCRIPTION", "STATEOFFENSEFILECLASS", "INCIDENTDATE", "HOUR", "SCA", "PRECINCT", "COUNCIL", "NEIGHBORHOOD", "CENSUSTRACT", "LOCATION"), row.names = c(NA, -6L), class = c("tbl_df", "tbl", "data.frame")) df %&gt;% mutate(LOCATION = stringr::str_replace(LOCATION, "^.*\\n", ""), LOCATION = stringr::str_replace_all(LOCATION, "\\(|\\)", "") ) %&gt;% separate(LOCATION, into = c('block','latlong'), sep = ", " )
dplyr::case_when() is a fantastic succinct way of converting lots of values, especially when in combination with %in% # Make a sample dataframe df &lt;- data_frame(factor_column = c(paste0("bad_value", 1:7), paste0("good_value", 1:13)) ) %&gt;% sample_frac(1) # A few ways of matching multiple bad values and convert into the value you want to replace it with df %&gt;% mutate(factor_column = case_when(factor_column %in% c("bad_value1", "bad_value2") ~ "good_value14", factor_column %in% paste0("bad_value", 3:6) ~ "good_value15", factor_column == "bad_value7" ~ "good_value16", T ~ factor_column ) )
THANK YOU!!!! I had to make one small adjustment that really worked out for what I needed. For whatever reason lat and long came in 2 separate columns. But that really worked out for the better! Thank you again.
Calling "dput\(\)" on your dataframe and pasting it here would be helpful for us to give an answer. Are you simply making better labels for the factors? You can use Hadley's forcats package. set.seed\(9000\) my\_data \&lt;\- data.frame\(col1 = sample\(letters\[1:5\], 5\), col2 = runif\(5\)\) col1 col2 1 e 0.5547323 2 b 0.2610336 3 c 0.1125777 4 a 0.1562947 5 d 0.5060802 ## Recode certain levels my\_data$col1 \&lt;\- fct\_recode\(my\_data$col1, good\_value1 = "a", good\_value2 = "b"\) col1 col2 e 0.5547323 good\_value2 0.2610336 c 0.1125777 good\_value1 0.1562947 d 0.5060802 If you want to collapse all the stinky levels into just one grab\-all level, use fct\_other ## keep levels a and b, rename all other levels as "other" my\_data$col1 \&lt;\- fct\_other\(my\_data$col1, keep = c\("a", "b"\), other\_level = "Other"\) col1 col2 Other 0.3210491 b 0.4197048 Other 0.7845661 Other 0.3371175 a 0.3071949
It's not a problem. You just get one negative coefficient and one positive. 
If I understand what you want correctly, using tidyverse tools: library(tidyverse) team.assignment &lt;- mutate(team.assignment, start.date = as.Date(start.date), end.date = as.Date(end.date), dates = map2(start.date, end.date, seq.Date, by = "days")) %&gt;% unnest(dates) By the way, there are a couple of typos in your code (missing bracket for the name vector, and "assignement" in the `all.dates()` function.
That is nice. Thanks for demonstrating unnest and map2!
use plot() with xlim=xrange and ylim=yrange. The line is being created by `plot(xrange, yrange)`.
Thanks, now I see how it is being created and I have changed the code. It works perfect now.
Thanks again, however I wasn't quite sure how to use the functions you provided. Where do I plug in what column I need to check and the name of the dataset itself?
I love this solution! It works on my work computer but not on my laptop. I get the following error associated with the unnest: Error in UseMethod("unnest_") : no applicable method for 'unnest_' applied to an object of class "list" any idea what's going on? Same version of R and all packages are updated 
This looks like a point/line size + output size issue to me. Make your plot window smaller and your output will better match the example, then up the sizes. That should get you the ‚Äúoverlap‚Äù that you seek. Notice how the font sizes are larger on the right too. 
Oh wow, yeah I was confused as in the examples the points changed size but the code didn't show anything new, thanks. I'm just glad I actually did it right haha.
FWIW, you can use geom_jitter() to prevent point data from overlapping. It incorporates random noise to your points so they no longer overlap. I don‚Äôt think the r4ds example is using it in this case, but it can sometimes help illustrate the density distribution of your point cloud when plotting integers instead of numeric values. 
I'm not sure if *r4ds* introduced it yet, but there are also options to control, among other features, the size of points. For example, you can set `size=0.5` (or since you wanted larger points and line, you would set the size to be larger values). I'd suggest that this type of manipulation is a better strategy than resizing the plot window. ggplot() + geom_point(data = mpg, mapping = aes(x = displ, y = hwy), show.legend = FALSE, size=0.5) + geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy, group = drv), se = FALSE, size=0.5) Also, note that your plot is not creating *quite* the same figure as the exercise requests. Look at the relationship between the points and the lines. Hint if you need it: &gt;! Which is "on top"? !&lt;
Thanks I‚Äôll look into and yeah sizes were introduced but it didn‚Äôt look the same when I tried changing, I think changing the window size made it look more like the example. But I‚Äôll look into what you said 
I'm not sure if you already know this but there is a solution set to the R4ds available on the web, you can Google for the url.
I think you want to try something of the format Y = mX + E Where X is drawn from a random normal distribution. The E is an error term which should be drawn from another random normal distribution. The magnitude of error will tune the correlation between X and Y. m is a scaling factor (will play into average x,y as well as SD since SD has units of x)
``` #cov(t(M)) # [,1] [,2] [,3] #[1,] 10 6 0 #[2,] 6 8 -8 #[3,] 0 -8 32 ```
The "correct" output is a 3x3 matrix, i.e. the covariance between *rows*. The output from `cov` is a 4x4 matrix, which appears to be the covariance between the *columns*. And yes, the is *exactly* what the help page for `cov` states. I leave it to yourself to figure out how to get the covariance between the rows with `cov`.
 #cov(t(M)) # [,1] [,2] [,3] #[1,] 10 6 0 #[2,] 6 8 -8 #[3,] 0 -8 32 
What do you mean with "correct" here?
What does column-major and row-major order mean?
The one in the book.
But why doesn't R give the correct one?
It does. Why is the column covariance more correct than the row covariance?
Well I don't know really, that's why I'm asking.
Whether you want the column or row covariances depends on the context. The rows or columns are usually pairs (or tuples) of data in a system of equations, hence which direction of covariances you need depends on what you are doing. (But I would say column-wise is more useful due to the system of equations point.) Have you noticed that both covariance matrices are symmetric?
Ok so that would in some sense be how the variables relate to eachother? Yes I can see that they are symmetric. I guess it is because if a relates to b the b relates to a or something like that?
Yes, the covariance tells us how much two random variables change with each other. So the 3x3 covariance matrix shows us how much the three measurements - across the four samples - changes with each other.
Ok, thanks!
The book expects you to operate over the rows (row-major ordering). R is a column major language, so it operates over the columns expecting the variables to be stored this way (and the samples to be stored in the rows). It's just convention though, easily circumvented (as I showed).
It is conventional for rows to refer to units, and columns to refer to variables. If you had 1000 observations then you would have a 1000x3 data matrix, which you would describe with a 3x3 covariance matrix. 
It is conventional for rows to refer to units, and columns to refer to variables. If you had 1000 observations then you would have a 1000x3 data matrix, which you would describe with a 3x3 covariance matrix. 
As others have stated, it is a matter of row-major vs. column-major order. You can see that when each variable is defined differently, i.e. in rows vs columns, we obtain the two different tables that you provided. Example of row-major order: #Load libraries library(corpcor) library(tseries) #Array of var var1=c(1,4,7,8) var2=c(2,2,8,4) var3=c(1,13,1,5) combinedvar &lt;- data.frame(var1, var2, var3) #Range Names range.names = c("var1", "var2", "var3") names(combinedvar) = range.names covmatrix = matrix(c(cov(combinedvar)), nrow=3, ncol=3) dimnames(covmatrix) = list(range.names, range.names) #Covariance matrix covmatrix &gt; covmatrix var1 var2 var3 var1 10 6 0 var2 6 8 -8 var3 0 -8 32 Example of column-major order: #Load libraries library(corpcor) library(tseries) #Array of var var1 = c(1,2,1) var2 = c(4,2,13) var3 = c(7,8,1) var4 = c(8,4,5) combinedvar &lt;- data.frame(var1, var2, var3, var4) #Range Names range.names = c("var1", "var2", "var3", "var4") names(combinedvar) = range.names covmatrix = matrix(c(cov(combinedvar)), nrow=4, ncol=4) dimnames(covmatrix) = list(range.names, range.names) #Covariance matrix covmatrix &gt; covmatrix var1 var2 var3 var4 var1 0.3333333 -2.166667 1.333333 -0.8333333 var2 -2.1666667 34.333333 -22.166667 -1.3333333 var3 1.3333333 -22.166667 14.333333 1.1666667 var4 -0.8333333 -1.333333 1.166667 4.3333333
This solution wouldn't convert from PDFs as such, but you may find it useful. It might be worth checking out a library called **finreportr**. Basically, you can download financial statements \(balance sheet, income statement, cash flow statement\) into R, and then convert it into a data frame for analysis. I've used it in conjunction with dplyr to extract information and calculate financial ratios. It might be easier than having to convert from PDFs directly and have to do significant editing of your data before being able to run the analysis. Hope this helps.
That sounds real cool! Unfortunately, due to work nature, my solution will require manipulation of offline/local data. 
www.docparser.com is what I use
I am not entirely sure what you want from this thread. Is it the solution? What were your attempts to solve this? What do you mean, if you are talking about multiplication of vectors? Matrix multiplication? The dot product? Or a normal multiplication by the scalar that is `C`? Do you want one line of code? Do you want multiple? Do you know the answer?
He is looking for element-wise multiplcation - he is using vectors. But yeah I agree. He wants us to do his homework. Very straightforward homework at that which shows he really needs to sit down and work through it. 
No it's a task for me to solve I kind of know how to do it but, not sure about the answer? Can you calculate the product of those elements like written there? 
I'll give you a hint - it is way easier than you think due to the vectorized nature of R. Its like a few tiny, tiny lines of code.
Can you calculate the asnwer? I want to compare with what I got
If I were on my pc I would. Just follow it step by step and you will get it first time (maybe second if you accidentally take the indexes themselves instead of the index values). D = the concat of A and (B * C) Select values from D at given indices and multiply them together. Two steps which can be made into three. 
Then look at [this answer](https://www.reddit.com/r/Rlanguage/comments/8ogtzs/can_you_solve_this/e037ssq/) that sould give you a good idea of what to do. As said, you can do this in 1-3 steps, as you please. You could also challenge yourself to get from the 3-Line-Solution to the one line of code.
OK thanks 
Unterstood, thanks for response.
/r/homeworkhelp
Let me be a little more helpful: How do you do define a vector alike A, B and C? What is the symbol of multiplication in R? Is there an easy way to a vector to another vector? How do you select indices from a variable? If you are able to answer those questions, the assignment should be no problem for you ;)
Regardless of your solution, cleaning the data up is going to be a nightmare. I've used Python's pdfminer before to read in strings (as well as C++'s libpodofo, but I really don't recommend that), and it takes a long time. If you can, I recommend parsing the string unless someone comes up with a particularly useful library. Even then, the way PDFs are generated can be unique to the program that made them, so their underlying structure is different (unless you convert to image and use OCR or something like that).
You might try adobe's export to excel feature? there are tutorials online for how to apply that in a batch mode if you have a bunch of files. Once the files are exported, you will have to subset the rows which correspond to your desired tables, but that's a pretty straightforward task in R.
You can use a combination of expand.grid(), sample(), and seq() to create sample datasets. I recommend creating a large sample dataset and then training models on smaller sunsets of it. Even better, use an existing real dataset. ;)
Yes because merge is like joins in dplyr. it uses the 'by = ' argument to specify a key to join dataframes by (dont use merge at all, if you want to join use dplyr). What you're looking for is full_join( ) from dplyr.
1) dplyr is the best package ever created 2) Depending on what you want (and the format of your DFs), you can also use function like *bind_rows* or *bind_columns* to "force" dataframes together as well.
He isnt looking to bind though, he is trying to join by a key (ID)
Oh, I agree. I was posting as more of a PSA. There have been plenty of times where I have DF A and B, that both contain two halves of a whole picture, and I just need to glue them together; I spent longer than I care to admit googling around for "JUST STICK THSE THINGS TOGETHER AND GIVE ME NA IF NEEDED."
Not done in R but if it‚Äôs not a question or requirement of practicing R then mockaroo is a great dataset generator that I‚Äôve found. May or may not be appropriate for your application but it‚Äôs pretty flexible. They offer a lot of different options. Great to know about either way. 
Thank you very much! Very helpful.
No problem. Work through that book it is so worth it. 
Thanks! But how do I create datasets which has classes that I in the end can predict on?
Thanks! But how do I create classes that are dependent on some variables? I mean if I want glmnet to work then I need it to be able to predict correct classes using a subset of variables.
library(&lt;package name&gt;) loads any installed package
Install vs Launch in OS terms Installing a package is different from loading a package. In the former, you download the package to your set up say from CRAN. In the latter, you'd summon to your current working environment. e.g. install.package("ggplot2") #install package to your local set up library ("ggplot2") #launch by loading your package to current environment for use right now 
As others said, you have to load it using library(psych) first; you can also skip this by calling it specifically with psych::principle(args go here)
With the base merge() function, you can use the arguments all.x, all.y, or all = TRUE to define a left join, a right join, or a full join. Obviously, I and most others prefer to use dplyr, but the functionality exists in base r too. merge(x, y, by = ‚ÄúID‚Äù, all = TRUE)
Put this at the top of your script. # Install Pacman (Package installer/loader) if(!require("pacman"))install.packages("pacman", repos = "http://cran.us.r-project.org") pacman::p_load(package1, package2, etc) *pacman::p_load()* then acts like a combination of *install.packages("packagename")* AND *library(packagename)* to load your package into the working environment.
Do you prefer dplyr's join() over base merge() because the function name is more descriptive, or is it functionally better in some way? Just curious because I learned merge first so I tend to use it a bit more often.
Do you get an error? I don't understand your problem. Here: data.frame(fulllist, row.names = NULL, check.rows = FALSE, check.names = FALSE, fix.empty.names = FALSE, stringsAsFactors = TRUE) What is fullist?
I reworte it in this comment since there were 3 l's in a row, its listall, but in the code all instances of listall are "fulllist" (its easier on the eyes).
Did you accidentally change "fulllist" to "listall" and forget to update it everywhere? You have the code &gt; subset(**fulllist**, Procedure\_Name="extractlist", select = MRN:Hospital, drop = FALSE) But fulllist isn't defined anywhere. I think maybe you mean "listall" from above &gt; listall \&lt;\- read.csv("To Me.csv")
I reworte it in this comment since there were 3 l's in a row, its listall, but in the code all instances of listall are "fulllist" (its easier on the eyes). 
the problem I am trying to fix is that the subset function is not cutting apart the "listall" / (fulllist) based on the colors only in the extractlist .csv file, if that makes sense. It works fine up until that point, I dont get any errors before.
Because dplyr code runs faster (much faster), is more flexible usually, and because it all fits within the tidyverse and is compatible with all the tidy tools which are by far the most important aspects of R
That makes sense. Thanks!
https://cran.r-project.org/web/packages/simstudy/index.html This might work for you.
\`\`\` x \&lt;\- 43241.25 lubridate::as\_datetime(60\*60\*24\*x, origin = [as.Date](https://as.Date)('1899\-12\-30')) \`\`\` Explanation: Excel counts time January 1, 1900. But there's a bug where it counts 1900 as a leap year even though it's not. \`lubridate::as\_datetime\` converts to date by adding the number of seconds since the origin. So you convert the number of days (43241.25) to the number of seconds and count off from there.
Do you know how I would use it on a data.frame? When I do the following for the 1st column of a dataframe: as_datetime(60*60*24*data[1], origin = as.Date('1899-12-30')) I get the following message: Error in .local(x, ...) : unused argument (origin = as.Date("1899-12-30"))
Is your x the same as my x? Also, not in front of my computer but readr::read_xlsx will probably convert the datetime automatically
Maybe make it into a function and lapply it?
This seems to work: library(tidyverse) i \&lt;\- 0:20 slope\_y \&lt;\-0.5 slope\_x \&lt;\- 0.5 mean\_x \&lt;\- 35 mean\_y \&lt;\- 55 sd\_x \&lt;\- 5 sd\_y \&lt;\- 5 x \&lt;\- i\*slope\_x \+ rnorm(n = 21, mean = mean\_x, sd = sd\_x) y \&lt;\- i\*slope\_y \+ rnorm(n = 21, mean = mean\_y, sd = sd\_y) df \&lt;\- data\_frame(i,x,y) ggplot(df) \+ geom\_point(aes(x = i, y = x), colour = "red") \+ geom\_point(aes(x = i, y = y), colour = "blue") \+ geom\_abline(slope = slope\_x, intercept = mean\_x) \+ geom\_abline(slope = slope\_y, intercept = mean\_y) \+ ylim(c(10, 80))
Sorry for the delay, I'm at the end of my quarter right now. So, I finally got a look at the page you want to scrape. At least for me, on Android, it doesn't seem to be an infinite scroll page, but it does have a "show more results" button. It also shows there are 46585 entries (at least for the page you have in your source (dead horses brings it to 72k). So, a couple of things you could do it this is the same thing you're seeing. 1) Brute force. Jam on the show more button until they're all displayed. Around 920x. Then scrape the page. 2) Divide and conquer. There are 3 breeds and 20-30 countries. Work through the filters in two for loops to keep the pages manageable. (You'll still be jamming that show more button though). Filtering by age is messy though because it excludes all the deceased horses. But, if you want even *more* detailed information though, and you're planning on scraping each, individual, horse's page. That's another 72k pages views. You might be able to scrape those with rvest though, and you might be able to find a way to parallelize the scraping to speed it up. I'm any event, I don't know how large the organization is, but before hammering away on their web server, I'd look into seeing if there are other avenues for getting the data. Look for an API or CSV files they might offer. Or, just email them and ask, if you aren't setting up a competing site and you explain why you want the data and what kind of analysis you plan on doing with it, there's a chance they'll just give it to you.
1. Go to http://colorbrewer2.org/ 2. Note down the hex codes for the colors you want 3. Implement them by hand with scale_fill_manual() Hacking the actual palette function is going to be a ton of effort unless you plan on doing this a lot.
Yes, periods is fine in identifiers. In the Google Style Guide for R, it's the preferred way. https://google.github.io/styleguide/Rguide.xml#identifiers variable.name is preferred, variableName is accepted GOOD: avg.clicks OK: avgClicks BAD: avg_Clicks 
It may be worth noting that the Hadley style guide is based on the google style guide but with a few differences. Naming variables and functions is one difference. The preferred method is lower case with an underscore separator instead of a dot. This is because the dot is used in a ton of different contexts in modern r. You don‚Äôt have to use anyone‚Äôs style guide, but you should be consistent. In order to be consistent with the majority of modern r users, the underscore is the preferred method. http://adv-r.had.co.nz/Style.html
While . in a variable or function name is OK I think that style guide is bad advice. The period does mean something. It seperates function name from object class in the s3 object oriented system method dispatch. I.e plot.lm is the method definition that will be called when using the plot function on an object of class lm. It is possible that using . In a function name will accidentally cause unintended breaking behavior if you are unlucky in your name choice. R's oop systems are a bit of a mess. Short version: . A bad choice in names 
Good point. Many years ago, _ was a synonym for &lt;- in R, so it couldn't be used. But times have changed. I have to change too.
Yes, dots are just another legal character in R variable names.
The R equivalent of a dot for member access is the dollar sign. foo$bar means get the 'bar' element from list 'foo'. In the olden days the dot character was used as a separator in Unix file names, and the S language picked up the convention. For the most part a dot in R works like an underscore in python. 
What a gracious response! Thank you for being awesome. 
Yeah, you're reading that line correctly. Periods are also used for method dispatch, though, so I avoid them.
Share it so we can look at it and then send you updates/changes. One of my favorite things is to share code with people and see what they do to expand on / clean up / think about it.
What would be the best way to do that? I only actually have one function written (a very brief series of pipes to get the text from webpage to R). Everything else is in a .RData file.
If you have enough time and enthusiasm, you can try GitHub for sharing your work. The learning curve of git is quite steep, but related skills are very useful and valuable in many IT fields including Data Science, which seems to be your area of interest, and participating in open source communities is really fun. 
I think I will try that. The tutorial I followed on using RStudio touched on it a bit but I kind of skimmed that part. My impression was that GitHub was for more established, finished-product or collaborative work stuff. Is that not the case? Could I just throw...whatever up there?
It is designed for sharing your code and for collaborating on it too. Once upon a time I have personally found a very useful piece of code while doing some research in my student project. Seems like some other student was solving a similar exotic problem years ago and just put her solution online for everybody who may be interested in it. At the same time the most noticeable part of GitHub is a bunch of big projects with hundreds of contributors, but everything there has also started as someone‚Äôs hobby or a pet project.
Sounds perfect. Thanks for the recommendation. :)
Sounds like Tidytext can help you out. Google the package and you'll find the open book online as well... You'll see a lot of different types of exploratory analyses to do. using bar plots, network plots, sentiment analysis... etc. Also you'll see examples of unsupervised learning at the end. So in general tidy text , but when looking at the book you'll come across topicmodels and other packages to consider.
I'll try to look at it in a few minutes since I don't think the function argument to rollapply is the only issue here, but because I never quite figured out how to pass functions as arguments to some of these packages, I just create the function separately and pass it in. madeup &lt;- function(x) length(is.na(x)) df &lt;- mutate(df, AVGSAMPLECOUNT = rollapply(df$MEASUREMENT, width=8, FUN=madeup, fill = NA, align="right")) That'll run your code without an error, though the result isn't yet what you'd want.
I think you misunderstand, I need a tally of each NA calculaed for each rolling mean not a count of all NAs. The purpose of this is to flags rollingaveragess that don't meet some threshold of measurements.
Did you try the code? What you're asking for is what it does. The column na.tally will increment every time there is an NA. na.tally is not a single number. Let's say you're at row 50 in your data. Your run\_number at that point is 50. If you have NAs at rows 20 and 30, then you've had 2 NAs. Whatever the mean is at 50, the denominator in that calculation will be 48, which is run\_number \- na.tally.
Ok I'll try it, thx
Sure. One other thing to go along with it... as long as you're going this far, you might consider leaving the first line of the na.tally code separate as na.marker, so it'll have a 1 every time there is an NA. Then, you can change all of the NAs in MEASUREMENT to 0s and then just calculate the mean yourself without a function from zoo. set.seed(100) df &lt;- tibble( run_number = 1:100, MEASUREMENT = c(runif(24, min = -1, max = 1), NA, runif(24, min = -1, max = 1), NA, runif(24, min = -1, max = 1), NA, runif(25, min = -1, max = 1)) ) df$na.marker &lt;- ifelse(is.na(df$MEASUREMENT), 1, 0) df$na.tally &lt;- cumsum(df$na.marker) df$MEASUREMENT[df$na.marker == 1] &lt;- 0 df$rollmean &lt;- cumsum(df$MEASUREMENT) / (df$run_number - df$na.tally) df$MEASUREMENT[df$na.marker == 1] &lt;- NA For the record, I'm more of a data.table guy, but at least this way shows where everything comes from.
That sounds great! I'll check it out tomorrow. Thanks so much!
We can help you more effectively if you share your code with us. What do you mean by "lines of fit"? Since we are talking about histograms do you want to fit density function? 
Yes i need to superimpose the nonparametric density curve on the graph! I have no problem finding it just can't get both to be superimposed. I posted my code above in the edit! Thank you for helping!
http://ggplot2.tidyverse.org/reference/geom_density.html
In your case - just remove the quotes around the `add = "TRUE"`. It should be `add = TRUE`.
wow thank you total brainfart!
Thank you I was able to solve the problem using a helper function as you suggested and defining that fuction like this &gt; newfuct &lt;- function(somerow) {sum(!is.na(somerow))} Then passing that to the rollapply call to AVGSAMPLECOUNT. This concept is so simple, I can't believe that I didn't see this. Thank you.
If anyone is interested, I uploaded the project to GitHub (https://github.com/HarryVeilleux/candidate_comparisons). I haven't worked at all with Tidytext (per /u/Wusuwhey 's suggestion) yet, but plan to start later today. /u/fdren /u/evgef
I'm still looking the documentation over but just wanted to say this package looks exactly like what I was hoping for. Thanks again!
Are you sure UNIX\-like permissions will work on win-based filesystem? Have you read through `?Sys.chmod` - especially the third paragraph of Details section.
I have, however even that description doesn't illuminate for me the owner/group/others distinction. Perhaps it means that there is only one "read-only" attribute, and that there is no distinction between owner and other people, but this is not a straightforward reading of the paragraph. That being said, if that's the case, then I suppose I'll have to just settle for quick changes of permission and hope nobody opens the files in the narrow windows (heh) that writing makes.
Someone correct me if I'm wrong, but I think that while ntfs technically has support for POSIX file permissions, windows is not using them...at least not by default. Also `chmod` is a unix system utility. https://en.wikipedia.org/wiki/Chmod windows probably has something own. Also it feels like you're trying to solve a windows deficiency with hacky workaround. Seems like you have a http://xyproblem.info/
look at `?as.factor`. The function does not take the `ordered` argument. Only `factor` function does. That's why you get the error. a &lt;- factor(1:10, ordered = F) b &lt;- factor(1:10, ordered = T) a b class(a) class(b) b &lt;- factor(b, ordered = F) b class(b) Also, I really hate I what I'm about to do now, [but have you even tried to search the issue?](https://duckduckgo.com/?q=R+factor+remove+ordered&amp;t=ffsb&amp;ia=qa)
I like to think that I'm trying my best to move through that "What to do about it?" list :P From [some other discussions](https://superuser.com/a/525323), and documentation about [attrib](https://www.lifewire.com/attrib-command-2625802) and [read only](https://www.lifewire.com/what-is-a-read-only-file-2625983) as Windows understands it, there's not a way to set these permissions differently for owner v. "group" (group is probably not a thing in Windows). Maybe for admin level access(?), but I am not an admin. So it seems I have two options: do what I am doing now (or some equivalent in effect that doesn't use Sys.chmod(), like maybe **shell('attrib -r "file_name.csv"')**), or have the "main" file always be read-only, but I just move its contents into R and just completely delete the file and replace it with the updated one from R. That solves the gap problem but is less elegant. Not to mention some files are just going to be very big and I'd rather not try to read/write them in full all the time. Since any file would be write-able for no more than a fraction of a second at a time, I think I'll stick with the shell("attrib {+|-}r ...") option.
No problem.
You can use the `cut` function and `apply` with parameter MARGIN = 2. The cut function can be used to group numerics into categorical data, and apply can be used to easily loop over columns. Something like this might help as an example ```df &lt;- data.frame(dog = 1:10, cat = 10:1) apply(X = df,MARGIN = 1,FUN = cut,breaks = c(0,5,10)) ```
It‚Äôd be helpful if your code was not squished all onto a single line. It‚Äôs just REALLY hard to read what‚Äôs going on there.
Yeah, OP you need to double-line break for Reddit's formatting.
OP, there is a second "bug" in Excel. Excel and R do not index from the same starting date. What R calls If you're reading in a number from Excel, instead of reading dates in as characters or actual Date objects, then your dates in R will not be the same as in Excel. For illustration, open up an empty Excel sheet and convert the number "1" to a date format. Newer Excel versions use the 1900 origin, meaning "1" is January 1, 1900. However, if you type this into R: &gt; as.Date(1, origin=as.Date("1900-01-01")) You'll find that the output date is "1900-01-02", or January 2, 1900. Specifically if you use fractions of dates (so POSIXct), R calculates times from midnight of the given date. So "1" here is 1 day past midnight of January 1, 1900, which is midnight of January 2. It's not even correct to say that Excel calculates times from the day prior, because it cannot handle &lt; 1 inputs for date formatting. It tries to give something from January 0, 1900, which is of course not a date. If you give 1.5 as an input, it's noon of January 1, 1900. So I would consider that a bug as well. Please be careful moving between Excel and R. If you read the expressed date formats as strings, you should be OK. If what makes it into R are numbers, then you'll have other problems.
Can you go from long to wide after you order or arrange by name and type? This way they will be on the same row?
here is a shabby code to get things started atleast:: ds = data.frame(s = c("good","bad","good","bad","bad","good","bad","bad","bad","good","bad","good","bad","bad","bad","bad")) good = which(ds$s == "good") j = 1 for (i in 1:nrow(ds)){ if ((good\[j\] \- i)\&gt;=0) { print (good\[j\] \- i) } else{ j = j\+1 print(good\[j\] \- i) } }
https://paulvanderlaken.com/2017/10/18/learn-r/
Should be fixed now, sorry about that!
John Hopkins had a cycle of 9 R courses for data science on Coursera. They are increasing in complexity and focus so you can choose the complexity level that suites you - here https://www.coursera.org/specializations/jhu-data-science
It's not very clear in your example how the diff column is being calculated, but this sort of thing is very easy using 'tidyverse' methods. For example, to find the time differences within individual names, ignoring the 'Type' column: library(dplyr) # Recreate your sample dataset df &lt;- tribble( ~Name, ~Type, ~seconds, "Dave", "Good", 3, "Steve", "Bad", 4, "Steve", "Good", 6, "Dave", "Bad", 9, "Tom", "Bad", 10, "Marianne", "Good", 12, "Tom", "Bad", 13, "Steve", "Bad", 14, "Marianne", "Bad", 15 ) # Find the differences in time, by name df %&gt;% group_by(Name) %&gt;% # analyze differences within the same name arrange(seconds) %&gt;% # sort by seconds, just in case not in order mutate( diff = seconds - lag(seconds) # create diff column, which is current seconds minus previous seconds ) %&gt;% ungroup %&gt;% # ungroup, because normally dplyr 'grouping' is followed by generating a summary statistic mutate( diff = ifelse(is.na(diff), 0, diff) # replace all the NA's that resulted from no prior row with 0 ) Results in: # A tibble: 9 x 4 Name Type seconds diff &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Dave Good 3 0 2 Steve Bad 4 0 3 Steve Good 6 2 4 Dave Bad 9 6 5 Tom Bad 10 0 6 Marianne Good 12 0 7 Tom Bad 13 3 8 Steve Bad 14 8 9 Marianne Bad 15 3
From looking at the data frame itself, I would say the most efficient way is by subsetting the data based on the condition you want. For instance, if you only wish to include Marianne in the table, then you could subset as follows: newdf \&lt;\- subset(df, name = 'Marianne', select=c(Name, Type, seconds)) Once you have the data subsetted, then you should be able to calculate the new values for diff based on the name you have selected. Hope this helps.
Seems the issue is that R numerically solves this when you use &gt; deriv(deriv(...)) Instead, if you check out [?deriv](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/deriv.html), you can see this: &gt; **hessian**: a logical value indicating whether the second derivatives should be calculated and incorporated in the return value. So when I run your code now, with this replacement for e2nd: &gt; e2nd = as.function(deriv(e, hessian=TRUE)) I get this output: &gt; [1] "Critical value 1 is an inflection point." &gt; [1] "Critical value 1 is an inflection point." This may or may not be desirable, since the two complex roots for the derivative of **e** are indeed +1+0i and +1‚Äì0i. If you want to issue a single output when a root is repeated, then you first need to fix the issue that the elements of eCritVal are actually not equal: &gt; eCritVal[1] == eCritVal[2] &gt;\&gt; [1] FALSE These are numerical approximations, again. They differ by some extremely small amount, so you can say something like: &gt; eCritVal = unique(round(eCritVal,15)) and now equality to 15 decimal places is considered equality, and you get a single output.
I started R through Coursera. The rest I learned through Google and reading r4ds, currently reading advanced R.
Are each of the 9 courses done specifically in R? If so, this seems ideal for what I need.
Thank you, this makes sense why it wasn't working properly. I ended up saving the excel file as a CSV before reading it into R, which remedied the date/time situation I was having.
I like [www.datacamp.com](https://www.datacamp.com) for the introductory and intermediate stuff. Besides that, I find it most useful to work on personal projects.
Swirl can help you learn some R while working in R. [http://swirlstats.com/](http://swirlstats.com/)
Say your data is in a data frame called **DATF**. Say also that the column classes are character or numeric, appropriately. Consider this code: &gt; diff.fun = function(r, datf, type_want){ &gt; d = datf[r,]$seconds - &gt; datf$seconds[which(datf$Type==type_want)] &gt; return(min(d[d &gt;= 0])) &gt; } Now: &gt; DATF$diff = apply(X=cbind(1:dim(DATF)[1]), &gt; FUN=diff.fun, &gt; MARGIN=1, &gt; datf=DATF, &gt; type_want="Good") What value would you like in the rows whose Name values are not, e.g., "Marianne"? Do you want NAs?
The diff in the example is getting how many seconds it has been since "good" for that current. So on the fourth row diff=3 because it's 9 - 6.
So, I'll post what I've used to get pretty good results. "Good" meaning I had no programming experience, now after 4-6 months I'm at least competent enough in R to not need outside help (unless things get weird). Having said that, you may find some of these sources unnecessarily basic, but I'll include them anyway. Side note: You'll see the Paul van der Laken thing posted. It's actually really good and is basically what I suggest. I'm just giving you links directly and adding an additional course for stats. 1) Use the [swirl package](http://swirlstats.com/students.html). This covers the basics of R and outlines the structure of the language. 2) [YaRrr!.](https://bookdown.org/ndphillips/YaRrr/) Again, this will cover some basics, but also gets more into the "ok so how do I do stats here?" kind of thing. 3) [PH525x series - Biomedical Data Science.](http://genomicsclass.github.io/book/) This is more directly focused on data analysis in R. I think it touches on ML, but there are obviously more in-depth guides/outlines for that in R. 4) [R for Data Science.](http://r4ds.had.co.nz/) The "first" author here is Hadley Wickham, who is basically the modern-day R guru. He wrote the tidyverse, making him the de facto man to fanboy over.
OK, I get it now. First without dealing with names: library(dplyr) # Recreate your sample dataset df &lt;- tribble( ~Name, ~Type, ~seconds, "Dave", "Good", 3, "Steve", "Bad", 4, "Steve", "Good", 6, "Dave", "Bad", 9, "Tom", "Bad", 10, "Marianne", "Good", 12, "Tom", "Bad", 13, "Steve", "Bad", 14, "Marianne", "Bad", 15 ) # Find the differences in time since the *last* 'Good' Type time df %&gt;% arrange(seconds) %&gt;% # sort by seconds, just in case not in order mutate(lastgood = ifelse(Type == 'Good', seconds, NA)) %&gt;% fill(lastgood) %&gt;% mutate(diff = seconds - lastgood) Results in: # A tibble: 9 x 5 Name Type seconds lastgood diff &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Dave Good 3 3 0 2 Steve Bad 4 3 1 3 Steve Good 6 6 0 4 Dave Bad 9 6 3 5 Tom Bad 10 6 4 6 Marianne Good 12 12 0 7 Tom Bad 13 12 1 8 Steve Bad 14 12 2 9 Marianne Bad 15 12 3 ... which matches your original request (now that I understand it!) Adding two lines, to group by name, and then (not needed, but cleaner for any future steps) to ungroup: # Find the differences in time since the *last* 'Good' Type time, by individual df %&gt;% arrange(seconds) %&gt;% # sort by seconds, just in case not in order group_by(Name) %&gt;% mutate(lastgood = ifelse(Type == 'Good', seconds, NA)) %&gt;% fill(lastgood) %&gt;% mutate(diff = seconds - lastgood) %&gt;% ungroup() Results in: # A tibble: 9 x 5 Name Type seconds lastgood diff &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Dave Good 3 3 0 2 Dave Bad 9 3 6 3 Marianne Good 12 12 0 4 Marianne Bad 15 12 3 5 Steve Bad 4 NA NA 6 Steve Good 6 6 0 7 Steve Bad 14 6 8 8 Tom Bad 10 NA NA 9 Tom Bad 13 NA NA The NA's in 'diff' occur whenever there was never a 'Good' before the first 'Bad' for that name, so it's not possible to calculate how long since the last 'Good'. 
I did not take this particular one but I believe it is. At least two guys who are teaching it - Peng and Leek are two major R developers with close to a dozen R related books between them.
That worked, thank you so much! The unique() and round() functions were exactly what I needed.
Obviously I am highly biased in favor of the "Paul van der Laken"-thing, but regardlessly new useRs might want to bookmark this overview of handy R links I try to keep up to date: https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/
R For Data Science is a free textbook online. [http://r4ds.had.co.nz/] I'd definitely recommend working through it if you don't have the budget for datacamp.
Adding to my "Paul van der Laken stuff" folder as we speak!
Glad to help!
To piggyback on this question, I'm interested in the medical part of biomedical data science, but this seems to be mostly genomics/multiomics/bioconductor. Are there any resources for learning R in the context of medical/health data science?
&gt; Are there any resources for learning R in the context of medical/health data science? There may be, but I haven't used any. I work in a genomics/epigenetics lab, so all of those kinds of discussion are directly relevant to me. Personally, I'm a proponent of learning at least a bit about R as a programming language in and of itself before diving into deeper analysis. If you have no programming background, you need to understand what exactly it is R is doing, what any errors you get mean, etc. Obviously Google is a big help here, but having a least a basic understanding of programming makes life easier. After that, I'd honestly just go grab some kind of public data set and get to analyzing whatever you want. In my experience, once you have some sort of "core fundamentals" down, the best way to learn how to do X is just by *doing* X.
Learn the basic syntax with any MOOC then literally just Google every function you need
Try datacamp :-) 
Hi I originally misunderstood your requirement. Following updated code gives what you want.. Only condition is seconds is in ascending order before starting: `ds = data.frame(s = c("good","bad","good","bad","bad","good","bad","bad","bad","good","bad","bad","bad","bad","good"), time = c(2,4,7,9,15,16,18,22,29,35,39,42,44,47,52)` `good.time = ds$time[which(ds$s == "good")` `j = 1` `for (i in 1:nrow(ds)){` `if(((ds$time[i] - good.time[j])&gt;=0) &amp;&amp; ((ds$time[i] - good.time[j+1])&lt;0)) {` `ds$diff[i] = ds$time[i] - good.time[j` `}` `else{` `j = j+1` `ds$diff[i] = ds$time[i] - good.time[j]` `}` `}` &gt; &gt; &gt;ds = data.frame(s = c("good","bad","good","bad","bad","good","bad","bad","bad","good","bad","bad","bad","bad","good")) &gt; &gt;good = which(ds$s == "good") &gt; &gt;j = 1 &gt; &gt;for (i in 1:nrow(ds)){ &gt; &gt;if ((good\[j\] \- i)\&gt;=0) { &gt; &gt;print (good\[j\] \- i) &gt; &gt;} &gt; &gt;else{ &gt; &gt;j = j\+1 &gt; &gt;print(good\[j\] \- i) &gt; &gt;} &gt; &gt;}
Yes, there are such things. First though, you'll want to go back from frequency to "count" and "exposure". Ie, if your frequency is "0.10 counts of X per month", you'll want to recover the original count data and the number of months. Why is this important? Because you have thrown away information by encoding count + exposure as frequency. And second, having count data allows you to use or "zero-inflated binomial" models. This will model two things separately: first, the probability of giving a zero or anything else than zero. And second, if it's not zero, it models the success rate, which is like your frequency data. I don't know if there are ways to combine this with random effects usually, the packages I've seen that does zero inflated models usually don't include random effects, but you can fit a (bayesian version of) model like this probabilistically in stan with the excellent `brms` package: library(brms) m = brm(count | trials(n) ~ time + (1+ time | Subject), data = my_data, family = "zero_inflated_binomial") 
This is fantastic, thank you! You're a lifesaver.
It‚Äôs worth it. And you appear to be a poster child for Hadley‚Äôs preference for starting with plotting when learning R: http://r4ds.had.co.nz/
Great, thank you. I didn't realize that book was broken out outline, what a resource. Hadley, Garrett, and I are becoming quite good friends as I progress through Datacamp... I want to continue that progress through Datacamp, but perhaps completing the book is the next step and will silence that voice looking to explore something real. Either way all paths lead to the same conclusion of being savvy in R and data analysis. Thanks rev!
It's time for you. Drop the courses, and start up a project to learn portions of your skillset through. I'm serious. I've just picked up R in the last 6 months, and courses stopped helping very quickly. Get to the nitty gritty. Hadley's repos that host his books are the place to be, for reference. 
Great feedback, thank you.
Look up kaggle. It's a data science competition with tons of data. Browse through the data for something that interests you and then play around with it. 
I've been pouring through the Datacamp courses to get the foundation for working Kaggle items. The learning curve was too steep to jump in, and it's I'm right at the cusp of being able to work them with some level of proficiency.
You say you've developed a bunch of VBA macros for your team. I would start there. Take a problem you worked out an Excel solution for and see if you can recreate the output in R. Doing so should give you a better idea of what the pros and cons of making the switch for your work, and should give you a better understanding of the underlying problem as well.
Positives over Excel? - Reproducibility. You can share a script with a colleague, they can see every step you took, and reproduce it instantly, often saving you the trouble of emailing them the data in an Excel sheet or whatever else. - Packages. I could be wrong, but finding, installing, managing and using packages is way easier in R. There is a package for everything and, especially in something like RStudio, the documentation is super easy to read and find. CRAN is great. - Scalability. You can run R on certain distributions of Hadoop. You can run R on a server with way more memory and computing power than your laptop could ever hope for. You can connect it to a MySQL or other database too (though Excel can do this too, R does it very well) - Plots. Much more flexible in R than I've found them to be in Excel. - Compatibility. I don't think Excel can read Stata data files. I'm guessing this is true for other formats as well. - It's free! Upgrades are free, packages are free, lots of graphical frontends are free. - Excel doesn't have Shiny. Shiny is awesome.
For me there's the tangible aspect and the "fun" aspect. I'm only a free user on data camp, so I can't really get deep into the "real\-life" and tangible parts of R, like modeling and stats. The good thing is, there's lots of resources online that I can use to remedy this problem. Regarding the "fun" part of R, geez, there's a lot that'll make it for you. Shiny has been mentioned. Also, blogdown. You can turn your R analyses or markdown documents into blog posts! Make interactive maps with leaflets or interactive charts with html widgets! You can also plot social networks with ggraph and tidygraph... Don't think I've heard of that being done so well in Excel. Eventually you'll do enough things and it'll lead you to creating unique functions or maybe even a package... For me, it's mostly about the "fun" aspect. I'm definitely trying to get into the tangible though. 
Fantastic! Thanks very much. You're a beaut.
I would suggest splitting the data into columns first, something along the lines of read.delim(file, delim = " ", skip = 5, col.names = ..., colClasses = ...) Then, you can run your regex on the last column only, and anchor it at the beginning. Should be a lot faster. Alternatively, venture off the base-R path and use `tidyverse` or `data.table`.
I've got it in a table like this [https://i.imgur.com/1xshneO.png](https://i.imgur.com/1xshneO.png) though even very basic commands take much longer than I'd expect, eg. the following simple query to retrieve the first 3 characters from the first 3 rows of a column takes a good minute: sqldf('select substr(V17, 1, 3) from df limit 3') I'm quitting/reopening R studio and running my queries afresh to see if that makes any difference.
I would recommend this command instead: &gt; df2 &lt;- df[grep("W1.", df[,17]),] I'll try some benchmarking on those two, but some things come to mind: (1) searching for strings which *don't* contain a pattern will certainly take at least as long, or longer, than a command that searches for if a pattern exists; and (2) searching all of **df** for a pattern seems incorrect because of how the data frame is formatted. Only the 17th column, if any, has those characters, so it would be best to only search the 17th column. If the strings are formatted consistently too, then you can do this: &gt; df2 = df[substr(df[,17],1,3)=="W1.",] This should run faster than **grep**.
&gt; retrieve the first 3 characters from the first 3 rows of a column Try instead: &gt; first.chars = apply(X=df[1:3,],MARGIN=2,FUN=substr,start=1,stop=3) I think this should give you a matrix with the data you want. My own R is running a long script now so I won't be able to check this yet...
I‚Äôm a guy who just LOVES R
Hmm, thats odd, a similar table is working marvelously well for me. Anyway, check this script out, and see if it works better for you https://gist.github.com/westernmagic/68280c9546ced727f6c75b5419ed13c1 
Thanks for that, I can run that script in a couple of minutes though it's still taking me vastly longer than I'd expect to run simple commands querying the table.
if youre using igraph, adding node attributes is something like V(my_graph_dataframe)$node_attribute &lt;- vector tidygraph is a bit better id you ask me, make your dataframe into a table graph object and then activate(nodes) and left join the node attributes 
Sounds like an issue on your end then; once the data is loaded, cleaning takes maye 10-20 seconds, but the filtering by "\^W1\\." is only a few seconds. And my machine is comparable to yours.
I haven't looked at the raw data there, but the links included suggest it's actually a file with one json object per line. If that's the case, you could add a '[' at the beginning and a ']' at the end which might work. You might also do an: lapply(readLines(file.choose()), fromJSON, flatten=T)
What you are seeing here is a "factor" data type. Per element, it is as memory efficient as an integer array but by including levels, it can encode categorical information. If memory/space is not a concern, you can convert the factors to pure strings. Do this by either using the `as.character` function or changing your data frame import to disable factors. One other option is to force the factor levels to align with your true data. Since you are using basketball data, the position names naturally correspond to numbers (PG being 1, C being 5). Unfortunately, the default factors are alphabetical. Instead, when you parse the column, you can manually set the levels. 
/u/geosoco 's point about opening &amp; closing brackets is a great suggestion to try. I just wanted to chime in and say, `jsonlite` offers `read_json()` as a convenience wrapper around `fromJSON()`, as the latter is often finicky regarding its passed arguments. Sometimes it doesn't understand if what you gave it is a file path, or a string.
Check the data type of Position using str: str(Position) Is it a character variable? If so, then the levels should not convert to number format. Example: `&gt; letters&lt;-c('A','B','C','D','E')` `&gt; str(letters)` `chr [1:5] "A" "B" "C" "D" "E"` `&gt; as.factor(letters)` `[1] A B C D E` `Levels: A B C D E` You can see that even when converting to factor format, the levels stay in the same format. Depending on what data type Position is, you might wish to convert it. I can't clarify this without working with the data, but you might have some luck by converting to character format: `as.character(Position)`
Thanks u/geosoco and u/ryapric \-- I'll try these things! And yes, it is indeed a file with one JSON object per line. I am going to save my 27 objects that I managed to glean from the data with the hex editor, and see if I can get that to behave using your suggestions, and then tackle the big data afterwards.
This example provides quite a good overview of how you could use **rep** to carry out what you want to do: [https://stats.stackexchange.com/questions/25148/how-to-expand-data-frame-in-r](https://stats.stackexchange.com/questions/25148/how-to-expand-data-frame-in-r) Adjusting it for your specific example: `df &lt;- data.frame(` `name=c("a1", "a2"),` `group=c("b1", "b1"),` `count=c(3))` `expanded &lt;- data.frame(name = rep(df$name, df$count),` `group = rep(df$group, df$count))` `v&lt;-c("v1","v2","v3","v1","v2","v3")` `df2&lt;-data.frame(expanded,v)` `df2` Upon running the above, you see that we end up with the following: `&gt; df2` `name group v` `1 a1 b1 v1` `2 a1 b1 v2` `3 a1 b1 v3` `4 a2 b1 v1` `5 a2 b1 v2` `6 a2 b1 v3`
In database terms, this is a cross-join or a Cartesian product. You can do these with dplyr... https://github.com/tidyverse/dplyr/issues/197
Super easy in base R: you're looking to create a Cartesian Product, and the simplest tool for that is [expand.grid()](https://stat.ethz.ch/R-manual/R-devel/library/base/html/expand.grid.html). 
I think this should do it for you: library(dplyr) library(purrr) library(tidyr) data.frame(A = c('a1', 'a2'), B = c('b1', 'b2')) %&gt;% rowwise %&gt;% mutate(C = list(c('v1', 'v2', 'v3'))) %&gt;% unnest(C)
 mat1_test &lt;- data.frame(matrix(c("a1", "a2", "b1", "b2"), ncol = 2)) vec1 &lt;- c("v1", "v2", "v3") expand.grid(vec1, mat1_test[,1], mat1_test[,2]) 
 v &lt;- c(0, 0, 1, 2, 3, 0, 0) v[v == 0] &lt;- 0.01
Thank you. I really appreciate the help. 
I would also add that modelr::data_grid() works the same way as expand.grid() but adds some cool functionality for continuous variables. It is a wrapper around tidyr::expand(). Pick your poison. 
.rdata will work just fine. I prefer saveRDS(), which allows you to assign the data that you are reading in to a variable. 
`ifelse()` is lightning fast for this: ifelse(yourVector == 0, 0.00001, yourVector) This will return a _new_ vector with the zeros replaced.
I also used `expand.grid`, but i think this example might be helpful. lst &lt;- list(A = c("a1", "a2"), B = "b1", C = paste("v", 1:3, sep = "")) df &lt;- expand.grid(lst) df1 &lt;- df[order(df$A, df$C), ] df1 [Run it yourself](http://rextester.com/JFOJP60245)
Typically the standard transform is to add one to every observation, not just transform 0. Since log(1)=0
thanks for the advice. i'll read into the difference between rdata and saveRDS as not very familiar with either. thank you.
If you're using the "prob" argument in sample() you need to supply it with the probability of being chosen for each element. So you need to set a vector of probabilities of an equal length to the number of rows on your dataset. Or leave out "prob" and it'll default to equal chance for each element. 
I followed your advice, and the code looks like this now: set.seed(1234) ind = sample(1, nrow(seedData), replace = TRUE) library(party) Formula = class ~ area + perimeter + compactness + kernelLength + kernelWidth + asymmetry + kernelGroove seedData_ctree = ctree(Formula,data = ind) However, the following error is shown: &gt; seedData_ctree = ctree(Formula,data = ind) Error in eval(predvars, data, env) : numeric 'envir' arg not of length one
I focussed a bit too much on the sample() error, I think the following does what you want it to. Take a random 70% of your data as a training data and the remainder to test. set.seed(1234) ind = sample(nrow(seedData), nrow(seedData)*0.7) testInd = 1:nrow(seedData) testInd = testInd[!(testInd %in% ind)] trainData = seedData[ind,] testData = seedData[testInd] library(party) Formula = class ~ area + perimeter + compactness + kernelLength + kernelWidth + asymmetry + kernelGroove seedData_ctree = ctree(Formula, data = trainData) 
Perhaps you‚Äôve found an instance of the xy problem... https://meta.stackexchange.com/questions/66377/what-is-the-xy-problem
old-school, I like it.
Thank you again for your help, but there is a slight error that shows, but I'm not sure how to change it. &gt; trainData = seedData[ind,] &gt; testData = seedData[testInd] Error in `[.data.frame`(seedData, testInd) : undefined columns selected
&gt; testData = seedData[testInd] I made a mistake in that, it's supposed to be &gt; testData = seedData[testInd,] Can I recommend you do a basic R course or tutorial online?
You aren't using the `sample()` function correctly. However, you need to make some sample data available in an easy-to-use format before I will attempt to write some working code for you. If your data set is small, just use `dput(trainData)`, copy the output, and edit your post to contain the output.
No problem!
Many thanks. Now that I look through the answers, expand.grid() may admittedly provide an easier way of carrying it out - it might be worth checking out geocompR's answer in that regard.
Try the stringi package. It has functions to extract the last match it finds automatically. ‚Äústri_extract_last_regex‚Äù or similar...
can you explain why this code doesnt work? stri_extract_last_regex(df$column, "^([1-9]|1[012])[- /.]([1-9]|[12][0-9]|3[01])[- /.](19|20)\\d\\d$") This is the regex: ^([1-9]|1[012])[- /.]([1-9]|[12][0-9]|3[01])[- /.](19|20)\d\d$ I had to add double back slashes in R - is that what's messing it up??? I confirmed this regex works in regex101
i hate complicated regular expressions so here's a hack that works pretty well. requires the lubridate and stringr packages " blah blah blah, 3/02/2014: blah blah blah, 3/29/2016: blah blah blah, 7/08/2017: blah blah blah" %&gt;% str_split(",") %&gt;% .[[1]] %&gt;% .[4] %&gt;% mdy
I like tidycensus
Does tidycensus allow for US mapping like that? I don't know why I didn't think so, but if it does I'll look back into it
Tidycensus to get census data and shapefiles, including lower quality ones. Then the dev (soon prod) version of ggplot2 that has geom_sf.
Ah. Thanks my dude!
Late to the party, but try this: it uses `stringr` and so it is vectorized as shown in the example. library(stringr) extr_date &lt;- function(x) { start &lt;- tail(str_locate_all(x, "\\d+/\\d+/\\d+")[[1]], 1)[, 1] end &lt;- tail(str_locate_all(x, "\\d+/\\d+/\\d+")[[1]], 1)[, 2] return(str_sub(x, start, end)) } str &lt;- "blah blah blah, 3/02/2014: blah blah blah, 3/29/2016: blah blah blah, 7/08/2017: blah blah blah" extr_date(str) str_df &lt;- data.frame(str = rep(str, 50)) extr_date(str_df$str)
Ahh, regex. I think it's basically the aspect with the worst aspect as far as "usefulness:time investment" is concerned. Finding a regex that works for you usually takes a decent amount of tinkering if you haven't worked with strings very much. 
This is a pretty basic question so you're going to get a lot of answers about this I reckon. I think the best suggestion is to forget about plotting in base R and simply learn ggplot2. ggplot2 prefers tidy(long) format so it's better to have your data gathered up into Key-Value pairs. Basic steps will be to take your list and get it into a data frame, get data into the tidy format (Specimens in one column, Variables in another, if it isn't already so) then ggplot it. 
Since no working example was given, I'll use the following list for illustration: exl &lt;- list() exl[[1]] &lt;- data.frame(matrix(rnorm(30), nrow = 10)) exl[[2]] &lt;- data.frame(matrix(rpois(30, 1/2), nrow = 10)) exl[[3]] &lt;- data.frame(matrix(rnorm(30, 3, 2), nrow = 10)) names(exl) &lt;- c("Specimen 1", "Specimen 2", "Specimen 3") To get to the part you asked about, here is some code that will do it: library(ggplot2) exl &lt;- Map(cbind, exl, name = names(exl)) exl &lt;- do.call(rbind, exl) ggplot(exl, aes(x = X1, y = X3, colour = name)) + geom_line() Basically, the use of Map adds a column to each data frame which contains the name for that member of the list. Then the data frames are stacked and finally use ggplot2 to plot.
Thank you! This has mostly done what I want it to do, the only question I have is in the "colour = name" part. In this case I want all of the lines to be the same colour (green) and I will later add more lines to this from another list that will be a different colour (red). When I change it to "colour = green" it does some funky stuff that I don't quite understand. Is there a way to do this?
To your second point, this will do the trick ggplot(exl, aes(x = X1, y = X3, group = name)) + geom_line(colour = "green") For your first point, PM me and I'll gladly help. 
Late to the party, but I'd recommend the data.table package. If you are comfortable with SQL you'll pick it up, and after the learning curve of maybe a few days. 17 cols and 3.5 million rows should take maybe 2-5 seconds tops. I regularly process 10-15 million rows and it's worth learning the package. Not at a computer right now, but with a fairly small data set like yours I'd expect this operation to take maybe a second with idiomatic code. If it's still an issue for you, PM me and I'll write up how to do it faster later today 
`Here's a data.table solution. I didn't answer the whole cleaning issue, just the problem you're having with column 17. Pulled a little bit from u/_`[westernmagic](https://www.reddit.com/user/_westernmagic) and his Gist. The column 17 search takes maybe 3 seconds, and that's with creating a new column just to do the search and then deleting it. str\_detect takes about twice as long but gets you where you're headed. library(data.table) library(readr) library(stringr) chess_dat &lt;- read_lines(paste0("data\\", list.files("data")), skip = 5) chess_dat &lt;- str_split(chess_dat, " ", n = 18) chess_dat &lt;- rbindlist(lapply(chess_dat, as.list)) colnames(chess_dat) &lt;- c("id", "d", "result", "welo", "belo", "len", "date_c", "resu_c", "welo_c", "belo_c", "edate_c", "setup", "fen", "resu2_c", "oyrange", "bad_len", "hashes", "game") chess_dat[, search_vec := substr(game, 1, 3)] chess_cleaned &lt;- chess_dat[search_vec == "W1.", ] chess_cleaned[, search_vec := NULL] Cheers
Actually, plot is your random factor. You have four different plots per treatment these are independent I assume (unless they're close together and there could be overlap?). But then for each plot you have your five samples per year. So the structure you will want to account for is that from each plot having 5 measurements. The event column won't be needed in the model (unless you fear there's a "time effect") and neither will the replicate. No plot had multiple treatments, so you'll want only random intercepts (no random slope). So, something like this, for each of your insect species. library(lme4) m1 = lmer(Araneae ~ Treatment + (1|plot), data = TP17Grouped) Or you could reshape your data to include all the insects in one model. library(tidyverse) longdata = gather(TP17Grouped, taxonomic, count, Aranenae, Species2, Species3, Species4) m2 = lmer(count ~ Treatment * taxonomic + (1|plot), data = longdata) Or, now that we reshapes the data, it became obvious that you're working with count data, so you should consider a poisson or a negative binomial model instead of anova/regression. 
I'm guessing your API call is wrapped in a function? I'd suggest writing a second function that stores each XTS object in a list as you iterate through your calls, and use the data.table package function rbindlist (or do.call(rbind...) and have your function return your data frame. This way you're keeping everything in the function scope and aren't cluttering your global environment 
Thanks for the reply, the function does ask where you want to data, however I used a loop to call each stock ticker. The function asks what environment you want to store the data in, with the default being the parent.frame(), ie store it as an xts for the stock ticker
I'm not familiar with xts data, but in that case I'd define an environment to hold your data temporarily www.r-bloggers.com/environments-in-r/ and just remove it when you're done processing it üëç 
Save your code into a file, then run `Rscript {your_filename}.R` on command line. 
Or learn Shiny. There is a file upload tool that would make it even easier for your boss to run it without blowing everything up
Looking at the `MASS` documentation, I see that `lda()` has `prior` defined as "the prior probabilities of class membership". It should have one value for each level in the factor on the left side of the function. `length(levels(iris$Species))` is 3, so `prior = rep(1, 3)/3` works -- it's three values long, each equal to one-third, which implies an even chance for each level.
Take a look at the [Data Wrangling Cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) section on reshaping data with tidr. gather() should get you the result your looking for more or less. 
Check out the [fuzzy join package](https://cran.r-project.org/web/packages/fuzzyjoin/index.html)
Thank you! This is great!
When in doubt, data.table
That is excellent cheatsheet for R use, but I think in this case spread() is the correct function
Thanks I'll look into that. 
Or even easier (on non-Windows machines): put a shebang (#!/usr/bin/env Rscript) in the first line of your file, enable executing it (chmod +x your_file.R), and run it any other executable.
to piggy back on this, all of the cheatseets on that site are incredibly useful. I find myself using the lubridate and stringr sheets frequently as well
Gather makes two columns into one column, spread makes one column into two columns. 
up vote!
up vote!
,Col_type=‚Äútext‚Äù
&gt; ,Col_type=(‚Äútext‚Äù) Do you mean Col_types? I am using the example from [this site](https://readxl.tidyverse.org/articles/cell-and-column-types.html) And it still doesn't work. My dataset has 5 columns: calpads.csv &lt;- read.csv( "filepath\\file.csv", header = T, col_types = c("text","text","numeric","numeric","numeric") )
r is case sensitive there is no such class as "Character" but there is a class called "character".
Yea gather or reshape2::melt
Thanks so much! Was working with this function but incorrectly. The guide was so helpful. 
Do all the ID numbers have the same number of leading zeroes or does it vary?
it's always something like that. thanks. worked like a charm
Wouldn‚Äôt you just apply scale to the mins. And then to the maxs 
uhm... I think scale needs center = T, scale = T 
it still doesnt work with center=T and scale = T 
Your comment helped so much! It works now, thank you. Would you happen to know how to see the associated output table that goes with LDA? I can't figure out how to see that and the other LDA plot I have gives different &amp;#37; variation explained than the ggord function 
I know how to show the group means, coefficients of linear discriminants, and proportion of trace. &gt;ord &lt;- lda(Species ~ ., iris, prior = rep(1, 3)/3) &gt;ord Call: lda(Species ~ ., data = iris, prior = rep(1, 3)/3) Prior probabilities of groups: setosa versicolor virginica 0.3333333 0.3333333 0.3333333 Group means: Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 5.006 3.428 1.462 0.246 versicolor 5.936 2.770 4.260 1.326 virginica 6.588 2.974 5.552 2.026 Coefficients of linear discriminants: LD1 LD2 Sepal.Length 0.8293776 0.02410215 Sepal.Width 1.5344731 2.16452123 Petal.Length -2.2012117 -0.93192121 Petal.Width -2.8104603 2.83918785 Proportion of trace: LD1 LD2 0.9912 0.0088 &gt; Is that what you needed, or are there other tables you want to see?
Hm. I'm a little confused now. I get pretty different proportions of trace using the prior = rep(1, 3)/3) vs no prior. I'm still a newbie so maybe it's obvious. 
Also do you know how to change the colors with ggord? I have 15 groups so it's hard to distinguish between groups with the color palette used. 
I've never used the package, but the README.md gives an example: `p &lt;- ggord(ord, iris$Species, cols = c('purple', 'orange', 'blue'))`
Ahhh, thank you for your very patient answers, that's perfect! I'm still a newbie. 
No worries! It's something to do while I watch knitr do its thing with my thesis. :-) I am new to serious R work myself, having tinkered with it in undergrad stats classes. It's a strange language in some ways, but it has been very very good to me so far!
Woah... I haven't even heard of knitr, that is amazingly useful! I'm glad I haven't already done the integration of code into my thesis yet. Agreed though, R has proven to be relatively easy and I wouldn't have been able to do half the data exploration I've been able to do without it. 
Are you always extracting numbers out? If so, this code works. library(tidyverse) df &lt;- df %&gt;% # Extract number into new column mutate(replicate = str_extract(identifier, "\\d")) %&gt;% # Replace numbers with empty string, then trim extra spaces at end of string mutate(identifier = str_trim(str_replace(identifier, "\\d", ""))) %&gt;% # Rename identifier column rename(name = identifier) But to answer your question specifically, you're looking for the function ifelse(). library(tidyverse) df &lt;- df %&gt;% # If type == "unkn" then extract the number, otherwise make the value NA mutate(replicate = ifelse(type == "unkn", yes = str_extract(identifier, "\\d"), no = NA)) %&gt;% # Replace numbers with empty string, then trim extra spaces at end of string mutate(identifier = str_trim(str_replace(identifier, "\\d", ""))) %&gt;% # Rename identifier column rename(name = identifier) The second one works, but technically ifelse() isn't necessary if you're always extracting numbers. Up to you which one you need. If you have any questions let me know, happy to help.
Like /u/GoodAboutHood says, regular expressions are what you are looking for.
Tons. R, Python, SQL knowledge can land you a junior data analyst role anywhere. Once you have business experience go for more senior roles. 
as.data.frame(apply(year,2,function(x) scale(x,center=T,scale=T))) 
Thanks a lot for you prompt answer! All my plots are independent. I have tried the script above and I am getting results, finally! Once that I have reshaped my data, I was able to use the second script "m2" When I tried with the third script"m3", my R doesnt give me any results. When I stop the process because I dont get any results, I am getting this message: &gt;Warning messages: &gt; &gt;1: In commonArgs(par, fn, control, environment()) : &gt; &gt;maxfun &lt; 10 \* length(par)\^2 is not recommended. &gt; &gt; 2: In optwrap(optimizer, devfun, getStart(start, rho$lower, rho$pp), : &gt; &gt; convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded Does that mean that my data sheet is too long? I would like to know the time effect as well because I have sampled my plots at different time in the growing season and I want to see the difference between Events. Thanks again for you help, I really appreciate it!
&gt; convergence code 1 from bobyqa It means the model didn't converge. You can do two things from here: simplify the model (ie remove the random slope, so it's `... + (1|plot)`), or venture into bayesian statistics: library(brms) m3_v2 = brm(count ~ Treatment * taxonomic + (1+taxonomic|plot), data = longdata) As for event, there are at least a couple of different ways this could be included in the mode. a) You can include it as a linear effect by coding with the earliest event = 0, then 1 etc. b) You can "one-hot" encode it by including it in the model as a factor. c) You can model it as some kind of non-linearity, either a 2-polynomial, or a periodic function or something. I can't really help you too much with choosing between these options: it's probably something you should look around in literature as to how it's usually done in your field, and discuss with a collegue / supervisor. 
Anecdotal, but I have found that companies care less about *how* you clean/analyze data and more about if you can do it well in the first place. Then they want to know if you can pick up another language (python) as needed. Emphasis on showing you understand statistical/cleaning/analyzing concepts and being able to use Python would be well worth your while and open a lot more doors IMO.
Are you specifying in a similar way to the below? Essentially, we are creating a pdf file and embedding the image. `&gt; pdf(filename="file.pdf")` `&gt; plot(image)` `&gt; dev.off()` You might find this link useful for further information: [http://www.dummies.com/programming/r/how-to-save-graphics-to-an-image-file-in-r/](http://www.dummies.com/programming/r/how-to-save-graphics-to-an-image-file-in-r/)
I just tried that and I get `&gt; Error in pdf(filename = "test.pdf") :` `unused argument (filename = "test.pdf")`
I just tried the below and it ran successfully for me. In your case, make sure you are setting the directory and you can see that we're plotting a variable **a** as below, which you would simply substitute with your plot. `setwd("C:/Users/Username/Documents/Desktop")` `a&lt;-c(1,2,3)` `pdf("file.pdf",width=6,height=4,paper='special')` `plot(a)` [`dev.off`](https://dev.off)`()`
Still a blank pdf unfortunately :( updated all my packages just now too.
Let's link people directly to the case study and not to some SEO blog that's 2 steps removed from it: [http://services.google.com/fh/files/misc/ga360_magicbricks.pdf](linkypoo)
Data/ BI analyst here. Microsoft have integrated R to many of it's products. Power BI, R server in SQL, Azure machine learning etc. Just learn basics of these Microsoft technologies and you can waltz into almost any mid-size company and land a data analyst job.
Hey, I figured out what it was. I was using an imported text and when I removed it it was able to generate the pdf again. Pity I can't do both :/
Pull up glassdoor, search for jobs in your area and put your resume out there man. Cold call/email. Show up at meetups in your area and pass out your card/resume. Go and get it! If you want it, you're gonna find it.
Looks like they used decompose(data$sessions) - I wrote some detail on how to do this with digital analytics here http://www.dartistics.com/timeseries.html
There's a ton of work out there for this. Knowing SQL and R can land you an EL analyst job easily. I got hired 3 months after graduation, and I probably don't know R as well as you do. 
Thank you!!!! I am such a dork. I knew about ifelse and regular experssions, but I was stuck on trying to make them work within the separate command. I really appreciate the super detailed comment! 
I see. I don't know what your plot looks like or the structure of your data so I'm not really sure what advice to give beyond that.
There's an extensive lesson in swirl that talks you through the time syntax in R. It might be helpful to you. 
We just closed out recruitment on a junior analyst doing a lot of work in R. It's a great skillset. We are an enormous multinational entertainment &amp; e-commerce company.
I ended up solving my own problem, thanks anyway :) editing the original post with the solution in case anyone else has this issue in the future.
I'm not able to answer fully with code right now (mobile, should stop neglecting human beings) and there might be a cleaner answer than I'm about to give you, but off the top ofy head you could create a separate indicator for the event of interest (0/1) and use rleid()... A running length variable that resets every time the indicator or person changes. Then the max of the running length variable by person will be the max distance between events. I forget whether it resets to 0 or 1 so you may need a minor adjustment to get the correct figure at the end. I usually do this with data.table... but maybe just the idea of a running length that resets will get you what you want. Sorry for the incomplete answer!
Are the numbers in groups of 4 like you have here?
If your fixed value D needs to be at position N in a vector of length L, shuffle all values except D to give you a vector Q of length L-1 with random order. Now split Q at N into Qleft and Qright. Bind Qleft, D, and Qright.
They should be 2 or 3 in every seven - they are days of the week, labelled as unique events on those days and I need to preserve one group of days. Preferably I would rather not assume that they are following a pattern and give the computer a bit more work, if it's possible. I suppose one way would be to make a note of the positions of D, scramble the rest and add it into the missing spaces, but I wonder if there is any easier way to do it 
As example, a random vector 'vect' with four elements of size N N &lt;- 20 vect&lt;-sample(c("A","B","C","D"),N,replace=T) I want the same position for entries of value "D" fixed&lt;-which(vect=="D") You just need to shuffle the indexes of the others entries : vect.shuffle &lt;- vect vect.shuffle[(1:N)[-fixed]] &lt;- vect.shuffle[sample((1:N)[-fixed])]
I still don‚Äôt totally understand they data, but your second paragraphs here was my first thought. Maybe there‚Äôs a more elegant solution. Sometimes I have to work thru less elegant solutions to finally identify a more elegant solution.
I have a distillery, I make whisky, rum and brandy but only one per day so I can clean my still and some days I'm too drunk to do anything. My vector is WWRBWDDRD, I can't change when I drink (because I only drink at other people's parties) so D has to be fixed in place, and I want to rearrange what I make on which days, but keeping the days when I'm hungover where they are.
That works perfectly, thank you 
Slick :)
It‚Äôs best to provide a data frame of (a subset of) your data, which allows us to provide code that is most likely to help you. Sometimes writing out a data frame definition can be time consuming, but coming up with a solution can take some time too.
Nice. 
Here‚Äôs what you do: 1) Get a job as a Data Analyst - almost every company needs one. Find a job that you‚Äôll be able to use sql. You‚Äôll want to automate pulling data from a sql server (using R), then the R script will clean and output results. Having these skills combined puts you in the top 5% of analysts, and you can get paid as a Senior Data Analyst in no time. 2) Practice modeling - while at your data analyst job get good at modeling (regression, ARIMA, etc). Practice machine learning. You can now get any analyst job you want in the country, and can move onto Data Scientist positions as well.
Use the argument ylim or xlim. For example: y &lt;- 1:10 plot(y) plot(y, ylim = c(-10, 20)) plot(y, xlim = c(-10, 20)) 
If you need to convert all your R code to JS, then you will have to do it by hand. If it's fine to have an R backend powering the charts, then this is certainly doable - have a Shiny server serve up charts or snippets that the frontend can use. This would be my preferred way. You can also take a look at the [htmlwidgets](https://cran.r-project.org/web/packages/htmlwidgets/index.html) package, which allows you to generate HTML + JS code from R. There are bindings for JS vizualization libraries such as [D3.js](https://d3js.org/) already - see the "reverse imports" section. Alternatively, you can have R output the data as eg. JSON, and then the frontend guys can choose a JS charting framework of their liking to make the charts, calling your API for the data. 
Yeah as _westernmagic says, you can use htmlwidgets to create your plots instead, which output HTML/JS that can be given to web developers. If you are using ggplot at the moment you may also want to look at plot.ly, which will convert your ggplot charts into interactive JavaScript ones. Finally, another option would be to use R to create an API of the data, then give that data in JSON form to the web dev team to develop the visuals themselves. For that, the packages plumber or OpenCPU https://www.opencpu.org/ running on a server would do it. 
HTML Widgets Is What you Need: [https://www.htmlwidgets.org/](https://www.htmlwidgets.org/) Heatmaps: [https://www.htmlwidgets.org/showcase\_d3heatmap.html](https://www.htmlwidgets.org/showcase_d3heatmap.html)
Another option is to use plotly. Build your plots in R, serialize the layout and data to json, then rebuild the plot in JS with plotly again.
Adding to this; Just create a shiny dashboard and use whatever package you like. No need for added complexity.
Well this was officially a doozy of a problem. To start, you need to make your one column data frame into a single vector instead. That way you can do operations on the vector. I commented out the code to do that, but it's in there. Let me know if this doesn't work. library(tidyverse) library(data.table) # Allows you to use %like% library(lubridate) # Easy way to reformat your dates at the very end # Change your data into a vector so you can split it # vals &lt;- as.character(your_df$column) vals &lt;- c("Person A","Coolcool","2018-06-25 19:34","Person B","See you later",":D", "2018-06-25 19:34","Person A","You called Person B","Duration: 30 seconds", "2018-06-25 19:19","Person B","What's up?","2018-06-25 19:09","Person A", "Hey!","2018-06-25 19:09") # Write out the list of names people_list &lt;- c("Person A", "Person B") # Create an empty messages vector message &lt;- c() # For loop that goes through each value of your vector for (i in 1:length(vals)) { # If value is in your people_list and two values later starts with "2018" # then return one value later. # If not, test again to see if three values later starts with "2018" # then return paste together the split up message into one value. x &lt;- ifelse(test = vals[i] %in% people_list &amp; vals[i+2] %like% "^2018", yes = vals[i + 1], no = ifelse(vals[i] %in% people_list &amp; vals[i+3] %like% "^2018", yes = paste(vals[i+1], vals[i+2]), no = "NA")) # Append your x value found above to the message vector message &lt;- c(message, x) # Assign the value of message above to the message vector outside the for loop assign("messages", message, .GlobalEnv) } # Complete creation of each vector message &lt;- message[message != "NA"] person &lt;- vals[vals %in% people_list] date &lt;- vals[vals %like% "^2018"] # Paste the vectors together into a data frame, reformat the date column as a date df &lt;- data.frame(person, message, date) %&gt;% mutate(date = ymd_hm(date))
this is awesome! thanks so much, I wouldn't have known where to begin. I'm now trying to figure out how to modify the code so that it's not just limited to searching the next two rows for the message, since sometimes the message more than 2 rows long. Will keep trying myself, but let me know if you have ideas as well :)
So I'll let you work on this a bit yourself, but here's your hint: In the first ifelse() statement, see how the first "no" equals another ifelse statement? This can continue on as long as you like. Good luck! If you're unable to figure it out or have any other questions let me know, happy to help.
\# rebuild your data df &lt;- scan(what=character(),text= "V1. Person A Coolcool 2018-06-25 19:34 Person B See you later :D 2018-06-25 19:34 Person A You called Person B Duration: 30 seconds 2018-06-25 19:19 Person B What's up? 2018-06-25 19:09 Person A Hey! 2018-06-25 19:09",sep="\\n") df &lt;- data.frame(V1=df\[-1\]) \# flag date cols df$is\_date &lt;- grepl("\^\\\\d{4}-\\\\d{2}-\\\\d{2} \\\\d{2}:\\\\d{2}$",df$V1,perl=T) \# create type cols df$type &lt;- "message" df$type\[df$is\_date\] &lt;- "date" df$type\[c(1,which(df$is\_date\[-nrow(df)\])+1)\] &lt;- "Name" \# create group cols df$group &lt;- cumsum(df$type=="Name") \# aggregate to your taste df2 &lt;- aggregate(V1 \~ type + group, df, paste,collapse="\\n") \# develop in 3 col data.frame data.frame(matrix(df2$V1,ncol=3,byrow = T)) \# X1 X2 X3 \# 1 2018-06-25 19:34 Coolcool Person A \# 2 2018-06-25 19:34 See you later\\n:D Person B \# 3 2018-06-25 19:19 You called Person B\\nDuration: 30 seconds Person A \# 4 2018-06-25 19:09 What's up? Person B \# 5 2018-06-25 19:09 Hey! Person A
Here's a general approach: # install.packages("combinat") library(combinat) x &lt;- LETTERS[1:4] special_letter &lt;- "D" i &lt;- which(x == special_letter) permn(x[-i],append,special_letter,i-1) # [[1]] # [1] "A" "B" "C" "D" # # [[2]] # [1] "A" "C" "B" "D" # # [[3]] # [1] "C" "A" "B" "D" # # [[4]] # [1] "C" "B" "A" "D" # # [[5]] # [1] "B" "C" "A" "D" # # [[6]] # [1] "B" "A" "C" "D"
You're looking for a running length of non zeroes : flag_ &lt;- dat$NewEvent &gt;0 rle_ &lt;- rle(flag_) num_ &lt;- rep(rle_$lengths,rle_$lengths) dat$NewEvent[which(flag_)] &lt;- num_[which(flag_)] # Person Day NewEvent # 1 1 1 NA # 2 1 2 NA # 3 1 3 0 # 4 1 4 3 # 5 1 5 3 # 6 1 6 3 # 7 1 7 0 # 8 1 8 1 # 9 2 1 NA # 10 2 2 0 # 11 2 3 3 # 12 2 4 3 # 13 2 5 3 # 14 2 6 0 # 15 2 7 1 # 16 2 8 0
here's a fix on what you tried: df %&gt;% separate( identifier, c("name", "replicate"), sep =" (?=\\d*$)") # type name replicate # 1 unkn Thing 1 # 2 unkn Thing 2 # 3 unkn OtherThing 1 # 4 unkn OtherThing 2 # 5 stnd High &lt;NA&gt; # 6 neg Negative Control (Type A) &lt;NA&gt; # 7 neg Negative Control (Type B) &lt;NA&gt;
Thanks! That's initially what I did, but I wonder if there's a cleaner way to get around it? Like maybe counting the rows between the name and the date, doing a range loop? I tried to run it on a larger dataset, and I'm thinking that one reason it might not be working is that there may be one message or two that is longer than 15 rows (which is what I have right now). I'll keep looking into it :) Thanks again for your help!
Ah yep, I didn't realize it extended to that many rows for one message. Another guy posted, might be worth checking out his answer. He took a completely different approach, might work better for ya.
Thank you VERY much, simple and effective.
`scale` doesn't work like you think it does, read `?scale`
This works iris2 &lt;- iris[-5] maxs &lt;- apply(iris2, 2, max,na.rm=TRUE) mins &lt;-apply(iris2, 2, min,na.rm=TRUE) scaled &lt;- as.data.frame(scale(iris2, center = mins, scale = maxs - mins)) Remove columns and rows until you have a minimal example to show, it will probably solve itself on the way.
Since you know that all messages end with a timestamp, you can use regex to split the string everytime there is a timestamp. Here is some code that should work. It assumes that you have your messages in a .txt file in your working directory. When there are multiple lines they are joined with a ". " between them. Depending on your use case this might not be the best solution. Feel free to ask questions. If this is also a learning experience for you, try running the code line by line and at each point print the variables to see how it changes. # read text as single string from .txt file # stolen from https://stackoverflow.com/questions/9068397/import-text-file-as-single-character-string#9069670 fileName &lt;- 'str.txt' messages &lt;- readChar(fileName, file.info(fileName)$size) # regex pattern that matches the timeformat pattern &lt;- "\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}" # split by times messages_split &lt;-strsplit(messages,split = pattern) messages_split &lt;- unlist(messages_split) # split by newline messages_split &lt;- strsplit(x = messages_split, split = "\n") # extract times times &lt;- gregexpr(pattern = pattern,text = messages) times &lt;- regmatches(x = messages, m = times) times &lt;- unlist(times) # create dataframe dataframe &lt;- data.frame(times = times, person = sapply(messages_split, FUN = function(x) x[2]), messages = sapply(messages_split, FUN = function(x) paste(x[3:length(x)],collapse = ". "))) 
Build fake data: data(sample_matrix) sample.xts &lt;- as.xts(sample_matrix, descr='my new xts object') sample.xts2 &lt;- sample.xts class(sample.xts) # [1] "xts" "zoo" class(sample.xts2) # [1] "xts" "zoo" solve: all_object_names &lt;- ls(globalenv()) all_xts_names &lt;- all_object_names[sapply(mget(all_object_names),inherits,"xts")] list2env(lapply(mget(all_xts_names),as.data.frame),globalenv()) check class(sample.xts) # [1] "data.frame" class(sample.xts2) # [1] "data.frame"
gsub("(.\*)?(\\\\d{1,2}\\\\/\\\\d{2}\\\\/\\\\d{4})(\\\\D\*$)","\\\\2",test) \# \[1\] "7/08/2017"
Remind me to test this tomorrow
I don't understand what you are asking for. Your code appears to be well written, and you seem to have a good grasp of how to use Leaflet. Are you looking for additional suggestions of how to visualize your data to get more insight? 
Thanks for the reply. I appreciate the complement but this is the first project I have attempted, most of this is a collection of code I found on the internet and help from someone who is way better at coding than I am but has limited time/ability to give me input. So my main question is, instead of plotting each individual route (180 unique route Id's in this case), how can I plot each route by route_type as in regular bus, express bus, intermediate MAX, and MAX bus, which I have altered the route type variable to indicate. I know route_type would be the best way to visualize the information as opposed to having 180 different route_ids and colors on my map and my legend. I need to know what is missing from the code to make the proper routes between each latitude and longitude show up, colored by route_type. 
After doing a lot more looking into the problem I think what I need to do is "Concatenate", or order by sequence the lat and lon for each point. The shape file already does this with the shape_pt_sequence variable. It counts the sequence of each lat lon for a specific shape_id which is why everything plots perfectly when I just plot the shape file. I need to figure out how to join the routes file (specifically the route_id and route_type variables) to the shape file via the trips file in order to make sure that the route_id's are connected to the shape file in sequence so they plot the correct bus routes. Hopefully that makes sense. 
Some use of duplicated() should do it
You can use the function table(). An example: row1&lt;-sample(c("A","B","C"),size = 20,replace=T) row2&lt;-sample(c("1","2","3"),size = 20,replace=T) table(row1,row2)
Why did you delete your question ? I'm hoping to help more than one person when I take time to answer a question online.
thanks very much! learning a lot through this 
 #Create a sample dataset df &lt;- tibble(col1 = sample(c("A","B","C"), size = 1000, replace = T, prob = c(.1, .3, .6)), col2 = sample(c("1","2","3"), size = 1000, replace = T, prob = c(.2, .75, .05)) ) # r/Ligerian solution table(df$col1, df$col2) # Another way of looking at it that gives you tidy output count(df, col1, col2, sort = T) 
I would do the following, just cause that's the way I would think about the problem. &gt;df %&gt;% &gt; group_by(col1, col2) %&gt;% &gt; summarise_all(count = n()) (My inner pedant won't let me comment without mentioning that you should really use the literal `TRUE` not the variable `T`)
Summarise_all() will break ops dataframe if there are more columns. Count() is acceptable shorthand for group_by() %&gt;% summarise(n()). If anyone else is wondering... TRUE is a reserved keyword in r while T is not. Someone can reassign T like this T &lt;- FALSE and screw with you code. I find this very unlikely, easy to diagnose, and easy to trace back to who broke the code. The saved keystrokes are worth it to me and simple to change for production code if necessary. But my inner pedant understands the argument and half agrees. My inner pedant does not agree about count() though. The explicit, easy to understand, and short one liner should be preferred. 
Summarise_all() will break ops dataframe if there are more columns. Count() is acceptable shorthand for group_by() %&gt;% summarise(n()). If anyone else is wondering... TRUE is a reserved keyword in r while T is not. Someone can reassign T like this T &lt;- FALSE and screw with you code. I find this very unlikely, easy to diagnose, and easy to trace back to who broke the code. The saved keystrokes are worth it to me and simple to change for production code if necessary. But my inner pedant understands the argument and half agrees. My inner pedant does not agree about count() though. The explicit, easy to understand, and short one liner should be preferred. 
You could use the regex `(\d{4}-[01]\d-[0-3]\d) ([0-2]\d:[0-5]\d)` to match the date-times. Maybe split up the list on each match and check to see if it's a special case (like a call) otherwise the name is in the next entry, then messages after that - until the next date-time. Does that make sense?
Create the query first as a string stored as a variable. Then pass that along to the dbGetQuery function instead of trying to do the paste inline for results_query.
Thanks, I learned something about summarising! I think you're right about count() too, but hopefully providing the elaborated version might just help someone understand what's happening - which is good. Reddit discussions are awesome.
So are you saying do it like `AND` [`a.id`](https://a.id) `IN (SELECT ........ FROM ......)` The problem is, these two queries I've listed, come from completely seperate databases. Or are you suggesting something completely different?
Hey, GoopOnYaGrinch, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Fuck off, u/[CommonMisspellingBot](https://www.reddit.com/user/CommonMisspellingBot)
My guess would be that that a 'paste' statement within a 'dbGetQuery' call is overridden to use a different function (the "COnnection.prepareStatement" that isn't implemented here). This would be done to prevent SQL injection attacks as you never want to put unsanitized data directly into a string you pass to the DB. Try as epembo said and do the paste statement outside of the dbGetQuery call first, and then use that string in the call. If the above is the problem, then this should fix it (although you will still be vulnerable to SQL errors as you are passing in unsanitized data). A second guess would be that "id_query$id" is not just a list of integers, but is some class that presents as a list of integers, and calling paste on it is overriding the normal paste function. In this case you would just need to cast id_query$id.
Okay, so in terms of what you and epembo are saying, what are you suggesting? I guess I'm a bit confused
 s = paste("SELECT i.event_date, i.id, i.id2, i.id3, i.id4, i.id5, COUNT(i.sales) sales, COUNT(c.volume) volume FROM table1 i LEFT JOIN table2 c ON i.id = c.id AND i.id2 = c.id2 AND i.id3 = c.id3 WHERE i.event_date = DATE('2018-06-18') AND i.id IN (", paste(id_query$id, collapse = ", "), ") GROUP BY 1,2,3,4,5,6 LIMIT 10 ;", sep = '') results_query &lt;- dbGetQuery(con, s) Is the first solution. Also the code you paste had a typo as it is missing a second end parentheses 
Oh snap that worked! Thank you. Y'all are kings.
Bueno. It's generally a good practice to incorporate dates, data frames, etc. into a string before passing it to a function (at least as far as SQL queries and such go). Like /u/natched said above, one huge side benefit to doing that is you can then print "s" (in his example) to see exactly what the query is after it's processed. That way you'll catch grammatical errors, missing parentheses, unquoted text, etc. a lot more easily than if you're trying to step through the logic of what *should* be happening.
Agreed. And you are awesome, too!
Try this variable &lt;- 12 NameYourQuery &lt;- "Enter query here ###variable1##" NameYourQuery2 &lt;- gsub('###variable###', caseStatements2, NameYourQuery ) ## this would result in your query being "Enter query here 12" dbGetQuery(con, NameYourQuery2)
Why couldn't you restrict the left join to table2 using the criteria from your first query, so you would only need one query?
Is there a reason you couldn't simply run the first statement to create a temp table in the DB and then use that in the second query? I see two different variable names for the connection, but from the code you posted I can't quite tell if you're interacting with two different databases.
Also, as a general rule, you never want to create temporary tables in a RDMS because doing so can seriously degrade performance and will not be scalable.
I'm not sure where you get this idea from. Like anything else, temp tables have their place and uses, and multiple options should be tried to determine what is most efficient. In this case I tend to doubt he's feeding data to an API or building an ETL process, so I don't see any realistic problem with it. Of course, if it were all in one database the obvious choice would just be to nest the ids in a sub select, or join the query results, but avoiding a temp table in this case seems overly picky.
There's a reason "best practices" exist. In the case of "temp tables," even if performance were not an issue (LOL!), there is still absolutely no reason to use a temporary table if you are even moderately proficient with SQL. Even if you aren't proficient SQL, creating a temporary table in a database is wrong for so many reasons, but just security, performance, and maintainability. --- **TL;DR**: I'd rather drink out of a toilet than create temporary tables because the former is far safer than the latter. ---
There are a variety of ways to do this, but one of my favorites is to use r notebooks. You can use SQL, R, and bash chunks inline in the notebooks and save objects back and forth. A common use case might be to programmatically select a date range or a selection vector string and use that variable in the sql chunk. Since you can use connections from more than one database at a single time, this makes it very easy to see what is happening and it is very reproducible. 
I decided to go look for support for your position. I did not find much, and given your lack of humility I see no reason to continue further. Insistance on "best practices" which are not consistent from system to system, is a pretty clear sign of a mind that wants a dogma to stick to because it can't think the problem through.
Twitter is probably a better source of information, or google news. &gt;headlines = read_html("https://www.google.com/search?hl=en&amp;tbm=nws&amp;authuser=0&amp;q=american+president") %&gt;% html_nodes(".r") %&gt;% html_text() &gt;headlines [1] "Iranian President: No American President Can Renegotiate the Now ..." [2] "US: President Barack Obama vetoes 9/11 bill" [3] "President Obama Wants Donald Trump to Visit New African ..." [4] "President Obama: Discrimination Should Concern 'All Americans ..." [5] "Conrad Black: The Middle East watches, and waits, for the next ..." ... https://stackoverflow.com/questions/39672911/how-to-fetch-headlines-from-google-news-using-rvest-r 
Could this be a semantics issue? I.E., in this context is there a difference between a 'temp' table and a 'volatile' table? Because if so, may be you are not even disagreeing with each other.
You must be joking, right? Humility? # HAHAHAHA
Is this allowed per Google's TOS that, as I recall, forbids automated search queries? I ask because I would also like to run automated search queries on Google, but I'm afraid. Since I've been running automatic queries on other sites the number of captcha images I've been forced to click has gone up significantly.
Please don't. This is the most common source of security holes. Only use *prepared statements* with SQL, *never* manipulate the raw SQL string.
As mentioned elsewhere, **do not do this**. It's usage, and the most common reason for security breaches. Use *prepared statements*, that's what they're there for.
No. The exact opposite is true. What you're suggesting subverts the type system and its phenomenally unsafe, and a proven way of getting security holes. Never stringify data yourself, pass the actual data to appropriate functions.
I agree with you that this is unsafe, but if you could provide an answer to the problem using prepared statements, that would be very useful. I know that's the right way to do things, but I haven't done DB work in R (I mostly do that in Python or PHP) so I can't help with the specifics in this situation.
I think you're misconstruing what I meant. If you're trying to incorporate, say, Sys.Date into a SQL query, or pass along a variable stored in your Renviron, etc., it's a better idea to do that incorporation first (i.e. build the SELECT statement as a string) and then pass that along to the function running the query vs. trying to pass the variables along inline within the query function call. Doing the latter can often cause problems, in my experience. I'm assuming here, of course, that the data is already sanitized. But we're also not talking about building a production system where user-submitted data can be injected - the OP is just trying to query sales tables and pass along the result of that query into another query that is also hitting sales tables. Presumably, sales IDs and volume/dollar amounts are not user-inputted values that present a security concern.
You might be able to configure a Google Alert (RSS feed) for the name you are interested in and then access the RSS feed programmatically and process it locally for keywords.
Right, I seem to have understood you correctly, and this is simply a *terrible* idea. What you suggest goes under the moniker [‚Äústring typing‚Äù](http://wiki.c2.com/?StringlyTyped) (a parody of ‚Äústrong typing‚Äù) and, like I said, subverts the type system. &gt; I'm assuming here, of course, that the data is already sanitized This is a downright dangerous assumption (and not theoretically: as mentioned, this is the single most frequent source of security breaches!), because character strings don‚Äôt give you any such guarantees. &gt; But we're also not talking about building a production system This is really not an excuse to throw out good coding practices. &gt; Presumably, sales IDs and volume/dollar amounts are not user-inputted values that present a security concern. Notice again that you need to prefix your statement with ‚ÄúPresumably‚Äù. These presumptions are the first thing that goes out of the window in the real world. Countless systems that break or are hacked were built with the assumption that they (or parts of them) wouldn‚Äôt be security relevant.
All true. But again, the OP is looking for a solution for his specific problem, and as he said, it worked for him. You can certainly disagree with the method, but you're also free to provide your own alternate solution to the OP instead of browbeating those who've tried to help.
Countering dangerous misinformation ‚Äúbrowbeating‚Äù is pretty low. As for providing my own solution, I would but somebody else has already given the appropriate solution: using a temporary table (which is itself far from ideal but the alternatives are worse). Better yet would of course be to merge the two queries into one.
The only way to *safely* interpolate values into SQL queries is to use the `?` placeholder. **Do not use anything else** (it‚Äôs dangerous, as discussed in other comments). This means the relevant clause of your query should read: AND i.id IN (?, ?, ?, ?) Where the number of `?` is the number of values you want to compare against. Unfortunately there‚Äôs no ‚Äúsplice interpolate‚Äù equivalent. This means you need to generate the placeholder dynamically: placeholder = sprintf('(%s)', paste(rep('?', length(ids)), collapse = ', ')) And then construct your query: query = sprintf(" SELECT i.event_date, i.id, i.id2, i.id3, i.id4, i.id5, COUNT(i.sales) sales, COUNT(c.volume) volume FROM table1 i LEFT JOIN table2 c ON i.id = c.id AND i.id2 = c.id2 AND i.id3 = c.id3 WHERE i.event_date = DATE('2018-06-18') AND i.id IN %s GROUP BY 1,2,3,4,5,6 LIMIT 10", placeholder ) res = dbSendQuery(con, query) dbBind(res, list(ids)) results_query = dbFetch(res) dbClearResult(res) 
Maybe, but I think it seems clear from the bolded type and colorful metaphors that we couldn't get far enough in the conversation to figure that out. His is just a personality type that exists in tech, and many other places unfortunately.
Can you explain how this would be the cause of a security hole?
Three explanations: 1. [The classic](https://xkcd.com/327/) 2. [General description of SQLi](https://en.wikipedia.org/wiki/SQL_injection) 3. [Applied to R](https://cran.r-project.org/web/packages/RODBCext/vignettes/Parameterized_SQL_queries.html) Bonus: [Injection flaws are the most prevalent security vulnerability](https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project)
Ok thanks for explaining...that's good to know...would this negate the issue? TableName &lt;- Table1 IDlist &lt;- "1,2,3" startDate &lt;- '2016-01-01' endDate &lt;- '2017-12-31' sqlQuery(connection, paste( " DROP TABLE IF EXISTS",TableName,"; CREATE LOCAL TEMPORARY TABLE",TableName,"ON COMMIT PRESERVE ROWS AS ( SELECT * FROM databasetable WHERE ID IN (",IDlist,") AND date BETWEEN",startDate,"AND",endDate," "); " ) 
I don't feel it was low at all, as it reflected what was seen in the thread up to that point. Data analysis/data science is a field with an enormous range of skillsets. Some folks are self taught and have limited resources, working in academic or small corporate enviornments. Some are working at Fortune 500 companies and have every resource they could ever want at their disposal, as well as the training/experience to fully utilize those resources in a proper and effective manner. Some folks are very strong in statistics and weak in engineering, while others are the opposite. Like I said, there's a huge range. While it's fine to point out where people are incorrect, it's counter-productive to just jump on people and make them feel stupid for contributing in the first place. If it's "wrong", show *why* it's wrong, and provide the learning opportunity that others may be expecting (i.e. show sample code, not a link that just reiterates your point in more technical terms). But simply commenting just to tell people their ideas are terrible and make it seem as though your way is the Only True Way is disrespectful to where they may be in their own personal development, and unhelpful in actually solving the problem (let alone whether the solution is even feasible for that specific person's circumstances). That being said, I did see the comment you posted with your solution, and I commend you for sharing that. It was an approach that I'm sure many reading this thread may not be familiar with, and it will help people who see this in the future to better understand how to approach this specific type of problem. Thank you for taking the time to post that.
Looks like it can't find the driver on the server. You may need to specify the path to the .jar location on the server (using [.jaddClassPath](https://www.rforge.net/doc/packages/rJava/loader.html)) for it to "see" the driver. If you run... print(.jclassPath()) ...on the server, you should be able to see where it's currently looking.
No, on the contrary: this exhibits the exact same problem. It‚Äôs a classical injection vulnerability. As mentioned (and explained in the links), the only good fix is to use prepared statements.
&gt; While it's fine to point out where people are incorrect, it's counter-productive to just jump on people and make them feel stupid for contributing in the first place. I‚Äôd seriously *love* to know how to tell somebody they‚Äôre wrong without being stupid. This would be the holy grail of online discussions. Unfortunately people seem to take offence no matter how you phrase it. Case in point: at no point did I ever say anything even slightly insulting. Rather, I merely stated that (a) the proposed solution is wrong and, indeed, dangerous and (b) I gave a pointer to a better solution. The terseness of the messages was due to being written on mobile.
so there's no way to safely pass in variables to sql query string? 
Thanks. So I ran that and it spit this back out. Any ideas? `Error in .jcheck() : No running JVM detected. Maybe .jinit() would help.` `Calls: print -&gt; .jclassPath -&gt; .jcall -&gt; .jcheck -&gt; .Call`
It's not that what you said was insulting or not, it's that it presumed a level of understanding and expertise on the part of those participating in the discussion. Your example code makes clear how to better address the specific problem, whereas linking to dense articles and Wikipedia pages doesn't necessarily help - it's very easy to read all of the "Parameterized SQL queries" CRAN page (for example) and still come away not understanding exactly how *the OP's code* would need to be changed to address this. Put another way, if someone is using a word incorrectly, you can either give them a modified version of the sentence that shows how to properly use it, or you can link them to a grammar website or dictionary. One of those things is directly helpful, while the other is more likely to cause frustration and confusion, while also not actually solving the problem at hand. That's where I was coming from. I agree about security, and I hate terrible production code as much as anyone else. But I also know what it's like to have a boss breathing down your neck for a deliverable and having no idea why your code isn't executing. It's good to have sympathy for people posting here who just need to get a quick answer that helps them solve for the immediate need, *then* educate and detail any other potential concerns. Or better yet, provide multiple solutions - a) easy quick fix, b) better, more scalable code, c) ideal solution, etc. - and let them choose what works for their needs.
You may need to add a line after the if statement to initialize the JVM before attempting to run additional dependent functions. Check [this](https://www.developer.com/java/ent/getting-started-with-r-using-java.html#Item5) for a bit more detail.
Thanks. But sorry to keep asking, but I can still use some more clarification. I would normally ask our engineer but he's on PTO. So like this? `library(rJava)` `library(RJDBC)` `library (RPostgreSQL)` `URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/AthenaJDBC41-1.1.0.jar'` `fil &lt;- basename(URL)` `if (!file.exists(fil)) download.file(URL, fil)` `.jinit(classpath = NULL,` `parameters = getOption("java.parameters"), ...,` `silent = FALSE, force.init = FALSE)` `print(.jclassPath())`
There is, via *prepared statements*. And this issue has nothing to do with R, it affects every interaction between different layers of formal languages (in particular, but not exclusively, any interfacing from a host language with SQL).
Try running .jinit without any parameters... .jinit() ...and see what happens. I don't have a ton of experience working with rJava, but the error messages you keep posting seem to indicate that the server is having trouble finding and loading the JVM, so I'd suggest stepping through one piece at a time. If the base .jinit() command works, great! If not, see what error you get and if it gives you some directional clues.
Thanks and appreciate whatever help you can give me because I really need to fix this haha. I ran this ```library(rJava) library(RJDBC) library (RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil) .jinit() print(.jclassPath())``` And hey! No errors this time. It returned this: `[1] "/usr/lib/R/site-library/rJava/java"` So I added back in the line that gave me the problems: ```library(rJava) library(RJDBC) library (RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil) .jinit() print(.jclassPath()) drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'")``` And unfortunately, it returned this error: `Error in .jfindClass(as.character(driverClass)[1]) : class not found Calls: JDBC -&gt; is.jnull -&gt; .jfindClass` So basically, I'm back to where I'm started :( 
So the .jinit() command is initializing the JVM, that's good. From there, it seems you just need to point to the .jar you're trying to utilize. Again, not super familiar with rJava, but I suspect it's something like this: .jaddClassPath(URL) If you add that in just before your drv (and after the .jinit() call), it should work, unless I'm misunderstanding the documentation for .jaddClassPath. The core problem seems to be that in drv, you reference the driverClass by name, but the JVM doesn't yet know the name. I suspect Athena is not supported out of the box by rJava, so you have to tell it that it exists before you can reference it.
Gah! So I ran this ``` library(rJava) library(RJDBC) library (RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil) .jinit() .jaddClassPath(URL) drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'") ``` And again, it gave me the same stupid error: ``` Error in .jfindClass(as.character(driverClass)[1]) : class not found Calls: JDBC -&gt; is.jnull -&gt; .jfindClass ``` 
well yeah we're in r/Rlanguage so I specified R library(RODBCext) connHandle &lt;- odbcConnect("cakesDatabase") newData &lt;- read.csv("newData.csv", stringsAsFactors = F) query &lt;- "UPDATE cakes SET price = ? WHERE cake = ?" sqlExecute(connHandle, query, newData) odbcClose(connHandle) is an example of a parameterized query...how does the value of ? get passed, is really what I'm asking. I know the RODBCext assists in this, but I'm not able to figure that piece out.
Development nightmares! :) Maybe try moving the .jaddClassPath(URL) before the .jinit() call and see if that does it? The fact that it works locally but not remotely is almost always going to be the result of config differences - a package is installed/not installed, a software version (local newer, server older, etc.), and so on. If the above suggestion doesn't work, maybe just check one thing after another to see where there may be differences (i.e. local rJava version vs. remote rJava version, and so on).
Still getting that same error :( I don't know if this helps, but when I run it locally, [this what is stored in the values within the global environment](https://imgur.com/a/950iILD)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ERrUzpk.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e1g8d3h) 
It looks like it's just dumping "fil" into your working directory. Can you check the remote version and a) see what your working directory is, b) see what the value of "fil" is, and c) confirm that you have read/write access to that location? I have a feeling you may need to be explicit about the path in order for the remote version to work (i.e. fil needs to be "path\\file.jar" and not "file.jar")
&gt; getwd() [1] "/home/xxxxx" (xxxx just represents my name in this situation). I'm pretty confident I have read/write access. I've done a ton of stuff on the server already that involves saving R scripts and the CSVs generated by them into that directory. &gt; fil [1] "AthenaJDBC41-1.1.0.jar" 
Maybe change... .jaddClassPath(URL) ...to... .jaddClassPath(fil) ...since the path is actually your working directory and not the remote URL. I should've caught that earlier, but had a bit of a brain fart. Presumably, it should be able to see the .jar if "fil" is being populated with the correct value and the file is actually present in your working directory.
GRRHGHGHEIF Still giving me the same dang error! FYI this is exactly what's running in case you notice anything glaringly missing or potentially out of order: ``` library(rJava) library(RJDBC) library (RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil) .jaddClassPath(fil) .jinit() drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'") ```
So I tried to run this code on my machine, and I was initially running into the same "class not found" issue. However, I did a little bit of digging and stumbled across [this](https://stackoverflow.com/questions/42683348/connecting-to-athena-via-r) Stack Overflow question. The suggested solution there indicates that you'll want to instruct download.file to download the file as a binary. After doing that, I got it to work. So, delete the Athena driver(s) present in your working directory. Then, run the updated code below: library(rJava) library(RJDBC) library(RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC_1.1.0/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil, mode="wb") drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'") I was able to successfully run this on my local machine without any errors. Try it and see if it works for you.
fnewpoogewrpigorpijg Just got my hopes up. But no, same error :( :( Here's what comes up for `URL` and `fil` ``` &gt; URL [1] "https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC_1.1.0/AthenaJDBC41-1.1.0.jar" &gt; fil [1] "AthenaJDBC41-1.1.0.jar" I deleted what .jar file was in my wd() and just did `ls` and this is what was present: `AthenaJDBC41-1.1.0.jar `
Try deleting the file, ensuring there's no Athena .jar files in the working directory, then restarting R. Do all that before running the script and it should re-download the file before executing. If that still doesn't work, I suspect it's a Java version issue (i.e. you have Java 7 installed, but are trying to use a Java 8 .jar, or vice versa). In that case, check and see what version of Java you have installed. You can do that with the code below: library(rJava) .jinit() .jcall("java/lang/System", "S", "getProperty", "java.runtime.version")
Ran it and again: ``` Loading required package: methods Loading required package: DBI trying URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC_1.1.0/AthenaJDBC41-1.1.0.jar' Content type 'binary/octet-stream' length 11074334 bytes (10.6 MB) ================================================== downloaded 10.6 MB Error in .jfindClass(as.character(driverClass)[1]) : class not found Calls: JDBC -&gt; is.jnull -&gt; .jfindClass Execution halted ``` So I did the troubleshooting you suggested: ``` .jinit() .jcall("java/lang/System", "S", "getProperty", "java.runtime.version") [1] "1.7.0_91-b02" .jinit() [1] 0 .jcall("java/lang/System", "S", "getProperty", "java.runtime.version") [1] "1.7.0_91-b02" ```
Try to replace URL with the [latest version of the Java 7 .jar](https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.2/AthenaJDBC41_2.0.2.jar).
This is again going to sound extremely stupid but like this? ``` library(rJava) library(RJDBC) library(RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil, mode="wb") ``` Because I tried that and it gave me this: ``` xxxxx@52:~$ /usr/bin/Rscript /home/xxxx/test_file.R Loading required package: methods Loading required package: DBI trying URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar' Error in download.file(URL, fil, mode = "wb") : cannot open URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar' In addition: Warning message: In download.file(URL, fil, mode = "wb") : cannot open URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar': HTTP status was '404 Not Found' Execution halted ``` Thank you again SOOO much for your help. You've been incredible and you'll be getting gold whether we can fix this problem or not. 
No stupid questions, just stupid problems! :) The code you put in the last reply got cut off - after the "DBI trying URL" part, it gets truncated. Can you split it up with some carriage returns, or just paste it without the code formatting so it'll wrap?
This work? xxxx@52:~$ /usr/bin/Rscript /home/xxxx/test_file.R Loading required package: methods Loading required package: DBI trying URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar' Error in download.file(URL, fil, mode = "wb") : cannot open URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar' In addition: Warning message: In download.file(URL, fil, mode = "wb") : cannot open URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC41_2.0.2/AthenaJDBC41_2.0.2.jar': HTTP status was '404 Not Found' Execution halted 
For whatever reason, it can't find the file on the server (the 404 error just means the file can't be found at that location). Try going to [Amazon's page for Athena drivers](https://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html) and copying the link for the one that's compatible with Java 7 (JDK 7.0), and then replacing your "URL" with that link. I'm pretty certain this is the issue, as the one you were originally linking to is listed as being for Java 8, and your earlier reply shows you're working in a Java 7 environment. In fact, this may have been the root cause all along. If for some reason the above isn't working, see if you can update the remote server to install Java 8. If using a different driver won't fix it, maybe we can just change the environment instead. :)
Yet again, the same fucking error strikes: Loading required package: methods Loading required package: DBI trying URL 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.2/AthenaJDBC41_2.0.2.jar' Content type 'binary/octet-stream' length 7645822 bytes (7.3 MB) ================================================== downloaded 7.3 MB Error in .jfindClass(as.character(driverClass)[1]) : class not found Calls: JDBC -&gt; is.jnull -&gt; .jfindClass Execution halted This was my original script btw: library(rJava) library(RJDBC) library(RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.2/AthenaJDBC41_2.0.2.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil, mode="wb") drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'") Soooo... I guess see if I can upgrade it to Java8? 
Okay, so I had our SysAdmin upgrade me. root@52:~# java -version java version "1.8.0_171" Java(TM) SE Runtime Environment (build 1.8.0_171-b11) Java HotSpot(TM) 64-Bit Server VM (build 25.171-b11, mixed mode) So now what? Do I run the same script I started with?
Yeah, try running the last version I sent you: library(rJava) library(RJDBC) library(RPostgreSQL) URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/AthenaJDBC_1.1.0/AthenaJDBC41-1.1.0.jar' fil &lt;- basename(URL) if (!file.exists(fil)) download.file(URL, fil, mode="wb") drv &lt;- JDBC(driverClass="com.amazonaws.athena.jdbc.AthenaDriver", fil, identifier.quote="'") At this point, the only thing that is likely causing issues is the version of the driver. So if the above is still not working, try swapping out the URL to be this instead: URL &lt;- 'https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.2/AthenaJDBC42_2.0.2.jar' The difference is the first URL (your original one) is the older 1.1.0 version of the .jar, and the one directly above this comment is the 2.0.2 version of the .jar. However, both should be compatible with Java 8.
No dice. I don't get it. I've never been so stumped on something. Any other ideas? 
if you want to make a post with the .rmd, you need to create\_post("title", ext = ".rmd"). Then after you finish the post, you need to call the "serve\_site()" command. If you forget to call the serve\_site, it'll still work in the browser, but it won't work when you push it all to your github. So remember to call serve\_site. 
You don't want Netlify to host the .Rmd files, you need it to host the generated HTML files that are created when you build the website. 
Right, but See that's the thing: the html files for my rmd ones *are* in the public folders
Sure, I'll serve the site and it'll look correct in the viewer but when I commit and push it and deploy it, netlify doesn't include any of my pages that have come from an rmd file. The output on netlify says 13 pages when there should be 16 like it says in the console (I have 3 test rmd files).
when you encountered this problem, did you accidentally already have .md files created? I think if you have a .md file in the public folder then those will get read by default instead of the html files created by hugo... I prefer the blog posts as I write them and not the markdown formatted ones. However anytime I have ever accidentally clicked "knit" it got displayed on my site and the html files created by serve\_site did not get displayed and i had to delete the .md ones from my folder and try again the right way before it worked. If so delete the .md ones from your public folder, reopen the .rmd and serve\_site again. then handle the commit or push (Not sure how git handles a push with files deleted from local repo that are uploaded to the master branch) 
No there's no .md files anywhere except tyre one I created as an .md to test. I created a new post as an .rmd file, served the site, site shows .rmd as html locally, I can click, looks good, etc. These files are both in content/post and publicn I commit, push, and deploy thru netlify and anything related to my .rmd files (that have been converted to html) does not show up
after serving site you also committed/pushed to GitHub right? and GitHub repo also doesn't have those .md files in there? and also do other new posts show up? (IE, its only .rmd / html posts not updating?) did you change the netlify name but forget to update the config? That's all I have for ya in regards to troubleshooting... Good luck!!! You might also check if you need the preview version of R Studio.
Mudskipper, do you know of a way to restructure my original dataset, which counts the number of days between events for each person in ascending order, to have the EventCount variable show the number of days between events in DESCENDING order? So instead of: EventCount = c(NA,NA,0,1,2,3,0,1,NA,0,1,2,3,0,1,0) EventCount = c(NA,NA,0,3,2,1,0,1,NA,0,3,2,1,0,1,0)
Sure, it will be much messier though: library(tidyverse) dat %&gt;% mutate( grp= cumsum(is.na(NewEvent) | NewEvent==0)) %&gt;% group_by(Person, flag, grp) %&gt;% mutate(NewEvent = rev(NewEvent)) %&gt;% ungroup %&gt;% select(-flag,-grp) # # A tibble: 16 x 3 # Person Day NewEvent # &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; # 1 1 1 NA # 2 1 2 NA # 3 1 3 0 # 4 1 4 3 # 5 1 5 2 # 6 1 6 1 # 7 1 7 0 # 8 1 8 1 # 9 2 1 NA # 10 2 2 0 # 11 2 3 3 # 12 2 4 2 # 13 2 5 1 # 14 2 6 0 # 15 2 7 1 # 16 2 8 0
Please use x-posting features of reddit and do not do what you have done here.
noted and apology 
Heyo, sorry for the delay. Unfortunately, I don't know what else to try here. The best suggestion I'd have is to reboot your remote machine, reinstall those packages (rJava and RJDBC + dependencies), and try running the above script again. It works on my end, and should work in yours if everything is configured properly. Sorry this has been so rough for you!
The age old question. In short: don't do this, put your data in long format then map color to a column. https://stackoverflow.com/questions/10349206/add-legend-to-ggplot2-line-plot
Seems like an extreme response. This isn‚Äôt stack overflow. I have no idea what the crossposting features are ( I use a smartphone app ), and I wonder if I would care to dig and find the features rather than just posting it in two places. Isn‚Äôt it just as good for you to post the link to the other subreddit as a comment?
R-3.4/bin/R.exe
thank you!!
Try global option for R: options(digits = 5) print(1e5) options(scipen = 3) print(1e5) 
R is capable of web scraping 
Thanks a lot
Are you sure this dataset fits in your memory? I recall getting stalls followed by console dumps when trying to read datasets larger than my RAM.
No, I'm not sure of that. I tried &gt;memory.limit() And got the output "8083". I don't know what that's supposed to tell me. The original .csv is 4.5gb. 
IIRC, memory.limit() tells you the maximum memory allocation, but you may not be able to use it all. I'm not completely sure, but I think you need to actually have enough free memory space at the time you import the dataset. Maybe take look at your computer memory usage (use task manager if you're on windows) and see if you have more than 4.5gb available.
How much RAM does your computer have?
The machine I'm working on has 8gb RAM
I tried increasing the allocation limit (with "memory.size()" but so far, no effect. 
So just to make sure, you do have 13 million *columns* of data, and not 13 million rows?
Yeah, it's 45 rows, with observations at 2400hz for about 90 minutes. 
Gotcha. So if you‚Äôre using: library(readr) your_df &lt;- read_csc(‚Äúfilepath.csv‚Äù) and it‚Äôs not working, can you read in half the columns at a time? The data will be in a compressed format once you actually get the data in R. So try to read in chunks at a time
Do you have more than 4.5gb free when you read this large table? Kill any unneeded processes and try reading the table again.
Thanks, this is the only thing going on at the moment, so I've done that. 
Good advice, I'll give that a shot. Thanks. 
If you have an control *at all* over the process generating the data, change it to a long format (i.e. one row per observation, rather than one column). Due to how data is stored and read into R, this will make reading the data a lot easier. That said, 4.5 GiB on disk might require more than twice as much memory in RAM, indicating that you‚Äôre going to have trouble no matter what (given 8 GiB of physical RAM). In that case, you‚Äôll need to access the data using memory-mapped files or an SQL database instead.
I don't know if this will help but I just did the intro to R on coursera and the teacher specifically said that because R has not ideal memory usage (loading everything into RAM), you generally need double the memory of what you think you need. Perhaps see if you can try to load the data into a machine with 16gb RAM? Obviously make sure it's a 64bit operating system as well. 
Thanks. I'm familiar with tidy data, unfortunately it's entirely out of my control. I'm looking into my SQL options. Thank you. 
Yeah, subsetting is an attractive option. Thanks for your help. 
I don't understand. "Observations" are usually represented as rows. You may not have control over it, but it does seem like an odd format.
Try a combination of data.table::fread, substring the rows, transposing, and saving as a feather or fst file for faster and easier use later on. 
Odd or not, it's rectangular, so it should be usable. 
Thanks; I'm not familiar with all of the stuff you mentioned. Is fread from the ff package? What is a feather? 
`data.table::fread()` is the function to read csv's, which is from the data.table package. It's exceptionally fast when compared to other csv reading functions. Feather and fst are both binary file formats which are much faster for reading and writing, and allow you to access subsets of columns when reading. 
Thanks, I'll look into this. 
Comparisons like these sucks. At least group similar packages together like dplyr with pandas, matplotlib with gglot2, etc. Using commits and contributors as a metric? Fail.
I just tackled this a few weeks back. This is the correct format (if I remember right...): sftp://username:password/some.server.com/path/filename.ext You may need to urlencode the password.
I am not szre it works but try this: Crate a folder, put the file in it it then dowload the folder insted of the file.
What does urlencode mean? 
This particular comparison is flawed in a number of places. The info graphic is not proportional, the categories are way off, and I'm not even sure the writer has experience with the libraries they are talking about.. 
 p + scale_x_continuous(expand=c(0, 0)) You should really post these types of questions on stackoverflow.com 
Just create your own function that breaks the output into multiple lines if it's too long.
Try setting your Chuck's to `asis=true`
Thanks! Just a head up if someone needs to do this. I had to use scale_x_date(expand=c(0,0)) since my X axis is a date type. 
https://github.com/jimhester/lintr for Static code analysis
lintr seems to be a great tool, but it doesn't give me any warning for code like `c(1, 2) + list(a = 1, b = 2)`. Maybe there are some options to catch exactly that, but I couldn't find them.
I‚Äôd love to have such a tool too, but the endeavour is doomed to failure from the outset in R. The reason is that a core feature of some important modern R libraries crucially depends on runtime evaluation. Namely, R implements Lisp macros indistinguishably from normal functions and uses non-standard evaluation of function arguments to compute on the language. Since *any* function can do this (at runtime), no function argument can be statically checked. Lacking annotations, the only solution for a static type checker would be to hard-code a list of known functions that do or do not use non-standard evaluation, and additionally impose restrictions on how names can be reassigned (due to dynamic dispatch of S3/S4 being predicated on a value‚Äôs type). I fear that this would lead to prohibitively large false positives *and* false negatives that effectively render the result of the type checker useless.
&gt; it doesn't give me any warning for code like &gt; `c(1, 2) + list(a = 1, b = 2)` It can‚Äôt, because this is completely valid R code. Nothing prevents me from overriding `+` at runtime and make it handle list arguments. In order to find the correct invocation for the `+` operator, the checker *must* evaluate the code ‚Äî no way around it.
types is a package that lets you add type annotation via its ? operator https://cran.r-project.org/web/packages/types/README.html
It's true that I *can* override `c`, `+` and `list`, but wouldn't the type checker, at least in trivial cases, be able to see that I *didn't*?
Both mypy and TypeScript are used with very mutable languages and quite successful. As for non-standard evaluation and dynamic dispatch, clojure.core.typed seems to work reasonably well with macros and multimethods -- but R fexprs are not really macros and may be more difficult, I haven't thought enough about it.
This looks nice. Are there tools that understand this notation?
TypeScript‚Äôs type checker only works for annotated and inferrable code. It mostly gives up for normal JavaScript (beyond very basic linting). Same for mypy: it relies crucially on Python‚Äôs new type hints. There are similar projects for R but they require changes to the core language and don‚Äôt seem to have gotten very far.
What trivial case do you have in mind? *Any* function call can modify that code. This isn‚Äôt hypothetical. In fact, for a very long time I had overrides for `+` and `:` in my `.Rprofile` to enable string concatenation and interval ranges (e.g. `1 : 10 : 0.5`). I‚Äôm also routinely using a module that‚Äôs overriding the `&lt;-` operator to define lambdas rather than perform assignment.
I am not sure what you are asking for but you might want to check out the R4ds book (free online, Google R4ds) and no see chapter 5 and 13.
You need to paste the exact code you‚Äôre using and, if you really want help, an example of the data you created that acts like the real thing.
Look into cosine similarity, Euclidean distance, collaborative filtering, neural networks. Sounds like there are 2 sides to your project... the recommendation logic, and the service (REST). Technically, you can do it all in R, but R may be better suited for prototyping. That, said, I‚Äôve never served recommendation output using R.
Yeah, that'd be probably very hard. Still, if the type checker has access to `.Rprofile` and loaded packages, it surely would be able to see whether `+` is overridden or not, unless there is something really weird going on (like overriding or not overriding it depending on date/time or user input, which, of course, is easily doable in R, but, at least I hope so, not common).
&gt; There are similar projects for R I would like to know more.
Anybody actually doing should check out Rob Hyndmans forecastHybrid package 
If you want more/better help I recommend putting more effort into your post. poliscidata is a separate R package `library("poliscidata")` `df &lt;- data.frame("party" = as.factor(nes$pid_3), "sex" = nes$gender, "gaymarry" = nes$gay_marry)` `table(df[which(df$sex == "Female"), c("party", "gaymarry") ])`
Sorry this kinda thing isn‚Äôt my thing. I don‚Äôt know much about coding so idk what would be useful to put. Idek why I have to use this program when I‚Äôm in a business major I‚Äôve only used excel til now:// what do details are needed
We're not sure what you're asking for (for instance, I thought I solved your problem with my code snippet). You suspect female democrats are more supportive of gay marriage; more supportive than what group? How will you test your hypothesis?
More supportive than democratic males, I haven‚Äôt tried your code yet because I‚Äôm at work for 3 more hours. And to clearify I‚Äôm having a problem putting the female /male(gender) with political standing (pid3) as to graph the male/female dems with approval of gay marriage.
change the third line of my code snippet to "plot(table(df))" and it will give you what I think you want (for all combinations of your data). If you want to just see the democratic party you can select only the parts of the dataframe that you need. plot(table(df\[which(df$party == "Democrat"),\])) Depending on the level of the class though you might need a more quantitative method of testing your hypothesis.
What... you couldn't read his mind? 
&gt; Idek why I have to use this program Business intelligence is increasingly moving away from excel and into statistical programming languages.
You might want to look into [OpenCPU](https://www.opencpu.org/) or [Plumber](https://www.rplumber.io/) for the API. 
You‚Äôd still need to *execute* the code, parsing isn‚Äôt enough. To illustrate, here‚Äôs a rough draft of what my redefinition used to look like: ~/.Rprofile ‚Üí ~/.config/R/init.r local({ profile_env = new.env() local_r_path = '~/.config/R' config_path = c(local_r_path, if (interactive()) file.path(local_r_path, 'interactive')) source_files = dir(config_path, pattern = '\\.[rR]$', full.names = TRUE) source_files = source_files[- grep('/init\\.r$', source_files)] for (source_file in source_files) { try(eval(parse(source_file), envir = profile_env)) } }) ~/.config/R/iteractive/concat.r `+` = function (e1, e2) UseMethod('+') `+.default` = .Primitive('+') `+.character` = paste0 In other words, whether `+` refers to `` base::`+` `` or something else is essentially *impossible* to ascertain without *executing* the code in my `.Rprofile`, even without using artificially weird code.
This definitely sounds like a Hugo question. I'd start by looking at [the docs](https://gohugo.io/documentation/) over on their page. To start, I'd personally have a random/ folder in my public/ directory, and an index.html file with a javascript redirect there. The tricky part is getting to the list of pages on your site from which you'll pick one at random. Maybe check into reading in your sitemap.xml and pulling out all &lt;loc&gt; tags without an associated &lt;priority&gt;0&lt;/priorirty&gt; value? 
When I type in that one right there I get a plus sign what does that mean
Never mind I messed up thank you so much sir you are a god
It means you left off a closing bracket/paren