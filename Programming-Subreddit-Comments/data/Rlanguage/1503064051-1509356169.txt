This so needed to happen. It would be so good to get the ability, for example, to also "build up a DESCRIPTION file" with packages that were installed (a la `npm install`)
Tried it, [didn't go well](http://i.imgur.com/iR7VAVW.png)
It will be really hard to help you with this without a reproducible example. Make the smallest amount of code and toy data that you can that will demonstrate the problem you're having - then we can play around with the object to find what's happening. You should also show your code because it's possible you're making some elementary error like a typo or something.
I get it - but in general, any question that doesn't at least have a reprex will be a slog. And you don't want to make life more difficult for strangers upon whose kindness you will rely.
I've found for myself the following youtube channel: https://www.youtube.com/playlist?list=PL8eNk_zTBST8olxIRFoo0YeXxEOkYdoxi which is the best source I've found so far, and thought it would be good to share here. I will be keeping an eye out for other sources as well.
Check out Tidy Text Mining also, it's more of a data wrangling book for NLP, but those skills are also useful.
This book might be useful: http://hbanaszak.mjr.uw.edu.pl/TempTxt/(Quantitative%20Methods%20in%20the%20Humanities%20and%20Social%20Sciences)%20Matthew%20L.%20Jockers%20(auth.)-Text%20Analysis%20with%20R%20for%20Students%20of%20Literature-Springer%20International%20Publishing%20(2014).pdf
https://paulvanderlaken.com/2017/08/10/r-resources-cheatsheets-tutorials-books/
I figured it out so answering this in case it ever helps someone: You just need to set IDs = TRUE as a parameter when creating the hexbin, which saves bin IDs as @cID: hbin &lt;- hexbin(SiteData1$X, SiteData1$Y, xbins = 100, IDs = TRUE) SiteData$BinID &lt;- match(hbin@cID, hbin@cell)
Could you describe this functionality in more detail? As I understand NPM, it will install packages to a project-local library unless you ask it specifically for a global install (`npm install -g`). That's like what [Packrat](https://cran.r-project.org/package=packrat) does. Not sure if that's what you had in mind.
I almost did it that way! Quick and dirty. But it seemed silly to write a Bash script to call R functions when you could just write an R script. That way you get to avoid the nightmare of trying to ensure portability across shells (Bash, Ash, Dash, Ksh, Zsh, ...)
The particular feature I want is like npm install --save but for r packages. The specific version in is an interesting issue though. Maybe integration of packrat into your tool
I see, thanks! There's definitely a lot of opportunity for integration with Packrat, Roxygen2, and the like here. Specifically there is an old package called [Rbundler](https://cran.r-project.org/package=rbundler) that might be worth forking and reviving to support Rpkg. There is also a `DESCRIPTION` file manipulation package called [desc](https://cran.r-project.org/package=desc). I added some of these packages to the `TODO` file in `Rpkg`, to evaluate where and how Rpkg fits in with all this stuff.
Looks like you just need a regression model. Look into basic linear modeling in R with lm(). Best of luck!
Least squares regression.
Least squares regression (onto some function, likely linear in your case).
four spaces at the start of each line format it as code, you might also want to use more vectors and vector operations or for loops etc to reduce the repetitive code Regarding your problem, ggplot uses "long" format data.frames. It basically assumes that each data.point you want to plot is in a single column and the other columns give informations such as asis vs asis2. That said I never used it to extensively, so there might be a trick to get around it. Generally, the reshape package (or a reshape function in another package, not quite sure) allow you to convert between "wide" and "long" format data.frames
Generally: (with caveats, of course) my_model &lt;- lm(stuff ~ other_stuff, df, other_arguments) predictions &lt;- predict(my_model, new_df, other_arguments) Do ?lm, ?predict
Thanks. I thought it was the data type. I have seen melt() being used prior to calling ggplot. Will try find a method to achieve what I need. 
It's hard to imagine the hat your code looks like without any provided example but the easiest way would be to have some debug flag that toggles the file writing off. 
I'm not able to test now, but I think creating such a function may cause scoping problems that would be annoying to handle, i.e., all your objects created in the script wouldn't be saved to the global environment. I'd use a logical debug flag, and test it to run or skip the write commands. For example: dbgMode &lt;- T If (!dbgMode) { #stuff } 
This is a great reference thanks!
If I were you, I'd start getting out of the mindset of having "scripts". That's the easiest way to avoid this being an issue. Even if your script files end up looking like this: the_script &lt;- function() { #all the code that was your script before } the_script() This is a useful exercise, for a number of reasons. The most important in this scenario is that it helps you get into the mindset of writing reusable code and making small functions that do a small number of things, but the second reason is that you make it really easy to add switches to your code: the_script &lt;- function(write_to_disk = TRUE, messages = TRUE) { #all the code that was your script before if (write_to_disk) { #some optional code pathway that writes to disk } if (messages) { #some optional code pathway that makes messages #though note that you should use message for messages in real use cases #your users can then use suppressMessages to remove them } } the_script(write_to_disk = FALSE, messages = FALSE) One of the other commenters noted that this will prevent you from writing to the global environment. I would actually consider that a benefit since you should almost never need to do that.
&gt; all your objects created in the script wouldn't be saved to the global environment I would actually consider this a benefit ;)
Look at lm()
Would you mind explaining why, or pointing me to a persuasive argument to avoid the global environment? I haven't worked on any massive projects and haven't had a compelling reason to avoid it, but I've seen similar sentiments, and am curious what's "wrong" with using the global environment.
In general, if you have lots of variables in a namespace, it increases the chance of you accidentally using the wrong one somewhere and it going unnoticed. Other reason would be that R keeps memory around for objects as long as they are in some scope. Generally when you are doing some processing, you create a bunch of temporary copies of things at intermediate steps. If these are wrapped in a function, then that space can be freed up when the function is done because you can't access those items anymore. If they are all in the global environment, they just stick around, and your session will consume more and more RAM. Not a problem if your data is very small, though.
I appreciate the response, but that seems more like an argument for using functions, which I do use extensively. But I still save (some of) the results of those functions to the global environment, while the other commenter was recommending wrapping the entire script in a function. For OP's question, if writing files and messages were turned off through this function, it seems that the only evidence that anything ran at all would be any displayed plots. This seems undesirable to me given the way I've used R, and I don't understand the advantage of this style of debug mode when there's nothing to debug after the script execution. I realize you aren't /u/fang_xianfu, but if you have any counterpoint, I am still interested to learn some additional functionality or analysis approach. 
You got a very practical answer from /u/Deto, so I'll give you the philosophical answer: you almost never need to do it. If you're encountering a situation where it's problematic (such as the situation OP is describing where he wants to be able to define optional code pathways and a way to choose which pathways are followed) then the right thing to do is almost never to try to make it continue to work with global environment assignment but rather to use existing functional programming paradigm instead. It's bad practice insofar as it holds you back from being able to do that. In OP's particular case, the reason for that is that functions are designed precisely for that use case of different pathways and execution strategies. That's exactly what they're for. If you use functions you get useful features like default values, the ability to use restarts and error handlers to recover from conditions, and many more useful language features that mesh well with that idea of being able to control what happens. I accept that there are some cases where global environment assignment is the easiest thing to do. But as soon as that stops being the case, philosophically you shouldn't feel too attached to it, and you need to be able to throw it away and functionise your code easily.
My response to /u/Deto: &gt; I appreciate the response, but that seems more like an argument for using functions, which I do use extensively. But I still save (some of) the results of those functions to the global environment, while the other commenter was recommending wrapping the entire script in a function. For OP's question, if writing files and messages were turned off through this function, it seems that the only evidence that anything ran at all would be any displayed plots. This seems undesirable to me given the way I've used R, and I don't understand the advantage of this style of debug mode when there's nothing to debug after the script execution. In particular, for your recommendation to OP, do you have an implicit redesign to have the wrapper function output a list rather than individual objects, or is your thought to just make sure it runs without an error when the flags disable file output? I just don't see the value in wrapping a function around the whole script if it doesn't save *anything*. 
Yes, of course you want the function to return something useful if it makes sense for it to do so. It's the difference between: #do a lot of things to calculate the value a &lt;- value and calc_value &lt;- function (...) { #do a lot of things to calculate the value return(value) } a &lt;- calc_value(...) It's not a large difference in itself but in terms of how you approach the problem, it's very different. It's not particularly clear if it makes sense for OP's function to return something since the only thing he explicitly says it does is write some files, and that's what he wants the option to deactivate. In those situations, I usually have my function return `invisible(TRUE)`, but usually there is something worth returning.
Good stuff - thanks for sharing!
I feel obliged to say that if you didn't really enjoy learning math, data analytics might not be for you. That said, as a data analyst without an expansive background in math, there are great opportunities to learn. I've taken a number of EdX classes and found them very useful, off the top of my head there was one from MIT called Analytics Edge that was a good introduction to regression analysis. There's another titled something like Statistics For R that's good. If you're not familiar with Excel there are some good courses for that on EdX too. Plan on doing a lot of self-teaching and learning on the go as well, that's part of the fun!
As a former math teacher turned data analyst I would suggest just going to Khan Academy and doing all the math pre-cal through the end of Calculus as well as statistics. If you eventually decide to get into making AI you'll need to go further, but there is really no need for 90+% of analytics jobs. When you first start running statistical tests in R you will come across a lot of unfamiliar variables and terms. However, it's easy enough to look them up as you come across them and it's easier to remember them when you are actually using them for a practical purpose. There are also a bunch of models to learn, but I would either learn that in a data analytics course or just tackle them as you come across them.
I suspect that the issue is with trans: you have " " as the pattern instead of the replacement. I usually replace special characters before going into the tm package. the `stringr` package is pretty nice because it has a consistent order of arguments - I'll use that here. library(stringr) texts &lt;- c("Just words", "Sp#c!a| Ch&amp;r@ct3r$") # This one removes special characters and leaves numbers alone str_replace_all(texts, "[^[:alnum:] ]", " ") [1] "Just words" "Sp c a Ch r ct3r " # This one removes everything that isn't a space or an alphabet character str_replace_all(texts, "[^[:alpha:] ]", " ") [1] "Just words" "Sp c a Ch r ct r " The "[\^[:alpha:] ]" bit is called a regular expression. They're generally a pain in the butt, but they are incredibly useful for problems like this where you don't want to list out every possible special character. This one says "match anything that isn't a letter or a space"; then `str_replace_all` replaces those matches with spaces. 
&gt; I feel obliged to say that if you didn't really enjoy learning math, data analytics might not be for you. That's a fair enough point. I would counter that, at least for me, the reason I hated math (s) in school was the lack of application. Math without application is great... for people who enjoy math. It's like a "fun puzzle" (as my parents/teachers used to say). To me, it was just a bunch of abstract (well, not abstract, but otherwise unrelated to anything I could conceive of at the time) concepts. Now that I've put some years on, I'm into amateur electronics, DIY projects from woodworking to blacksmithing. Building racing drones. Now I've got nothing but curiosity and interest in advanced mathematical concepts. Instead of just "solving for x", I'm trying to figure out the power curve of a given motor, at a given load, for a given amount of current. While your point is likely accurate more often than not, there are a LOT of people who gave up on math early on, but come back to it out of necessity later on.
This would be easier to answer if you gave more in the question but will give it a stab anyway. pnorm gives you the Pr(X &lt; x) so effectively gives you the quantile given the z score. pnorm(0) therefore gives 0.5, pnorm(1.96) approx 0.975 the qnorm function is the inverse of pnorm. You pass a quantile and get back a z score. So qnorm(0.5) is 0, qnorm(0.975) approx 1.96. apologies for naff formatting with code etc, on my phone so not the easiest to type out. 
Kind of a hack, but you could join the strings together using a character that won't be in them, then call your function, then split again.
A simple solution would be to define another function that wraps the call to vapply 
See https://github.com/gaocegege/Processing.R for more details
I have to say, that’s a very unnatural API for R. Modifying objects isn’t how you’d use a functional programming language, and the resulting code is unnecessarily verbose. Case in point: why does the `oscillate.Cell` code need to modify its argument, rather than being able to return a new value on the fly? From a functional language I’d expect an API such as this: oscillate.Cell = function (cell) { with(cell, angle = angle + 0.2) } Or, using a streaming notation: oscillate.Cell = function (cell) { cell %&gt;% with(angle = angle + 0.2) } (This reuses the existing `with` function in a maybe confusing way; `within` is closer in meaning but sounds awkward. The point here is merely that a functional programming language eschews side effects, and assignment is a side-effect.) Furthermore, the API seems to largely rely on side-effects (beyond assignment) rather than function values. That’s another very un-R-like aspect. Your Github project says: &gt; Processing.R supports: native R programming and syntax But as explained, I wouldn’t call that native: it’s not how I would ever write R code.
Can't really comment on the global environment thing as right or wrong - the way I handle this is a cron job bash script which activates an R script with params: (cd /mnt/path/to/dir/here &amp;&amp; ./compile_script.R daily) exit Which initiates the below: #!/usr/bin/env Rscript args = commandArgs(trailingOnly=TRUE) if (length(args)==0) { stop("At least one argument must be supplied (input file).n", call.=FALSE) } else if (args == 'daily') { part.daily &lt;- TRUE part.weekly &lt;- part.monthly &lt;- part.mtd &lt;- FALSE } else if (args == 'weekly') { part.weekly &lt;- TRUE part.daily &lt;- part.monthly &lt;- part.mtd &lt;- FALSE } else if (args == 'monthly') { part.monthly &lt;- TRUE part.daily &lt;- part.weekly &lt;- part.mtd &lt;- FALSE } else if (args == 'mtd') { part.mtd &lt;- TRUE part.daily &lt;- part.weekly &lt;- part.monthly &lt;- FALSE } else { stop("Please specify a valid report period.", call.=FALSE) } And then I set all my run blocks according to triggers like this: if (part.daily | part.weekly | part.monthly | part.mtd){ #do stuff here } And then because each script is run as part of a terminal instance, global env is cleared on exit. Or, for debugging, I remove the exit operator and then I have the global env available to dig into and find out what caused the issue. In your circumstance, build your script something like: #!/usr/bin/env Rscript args = commandArgs(trailingOnly=TRUE) if (args == 'write') { #write files lines here } else { } #rest of script here And then execute from terminal / cmd like: ./compile_script.R write OR ./compile_script.R 
Hi, thanks for your reply. The project is based on renjin, which is the JVM interpreter for R. And it do supports native R syntax in design. The example may be not good enough, since I am not familiar with the object-oriented programming in R, I will try to re-write it. Thanks for your comments:)
You should always produce the code used to create your data. My guess is you created a data frame like this: dataSet &lt;- data.frame(x = rep(letters[1:25], 4)) Unfortunately R saved the column of strings as a [factor](https://www.stat.berkeley.edu/classes/s133/factors.html). You can fix it with this: dataSet &lt;- data.frame(x = rep(letters[1:25], 4), stringsAsFactors = FALSE) Also, a general coding comment, there's no need to do the `n &lt;- n+1` in both the `if` and `else`. Closing braces shouldn't be at the end of a line. n &lt;- 1 while (n &lt; 100) { if(dataSet[n, 1] == "a") { dataSet[n, 1] &lt;- "A" } n &lt;- n + 1 } Edit: From below there are several solutions which are vectorized and cleaner in R. Generally you don't need to loop through an object in R. library(dplyr) library(microbenchmark) dataSet &lt;- data.frame(x = rep(letters[1:25], 4), stringsAsFactors = FALSE) capitalize_a_loop &lt;- function(dataSet) { n &lt;- 1 while (n &lt;= nrow(dataSet)) { if(dataSet[n, 1] == "a") { dataSet[n, 1] &lt;- "A" } n &lt;- n + 1 } dataSet } capitalize_a_dplyr &lt;- function(dataSet) { mutate(dataSet, x = recode(x, a = "A")) } capitalize_a_vectorized &lt;- function(dataSet) { dataSet[dataSet == "a"] &lt;- "A" dataSet } microbenchmark( capitalize_a_dplyr(dataSet), capitalize_a_loop(dataSet), capitalize_a_vectorized(dataSet) ) From the results of this, the `dplyr` solution is slowest but the base R solution is significantly faster than the other two. Unit: microseconds expr min lq mean median uq max neval cld capitalize_a_dplyr(dataSet) 4913.873 5214.9655 5769.9387 5447.552 5983.898 8670.961 100 c capitalize_a_loop(dataSet) 3008.050 3286.3755 3824.3378 3395.081 3669.509 9129.573 100 b capitalize_a_vectorized(dataSet) 150.957 222.5375 284.6348 252.483 282.223 3547.063 100 a 
If x isn't the right class, then x=="a" might return NA, which cannot be evaluated in the IF statement. Also, you could vectorize this. rows2change &lt;- dataset[, 1] == "a" dataset[rows2change, 1] &lt;- "A" 
So first of all, if this example is close to what you are actually trying, this is not the most 'R' way of doing things. As a (overly simplified) general rule of thumb, loops are the last choice in R. Sample dataset: dataSet &lt;- tibble( x = c('a', 'A', 'b', 'a', 'c'), y = c(1, 4, 4, 7, 8) ) In base R to do the same thing I'd do: dataSet[dataSet$x == 'a', 1] &lt;- 'A' or simply: dataSet$x[dataSet$x == 'a'] &lt;- 'A' Dplyr has a recode function: dataSet &lt;- dataSet %&gt;% mutate(x = recode(x, a = 'A')) There would be a way to do it with purr, I'm sure there are a bunch of other solutions. If you are just trying to do loops though, my guess is dataSet is not 100 rows long so its working, just erroring out when there are no more conditions to check. The following code works on my sample dataset: n &lt;- 1 while (n &lt; nrow(dataSet)){ if(dataSet[n, 1] == "a") { dataSet[n, 1] &lt;- "A" n &lt;- n + 1 } else { n &lt;- n + 1 } } 
It would help to know the end result you are trying to accomplish. From your code it looks like you're trying to make the letter 'a' uppercase. If you want all of the letters uppercase try: dataSet[,1] &lt;- toupper(dataSet[,1]) Or just the letter 'a': dataSet[,1][dataSet[,1] == 'a'] &lt;- toupper(dataSet[,1][dataSet[,1] == 'a'])
Several small things in addition to the existing comments: 1. `if` statements are *not* loops. It doesn’t make sense to speak of “`if` loops”. 2. You’re right that this looks like C code rather than R code. So here’s some advice for R: 1. Be wary of mutating statements; stuff like `x &lt;- x + 1` is almost never needed, and almost always a bad idea. The exception (potentially) are statements that modify data.frame or array subsets (such as in your case when assigning the values of the first column of `dataSet`). 2. Don’t use loops unless actually necessary, and use the appropriate loop. Unbounded (= `while`) loops are virtually never a good idea. If you iterate over stuff, use a `for` loop. Better yet, don’t use a loop but rather either vectorisation or vector application functions (`lapply`, `sapply`, `Map`, etc). As mentioned in the existing answers, R already vectorises assignments and tests, so you can perform the assignment directly rather than inside a loop.
&gt;If it's all in the 5th column, I still do dataSet[,5][dataSet[,5] == "Nev"] &lt;- "Nevada", right? Right. For the NA's you can subset the data to remove the NA's first (using `subset()` or `na.omit()`). You might want to make the whole column the same case (upper or lower) as well. If your data has several abbreviations for all of the states using `unique(dataset$columnname)` or `unique(dataset[,columnnumber]` will at least let you know how many different abbreviations you have.
In this case I would check out the dplyr function [case_when\(\)](http://dplyr.tidyverse.org/reference/case_when.html) 
 rowThatAreNA &lt;- is.na(mydataframe$mycolumn) mydataframe$mycolumn[ rowsThatAreNA ] &lt;- 0 
Have you tried to install from R not RStudio? That's the first I try whenever I get install error. After you installed from R , you can load from RStudio.
The documentation on install.packages supplies some information on installing from local binaries, maybe that can help you on your way.
If I remember correctly (answering on phone not laptop) `install.packages("path/to/file", repos = NULL)`
This doesn't automatically install dependencies as well. That's what I need. Also, the exact syntax to specify the "path/to/file" is a bit unclear. Slashes, double slashes, adding something like "file://" in front, I haven't been able to figure it out.
It's literally just the path to the file. Like: install.packages("C:/Users/fang_xianfu/packagefile.zip", repos = NULL) on Windows or install.packages("/home/fang_xianfu/packagefile.tar.gz", repos = NULL) on other systems. If you're having trouble with the slashes, you'll have the same trouble every time you need to use a file path in R or practically every other language for that matter. In file paths it's easier to use forward slashes for everything, even on Windows, and then you only need one of them. With regard to dependencies, automatically installing dependencies locally isn't possible in the same way that it will from CRAN. But you can only install packages locally that you've already downloaded, so why do you need to do it automatically? Just install everything that you've downloaded. You can use `list.files` to return a list of files and then install them all.
If you do it this way, you have to add another instruction for each thing you want to change. This is a lot of duplicated code. And conceptually, the assignments of abbreviations to full names seems more like data; you might for example want to store it in a spreadsheet later. Another way is using named vectors as a map: statemap &lt;- c("NV"= "Nevada", "Nev"="Nevada", "NEVADA"= "Nevada") # and so on dataSet[,5] &lt;- statemap[dataSet[,5]] # or, if you want to keep entries that are not explicitly listed in the map dataSet[,5] &lt;- ifelse(dataSet[,5] %in% names(statemap), statemap[dataSet[,5]], dataSet[,5]) this makes it very easy to add new mappings, and if the column index changes you only need to edit a single line. (And in R it's usually preferred to use column names rather than indexes, as it's easier to read, e.g. `dataSet$state` instead of `dataset[,5]`)
Thanks. But here's the thing. Suppose I download Rcmdr. This requires a whole lot of other packages to run properly. I don't know which ones are needed. (I should mention that my work computer has a good internet connection. So I have a USB stick with a snapshot of all the binary packages on it.) So I install Rcmdr. Then I try to load it. It gives me an error message with a list of packages to be installed. I install them and try again. Now I get a fresh list. And so on.
I'm still confused - I think maybe you mean your work computer *doesn't* have a good internet connection? Do you mean that you downloaded every single package from CRAN and put them on a USB stick? If you did that already, why not just install them all and save yourself some bother?
Sorry, your original question did not specify that is what you need. You could achieve this sort of thing using a package called miniCran. Effectively you create your own version of the CRAN repository with whichever subset of packages you care about. Obviously you would need somewhere with an Internet connection first before installing on a different machine from local file. With regard to file paths, if you are ever unsure about file paths you can right click and go to properties on an item in your windows file browser. Be aware however that windows has a horrible habit of using backslash in it's paths where it should be forward. So you could copy the path from properties then switch \ for / 
You'll probably want to use the and operator (&amp;) to select the rows. try: wanted_rows = which(example_data.frame$Age == 21 &amp; example_data.frame$Gender == "M") example_data.frame[wanted_rows,] 
Base: df[df$Age == 21 &amp; df$Gen =="M",] Assign to variable if you want to keep them. Look at dplyr package. It will simplify tasks like this and allow you to stack them in one line of code. install.packages("dplyr") df %&gt;% filter(Age ==21 &amp; Gen == "M") Dplyr essentially lets you manipulate your data with 5(?) Main verbs or commands. For the gen variable you may or may not need the quotations depending on what class the variable is, eg if its char. 
Thanks for the note on dplyr. I only recently started to explore its shortcuts. This works perfectly.
Thanks! I was too used to &amp;&amp; in C++. This is perfect, though. which() is also a new function for me, so thanks for that.
This is a link for a wrapper of Fasttext. It performs both state of the art text classification and generates word reprensentations. It also includes some features not available in the original command line client.
You can check out the source code for the plot method to make sure it's not overriding your margin settings by typing plot.dendrogram. You are using base r for plotting, right? (Why load ggplot?) Hope that helps!
You'd basically just want to define a distance metric that, say, takes a difference in column one as being worth +10 and a distance in column 2 as worth +5 or something like that.
Yes, use df[, colname].
There are three syntaxes that do this and they differ in what they return: df[colname] df[[colname]] df[,colname] If your dataframe is a tibble the returns will be slightly different.
Thank you
When importing, set `stringsAsFactors = FALSE`
Perhaps opening the .csv file in a text editor will clarify matters. For example, if Excel saved the numeric columns with quotation marks around it, that might explain why R read it as text.
Better yet: use readr::read_csv for full control over your column types.
I definitely support this. The Tidyverse and Readr make life much easier 
You're missing a dependency it's trying to link against when building itself. Make sure you install all dependencies first before installing ffanalytics: install.packages(c("reshape", "MASS", "psych", "Rglpk", "XML", "data.table"), dependencies=TRUE) I've got to say though, the number of dependencies for this package is absurd. You could also try acquiring just TCL with devtools::install_github("tcltk/tcl") 
Try a different method inside install.packages(). Without knowing more about your system it's hard to say what the problem is.
 &gt; vec = c(1,2,3,4,5,6,7) &gt; vec[a] = "b" &gt; vec [1] "b" "2" "3" "b" "b" "b" "7" This is what I got, I don't want to replaces the values that are already there? how do i do it? I am new R, so I am kinda stuck at this place for quite a while now.
Sorry, I misread your question at first. Here is an ugly way to do it vec = c(1, 2, 3, 4, 5, 6, 7) a = c(1, 4, 5, 6) library("stringr") for(val in a){ vec[val] = paste0(vec[val]," b") } vec = paste(vec, collapse = " ") vec = unlist(str_split(vec, " ")) 
a cheaty way to do it would be to make dfs' with a column of indices, then insert decimal values (so like 4.1 would land you between 4 and 5), and then you could bind the two dfs' together, then you could sort by the indices column, then remove it. also remember to set stringsasfactors=false when ur making dataframes, since that could be an issue in the future
this worked perfectly thanks
I'm intrigued for what you would need it, but something like this should do the trick: a &lt;- c(1,a) # for the first section c(unlist(lapply(1:(length(a)-1), function(i) c(vec[a[i]:a[i+1]], b), vec[a[length(a)]:length(vec)]) EDIT actually LoveFromTheLoam's approach is also good vec2 &lt;- as.list(vec) vec2[a] &lt;- lapply(vec2[a], 'c', b) unlist(vec2) 
Two articles in this matter: [Python vs. R: The battle for data scientist mind share](https://www.infoworld.com/article/3187550/data-science/python-vs-r-the-battle-for-data-scientist-mind-share.html) [Python vs R. Which language should you choose?](http://byteacademy.co/blog/python-vs-r/) Comparison of R and Python for data analysis: [Python vs R: head to head data analysis ](https://www.dataquest.io/blog/python-vs-r/) [Pandas Vs data.table](http://datascience-enthusiast.com/R/pandas_datatable.html)
Thanks. Unfortunately, none of those worked. It still produced the same error on: devtools::install_github(repo = "dadrivr/ffanalytics") 
It's hard to know without knowing your use cases. I regularly use both Python and R in an academic environment, and I see them as having strengths for different types of work. I prefer R for: * Working with data frames * Plotting * Parametric statistical inference where confidence intervals are important I prefer Python for: * Working with text data * Machine learning * Interfacing with web APIs * Working with network (e.g. social network graph) data * Writing custom code (as opposed to using someone else's packages) The tidyverse in R is wonderful and hard to beat. Python has no equivalent. Once you get outside of tidyverse type things, R native code is substantially slower than Python. Plus, I find Python's language features make it easier than R to code in.
Well yes, they're used together all of the time. Both are packaged as part of Anaconda for a very good reason... their synergy is excellent. Python is very good for the general purpose stuff and data preparation (with the added benefit of numpy, matplotlib, and pandas), while R is great for the analysis and presentation. You can typically do with one what you can do for the other but they're definitely designed for slightly different things with a good amount of overlap. Python's syntax is very easy. You'd be doing yourself a favor by getting into it.
Trying to work with all kinds of data (for finance purposes) and apply machine learning/ANN's. But I need to be able to work with text (I've been using tm and quanteda for that so far), geospatial and currencies for the most part. So kind of looking for the best solution. I guess it's just kind of hard for me to see where the R should stop and the Python should begin. But from what you're saying seems like generally R is better for pre-processing/visualizations then Python is better for everything after that? 
&gt; Working with text data You may find this useful: http://tidytextmining.com/
Yep, I own the book. I still don't think it's at Python levels yet, although R has improved remarkably in the last 2 years.
It depends on what your pre-processing looks like. If you are handed a data frame, then yes do the preprocessing in R. I am often using web APIs, so I'm doing the data collection in Python and then moving to R once I have something cleaner. Not the only way to do it, but it's what I've become comfortable with over time.
Thanks! A lot of good info here. So one last thing. Being R dominant what would you say is the best method for integrating Python and R for a beginner? I've gotten pretty comfortable/familiar with Rstudio. Actually didn't even know about Anaconda...I was checking out rPython though
Can you verify whether tcl86.dll exists at the location specified in the error message?
Tidyverse is better than Python can offer in terms of working with and manipulating data. Anything you can do in R, you can still do Python.Python has much better performance in general coding tasks, but most computation modules in popular R packages are written in C/Haskell/etc anyways If you are working with parametric models, R can't really be beat. You can find all kind of exotic bio- and statistical papers implemented in R. Python has a more robust machine learning environment.
I'm not sure if this is below your level, but swirl() has some good exercises.
I'm still (mostly sort of) a beginner too. When it comes to integration and which tool to choose it is really down to the one you think could accomplish the task best. R, for me, makes it really simple to take data that I have and manipulate it. The way I use it it's more like SQL with pretty pictures, if that makes sense. Python is for if I need to do surgery or need a general purpose language. Once you're familiar with both it's more obvious which works better at what level.
It looks like it is: [Here is a screenshot](http://imgur.com/a/9CO9Z)
Not exactly a course, but [Text Mining with R](http://tidytextmining.com/) is a free ebook that complements the tidytext package, and one of the package authors, [Julia Silge](https://juliasilge.com/blog/), has a blog with many easily-readable examples of text analysis (e.g. reddit threads, tweets, the collected works of Jane Austen, etc).
This worked and I have no idea how this works though. I am a beginner.
I am working a exercise for practice. Following is the problem statement: Consider the following vector ‘tels’ which contains telephone numbers from “KANSAS”, “TEXAS” and “NEW YORK” regions. tels &lt;- c("510-548-2238", "707-231-2440", "650-752-1300", "510-674-3482", "510-853-5695", "510-882-9898", "650-555-6311", "707-885-6351", "650-231-1234", "650-096-0023", "707-691-6763") If the number starts with 510, the phone number is from “KANSAS”, if it is 707, then “NEW YORK” and if it is 650 then the number is from “TEXAS” Use R concepts and obtain the following dataframe as ouput. Expected Output: PhoneNumbers State 1 5105482238 KANSAS 2 7072312440 NEW YORK 3 6507521300 TEXAS 4 5106743482 KANSAS 5 5108535695 KANSAS 6 5108829898 KANSAS 7 6505556311 TEXAS 8 7078856351 NEW YORK 9 6502311234 TEXAS 10 6500960023 TEXAS 11 7076916763 NEW YORK any idea how to do this in a simple way?
 for(val in a) The for loop goes through a. val is the name assigned each iteration vec[val] Find the values in vec that == val = paste0(vec[val]," b") Assign that portion of the vector to itself, concatinated with a space vec = paste(vec, collapse = " ") Collapse it into a single character. vec = unlist(str_split(vec, " ")) Split the string by spaces. The last part feels kinda weird. Whenever I do a bunch of type conversions I later end up learning more straightforward ways. I would recomend running code you don't get one line at a time and use str() on the objects you're modifying.
Try hunspell library 
You want a named vector to map from nr to state (that is the most important idea for a nice vectorized solution), then you want to split or substring the nr strings to get a vector of the start numbers and "subset" the first vector by the new vector. Then maybe also gsub to get rid of the '-' in the numbers.
Any suggestions?
I will and I'll let you know, thanks!
Displayr has a free trial - you could just try it. You could also just write an interface with shiny, that shouldn't be hard.
so, ive never used shiny. woud this be hard to recreate? My base data set is 75k rows. by 20 columns. each row is a customer and each column is an attribute regarding the customer. My out put data set is 75k rows by 100 columns. A column for each product that we have. i have a third data table that is 3 columns by 1 mil rows. It is the spend by product for each customer. I pivot this to to a table that is 75k rows and 100 columns with the spend data in order: 1. i have paramaters that filter what customers qualify based on their atributes by product. I create a matrix of what customers qualify for what products off this 2. I create an estimated price for each customer for each product based on their attributes. i create a price matrix for each customer and product using this 3. I subtract the estiated price from their current revenue. to show if there is positive opportunity(if statement, if negative i 0 out) 4. I 0 out all product pportunity based on if the customer qualifies.
None of what you describe requires parameterisation, though. What are the choices you're modelling? You could just write R code to do everything you've described.
 Here's one way to do this: tels &lt;- c("510-548-2238", "707-231-2440", "650-752-1300", "510-674-3482", "510-853-5695", "510-882-9898", "650-555-6311", "707-885-6351", "650-231-1234", "650-096-0023", "707-691-6763") # Create data frame with phone numbers minus dashes output = data.frame(PhoneNumber = gsub("-", "", tels), stringsAsFactors = FALSE) # Function to map phone number to state ph_to_state = function(phone){ area_code = substr(phone, 1, 3) switch(area_code, "510" = "KANSAS", "707" = "NEW YORK", "650" = "TEXAS") } # Apply function and store result in a new column output$State = sapply(output$PhoneNumber, ph_to_state, USE.NAMES = FALSE) print(output) Cheers
so, before i embark down the path of shiny. Everything i said can be written in R code and then used in a shiny app with an interface for the end user to change values?
It really depends on who the end user is and what values they might have to change. If I was writing that for myself, I'd just write a function and call it with different parameters. Shiny could create a web front-end to let the less-technically-minded manipulate the inputs and see the outputs.
In figure 1, smooth only sees transport ~ earn, and calculates the regression of that. You added colours, but only to the points. In figure 2, you added colours for the entire plot; the effect was an implicit `group`, and smooth was calculated for each group (i.e. country). Smooth calculates the regression (slope *and* intercept) for each group independently. What you want is not one or the other, but a mix. So you *are* looking at a linear model, although not mixed. Estimate the fixed effects and *model* intercept prior to plotting it. To then plot it, do as fig. 1, but not with `geom_smooth`. Add a `geom_abline` with a separate data.frame for intercept and slope. 
I just made my first shiny app. I was shocked how easy it is. But as with everything, tidy data will make this easier. Shiny has two real parts, the ui and the server. The ui drfi es how it will look and what of the items will be used to effect the server. The server takes data and the results from the ui objects as inputs as a function. The output is graph or table info. You can do it. But for the end user do you want the final product as what? Like print to excel or what
this doesn't address your question but could you please eliminate that leverage point on the far left? It's really bugging me and it's screwing with your regression. :) 
Thanks man! It worked. I didn't know how to use switch before going through your code. 
I saw a presentation at a conference from a guy at Transport for London, the company in charge of London's public transport network. They use shiny for all their reporting, dashboarding, etc.
I haven't developed any Shiny apps personally at the moment since I am primarily a Python guy, but we have a commercial (i.e., licensed) Shiny server in our cloud that gets a pretty good amount of use by our data scientists (and business end users). As for examples, my company takes IP very seriously, so that is all proprietary. 
We use shiny extensively. The best thing about Shiny is that dashboards are just the beginning. You have a whole world of HTMLWidgets at your disposal which lend themselves really well to building general purpose web apps. For example; we built a project management system entirely in Shiny. The system stores user accounts, allows users to collaborate on timelines, has notifications, can generate custom reports, manages budgets, interacts with databases, and does much much more. I can't post examples but maybe I'll show some screenshots later. Edit: [Screenshots](https://imgur.com/a/w8X8Z) 
I'm a heavy user of Shiny Server in a cor port environment. Best BI tool ever 
I'm a Shiny consultant, so by definition that's *all* I use at work :)
would you mind if PM you a question to see if this project idea i have is FEASIBLE in shiny before i waste a bunch of time?
We use shiny to create dashboards for our clients. Access jotacom.shinyapps.io/modelo to see a model of what we use. We have more beautiful dashboards today but with this model you can have an idea. 
I have been working on this https://sulock.shinyapps.io/height_distribution/ for letting my intro students mess around with some different distributions/their parameters/and sampling from them. 
Are you.... Dean Attali? If so, I owe the past week of success to you. I'm up and running my first shiny app on a digital ocean server (first server/website I've ever launched) thanks to your incredibly well written tutorials.
That sounds really cool. I always wanted a custom made project management tool for workplace. Where do you host the shiny app?
I actually host it on a Shiny Server running in a VM. So anyone within the organisation can access it, and it can access databases within the org.
That is awesome. Thanks for sharing.
Oh okay. I was too afraid to host sensitive data on the Shiny Server. But good to know.
As long as the server is within your corporate intranet there isn't any more risk to Shiny than to any other system that has data access. 
I use Shiny to make analysis results available to end users on a dynamic basis without making them learn/install R, RStudio, etc. It works fantastically for the things I've done with it so far - everything from creating visualization tools (wordcloud generator) to dashboard-style monitoring systems for very specialized/technical data. 
Homework due soon? We'd be happy to help, but one poorly asked question from a new account isn't going to get much from me. Something along the lines of `for(i in 1:w)` starts your loop. Might want to use `paste0()` too. Could get you down to 1 or 2 lines. `seq_along()` tends to be better than `1:w`, and I'm sure there's an `*apply()` way to do this too.
Hi ReimannOne, Please expand on why you believe this to be a poorly asked question? I have little experience, and do not know how to go about creating such a loop in R. This is not homework. My concern is how to create the desired variables within the loop, I have for(i in 1:w) thus far.
Are you required to use a loop? 
Hi paul, no loop is required if there is a better solution.
_Hi paul, no loop is_ _Required if there is a_ _Better solution._ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^SmallSmallSmall1 ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
At present I have: for(i in 1:NoVariables) { if(NoVariables &gt; 0){ print(paste("Variable", NoVariables)) df[(NotInterestingMarkers + (NoVariables * Markers)):((NoVariables +1) * Markers), 1:2] NoVariables &lt;- NoVariables - 1 } } I do not see how to create multiple variables equal to NoVariables (w in the posted example). For example, variable1, variable2, variable3, variable4... 
I started to use Shiny to build some basic dashboards, and then I discovered that we had Power BI. I'm learning to incorporate R scripts within the Power BI environment. 
The question would be better if we knew both what you have &amp; what you want to end up with. It would be best with a minimum reproducible example. Put `if(NoVariables...` outside the loop, you'll probably need an else after the loop to at least return an error message. I'm not sure why you print, but the paste is a decent way to make the names. Try `paste0()` instead. Instead of `NoVariables` use `i` in the paste function.
I'm a little rougher on users who have a brand new account with just an r question. [Here's an example](https://www.reddit.com/r/rstats/comments/4nlkzt/please_help_with_this_function_noob_question/) of a pretty good question.
Thanks! I'm going to (hopefully) use it in a lab today for the first time ever. 
I suppose you could get R onto a flash drive and do it that way, but otherwise I'm not sure how you would do it besides completely recreating R. 
Details depend on your OS. I've done this a few times for an old employer with isolated CebtOS servers on their own intranet. You could build from source or get the .deb, .rpm. or whatever installer your OS can use for R. Installing libraries is tedious, I install those from source with devtools when I can. 
creating R variables dynamically (within a loop) is unnecessarily complicated, much better to just store things in an array. Here is an lapply solution that (I think) does what you want: lapply(1:NoVariables, function(x){ df[(NotInterestingMarkers + (x * Markers)):((x +1) * Markers), 1:2] }) note that in the loop created by lappply the variable X is the value of 1:NoVariables. Try this for example: lapply(1:10,function(x){cat(x)}) If you want something different please do as ReimannOne suggested and include expected input and output
Yup that's me, glad you found it useful!
If it's a quick question and easy to answer, sure. I'm on vacation right now and trying not to do much work, so if it's more involved I suggest describing your usecase and asking about it, either here or on the google discuss board https://groups.google.com/forum/#!forum/shiny-discuss
http://nafiux.github.io/portableR/
My quick google returned: https://cran.r-project.org/doc/manuals/r-devel/R-admin.html Which is part of the official manual and has a whole section on installing R for the different OSs. Packages can easily be installed from their compressed source files. Dependencies might/will be more difficult, but assuming linux it will end up being 'download source' -&gt; 'install by hand' a lot. EDIT portableR or creating a docker/something similar with R+packages on a machine with internet and then moving it over is likely simpler.
Hey, thanks again for tacking a crack at this back then. I just created a ShinyApp of it [here](https://acm9q.shinyapps.io/cryptoapp/) that I'm ready to call "in beta" (I need to handle the error thrown when input text is NULL and then I think I'm done) P.S. I just did it with the declaration of independence, rot 13, and got a very readable: "thst to aeucre theae righta, governwenta sre inatitctel swong wen, leriving their jcat fopera mrow the uonaent om the governel; thst phenever snb morw om governwent yeuowea leatrcutive om theae enla, it ia the right om the feofde to sdter or syodiah it, &amp; to inatitcte nep governwent," for my output, so maybe it's not so bad after all! 
Thank you very much for all the suggestions.... Docker solution worked, I was able to pull docker r-base image (https://hub.docker.com/r/_/r-base/) and install other packages on top of that.... Best 
I honestly haven't worked with shiny yet. But I would think that you could make it that you can select the product from a drop down (or new) then have filter rules for each column (this might be limiting to predefined &gt;,==,&lt; + value or so) and then maybe an additional button for 'update' if you don't want it to happen instantly. That seems reasonable. Recreating it doesn't sound difficult, I mean only 2. looks like possibly work to get it parsing the restriction statements correctly.
People are [increasingly](http://www.kdnuggets.com/2017/08/python-overtakes-r-leader-analytics-data-science.html?imm_mid=0f5ddc&amp;cmp=em-data-na-na-newsltr_20170906) using both together. 
Now I just want to try Rcall from within `JuliaCall::julia_console()`
I don't run into issues with: library(haven) read_sav("path/to/GSS2016.sav") Where "path/to" is the location from the disk drive to your .sav file.
That must be interesting. I haven't tried that myself.
I've been working with the package haven that way importing surveys and time series sets from stata and never got a problem 
You could fit a single model for all of the data. Just include a variable indicating which drug the observation is for and add the appropriate interaction terms to effectively get different fits for each drug. But since everything is in one model the interaction term for the half life will get at the question you're asking. 
This Julia sounds hot. Any pics?
a pic to illustrate the basic usage: &lt;https://non-contradiction.github.io/JuliaCall/articles/JuliaCall.png&gt;
Screenshot for using `JuliaCall` within `RCall` within `julia_console()`: &lt;https://non-contradiction.github.io/JuliaCall/articles/RCall.png&gt;
Try geom_bar(stat = "identity").
So the problem is in the geom_bar(). If you add the stat arguement, stat = "identity", your plot should render.
perfect, thank you
thanks, this worked
in the latest version you can use `geom_col()`, which is like `geom_bar(stat = 'identity')` but shorter
Yeah, that is exactly what I thought. But how?
The other answers are right on. The explanation is that `geom_bar` calls `stat_count` (or `stat_bin`?), that counts the occurrences of the `x`-variable (it makes a histogram). `y` gets set to `..count..` which is the number of occurrences in each bin. This is why the 2nd example fails. To avoid the use counting in bins, specify `stat='identity'` which specifies that the given `y`-variable is to be used as is.
Just tried this, works as you said. Follow up, what's the best way to reorder the columns according to my categorical vairables? R defults it to frequency i think, I want it in the specific order it's set up in the matrix
The bars will order themselves alphabetically, so if ok need to order them differently you can do that by making the variable a factor and assigning the order with the levels argument. What I typically do the is use the reorder() function directly in my call to aes(). 
Make sure that the vector or column in your data set is a numeric value. You can check your dataset by using the function str() Also check out this for an easy example. http://www.r-tutor.com/elementary-statistics/quantitative-data/stem-and-leaf-plot 
And if, for whatever reason, it is classified non-numerically but clearly should be based on what you see using AlequeW's advice, change it using as.numeric().
Combine the two data frames into one, with a factor variable indicating whether the data is drug A or B. Then include that variable in your model.
Thanks for the suggestion!!
That's totally fine to do. Often times if you are generating the data or importing it might not be the data type you want and will need to adjust it accordingly. 
Replying myself: manova(cbind(v1,v2,v3)~A;data=foo) http://www.sthda.com/english/wiki/manova-test-in-r-multivariate-analysis-of-variance
Why do you need ncurses? Just set up your terminals with your editor and R shell running. A few sh scripts might help you. If you're crafty enough, you could even set up some fifo to help you. Since you know what ncurses is, this shouldn't be a problem for you.
just type "r" on console .... 
I made a small tool in order to organize my finances, it would be nice to lay out a cool interface for it. 
Have you considered making a shiny app?
I have. But I plan to share this open source tool, using shiny means that I have to keep a server up? Or can it be shared within the code?
When other people use your thing, you can set it up to kick off shiny locally
Create your own package and include [a shiny app](http://deanattali.com/2015/04/21/r-package-shiny-app/), then host it on GitHub and anyone can install it with `devtools` and run your app. 
If there is a function with the same name that exists in two packages the one which gets executed when you call it will depend on a couple of things. If the method is a generic and the object that you are calling it on has a class for which a generic method is defined then the correct method dispatch will take place regardless of the order they were loaded. if not they are typically checked in the order of most recent first, when a package is loaded using library it is prepended to the list of namespaces. If you want to see the order in which the namespaces/packages are checked you can use `search()`
When writing your code and you've two libraries (`x` and `y`) both with function `z()` you can explicitly write your code to use the function from the library you want... ## Call function z() from package x x::z() ## Call function z() from package y y::z() It would be wise to do this to avoid conflicts, particularly if you are sharing your code with others who might load their libraries in a different order to you.
Thanks guys, you guys are really helpful :)
Let's just get terminology clear: **installing** a package, you only do *once*, either from the gui or with `install.package`. You **load** a package with the `library` command. Now *this* affects in which order the namespaces are called; the *last* package *loaded* takes precedence. Usually when loading, you should receive a warning that package z is masking function x from package y. However, there is the case with method dispatching where package y supplies a function for objects of one class while package z supplies a function for objects of another. Then it is less of an issue, but I'm tired, so I won't write anymore.
Thanks 
Just add a comma after the last "50".
Yes i added but i do not understand the logic , as the logic says, that i have retrieve all rows with column (ab) &gt;= 50 , and my condition should come after rows space and done so. please help 
Try: baseball &lt;- baseball[which(baseball$ab &gt;= 50),] 
Thanks it worked by I am not getting the logic , you are trying to pass condition in row space ,thought the condition is applied on the column space !! ?? please help.
The condition is applied in different places depending on how you want to subset the data frame. You want to subset your dataframe to only have the *rows* which are greater than 50 in the ab column so you apply the condition to the *row* space. If you were trying to subset the dataframe by columns you would put the condition as you originally did.
Thanks man ! really helped me ! have a great day ! 
Have you tried dplyr? library(dplyr) baseball_50 &lt;- filter(baseball, ab &gt; 50) Where you filter rows that meet the condition of having more than 50 abs. Two resources: [intro to dplyr](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html) | [dplyr cheat sheet](https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/data-transformation-cheatsheet.pdf) edits: spelling/formatting
baseball &lt;- baseball[baseball$ab &gt;= 50, ] It'll work w/o the which(). The statement baseball$ab &gt;= 50 tests each row of baseball$ab for &gt;= 50. It returns a vector of TRUE FALSE that is equal to the length of baseball$ab and each TRUE is &gt;= 50. When you place that statement within baseball[,] it returns the rows that are TRUE (from your test) and all columns. Using which() returns the index values that are TRUE in baseball$ab &gt;= 50
Yep, and as /u/tomQ11 below said, you basically had it right from the start, you just needed to run it on rows instead of columns. Think of it this way, when you do indexing you are doing: dataframe[ &lt;which rows do I want?&gt;, &lt;which columns do I want?&gt; ] And the answer to `&lt;which rows do I want&gt;` is "The rows where ab &gt;= 50"
You select the rows (condition before comma) and want all columns (no condition after)
Whilst true that you can do this without `which()` I think it's interesting that for large data sets it's faster to use `which()` 
Just switch it up... remember [rows, columns] baseball.sub &lt;- baseball[baseball$ab &gt;= 50, ] Or use dplyr - it makes things a lot easier in these situations.
You don't need `which` in this situation. Here's the breakdown: selecting *rows* from `baseball` is done by passing an atomic vector to the first index: `baseball[c(1,4,7),]` (An atomic vector is the most basic type of vector in R.) To select rows, you either give integer row number, character vectors whose names correspond to the rownames, or a logical vector of `TRUE`s and `FALSE`s of same length as number of rows. When you pass `baseball$ab &gt; 50`, this picks out the column and compares each value. It returns a logical vector with `TRUE` for those entries that were strictly greater than 50. 
?boxed.labels gives you: Usage boxed.labels(x,y=NA,labels, bg=ifelse(match(par("bg"),"transparent",0),"white",par("bg")), border=TRUE,xpad=1.2,ypad=1.2,srt=0,cex=1,adj=0.5,xlog=FALSE,ylog=FALSE,...) Arguments x,y x and y position of the centers of the labels. x can be an xy.coords list.
Hey thanks I get that but when I changed it from 11 to 16 I didn't see much difference, is it pixel setting??
&gt; i dont understand what it is for &gt; I get that Make up your mind... I assume it is based on the y-axis scale, how about you try some larger values until you see a difference? Otherwise it would be based on the plotting region and only go from 0 to 1.
Is there a reason you want to do this in R? There are R packages out there for working with audio files (seewave, audio and tuneR) though I can't help but think a more general programming language would be better suited.
I'm not much help, but if I were you I'd start with tidytext
wow that looks amazing. I'm not quite sure yet, if that helps with my specific problem, but I will definitely try that out once I got the data sorted somewhat.
If you are on a Windows machine, try splitting your text with \n as your delimiter. Probably won't help, but I run into it often enough. Especially since I dual boot and have a share drive.
You need to learn regular expressions so you can find the pattern ##.## and work from there, moving through the text block chunk by chunk. Read up on grep too.
That looks very promising, thank you!
 First you can read in the file with readlines Look up the stringi package. There's a function stri_split() that you can feed your regex expression and your text file and it'll give you your desired output.
This is bang on, I think this is capable of doing exactly what I need. Thanks a lot!
I find stringr package easier than grep and grepl. I also find the rebus package handy for helping me write valid r regex 
Stringr is based in stringi but has some advantages and it more intuitive as a complete package for string manipulation. Str_split() will work
This is probably a bit of long winded way to do it but it should work. Edit: This is a very simplistic solution which you may getting a world of issues if any of those separators exist within the messages themselves. A proper solution will require more complicated regular expressions. Edit2: thinking about it some more it might be easier to use character locations since you know what length the dates and times will be, but you probably got this sorted already :P library(tidyverse) library(stringr) library(lubridate) text &lt;- read_delim("New Text Document.txt", "&gt;", col_names = FALSE) text %&gt;% t() %&gt;% as.tibble() %&gt;% separate(V1, into = c("date", "data"), sep = ",") %&gt;% separate(data, into = c("time", "data"), sep = "-") %&gt;% separate(data, into = c("name", "data"), sep = ":") %&gt;% mutate(data = str_replace(data, "&lt;", ""), date = dmy(date), time = hm(time)) 
 I recommend Coursera Data scientist specialization courses from John Hopkins university. First 3 courses set me up.
Check out Swirl - there's a link in the right column. It was pretty helpful to me. I struggled with R until I started working with it daily. Track down the cheatsheets too. They are available from inside Rstudio - one of the menus on the right side.
I learned R mostly through datacamp and think the intro, intermediate, and maybe dplyr courses would have everything you need.
R for Data Science: http://r4ds.had.co.nz
If you didn't start programming until later in life, the process of learning is much like learning a new spoken language: basically a whole deluge of new words that have no discernible structure, reason, interpretability, or rules. But when you get thrown into the language class, they don't teach you rules and structures first; they teach you common phrases and colloquialisms, like greetings and instructions. That's really how you should approach learning a programming language. Just repeat what the teacher is telling you, word for word, and once you begin to pick up certain nuances, you'll start to unlock the linguistic theory on your own, and learn by questioning. *This is why most programming advice is pretty much summed up by saying "just start"*. Literally, just start doing it, even if you are copying lines verbatim, and you'll start to find patterns in the way the language is structured; eventually you'll begin to question those patterns, which eventually leads to understanding. That being said, I second the advice of others with Swirl and Coursera. Just do it now, you'll begin understand later. See [how to learn anything - TED talk by Josh Kaufman](https://youtu.be/5MgBikgcWnY) . I don't agree with the whole philosophy, but he does mention how spending too much time in trying to comprehend what you are trying to learn actually _hinders_ your ability to learn. In other words, analysis paralysis.
Relative R newbie here also, but if I am understanding your question, using base R, all you would need to do would be to use the merge () function for dataframes and use the argument all=TRUE (and appropriate 'by= ' arguments) which will have a similar effect as a FULL JOIN in SQL. However, where there are no matches, NA will be populated, so you will need to change the NA values to zero for QTY afterwards via another line of code. I don't know an easy way off the top of my head to populate the NA region values with the values from the sales dataframe (or vice versa) but I'm sure you'll get some suggestions soon. There are probably better ways to accomplish what you are doing using other packages like 'dplyr', but above is how I think you'd do it in base R. [merge documentation](https://stat.ethz.ch/R-manual/R-devel/library/base/html/merge.html) edit:formatting
The sp package has a command %in% that you can use to see if a point is within a polygon. You need the polygons defined though. Maybe draw them manually with a mapping program? Maybe you can specify the points and manually construct it?
Thanks! I'll take a look into sp. Do you know any specific mapping program? I'm legit really new to gps plotting of any kind so I wouldn't know what's reliable or not for scripting. 
qgis. But I only use R. If it were me, I'd manually identify the points and construct the polygon objects from scratch in R. It's hard to advise you without knowing what you already know and how far you want to go. What you're describing is a lot of work. Are you already good at R? 
Likelihood is not spelled with a "y"
Awesome, I'll look into this later today. Does anybody else have any suggestions? The more the merrier!
Try the [`ggrepel`](https://cran.r-project.org/web/packages/ggrepel/vignettes/ggrepel.html) package. Also, you can just plot points at `x = "Low"` etc : points &lt;- data.frame(Impact = c("Low", "Medium", "Low"), Likelihood = c("Low","Medium High", "Low"), label = c("test1", "test 2 label", "not blocking")) ggplot(bg.melted) + geom_tile(aes(x = Impact, y = Likelihood, fill = Value)) + geom_point(aes(x = Impact, y = Likelihood), data=points) + geom_label_repel(aes(x = Impact, y = Likelihood, label=label), data=points, box.padding=unit(.5, "lines")) + coord_equal() + scale_fill_gradientn(colours = hm.palette(100),guide = F)
As /u/Procyon2014 said, you'll want to just merge the two data frames and then do something about the missing values. The merge can be done with [`full_join()`](http://r4ds.had.co.nz/relational-data.html#outer-join) from the package `dplyr`, part of the wonderful [tidyverse](https://stackoverflow.com/documentation/r/1395/tidyverse#t=20170916132236761424). After a bit of googling I [found](https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-an-r-dataframe) a cool trick with `coalesce()` for the second problem output &lt;- full_join(sales, forecast, by = c("region", "Date", "part")) %&gt;% mutate(Variance = coalesce(Qty.x, 0) - coalesce(Qty.y, 0))
Thanks. I'll give it a shot. Is there a better subreddit/site that you know of for posting questions? I'm not looking for someone to solve my problems, just get more ideas once I'm stumped and my google searches are not being successful. 
Try stack overflow 
You can use the xts package, create an xts object, then plot it.
Have a look at lubridate. Once you have parsed your columns into dates, you can plot them directly onto your x axis, no conversion necessary. 
So I work for a transportation company. I use R for data analysis, nothing heavy, just general reporting. I'd say I'm intermediate skill at best. I got interested in GIS-related stuff when I realized our transportation dry-runs costs went up a bunch in a couple states. It looks like they're not giving the correct pickup addresses when they're requesting transportation, so a carrier gets out there and the product isn't there. We do have gps trackers for these products and I have a gps ping that is at least 23 hrs fresh. I can plot the gps points just fine, and we can see the points on Leaflet are ALL over the place --&gt; meaning, yeah the pick address provided is wrong. But I wanted to take it a step further and somehow have R return 'False' next to product ID for products that are not within a created "geofence". The geofence in this scenario would be an address with a radius of a couple miles or so. I looked into some of the stuff you said and it looks like there's no simple function in any package that can just test whether a point is in a shape. And the package that does allow this - sp I think - isn't really made with maps in mind. So I wouldn't know how to plot a shape on TOP of a US map to test for point_in_shape. 
Objects of class POSIXlt, which you can make with the strptime and format functions in base, are really just special kinds of lists - each element in the list is named according to 'year', 'month', etc. I'd use an apply or for loop to populate the POSIXlt object with your column data, then plot that. Alternatively: data.sub &lt;- data[,1:4] # subset your time data data.sub &lt;- apply(data.sub, 1, function(x) { paste(x, collapse = ".") } # collapse the rows into a single vector, seperated by decimal # points Is that what you meant by decimal form?
You can probably just copy them over once installed. Use `.libPaths()` to find where the library paths are, and then just give them a copy. Any reason you can't bridge your network connection to your VM?
It’s a phi proof of concept we’re attempting at a university right now
I don't recommend converting the date to decimal form as it is much more versatile as a date. However, you can use the lubridate package to create a new column with a date from the data you have. Below, I: 1) source a dataset, 2) convert into the same format as you have, 3) show how to plot it in ggplot. Feel free to copy and paste code below and experiment. # Download dataset from: https://www.kaggle.com/stackoverflow/rquestions # Install and load tidyverse and lubridate pkgs &lt;- c("tidyverse", "lubridate") if (length(setdiff(pkgs, rownames(installed.packages()))) &gt; 0) {install.packages(setdiff(pkgs, rownames(installed.packages())))} sapply(pkgs, require, character.only = T); rm(pkgs) # Load data into memory and convert into format in original question df &lt;- read_csv("r_users.csv") %&gt;% select(date = CreationDate, score = Score, IsAcceptedAnswer) %&gt;% mutate(year = year(date), month = month(date), day = day(date), hour = hour(date) ) %&gt;% group_by(year, month, day, hour) %&gt;% summarise(average_score = mean(score, na.rm = T), accepted_answer_rate = mean(IsAcceptedAnswer, na.rm = T), num_questions = n() ) %&gt;% ungroup #Create date out of year, month and day and use it to plot in ggplot df %&gt;% # use mutate and paste to create a date, then parse with ymd() from lubridate mutate(date = ymd(paste0(year, "-", month, "-", day))) %&gt;% # group by the date to get count of questions and average score group_by(date) %&gt;% summarise(num_questions = sum(num_questions) ) %&gt;% # Plot num of questions and manipulate date on x axis from within ggplot2 ggplot(aes(date, num_questions)) + geom_line(alpha = .2) + geom_smooth() + labs(title = "R Related Questions on Stack Overflow", y = "Number of Questions per Day", x = element_blank()) + # select any options for breaks, labels, etc for dates here scale_x_date(date_breaks = "1 year", date_minor_breaks = "1 month", date_labels = "%Y")
copy package files to .libPaths()
Mainly R is free, but I would also take any suggestions or feedback as well!
Well, "general reporting" can be quite sophisticated! Don't sell yourself short! Sounds to me like you could do a lot by taking the difference between the actual pickup and the expected pickup. You have coordinates for the package, and you could geocode the coordinates for the address they give you. The difference between the two sounds to me like what you want. My favorite R map resource is Robin Lovelace https://github.com/Robinlovelace/Creating-maps-in-R The sp package manages the spatial points that you want to map and how they should be projected onto the map, but yeah it doesn't actually make the map. 
Also, if for some reason you need to get decimal dates (some models won't play nice with date objects) you can use the `decimal_date` function in the lubridate package.
Also, if for some reason you need to get decimal dates (some models won't play nice with date objects) you can use the `decimal_date` function in the lubridate package.
Hi, thanks a lot for getting me on the right track. To be perfectly honest, I could not understand your solution, but now I ended up using the str_locate_all function and writing the starting points of all the matches of a date into a data frame. From here I am pretty confident that I will be able to construct the 'Date', 'Time', 'Name' and 'Msg' columns using the fixed lengths and the start point of the next date. I have to admit, that I am experiencing a bit of a steep learning curve but it is so awesome to see it all come together, even though this is such a simple thing. Definitelty thanks a lot for the pointers to the right direction!!
Hi again, thanks for the tip with regex, I think I managed to find a rather uncomplicated way to utilise the str_locate_all function to make a usable data frame from my txt file. Thanks a lot! :0
Glad you found a solution that worked! 
looks cool. what library do you use for the tables?
This is awesome!! I had to do some minor format changes in the dataset I was applying this to because it had a hard time subtracting doubles from a numeric class, I also had to convert the part number column in both data frames into a character vector but seems to work :) I got the following error message and not sure if it means anything, but: Warning message: In full_join_impl(x, y, by$x, by$y, suffix$x, suffix$y) : joining character vector and factor, coercing into character vector
Cheers. I use Rhandsontable.
I think one improvement is using three 's before the {r} and close with three 's. I've always used that in my RMarkdown documents to show the beginning and opening of some r code. '''{r} Some R Code '''
It's not clear to me what's going on here. Is Act a count by csct or by dhms? In what way do you want this count to interact with the pseudo code? Regardless, you can call functions in the j argument with by grouping, so it seems as though there's a solution like Randall [, forecast(time_series(.SD), by = csct] Possibly preceded by creating the necessary counts in Randall, but as I said, that part's a bit mysterious to me. It's likely that I'm missing some feature of what you're trying to do. As an aside, for loops aren't as bad as they're made out to be in R when used well, but in my experience they're often the first tool people reach for when they're missing a more idiomatic R approach. Deeply nested loops like this are a good indicator to look for another solution, so I think you're on the right track. The vectorized approach really just means the loops are implemented in C, which is where the speedup comes from.
I'm not sure I understand what you're trying to do 100%, but I would write a function and then use sapply.
numbers &lt;- data.frame ("Name" = c("A","B","C"), "Bio" = c(85,45,80),"Math" =c(23,89,85),"chem"=c(95,90,34)) logic &lt;- data.frame ("Name" = c("A","B","C"), "Bio" = c(T,T,F),"Math" =c(F,T,T),"chem"=c(T,T,F)) sums &lt;- data.frame ("Name" = c("A","B","C"), "Sums"=c(rowSums(numbers[,2:4]*logic[,2:4]))) As you can see you can just multiply a number by a truth value with False counting as 0 and True counting as 1. Further you should do a join on the data.frames and add a new column for the sums so that everything is stored in one object. 
You need to use **backticks** (accent grave, \`), not apostrophes. And be careful to add the necessary line breaks: ```{r} Code ```
Thanks! I used the method for my needs. If I am to get a mean of the true values from the numbers dataframe based of the logic dataframe, how can it be done? Using your method will consider 0 values as part of the mean
Change False to NA in logic and give argument na.rm=T to rowMeans. 
Yes, my apologies. This is correct. 
Use apply whose arguments include a function definition of percentage.
`length(x[x == 3])` can be shortened to `sum(x == 3)`; and there’s no real need for the temporary variables in this case: percentage = (sum(x == 3) + sum(x == 5)) / length(x) Additionally, you can avoid creating a temporary variable for the column (i.e. `x`). There are several ways to achieve this but in this case I’d probably simply do percentage = with(mtcars, (sum(gear == 3) + sum(gear == 5)) / length(gear)) And then you can shorten this slightly by combining the condition: percentage = with(mtcars, (sum(gear == 3 | gear == 5)) / length(gear)) And when you’ve reached this point, then you can replace `sum(x) / length(x)` with the simplification `mean(x)`: percentage = with(mtcars, (mean(gear == 3 | gear == 5))) If you have more than just two values, it becomes tedious to write all comparisons; an alternative at this point is to write `x %in% c(3, 5)`. And since we’re now using the variable only once, we don’t need `with(…)` any more: percentage = mean(mtcars$gear %in% c(3, 5))
cool thanks for that - all makes sense
Yeah, I can throw in some sample data. I'm trying to get a count, and then use that count later, so the time series will be created with two of the columns. Let me add some sample data, maybe that will help. Thanks for the reply, too! I think this gives me an idea of where I can take it, so it helps!
so, a function trying to do what I'm trying to do? I've tried that before, and I ran into a problem when it tried to handle a vector (Still thinking scalarwise, I guess.) Thanks for the reply!
Looking at your edit, it might be easier to set this up by splitting up the data.table into slices and writing a function that acts on each slice. Then you can just call that function in an lapply, and you'll get back the list of return objects. That will allow you to test the function on a single slice to make sure everything's working as expected. If the function needs different parameters depending on the slice, it may be trickier, but it should still be workable. (As always in R, this is not the only possible approach.) I'd start with randall_list &lt;- split(Randall, by = c("Country", "State", "County", "Town") Then write a function that does what you need for `randall_list[[1]]`, for instance: pred_fun &lt;- function(Randall_Slice) { TimSer &lt;- ts(Randall_Slice, start = c(RS[1]$Year,RS[1]$Week_)Num), end=c(RS[.N-F]$Year,RS[.N-F]$Week_Num), freq = 52) seasdec = stlm(TimSer, some args) fc &lt;- forecast(seasdec) pred &lt;- c(rep(NA,len_new),summary(fc)$'Point Forecast'[2:(forecast_weeks+1)]) resids = slice[,Some Cols] # if you need to return all the objects, put them in a list list(forecast = fc, predictions = pred, residuals = resids) } # then lapply the function to each list element results &lt;- lapply(randall_list, pred_fun) You can then pull out all the list elements and stack them if you like. The purrr package's `transpose` is useful for flipping the list order, and then you can use data.table's `rbindlist` to stack them together. 
In the example file, the code is ```{r include = FALSE} library(viridis) ``` And that's exactly what I have. After rendering I get an HTML file. I double click the .html file and get {r include = FALSE} library(viridis) displayed in the web browser. none of the r code seems to work. 
It’s very hard to diagnose what’s going wrong here since none of us can reproduce the problem. Here’s what I tried: 1. download the [example document](http://rmarkdown.rstudio.com/demos/1-example.Rmd) 2. `cd` into that directory 3. Start R 4. `rmarkdown::render('1-example.Rmd')` The output is a correctly rendered HTML file, as expected. The above also works if R is run from a different directory than the R Markdown file, provided the correct path is specified.
&gt;It’s very hard to diagnose what’s going wrong here since none of us can reproduce the problem. I try to figure out issues on my own, so I don't post until I'm at a complete loss. Thank you for verifying that the script works. I thought it might be a problem with an old example not complying with new rmarkdown formats, but now I can rule that out. I get an HTML file, and everything seems to work except the output that should be generated by the r code doesn't seem to work. When I open the HTML file with notepad or use view source, the HTML file is huge and has lines like this: &lt;script src="data:application/x-javascript;base64,LyohIGpRdWVyeSB2MS4xMS4zIHwgKGMpIDIwMDUsIDIwMTUgalF1ZXJ5IEZvdW5kYXRpb24sIEluYy4gfCBqcXVlcnkub3JnL2xpY2Vuc2UgKi8KIWZ1bmN0aW9uKGEsYil7Im9iamVjdCI9PXR5cGVvZiBtb2R1bGUmJiJvYmplY3QiPT10eXBlb2YgbW9kdWxlLmV4cG9ydHM/bW9kdWxlLmV4cG9ydHM9YS5kb2N1bWVudD9iKGEsITApOmZ1bmN0aW9uKGEpe2lmKCFhLmRvY3VtZW50KXRocm93IG5ldyBFcnJvcigialF1ZXJ5IHJlcXVpcmVzIGEgd2luZG93IHdpdGggYSBkb2N1bWVudCIpO3JldHVybiBiKGEpfTpiKGEpfSgidW5kZWZpbmVkIiE9dHlwZW9mIHdpbmRvdz93aW5kb3c6dGhpcyxmdW5jdGlvbihhLGIpe3ZhciBjPVtdLGQ9Yy5zbGljZSxlPWMuY29uY2F0LGY9Yy5wdXNoLGc9Yy5pbmRleE9mLGg9e30saT1oLnRvU3RyaW5nLGo9aC5oYXNPd25Qcm9wZXJ0eSxrPXt9LGw9IjEuMTEuMyIsbT1mdW5jdGlvbihhLGIpe3JldHVybiBuZXcgbS5mbi5pbml0KGEsYil9LG49L15bXHNcdUZFRkZceEEwXSt8W1xzXHVGRUZGXHhBMF0rJC9nLG89L14tbXMtLyxwPS8tKFtcZGEtel0pL2dpLHE9ZnVuY3Rpb24oYSxiKXtyZXR1cm4gYi50b1VwcGVyQ2FzZSgpfTttLmZuPW0ucHJvdG90eXBlPXtqcXVlcnk6bCxjb25zdHJ1Y3RvcjptLHNlbGVjdG9yOiIiLGxlbmd0aDowLHRvQXJyYXk6ZnVuY3Rpb24oKXtyZXR1cm4gZC5jYWxsKHRoaXMpfSxnZXQ6ZnVuY3Rpb24oYSl7cmV0dXJuIG51bGwhPWE/MD5hP3RoaXNbYSt0aGlzLmxlbmd0aF06dGhpc1thXTpkLmNhbGwodGhpcyl9LHB1c2hTdGFjazpmdW5jdGlvbihhKXt2YXIgYj1tLm1lcmdlKHRoaXMuY29uc3RydWN0b3IoKSxhKTtyZXR1cm4gYi5wcmV2T2JqZWN0PXRoaXMsYi5jb250ZXh0PXRoaXMuY29udGV4dCxifSxl That doesn't seem like it would be correct.
No, that line is correct. Pandoc creates self-contained HTML files. This means that external JavaScript, CSS and image data is embedded into the HTML document via [data URLs](https://en.wikipedia.org/wiki/Data_URI_scheme). This is what you’re seeing here. In fact, what you’re seeing here is the beginning of the Base64 encoded jQuery JavaScript library.
Thank you. That's what I *thought*, but I didn't know for sure. You're helping me rule things out and I appreciate it.
Another way is to use `table` to count instances and `prop.table` to determine the proportions: `prop.table(table(mtcars$gear))` You can then subset the result and sum it to find the proportion of 3 and 5 gears: `sum(prop.table(table(mtcars$gear))[c("3","5")])`
If speed isn't an issue: mean(mtcars$gears %in% c(3, 5) ) Using two == connected by &amp; is faster in principle
'=' is assignment. newMatrix &lt;- for(i in myMatrix){ if(i%%2==0){ print(i)} } 
Also, `for` loops do not return any values. `newMatrix` will most likely be `NULL`. *Edit:* Ah, I better give ya a solution. `myMatrix %% 2 == 0` returns vector of 6 elements, with `TRUE` for those entries in `myMatrix` that are even. See if you can figure if R lists columns or rows first. Then, `myMatrix[myMatrix %% 2 == 0]` returns those elements for you.
https://stackoverflow.com/questions/7070173/r-friendly-way-to-convert-r-data-frame-column-to-a-vector
Thank you so much!! This is super helpful
Of course. ;) I try to give complete answers because those were most helpful when I was learning. 
Thank you very much!
Looks like you probably wanted either dataframe[,1] or dataframe[[1]]
1:1 line is probably just a line in which y = x. For ggplot help is [here](http://ggplot2.tidyverse.org/reference/geom_abline.html). `ggplot() + ... + geom_abline(slope = 1, intercept = 0)` The other part looks about right.
thanks that worked!
Alternatively, if you really wish to loop through every element and return `NA` rather than omit the elements entirely: myMatrix &lt;- matrix(c(1,2,3,4,5,6),2,3) newMatrix &lt;- sapply(myMatrix, function(x) if(x %% 2 == 0){x}else{NA}) 
Careful with `dataframe[, 1]`. tibbles will return a single column dataframe, not a vector. So if you're using dplyr you have no idea if it's silently converting your dataframe into a tibble or not.
Although, `sapply` doesn't respect the matrix' dimensions and will return an atomic vector (i.e. a normal vector). So, if you really wanted to keep the even entries and replace uneven by `NA`, I suggest `myMatrix[myMatrix %% 2 != 0] &lt;- NA`
A data.frame is really is list object that allows matrix-like subsetting, while allowing for different data types on its columns. It therefore goes, as with lists, * `df[1]` returns a list, i.e. the column as a list object with one element. You may select multiple columns with `df[c(1,2)]` but they are returned as a list object. * The same, but as a matrix: `df[, 1]` which returns the vector. * As with lists, double brackets selects tye element: `df[[1]]` 
Yeah that's a pain in the ass. One of worst features of the tidyverse. 
If you're interested, we can set up a time and I can e-mail you the .rmd file and the .html file that it produces using 10 minute mail. Just let me know what would be a good time for you. We'll synchronize, each get a 10min mail acct, then you can tell me what yours is and i'll mail you the files from mine.
It's annoying after you're used to working with data.frames but honestly it's probably what the default for data.frames should be.
Without knowing what the true file format of the files is we can't help you much. Maybe you just need to rename them from .xls to .csv but how should we tell
Have you tried the `readxl` package for reading directly into R to then be exported again as .CSV or other file format? As far as I'm aware `readxl` can handle the older excel file formats. 
Have you tried the `readxl` package for reading directly into R to then be exported again as .CSV or other file format? As far as I'm aware `readxl` can handle the older excel file formats. 
I'm in the same boat. My grad class is requiring it too. Our first assignment was accessing a resource called "Codeschool.com". You can earn badges and learn R. Seemed kind of helpful. http://tryr.codeschool.com/levels/1/challenges/1 It goes through level to level. 
`[[y]]` extracts one element. `[y]` extracts a vector. Try `st[,1]` and `st[3,]`
or st$high_school for columns (assuming column name is high_school) 
seqVec &lt;- seq(from= 0, to=.01, length.out= 100)
that will give you evenly divided intervals of values equaling a vector the length of 100. You could also specify intervals like this: (note I am using different numbers here) seqVec &lt;- seq(from= 0, to=100, by= 10)
seq gives you a vector of numbers with a defined spacing. And I don't get what your switch function is ment to do. Maybe describe underlying problem you're trying to solve. Edit you might want to read up how switch works, but it definitely doesn't do what you are doing with it, maybe ifelse ?
He is thinking like c switch instead of embedded ifs - at least thats what I think he is getting at. 
Yeah, I was thinking I would be able to do a case switch like in another language(I am just learning r). Anyway, I am trying to create a new vector that assigns labels based on numbers do a large data set. if x&gt; .1 then it is small if .1&lt;=x&lt;=.2 then it is medium if x&lt;.2 then it is large
//Let me just show you some other uses of this function just to give a clear picture (I am sure you can interpret the //code): foo &lt;- 5.3 mySeq &lt;- seq(from=foo, to=(-47+1.5), by=-2.4) //Here I will use the repeat function to repeat a vector in a few ways. Maybe this will add to your discovery. rep(x=c(3,62,8.3),times=3,each=2) foo &lt;- 4 c(3,8.3,rep(x=32,times=foo),seq(from=-2,to=1,length.out=foo+1)) bar &lt;- c(3,8.3,rep(x=32,times=foo),seq(from=-2,to=1,length.out=foo+1)) //Now the sort() function will, well, sort... sort(x=c(2.5,-1,-10,3.44),decreasing=FALSE) foo &lt;- seq(from=4.3,to=5.5,length.out=8) bar &lt;- sort(x=foo,decreasing=TRUE) // I will throw a bunch of uses of all these functions as well as the length function below.. mySeq &lt;- seq(from=5, to= -11, by= -0.3) mySeq &lt;- sort(mySeq, decreasing = FALSE) myRep &lt;- rep(x=c(-1,3,-5,7,-9), times = 2, each = 10) sort(myRep, decreasing = TRUE) pracSeq &lt;- c(-3, seq(from=6, to= 12), rep(x=5.3, times= 3), seq(from=102, to= length(myRep), length.out = 9)) 
you got that exactly right, down to my gender.
I think you need two ifelse(...) calls. I believe switch only works on character strings, it also surprised me that you can't generally use it as a replacement for multiple ifelse's (or at least I didn't get it to work)
Keep looking at forums until you can answer it. I am almost sure I have done this in the past. It can work. Maybe look through the code these guys use? https://stackoverflow.com/questions/10393508/how-to-use-the-switch-statement-in-r-functions
thanks - getting used to the indexing still
ah... I tried $"high school" and stuff like that... didn't underscore though &gt;.&lt; 
If the column name has a space you'll have to use backticks ` for that syntax: st$`high school`
use cut
It can be difficult to remember what R returns based on subsetting. Sometimes it's a vector, or a dataframe, or something else. Since you seem to be running into this kind of problem often, here are a few commands to look into. They can help you figure out what you've got and make it what you want. str(x) -- structure of x class(x) -- class of x as.data.frame(x) -- make x a data frame or give an error. There are other as.functions, take a look at helpfiles. is.factor(x) -- is x a factor? Math doesn't work well with factors. There are other is.functions too, is.character, is.data.frame, etc. They'll help you figure out what you're returning. You might be looking for the `prop.table()` function. 
did you try brute forcing it with as.numeric() ?
For things like this, you're better off coercing the item into a data.frame {base R} or into a data.table {data.table package} Also look into the tidyverse as it helps with data.frames in easy to remember "verbs" http://r4ds.had.co.nz/transform.html I would use DF_plants &lt;- data.frame(plants) DF_plants$flower / sum(DF_plants$flower) or install.packages("data.table") library(data.table) DT_plants &lt;- data.table(plants) DT_plants[,flower/sum(flower)]
I disagree in most situations, only because you're not showing them the hard way to do something because "you did it and are marveling at how easy it is now", but because you should be showing them the hard way "in cases where the easy way doesn't get you exactly what you want, you can see what your quest entails to get there". I.E. If I don't show you for loops and you have no idea that something is operating on each entry in a specific way, when you can't get what you want from "summarize" or some nice vectorized code, how can you put into words what you're trying to do? I'm teaching the hard way so you can understand the guts of what is going on when, inevitably in the practical world, you need to write your own code, you can do it, or at least ask with some direction. Imagine you have a car, and you don't care how it works, you just need to drive it (hard, right?). Well, yes, don't tell me about the transmission, or the combustion engine, just show me how to press the pedal, and what rules I need to follow. If you live in a place where there are no mechanics, and all you have access to is tools and youtube videos about how to fix cars, then yes, I would hope that my instructors gave me a rudimentary explanation of what's going on, so that I can not only have an inkling of where my problem is, but if I need help, I can go to YouTube with more than "my car doesn't run" and access a video quickly that will help me "program" my car into working right again.
for the question you're asking, you can use ifelse print(ifelse(bob &gt; .05, "big","small")) where "print" is whatever you want it to do with that string. if you want to do 3 or more, you need nested if-else constructs; no switch. ifelse is the closest to ?: that you'll get (from C), but not switch 
;) use data.table! 
Do you have a vector of names? or are you using a scheme with incrementing numbers? you can use do.call("&lt;-",list("name",data_frame)) where data_frame would be some temporary name in your loop to hold before naming it. If you're incrementing, use paste0("df",i) [or similar] where "name" is in the previous statement to get df0, df1,df2 ....
I have a vector of names but the incrementing numbers case is also interesting. Thanks
no i didn't - i assumed that as they were numbers, and I thought that I was being returned a vector, operations ( such as sum() etc ) would work. this works tho , thanks
I've heard of this tidyverse - it's controversial-ish isn't it? In that some people don't like it and some do? Or maybe I'm confused with something else. thanks though
Some people love it, and some people just use it. I prefer data.table mostly because I work with a lot of SQL people, and it's very easy to translate back and forth for them. Also, I like elegant code, and the tidyverse, while tidy, has a very sentence like structure, and as such, can create run-on sentences. I prefer code where I can see what the code is doing line by line, and data.table makes each line pretty compact, yet understandable once you crack the syntax.
 L = list() L[i] = Store output of your loop here, into i-th element of list. # Set name of sub-elements names(L) = blah # Dump all element to global environment list2env(L, envir = .GlobalEnv) Alternatively, use `assign()` for the general case. Assign is the non-environment specific general case of `&lt;-`
Yep, I keep running into some true tidyverse/dplyr abominations which could have been achieved with one line and the most basic understanding of subsetting.
Creating objects in a loop using assign is nearly always the wrong way to do something. Why not store the dataframes in a list?
At no point does DR say "don't teach base R", he's saying it's easier to get off the ground using tidyverse than using base R. The hard stuff can come later.
Such as?
Honestly didn't even pay attention to the matrix format. In keeping with "for" loops (I know direct indexing is much faster), `apply(myMatrix, 2, function(x) sapply(x, ...`
What do you mean by "inputting their own coordinates"? As in, they would upload thei own data frame? Or that they would choose what gets plotted on the axes from an existing data frame?
Sorry for being unclear. I meant they could use two numericInput fields to enter an X and Y coordinate and hit an action button, which would plot the point on a graph. 
&gt;magine you were going to a party in an unfamiliar area, and asked the host for directions to their house. It takes you thirty minutes to get there, on a path that takes you on a long winding road with slow traffic. As the party ends, the host tells you “You can take the highway on your way back, it’ll take you only ten minutes. I just wanted to show you how much easier the highway is.” I think that's a really bad analogy. The author is *trying* to make the case that you have two different ways to get to the same result. One is hard and the other is easy. But, in his scenario, there is no real reason to need to know the 'hard' way to get to the party. The back roads do not impart any underlying knowledge of how the highway works. The author's analogy basically is "you shouldn't bother learning assembly if you are going to learn python". Well duh! A better analogy would be "Imagine you are a student and a teacher asks you to add 3+3+3+3+3+3. Wouldn’t you be annoyed? The teacher could have just told you to multiply 3 times 6." That's great IF you know multiplication, but you kinda need to understand addition first. Can you learn multiplication without understanding addition? Sure, but at that point you are just memorizing multiplication tables, you don't understand what you are doing. Great! You now know that 12 x 12 = 144... but do you know why? And do you know when you should use it? &gt;I talk to people about teaching a lot, and that phrase keeps popping up: “I teach them X just to show them how much easier Y is”. It’s a trap- a trap I’ve fallen into before when teaching, and one that I’d like to warn others against. If someone is teaching like this, they are teaching wrong. It's not the *material* that's wrong, it's the method of teaching. You should never teach something 'hard' just to show how 'easy' something else is. That's just pointless and wastes people's time. But, students should be taught the underlying system, the addition before the multiplication.
I believe the latest version of the tidyverse added pull() for this use case.
I'd love any examples you have, I suspect I'm guilty of this sometimes. 
Loops are way more applicable than group_by.
Here's an example, which to be fair has more than one problem. A colleague (who had learned just enough dplyr to be dangerous and had been taught base R is bad) wanted to standardize the columns in their data frame but didn't know why it wasn't working properly. data %&gt;% mutate_at(vars(as.numeric(which(sapply(., is.numeric)))), funs(. - (mean(., na.rm = TRUE) / sd(., na.rm = TRUE)))) -&gt; data Which could be written as: num_cols = sapply(data, is.numeric) data[, num_cols] = scale(data[, num_cols]) 
Hmm. That code is pretty bad, and if I use it on the iris dataset I don't get the same results as when running the base R code. However your base R code could simply be written in tidyverse as: data &lt;- purrr::modify_if(data, is.numeric, scale)
Not sure abou Shiny, but in base R I use the 'readline' and 'identify' functions, maybe look into those?
I'll give those functions a look, thank you
This is coming from someone experienced, though. As he said, this was someone who had learned "just enough dplyr..." To your original response, though, why would you ever teach the hard stuff then? You're teaching the easy stuff and going from there. There'd be no reason to go back to "the foundational (hard) stuff"
&gt;This is coming from someone experienced, though. As he said, this was someone who had learned "just enough dplyr..." Well, we're not going to blame the tool when we see someone applying that tool poorly, are we? &gt;To your original response, though, why would you ever teach the hard stuff then? You're teaching the easy stuff and going from there. There'd be no reason to go back to "the foundational (hard) stuff" Eventually, someone learning R and looking to go further down that road of learning the language will be exposed to someone else's code that uses base R. At the point the person is comfortable enough, it's always good to be flexible and at least be able to understand what the base R code means. Some aspects of base R are still very useful. One example is using for loops instead of `purrr::map()`. While `map()` is generally faster than loops and uses less code, it doesn't allow you to reference the *i-1*th iteration during the *i*th iteration. Another example is if for whatever reason you need to change a specific value in a data frame, it's easier to do something like `df[2,3] &lt;- 3`. Many statistical and modeling tools in R, starting with `lm()` or `t.test()`, are not tidyverse-friendly. Also, and this is my personal opinion, if someone goes down the route of building their own sets of functions and packages, it would be preferable to code the functions in base R. The whole NSE/tidyeval aspect remains more trouble than what it's worth to me at this point (although I'll probably learn tidyeval once it reaches ggplot2). Anyway, I don't mean to sound like a tidyverse fanatic who hates base R or whatever. People can do whatever makes them more productive. It's just been my personal experience that when I teach tidyverse functions people are able to do stuff faster than with base R, and they feel rewarded and encouraged to keep on going. Frustrations arising from learning base R are more likely to lead them to quit R and go back to Excel/PowerPoint/SPSS. 
Why would you teach that *first*?
That doesn't look to me like the work of someone who was taught dplyr-only and is misapplying it. That's using both a poor choice of dplyr functions (they should be using `mutate_if`, which serves that precise purpose), and they're using several base functions such as `which` and `sapply`. If you're blaming it on "they thought base R is evil", why did they use so much of it? (If anything, I tend to see patterns like that in people who are used to base R ways of thinking and trying to fit them into dplyr). I think people learning in any system end up making mistakes in the form of convoluted lines of code, and it's worth correcting them. I'd used R for many years (pre-dplyr etc) before I noticed how much shorter some of my worst lines of code could be.
Here is a link to my [stackoverflow post](https://stackoverflow.com/questions/46396207/how-to-read-pgn-data-into-a-dataframe)
I am using often russian cyrillic encoding and for me merge works fine, but with dplyr I had some problems...
In your last post I went along with your variable names, but what you have is not really a variance (technical term) but rather a difference. I would plot it as a time series line kinda thing, you could do something like: ggplot(output, aes(Date, Difference, colour = part)) + geom_line() You may need to make sure your Date variable is [coded as a date-time object](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) for the x-axis to work properly.
Are you asking why you would teach foundational concepts first? or are you referencing the car thing? If it's the car thing, I would say 1) it's not a great analogy, but 2) I'd also say that if I was teaching someone that I knew would be in that situation, I would teach that first, arguably, because it's more important than knowing how to drive in that situation.
Hey
Can’t you just add a limit to the query to only pull the first x rows?
possibly? I'm new to pulling data in this way. Do you know how I can go about that? Currently we create our own archives by reading in manual data exports and writing it to a csv that I can read in through R so I can run analysis. I don't know SQL, and what I have already was basic stuff I found online. 
Hi, The reason it is not doing what you want is because you are effectively running the assignment `newVec[ "low"] ` etc a number of times since `i` is taking the values of `oldVec`. You could utilise the loop structure by running along the indicies of `oldVec` newVec &lt;- c() for(i in seq_along(oldVec)){ if(oldVec[i] =="low"){ newVec[i]&lt;-1 }else if(oldVec[i] =="medium"){ newVec[i]&lt;-2 }else if(oldVec[i] =="high"){ newVec[i]&lt;-3 } } However I would note that this wouldn't be very efficient since you are starting with an empty `newVec` and then growing the object at every iteration. More efficient would be to have newVec = numeric(1000) first which initialises 1000 zeroes ready for the answer. A neater and different solution is to use `match` which can be used to match up values in one vector with another and turns this into a one-liner: newVec = match(oldVec,c("low","medium","high"))
Just add LIMIT 5 to the end of the query string. 
There are probably numerous ways of doing this. jowen's post explained why your for loop doesn't work as expected. Here are a few more ways. # Believe it or not R has a switch statement, use R's switch statement sapply(oldVec, switch, "low" = 1, "medium" = 2, "high" = 3) # Abuse R's built-in way of storing factors oldVec2 = as.factor(oldVec) levels(oldVec2) = levels(oldVec2)[c(2, 3, 1)] as.numeric(oldVec2) # Nested ifelse ifelse(oldVec == "low", 1, ifelse(oldVec == "medium", 2, ifelse(oldVec == "high", 3, NA))) 
&gt; # Believe it or not, use R's switch statement &gt; sapply(oldVec, switch, "low" = 1, "medium" = 2, "high" = 3) &gt; Can I use the switch function like I would in C? Say, mark something as low if was less than 2? Also would the switch need parenthesis afterwords? I would try this out but Im not at my PC now
hello
Are you looking for duplicated rows? or (which it doesn't sound like) duplicated data within a vector (column). If it's the former, how duplicate is it? if something's different do you care, say a row for graduated and not-graduated (say left over from some database snafu's)
try something like: df[duplicated(df[,1]),1] I think finding duplicated rows in the data.frame is more difficultin base R, but the tidyverse package contains a function that can do that more easily (I believe via count(...) )
Take the first column in the picture I gave, for example. I want to find out if, say, the number 5 is repeated twice, or 25, etc. From there I was hoping to just remove that duplicated row from the column.
If you're looking for just "duplicate" users (i.e. you want the system to record only the "latest" value that should be there, something like "Dropped out" would be &gt; than "Enrolled") you can use [for duplicated id's only]: df[duplicated(df$id),] if you want to do the whole row, you'd need a bunch of "&amp;" to hit all the TRUE's Explanation: The first part of df (before the comma) is saying "give me all these rows." What you're doing with the duplicated statement is "creating a vector of TRUE/FALSE whether it has encountered this value before" so for example data: chk &lt;- data.frame(id = c(1,2,3,3,4),status=c("A","B","A","C","A")) chk[duplicated(chk$id),] will only give you one row(where it has seen the id "3" before) because if you look at duplicated(chk$id) you should see "FALSE" for all "unique" rows (unique being the "first entry of that row". This means that the first 3 encountered, though not unique in an english sense, is unique from R's perspective, and any row "After that" will be printed. After that, you'll need to create some logic that orders it by what "is newer" so you'd create an ordered factor that had something like "Dropped out" as &gt; than "Enrolled" to automate filtering out "older" rows.
I'm working with 10,000+ rows, so (lib plyr) count() isn't working, unless there's a way to remove all those where count(df$col1) == 1... Perhaps there is?
Please note, you'll need the comma after the function within the [] because for data.frame, R requires a definition for the columns, where a blank ([something,___]) is telling R that you want to see all columns.
To remove those rows, just add a ! to the beginning of duplicated AFTER you've sorted the data.frame so that any rows you'd want to keep are on top. Also consider using data.table if your dataframe is huge (for sorting speedups) df[!duplicated(df$id),] or install.packages("data.table") library(data.table) dt &lt;- data.table(df) dt &lt;- dt[order(id,status)] dt[!duplicated(id)] note, you don't need the comma, and you also don't need dt$ because data.table handles that for you. 
&gt; sapply(oldVec, switch, "low" = 1, "medium" = 2, "high" = 3) We use a `sapply` here because `switch()` requires an expression as its argument. `switch` or `switch()` because it is being passed as a function to `sapply`. It is possible to switch into anything since switch takes an expression, which can be a function in R. The expression has to resolve to a string or a integer. 
As I said, if you only have 1 column duplicated(...) should work. But with 10k+ elements any search for duplicates might end up slow, because (without some trickery) you are talking of n^^2/2 comparisons, i.e. 50M+. (Possible trickery includes sorting and then only searching for neighbouring repeats, which should be O(nlog(n)), but I somehow doubt duplicated does that )
So... `&gt; dataset[!duplicated(dataset$COL1,)]` &gt; Error: Length of logical index vector must be 1 or 26 (the number of rows), not 13579. I can't make heads or tales out of that. I also transferred the data set into a data frame, but I cannot *see* a difference between a data frame and data table. Is there something that should be a tip off? For what it's worth, I imported it directly into RStudio from a CSV. I ended up doing this: &gt; hmID &lt;- hmNew$ID &gt; hmID[duplicated(hmID)] I still don't know why you need the name of the data frame (or table) with brackets around the function. I think I overlooked that lesson, but at least it works.
Makes sense, and I got it working now. Thanks!
For what you are doing I think using the limit function inside the query string is best because a number of bad queries could potentially burden the SQL server. However, if you did want to limit the processing time specifically in R then I would look into the evalWithTimeout function.
Use unique()
Yeah, it's very important to limit your queries to certain number, especially if you aren't familiar with the tables. Not only will it freeze your program, but you will be clogging up the database for others. As others have mentioned LIMIT 5 or with some other brands of SQL you can use TOP 5 right after SELECT.
Thanks for the new function! That still, though, functions like count(), in that it gives me every unique value in the column. How then do I find where it's duplicated once or twice or so?
Looks like your comma was in the wrong place on the index? You need brackets around the function because duplicates returns a logical vector, NOT the actual values stored - assigning the new data frame requires subsetting the original data frame by the filter index. Hopefully that helps with your confusion? 
I saw you got it working, but here's an untested tabular solution: dups &lt;- which(table(your.data$col) &gt; 1) your.data[which(your.data$col %in% dups == FALSE),] This should drop all the duplicated rows, and only leaves IDs that appear only once. Maybe that will help later!
Indeed it does. That makes a lot of sense. Thanks!
I know you said it was untested, I don't think that worked. Instead it just printed the first five and last five rows of all columns.
Did this work? yeah, your comma goes outside the parenthesis: &gt; dataset[!duplicated(dataset$COL1),] I know coming from C that seems so weird.... but you'll get used to if you keep working with data-frames, understanding that there's a missing argument that means default. If you don't like it, I'd further encourage data.table 1) to remove using the name within the data-frame, and 2) to remove that comma. They got a good training on datacamp from the guy that wrote the package (free). 
OK, so the original thing that led me to this was the following idea. I have a data.table with distribution of Locations and some associated metric in a time series. something like: Time, Place, Hits, MaxAllow # in a data.table called NW The question is: for each, how do I find, within a place, how many hits occurred in a 4 hour window, and then "is it above the allowable threshold (MaxAllow). I went about this like this, but figured there was a better way (note, instead of using MaxAllow, I've just set a threshold of 5, and believed I could do a lookup of threshold later. it does not stay constant over time within a place): windowcheck &lt;- data.table(NULL) Places &lt;- unique(dt[,Place] for(p in Places){ local &lt;- dt[Place == p, .(Time,Hits,MaxAllow)][order(Time)] winchk &lt;- data.table(HOURS = unique(date(local$Time)+hours(hour(local$Time))),Place = p) winchk[,WinViol := sapply(winchk$HOURS,function(y) sum(local$Hits[local$Time &gt;= y &amp; local$Time &lt; y + hours(4)]))] windowcheck &lt;- rbind(windowcheck,winchk[WinViol&gt;5]) remove(winchk,local) } any optimization help would be appreciated. The "data.table acting on another data.table" comes from winchk acting on local through the sapply.
First things first, you're growing an object in a loop. Google R Inferno and check out circle 2. The short version is that you're performing a bunch of deep copies when you don't need to. Rather than rbind on every loop, put each output in a pre-allocated list slot, and call rbindlist on the result outside the loop. You can also skip the remove step, as the objects will be overwritten on each loop. Places &lt;- unique(dt[, Place]) windowcheck &lt;- vector("list", length(Places) names(windowcheck) &lt;- Places for (p in Places) { ... windowcheck[[p]] &lt;- winchk[winviol &gt; 5] } rbindlist(windowcheck) Second, sapply can be dangerous if your data might change: it doesn't return a consistent class. That may not be a concern for you here, but I avoid it completely in production environments. As for your overall approach, it looks that the fundamental problem is essentially a rolling sum. I suspect there's a simpler way to go about that, but I don't work with time-series much, so I'm just not familiar with the available tools.
If you simple want all duplicate entries: df[duplicated(df$id) | duplicated(df$id, fromLast = TRUE),] or the dplyr way: df %&gt;% filter(duplicated(id) | duplicated(id, fromLast = TRUE))
What if you don't know how big the list will be when it's done? That's the case with most things I build within loops. I don't know if I'll have one element or 1000 at the end. in this case, I don't know how many hours have elements. So imagine a place where all hits are within the same hour, and then imagine another where all hits are 10 hours apart.... doesn't make for good pre-allocation. also, does remove take that much overhead? I really don't know. I do it to ensure that data doesn't bleed over between loops, just in case I coded something wrong, but I am not against removing the remove(). After reading the inferno thing, I ask for an explanation for a layman as to why building the list doesn't do a similar thing to rbinding? ("Often there is a simpler approach to the whole problem—build a list of pieces and then scrunch them together in one go.")
If you're using tibbles and/or data.tables, they only print the first and last 5 to save printing/screen space. Check the row numbers to see if you got the number of rows you expected, and likely it did what you wanted, it's just not showing it. 
It unfortunately just printed the first five and the last five of my whole data set, which aren't duplicated.
In those cases you'd over-allocate, but here you only need length(Places) list slots, so you'll be fine. (The size of the data.tables going into each slot is irrelevant.) I've seen tricks for allocating in blocks when an upper bound can't be approximated, but I've never encountered a case where that was necessary. Not sure how much overhead remove has. Use `profvis` or `microbenchmark` if you want to find out. Since the removed objects are overwritten on every loop, it does nothing here. Not sure how layman this is, but the difference is the number of copies taken internally. Vectors are stored in contiguous blocks of RAM, and each rbind extends ncol vectors. To accomplish this, R creates a new block of memory and copies the data over. In a long loop, that's a lot of deep copies. I don't fully understand how lists are implemented internally in R, but my understanding is that the list only needs to be copied when it is grown (in number of elements, not size of data), so filling in pre-allocated slots avoids copies. Basically, changing the length of an object typically triggers a copy in R. Assigning an object of arbitrary size into an existing list slot does not trigger a copy of the entire list because the list itself stays the same length. One of the reasons for recommending lapply is that it handles these allocation issues for you. Many of data.table's speed advantages come from internal overallocation and modify-by-reference functions (as opposed to the more standard copy-on-modify).
[removed]
Hi! Realize this is quite old, but feel free to PM me if you still need some tutoring help. Have three years experience in R and python and am using them daily. Cheers
Cool thanks going to check it out more later. 
Thanks yet again. Can you show me how I should loop, then (i.e. what the condition in for is)? Or would that be a l/s-apply with my stuff in the function and the allocated list in the apply argument? Also, for functions like that, using data coming from the process (in this case global) is it better to pass the arguments to the function (programming "no globals" practice) or to just treat them as global variables and just have it grab from the global environment. i.e. `function () { operate on things from the calling scope }` vs `function(output list, my NW data table, any other parameters from calling scope) {nothing out of scope}` ?
Your for loop should be fine as is as long as you allocate your list first (as I showed in the code example). You could switch to a function if you want, but my point about lapply was that it allocates for you, so wouldn't want to pass it an empty list. You'd pass it the same thing the loop is currently taking: the vector of places (plus the data it needs). Or just a list of data.tables that are already divided by place. Always pass everything a function needs in as a parameter, never rely on objects further up the scope. This can save you a lot of headaches later if you try to parallelize, or just use the function elsewhere.
I guess what the passing question was getting at is "Does R make a copy of what it passes (by value) or just passes it by reference?" Can I make changes to the original object? As well, if it passes a copy, then should I be passing it tables that contain tens of millions of rows. Does what works for small data.tables/frames scale well, or does R gorge itself on memory?
That's... a lot of questions. The answer to many of them is "it depends". In a functional programming paradigm, functions should not have side effects, so you shouldn't change original objects. You take inputs and produce return values, leaving inputs unchanged. Data.table can complicate this, which is why it has a copy function (for explicit copies, e.g. when an input will be modified with :=). Passing an object into a function will not generally create copies. Sum(x) doesn't copy x, but it also doesn't modify x. If it did it would copy and modify. I don't think much of this is relevant to the problem you started with, but Hadley's Advanced R book is free online and goes into many of these topics in detail.
Cool, I'll take a look. The original problem was really only a subset of a larger task, and most of it involves altering in place. I realize that my small snippet doesn't give insight into my larger problem, but the questions come from that, and are not in any way conjectures of what I may need to do some vague time in the future. Thanks for all your help. I've always liked Hadley's explanations and was extremely thankful to have had him for a stats class way back when he taught at Rice!
Thanks for the quick response! I will give it a shot tomorrow. Also, I'm not so new that I didn't format the dates as a date-time object, haha I'm one step ahead of you... almost, but not really :-P
Excel is not data manipulation friendly, see reinhart’s famous blunder.
If you get the hang of the 'tidyverse' packages for R, your data manipulation frustrations will evaporate: https://www.tidyverse.org/packages/ In particular the 'dplyr' and 'tidyr' packages (part of the 'tidyverse') are great for data wrangling, and if you have any experience with SQL you'll pick them up quickly. Excel functions like VLOOKUP and INDEX-MATCH are basically just LEFT JOINs in this world. I highly recommend the 'R for Data Science' (free!) ebook as a starting point: http://r4ds.had.co.nz/
Briefly: if you want to write commands load the library to import the data and create a dataframe, use the name of the dataframe as input in the code provided by your teacher. If you don’t want to use commands, because you stated not knowing R, use an IDE like R studio and it will do it for you, drag drop and click. Any doubts: https://cran.r-project.org/doc/manuals/r-release/R-data.html
Is there more data than you provided? You're doing matrix multiplication but there's only one matrix. Also, what's the function for onesampletest? There's no library for that function. 
I would recommend data.table if you use SQL. the syntax is very SQL translation friendly, and data.table is very fast. If you have specific functions you'd like a "excel translation" for, let me know in reply, and we'll make little demos for them.
So, RNG from computers is usually not truly random... it's a set of equations that approximates randomness... to "kick things off" the equations need a number to start from, or a "seed" to grow a random tree. The thing is, if you use the same seed, you get the same numbers every time. If you want really random, you seed with like the time, or ask the user for like a 10 digit number or something. But when you're teaching, so you can get the SAME results every time using the same functions, you would give out the seed.
example. execute this code: set.seed(10) rnorm(10) set.seed(12) rnorm(10) set.seed(10) rnorm(10) and then compare the first and third vectors to eachother
Thanks!
https://www.youtube.com/watch?v=9rIy0xY99a0 https://www.youtube.com/watch?v=SxP30euw3-0 if you're really interested in how randomness 'works' not just in R
To put it a bit differently, random numbers generated on a computer are not truly random. They are from instead a *sequence* of numbers, where each number is calculated based on the previous numbers. The randomness stems from that the numbers appears sufficiently random to be usable. The seed can roughly be seen as the first number in the sequence, from which subsequent pseudo random numbers are produced. Give the same seed, and the sequence of numbers will be the same.
 # He is probably reading this data as a data.frame data = read.table(text = "V1 V2 V3 V4 1 426 609 556 600 2 253 236 392 395 3 359 433 349 357 4 432 431 522 600 5 405 426 513 513 6 324 438 507 539", header = T) # This turns the data.frame to a matrix data &lt;- as.matrix(data) # This is to multiply the 4x4 data matrix with a presumably 4x3 matrix datanew &lt;- data%*%t(C) # This is a user defined function, based on the name and parameter # I'm inclined to guess that it is a columnwise one sample t-test function onesampletest(data=datanew, mu0=c(0,0,0), alpha=0.05) # This probably does some confidence interval calculation onediminterval() 
To observe duplicated rows I usually do this: df %&gt;% add_count(col1, col2) %&gt;% filter(n &gt; 1) Most of the time, I'm only interested in the duplicated combinations of n columns (typically ID-columns. For other cases, I think this thread covers it.
&gt; If you want really random, you seed with like the time, or **ask the user for like a 10 digit number or something.** I had a TA who used to instruct students to use their phone numbers, until the realization dawned that that might be inappropriate. Then we went with the value of birthday (dd/mm or mm/dd user's choice!) added to zip code of home town. It was only five digit seeds but at least it wasn't personally identifiable. 
Context? Like, literally any context?
yeah, of course , sorry about that. Well, we assume that all the trades executed are part of a bigger parent order ( which is equal to 70000). Also, all the trades were done through broker HBC. I have to calculate the implementation shortfall for the parent order. Moreover assume that the trade is initiated at the time of the broker’s first trade in the week, and that it is evaluated at the end of the week
How would you calculate the implementation shortfall from this data? I mean, what exactly -- in relation to the columns/variables in yr screenshot above -- do you want to do in R?
Yes, i need to calculate the I.S from the above data and i have to do it in R. But i dont know which formula i am supposed to use and how to do it in R
It looks like you're doing two fairly different things with var_a and var_b. Have you tried printing out diff and diff * probability?
Do you know how to calculate this shortfall by hand from the data above?
No, the formula gives me a hard time to understand. I know that it is: IS=κq(p −M2)+(1−κ)q(M2 −M1) and i know that kq= 56.734 ( which is the portion of 70000 which is executed) (1−κ)q= 13226 ( the unexecuted portion) p= 154.2955 ( the average of the price column) but i dont know what M2 and M1 stand for in my example, i think that they have to do with the Mid.Price column
If you don't set a seed it is set by the current time and process ID when it is required. `set.seed(NULL)` achieves the same. if you come to do this in parallel you should also consider looking at seed generators. 
`table` can also give you the cross-frequencies if you give it a df of both columns, instead of just one. Alternatively, `tapply` for base R or group_by when using tidyverse/dpylr if you don't want a full matrix.
Here are several approaches using data.table, plyr and dplyr: x &lt;- data.frame(replicate(6, 1), c('NK', 'NK', 'HA', 'B6', 'B6', 'B6'), c(25,43,15,20,85,89), stringsAsFactors = T) names(x) &lt;- c('MONTH', 'AIRLINE', 'ARRIVAL_DELAY') library(data.table) y &lt;- data.table(x) y[, .N, by=(AIRLINE)] library(plyr) ddply(x, .(AIRLINE), nrow) library(dplyr) x %&gt;% group_by(AIRLINE) %&gt;% summarize(n = n()) x %&gt;% count(AIRLINE) table(x$AIRLINE) I personally prefer data.tables. Once you get your head around the syntax, they feel much more convenient to use.
data.table is blazingly fast compared to data frames. I try to avoid using data frames at all and since data.tables are derived from data frames, every function that accepts a data frame will also accept a data.table. The one annoyance is that I always need to remember to type library(data.table) When I forget and try to display a massive file my computer takes forever to return anything. data.table always returns data immediately.
R help is admittedly horrid but the vignettes are quite solid. You can see the latest ones from RStudio with this command: browseVignettes("data.table") The web page that will open is actually locally hosted and includes the vignettes that come installed with the package. Here's the Intro vignette hosted by the cran website for those not currently on RStudio: https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html 
I have got a ts with a stock's prices and that's the plot that i get. What is the easiest way and command to check for potential outliers ?
Log and difference it, then check for outliers there.
Should i use a command to check for outliers ? or should i check it just by looking at the plot ?
Make a boxplot 
Could you elaborate more on how i could find the existence of outliers by using a boxplot ? thank you 
Right, but in a classroom setting you want to use a seed that it memorable so that you can include it in your report or source code, so that TAs can verify that your work is correct. You can't just say "I used set.seed(NULL)" because then they won't be able to reproduce your work if there's any weirdness. In a production setting, yes, you should take care to make sure you are not constantly resetting your seed, but that's totally different than the needs of a classroom.
You could put them into a matrix / data.frame or use named vectors to create a better link between probability and x, but I probably wouldn't bother for such a case personally. Side note, your calculation of bar_x (and hence the rest) is wrong. Compare it to: mean(sample(x, size=10000, replace=TRUE, prob=probability))
Check out the SEE program. It's in R if you have some problems you can private msg me 
Twitter has a great package for anomaly detection. A search for twitter anomaly should pull it up
This might help you http://www.stat.wmich.edu/s160/book/node8.html
Well look at that, table back at it again. Seems like the easiest way to do this, but I'm going to go through all methods to get some practice, thanks!
Your error is coming from the predict() function, not the confusionMatrix function. It means one of the variables you are using to predict has a level in the test set that didn't exist in the training set. The easiest solution is to factor the variables before splitting into train and test. Alternatively, you can take the unique set of the combined levels from both train and test for each variable and use those in the levels() argument of the factor for each variable in both train and test. Let me know if this helps!! 
Do an STL decomposition. You can try transforming the series (log is one way, taking the first or second difference is another) but either way STL will give you an option to define and display outliers (see argument 'robust'). 
I don't disagree, I only mentioned it since you mentioned seeding with time at the top of your post. 
....did I? I never said timestamp anywhere. UTC time and system voltage are just about the best real random seeds you can easily measure, but I didn't mention it in this comment thread. Maybe somebody else?
Sorry, it appears to be quoted at the top of the post I replied to. oh well, no harm done :)
OK thanks, I'll have to go with your second option, because the splitting of the data into testing and training portions was not done by me. I'll see what I can do and report back.
Not that I know of right now - I'm just converting everything over to s3 for continuities sake in the future. You're using rdrop2 right?
Yeah just do it correctly. Kidding aside you really didn't give any details so it's hard to know exactly what it causing the error or what the error really even is. 
You can also combine the data for the offending variable from both train and test and then factor it just to get the levels. Then use those levels in your factors in the original datasets. 
OK so I'm trying to see which variable I'm using to predict that has a level in the test set that doesn't exist in the training set. I call str() on my training and testing sets and I find that the only variables I'm using to predict are of type 'numeric' in both cases. There are no factors involved in the prediction other than that of the outcome, yet I'm being told about levels. My current error is now: &gt; The data must contain some levels that overlap the reference.
Here is some code for checking that all variables in both train and test contain the same levels. If you have any variable that is a character, I'm guessing it gets converted into a factor during the train and predict process: library(tidyverse) # Make a sample train and test data set. Test has one additional dessert type ("cake") train &lt;- data_frame(target = rnorm(70), flavor = (runif(70)) %&gt;% round, dessert_type = (runif(70) * 2) %&gt;% round) %&gt;% mutate(flavor = case_when(flavor == 0 ~ "apple", flavor == 1 ~ "orange"), dessert_type = case_when(dessert_type == 0 ~ "tart", dessert_type == 1 ~ "pie", dessert_type == 2 ~ "pudding")) test &lt;- data_frame(target = rnorm(30), flavor = (runif(30)) %&gt;% round, dessert_type = (runif(30) * 3) %&gt;% round) %&gt;% mutate(flavor = case_when(flavor == 0 ~ "apple", flavor == 1 ~ "orange"), dessert_type = case_when(dessert_type == 0 ~ "tart", dessert_type == 1 ~ "pie", dessert_type == 2 ~ "pudding", dessert_type == 3 ~ "cake")) # Look at levels of each variable in each dataframe to find which variable has unequal levels count_levels &lt;- function(x) factor(x) %&gt;% levels %&gt;% n_distinct train %&gt;% summarise_all(count_levels) == test %&gt;% summarise_all(count_levels) # Collect all levels of the variable from both datasets and combine into one vector complete_factor_levels &lt;- c(train$dessert_type %&gt;% factor %&gt;% levels, test$dessert_type %&gt;% factor %&gt;% levels) %&gt;% unique # Assign the complete factor levels to the variable in both datasets train &lt;- train %&gt;% mutate(dessert_type = factor(dessert_type, levels = complete_factor_levels)) test &lt;- test %&gt;% mutate(dessert_type = factor(dessert_type, levels = complete_factor_levels)) #Confirm that levels are now the same between the datasets levels(train$dessert_type) == levels(test$dessert_type)
I have the same issue. We have send this issue to the rdrop2 developers but they don't have uploaded the corrections into the cran. I hope they do it today. 
TIL - data_frame() != data.frame()
Correctumundo! I usually use data_frame for the reasons listed in the vignette. ;)
OK, many thanks. After much fannying around I have finally managed to get it working by doing what your first suggestion was - I just ignored the testing data I was given, split my training data into more training and testing, and now when I use that everything works fine. I'm not clear on why this works though because every variable in the original training and testing datasets were of exactly the same format, aside from the fact that the testing dataset lacked the outcome column that the training has. My expectation was that if I made the testing dataframe have a column for output too, with the values left blank, the prediction step would determine what they should be filled in with. Apparently not. Another strange thing I've noticed is that the accuracy of my model differs significantly based on whether I use the train() function with method = "rpart" vs. the rpart() function. It's about 48% vs 74%. Have you got any idea why this might be?
Did you make sure the train and test split is truly random? Is there any chance it is ordered or something like that?
Notice that the train() function by default will train the model on several cross folds so it will likely avoid overfitting much better. 
You mean when I've split my original training set down to a smaller training set + testing set? I did it like this: partition &lt;- createDataPartition(pml_training_smaller$classe, p=0.7, list=FALSE) training_subset &lt;- pml_training_smaller[partition, ] testing_subset &lt;- pml_training_smaller[-partition, ] My assumption was that this createDataPartition() function would be random.
Yeah that's random, so you should be good there. My guess is that the rpart is wildly overfitting without cross validation. 
Ahh right, I'll give that a go. That was on my to-do list anyway actually. 
Is there any reasoning behind how many folds to use? At the moment I'm just going with 4 or 5 because that's what I've seen in other examples. We're talking about a training set with about 14000 observations here.
That's enough for now. More folds can give narrower confidence interval but at the expense of longer run times. 
OK. My runtime is bad enough as it is. 
Something else I've been wondering, since I blindly copied this from other examples, is about the following command: &gt; partition &lt;- createDataPartition(pml_training_smaller$classe, p=0.7, list=FALSE) I notice it is not simply passed the training set as an argument, but rather it's the 'classe' column of the training set. Which happens to be the outcome I want to predict. Why is the specific column of any interest to this function, when it's dividing the entire dataframe 70/30? 
Hi, this doesn't do what you want, perhaps because attach is not quite doing what you expect. when you attach you are making a copy of the variable within the data frame. Modifying one will leave the other unchanged. So `ds &lt;- ...` is overwriting the data frame but not the attached copy of the `weight` variable. In general `attach` is considered bad practice, partially because of the confusion you have experienced. And in addition because it can cause conflicts with existing variables in your workspace. 
Using that column doesn't make any difference when you make a single data partition and the data is not a time series. However, it is useful for sampling from quantiles to make random splits with balanced classes, or making partitions in time series. It is also useful for making your own folds and bootstraps. 
Thank you so much! I've been butting my head against a brick wall for most of the day, and the relief of finding out why is wonderful.
No problem, my advice in general would be to avoid using `attach` at all though. 
Hmmm, OK, seems a bit advanced for me but at least I haven't majorly misunderstood that function then.
https://duckduckgo.com/?q=outlier+detection+in+R&amp;t=canonical&amp;ia=web
Yeah, I'm going to be staying away from that!
Yea, good point. Seems to be related to the dropbox api I had been using. Need to learn how to debug better.
Yea I think something has changed so might just move away from this setup.
Yay - no more stringsAsFactors = FALSE
read_csv() is another one I use, along with as_tibble()
dropbox retired their v1 api today. They gave a year's notice but I guess some developers didn't update their projects
Try reducing the dpi: https://stackoverflow.com/questions/32428819/knitr-rmarkdown-reducing-html-file-size
Not sure if you're already doing this, but in the chunk options you can adjust the size of the graphic output. This should limit the biplot to be a size reasonable for the document. 
I can't promise that I'd be able to help, but it would be helpful to provide a [reproducible example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) of your code so someone can help debug. This basically means the minimum amount of code that reliably hits the error in question. 
You may need more available RAM. (In a pinch try closing a few dozen stackoverflow chrome tabs)
17.6 GB? WOW! Without a code sample, there's no real meaningful insight we can give. Still, I'm curious. Are you plotting using *base* graphics, *ggplot2*, or something else? 
 per.zyl &lt;- zyl %&gt;% summarise_if(is.numeric, funs(mean, length, median, min, q25 = quantile(.,c(.25)), q75 = quantile(.,c(.75)), max)) %&gt;% filter(rowSums(is.na(.)) != (ncol(.) -2)) 
There's probably also a nice way to do this with purrr::nest() and purrr::map() but this should do the trick.
Thanks, but this just gives me more output-variables but doesn't solve the initial problem. I still got no output for combinations of the two variables "cyl" and "gear" that doesn't exist in the data. For example cyl = 8 and gear = 4. I just edited my initial post to show the difference between my current and desired output. Would really appreciate a solution. 
You can use tidyr::complete() for that. mtcars %&gt;% complete(gear, cyl) 
I still get the same output if i try this: data(mtcars) mtcars2 &lt;- mtcars %&gt;% complete(cyl, gear) mtcars2 zyl &lt;- group_by(mtcars2, cyl, gear) head(zyl) per.zyl &lt;- summarize(zyl, Mittelwert = mean(mpg), Anzahl = n(), Median=median(mpg), Minimum = min(mpg), Q25 = quantile(mpg, c(.25)), Q75 = quantile(mpg, c(.75)), Maximum = max(mpg)) per.zyl Another problem ist that I get as many rows as observations in the dataset (n=32), while I need the summarized statistics per group: cyl gear mpg disp hp drat wt qsec vs am carb 1 4 3 21.5 120.1 97 3.70 2.465 20.01 1 0 1 2 4 4 22.8 108.0 93 3.85 2.320 18.61 1 1 1 3 4 4 24.4 146.7 62 3.69 3.190 20.00 1 0 2 4 4 4 22.8 140.8 95 3.92 3.150 22.90 1 0 2 5 4 4 32.4 78.7 66 4.08 2.200 19.47 1 1 1 6 4 4 30.4 75.7 52 4.93 1.615 18.52 1 1 2 7 4 4 33.9 71.1 65 4.22 1.835 19.90 1 1 1 8 4 4 27.3 79.0 66 4.08 1.935 18.90 1 1 1 9 4 4 21.4 121.0 109 4.11 2.780 18.60 1 1 2 10 4 5 26.0 120.3 91 4.43 2.140 16.70 0 1 
You're looking at your old per.zyl object, because that code produces an error. Try it the other way around instead: mtcars %&gt;% group_by(cyl, gear) %&gt;% summarize(Mittelwert = mean(mpg), Anzahl = n(), Median=median(mpg), Minimum = min(mpg), Q25 = quantile(mpg, c(.25)), Q75 = quantile(mpg, c(.75)), Maximum = max(mpg)) %&gt;% ungroup() %&gt;% complete(cyl, gear) If you really want to complete your cases first you have to add NA handling to most of the summarise functions.
Thanks alot, with your edit it works like a charm. Thanks alot!!! It's for my work, and you helped me alot. =))))
Maybe try lapply(X = liste, FUN = lsp). Should apply the function lsp() to each element of liste.
Why not just give your string straight to `ls`, since it accepts a string as an argument? &gt; ls("package:stats")[[1]] [1] "acf" &gt; liste &lt;- c("package:stats","package:graphics","package:datasets") &gt; for (things in liste){ + print(ls(things)) + } [1] "acf" "acf2AR" "add.scope" "add1" "addmargins" [6] "aggregate" "aggregate.data.frame" "aggregate.ts" "AIC" "alias" ...etc... I would also use lapply for this rather than for loops as well as /u/SabotageTheWrit suggests: &gt; lapply(c("package:stats","package:graphics","package:datasets"), ls) [[1]] [1] "acf" "acf2AR" "add.scope" "add1" "addmargins" [6] "aggregate" "aggregate.data.frame" "aggregate.ts" "AIC" "alias" ....etc...
Hi I tried your first example and it gives me still the same error: &gt; print(ls(things)) Error in as.environment(pos) : no item called "things" on the search list In addition: Warning message: In ls(things) : ‘things’ converted to character string And that's what I don't understand, somehow my R can't do this Edit: After running apt-get update &amp; upgrade it now works. Really weird
File extensions should be .R for scripts and .Rdata for workspaces. Add those to your filenames and you should be good.
Thank you for your quick reply, I've changed the file names and added .r to each one of them. They now -show up as "R Files" in my homework folder but I am still getting the same error message and can't access my code.
Check out datacamp.com Definitely one of the best offerings that I’ve used.
Any suggestions for other data to use while experimenting with this, maybe similar to your ping data.. I haven't used ggmap before and I'm getting into it but want more data
I strongly recommend " R for data science" by Hadley Wickham, available here: http://r4ds.had.co.nz/ Great intro to using R and the "tidyverse", everything from importing data, exploratory analysis, regression, plotting, and function writing. It's been a great resource for me. 
Have you tried opening them in Notepad?
It sounds as if you are using the pdf device for plots, coupled with lots of data where each datapoint is plotted as e.g. a dot. This could cause your pdf viewer lots of issue displaying it. First solution is to switch, perhaps just for that plot, to a raster based image (e.g. png), as this won't explode in instructions with increasing data. Other solution is to reduce the magnitude of data you are plotting. This could be without loss, if there is a lot of overlapping points.
Would switching to png for the entire document be as simple as as running the following code? Or is this only for whichever plots are in that block? I saw this in another thread and am trying to implement it correctly. knitr::opts_chunk$set(dev="png")
Yes I have. You can see in image 6/6 that I have attempted to open the file in notepad, but it just returns random characters &amp; gibberish. I don't understand why or how.
What exactly are the file names, before you changed anything? I can't quite read them in your image
open it in whatever gui you wrote it in, if you cannot, then the file seems to be corrupted.
Prior changing the file names they were an unknown file type (I looked up the properties of each file by right clicking, etc.) I did what wdmc2008 said to do and renamed the file names so that they end in ".r." After doing this each file changed to a "R File" but the same error message keeps happening.
What is the specific string of letters and numbers in the file name?
The first file's name is: "TOLLERIS ASSIGNMENT 3" and the second file's name is: "TOLLERIS ASSIGNMENT 3 PROBLEMS 1 PART C AND D" and they both end in .r
`opts_chunk` changes the default chunk options in the following chunks. So if you include that command in the first chunk, then yes. *Edit:* if you have caching enabled, you might have to clear that before you get an effect.
That looks amazing, thank you. 
will do! thanks heaps
This isn't helpful at the moment but I know R is not a fan of file names with spaces in them, in the future use an underscore to separate words. 
How and what did you save your assignment? Specifically the code that was used.
Yes this is vital information. My guess is you saved your workspace and not your script. 
On my computer, I use the following procedure. 1. Open R 2. Click on file near the top 3. Open Script 4. Select the file.
I was gonna suggest him trying to read the .rds or .rda file but those files won't recover his code.
They are probably also workspace files, not the script files. You may still be able to recover from your R history file or history() once the workspace is loaded. 
Is there a PDF of the whole thing on the site? I could not find one. And since it is CC...
Caret has that out of the box. Can you just use that? Otherwise look up Andrew ng lectures on cost function construction and regularization. He uses Matlab, but the matrix ops are easily transferrable.
No necessarily corrupted. It just might be a workspace and not a script. OP hasn't clarified *how* they saved their work. If they just used save or save.image in the console then that wouldn't save their scripts. If they quit R and relied on the "save workspace image" prompt then that wouldn't save the scripts - just the workspace.
If you have it working for one file, use that code as a template to write a function that takes one of your list elements as an input argument. Then use something like sapply() to apply that function to each list element. By default, sapply() will try to simplify the results into an array, which it sounds like is what you want.
I cannot think of any situation where you would want to store data in a matrix over a list other than for the purpose of matrix algebra. Since your data is textual in nature, there is really no reason to do what you want to do. If you are text mining, use one of the text mining data classes. Since a list() in R can be LITERALLY anything. Do str(yourlist) and post the results if you still want to convert it into a matrix format.
You know... If you are using `glmnet`, `cv.glmnet` is the built in function. As mentioned above, `caret` supports ridge regression as one of its methods.
Definitely my preferred way of doing this. [Vingette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)
I'm not sure, I've always just used the web page. It might be available as an ebook somewhere. 
also, data.table!
Very useful tutorial. Although I would recommend newcomers to do most of these things using the tidyverse these days. filter() and the geom_*() are so much more intuitive and transparant than base R
Thanks for the comment. I will check out the tidy verse stuff and see if I can include it in later posts. 
Thanks, but I need to create a function manually as this is a class exercise. 
Yes, ng constructs CV using primarily matrix operations in his Machine Learning Coursera mooc. Again, it's in Matlab, but the logic remains the same.
Sounds good! I'll try to do that. 
Use seq_along to make a vector of numbers, then apply your function to the vector.
 # Create vector x = seq(0, 5, 0.001) # Plot plot(x, (pn(x)-pnorm(x))/pnorm(x)*100, ylab = "% Error", xlab = "x") # Max is around 1.6036 x[which.max((pn(x)-pnorm(x))/pnorm(x)*100)]
geom_segment? http://ggplot2.tidyverse.org/reference/geom_segment.html
*How* did you create the circles? Was it using base R and points (`plot(..., type='p')`) and changing the size of the points? Or with ggplot and `geom_point`? Because if so, extracting the endpoints of the circles is problematic. Possible with ggplot2, and I have no idea how with base R. It is problematic because the points are plotted at the x,y coordinates, but resized based on the plotting devices resolution, thus making it difficult to find the endpoints. Please tell us *how*, and we'd have a better option of helping you.
 library(plotrix) draw.cone &lt;- function(x, y1, y2, s1, s2) { draw.ellipse(x, y1, s1, s1/2.5) draw.ellipse(x, y2, s2, s2/2.5) lines(c(x-s1, x-s2), c(y1,y2)) lines(c(x+s1, x+s2), c(y1,y2)) } plot(NA, xlim=c(-1,1), ylim=c(-1,1)) draw.cone(0, -0.5, 0.5, 0.2, 0.1) draw.cone(0.5, 0.75, 0.2, 0.2, 0.1) 
1) for b) use a named vector/list eg: multiplier_vec &lt;- c("A"=1, "B"=as.integer(Product Count), ...) multiplier_vec[Algorythym] If product Count also depends on something, you may have to generate a matrix and then do an also slightly awkward subsetting, but you should be able to get rid of the ifelse nesting. For a) I would also iterate over the different .x/.y columns 2) ... %in% ... | 3) pmax(... , 0) 
Your two images are the same.
Is this Dynamics online or on-prem? If it's on prem, then you can access the backend database directly. Use the R-SQL server connector package to query the database and get information into R. If you are using online, you are not able to access the database directly. In that case, you are limited to using FetchXML queries. For a little bit of money, there is the possibility to set up a "Data Export Service". This duplicates the CRM database (that you cannot access) to an Azure SQL instance that you can then connect to. It requires an Azure SQL server instance and an Keyvault entry. Synchronisation is very quick ~ 30 seconds in my experience. Hope this helps!
Best I can come up with is `stringr::str_pad()`. You'll have to use it twice. Once with 'left' and '[' and again for the other side.
I figured it out: f &lt;- function(x) { paste("[", x[i], "]") } x &lt;- vec %&gt;% map(f)
Even better. 
I'd try to answer, but you haven't told us what your data looks like or what you're trying to accomplish.
If you have a vector x, you can make a simple histogram using the following code: library(ggplot2) qplot(x)
sprintf("[%s]", x)
&gt; This is online instant. I have seen multiple solutions using either Azure machine learning (native D365 connection) or Azure SQL solution you mentioned. But both of these have the extra cost issue. Is there no way to use XML queries from R? I don't need to copy the whole CRM but only a small sub-set from one entity.
 Your numbers need to be numeric. Look up as.numeric(). Yes use ggplot2 this will require you to specify x and y axes. 
I believe it's possible. You'll need to create your FetchXML query. If it's just a single query, then it's probably easiest to create using XRM toolbox FetchXML builder, or creating a view of the data in Dynamics and then extracting the FetchXML from the exported view. Once you've got the query, you could then use the Dynamics WebAPI to run the query. I'd check out the WebAPI documentation : [Query Data with the WebAPI](https://msdn.microsoft.com/en-us/library/gg334767.aspx) [Use Custom FetchXML](https://msdn.microsoft.com/en-us/library/mt607533.aspx#bkmk_useFetchXML) Packages to consider would be `httr` for making the requests, `jsonlite` for working with the response. Good luck!
This will put spaces between your string elements. Use `paste0` instead or the `sep` argument. Also, the map is unnecessary. `paste` takes a vector as it's argument: paste(1:5,"-",11:15)
Note that “[algorithm](https://en.wikipedia.org/wiki/Algorithm)” is conventionally spelled without any “y”.
**Algorithm** In mathematics and computer science, an algorithm ( AL-gə-ridh-əm) is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks. An algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
+1 but why are you quoting some of your identifiers (the vector names)? That’s unnecessary and quite confusing, not only for beginners. --- ^(Rant: R should really disallow this. R’s syntax is a mess.)
Those documentations are written in C#. Could you show some simple fetch query in R? I don't know how to use httr or jsonlite.
Okay. Sorry, I wasn't sure what your familiarity with the technologies was. Let's take a quick step back first. What exactly are you trying to achieve? Is this a one off extract of data, or a more regular extraction? There may be a better way to achieve your goals.
I'am professional in R and data analysis. Here I try to fetch Quotes with account IDs, amounts, dates etc. After this I can run few scripts that do k-mean grouping and predicted sales figures. Right now I do this by getting csv from CRM manually and running the script. But this is part of larger whole where this data fetch have to be done automatically according to other integration program. Integration can call SQL R service as web service and this seems to be best way to archive the goal.
Thanks … I played around this a little bit, but was not able to make it do what I want.
First of all … crazy how close our usernames are! The work I have done so far is primarily with ggplot, but I am willing to try something different if that is what I need to do. The image is just a “dummy” that I drew on a whiteboard trying to show what I am doing. I will try to get some data I can post as an example on what I am trying ot do.
this is a combinations problem. What you want is all possible combinations of numbers from a set of 20, then compute the sums.
This should give you a list with all possible combinations. you can *apply sum to them to get the rest. test &lt;- 1:5 all_combn &lt;- function(x = test){ a &lt;- list() for(i in i:length(test)) a[[i]] &lt;- combn(test, i) return(a) } 
Okay then. That makes sense. Connecting R to Dynamics CRM is theoretically possibly, but I don't think it's a very common path for people. As such, you'll have to build the process yourself if you wish to do it all via R. Not an easy task if you're unfamiliar working with APIs and the like As an alternative, you might want to consider collaborating with someone else in your team. Someone familiar with the WebAPI could create a regular process to extract the data to a local Database. SQL Server Integration Services (SSIS) is commonly used to do this. KingswaySoft make an adapter that connects SSIS to Dynamics, allowing you to easily extract data from CRM to a desired location. From there you could run R analysis as usual.
Juicy. We use ReporteRs for some exec-level reporting and I'd love not to have to use Java for it ;)
I use ReporteRs from time to time, and this new package seems to be a full replacement. There are a couple cons though, such as the need to save figures as images first before putting them into the document, and tables are the default ugly Powerpoint style. But it's easier to place things in arbitrary locations, so that's a plus.
I find it clearer. If you wanted to access the element 'A' you would also write multiplier_vec["A"], so quoting it during the assignment also makes it clear: this is the string name that is assigned a value. Up until now I hadn't realized that it breaks/is different when looking at it from the function call perspective where you don't quote. So maybe it was just clearer in my head :)
R's limit tends to be with RAM. That's where R keeps and works on data. With 64GB you should do just fine. I don't think R has a hard limit like excel does for either variables or observations. I'm not familiar with any ways to 'soup up' R to be faster. There are a few packages that allow multiple processors, but I have no experience with them. R isn't lightning fast, but it does have some of the best packages for data analysis and RStudio makes things a lot easier too. 
So was able to make a dummy script to show what I current have … hopefully this helps illustrate the goal better. #Define the data frame person = c("Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane") period = c("2016", "2016", "2016", "2016", "2016", "2016", "2016", "2016", "2017", "2017","2017","2017","2017","2017","2017","2017") metric = c("Metric1", "Metric1", "Metric1", "Metric1", "Metric2", "Metric2", "Metric2", "Metric2", "Metric1", "Metric1", "Metric1", "Metric1", "Metric2", "Metric2", "Metric2", "Metric2") value = round( runif(16, -0.005, 1.0049), 2) tmp = data.frame(person, period, metric, value) ggplot(data = tmp, aes_string(x = "person", y = value, color = "person", fill= "person")) + geom_point(aes(size = period))+ geom_line() + scale_size_manual("Year", values=c(3, 6)) + facet_wrap(~metric, nrow=1, labeller = label_wrap_gen()) + labs(y = "Dummy Example", fill="person") + theme(legend.direction = "horizontal", legend.position = "bottom", legend.key = element_blank(), legend.background = element_rect(fill = "white", colour = "gray30"), plot.title=element_text(hjust=.5, size=22), axis.title.x=element_blank(), axis.text.x = element_text(angle =90, hjust=1)) I was not able to make the plotrix work.
As a general rule, in R you'll want to avoid loops like the plague when working with big data. If you vectorize your code, it should be able to compete with any other language you're likely to use for analysis. As another commenter said, memory can be an issue. This [task view](https://cran.r-project.org/web/views/HighPerformanceComputing.html) has some solutions for working with large data sets, both in terms of speed and memory. 
I added a function to apply the sum on each matrix in the list. I also made the test vector more like yours, and I changed the for loop in all_combn to go from 1:length(test) test &lt;- sample(1:100,20) all_combn &lt;- function(x){ a &lt;- list() for(i in 1:length(test)) a[[i]] &lt;- combn(test, i) return(a) } combn_sums &lt;- function(x){ a &lt;- list() for(each in 1:length(x)) a[[each]] &lt;- apply(X=x[[each]],MARGIN=2,FUN=sum) return(a) } 
Thanks very much!
Thank you! I appreciate the help.
A heads up, a lot of people are recommending rstudio like it's the only ide out there, but I much more prefer rkward as my ide.
Interesting. I've always used RStudio, can you elaborate a little on why you prefer rkward?
I tried … and couldn’t get it to be flexible enough to work with my data. Down below I provided an example of what I currently have. http://reddit6.com/r/Rlanguage/comments/740sqn/trying_to_creat_a_plot_similar_to_this_with_r_for/dnwotmo?context=3
Sorry, I don't use ggplot. Maybe you can briefly tell me what your data looks like?
This should work? a&lt;-1:20 # Your vector... could be anything b&lt;-lapply(1:length(a), function(x) combn(a,x,sum)) unique(do.call(c,b)) Explanation a: storing a to your vector to a is just for display purposes, maybe you already know how to do it inline, but to explain shows it simply. b: "give me a list that, for each number of potential operands, sums every combination of them". lapply is a function that applies another function to every member of an input list, and returns a list. the input list is a vector of 1 to the length of your vector (in this case 20) and then sends that to a function i define inline (lambda function) that just applies the sum function to "every combination of length x (coming from the lapply) of the vector a". This returns a list where each element is a vector of all possible sums of length x (so the first list element is all sums of length 1 combinations, the second of length 2, etc.). unique: this says, "for this list, combine them using the c() function, and then for that giant vector, only give me the first appearance of a number" so you get your original answer to "what are all the possible sums of all possible combinations of my vector. (This does not give what numbers summed that, but would be how you'd go about it)
Native Qt toolkit (not that weirdness rstudio uses), and a more sensible/traditional but custimizable window placement.
This is the dummy code that I did that has the key components of my data, and the current visualization I have. I don’t “need” to use ggplot - although that would be ideal since the rest of this project has been built using it. #Define the data frame person = c("Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane", "Bob", "Joe", "Sue", "Jane") period = c("2016", "2016", "2016", "2016", "2016", "2016", "2016", "2016", "2017", "2017","2017","2017","2017","2017","2017","2017") metric = c("Metric1", "Metric1", "Metric1", "Metric1", "Metric2", "Metric2", "Metric2", "Metric2", "Metric1", "Metric1", "Metric1", "Metric1", "Metric2", "Metric2", "Metric2", "Metric2") value = round( runif(16, -0.005, 1.0049), 2) tmp = data.frame(person, period, metric, value) ggplot(data = tmp, aes_string(x = "person", y = value, color = "person", fill= "person")) + geom_point(aes(size = period))+ geom_line() + scale_size_manual("Year", values=c(3, 6)) + facet_wrap(~metric, nrow=1, labeller = label_wrap_gen()) + labs(y = "Dummy Example", fill="person") + theme(legend.direction = "horizontal", legend.position = "bottom", legend.key = element_blank(), legend.background = element_rect(fill = "white", colour = "gray30"), plot.title=element_text(hjust=.5, size=22), axis.title.x=element_blank(), axis.text.x = element_text(angle =90, hjust=1))
Yes, loops can definitely slow it down. The tidyverse framework works great with large data and you can easily avoid loops (master the do() function). And with that much RAM you'll be fine.
Maybe this can get you started: library(plotrix) draw.cone &lt;- function(x, y1, y2, s1, s2, col="black") { draw.ellipse(x, y1, s1, s1/5, border=col) draw.ellipse(x, y2, s2, s2/5, border=col) lines(c(x-s1, x-s2), c(y1,y2), col=col) lines(c(x+s1, x+s2), c(y1,y2), col=col) } conePlot &lt;- function(x, y1, y2, labels, cols, ylim=NULL, ...) { stopifnot(length(labels)==length(x)) ydiff &lt;- diff(range(y1, y2))/2 if(is.null(ylim)) { ylim &lt;- c(min(c(y1,y2))-ydiff, max(c(y1,y2))+ydiff) } plot(NA, xlim=range(x)+c(-0.5,0.5), ylim=ylim, xaxt="n", xlab="", ...) axis(1, at=x, labels=labels) invisible(Map(draw.cone, x, y1, y2, 0.1, 0.2, col=cols)) } # sadly you need to make your data non-tidy dats &lt;- split(tmp, metric) dats &lt;- lapply(dats, function(x) { dts &lt;- split(x, x$period); merge(dts[[1]], dts[[2]], by="person") } ) cols &lt;- c("orange", "cornflowerblue", "palevioletred", "limegreen") ylim &lt;- range(tmp$value) + c(-0.1, 0.1) par(mfrow=c(1,2)) conePlot(1:4, dats$Metric1$value.x, dats$Metric1$value.y, labels=dats$Metric1$person, ylab="value", las=2, cols=cols, ylim=ylim ) conePlot(1:4, dats$Metric2$value.x, dats$Metric2$value.y, labels=dats$Metric2$person, ylab="value", las=2, cols=cols, ylim=ylim ) But you will need to know some base plotting in order to make this pretty.
I code primarily in R and my boss uses Stata so I've worked with both but am biased toward R. I would say for 90% or more of our work, R is a far better solution than Stata. There are some very specific models implemented in Stata that have not been done for R but especially for data manipulation R is a much better solution. To your question, I've struggled with data problems far more often with stata. Having more than 32k variables is very manageable in R (though some popular packages have problems with very wide data). The variable name size limit in stata always kills me, not something to worry about in R. If I'm not mistaken Stata holds all data in Ram just like R does so while stata has a programed limit, R will just let you do as much as your computer can handle. But in the broader context, the R community is much larger and more active than Stata so its much easier to find help or code or whatever. Stata is great for some things but R is a much broader tool (better visualizations, markdown support, shiny etc). 
Thanks. This should help me a lot. Let me see if I can figure it all out. Appreciate your help. 
Vectorize and parallelize. Parallelizing code is much easier in Linux. Learn Linux.
I'd like to take the opportunity to mention that vim has a solid plugin or two (and, if you insist, emacs ess mode)
As a 15 year Stata vet who has recently been shifting some work to R (but who is no expert in R), a couple of observations: Both Stata and R handle data in-memory. In general, Stata encourages a very restricted utilization of data structures (in abstraction, one rectangular data frame at a time) until you ( or a command you invoke) are ready to build the specific matrices you need to "do the math", to the point that it utilizes macros for what most other scripting languages would use variables for in the course of most ordinary programming busines. Most operations you perform on the data are done in place. Other data structures (almost exclusively in practice numeric vectors and matrices) created either explicitly or by ado files (commands) are also modified in place, and when passed, passed by reference. While not nearly as flexible as R as a programming language (the way most of us use it, anyway), and not supporting a "functional programming" model in the same way, this does make memory use very efficient. I can usually do most of what I need to do with a data set (through estimations) in perhaps 1.3 times the memory consumed by the original data set. Which is actually quite amazing (for instance, most regression commands will accumulate the moment matrices directly from the original data set, so an N length matrix is practically never produced, beyond the original data set, and avoid creating other significant data structures unless you specifically tell it to with post-estimation commands). R on the other hand has a couple of design elements that militate against very efficient use of memory. While there are some newer packages (e.g. data.table) that try to address this, stuff is usually copied when it needs to be modified, and is practically always passed by value. Given the heavy use of functional programing, this means that you end up making (inadvertantly, for the neophyte), numerous copies of sometimes very large data structures in the course of working. Estimation commands (even the humble lm), seem to do quite a bit of this internally, so even with careful programming, it seems to often unavoidable if you are using packaged commands to do things. I think this is mostly because R was originally developed by academic statisticians for their style of research, which would mostly have involved relatively small proof of concept data sets. As R has expanded its audience into more and more applied work, a lot has been done to address this, but it is relatively advanced R skills to be able to get the most out of memory available and get work done on reasonable real-world data sets in a lot of fields. By which is meant requiring system memory of perhaps 3-5X dataset size rather than 5-20 times the size of the dataset. Commonly used estimation commands, particularly, tend to be gratuitously greedy memorywise in the creation of the data structures during their operation and for their return values, at least by default. The programming style encouraged by R is a lot more fluid and encourages thinking in cleverer ways about doing things, but there's no free lunch. Another thing you will find (which surprised me at first) is that R benchmarks as a bit faster than Stata for a lot of the common data preparation and manipulation operations, but most estimation commands benchmark out a lot slower than Stata's (3-10 times as long to run). From what I understand, this is a function of the C/Fortran libraries used for the matrix operations respectively. I think what I understand is that you can actually compile R to utilize much faster (platform specific) libraries, but the ones used by the pre-built packages are slower than Stata's (this seems to me a rather advanced solution, to say the least). This can be a bit of a surprise if you aren't expecting it, at the point where you find that you have a bunch of big regressions to run that take around six hours each in Stata, but now will take 30-60 hours each in R. My take is that there are some estimations that one is better off going back to Stata for. Paralelization is available in R for some things, but once again, it has to be implemented explicitly using specific packages, and seems to be a relatively advanced skill relative to the transparent multiprocessing gain you are used to in MP. For day to day work for those of us less expert, it is generally a single core proposition. R has a lot of advantages, but there are definitely some things to be aware of. 
Big upvote! Thanks so much!!
Very well-reasoned and well-specified reply. Thanks so much!
A vote for ESS here.
There is nothing wrong with scale() even with the default arguments. You must have messed up your math in Excel.
I'd add, more for others, as you are probably familiar, that Stata's programming language capabilities are much more sophisticated than is widely recognized. It has a scripting language (which has the ability to do matrix operations), and additionally a Matlab like language called Mata. But even the ordinary scripting language has object model support and lots of other capabilities beyond basic flow control , ifs, loops, program calls, etc. that we all use for basic utility programming. Most of us who use it don't know whether these more sophisticated elements are any good or not, or how they stack up to R, because I think frankly very few people who don't work for stata corp. actually use any of them. Most people I know who use Stata are economists, and if they want to do anything more sophisticated than basic data prep and standard estimations, they are going to be using Gause, Matlab, R, etc (in some masochistic cases, raw C). But it's only fair to recognize that Stata's programming environment at least can do a lot more than most people realize. 
Try printing out the results of the scaling. It will provide you the mean and SD used to compute the z-scores at the very end of that string of numbers.
So you recommend that I build SSIS package that downloads relevant data to SQL and then I call SQL R service from integration to run propitiate script and return relevant results? Only problem here I see is urgency. I can schedule SSIS package to run (lets say every hour) but if R service is called between these updates data is old. Not a huge issue in this case but in future ones this could cause problems.
One more thing about to consider besides RAM is multi treading. I am not sure how Stata is set up but the base version of R is single threaded. You have awesome packages like Caret to let you use multiple cores by training a model on each core. However this is still not complete multi treading. There are solutions out there which are 100% compatible with the base version of R but which do support this like Microsoft R open (uses 2 cores, free). There is also Microsoft R server which let’s you use all cores and offloads data to disk so then you are not bound to RAM limits (paid solution). Disclaimer I work for Microsoft 
Thank you for the feedback. I'm embarrassed to admit I don't know what all of this means. I know that using more cores makes it faster, but the computing mechanics of that are extremely fuzzy to me. Multi-treading is also a phrase I'm not familiar with. Can you elaborate a bit?
Seconded. For those not in the known ESS == [Emacs Speaks Statisics](http://ess.r-project.org/) a mode for the [Emacs](https://www.gnu.org/software/emacs/).
There are also things like ff so you're not limited to ram size you can write to cashe or storage Also snow will allow you to parallel process over all cores which R doesn't do naturally 
One thing that may have messed you up with scale() is that it always returns a matrix. If you gave it a vector input and expected a vector output, you'll need `scale(x)[,1]`.
Snow will give you parallel core processing capabilities and multi networked node CPUs. Pretty simple to get used to. ff will get you passed ram limitations by writing to disk cashe or as hard writes
ff package for R can offload to disk / cashe and the package snow can do 100% parallel Both quite easy to use
Still working on this .... any quick tips on how to fill them?
Set "col" parameter in draw.ellipse calls
I did that, bu that only fills the circles, not the cones.
you will probably want to use something like `polygon()` inside the draw.cone() function.
Depends what you mean. There are plenty of packages and base functions that do things similar to what functions in Excel do. But none that I know of that try to imitate the names and semantics of Excel functions, because that would be awful.
part of the reason this really wouldn't work is that excel functions are based on cell location [ like average(A2:A15) ] instead of named vectors. I don't think you could ever get this to actually be productive in R.
well dataframes and matrices have row and column numbers, so it could work something like this: average(start location, end location). I could see alot of the functions only working for dataframes/matrices but it should be feasible no?
Ugh I guess? But why would you want to?
easier to recall and use functions for people new to R but who have excel experience
If they are new to R then they need to learn the R way of doing things rather than trying to fit their Excel shaped peg into an R shaped hole. The learning curve for getting up to speed in R can be steep, but its inclination has eased dramatically over recent years thanks to the [tidyverse](https://www.tidyverse.org/). Once familiar with that (or even before) you can learn base R as there are lots of resources and documentation.
Sorry, did you say regressions that take 30-60 hours to run?!? 
We have large data sets. But these are regressions that take a few hours each in Stata (albeit 8 core MP) on the same box. I was a bit surprised, but this ratio isn't inconsistent with some benchmarks that are out there (especially considering the MP). It seems from reading around that a lot of the disparity, after accounting for the MP, is explained by the difference in underlying math libraries, and that R gets a lot faster for this sort of thing if one builds it so as to link faster libraries (which I understand is possible but is further than most users probably want to go, unless they have relatively sophisticated configuration management support running the box for them).
Both approaches has their merits, but this is a more simple way to do it, as opposed to working with the WebAPI. Going directly through the WebAPI with R is probably more streamlined, but has additional complexity if your not familiar with API's
If you need an excel package... You just haven't learned R enough. The point of R is to not need excel lol. What functions are you missing??
I have written few scripts with C# using early bound and crm sdk. But these tools have been well documented and I have managed to dig up examples and moved on from there. I havent managed to find any examples on R though. I don't know what libraries to use, how to create credentials, connection string or most importantly fetch call. If I could just have simple example where R calls crm and gets just list of accounts names. Then I would have a starting point where I could move forward. I really appreciate if you could help with this.
I usually can figure out solutions, ( you all have been very helpful too) but sometimes I get stuck and realize there is a nice excel function that could do this quickly like sumifs/averageifs etc. that don't require me to subset data first. I can build the custom functions to do that but I wanted to see if they already existed in some form first
`df %&gt;% count(var)` 
Hmm. Are you familiar with dplyr? It may have something for you. 
I've been using Stata for 3 years, and now migrating to R since most of people I know are doing the same thing. I've never had a chance to benchmark both Stata and R on a really big data set like @inarchtype did. I tried few million observation on both R and Stata and R resulted better on my computer. May be this is not related, but if you try the very big data set (dozen of TB, may be?), then why not Python?
I'd love to create a working example, but unfortunately I don't have time at the moment :( [The httr quickstart vignette](https://cran.r-project.org/web/packages/httr/vignettes/quickstart.html) gives a good overview of what the `httr` package is and the basic functionality. You'll need to create a GET request against the WebAPI for your instance. There will be examples of using the WebAPI, but you'll need to adapt it for R. The basic URLs should be similar, though `httr`'s syntax will differ to the examples given in the MS docs. Authentication via OAuth will also be necessary. Microsoft give C# examples (with a supporting library). You'll need to do it yourself if you wish to do it in R. FetchXML is pretty straight forward. It's a proprietary query language for dynamics CRM, but there are lots of examples of it online, and a few tools to help create it. If you haven't come across it before, XRM toolbox is a very handy tool for working with Dynamics CRM, and contains a tool to build and test FetchXML queries. Good luck!
Is the image the table you're trying to accomplish, or is it the result of your code? library(dplyr) completed %&gt;% full_join(scheduled) %&gt;% full_join(offers) %&gt;% full_join(attended)
The posted table is the result of the code. My goal was to only have 1 row for each date. I'll try playing around with the full_joins, but it will require structuring the data differently before stitching together. Earlier I was stitching together by date.
Do you mind if I ask what estimation functions/packages your were using in R for your test case? &gt; why not Python? I haven't used it for analysis (or anything else beyond basic utility scripting to play around with it). The benchmarks I've seen have Pandas slower than R or Stata for just about everything (although Python and numpy themselves are reputed to be faster). At any rate, there is considerable productivity cost to me of learning to fluency/proficiency a proliferation of different tools, and the kind of work I do (basically applied econometrics) seems to be much better supported in both Stata and R than in Python so far, unless most of the estimations you want to run are going to be "roll your own" anyway. - Edit Also, Scipy uses LAPACK and BLAS under the hood for a lot of the calculations needed for statistical operations, just like pre-built R does, so there is no reasons to expect the heavy crunching part of the estimations to be any faster than R (and one would expect it to likewise quite a bit slower than Stata for this). 
If you have 4 data frames each with a date variable and the 2nd unique variable, you can just run this (changing the names to whatever you called those data frames) and it will join them all.
You can use a "for" cycle. The pseudo code would be: for(country in dataset){x=ggplot(country$data1), y=ggplot(country$data2)...etc}
There sure is. You can use a `for` loop, but I did this recently with purrr and it was a blast. Assuming your `Location` field is a factor, you can do something like: data %&gt;% split(.[, 'Location']) %&gt;% purrr::map(~{ p &lt;- ggplot(aes(x = ..., y = ...) data = .x) + geom_col() + labs(x = 'X Axis Title', y = 'Y Axis Title') }) %&gt;% purrr::walk2( .x = ., .y = names(.), .f = ~ ggsave( plot = .x, device = 'png', path = './outputs/plots', filename = paste0(.y, '_coefficients.png'), dpi = 500, height = 14, width = 10 ) ) This code splits your `data` dataset into a list of datasets for distinct values of `Location`. By mapping over each dataset, we can build a plot for each one (fill in the `...` as you see fit. If it helps, just imagine you were making one plot for one of the factor levels – plug it in and it'll work just fine. Then, I walk over each (ggplot object, factor name) tuple and call `ggsave` to save each one into a distinct file. I'm happy to help if you want actual code specifics, but I think this is super neat.
awesome, I was struggling to make the for loop work with all the different factors. Thanks!
I'm an R neophyte, full disclosure, but maybe consider using another language's bindings for socket.io, then point your R script at that? For instance, set up a socket client in Node, whenever cryptocompare sends you data, add it to a list, and whenever you get a request on some other port, dump all the data you've gotten since the last query. Then, every 5 seconds or so, you can ping your local server through R and pull in 5s of data. Not a pure R solution, but it may work. Good luck! 
https://pypi.python.org/pypi/cryptocompare/0.3 Python has a library for it..might use Python. Pandas on Python uses data frames very similar to R if that's your structure. Otherwise Python lists are pretty similar to vectors. I don't know what kind of math you're doing but there might not be much of a difference. Especially if you get Numpy too. It's good at maths.
I did consider that and it's probably my next step if I can't get websockets working. Thanks for the response!
It might come down to that but I don't know any Python and my time is limited. I have a program fully working but the data is just a little too stale. Thanks for the suggestion!
Eh, I'm honestly not trying to brag but I learned Python on Wednesday after my kid went to bed because Javascript promises were pissing me off playing with crypto apis. It's got to be the most easy to learn language there is. Loops end in : and arrays are in [] (I think they're called lists), dictionaries are ['key':'value', ....] kind of like json returns..can be multilevel. Easily declare different types by just doing float(var) or str(var)...looping a json return is like this pseudo for inner_data in data: for item,val in inner_data.items(): do stuff with the item and/or value maybe return something or use a class of global variables and do a class.var = so you don't even have to bother with returns Honestly, after learning it, I replaced some VBS and R stuff I had at work with it this afternoon. I like it and I'm a 'professional'. I suffer from that impostor syndrome, only reason for the quotes..I'm paid pretty well..shockingly..
I was just hoping the websocket situation was going to be a bit easier. You're definitely right about learning this stuff quickly. A 2 hour Python YouTube video would probably get me pretty far. Thanks for the advice, if I can't figure out how to do this in R, I will probably redo everything in Python or Java.
It might help to look at the file in notepad++ or BBEdit (some editor that can show invisible characters). Can you describe how line 8 is different? Is is 2 tabs between entries? Are there no tabs and just spaces? Perhaps the file should be read by character position rather than delimited text. The read.table function has a whole host of options (delim is a special case)
R does a much better job of this than just about any other language. Rcpp code runs on Linux, Windows and Mac without any changes, as long as you're a bit careful to avoid any platform-specific syntax.
For example: this is my R package, written using Rcpp and Armadillo. On CRAN there are binaries available for both Windows and OS X. https://cran.r-project.org/package=bayesImageS Packages are always built from source on Linux, because Gnu g++ is always available. It is also possible to use Intel compilers on Linux.
I'm not using any platform-specific syntax. There are two possibillities with Rcpp: 1. I could provide sources written in C++ and users of my package should compile it themselves (implying that in this case they must have C++ compiler) or 2. *I'm* going to compile it and provide out-of-the box working package. I'd like to take the second option. However, compiled versions differ between platforms. This is my problem. I don't know how to create one single package that contains both .so and .dll files in order to "ship" only one version. Ideally, at the time when user is installing my package, R would choose wether to use windows or linux binaries. Is this possible? How did CRAN solve this (I don't see separate packages in the repository).
Yes, there are separate binary packages for Windows, but not for Linux. R packages on Linux are *always* compiled when you call install.packages from inside R
R packages on CRAN *must* generally be applicable on Windows, Linux, and Mac. If you code something that is platform dependant, it probably won't be accepted to CRAN. Rcpp, and associates, makes it really easy to develop R packages with C++ that can be used on most major platforms. You will literally write **1** package and in most cases, it can be installed on Windows and Linux. 
Can you provide an example of what you're looking for, using something like the mtcars dataset? I just want to make sure I understand what you're looking for.
I don't actually understand what you want, but based on "arduous task of all the SummaryBy()": vars &lt;- unique(colnames(my.df)) new.df &lt;- data.frame(data = NA ...) for (i in vars) { new.df &lt;- cbind(new.df, SummaryBy(i, ...) } I'd try something like that. There's also probably solutions in the reshape, plyr, and dplyr packages. 
On mobile right now. Thought I could ask the question while on the go. Ha
ddply(.data, .variables, .fun = NULL, ..., .progress = "none", .inform = FALSE, .drop = TRUE, .parallel = FALSE, .paropts = NULL) .variables=factorColumn And factorColumn categorizes my 40 row .data into 10 sets of 4. The ddply sends the set of 4 to the .fun. The problem is is that the set of 4 includes the factorColumn whereby breaking the .fun. I can't fix the function to accept the extra column. It's too complex for me. 
You can build a wrapper function, something like: wrapper_fun &lt;- function(data) { call_the_actual_function(data[,-1]) # or whatever subset you need }
Great, this seems like a good place to start. Thank you.
I'd investigate doing this with the [purrr package](https://github.com/tidyverse/purrr). From the Overview... &gt; purrr enhances R's functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you've never heard of FP before, the best place to start is the family of map() functions which allow you to replace many for loops with code that is both more succinct and easier to read. The best place to learn about the map() functions is the [iteration chapter](http://r4ds.had.co.nz/iteration.html) in R for data science. See also the [Jenny Bryan's tutorial on purrr](https://jennybc.github.io/purrr-tutorial/).
[plotly](https://plot.ly/d3-js-for-r-and-shiny-charts/)? This may not exactly be what you're looking for. Out of curiosity, why shiny instead of d3.js? Isn't purpose of shiny to make quick and dirty user interface apps for R users?
I think this resource list was posted here a while back, it has lots of examples. https://github.com/grabear/awesome-rshiny/blob/master/README.md 
What a fantastic resource. I do a fair amount of shiny development but hadn't come across a couple of these. 
The if statement is not properly indented
Thanks for the comment,I must review that. I keep getting an error "Error in nrow(To) : object 'To' not found" however. I'm was certain the syntax is correct and no variation I can think of seems to work. I'm at the point of thinking I might be way off the mark and need to restart.
You need to reference `i` as you loop. Something like: &gt; pollTable$To[i] = min(pollTable$To[i-1], pollTable$To[i]) inside of the for loop
I also posted below but wanted to point out that in this situation `nrow(To)` wouldn't make sense because the `To` object doesn't appear to exist on it's own (at least in the sample you gave). `pollTable$To` references the column of `pollTable` named `To`, but unless you've assigned something to just `To` (i.e. `To = ...`) it will throw an error. Fixing that wouldn't make it work however.
This is the basic syntax for loops: for (i in n) { # do something summaryBy(i) } The 'i' is your indexer - it's the part that does the looping. The n is your index - it's the thing you're looping through. Everything in the brackets is being repeated, except for the i. For your case I would use the 'months' constant in R as your 'n' - do ?letters to find out more about the built-in R constants.
With Shiny you don't really need to know much front-end, but if you were doing d3.js, you'd have to wire it all up yourself (maybe using an R api package, like jug). I prefer the later because I have a lot of front-end experience, but for someone who doesn't, Shiny makes it easy to get something useful and not bad-looking up and running.
Thanks, I asked because I was looking into this myself. I started learning shiny and it's very hard. But in your opinion is the juice worth the squeeze to get all that precursory knowledge to do be able to make good d3.js vizs?
On mobile so can't do this fully, but this is easy. Load the dplyr package first. Df %&gt;% mutate(date2 = ifelse(date1 &gt; date2, date1, date2)) Date1 is your first date column. You'll first have to convert date columns to date objects. Use the lubricate package. 
I don't think so. I learned web-development for a side business I started a-ways back. It can be very messy and complicated and often the things you learn become obsolete after 5 years. 
On mobile now but ill try this later. I knew the answer must have have been easy but I spent 5 hours trying with the help of a colleague (not really an R user) and just could not do it. Thought I was going insane.
Thank you very much for your help. Ill try this when i get ofr mobile. It seems so simple now that it's explained. 
&gt; for(i in 2:nrow(pollTable)){ &gt; pollTable$To[i] = max(pollTable$To[i-1], pollTable$To[i]) &gt; } Apologies, yes I meant that I am using a previous row to assign a new row. 
Thanks. I was trying to work along the logic of using 'max(a,b)' but obviously couldn't get it to work to my liking due to poor syntax! Thank you for the code, it's a fantastic starting point and great reference. I must tweak it a tiny bit (unless I'm reading your information incorrectly) as it's copying a single date across all values in each row!
/u/deanat78 does a lot of work with Shiny and his web-pages/blog posts are well worth [checking out](http://deanattali.com/tutorials/).
[removed]
&gt; that don't require me to subset data first If you have a grouping variable `my_groups` and want to summarise your outcome `my_measurement` within each group then dplyr is totally for you... my_data %&gt;% group_by(my_groups) %&gt;% summarise(n = n(), mean = mean(my_measurement, na.rm = TRUE), total = sum(my_measurement)) 
Ok this is a dplyr method that should work. However, since you didn't produce a reproducible example, I can't try it out. If you do produce one I will verify it works: https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example df %&gt;% mutate(From2 = ifelse(lag(From) &gt; From, lag(From), From), To2 = ifelse(lag(To) &gt; To, lag(To), To)) again this requires that the From and To columns are converted to data objects, which the lubridate package makes very easy. I created new columns (From2/To2) in case this doesn't work like I think it will, so it doesn't overwrite your existing columns. 
Oh, I got your point. We are discussing about the applied econometrics with various types of regression. I was distracted to the popular "machine learning". Then, yes, I admit that you got the right point about the familiarity with one tool (STATA) and migrating to a new tool (R/Python) may cost more than it looks.
You have to print () the plot first
here's the more advanced, dplyr answer: df %&gt;% group_by(month, station) %&gt;% summarize(max_temp = max(temp))
https://gist.github.com/willium/e2f7713ad0415aaa5241 
 pollurl = getURL('https://www.realclearpolitics.com/epolls/other/president_trump_job_approval-6179.html') classes = c("character", "character","character","numeric","numeric","numeric") pollTable = readHTMLTable(pollurl, colClasses = classes, stringsAsFactors = FALSE)[[4]] head(pollTable) 
`pollurl = getURL('https://www.realclearpolitics.com/epolls/other/president_trump_job_approval-6179.html')` `classes = c("character", "character","character","numeric","numeric","numeric")` `pollTable = readHTMLTable(pollurl, colClasses = classes, stringsAsFactors = FALSE)[[4]]` `head(pollTable) `
that's exactly what I needed, thank you! 
https://github.com/cran/DMwR/blob/master/R/utils.R
ty
Look at ````lubricate```` package, especially ````int_overlaps````
Not sure why it would do that, maybe if you used `pollTable[i] = ...` instead of `pollTable$To[i] = ...`?
Well I've got something, it runs in quadratic time and it's taking awhile... I'm going to see if I can eek out any speed increases as at the rate I'm going this will take years... 
I actually highly recommend learning dplyr rather than my method of actually `for` looping over the dataset like my solution does. It'll be better in the long run. https://www.youtube.com/watch?v=jWjqLW-u3hc
The only difference between population sd and sample sd is the debiasing correction (n-1) in the denominator for the sample sd. So you could write a new function to return the unadjusted version: sd.pop = function(x) { sqrt(sum((x-mean(x))^2)/ (length(x))) # would be length(x)-1 for sample sd } 
Not quite sure what the issue is here seen as I copy and pasted exactly. Your code seems totally fine to me. It seems to be a basic Max function so I can't understand what is possibly going wrong. I might dig a little deeper and see what StackOverflow and Google say. I'm sure it's a simple syntax issue and I'm just leaving out a tiny bit of code! 
Thank you for the link. I'll watch this tomorrow evening when I get home from work. I'm new to R so any help is much appreciated. I guess this problem of mine continues! Hopefully I'll get a quick and easy solution soon.
maybe this syntax will work? my R is a bit rusty tbh for(i in 2:nrow(pollTable)){ pollTable[i, "To"] = max(pollTable[i, "To"], pollTable[i - 1, "To"]) }
Thank you!
Unfortunately this still seems to set the entire "To" column to the most recent date. Your'e finally beginning to understand my pain in this situation! Also, your chart is beautiful in its own artistic way. Upvoted.
I've used Eclipse and Rstudio. Eclipse was more difficult to get set up, at least for me, but seemed more flexible.
Playing with atom now, enjoyong it. I can write up my plugins more later if you're curious.
yes please!
Rkward
Sublime Text REPL
I just use Neovim and I find it more convenient than R-studio. The only thing I found R-studio useful for is sending code lines to R console. But with neovim you can open up terminal in a buffer and send text between buffers even more conveniently. [Here](https://i.imgur.com/xVlqs1D.png) is a screenshot of how it looks like. One time it really came handy when I had to translate some python code to both matlab and R. I was able to split the screen three times and open the REPL for each split below doing everything in parallel - Running python, matlab and R in one neovim instance. And of course add to that superb text editing abilities of vim.
I've used Emacs and [Emacs Speaks Statistics (ESS)](https://ess.r-project.org/) for about six or seven years.
Another neovim user here, using Nvim-R. I find it an incredibly lightweight and convenient setup.
This is what I used before RStudio became a thing. 
I use emacs + ESS with polymode for RMarkdown. Once you get the hang of the keyboard shortcuts it's awesome.
Not an R specific book but I quite like neuralnetworksanddeeplearning.com (partially because it's freely available online) it has lots of python code in so you could translate code if you wanted to do in R ( I have done this to experiment with some of the stuff) Interested to know if the book you link is any good though as packt have had good scientific books before. If you do buy it post up a review as I can't be the only one interested. 
will do
here's the code for the book: https://github.com/PacktPublishing/Neural-Networks-with-R
That would be awesome if you would share!
Pardon me if I'm not following because i've done a little GIS work but not in R. Earlier you pass the argument "+proj=longlat +datum=WGS84" to CRS() In the line throwing an error, it looks like you're trying to pass the argument "+proj=utm+zone=... ellps=WGS84" where the spacing could be causing problems? Maybe modify to something like paste("+proj=utm", " +zone=", zone, " ellps=WGS84, sep="") You did account for a space before the ellipse term, though, so...
Another way to do it is look for those users where the start time is between the start and end time of their other rows. Especially if all you want is flag of overlap. Could, depending on the use case, speed up by sorting by users and T1 first then loop with a shortcircut on true, to move to next user.
I already sorted as a "best practice". My issue at this time is that I don't know a good way of doing the "next" command. I'm NOT a great coder and have historically just run pre-made packages. In my mind if I could do binary search on times it'd speed up the problem a TON... this issue though is doing this all efficiently, within a loop. Also something that I discovered is that a non-0 proportion of my users activities overlap when they shouldn't... so I have to filter out on a sub-variable (not included... so imagine account, user-device, t1, t2 where the goal is to see if you have 50 users sharing an account BUT you have the same device showing up repeatedly even when that shouldn't matter)
Wow, yes that was exactly the problem. I didn't realize that lack of a space would throw it off. Thank you so much for taking the time. I greatly appreciate your help. 
Yes. Used to plotly sometimes but ggplot is the Truth
Nope. Ggplot2 is great but it's not the only plotting tool out there and sometimes base plot just makes more sense. 
Depends on what I'm doing. There are times where hist() without any arguments works just fine for my own data exploration. Same with plot().
I'll use base plot for any really quick and dirty plots with no layers or color (hist, scatter). But anything more than that I'm using ggplot. Nothing against base plot, I just use ggplot a lot for final plots anyway, and I honestly forget how to use base plot for anything complex 
Just yes
if the population is infinite: sd(x) * ((length(x) - 1)/length(x)) ^.5 if the population is non-infinite and you know the population size: go off of this (coding is left as an exercise to the reader) http://www.statisticshowto.com/finite-population-correction-factor/ 
I plot exclusively with base plotting. *ducks* To me it just makes more sense. Also I find ggplot defaults to be ugly.
Some ggplotters would probably agree with you re: the ugly defaults. I've been told to not treat any defaults as the final plot anyway Definitely agree with using whatever makes the most sense to you because what makes the most sense to you will help you make the best plots
Next release will use the viridis set of colours by default, if that helps.
Just wanted to second the data.table. Once I got used to the syntax, I forgot the purpose of a pivot table. 
I do, together with the plotly library. Wrap the ggplot output in ggplotly function and all works very nice. Some additional themes with gg themes and it looks stunning (to me) 
Thanks for that. Confusingly the README says MATLAB but the code is in R. I have to admit (at the risk of pre-judging) that I wouldn't personally have high hopes for the book based on the code that's available in the repo. 
theme\_bw() makes things pretty nice imho, but I'll agree that theme\_grey() sucks butt with the default colors. Makes it really hard for colorblind people to see anything at all on the plot.
Apart from a quick hist() here and there yes. Theme_bw() every time. 
No, when I need to present results I'll take the time and use ggplot2. If it's just to judge some data I have I'll stay with base plot. I also use base plot to create animations since ggplot2 is too slow.
+ theme_classic() helps a lot imo
If I'm making something to present I usually use ggplot2. If I'm doing exploration or want user interaction I'll mostly use tableau. 
I use ggplot2 the majority of the time- it's easy enough the remember the grammar there, and I've grown particularly used to the way in which it builds (through the use of `+` and not by making the function call quite as long). With that said, when checking the distribution of something, I'll start with plot(density(x)). 
Yeah, this seems like the better buy: https://www.amazon.com/Introduction-Learning-Using-Step-Step/dp/1484227336/ref=pd_sbs_14_1?_encoding=UTF8&amp;psc=1&amp;refRID=YM2CHQEHQBF9ZCKZ3PFJ 
This is super helpful. Thanks for the post. 
No. It's probably around the 50% mark but it depends on what I'm looking for: plotly, ggplot2, rbokeh, treemap, and occasional other stuff like Metricsgraphics. 
Distinguishing 25 colours simultaneous is near impossible. 6-7 is the usual limit. It's late for me right now, so I cannot offer more advice. Also, your for loop can be simplified as `lonlatcount$longitude &lt;- as.numeric(lonlatcount$longitude) lonlatcount$latitude &lt;- as.numeric(lonlatcount$latitude) ` Also, all your points are red because you include `col="red"`. This, I believe, overrides the aesthetics setting where you bind the points' colour to business type. 
https://stackoverflow.com/questions/29167265/why-do-results-using-carettrain-method-rpart-differ-from-rpartrpar
Oh wow, that looks like the very question I'm asking, I'm working through the same course. Always reassuring when others have had the same problem, lol. Many thanks.
I played with the color settings and it seems to be arbitrary, without the color it comes out as black. I tried using rainbow with it but the order of colors didn’t seem based off of anything. Thank you for the loop tip! I’m changing it now. 
wait so can you just pipe in a ggplot into ggplotly() at the end? that's so much better than adding the ly to the beginning
Based on your code, I suggest you work through some proper ggplot2 examples, because part of your mess is that you are mixing native R plotting arguments with ggplot2. They don't mix. Secondly, the business variable may or may not be what you think it is. Check that it is a character vector or factor. Then try working with a subset of your data with max 6 different business types.
Yes! The output of the ggplot can be used in the ggplotly function. Although that is the way I learned it 
Yup. https://github.com/hrbrmstr/pastebin
thanks - but this is specifically for pastebin ( so I assume it wouldn't work with something like vpaste?) I thought something might scrape the screen, recognize it's a CSV and import as that, or something... failing that I'll try this though, thanks 
Well, you might parse the html, recognise certain tags (pre, code), and paste their contents as a character vector, but... why? :-) Unless you want to scrape a large amount of "paste bins" from unknown sources, that is.
look into Packrat
I just save my work to a script with calls to library() at the top.
I mean is my claim true or not? I just want to make sure I am not misunderstanding anything.
Yes, you're saving your data, not the state.
well i thought it would be easier than moving the files about , if i wanted to send someone a script or whatever.... but yeah, probably not at this point... I'm not going to get into regex and html or something mental :P thanks
Are you really sure you want to execute code which has acces to all of your system accont from random page on the internet? Jesus crist.
Perhaps I wasn't clear enough I wanted to store the CSV information on something like vpaste and read that into a script. 
When you save the workspace when you quit R, the function `save.image` is called. This retrieves all objects in the global environment, including hidden objects, and saves these with the `save` command. To answer your question, it only saves the **objects**, not namespaces. Namespaces are loaded using the `library` command and refers to each package's place to store names. How `save.image` works on environments that are not stored in the global environment, I cannot say.
I just found this package: https://github.com/jennybc/googlesheets It's super easy. And you don't have to share your password. You just share a Google Spreadsheet with whoever and they can use their own authentication to access it from their Google account. So everyone is as safe as their Google accounts.
 1:5+rep(1:5,each=5)
wtf, Developer of package studies quantitative evolutionary morphology. So many fields out there.
What are you trying to accomplish with the project?
I'm still pretty novice at R but what I have done in the past is do a plot(y ~ x) for one linear model and then use points(y ~ x) to add additional lines to the same graph. Hope it helps. 
Plenty of different ways to go about this. I usually get the dataframe in a long format first, where you'd have one row per country per date, with a new variable indicating country 1, 2, or 3. Check out the melt function (package is called melt too I think?) for going from wide to long format. Then do a line plot with a 'by' or 'strata' variable for country.
Magrittr includes a function `extract` for this, and it has functions for most other operators too. You might also be interested in `pluck` from the purrr package, which is similar but much more versatile.
ticker_names &lt;- webpage %&gt;% html_nodes("strong") %&gt;% html_text %&gt;% .[1:388] 
If you're learning R I'd recommend you learning tidyverse packages. Look up Tidyverse cheat sheets. The way to do this is first gather the variables country1:country3 to a new column called country, using cost or whatever those numbers represent as your value column. Then you can us Ggplot together plot the data aesthetics x=year, y=cost, color=country. Then add a geom_line and you'll have yourself a perfect graph which will work no matter how many countries you you have (unlike the other example posted here) 
I agree strongly with your suggestion. I've been using R for well over a decade, so initially, I was very reluctant to use `ggplot2`, `dplyr`, etc. I spent years learning the base plotting and data manipulation functions. I didn't want to learn a whole new syntax and be dependent upon additional packages. Over the past few years, I've started transitioning to the *tidyverse*. Not only are the packages well supported, but they have excellent performance, and their syntax and operating paradigms are so much more logical than the base functions. I highly recommend that people new to R invest the time. [Here's a great place to start.](http://garrettgman.github.io/tidying/)
 ?lines
Extract throws an error saying it doesn't know how to operate on character. Thanks for reminding me about pluck :) 
Duuuuh,how could I forget about doing it this way. Thanks :) 
## A note on variable names In the following demo, I use `DF` rather than `df` for my dataframe. This is because [`df`](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/Fdist.html) is the base R function which calculates the PDF for the f distribution. It may seem pedantic, but I strongly recommend you get out of the habit of choosing variable names that override built-in functions. If you're not sure, you can always try running a variable name before you use it to check if it's already attached to something. That said, here's how I replicated your dataframe: DF=data.frame( MONTH=c('Jan', 'Feb', 'Mar', 'April'), YEAR=2004, COUNTY1=100, COUNTY2=200, COUNTY3=300 ) In the future, you should offer a code snippet like this as well to make it easier for people to build demonstrations relevant to your example and/or to reproduce whatever problem you're having. This is called, among other things, a [minimal working example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example). The better the example you give, the more likely people will be to help you. ## Creating a unified "date" variable (base R only) First and foremost, we need to create a variable that encodes your month and year together. A really quick and dirty way would be to map each month to an integer in [0,11] and then use `year + month/12` as our combined value, giving us 2004, 2004.083, 2004.167, and 2004.250 for Jan-April 2004, respectively. This will look fine for plotting purposes, but by itself it's clearly not very meaningful and we can do better. We have two date-specific datatypes readily available to us in base R: POSIX and Date. POSIX is more granular, but neither is particularly easy to work with (using the standard library, anyway). Using a date datatype here not only gives us a cleaner way to encode date values (for more information on why you shouldn't homebrew tools to work with datetime data, see [this video](https://www.youtube.com/watch?v=-5wpm-gesOY), but will also add meaningful labels to the x-axis. Date and POSIX have different default settings, so whether you use Date or POSIX here is really a matter of preference. I recommend you play with both of these options. Pretty much any method we use, we're going to start by combining the year and month columns into a single character vector: date_strings = with(DF, paste(YEAR, MONTH, '1')) # '2004 Jan 1' '2004 Feb 1' '2004 Mar 1' '2004 April 1' I added the "1" there because the functions I use next will complain if we don't specify the day. Next, we need to tell R how to find different date components in the string. We specify this using a "format string". You can read more about format strings for dates [here](https://stat.ethz.ch/R-manual/R-devel/library/base/html/strptime.html). fmt_str = "%Y %B %d" # %Y : Year with century. # %B : Full month name in the current locale. # %d : Day of the month as decimal number (01–31). Now to add a column of Dates: DF['x_date'] = as.Date(date_strings, format=fmt_str) Or alternatively, we could use POSIX instead. Coercing to POSIX is a little more involved because there are actually two different POSIX datatypes, and we will first convert to one and then the other: DF['x_date'] = as.POSIXlt(strptime(date_strings, format=fmt_str)) Alltogether now! Show me the POSIX! fmt_str = "%Y %B %d" date_posixct = strptime(date_strings, format=fmt_str) DF['x_date'] = as.POSIXlt(date_posixct) ## Using Lubridate Everything about that last section sucks, especially figuring out the format string and the unnecessary extra conversion to make POSIX useful... bleh. Fortunately, we have [lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) to make our lives easier. library(lubridate) DF['x_date'] = with(DF, ymd(date_strings)) You're going to find yourself thanking Hadley Wickham on a regular basis. ## Building a plot (base R), simple example If we follow up a `plot()` call with `lines()` calls, the lines will get added to the plot. The main plot attributes (e.g. axes and labels) will be defined by the original `plot()` call, so we need to make sure that whatever data we plot first covers the full x/y ranges we need for the rest of our data. In your example data, this should be simple: we just plot COUNTY3 first. Unfortunately, R will cut off the y-axis just below 200 and we won't see COUNTY1, so we need to specify the y-axis limits ourselves # Base plot plot(COUNTY3 ~ x_date, data=DF, type='l', ylim=c(0, 300), main="Counties, 1Q2014", xlab="", ylab="Values" # Set title and labels ) # Add lines to the plot lines(COUNTY2 ~ x_date, data=DF, col=2) lines(COUNTY1 ~ x_date, data=DF, col=3) ## Generalizing our plot This was simple enough since we only had two lines we wanted to add, but what if we had a bunch? A good trick is to actually start with an empty plot and then loop over the columns we want to add. Here's what I mean by an "empty plot": plot_cols = 3:5 # Indices of columns we want to plot y_max = max( DF[,plot_cols] ) plot(DF$x_date, # Using a POSIXct vector will put date names on the x-axis 1:nrow(DF), # We need a numeric vector of appropriate length here, can be anything type='n', # This translates to "don't actually draw any data on the plot" ylim=c(0, y_max), # Set y limits main="Counties, 1Q2014", xlab="", ylab="Values" # Set titles ) Now we can loop over the columns we're interested in and add them to the plot: for (i in plot_cols){ lines(DF$x_date, DF[,i], col=i) } Or if we want to be really fancy, we can use an "apply" function to get rid of that loop. This is really a stylistic choice, but will make our code more idiomatic: sapply(plot_cols, function(i) lines(DF$x_date, DF[,i], col=i) ) ## Putting it all together Using lubridate with base plotting: library(lubridate) DF['x_date'] = with(DF, ymd(paste(YEAR, MONTH, '1'))) plot_cols=3:5 plot(DF$x_date, # Using a POSIXct vector will put date names on the x-axis 1:nrow(DF), # We need a numeric vector of appropriate length here, can be anything type='n', # This translates to "don't actually draw any data on the plot" ylim=c(0, max(DF[,plot_cols])), # Set y limits main="Counties, 1Q2014", xlab="", ylab="Values" # Set titles ) sapply(plot_cols, function(i) lines(DF$x_date, DF[,i], col=i) ) ## Now, using ggplot Just kidding! This writeup is already crazy long for such a simple ask. I'll leave ggplot as an exercise for the reader ;) 
&gt; I strongly recommend you get out of the habit of choosing variable names that override built-in functions. In sorry but that's bad advice. *On the contrary*, get used to the fact that names have overloaded meanings in programming. It's unavoidable, and there are mechanisms to disambiguate them (namespaces), so use those. What's more, all caps is conventionally reserved for immutable values/constants, especially outside R (but even in R), so don't use all caps willy-nilly.
Video linked by /u/shaggorama: Title|Channel|Published|Duration|Likes|Total Views :----------:|:----------:|:----------:|:----------:|:----------:|:----------: [The Problem with Time &amp; Timezones - Computerphile](https://youtube.com/watch?v=-5wpm-gesOY)|Computerphile|2013-12-30|0:10:13|40,470+ (99%)|1,412,433 $quote A web app that works out how many seconds ago something... --- [^Info](https://np.reddit.com/r/youtubot/wiki/index) ^| [^/u/shaggorama ^can ^delete](https://np.reddit.com/message/compose/?to=_youtubot_&amp;subject=delete\%20comment&amp;message=$comment_id\%0A\%0AReason\%3A\%20\%2A\%2Aplease+help+us+improve\%2A\%2A) ^| ^v2.0.0
Stylistic decisions like whether or not to use caps are secondary to simply avoiding overriding a function in the core library. Moreover, the caps convention you suggest absolutely doesn't hold in R: * The standard naming convention for datatables (which are a widely adopted stand-in for the base data.frame) is ["DT"](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html). * There are a whole slew of base R functions (and non-base, of course) whose names are capitalized, e.g.: I(), NROW() and NCOL(). You're certainly entitled to your own opinions, but you're giving really strange advice here, especially since I only chose "DF" because it was close to OP's original variable name but avoided overriding any built-in functions. I wouldn't normally name something "DF", but even still my approach here seems pretty obviously superior to advising a novice programmer to be complacent about variable naming and overriding built-ins. It's actually pretty common for me to see bugs in other people's code because they complacently override values like "t", or even worse "T" without thinking about the consequences until they're debugging weird errors like Warning message in if (verbose) {: "the condition has length &gt; 1 and only the first element will be used" It takes so little effort to just pick an unambiguous variable name and it can save so much headache down the line.
&gt; advising a novice programmer to be complacent about variable naming and overriding built-ins This is not at all what I’ve done, on the contrary. Rather than complacency I advocate awareness, and using the correct techniques to avoid errors (rather than impractical advice, which avoidance to name shadowing is). This isn’t new stuff either, it’s pretty established knowledge. After all, that’s why namespaces exist.
&gt; It's real encouraging how after spending a good deal of time and effort on that explanation for OP, I now see you made it a whole three sentences into my top-level comment before downvoting it simply because I capitalized "DF" and told OP to be wary of overriding things. &gt; It's real encouraging how after spending a good deal of time and effort on that explanation for OP, I now see you made it a whole three sentences into my top-level comment before downvoting it simply because I capitalized "DF" and told OP to be wary of overriding things. The capitalisation was a side note. I shouldn’t have mentioned it at all. The 
https://stackoverflow.com/questions/31575585/shared-memory-in-parallel-foreach-in-r
Instead of getting the filenames in the loop, you can get the index and then subset on that index. e.g file_names = c("a.dat", "b.dat", "c.dat") for (i in seq_along(file_names)) { file = file_names[i] # get the i'th row from the file example_row = file[i, ] } 
Thanks for helping! Yeah indexing the files would help a lot but I guess the issue is that I was trying to avoid reading them in individually because I have to do it several times with different number of files with different names.
What kind of server is it? If you've got a Linux or Unix machine you can use a "FORK" cluster instead of the default PSOCK. A FORK cluster will point to a shared memory space. You would do this when you initialize your cluster. Something like... ``` no_cores &lt;- detectCores() - 1 cl &lt;- makeCluster(no_cores, type="FORK") registerDoParallel(cl) ``` If you're using a Windows machine or a cluster of seprate Linux machines, you have to stick with PSOCK though. http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#fork_psock
This...can't beat Nvim-R.
You can send lines and more with Nvim-R plugin.
I'm having trouble grasping what the structure of firstqtd, secondqtd, etc. are like. Is one column all the team names and the next column over their total touchdowns? Rather than combining all four of those data frames together and repeating the team names, you may want to add only the touchdown total of each quarter. This should be easy if the list and order of the teams is the same for all four quarters. 
Well if the files are small enough that you could fit them all into memory at the same time, then you could load them into a list. Without more info it's a bit difficult to offer more advice.
Right now I can't help you, just wanted to say thanks for the NFL data. I have been looking for something similar.
Actually each of these are a one variable data frame with row names for the teams. I had to do it out by quarters because the underlying data frame column name for quarter is a variable containing all the quarters. So I filtered each by the specific quarter so I could recombine them and plot them against one another 
Okay I see. So you are ending up with a data frame of just one column per quarter, and the row names are retained. A couple things I'm noticing: you create tdnames after using it. That could be an issue. In ggplot() there's a typo where you assign the column names to the X axis (touchtowns --&gt; touchdowns) Other than that I'm not sure why it wouldn't be working. I'm not as practiced with ggplot as I should be, so I'd have to play around with it.
You could do something like this I suppose. ``` library(purrr) library(lubridate) df1$Inside &lt;- map_int( df1$Date, function(d) map2_lgl(df2$FirstDate, df2$SecondDate, ~ between(d, .x, .y)) %&gt;% sum() ) ``` And the result: ``` Date Inside 1 2015-01-01 3 2 2015-01-02 3 3 2015-01-03 4 4 2015-01-04 2 5 2015-01-05 1 ```
reddit-overflow? So without having your data I had to re-create a situation. I did this by sourcing a bunch of random datasets from base R and then saving the data.frames to individual files in a sub directory with file extension `.dat`. If i'm understanding your question, the iteration within the loop is integral to which `row` will be needed from each file in our array of files. So to accomplish, in a quick and dirty way, let's do this: create an object we will use for counting. `iter &lt;- 0` The idea here is that each iteration of our loop, we will add `+1` to the counter so that we can extract the proper row. So now, we can loop through each file that matches our patter, and which contains a data.frame object presumably, extract only that row, and not load all of the file into the `.GlobalEnv`(global environment), but only into the calling environment inside the loop... and therefor discard the extraneous info before exiting the loop. `lapply(Sys.glob('tdata/*'), function(i){ iter &lt;&lt;- iter + 1 raw_tbl &lt;- read.table(i) if(iter &lt;= nrow(raw_tbl)){ raw_tbl[iter, ] }else { raw_tbl[nrow(raw_tbl), ] } })`
check out the lubridate package. It has a lot of useful functions for working with dates.
It's a Linux machine (Red Hat 7). I'll try this out, thanks. To be sure, I'll copy in this and then call foreach like I did before?
 sapply(df1, function (x) sum(x == df2[,1] | x == df2[,2])) alternatively this might also work: colSums(table(df2)) + rowSums(table(df2)) - trace(table(df2)) (basically occurences in col 1 plus col 2 independently minus occurances in both)
This is exactly what I was looking for. Thank you!
Put the file names in a list and run an lapply() over them, with the function described above, just be careful where you're putting the reads or you'll kill R with enough data 
That worked thanks so much! 
Why bother? Pivot tables do calculations based on existing data. Just use that data and dispense with the pivot table. 
well how does the code look that doesn't work?
I forgot to mention, the data is scattered across a lot of spreadsheets in different formats. My plan so far has been to hunt them all down and group them together for each report, but if I can just manipulate the pivot that would be easier. 
Right now I have storage_vec &lt;- NULL for (i in 1:storage_vec) { storage_vec[i] = mean(bd$bdeadbes[bd$year[(i-5):(i-1),"year"&gt;1950]] Year starts from 1946, but I need to get average from 1951. Thanks!
Take a look at this then post some code so we can help you: https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
tmp is the dataframe name: &gt;for (year in tmp$year){ &gt; &gt;if (year &lt; 1951){ &gt; &gt;next &gt; &gt;} &gt; &gt;year.result = mean(tmp$bdeadbes[(year-5) &gt;= tmp$year &amp; tmp$year &lt; year]) &gt; &gt;print(paste("Result for year", year, "=", year.result)) &gt; &gt;}
four spaces at the beginning format it to code &gt; 1:storage_vec you would want the length of the storage_vector, if it actually had a length. For the first solution I would just assume the years are ordered correctly, you can always add that later. An initial solution might be: result_vec &lt;- rep(NA, nrow(bd)) for(i in 6:nrow(bd)) { result_vec[i] &lt;- mean(bd$bdeadbes[i-(5:1)]) } If the years might be in another orders, the simplest solution would probably be to assign them to the rownames, then you can select the columns more easily with them (but you have to switch between numeric and character representation for that to work!)
If the formats are the same, just write a script to import them. If they aren't the same, get a piece of hose and whip your upstream dependencies til they stop being so fucking stupid. It's less pain in the long run.
Here you go yearset = NULL yearset$year = as.integer(c("1946","1947","1948","1949","1950","1951","1952","1953","1954","1955")) yearset$value = as.integer(c("295541","396708","472363","434321","546501","393740","147221","96986","69107","29888")) yearset = as.data.frame(yearset) results = NULL for (i in 1:nrow(yearset)) { tmp = NULL tmp = as.data.frame(yearset[i:(i+4),]) tmp = tmp[!is.na(tmp$year),] tmp = as.data.frame(tmp) tmp$year = as.integer(tmp$year) meanval = as.double(mean(tmp$value)) lowyear =as.character(tmp[(nrow(tmp))-(nrow(tmp)-1),]$year) upyear =as.character(tmp[(nrow(tmp)),]$year) tmp = NULL tmp$lowyear = lowyear tmp$upyear = upyear tmp$meanval = meanval results = bind_rows(results,tmp) } 
Ok so I was able to accomplish what you were shooting for and I also made my own plot which is less noisy and probably closer to what you are looking for. Just a couple notes on your code before I dive in: * Having the team names as row-name instead of the variable makes it a lot harder to plot the data the way you want * ggplot is much easier to work with when your data set is tidy * You can use data.table to compile a tidy data set for plotting in 1 line of code The main thing that I have found in working with ggplot2 is that it is much easier to accomplish most tasks when you are working with a tidy data set. If you want to read up on tidy data I've found [this article to be super useful](http://www.stat.wvu.edu/~jharner/courses/stat623/docs/tidy-dataJSS.pdf). The data set that you have compiled violates the tidy data principle that each column should contain information about only 1 variable. In your data, columns Q1, Q2, Q3, Q4 contain information about the quarter that the touchdowns were scored in AND the number of touchdowns. For us to make this a tidy data set, we would want it to look more like this: Team | Quarter | Touchdowns -------|----------|-------------- ARI | 1 | 9 ARI | 2 | 17 ARI | 3 | 12 ARI | 4 | 16 ATL | 1 | 16 ... | ...| ... WAS| 4 | 17 As I mentioned in my post, you can compile a tidy data set with ease using data.table. If you want to learn basic data.table code, [datacamp has a free course for beginner data.table syntax](https://www.datacamp.com/courses/data-table-data-manipulation-r-tutorial) where I learned how to do these type of manipulations. library(data.table) # Read in data nfl &lt;- read.csv(filepath) #Create Tidy Data Set with data.table touchdowns &lt;- setDT(nfl)[IsTouchdown == 1, .(Touchdowns = sum(IsTouchdown)), by = list(OffenseTeam, Quarter)][order(OffenseTeam, Quarter)] 
Thanks for the links, I will check them out. Did you read my edit?
Wow, no I totally missed your edit, got too excited manipulating data (apparently unnecessarily haha). Guess we both learned something today! 
No no, thank you for the links. It inspired me to sign up for a month of datacamp(finally)I'm reading the publication now too-it's always good to learn from Hadley Wickham. You are right about the noisey data- my intention is to make this into a shiny app where you can select what teams to view. 
I suggest you check out Power BI
Reactive variables are tricky to figure out when you first start with Shiny. You have to call reactive variables as a function to access their values: current_value &lt;- reactive(input$variable) current_value_plus_two &lt;- current_value() + 2 
Not sure what you actually mean, but I feel like outer is the function you are looking for
What about just table()?
Actually i think im looking for an edge table and adjacency matrix. But I will ask the question again later with some data as my description above isnt very clear I admit. Thanks
Are you looking for something like this? library(dplyr) library(tidyr) x = c(1, 2, 3, 4) y = c(9, 8, 7, 6) expand.grid(x, y) %&gt;% as.data.frame %&gt;% mutate(sum = Var1 + Var2) %&gt;% spread(key = 'Var2', value = 'sum') %&gt;% select(-Var1) %&gt;% as.matrix 
/u/loyalservant gave you part of the answer - things you define with `reactive` are functions and must be called to get their most recent value. The important piece of context that is missing from that answer is what exactly an "active-reactive context" is. The error message as you don't have one, and that's causing a problem. So, what is one, why is it important, and how do you get one? The first thing you have to understand is that when you have some kind of input mechanism on the page, like a slider or a text box, you need to have a way to say to the server part of your application, "hey, this thing just changed, so go update the app". The way you do this is with a reactive context. "Reactive" just means "is listening for changes and will run some code when those things change". So when you put your `input$variable` inside `reactive`, you're creating a reactive context - it's a function that will return whatever value it had at the start, except if one of its inputs gets changed; then it will re-run the code to calculate a new value. `reactive` is a very general case of reactive contexts though. Usually you don't need to use it when you're making simple applications. The `shiny` package also provides some specific cases of reactive contexts and they're what you probably want to use most of the time - they're the `renderX` functions like `renderDataFrame`, `renderPlot` and `renderText`. They also define reactive contexts and have the added benefit of defining their output - presumably your code has an output eventually, so you could just put all your code inside the `renderX` function that makes the kind of output you're looking for, and then it will all be in a reactive context without having to worry about creating your own contexts and what you assign them to and whatnot.
Ok, cheers.
Can you help with a few data samples? Are there any common fields? Any joins? 
 mat &lt;- matrix(1:9, ncol=3) outer(colSums(mat), colSums(mat), FUN=`+`)
Right, many thanks. Just for some context, I'm generating a world map and I want the user to select a latitude and longitude. I want each of those quantities to be chosen by three drop-down lists where the user can select the degrees, arc-minutes and arc-seconds. This much I have achieved. But I want to then convert this into a single decimal number to then place the marker on the map. This is where I ran into trouble - accessing these values so I could make this conversion, and pass the decimal latitude &amp; longitude to addMarkers() or whatever it's called. I'll have a crack now and see what happens.
Assuming the following is rougly what you're dataframes look like you can subset these. df = read.table("10001.csv", sep = ",", header=TRUE) df.rtu01 = df[df$col3 == "RTU 01", ] df.rtu01 = df[df$col3 == "RTU02", ] col1 | col2 | col3 | col4 ---|---|---|--- 10001 | Chicago | RTU 01 | Stage 01 Command 10001 | Chicago | RTU 01 | Stage 02 Command 10001 | Chicago | RTU 01 | Setpoint 10001 | Chicago | RTU 01 | Zone Temperature 10001 | Chicago | RTU 02 | Stage 01 Command
Yes, and the point I'm making is that you don't need to do any of this if you only evaluate things in the context of the map. In your call to `renderMap` or whatever function you use to render out the map, you can also do all the maths and evaluation on those variables. Then you don't need to define more than one reactive context.
Lubridate could probably help. Something like this maybe? library(lubridate) # Your call duration in hours-minutes-seconds format. call_duration &lt;- hms("00:02:34") # Some sort of base date/time, if you have it. base_time &lt;- ymd_hms("2017-10-18 12:00:00") #Result base_time + call_duration https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html
So... the way my Rmd file is currentlt structured is, I've generated my map before I've done anything else, like this: &gt; my_map &lt;- leaflet() %&gt;% addTiles() Then I have my inputPanel() with the calls to selectInput() inside it. Then I have just those lines I've already pasted here. I haven't had any render&lt;Whatever&gt; calls. Are you saying that I could put everything inside leaflet() or addTiles()? 
Something like this? outer(1:5, 1:5, `+`) [,1] [,2] [,3] [,4] [,5] [1,] 2 3 4 5 6 [2,] 3 4 5 6 7 [3,] 4 5 6 7 8 [4,] 5 6 7 8 9 [5,] 6 7 8 9 10 
You should have a server configured somewhere, right? If you take a look at [this example](https://shiny.rstudio.com/tutorial/written-tutorial/lesson1/), it shows the server under the heading "server". The UI is where all your inputs and panels and things live, but the server is where the reactive processing happens. Does your shiny app not look like those examples? If you're using an [interactive document like this](http://rmarkdown.rstudio.com/authoring_shiny.html) then your server and UI coexist a bit, but you should still be rendering your outputs from a reactive context as the examples in that document do.
[Here's](https://rstudio.github.io/leaflet/shiny.html) a page on leaflet and shiny. You'll need to generate your map within the renderLeaflet function, which will likely include a call to leaflet() and addTiles(). Here's a sample document that runs for me. --- title: "Leaflet Example" output: html_document runtime: shiny --- ```{r setup, include=FALSE} knitr::opts_chunk$set(echo = TRUE) library(leaflet) ``` ```{r points, echo=FALSE} inputPanel( selectInput("lat_degree", label = "Latitude", choices = seq(-90, 90, by = 10), selected = 0), selectInput("long_degree", label = "Longitude", choices = seq(-180, 180, by = 10)) ) renderLeaflet({ # This is your reactive context. # render___ tells Shiny to make this thing that will change with changing input values # Create some random data points &lt;- data.frame( lat = rnorm(100, mean = as.numeric(input$lat_degree)), long = rnorm(100, mean = as.numeric(input$long_degree)) ) leaflet() %&gt;% addTiles(urlTemplate = "//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png") %&gt;% setView(as.numeric(input$long_degree), as.numeric(input$lat_degree), zoom = 5) %&gt;% addMarkers(data = points) }) ``` 
I don't know about this server + UI stuff. I have seen both of those functions discussed before, but I've not used them here. [Here](https://lesbattersby17.shinyapps.io/week_2_assignment/) is everything I have so far. I should read more about them. What I've been doing is making an R markdown file
Right, thanks, I'll see what I can do with this. I notice the following though: &gt; renderLeaflet({ what's with the immediate { after the ( ? Is that something specific to render___ or something shiny use involves in general?
You enclose any multi-line expressions in {} so that R knows that these lines are a chunk of code and that you didn't forget a parentheses instead. You're passing the whole expression into Shiny. If you had only one line, you could leave the curly brackets off.
Ahhh... I've used R for about 6 months without realising this lol
It isn't something that comes up that often outside of function/if statement contexts (where you'd expect to have curly brackets anyways). You can technically have a function defined like this: x &lt;- function(z) z^2+4 and that will work, but it is much nicer to use the expression notation even for a single-line function. Same with if/else statements... you can do them without the curly brackets, but it's easy to mess up, so most people don't. 
I want to loop through each CSV and create a data frame for each stage. From my example, I want a data frame called “10001 RTU 1 Stage 01 Cooling Command” that includes columns: 10001 RTU 01 Stage 01 Cooling Command, 10001 RTU Zone Temperature, 10001 RTU Cooling Setpoint, with the next data frame called 10001 RTU 1 Stage 02 Cooling Command” that includes columns: 10001 RTU 01 Stage 02 Cooling Command, 10001 RTU Zone Temperature, 10001 RTU Cooling Setpoint, etc for RTUs 01-06 in this CSV. I need to do this for over 300 stores and each store (CSV) has 6-10 RTU. I ultimately need a way to loop over each CSV and pull out each compressor Command with its associated RTU zone temperature and its associated RTU temperature setpoint. There should probably be a substring matching (going through a list of sites (ie 10001, 45555, etc. ) and pull out a separate data frame for each compressor and its associated RTU (each RTU should have 1-4 compressors, and there are 6-10 RTU per store). Thank you so much for the help I’m kind of panicking ha 
Sure I will post some when I get back home. I responded to the comment below with more info - thanks for your help. 
Oh, wait, I misunderstood. I do this regularly.
You're most likely going to want to create a function to accomplish this. But first, it's hard to understand what you're trying to accomplish. It seems like you want some rows in more than one resulting dataframes? Second: &gt;I want a data frame called “10001 RTU 1 Stage 01 Cooling Command” that includes columns: 10001 RTU 01 Stage 01 Cooling Command, 10001 RTU Zone Temperature, 10001 RTU Cooling Setpoint We don't know what's a column and what is just a space between words here. Can you format some tables to show us what the dataframes look like, and what the resulting tables need to look like. Hit the source button under my previous comment if you don't know how to format tables. 
Nice, I will checkout lubridate, thank you!
Sure I’ll post data when I get home but the columns are separated by a comma here. 
If you are just asking about estimating that function, here you go: https://www.r-bloggers.com/fitting-a-model-by-maximum-likelihood/ If you are estimating other parameters in your later model, you should estimate the full model jointly, which is more difficult.
I think I understand, you're data looks like: names = c("10001 Chicago RTU 01 Stage 01 Command", "10001 Chicago RTU 01 Stage 02 Command", "10001 Chicago RTU 01 Setpoint", "10001 Chicago RTU 01 Zone Temperature", "10001 Chicago RTU 02 Stage 01 Command") data = rep(1, 5) data2 = rep(2, 5) data3 = rep(3, 5) df = as.data.frame(rbind(data, data2, data3)) colnames(df) = names #So "df" looks like: 10001 Chicago RTU 01 Stage 01 Command | 10001 Chicago RTU 01 Stage 02 Command | 10001 Chicago RTU 01 Setpoint | 10001 Chicago RTU 01 Zone Temperature | 10001 Chicago RTU 02 Stage 01 Command ---|---|----|----|---- 1 | 1 | 1 | 1 | 1 2 | 2 | 2 | 2 | 2 3 | 3 | 3 | 3 | 3 You're going to want to use the "grepl" on your dataframe column names. "grepl" returns TRUE if matching a regular expression and FALSE otherwise, so you can use it in the subsetting process: #Find columns with RTU 01 and extract df[ , grepl("RTU 01", names(df))] #Returns: 10001 Chicago RTU 01 Stage 01 Command | 10001 Chicago RTU 01 Stage 02 Command | 10001 Chicago RTU 01 Setpoint | 10001 Chicago RTU 01 Zone Temperature ---|---|----|---- 1 | 1 | 1 | 1 2 | 2 | 2 | 2 3 | 3 | 3 | 3 Is this the basis of what you're trying to do? But then for each csv you want to split by RTU # into separate dataframes?
OK I have almost finished what I'd like to do, apart from one last little bit that it'd be great if you could advise me on. Here's my current app: https://lesbattersby17.shinyapps.io/week_2_assignment__v2/ The problem I have is, there's an input field called 'location name', it starts off empty and I want the user to enter it so that it appears beside the marker. I can get a word to appear if I specify it myself in the code (it just says "test" here) but my attempt to get the user-inputted string from textInput() to addMarkers() fails and I'm not sure why? It looks like the same method as I've used for the latitudes etc.
Cool. Sorry about the reformatting... I tried like 10 times to clean up then just threw in the towel...
It's a data frame that you want. I'm on mobile, so I can't give a code example, but look up the syntax, it should be pretty cut and dry.
stargazer, in my opinion, is indeed the easiest way to do this. And yes, you can specify the desired statistics with the "summary.stat" argument: stargazer(dat, summary.stat = c("mean", "median", "sd"))
Thanks for the link. Is it really necessary to estimate the full model jointly? I believe I can use the parameters estimates in my aggregate and then run the analysis. Anyway, I'll see about that if I can figure it out...
&gt; I've just joined Stackoverflow so I can't post anything there yet. You can post questions as soon as you’re registered; see [Stack Overflow Privileges](https://stackoverflow.com/help/privileges/create-posts) To answer your question: you’d generally read the data in as *one* table, and then continue operating on it as a single table. The ‹dplyr› package contains tools for operating — in parallel — on subsets of the table (notably, the `group_by` function). That said, you can split the table using the same workflow: library(tidyr) library(dplyr) whole_table = readr::read_csv('filename.csv') table_list = whole_table %&gt;% group_by(Stage) %&gt;% nest() %&gt;% with(setNames(data, Stage)) Since I don’t have your data, this might need to be adapted slightly. However, here’s a working example using the builtin `iris` data set, using the same approach as above: iris %&gt;% group_by(Species) %&gt;% nest() %&gt;% with(setNames(data, Species))
Do not use the stargazer package. It's completely unnecessary and you NEED to understand different variable types or you will never be able to use R properly. You have two base options for creating a table. One is a matrix, the other is a data.frame. You can google the differences and how to create each one and how to manipulate them.
You can, but your standard errors will be wrong. The other option is to do it in two steps but bootstrap the entire process. 
&gt; arc-seconds drop-down list I wanted to have it 0-60 but going up in non-integer steps - yet the highest number it will allow is 10 when I try to make it include decimals This sounds like you might be better off with a slider? inputPanel( selectInput(inputId = "long_degree", label = "longitude degrees", choices = c(-180:180), selected = 45), selectInput(inputId = "long_arcmin", label = "longitude arc minutes", choices = c(1:60), selected = 30), sliderInput(inputId = "long_arcsec", min = 0.01, max = 60, value = 30, step = 0.01, round = -2, label = "longitude arc seconds") # selectInput(inputId = "long_arcsec", label = "longitude arc seconds", # choices = seq(0.01, 60, by = 0.01), selected = 30) ) As for the other... setting the text to input$location_name worked for me leaflet() %&gt;% addTiles() %&gt;% setView(dec_lat_degree, dec_long_degree, zoom = zoom_level) %&gt;% addMarkers(lat = as.numeric(dec_lat_degree), lng = as.numeric(dec_long_degree), #label = as.character(location_name), label = input$location_name, labelOptions = labelOptions(noHide = T, direction = "topright", style = list("color" = "red", "font-family" = "serif", "font-style" = "italic", "box-shadow" = "3px 3px rgba(0,0,0,0.25)", "font-size" = "12px", "border-color" = "rgba(1,0,0,0.5)") ) )
 A &lt;- rnorm(100) B &lt;- rnorm(100, mean = 1) C &lt;- rnorm(100, mean = 2, sd = 2) library(dplyr) # package to work with datasets # Create a data frame: myTable &lt;- data_frame( variable = c(rep("A", length(A)), rep("B", length(B)), rep("C", length(C))), value = c(A, B, C) ) %&gt;% # This says complete any other operations on each variable one at a time, # then combine everything back together group_by(variable) %&gt;% # summarize each variable: summarize( mean = mean(value), median = median(value), std.dev = sd(value) ) If you want to make a pretty, well formatted table, you could use a library like knitr: library(knitr) kable(myTable) kable() will output tables in HTML or LaTeX markup if you prefer.
"Could not find function group_by" The program also couldn't find data_frame but had no problem with data.frame, so I tried group.by and groupby but to no avail.
I tried that exact code before and it caused R to display the entire contents of the data sets.
I'm not sure why it would. Anyway, my suggestion of doing it this way seems to be somewhat frowned upon, so you might want to follow the other commenters' advice. Sorry
UPDATE I installed packages rlang and dplyr. Now when I run library(dplyr), I get the following message: Error: package or namespace load failed for ‘dplyr’ in library.dynam(lib, package, package.lib): DLL ‘bindrcpp’ not found: maybe not installed for this architecture?
If it's any consolation, I've also consistently had the exact problem with all of the table commands I could find on Google, stack overflow etc. I strongly suspect that my operating system is interfering with R somehow, as I'm constantly having to install several packages to access a single library. Uninstall + reinstall hasn't helped.
After installing six new packages, I finally got R to access dplyr. Now I'm getting this error message: Error in group_by(variable) : object 'variable' not found
googling the error message: https://stackoverflow.com/questions/24383746/mle-error-in-r-initial-value-in-vmmin-is-not-finite Your function is producing bad numbers.
You're probably getting that error because you haven't formatted your data set like I did. If your three variables are called `A, B, C` then you would indicate which variable a value is from in the "variable" column and you'd have a corresponding value column. That's what this code does: # Create a data frame: myTable &lt;- data_frame( variable = c(rep("A", length(A)), rep("B", length(B)), rep("C", length(C))), value = c(A, B, C) ) Once you have that data frame, you can calculate the statistics you want using the group_by() code and the other stuff that follows. If you can post your data, I can change the names of things to help you... or you can try to figure out how to translate my code into stuff that works with your variables. 
After copying and pasting the code provided: Error: cannot open file 'C:/Users/IDK/Documents/R/win-library/3.4/magrittr/R/magrittr.rdb': No such file or directory &gt; install.packages("margrittr") Installing package into �C:/Users/IDK/Documents/R/win-library/3.4� (as �lib� is unspecified) Warning in install.packages : package ‘margrittr’ is not available (for R version 3.4.2)
You need to show some sample data and what you expect your output to look like - your description is way too hard to parse as-is.
So the complete data set looks like this https://i.imgur.com/iYyZVKC.jpg and I'm trying to get each individual column (PW1B03, PW1B09 etc) into its own matrix with the year like this https://i.imgur.com/uYyDLPO.jpg . So this is an example of the code I'm currently using to get that result https://i.imgur.com/DovB2HE.jpg . I'm going in for each individual and inputting the year span and rows that have data in them (not NA) and therefore the number of rows the matrix needs. I'm also individually naming them the same as the column they are taking data from in the original data set. I'm looking to write a loop that would allow me to do this without having to individually figure that out for each column. Hope this helps clarify and thank you in advance for your help!
Maximization of the entire model at once would probably be hard to do, though... I think I'll try estimating my parameters for the aggregate function and use that later in the RE and see what I get... I appreciate the help! Cheers
You probably don't want to do this the way you're describing, then. There are a few different paradigms you could use. Why not iterate through your data columnwise? df &lt;- data.frame(a = 1:5, b = 11:15, c = -1:-5) apply(df, 2, sum) #&gt; a b c #&gt; 15 65 -15 You can change `sum` to whatever function you want. or, "[tidy](www.tidyverse.org)" your data: df &lt;- data.frame(a = 1:5, b = 11:15, c = -1:-5) rownames(df) &lt;- 1956:1960 df #&gt; a b c #&gt; 1956 1 11 -1 #&gt; 1957 2 12 -2 #&gt; 1958 3 13 -3 #&gt; 1959 4 14 -4 #&gt; 1960 5 15 -5 library(tidyr) library(tibble) df %&gt;% rownames_to_column() %&gt;% gather(individual, measurement, -rowname) #&gt; rowname individual measurement #&gt; 1 1956 a 1 #&gt; 2 1957 a 2 #&gt; 3 1958 a 3 #&gt; 4 1959 a 4 #&gt; 5 1960 a 5 #&gt; 6 1956 b 11 #&gt; 7 1957 b 12 #&gt; 8 1958 b 13 #&gt; 9 1959 b 14 #&gt; 10 1960 b 15 #&gt; 11 1956 c -1 #&gt; 12 1957 c -2 #&gt; 13 1958 c -3 #&gt; 14 1959 c -4 #&gt; 15 1960 c -5 Then you can use other verbs to do whatever groupwise operations you want to do to each group: library(dplyr) df %&gt;% rownames_to_column() %&gt;% gather(individual, measurement, -rowname) %&gt;% group_by(individual) %&gt;% summarise(total = sum(measurement)) #&gt; # A tibble: 3 x 2 #&gt; individual total #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 a 15 #&gt; 2 b 65 #&gt; 3 c -15 And this is more flexible for many kinds of operations.
Base R solution as opposed to fang_xianfu: df &lt;- as.data.frame(matrix(1:25, ncol = 5)) df[1:2, 2:3] &lt;- NA names(df) &lt;- c("year", letters[2:5]) # Make a list of all combinations of columns (except first column containing "year") # when choosing 1 out of n columns columns &lt;- combn(x = df[,-1], m = 1, simplify = FALSE) # Bind each of the resulting columns with "year" df_combs &lt;- lapply(columns, FUN = cbind, year = df[, "year"]) # Apply na.omit to each dataframe df_sans_na &lt;- lapply(df_combs, FUN = na.omit) # names(df_sans_na) &lt;- names(df)[2:5] # data.table::rbindlist(l = df_sans_na, # idcol = "columns") The last part is if you want them recombined to the same format as fang_xianfu's example.
Ah, that slider does look like a good option but I wondered how hard it would be for the user to drag it to a specific value. I suppose it's possible to combine the slider with a user input? Perhaps have the user input become the variable that gets passed on if there's anything other than "" in that input field? I might have a go at that. Also, why is it that I get an error when I store input$location_name in location_name and then pass location_name to addMarkers(..., label = ...), but you don't when you pass input$location directly to addMarkers(..., label = ...) ? 
Might be a new R dep? Is there a libreadline or a readline Conda package? Could try installing that.
I’m lazy so can you tell us more about Discord and what you are thinking. 
Think of it as a platform that hosts servers(let's say an RLanguage server) which are comprised of Channels. We could have topical channels, for example "General chat", "Announcements", "Projects", "Resources". Most people use those channels for text messaging and media sharing. However voice chat (like skype) is also supported if people feel like speaking :). As a bonus the server is completely customizable through bots, which we can code to do anything we want, like play youtube music by just typing "rickroll" for example.
Found this discord server invite from r/learnprogramming if anyone is interested. Maybe we could integrate there under an R channel. https://discord.gg/9zT7NHP 
What is the advantages over slack or IRC?
Haven't really used IRC but I'd say it's generally more accessible. You just need a browser. I suggest to join the discord server on my other reply to get a better sense of a well run server. You will have to wait 3 minutes before you can view the channels.
I thinked that the problem was about the choice of the starting values of the parameters, but i simulated another set of data from a Gamma distribution (my function is a Gamma mixture) and I've got some results. Unfortunately those results are very incorrect, so i thinked to see if all my routine is working well simulating realization of random variables with the above probability density function. There's a way to simply do this in R? 
There's a slack community for a people going through R for Data Science and a bunch of mentors to help out. I'll get the link if interested?
Yeah! Share it!
Thank you!
You can use the library "DBI". # Loading library. library(DBI) # Opening connection to SQLite database. mydb &lt;- dbConnect(RSQLite::SQLite(), &lt;database name&gt;) # Getting data. dbGetQuery(mydb, &lt;SQL statement&gt;)
So I load this, and then just actually type in SQL? I think I was told such a thing is possible but then forgot :(
I assumed that you are trying to access an SQL database through R. If so, have it in your working directory when you run the code, fill in &lt;database name&gt; with your database name as a string, and &lt;SQL statement&gt; with the SQL statement you want to use as a string.
Actually not-- I'll be reading in csvs that I'd prefer to load into a SQL database but can't.
Look at the [data.table](https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html) package which uses SQL-like syntax which you may get along with.
`install.packages(sqldf)`
You may need to debug a bit since I'm doing this from the top of my head, but this should do it using dplyr: library(dplyr) **Query 1** table %&gt;% group_by(variable1)%&gt;% mutate(v2 = sum(!is.na(variable2)), v3 = n_distinct(variable3)) %&gt;% ungroup()%&gt;% select(variable1,v2,v3)%&gt;% distinct() **Query 2** There might be naming issues with the join column (variable2) table1 %&gt;% left_join(table2, by=c('variable2' = 'variable2'),suffix=c(".A",".B")) %&gt;% group_by(variable1.A, variable1.B) %&gt;% mutate(av2 = n_distinct(variable2.A), BV = n_distinct(variable2.B)) %&gt;% ungroup() %&gt;% select(variable1.A,av2,variable2.B,BV) %&gt;% distinct()%&gt;% arrange(desc(variable2.B),av2) #I assume you meant av2 and not av1 here 
Thanks. Dplyr is what I was trying to use initially, and I was just moving too slow to be practical on a short timeline. This will be very helpful for next time, since I would like to be moving away from SQL/expanding R knowledge generally.
When you run: STARLevel2&lt;-rep(NA,dim(OCDEL)[1]) You create a vector of length `dim(OCDEL)[1]` entirely populated by `NA` values. This means that the following lines of your code don't really do anything. There is no "No STAR Level" or "STAR 1" etc to replace with 0 and 1, because `STARLevel2` just consists of `NA`.
Does the code run properly if you add `STARLevel2` (after you're done creating it) as a column to the `OCDEL` dataset? Assuming `OCDEL` is a dataframe: ``` OCDEL$STARLevel2 &lt;- STARLevel2 ```
Lifesaver!!!
Reach out to https://twitter.com/kierisi - she will be setting up the second cohort soon.
You didn't post the actual assignment. Post it or pm me and I'll let you know if I can help out.
Well thats was smart haha, I was posting it somewhere else at the same time. Awesome I will send you a pm now. Thanks heaps!!
Goal: This final assignment will provide you with the opportunity to demonstrate the full range of skills you have developed during the course. This Task will comprise mainly summative elements. Product: You will work individually to provide an R script of no more than 3000 words that details your working and outlines (as script annotations) how you would go about solving a real-world ecological problem. Submission should comprise a single MS Word file. Construct this file by copying the contents of your R script into a blank Word file, and then saving it with the appropriate name. Format: From the list of problems provided, select ONE, and write an annotated R script to: 1. Import the data into R; 2. Manipulate the data into a form that will allow exploratory analysis and model building; 3. Undertake basic exploratory analysis of the data, including plots, where appropriate; 4. Construct, fit and assess statistical models, including, where necessary, appropriate hypothesis tests; 5. Summarise outcomes of the model-fitting process; 6. Interpret the results clearly and succinctly; 7. Output MS Office-compatible Tables and Figures, as appropriate, to aid in the interpretation of your results; 8. Store outputs, as needed; and 9. Using annotations in your script, explain the rationale of each step and summarise associated results; interpret main patterns from plots or findings from hypothesis tests; and draw conclusions. Assessment Criteria: You will be assessed on: • Clarity and completeness of scripting; • Appropriateness of data manipulation and analysis; • Quality of resulting outputs; and • Depth and appropriateness and accuracy of rationale/explanation and interpretation of results. You’re a wildlife manager, who is currently working on badgers. You are investigating the factors predicting badger sett (burrow) presence in England and Wales. Specifically, you’re interested in which of the available habitats in the landscape is preferred by the badger. Previous research suggests that badgers might prefer either woodland or grassland habitat. On the other hand, you know that, unfortunately, a good number of badgers get killed on the roads every year. And sadly, badgers are still nowadays culled by farmers, either to protect their produce or for the misconception that badgers are going to increase the risk of tuberculosis on their cattle. Over the past eight years, you collected data each time an inhabited badger sett was detected, based on badger presence or signs of recent badger activity. You have also collected data on random sites to know the environmental properties of sites where badgers are not present. The English government has made available a map of the land use of your study area (habitat composition). You classify that into woodland, grassland or human disturbance (including crops and suburban infrastructure). The geographic coordinates of the badger setts allow you to assign them to a habitat type and region and to calculate distance to the nearest road. Finally, you source a historical database of culling licences granted to farmers by local governments, and you identify whether the site was being culled during the sampling interval. The file Badgers.csv contains your results. The first sheet in the file contains metadata (a description of the variables), whist the second contains the data. Construct models to address the following questions: a. Are the number of surveyed points (for presence/absence) on culling areas relative to those on non-culling areas independent of region and habitat type? Fit one model and produce one figure. [20 %] b. Did habitat type, culling practices or distance to road affect likelihood of badger sett presence, and did this hold equally across the regions sampled? Fit one model and produce at least two figures. [80 %] Be sure to explain all significant results in terms of your research questions (i.e., interpret model coefficients both graphically and in words), and speculate about what this result might mean in terms of badger conservation. 
Goal: This final assignment will provide you with the opportunity to demonstrate the full range of skills you have developed during the course. This Task will comprise mainly summative elements. Product: You will work individually to provide an R script of no more than 3000 words that details your working and outlines (as script annotations) how you would go about solving a real-world ecological problem. Submission should comprise a single MS Word file. Construct this file by copying the contents of your R script into a blank Word file, and then saving it with the appropriate name. Format: From the list of problems provided, select ONE, and write an annotated R script to: 1. Import the data into R; 2. Manipulate the data into a form that will allow exploratory analysis and model building; 3. Undertake basic exploratory analysis of the data, including plots, where appropriate; 4. Construct, fit and assess statistical models, including, where necessary, appropriate hypothesis tests; 5. Summarise outcomes of the model-fitting process; 6. Interpret the results clearly and succinctly; 7. Output MS Office-compatible Tables and Figures, as appropriate, to aid in the interpretation of your results; 8. Store outputs, as needed; and 9. Using annotations in your script, explain the rationale of each step and summarise associated results; interpret main patterns from plots or findings from hypothesis tests; and draw conclusions. Assessment Criteria: You will be assessed on: • Clarity and completeness of scripting; • Appropriateness of data manipulation and analysis; • Quality of resulting outputs; and • Depth and appropriateness and accuracy of rationale/explanation and interpretation of results. You’re a wildlife manager, who is currently working on badgers. You are investigating the factors predicting badger sett (burrow) presence in England and Wales. Specifically, you’re interested in which of the available habitats in the landscape is preferred by the badger. Previous research suggests that badgers might prefer either woodland or grassland habitat. On the other hand, you know that, unfortunately, a good number of badgers get killed on the roads every year. And sadly, badgers are still nowadays culled by farmers, either to protect their produce or for the misconception that badgers are going to increase the risk of tuberculosis on their cattle. Over the past eight years, you collected data each time an inhabited badger sett was detected, based on badger presence or signs of recent badger activity. You have also collected data on random sites to know the environmental properties of sites where badgers are not present. The English government has made available a map of the land use of your study area (habitat composition). You classify that into woodland, grassland or human disturbance (including crops and suburban infrastructure). The geographic coordinates of the badger setts allow you to assign them to a habitat type and region and to calculate distance to the nearest road. Finally, you source a historical database of culling licences granted to farmers by local governments, and you identify whether the site was being culled during the sampling interval. The file Badgers.csv contains your results. The first sheet in the file contains metadata (a description of the variables), whist the second contains the data. Construct models to address the following questions: a. Are the number of surveyed points (for presence/absence) on culling areas relative to those on non-culling areas independent of region and habitat type? Fit one model and produce one figure. [20 %] b. Did habitat type, culling practices or distance to road affect likelihood of badger sett presence, and did this hold equally across the regions sampled? Fit one model and produce at least two figures. [80 %] Be sure to explain all significant results in terms of your research questions (i.e., interpret model coefficients both graphically and in words), and speculate about what this result might mean in terms of badger conservation.
R for Data Science by Mr Wickham. 
Cheers! Will have a look if I can get that somewhere nearby, or order online.
&gt; Perhaps have the user input become the variable that gets passed on if there's anything other than "" in that input field? I might have a go at that. You can definitely do this - you'll have to use some observe() statements to make it happen, but it's doable. Look into the updateSliderInput() function. You're storing input$location_name in location_name outside of a reactive context (e.g. outside of the renderLeaflet() function). You can't access reactive variables outside of a reactive context unless you use the isolate() function (and using that will keep the plot from updating when inputs change, so don't!) 
You'll want to do `install.packages("magrittr")` instead of "margrittr" :)
The following are available online.... * [R For Data Science](http://r4ds.had.co.nz/) by Hadley Wickham &amp; Garrett Grolemund * [R Packages](http://r-pkgs.had.co.nz/) by Hadley Wichkam * [Advanced R](http://adv-r.had.co.nz/) by Hadley Wichkam * [Data Science with R](http://garrettgman.github.io/) by Garrett Grolemund * [Text Mining With R](http://garrettgman.github.io/) by Garrett Grolemund * [Advanced Statistical Computing](https://bookdown.org/rdpeng/advstatcomp/) by Roger Peng
So I understand and can replicate what you've done right up until the very end, and I can't figure out how to get each of the individual ones into their own matrix/dataframe like the example I posted at this point?
They should already be individual dataframes in df_sans_na. `df_sans_na` is a list consisting of multiple dataframes. `sapply(df_sans_na)` (this bit of code checks/returns the class of every list element). In order to access the dataframes in the list you make use of list indices: `df_sans_na[[1]]` for dataframe 1, etcetera. If this is not what you wanted then I must have misunderstood you. There is no need to create 33 different variables with an individual dataframe in each of them. Instead we store the 33 dataframes inside a single list. The last three lines of (commented) code aren't necessary. They use the package data.table to recombine the dataframes to a single one in a long format: columns b year 1: b 8 3 2: b 9 4 3: b 10 5 4: c 13 3 5: c 14 4 6: c 15 5 7: d 16 1 8: d 17 2 9: d 18 3 10: d 19 4 11: d 20 5 12: e 21 1 13: e 22 2 14: e 23 3 15: e 24 4 16: e 25 5 It's usually more convenient to work with similar data in a long format than it is to keep them in multiple dataframes. But you specifically asked for 33 dataframes. The column `columns` in this case takes on the name of your variables, so that you know which variable belongs to what year and which value. Nevertheless, `df_sans_na` should contain all the individual dataframes. Extract with `df_sans_na[[i]]`. Lists are convenient objects to store the results of loops. You could also use the list you have to fit 33 models: model_storage &lt;- list() for (i in 1:length(df_sans_na)) { model_storage[[i]] &lt;- generic_ews(df_sans_na[[i]], winsize = 50, detrending = "gaussian", bandwidth = 50, logtransform = TRUE) names(model_storage)[i] &lt;- paste0("Model", i) }
Yeah, a majority of the good ones are available for free online.
could you post the dataset?
There are so many great books available online for free. Imo, books are not that fun anymore for learning computer skills. You can look into getting a subscription to a website that teaches R like DataCamp, etc.
I'm finally getting a handle on this and getting the results I'm looking for, just one last question, I need the Year column to come before the column with my data in this matrix at the end, I'm assuming I would be able to specify that in this chunk of code: df_combs &lt;- lapply(columns, FUN = cbind, year = df[, "year"]) but can't quite figure out how, do you have any insights? 
http://r4ds.had.co.nz
&gt; Text Mining With R by Garrett Grolemund I think you mean [Text Mining With R by Julia Silge and David Robinson](http://tidytextmining.com/) 
you can wrap your read.delim/table/csv in if.else
Thanks for the correction, was copying and pasting and cocked up.
I'm not a great expert, but I have struggled with this kind of problem, when using nls(). Here's a quote that sounds promising: "The bbmle package has mle2() which offers essentially the same functionality but includes the option of not inverting the Hessian Matrix." This method is less fussy about the starting values - worth a shot!
You can use the following code to loop over a file line by line ``` # https://stackoverflow.com/a/35761217 con &lt;- file(path_to_yr_input, "r") while ( TRUE ) { line = readLines(con, n = 1) if ( length(line) == 0 ) { break } } close(con) ```
You can use the following code-snippet to loop over a file line-by-line: # https://stackoverflow.com/a/35761217 con &lt;- file(path_to_yr_input, "r") while ( TRUE ) { line = readLines(con, n = 1) if ( length(line) == 0 ) { break } # Do something with line here. print(line) } close(con) You can then use `substr` (or `str_sub` from the `stringr` package) on the current line to determine the prefix of the current line: row_type &lt;- substr(line, 0,1) Once you know what row type you're dealing with, you can use `substr` or `str_sub` again to parse your data into appropriate columns using your col spacings which you can merge into your tables.
maybe not the best solution "R" wise, but I would recommend doing your string stuff in a different language, like C or Python, and then sending it to R. You may see better performance if it's a huge file. 
`?optim`
What are you trying to do and based on that I can offer some advice
The goal is to gain more depth of understanding behind multiple methods in this book. 'Machine Learning with R' gives a high level overview of the methods and basics behind applications. I want to further understand tweaking these methods and more of the math behind each method. Preferably in R code. I am still at an introductory level in terms of data science, but have much wrangling experience.
Try reading The Elements of Statistical Learning 
It can be done with lapply again by writing our own function to apply on every element of the list `df_sans_na`. df_sans_na &lt;- lapply(df_sans_na, FUN = function(df) df[, c(2, 1)]) `function(df)` in this case will take on the object inside list element 1, then list element 2, list element 3, etc. The objects that reside inside the list elements are your 33 dataframes. So `df` will take on the value of each of your 33 dataframes in sequence. Then it applies `df[, c(2, 1)` to each of those objects (dataframes). It swaps column 2's och 1's places. In R a lot of people use different functions from the `apply`-family instead of writing for-loops. It takes a while to understand them as a beginner though, even though most R users argue they're more readable. I'm not convinced they are myself, since a minority of R users actually understand how to use them :)
Sure can
Region Habitat DistRoad Culling Setts North Woodland 1977 No 0 North Woodland 3754 No 1 North Woodland 2582 No 0 North Woodland 2380 No 0 North Woodland 3440 No 1 North Woodland 5210 No 1 North Woodland 3653 Yes 1 North Woodland 4498 No 1 North Woodland 2651 No 0 North Woodland 4457 No 1 North Woodland 3480 No 0 North Woodland 2224 No 1 North Woodland 1552 No 0 North Woodland 160 No 0 North Woodland 2169 Yes 0 North Woodland 3327 No 0 North Woodland 420 Yes 0 North Woodland 3920 Yes 1 North Woodland 1571 No 0 North Woodland 4992 No 1 North Woodland 311 Yes 0 North Woodland 3798 Yes 1 North Woodland 3835 No 1 North Woodland 2433 No 0 North Woodland 2678 No 0 North Woodland 4157 No 0 North Woodland 1983 No 0 North Woodland 893 No 0 North Woodland 1397 No 0 North Woodland 2710 No 0 North Woodland 1880 No 0 North Woodland 4983 No 1 North Woodland 3752 No 1 North Woodland 329 Yes 0 North Woodland 913 No 0 North Woodland 3130 No 1 North Woodland 1901 No 1 North Woodland 2035 No 1 North Woodland 381 Yes 0 North Woodland 4858 No 1 North Woodland 3931 Yes 1 North Woodland 4630 No 1 North Woodland 2239 No 0 North Woodland 641 Yes 0 North Woodland 2447 No 1 Central Woodland 4763 No 1 Central Woodland 1369 No 1 Central Woodland 755 No 0 Central Woodland 3729 No 1 Central Woodland 166 No 0 Central Woodland 1595 No 0 Central Woodland 5181 Yes 1 Central Woodland 903 No 0 Central Woodland 4662 No 1 Central Woodland 4066 No 1 Central Woodland 4608 No 1 Central Woodland 1945 No 0 Central Woodland 3967 Yes 1 Central Woodland 2917 No 1 Central Woodland 3158 No 1 Central Woodland 1351 No 0 Central Woodland 2800 No 1 Central Woodland 1691 Yes 0 Central Woodland 3033 No 1 Central Woodland 3634 No 1 Central Woodland 3690 No 1 Central Woodland 3248 No 1 Central Woodland 4839 No 1 Central Woodland 1283 Yes 0 Central Woodland 172 No 0 Central Woodland 1907 No 1 Central Woodland 29 No 0 Central Woodland 4910 No 1 Central Woodland 4467 No 1 Central Woodland 2891 No 1 Central Woodland 4507 No 1 Central Woodland 3602 No 1 Central Woodland 1468 No 0 Central Woodland 4343 No 1 Central Woodland 2001 No 0 Central Woodland 2106 Yes 0 Central Woodland 1649 Yes 1 Central Woodland 4309 No 1 Central Woodland 1937 No 1 South Woodland 249 No 0 South Woodland 2626 No 1 South Woodland 2524 No 1 South Woodland 1706 No 0 South Woodland 554 No 0 South Woodland 4580 No 1 South Woodland 4558 No 1 South Woodland 198 No 0 South Woodland 4873 No 1 South Woodland 3210 No 1 South Woodland 2817 No 1 South Woodland 3640 No 1 South Woodland 5123 No 1 South Woodland 2667 No 1 South Woodland 3210 No 1 South Woodland 4199 No 1 South Woodland 3151 No 1 South Woodland 3045 No 1 South Woodland 3303 Yes 1 South Woodland 3744 Yes 1 South Woodland 4898 No 1 South Woodland 3366 No 1 South Woodland 209 Yes 0 South Woodland 2255 No 1 South Woodland 263 No 0 South Woodland 5124 No 1 South Woodland 4821 No 1 South Woodland 581 No 0 South Woodland 3850 No 1 South Woodland 3675 No 1 South Woodland 4706 No 1 South Woodland 1352 No 1 South Woodland 2134 Yes 1 South Woodland 2023 No 1 South Woodland 4797 Yes 1 South Woodland 4814 No 1 South Woodland 534 No 0 South Woodland 4972 No 1 South Woodland 4265 No 1 Wales Woodland 3184 No 1 Wales Woodland 1775 No 1 Wales Woodland 498 No 0 Wales Woodland 2589 Yes 1 Wales Woodland 4725 No 1 Wales Woodland 2533 Yes 1 Wales Woodland 4895 No 1 Wales Woodland 516 No 0 Wales Woodland 836 Yes 0 Wales Woodland 1474 No 1 Wales Woodland 1190 No 1 Wales Woodland 4310 No 1 Wales Woodland 5056 No 1 Wales Woodland 3704 No 1 Wales Woodland 2394 Yes 1 Wales Woodland 5190 Yes 1 Wales Woodland 331 No 0 Wales Woodland 2627 No 1 Wales Woodland 1910 No 1 Wales Woodland 3964 No 1 Wales Woodland 1466 No 0 Wales Woodland 2657 No 1 Wales Woodland 794 No 0 Wales Woodland 1009 No 1 Wales Woodland 2110 No 1 Wales Woodland 5217 Yes 1 Wales Woodland 3046 No 1 Wales Woodland 3623 No 1 Wales Woodland 4489 No 1 Wales Woodland 1802 No 1 North Grassland 3729 No 0 North Grassland 4474 No 1 North Grassland 4769 Yes 0 North Grassland 8 No 0 North Grassland 2272 No 0 North Grassland 2744 No 1 North Grassland 262 No 1 North Grassland 1235 No 1 North Grassland 1404 No 0 North Grassland 1910 No 1 North Grassland 4174 No 0 North Grassland 4671 No 1 North Grassland 212 No 1 North Grassland 4943 Yes 1 North Grassland 704 No 1 North Grassland 2129 Yes 1 North Grassland 4344 No 0 North Grassland 3635 No 0 North Grassland 2342 Yes 1 North Grassland 267 No 1 North Grassland 4073 No 1 North Grassland 2695 No 0 North Grassland 1004 No 0 North Grassland 1260 Yes 1 North Grassland 1413 No 1 North Grassland 3163 No 0 North Grassland 1519 No 0 North Grassland 2402 No 0 North Grassland 3823 Yes 1 North Grassland 4111 No 0 Central Grassland 700 No 0 Central Grassland 145 No 1 Central Grassland 855 No 1 Central Grassland 2529 No 1 Central Grassland 3735 No 0 Central Grassland 1085 No 0 Central Grassland 817 No 1 Central Grassland 2680 No 0 Central Grassland 4658 Yes 0 Central Grassland 2687 No 0 Central Grassland 1445 No 1 Central Grassland 2847 No 1 Central Grassland 1284 Yes 1 Central Grassland 3282 No 0 Central Grassland 4243 No 0 Central Grassland 4504 No 1 Central Grassland 773 No 1 Central Grassland 3421 No 1 Central Grassland 2003 No 0 Central Grassland 3420 Yes 0 Central Grassland 585 No 1 Central Grassland 549 Yes 1 Central Grassland 4278 No 1 Central Grassland 4147 No 1 Central Grassland 4954 Yes 1 Central Grassland 4039 No 1 Central Grassland 2180 Yes 0 Central Grassland 4723 No 1 Central Grassland 651 No 0 Central Grassland 1043 Yes 1 Central Grassland 2121 No 1 Central Grassland 1262 No 1 Central Grassland 3464 No 1 Central Grassland 2484 No 0 Central Grassland 1583 No 0 Central Grassland 956 Yes 1 Central Grassland 4933 No 1 Central Grassland 3375 No 1 Central Grassland 3115 No 0 Central Grassland 1903 No 1 Central Grassland 176 No 1 Central Grassland 3365 No 0 Central Grassland 123 No 0 Central Grassland 2205 No 1 Central Grassland 3507 No 1 Central Grassland 3985 No 1 Central Grassland 3435 Yes 1 Central Grassland 1012 No 0 South Grassland 4195 Yes 0 South Grassland 1856 No 0 South Grassland 1073 No 0 South Grassland 1541 No 1 South Grassland 2992 No 1 South Grassland 1018 No 1 South Grassland 1162 No 1 South Grassland 2542 Yes 0 South Grassland 2779 No 0 South Grassland 2437 No 0 South Grassland 2108 No 1 South Grassland 2197 No 0 South Grassland 1829 No 1 South Grassland 4089 No 1 South Grassland 1508 No 1 South Grassland 6 No 1 South Grassland 171 No 1 South Grassland 4538 Yes 1 South Grassland 3463 No 1 South Grassland 2569 No 1 South Grassland 172 No 0 South Grassland 5014 Yes 1 South Grassland 981 No 0 South Grassland 864 No 1 South Grassland 4344 No 1 South Grassland 1583 No 0 South Grassland 4249 No 1 South Grassland 375 No 1 South Grassland 2893 No 1 South Grassland 4741 No 0 South Grassland 3754 Yes 0 South Grassland 375 No 0 South Grassland 4288 No 0 South Grassland 1922 No 1 South Grassland 3391 No 0 South Grassland 751 No 0 Wales Grassland 1253 No 1 Wales Grassland 1302 No 0 Wales Grassland 524 No 1 Wales Grassland 2300 No 0 Wales Grassland 1224 No 1 Wales Grassland 3678 No 0 Wales Grassland 1219 Yes 1 Wales Grassland 4921 No 1 Wales Grassland 2549 No 0 Wales Grassland 4730 No 0 Wales Grassland 3284 Yes 0 Wales Grassland 1002 No 1 Wales Grassland 3969 No 1 Wales Grassland 2153 Yes 1 Wales Grassland 2605 No 1 Wales Grassland 1170 No 1 Wales Grassland 3023 No 1 Wales Grassland 804 No 1 Wales Grassland 1113 No 0 Wales Grassland 353 Yes 1 Wales Grassland 2947 No 1 Wales Grassland 4963 Yes 1 Wales Grassland 1538 No 1 Wales Grassland 5212 Yes 1 Wales Grassland 1742 No 1 Wales Grassland 335 Yes 1 Wales Grassland 1860 No 1 Wales Grassland 4056 Yes 1 Wales Grassland 3288 No 0 Wales Grassland 3157 No 1 Wales Grassland 4248 No 1 Wales Grassland 907 No 0 North Disturbed 3478 No 0 North Disturbed 4651 No 1 North Disturbed 1076 No 1 North Disturbed 3961 No 1 North Disturbed 1314 No 1 North Disturbed 4447 No 0 North Disturbed 2138 No 0 North Disturbed 2581 Yes 1 North Disturbed 1873 No 1 North Disturbed 1223 Yes 0 North Disturbed 1068 No 1 North Disturbed 1895 Yes 0 North Disturbed 1063 No 0 North Disturbed 3063 No 0 North Disturbed 4822 No 0 North Disturbed 1518 No 0 North Disturbed 2034 No 0 North Disturbed 2459 No 0 North Disturbed 3875 No 0 North Disturbed 3485 No 0 North Disturbed 731 No 0 North Disturbed 759 No 0 North Disturbed 3750 No 1 North Disturbed 696 No 0 North Disturbed 5175 No 1 North Disturbed 4978 No 0 North Disturbed 3718 Yes 0 North Disturbed 1674 No 1 North Disturbed 2518 No 0 North Disturbed 4465 Yes 0 
Take the Coursera course on machine learning by Andrew ng. He writes it in Matlab, but the principles are what's important. You're limitation now is likely understanding the underlying statistical mechanisms, it's not really about R or any other language. So take that course, it will break down the math using code. It's awesome.
more answers here: https://www.reddit.com/r/rstats/comments/78g9bv/i_need_help_figuring_out_how_to_use_the/
Discovering Statistics with R is one of the best books I have read. It doesn’t really get into machine learning but it probably the best stats/r books out there. At some point I think you’re gonna have to start sticking to university grade books, research papers, etc. as I’ve found that there just aren’t many good non-academic books that go too in-depth on statistical or ML concepts. The good part is it’s a rewarding challenge to code cryptically written algorithms in R or Python.
update: I ran my select statement without piping and it pulled all the observations.
Ah I see. Actually it turns out that the slider wasn't as hard to adjust as I thought it'd be so I left it as it was. However, I have a new (but related) question. I'm now trying to deal with renderPlotly() rather than renderLeaflet(). I'm taking the same approach as my previous question, i.e. I have a call to inputPanel(), within which I have a few calls to selectInput(), then I have my renderPlotly() part. I want to access these inputs, for example one called "x_var", inside renderPlotly(), but when I put "x_var &lt;- input$x_var" inside renderPlotly(), I get the following: &gt; Error: no applicable method for 'ggplotly' applied to an object of class "character Do you have any advice?
Sure. try adding as.numeric(input$xvar) instead of just input$xvar. Shiny passes values that you enter as characters instead of numerics (because JavaScript handles them that way iirc). 
Show your code when you ask for help, always.
Have you restarted your computer? Cant you export your data in text format and read it with read.csv()?
I have restarted my computer, but it didn't help. Also, all of the code is written out neatly and I don't want to go through the trouble of exporting the data on the 6 excel files that I used, but if that's the last resort I'll have to do it
Also try update or downgrade R. Version 3.4.1 was very buggy to me!
Step 1 would definitely be putting all your data in one data frame
I thought I would need them in the separate data frames in order to keep all the data paired with the respective factor? All the data frames are different lengths and are for different models and I need the output to be for each model.
Is there a chance you have multiple R installations? Have you upgraded R versions, installed R in anaconda, anything like that?
Probably depends on the database engine. 
Give us a simple minimal working example that demonstrates roughly what your data looks like and what your desired output looks like. 
Can you post the code your trying? Also your operating system. 
Make sure you have the code to download and load up the package in your Markdown file as well as the console. Just having it in the console isn't enough. 
You don't need to download all the packages from the internet every time you knit the document.
I may have read your question wrong.... Could you give a simple example of what your data looks like?
My data looks like this in the environment https://i.imgur.com/ivY1tyM.jpg but this when "printed" https://i.imgur.com/0CXwK2k.jpg. Most of them are in the range of 40-50 rows long. My working code to extract what I need individually per model is https://i.imgur.com/O8Y7rzc.jpg and the output is this https://i.imgur.com/rcnE08i.jpg. I want to get all of the outputs for each model in 1 "file", so 36 outputs in one list.
My data looks like this in the environment https://i.imgur.com/ivY1tyM.jpg but this when "printed" https://i.imgur.com/0CXwK2k.jpg. Most of them are in the range of 40-50 rows long. My working code to extract what I need individually per model is https://i.imgur.com/O8Y7rzc.jpg and the output is this https://i.imgur.com/rcnE08i.jpg. I want to get all of the outputs for each model in 1 "file", so 36 outputs in one list. 
No, but you need the specific package downloaded and loaded in your file separately from the console when you knit, otherwise it's not going to understand what you want. 
I meant sample code that works, not screenshots. https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example
Not sure about the whole dput or whatever function from R but this should do the trick? df1&lt;-data.frame(a=1:5, b=11:15) df2&lt;-data.frame(a=3:9, b=8:14) df3&lt;-data.frame(a=6:15, b=9:18) list&lt;-list(df1, df2, df3) cor.test(df1$a, df1$b, method = "pearson") cor.test(df2$a, df2$b, method = "pearson") I am indeed only really interested in the correlation and pvalue, also when I try "lapply(ARSD, function(x) cor.test(x, method='pearson))" I get an error stating it needs a y value, the format for cor.test is cor.test(x, y, method).
Not sure about the whole dput or whatever function from R but this should do the trick? df1&lt;-data.frame(a=1:5, b=11:15) df2&lt;-data.frame(a=3:9, b=8:14) df3&lt;-data.frame(a=6:15, b=9:18) list&lt;-list(df1, df2, df3) for (i in 1:3) {names(list)[i] &lt;- paste0("Model", i)} cor.test(list$Model1$a, list$Model1$b, method = "pearson") cor.test(list$Model2$a, list$Model2$b, method = "pearson") I am indeed only really interested in the correlation and pvalue, also when I try "lapply(ARSD, function(x) cor.test(x, method='pearson))" I get an error stating it needs a y value, the format for cor.test is cor.test(x, y, method). 
 lapply(ARSD, function(x){ v = cor.test(x$at, x$sd, method='pearson) return(v$cor, v$p.value) }) 
It won't actually give me an output when I try this? It just still has the "+" after instead of &gt; as if it thinks my command isn't complete.
I really do not understand your problem yet you can acquire names with colnames() or names() function. If it gives you a text like “x y z” that you want them as your column names x,y and z, use strsplit() function to split them and create a vector of names. Then write colnames(your.data) &lt;- vector that contains names. This solution is probably not you are looking for yet i did my best.
Try pressing tab after typing the df$. Might be an IDE issue, so try restarting that as well. 
Check if "df" is still a data.frame. class(df) If not then that's the problem. You can usually make it (matrix probably) into a dataframe with as.data.frame(df) 
I realize the question is confusing. It's an odd assignment problem. I'll update my post with a picture to perhaps explain it better. 
yea it was an issue with R studio. A re-install fixed the issue. Thanks 
 $ sudo sh -c 'echo "deb http://cran.rstudio.com/bin/linux/ubuntu precise/" &gt;&gt; /etc/apt/sources.list' do you mean $ sudo sh -c 'echo "deb http://cran.rstudio.com/bin/linux/ubuntu xenial/" &gt;&gt; /etc/apt/sources.list'
Yes sorry, I copy and pasted from the original tutorial. I did use the xenial command when installing I'll fix the original post 
This is what I use on debian/ubuntu boxes. #!/bin/bash # script to update R sudo add-apt-repository "deb http://cran.rstudio.com/bin/linux/ubuntu $(lsb_release -cs)/" sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9 sudo add-apt-repository -y "ppa:marutter/rrutter" sudo add-apt-repository -y "ppa:marutter/c2d4u" sudo apt-get update -qq sudo apt-get install -y --no-install-recommends r-base-dev r-recommended qpdf 
You should check out dplyr. You’ll need to use the lag and mutate function, essentially something like this: mutate(data, column_name = ifelse(abs(lag(column_name) - column_name) &gt; 15, NA, column_name) 
library(tidyverse) install.packages("readxl") library(readxl) library(stringr) Now I'm getting the error: Error in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]) : there is no package called 'readxl' Quitting from lines 16-22 (Data_Wrangling_MiniProject_Matt_Scocozza.Rmd) 
After running *sudo apt-get update -qq*, I get: W: The repository 'http://cran.rstudio.com/bin/linux/ubuntu Release' does not have a Release file. E: Failed to fetch http://cran.rstudio.com/bin/linux/ubuntu/Packages 404 Not Found E: Some index files failed to download. They have been ignored, or old ones used instead. I also tried your same steps but using explicitly ubuntu xenial (instead of lsb_release) but ended up in the same place I was before (unmet dependencies)
Most people who know r well earn a more than what you're willing to pay for their time probably. 
I was missing a quote around "pearson". I'm coding on a cell phone right now. Anyway, try it now. 
Do you mean like a lookup? You could use the match function in R. 
[removed]
I have the csv file with only the latitude and altitude data, but I want to incorporate the country of each entry without having to put it in manually. Is this possible? Thanks
A Google search helped me find a lookup table : https://developers.google.com/public-data/docs/canonical/countries_csv
A quick example: x &lt;- c(1,3,5,6,7,3,2,5,6,32) y &lt;- c(2,5,6,7,5,7,8,7,8,2) d &lt;- data.frame(x=x,y=y) fit &lt;- lm(y~x,data=d) d$cooks &lt;- cooks.distance(fit) fit2 &lt;- lm(y~x,data=d[d$cooks&lt;1,]) Note that the first point isn't an outlier by your definition in the first fit but it is in the second fit. To loop this and remove points from the d data.frame that are outliers in the new model: while(TRUE) { fit &lt;- lm(y~x,data=d) d$cooks &lt;- cooks.distance(fit) if(all(d$cooks &lt; 1)) { break } d &lt;- d[d$cooks&lt;1,] }
while I also encourage the tidyverse if you are using data.frame style data, you can achieve the same effect as lag() by removing the first x[-1] and the last x[-length(x)] element of a vector to get the neighbours and do the ifelse or boolean subset based assignment of NAs
not sure how you fix it (probably easiest remove and reinstall r-base-core), but the problem seems to be that r-base is the newest stable ubuntu release (3.2.3-4), while you are now trying to install the newest r-base-dev from the rstudio repository (3.4.2-1), which of course relies on the newest r-base-core from the rstudio repository, not from the (always) outdated stable ubuntu release
In general the df has to be in the R session for Rstudio to give you suggestions (having it in the file might work, I'm actually not 100% sure). And the whole suggestion system can get sluggish if other parts of Rstudio are clogging up.
If I'm understanding you correctly, wouldn't that have been fixed by switching the repository? Like if I used the r-project repositories instead of rstudio's.
The easiest way is to use a reverse geocoding API, such as [Google's](https://developers.google.com/maps/documentation/geocoding/start). I did this a few years ago for the first time, and once I found some sample code, it was easier than I thought it'd be. I think I used the XML or CURL packages. Let me know if you have trouble finding sample code, and I'll post mine for you.
Only if you switch back to the basic ubuntu repository. I wouldn't be surprised if a package installed from one repo (especially I ubuntus own) is not updated based on another repo to prevent malicious behaviour. But in general, the version numbers of r core and r dev have to be the same.
Thanks!
I think the function you want is strsplit, if I'm understanding you correctly, using " " to parse the single string into a character vector. You can then use names or colnames to assign. Does that help?
Homework problems are easy though.
Thank you, that was very insightful. I was trying to patch the georeverse package before I read your message but it was kinda sketchy. I was wondering if you could send me that code. Thank you again.
The function ggmap:geocode() will handle this just fine. All it does is query Google's API. Be careful as you only get something like 1200 a day. 
I understood it correctly. You just need to split “af3 f7 f3......” and get them as “af3” , “f7” , “f3” .... and assinging them to the new headers. I assume you know basic functions and indexing btw. My first answer will work too.
Have you put "require(readxl)" on the actual .rmd file? Because knit it try to "compile" everything from scratch, without having regards for what is in the workspace or in memory. 
I just worked on it and what you said works. Thank you! 
Thank you! 
Ah, but they actually are character values that I'm trying to pass. I'm now trying to make a plot that lets the user choose which column to plot on which axis, by choosing the column name. I'll see if I can do it numerically.
 df$year_column &lt;- year_vector creating the year_vector depends on what class/package you are using for your df$dates column, if it's base R use format(...), if you are using lubridate it's year(...) 
To add to this: strptime is also worth looking into, as well as the difference between POSIXlt and POSIXct - getting it into POSIXlt format will let you pull the year out by list subsetting.
Yes, you can do that, but you'd have to be more specific about what you mean by "safer". I'd suggest reading Hadley Wickham's book *Advanced R*, which has several use cases for creating your own environments as well as reference classes and closures. 
You can do that and it has advantages but is burdensome. A better solution is to use [klmr/modules](https://github.com/klmr/modules), which does this and more. For a quick and dirty alternative, just wrap the entire script body inside `local({…})`. But really, it's more robust to replace individual scripts with modules (see above) or packages (but that has its own disadvantages).
A more common solution to this is to put code that cares deeply about its own environment in a package. Packages have their own environments and include all the supporting things you need to make them work, like methods to import things from other environments, and to export things.
I'm concerned that some variable I use in the global environment will also be available in a different env..so I thought of this approach. Yup, in the process of reading the environment chapter in Advanced R. Also, do you have any general suggestions for writing safer code?
My code is in functions. By using a custom environment, I was trying to mimic the features of a package without having to create one. Referencing the env for every variable will also likely make the coder a little faster? 
Interesting, will check it out.
I don't think it makes the code any faster. Any reference to a variable does a search up the current tree looking for that name. That includes your variable `A.env`, so you're not skipping that search by referencing the environment. It's a good idea to ask yourself why your code even cares about the environment it's running in. Functional code (as in functional programming, not as opposed to broken) usually shouldn't care too much about the external state, so provided your code is in functions, you probably don't need to worry too much about it. If all you're worried about is namespace collisions, then a package is by far the easiest way to go. They're honestly very simple, especially if you use the devtools package.
I think this might be related to the rest of the levels of the factor. When you compare factors it looks at the complete list of levels. If you compare as follows do you still get the same error message?: a[1] == droplevels(acd$Date[1:10])[1] 
Thanks, so I've run this : &gt; a = dput(droplevels(head(acd$Date,5))) structure(c(4L, 2L, 3L, 5L, 1L), .Label = c("10/17/1913", "7/12/1912", "8/6/1913", "9/17/1908", "9/9/1913"), class = "factor") &gt; a[1] == droplevels(acd$Date[1:10])[1] Error in Ops.factor(a[1], droplevels(acd$Date[1:10])[1]) : level sets of factors are different &gt; length(a) [1] 5 &gt; a[1] == droplevels(acd$Date[1:5])[1] [1] TRUE &gt; Just had to correct for the size (`1:5` rather than `1:10`), but that worked. So what's going on here? And how do I make a small vector or whatever that I can use to ask questions about? Thanks!
The output of the second line is false, because `a` consists of the first five dates and `acd$Date[1:10]` gives you the first ten dates. Since you have dropped levels on both, each vector will have a level set equal to the number of unique dates in the vector. In general, testing for factor equality also includes testing the equality of the level sets. In this case, you could also coerce date to be a character or date object. You already answered your question as to how to make a small vector. The first line of your code produces this. You can share the output with others and this is the same as `a`, which you have in memory. 
&gt; Referencing the env for every variable will also likely make the coder a little faster? Unfortunately it won’t, on the contrary: When you write `env$var`, R doesn’t only have to look up one symbol (`var`) but *three*: `var`, `env` and `$`. So while using environments can be good for explicitness (and I strongly advocate using environments to create scope in R), it’s rarely useful from the point of view of performance.
Well, what's the condition for extracting them? Should they the drawn randomly? Then try sample(). Is there a specific condition? The vector == *condition* might work. If you have your condition, then you can simply assign the values to a variable. # extracts all elements bigger than five and stores them in new new &lt;- vector &gt; 5
Thanks. What do you mean by level set? This is a term in maths but I'm not sure that it has the same meaning in this context. &gt; In this case, you could also coerce date to be a character or date object. Perhaps that would be a better approach. I can use structure(c(4L, 2L, 3L, 5L, 1L), .Label = c("10/17/1913", "7/12/1912", "8/6/1913", "9/17/1908", "9/9/1913"), class = "factor") to assign to a vector in a script and share that on here etc, ok cool! Thanks
Can’t you you split the data according to these splits and train different trees on each subset?
That’s the only way I’ve found to do it. 
The issue would be if i wanted to split it twice in different stages of the process. then splitting the datasets would become cumbersome. unless there is a simple way to do that
It sounds like an interesting problem. Can you post here what you end up doing?
Will do. This is what I just found now, that Im going to have to read over a few times(my R is only ok, still learning) that may answer it. Any idea of this will work? before i waste an afternoon lol https://cran.r-project.org/web/packages/rpart/vignettes/usercode.pdf
Ah cool! It could write an ifelse() functions n into the splitting function! Cool!
So for level set factors in R are a hybrid of numeric and categorical (see [here](https://www.stat.berkeley.edu/classes/s133/factors.html) for more detail). Under the hood they are just a vector of integers. What makes it a factor are the *attributes* which 1) specify that the vector is a factor, and 2) say what the integers in the vector mean by giving them labels. Particularly they have an attribute called *levels* which lists all the possible values of the factor. This can be a larger set than the actual values of the factor. For example your factor initially has levels "A", "B" and "C", but if you subset or run head and only have values "A" and "B" left the new factor still has the original set of levels as an attribute. When you compare factors with '==' the attributes as well as the vector values are compared. Your original factor has 5100 levels, when you manually specify a factor from the first 5/10 entries it will only have those first 5/10 levels and so the attibutes will not match. There are a number of ways to fix this: * specify all 5100 levels in the attributes of the manually constructed header * change the comparison by first coercing both factors to character * drop unused levels from the subsetted larger factor 
Is it RStudio or the R session that crashes? What happens if you run the script outside of RStudio?
Also, what is the script? Can you post the code? Are you keeping track of the RAM being used?
have you tried upgrading to the latest version? Have you tried using microsoft open R? Can you post the code - at least the part that causes it to crash?
A big advantage of languages like R over excel is reproducible research. If you clean and edit a spreadsheet, there's no simple way to verify that you didn't make a mistake. It may be possible to review your formulas, but it can be pretty obtuse. Also, Reinhart and Rogoff did an incredible amount of work to advance the adoption of R and Python and accelerate the demise of excel: https://www.bloomberg.com/news/articles/2013-04-18/faq-reinhart-rogoff-and-the-excel-error-that-changed-history
Great language to know, applicable to many disciplines. Over excel, it's open source, but more importantly, there are ways to ensure reproducible analysis through RNotebooks in Excel, you can't document well, your analyses (okay setting up a formula maybe enough) nor plot a box plot, my pet hate. The other thing is the risk of overwriting your data in Excel without knowing you had. 
A simple reason to use R over Excel is that with a just a few models in an Excel workbook, or a substantial amount of data, it can get really difficult to use Excel due to the amount of crashing. At least in my experience. I don't really understand your second question. You need to understand your field to do any kind of statistical analysis for that field - it is somewhat irrelevant to whether you use R, Python, or Excel. I will say though, that the incredible amount of packages documentation, and tutorials online for R make it possible to hack through tons of industry-specific challenges while being somewhat new to that industry.
h2o.ai is a great resource + there was a book put out. 
It goes further than this, too. With R you can check your code (essentially, the calculations that create your results) into a version control system. Everyone can stay in sync and know precisely which version they are using, and you can institute formal processes for requesting and approving changes, but your data isn't mixed with the calculations. There is no confusion about who is doing what with which version. That alone is huge.
You don't need shiny. Shiny will allow you to produce web applications, which Excel can't do anyway, so it's hardly comparable. Knitr will allow you to create many kinds of static document outputs via Pandoc.
If you cannot post the code that's causing it, you're on your own, pal.
First, you need to save the result of the fileInput in a variable. This tells you where the file has been put on the server, and some other metadata. Look at the documentation for fileInput to understand that. Once you get the file path for the uploaded file (hint: it's in the datapath column of the data frame that fileInput returns) you can use one of the standard methods to read in that file. Personally, I prefer the readxl package. Look at the documentation for the readxl function in that package to learn how to import the file from that path.
Have you done much VBA programming? R is much more efficient to program in, even when you're doing things that R isn't meant for. God I hate VBA
It makes little sense to compare R to Excel in terms of software development. R is a normal, general-purpose language, not unlike the C++ you mentioned, but R is an interpreted language by design, so you need to take a different approach when developing applications in R. CIt has a built-in (bytecode) comp
&gt; it can't compile standalone programs without the shiny package I’m not sure what exactly gave you this idea. First off, the Shiny package facilitates the production of interactive web applications. It’s unrelated to making the program standalone (whatever exactly you mean by that). Secondly, R allows creating standalone programs in the same way as other interpreted languages, such as Python, Ruby, Perl, JavaScript, you name it. All of these languages require a runtime environment. But then, so do most compiled languages (Java, .NET, even C and C++ usually do). I think you misunderstood what “standalone program” actually means.
thank you, I have tried that and that is exactly where I get that reactive element error, when I try to read the &lt;dataframe&gt;$datapath variable.
[https://shiny.rstudio.com/reference/shiny/latest/fileInput.html](https://shiny.rstudio.com/reference/shiny/latest/fileInput.html) The example here shows how to render the output I don't want to render the output I just want to write the output to a R dataframe for further processing.
You didn't provide your server code, so there's no easy way to help you with that. A problem with reactivity is caused by you not properly accounting for reactivity in your server code, but that's not very helpful :P
I didn't provide my server code because it didn't work.
server &lt;- function(input, output) { data &lt;- reactive({ print(input$inputfile$datapath)}) } so I added this to the server function just to see if I can print out the datapath and it does nothing. What am I doing wrong?
R-simmer (org) is a great framework for agent based models
I'm a bit late to the party here, but let's assume you wish to rename all the column headings while preserving the first ones you have. Assuming your data frame is called "Table", you can assign names as follows: col_headings &lt;- c("Title", "Recorded", "Sampling", "Subject", "COUNTER", "chan:36","units:emotiv","AF3","F7","F3","FC5") Table names(Table) &lt;- col_headings This method does mean you also have to include the names for the first few columns you want to keep, but it can work if you want a simplistic method.