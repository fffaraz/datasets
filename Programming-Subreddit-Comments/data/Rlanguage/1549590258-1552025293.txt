This? lst &lt;- list( col_1 = LETTERS[1:3], col_2 = LETTERS[1:5], col_3 = 1:10L, col_4 = 1L ) max_len &lt;- max(vapply(lst, length, integer(1))) # fun to pad a vector to the maxlen pad_list &lt;- function(f, maxlen) { if(length(f) &gt;= maxlen) { return(f) } f[max_len] &lt;- NA f } # pad your list lst_expand &lt;- lapply(lst, function(f) pad_list(f, max_len)) # you could also use do.call(cbind, lst_expand) but this # will preserve your list types, assuming this is important out &lt;- do.call(data.frame, list(lst_expand, stringsAsFactors = FALSE)) # then simply write out write.csv(out, "test.csv", na = "") &amp;#x200B;
Yes 
Well, now, your question makes a little more sense. However, I think you've asked the wrong question. If you simply want to change the limits of the X and Y axes, you need to use the `xlim` and `ylim` options of the `plot` command. To set the specific ranges you specified in your title, you can use the code shown below. Also, note that if you include 4 spaces at the start of each line, your code will be properly formatted, as so. # Read in the data men &lt;- read.table(file.choose(), header = T) women &lt;- read.table(file.choose(), header = T) # Produce a line plot – using the male data plot( x = men$Year.1, y = men$MWE, type = 'l', lty = 3, lwd = 2, col = 'red3', ylim = c(500, 900), xlim = c(2003, 2013), main = 'Quarterly earnings in US by sex (aged 25 or older)', ylab = 'Median weekly earnings ($)', xlab = 'Year', sub = 'Source: US Bureau of Labour Statistics', cex.sub = 0.8 ) # Add the female data to the plot points( x = women$Year.1, y = women$MWE, type = 'l', lty = 2, lwd = 2, col = 'seagreen' ) # Add a legend legend( 'topleft', legend = c('Male', 'Female'), title = 'Sex', lty = c(3, 2), wd = 2, col = c('red3', 'seagreen'), cex = 0.8 ) After seeing your code, I don't believe the purpose of your assignment is for you to specify the exact ranges. Rather, I believe that your instructor wants you to use code that will specify the X and Y limits based on the values in the data set you were given. However, without seeing the data set, I can't be sure.
Thank you so much for your time. But I have to use exactly what I was given. So I can’t just add or remove lines. It’s like a blank space that I need to fill. So I can’t simply add ylim and xlim as you suggested I am afraid 
I regret that I can't really help you. I don't want to criticize your instructor because they likely have some strategy they are using to teach you, but I've coded in R for over 10 years, and the constraints you are specifying do not make sense to me. If I were just learning R, I think I would be very confused by this assignment. Definitely talk to your instructor.
&gt;I just reverified those numbers, how are you finding it off? I think your 1 in the second row/first column is off. Look at this: &gt;Percent of unique ids that returned their items that were emailed but not called If I understand this right, then you're looking for what proportion of emails that were returned. If we list all emails in your data, then this is what we get: &gt; x[x$called=="Not Called" &amp; x$emailed=="Emailed",] id comm called emailed returned 3 1 Emailed Not Called Emailed Returned 6 3 Emailed Not Called Emailed Returned 8 4 Emailed Not Called Emailed Not Returned 10 5 Emailed Not Called Emailed Not Returned So as far as I can tell, that proportion shouldn't be 1, but 0.5, as there were two unique ids that returned their emails, and two that didn't.
Well it's not entirely clear. Here, I'll give my interpretation of what you want and then you can correct me. |proportion of unique ids that returned a phone call AND an email (so they must have received at least one of both to count in this group)|proportion of unique ids that returned a phone call AND didn't receive any emails| |:-|:-| |proportion of unique ids that returned an email AND didn't receive any phone calls|proportion of unique ids that returned some other means of communication, and didn't receive a phone call nor an email| Is this what you mean? &amp;#x200B;
Thank you 
Thanks a ton for this! The matrix I have is gigantic, yes it is definitely possible that every column has at least one correlation &gt; 0.9. Although the code did not apply the threshold to the matrix for whatever reason, I was able to use it to generate a new data frame with 3 columns (first variable, second variable, and correlation value). This only returned values from the matrix that met the threshold (and properly excluded the diagonals!) . I used your example in this way : &gt; filtered &lt;- data.frame( cor_mat) %&gt;% + rownames_to_column() %&gt;% + gather(key="variable", value="correlation", -rowname) %&gt;% + filter(abs(correlation) &gt; 0.9) When I used the example *as-is* , it printed the list of criteria-passing correlations, but did not apply it to the matrix : data.frame( cor_mat) %&gt;% rownames_to_column() %&gt;% gather(key="variable", value="correlation", -rowname) %&gt;% filter(abs(correlation) &gt; 0.9) Now I tried the example you've just provided (with my own data frame still) and get this : &gt; sum(apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) **[1] NA** &gt; sum(apply(cor_mat, 2, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) **[1] NA** However, when I use this example on mtcars, I get this : &gt; sum(apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) [1] 2 &gt; sum(apply(cor_mat, 2, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )) [1] 2 
Use lines() for your second and third use of plot(), that will stop the axis from overwriting eachother. Your first use of plot() needs x values, so use: plot(**thetas**, lhood(10,2,thetas),col="blue",type="l",lwd=4) It still doesn't match, your y-scale is off between the first and other two plots but I'm not sure where you're going wrong. Maybe your lhood needs to be normalized or something.
&gt; Maybe your lhood needs to be normalized or something. their lhood ranges from [0,3] ish though. unless I've misunderstood you. Going to try your suggestions now thanks
OK, I have https://i.imgur.com/QtyIkhA.png , which looks near enough. it seems to me that lhood() was multiplied by 10 though,and I'm not sure what the justification for this is **other** than having the graphs more similar. Do you know whether there's a statistical reason for this? I've added the updated code to the OP
Getting NA values when you run the `apply()` based filter is a sign that something is wrong with the `cor_mat` matrix. Most likely you are running correlations on variables with missing data. You have two main options: 1. Do casewise deletion (remove cases which have missing data). This is probably a bad idea when you have many columns but few rows. 2. Do *pairwise* deletion, which runs correlations using all complete pairs of observations for each variable. This may result in a correlation matrix which is not positive semi-definite but that is not an issue if you're just interested in looking at the correlations and not using them for a model. Pairwise deletion: cor_mat &lt;- cor(mtcars, use="pairwise.complete.obs") cor_mat &lt;- cor_mat[apply(cor_mat, 1, function(x) any(x &gt; 0.9 &amp; x &lt; 1) ), apply(cor_mat, 2, function(x) any(x &gt; 0.9 &amp; x &lt; 1) )] cor_mat 
 library(dplyr) library(tidyr) df %&gt;% spread(type, diameter)
&gt; library(dplyr) library(tidyr) df %&gt;% spread(type, diameter) thank you so much. I knew it was something awfully simple. 
How would you compute a Fisher Exact Test without a complete 4-value table?
What test should I use then? 
What is your objective? What's the question you are trying to answer? 
In addition to needing a complete 4-value table, that table should also be of counts. Where are these proportions coming from? Specifically, did they come from some calculation on counts? Why do they not add to 1 in any way?
Is there a significant difference in the rate between those that were called and emailed, called but not emailed, emailed but not called, neither emailed nor called. 
The calculation is number of accounts in the category that returned their items/ total accounts in that category.
So, in one possible Fisher's Exact Test, you might make a 2x2 table with "number of accounts in this category that returned their items", "number of accounts in this category that DIDN'T return their items", "number of accounts NOT in this category that returned their items", and "number of accounts NOT in this category that DIDN'T return their items" This would answer the question of if the proportion of accounts in that category that returned their items was greater or less than or not significantly different from accounts not in that category.
Looks like you divided the table by the total or something like that? The table should have integer numbers in it. Something like this tb2 Emailed Not Emailed Called 97 40 Not Called 70 0 &amp;#x200B;
Also not seeing why they would scale up the likelihood. Though in that, I see that you updated your likelihood function, but it can be made even better be replacing the hard-coded 45 with `choose(n,y)`.
the dplyr command in this case is only imported for the %&gt;% command. So to be fair you could just as easily do spread(df, type, diameter) and only import tidyr. That being said dplyr is awesome, but not sure about its need i nthis use case! 
So I actually have a table like that and used that to get the current table. Instead of doing a rate I'll just do test a subset of the table.
Haha, yeah, shows how much I use it when I import just to insert something into the first argument.
Same same 
I am so tired of picking and choosing libraries and eventually end up having a dozen listed at the top that now I just load 'tidyverse' and get on with my work.
I don't understand what you're trying to accomplish. If you'll post some sample data (*e.g.* less than 100 rows), I'll do my best to help you.
So I have a data set of 40k entries with 30 variables. I'd like to take a subset of this dataset of 1k entries in which the overall averages for the variables in the subset are as close as possible to what I have specified. It's similar to matching but not one to one, and allowing me to specify a max number of values to retain.
First I would normalize the dataset by mean and SD. Then I would create and index based on this normalized dataset and use that to subset the orginal dataset. I am not sure about how to specify the size in advance though. `head(cars) carStd = (cars - apply(cars, 2, mean)) / apply(cars, 2, sd) index = with(carStd, abs(speed) &lt; 1, abs(dist) &lt; 1) #the 1 here represent how many SD from the mean to include in the subset. Tune as needed. carsSubset = cars[index ,] ` 
1. Do you need a random sample? 1. Why not just save the row indices from the previous samples and reuse the same rows? 1. If you aren't reusing the same rows, are you concerned about matching your subsequent population in multidimensional space? 
&gt; I'm using R but have no problem using SAS as well if need be. Drop the SAS on the ground and go back to your terminal. No need to threaten us. If the data was created based on a distribution, sampling randomly from your data should return a similar distribution. Why not just do ``` df[sample(100000,100),] ```
magrittr is a lighter weight package that just includes the pipes and some simple pipe related helper functions. If you know about them, the other pipes in magrittr, especially %&lt;&gt;% and sometimes %T&gt;% will change your life.
Your bottom right cell is the issue. If you look up the equation that the test is performing, it will be clear why a zero or 'NaN' value is causing an error. TL;DR you can't have zero or NaN cells in a 2x2 to do this test
You can write a script that loads a bunch of normal ones, then source it? e.g. &gt; source('~/Documents/setup.R') Or there's ways of loading them into your .Rprofile but that's not considered super reproducible. I work on a lot of different computers so I don't have a bunch of *library()* lines. Instead I put something like this at the top of my script: &gt; load.packages = function(a){ &gt; if(!require(a, character.only = TRUE)){ &gt; install.packages(a) &gt; library(a, character.only = TRUE) &gt; } &gt; } &gt; x = c('plyr', 'tidyverse', 'MASS', 'broom', 'MuMIn', 'glmnet', 'devtools', 'cowplot', 'agricolae', 'car') &gt; lapply(x, load.packages) That installs uninstalled packages listed in 'x', then loads them all. 
So I have the plotting part down, I would just need help to save each plot automatically to a specific file. Here is my code: for(i in c(1:91)){ barplot(mydf[,i],type="l",main="colnames(mydata)[i]") } print(barplot) 
for(x in 1:150){ jpeg(paste0("plot", x,".jpg")) barplot(df\[,x\], main = names(df)\[x\]) [dev.off](https://dev.off)() }
Thanks for your answer! Do you happen to know would I save this to a specific file though. I would like to save it to a different file than the working directory^ 
In jpeg function you can add path to specific directory. In string "plot" just write path to dir "/plot". You can also save plot to PNG or PDF.
You are a coding legend thank you so much!!Do you mind if I PM you with other questions?
A couple of options: first, double check that you’ve *saved* the script. Second, I recommend the `here` package which allows for easier file location. 
Thanks but I'm just student...sure fell free to pm me.
One of the best ways to organize is with Rproj (r projects). An rproj organized all the files inside a directory and when you open the project, it (among other things) sets the directory as your working directory. 
(Fyi the r cookbook can be found for free online in pdf form as well) 
If you do not input an absolute path, R will only check your current WD for the file. It does not check where the script is saved unless it happens to be in your WD. You can permanently change your WD but I would not recommend that as a solution. Check out r projects or use absolute file paths for the time being. Are you establishing your R workflow (and will use R often), or do you use R very infrequently? I only ask because it will help guide commenters on solutions to your problem :)
Is there a way to create a map legend that is based off the same color codes set to my map icons?
You should be able to accomplish that with par/mfrow. If you wanted the pairs plot on the left, and the ggplot on the right, the code would look like this: par(mfrow(2,1)) # set up R graphical device to display the next 2 plots side by side pairs_plot # display pairs plot ggplot # display ggplot if you want them the other way around then just plot the ggplot first. 
I haven't used this functionality but I would suggest looking at some examples of its usage here [https://github.com/rstudio/leaflet/blob/master/inst/examples/awesomeMarkers.R](https://github.com/rstudio/leaflet/blob/master/inst/examples/awesomeMarkers.R) 
Check this package https://github.com/thomasp85/patchwork
Hey that’s neat!
There should be. Will follow up tomorrow. Out all day.
Haven't tried it myself but have you come across cowplot? Might work too
Thanks!
This just plotted them underneath each other
Look up regex
One way to extract the folder part would be to use strsplit with a sep of '/'. A convenient wrapper for this is `tidyr::separate()` separate(data = x, col = "V1", into = c("dot", "folder", "file"), sep = "/") One you have manipulated the folder name, you can then merge back together with `tidyr::unite()`. On the whole though, if you just wanted to rename the `see0` part to something else you could use gsub: x$V1 &lt;- gsub("see0", "newfolder0", x$V1) 
stringr::str_extract_all(x, "(?&lt;=/).*?(?=/)"). The middle ? is just in case you ever have nested directories.
The forward slash is no problem! If you're using the stringr package, you can do it like this: str_extract_all(x[,1],"/see[0-9]{2}/",simplify=T) This would give you a vector of all the matches. &amp;#x200B;
&gt;separate(data = x, col = "V1", into = c("dot", "folder", "file"), sep = "/") Worked like a charm. Thank you so much. 
Thank you for this piece of code. I'll use it for some testing later. I need to learn REGEX asap.
Thank you a lot for this. I think your code drove home the gist of the whole \[#-#\] thing for me. 
Thank you!!
In this particular case you **should not** use during functions and regex. Instead, use file path operators. Either via the `fs` package or, in base R, via `dirname` and `basename`. This both makes your intent clearer and thus creates more tradable code, and it's more robust against weird edge cases that you might have forgotten about.
Split the column on 4 characters, turn the numbers into a numeric, and then resort. To split you will probably want to use a mutate with a stringr function. 
 library(dplyr) library(stringr) df &lt;- data.frame(Seed = c('seed1', 'seed10', 'seed2', 'seed3'), Result = c(12, 15, 2, 6)) df %&gt;% mutate(seed_index = as.numeric(str_extract(Seed, '[[:digit:]]+$'))) %&gt;% arrange(seed_index)
Provide your actual values in a 2 x 2 contingency table format. P.S. keep reading about Fisher's exact tests. Read til you get it.
Thank you, especially for the regex part. Your answer worked just like I needed. 
Are you using RStudio? If so, could I add one thing, to start using [https://rpubs.com/](https://rpubs.com/) . Register for a free account and set up RStudio to 'knitr' your projects to be displayed on the web. It is so much easier to provide a like then explain everything over again. P.S. I do not work for RStudio I just like [rpubs.com](https://rpubs.com) as a great resource and teaching tool.
Apologies I had it flipped, it should have been: par(mfrow(1,2)) And then the 2 plots.
 df %&gt;% arrange(seed %&gt;% str_replace(“seed”, “”) %&gt;% as.numeric)
This doesn't work. I'm working in a Rmarkdown doc using (2,1) or (1,2) just plots them underneath each other
If they're in an rmarkdown, what you need instead is to make each of the plots narrow enough that two can fit on the page and they will be side by side automatically.
Doesn't seem to be the issue
Did U just google “what do data scientists do” then paste that here?
Ah well there’s your problem ;-) I googled this question and it appears to be a common gripe. Have you tried adding the option fig.show = “hold” in your options for the code chunk with the plots? Another very clunky way to do this would be to use the code I provided to generate the plot in the console, save the plot in an image format and import it into the document as an image.
You could build a shiny app with the NOAA API. It's updated pretty frequently. 
No I did not. Bu thank you for your useless answer!
Thanks - I will definitely give that a try.
What error message do you get? I doubt this is the problem as you most likely just missed the last line of your code when posting here. But as it is here, you’re missing a line with ``` at the end to close the chunk. 
Sorry, I don't know much about the password issue so I can't help there, but have recently added an "Export a report in word" button to a self-contained mathematical model which runs locally built with a shiny frontend (i.e. no password requirements). There's some additional faff with getting it to take the variables from the shiny server, and also some additional faff with getting it to accept a reference\_docx. Shout if there's anything with respect to that stuff I might be able to help with. Sorry I can't help more...
`label="User Name"` - with space? Are you sure?
Np. There's a lot of government data APIs you could access. The weather ones are updated frequently and require little technical subject matter background to understand. The APIs themselves are typically fairly archaic and rate limited, but it's a good place to start. 
Using shiny embedded in an RMarkdown document doesn't work the way you seem to think. Given your current approach the subsequent code block will not depend on the reactive inputs you've created. A slightly different approach that is much more likely to work is using RMarkdown's parameterized report functionality - see more [here](https://bookdown.org/yihui/rmarkdown/parameterized-reports.html) and [here](https://rmarkdown.rstudio.com/lesson-6.html)
Can you share the full code and df? 
I will look at your references but what I posted here was intended to just be "skeleton" code just to see if I could get the interface working. I know you can only read reactive inputs only in reactive environments.
Sorry that last line must have been cut off by mistake when I copy &amp; pasted my code. I do have that in my code though.
Still though the inputboxes do not appear when I run the notebook.
Ok, it would be nice if I could just use the shiny GUI to ask the user to input their credentials but I guess this parameterized approach will have to do for now.
You need to provide more information.
Maybe It is a manifesto?
haha - I almost replied "Cool! Sounds fun"
I want to recreate this network sample_gnp(100,0.05) 500 times to store each time in a vector the number of edges each iteration will have I tried using a for loop but it doesnt run it as many times. The code i used was Links&lt;- c(500) For (i in 1:500) { Network[i] Links[i]&lt;- gsize(Network[i]) } Which didnt work 
for (i in 1:500){ gsize(Network[i],Links[i]) }
It brings an error for unused argument about Links[i]
I am trying to make 500 random networks measure their edges and then prove the Erdos and Renyi theory for a university assignment
Not sure what you're doing, but you may want to do Links = rep(0,500) before doing that so you have a vector of length 500 to put your results into
Ok well first focus on creating the random networks. https://stackoverflow.com/questions/39880873/how-to-generate-a-series-of-random-networks-in-r Then you have build the for loop correctly
Thanks for the tip i used Links &lt;- c(500) To create 500 entries If you could understand i used this code and except the first entry i get NA on the others The code is : for (i in 1:500) { Links[i] &lt;- gsize(Network)[i]}
I was able to solve, but uncovered another issue. &gt;getColor &lt;- function(df) { sapply(df$Status, function(Status) { if(Status == '80% Undersaturated') {"red"} else if(Status == '50% Undersaturated') {"orange"} else {"green"} })} icons &lt;- awesomeIcons( icon = 'home', iconColor = 'black', library = 'ion', markerColor = getColor(df) ) popup &lt;- paste( "&lt;strong&gt;Rental ID: &lt;/strong&gt;", df$Rental.ID, "&lt;br&gt;&lt;strong&gt;Total Units: &lt;/strong&gt;", df$Units, "&lt;br&gt;&lt;strong&gt;Availability %: &lt;/strong&gt;", df$Availability, "&lt;br&gt;&lt;strong&gt;Relative Price 0BR: &lt;/strong&gt;", df$X0BR, "&lt;br&gt;&lt;strong&gt;Relative Price 1BR: &lt;/strong&gt;", df$X1BR, "&lt;br&gt;&lt;strong&gt;Relative Price 2BR: &lt;/strong&gt;", df$X2BR, "&lt;br&gt;&lt;strong&gt;Relative Price 3BR: &lt;/strong&gt;", df$X3BR, "&lt;br&gt;&lt;strong&gt;Link: &lt;/strong&gt;", df$Slug ) pal &lt;- colorFactor(c("orange", "red", "chartreuse3"),df$Status) leaflet(df) %&gt;% addTiles() %&gt;% addLayersControl( overlayGroups = c(df$X0BR, df$X1BR, df$X2BR, df$X3BR), options = layersControlOptions(collapsed = FALSE)) %&gt;% addLegend(pal = pal, title = '2018 Saturation Status', values = df$Status, opacity = 2, position = 'bottomright') %&gt;% addAwesomeMarkers(~Lon, ~Lat, icon = icons, popup = popup) 
&gt; Thanks for the tip i used Links &lt;- c(500) To create 500 entries Yeah, that creates a vector of length 1 with "500" as its entry. Do what I put above.
I made the random network using the function from igraph i just need to replicate it and store the edges. I am having trouble with the for loop I made this code now: For (i in 1:500) { Link[i] &lt;- gsize(Network)[i] } It makes 500 entries in Link but only the first is the number of edges the others are NA i dont know what is wrong.
So the loop is just replacing gsize(Network)[i] with whatever iteration of the loop it's on. Link[1] &lt;- gsize(Network)[1] Link[2] &lt;- gsize(Network)[2] Link[2] &lt;- gsize(Network)[3]...
do you find it wrong? if yes can you propose a better way
I did it and still get NA. I want the loop to run my network 500 times and each time store the number of edges in the empty vector.
what is stored in Network? 
I did and it doesnt work as well network i am not sure what stores exactly.
To be honest, you just seem like you want an answer without really providing any context, and it's impossible for me to help you. Find out what is stored in Network, and get back to me. 
or `str(Network)` can also be helpful
You can use `n%%2` to check evenness.
Have you tried just running the command `(1:13)/(1:2)`? R gives a warning, but it will still do it.
Hey so we are getting the error: &gt; Warning message: In (1:13)/(1:2) : longer object length is not a multiple of shorter object length Though that said it does not run after the error
If you just want to divide every even indexed element (e.g. elements 2, 4, 6...) by two, why not just do it directly? example_vector &lt;- 1:13 example_vector / rep(c(1,2), length.out=length(example_vector))
Thank you so much this did it, still working on it for this ill get us where we need to be. Thanks again!
x / rep(c(1,2), length.out=length(x)) if(x[length(x)] %% 2 != 0) {x &lt;- head(x, -1)}; x / 1:2
This is a warning, but it will still produce an output as you desire.
Well it shouldn't matter which input you are using, passwordInput, button, selectBox or etc it should all work the same.
ok I was able to get parameters working, but if rshiny+rmarkdown doesn't work with parameterization then whats the point? I thought the entire point of using both together was parameterization and/or dynamic input/output.
Makes sense now thank you very much, still learning R for the most part, appreciate it
The majority of an rmarkdown document is meant to be static and created at compile time (knitting). Embedding a shiny app is exactly that, embedding a dynamic app within the larger static document. In terms of authentication and generating parameterized reports this is one of the key features of RStudio Connect which is one of RStudio's paid products.
Unfortunately I there is no way the organization that I work for is going to pay for that else I'd get it. I would certainly like to have paid support but that's a pipe dream.
You can remove the entire column that has many missing values. If it's more than %75 missing value, it's easy to justify this decision. 
[http://chartmaker.visualisingdata.com/](http://chartmaker.visualisingdata.com/)
It's nothing that drastic. I think there's max 15% missing in a single file. But the larger chunks make it way more time consuming to clean (like upwards of 7+ hours) than it has to be and I figure I'm not missing out on too much information removing a couple thousand obs.
Holy shit 
Look into the Mice package. It can interpolate the missing values for you. 
ggplot(YourData, aes(Xvalue, Yvalue)) + geom_bar(stat = “identity”)
That's a lot of effort just for her to tell you that you're still wrong about whatever argument you're trying to win here. :)
Caught me. 
I'd do a loop that tracks the most recent speaker. Start with grepl to tell you if it is husband or wife and then add a row for each line where the speaker doesn't change. I think long format would be easiest for storing data: s &lt;- 'Husband: What time will u be home? Wife: Well...I was going to stop at the salon on my way home So...probably like 5:30ish Husband: Alright. Do u want me to make spaghetti? And have it ready around then Wife: I guess that works ' lines &lt;- unlist(strsplit(s, split="\n")) df.long &lt;- data.frame(speaker=character(), message=character()) for(line in lines) { if(grepl("^Husband:$", line)) { speaker &lt;- "Husband" } else if(grepl("^Wife:$", line)) { speaker &lt;- "Wife" } else { df.long &lt;- rbind(df.long, data.frame(speaker=speaker, message=line)) } }
Is there an order to your data? If there is no order then the fact that there are large chunks is immaterial, you can remove all or a fraction of them. If there is order to your data then it gets more complicated and the best way would be to just write a for loop to count the consecutive missing rows and if its under a certain threshold then keep it else remove it. 
I wouldn't necessarily suggest imputing the large chunks of missing data. Based off the initial post the large chunks of data appear to be missing for a reason unlike the "random smaller missing pieces" the author wants to preserve. It might be okay for the information to be missing. For example, if a survey is asking about you to rate the quality of your primary care physician over the last year and you have not gone to the doctor last year imputing a rating for you would not be appropriate. &amp;#x200B; Concerning the initial question, without knowing more about how you are identifying which missing values to keep and which to remove it is hard to give a suggestion. If you are literally going through and looking for large chunks of missing values and just removing them you could write a loop that divides the data into even pieces and calculate the percent of missing values. That will narrow down where you have to look at least.
Worked like a beaut. Thanks! Now I need to work on some ways to tally up counts per person. :)
I've veen using na.ma from imputeTS. Do you know if mice does it better? (also faster, for files with larger chunks of missing data, na.ma takes insanely long) 
I was hoping there was already a way to do it but yeah my friend also suggested writing a function. Thanks
Yeah it's time series data collected every couple milliseconds over a couple of days. The data looks somewhat stationary so losing a couple minutes worth hopefully doesn't hurt too much Thanks for the suggestion 
I tried that and it doesn't work because I want 5 different variables on the X-axis side by side
Well, your scale needs to have data in order to plot. Surely there is a column detailing this score of 1-7 somewhere. Even an index. 
Ha ha
Should I just make a column and put a 7 in it? My data only has one row as I'm trying to make a graph for a single individual
You can use tibble::rowid_to_column() and it will create a column with values 1-7 in the order the rows are in. A bar chart needs data to fill the bars up with. 
Thanks I'll look into that and see if it works!!!
Hmm, I read the description of the function and I don't think it pertains to my issue - so I'm probably explaining my problem poorly. &amp;#x200B; I asked participants 7 Likert questions all on a 5-point scale. I want to make a bar chart with the 5-point scale on the y-axis, and a different bar for each of the 7 questions. I want all 7 bars on the same chart, and I want such a chart for each one of my participants. 
Ahh, your y axis is the score they gave each question. 
Yes! But even if they answered 1 for every question, I still want the Y axis to go to 7. 
Ylim(0,7)
But then what do I put for y in aes()?
I really like how MICE will produce an md.pattern that visualises which data you're missing. Give it a go, maybe? It'll save you a bit of coding, anyway. Also, this might be heresy, but if you have very fine grained data set where you're just missing a millisecond here or there, is it possible to collate it into larger chunks - like seconds - without loss of meaning? 
Try dplyr drop_na() E.g. library(tidyr) #drop all rows with na df %&gt;% drop_na() #drop rows where na occur in specific columns (also see the other dplyr select options) df %&gt;% drop_na(colname1, colname2)
Look into run-length encoding ```rle()``` to identify the blocks of missing values maybe. Is your time series univariate?
My wife and me.
You aren't specifying the vars correctly in melt. It has to be either the variable name (as a string) or the column position (as an int). This works: df.wide &lt;- data.frame(ID=c(1), Q1=c(5), Q2=c(3), Q3=c(2)) df.long &lt;- melt(df.wide, id.vars = "ID", measure.vars = c("Q1","Q2","Q3"))
For anything else than a trivial amount of data, this will get exponentially slower since it has to copy all the data every time you rbind. grep* (and the tidy versions from tidyverse) are vectorized, and there is a handy function from the zoo package which fills in missing values with the last non-missing value, allowing you to do this: long.df = data.frame(lines) %&gt;% mutate(speaker = str_extract(lines, "Husband|Wife") %&gt;% na.locf()) %&gt;% filter(!str_detect(lines, "Husband|Wife"))
Give us a sample of your data and you'll get much more helpful feedback. NOTE: you don't have to post your actual data, some made up data in the same format will do.
Here's the tidyverse method: gather(data, variable, value, -id) 
For each plot you need to make a data.frame with 2 columns: question.num, and likert.score. Then your mapping is aes(x=question.num, y=likert.score)
Ggplot requires “tidy” data, so you’ll have to do some transformations
It's not very neat but this works if you're looking for ideas to go on from: s &lt;- "Husband: What time will u be home? Wife: Well...I was going to stop at the salon on my way home So...probably like 5:30ish Husband: Alright. Do u want me to make spaghetti? And have it ready around then Wife: I guess that works" s &lt;- strsplit(s, "\n")[[1]] s1 &lt;- grepl("^Husband:", s) + grepl("^Wife:", s) * 2 label &lt;- c("Husband", "Wife")[s1[s1&gt;0][cumsum(s1&gt;0)]] d &lt;- data.frame(s, s1, label, stringsAsFactors = F) d &lt;- d[d$s1==0,] table(strsplit(gsub("\\.|\\?", " ", paste(d$s[d$label=="Husband"], collapse = " ")), "\\s+")) &gt; Alright And around be Do have home it make me ready spaghetti then time to u 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 want What will 1 1 1 table(strsplit(gsub("\\.|\\?", " ", paste(d$s[d$label=="Wife"], collapse = " ")), "\\s+")) &gt; 5:30ish at going guess home I like my on probably salon So stop that the to was way 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 Well works 1 1 
Create a dataframe with 3 columns. Respondent, question, score. When you have that, your ggplot(dataframe, aes(x = question, y=score)) +geom_bar() +facet_wrap(~respondent) Geombar likely won't work, as it wants to count data to add to the bars. You'll likely end up using a dot plot like geom_point(). I might even say fuck the y axis, and do a size=score mapping on a line, then do y = respondent and not need the facet. 
How large is the list of phrases you are looking for? While I am sure there is a way in R, you would probably first have to tokenize the sentences put them in a dataframe and then compute statistics from there. However you would have to do some type of sentence length normalization depending on if the sentences were row or column vectors. There is an "NLP" package for R that has some of the capabilities you are looking for but I am not really familiar with it as I find preprocessing more convenient with NLTK/python/ipython Personally, if you just need frequency/collocation information, I would just use a few grep/sed/awk commands on the command line. Maybe with a little preprocessing of the text beforehand to deal with contractions, upper/lowercase issues etc. 
What location do you need this data for? The package [weathercan](http://ropensci.github.io/weathercan/) pulls historical weather data from Environment Canada, but that obviously won't work if you're somewhere else. &amp;#x200B;
Yes, this depends most on your location and weather data provider.
If there's a ton of data, then the loop will be much slower than the vectorized solution. I was assuming we're on the order of 10k at most, which will run fairly quickly regardless. I used a loop because it is generally more understandable to beginners.
This has helped me in the past https://cran.r-project.org/web/packages/rwunderground/rwunderground.pdf
I’ll take a look at this tonight — thanks for the feedback. 
Happy to help. Depending on what you plan to do with the data it might be better to determine your granularity before deleting. If it's minutes then delete whole minutes. Gaps in time series can be difficult to handle. If it's generally stationary then interpolation might do a good job. 
Try using some of the functions in the `scale` package. For example, add `scale_x_continuous(labels = scales::number_format(scale = 100))` to your plot.
You can use darksky.net api. 1000 calls per day is free. Also has historical data. There is also an R package https://cran.r-project.org/web/packages/darksky/index.html
I’ve recently discovered this package and I believe this will make your analysis possible: https://github.com/jaytimm/corpuslingr 
Convert the column that contains the value NA as character and then you can use na.omit data$nacolumn&lt;-as.character(data$nacolumn) data&lt;-na.omit(data) 
Clarifying question -- are the NA in your columns (which are the abbreviations) strings? If so, this should be pretty easy--they shouldn't be picked up by the various ways to remove NA (which are a logical type), such as the na.rm option in various functions or drop_na function from the tidyverse If not (and I'm not sure why/how this would come about...), this is probably far more complicated and best dealt with by somehow changing the dataset
Replace the data - &amp;#x200B; library(tidyverse) df %&gt;% mutate(mycolumn = ifelse([is.na](https://is.na)(mycolumn), "NA", mycolumn))) &amp;#x200B;
Holy guacamole that package seems very useful for the task I want to accomplish. Here comes the best part: I didn't say I'm Spanish and I'm working with Spanish texts. This package is english-specific so I can't use it :(. Very foolish of me. Thank you very much though!
ace, thanks. The first of those worked fine
I haven’t tried with other languages but I think it could still work. You can select Spanish language model and use the lemma and pos tag information to search for the phrases. 
`str\_replace\_na` in the `stringr` package will do this!
ah wait, I may have misunderstood. `str_replace_na` replaces all NAs with the string "NA" It sounds like your issue might be at read-in... are "NA" and missing cells in your original data both reading as NAs, so that you can't differentiate them?
This doesn't sound like an R issue per se. Having "NA" as a value that means something other than NA in the same data that actually has NAs is going to cause problems in almost any software. I would suggest changing the "NA" signifier to something else in pre processing so that it doesn't get confused with actual NAs
why not create a new column multiplied by 100 from the original with mutate, then use that as the x axis?
This was my interpretation of OP's issue. If that's true, use the na.strings command on import to specify what your real NA values are. E.g., if you use -999 for missing data, something like this: d &lt;- read.csv("mydata.csv", header = T, na.strings = "-999")
You had to post a picture of the table didn't you?
And mutate_all or mutate_at will let you run it on multiple columns
Fair warning that its estimates are often completely off. My company looked into using them and we found things like “February 2018 had no snow in Chicago” even though they had a blizzard.
My company looked into this. There are no good solutions AFAIK for zipcode or finer-grained weather data. The issue is that no one does a good job keeping track of weather data outside of weather stations. So backtracking and saying “this zip code not near a weather station for sure had an temperature of X degrees” involves interpolating between weather stations, which are typically at airports. [This blog post delves more into the details](https://theoutline.com/post/3826/crap-weather-apps-meteorolgists). In particular I’m annoyed with Dark Sky because their data was often completely wrong. Like I pulled a zipcode in Chicago in February 2018 and they said it got no snow the whole month, even though Chicago had a blizzard.
Thank you. I have encountered some errors (as usually :\_\]): &gt; cnlp_init_udpipe(model_name="Spanish",feature_flag = FALSE, parser = "none") Error in udpipe::udpipe_download_model(language = model_name, model_dir = model_loc) : Please provide a valid language. &gt; ann_corpus &lt;- cnlp_annotate(corpus1$text, as_strings = TRUE, doc_ids = corpus1$doc_id) Error in cnlp_annotate(corpus1$text, as_strings = TRUE, doc_ids = corpus1$doc_id) : No initialized backends found. So yeah, I don't know how to select the Spanish language model (assuming it's available). Thank you for your reply!
Try spanish in lower case: `udpipe::udpipe_download_model("spanish")` Works for me. &amp;#x200B;
I had a similar terrible experience with my university first offering a "Data Science" course. Little to no coding experience required. It was the worst class I've ever taken --- the guy lectured at random topics that had nothing to do with our assignments or homework expectations. One week we were supposed to learn and code in SQL out of nowhere. The class complained as a whole. Since it was a post-doc teaching it, they promised (and did follow through) on not renewing the person's contract. Which is too bad too - because the person is even a package developer for a decent R package. Just could not teach for the life of them. 
&gt;we have jumped right into creating functions, creating regression models with lm, and applying the(a?) knn-algorithm (which I really don't understand at all). This is *so* wrong... I'm not sure what possessed your department to offer this kind of course, but they screwed up. To answer your question, this does *not* seem ok for a course where previous programming experience is not required. I'm cringing thinking about all the students who will come away from the class thinking, "I know how to use [technique]" without actually understanding the underlying mechanics and assumptions. You may try reading Hadley Wickham's book *R for Data Science*. It should at least give you some help with coding in R.
That sucks big time, especially without an introduction to the basics of the language. If you want to learn on your own, spin up Rstudio and go through the [swirl courses](https://github.com/swirldev/swirl_courses) they're dope. Then you can read R cookbook or R for data analysis and you'll be on your way :) You can follow the [Learn R track](https://www.r-bloggers.com/how-to-learn-r-2/) on R bloggers too
This is exactly what happened to me in grad school. It was awful and I had to learn R on my own, and didn't get good at it till well after the course was over! 
Datacamp my dude. It’s a lifesaver.
Yeah, I've been considering it. Would you say the subscription is worth the money? 
To be fair I got a subscription through a class so I’m not paying for it but it’s very good quality content. I think you can do a trial period or get a few of the intro courses for free.
Start with the free courses it will help
This does not sound like an appropriate approach for the course. It seems like they are trying to claim that they can take someone who is a beginner (in both programming and machine learning algorithms) and turn them into a practitioner after taking 1 course. I took Andrew Ng's course on Coursera, and while it was very helpful, I didn't feel like I was actually any good at matlab. It was definitely more of a course on algorithms than a course on programming. The problem with some courses out there (like yours) is that they try to teach both programming and machine learning algorithms, which I feel is inappropriate. If I were to teach an "Into to Using R for Machine Learning" course at university, it would probably go something like: 1) What is R good at? What niche does R fill in the programming language space? R is interpreted, explain what actually happens when you download R, where the commands go, what different things in the R directory mean. 2) RStudio. I think a lot of us take for granted how much we know about Rstudio and how to work it. It is simple to get the hang of it, but it can be overwhelming when trying to learn both R and navigate Rstudio. 3) R stores things as lists/vectors. Basic object types and operators ($, &lt;-, ?, etc.). Differences between vectorized operations and scalar functions. Since we are talking about functions, discuss how to get help with functions using documentation (?) and understand the parameters that go into them. How to view function code. 4) Discuss object classes. Show how different functions can return different classes. Don't get into S3 generic functions quite yet. Classes and the implied functionality are referenced throughout the entire course, but main introduction is here. 5) This is a good time to introduce packages. What do you get when you download a package, why are they useful? Explain CRAN, how to look up package documentation, how to load, etc etc.. Discuss how to view exported functions using ::. 6) data.frames and matrices, how to manipulate them, create a data.frame from vectors, extract data, summarize them, etc etc. data.frame interrogation is extremely important, you will use them on (pretty much) any data science project. This is so important, it is possible to take several weeks with useful data.frame operations, especially if you want to move into dplyr or data.table. This is all a bare minimum to be able to interact with R. From here, if you are impatient, you can move on to creating simple models. 7) Create a simple linear model. Show how to interrogate model. The model is stored as a list of a certain class. Look at other functions exported by the package. This is a good time to discuss S3 generic functions, and how they work with classes. How can you run predict() on linear models that have 2 different classes? 8) Depending on time left in the semester, start discussing R implementations logistic regression and then neural networks.
Blows big time. Well be sure to ask questions here, god knows I'll try to answer them to the best of my abilities if I see them. I got lucky when I learned R. They did it as a part of our Biometry course (really just a basic, graduate-level statistics class), but did the math and graphing in R. Gave me a real opportunity to learn more of the language on my own but with a good starting place. I'm quite proficient these days, though I don't do much ML in the "big data" sense of the word. Still all the fundamentals are the same, regardless of application. 
I believe the department can get you free access. If they are aware this many students are struggling you probably have a good chance of convincing them.
I'm having some troubles understanding what your data look like. Could you give an example of, say, the first 50 rows of your dataframe? PS: you can extract the first 50 rows by running your_df[1:50,] &amp;#x200B;
Anyone able to assist??
While this could be done in base, and you are almost there with your code; I would go right to the `tidyverse` for this. The resulting code will be more readable. First let me load dplyr (a package in the tidyverse) and generate some sample data: library(dplyr) tibble(condition = rep(c(-10, -5, 0, 5, 10), each = 10), timestamp = rep(1:10, 5), SWA = rnorm(50, 0, 0.06)) -&gt; pilot_clean Now the following code will select observations on both condition and timestamp when the current observation for SWA is not in the deadzone and the previous observation was in the deadzone (i.e., the first true value after one false value). See if this does what you want: pilot_clean %&gt;% mutate(deadzone1 = abs(SWA) &lt; 0.05, deadzone0 = lag(deadzone1)) %&gt;% filter(!deadzone1 &amp; deadzone0) %&gt;% select(condition, timestamp) -&gt; pilot_clean_out pilot_clean_out Note, the above does not take condition into account and allows SWA observations to "bleed" over from condition to condition. I suspect you wish to keep the conditions separate and capture the very first observation within each condition if it is out of the deadzone. The above code will not do this if the last observation in the previous condition was not in the deadzone (i.e., "bleed" over). If you wish to treat conditions as separate try this code: pilot_clean %&gt;% group_by(condition) %&gt;% mutate(deadzone1 = abs(SWA) &lt; 0.05, deadzone0 = lag(deadzone1)) %&gt;% filter(!deadzone1 &amp; (deadzone0 | is.na(deadzone0))) %&gt;% select(condition, timestamp) -&gt; pilot_clean_out pilot_clean_out 
For deconvolution of a wave into it's periodic components the most common method is called Fast Fourier Transform and you can find an R introduction [here](https://cran.r-project.org/web/packages/mwaved/vignettes/mWaveD.html) For absorption/emission spectra (just in case somebody else google this response) the common approach is called Gaussian deconvolution, and the package [Peaks](https://cran.r-project.org/web/packages/Peaks/Peaks.pdf) deals with that. 
This!
As u/LoveFromTheLoam noted: I strongly suggest looking at the Tidyverse packages - [tidyr](https://tidyr.tidyverse.org/) is fantastic, and combined with dplyr is a very conceptually coherent way to do data management ahead of statistical and modeling work. &amp;#x200B; Just take a look at the [tidyr cheatsheet](https://github.com/rstudio/cheatsheets/blob/master/data-import.pdf) (page 2 of the linked PDF) - I keep a stack of the Tidyverse 11x17s by my desk, hanging on a binder clip on a hook. &amp;#x200B;
Dude fr I am so lost, I'm learning R in class too and it's really really fast paced and I feel like I'm getting rushed 
Bro, hit me up if you wanna toss some thoughts back and forth, I think it helps to have someone to talk to when you're starting out
Ight, I msged you. I'm exhausted at trying to write this code that seems impossible for my brain
Without more details, this seems more like a math problem than an R problem. Isn't (linear) projection just going to be a matrix multiplication? The matrix multiplication operator is %*%.
That is bizarre. Either you are mistaken (provide an example?) or you should report this to the maintainer (with an example).
This is like blaming the victim for getting attacked. If you are using `read.csv` then the na.strings parameter can redefine which character strings should be interpreted as `NA`. Other data input functions should have similar features, or switch to `read.csv` if your favorite does not.
OP said he used "NA" as a representative signifier for example as an abbreviation for North America and was having trouble differentiating it from R generated NAs. The best way to fix this is to not use "NA" as a representative signifier. 
Once the data are read in properly it is easy to distinguish between an `NA` (missing, has no quotes) and an `"NA"` (a character string, has quotes). The problem was that the data were not being read in properly. And no, your solution damages the data and is unnecesary... I have worked with missing data and `"NA"` values together many times.
Thank so much for your help. I ***think*** this code is what I'm looking for, however I have noticed a problem: Below is the screengrab of my updated experimental dataframe: As you can see, I now have a trial number column that indicates how many trials have gone by. This column provides an additional problem - I now need the first SWA outside the deadzone within each trial i.e. if the SWA goes outside the deadzone, then back within, and then outside it again, I don't want two value - I only want the first value for each trial. Is there a way to alter the code so that is only picks the first value of SWA outside of the deadzone within each trialn? &amp;#x200B; &amp;#x200B;
Hello! See my comment below with the table - my data looks like this now. Do you have any suggestions for my problem?
I used Hadleys book when I had run in to a roadblock and couldn't speed something up any other way or at least I didnt know how. I only go down that path if it's really necessary though.
Start with googling it. That's all I did. This was the first search result, it has all you need. http://www.rdatamining.com/examples/time-series-forecasting
Check out [tidytext](https://www.tidytextmining.com/tidytext.html). It's an R package, but it also has a nice book-size online tutorial.
Stick with Hadley. If you really need to master rcpp for a project then advance to eddelbuettels.
shouldn't it be \`t(prtest$rotation) %\*% prtest$x\`?
&gt; t(prtest$rotation) %*% prtest$x Thanks for the suggestion, but I'm afraid not: ``` &gt; t(prtest$rotation) %*% prtest$x Error in t(prtest$rotation) %*% prtest$x : non-conformable arguments ```
I'm sorry for the confusing and wrong answer in the beginning. &amp;#x200B; If `retx = T`, then prtest$x is the transformed matrix you got by `t(A)%*%prtest$rotation`. If you then want to calculate the covariance of `prtest$x` the result will be a diagonal matrix.
Yep. Easy peasy. Here's what I would do. # split data frame into groups of observations based on the trial column, a &lt;- lapply(split(your_df, your_df$trial), function(data) { i &lt;- data[abs(data$SWA) &gt; 0.05,] # find all observations that exceed threshold if (nrow(i)==0) return(NULL) # handle cases where no observations meet critera return(data[1,]) # return only the first match }) # a is now a list, let's reconvert it back to a dataframe a &lt;- do.call(rbind.data.frame, a) &amp;#x200B;
Onto which subspace? I can help with the math but there's not enough information here. As the other reply said, once you've figured out *what* you want to do, you'll probably use the R matrix multiplication operator to do it. You may also find it necessary to rearrange the matrix into a column vector and vice versa. The easiest way to do this is probably to change the "dim" attribute directly. 
Correct me if I am wrong, but I think you want to swap out `data` for `i` in the last line of your nameless function. Another issue to consider is that OP said: &gt; first true value after a set of falses This says to me that OP wants possibly more than one row from each trial. OP?
If you need complicated stuff, you'll have to really learn it. If you just dropping down a little bit, Hadley and then googling if you get stuck is sufficient. I've basically done the latter, but I also did C before this, so I don't have a good gauge on this. I think unless you're getting really in the weeds, this is enough.
So then `prtest$x = t(A)%*%prtest$rotation` ok, yes, that does work out. Thanks very much for your help. For anyone else who might be in a place to edit the documentation: I do feel like saying that I think this convention really weird, and the documentation a bit lacking. Like, so \`x\` is the rotated data multiplied by the rotation matrix ..? why not just give the rotated data directly? and is it left-multiplied, or right-multiplied? And why not explicitly state the relationship in the help menu? These sorts of things would make the package a whole lot more useable. but for moment, yeah, thanks very much to /u/panda_yo
&gt;Correct me if I am wrong, but I think you want to swap out data for i in the last line of your nameless function. Ah, yes, well spotted! &gt;This says to me that OP wants possibly more than one row from each trial. OP? Sure, that could be. Hopefully he comes around to clarify what he wants. Anyway, getting multiple values isn't all that complicated. Here's one way you could modify the above code grab all rows with an SWS value above the set threshold that are ALSO preceded with one row below the threshold: a &lt;- lapply(split(your_df, your_df$trial), function(data) { rownames(data) &lt;- 1:nrow(data) candidates &lt;- data[data$SWA&gt;0.05,] preceding_row_indices &lt;- as.integer(rownames(candidates))-1 preceding_row_okays &lt;- data$SWA[preceding_row_indices] &lt;= 0.05 if (0 %in% preceding_row_indices) preceding_row_okays &lt;- c(TRUE,preceding_row_okays) candidates[preceding_row_okays,] }) &amp;#x200B; &amp;#x200B; &amp;#x200B;
Sorry to bug you again, but this is still not working out. How do I actually get the data itself in the rotated coordinates? With the example above, I have: `t( prtest$rotation )` `A` `PC1 0.4911572 -0.1972022 -0.4184729 -0.7380761` `PC2 -0.7922696 -0.4517617 -0.3551502 -0.2051548` which is clearly scaled (the original data varied by as much as 50), even though I specifically set "scale=False, centre=False". Again, sorry for creating a hassle.
Eddelbuettel's book is complicated. I gave up after a few chapters. Hadley's book is probably sufficient. The times I've had to write Rcpp, I've only had to write a small amount of C++ code, usually some matrix algebra with Armadillo. Just skimming Hadley's book and googling Armadillo was enough for me. Depending on how complicated your problem is you can get away with writing very little C++ code. A lot of the time, you can speed up your code a lot just by writing a function or two in C++ while keeping the rest in R. For example if you're maximizing a function, you'll notice big improvements if you write the function in C++ but still use R's optimization library. 
Example plot with blurred polygon here: [https://imgur.com/a/ybVzaDz](https://imgur.com/a/ybVzaDz) 
Yes, for my needs that is precisely what I'd want; writing select functions (e.g. objective functions) in C++ and the rest in R. I only use R for statistics/econometrics and don't foresee any need for 'advanced' C++ features. Thanks for your input, it's very comforting to know that I won't have to delve into such a large, complicated language to benefit from it.
I'm not even sure what C++ has to offer for someone only interested in statistics (other than efficiency). Hadley + Google sounds like the way to go.
Yeah, the C++ is only for speeding up long, time-consuming calculations (say, you do a ton of matrix manipulation and multiplication or you do MCMC or something).
You definitely won't. Learn a bit of (Armadillo)[http://dirk.eddelbuettel.com/code/rcpp.armadillo.html] and you'll have everything you need.
What exactly is the purpose of Armadillo? Do linear algebra operations not exist in C++? Can't we compute t(X)%*%X?
It's a C++ library. Not sure how well linear algebra is supported otherwise. Armadillo is fast, straightforward, and easy to learn. I don't know anyone who doesn't use it for Rcpp.
Alright, good to know. Lots to learn. Thanks.
Thank you, I essentially did this in a tidyverse manner. `library(tidyverse)` `library(gridExtra)` &amp;#x200B; `dataset %&gt;%` `select(InDegree, OutDegree, Attribute, Label, Value) %&gt;%` `mutate(Attribute = as.character(Attribute)) %&gt;%` `drop_na(InDegree, OutDegree) %&gt;%` `group_by(Attribute) %&gt;%` `summarise(In.Correlation = cor(Value, InDegree, method = "pearson"), Out.Correlation = cor(Value, OutDegree, method = "pearson")) %&gt;%` `grid.table()`
I disagree. This is not the best solution when the abbreviations are someone else's and "NA" is meaningful to them. I'm not changing a taxonomy that I don't own for my own convenience.
If the issue is merely clarity and not mixed meanings then you could just as easily change NA to “missing” or any other signifiers as others already pointed out. This would preserve the original taxonomy if you prefer. 
Your original question was vague and open to multiple interpretations. As using NA as a signifier might be to anyone reading your outputs. If you insist on using it in a non standard way you should definitely use a key on all outputs. 
I think you're confusing *rotating* a matrix with *transposing* it. `x` is the *rotated* data. This is equivalent to saying that its the result of multiplying the input to the function with the rotation matrix. I do agree that the documentation is confusing, and I've accidentally used the rotation matrix instead of the result data (i.e. `x`). If your input matrix is square, you don't necessarily notice easily.
prtest$x should be your rotated data.
You need to read Rob Hyndman forecasting principles. Then you need to install the packages tsibble and fable, and get to work with tidy forecasting. 
For matrix multiplications it’s pretty much as fast to take the simpler option of using an optimised BLAS. 
If Google has a style guide then use that. It's written by a team of people much more experienced than you. Trust them. Also, making your own unique style is not a good plan. Being consistent within your code is one goal. But a better goal is also being consistent with best practices, so that other people can easily read your code and you can easily read others. 
Sorry if this is a ridiculous question, but do you use the tidyverse? It’s one of the main reasons I prefer R over Python - the readability of the code when using tidyverse
Google styleguide isn't really the best for R, it is rather old and apparently reflects some aspects of internal structure that don't really port well. Hadley, of course, has some recommendations. But style shouldn't be an overarching template.
Agreed. Last I saw they recommended using a period instead of underscores for variables. I.e monthly.revenue instead of monthly_revenue. I am not sure if Hadley / tidyverse has an official standards document. But I think emulating anything they do have will be much more relevant than the Google one 
The [Tidyverse Style Guide].
&gt; Last I saw they recommended using a period instead of underscores for variables. I.e monthly.revenue instead of monthly_revenue. Pet peeve of mine but I really don't like that. yeah, don't do that in R, that's confusing because the . thing is also used for methods. The _ thing used to be discouraged because in very early versions of R it could be used for &lt;- but that died out quickly. 
&gt;I think another source of my problem is that I am still learning and so often I'm doing something for the first time. I find that this is usually very buggy and that my way of learning by trial and error destroys any notion of clean code. code first and then make clean. I think you need to realize there's a couple different things going on here as "style": 1. formatting of code, perhaps including naming of variables and functions. this can even be somewhat automated, and there are good and bad ways of naming variables and functions to be sure, and it's good to be consistent. 2. documenting code - "self-documenting" is the ideal, and best practice is roxygen2 documenting of every function and such 3. actually being a good programmer - this can't be turned into a universal template, but there are some big ideas like try to break things into a bunch of functions, it's helpful to program things as packages to organize them, write tests as you develop and to prove it works (ie figure out what you want the function to do, write a test that will succeed when it does that), use version control, etc. I think Wickham's book on R packages and Advanced R will help you get some of those ideas.
&gt; They also require discipline and effort to maintain and it's tempting to just ignore the system. Isn't that always going to be the case? You have to put yourself in the proper mindset. If your code is sloppy ,then when you need to revisit it in 3 months, a year, whatever, you might have to rewrite it all anyway. If it's clean, you shorten that time spend deciphering your old code. Making sure you're writing clean code *now* saves you time and effort *then*. &gt; I think another source of my problem is that I am still learning and so often I'm doing something for the first time. I find that this is usually very buggy and that my way of learning by trial and error destroys any notion of clean code. Follow clean code practices while developing to the best of your ability. When you have it "figured out", go back and rewrite it cleanly, using just what you actually need.
Hello! Thank you so much for this guidance! I'm currently away from my office at the moment so won't be able to implement this code until Monday, but I'll let you know how I get on. To answer your question, with the updated experiment having a trial number column, I would only be wanting one row i.e the first row where SWA is above the threshold for each trial. Then using the timestamp, I would have the time for the steering action for each trial. However getting multiple rows for each trial may still be useful in the future so i will bare that in mind. Thank you once again for your help! I'll let you know how I get on.
I think a simple, clear, comprehensive template is too much to hope for. In some cases you really have to ask yourself "What would make this code most readable?" For a common example, suppose you have a big nasty sequence of nested function calls (and/or pipes) and you're deciding where to insert line breaks. A general rule might be "Put each function invocation and/or each argument on a separate line," perhaps making exceptions for sufficiently short lines or something. But really, you should think about what the code's doing. If there's a series of simple string-processing operations that add up to produce one function argument, and you kinda think of them as a unit, maybe they can all go on one line. If several sections serve a similar purpose maybe they should go on separate lines at the same level of indentation to emphasize the parallelism. Clean code is certainly extra effort and it's always tempting to cut corners; I'm sorry to say there's no way around that. For motivation, you have to remind yourself of the work it's saving you in the future, when you may have to read and understand and even debug your own code. (Or, when somebody else will pester you because they can't understand it.) Then, there's also the benefit of presenting a clean, professional image—for yourself, and also for your company if the code is ever shared with outsiders. Personally, I've also learned to take some personal satisfaction from my code quality, in terms of style as well as function: I really spend *too much* time playing around with whitespace. I don't know if that attitude is for everyone, but it's worth thinking about. As for buggy first attempts...code is like any other document, in that it benefits from revision and editing. Don't stop when you finally get the program working: give it another pass or three and clean it up. And I don't just mean the comments and variable names and such: I mean real refactoring too. Is there repetitive code that could be replaced with a function? Are there two functions that should really be special cases of a single function? Of course there are such things as deadlines and competing priorities. By the time you get the code working, you may not have time for this stuff, and supervisors don't always appreciate the value of spending that time. But you probably don't need to be told how to make compromises. 
I also struggle with this. R is pretty nonstandard as programming languages go, and most of what I write ends up being more what I'd consider "routines" than "programs" If you're using RStudio, I find that increasing the base indent from 2 to 4 spaces makes the code feel more organized and easier to scan. Though this itself violates Google's style guide, it's very easy to fix before sending it to someone else or committing to a repo. I don't understand the two space indent rule, maybe someone else can explain it. IMO, you shouldn't be afraid to use long variable names (within reason) as a method of making your code more readable and clear. Comments are nice, but readable code that doesn't need them is nicer.
Using periods is a very bad idea in R, as this causes considerable confusion when doing OOP.
Going off the google bandwagon. I currently only code my own "efficiency" scripts that make my work easier. (pre- and post- processing really) However with the exception of the '&lt;-' instead of '=' rule, I've been trying to use their guidelines in my code to make it more readable. Ultimately, I'm the only one that ever will be using them, so it may or may not have been worth my time, but only future more_momus will know. However, to help me out in being more organized (especially when I'm feeling lazy), I've been using the [styler](https://github.com/r-lib/styler) package. It will re-format the code you've written to adhere to the correct style. I really should be doing it on my own, but it does make things a bit easier at times. 
Use variable and function names so you don't need comments. If you want to write a comment above a section "# does thing" then instead pack it into a function called do_thing. If you can't separate the blocks out, you either have to make your code more "functional" style or object oriented. If you share code between scripts, move it into a separate file you source. If you share it between several scripts or ones that don't belong to a package, move them into a package. Use roxygen to document, but only the public part of the packages. The other stuff should be self explaining. If possible write tests, or have know output of scripts you can check when refining, it enables you to improve the structure more freely and quickly. Part of all of this is learning IDE shortcuts etc to make refactoring quicker. 
Search for a package called Styler. I guess it's in Tidyverse. It reads your code as text and outputs in pre-formatted structure or one you can custom. Give it a try. Is especially good for a team to have all scripts padronized (padronized is even a word in English? Lol).
https://style.tidyverse.org/
It looks like `file` hasn't been defined. There should be a line above this somewhere that says &amp;#x200B; `file &lt;- "NAME_OF_FILE.txt"`
“file” was just something generic I put in this post. In my code I used the actual file name as it appears in the directory. Does it make a difference if I assign the file to a variable first rather than reading directly from it? It worked before. 
Holy crap it worked. If you know why, could you explain why assigning the file to an object first matters?
Learning to code cleanly isn't an easy task. "Handy rules of thumb" tend to be overgeneralizations, which need proper understanding to apply in practice. I think that learning from your colleagues and reading some books about clean programming would be the best route. One of the Robert's C. Martin books helped me out, but, frankly, I forgot the title.
&gt; Holy crap it worked. If you know why, could you explain why assigning the file to an object first matters? This call will work regardless of whether you assign the string literal to a variable and pass it to the function or simply pass the string literal directly to the function. If you aren't sure of the filenames in your current working directory, run `list.files()`.
Shouldn't matter. My best guess is you forgot to put the filename in quotation marks. You wrote &gt;object 'file' not found indicating R tried to interpret "file"—or whatever you actually wrote—as the name of an object. That's what it would do if there were no quotes.
Thank you for that command. Alright, so I actually tried exactly that before with the exact same file and it didn’t work.
That might be it. Though I could swear I did that and it didn’t work. I’ll try it again then. Thanks!
I use this command all the time and, literally, I never pass it a variable. Instead, I always pass it a string literal. If you post the following information, I'll help you diagnose it. 1. A copy of your code for both cases. 1. The error output. 1. A copy of the file you're trying to read.
On further thought, it would also be interpreted as an object name if you replaced the quotes with backticks. Though it seems unlikely you would have done that by accident. The backticks let you use variable names that would otherwise be illegal, containing spaces or other forbidden symbols. Notably, [user-defined binary operators](https://www.oreilly.com/library/view/r-in-a/9781449358204/ch06s02.html) must begin and end with %, so you use the backticks to define them. Conversely, all binary operators (even the built-in ones) may be called as functions: `\`+\`(2,2)` returns 4.
# Example myFilename &lt;- "MyVars-table.txt" set.seed(9282) myvars &lt;- sample(as.factor(1:10), 50, replace = TRUE) write.table(myvars, myFilename) myvars1 &lt;- read.table("MyVars-table.txt") myvars2 &lt;- read.table(myFilename) cat("The files are", ifelse(identical(myvars1, myvars2), " ", " not "),"identical.\n", sep = "") 
Have you ever used a linter? There is a library for it. I believe it is called “lintr”. I’m on mobile or I would link you. 
That function is creating a new graphics device every time it is called. A graphics device just is some thing where you can make a plot appear. So, instead of plotting the new data from each participant in the same graphics device (i.e. overwriting the previous), each participant will essentially have it's own plot in memory I believe.
I am confused, why are you declaring df as a data.frame then immediately converting it to a tibble, why not just make it a tibble? Since tibbles never convert strings to factors in the first place and since you don't seem to be doing anything useful with the data frame except immediately converting to a tibble why not just only use tibbles?
This is the answer. For a good introduction on working with analysis and the tidyverse package check out: https://r4ds.had.co.nz
maybe try nesting your data and plots then using `walk()` to save them all at once? like in this blog post here: https://www.brodrigues.co/blog/2017-03-29-make-ggplot2-purrr/
I'll look into this and try it out tomorrow and let you know how it goes! Thanks!
Add the ggsave() function underneath your theme_classic line and use the last_plot() parameter to save the last plot you created. And to give each plot a unique name, try something like paste0(“*path where you want to save the file*/“, Participant, “_plot.png”). So all together, it would look a something like this `for (Participant in unique(TidyDatat$Participant)) { print(ggplot(TidyDatat[TidyDatat$Participant==Participant,], aes(Var 1, Var 2)) + geom_col() + labs(x=“Var1”, y=“SVar 2”) + ylim(0,5) + theme_classic()) ggsave(paste0(“C:/Desktop/“, Participant, “_plot.png”), plot = last_plot()) } `
Use the jpeg command instead of the print. jpeg (paste (filename,".jpg", sep="") ggplot(...) dev.off () You need to define filename for each plot 
I have zero experience with ggplot, I usually do plain plot() or xts plots. for(i in 1:nStrategies){ png(paste("density" , i , ".png")) plot(density(lReturns[[i]]) , main = paste0("Density estimation for\n the returns of strategy",i)) dev.off() } this is how I saved plots for 8 density estimates. I don't know if it will work with ggplot though.
In R Studio, there's inbuilt styling. Just select the code and CTRL+SHFT+A, then tweak it to taste.
The `linter` function is also exported by the "devtools" package.
Right, this was just one attempt of many at solving my problem. I was trying to force the stringsasfactors = false in the DF to see if it’d help me in the tibble. When I just use the tibble, I get the same error about it needing to be a char vector of length one. 
Can you give an example of your data and how it's defined, that shouldn't be happening.
Ill get ya the details/code later today — on mobile right now. 
I see that you have blocked out part of your data in your screenshot if your data has sensitive information in it just make up some dummy data. I'm more concerned with how that data was formatted .
Depends on the OLAP Cube, I guess. You should be able to connect to a TM1 Instance by using their REST-API.
This worked!!! I'm so excited!
Thank you! I used what /u/Rawsley suggested first and it worked, though.
Thank you! I used what /u/Rawsley suggested first and it worked, though.
No problem! Good you got it to work :)
Slightly different data set but same issue. https://imgur.com/a/ifGI9zS
This does not really help I would need to see what is going on in "convo.txt" again I don't need actual data but I would need some kind of dummy data to gain context.
Thanks for your help. Unfortunately, I’m not sure this will work for my very specific case. But this is still good to know for future reference? Have you connected to a TM1 instance before?
Ok, I’ll see what I can do. It’s just a copy paste of iMessages into a text file. So each line is the name of the sender and then the next line is the message. 
Hmmm does the olap cube live on top of a SQL db? Any chance you can get access to that?
We get the data from one of our partners. They only share it through an xmla feed. We plan on getting them to push the data into our data warehouse but that’s not the current situation. 
Before going through all that trouble try importing the data with the readr library first. That could be the source of your problems because readLines() is importing your data as a data.frame.
Ugh - I read it in using readr and the same error occurred at the unnest_tokens. Here is an excerpt of the real data and the code/error I'm producing: c&lt;- ("Me: Is it fun?? Everyone nice? Her: It's been a lot of fun...everyone's really nice Just hoping I sleep better Ashley had a room with 2 full beds to herself Last night that is Me: Ridiculous. Y'all should make her sleep on couch tonight? Makes sense Her: No kidding, right? Haha") c2 &lt;- as_tibble(c) unnest_tokens(c2, word, c2$value)
c&lt;- ("Me: Is it fun?? Everyone nice? Her: It's been a lot of fun...everyone's really nice Just hoping I sleep better Ashley had a room with 2 full beds to herself Last night that is Me: Ridiculous. Y'all should make her sleep on couch tonight? Makes sense Her: No kidding, right? Haha") c2 &lt;- as\_tibble(c) unnest\_tokens(c2, word, c2$value)
I am confused here this code won't even run. first I had to load tidyverse and tidytext libraries then I recieved an error Error in check_input(x) : Input must be a character vector of any length or a list of character vectors, each of which has a length of 1. This code won't even run.
Yeah, sorry, have to load those libraries first. The "Error in check\_input(x)..." is what I keep getting anytime I try to unnest\_tokens. 
r = recuperacoes$Movimemtacoes for (i in 1:length(r)){ if(any(grepl("&lt;CHR&gt;", r[i]$Detalhes))){ print(r[i]$id) print(r[i]$Detalhes) } }
I would convert the entire list to a data.frame with 6 columns: NumerodoProcesso, Detalhes, I'd, ... Then use dplyr filter and select to get the rows and columns that you need.
A single lapply() should work here. It would look like this: lapply(recuperacoes, function(x) { if (grepl(x[[2]]$Detalhes,"String")) return(list(Id = x[[2]]$Id, Detalhes = x[[2]]$Detalhes)) } )
lol I have been sitting here how to properly use unnest_tokens I don't know if its that I'm stupid or if the documentation on the unnest_tokens function is very poorly written because I am at a loss on how to use that function and the examples in the help file don't make sense at all.
On second thought, this isn't a bad use of a for loop. lapply returns a vector of the same length of your original list, so that would be filled with nulls. ret &lt;- list() for (i in recuperacoes) { if (grepl(x[[2]]$Detalhes,"String")) ret &lt;- c(ret,list(Id = x[[2]]$Id, Detalhes = x[[2]]$Detalhes)) } &amp;#x200B;
You can use a for loop, the actual operation is not that complicated, you just need to loop through the 2648 objects, check for criteria, and add to the list if it is met: ret &lt;- list() for (i in recuperacoes) { if (grepl(x[[2]]$Detalhes,"String")) { ret &lt;- c(ret,list(Id = x[[2]]$Id, Detalhes = x[[2]]$Detalhes)) } } &amp;#x200B;
Thank you for confirming I’m not crazy. I’ve followed a tutorial I saw somewhere, and it worked for that exact use case and data, but I haven’t been able to use it on my own data. Screw it. I’ll move onto another library. Appreciate the effort. 
Yeah that documentation is terrible. It may have been written by someone who isn't a native English speaker.
Here is one way: library(plyr) mydf &lt;- ldply(mylist, function(recuperacoes) recuperacoes$Movimentacoes[c("Id", "Detalhes")])) 
You can copy the olapR-package from a Microsoft distribution to your preferred distribution built on the same R-version. I have also used RDCOMClient for similar purposes. It’s a bit more work to fetch the rowsets, but you are not bound to an older R-version. I have used it for running MDX-queries against SSAS which I think would be a similar use case to yours. I remember struggling with finding documentation for the required ADO-functions. Let me know if I should dig for some examples.
No, I did not unfortunately. :(
purrr could work for you. You might need to adjust the .x in the article below, but this should get you there: https://community.rstudio.com/t/filtering-lists-using-purrr/12976/4
Glad to hear it! ggsave is just the built-in saving function that comes with ggplot2. It makes saving plots a lot easier compared with vanilla R. All we’re doing is telling R to save the last plot that has been outputted, which is your print() line, to the path we want. If you want to change the size of the plots or any of those kind of customisations, just type: `?ggsave()` into the console and you’ll get a help page with all the accepted parameters.
I wish I had thought of copying the package! I will work on that tomorrow and let you know how it works. I’ll also look into the documentation of RDCOMClient. Thank you!
Sounds like you want the [approx](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/approxfun.html) function
... or `predict`, or `spline` (read about Interpolation https://cran.r-project.org/web/views/NumericalMathematics.html). The `merge` function can be useful as well.
Hey I don't mean to hijack this persons' post but I'm doing a similar operation in R and this persons code looks perfect for my, but I don't understand what the unique() function is doing exactly, and I don't understand what this... `TidyDatat[TidyDatat$Participant==Participant,]` ... means in their ggplot. Could you explain it to me? 
Paying someone to do your Uni homework without crediting them is plagiarism. Install R, work through the syntax as ebst you can. Explain the situation to your tutor tomorrow. They'll likely offer additional help 
My rate is $60/ h. 
It's not as hard as you think it is. Just take a breath and use one of the billion guides on how to do this online. There are steps with pictures and arrows that you can follow. This is how you learn. Paying someone is cheating.
thank you. i'm following a guide now. 
How would your teacher feel about this post?
You are right. I'm watching her explain R right now. And watching another guide. Thanks. 
Many Thanks for the response. However, this seems not to work exactly as I want. What I want is to get all the objects that CONTAIN the string that I want, not only the ones that are exactly equal to that string. That is, I want to find all objects that contain the string "quadro geral de credores", not only those equal to that short string. Such it is a problem that I keep getting the problem: `Error in grepl(i[[2]]$Detalhes, "quadro de credores") :` `invalid 'pattern' argument`
Have you installed r studio?
I’m a fan of glue: https://cran.r-project.org/web/packages/glue/readme/README.html For multi line formatting you can use \n. 
sprintf with \n would be my go-to
#Code mylist &lt;- c( a = 45, b = "something", c = pi, d = Sys.time() ) for(i in 1:length(mylist)) { cat("This is ", names(mylist)[i], ": ", mylist[[i]], "\n", sep = "") } #Output This is a: 45 This is b: something This is c: 3.14159265358979 This is d: 1550459330.989
Could you do.it buddy
use `glue`. ``` library(glue) a = 45 b = "something" c = 0 glue(" This is a : {a} This is b : {b} This is c : {c} ") ```
God, even reading the function descriptions is helpful. Looks cool, thanks!
This is wonderful and beautiful in equal measure. &amp;#x200B;
This looks like a great idea!
Thanks for this. It's seems like a really sensible and reasonably reply to my question. I like your point about keeping similar ideas together, rather than functions or variables etc. Much more logical.
So it looks like op has got a dataset (TidyDatat) that holds data for a number of participants, with a column called “Participant” that holds a participant identifier of some sort. The unique() function returns each unique participant id, meaning that they’re looping over each participant id to make a plot for each participant. The line you’ve linked is where op is subsetting the dataset to limit the values to just the ones for the participant that is being iterated on in this particular loop. So basically, every time the loop executes, the Participant value will become equal to the next unique participant id, and then the line you’ve linked will subset the dataset to only create a plot from the data for the participant currently being looped over. I hope that makes sense.
Hey no problem! Happy to help. I have had work experience with folks who follow the style guide and folks who don't. I prefer the former every time!
This works perfectly! Thank you so much for your help!
/u/alguka After sleeping on this, I realized that I'd be remiss if I didn't show you the "R way" of accomplishing your goal. From an output perspective, the code shown below accomplishes, effectively, the same thing as the previous code I posted. However, there are two key differences. First, this code does not use a loop. Rather, it uses the `vapply()` function, which can be faster than a loop because the output is all of a single type. Second, it stores all of the output in a single vector, `vapp_results`, where all members are of the same type, which is `character` in this case. vapp_results &lt;- vapply(1:length(mylist), function(x) { cat("This is ", names(mylist)[x], ": ", mylist[[x]], "\n", sep = "") return(as.character(mylist[[x]])) }, FUN.VALUE = character(1)) names(vapp_results) &lt;- names(mylist) vapp_results I'm showing this result only because you'll see a lot of older guides to R programming telling you to avoid loops. In the early days of R, loops where incredibly inefficient compared to using the `apply()` family of functions, which are *vectorized*. Sometime within the last 5 - 8 years, the loop code in R was optimized, and now, in many cases, loops can be almost as fast (if not just as fast) as the vectorized functions. By studying this example, you'll get a good appreciation of *thinking in R*.
You're very welcome! :) 
I figured out your problem. Take a look at the details on the sample function ?sample, which should point you in the right direction. Another hint would be to print the size of balls in the while loop. It will sometimes mysteriously stay at a particular value. 
You're running into an issue with the specification of `sample`. When your first argument to sample is a vector like `1:60` it behaves the way you want it to behave. But when there's only one ball left, let's say `c(26)`, then `sample(26,1)` is interpreted as `sample(1:26, 1)`. So when you're getting to one ball left, you're drawing from a bunch of balls that have been thrown away already, not crossing anything off your chart, not making the pool smaller, but still counting it as a draw.
You forgot to reset "cart" after each loop of B.
Hope it helps! One thing I forgot, on the topic of "clean code takes effort": get yourself a good text editor or IDE like RStudio and learn to use it effectively. There are lots of little features there to make you more efficient. (Someone already pointed out RStudio's CTRL+SHIFT+A which is a nice example.) Make keyboard shortcuts for common code snippets, with the proper formatting already applied. Even simple things—say, a shortcut that generates an empty block comment—will regularly save you a few seconds of typing. Besides, laziness works in irrational ways. A few seconds' hassle might just be the hurdle that trips you up, whereas applying your power-user tricks can be a positive feeling that makes it fun to be good.
Why not make your [y-axis on a log scale?](https://ggplot2.tidyverse.org/reference/scale_continuous.html) You could either use `scale_y_log10()`, or specify a transformation inside of `scale_y_continuous`.
Thank you for your comment. Yes, I tried that but it introduces some values that are infinite and the graph is very weird. Is there a way I could remove those values? 
This is a fundamental challenge with trying to use a log scale on the y-axis with `geom_density`, if there are areas of density at or near 0. It's hard for people to help you without a reproducible example -- without your data it's unclear exactly what the issue is. If it's just at the margins, you might change your x-axis limits. If the density is zero in the middle, that's another issue. But fixating on densities around 0.001 or 0.01 suggests that you might be trying to emphasizing something in your data that is not worth emphasizing. Not sure if I can help more without clear, specific details about your data and why you want to visualize it in this way.
yeahh, thanks a lot guys, just corrected and is working fine.
Thanks maaan
There's a couple ways to do this. There's the base R /for loop version: CERCHID &lt;- c("01001", "01024", "01124", "01003", "01005") Gender &lt;- c(NA, 2, 2, 1, 1) Plasma.viral.load &lt;- c("3450", NA, "Und", "50", "552") Mutations &lt;- c("ND", "I50L", "M46I", NA, "Could not be amplified") base_df &lt;- data.frame(CERCHID, Gender, Plasma.viral.load, Mutations) column_list &lt;- colnames(base_df) for (column in column_list) { # Create name of new clumn new_column &lt;- paste0(column, ".x") # Create new column using ifelse() and is.na() functions base_df[,new_column] &lt;- ifelse(is.na(base_df[,column]), 0, 1) } And there's the dplyr/tidyverse solution: library(dplyr) tidy_df &lt;- data.frame(CERCHID, Gender, Plasma.viral.load, Mutations) tidy_df &lt;- tidy_df %&gt;% mutate_all(funs(`x` = ifelse(is.na(.), 0, 1))) Either one works, but the dplyr/tidyverse solution helps you avoid for loops. Hope this helps, let me know if you have any questions.
You can do it inside a `theme()` call: + theme(axis.text.x = element_text(angle = 45))
Looks like the theme component has an angle parameter that can be adjusted https://stackoverflow.com/questions/1330989/rotating-and-spacing-axis-labels-in-ggplot2/1331400#1331400
Thanks! I tried this before but I kept putting it inside theme\_classic()! Now that I add it on a new line, it works, but it causes another problem. I've attached an image to the OP, but is there a way to now move those labels down? 
You can add a call to adjust the vertical justification, giving `vjust` a value between 0 (lowest position) and 1 (highest). + theme(axis.text.x = element_text(angle = 45, vjust = 0)) However, I and many others might caution against angled axis ticks if possible. They are annoying to read. Another option is to make the text slightly smaller (hint: you can specify a `size` parameter inside of `element_text()` as well).
&gt;I and many others might caution against angled axis ticks if possible. They are annoying to read. Agreed, it's that I need to fit three of these images into a qualtrics survey side-by-side, so the images are already quite small and I'm concerned that reducing the font size on the x-axis ticks would be even more hard to read. It's a tricky position
Yep, horizontal definitely better if possible. Angle might be better for this application, though, if OP is already shrinking it to fit somewhere small and has more vertical than horizontal space to work with. Text might end up too small. Fine balance between size / running together, angle, and complexity (alternating offsets worse than angles on this end, I think).
Lot of different methods. [`cowplot`](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html) offers one of the easiest methods with `plot_grid()`
The [patchwork package](https://github.com/thomasp85/patchwork) is the best way to do this. It lets you use the `+` operator to chain plots together into a single image. p1 &lt;- ggplot(data = d1, aes(x = blah1, y = blah2)) + other_stuff() p2 &lt;- ggplot(data = d2, aes(x = blah1, y = blah2)) + other_stuff() p3 &lt;- ggplot(data = d3, aes(x = blah1, y = blah2)) + other_stuff() p1 + p2 + p3 + plot_layout(ncol = 1)
This is super helpful! Thanks!
Interesting. I'm not sure how I could apply this to my code. I didn't put this in the OP, but I'm actually making \~250 graphs for each variable. So, in the context of your example, I would 250 p1's, 250 p2's, &amp; 250 p3's. I've so far done this using the code below (which is analagous to just making the 250 p1's). I'm wondering if there is a way to make the p1's, p2's, and p3's all within the same loop, and then end the loop with stacked\_plots() such that I'd have a stack for each participants' p1, p2, and p3. Does that make sense? `for (Participant in unique(TidyPolicyEmotionsTest$Participant)) {` `print(ggplot(TidyPolicyEmotionsTest[TidyPolicyEmotionsTest$Participant==Participant,], aes(Emotions, Ratings)) +` `geom_hline(yintercept = 1:5) + #adds lines at each reference point` `geom_col(fill="steelblue") +` `labs(x="Emotions", y="Strength of Emotion") +` `ylim(0,5) +` `theme_classic() +` `theme(axis.text.x = element_text(angle = 45, hjust = 1)))` `ggsave(paste0("Dropbox/Projects/Folk Moral Psychology/Data Analysis/Target Study 1A/Plots/PolicyEmotions/PolicyEmotions_", Participant, "_test.png"), plot = last_plot())` `}` &amp;#x200B;
Coming back to this. Although I have not tested it out yet. I think the electron solution will be the best. &amp;#x200B; Hopefully I can figure it out myself now. &amp;#x200B; Thank you for the lead. 
It looks like your data already has the label in a column, which is good. So you want [facet_wrap_paginate](https://cran.r-project.org/web/packages/ggforce/vignettes/Visual_Guide.html#pagination) from the [ggforce package](https://github.com/thomasp85/ggforce). Read the instructions on the `n_pages` function to figure out how many pages you need. Suppose it's 10. Then write the following loop. Make sure you have the `glue` package installed to make use the glue function. for (i in seq_len(10)) { plot_to_save &lt;- your_plot + facet_wrap_paginate(~Participant, ncol = 3, page = i) ggsave(glue("output_plot_{i}.png"), plot_to_save), other_ggsave_stuff) }
You may also want to add `scale_y_continuous(expand = c(0, 0))` to avoid the (IMO) ugly space at the bottom. I would also make the `geom_hline` dark gray or something, as to not stand out as much.
Problem is probably one version using the library directory of the other? What happens if you run .libPaths() in each version - are they both pointing at the same directory for one of their paths?
Alternatively: + theme(axis.text.x = element_text(angle = 45, hjust = 1))
you are storing the kable result in `df` but never printing it. Note that `df` is the name of a function in base R... re-using this name can lead to unpleasant surprises later.
Is there a course for a complete beginner?
So I should call print(kable(df1)), where df1 is the new name of my tibble?
you don't need print. just say kable(df) 
Thank you I'll try that, I was under the impression that the kable function also would print it. I guess I was wrong.
Wait didn't I do kable(df1) when I did df1 %&gt;% kable 
You never did `df1 %&gt;% kable`. Look at the top level comment - in R when you store the result in a variable (e.g. `df &lt;- tibble...`) it wont display the result. --- title: "tabletest" author: "tabletest" date: "2/17/2019" output: html_document: default --- ## Plots ```{r printtable, echo=F, results='asis', warning=FALSE, message=F, include=T} library (tidyverse) library(lubridate) library(knitr) library(rmarkdown) library(pander) dates &lt;-seq.Date(from=ymd("2017-01-01"), to=ymd("2017-12-31"), by="day") counts &lt;- (length(dates)) someotherstuff &lt;- rep.int("test", 365) df &lt;- tibble(dates = dates, counts = counts, someotherstuff = someotherstuff) df %&gt;% kable() # or just kable(df) ```
Ok that must be my issue then. I don't think I ever realized this.
&gt; (Years on the x axis, the frequencies for received and closed grouped together side by side for each year). Then your data isn't set up correctly. rec_cloYear &lt;- c(1:19) Year &lt;- c(1999:2017) Received &lt;- c(13,6,0,15,25,836, 11596, 23272, 24006, 21685, 20807, 17294, 16152, 15397, 14563, 13529, 12503, 10601, 90) Closed &lt;- c(0,0,0,0,0,0,0,0,21444, 24267, 21047, 19517, 16823, 11180, 18708, 14970, 15682, 13463, 1151) data &lt;- data.frame(rec_cloYear, Year, Received, Closed) #Rename to set up reshape data &lt;- plyr::rename(data, c("Received"="freq_Received")) data &lt;- plyr::rename(data, c("Closed"="freq_Closed")) #Reshape datalong &lt;- reshape(data, direction='long', varying=c('freq_Received', 'freq_Closed'), timevar='Type', times=c('Received', 'Closed'), v.names=c('freq'), idvar='Year') ggplot2::ggplot(data=datalong, aes(x=Year, y=freq, group=Type))+ geom_bar(stat="identity", position="dodge", colour="black", aes(fill=Type))
if you just wish to share the html output file, why don't you just upload the output file to a webserver? &amp;#x200B; if it doesn't have any shiny/runtime dependency you don't need to have a machine running r. any *dumb* webhost will do.
I should have mentioned that I work for my countries federal government and it is a report about sensitive information which cannot be hosted on a server. 
Then to flip this question around, why is sharing an RMD file different than sharing ANY file? It is just an html file. How do you share any documents where you work?? If they can't be hosted on a web server do you send them by email? Note that you could also host these files to a local network that is not open to the rest of the world.
I have tried to share the html file but it does not look like it should when opened. The formatting is all over the place. 
Could you use [Sphinx](https://www.sphinx-doc.org/en/master/usage/markdown.html) to convert the markdown to html and host the page on a [github/gitlab ci/cd runner](https://docs.gitlab.com/runner/)? By default these will be exposed to the network, but I think there is an option [github](http://kinlane.com/2013/10/15/securing-site-that-runs-on-github-pages-with-json-backend-in-private-repository/)/[gitlab](https://gitlab.com/gitlab-org/gitlab-pages/merge_requests/94) to control access.
Go back to this, it is 99% likely this is what you need. By default, rstudio should create a self-contained html which includes inline css and images. Try going back to it and either googling or writing here the specific problems you're having. If you really can't get it to work, would it be a problem to knit it to pdf instead? Those are usually slightly more portable.
Okay I will keep trying that. No pdf won't work as there is a lot of interactivity within it
How are you implementing that interactivity? Javascript of some kind I assume. Is this javascript loaded from a website? from the project folder? Try looking into that. For example, view the html on your local computer and go to view source mode (ctrl+u in most browsers). See if you can find any javascript which is referencing an external file. maybe all you need is just zip your html with that file so that when the user unpacks the zip, they will have the js you need.
Great tips! I think I'm decent with R studio shortcuts, but it sounds like there's a lot more to discover. Cheers!
 split(df, df$year) 
I would go also say that you should embed any CDN delivered JS and local JS files within the output. 
 `ggplot(tp2log, aes(x=x-variable, y=y-variable)) + geom_point(size=2) + geom_smooth(method=lm, size=2) + geom_rug(col="darkred", size=0.9, sides="topright", alpha = 1/2) + geom_point()` &amp;#x200B; The problem is that I need to find some way to automatically fix this so that when I make the next plots (100s of plots) they will all be fixed. I was thinking that manually adding extra space to this plot may fix just this plot (and not everything else since the scale will be different for different plots). Any help appreciated!
Where are the points cut off? Unclear what your problem is. Are you aware that log(0) does not exist? If you have any zeros in your variables those points will automatically be dropped at the transformation stage.
Is there a reason you're altering all of the sizes? Does leaving the default size fix your problem? Might not be the answer you want, but without more detail on which points are being cut off, that's the best I can say. Also, the other comment about log(0) might be relevant.
When I make the exact plot on the non-transformed data, everything is fine. However - most of the points are bunched up very tightly in one area. to get around this, I've seen people take the Log(2) values the x and y variables, then plot them. The correlation coefficient of course doesn't change, but now the distribution is much 'prettier' and easier to look at. &amp;#x200B; I plotted this transformed data however and now the points that are at the axis perimeters are cut in half. Do you see on the y-axis there are 3 points cut in half, and there is 1 on the x-axis. I should have more clearly labeled these points ; they're what I'm trying to fix.
sorry - I clarified in my response to /u/Peter-Cottontail below 
Okay I think I have a potential fix. See the `lims` and `expand_limits` commands in ggplot2. Something like `+ expand_limits(x = c(min(x-variable) - 1, max(x-variable) + 1), y = c(min(y-variable) - 1, max(y-variable) + 1))` might do the trick. Also, any reason why you have `geom_point` twice in your object?
&gt; I've seen people take the Log(2) values the x and y variables, then plot them ... the distribution is much 'prettier' and easier to look at. If you want a plot that's easier to visualize, you might alternatively consider changing the points to being empty shapes rather than solid, or adjusting the color so that they're not fully opaque (I think it's controlled by the parameter `alpha`). In both of these cases, you will be able to better see points that are getting stacked on top of each other. &gt; The correlation coefficient of course doesn't change Taking the log of both variables most certainly DOES change the correlation. nn &lt;- 100 set.seed( 42 ) xx &lt;- rexp( nn, 0.25 ) + 20 ee &lt;- rnorm( nn, 0, 2 ) yy &lt;- 1 + 0.5*xx + ee cor(yy,xx) cor( log(yy), log(xx) ) layout( matrix(c(1,2), nrow=1) ) plot( yy ~ xx ) abline( lm(yy~xx), col=2, lty=2 ) text( 30, 25, labels=paste0("R2 = ", round(cor(yy,xx),3) ) ) plot( log(yy) ~ log(xx) ) abline( lm( log(yy) ~ log(xx) ), col=2, lty=2 ) text( 3.8, 2.25, labels=paste0("R2 = ", round(cor( log(yy), log(xx) ),3) ) ) 
Hmmm....okay I didn't know about that function. Thanks for the example. For some reason I'm getting this : "Error: Discrete value supplied to continuous scale" when I use your code. Any idea what I could do to fix that? I tried putting the variables in quotes (even though they are not in quotes at the beginning of the code) and got this : "Error in min("CYP2A6") - 1 : non-numeric argument to binary operator" &amp;#x200B; Also, thanks for catching that geom\_point is in there twice. I've removed it. &amp;#x200B;
You are trying to pass whole `data.frame` to the function and hope it knows what to do. Instead you need to specify all the parameters one by one: barplot(rbind(rec_cloYear$Received, rec_cloYear$Closed), beside=TRUE, names=rec_cloYear$Year, las=2)
Yeah I just whipped that up on the fly based on previous work with the functions. The ultimate issue you're having is related to the plot limits. Dig around the documentation for lims and expand_limits and I'm sure you will work something out. You might need to get creative when it comes to creating a reproducible method (ie not manually setting the limits for every plot). Ggplot and other tidyverse packages don't play super well with programming; they are intended for exploratory analysis. 
Very interesting! I'm quite confused abut the regression coefficient changing though. I checked an online tool that also calculates correlation coefficients and makes plots very similar to what I'm trying to acheive. I notice that changing the scale to Log changes the Pearson coefficient but not the Spearman. My data is all Spearman correlations. Is it now acceptable to say Spearman correlation coefficients are unchanged even though I've transformed to Log scale? Regarding the visualization : I've been able to use transparent colors to better show the density, but it is the *spread* of the data I'm trying to correct for. Here's an online tool that accomplishes this by looking at gene expression - can you tell me if this is acceptable? : [Left side is non-transformed, right side is log transformed](https://i.imgur.com/EDsWk1M.png) Thanks for your input!
&gt; "Error in min("x-variable") - 1 : non-numeric argument to binary operator" You might need to calculate the bounds of your x and y variables in advance of making the ggplot. I think that bvdzag was using `x-variable` and `y-variable` as stand-ins, not as the actual syntax.
&gt; I notice that changing the scale to Log changes the Pearson coefficient but not the Spearman. Ahh, the Spearman correlation is the correlation on ranks. Since the log-transformation won't change the *order* of points, then the Spearman correlation shouldn't get changed. &gt; I've been able to use transparent colors to better show the density, but it is the spread of the data I'm trying to correct for. Which I think is perfectly fine. I wasn't sure of all of the motivations of taking the log-transform. Sometimes I've used it or seen it used because points were too bunched up to really "see". In case that was the motivation to take the transformation, I was just mentioning another potential way to adjust the figure to picture the distribution.
Okay great, thanks a lot!
OP I don’t know this will certainly solve your problem but from what I read it seems like you need R Notebook. You have some interactivity in the document and the output created by R Notebook in the form *.nb.html will be able to retain that. But it also means that it carries with it your data so it’s less safe than just sending simple html file. 
I'm not sure what's happening exactly in your code, since I don't have your data. My best guess is that the second `geom_point()` is throwing you off - you are plotting the same points again, at size 1, and so the final plot margins are being set by that plot and thus cutting off bits of your size 2 points. In any case, I suspect it would work better for you in general to use `scale_*_continuous` functions instead of manually transforming all the data. For example, `ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) + scale_x_continuous(trans = "log") + scale_y_continuous(trans = "log") + geom_point(size=2) + geom_smooth(method=lm, size=2) + geom_rug(col="darkred", size=0.9, sides="topright", alpha = 1/2) `
I will look into this! thanks
Is basilica open source?
The package is open source. The code that runs the API is not, but we're considering open-sourcing the models.
It's probably a rounding thing. The value reported when fn is printed is rounded version of the actual value. R can handle a certain amount of decimal places but at a certain point, it's all equal. fn1 &lt;- c(0.2000000000000001, 0.19999999999999999, 0.2, 0.2, 0.2, 0.2) fn1 &gt; [1] 0.2 0.2 0.2 0.2 0.2 0.2 fn1 == 0.2 &gt; [1] FALSE FALSE TRUE TRUE TRUE TRUE fn2 &lt;- c(0.20000000000000001, 0.199999999999999999, 0.2, 0.2, 0.2, 0.2) fn2 &gt; [1] 0.2 0.2 0.2 0.2 0.2 0.2 fn2 == 0.2 &gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE identical(0.2000000000000001,0.2) &gt; [1] FALSE identical(0.20000000000000001,0.2) &gt; [1] TRUE &amp;#x200B;
When I try to use the parse_date_time function I get an error due to not having a default "orders" set. Is there a way to do this that I am missing?
I don't know much about NLP. Can you ELI5 how you arrived at 512 features per sentence? Are most of those values *NULL* for such short sentences? Also, for images, are your algorithms translation invariant? TIA! 
Why do people think this is specific to R, for God's sake? Go back to Python as perform the same calculation sequence that lead to the vector you have in R and you will obtain the same behavior. This is called floating point math, and it has been this way since it was invented. There are many many discussions of this problem out there... consider https://randomascii.wordpress.com/2013/07/16/floating-point-determinism/ as one place to start.
Don't try to get the data frames back into the global environment. Either it makes sense to have them in a list and reference them from there. Or it is cleaner to just define the function and call it on each data frame separately. If you really do need to, my guess is that you have to name the list elements with the names of the data frames, so that list2env actually knows how the data frame variables should be called. But again, I advise against using list2env or similar functions. Regarding the other part, idk, but I never used the viewer but something like column\_names(dataframe) to figure stuff like that out.
Yes! This is an introductory course which help you get on GIS in R.
the dplyr solution worked perfectly...i try to stay away from loops a lot but never had the opportunity to utilize any of the other mutate functions! Thank you!
I think you are looking for the 'cbind' function. This just combines the two vectors to a data.frame with two columns. 'bind_cols' is the same function in the dplyr package.
I'm very interested to play around with this! 
Please confirm if I understand correctly: For every i, either class1[i] is NA and class2[i] is numeric, or vice versa. You want to create a new vector that includes the numeric variables and not the NAs. If so, give this a try: `class1_and_2 &lt;- cbind(class1, class2)` `rowSums(class1_and_2, na.rm=TRUE)`
&gt; It's probably a rounding thing. Indeed. Are the rounding rules documented somewhere? I haven't been able to find them. More importantly, how do I print the results in full without rounding? 
Pretty sure you're looking for `unite() from the `tidyr package. That will allow you to combine two columns into one.
&gt; Why do people think this is specific to R, for God's sake? Because it *is* specific to R. None of Python, Ruby, Javascript or Lua (to pick just a handful of languages) default to printing distinct and unequal numbers as if they were equal. Here's R, displaying three unequal and distinct floats as if they were equal: &gt; c(0.199999999, 0.2, 0.20000001) [1] 0.2 0.2 0.2 Now Python, Javascript and Ruby: py&gt; [0.199999999, 0.2, 0.20000001] [0.19999999, 0.2, 0.2000001] js&gt; [0.199999999, 0.2, 0.20000001] 0.199999999,0.2,0.20000001 irb(main):001:0&gt; [0.199999999, 0.2, 0.20000001] =&gt; [0.199999999, 0.2, 0.20000001] And Lua: &gt; values = {0.199999999, 0.2, 0.20000001} &gt; for i, v in ipairs(values) do print(i, v) end 1 0.199999999 2 0.2 3 0.20000001 &gt; Go back to Python as perform the same calculation sequence that lead to the vector you have in R and you will obtain the same behavior. Do try to *read the question* before firing off a patronizing, angry, knee-jerk response that doesn't even come close to being relevant. This has nothing to do with floating point determinism, it is entirely about the way R displays the floats when printing them in the interactive interpreter. If you don't believe me, maybe you'll believe /u/and-another-one who gave [a much more helpful answer](https://old.reddit.com/r/Rlanguage/comments/asif4x/beginner_why_are_these_values_different/egum0ue/): "The value reported when fn is printed is rounded version of the actual value." I'm not asking why the values are different. I'm asking why they are displayed as equal when they are different, and how I can see the values using full precision. 
It sounds like your data set is in a strange format - normally, columns correspond to variables, and rows to observations. Could you describe your data set?
All floating point display mechanisms have to round at some point, so whether they look the same at 2 sig figs or 12, you cannot go around being surprised when floating point comparisons fail in any language even when they look the same. R has display options that control this, and for context-specific output (e.g. generating HTML tables) many functions will allow you to specify similar options. (https://stat.ethz.ch/R-manual/R-devel/library/base/html/options.html). In many cases (but not in the default console output) the formatting practice is organized around significant figures rather than fixed format, but either approach will hide some differences. None of the other languages you mention use vectors as fundamental objects. The default display for vectors in R is designed to provide some consistency in display in a way that limits the distraction of negligible magnitudes in numerical analysis. As a troubleshooting practice I often use the dput function as a quick way to obtain a maximal precision representation for review or transfer to other calculating tools, but even this technique has its representational limits.
Maybe c(table$col1, table$col2) is working. I did not test it. 
Give it a try and let know how it goes!
The reason there are 512 features for sentences (there are 2048 for images) is just that that's how many coefficients there are in the last layer of our neural network. This has to do more with deep learning than with NLP. We could have added more or fewer features if that would have explained more variance, but 512 is enough. Values will never be null for short sentences. Even for an empty string, you'd get no null values. For images, the embeddings are not translation invariant, but they should work really well for object detection in images. If you look at our blog post on writing a classifier for cats and dogs (in Python) you'll see that you can get pretty good results with this: [https://www.basilica.ai/tutorials/how-to-train-an-image-model/](https://www.basilica.ai/tutorials/how-to-train-an-image-model/) &amp;#x200B;
Have you tried any of the join options? Left_join, etc.? It's hard to help without an example.
this worked perfectly, thank you so much!
thank you, i used cbind and it worked perfectly!
thanks for your help- will remember the function in future !
thank you, i’ll remember this one for future reference !
thanks for your help- i just used cbind in the end. sorry i couldn’t provide a better example, still new to R so i’m not great at explaining myself !
https://www.reddit.com/r/learnrstats/comments/98ddij/lesson_5_simple_linear_models_and_working/?st=JSDKHC3N&amp;sh=5027009d
Try this # dummy data dfA &lt;- mtcars[0:10,] %&gt;% mutate(vartobechanged = sample(c(0,1), nrow(.), replace = TRUE)) # random 0s and 1s dfB &lt;- iris[0:10,] %&gt;% mutate(vartobechanged = sample(c(0,1), nrow(.), replace = TRUE)) # random 0s and 1s dfC &lt;- trees[0:10,] %&gt;% mutate(vartobechanged = sample(c(0,1), nrow(.), replace = TRUE)) # random 0s and 1s # dummy data as list list_of_df &lt;- list(dfA = dfA, dfB = dfB, dfC = dfC) # apply to transform vartobechanged to factor list_of_df &lt;- lapply(list_of_df, function(x) x %&gt;% mutate(vartobechanged = factor(vartobechanged, levels = c(0,1), labels = c("label1", "label2"))))
If I'm not mistaken, the script does not work because "dependent" is not a column in "table", it is a separate vector. Either you need to use the actual name of the last column of your table, or you need to rename the last column of your table. You could use names(table) or head(table) to look up the names of your columns. Then delete the 3rd step and change "dependent" in the call to lm to the actual name of the column. Alternatively you can rename the last column to "dependent": colnames(table[ncol(table)])="dependent" and then doublecheck with "head" that you renamed the right column. Then delete the 3rd step and it should work.
I got 16 using dtm[,grepl("ad$", dtm$dimnames$Terms)]$dimnames$Terms instead of the inspect function
Is it absolutely necessary to use the inspect function? If you just need the words, you can view them using dtm$dimnames$Terms[grepl("ad$", dtm$dimnames$Terms)] or dtm[,grepl("ad$", dtm$dimnames$Terms)]$dimnames$Terms
Thanks for the reply and the link. I'm interested to learn more.
Def DM me if I can help you out in any way or just to lmk how it goes
Yeah, I tried that, but it says the variable lengths differ. args &lt;- commandArgs(trailingOnly = TRUE) table = read.table(args[1],header=TRUE) dependent = rev(names(table))[1] lm(dependent ~ ., data = table) 
That doesn't mean anything to me, but thanks... I guess. 
&gt;table[ncol(table)] I'm more familiar with data.frames than tables, but don't you need a comma to specify that you're trying to pull the column instead of a row?
Seems to give the same incorrect results either way, but I'll keep the syntax in mind for the future, thanks.
 &gt; test = data.frame(c(1, 2, 3, 4, 5, 6, 7, 8), c(1, 2, 3, 4, 5, 6, 7, 8), c(1, 2, 3, 4, 5, 6, 7, 8), c(1, 2, 3, 4, 5, 6, 7, 8)) &gt; unlist(test[,ncol(test)]) [1] 1 2 3 4 5 6 7 8 &gt; ncol(test) [1] 4 &gt; unlist(test[ncol(test)]) c.1..2..3..4..5..6..7..8..31 c.1..2..3..4..5..6..7..8..32 c.1..2..3..4..5..6..7..8..33 c.1..2..3..4..5..6..7..8..34 c.1..2..3..4..5..6..7..8..35 1 2 3 4 5 c.1..2..3..4..5..6..7..8..36 c.1..2..3..4..5..6..7..8..37 c.1..2..3..4..5..6..7..8..38 6 7 8 
How about something like this? args = commandArgs(trailingOnly = TRUE) table = read.table(args[1],header=TRUE) dependent = colnames(table)[ncol(table)] formula = as.formula(paste(dependent, '~ .')) lm(formula, data = table) You just need some way to grab the actual name of the last column. This just pulls the name of it out of colnames(), and then stitches together a formula to give to the regression model.
How about something like this? args = commandArgs(trailingOnly = TRUE) table = read.table(args[1],header=TRUE) dependent = colnames(table)[ncol(table)] formula = as.formula(paste(dependent, '~ .')) lm(formula, data = table) You just need some way to grab the actual name of the last column. This just pulls the name of it out of colnames(), and then stitches together a formula to give to the regression model.
Yeah, I think the problem is I need to select the column name programmatically? I'm not sure, but I get: Coefficients: (Intercept) a Percent_a Percent_b -6.693e-14 5.530e-18 2.080e-15 1.000e+00 Where a is 400 to 6000 and neither percent is in decimal form (i.e. both bound 0 to 100). If I use those as linear coefficients I always get numbers way way below anything they should be given the bounds of my data. Am I just misusing the coefficients? 
Oh! I tried to use ``quote(dependent)`` but it kept outputting dependent instead of the name. Thanks a ton, I really appreciate it. 
I want the frequency of them just like the output, I should have said it in the post. Mb. Thanks for the reply!
I would never dump data frames out of a list into my global environment, so cannot help there. Regarding viewing column numbers, that would be a feature request to RStudio. However, you are building a fragile analysis process as soon as you spend time doing this yourself anyway. Either the file format has consistent column positions and the same numbers should always work, or the column names don't change and you can reference the columns using a string vector. I will also say that data frames are lists of columns...they are not matrices. The advantage of using data frames is that each column can have a different type. The disadvantage is that working with sets of columns that are in fact all the same type tends to be a little less efficient. Finally, making changes to a data frame the way you are doing it in blocks is liable to lead to surprises: colstochange &lt;- c("Col1","Col2") xxx &lt;- data.frame( Idx = 1:5, Col1=c(1:4,-5), Col2=c(1:3,-4,5) xxx[ xxx[ , colstochange ] &lt; 0, colstochange ] ## Col1 Col2 ## 5 -5 5 ## NA NA NA which is because `xxx[ , colstochange ] &lt; 0` is a logical matrix but there is no special treatment for indexing data frames with logical matrices (there is for numeric matrices as indexes... read the Introduction to R document that comes with R) but matrices are really vectors with dimensions so R treats the logical 5x2 matrix as a length 10 logical vector with `TRUE` values in the 5th and 9th elements. Since there are only 5 rows in `xxx` the 9th `TRUE` yields an `NA`. You can do this using `apply` as xxx[ , colstochange ] &lt;- apply( xxx[ , colstochange ], 2, function(v) { v[v&lt;0] &lt;- NA; v } ) and you can also do it with the `dplyr` package: mutate_at( xxx, vars(starts_with("Col")), funs(ifelse(.&lt;0,NA,.)))
 nms &lt;- names(table) frm &lt;- as.formula( paste( nms[ length(nms) ], "~." ) ) lm( frm, data=frame )
But `cbind` makes a matrix of two columns of you give it two vectors... not a single column. The `paste` function seems like one good way though I don't see the value in converting to numeric for this kind of data.
[Check out appendix G for info on floating point arithmetic](https://link.springer.com/content/pdf/bbm%3A978-1-4939-2122-5%2F1.pdf). You can print more significant digits with sprintf, though this gets imprecise as well. Compare sprintf("%.17f",fn1) and sprintf("%.15f",fn1).
Shit... and here I’ve been doing mutate(paste0(col1, col2)) all this time...
When you simply print an object like that, print.default is called. Check out ?print.default. It looks like when the digits argument is NULL, it will default to getOption("digits") which is a global option. &amp;#x200B;
That's very kind of you. I will. 
Can you elaborate on your first sentence- specifically why you never would?
The similarity of the data frames that allows them to be read into a single list should also support using `lapply` to analyse them as well, yet you lose that easy access to all the data when you dump them all out into the environment with all the other objects that don't look like that. If there is one particular example that you want to look at you can simply reference it within the list using double brackets.
Change “elpe” to “else”.
The classic IF/ELPE statement 
ELPE, I need somebody ELPE, not just anybody ELPE, you know I need someone, ELPE When I was younger, so much younger than today I never needed anybody's ELPE in any way But now these days are gone, I'm not so self assured Now I find I've changed my mind and opened up the doors ELPE me if you can, I'm feeling down And I do appreciate you being round ELPE me, get my feet back on the ground Won't you please, please ELPE me
Sorry, that’s a typo. But I still receive the same error after changing it
Can you explain how you are using apply here and why you are passing it an argument called "drop?" 
So I tried to apply the function “wei” to each “p”i”table”, say p2table. But then I got the error as stated in the question. So I search up online and it says I got the error because my data is not a matrix; so I tried “as.matrix” but it states I can’t mix positive and negative values. Alternatively, I can use “drop=FALSE” to remain the data as matrix, so that I put drop. Thanks you!
Could you give us a description of the high level thing you're trying to achieve? For instance: I want to change values in a column based on... 
I think "p1table" on line three should be in quotes, since it's not defined. 
I believe this is the problem: In apply(paste(“p”, i, “table”), drop = FALSE, 2, wei) note that paste creates a character object. For example, let’s consider the case when i = 2. Then, paste(“p”, i, “table”) will output “p2table.” So, in the apply function, we will have apply(“p2table”, drop = FALSE, 2, wei). “p2table” is a character vector of length one. I am guessing the behavior you desire is for R to treat “p2table” as the name of some object that already exists in your workspace so that R will then apply the function wei to that object by the columns. For that behavior, you will need to use as.name(). Additionally, and I’m not in front of my computer to play around with this, but you might have to wrap as.name inside of eval(). However I’m not 100% sure on this. So, with this you would have apply(eval(as.name(paste(“p”, i, “table”))), drop = FALSE, 2, wei) 
Then on line 5 you're applying apply to a string. Not sure if that will work, but that can't be what you want. Apply is for two dimensional things. 
Then after the elpe, you're assigning things to a string. You can't do that. You can't change the string. I think you want to use the assign function (or is it "set"?) However this is almost never the way you want to do things.... But it's a good learning experience. 
&gt; I often use the dput function as a quick way to obtain a maximal precision representation for review Thanks for the suggestion, but it doesn't seem to work for me: &gt; fn[1] == fn[5] [1] FALSE &gt; dput(fn[1]) 0.2 &gt; dput(fn[5]) 0.2 &gt; All floating point display mechanisms have to round at some point [...] &gt; you cannot go around being surprised when floating point comparisons fail in any language even when they look the same Both of those statements are incorrect. Although, technically, in the first case you may have to print a *lot* of digits in the first case to avoid any rounding at all. But in a more practical sense, there is never any technical reason for unequal floating point numbers to display as equal unless you choose to artificially limit the number of decimal places shown. For single-width floats, 9 digits is sufficient to guarantee that different floats look different (and also guarantee that they will correctly round-trip from binary to text and back again); for double-width floats, you need 17 digits. (You can often, but not always, get away with a lot fewer than that.) Perhaps you ought to read [Bruce Dawson's blog](https://randomascii.wordpress.com/category/floating-point/), starting [here](https://randomascii.wordpress.com/2012/02/11/they-sure-look-equal/). And if you're tempted to tell me off for testing floats with equals, please don't until you have read *all* of Dawson's float posts. 
As /u/standard_error said, columns are variables and rows are observations. You can subset a data frame in a few ways, using the variable name or position. If you use bracket notation, note that subsetting is \[row, column\]. So if you call df\[1,\] you get the first row of data and df\[,1\] gives you the first column of data. Example with mtcars: mtcars &lt;- mtcars colnames(mtcars) mtcars[,1] # first column mtcars[,"mpg"] # column named mpg mtcars$mpg # named extraction without quotes Those last three all give you just the mpg column.
I’m done, Jesus Christ that’s hilarious. 
You need to learn how vectors and lists work. None of your `paste(…)` code does what you think it does. You are concatenating *strings*, not accessing variables of different names. Instead of having variables of the form `p2table` etc, use a list `ptable` and index it with your `i` variable. This is covered in any good introduction to R, and you need to work through one of them — I recommend [*R for Data Science*](https://r4ds.had.co.nz/) by Grolemund and Wickham.
Thanks, that's useful.
Thanks, that's helpful.
Do you still get the error if you use: quantile(as.numeric(x), probs=c(.25, .75), na.rm = TRUE) 
Look up what na.rm does. It removes NA values. In some functions you can specify what's considered NA. Apparently here NA and NaN get removed from your vector, before computing the quantiles from it.
Some R functions throw error when the variable name is identical to the function parameter names. It creates some issue with parsing the parameters. Try naming your na.rm variable to na_rm and set na.rm=na_rm. It should work. 
I think I finally got it. See below.
df %&gt;% mutate(Billing = ifelse(Billing == 0, 6, Billing) for #1. &amp;#x200B;
It sounds like you might be better off doing a discriminant analysis with age predicting the billing term. (Billing term sounds like a synonym for "payment plan", which is categorical even though billing term looks like it should be treated as ordinal.) Before you do that, a histogram (or box plot) of age (Y) versus billing term (X) will give you a rough idea of how well separated the individual populations are. Then if you have it you might try looking at the data longitudinally (how individual billing plans change over time for billing customers) since the correlation you described above is only a snapshot and billing plans might be more fluid for one age cohort and more stable for others.
Another option: in base you could do: df$billing.term[df$billing.term == 0] &lt;- 6
Hi, thanks for the reply. After I use your code, I receive the error ("target of assignment expands to non-language object"). How can I fix this? Thanks again! 
I changed it, thanks!
Gotcha, thanks!
Thank you for the help /u/menckenjr. Yes, the Billing Term is categorical which is throwing me off here for correlation. I have Date Sold field available in the data frame. Would you reccoemmend the following adjustments to the df first? * Make Billing Term a categorical field * Cluster Age into groups * Cluster Date Sold into Month/Year format &amp;#x200B; The histogram and box plot are below. Unless I'm misinterpreating, it appears the monthly plans may have a correlation with a younger age. &amp;#x200B; **Age Histogram:** **BoxPlot:** &amp;#x200B;
Thank you /u/denzelswashington this worked great. Is the interpretation here that within the brackets is a boolean condition where, if true, recieves the &lt;- value?
Exactly. It is also nice for recoding NA values (depending on your coding scheme) in your dataframe. E.g., df[df == -999 | df == -998] &lt;- NA Or for ranges of columns too, if you need to recode a subset of columns. If you have multiple conditions to change at once you could string together a bunch of ifelse statements but I would consider using case_when in the dplyr package. I am sure there are other options too, but I have found case_when very intuitive. 
Make a reproducible example so we can copy and paste the code you have. [https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example)
```mutate_at(vars(var1, var2), list(val = useful_values$value[match(., useful_values$label]))```
You should join the two tables based on the keys you are matching.
Assuming your data set looks something like... &amp;#x200B; |user|question\_1|question\_2| |:-|:-|:-| |1|neutral|extremely useful| |2|extremely useful|neutral| &amp;#x200B; Then another good approach could be using tidyr's `gather` function to gather the question columns, then `left_join` the tibble with the desired answers, and finally use tidyr's `spread` function to get the table back the way you want. &amp;#x200B; Using the example table above, as `df`, it would look something like: \`\`\` `df %&gt;%` `gather(key = "question", value = "answer", -user) %&gt;%` `left_join(numeric_answers_df, by = c("answer" = "corresponding_column_in_numeric_df") %&gt;%` `spread(question, answer)` \`\`\` &amp;#x200B;
I recently developed a shiny and leaflet app and it went way better than usual. I think it's because I was previously overwriting some reactive functions with the same name within other functions. Technically it should be out of scope, but I think it may have caused problems. 
you need to supply the login info in the request header
This is so relevant to my research thank you for asking this question. 
Hard to be sure without having the login, but you want to post to user: test key: test login: 1 redir: http://rotoguru1.com/cgi-bin/nba-dhd-2019.pl? to http://rotoguru1.com/cgi-bin/nba-dhd-init-1819.pl Looks like you want the postForm function. 
So... Mastermind? What part(s) do you need help with?
That would just be a different path to the same problem though, wouldn’t it? I still end up selecting one column to join on and then would need to rename that column and repeat this for each column in the original data. 
I’m sure we are in this boat with many more people. Seems like a common enough issue. 
I will give this a look when I’m back at my office. Thank you for posting this. 
Ah yes, that's the name! I guess I'm looking for some idea of how to begin writing the code. Specifically, do I have to write conditional statements for each digit (so for e.g. if the first digit is correct then Y=1, but if the second digit is incorrect then Y remains equal to 1)? I figure that the 4-digit number should not actually be treated as a 4 digit number but rather as 4 separate integers?
Yes, I'd treat them as four separate integers. I wouldn't write a statement for each integer - for X there are better ways to do that in R, and for Y you might run into trouble with repeated digits that way and you might prefer to count the times each digit is used. But it probably wouldn't hurt to try what you're thinking and see where you go wrong/whether we can suggest something neater.
OK, maybe I'm not fully understanding what is going on here. I am working on producing a report using knitr at work and I am having a similar problem there. The code is probably too long to post on reddit but for the most part I am performing a similar idea as what I have posted here. I have a sql query that returns a result as a tibble and I want to display the output of that tibble on my pdf_document. Before I call kable or pander I perform a few mutate/select/filter fuctions to manipulate the data in a way that makes the data useful and I have ensure that the data is correct by inspecting it. For some reason when I kable/pander the data frame nothing is printed. I have tried different approaches, like (this is not my actual code but an example of what I'm doing). (examples) * 1) df1 %&gt;% kable * 2) df1 &lt;- &lt;a lot of mutate/select/filter functions&gt; %&gt;% kable * 3) df1 %&gt;% &lt;a lot of mutate/select/filter functions&gt; %&gt;% kable * 4) df1 &lt;- &lt;a lot of mutate/select/filter functions&gt; the df1 %&gt;% kable even pander does not seem to work. if I wrap the kable function in a print statement it will print Latex commands or html if I set to html_document. I have litterally wasted my entire day trying to figure this out and at this point I think my head is going to explode as this seems like a fairly easy thing to figure out but I'm stumped as to what is going wrong.
Sounds like you just want to convert the column to an ordered factor and reorder the levels, rather than converting it to an integer via a lookup table.
Your `x` might be zero-length or consist entirely of NA and NaN, which after being removed, leaves `x` with no elements.
That all looks good - is the problem fixed, or does it manifest when doing something else?
Bold move linking to Stackoverflow here... It's almost as if it would be better to ask there?
From mobile but hope this helps. The random number generated as a list of characters. Random number = character vector with single digits Input string can be split using strsplit(input, split="")[[1]] For the number the number of correct digits irrespective of position = sum(input %in% randomNumber) Correct position = sum(sapply(1:min(length(input), length(randomNumber)), function(i) input[i]==randomNumber[i])
I apologize for the horrible formatting. If you need help or explanation, just let me know. I'll reply when I get on my pc. 
Hi, a simple way to clean up your code might be to make a named vector, such as lkup &lt;- c('Extremely useful'=5, 'Somewhat useful'=4) And then your mutate will be a lot cleaner %&gt;% mutate(cohes = lkup[cohes])
Also depending on how you are using these values you might want to consider converting to a proper ordered factor instead. Cheers.
I want to calculate the average response and there are some related questions which are added together for a total score on a particular topic, therefore I do want a numeric here.
I have done something very similar to this and it worked. Thank you very much. The only hiccup was that I ran into an error (can't create call to non-callable object), but was able to circumvent that by creating a function and using this within list() &amp;#x200B; `likert &lt;- function(x){useful_values$value[match(x,useful_values$label)]}` `survey %&gt;% mutate_at(vars(-email, -name),list(val = likert))` &amp;#x200B; /u/Juice-drinker you may find this useful in your work
 split(mtcars, mtcars$cyl) %&gt;% kable
A for loop is certainly not essential! Best try `combn()` and `replicate()`. I don't have a computer with me to test it out now, but you can make it work by using them.
I have mixed feelings about `replicate()`. I'm sure it can be useful for simple tasks, but the few times I tried to use it ended up getting... messy. But that could just be a PEBKAC thing.
I dont know if there is but I feel like there would be some package like in Bioconductor that might do this
Take this code with a grain of salt, but it works with the example you provided. They key parts are from Biostrings that contains all the IUPAC code maps, as well as expand.grid which will generate all the permutations for you. &amp;#x200B; `library(Biostrings)` &amp;#x200B; `IUPAC_CODE_MAP #note the dictionary of definitions` `seq &lt;- c("A", "R", "C", "C", "N") #define vector of bases` &amp;#x200B; `seq_iupac &lt;- as.vector(IUPAC_CODE_MAP[seq]) #gets all possible values for the iupac codes` &amp;#x200B; `seqs &lt;- c() #vector will be generated with all values` &amp;#x200B; `for (i in 1:length(seq_iupac)) {` `if( i == 1) {` `seqs &lt;- unlist(strsplit(seq_iupac[i], split = NULL)) # first entry will not need appending` `} else {` `seqs_new &lt;- expand.grid(seqs, unlist(strsplit(seq_iupac[i], split = NULL))) # now to that intial vector we add all permutations` `seqs_new &lt;- apply(seqs_new,1,paste0, collapse = "") #collapse the result into strings` `seqs &lt;- seqs_new #update the seqs vector` `}` `}` &amp;#x200B; `print(seqs)` `print(unique(seqs))` &amp;#x200B; &amp;#x200B; &amp;#x200B;
I am not familiar with the survey, but assuming this question was presented the same way to every respondent (i.e., that the scale was not randomly reorganized for every respondent), that the responses were recorded using the numbers of the question, and the responses have not already been recoded for you, then I can hazard educated answers to your questions: &gt; 1. Why have they randomized this? The scale has likely been randomized in an effort to promote the validity of the responses. Because the scale is not in order, responding requires more thought and effort on the part of the respondents. This extra thought and effort is thought to produce more accurate answers and/or answers that can more easily be cross-validated with answers to other questions. Google "insufficient effort responding" for more detail about these ideas. &gt; 2. Is it necessary for me to recode this in order to carry out a test for correlation? In short yes. If you are okay with the assumptions tied up in rescaling ratio scale data (i.e., household income in pounds) down to an ordinal scale (i.e., in this case differently sized but ordered categories of income), the coding of the data should continue to represent the underlying order and must for valid use of a correlation. &gt; 3. Is there a simple way of doing this, rather than doing it manually? Assuming y01 is stored as numeric, you could use dplyr::case_when() to recode: your_dataframe %&gt;% mutate(y01.recode = case_when(y01 == 1 ~ 6, y01 == 2 ~ 2, ... so on and so forth ...)) -&gt; your_dataframe 
&gt;1. Why have they randomised this? I'm not a social scientist but I can imagine two possible reasons: * Respondents are forced to think about it a little harder, and might give more accurate answers. They can't just say "Eh, it's somewhere between C and E" and write D. * Respondents might be more honest if they can report their answer as a meaningless, neutral alphabetical code. &gt;2. Is it necessary for me to recode this in order to carry out a test for correlation? Yes, assuming you're using a test that respects and uses the ordinal structure of the data (i.e. a test that "understands" that these categories are in a specific order, from lowest to highest or vice versa). And you absolutely should be using that type of test. It will be more accurate because it asks the right type of question, and also more powerful because it has more information to work from. The Spearman or Kendall correlation are common choices. If it makes you feel better, you would probably have had to recode it to a numerical value to use these tests anyhow, even if the alphabetical codes were in a sensible order. I don't think R's correlation test functions accept character values for ordinal tests. Think of it as a safeguard to prevent less-careful people from making this exact error. On the other hand, you should **not** use a correlation test such as Pearson's, which assumes your data has even more than just ordinal structure. The only way you could do this would be to assign "real" numerical values: for example, you might choose to code "under £2600" as "2600" or "1300," which is dangerous for reasons that I hope are obvious. (Your social-class-identity variable would have to be similarly meaningful, which is an even harder problem.) These kinds of methods can be more powerful. If you absolutely needed that kind of power you could try supplementing them with an ordinal method to validate their results, while also providing substantial scientific justification for the numerical values you used. (For example, you could search the literature for another paper to help estimate the mean income of the under-£2600 group.) But this sort of thing is complicated and dangerous and look, if you're in a position where Reddit is your best source of statistical advice, just don't try it. &gt;3. Is there a simple way of doing this, rather than doing it manually? Um, depends what you consider "manual." Since the alphabetical codes seem to be arbitrary, not following any sort of pattern, of course you can't simply program R to follow the pattern. I'd suggest creating a list: `codes &lt;- list(H=1, B=2, J=3,...)` and then you can decode the variables with something like `new_numeric_code &lt;- codes[[alphabetic_code]]` For example if your data is stored in a dataframe "df" then I think you could do something like `df$new_numeric_code &lt;- codes[[df$alphabetic_code]]` to recode it all at once.
Ok it looks like I've finally got this to work by setting the code chunk option results="asis"
glad you figured it out! Since you're a newbie, when you ask for help and don't provide a small bit of data or an example of executable code, people will usually gripe at you. :) 
Hey, thanks a lot for writing this out. I was going to something like this, but the command that I was missing here was `expand.grid.` I had hoped there was a one-liner for this, but I guess i'll just define this as my function, and ... now it is! :) &amp;#x200B;
Wait what?
Thanks for gold if that was you!! Expand.grid is extremely useful. Keep it in the back of your mind for anything that involves all possible combinations. I have failed trying to write complex code many times, but when using expand.grid it is much easier!
I'm sure this isn't homework **at all**. 
They want someone to do their homework for them.
worth a shot &amp;#x200B;
&gt;seq\_iupac actually one correcton: I think you meant `seq_iupac[[i]]` where you wrote `seq_iupac[i]`, otherwise I get `Error in base::strsplit(x, ...) : non-character argument`. Otherwise, I just tested it and it looks good!
From the plot you supplied it looks like your subjects are older. 
&gt;library(Biostrings) &gt; &gt;IUPAC\_CODE\_MAP #note the dictionary of definitions &gt; &gt;seq &lt;- c("A", "R", "C", "C", "N") #define vector of bases &gt; &gt;seq\_iupac &lt;- as.vector(IUPAC\_CODE\_MAP\[seq\]) #gets all possible values for the iupac codes &gt; &gt;seqs &lt;- c() #vector will be generated with all values &gt; &gt;for (i in 1:length(seq\_iupac)) { &gt; &gt;if( i == 1) { &gt; &gt;seqs &lt;- unlist(strsplit(seq\_iupac\[i\], split = NULL)) # first entry will not need appending &gt; &gt;} else { &gt; &gt;seqs\_new &lt;- expand.grid(seqs, unlist(strsplit(seq\_iupac\[i\], split = NULL))) # now to that intial vector we add all permutations &gt; &gt;seqs\_new &lt;- apply(seqs\_new,1,paste0, collapse = "") #collapse the result into strings &gt; &gt;seqs &lt;- seqs\_new #update the seqs vector &gt; &gt;} &gt; &gt;} &gt; &gt;print(seqs) Glad it works, &amp;#x200B; Odd I dont get the same error on my end!
I realized why!!! You are using base::strsplit(). Because I loaded in the library Biostrings, I was using Biostrings::strsplit()
Maybe do substr() with regex to isolate the number of R or N you have in the string, put them in separate column and then tidyr::unnest( , .preserve = ) on the column that you form from the c(A,C,G,T) or (A,G) list? It depends on the number of possible N and R combinations you could have in the string for this to be a useful way.
Here's a *very* long line that does away with the explicit for loop: `apply(expand.grid(unlist(lapply(seq_iupac, base::strsplit, split = "", fixed = TRUE), recursive = FALSE)), 1, paste0, collapse="")` Everything remains the same until the `seq_iupac` line, but again would not be possible without `expand.grid` from [bozymandias](/u/bozymandias)
For sure. It seems within the greatest density range, a 5 year difference in the mean age for billing term could still be significant within the [65-100 range.](https://imgur.com/a/vkVBbem) 
It might be useful to point out that this code requires some of the tidyverse packages.
What is the transformation... For example, if x is some vector of values, we can get a mean(x):y, and range(x):z. Now if we log transform x, obvious mean(logx):&lt;y, range(logx):&lt;z. &amp;#x200B; Why would you expect these values to be the same after a transformation? &amp;#x200B;
If I obtained the same mean, it is possible to obtain the same range:z with the log data? Say original: mean=0 , range=\[-2,2\] , after transformation: mean=0, range=\[-1,1\] &amp;#x200B; My transformation is a combination of trig, log, and addition. Lol Thank you!
Thanks a lot, This does look more compact, but I think I would have a harder time coming back to this and reading it/making sense of it, so I'm going to stick with the for-loops, but I appreciate the suggestion.
Have you tried changing its class, say, to numeric? 
From the `str` of `cc85bi5005`, it looks like you're working with spatial data--have you checked [the docs](https://www.rspatial.org/analysis/7-spregression.html#basic-ols-model) for `raster` and made sure your data is structured properly for fitting a linear model? A [reproducible example](https://www.tidyverse.org/help/) would be necessary for fully diagnosing the problem; the two lines of code and console messages aren't much to go off of.
Great, thank you so much for your help. That's a really interesting point about the validity of the answers- I'll check that out on Google. Thank you! &amp;#x200B; &amp;#x200B;
Awesome, thanks for such a thorough answer. And thanks for explaining which test I need to use. Unfortunately my course at university is not very informative at all, and I have to produce a report by the end of next month on an independent project. This is my first time working with R or statistics, so Reddit has been very useful for a beginner like me haha. Thanks so much!
Not if you gather first ;)
Can you be any more precise? I assume you want to use R to do monte carlo simulations on dice rolling to find the probability of certain situations.
I have generated the function: Rollingdice= function(n) sample(1:6,n,rep=T) d= Rollingdice (xx) #xx is any integer If I generate 20 “d”, I would like to see if these number(say, d=1) converges to 1/6
Thank you! This is helpful, however my result ends up being several side by side tables that I can't individually select. My goal is to export/copy each table into a Word doc. Also I can't see how this would work with graphs.
I will have to give that a go. 
Why don't you just use the definition of a limit?
what about using rpy?
It's really important, to save both my time and yours, that you provide a reproducible example. You need to show me exactly what the data looks like before you start (preferably by providing a sample, or code that makes a dummy dataset) and exactly what you want the data to look like when you're finished. Otherwise there is a high risk that we will both waste our time talking backwards and forwards because I didn't understand your question properly. For example, the term "data points" is not R terminology. You could mean columns, or rows, or perhaps even cells in a matrix. You didn't explain how time is represented in the data, either. You didn't specify the method we should use to decide which of the 872 data points should survive in the 100 data point dataset. To answer your question as-is, I would have to make assumptions about those things, and if I make the wrong assumptions, I will give you the wrong answer. Please make it easy to help you by asking a complete question with as much detail as you can about what you have at the start, and what you expect to have at the end, as you can provide.
Ditto.
I can break down something in a few steps for you: choose a really big integer `xx`. make a vector that counts a running proportion of how many `1` or `2` or whatever have shown up so far in `xx` plot that running proportion, perhaps with a horizontal line at your theoretical limit. You can turn this whole thing into a function that returns this as an object, too, with a plot method, but that's more advanced.
that's my backup solution. Deployment is java calling python. I'm not sure how performance is affected if I add another interface from python to R. If there is a pure python solution, I would prefer that 
Do you have to escape special characters with two backslashes in Python (like in R)? Are there the same special characters in python? It may be helpful to post some examples of the errors you are talking about :)
To add to the above, and as a good general guide: [https://stackoverflow.com/help/how-to-ask](https://stackoverflow.com/help/how-to-ask)
[https://opensource.t-mobile.com/blog/posts/r-tensorflow-api/](https://opensource.t-mobile.com/blog/posts/r-tensorflow-api/)
Economic consulting firms.
[Pinnacle Sports](https://twitter.com/hugobowne/status/1097879979961040896?s=21)
Banks
Pharma and biotech 
Utilities... believe it or not, they are for-profit. 
You might be interested in [my post](https://www.reddit.com/r/dataisbeautiful/comments/au25nq/simulating_6000_die_rolls_visualization_created/) on this subject.
https://www.indeed.com/jobs?q=R+programming
Idk if this is the best way to do it, but I think you could do a Cartesian join on site name (and maybe day?) and then filter out the superfluous rows from there. Like once it's joined then you can do compare the dates, find the minimum difference per site name, and then filter to only those rows.
Banks! Airbnb is a good example as well. Everything in their company is connected with R, Rmarkdown, Shiny and Git. There's some Medium's articles about it
Hey now! There are Public sector utilities too that use R. I happen to be one that is.
I run an internet ad agency using r
I've worked for S&amp;P500 companies in hospitality, medicine, and real estate that all used R
That was a really interesting post
Not for profit but in social services and public education administration
The only issue is if the time difference rolls over to the next day for an observation. Joining on site name and day would remove some of the duplication in a full join on just site name. Try: library(lubridate) library(tidyverse) merged &lt;- pressures %&gt;% full_join(temperatures, by="site") %&gt;% mutate(time.diff = abs(pres.time - temp.time)) %&gt;% group_by(pres.time) %&gt;% # or the observation ID number filter(time.diff == min(time.diff)) &amp;#x200B;
I work in heavy industry / manufacturing and we use R for both descriptive stuff as well as modeling. 
Yeah I just threw the day thing out there because I didn't know know what the guy's row count was per site. If it was like a 1000 per site or something the full join could get nasty. If the times are happening at all hours of the day then joining by day would be a bad choice, but if it's during business hours or something it'd be fine. 
Thanks mate, that is amazing content.
Actually, I don't think it does. You've got the same shape distribution across all of the billing terms and what it does show is that annual billing is most frequent and your sample skews older. Your edit has an interesting restatement of the question - "Which age group is most likely to have monthly billing?", which is one of the things a marketing person might want to know. You can get that by coming up with age ranges and computing a contingency table of Age Range versus Billing Term and examining which cells have the highest joint frequency. Or you could do like I suggested earlier and try a discriminant analysis with age as the predictor and billing term as the measurement variable. If you were *really* after statistical significance I would go back to the source of the data and try to get data where you've got better representation across a whole range of ages rather than try to generalize a single sample that's so heavily weighted to older viewers. Hope this helps.
A software company I worked at used it for all their statistics infrastructure on their operational analytics team. We worked primarily on large scale real time sensor data on a custom pipeline.
 list_of_tables = split(mtcars, mtcars$cyl) %&gt;% map(~select(., -cyl)) %&gt;% kable For graphs, it would depend on how you're making the graphs. If you're using ggplot, look into `facet_wrap`. If you're using base plotting, consider making a function that takes a value of the variable you want to split by, for example: plot_by_cyl = function(cyl_value) { d = filter(mtcars, cyl = cyl_value) ## do your plotting with d } plot_by_cyl(4) ## make the plot for only data with cyl = 4 
Perhaps I'm just not thinking about it properly. The data is the complete past 2 years of accounts and the product is specifically for those 65 and older so I'm not sure how I'd get better data. The marketing and finance teams do want to know which variables / personas are more likely to choose a monthly plan vs. annual, since annual is greater LTV. I will try the contingency table and disciminant analysis. I'm pretty new to R and appreciate your guidance here, thanks again.
Best Buy, Seagate, Healthcare companies, Target, and a bunch others.
what do those mainly do with it?
Seagate uses R to detect faulty platters and probably some other stuff, Target I think uses it for fraud detection, but I can’t remember. It was something with credit cards. Not sure what Best Buy uses R for, but probably customer retention or market trends to see what to stock. 
Do you always have a 1:1 count of readings for each temp and pressure? If so, you could assign an index for each site after sorting. Otherwise, it depends on how critical it is to align the temp for each pressure. If some fuzziness is acceptable, you could try to bucket the temp times into buckets according to pressure time's using e.g. findinterval. Then dt merge with mult=first, or average by the bucket. You could also come up with your own custom buckets using the integer hour for each site and day. Maybe identify the first and last hour to bracket observations and then handle the middle values?
Investment firms. Had several clients use it for analyzing market data.
interesting. What kind of data. I have made investment models too, but never thought to use R.
n &lt;- 1e6 summary(as.factor(sample(1:6,n,replace=T)))/n
do you mean dataframe? anyway: `dice.combos$sum &lt;- rowSums (dice.combos)` 
Woops, yes I did. Thank you so much, I'm ashamed how simple that was 
[A reproducible example would help](http://adv-r.had.co.nz/Reproducibility.html). Anyway, if you look at the docs `?boxplot()` states: See Also boxplot.stats which does the computation, bxp for the plotting and more examples; and stripchart for an alternative (with small data sets). Suggests to me that there should be no difference.
data=matrix(rnorm(10\*100),10,100) a= length(boxplot.stats(data)$out) b= length(boxplot(data, plot=FALSE)$out)
I am not sure why, but reducing your columns from 100 to 10 yields identical outlier results between the two functions: data=matrix(rnorm(10*100),100,10) a= sort(boxplot.stats(data, coef = 1)$out) b= sort(boxplot(data, range = 1)$out) As the number of columns increases, `boxplot()` seems to add additional outliers to the insides of the range. This doesn't seem like it should be the case. 
`boxplot.stats` takes a *vector* argument. What ends up happening is the whole matrix is being treated as one long vector.
Wtf!
 This should do it: split(df, sample(1:floor(nrow(df)*.1), nrow(df), replace=T))
I think the rio package (https://cran.r-project.org/web/packages/rio/vignettes/rio.html) can import zip files - could that be of help?
How about something like: n &lt;- 1e3 df0 &lt;- data.frame(x = rnorm(n)) # random subset # ind &lt;- sample(1:10, n, TRUE) # sequential subset 1-100, 101-200, ... ind &lt;- cut(seq(1, n, length.out = n), 10, labels = FALSE) out &lt;- split(df0, ind) length(out) # 10 colnames(out[[1]]) # x rownames(out[[1]]) # 1, 2, ..., 100
Can you run a command-line unzip using system()?
rows&lt;-floor(nrow(df)*.1) df1 &lt;- df[1:rows,] df2 &lt;- df[(rows+1):(2*rows),] df3 &lt;- df[(2*rows+1):(3*rows),] df4 &lt;- df[(3*rows+1):(4*rows),] df5 &lt;- df[(4*rows+1):(5*rows),] df6 &lt;- df[(5*rows+1):(6*rows),] df7 &lt;- df[(6*rows+1):(7*rows),] df8 &lt;- df[(7*rows+1):(8*rows),] df9 &lt;- df[(8*rows+1):(9*rows),] df10 &lt;- df[(9*rows+1):nrow(df),] 
Divide and conquer... take RStudio out of the picture to troubleshoot first. Run R at the command line. You may need to straighten out your PATH environment variable to get that working. Then go back to RStudio.
&gt; new.df.list = list() &gt; x = dim(old.df)[1] &gt; breaks = round(seq(0,x,length.out=11),0) &gt; for(k in 1:10) new.df.list[k] = old.df[(x[k]+1):x[k+1],]
&gt;colnames(out\[\[1\]\]) # x rownames(out\[\[1\]\]) # 1, 2, ..., 100 Hey! Wow this is exactly what I was looking for - very elegant! Thanks a ton, it works perfectly 
How large do you think your data frames will be, and are you processing many of them?
This is wonderful thanks! It was extremely easy to modify this and adapt to split by columns too. I'll need that in the future. Also if I ever need to change into splitting from 10 to , say, 100 that can easily be adapted here. Thanks a lot!
150 by 20,000 on average. I am working on figuring out the best method for my code to work properly, so I'm playing around with transposing. Therefore I'm playing with having a df as 150 columns x 20k rows - and also 20k columns x 150 rows. &amp;#x200B; I guess the next thing will be to figure out if this still works when there are an odd number of rows. 
I think any method here will work for any whole number of rows and columns, so long as you have at least as many along the dimension to be split as you want groups split. BUT, /u/ddoom3832 also gives the correct number of splits if you make a function out of it, and specify a number of cuts in the "cut()" part of the function which is larger than the available number of rows/columns. So, if you want to split an 8-row data frame into 10 rows (either intentionally or incidentally, if you're not checking number of rows), then their method is better, while I believe my method will be wrong. That being said, if the number of columns is not very large (~1000 or higher), then my method made into a function is slightly faster. If the number of columns is very small, about on the order of &lt; 10, then the speed savings is about 40-60%. At the number of columns you expect to have, the speed savings is minimal, maybe 8%. We're talking about a difference of 1-2 ms on average time difference for your case, probably not too important.
/u/ddoonn3832's method is also quicker and more concise when used to randomly sort rows into the different groups. There is not as convenient way for my method to do that except for redefining a new data frame which is so-sorted, which takes up too much time and memory.
I think I found the problem - my library path hadn't been updated from 3.5.1. I changed my libpath to the correct location and that fixed the problem.
You should be able to use the `unzip()` function to extract the files and save them locally. I am on mobile so I cannot test atm. [A source, if you want to read more](https://hydroecology.net/downloading-extracting-and-reading-files-in-r/) [Documentation](https://www.rdocumentation.org/packages/utils/versions/3.5.2/topics/unzip)
Yes my original thought was to just go into the command line and do it, but I live so often in Rstudio that I was curious if there was a way to grab it in there without issue. Thanks.
Thank you I will look into how I can use this with the aws.s3 library. There might be a way to use a function from rio along with the save\_object.
Maybe this is not the easiest way, but I would run something like this: `data(iris)batch_size &lt;- 12` `batches &lt;- split(iris, sort(rep(1:batch_size, nrow(iris) %/% batch_size)))` `batches[[length(batches)+1]] &lt;- iris[(nrow(iris) - nrow(iris) %% 12 + 1):nrow(iris), ]` &amp;#x200B;
Changing p+geom\_smooth(method=lm, aes(col=c("White","Black","Other"))) for p+geom\_smooth(method=lm) should be enough to have three regressions lines, each in the colour of the default ggplot palette. &amp;#x200B; Tha aesthetic "col" is not used for what you are thinking. If you want to change default color palette you must do it in the ggplot way. &amp;#x200B;
Simple: Don't. Reshape your data to the correct format.
How do I do that? Also, this is technically 3D data so I cannot reorder the columns.
Show an example of your data that people can reproduce: [https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example](https://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example)
God is written as Hadley
F &lt;- TRUE T &lt;- FALSE
Here's what I would do: library(tidyverse) # generate some fake data Smoke &lt;- c(rep('Smoker', 120), rep('Nonsmoker', 120)) Race &lt;- rep(rep(c('White', 'Black', 'Other'), each = 40), 2) Race &lt;- factor(Race, levels = c('White', 'Black', 'Other')) df &lt;- data.frame(Smoke, Race) df$AGE &lt;- round(rnorm(nrow(df), mean = 27, sd = 3)) df$BWT &lt;- round(rnorm(nrow(df), mean = 7, sd = 1.2)) df$BWT[which(df$Smoke == 'Smoker')] &lt;- round(rnorm(length(which(df$Smoke == 'Smoker')), mean = 5, sd = 1.2)) # plotting: df %&gt;% ggplot(aes(x = AGE, y = BWT, colour = Race)) + # color or colour, not col geom_jitter(alpha = .3, width = 1, height = .1) + stat_smooth(method="lm") + coord_cartesian(ylim = c(0,10)) + ggtitle('BWT~AGE+Race (Smoking not in the regression)') # But this doesn't include Smoking in the regression lines, and you probably care about the relationship between smoking and birthweight df %&gt;% ggplot(aes(x = AGE, y = BWT, colour = Race, shape = Smoke)) + geom_jitter(width = 1, height = .1) + stat_smooth(method="lm") + coord_cartesian(ylim = c(0,10)) + ggtitle('BWT~AGE+Race+Smoke (notice there are 6 lines now)') # I don't like to plot color and shape in the same plot. I think it looks too busy. # Facets are nice for this: df %&gt;% ggplot(aes(x = AGE, y = BWT, colour = Smoke)) + geom_jitter(width = 1, height = .1) + stat_smooth(method="lm") + facet_grid(~Race) + coord_cartesian(ylim = c(0,10)) + ggtitle('BWT~AGE+Race+Smoke (Facets are nice)') Sorry if my markdown formatting isn't great. Depending on your dataset structure, you might not be able to plug and play with that (which is probably for the best haha).
As u/jackbrux said, a reproducible example would help. It sounds like your data is in long format, and you’re wanting it in wide. Here’s a link about tidy data: https://r4ds.had.co.nz/tidy-data.html
Just use foo and bar and everyone will wink at you and give you discounts
How can I forget, the class foo and bar
`tapply` is your friend here.
It is because you are calling your plot wrong. You should provide x first, then a y. `plot(x,foo(x))` Notice this is the points, not a density estimate. When you call stat\_function in ggplot it is computing the path through these points and fitting a curve to it. The default is, geom = "path" [https://ggplot2.tidyverse.org/reference/stat\_function.html](https://ggplot2.tidyverse.org/reference/stat_function.html) &amp;#x200B; Notice how the below plot is identical to the original base R plot? This has nothing to do with margins, instead with what you are asking to be plotted in each case. `y &lt;- as.data.frame(x)` `ggplot(y, aes(x=x)) + stat_function(fun=foo, geom = "point")`
Thank you very much for your detailed information! Thanks for the correction!
easiest way is just &amp;#x200B; for(v in c(A,B,C){ nrow(subset(df, df\[1\]=v &amp; df\[2\] = T)) / nrow(subset(df,df\[1\] = v)) } &amp;#x200B; Dirty and hacky sure but also flexible &amp;#x200B;
You could use [blogdown](https://bookdown.org/yihui/blogdown/) and github pages. If you want something more like Wordpress, you could do a $5/m Digital Ocean dropplet with the Wordpress One-click app. 
With a $5/m droplet you can also host your own shiny server. A lot of options from there if you're willing to go that route and if you want to use something like wordpress you can just embed the shiny apps you're hosting on DO in iframes.
This can be done using `dplyr` with the `group_by`, `summarize`, and `mutate` functions. Or you can use base R: tb &lt;- table( DF ) 100 * tb / rowSums( tb ) 
colSums gave me what I (mostly) needed. There's a slight oddity in the result that I can't explain (for False on one only 1 group I get 103% for some reason and I haven't the faintest clue why. All other groups have the correct %s) I saw various examples using dplyr but they didn't quite fit what I was I was facing since those problems didn't have the subproblem of needing to find percentages with a group by clause. 
Which school? Just curious. 
 library(data.table) dt &lt;- data.table(df) dt[,lapply(.SD,mean),by=class]
thanks for showing me. I will need to format the data.
thank you, I will need to format the data.
I'm not sure I like this visualization. It colludes two types of different data (gender and field) into one type of visualization space (the circle). As much as I don't like the recent overuse of sankey diagram, I believe in this case it would be a more proper method.
You have a good point with "it colludes two types of different data (gender and field) into one type of visualization space (the circle)". I am thinking in use an alluvial diagram: is similar because it is still using flows, but I can set up a separate space for each type of data (gender and field). 
To add on to /u/takeasecond , I found this blog post recently and am in the process of setting it up for myself: https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/
`df` and `data` are the names of a functions in base R. I use `DF` or `dta`. I use `Dtm` for a `POSIXct` value and `Dt` for a `Date` value, though I don't think that is widely used.
Tidyverse. A lot easier to learn and remember 
Okay I believe you that it will be easier for me. But is it easy to translate, say, syntax from sql injection into tidyverse R?
Agreed, once I learned about tidyverse, I feel like i wasted weeks learning archaic base R functions that I never used again.
Since you mentioned Excel specifically: The tidyverse package readxl can read Excel data, but can’t write data to Excel format. However, the xlsx package can read and write Excel files. (There are other packages for working with Excel data, but I only have experience with readxl and xlsx.) https://readxl.tidyverse.org
R is perfectly capable of handling the kind of task you describe. Base vs tidy is a personal preference, although most people find tidyverse easier to grasp, compared to base. Base is not about for loops any more than tidy is about pipes; you are still working in the same language, and the fundamental principles of how R expects to be used (functional) apply regardless. 
 In this example, you will loop through this for the entirety of df$col. n = floor(length(df$col) / 80) result &lt;- data.frame() for (a in 0:(n-1)){ start = 1 + 80*(a) end = 80 + 80*(a) rbind(df$col[start:end]) }
What does the structure of the data look like?
My vote would be for Jekyll. It's very easy and free with github. You can start off with a theme like the free ones [here](https://jekyllthemes.io/free). 
&gt;example &lt;- data.frame(X = c("A","A","A","B","B","B","C","C","C","D","D","D"),Y = rnorm(12)) similar to above but theres 80 A's, 80 B's, and so on. so I would want example$B &amp;#x200B; result I would want from above would be 4 X 3, 4 rows each for alphabet, 3 columns for number corresponding to a letter.
superb! Worked wonderfully. Thank you very much!
Yes, they are very similar. SQL = dplyr (R) like... Select = select from = dataset you pipe with %&gt;% where = filter / subset / many commands inner join = inner join, etc. min = min max = max they share a lot of common features. The biggest diff is R is more functional/object oriented, you're using functions or creating objects. SQL is more declarative, you're saying what you want in SQL terms, you may have to write it out a bit longer, but it's going to find an efficient way to pull things together. R, you may pull things together via your own workflow process, so you may have some performance issues, but don't optimize before you find performance issues!
I'd say you need both to use R well. Base R has a lot of nifty features that are 100% good to go. Tidyverse has a lot of nifty features that are improvements upon or didn't exist in base R. You can't be a great R programmer without base R, however tidyverse itself has many advatanges such as readability, piping, popularity/user base, etc.
Swirl is great for base R, but I’d also recommend working through the r4ds book. It will give you some great exposure to the tidyverse workflow while hopefully keeping things interesting. Also, go check out examples on Kaggle. This is the epitome of gaming in terms of R development, and shows just how powerful of a language it can be. DataCamp provides XP and certificates which are nice learning incentives, but it costs money and has been described as a formulaic, or fill-in-the-blank method by experienced R users.
Check this out: [New to R? Kickstart Your Learning and Career With These 6 Steps!](https://paulvanderlaken.com/2017/10/18/learn-r/amp/) 
You might want to try [project Euler](https://projecteuler.net) it's a series of math/programming problems that you can solve using R or any programming language. It may be a bit less guided than what your looking for however. The later problems are probably well beyond most people's math expertise, but the first 50-100 can be solved by most technically minded people.
Have you tried DataCamp?
Now that you've got it as a loop, try to translate it to either the foreach package or use lapply/mapply, which will both be many times faster than looping in R
https://www.tidyverse.org/learn/ has lots of resources.
Can't you just define a global variable for the path string in the first source file (/ "config" file)?
What do you mean global variable with the path name? I did try sourcing the same config file in each script, however when I copy the directory the scripts would still be sourcing the old config file, so the working directory would still be set to the old project directory. Even if I changed the new config file, I would have to change all the scripts to point to the new config file. I could go and change the sourcing path to the correct config file, but that would defeat the purpose. I'm looking for a lazy (more elegant?) way to set a dynamic working directory based on either 1) The directory the actual scripts live in 2) Some type of config file specific to that directory
If you are always going to have to run them in order, I'd just use the setwd in the first script and the just use the source() command to refer to prior scripts in the future scripts.
?merge
Unfortunately they won't always be run in the same order.
[Imgur album](https://imgur.com/a/XNfZzj5) with the same graphic for other programming languages - for comparison! | If you want more information on how these were created, it can be found [here](https://www.globalapptesting.com/blog/picking-apart-stackoverflow-what-bugs-developers-the-most)
##r/DataArt --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/9wy10w/ignore_list/)
TL;DR: Fuckin' ggplot, man.
I avoid `setwd` like the plague. Even if the script is in a subdirectory I assume the main working directory is always current.
I saw a Twitter thread the other day, talking about how ggplot is great for building 90% of the complicated plot that you want. But that last 10% is a nightmare trying to find some weird/obscure trick.
Use `here()` so you can always give paths based on the project root. Here I assume you are keeping 6 copies of the entire project. 
Powerful tool, but definitely not intuitive 
If I understood your problem correctly this should work: library(dplyr) library(tidyr) # Example df1 df1 &lt;- tibble(flowername = c("Tulip", "Rose", "Sunflower")) # Example df2 df2 &lt;- tibble(group = c("group1", "group2", "group3"), Tulip = c(50, 73, 100), Rose = c(29, 86, 51), Sunflower = c(110, 15, 72)) # Find average seed number for each flower name df2_summary &lt;- df2 %&gt;% select(-group) %&gt;% # Unselect "group" column gather(key = flowername) %&gt;% # Make wide data long group_by(flowername) %&gt;% # Group by flower name summarize(seed_avg = mean(value)) # Find average value # Join df1 with df2_summary df3 &lt;- df1 %&gt;% left_join(df2_summary) If this isn't what you were going or if you have any questions let me know.
One of my friends came running to me a ways back with his Twitter open yelling "LOOK AT THIS SHIT." Apparently Hadley Wickham had tweeted asking about how to make some specific plot feature in ggplot.
It's always something involving goddamn theme()
Turns out, I didnt need loop, I couldve just done &gt;matrix(df$col, byrow=T, nrow=n, ncol=80) Haha... 
Check [this](https://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html) reference. It's my go-to reference on merging data in R. 
damn i really thought it'd be dirk eddelbuettel
Not sure about caret specifically but you might find this helpful https://github.com/ropensci/RedisAPI/pull/6
You're absolutely correct!
That error means you're selecting a column that doesn't exist in df2. Are you sure they're the same dimensions?
The speed of Usain Bolt has nothing to do with the width of his bed.
t.test() with the paired = TRUE statement.
Wouldn't this be more like him having the correct size shoes ready to put on versus needing to find a pair?
Posted an answer on SO.
I'm too lazy to look at that dataset, but you'll want to filter the dataset to just the country of interest. Here's some starter code you could adjust, although you'll probably have to do some more data work before plotting. With base R you could write something like ``` austrailia &lt;- gapminder[gapminder$country == 'Austrailia', ] plot(austrailia$time, austrailia$population) ```
Someone find this! 
Just initially split by two columns as such: `pilot_clean_new &lt;- lapply(split(split_ppid, list(pilot_clean$ppid, split_ppid$trialn), drop = TRUE) function(data) { ...` Or you may want to look into `tapply()` Best of luck
Hi courtney I haven't worked through your problem but this sounds like it would be made much easier by using the dplyr package (part of Hadley wickham's tidyverse). You would do something like Mydata %&gt;% group_by(trialn) %&gt;% filter(steer &gt; deadzone) %&gt;% summarize(timestamp = first(timestamp)) And then join that back onto some other table with merge or left_join
This article presents a concise introduction to linear models. The primary objective is a brief introduction to linear models, how to use them in R and an overview of statistical concepts involved.
Thank you for the advice!
Give fake data explaining your issues that we can work with.
Hi thank you for the comment! I attempted to do this previously however came unstuck when I needed only the first SWA above the deadzone threshold. &amp;#x200B; Filter in that example gives me every instance of SWA being above threshold for each trialn
Good point, that would make it easier to show what I'm trying to do. I've edited the OP with this, thanks!
you have to use the "group_by" function first: group_by(&lt;data.frame&gt;,`Bird species`, `Location`, `Month`) Summarize(&lt;data.frame&gt;,n(Infection))
The list of column names is a start, but your example table should have some data and you should add an example of what you have already coded (what you say you can do) because there is often more than one way you could have done it. For example you probably want to convert your Month/Day columns to a `Date` or `yearmon` if you want to plot the data or otherwise analyze it. Regarding what you asked, one approach might be dta %&gt;% filter( 1 == Infection ) %&gt;% group_by( Year, Month, `Bird species` ) %&gt;% summarise( Count = n() ) %&gt;% ungroup() and another might be dta %&gt;% group_by( Year, Month, `Bird species`, Infection ) %&gt;% summarise( Count = n() ) %&gt;% ungroup() and to accomplish what your friend proposed: library(tidyr) dta %&gt;% group_by( Year, Month, `Bird species`, Infection ) %&gt;% summarise( Count = n() ) %&gt;% ungroup() %&gt;% mutate( Infection = ifelse( 1 == Infection, "Infected", "Uninfected" ) ) spread( Infection, Count )
Add the timestamp condition to the `filter` condition Mydata %&gt;% group_by(trialn) %&gt;% filter(steer &gt; deadzone, timestamp == min(timestamp)) Additionally, this methods lets you easily add a second grouping: Mydata %&gt;% group_by(trialn, ppid) %&gt;% filter(steer &gt; deadzone, timestamp == min(timestamp)) 
I would start with something like this, and then massage it a bit until you get what you're looking for: library(tidyverse) gather(df, "variable", "mean", -Cluster) %&gt;% ggplot(aes(Cluster, mean)) + geom_point() + expand_limits(y = 0) + facet_wrap(~variable) 
This is fantastic and gives me the kind of output I needed, thank you very much for your help. 
Can you be a more specific? Is it a static csv and you would like to generate an extra column based on the answers? Or you would like to write a for loop based on a streaming data? I do not get what you are trying to do. But you will certeanly will need paste0()
I don't know how much R you know, but if you can import it using read.table or something, it should be pretty easy with the paste function and a loop: outputText = "" for( i in 1:length()){ string1 = as.character(input[i,1]) string2 = as.character(input[i,2]) paste(outputText, "Participant ", i, " is a ", string1, " year old ", string2, ".\n", sep = "") }
If it’s not that many individual plots, you might look at facet_wrap() or facet_grid(). You can also simply assign a plot to an object, e.g. : for(i in range(1, num_plots)): Plot_i &lt;- ggplot() ... Then feed those objects into cowplot plot_grid. Otherwise, it may be easier to save all the plots as png files or the like and assemble them together for your specific use-case in an image editor or PowerPoint tbh. 
&gt;If it’s not that many individual plots, you might look at facet\_wrap() or facet\_grid(). Unfortunately I have 864 plots :/ `for(i in range(1, num_plots)): Plot_i &lt;- ggplot() ...` Can you unpack this because I think it could solve my problem, but I don't know how to implement it in my code... `for (Participant in unique(TestData$Participant)) {` `print(ggplot(TestData[TestData$Participant==Participant,], aes(Emotions, Ratings)) +` `geom_hline(yintercept = 1:5) + #adds lines at each reference point` `geom_col(fill="steelblue") +` `labs(x="Emotions", y="Strength of Emotion") +` `ylim(0,5) +` `theme_classic() +` `theme(axis.text.x = element_text(angle = 45, hjust = 1)))` `}` This is what I've tried so far and it isn't working... `for (Participant in range(unique(TestData$Participant), 288) {` `Plot_i &lt;- ggplot(TestData[TestData$Participant==Participant,], aes(Emotions, Ratings)) +` `geom_hline(yintercept = 1:5) + #adds lines at each reference point` `geom_col(fill="steelblue") +` `labs(x="Emotions", y="Strength of Emotion") +` `ylim(0,5) +` `theme_classic() +` `theme(axis.text.x = element_text(angle = 45, hjust = 1))` `}` &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thanks
Ok, couple things you could try (sorry, I'm on mobile and can't test this out): \-If Participant is categorical, loop over for (Participant in unique(TestData$Participant) instead of the range \-then, make an empty list beforehand e.g. plots &lt;- list() and then you can append each plot to the list with plots\[Participant\] &lt;- ggplot...
When I need to do this, I find it simplest to approach the problem using a function and a list of data frames. The function should handle the ggplot logic, and you will need to use e.g. aes_string to pass quoted variable names. Once you get the function working for one data frame, you need to figure out how to split your data into a list of data frames. Each list element represents one output. Then it's a simple matter of iterating using lapply or an explicit loop. If you have accompanying inputs you need to handle in tandem as variables, use Map. I can probably dig up some old code where I did just this, including, I believe, cowplot to arrange things, if you like.
Save 'em in a list. Lists are just generic containers for multiple objects. all_plots &lt;- list() for (i in 1:x) { all_plots[[i]] &lt;- ggplot()... } You'll end up with a giant set of plot objects that you can then do whatever you want with them. Given the number you're talking about, you probably don't want to print all of them, but depending on what you're looping through exactly, you could try printing every 20th one or whatever, if that is more useful...
These kinds of examples are the ones that give for loops a bad name with regard to performance. You only need to make one change in the pattern to make a dramatic difference in performance. all_plots &lt;- vector("list", x) because this prevents memory thrashing as the list gets longer as longer. Or you can replace the entire example with all_plots &lt;- lapply( 1:x, function(i) { ggplot(...) })
Is there a reason you couldn't just use lapply() and purrr::map()? 
You need to do mutate(c=columnA-columnB) dplyr works with unquoted column names.
as the others said, group\_by() is your friend...
Yeah using df$colnam is a base R was of doing things and if you are using DPLYR piping like you are here, using base R syntax can throw things off. Since you started with your dataset and are piping things in, it assumes which data frame or tibble you are using so you don’t need to specify. You can just use Df &lt;- Df %&gt;% Mutate(c = colA - colB)
I'm so stupid, of course! Thank you!
```c``` is a function used for making vectors. While it's not a reserved word I wouldn't recommend using it for variables.
`df` is the name of a function in base R... I avoid using it as a variable name in the global environment. `seq` is also a name of a commonly-used function... putting it in a data frame is a little less likely to cause problems than in the global environment, but if you have the option then capitalizing can make things clearer to the reader and to R. The `seq` column already serves as a grouping column. For example it can be directly used to mark rows with how many instances of that SEQ exist: DF$count &lt;- ave( rep( 1, nrow( DF ) ), DF$SEQ, FUN=sum ) But if you are really determined to get an integer column the factor type does this: DF$grp &lt;- as.integer( factor( DF$SEQ ) ) You can convert the integer to character per your example... but SEQ was already a perfectly usable grouping character string, so I don't see the point. 
Or: library(magrittr) Df %&lt;&gt;% mutate(c = colA - colB) Magrittr is part of the tidyverse and if you have dplyr, you have magrittr. It gives you some pretty useful additional pipes.
When it's a column name in a data frame it's probably ok, but in general is very bad to overwrite functions.
[https://stackoverflow.com/a/31994539/1968](https://stackoverflow.com/a/31994539/1968)
This most likely won’t work if `i` is used in the construction of the plot.
I’ve never used Magrittr, does the %&lt;&gt;% replace the need to use &lt;- to assign DF to itself? 
Well, looking on Wikipedia: &gt; The standardized mortality ratio is the ratio of observed deaths in the study group to expected deaths in the general population. Using your code, it looks like you have: 1) Deaths 2) Expected total Deaths It looks like all that's left is to calculate the ratio.
Yeah i've been trying to convert over the last few weeks - tough but worthwhile I hope! 
Yeah. It saves you from having to write out the variable name twice &gt;X &lt;- X%&gt;% Becomes &gt;X %&lt;&gt;%
yeah, but &amp;#x200B; `sum(df$Death) / sum(df$ExpectedDeathCount )` &amp;#x200B; always gives me 1. that's why I'm not sure 
You want to calculate the standardized mortality ratio for every group, not just the total smr. The smr for the entire data will be 1 by definition.
Thank you! I might try this later, but I've never worked with a function before and I've had a lot of success with my current forloop. I think I'll try this later if I can't work with my loop
I tried this, and I understand that logic of what you're trying to say and I understand what the `i` is but when I run the code with the `i` I get an error saying that it doesn't know what `i` is, and when I try and replace the `i` with the number of objects my loop will produce, it only places an object in the last item in the list, and doesn't actually fill the list. 
What does that mean? I just tried it and realize that the `i` doesn't work
It’s fairly complex, I’ve posted a link to an explanation and a solution in a top-level comment: https://stackoverflow.com/a/31994539/1968
This seems really helpful - how can I change my loop into a function, though? &amp;#x200B; `for (Participant in unique(TestData$Participant)) {` `ggplot(TestData[TestData$Participant==Participant,], aes(Emotions, Ratings)) +` `geom_hline(yintercept = 1:5) + #adds lines at each reference point` `geom_col(fill="steelblue") +` `labs(x="Emotions", y="Strength of Emotion") +` `ylim(0,5) +` `theme_classic() +` `theme(axis.text.x = element_text(angle = 45, hjust = 1))` `}` &amp;#x200B;
Yes - because I don't understand the apply() funcitons and I don't know what purrr::map() is
Thank you!! this is so helpful. And I'll be sure to post data/code next time.
When in doubt search Hadley wickham. http://adv-r.had.co.nz/Functional-programming.html
Here's a version where you call the function using an explicit loop, which you can just append to the above: &amp;#x200B; # let's imagine we want to plot each row # take a 5-row subset of diamonds dat_sub &lt;- dat[1:5, ] # instantiate the list plt_list_loop &lt;- vector("list", nrow(dat_sub)) # iterate through; we assign 'row' as the loop index var, but you could of course # use something else for(row in seq_len(nrow(dat_sub))) { plt_list_loop[[row]] &lt;- plot_fun( data = dat_sub[row, , drop = FALSE], xvar = "carat", yvar = "price" ) } # this does not plot anything useful, but it does work plt_list_loop &amp;#x200B;
Vectorization takes a different way of thinking about problems in R. I used to program in Basic or Python but found vectorization to be fast and efficient. When I first started using R I used a lot of For-Next loops, but after 3 months, I pretty much stuck to vectorization. &amp;#x200B; Below is a simple example: first using a loop, then using vectorization. &amp;#x200B; &amp;#x200B; `# using loops` `for (i in 1:6){` `a[i] &lt;- i^2` `}` `a` &amp;#x200B; &amp;#x200B; `# # different ways using vectorization` &amp;#x200B; `# vectorization using base R` `lapply(1:6, function(x) x^2) # yields a list` `unlist(lapply(1:6, function(x) x^2)) # yields a vector` `sapply(1:6, function(x) x^2) # recommend you do NOT use sapply(). It can give some weird results sometimes` &amp;#x200B; `# vectorization using the purrr package from the tidyverse` `purrr::map(1:6, function(x) x^2) # yields a list` `purrr::map(1:6, ~.x^2) # yields a list` `purrr::map_dbl(1:6, function(x) x^2) # yields a list` `purrr::map_dbl(1:6, ~.x^2) # yields a list` &amp;#x200B; All these methods yield the same result, either as a list or as a vector. My personal preference is to use purrr::map or purrr::map\_dbl, or another of the map\_XXX family &amp;#x200B;
You don't... that is what the `lapply` does. all_plots &lt;- lapply( unique(TestData$Participant), function(Participant) { ggplot(...) })
None of these examples are vectorized... other than the for loop they are all functional (which has some advantages, but vectorization and its associated speed boost is not one of those), but they make no use of the intrinsic performance boost that vectorization provides. There is one vectorized version: a &lt;- (1:6)^2 For more complicated tasks `ifelse`, `cumsum`, and the usual math operators will all do calculations on many elements of a vector real fast.
You are in the [fourth circle of hell](https://www.burns-stat.com/pages/Tutor/R_inferno.pdf#chapter.4). &amp;#x200B; &gt;A common reflex is to use a function in the apply family. This is not vectorization, it is loop-hiding. The apply function has a for loop in its definition. &amp;#x200B;
what the other guy said, this is cumulative. delete the sums functions and you're good to go.
this text is incredible, thank you
Yeah that's a fair point. Pre-allocate the size of your lists/vectors/matrices/etc. whenever possible when using loops. Absolutely good advice for performance. &amp;#x200B; I don't particularly like the lapply() method when what you're iterating over is just a vector of numbers used as a counter, but that's entirely a personal preference of mine. They're entirely equivalent (as long as you've pre-allocated your list).
This post needs more upvotes!
The apply functions are all written in C++ though, aren't they? Certainly a good exercise to try and vectorise your code, but if OP is looking for efficiency-gains then he'll be looking for a while.
options(warn=-1)
What a bizarre request. Just don't give a character vector to `mean`.
Why not just do this: i = 1:6; a = i^2 That's vectorised. 
its for a project that has a test case with a character. That's why
There is no need to test the`mean` function for this behavior. So don't. If your code under test needs to have an if-else test to avoid it then that is how it should be handled.
As others have said, vectorized code is code that automatically runs on vectors and not code that you use a "loop-hiding" function like `lapply` to run. Using explicit loops isn't necessarily a bad thing at all. It's not slower than say using lapply etc. However, as in your example, often times you just don't need to be using a loop (i.e., the square function `^2` automatically works on the entire vector already). And if you do need a for loops, a lot of times you're doing something decently simple, and the code might read a lot better with a function like `lapply` since when you get used to reading that type of code, it's more expressive of the code's intent. However, when you do have to do use a for loop, that might just be the best way to do it.
In my experience, calling the `View()` function on a dataframe in R is the closest thing to visualizing a spreadsheet in either language. That being said, if you're only comparing prices on a consumer level (i.e. a dataset smaller than 20 or so rows) and you're used to using formulas and cell references, you're not going to get much out of R or Pandas. If libreoffice truly is that slow on your hardware, it sounds like your hardware is very out of date. In which case you could try Gnumeric I suppose. It's just that you're going to have to change your workflow a lot and with small datasets, there just isn't that much benefit to using a powerful tool like R or Pandas in my opinion.
I've worked with both. My opinion:. Both are great for working with database-style tables of information. The R syntax has lots of implied syntax, such that a variable doesn't need to be in scope to use it. This makes the R expressions shorter and, in some ways, easier to read. But coming from python, where "explicit is better than implicit," it makes me want to rip out my hair. And it limits the clarity of the traceback when your code throws an error. I would stick to pandas if I were you. R would be good to learn if you're planning on working with statisticians sometime in the future. But lots of data scientists are using python now too, especially for machine learning.
 edit(df) in R allow you to directly change values in a "View" like window. but it's not really recommended 
`Error in edit : Editing of data frames and matrixes is not supported in RStudio` That's weird...
works on my version (1.1.447, linux) but it shouldn't be used anyway, it breaks the whole *reproducibility* concept.
On the r side also check out the DT library for viewing data. https://rstudio.github.io/DT/ In general with dplyr/tidy verse data manipulation is quite easy and well documented. I have been burned by a groupby bug in pandas so maybe I am biased for ever now. 
So what? Loop hiding is the whole point. It removed irrelevant mechanical detail from the user code, forces you to formulate the logic clearly and think about side effects, and leads to cleaner code.
Thank you for your advice, I changed the variable names as you suggested. Your code worked perfectly. I didn't realise that the function "ave" can do things apart from finding the average. Because here you are using it to call the function "sum", correct? Or is there some averaging going on behind the scenes?
Nothing special behind the scenes. The `ave` function is useful any time you want to compute results among a grouping of related values without changing the number of values... like if you want to augment a data frame with a new column. One of the more cool tricks you can do with `ave` is use the `cumsum` function on a vector of 1s... you get a set of unique increments within each group that lets you subsequently find e.g. "the first ten values" in the group (filter on less than 11)... and they don't even have to be contiguous groups for it to work. You can do similar things with `group_by` from `dplyr` but this can be faster to execute for some cases.
What about fix(df)
This disables warnings globally so on its own it's a pretty bad idea.
So test for the correct type explicitly. You *can* suppress the warning (hint, Google these words) but it me be much better to be explicit, and this document that incompatible types are expected, accepted, and yield a given result (i.e. NA).
If your current tool is a spreadsheet, why not just move that to a Google Sheet? It has the formulas and formatting I would expect to see in a price comparison spreadsheet. 
R and the tidyverse is a comprehensive set of tools that can easily replicate what spreadsheets can do. Python’s data science library is comparable but if you are entering programming for the first time, the learning curve can be a bit steeper than R and Tidyverse. The good news is once you know one you can apply the techniques to the other, just have to learn the syntax differences. Pick what feels right and run with it!
Why do you think the code is running the loop that often? What happens for n=1? general suggestions: you can use ifelse to do this kind of deciding on the whole vector at once, instead of using a loop. alternatively, you can put the three hair vectors into a list where the name of the list element matches the eye colour. now you can get the correct list to sample from without an if statement. last but not least, sample() with replacement also works with weighted lists if I'm not mistaken. 
Seconded. Unless it’s for statistical purposes, pandas/python is generally going to be more worthwhile to use for spreadsheets. 
With RStudio you just click and edit?
Packaging R and its libraries is from my experience a pain and takes a long time to unarchive because of how many files you'll have in there. It doesn't look super professional to have a single shiny app requiring a 15 minute install. The best solution, especially for shiny app, is the Rinno package, I invite you to check it out on github. It makes nice installers and run your shiny app in its own electron window and it hammers down every kink I encontered from packaging entire R environment by checking the registry etc.
I forgot to answer your actual question: I don't remember the default on R portable but check the value of:.libPath() By default R will install and read libraries in your home folder (document folder on windows). The easiest solution is to have an Rprofile in the Rterm/Rscript folder that assign another relative path as your libPath 
So I had to use R Portable on campus, but that resulting libraries were large and campus IT limits the My Documents storage. I ended up getting some network storage from my department and just making a batch (.bat) file in My Docs that I could run that would mount the network storage as a specific letter drive. I say all that as a suggestion of how to deal with multiple users using the same library/ies: run the library installs yourself into the mapped network drive and then have them load the libraries specifically from that location. install.packages("ggplot2", lib="H:/Rpackages", repos="http://cran.us.r-project.org") library("ggplot2", lib.loc="H:/Rpackages")
right, thanks !
Attach an indicator set to 0 for data set 1. Attach another indicator set to 1 for data set 2. Then rbind the data sets. 
Check out the modulus/division remainder operator? I'm not an R expert, but it's how I'd do it in other languages,
odds &lt;- seq(1,length(df), 2) dfOdd &lt;- df\[,odds\] dfEven &lt;- df\[,-odds\]
Thanks ! 
Hmmm I'll test this out. I wonder if R actually creates a new folder (which would make it easier for users) by itself. &amp;#x200B; Something like what's shown \[here\]([https://stackoverflow.com/questions/4216753/check-existence-of-directory-and-create-if-doesnt-exist](https://stackoverflow.com/questions/4216753/check-existence-of-directory-and-create-if-doesnt-exist)): ifelse(!dir.exists(file.path(mainDir, subDir)), dir.create(file.path(mainDir, subDir)), FALSE) Building off that I can check if the libraries I want exist under that directory. The user would only have a larger wait time the first time they run the shiny app. &amp;#x200B;
Technically, I'm using Electron now. I'm following along with [https://github.com/ColumbusCollaboratory/electron-quick-start](https://github.com/ColumbusCollaboratory/electron-quick-start). This approach was so I could also package the Shiny App with any data I have in the future. But I when I tried to install packages it said they were already present, because it's using my non-R-portable library as reference. &amp;#x200B; I was looking at Rinno but can't find any walkthroughs. Do you know where I could find some? &amp;#x200B; Thanks! &amp;#x200B;
Yeah as I said, R, any R in your system will look for libraries at certain places in your system unless told otherwise. To tell it otherwise you need to use the libPath function. The best solution is to have an Rprofile in your portable R that set one of the folder in R portable as the library folder. Otherwise Rinno is pretty straigjt forward if you read their github pages, then even have a template built in so you can see what the general structure looks like 
How do you expect to column bind vectors of different lengths?
Usually when you say you have two groups you imply that there may be many rows in each group, but you cannot compare more than two values for a given column so for now I will assume you only have two rows. I also assume that the attributes all have the same comparable type (e.g. numeric). m &lt;- as.matrix( DF[-1] ) ans &lt;- names( DF[-1] )[ m[ 2, ] &gt; m[ 1, ] ] DF[ , c( "Group", ans ) ]
Ummm... how to fix this? Should I fill the rest with NA ?
 library(tidyverse) qq &lt;- 1:998 ww &lt;- 1:921 qw &lt;- rbind( tibble(response = qq, group = "qq"), tibble(response = ww, group = "ww") ) bartlett.test(response ~ group, qw) 
My only intention here is to confuse a novice, but try this for fun: &amp;#x200B; df is your dataframe df\_odd = df\[,c(T,F)\] df\_even = df\[,c(F,T)\] This is based on recycling of vectors in R. 
This is exactly the answer I thought of when I read the question. It's succinct and clear. Tidyverse may have something "savvier" but may be unnecessary for what you're trying to do. 
So the reason you're having trouble is that R is good at comparing values in different columns, but not as good in comparing it in different rows. So you need to reshape your data frame to make it "long" instead of "wide". Here's the code: library(dplyr) library(tidyr) # Example data frame your_df &lt;- tibble(Group = c("A", "B"), Attribute1 = c(7, 3), Attribute2 = c(5, 8), Attribute3 = c(3, 9), Attribute4 = c(7, 4), Attribute5 = c(8, 8)) # Get column list reshaped_df &lt;- your_df %&gt;% # Reshape your data frame for column-wise comparison gather(key = Columns, value = value, -Group) %&gt;% spread(key = Group, value = value) %&gt;% # Apply your filter filter(B &gt; A) # Extract column list column_list &lt;- reshaped_df$Columns # Select only the columms you want new_df &lt;- your_df %&gt;% select(Group, column_list) Hope this helps. Let me know if it doesn't work or if you have any questions.
Gotcha!! Thanks so much!!
In the tidyverse you can use something like this: &amp;#x200B; df %&gt;% dplyr::filter(row\_number() %% 2 == 0) ## Select even rows df %&gt;% dplyr::filter(row\_number() %% 2 == 1) ## Select odd rows &amp;#x200B; &amp;#x200B;
OP wants columns not rows but you can do something very similar with dplyr::select instead.
You're right.
How about something like `cut(income, breaks=c(seq(0, 5000, 1000), Inf))`?
Awesome, this seems to have worked! Thanks a lot. Just curious, what does the ", Inf" option do?
It's not an option - it is concatenated (with `c`) to the sequence of `0, 1000, 2000, ... 5000` so that the category `(5000, Inf]` is also included.
Ah, I see. Much appreciated!
&gt; names( DF[-1] )[ m[ 2, ] &gt; m[ 1, ] ] &gt; DF[ , c( "Group", ans ) ] Hi, I am trying to make sense of your code to make sure that I understand it. I see that you converted the data frame to a matrix. What is the purpose or advantages of converting a data frame to a matrix? For the second line of code, I see that you are getting the names of the columns where Group B is bigger than Group A. Why does indexing with the matrix return those column names as opposed to only directly working with the matrix or dataframe? For the third code, you are subsetting a dataframe of DF where it contains the column Group and columns where Group B &gt; group A. For the third line of code, you are indexing a dataframe where So for the c("Group", ans) 
I don't have any experience with that particular package, but a fair bit with capture-recapture models (which is where your problem lies). Simply put, the model can't run if there are no recaptures, as the recapture rate is used for calculating abundance. The only information you have is the number of captures, which sets a lower bound for the abundance estimate. But how could you estimate the upper bound without further information? This type of model will not work with your data, I'm afraid.
 m &lt;- as.matrix( DF[-1] ) A data frame is a list of columns (where the columns are all the same length), while a matrix is a vector with dimensions. When you `[(,j)` index one column of a data frame you get a vector, but when you `[(i,)` index one row of a data frame you get a list of one element columns. With a matrix it doesn't matter whether you index columns or rows, you always get a vector. Try `str(unclass(DF[2,]))` and `str(m[2,])`. The `unclass` function causes R to show you what is underneath the class behavior of a data frame. ans &lt;- names( DF[-1] )[ m[ 2, ] &gt; m[ 1, ] ] The `names()` of a data frame is a vector of character strings. Check `str(names(DF[-1])`. Notice that this is the same as `str(names(DF[,-1])` because without the comma it indexes `DF` as a one-dimensional list of columns and with the comma it indexes it as a two-dimensional array of elements where the entire column is wanted, yet the result in both approaches is a list of columns. The `m[ 2, ] &gt; m[ 1, ]` uses the two (numeric?) vectors to produce a logical vector that is true wherever the second row element is greater than the first row element. Using that to index the vector of Attribute column names removes all of the column names that do not meet the criteria. DF[ , c( "Group", ans ) ] Indexing by a character vector in the second position (after the comma) maps the character elements to column names. Because the `ans` excluded the non-numeric `Group` column the `c()` function concatenated that column name to make a more complete set of column names before the indexing step.
The findInterval function can do what you're looking for
The comments on that post should have been clear enough. Of course, you always have the option of investing in hardware. My team at work uses AWS r3.2xlarge machines with 64 GB of RAM each, so fifty of those puts you somewhere in the ballpark of 3200 GB RAM and only costs $0.18 per machine per hour, for a total rental of $9 per hour for the whole cluster. So that's an option, *I guess*. Granted, you'd have to do everything in parallel and it wouldn't really be an *array* in contiguous memory, but it's still *technically* correct. However, harkening back to literally the very first comment on your post by neilfws: &gt; I'd suggest rethinking the approach - **do you really need to store every combination?** You can reduce your memory cost to constant size if you just consider one random lottery number at a time, then throw it out and move on to the next. Send it to a logfile if you really need to. Hard disk is much less expensive per gigabyte than RAM. As to your final question: &gt; Shouldn’t there only be 13,983,816 combinations ? How did you come to this number? There's seven slots of 49 indistinguishable numbers. You can have {1,1,1,1,1,1,1} then {1,1,1,1,1,1,2} all the way up to {49, 49, 49, 49, 49, 49, 49}. The number of combinations is equal to the base (49) to the exponent of length (7). This is much more than 13 million, it's 678 **billion**.
Forget about doing every combo with today's technology. There's a function I think its called sample(). You could randomly sample x number of items from the list of 50 possible numbers, set replacement=false so they so dont get repeated and generate however many samples you want and analyse those.
But I don’t want any replacement. From wiki: If the six numbers on a ticket match the numbers drawn by the lottery, the ticket holder is a jackpot winner—regardless of the order of the numbers. The probability of this happening is 1 in 13,983,816. That’s why I think there should only be 13983816. So will I be able to code all these combinations? Thanks for the reply !
I think they got 13 million from 49\^6
49^6 is still 13 **billion** though.
Consider that not all probabilities are modeled exactly like coinflips. Although it is intuitive to say that a coin has two sides and therefore an equal 1/2 (50% chance) for either outcome, it is not always the case. In your situation, it is not that there are 13 million unique combinations that could be coded up (besides the fact that 13 million rows of lottery numbers would still be some significant amount of memory, perhaps in the single-digit GBs and still not efficient for your use case). Rather, it is the case than in the 678 billion actual combinations, given a specific "winning lottery number", any random selection has a 1/13mil chance of overlapping at least 6 numbers. So, suppose 678 billion people all put in a unique lottery entry, 52 thousand of them will win the jackpot. &gt; So will I be able to code all these combinations? No.
Yes that fixed the problem. Thank you for your help.
Got cha, thanks a lot!
isn't 13,983,816 obtained from 49 picks 6? (i.e., 49C6)
Why are you sub-setting the data df in the kmodes command? If I'm reading this right, data has five columns and you are removing them all. Try data=data.
That completely makes sense! Thank you - it's my first time doing capture-recapture and I think I was naively assume I'd be able to get output I'd be sort of 'happy' with from every site for every species, but it's definitely been an experience :) 
Ding! Ding! Ding! Most lotteries that I know of have all numbers drawn from the same pool without replacement, which gets you to exactly this value. This is small enough to simulate in memory, but the question still remains... "Why?"
It's driven by licensing. See https://en.m.wikipedia.org/wiki/Permissive_software_licence as an example. Most R packages use one of the well known licenses and so what you can be can't do with it is well defined. A small number of packages on CRAN use rare or bespoke licenses that are likely to require legal interpretation and so in a commercial environment, it's generally best to avoid these. For 99.9% of things you'd do in R you're absolutely fine. 
Desktop link: https://en.wikipedia.org/wiki/Permissive_software_licence *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^242280
**Permissive software licence** A permissive software license, sometimes also called BSD-like or BSD-style license, is a free-software license with minimal requirements about how the software can be redistributed. Examples include the MIT License, BSD licenses, Apple Public Source License and the Apache license. As of 2016, the most popular free-software license is the permissive MIT license. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thanks for the quick answer. All the packages I'm interested in are under GPL-3 license, so I guess that, as long I don't sell the code but only the analysis, I should be fine?
&gt; In one of the documentation related to the packages it says that the work is not for commercial purposes Can you link to that? Such a license clause is fairly unusual for (free) R packages. For *most* packages, as long as you don’t redistribute or resell *code* based on the packages you can do whatever you want.
Yeah, you're fine to use GPL-3 packages for analysis in any setting. If not, R would be nowhere near as popular!
I'm pretty sure you can't say this and be GPL-3. I've seen packages state this, but they have wacky or bespoke licenses.
Well I've been quite approximative reporting that, because that is said in a book explaining methodologies and its applications with R. However, the related R-package(s) from the same author are all under GLP-3. Thanks for the help!
I think it actually relates to the book contents that was explaining, among other things, applications with that package (see mi other comment)
Getting plausible-looking output that doesn't *actually* make sense is another problem you can have, of course! ;) Good luck with the project!
This explanation was very thorough and easy to read through; I took down some notes. Thank you so much! When I unclass it, there were many 0s in addition to the values in the list. I guess you wanted it's a vector of values. 
df$swa_diff &lt;- df$swa[-length(df$swa)] - df$swa[-1]
My brain is still booting up; this is better.l (i.e. it will actually run) df$swa_diff &lt;- df$swa - c(0,df$swa[-length(df$swa)]
Do you know how to code this? Thanks!
Lmgtfy
you're right. that's my mistake i changed it to this `x=kmodes(data=data,nclust=6, nloops=30,seed=123121)` but still end up with the same error
Most R packages have a (GPL-3)[https://en.wikipedia.org/wiki/GNU_General_Public_License] license. Here's how it was explained to me: You can use these packages for to produce a product but the product cannot be the package itself. So you can do an analysis using ggplot2, but you can't fill a flashdrive with ggplot2 and sell it for a profit. 
**GNU General Public License** The GNU General Public License (GNU GPL or GPL) is a widely used free software license, which guarantees end users the freedom to run, study, share and modify the software. The license was originally written by Richard Stallman of the Free Software Foundation (FSF) for the GNU Project, and grants the recipients of a computer program the rights of the Free Software Definition. The GPL is a copyleft license, which means that derivative work can only be distributed under the same license terms. This is in distinction to permissive free software licenses, of which the BSD licenses and the MIT License are widely used examples. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Rlanguage/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
 # Get 3 integers from 0 to 100 # Use max=101 because it will never actually equal 101 floor(runif(3, min=0, max=101)) #&gt; [1] 11 67 1
This needs some nuance: you can sell the product, but you need to make the source code available.
In a tidy way I would do: &amp;#x200B; library(tidyverse) &amp;#x200B; new.survey &lt;- [survey.data](https://survey.data) %&gt;% group\_by(sex) %&gt;% sample\_n(680) &amp;#x200B; &amp;#x200B; You will get a dataframe with 680 male (all of them) and another sample of 680 different women. &amp;#x200B;
 df %&gt;% group_by(Date) %&gt;% summarize(DailyVolume = sum(Volume)) %&gt;% ungroup() %&gt;% group_by(Group) %&gt;% summarize(GroupShare_DailyVolume = sum(Volume)/DailyVolume)) %&gt;% ungroup() &amp;#x200B;
I see you used ungroup () both times, would you recommend doing that? Because the dataframe that I created above is based on other df's where I've grouped them, so could that be messing with the results here? 
the first ungroup() is necessary in this case, but the second one is just preference. The second one's mostly there if you wanted to continue creating further calcs with that df
If you're having issues, you can try ungroup() before running this code. that can "wipe" any of the group_by calls done earlier. 
Is it that you need to make the source available to those you sell it to, or that you have to make the source available to *anyone* who asks?
They're probably just saying that to cover themselves from legal liability. They don't want someone to use the package, then find that something was wrong because of a bug, and sue the authors for damages. This is perfectly understandable because most/all of these packages are just created by people in their free time for no compensation.
Ah, I see. I'll try it out, thanks for the reply!
For some reason, right now I'm getting an error which says it can't recognize the column "Group". So it finds the daily volume and then stops there 
That's a good question. Those you sell or otherwise distribute the software to.
Is it necessary though? I am under the impression that any `group_by` call overrides earlier groupings...
As long as you are the sole developer you can do anything you want. GPL-3 is there to bind other people that uses your code. You can distribute to private individuals under a different license of you want.
Not all R packages are gpl compatible and they don't have to be. R is released under gpl but that only binds you if you make modifications to R and redistribute it. Your own code written in R language generally isn't considered a derivative product and R consortium clearly state they don't consider it so
This is hard to do without knowing what exactly you're up to. Could you try to provide a [reproducible example](https://reprex.tidyverse.org/)? 
Surely he would want to set replace =FALSE
Are you just typing code into the console or are you writing scripts (text files of code with a .R extension)? You should be writing scripts If you want to weave code and human-readable text together, you want to be working in R Markdown (.Rmd) documents. Are you using an IDE like RStudio? Or good editor like Emacs or Atom?
Use the Script window in RStudio.
Are you using RStudio? If not, you should be. It allows you to have multiple panes, which can include one for your script (a .R file) and also a command window. You can also use it to create a R Markdown document, which might be what you really want to do anyway.
Hey, don’t worry I’ve been where you are. Try R using R studio and saving the workspace as a project. Essentially, it just makes sure that all the files you create while working on an analysis live in the same spot, and reopen when you close R studio. Also, you may find R markdown useful for your project. It allows you to write code and text side by side. I used it all the time in graduate school for homework’s. This cheat sheet has everything you need: https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf
You need to better explain what you mean by "R updates automatically". There is no inherent difference between working with Java, Python and R (except you need to compile Java code). You write code and run it. What you say suggests you are doing something you shouldn't be doing. You can execute your scripts using Rscript from command line, using source from another interactive R session or fancy run code buttons that rstudio has
Yeah I am. So I'm assuming I write anything I want to test in the command window and write my correct code in the script?
I was typing in the console and yeah I'm using RStudio
Yeah what I'm gathering from this thread is that I was writing my code directly into the command line, rather than writing in scripts.
Thanks!
Rstudio can indeed make your life easier. Letting you edit scripts and keep an interactive session open at the same time. It's like Spyder for Python if you used it.
Use RStudio. Write your code as script files rather than the interactive console (although the console is useful for trying things out and debugging). In RStudio you can run a single line (or a selection) using ctrl + enter, which kind of gives you the best of both worlds in terms of interactivity vs scripting. If you're struggling with just getting started on a project, I've found the Project Template package (http://projecttemplate.net) really useful; it gives you a folder structure and some useful config and data caching options which makes it very easy to get started just by filling in some blanks basically, and helps to keep your research easily reproducible. 
[This is fundamentally wrong](https://old.reddit.com/r/Rlanguage/comments/63ha4q/gpl_license_and_r_programs/). The issue isn’t the license of R. The issue is the license of the *R base library*, which every R package links against (you cannot write R code without using base code!). The FSF is completely clear on this (and they consulted lawyers): If you link against runtime libraries under GPL, your own code must be GPL-compatible (if you distribute it). &gt; R consortium clearly state they don't consider it so Can you provide a link for that? The only official pronouncement I’ve heard comes from the R *Foundation*, and they [refuse to clarify the issue](https://stat.ethz.ch/pipermail/r-devel/2009-May/053248.html).
But if you don't call group_by() again and move on to manipulate data unwittingly, it will provide erroneous calculations.
How does this work in practice? Let's say I use ggplot2 in a newspaper article - then what? 
You are not selling a product, then. You are selling output. By product, it means software. If you take and alter the source code of ggplot and sell that, you have to provide the source code. You do not have to provide source code for plots you make and sell.
How do I open it?
Usually I open a second script to do some testing and than paste it into the 'real' script. But yes, console can work fine one just single lines
Apologies, I'd got mixed up about what product meant here. 
No prod, the wording was a little unclear.
The help page for Extract (`?Extract`) provides some insight. In the section *Recursive (list-like) objects* it states: &amp;#x200B; &gt;When $&lt;- is applied to a NULL x, it first coerces x to list(). **This is what also happens with \[\[&lt;- if the replacement value value is of length greater than one**: if value has length 1 or 0, x is first coerced to a zero-length vector of the type of value. &amp;#x200B; Therefore your second and fourth examples work. The `[[&lt;-` replacement coerces the NULL object to a list. In the first and third examples you're trying to replace 1 element with multiple elements. This will always produce a warning. &gt; x &lt;- 1 &gt; x[1] &lt;- c(1,2) Warning message: In x[1] &lt;- c(1, 2) : number of items to replace is not a multiple of replacement length The help page for NULL (`?NULL`) states that "Objects with value NULL can be changed by replacement operators and will be coerced to **the type** of the right-hand side." Notice it only says type, not length. It appears it will be coerced to length = 1 when the `[&lt;-` replacement operator is used. Hope that helps. &amp;#x200B;
I also have been thinking about this...what some instances where you want to convert the dataframe to a matrix? And why? When is a matrix workable over say a dataframe?
Get on RMarkdown. If you need to test things, you can make a chunk for testing.
I believe I was thinking of [this](https://cran.r-project.org/doc/FAQ/R-FAQ.html#Can-I-use-R-for-commercial-purposes_003f) when I said that but that is fairly vague. It only acknowledges existence of packages with different licenses without any endorsement. There is a [fairly recent](https://www.google.com/amp/s/www.r-bloggers.com/how-gpl-makes-me-leave-r-for-python/amp/) review of this whole issue from a company point of view and indeed things seem murky as hell. We'll probably have to wait for a lawsuit. I find it unlikely that lawsuit will be about any R software though. 
Gonna put another voice in favor of Rmarkdown
Why toss out, rather than treat as a stratified random sample and adjust?
This is the correct answer
"New Script," under File. Also, Google is your friend!
Discovering statistics using R by field miles and field and R for Data Science by Hadley wickham. 
I found the [documentation](https://github.com/cran/klaR/blob/master/R/kmodes.R) and it looks like you have to specify modes in the function or it will throw that error. It either has to be a number of modes or a data frame with the initial set of modes. Do you know how many modes you want?
R for Data Science by Wickham is a fantastic, very approachable way to get going quickly with R. Highly recommend it. 
To tag on to this, if you want to do the console route, you can go into History (which is usually located next to Environment), click on your code and To Console or To Source (whatever document is currently open).
A data frame can have different types in each column... a continuous numerical measurement adjacent to a discrete status column (I.e. "Ok", "BrokenSensor", "EquipmentOffline"). A matrix cannot. In general, if the analysis you plan to use involves (or could involve) matrix multiplication, then it is a good candidate for being a matrix. If each column generally represents something distinct from the other columns then a data frame is better.
Check your R version (read what is printed when you start R, or use the `sessionInfo` function). Some packages are only built for the current version of R.
Lol, thanks!
A few I really like: * An Introduction to R * R in a Nutshell * A Handbook of Statistical Analyses Using R * An R and S-Plus Companion to Applied Regression * Practical Statistics for Data Scientists * Introductory Statistics with R * Data Analysis and Graphics Using R: An Example-based Approach
So I would probably do that by creating a weight variable and assign all the males a value of 1 and all the females a value of 0.8292683 (680/820). Is that what you mean?
Rather than a loop, try map from the purrr library. You can insert dummy arguments into the function (.) to represent the value that is changing and set the data and carbn whatever you want. Just remember to use a tilda (\~) before the function so it treats it as a formula. map(list, ~ func(data = mtcars, cyln = ., carbn = 4)) &amp;#x200B;
Gotcha. Thank you. &amp;#x200B; Your explanation above was very thorough as well! Much appreciated. 
Yeah, something like that. I don’t have the stats models in my head, but there are ways to account for the difference that work similarly. The key is that your weights need to match the population your are describing. Are there actually 50% males in the group your are looking at? (Confusing variables: why were the response rates different, etc)
Thanks! I'll try it when I get to work tomorrow and let you know how it goes with the real function. 
Thanks! That was a misconception I had - that being GPL basically meant you had to give away your software for free.
 5. First thing you should know that practically everything in R is a list. List is R's best and possibly most confusing data structure. It is what experienced programmers love and newbies hate. 1. Understanding the behavior requires one to understand the function `[&lt;-` and `[[&lt;-` which I don't think sane new R programmer should try to do UNLESS they are trying to code golf. Fun claim, `[&lt;-` can save many bytes when code-golf. It is a index, store, AND return function. 3. Double bracket `[[` in R extracts the object stored inside the list indexed by an index or name. This returns an object. When you use double bracket, you are telling R's `[[&lt;-` function that you want to store the object on the right side into a sublist with the name 'best'. R does not care about the right side and will not complain or warn if you supply a legal R object. This could be ANY R object. 3. Conceptually, single bracket [ in R extracts the sub-list inside of the list indexed by an index or name. This returns a list containing an object(s). When you bracket assign an object of length(object) &gt; 1, this results in the first sub-list of that object being stored inside your indexed slot (which always has length(x) = 1 since it is a sub-list). 5. Try the following, this will store the first column of the built-in dataset iris into the first sub-list of the list node. # Note the behavior node &lt;- NULL node['best'] &lt;- iris # Also check the class of these to understand the difference between [ and [[ class(as.list(iris)[[1]]) class(as.list(iris)[1]) 
 Look at this behavior node &lt;- NULL node['best'] &lt;- iris Also check the class of these to understand the difference between [ and [[ class(as.list(iris)[[1]]) class(as.list(iris)[1]) 
That's how I do it. You can also press Control + Enter to execute the current line or highlighted lines in the script editor window if you don't like going back and forth.
Yeah, you can divide this into three subproblems: 1. Transform each list into a matrix where the rows are each consecutive three elements in the list 2. Test if each row consists of n, n+1, n+2 3. Return true if any of these rows are consecutive &amp;#8203; toMatrix &lt;- function(l) { return(matrix(cbind(l[seq(1,length(l)-2)], l[seq(2,length(l)-1)], l[seq(3,length(l))]),ncol=3)) } connected &lt;- function(l) { return(l[1]+1==l[2] &amp; l[2]+1==l[3]) } anyConnected &lt;- function(l) { return(any(apply(toMatrix(l), 1, connected))) } l &lt;- c(1,2,3,4,5,7,9,11,12,13) anyConnected(l) You can then apply this to the list of combinations.
How would you handle 3 5 9 8 9 45 32 ? They are adjacent but not changing monotonically.
I assume your vectors are monotonically increasing (otherwise use "sort" on the row before checking) 7 is the length of the vectors, so subtract the sequence from 1 to 7. Then use "table" to see if an object appears several times. Then see if max of that table is greater than 2. In short: v=c(1,5,6,7,9,10,12) w=c(2,5,7,8,11,22,33) max(table(v-1:7))&gt;2 #TRUE max(table(w-1:7))&gt;2 #FALSE
I'm pretty new to R so I don't have experience setting that up, but I'll try to do so today. 
Cheers - I've got Microsoft R Open 3.3.2 which I think is up to date 
I uploaded the script with previews to Github, here it is: https://github.com/izzysh/liquidity/blob/master/ILLIQ.R 
Sorry for the late reply, thank you this worked perfectly!
For now, get in the habit of not writing *anything* into the command window manually (you can relax this slightly but in general I’d stick to it). Everything goes into the text editor. If you want to execute individual expressions, RStudio (or whatever other IDE you use) allows you to send individual lines or highlighted expressions to the commmand window (in RStudio it’s Cmd+Return).
&gt; practically everything in R is a list Absolutely not. In fact, almost everything in R is a *vector* (with few exceptions). Lists are special kinds of vectors, but not every vector is a list. And then there are other things which are either vectors or lists (and thus also vectors), and which have additional classes attached to it, such as `data.frame`, which is a list of S3 class `data.frame`, or `table`, which is a non-list vector of S3 class `table`. Confusingly, the `is.vector` function only returns `TRUE` if the object in question is a *bare* vector (i.e. a vector without any attributes other than names), whereas `is.list` returns `TRUE` for *all* lists, including those with further attributes/classes.
I use emacs and ESS which can be run through the terminal, which I've also done in the past (it's emacs). I don't know how you set breakpoints directly but I typically use `debug()`, `browser()`, or `trace()` (to debug on error). It's been a very long time since I used gdb but I think the key commands are similar (`n` for next, `c` for continue, etc.).
So you need to store all unique six number combinations drawn from 49 numbers (1-49)? &amp;#x200B; One way to do it is with **arrangements** package: `library(arrangements)` `v &lt;- combinations(49, 6)` `nrow(v)` `[1] 13983816` Resulting matrix is 320.1Mb in size
Got it! So I don't exactly stlill understand what you're rying to calculate, but going off of: Volume &lt;- Volume %&gt;% group_by(Date) %&gt;% summarize(DailyVolume = sum(Volume)) %&gt;% ungroup() %&gt;% group_by(LiquidityGroup) %&gt;% summarize(GroupShare_DailyVolume = sum(Volume)/(DailyVolume)) %&gt;% ungroup() head(Volume) For These long chains of pipes, troubleshooting can be a little opaque. Wht you need to do is to just take chunks of it. I like to check on it after each actual data alteration (mutate, summarize, etc). So let's run this: Volume1 &lt;- Volume %&gt;% group_by(Date) %&gt;% summarize(DailyVolume = sum(Volume)) now, `head(Volume)` probably isn't giving you waht you expect in this case. You'll see that the dataframe at this point doesn't have a column named Liquidity Group. There are 2 options here: * If you want the daily volume calculated, but put as a column in the original large dataframe, try `mutate` instead of summarize`. * If you want the daily volume calculated as a function of Date and Liquidity Group, try `group_by(Date, LiquidityGroup)`. I believe the former is what you want but I can't say I 100% undersand what you're trying to do. 
Pretty sure browser() does what you want
MRO has a default repository that is frozen in time. You either need to follow their instructions for choosing a newer repo date or change your repo list and `.libPaths` to a standard CRAN configuration per the R Installation and Administration Manual in order to "see" newer packages. However, R 3.3.2 is rather old... you may need to update that as well in order for newer packages to even work at all.
Check out rstudio connect. It won’t be cheaper than shiny server; however, it is amazing. It’s a publishing platform for many shiny apps and its made for teams with their own infrastructure. Also you can publish .Rmd files and schedule their refresh (hourly/daily) which is honestly gamechanging. The cheap option i can think of is to use the open source rstudio server to host the shiny app, but i think you give up a lot here
`?embed` though I think there is a more efficient way than this proposal using `diff`.
There is a package called Rinno which let's you build everything into a self-contained app. Im too much of a n00b to make it work, but others have had success, so it must be possible! 
I think I got it to work by splitting it up, thanks for the help! 
I have done this recently for my company. What we did was installed shiny server (free version) on an AWS EC2 instance, transferred the app to the server using a packets bundle and ran it from there. Managed user authorisation by limited access to the server’s IP address to specific employee IP addresses. Probably not the most secure method, but it works!
I have deployed Shinyproxy apps for small scale internal use. Does what it says, though its a bit finicky - we're using nohup to keep the app running. But sometimes it just dies and then I have to restart the instance. But I have like zero sys admin experience, so maybe that's the problem. Right now access is controlled by hardcoded passwords in a config file. And we'd like to move to using something like Keycloak to control access. I haven't tried the LDAP thing. &amp;#x200B; Deploying apps is pretty simple once you get used to it. You copy the app folder to the server. Then update the dockerfile. Then build/rebuild the docker container. Well thats how I do it, probably not the best practice.
I'm extremely happy with Neovim and the Nvim-R plugin, but it might not be what you're after.
Ummm... thank you for the reply. But if I let... v=c("1","2","3","4","5","6") max(table(v-1:6)) &gt; 2 I result in FALSE statement
Does it? I copy-pasted what you wrote into an R prompt and it returned TRUE. Also... if you define v the way you did, it is *not* a data frame. I'll give you the partial results after each step, please compare and tell me what happens. &gt; v=c(1:6) #v is a data frame &gt; max(table(v-1:6)) &gt; 2 [1] TRUE &gt; v [1] 1 2 3 4 5 6 &gt; v-1:6 [1] 0 0 0 0 0 0 &gt; table(v-1:6) 0 6 &gt; max(table(v-1:6)) [1] 6 &gt; max(table(v-1:6)) &gt; 2 [1] TRUE &gt; By the way, it still works even if it *is* a data frame: w=data.frame(v) max(table(w-1:6)) &gt; 2 still returns TRUE. 
I did the same for university (internal, small-scale use). It was acutally very easy to deploy the LDAP functunality. We used shinyproxy on a redhat server. 
This worked great! Thanks!!! 
You are right! I code something differently. Oops! But how can I remove these row with TRUE statement? right now I have: if (v = "TRUE") { xy=xy[-v]} But I got the Error: unexpected '=' in "if (v=" :(
Hi... please don't completely haul over the content of something that has gotten a reply. Just add what you't like to say with "*edit:" or something. My reply looks really weird now. To answer your question: It depends on how your data is structured. Based on something you wrote earlier, I assume that you have a data frame of them. Let me call my data frame DF and fill it with random sequences, just to try. As tables work differently on data frames, i unlisted each row to return a vector set.seed(2) DF=data.frame(matrix(nrow=100,ncol=6,data=sample(1:10,600,T))) entries=1:100 for(i in 1:100){ entries[i]=max(table(sort(unlist(DF[i,]))-1:6))&gt;2 } DF[!entries,] What I did, was I constructed a vector "entries" the same length of the data frame, and put a 1 if the row has an undesired sequence and a 0 otherwise. Then I want to invert that vector, so I know which rows to *keep*. And the last line then keeps all desired rows. 
I advise you to use a different symbol than "v" in this case, as you already used it for the sequence earlier. Please see my other comment for a possible solution to your problem. Now, concerning the code you wrote here, several problems occur. Firstly "TRUE" is a string. If you want to compare "v" to a logical value, you'd compare it to TRUE without the quotation marks. Second, if you want to find a TRUE/FALSE value by checking if 2 obects are equal, you need to put **2** equal signs between them: 2=3 will return an error, as "=" is only used for assignments in this context, whereas 2==3 will return FALSE. Thirdly, if you compare against TRUE, you can just put the statement into the if-brackets, in other words if(v==TRUE){ will have the same effect as if(v){ 
You might want to ask https://security.stackexchange.com. Using a package like rvest should be safe since I can't think of anything that might harm your device. But I am no security expert.
I fix it! Sorry about that and thanks !
I set up a R Studio and shiny server on a VPS (shameless plug for [figured.io](https://figured.io), it's not much but I have fun with it). I've mostly just tinkered with small demo apps - but have had success with a variety of project types and data sources. Im also a noob, so its definitely possible to figure out if you're moderately savy. If you can code in R, you probably have the technical capacity to do this as well. &amp;#x200B; The first time I set up an R Shiny server, I did it using this guide [https://www.simoncoulombe.com/2018/05/protected\_free\_shiny/](https://www.simoncoulombe.com/2018/05/protected_free_shiny/). It details how to use a free micro instance of google's virtual machine to host a shiny server. With your level of concurrent users, seems like that would be doable. I now code apps in the RStudio Server, and deploy them directly in R Shiny. No need to spend 10K for what it sounds like you'll be using it for. Digital ocean has some decent guides on setting up SSL, and somehow I managed to figure out how to include google analytics. Would be happy to pm my code over, though I think some examples are on my github.
I‘m developing a software for big data analysis using R and shiny for my company. As we also have confidential data sets, we decided to host our own MySQL and shiny server. The problem with shiny server was, that only the pro version allows to create a new process for each session. We didn‘t want to pay for shiny server pro, so I wrote a small php script, which launches a new r session, calls runApp for our app and redirects the user to his own session. All session are protected by custom session cookies. This solution is a little hacky, but offers complete control and is a 100% free. I hope this helps you.
It’s great! Another alternative is DesktopDeploy, but these days RInno seems to have way more functionality. 
Thanks a lot. Will look into it. The data needs be be on our own server, we have a linux server running in house 
Same setup here.
Oh, ok cool, Do you suggest reading some tutorials on how to go about it? I am not very familiar with server configurations, I can forward some tutorials to my company IT department 
RemindMe! 1 week "R Shiny deployment question"
I will be messaging you on [**2019-03-13 21:15:18 UTC**](http://www.wolframalpha.com/input/?i=2019-03-13 21:15:18 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/Rlanguage/comments/axzh2j/r_shiny_deployment/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/Rlanguage/comments/axzh2j/r_shiny_deployment/]%0A%0ARemindMe! 1 week ) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
The basic configuration is pretty simply. You have a number of ports which can be used for a shiny server. Lets say port 3900-3999. Additionally you have a database, to mark ports as used. When your user visits your port distribution page, a simple php script searches the database for the first free port, launches a new session, assigns a unique key to the port in the database and stores the key as a cookie in the users browser. Afterwards, the php script redirects the user to the newly launched shiny app. The shiny app first compares the key in the cookie to the key in the database. In case they don‘t match, the user is redirected back to the port distribution page. Otherwise, the user is able to perform data analysis. Afterwards, the session must be terminated and the port marled as available in the database.
I really don't know about econometric packages so I don't think I can help you with that one. For exporting results, you can used knitr/markdown to produce nice looking PDFs, which will format the tables. I think there is an option to produce to a latex document instead of compiling the latex and giving the the whole document, so you could copy the table code from that. Stargazer is also good extension for publication style tables. Knitr/markdown can produce: html, slides, word and latex. [https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) [https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf](https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf) [https://yihui.name/knitr/](https://yihui.name/knitr/)
We recently deployed a shiny app for our company, and it's web hosted, just using docker containers. [Check out this docker container](https://hub.docker.com/r/rocker/shiny/) There's some setup to it, to get all of the base packages needed for other R packages you might want to use. Mind you it is compiled in Linux. In my experience, setting up libraries for it is a bit more fumbling than doing it Windows with RStudio. On that docker container, install the [apt-transport-https package](https://packages.debian.org/jessie/apt-transport-https), and you can setup what we call "poor man's security". No user registration or password changing. You generate passwords [here](http://www.htaccesstools.com/htpasswd-generator/) and add them to a list (we use a separate docker container to do the authorizing). Not going to go into full details on the whole setup, because there's a lot, but... We spend $20/mn on a single EC2 instance that runs *many* docker containers, hosting a range of apps including Shiny. Highly cost effective, if high maintenance, solution. Plus access anywhere on the web behind user/password.
I really like the package huxtable for outputting regression results. It’s a bit more flexible than stargazer and can write to excel in a format that makes creating work product efficient at least for me.
Many thanks for this. Gonna try these out!!
Sorry for another question :( I tried your method and it works totally fine, but I now have &gt;74560 rows, which takes pretty long time to loop. (I have been waiting for more than an hour) Is there a faster way without looping? maybe?
well... it shouldn't take that long, unless you're on a rather slow machine. Remember that you don't need "sort" if your rows are already sorted. Maybe try it on a subset of your data first to see how long it should take. There are differnt ways to code this loop, but I can't think of ways that are **faster**, as you'd always have to perform the same calculations for each row with my method.
Thank you! I downloaded the latest r package and it is now working. Wrongly assumed I had the most recent one. 
One option is to host the free version yourself, have it only listen locally, then have Apache or any other webserver just act as a proxy for that local-facing Shiny server. Then you can do access control using the standard web-facing webserver. This blog has a walk-through using Nginx: [https://www.datascienceriot.com//r/shiny-nginx/](https://www.datascienceriot.com//r/shiny-nginx/)
Maybe the answer here will help? https://stackoverflow.com/questions/26219501/mutate-multiple-columns-in-a-dataframe
Thank you for the tip! I will start fiddling based on these suggestions and see what I can muster (and it's always nice to see another lost Stata user on stack exchange, doing what I'm doing)
R For Data Science is fantastic - it's really meant for beginners with minimal programming experience but still provides a comprehensive introduction to the language. You can find a hard copy on Amazon but the author also hosts it in [its entirety on his website](https://r4ds.had.co.nz/).
The accepted answer at the following link is really good: stats.stackexchange.com/questions/237220/mclust-model-selection The authors have intentionally left a negative term out of the BIC formula, so you are looking for the highest BIC value -- still worth looking at the whole SO answer for details (and the help file). Welcome to the club of confusion when seeing it for the first time and good luck with your analyses!
You can explicitly pass them in by naming them or explicitly pass them in by including `...` at the end of your subfunction where you can then call them by the parameter name in the main function. &gt; main &lt;- function (a) { &gt; a &lt;- sub(b=a) &gt; return(a) &gt; } &gt; &gt; sub &lt;- function(b) { &gt; b &lt;- b + 1 &gt; return(b) &gt; } &gt; &gt; main(1) &gt; 2
this might be easier to to in base R for(index in 1:152){ hosp_diag_linked[paste0("gap",index)] &lt;- hosp_diag_linked[paste0("hosp",index)] - hosp_diag_linked["collected"] } dplyr can get tricky when you're not referencing columns specifically by name. There are ways to do it, but base R is easier at least in this case.
Thank you for your suggestion! I spent an embarrassing number of hours on this today and I'm loathed to open it at this minute, but I am monitoring replies and definitely will try this one tomorrow, hopefully I hit success before I lob the laptop out the window.
Good luck! One general piece of advice I would give, is that using anything with [] inside a dplyr function to reference something within the table probably isn't going to work unless you know dplyr on a more advanced level and are using it for a specific reason.
I'm not advanced, and thank you for that advice and for steering me away from future headaches! I'm very much 'self-taught' and muddling through the best I can with tutorials and forums. I will get there with tenacity and the kindness of strangers I'm sure, thank you again!
Thanks! This is very helpful. Particularly that last bit about BIC having different definitions in different contexts made a lot of sense. &amp;#x200B; Also, do you know anythign about my second issue, the ever-growing BIC? From me it looks like BIC will continue to increase asymptotically the more profiles I add. Of course higher number of profiles means higher chance for non-convergence, but apart from that: Won't log likelihood, BIC, entropy etc. always be biased towards higher k solutions? 
You could turn that into a lapply and do.Call('rbind') combo to speed it up too :) 
Every language is different. Some things that you know how to do easily in one, might not be so straightforward in another. You might want to look at [swirl](https://swirlstats.com) it's an R package that contains some interactive courses on various topics in R. The R basics and data cleaning courses really helped me learn the basics of R and dplyr. A lot of learning R or any programming is trial and error.
Yes that's definitely what I'm finding. Stata is so..I don't know, obvious? I assume that's why it appealed so much to me! I told it to do a thing and it did the thing, and seemed to be more user-friendly. That's probably just because it was the first thing I learned, though, and I feel a fondness for it. R is not so obvious to those of us like me who are probably not so flash at coding (understatement) but wow, so much more powerful, flexible, just...better. I'm determined to get there. I still do it all in Stata first so I'm sure I'm doing it 'right' and then replicate it in R so I can learn the language. I'll make the leap one day when I'm much, much better! I've joined a local R-ladies group and am trying to get into a community and not shame myself so much in public forums...I promise.
If you just want to plot the number of observations that fall into the categories you could use `geom_count()`. The output is a little plain unless you style it a bit. Or if you are more comfortable with creating counts by the two variables yourself, you could use `geom_tile()` and specify the fill to be your count measure - this would be my go to approach. Let me know if you want more details on either of these approaches!
Thank you for the reply! I'm not concerned with designing the output tbh I just want to have a finished project lol. Could you provide more details on both of these approaches? Thank you so much! Also, is it possible to fill a bar with data? 
I'd like to think R is relatively intuitive, but it certainly is different from stata and some things will require a bit different approach. Never be afraid to ask questions. Also bookdown.org has some very good free books on R. I'd really recommend the ones by Hadley Wickham (he had a hand in writing dplyr, and pretty much everything involved with R).
Here's some dummy data that I'll assume is similar to what you have. ``` df &lt;- data.frame( income = sample(1:5, 100, replace = TRUE), party = sample(1:3, 100, replace = TRUE) ) ``` 1.) `geom_count()`: ``` ggplot(df, aes(income, party)) + geom_count() ``` 2.) `geom_tile()`: ``` library(dplyr) count_summary &lt;- count(df, income, party) # use count function from dplyr package to aggregate - this creates new column called "n" ggplot(count_summary, aes(income, party)) + geom_tile(aes(fill = n)) ``` 3.) Colored bars: This might be my favorite, depending on the goal of the plot you might think about changing which is the fill variable. ``` ggplot(df, aes(party, fill = factor(income))) + # need to change income to be a factor to get colors to work geom_bar() ```
Thank you again, you have been incredibly helpful and made me feel better. I'm not throwing shade on the R community (or any other), but I'm hesitant at times to post publicly here (or on other forums) - mostly because there are lots of *very* knowledgeable people and it's hard to reach out and look stupid I guess. I work in an area where people are almost all SPSS-users, and I can't even bounce things off other people. I have found it much less scary when I just got on and tried it out, so I'm not put off by realising how little I know! It's been very forgiving in many places but just really tricky in others. I'll check out bookdown (I've bookmarked it) and keep forging ahead. Even being super new I'm very impressed by what I can potentially do for me when I learn more!
 hosp_diag_linked[paste0("gap",1:152)] &lt;- hosp_diag_linked[paste0("hosp",1:152)] - rep(hosp_diag_linked["collected"],152) I think this may be even faster, but I don't like it.
Thank you so much for this! This helps a TON. I tried to do the geom\_tile() and the output came out like this: [https://imgur.com/a/SyTTZTl](https://imgur.com/a/SyTTZTl). On a previous graph I tried to make I tried to change the title of the scales in the x-axis and it was unreadable for some reason. 
 data %&gt;% group_by(trialn) %&gt;% arrange(timestamp) %&gt;% mutate(bin = floor((timestamp - timestamp[1]) / 4) %&gt;% group_by(trialn, bin) %&gt;% summarize(...) I think something like above should work (I'm assuming your timestamps don't start at 0, so the first in your starting time). I am also unclear on what you want to do with the binned data so I've just left it as summarize(...)
I just did the last one and you're right, it is the best! Would you be able to explain how I can change the legend and the scales(I'm not sure if that's the right word for it). Right now it shows, 2.5, 5.0, and 7.5. Thank you!!
I suspect the reason the `geom_tile()` looks like that is probably because those middle combinations have n = 0. Before creating the plot you could subset `count_summary &lt;- count_summary[count_summary$n != 0, ]` to remove those. To change the labels on the ticks of the x-axis you should be able to use `scale_x_discrete(labels = your_label_vector)`. Here's an example of that. Note that for this to work the party variable also has to be a factor. ggplot(df, aes(factor(party), fill = factor(income))) + geom_bar() + scale_x_discrete(labels = c("Dem", "Rep", "Other")) To change the label for the x-axis use `labs(x = "New label")` ggplot(df, aes(factor(party), fill = factor(income))) + geom_bar() + scale_x_discrete(labels = c("Dem", "Rep", "Other")) + labs(x = "Political Party") 
Okay so this creates a bin of 0s that specify the 4 seconds gone by... &amp;#x200B; Ideally based on that, I would like to select all rows and columns within the dataframe that are within those 4 seconds and save them separately for me to plot later. Is there a way of using the filter() function in dplyr within summarise to do this?
You are incredibly helpful considering you do not know me at all. I really, really, really appreciate all of this. I attempted to change the ticks from the decimals to the party names and now it does not show anything. It only shows the label "party". I used this code: ggplot(df, aes(factor(party), fill = factor(income))) + geom\_bar() + scale\_x\_discrete(labels = c("Dem", "Rep", "Other")) to make the changes. In addition to that, I converted party into a factor using as.factor(df$party)
It is best to avoid using `data` as a variable name in the global environment since that is the name of a base R object (a function). Create a grouping column with integer division and split the data frame using `nest` from the `tidyr` package: library(dplyr) library(tidyr) dta &lt;- ... # your data frame dtaL &lt;- dta %&gt;% %&gt;% mutate( grp = timestamp %/% 4 ) %&gt;% nest( -trialn, -grp ) The `dtaL` result will have 3 columns... the two key columns and a column that is actually a list of data frames. You can use `lapply` on `dta$data` to process through individual data frames, or you can access the `i`-th one using `dta$data[[i]]` (e.g. for testing your processing code).
This might be because your variables are being treated as numeric so the plots are displaying actual numeric cuts. Since these variables are categorical measures try changing them to factors (factors are how R represents categorical measures). My above answer does this, but let me know if you still need help.
Thank you! I realized I forgot add the factor(party) into my line of code. It now looks wonderful. I can't thank you enough for your help. You are a lifesaver. Hopefully I have all the information I need now to do well on this assignment. I have to make 5 plots all together D:
Yeah hard to read, agree. Lapply gives a happy medium I think? 
I have one last question, how can I change the labels for my legend? Right now it just shows the numerical responses. I want to replace it with the different income levels. Thank you! 
I tend not to use the apply functions, but wouldn't sapply or vapply be more appropriate? R lists are horribly inefficient when vectors are an alternative.
You could use `scale_fill_discrete(labels = your_labels)`. Here's one plot with all the bells and whistles when it comes to labels: ggplot(df, aes(factor(party), fill = factor(income))) + geom_bar() + scale_x_discrete(labels = c("Dem", "Rep", "Other")) + scale_fill_discrete(labels = c("High", "Med", "Low", "NA", "Other")) + labs(x = "Political Party", y = "Count", fill = "Income Range") 
For future reference, this can help: [https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf](https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf)
Okay!! I will make some subset, Thank you so much !!! :)
Thank you! This probably says more about me than the actual cheat sheet but I did look at that earlier but it wasn't as helpful as I had hoped. 
Thank you! This is extremely helpful. I really cannot thank you enough. 
You're welcome! Good luck on your assignment!
This is great, thank you for the pointers! :) 
You have more certainty with lapply, as sapply does lapply and then attempts to stick the lists together into a data.frame. The orientation of that data frame is unpredictable which is really annoying. A lot of us that use apply a lot as it's waaaay faster than looping in R mainly use sapply. If I really must loop I use the foreach package which is a bit slower than lapply, but much faster than for() {}. Either way good luck OP! 
Loops have gotten pretty good in more recent versions of R I think. There isn't much difference between a properly structured loop and an apply function at this point. However allocating memory in each loop iteration, which I did in my example is not the most efficient.
replace for loops with apply()
Do you have the same assignment as this person? https://www.reddit.com/r/Rlanguage/comments/ayckov/two_discrete_variable_plotting/ 
Is this similar to what you wanted? library(tidyverse) # raw data data &lt;- data.frame( party = sample(c('Party A', 'Party B', 'Party C'), 100, replace=TRUE, prob=c(0.45, 0.45, 0.1)), education = sample(c('High School', 'Bachelors', 'Masters', 'Doctorate'), 100, replace=TRUE, prob=c(0.2, 0.4, 0.3, 0.1))) # clean data - show % than raw counts data &lt;- data %&gt;% group_by(party, education) %&gt;% tally() %&gt;% mutate(percent = n / sum(n)) # plot ggplot(data, aes(x = party, y = percent, fill = education)) + geom_bar(stat = 'identity') + scale_y_continuous(labels = scales::percent) + labs(title = 'Percent Distribution by Party and Education') + theme(plot.title = element_text(hjust = 0.5)) + theme_minimal() 
Before thinking about fixing the loop, I don't think your code is doing what you think anyways. When you refer to things like "i", you are referring to a string with one character in it--i. You are not refering to the variable i which contains an integer.
what are you even trying to do? First of all, the command in the middle of the loops does not depend on the loops and indices at all, since you're comparing the entry of the variables in subI to the strings "i","j" etc. so, first of all, you need to get rid of the quotation marks around the indices in the central line. Secondly... change a loop into a faster code? What are you trying to do? It looks to me like you're trying to eliminate all lines from the dataframe where the first variable is a 3,4 or 5, the second one is 4,5 or 6, .... and the last one is 7,8 or 9 at the same time. Is that right? so you want to keep a line like this: 1,4,5,6,7 and delete a line like this 3,6,6,6,9 In that case, instead of looping, you could use the %in% command. Try this, no loops: subI = subI[! (subI$X1 %in% 3:5 &amp; subI$X2 %in% 4:6 &amp; subI$X3 %in% 5:7 &amp; subI$X4 %in% 6:8 &amp; subI$X5 %in% 7:9),] 
That's funny. OP may indeed have the same assignment
I think you need to anchor your timestamp column so that for each ppid_trialn the range goes from 0 to 4 like you describe. Then the lines will get stacked over the same x-axis range. If you are familiar with the dplyr package you could do this my using `group_by()` and `mutate()`. Let me know if you need more help!
Small loops are very inefficient in R. There is some overhead associated with creating the loop that isn't a big deal for larger loops, but is significant for looping over only a few values. Putting the various variable names in quotes makes them a string literal, and doesn't reference the variables themselves. Some more information about what you're trying to do here would be helpful.
Ahhhh the classic quintuple nested for loop.
install.packages ('stargazer') library (stargazer) lm1 &lt;- lm (y \~ x1 + x2 + x3, data = df) &amp;#x200B; stargazer (lm1) # LaTeX output stargazer (lm1, type = 'text') # if you want a preformatted text table &amp;#x200B; et cetera
I’m trying to delete row like 3 4 5 6 7 10 4 5 6 7 8 94 5 6 7 8 9 123
I'm unsure what you mean by anchoring. Would I group\_by() ppid\_trialn? And then what would I have to mutate in order to anchor the timestamp 0-4?
so... rows where the first 5 numbers are in sequence? please give the precise definition of what you're trying to do
I mean to make it so that the timestamp column only has values between 0-4, sort of shifting all of the values so that within each group the range is bound in 0-4. Yes! You could `group_by(ppid_trialn)` and then any mutates after that would act in a group-wise manner. Then if you subtracted the minimum of timestamp in each group from each value the range should go from 0-4. data %&gt;% group_by(ppid_trialn) %&gt;% mutate(anchored_timestamp = timestamp - min(timestamp)) %&gt;% ggplot(aes(anchored_timestamp, YawRateChange)) + geom_line() + geom_point() 
Yes ! That’s what I’m trying to do !
Of course! That makes perfect sense! Thank you for explaining that to me :)
That is surprisingly close to the problem stated here yesterday: https://www.reddit.com/r/Rlanguage/comments/axv7c2/remove_combinations_that_have_series_of_numbers/ Feel free to take my or the other solution from there (replace 2 with 4, fill with 1:5 and zeroes), but have a word of advice.... if this is some kind of homework assignment, teachers are very likely to compare your code to things written on the internet.
You're welcome!
dplyr gather() spread() I think are useful commands for you.
 x &lt;- rbind( at_head = c(1:5, 3, 4, 5), # should be flagged at_tail = c(1, 1, 1, 5:9), # should be flagged at_mid = c(1, 11:15, 3, 199), # should be flagged rep_6 = c(1, 11:16, 199), # should NOT be flagged rep_3 = c(1, 11:13, 1:4) # should NOT be flagged ) xr &lt;- apply(x, 1, function(f) rle(diff(f))) filt &lt;- vapply(xr, function(f) 4 %in% f$lengths, logical(1)) x[!filt, ] # result: # [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] # rep_6 1 11 12 13 14 15 16 199 # rep_3 1 11 12 13 1 2 3 4 There's probably quite a bit you could do to streamline this, but should give you a starting point.
I was under the impression that this was generally not faster--more readable, yes, but still essentially a for-loop under the hood. Am I mistaken?
Can't you just use t function? https://www.rdocumentation.org/packages/base/versions/3.5.2/topics/t
Yeah, this is just a transposition problem: vertical_data &lt;- t(horizontal_data)
Lol it's 
This
I’ve been trying to use these but when you have multiple value variables it seems the only option is datatable::dcast, unless I’m missing something? They also seem to be much slower in some cases than melt/dcast
The term you needed to google was “transpose”. As others have stated, use the t() function. 
Those are useful, but they just want to transpose the data.
I would just transpose it on excel 
Sorry to bug you again. I've been stuck on this for a few hours(sad, I know) but I am trying to create a boxplot per my professor's recommendation and for some reason it's only showing a straight line. Would you know how to fix that? There isn't much on google. I'm comparing US region to political party, they're both integers in the dataset. This is what I have as far as code: plot5test &lt;- ggplot(dataset, aes(x = as.factor(live1), y = party, group = 1, fill = party)) plot5test + geom\_boxplot()+facet\_wrap(\~party, strip.position = "bottom", scales = "free\_x")+ theme(panel.spacing = unit(0, "lines"),strip.background = element\_blank(), strip.placement = "outside") &amp;#x200B; Thanks for your help! 
Looks like you have your X and Y mappings backwards. Swap them. 
Thank you! That fixed it. Would you also be able to explain how to change ticks in the label? Here's a picture of my issue: [https://imgur.com/a/o2ETeJd](https://imgur.com/a/o2ETeJd) I'm just trying to make it so that instead of it showing the numerical responses it shows the actual party that the numbers represent. Thank you!! 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/cTafmRA.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ei1xewc) 