Can you explain the exact commands that you typed just so I get a better understanding, and also how you got all the data points on this graph
u can use tracemem https://stat.ethz.ch/R-manual/R-devel/library/base/html/tracemem.html can see if things r being reallocated or appended and stuff 
What would I have to change to make it a scatter plot
How do you change that formula to be used for scatter plot because when I do scatterplot(table(df)) it kicks the error argument “y” is missing, with no default But when plot(table(df)) it works perfectly
You can't use a scatterplot with these data, think about it. Your x-axis is categorical (male/female) and your y-axis is binary (0,1). When you have data like this (counts of some categorical/binary outcome) you need to use Pearson's Chi-square test. [https://en.wikipedia.org/wiki/Pearson&amp;#37;27s\_chi-squared\_test](https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test)
So if I changed male/female to as.numeric it would work then?
R is a language structured around the vector - an ordered list. Vectors are special in R, they can basically hold any type of "programming object" you can create AND they can be passed into other functions/methods to simplify your life. A data.frame is essentially just a list of vectors, and it can be indexed a bunch of ways. With the square brackets df\[1,1\] you are saying give me the entry in row 1, column 1 of my dataframe df You could also pass a vector of rows into the "brackets" function (it is a function for indexing) using the concatenate function (c()) like this (also, since our data frame columns are named, we can also access them by name instead of by number): df\[c(1,2,4), "party"\] This says "return the values from column "party", rows 1,2,4 of dataframe df" [http://www.cookbook-r.com/Basics/Indexing\_into\_a\_data\_structure/](http://www.cookbook-r.com/Basics/Indexing_into_a_data_structure/)
You're not thinking about your data the right way. Your independent variable (sex) is not continuous. We use a scatterplot when we want to see the relationship between a continuous variable (like age) against a continuous dependent variable (# of gay married acquantainces).
What would be the best way to show a regression model then cus it’s the only part I have left and can’t think of any way
If your assignment was to use regression analysis you done messed up. For paired binary data you basically need to use chi-square to see if there is a difference in the means of the two groups. There are other (continuous/discrete numeric) independent variables in the dataset... all is not lost. Evidently the appendix to your textbook has some information the source of those data... go back through it, I think you may find an analogue to political party.
I woulld say it does not matter. Lots of people use different layouts (Look into different layouts arround the globe). There is no real point in Dvorak if you are not already using it.
Here’s a sample call to `ftpUpload`: status &lt;- ftpUpload( what = path_to_file, to = url, verbose = TRUE ) You need to use [`URLencode`](https://stat.ethz.ch/R-manual/R-devel/library/utils/html/URLencode.html) if the password contains any special characters. Hope this helps!
Nice!
Oh. I see.
r/machinelearning
Hi, I'm a mlr developer, so not totally unbiased ;) . I would say that mlr can act on more complex machine learning problems, but has a steeper learning curve than caret. There is a recent addition for machine learning pipelines and general preprocessing, similar to the recipes project, but for mlr: https://github.com/mlr-org/mlrCPO This allows to quite nicely create and configure/tune machine learning pipelines. Anyways, if you work with mlr check out the tutorial http://mlr-org.github.io/mlr 
Hello, Thanks for your comments. I had clocked the pipeline - though not used it. Have used the tutorial a fair bit already, it’s a great resource. Am very interested in the pipeline in particular given the Python people I know constantly going on about it for sklearn. And thank you for the excellent package in the first place. It is very nice to have a unified way of approaching things - makes learning about machine learning a whole lot simpler, for starters. I’ve converted an e1071 SVM script I did to an mlr based approach already. 
Caret is great but its API is a bit crufty and suffers from discoverability problems. MLR to me feels to me like an effort to rebuild it in a more principled way, similar to sklearn perhaps.
Thanks. That’s sort of what I figured from scanning the caret book that the package author released. Although it is nice to have a book like that. I also really like the sklearn documentation that it isn’t just a how to, it also gives some ML background info as well. 
RStudio. I use Atom for other languages but would definitely suggest RStudio for R.
Rstudio hands down
If you are used to more comprehensive IDEs like PyCharm or MatLab you will find RStudio a little lacking but it is probably the best thing available. You may even start to like it after a while ;-)
Rstudio of course.
I’ve never used those, could you elaborate? I think RStudio is pretty comprehensive in terms of inspecting objects, automatically changing functionality depending whether you’re looking at a rmarkdown file, script, TeX etc. The notebook preview is pretty useful too. Then there’s automatic kniting/sweaving, git version control, packrat for project package management, projects, etc etc. Also, for Python I was under the impression Spyder was best (at least for science stuff) - which seems rather like RStudio. 
RStudio, or VSCode.
A couple examples of things PyCharm (and other JetBrains IDEs) can do that RStudio can't: Automatically rename all instances of a variable Find usages of a function across your entire project Highlight style formatting issues Highlight functions and variables that aren't defined and/or imported There's probably more, but those are a few of the big features that I use in PyCharm that aren't available in RStudio.
Ah I see, thanks. Sound quite useful. 
Yea, the re-factoring features are SUPER useful. You can rename and/or move functions, variables, modules, and not worry about missing a reference somewhere. Basically, all syntax and importation errors get resolved before something is run because the IDE will tell you.
I like RStudio myself but: - MatLab's documentation is so much better and easier to access, - hovering over a variable in MatLab previews it, - RStudio has almost no plugin ecosystem, - debugging in more mature IDEs is much more sophisticated and easier to use, - RStudio doesn't have many features that developers like, e.g., easy code refactoring, project management (RStudio has some basic features), collaboration tools (yes, it does have git integration), - I think RStudio still doesn't really have right click support but I can be wrong. Again, I like and use RStudio. It is good for the analysis part but not as good for development as other IDEs, IMHO.
Vim with [Nvim-R](https://github.com/jalvesaq/Nvim-R).
I am working on migrating to Nvim-R, but learning vim keys takes soon much time...
Emacs + ESS
Rstudio but I occasionally just write stuff in sublime text.
That makes sense, considering I only ever really do analysis stuff that I can shoehorn into my “normal” science-y job, and next to no proper software development. 
&gt; Highlight style formatting issues &gt; Highlight functions and variables that aren't defined and/or imported Rstudio has those features! Go to Tools &gt; Global Options &gt; Code &gt; Diagnostics. Check the two boxes, "Warn if variable used has no definition in scope" and "Provide R Style Diagnostics". There is also an Rstudio plugin for automatically formatting your data to tidyverse conventions. It is called [stylr] (http://styler.r-lib.org/index.html). I have never used it though.
RStudio 4 lyfe.
[This SO post ](https://stackoverflow.com/questions/29861117/r-rvest-scraping-a-dynamic-ecommerce-page) describes a solution where you RSelenium to programatically scroll down the page before getting the source code.
Yeah I’ve seen a bunch of people say to use RSelenium but when I try to install it I get a message saying it’s not there. Am I crazy?
Vim, sublime or vs code. Switching editors when you switch languages sucks.
I've been using R-Studio lately, but have used Eclipse in the past. Eclipse can be challenging to set up but has some of the features mentioned above that R-Studio lacks. I've also heard good things about JetBrains IDEs.
RStudio for R, typically vim for everything else; I've been curious about Nvim-R
If I recall correctly, I believe RSelenium was deprecated because one of it's dependencies was removed from CRAN
Ah damn, I missed that, sorry.
It’s okay thanks anyways. 
Thanks I didn’t know that. Do you know if there is something people are using instead of it? 
Honestly I haven't done much with webscraping, but I do know that Selenium is a package in python that can be used for the same purpose 
Yeah I know about that one but I’ve never used python but it seems like I might have to learn. Thanks 
[WebDriver](https://github.com/rstudio/webdriver) looks like a solid alternative to RSelenium. Relies on PhantomJS which (IIRC) is the defacto headless browser client/emulator.
If you’re on OSX, R.app RStudio has some features I’d like, but I can’t bear the clunky UI. It’s like using a barely translated, out of date windows app. 
Why is Atom no good for R? I was under the impression that VSCode is similar to Atom. Thank you so much for all the responses so far!
Vim
It is totally worth it, it is so easy to do typical programming operations and there is so much stuff I didn't realize I wanted in the vim ecosystem.
I use VSCode as I was able to make it work with R nicely which I really couldn't do with Atom or Sublime. Still switch to RStudio if I need to do some debugging tho. 
RStudio and that console in power bi (lol)
Do we count sublime as an ide? I develop there but run it in rstudio
Id say editor.
I think r studio is a no brainer tbh 
It's ok, but RStudio is just so much better.
I agree. I was tired of RStudio crashing whenever some process gets stuck. NVim-R is perfectly functional and has a low memory footprint. Never ever going back to RStudio.
Clunky!? Every time you call RStudio clunky a shiny app dies. What about auto fill? What about color coded text? What about in program shiny hosting? What about markdown previews in a panel, in its own window, or external?! I've never used R.app but it can't be THAT much slimmer than RStudio. 
I hate this RStudio monopoly. I swear by RKWard and hate rstudio with passion.
General consensus is that Atom and VS Code are generally very similar but that Atom has a richer plugin ecosystem, but conversely VS Code (and its core plugins) are a lot more mature and *a lot* less resource hungry: Both VS Code and Atom are memory and CPU hogs compared to other editors (because they’re both based on the Electron JavaScript framework) but VS Code is a lot less problematic than Atom.
Unfortunately the diagnostics have a prohibitively high false-positive rate, which renders them useless (I had to switch them off because they had a near 100% false positive rate). This is a fundamental problem in R due to non-standard evaluation (as used, for instance, in {dplyr}).
Download 3.5.0?
Really? I use style diagnostics with no problem.
Go back to the old version? Usually, what happens when packets become incompatible. Should I just wait?
I can't remember the issue with style diagnosis but the warnings for undeclared symbols failed as described.
Find the data online, download it, load it locally. 
try using devtools?
try using devtools?
I upgraded to R 3.5.1 earlier today. I agree with you `devtools::install_github("dgrtwo/gganimate")` Failed because I didn't have `tweenr` and `transformr` `install.packages("tweenr")` then worked fine, but `devtools::install_github("derek-damron/transformr")` gave me the error `Warning in system(cmd) : 'make' not found`. I'm guessing this is a similar error to yours?
&gt; devtools::install_github("dgrtwo/gganimate") Not OP, but this still failed for me. I can't install the dependency transformr (`devtools::install_github("derek-damron/transformr"`), which I can't install because R couldn't find 'make'.
What's RKWard, and why do you like it so much?
That was a very helpful response - thank you so much.
this is all fine but dgrtwo/gganimate is no longer the development version and you should use thomasp85/gganimate
zeppelin . i must use other tools with R..
So apparently Rtools version 35 have some problems in the installation process. When installing Rtools you need to check off add rtools to system PATH. (Follow this [guide](https://stackoverflow.com/questions/50034966/no-rtools-compatible-with-r-version-3-5-0-was-found)). Then I was able to install the following packages in order. devtools::install_github("thomasp85/farver") devtools::install_github("thomasp85/transformr") devtools::install_github("thomasp85/gganimate") And gganimate works as intended 
Ah yes, 3.5 was an issue for datatable as well for a while until they updated
I've got version 3.5.1 too, and the nycflights13 data set just works for me. It's part of dplyr, maybe you could reinstall the tidyverse package?
Thanks for reporting back, OP! This worked for me too
I haven't used any of the apps but you could run rstudio server on a cloud provider like AWS or digital ocean and then login using your phones browser. Would also mean you aren't frying your phones processor running things. 
What apps? I'm curious.
I recently looked for this too and found that you can do this by installing a Linux terminal emulator (or are they virtual OSes? I honestly don't know...). One solution is described [here](https://www.reddit.com/r/rstats/comments/7mwvt0/using_r_on_android/), using Termux.
The ideal way to do this is to include the source code for the external binaries in the package and then configure the package to build them on the users machine when the package is installed. Otherwise, you'd have to build binaries for every platform and make sure to fetch the correct one. 
You're right. I turned on undeclared symbols for a day to see what the problems were. It's almost a 100% false positive rate. It not only doesn't work with dplyr, it seems to raise issues with the apply family.
Having your package download executables would be a big no no in my view. Unless the executables could be re-written to be interfaced using the .call or .c methods or rJava or the python interface, then I think the best practice is to treat them as separate software. Have a repository for your executables, and write the R package to interface to them under the assumption your user will install/build your other software.
or maybe using JuiceSSH 
I just typed "R" into Google Play's search and had 5-6 programs pop up saying you can do R on the go with them. I haven't tried any. I figured I would post this during lunch today and the wisdom of others might guide me.
Be the change you want to see in the world! I'm curious about it too. Can't imagine typing in R code on my Android's keyboard though... Or Apple with that text prediction stuff. I just can't imagine doing it on a keyboard on my phone, actually. 
I know you want R IDE of sorts, but if you are looking to learn R basics, DataCamp app does a super good job imo!
Maby: dir.create() can only create the last folder in path. Example: I don’t have a C:/temp2 folder. dir.create(’C:/temp2/folder’) will produce the same error as you get. But dir.create(’C:/temp2’) dir.create(’C:/temp2/folder’) works. So, check if you have all folders except the last one in you’r path. 
No
I'm just learning, so it would just be for working through textbook examples and looking at test cases, not serious programming. I'll have to check them out.
I tried most of them and sadly not - they're all pretty weak. The Datacamp app is the best for learning but that doesn't go very deep for R yet and is no use as a reference or for trying live code. They've had a second module coming for a while now but it's got more available for python right now. There is a way to get R on an android phone for what you're suggesting but it's very slow and is entirely console based so not very good for graphics. Also a lot of packages won't install. If you want to try it - http://www.r-ohjelmointi.org/?p=1434 
First a couple of things: * You can help us answer your question by giving us some sample data to test our suggestions on * You can post code lines on reddit by indenting each line with four spaces. Like this My line of code I think what you're trying to do is this: #sample data rhs &lt;- c("steak", "chicken", "pork", "turkey", "tofu") support &lt;- c(.00991, .00992, .00993, .012, .013) confidence &lt;- c(1.0, 1.0, .83, .97, .3) lift &lt;- c(4.41, 4.42, 4.40, 4.39, 4.0) count &lt;- c(4, 5, 10, 12, 3) #combine into DF mydf &lt;- data.frame(rhs, support, confidence, lift, count) #subset based on count mydfSubset &lt;- mydf[mydf$count &gt; 5, ] #order the output mydfSubOrdered &lt;- mydfSubset[order(mydfSubset$lift, decreasing = TRUE),] head(mydfSubOrdered, 1)
Request installation. Banks use this software. Governments use this software. 
I hear you but just for my situation, it won't work. Thanks
There are some online editors for R like http://rextester.com/l/r_online_compiler Probably not very performant, but good enough for some basic demos.
Did you try to install it? I work with similar restrictions, but when I tried it worked.
How’s your Unix/sysadmin fu? Try creating an instance on cyverse. Then you should be able to use command line R through a web-based shell. I don’t know about a GUI option (like RStudio instead of text based R) but that may be possible too. 
serve an rstudio-server from a home computer. forward port. access from work.
[datacamp.com](https://datacamp.com) has an editor built into the browser window, you can learn the syntax and methods etc through that without having to install rstudio directly
Ha! I’m sorry, shiny apps. Truly, those are features I would really like (R.app only does auto fill in the console window). By clunky, I mean that all the RStudio sub windows are contained by one master window, and the controls are windows-like. My workflow has become very Mac-adapted, so I rely on being able to view and select windows from other programs that are interspersed among the windows from the program I’m currently using. RStudio would have been an easy transition from the Visual Basic editor I was using in the 90s, but it’s not now. (Zing! I don’t actually mean that as a zing, it’s just the path that my computing use took. It also reflects how contextual my preferences are) In addition to features present in RStudio and not in R.app, R.app has plenty of its own bugs, especially crashes navigating the file system. I haven’t found a solution I’m happy with, just a “currently least bad” solution. If Xcode wasn’t such a monster, it might be a contender. I expect I’ll prefer anything that has good OSX ux integration. 
I'm just giving you crap. IDEs exist because people like you looked around and said "This isn't the best way to do this" and they developed something they feel IS the best way to do it. We should all come together to make one IDE to rule them all. Full funtionality for every language and maybe it also makes fresh waffles. Just spit balling.
My issue is that i have multiple lifts that are the same that i need to also be ordered by its count.Like i have multiple lifts of 4.4 but varying counts that need to be ordered in decreasing order I tried doing but i got the same result. I also tried your method but to no avail subrules&lt;-sort(subset(myrules,count&gt;5),by="lift", decreasing = TRUE) inspect(subrules[1:20])
Both [https://cran.r-project.org/bin/windows/base/rw-FAQ.html#Can-I-run-R-from-a-CD-or-USB-drive_003f](the base R installation) and [rstudio](https://support.rstudio.com/hc/en-us/articles/200534467-Creating-a-Portable-Version-of-RStudio-for-a-USB-Drive?mobile_site=true) can be installed on a data stick. I don’t know if they are installable together on the same data stick. I used to run just base R off a USB 2.0 flash drive. Not blazing fast, but not terrible, either. The other option is to either [host RStudio-server on AWS](https://aws.amazon.com/blogs/big-data/running-r-on-aws/) or on your home machine. If you’ve never had an AWS account before, you’ll get a year of free tier credits that will allow you to run rstudio-server essentially for free on a t2.micro. You’ll only have 2GB of RAM, but it’s enough to learn with if you aren’t using big datasets.
This is what I've been doing for the past 8 months and it's working great for me!
R runs on Android with Termux, kinda finnicky to setup. USB keyboard and an Android tablet would make it tolerable. Your best bet is definitely setting up a server or running it on a USB drive.
Thank you! 
ggplot should automatically expand your axes out past the limits of your data and this shouldn’t happen - unless you are using coord_cartesian(...) to “window” in on your data, in which case it will happen. Kinda hard to tell without a minimal reproducible example of your code. Maybe try reordering your calls and your theme() function before you call your geom_point() and see if it draws on top. I know for geoms, at least, the order you call them is the order they are drawn in; not sure about theme.
yeah, so my problem is that I'd rather not have it expand the axis out automatically. Here's an example of the type of aesthetic I'm looking for: https://imgur.com/a/D7CJlki As for the code, this is for a *slightly* different application (so don't mind the geom_line or geom_ribbon parts) 1) the function I'm using to supply the theme (done as a function so I can hot swap it out for a different aesthetic given the application): Publication.Themes = function(...){ require("ggplot2") theme = theme( panel.border = element_blank(), #element_rect(fill=NA), panel.background = element_rect(fill=NA), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.spacing = unit(1,"lines"), strip.background = element_rect(color="#e6e3f2",fill="#e6e3f2"), plot.background = element_rect(color=NA), plot.margin = unit(c(10,5,5,5),"mm"), plot.title = element_text(face = "bold",size = rel(1.2),hjust = 0.5), axis.line = element_line(color="Black",size = 1), axis.title = element_text(color="Black",face="bold",size=12), axis.text = element_text(color="Black",face="bold",size=10), axis.title.x = element_text(vjust = -0.2), axis.title.y = element_text(angle = 90,vjust=2), axis.ticks = element_line(), legend.text = element_text(face="bold",size=8), legend.key.size=unit(0.5,"cm"), legend.spacing = unit(0,"cm"), legend.title = element_text(face = "bold"), legend.key = element_rect(fill = "transparent"), legend.background = element_rect(fill = "transparent"), legend.position = "bottom", legend.direction = "horizontal" ) return(theme) } 2) the main body of the code that is handling the graphics: for (i in seq(1,nlevels(Sim.Summary[["Run.ID"]]))){ name = levels(Sim.Summary[["Run.ID"]])[i] name = name[!is.na(name)] plot.log=ggplot(data=Sim.Summary[Sim.Summary[["Run.ID"]] %in% name,],aes_string(xvar,yvar,color = "Run.ID",fill="Run.ID"))+ geom_line(size=.8,aes(color=Run.ID))+ scale_y_log10(limits=(y.limits.log),breaks=y.bins.log,expand=c(0,0),labels = function(x){format(log10(x),digits = 0)})+ scale_x_continuous(limits=x.limits,breaks=x.breaks,expand=c(0,0))+ geom_ribbon(aes_string(x = xvar,ymin = ylower ,ymax = yupper), alpha=0.25)+ labs(x=paste(IV.Name," (",IV.Units,")",sep=""), y =paste("LOG ",DV.Name," (",DV.Units,")",sep=""),fill =as.character(name),color=as.character(name), title=as.character(name))+ Publication.Themes()+ Publication.Colors(Use.Default.Colors,BT.Colors,Reference.Color.Black,Manual.Color = i) plot.log2 = plot.log + theme(legend.position = "none") #PLOTS[[paste(as.character(name),"_LOG",sep="")]] = plot.log PLOTS[[paste(as.character(name),"_LOG_NOLEGEND",sep="")]]=plot.log2 } I have tried to use the *expand* function to deal with the axis placement, but my goal is more with the layering when they are on top of each other, which is desired. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/kKvm3nO.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e29v1m3) 
What?
Could you please format the code as code with [proper indentation](http://style.tidyverse.org/)? That'll make it easier to see :).
Shoot, thought I did. Will do.
That's easy! library(dplyr) #new sample data rhs &lt;- c("steak", "chicken", "pork", "turkey", "tofu") support &lt;- c(.00991, .00992, .00993, .012, .013) confidence &lt;- c(1.0, 1.0, .83, .97, .3) lift &lt;- c(4.41, 4.42, 4.40, 4.39, 4.40) count &lt;- c(4, 5, 10, 12, 7) #combine into DF mydf &lt;- data.frame(rhs, support, confidence, lift, count) mydfSubset &lt;- mydf[mydf$count &gt; 5, ] #order the output mydfSubOrdered &lt;- mydfSubset[order(mydfSubset$lift, mydfSubset$count, decreasing = TRUE),] head(mydfSubOrdered, 2) ##output rhs support confidence lift count 3 pork 0.00993 0.83 4.4 10 5 tofu 0.01300 0.30 4.4 7 OR with dplyr: mydf %&gt;% filter(count &gt; 5) %&gt;% arrange(desc(lift), desc(count)) %&gt;% slice(1:2) ##Output # A tibble: 2 x 5 rhs support confidence lift count &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 pork 0.00993 0.830 4.40 10.0 2 tofu 0.0130 0.300 4.40 7.00
Not sure if you've tried this already but usually the way ggplot builds plot layers is exactly in the order in which they are specified in code. So if you put in the geom for your data at the very end of your code, they should overlay the axis.
Wow, your boss likes a really ugly ass plot style. Like, super ass ugly. It really sucks that you have to put this much effort into using “the grammar of graphics” package to construct ugly ass plots that violate every convention for making beautiful, informative and elegant plots. I’ve had bosses like this before, so my deepest sympathies. Without seeing your data structure I can’t be sure, but it looks like you are iterating over factors and plotting 2 plots per factor, with each plot only containing the trace for a single factor? I would try moving Publications.Themes() and your scale_axis() functions to the line directly after ggplot(), and only then calling your geometry functions. ggplot typically draws your graph in the order you feed it instructions. Anything that modifies or draws your axis should come before you ask ggplot to draw your geometries. That said, I kind of doubt this will work. ggplot by design enforces good graph practices. Good graph practices typically doesn’t include drawing points outside the confines of the graph’s axis. This is a tough one, but you might have to settle for expanding the boundaries of your plot and manually setting breaks to zero. Honestly, you might want to try using the base plot package instead. I know that’s a shitty answer after you’ve put all this work in, but you can force base plot to do things ggplot absolutely won’t do because it gives you control of drawing at a much lower level. The base plot package makes plots in exactly the style your PI wants - which is why people refuse to use it these days... 
Set up a google cloud instance and set up your own R-studio server: [http://grantmcdermott.com/2017/05/30/rstudio-server-compute-engine/](http://grantmcdermott.com/2017/05/30/rstudio-server-compute-engine/) I use GCP and Rstudio server in this way so that I am computer-agnostic and have access to my R environment no matter what computer I am on.
Completely agree that datacamp is a good resource. I bought enterprise seats for my entire team to learn both R and python in their down-time.
Yeah, I'm not the biggest fan (they're out of SigmaPlot...which also suffers from being insanely expensive). Though the more I see other people's graphs in the field, the less I'm put off....other peoples' can be **really** bad haha So I'm a pharmacomatrician so the data itself is going to be drug concentrations versus time or (much more often) bacterial concentrations versus time. I work on antibiotics and quantifying the effects, meaning the plots are typically on a log scale (the plot I posted is actually log Y vs. time). The data itself is in a long format with the main variables being ID, Time, DV, and dealer's choice of covariates The iterations you're looking at for this particular script are for a different (though related) script to graph the median and 90% prediction intervals for simulation studies. And so the iterations go over the different simulation conditions in the raw data file to plot them. It is part of a larger function that creates a series of graphs and saves them as a list of ggobjects that is returned at the end. yeah, I kinda figured that ggplot would try to force me to do it the 'correct' way, but thanks for thoughts and advice. I had a feeling that the final answer was going to be me getting over it...pretty trivial in the grand scheme, anyways. 
Just open an IT ticket to have someone with admin privileges in your organization install it
Atom with all the bells and whistles to complement it for EDA
I wouldn't mind taking a look at this however according to the plot posted, I'm having a difficult time seeing how it isn't suitable for what you want. The problem is that the points at time = 0 are plotted underneath the Y axis? When i open the plot, it appears all those points are on top of the y-axis line. IE, if the Y-axis were overlapping them, we'd see the black axis line dissecting them. 
Most likely you have some 0 or negative values in your price variable (or age, somehow). You cannot take a log of 0 or a negative. You could exclude these values, but note that your results will be biased due to missing data. The revised code below will replace non-positive values with NA: logprice &lt;- ifelse(hp$price&lt;=0, NA, log(hp$price)) logage &lt;- ifelse(hp$age&lt;=0, NA, log(hp$age)) hp_loglog &lt;- lm(logprice ~ logage) # Note here your "data=hp" argument was doing nothing. 
Thanks. That data=hp was left from previous attempts, you're right :D
In addition to censoring values of 0, you can also add a small offset (like 1) to your price/age variables to remove the zeros. This doesn't work if you have negative numbers, but price and age should always be &gt;= 0.
Going to come out and say it isn't possible. 10,000 points, each with its own label? Why does every point need its own label? Graphics need to be able to communicate insight, so 10,000 labels doesn't really seem like the way to go here anyway. Using size, color, space, and faceting sounds like a lot better way to go. 
&gt;Going to come out and say it isn't possible. 10,000 points, each with its own label? Why does every point need its own label? For something like this. https://github.com/alexcritschristoph/PaperGraph You would need to zoom in on the individual parts. I am able to produce the graph just find in python pyplot, but the issue is that at many parts the labels overlap and they're hard to read. 
Why don’t you use visualization packages based on JS libraries to create interactive plots? They will allow you to make plots with tool tips. So when you hover your mouse over a point the label will show up. R has a fantastic package called htmlwidgets (http://www.htmlwidgets.org) which enables you to use powerful JS libraries. I use highcharter package that makes use of htmlwidgets and JS library Highcharts. The plot will be in html only. You can’t have pdf, png, etc. with interactivity. 
Thanks, will look into this
These are all amazing. I'm familiar with leaflet and plotly, but the rest are new to me. Thanks for sharing the link!
You are welcome! Ramnath Vaidya has done a phenomenal job of building this platform. I specifically love highcharter. Most of the documentation is actually available for highcharts and it’s pretty detailed. As an aside Bob Rudis has a package that builds upon Tau Charts. Check it out here: https://github.com/hrbrmstr/taucharts
The limma package is a good place to start when working with array data.
It makes it return standard errors. On the [help page for predict](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.html) in the section "Details" it says: "Many methods have a logical argument se.fit saying if standard errors are to returned."
I would also recommend R 4 data scientist by the same author as Advanced R both books are available for free on the internet. As far as helping you come up with a project to do. Your graduate advisor should be helping you with that. A master's project is pretty advanced and is probably above the level of help your going to find on this sub.
Check out bioconductor for many applicable packages in R. Biostars might be useful for you. They have a learn bioinformatics in 100 hours course that I've been meaning to try.
Learning R is challenging if you do not feel it is applied to what you do in your research project. I would second the advice listed above and go to R 4 Data Science for a nice and friendly introduction to key skills you will have to develop in R: data wrangling, visualization and some basic statistical models. I would suggest then applying these skills to your project. There is really no shortcut: R is a language, and you can only get better by using it regularly.
How is that? I keep getting: Warning in install.packages : package ‘nycflights2013’ is not available (for R version 3.5.1)
CRAN definitely allows shared libraries. (Or I feel like there's at least tons of packages that compile a .so file while installing them at least)
I'm not planning to add source code to be compiled into a shared library during installation, I want to include a pre built .so file into my R package
Please don't. You'd have to build for each architecture, and it makes the package more brittle. 
Just use github or something. Most R package repositories support open source principles where the source code is provided and shared, even if what you want is allowed it won’t make people in the community too happy.
Tidytext makes this extreme easy. There is a free Online textbook too. 
Oh awesome - thank you!
This is **very** simple in base R. You just need to use strsplit twice: First on each new line (\`\\n\`) and next on each whitespace: paragraphs &lt;- "This is a short bit of text. Here is a longer one on a new line. This is the longest, and it is all being fed into base R." unlist( lapply(strsplit(paragraphs, "\\n")\[\[1\]\], function(x){ sentLen &lt;- length(strsplit(x, " ")\[\[1\]\]) paste0(x, " (", sentLen, ")") } )) All that does is loop through each line with an lapply (\`lapply(strsplit(paragraphs,"\\n")\[\[1\]\] ...\` ), count the length of the list resulting from \`strsplit(x, " ")\`, and then paste that number at the end of each line before returning it.
Ha, it churned through the example and then through another input with paragraph lengths of 82, 65, 125 words. Thank you so much! 
Which version of Mint are you using? Which desktop? (Mate, Cinnamon?) What version of R do you have installed, and what version of RStudio do you have installed?
It's on b-ok.org.
Thanks!!
The answer is always [RCurl](https://stackoverflow.com/questions/22235421/using-r-to-download-newest-files-from-ftp-server)
Thanks, but I just tried this: `url &lt;- "`[`ftp://ftp.info.com/jsmith/Up`](ftp://ftp.infores.com/lvabftp/Up)`"` `userpwd &lt;- "jsmith:testpassword123"` `filenames &lt;- getURL(url, userpwd = userpwd,` `ftp.use.epsv = FALSE,dirlistonly = TRUE)` And it gave me this error: `Error in function (type, msg, asError = TRUE) :` `Server denied you to change to the given directory` Any idea what it means? Any
Pls keep security in mind...storing acc and pw in code is certainly an infosec violation at any company. Does your org have some package you can import that retrieves credentials?
If you read my initial post, you'll see that everything I put in is dummy data/fake information. None of it is real.
I've gotten it cleared.
Can you access the directory with an ftp client? Can you upload to this directory or the root directory with an ftp client / credentials? I'd take a look at a simple working code snippet and edit my way to a working solution. Another common problem with ftp uploads in company networks are blocked ports. Try it from another network (e.g. sim card in laptop, home wifi).
What do you mean by "Can you access the directory with an ftp client?" Do you mean like, if I go in and upload it manually through the FTP client (which btw is Cyberduck)? The answer is, yes, I can do that. Unless that wasn't what you were asking.
As others have indicated, you have a permissions issue. The server has refused the request to CD into the jsmith/Up directory. Either your credentials are wrong, your account doesn’t have the necessary permissions, or the strings are formatted wrong going into the function. 
Now I see... The package is called 'nycflights13', not 'nycflights2013', so without '20'.
God I’m an idiot... -_-
Could it also be a connection protocol issue? I don't use R for ftp stuff typically, but I remember some ftp servers are set up to only allow certain TLS versions. 
I don't know exactly what is causing the problem but some things I would try are maybe adding aanother / after Up, etc. Try stackoverflow as well.
Very well could be but networking is out of my wheelhouse, sorry.
Adding on to this, there are different Microarray platforms that use their own proprietary probes (Illumina, Affymetrix). Before using limma, you can convert such probes to genes using packages like "illuminaHumanv4.db" that can be installed from either CRAN or Bioconductor.
Looks like you might not have an "Up" subdirectory, or RCurl is unable to see/write to it if you do, or you may need to explicitly navigate to it before attempting to write the file. Also: confidentiality/security is relative; are we supposed to know that you work for (or are working with) IRi and what the general structure of their FTP server URLs might be? Pinging the URL gives me the actual full IP address, so the 170.111.111.11 trick doesn't work here. ;)
I may be reading this wrong, but: You're already in /jsmithftp after login. it tries a CWD jsmithftp (no "/", so it's a relative path to the current directory) unless you have a path that's /jsmithftp/jsmithftp/Up, it looks like this will fail.
I haven't used this package but I've been meaning to. I've heard good things: https://github.com/swarm-lab/Rvision
Check out tidyverse library. Functions filter(team name) and arrange(salary) will give highest salary Use group_by and summarize to find max salary in each team. (A bit more complicated)
This would be the ideal way to go. If you want to just use base R packages and/or you can't install additional packages, you could do something like: nets &lt;- nba[nba$team == "Brooklyn Nets", ] nets[order(salary,decreasing=TRUE),c("name","salary")] ...and then adapt that approach to nest it within a for loop in order to cycle through all of the teams.
Interesting! I would have thought that one of those would have a clear lead over the others, but it's pretty evenly distributed.
Are you looking to join or append?
Sounds like you're just looking for: library(dplyr) combined_df &lt;- views_df %&gt;% left_join(clicks_df)
So if the times differ, how do you want to link a page view event in one dataframe with the clicks in the other? Assuming common fields are named the same in both data.frames, simply append one to the other, rowwise, with `bind_rows`, and sort by date and time. They should then appear chronologically together.
You may find some answers here: [http://r4ds.had.co.nz](http://r4ds.had.co.nz)
Fitst link is very helpful thank youu
We aren’t going to be at your job 24/7.
&gt;1 - Order the "Brooklyn Nets" (1 team) by highest salary ("Who has the top salary in the Brooklyn Nets?") &gt; &gt;2 - Find the top salary for each team Agree with others that *tidyverse* makes these sorts of tasks quite trivial while keeping the code readable. What each of your requests might look like would be: nba %&gt;% filter(team == 'Brooklyn Nets') %&gt;% arrange(desc(salary)) %&gt;% filter(row_number() == 1) And for the second: nba %&gt;% group_by(team) %&gt;% arrange(desc(salary)) %&gt;% filter(row_number() == 1) %&gt;% ungroup() If you don't want **all** the columns returned, pipe into a `select` at the end.
[Data camp - tidyverse class](https://www.datacamp.com/courses/introduction-to-the-tidyverse) I think DataCamp also has a R for python users tutorial. Good luck. Let me know if the link helped.
Could also use slice(1) 
What are some of the questions? Just really curious 
I think the margin parameter you're looking for is par(mar=...) as you have indicated. The default margin values are: par( mar = c(5.1, 4.1, 4.1, 2.1)) The order of the margins is bot, left, top, right. So to add space to the left margin, do this: par( mar = c(5.1, 4.1, 4.1, 2.1) + c(0, 3, 0, 0) )
if you still want to see the questions pm me
Do you wish to count the number of alternatives? If so, multiply the number of alternatives (1, 0) by the number of positions (a, b, c) to get the total number of alternatives. On a side note, this looks like a basic introduction to binary numbers, I would suggest reading up a little on this topic as it will help in getting your head around this and it's pretty fundamental in computing!
 # function that implements what you what sumb &lt;- function(n) { mat &lt;- matrix(NA, nrow=2^n, ncol=n) for(col in seq(n)) { mat[,col] &lt;- rep(c(0,1), each=2^(col-1), times=2^(n-col)) } df &lt;- as.data.frame(mat, stringsAsFactors=FALSE) nums &lt;- do.call(paste0, df) sum(as.numeric(nums)) } # test it for(n in 1:10) { cat("When n =", n, "sumb =", format(sumb(n), big.mark = ","), "\n") }
u/bezoarboy gives a nice example using tidyverse syntax. If you follow his method, be sure to put `library(tidyverse)` at the top of your code first. I like to use data.table -- here's how it would be done: library(data.table) # Rank salaries for each team using fast-rank-versatile function 'frankv' # and store ranking in new 'rank' variable nba[ , rank := frankv(salary, order=-1), by=team] # View Brooklyn nets in salary order nba[team == "Brooklyn Nets", .(name, salary, rank)][order(rank)] # Highest salary per team nba[rank==1]
https://www.rstudio.com/resources/cheatsheets/ Check out the strings cheat sheet and the lower left corner of the second page. You should try fixed() or coll() to make this easy. Cheers!
\&gt; par( mar = c(5.1, 4.1, 4.1, 2.1) + c(0, 3, 0, 0) ) This was it. The secondary axis was on the right side, but the idea is the same - I was adding to all the margins at once instead of just the right margin. Thanks!
I think this does what you want: n &lt;- 3 sum(as.numeric(apply(expand.grid(lapply(seq_len(n), function(i) {0:1})), MARGIN = 1, FUN = paste0, collapse = "")))
Two ideas: 1) change the size of the pdf file you generate with pdf(file=“your path”, height=x, width=y) where you specify x and y 2) If the axis label is cutoff because it’s too long, wrap the label onto 2 lines by putting a carriage return in the label with /n like this: ploy(..., ylab=“first line /n second line”) [apologies for the formatting - I’m doing thus from an ipad] 
I've already tried 1 (to no avail) and re 2 it's being cut off regardless of axis label length. I'll shoot you an email, I appreciate the offer!
Ha! I got it -- you have to specify margins *after* you invoke the graphics device. i.e., call pdf() first and then call par(mar=). 
Beautiful - makes perfect sense in retrospect. Thanks so much!
Use rollapply or rollmedian from the zoo package. Windows can be a bit tricky to figure out - align = “right” actually windows out to the left (in the past), align = “center” centers the window on the current observation, align = “left” takes the median of future observations. These are extra easy to use with dplyr and the %&gt;% pipe operator. Watch out for NA values. They can cause havoc and unexpected side effects, especially if you pass na.omit=T to your aggregation function.
Use rollapply or rollmedian from the zoo package. Windows can be a bit tricky to figure out - align = “right” actually windows out to the left (in the past), align = “center” centers the window on the current observation, align = “left” takes the median of future observations. These are extra easy to use with dplyr and the %&gt;% pipe operator. Watch out for NA values. They can cause havoc and unexpected side effects, especially if you pass na.omit=T to your aggregation function.
Remarkably elegant and simple! If I had gold I would throw some your way sir! Thank you for the help.
&gt;n &lt;- 3 sum(as.numeric(apply(expand.grid(lapply(seq\_len(n), function(i) {0:1})), MARGIN = 1, FUN = paste0, collapse = ""))) `expand.grid()` is a nice alternative to my loop. Great idea!
If there is no ranking between the options in each category I would recommend using random forrest. With RF you do nothing, (I think the options are converted to a one-hot-vector. )
It's an interesting question, but at the start I'll work as if there's no ranking between the options. I'll try the random forest and if it works well I'm done :) Thanks!
So I found a solution. It's explained here, in my own answer: https://stackoverflow.com/questions/51443489/change-number-of-minor-gridlines-in-ggplot2-per-major-ones-r/51445463#51445463
The code works. You’re probably confused because R’s default display of strings also escapes escape sequences. To display the *actual* string, use `message` or `cat`: 〉 result = gsub("\\(", "\\\\\\(", "test(") 〉 result [1] "test\\(" 〉 cat(result, '\n') test\(
You need to provide a lot more. The code you're running, an example dataset,, etc. 
Turns out it just needes a reboot
Thank you so much. Yeah, you are correct that the 'print' output confused me. Thanks for the `cat` or `message` tip. Also thanks for the final function. Cheers
Thanks!
I figured it out. Remove the "set.seed" line. I'm not sure what it does anyway. Leaving the post up for anyone searching for this info later.
on my phone but why not just wrap cli image magic and parse the output of you already know it works
If you've had trouble with over fitting a model, why did you design an incredibly complicated function to further over fit your model?
Size of window is to small. If you export the picture and set a large size everything should fit. Adjust the res and max.words to get a round cloud. png(fileNameOfPlot, width=16,height=16, units='cm', res=300) wordcloud( words = topWords$word, freq = topWords$n, min.freq = 10, max.words = 175, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(9, "Set1") ) dev.off()
My first guess would be changing aes (x="Target Position",y= "Guess Position") to aes (x =`Target Position`,y = `Guess Position`) Since you have spaces in the variable names, you need to use the back ticks to refer to them as variables. It's hard to tell without having access to the actual data. Also have you just tried doing ggplot(data = DataVP1[1:400,], aes (x = "Target Position",y = "Guess Position")) + geom_point() without any of the other stuff to see if you can get that working before trying to add all that other stuff to it?
Thank you so much those spaces were the cause of it. I just changed it to a word without spaces because i couldn't get it to work with backticks
It also might depend on which version of ggplot2 you are using. I have the latest and it worked both ways (with "" around the names and with back ticks around the names).
No offense, but your code is very messy. I would recommend looking up Wickhams «R for Data Science» (it’s free). For example, never name a variable «c» - you have now removed the option to make vectors! You also repeat yourself a lot. As for your error, you can use «scale_x_discrete» instead of continous, or make your variables numeric before plotting instead of factors. The error message is very clear here...
Because setting "scales" to FALSE gives you the optimal set of predictors such that the SSR is minimised and the adjusted R^2 is maximised. How exactly does that make things worse? It literally trims predictors that lower the adjusted R^2 ... As far as setting it to "TRUE" goes, it just gives me insight as to how I should pre-process my data.
I know its messy but i have never worked with it before. And besides that i have also never coded before I'm not even sure what a vector is and if i need it but just to be safe I'm going to change that. Thanks
Whenever you find yourself repeating code, like how you have with reading files, something in your brain should be trigger "there's a better way of doing this!". For this case you could do list.files to give you all the files in a directory, then read them systematically with somethings like map(files, read.table) 
Yeah, rstudio isn't that great, but it's probably the best IDE for R.
Thanks!
The problem is that to maintain significance your p-value would drop well below the standard .05 because you are running multiple models. Check this out. https://freakonometrics.hypotheses.org/19817
Make sure you are using backticks and not single apostrophes. 
As someone who has used over a dozen ides, Rstudio is so much better than anything else I have used. Nothing op experienced has anything to do with rstudio. It was literally that fact that she or he didn’t know that a space can’t be used as a name with backticks (every other coding language has the same issue). What is it exactly about rstudio that makes you think it’s not that great?
I'd look into the tidybayes package on CRAN, read its vignettes, and see if that'll do it for you. The package is designed to make it easy to work with MCMC samples.
Also, you could try using the 'brms' package (which uses Stan rather than JAGS under the hood). I've found that to be by far the most user-friendly package for fitting Bayesian models.
I never meant to use all the transforms at once if that's what you're implying. I haven't tweaked the function yet so to use at most 1 transform per variable if "scales" is set to TRUE. All in all I've never used the transforms yet, I just use the function to cut down the number if predictors. The code is complicated because calculating the pseudo-inverse is computationally heavy, hence why using a recursive algorithm was needed.
I used to use Rjags until I found the 'rstanarm' package for R (http://mc-stan.org/rstanarm/index.html). It's by far the easiest Bayesian models I've used. You can use the 'stan_glm' function like you would with a regular 'glm'. 
It uses code in first place. Having buttons is a luxury or a drop down menu for the different options but i prefer that over code. Saves me a buttload of time.
Wait, you’re saying that “having buttons” saves you time?
Yeah like in word Typing out 3 lines of code to change the font size would be unacceptable But for statistics its ok ?
But changing the font size of a plot (for example) is only typing one parameter (size = 13, for example). It’s not three lines of code. What word CANT do is programmatically change font sizes depending on other inputs and logic. 
The problem is i have to know the specific commands for this and can't just look for something fitting in a menu 
&gt; never name a variable «c» - you have now removed the option to make vectors That's incorrect, R allows you to shadow a function with a non function variable of the same name, and still use the function. It's not generally a good idea, but it works: c = 42 x = c(1, c) x # [1] 1 42
In case that’s what you’re using: On macOS and Linux, R can be installed on the user account without admin rights.
This works as long as you only ever need ~ 10 commands, and never have to customise them. But statistics fundamentally isn’t like that. You need to learn programming and embrace it, to work effectively in statistical analysis
Thanks for the recommendation. Having a look at it now. For anyone else looking into the package, it can't be installed using the standard install.packages("tidybayes") command at the moment. You need to follow the instructions on[ this page](https://github.com/mjskay/tidybayes).
Thanks for the recommendation. I've found runjags to be a hell of a lot harder to get my head round that standard R so anything that simplifies it would be welcome.
The help menus are as easy as looking through a list of menu options, honestly. 
The easiest would be to schedule these jobs in CRON on your computer (I’m guessing you are doing this locally), not on a server. You could use terminal to schedule, or RStudio gives you GUI access to cron scheduling. This is easy but brittle (ie if one job fails, it will all fail). You can also use a more sophisticated scheduler like control m to schedule r scripts. 
Thanks! Does it work on a Mac?
Airflow could be a very nice option to check out. It is fairly accessible.
Yes os x should have crontab
Yes cron is Mac and Linux. If you have windows you would need task scheduler. 
Why? 
Because I perfectly understand how to install packages in R, and I dont have perfect robot focus to inspect every minuscule detail in the code. A better way to ask this question would be to ask “how do you install dyplr and gglot2”, then have every example not show both package names, and leave tidyverse to be the correct answer.
I don’t know what to tell you man. Just pay attention to detail lol
Everyone eventually makes mistakes like that, it has nothing to do with your programming ability. I cant believe people disagree with my opinion on how the question should be framed, that is a way better test of your understanding of the language and its packages.
Its one question man. This is how these multiple choice tests go. They don’t have the capacity to evaluate your projects on the individual level. Pay attention 
Are all nine trials already done by the time you're running this?
Yes, I already have all the data.
First. I recommend wrapping your logic functions. The filenames sound pretty regular, I assume you can construct a list of them for each subject. With the logic wrapped into a function and a list of files for each trial, running the logic can be vectorized. ## Wrap your logic into functions ``` R_1 &lt;- function(infile_one, infile_two, infile_three, output_dir) { ... } R_two &lt;- function(summary_df, output_dir) { ... } ``` Such that the intermediate figures and summary.csv is still written to the `output_dir`. Additionally, have the `R_1` function return the summary dataframe so that it can be passed directly into your second function `R_2` ## Get your inputs args into lists. ``` inputs &lt;- list( list(infile_1="path1_AA", infile_2="path2_AA", infile_3="path3_AA", output_dir="outpath_AA"), list(infile_1="path1_AB", infile_2="path2_AB", infile_3="path3_AB", output_dir="outpath_AB"), list(infile_1="path1_AM", infile_2="path2_AM", infile_3="path3_AM", output_dir="outpath_AM"), ) ``` ## Run your R_1 function for each input set ``` summary_frame_list &lt;- lapply(X=inputs, FUN=function(x){do.call("R_1", args=x)} ) ``` That should leave you with a list of all the summary data frames. Those can subsequently be run through `R_2` in a similar fashion. 
Try changing the levels of your factor variable with, e.g. `levels()`. My guess is that right now your factor's levels are stored as integers instead of letters. If that doesn't work, read through the source code of `plot.svm` by calling `getAnywhere(plot.svm)` to see how the legend's labels are created. This should tell you what arguments to pass to `plot.svm` to change the labels or, in the worst case, allow you to manually recreate the plot yourself by tweaking the `plot.svm` function.
 for (i in 1:length(vcffiles) { vec[i]&lt;-vcffiles[[1]]%&gt;% summaris(sum=sum(columnName)) } ## Now you have a vector with the sums of the columns ## ## Find out which locations in the vector, the sum is 0 ii&lt;-match(0,x) names[[ii]]
A list of dataframes sounds like a bad idea. Merge them into one large dataframe with a new column specifying the name of the dataframe
the drgtwo is no longer in active development you'll want to download the one from thomasp85 as he has taken over development of gganimate. `devtools::install_github('thomasp85/gganimate')` 
Lists of dataframes are actually incredibly useful for many problems.
K-modes and ROCK are a couple good options you could start with. Both have implementations in R. Check out this [blog post](https://dabblingwithdata.wordpress.com/2016/10/10/clustering-categorical-data-with-r/).
Something I've noticed on discussion boards are a swell of people with little or no statistics experience trying to get some ML package to run but they can't because those pesky statistics in their way. I'm concerned that a significant amount of ML work performed in unregulated industries is total garbage built by chancers, but I'm only basing that on what I read online. Has anyone else noticed this trend?
Actually new entrants to data science field are mostly from the same herd which you are mentioning. People boasts about machine learning knowledge while they don’t even know about the tits and bits of tweaking algorithms. People who are in this field from more than 2 years are better off
&gt; the tits and bits of tweaking algorithms The whats?
I means to say, people who uses the existing algorithms, they don’t even know how to optimise it to give better performance. They just try to use vanilla algorithm with default values, which certainly don’t produce best results. 
I don't see why this post should belong in /r/Rlanguage. There's literally no R programming here (and having clicked through to the links, the most R you'll get is "here is a link to a Shiny app").
Pretty much any clustering method can be used, generally by recoding your categorical variable as a set of indicator variables. R often does this automatically (e.g. in regression) if your categorical variable is a factor. Any more help would probably require a bit more explanation of what your data looks like
Have you tried expand.grid? foo &lt;- list(1:3, 4, 5:7) expand.grid(foo)
Thanks! That's exactly what I needed. I know I've used that before too... I don't know why it didn't come up in the search on the Top 10 of google or something. 
Look up the gather() function in dplyr 
What about .... df &gt;%&gt; arrange(player, season)
t() could also be an option [https://www.r-statistics.com/tag/transpose/](https://www.r-statistics.com/tag/transpose/) I'm an R noob myself. It seems like there are usually at least 4 ways of doing anything!
t() will move all columns to rows and all rows to columns. Takes like these are actually pivots which will fix on or more columns and gather up or spread out the other columns as necessary into key-value pairs. So for tasks like this, use spread and gather from tidyr (or the reshape2 package, but most people like tidyr better)
&gt;t() will move all columns to rows and all rows to columns. Isn't that what OP is trying to do?
 df %&gt;% gather(Metric, Value, -`Full Name`, -Position) %&gt;% mutate(Year = substr(Metric, nchar(Metric)-5, nchar(Metric)-1), Metric = substr(Metric, 1, nchar(Metric)-7)) %&gt;% spread(Metric, Value)
Thanks for this but I can't get it to work. There's a mismatched bracket which I think I've fixed but I'm getting errors saying "object 'vec' not found. This is what I'm running: `for (i in 1:length(vcffiles)) {` `vec[i]&lt;-vcffiles[[1]]%&gt;%` `summarise(sum=sum(V1))` `}`
My data has two coulums: users and their performed operations like (loggedin/ login failed/ password reset .. ). So far count of operations of each user is the only thing which i was able to cluster them like power user/normal user . But looking to learn what all can be done to the data. 
Sorry. On my phone but first create the object vec in the loop. “Vec&lt;-rep(0,length(vcffiles). 
Curious to see if it worked 
It doesn't as it returns a list of vectors with one number in each, instead of a vector of 48 numbers. Therefore the match function doesn't work
Try using the which(vec==0) 
try using which(vec==0)
I tried this but it dropped my form 7, which I need to keep; df &lt;- df %&gt;% group\_by(CASEID) %&gt;% top\_n(1, abs(DTYvisit))
Always a good idea to post some reproducible data/code to get better help (images are less useful - but still helpful, so thanks!) . Here's a minimal version of your df: library(tidyverse) forms &lt;- data_frame( CASEID = rep(01012,5), VISIT = c(450, 450, 365, 365, 450), FORM = c(18, 8, 7, 2, 2), DTYvisit = c(2006, 2006, 2003, 2003, 2006) ) &gt; forms # A tibble: 5 x 4 CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 365 2 2003 5 1012 450 2 2006 Now, what is it exactly that you want? You say: &gt;I am trying to keep the participants with a matching VISIT and DTYvisit. , but none of the values in VISIT match any of the values in DTYvisit, so I'm confused. Then you say: &gt;What I am trying to do is return the most recent FORM 2 so that I have the latest ID's information to go with the other FORMs. I have tried filter, unique, and distinct without much luck. Any help would be awesome! Do you want only the most recent form 2 for each CASEID? (seems like not, because you didn't want a form 7 to be dropped, but maybe): forms &lt;- forms %&gt;% group_by(CASEID) %&gt;% filter(FORM == 2, DTYvisit == max(DTYvisit)) # note: will preserve cases where there were 2+ form 2s in the most recent year &gt; forms # A tibble: 1 x 4 # Groups: CASEID [1] CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 2 2006 Or do you want the most recent version of each form? forms &lt;- forms %&gt;% group_by(CASEID, FORM) %&gt;% filter(DTYvisit == max(DTYvisit)) # note: will preserve cases where there were 2+ of the same forms in the same year &gt; forms # A tibble: 4 x 4 # Groups: CASEID, FORM [4] CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 450 2 2006 Or do you want something else entirely? If so let me know - happy to help. 
Thank you for your help! I will make sure to include reproducible examples in the future. My problem is that I was overthinking it and not including FORM in the group\_by! haha. Your final set of code worked great and gave me the most recent version of the forms. It's hard to ask what I wanted when my brain is fried from running in circles. I'll edit the question for people who may need help with the same issue in the future. Thank you again!
Awesome! glad it worked :)
It seems like you can still do `devtools::install_github("ropensci/RSelenium")`
Okay, so another question. My boss has decided he wants to keep the form 2 that matches the visit/VISIT and year/DTYvisit of form 7. library(tidyverse) forms &lt;- data\_frame( CASEID = rep(01012,5), VISIT = c(450, 450, 365, 365, 450), FORM = c(18, 8, 7, 2, 2), DTYvisit = c(2006, 2006, 2003, 2003, 2006) ) &gt; forms # A tibble: 5 x 4 CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 365 2 2003 5 1012 450 2 2006 so now i need to keep the form #2 that coincides with the VISIT &amp; DTYvisit of form 7. 
I think this is close to what you want: form2.matchedOnForm8 &lt;- forms %&gt;% group_by(CASEID) %&gt;% filter(FORM == 8) %&gt;% select(CASEID, VISIT, DTYvisit) %&gt;% left_join(filter(forms, FORM == 2), by = c("CASEID", "VISIT", "DTYvisit")) Basically this just grabs all the form 8s per subject, selects only the columns with the info you want to match on, and then staples the form 2s that match those columns onto it. If you don't like the `filter(forms, FORM == 2)` within a dplyr pipeline (I find it kinda weird, but not too horrible), you could also create a separate df (`form2s = forms %&gt;% filter(FORM == 2)`) first containing only the form 2s, and then `...%&gt;% left_join(form2s, by = c("CASEID", "VISIT", "DTYvisit"))`. Others might have more elegant ways of accomplishing this - just my first thought. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/datascience] [Recommend the book to learn R in 14 days for medical research to a person that already knows programming and basic statistics.](https://www.reddit.com/r/datascience/comments/91v3k3/recommend_the_book_to_learn_r_in_14_days_for/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
R for Data Science by Wickham and Grolemund
I think there are many starting points, I used \*The Art of R Programming\* and \*A First Course in Statistical Programming with R\* when I was starting out. A good reference for more useful statistics is \*R in Action\*. I have a strong preference for base R which is emphasized in the books I recommend above, many suggest that you are better off starting with the tidyverse rather than base R. If you are doing a lot of data visualization that might be reasonable, in which case perhaps \*R for Data Science\* would be a good choice. A good follow up to whichever book you use first would be \*Efficient R programming\*, especially if you're working with larger datasets. Most of these books are available (legally!) online free.
it works but when i do it this way i lose the other forms (7,8,18 and 7). so i tried different joins to put it together and drop the other form 2s, but none are work. I mean left_join should work right??
i managed to make it work with semi_join(forms, form2.matchedOnForm8, by = c("CASEID","VISIT","DTYvisit"), all = T).........it drops my form 7 but i can join that in again. Just feel like it's very ham-handed.
ah, didn't realize you still wanted to keep those. You could just add them back in with `bind_rows()`, and then re-sort if you like: form2.matchedOnForm8 &lt;- forms %&gt;% group_by(CASEID) %&gt;% filter(FORM == 8) %&gt;% select(CASEID, VISIT, DTYvisit) %&gt;% left_join(filter(forms, FORM == 2), by = c("CASEID", "VISIT", "DTYvisit")) %&gt;% bind_rows(filter(forms, FORM != 2)) %&gt;% arrange(CASEID, FORM, DTYvisit) #or however you want to sort it
THIS
holy repost, Batman
This book has a major omission- survival analysis, which is key to medical statistics, so might need a bit of extra reading around to learn how to create survival objects, cox proportional hazards, plot kaplan-meiers etc, and getting a grasp of medical outcomes such as overall survival, progression free survival, relapse free survival etc etc...
Frank Harrell from Vanderbilt has an excellent primer for biostatistics that incorporates R. Very practical and free, and he has great thoughts on clinical trial analysis. http://www.fharrell.com/doc/bbr.pdf#page193
&gt; learn how to create survival objects, cox proportional hazards, plot kaplan-meiers That's a remarkably specific set of topics that probably shouldn't be covered in a generic book titled "R for Data Science". If it's key to medical statistics, then it should be in a book titled "R for Medical Statistics".
OP was asking about medical research, was just providing some terms he could Google, not criticising a book.
As an added bonus this available free online. http://r4ds.had.co.nz/
Based solely on what you said, I'd change the data frame to an xts object. Then use the command last() with the subset of data that you want inside. 
Dude needs an answer
If you know the statistics, you can learn the package quickly. If you don't know the statistics, you can't learn it that quick.
You should post this in /r/statistics or /r/askstatistics because while it involves R, it's also a heavily statistical question.
This was a great resource for me starting out, but i found that afterwards my 'basic' R knowledge (types, loops, etc) wasn't up to scratch. This book should be combined with some solid practice with base R. However, I also had a very tight timescale for learning, so ended up learning these bits of R on the job. 
Don’t mean to offend, but did you put R on your resume without knowing it :D ? If so, Godspeed, and congratulations!
There's no need to estimate the simple model, since it is nested in the full model. All you need is the t-test of C. If the null of C = 0 is rejected, then you probably want the full model. If not, you might be fine with the simple model. I think you can get that test from simply running summary() on the nls object.
Yeah I'm not sure what 'basic medical statistics' is. Assuming it's just the t-test and chi-squared... It's a lot to learn in 14 days, particularly if you have to learn R as well. And then there's the population-style medical statistics that use mortality rates and meta-analyses...
Am I wrong or are you not searching for automation, but more for a way to loop through all your data? If you want to loop through existing data-files: Save your filenames in a list and loop through the list. Automation is meant for things that should happen on a regular basis, say, every night at midnight. Scheduling can be done by taskscheduler (Windows) or cron (same for Linus, MAc).
Better use library(wordcloud2)
&gt;library(tidyverse) forms &lt;- data\_frame( CASEID = rep(01012,5), VISIT = c(450, 450, 365, 365, 450), FORM = c(18, 8, 7, 2, 2), DTYvisit = c(2006, 2006, 2003, 2003, 2006) ) \&gt; forms \# A tibble: 5 x 4 CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 365 2 2003 5 1012 450 2 2006 library(tidyverse) forms &lt;- data\_frame( CASEID = rep(01012,5), VISIT = c(450, 450, 365, 365, 450), FORM = c(18, 8, 7, 2, 2), DTYvisit = c(2006, 2006, 2003, 2003, 2006) ) &gt; forms # A tibble: 5 x 4 CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 365 2 2003 5 1012 450 2 2004 6 1013 450 8 2003 7 1013 450 18 2003 8 1013 450 2 2003 Any suggestions on how I could drop rows of FORM 2 that do not fall within a &lt; 2 year range of the FORM 8 DTyvisit? This worked great: form2.matchedOnForm8 &lt;- forms %&gt;% group\_by(CASEID) %&gt;% filter(FORM == 8) %&gt;% select(CASEID, VISIT, DTYvisit) %&gt;% left\_join(filter(forms, FORM == 2), by = c("CASEID", "VISIT", "DTYvisit")) %&gt;% bind\_rows(filter(forms, FORM != 2)) but now I am losing observations. I need the following: library(tidyverse) forms &lt;- data\_frame( CASEID = rep(01012,5), VISIT = c(450, 450, 365, 365, 450), FORM = c(18, 8, 7, 2, 2), DTYvisit = c(2006, 2006, 2003, 2003, 2006) ) &gt; forms # A tibble: 5 x 4 CASEID VISIT FORM DTYvisit &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1012 450 18 2006 2 1012 450 8 2006 3 1012 365 7 2003 4 1012 450 2 2004 5 1013 450 8 2003 6 1013 450 18 2003 7 1013 450 2 2003
HMMM...I will have to learn about xts, first time I've heard of it. :) 
S3 is just for generic function dispatch. It doesn't have anything to do with these questions. Forget anything else you learned about OOP in other languages.
You are right, sorry I did not express myself properly. I felt the problem was going beyond looping because I needed to pick specific files out of a very long list, then generate and store distinct outputs in specific folders. That's the part I needed "automated": create the folders, save an output or a number of outputs in that folder etc. That's why I chose that word. I did not realize it had a specific use that was somehow different.
Use the na.locf function from dplyr on the data frame. It will fill all blanks with the previous value in the column till the next value is found. Then repeats the new value and so on
Beautiful. Thank you! I wish I were more familiar with other libraries and tools in them. I'll get there one day.
This may not matter with your specific data set but how are you planning on handling when start\_times are in one day and end\_times are in another?
Oh... duh... of course.
Just store a list of Cards and assign it a "Deck" class label. I think any kind of lookup table is unnecessarily complicating things. S3 is dirt simple. I disagree with berf's claim that S3 has nothing to do with this kind of task...you can certainly use S3. However, he is right in that it is nothing like OOP in most languages. It's basically just run-time function overloading via generic functions. No real encapsulation or abstraction--just a simple mechanism for method dispatch. It's like a dumbed down version of S4, which is in turn a dumbed down version of the Common Lisp Object System, which is itself very different from the kind of objects you deal with in languages like Java (for something like you have in Java, try R6 classes, or just accept that Java-like objects aren't part of idiomatic R).
the data is in date time. ideally Im looking for a way for it to look at the start and end date time and fill in everything in between
What do you mean 'time spent in each 30 minute interval'? Like, percent of each half-hour that a user is active? Why can't you loop over it? I'd probably start by converting the times to POSIXct format and working with them as integers instead of times/dates.
you can use [difftime](https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/difftime) in minutes but you will need to decide whether you attribute the time spent to the start or the end.
If you ever want OOP in R you can look for the R6 package
xts is a lifesaver for working with time series data (also the lubridate package), I highly recommend it
Would you mind sharing your data? 
Take a look at "lead" and "lag" from the dplyr package. I was looking at values in a different column at 5 min intervals and comparing them to current interval and lagging interval. You should be able to sum the leading 30 min interval. Code is at work so I can't post it.
basically the number of seconds spent working in that interval so a max of 1800 per interval but if they came in 5 minutes late then it would get 1500 seconds. The real data is too large to loop over the whole thing efficiently
Have a look at the [lubridate](https://lubridate.tidyverse.org) package, especially the `intersect` and `setdiff` functions for interval objects.
Thanks, but I'm afraid I don't really understand. Of the two ways I could implement the sort of project I've described (or tried to describe), is one or the other preferred as a matter of principle?
also you could simply us the fill() function from tidyr, another tidyverse package. You might even consider separate() for the county, state columns so that you can use effective filter() queries in your pipeline 
tidyverse packages will solve 80 percent or more of the data tidying problems you encounter. install.packages(tidyverse) and library(tidyverse) will help you everytime 
I don't have experience with OOP in other languages, but most of the well-used R libraries I've dug into prefer the first approach using S4 classes. I think most non-CRAN developers prefer S4.
So, do you want to know how much EVERY SINGLE person spent or how much they spent COMBINED ? I mean supposedly John and Luke started in the same day at 8:00 and finished at 12:00, the time they spent between 8:00 and 8:300 would be 30 min or would it be an hour ? And if I get the idea, you need how much was spent in every half an hour by every ( or combined ) person .. Right ? Thinking about a way to solve it :) 
This
Read this. http://adv-r.had.co.nz/OO-essentials.html Your explanation of what you have don't makes no sense, because S3 has no concept of classes. You don't have a class "Card" because that doesn't mean anything in S3. S3 doesn't work the way you're expecting it to. There is no sense that you can nest Card objects inside Deck objects, except that a Deck object could be a list and each list element could be a data structure representing a card. I expect your next step is going to be making methods for the deck like "shuffle", "draw", etc. Doing this doesn't really make much sense in S3. S3 is essentially just for expanding the generic functions in R to apply to more types of objects. You have picked the wrong tool for the job.
I dunno how best to do that, im abit of a newbie. But heres what the data frame looks like, if thats any help? Data frame looks like this: 'data.frame': 718 obs. of 3 variables: $ Site : Factor w/ 4 levels "Allerthorpe",..: 4 4 4 4 4 4 4 4 4 4 .. $ Species : Factor w/ 3 levels "P. nathusii",..: 2 2 2 2 2 2 2 2 2 2 . $ No..Bats: int 1 4 1 3 1 3 3 2 1 2 ... I'm not sure if an Anova is possible to be carried out? or something like a chi test?
https://imgur.com/a/uDcR9GU
I think this is better directed to /rstats. My first reaction is that you could use one of the ANOVA models or regression modelling.
Thanks for pointing that out, I've crossposted on there now. I had considered ANOVA, but i'm still new to stats and was unsure based on the different variables being used. Would it help if I turned it into a contingency table?
Sounds to me like maybe it's yet another clinician about to make inferences that aren't statistically sound :')
this is the correct answer
It’s how much each person spent in that interval. So the end result will have a column for date, interval, name, time spent in that interval for that person
Ha ha, we all know at least one! ^hang on...^that includes me^oh shit^ah well
[http://r4ds.had.co.nz/](http://r4ds.had.co.nz/) This book will give you a great foundation in the tidyverse package. I became so much more efficient at data munging after reading it, and still use it as a reference.
I was just saying the obvious. The only thing S3 classes do is provide method dispatch for R generic functions like print, plot, and summary. They don't do anything else. In particular, they don't say anything about the structure of objects and don't associate methods with objects. You can say class(foo) &lt;- "bar" for any object foo. It might be unwise, but there is nothing stopping you. If you really, really like OOP (which is not the R way IMHO) you could look at S4, which provides more guarantees) or any of the several other OOP systems that are in CRAN packages. I find R package proto interesting (Javascript style OOP), for example.
R6 provides a good API for Java-style OOP but this is neither the only style of OOP nor the original. Ad-hoc OOP as provided by S3 is certainly less powerful since it lacks formal classes but it’s still a valid style of OOP and it often makes more sense than R6 in the context of R.
Why not just define a keyboard shortcut in your favourite editor to `R -e "rmarkdown::render('filename.rmd')"`?
because 'filename.rmd' is different from project to project. 
You can pass the current filename in a shell command, https://atom.io/packages/atom-shell-commands
This might be exactly what I'm looking for, thank you.
[A First Course in Statistical Programming with R](http://einspem.upm.edu.my/wopr2017/2016.pdf) [R in Action, Second Edition](https://www.manning.com/books/r-in-action-second-edition?a_bid=5c2b1e1d&amp;a_aid=RiA2ed) * [GitHub - R in Action, Second Edition](https://github.com/kabacoff/RiA2) [Efficient R Programming](https://csgillespie.github.io/efficientR/) [The R Inferno](http://www.burns-stat.com/documents/books/the-r-inferno/) 
As I recall, no.
All of this is super possible with R and there are good packages to help facilitate this. I would say you have 2 "projects" here. The first bring creating a training/testing set and fitting your model and the second wrapping your model in a web interface. 1. For getting your training data and model, do a lot of experimenting and exploration. Try different features that you can extract from text. I'm assuming they are proving you with labels and raw statesments of purpose to fit your model? Once you have a good model clean up your code and process for defining your feature data set. You can use this function or set of functions in your web app later. 2. Use R Shiny to build a simple web GUI. It sounds like you most likely need an text input box to copy and paste the raw text input (your code should be able to accept the raw input and behind the scenes get your features data row to feed in your model) and "submit" button the data processing and the model output. You can then present the model classification or have some email feature as well.
Unfortunately I'm not on Windows :( 
This is the correct answer. “Janet” is your base case. All coefficients represent how that employee did compared to Janet. 
Thank you for your response! Right, so I do understand how regression works, and especially how dummy variables work and how to interpret the intercept in that case. I wish I could show you my csv but this data is from the company I'm interning at and is sensitive. It's also why my data was given in such an awful manner... Anyway I think my regression output is OK since I've changed the variable names. [Hope this makes sense. I'm aware the intercept makes no sense; has to do with the fact that there's many more variables not included in this particular regression.](https://imgur.com/a/6dTrB8W) So maybe my issue is I don't understand the magic behind the lm() function? I am assuming when I get an output like this R is doing something like Y(Cost) = Some intercept + BJanetX + BBobX + BJimX+ BWidgetX +.... Is that not what's happening? All the factor(G1) Name are inputs from my first column... It just isn't showing Janet. Is Janet the intercept? 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/L3TGXzj.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e3f9ywi) 
Consider yourself lucky, R+Windows+Unicode is a shit show, it’s **much** easier on other systems.
Make sure that the RMD file is stored in UTF-8 encoding (`:set fileencoding=utf-8`). This *should* be the default but it doesn’t seem to be the case for you. Next, don’t use `R` to execute command line scripts, use `Rscript`: autocmd FileType rmd map &lt;F5&gt; :!echo Rscript -e "rmarkdown::render('&lt;C-V&gt;%')"&lt;CR&gt;
Google "dummy variable trap" Essentially 1 dummy variable has to be dropped... or you will get multicoliniarity and an r^2 of .99 Lm() automatically does this to avoid this common issue.
Ahhhh gotcha. Thank you for your comment. One more dumb question; Does this mean if I have multiple different categories of dummies, that the intercept is going to absorb all of them? So for example I also have color, would the intercept's estimate then be a combination of Janet's effect + the effect of Red? 
How many unique categoricals do you have in each column. That is a lot of classifiers for one column to the extent that I don't know if it is really supposed to be a classifier. Are you certain you don't have something like a "Sample ID" being treated as a category because it is in a column? Are you certain it isn't interpreting numerical data as a factor and thus treating "1", "1.116" "1.9" as discrete categories rather than categorical variables?
I have 3 columns 100 unique userIds(sam@google.com/johnF@google.com,) 87 operations of userIds such as (loggedin/loinFailed/Changepassword/...) 3 unique usertype (enduser/admin/system) All the data was initially categorical . I converted everything to factors using as.factor So. any thoughts on how to build a classifier for this for determining under which usertype a new user falls into if we build a model . so that was the reason i was experimenting with bagging/rf/boosting.
Yes. You really need to learn what dummies are vs. the intercept. It is the baseline of everything being set to zero. 
I haven't used this package in R, but could you try one-hot encoding your categorical predictors instead of encoding them as factors? (basically, instead of a column with category labels, you create a number of columns equal to the number of category labels, each of which tells you whether or not (1 or 0) a given row is an instance of that category -- so if you have k levels, each row contains k-1 0s and one 1). Check out `onehot::onehot()`. You'll want to set `max_levels` to the number of levels of your largest categorical predictor. I'm not sure how, if at all, this would change the performance of the randomForest. It must be doing something like this under the hood anyway, so I'm not too worried, but other people will likely know more.
I'm not understanding the objective of your model. You should theoretically already have all of that information anyway? You want to predict user types based on their operations? I can't see a case where you would have information about their user operations but wouldn't have their usertype information. 
I finally figured it out. It had something to do with the system variables. Thanks for the Rscript tip
randomForest package is also not the only package that implements this particular model class. Have you tried ranger. It is also much faster than randomForest. Note I haven't actually read the details of your OP just scanned quickly 
Visual Studio indeed only runs on Windows. That said, no one really uses Visual Studio for R, RStudio is much more commonly used: [rstudio.com](https://www.rstudio.com)
&gt; Visual Studio indeed only runs on Windows. That said, no one really uses Visual Studio for R, RStudio is much more commonly used: rstudio.com Do I need to pay for the software?
Did you visit the link? It's free. 
thanks
Definitely use ranger, been using it today, so much faster than randomForest. 
What is it that you're trying to do and why is it not working?
It's hard to tell what you're struggling with with just one picture. Are you asking how to use RStudio? Because it *is* a visual GUI. BTW, this might help: http://web.cs.ucla.edu/~gulzar/rstudio/basic-tutorial.html. It's a tutorial showing how to do some operations in RStudio.
Let’s divide the attention to the 3 panes that you see (as shown in the given screenshot). The left pane has 2 tabs. As you would have found out, they are for writing your scripts and the other is for showing the result of the commands given. The right pane you see has tabs which give you information about everything APART from your code. For example, Environment tab will show you all the variables, functions etc, Plots tab will show you the plots that you make. Now, you can change the layout of these tabs. In general, you can have 4 views open in front of your (here you have 3). Go to tools, global options and there in layout you will be able to change which tab is displayed where. Having resolved some immediate frustrations with your ide, next best thing to do would be to go to rstudios website and search for “rstudio cheatsheets”. It is a pdf file and gives a succinct overview of the ide. Familiarise yourself with that and play around with settings. In case you need further help, don’t hesitate to directly contact me. Cheers!
BTW, R is the language. It can be used anywhere, i.e. any ide (good ones), text editor etc. One variant of R is packaged by CRAN (which you most probably would have downloaded). Another variant is provided by Microsoft. Rstudio is an ide SPECIFICALLY focused towards R. Pro tip- the help tab is AMAZING. You can teach yourself R from that tab itself. There are also packages in R which can teach you R from within Rstudio as well, but go do that at a later time (after familiarising yourself with the workflow of Rstudio). 
&gt; &gt; &gt; Pro tip- the help tab is AMAZING. You can teach yourself R from that tab itself. There are also packages in R which can teach you R from within Rstudio as well, but go do that at a later time (after familiarising yourself with the workflow of Rstudio). How can I use CRAN package? I am starting out with statistics, It's already difficult by itself, I am not lazy, but honestly I can't learn a programming language/command line language, even if it's easy... my brain can get all that... at the beginning I would start with simple GUI, IDE... so where to go? How can I use CRAN?
&gt;It's hard to tell what you're struggling with with just one picture. Are you asking how to use RStudio? Because it is a visual GUI. BTW, this might help: http://web.cs.ucla.edu/~gulzar/rstudio/basic-tutorial.html. It's a tutorial showing how to do some operations in RStudio. Sorry it' s very basic. I want to make linear/non-linear regressions, to show which distribution follow the data, machine learning... this tut is basic
If you want to do the advanced stuff then you first have to learn the tools for the job. R is a programming language. Point and click is not the way things are done in reproducible statistics.
Hah, before RStudio had dark theme i tried Visual Studio with R and that much i can say - VS with R, not even once.
&gt; Sorry it' s very basic. &gt; &gt; &gt; &gt; I want to make linear/non-linear regressions, to show which distribution follow the data, machine learning... this tut is basic Dude, if you want people to help you you have to put a bit more effort into your posts. http://r-statistics.co/Linear-Regression.html https://lgatto.github.io/IntroMachineLearningWithR/an-introduction-to-machine-learning-with-r.html Some possible tut
Well since claim is both a y axis and a predictor... nothing? What is X? 
While I'm working with .Rnw files and not .rmd files, I think my process is similar enough that it could be helpful. I confess that I use a makefile. Reddit's editor eats Makefiles and destroys the format, so I'll just describe the relevant lines: `%.tex` is the target (left hand side) and `%.Rnw` is the dependency (right hand side), with the command being `Rscript -e "library(knitr); knit('$&lt;')"`. The nice thing about this solution is that I do not have to hardcode filenames -- the command `make foo.tex` will succeed if `foo.Rnw` is present and newer than any existing `foo.tex`.
X is number of months ... Y axis actually contain 2 variable Claim Amount and Premium amount ... so I am looking that after how many months claim amount and premium amount will be same ...
 Try a simple linear regression. The model will be able to forecast the values, plot them and see if they intersect. Having no idea about the y variables, maybe a simple correlation. That will give you the direction and amount of change of one variable for every unit change of another. But in this case, month variable will not be used. 
Thanks! I've settled on a happy medium for now with the shell command package, but I'll defintely look more into using makefiles!
Thanks, I’m going ahead and using a package called MAMI, I’ll attach the relevant documentation when I get back to my office. Now I’m looking at predicting values using my logistic regression model. So I could just combine all the imputed dfs into one giant test df but then the confidence interval of my model becomes super narrow. Alternatively, I could go ahead and predict using my model for each df individually, then take into account the confidence interval because there is variance both within and between each df. How would I automate it though? Side note: this journal won’t even demand a CI and at this point I’m just trying to bulk up my cv, but in the future I’d need to know regardless.
Thanks, I’m going ahead and using a package called MAMI, I’ll attach the relevant documentation when I get back to my office. Now I’m looking at predicting values using my logistic regression model. So I could just combine all the imputed dfs into one giant test df but then the confidence interval of my model becomes super narrow. Alternatively, I could go ahead and predict using my model for each df individually, then take into account the confidence interval because there is variance both within and between each df. How would I automate it though? Side note: this journal won’t even demand a CI and at this point I’m just trying to bulk up my cv, but in the future I’d need to know regardless.
Thanks! It helped. Was able to develop a model and deduce table. Looking if i can perform AUC curve after i get the table. 
One "obvious" approach is to represent the values of each category as some summary statistic of the dependent variable within each category. In code: `data to
One "obvious" approach is to represent the values of each category as some summary statistic of the dependent variable within each category. In code: `data %&gt;% group_by(highCardinalityFeature) %&gt;% summarize(mean = summaryStatistic(dependentVariable)) %&gt;% right_join(data)` This naive approach can work well if the classes are well balanced and all contain a reasonably large number of samples. When either of these conditions fail the resulting summary statistics can become unstable and problematic. A nice solution is presented [in this paper](https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf) where partial pooling is used to stabilize the within-category estimates. If you're not committed to sticking with random forests you can also leave the values as high cardinality categorical variables and incorporate them via mixed effect models.
Use the ranger function instead
Excellent answer!!
A quick thought, do you really want to use user ids in your model? Seems like a rather terrible idea.
I think they're trying to use \`Operations\`, not \`UserIds\`? I agree that using user ID as a feature is a recipe for disaster.
Not sure if this is what you are looking for https://www.rdocumentation.org/packages/effsize/versions/0.7.1/topics/cohen.d
&gt; Dude, if you want people to help you you have to put a bit more effort into your posts because we can't guess what you want based on one picture. Ok. I want to make two things 1) I want to make a linear regression between the number of Aces (in percentage) that a tennis player has made during 100 matches, and the number of mathes he won / lost. Example: Match of 22 january 2016, number of Aces 6, match lost Mathc of 21 july 2016, number of aces 15, match won I won to spot if there is a correlation, and how strong is the correlation between number of aces / match lost or won. Maybe a simple correlation could be the choise, but I would be also able to predict what could happen if the number of aces was 20, or 30, or 40. How can I do this in R? Thank you
&gt; As far as I know, Rstudio is the most “GUI” an IDE can be for R programming. well it's not so user friendly.. you need to code 
&gt; Forget I said anything about CRAN package. My bad. I shouldn’t have confused you. &gt; &gt; As I understand:- - you are a novice in R - you are a novice in any programming language - you don’t want to learn programming language - you want to “start out” with statistics &gt; &gt; Why did you download R? Was it because some article told you that it’s good for statistics? ( I mean no offence, I just want to know the motivation ) &gt; &gt; Basic statistics can be understood by using excel itself. I don’t think at this stage you NEED R to learn about statistics. Try learning about statistics in excel, familiarise yourself with that. After this, if you really want to learn R, start with the same things that you did in excel. Do those things using R. And THEN you will appreciate R for what it is. Excel it's too basic and minimal. I know about regression models, and a little about machine learning. Willing to implemente some random forrest and tree algoritms on R thing I can not do on Excel. 
Ok, in that case, you HAVE to code. 
You may need the tidyr::gather() function to get the dataset into the right shape. I’m in my phone so forgive me in advance. If you have a data frame named df with a column for name, a and column for win/lose called win_ind (each row represents one match) then you could do this: df %&gt;% ggplot(aes(name, fill = win_ind)) + geom_bar(position = “dodge”) If you give us a sample dataset, we can give you some working code. 
In the same flavor, he could perform Chi 2 tests of independence among all pairs of factor levels and iteratively regroup "similar" (w.r.t. to a pre-determined alpha level) levels. But it can be computationally intensive...
This is quite interesting. I can't figure out if there's a simple tidyverse solution. Below is a solution using base R. Some notes: * you will need an casewise identifier which I've called `id`. * the logic is to make a function which does what you want within a single category then give it a list of all the categories you want to do it for. * expand.grid gives you a dataframe of all possible pairs. It dupliates effort because you get {B,A} as well as {A,B}. You could filter these out in the function. * match looks for the identifier in the expand.grid column in the orginal dataframe to grab the x/y coordinates. * it would be conceptually simpler to to calculate all pairwise distances regardless of categories and filter the results, but I assume this would be too expensive (too many cases). df &lt;- data.frame( x = rnorm(20), y = rnorm(20), category = rep(1:5, each = 4), id = 1:20 ) diffing_function &lt;- function(i) { d &lt;- subset(df, df$category == i) R &lt;- expand.grid(i1 = unique(d$id), i2 = unique(d$id)) R$category &lt;- i R$x1 &lt;- d$x[match(R$i1, d$id)] R$y1 &lt;- d$y[match(R$i1, d$id)] R$x2 &lt;- d$x[match(R$i2, d$id)] R$y2 &lt;- d$y[match(R$i2, d$id)] R$xdiff &lt;- R$x2 - R$x1 R$ydiff &lt;- R$y2 - R$y1 return(R) } results_list &lt;- lapply(unique(df$category), diffing_function) results_df &lt;- do.call(rbind, results_list)
I'm in a bit of a rush now so can't type out the code but look up the apply functions. They will have what you need.
Wow! Thanks a lot! Interesting with no dplyr - especially when group\_by seems to be so obvious - but your solution still works. If I have multiple categories like this: df &lt;- data.frame( x = rnorm(20), y = rnorm(20), category1 = rep(1:5, each = 4), category2 = rep(2099:2109, each = 2), category3 = rep(1:2, each = 10), category4 = rep(2012:2016, each = 2), id = 1:20 ) How would it work then?
Why can't you do \`\`\`mylist &lt;- c("ac\*, be, cd\*; daa, efae\*, fge\*; gefa\*, h") Liste &lt;- strsplit(mylist, ";|,")\`\`\` ?
WOW! That is absolutely brilliant. I really appreciate the help!
It's supposed to output 2320726 rows - so it might take a while :D
If my understanding is right you want to process every string delimited by , or ;. If that's the case then I don't think that you would get any performance increase by unnesting the loop as you are doing operation per string. So even if you flatten the loop it will still be the same number of operations i = j*k where j is outer loop and k is inner loop. Also given the example you have given I agree with mLalush - it seems you could have simply split by either of those characters.
Hi, thank you for replying, I've managed to export the [dataset](https://www.dropbox.com/s/phtzgynq2c4vakb/top5.data.xlsx?dl=0) in Excel format. Here's what it looks like https://i.imgur.com/8enyFht.png and here is what I want the [barplot](https://i.imgur.com/c7e5mQC.jpg) to look like For df &lt;- top5.data, as you can see, for each row the Loser or Winner columns contains one of the top 5 players. This ensures that the dataset consists of matches only played by the top 5. As you suggested, I tried creating a "Name" column where each row entry consists of the name of the respective top 5 player whose match that the row represents. The problem with this, however, is when something like this occurs: https://imgur.com/kRK7IJR.png When there's a match where two top5 players face off against each other. For example, where Winner is "Nadal R." and Loser is "Federer R.". Then the "Name" entry for that row must contain both "Federer R." and "Nadal R.", however (as far as I'm aware) that isn't possible. I tried a different method: https://i.imgur.com/iXI5ZGz.png I created a column for each top 5 player, and for each row, where the Winner or Loser consisted of one of their names, I entered a 1 in that players column, and if not, I entered a 0. The idea was to plot a barplot where each bar's height is represented by the sum of that player's respective column. However, this would require multiple columns to be plotted on the same axis and therefore I run across the same problem
thank you, but that's as far as I understand just to calculate cohen's D. It is still unclear for me how to compare them.
Avoiding for loops is fucking stupid if you have a complex transformation or you want to adjust or reshape your data and actually save the changes.
Yep - it's just a tool like any other. Statements like "always use `apply` instead of `for`" are just as silly as saying "always use a hammer instead of a wrench". 
Make sure all the data that you are using is formatted to data.frame and the columns are either numeric for numbers and factors for categoricals 
Make sure all the data that you are using is formatted to data.frame and the columns are either numeric for numbers and factors for categoricals 
Make sure all the data that you are using is formatted to data.frame and the columns are either numeric for numbers and factors for categoricals 
Do you specifically need to do this with a loop? I don't see any reason you should. Since your example standings are framed as wins-losses (i.e. no ties), this is analogous to asking for the probability of getting exactly k heads in n flips of a (possibly biased) coin. In R: winExactlyKofN &lt;- pWin^kWins * (1-pWin)^(nGames-kWins) * choose(nGames, kWins) In English: Think of it as the probability of any *particular* sequence of *k* wins out of *n* games , multiplied by the number of possible such sequences. The number of possible arrangements is given by the binomial coefficient, or `choose(n,k)` in R. Since the events are independent the probability of a particular sequence is given by the probability of a win raised to the power of the number of wins, times the probability of a loss raised to the power of the number of losses (and since any non-win is a loss, by assumption, all you need is pWin, kWins, and nGames. 
Sorry, I should've clarified that each event has a different probability. But I figured it out and was able to build up by adding onto the end: (Prob of same win total as last week*prob loss this week) + (prob one fewer win last week*prob win this week) I started a table figuring game one then was able to expand the formula to populate the table and just add up that result.
Ah, gotcha. Nice!
It would probably help if you showed, with your toy dataset, what the output was supposed to look like.
Nice way to solve it. Classic use of Dynamic Programming.
I believe you need to ensure both matrices have equal factor levels for all factor variables.
You should look into list-columns. You can call nest() to create a column of data frames (that you want to run your regression on). Then you can mutate a new column with the model object or summary (or tidied results of model) as new list column. Finally you can unbearable in various ways to pull out the data you need. 
I don't think decision tree algorithms work well with that many categories in one variable, you might want to consider preprocessing the variable using a variety of methods: some of which have already been mentioned.
I spent several weeks like 20 hours a week learning how to use R before I was even able to touch or even write any statistical models. Youre either gonna have to suck it up and learn from the bottom up how to write code in R or find other programs that are more user friendly. Excel does everything you need it to do from what it sounds like. 
Haven’t used the package, but the output is a LaTeX table, i.e. it will render if copied and pasted into a LaTeX document. If this is gobbledegook to you, maybe the simplest way to render the table would be through [ShareLaTeX](https://www.sharelatex.com).
You could merge the tables...? Can you even regress like that?
Probably impute the missing data or use a maximum likelihood method. Check out MICE for a package.
How about being more selective on the variables you choose? 1700 for 250 is not nearly enough, and is a complete waste of time. 
So the best way would be to impute using k nearest neighbors. Look up the package DMwR2, it has a function knnImputation() that will do all of the heavy lifting for you.
&gt; Premature optimization is the root of all evil. -- Donald Knuth First, you should consider if this code is worth optimizing: will it be ran often enough that it will make a dent in your runtime? Or will the runtime be mostly determined by `DO SOMETHING`? Secondly, profile and benchmark your code - don't rely on your intuition when it comes to code performance - it is often wrong, even for people who optimize code for a living. Thirdly, optimized code is often more difficult to understand, maintain and port. If it is going to be integrated into real software, you better do a good job documenting it, or else you'll end up asking yourself in 6 months what this piece of code exactly does, how and why? With that out of the way, let's take a look at your code. At first, I misunderstood your question, and though it was about the string splitting, so I tried to optimize that (guided by this section of [Adv. R by Hadley](http://adv-r.had.co.nz/Profiling.html#be-lazy), which I strongly recommend reading if you haven't already). librarian::shelf(microbenchmark, magrittr, stringi) mylist &lt;- "ac*, be, cd*; daa, efae*, fge*; gefa*, h" kazyka1 &lt;- function(x) { strsplit(strsplit(x, ";")[[1]], ",") } kazyka1_stri &lt;- function(x) { stri_split_fixed(stri_split_fixed(x, ";")[[1]], ",") } kazyka1_unlist &lt;- function(x) { strsplit(unlist(strsplit(x, ";"), use.names = FALSE), ",") } kazyka1_fixed &lt;- function(x) { strsplit(strsplit(x, ";", fixed = TRUE)[[1]], ",", fixed = TRUE) } microbenchmark( kazyka1(mylist), kazyka1_stri(mylist), kazyka1_fixed(mylist), kazyka1_unlist(mylist), kazyka1_fixed_usebytes(mylist), times = 100000 ) %&gt;% print(order = "min") Which outputs: Unit: microseconds expr min lq mean median uq max neval kazyka1_fixed_usebytes(mylist) 2.210 2.730 3.233022 2.928 3.1680 98.089 1e+05 kazyka1_fixed(mylist) 2.523 3.061 3.955011 3.268 3.5170 4700.627 1e+05 kazyka1_stri(mylist) 3.377 4.173 5.020401 4.484 4.8210 4465.355 1e+05 kazyka1(mylist) 12.128 13.075 14.944784 13.543 14.1135 9404.985 1e+05 kazyka1_unlist(mylist) 12.827 13.881 17.304328 14.381 14.9950 153515.542 1e+05 Here we use the [`microbenchmark`](https://cran.r-project.org/package=microbenchmark) package to time our code (we might as well have used eg. [`rbenchmark`](https://cran.r-project.org/package=rbenchmark)). We see that passing `fixed = TRUE` to `strsplit` results in a ~5x speedup, which is quite decent. Continuing on to the actual question: Liste &lt;- kazyka1(mylist) kazyka2 &lt;- function(x) { for (c in 1:length(x)){ for (d in 1:length(x[[c]])){ # Extracting last character, matching if it is a wildcard # Liste[[c]][d] Prints all elements from the list # I need the double for-loop for this check if (stri_sub(x[[c]][d],-1,-1) == '*') noop(x[[c]][d]) } } } westernmagic_unlist &lt;- function(x) { unlisted &lt;- unlist(x, use.names = FALSE) for (i in 1:length(unlisted)) { if (stri_sub(unlisted[i], -1, -1) == "*") { noop(unlisted[i]) } } } westernmagic_unlist_filter &lt;- function(x) { unlisted &lt;- unlist(x, use.names = FALSE) filtered &lt;- unlisted[stri_sub(unlisted, -1, -1) == "*"] for (i in 1:length(filtered)) { noop(filtered[i]) } } westernmagic_unlist_filter_lapply &lt;- function(x) { unlisted &lt;- unlist(x, use.names = FALSE) filtered &lt;- unlisted[stri_sub(unlisted, -1, -1) == "*"] lapply(filtered, noop) invisible(NULL) } westernmagic_1 &lt;- function(x) { split_string &lt;- strsplit(x, "", fixed = TRUE)[[1]] n_commas &lt;- 0 n_semicolons &lt;- 0 last_substr &lt;- 1 for (i in 1:length(split_string)) { if (split_string[i] == ",") { n_commas &lt;- n_commas + 1 last_substr &lt;- i + 1 } else if (split_string[i] == ";") { n_semicolons &lt;- n_semicolons + 1 last_substr &lt;- i + 1 } else if (split_string[i] == "*" &amp;&amp; (split_string[i + 1] == "," || split_string[i + 1] == ";")) { noop(paste0(split_string[last_substr:i], collapse = "")) } } } westernmagic_lapply &lt;- function(x) { lapply(x, function(y) { lapply(y, function(z) { if (stri_sub(z, -1, -1) == "*") { noop(z) } }) }) invisible(NULL) } # check results noop &lt;- print kazyka2(Liste) westernmagic_unlist(Liste) westernmagic_unlist_filter(Liste) westernmagic_unlist_filter_lapply(Liste) westernmagic_1(mylist) westernmagic_sapply(Liste) # remove printing, otherwise it will dwarf the benchmark noop &lt;- function(x) {} microbenchmark( kazyka2(Liste), westernmagic_1(mylist), westernmagic_unlist(Liste), westernmagic_unlist_filter(Liste), westernmagic_unlist_filter_lapply(Liste), westernmagic_lapply(Liste), times = 100000 ) %&gt;% print(order = "min") Resulting in: Unit: microseconds expr min lq mean median uq max neval westernmagic_unlist_filter(Liste) 4.010 4.965 6.423077 5.223 5.598 4977.640 1e+05 westernmagic_unlist_filter_lapply(Liste) 6.170 7.523 9.575751 7.855 8.395 7838.514 1e+05 westernmagic_unlist(Liste) 11.851 13.995 17.366918 14.471 15.349 4975.402 1e+05 kazyka2(Liste) 12.060 14.129 18.069730 14.589 15.495 7265.146 1e+05 westernmagic_lapply(Liste) 20.043 23.441 31.081653 24.148 25.421 155677.294 1e+05 westernmagic_1(mylist) 31.101 36.293 44.208635 37.122 38.737 8007.797 1e+05 where we see a 3x speedup by using `unlist`, followed by filtering using `[`, which saves us a stunning **12 microseconds on average**. This is probably due to us limiting the amount of work we do in R, and doing as much work as possible in internal and primitive functions, which are written in C for performance; behaviour that is quite often seen in languages such as R and Python. Interestingly, `lapply` is slightly slower than the for loop. I also tested byte-compiled versions of all of the above, but they did not result in significant speedup. As you can see, optimizing was a lot of trial and error - including `westernmagic_1`, which only has one loop, but is significantly slower (and this is the fastest of about 5 different implementations in that style; the one I thought might be the fastest turned out to be 5x slower than your implementation), and not really what most other comments suggested would be the fastest solution - proving the point that intuition can guide optimization, but benchmarking it the golden standard. Now, one thing we haven't addressed yet is indices - in `kazyka2` you have access to `c` and `d` so as to specify in which list you are. We can also get those in the unlisted version, if needed, namely by calculating `cumsum(lengths(x))` - which will give us the lengths of the separate subgroups. Similarly, this could be done for the filtered version. **tl;dr: `unlist + subset + for` is the fastest solution I could come up with.** Now, keeping all that in mind, and remembering the beginning of this post, do you think it's worth optimizing this function?
[removed]
Basically, if the missing data is missing in a random or pseudo-random pattern, imputation allows you to "fill in" the blanks with data taken from what is in the data table. 
Firstly, thank you so much for replying with such a comprehensive answer and showing the solution with two different approaches. You're a genuinely good person for doing so, and I wasn't actually aware of the gather() function and was hoping there was something exactly like that, so thanks. My end goal is to create a barplot where the height of the bar is represented by the total number of matches played, filled in with the total number of matches won, filled in with the total number of matches against top 10 players, filled in with the total number of matches won against top 10 players. Something like [this](https://imgur.com/a/pOzmwMd). This is my first project and I'm doing it as I'm learning from the [R for Data Science](http://r4ds.had.co.nz/data-visualisation.html) book, so I'm bound to get stuck somewhere again along the line. When I inevitably do, do you mind if post a reply to your comment in this thread for some help?
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/k882p6T.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e3lz9qe) 
 BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volumeBASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volumeBASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volumeBASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor 
C+++HLJJ BEAN SCRIPT NON LOCAL PROM RAMN EXEEXXEXE CPU ### GPU BASHhttps://www.reddit.com/r/Cplusplus/comments/94lym4/c/https://www.reddit.com/r/Cplusplus/comments/94lym4/c/https://www.reddit.com/r/Cplusplus/comments/94lym4/c/V
I mean bash isn’t that bad 😳
zΩΩΩ≈ΩΩ≈≈Ω≈Ω≈Ω≈Ω≈Ω≈Ω≈Ω≈Ω≈ΩΩ≈≈Ω≈Ω≈Ω≈Ω+++((((({1,1,1,1?????HELLO!!! WHO DIS??? ME QED!! &lt;iframe width="966" height="543" src="https://www.youtube.com/embed/Kh5TT31NfSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;&lt;iframe width="966" height="543" src="https://www.youtube.com/embed/Kh5TT31NfSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;&lt;iframe width="966" height="543" src="https://www.youtube.com/embed/Kh5TT31NfSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;&lt;iframe width="966" height="543" src="https://www.youtube.com/embed/Kh5TT31NfSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen&gt;&lt;/iframe&gt;
4chan party van arrives FBI!!! FREEZING COLD!?
It sounds interesting! Please do ask for more help if needed. 
 Providing Free Software to the internet community Site Map Home Download Email us Get Bloodshed CD Programmers Free compilers list Delphi C and C++ Linux/Unix Resources Free URL submission Add a resource Dev-C++ 5 (currently beta) [Screenshot] Bloodshed Dev-C++ is a full-featured Integrated Development Environment (IDE) for the C/C++ programming language. It uses Mingw port of GCC (GNU Compiler Collection) as it's compiler. Dev-C++ can also be used in combination with Cygwin or any other GCC based compiler. Features are : - Support GCC-based compilers - Integrated debugging (using GDB) - Project Manager - Customizable syntax highlighting editor - Class Browser - Code Completion - Function listing - Profiling support - Quickly create Windows, console, static libraries and DLLs - Support of templates for creating your own project types - Makefile creation - Edit and compile Resource files - Tool Manager - Print support - Find and replace facilities - CVS support Source code : Delphi 6 Source code of Dev-C++ is available for free under the GNU General Public License (GPL) Authors : Colin Laplace, Mike Berg, Hongli Lai : Development Mingw compiler: Mumit Khan, Jan Jaap van der Heidjen, Colin Hendrix and GNU coders. System : Windows 95/98/NT/2000/XP Status : Free Software (under the GNU General Public License) Size : 13.5 Mb Downloads : Go to Download Page Dev-C++ resources page (libraries, sources, updates...) Dev-C++ 4 [Screenshot] Dev-C++ is a full-featured integrated development environment (IDE), which is able to create Windows or console-based C/C++ programs using the Mingw compiler system (version MSVCRT 2.95.2-1 included with this package), or the Cygwin compiler. It can also handle the Insight Debugger, which you can also download here. - C and C++ compiler for Win32 (Mingw) - Debugger (GDB or Insight) - Customizable syntax highlighting - Powerful multi-window editor with many options - Work in source file or project mode - Setup creator - Create console, windows and DLL programs - Resource file editing (menu creator...) - Project Manager, compiler, linker and resource results - Insert automatically C/C++ codes and statements - Makefile automatic creation - 2 different icon sets for menus and toolbars in Dev-C++ - Tool manager - Templates for creating your own project types - Much more... Source code : Delphi Source code of Dev-C++ is available for free under the GNU General Public License (GPL) Authors : Colin Laplace : Main IDE Development; Hongli Lai: IDE updates, splash screen and icons Mingw: Mumit Khan, Jan Jaap van der Heidjen, Colin Hendrix and GNU coders. System : Windows 95/98/NT/2000 Status : Free Software (under the GNU General Public License) Size : 7.5 Mb Dev-C++ 4 downloads : Please be sure to read the FAQ if you are having any problems using Dev-C++. You can also order Dev-C++ with other software on a CD here. (the CD version of Dev-C++ includes the Insight visual debugger, the Win32 API reference help file and the Strandard C library reference) Dev-C++ resources page (libraries, sources, updates...) If you want to use the Setup Creator feature of Dev-C++, you will also need to download this small file · Download from Sourceforge.net · Download from Simtel.net Get it on Bloodshed CD You can get all the software available on this site on a CD. This is the list of all software included on ONE CD: Dev-C++ 5 beta Dev-C++ 4 CD version Dev-C++ 4.01 update Dev-C++ for Linux QuickInstall 2.0 Avi Creator 1.0 Dev-C++ 4 sources Dev-Pascal 1.9 sources Multibox 4.0 Fast Cleaner 1.0 LaserWar LaserWar sources for Delphi Calc-By-Step 1.0 Dev-C++ Packages + Goodies ! Copyright Bloodshed Software If you find any broken links in this site, please send me an email.
https://embed.redditmedia.com/widgets/embed?url=https%3A%2F%2Fwww.reddit.com%2Fr%2FAlanWatts%2Fcomments%2F94mmde%2Fdrawing_further_upon_the%2F%3Fref%3Dshare%26ref_source%3Dembed
https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/
https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/https://www.reddit.com/r/Tom_Thistlethwaite/comments/90wpo2/a_bipolar_drugs_stack/
&gt; √Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volumeBASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor 
v&lt;figure class="op-interactive"&gt;&lt;iframe&gt;&lt;div class="reddit-embed" data-embed-media="www.redditmedia.com" data-embed-parent="true" data-embed-live="false" data-embed-uuid="1925a156-f0b3-421e-8f61-ac99230948e5" data-embed-created="2018-08-04T22:16:01.682Z"&gt;&lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/dde9co5/"&gt;Comment&lt;/a&gt; from discussion &lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/"&gt;AutoModerator now marks reported posts as spam&lt;/a&gt;.&lt;/div&gt;&lt;script async src="https://www.redditstatic.com/comment-embed.js"&gt;&lt;/script&gt;&lt;/iframe&gt;&lt;/figure&gt;vv&lt;figure class="op-interactive"&gt;&lt;iframe&gt;&lt;div class="reddit-embed" data-embed-media="www.redditmedia.com" data-embed-parent="true" data-embed-live="false" data-embed-uuid="1925a156-f0b3-421e-8f61-ac99230948e5" data-embed-created="2018-08-04T22:16:01.682Z"&gt;&lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/dde9co5/"&gt;Comment&lt;/a&gt; from discussion &lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/"&gt;AutoModerator now marks reported posts as spam&lt;/a&gt;.&lt;/div&gt;&lt;script async src="https://www.redditstatic.com/comment-embed.js"&gt;&lt;/script&gt;&lt;/iframe&gt;&lt;/figure&gt;&lt;figure class="op-interactive"&gt;&lt;iframe&gt;&lt;div class="reddit-embed" data-embed-media="www.redditmedia.com" data-embed-parent="true" data-embed-live="false" data-embed-uuid="1925a156-f0b3-421e-8f61-ac99230948e5" data-embed-created="2018-08-04T22:16:01.682Z"&gt;&lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/dde9co5/"&gt;Comment&lt;/a&gt; from discussion &lt;a href="https://www.reddit.com/r/zsh/comments/5sayu2/automoderator_now_marks_reported_posts_as_spam/"&gt;AutoModerator now marks reported posts as spam&lt;/a&gt;.&lt;/div&gt;&lt;script async src="https://www.redditstatic.com/comment-embed.js"&gt;&lt;/script&gt;&lt;/iframe&gt;&lt;/figure&gt;
zzxzxzxxzrsin0 √Drawing further upon the (self.Rlanguage) submitted an hour ago by lookingforsomehope1 BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volumeBASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume 管理する ええやん！ · 返信 · 6分 Tom Thistlethwaite Tom Thistlethwaite BASH YOU MOTHER FUCKING CUNT!@!!1212112€42####¢##¢#¢#¢#¢#¢#¢∞∞∞∞§§∞§§¶¶¶••••ªªªªººººº–º–º###############√Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, whic
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/u_lookingforsomehope1] [z](https://www.reddit.com/r/u_lookingforsomehope1/comments/94mzka/z/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Google Search modes AllVideosImagesMapsNewsMore SettingsTools "with" (and any subsequent words) was ignored because we limit queries to 32 words. Your search - √Drawing further upon the (self.Rlanguage) submitted an hour ago by ... - did not match any documents. Suggestions: Make sure that all words are spelled correctly. Try different keywords. Try more general keywords. Try fewer keywords. Footer links 
Base on your description, I think Shiny is the best for you. Check out the Shiny website. https://shiny.rstudio.com/
This may be doable; However, it seems like a fairly complex issue to solve in R given that you also have to set up the maze connections, etc. I'm thinking specifically about P5.js as a solution to this. [Coding Train has a good set of tutorials teaching you how to code a maze generator](https://www.youtube.com/watch?v=HyK_Q5rrcr4). Given the basic understanding you develop from that, you may be able to make some modifications to the code based on the probabilities of audio / visual cues to weigh the decision that is taken by the actor. 
#### [Coding Challenge #10.1: Maze Generator with p5.js - Part 1](https://www.youtube.com/watch?v=HyK_Q5rrcr4.) ##### 164,605 views &amp;nbsp;👍1,915 👎30 *** Description: In Part 1 of this coding challenge, using p5.js, I create the cells which are going to become our maze.Part 2: https://www.youtube.com/watch?v=D8UgR..... *The Coding Train, Published on May 2, 2016* *** ^(Beep Boop. I'm a bot! This content was auto-generated to provide Youtube details. Respond 'delete' to delete this.) ^(|) [^(Opt Out)](http://np.reddit.com/r/YTubeInfoBot/wiki/index) ^(|) [^(More Info)](http://np.reddit.com/r/YTubeInfoBot/)
oops, this should have been less than or equal to .75: maze$auditory = lapply(maze$go\_left,function(x){ ifelse(runif(1)**&lt;=**.75,x,abs(x-1)) }) %&gt;% unlist
Watch this playlist, it might help orient you a bit: [https://www.youtube.com/playlist?list=PLOU2XLYxmsIK9qQfztXeybpHvru-TrqAP](https://www.youtube.com/playlist?list=PLOU2XLYxmsIK9qQfztXeybpHvru-TrqAP)
–]reMashedup 2 points 20 days ago The Generalised Third law of Thermodynamics in the Quantum Dimension .auth....Tom Thistlethwaite In words, although mathematically at T=0 the entropy (S) of a perfect crystal is 0, a crystal is never perfect, because each action must be the result of a previous action (i.e. the conservation of momentum at Quantum scales.) So there is a sense in which the third law must NEVER be true, as perfect order requires stasis, or fixation of all fields. Either the big bang occurred as a result of a collapse of matter in a parent universe, or a partial and repeated contraction, S (entropy) is NEVER = 0. In other words: at absolute zero all isothermal processes are isentropic. 3: Classically, one is free to choose the zero of the entropy.... and [zero x1] can be different to [zero x2] It is my belief that we exist INSIDE a black hole, but the relevance of this is limited, because: the event horizon is 13.8 billion years away -no meaningful messages can be transferred through the event horizon, by definition. Although information cannot be deleted, our messages would never escape the black hole that we possibly live in. That being said, the mathematical formulation of the Third Law of Thermodynamics is below: 1: In the limit T0→0 the integral in Eq.(4) is finite.[clarification needed] So that we may take T0=0 and write S ( T , X ) = S ( 0 , X ) + ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=S(0,X)+\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X)=S(0,X) + \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (5) The value of S(0,X) is independent of X.[clarification needed] In mathematical form S ( 0 , X ) = S ( 0 ) . {\displaystyle S(0,X)=S(0).} S(0,X) = S(0). (6) So Eq.(5) can be further simplified to S ( T , X ) = S ( 0 ) + ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=S(0)+\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X)=S(0) + \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (7) Equation (6) can also be formulated as lim T → 0 ( ∂ S ( T , X ) ∂ X ) T = 0. {\displaystyle \lim {T\rightarrow 0}\left({\frac {\partial S(T,X)}{\partial X}}\right){T}=0.} \lim_{T \rightarrow 0}\left( \frac { \part S(T,X)}{ \part X}\right)_T = 0. (8) In words: at absolute zero all isothermal processes are isentropic. Eq.(8) is the mathematical formulation of the third law. 3: Classically, one is free to choose the zero of the entropy, and it is convenient to take S ( 0 ) = 0 {\displaystyle S(0)=0} S(0)=0 (9) so that Eq.(7) reduces to the final form S ( T , X ) = ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X) = \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (10) However, reinterpreting Eq. (9) in view of the quantized nature of the lowest-lying energy states, the physical meaning of Eq.(9) goes deeper than just a convenient selection of the zero of the entropy. It is due to the perfect order at zero kelvin as explained above. A p-adic number is an extension of the field of rationals such that congruences modulo powers of a fixed prime p are related to proximity in the so called "p-adic metric." Any nonzero rational number x can be represented by x=(par)/s, (1) where p is a prime number, r and s are integers not divisible by p, and a is a unique integer. Then define the p-adic norm of x by |x|_p=p-a. (2) Also define the p-adic norm |0|_p=0. (3)T perma-linkembedsavespamremovereportgive goldreply [–]reMashedup 2 points 20 days ago bash exe qed! &amp; Assuming that although the entropy tends to infinity at each individual point, each particle set or particle group actually tends to an entropy minimum 'We focus on the nearest proximity approximation, determine the conditions for dynamic stability, and calculate the equation of motion. The model has novel screening properties and the scalar field is actively adjusted to decouple itself from stress and swing the necessary values. In Newton's limit, these background values ​​of the scalar field completely reproduce Newton's law of gravitation. [K.A.S Crocker] With many programs, the temperature of any system can be reduced to zero temperature in a limited number of limited operations. The third law states that the absolute 0 is not absolute, because the entropy (S) must always be greater than or less than 0. Because S = E / k where k is the possible number of microscopic (ie E = mc ^ 2) infinities at the p-adic limit.–]reMashedup 2 points 20 days ago The Generalised Third law of Thermodynamics in the Quantum Dimension .auth....Tom Thistlethwaite In words, although mathematically at T=0 the entropy (S) of a perfect crystal is 0, a crystal is never perfect, because each action must be the result of a previous action (i.e. the conservation of momentum at Quantum scales.) So there is a sense in which the third law must NEVER be true, as perfect order requires stasis, or fixation of all fields. Either the big bang occurred as a result of a collapse of matter in a parent universe, or a partial and repeated contraction, S (entropy) is NEVER = 0. In other words: at absolute zero all isothermal processes are isentropic. 3: Classically, one is free to choose the zero of the entropy.... and [zero x1] can be different to [zero x2] It is my belief that we exist INSIDE a black hole, but the relevance of this is limited, because: the event horizon is 13.8 billion years away -no meaningful messages can be transferred through the event horizon, by definition. Although information cannot be deleted, our messages would never escape the black hole that we possibly live in. That being said, the mathematical formulation of the Third Law of Thermodynamics is below: 1: In the limit T0→0 the integral in Eq.(4) is finite.[clarification needed] So that we may take T0=0 and write S ( T , X ) = S ( 0 , X ) + ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=S(0,X)+\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X)=S(0,X) + \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (5) The value of S(0,X) is independent of X.[clarification needed] In mathematical form S ( 0 , X ) = S ( 0 ) . {\displaystyle S(0,X)=S(0).} S(0,X) = S(0). (6) So Eq.(5) can be further simplified to S ( T , X ) = S ( 0 ) + ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=S(0)+\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X)=S(0) + \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (7) Equation (6) can also be formulated as lim T → 0 ( ∂ S ( T , X ) ∂ X ) T = 0. {\displaystyle \lim {T\rightarrow 0}\left({\frac {\partial S(T,X)}{\partial X}}\right){T}=0.} \lim_{T \rightarrow 0}\left( \frac { \part S(T,X)}{ \part X}\right)_T = 0. (8) In words: at absolute zero all isothermal processes are isentropic. Eq.(8) is the mathematical formulation of the third law. 3: Classically, one is free to choose the zero of the entropy, and it is convenient to take S ( 0 ) = 0 {\displaystyle S(0)=0} S(0)=0 (9) so that Eq.(7) reduces to the final form S ( T , X ) = ∫ 0 T C ( T ′ , X ) T ′ d T ′ . {\displaystyle S(T,X)=\int _{0}{T}{\frac {C(T{\prime },X)}{T{\prime }}}\mathrm {d} T{\prime }.} S(T,X) = \int_0T \frac {C(T\prime,X)}{T\prime}\mathrm{d}T\prime. (10) However, reinterpreting Eq. (9) in view of the quantized nature of the lowest-lying energy states, the physical meaning of Eq.(9) goes deeper than just a convenient selection of the zero of the entropy. It is due to the perfect order at zero kelvin as explained above. A p-adic number is an extension of the field of rationals such that congruences modulo powers of a fixed prime p are related to proximity in the so called "p-adic metric." Any nonzero rational number x can be represented by x=(par)/s, (1) where p is a prime number, r and s are integers not divisible by p, and a is a unique integer. Then define the p-adic norm of x by |x|_p=p-a. (2) Also define the p-adic norm |0|_p=0. (3)T perma-linkembedsavespamremovereportgive goldreply [–]reMashedup 2 points 20 days ago bash exe qed! &amp; Assuming that although the entropy tends to infinity at each individual point, each particle set or particle group actually tends to an entropy minimum 'We focus on the nearest proximity approximation, determine the conditions for dynamic stability, and calculate the equation of motion. The model has novel screening properties and the scalar field is actively adjusted to decouple itself from stress and swing the necessary values. In Newton's limit, these background values ​​of the scalar field completely reproduce Newton's law of gravitation. [K.A.S Crocker] With many programs, the temperature of any system can be reduced to zero temperature in a limited number of limited operations. The third law states that the absolute 0 is not absolute, because the entropy (S) must always be greater than or less than 0. Because S = E / k where k is the possible number of microscopic (ie E = mc ^ 2) infinities at the p-adic limit.
Shinyapps.io is kinda expensive for what you end up getting if you choose to get the paid version (you can easily pass the free 25 hours even by your own testing) I use digitalocean for my hosting. [Here's](https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/) a tutorial to set it up. It's comparatively cheaper and you don't have to worry about active hours. Smallest machine will cost 5$ to keep on all month.
 even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt;
 even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; even at the SCALAR POTENTIAL!!!SSS C+++ vv &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt; &lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt;
Python C(OBJECTIVE P n ADIC bash QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37]&gt;&gt;&gt;
https://www.facebook.com/tom.thistlethwaite/posts/377991126067215
Reported for spam. 
Tom Thistlethwaite 1 hr · https://www.reddit.com/…/look…/comments/94q2iv/ruby_onrails/ REDDIT.COM Ruby onRails • u/lookingforsomehope1 C++ ]&gt;&gt;&gt;&lt;&lt;QUBIT !!! WITH THIS ONE ADDITIONAL CONSTRAINT! QED !!! C(C++ All tensors are written in abstract index notation.[36] Matching the... 1 Comment 1Tom Thistlethwaite Like Comment Share Comments Tom Thistlethwaite Tom Thistlethwaite #!/usr/bin/env ruby # frozen_string_literal: true # Profile require calls giving information about the time and the files that are called # when loading the provided file. # # Example: # tools/profile activesupport/lib/active_support.rb [ruby-prof mode] [ruby-prof printer] ENV["NO_RELOAD"] ||= "1" ENV["RAILS_ENV"] ||= "development" module CodeTools class Profiler Error = Class.new(StandardError) attr_reader :path, :mode def initialize(path, mode = nil) assert_ruby_file_exists(path) @path, @mode = path, mode require "benchmark" end def profile_requires GC.start before_rss = `ps -o rss= -p #{Process.pid}`.to_i if mode require "ruby-prof" RubyProf.measure_mode = RubyProf.const_get(mode.upcase) RubyProf.start else Object.instance_eval { include RequireProfiler } end elapsed = Benchmark.realtime { require path } results = RubyProf.stop if mode GC.start after_rss = `ps -o rss= -p #{Process.pid}`.to_i if mode if printer = ARGV.shift puts "RubyProf outputting to stderr with printer #{printer}" RubyProf.const_get("#{printer.to_s.classify}Printer").new(results).print($stdout) elsif RubyProf.const_defined?(:CallStackPrinter) filename = "#{File.basename(path, '.rb')}.#{mode}.html" puts "RubyProf outputting to #{filename}" File.open(filename, "w") do |out| RubyProf::CallStackPrinter.new(results).print(out) end else filename = "#{File.basename(path, '.rb')}.#{mode}.callgrind" puts "RubyProf outputting to #{filename}" File.open(filename, "w") do |out| RubyProf::CallTreePrinter.new(results).print(out) end end end RequireProfiler.stats.each do |file, depth, sec| if sec puts "%8.1f ms %s%s" % [sec * 1000, " " * depth, file] else puts "#{' ' * (13 + depth)}#{file}" end end puts "%8.1f ms %d KB RSS" % [elapsed * 1000, after_rss - before_rss] end private def assert_ruby_file_exists(path) fail Error.new("No such file") unless File.exist?(path) fail Error.new("#{path} is a directory") if File.directory?(path) ruby_extension = File.extname(path) == ".rb" ruby_executable = File.open(path, "rb") { |f| f.readline } =~ [/\A#!.*ruby/] fail Error.new("Not a ruby file") unless ruby_extension || ruby_executable end module RequireProfiler private def require(file, *args) RequireProfiler.profile(file) { super } end def load(file, *args) RequireProfiler.profile(file) { super } end @depth, @stats = 0, [] class &lt;&lt; self attr_accessor :depth attr_accessor :stats def profile(file) stats &lt;&lt; [file, depth] self.depth += 1 result = nil elapsed = Benchmark.realtime { result = yield } self.depth -= 1 stats.pop if stats.last.first == file stats &lt;&lt; [file, depth, elapsed] if result result end end end end end # ruby-prof printer name causes the third arg to be sent :classify # which is probably overkill if you already know the name of the ruby-prof # printer you want to use, e.g. Graph begin require "active_support/inflector" require "active_support/core_ext/string/inflections" rescue LoadError STDERR.puts $!.message class String # File activesupport/lib/active_support/inflector/methods.rb, line 150 def classify # strip out any leading schema name camelize(sub(/.*\./, "")) end # File activesupport/lib/active_support/inflector/methods.rb, line 68 def camelize(uppercase_first_letter = true) string = self if uppercase_first_letter string = string.sub(/^[a-z\d]*/) { |match| match.capitalize } else string = string.sub(/^(?:(?=\b|[A-Z_])|\w)/) { |match| match.downcase } end string.gsub(/(?:_|(\/))([a-z\d]*)/) { "#{$1}#{$2.capitalize}" }.gsub("/", "::") end end end if $0 == __FILE__ if (filename = ARGV.shift) path = File.expand_path(filename) mode = ARGV.shift CodeTools::Profiler.new(path, mode).profile_requires else STDERR.puts "No file path entered. Usage is tools/profile path/to/file.rb [ruby-prof mode] [ruby-prof printer]" end end
Thanks a lot for your throughout answer!
What are you trying to say?
G the gravitational constant and c the speed of light.[37] When there is no matter present, so that the energy–momentum tensor vanishes, the results are the vacuum Einstein equations, {\displaystyle R{\mu \nu }=0.\,} R{\mu \nu }=0.\, Left. Hook-SAA?A?/A?A?A?11§§1QQQQQQQQQQQQQQWWQQEQEDWQE323234423234234423234423423?????? つっこみ1件 1 ---particular, {\displaystyle R=g{\mu \nu }R{\mu \nu }\,} R=g{\mu \nu }R{\mu \nu }\, is the curvature scalar. The Ricci tensor itself is related to the more general Riemann curvature tensor as {\displaystyle R{\mu \nu }={R{\alpha }}{\mu \alpha \nu }.\,} R{\mu \nu }={R{\alpha }}{\mu \alpha \nu }.\, On the right-hand side, {\displaystyle T{\mu \nu }} T{\mu \nu } is the energy–momentum tensor. All tensors are written in abstract index notation.[36] Matching the theory's prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is Newtonian mechanics, the proportionality constant can be fixed as κ = 8πG/c4, with G the gravitational constant and c the speed of light.[37] When there is no matter present, so that the energy–momentum tensor vanishes, the results are the vacuum Einstein equations, {\displaystyle R{\mu \nu }=0.\,} R{\mu \nu }=0.\, Left. Hook-ED##### -2ND LAW//??????INFOMRATION CANNOT BE DLETED!!!! +@££££333 TH lamba delata ?//?/D/DX!!!! NO WAY!!!Bash.{&lt;/p&gt;&lt;p&gt; Just as the real numbers are the completion of the rationals Q with respect to the usual absolute valuation |x-y|, the p-adic numbers are the completion of Q with respect to the p-adic valuation |x-y|p. The p-adic numbers are useful in solving Diophantine equations. For example, the equation X2=2 can easily be shown to have no solutions in the field of 2-adic numbers (we simply take the valuation of both sides). Because the 2-adic numbers contain the rationals as a subset, we can immediately see that the equation has no solutions in the rationals. So we have an immediate proof of the irrationality of sqrt(2).&lt;/p&gt;&lt;p&gt; This is a common argument that is used in solving these types of equations: in order to show that an equation has no solutions in Q, = Absolute zero can be reached in a finite number of steps if S (0, X1) ≠ S (0, X2).ivvvQ, = Absolute zero can be reached in a finite number of steps if S (0, X1) ≠ S (0, X2).ihex brot fractyres &lt;&lt;&lt;&lt;&lt;&lt; BOOLEAN?????????&gt;? TRUE instein's equations Main articles: Einstein field equations and Mathematics of general relativity Having formulated the relativistic, geometric version of the effects of gravity, the question of gravity's source remains. In Newtonian gravity, the source is mass. In special relativity, mass turns out to be part of a more general quantity called the energy–momentum tensor, which includes both energy and momentum densities as well as stress: pressure and shear.[35] Using the equivalence principle, this tensor is readily generalized to curved spacetime. Drawing further upon the analogy with geometric Newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the Ricci tensor, which describes a particular class of tidal effects: the change in volume for a small cloud of test particles that are initially at rest, and then fall freely. In special relativity, conservation of energy–momentum corresponds to the statement that the energy–momentum tensor is divergence-free. This formula, too, is readily generalized to curved spacetime by replacing partial derivatives with their curved-manifold counterparts, covariant derivatives studied in differential geometry. With this additional condition—the covariant divergence of the energy–momentum tensor, and hence of whatever is on the other side of the equation, is zero— the simplest set of equations are what are called Einstein's (field) equations: Einstein's field equations {\displaystyle G{\mu \nu }\equiv R{\mu \nu }-{\textstyle 1 \over 2}R\,g{\mu \nu }={8\pi G \over c{4}}T{\mu \nu }\,} G{\mu \nu }\equiv R{\mu \nu }-{\textstyle 1 \over 2}R\,g{\mu \nu }={8\pi G \over c{4}}T{\mu \nu }\, On the left-hand side is the Einstein tensor, nt and c the speed of light.[37]&gt;&lt;&lt; all 1 comments sorted by: bestdisable inbox replies (?)pin to profile -bash: syntax error near unexpected token `&lt;' Tom-Thistlethwaite-macbook-air:~ tomthistlethwaite$ lookingforsomehope1 • 1 point • submitted 45 minutes ago -bash: lookingforsomehope1: command not found Tom-Thistlethwaite-macbook-air:~ tomthistlethwaite$ bash (POWERSHELL.C++ (C+C+++C+++C+++B++++++++!!!! GO&gt; SERVER! bash sudo ii (((LOGIN RING)0###PGP &lt;&lt;&lt;&lt;&lt;&lt;&gt;&lt;&lt;&lt;C+++C++++{{BOOLEAN #hex gate@ sha.256..salt#lyra2! public static void main = =init main shell. Bash??&gt;&gt;&gt;&gt;&gt; QUBIT!!! QEDJ++Therefore, at the scalar potentials, the effects are stationary projected to and communicated from their reciprocal opponent, shown as the following conjugate pairs: 18 mins · if f(z) is a holomorphic function restricted to the Real Numbers, it has the complex conjugate properties of f (z) = f (z), which leads to the above equation when {\displaystyle {\hat {x}}{}={\check {x}}} {\displaystyle {\hat {x}}{}={\check {x}}} is satisfied.
Yeah this might be the way to do it. Multi step but if it's accurate it'll be worth it in the end. 
Just consumed my time. Here ya go, this is what I accomplished. Took me longer than I would like to admit; Basically I don't have selector gadget on my mac and I took a while trying to find the right code that identified the color for each cell. And the data wrangling is an ugly mix of base R and dplyr, if you can improve it please let me know. library(stringr) library(tidyverse) library(rvest) page &lt;- read_html("https://www.transfermarkt.co.uk/arsenal- fc/ausfallzeiten/verein/11/plus/1?reldata=GB1%262016") ##Create basic data frame for players, positions, and an id column name &lt;- page %&gt;% html_nodes(css = "td.hauptlink.no-border-rechts.wsnw") %&gt;% html_text() position &lt;- page %&gt;% html_nodes(css = "td.ausfallzeiten_pos.no-border-links.zentriert") %&gt;% html_text() player_positions &lt;- data.frame(name, position, player_id = 1:length(player)) ## Get game status for each player via color code df &lt;- page %&gt;% html_nodes(css = ".afz") %&gt;% html_attrs() %&gt;% matrix(nrow = 42, byrow = FALSE) ##Text extraction - extract the color codes via the pattern "ausfallzeiten_" pattern. df2 &lt;- df df2[] &lt;- apply(df, 2, function(x) unlist(str_extract_all(x, "(ausfallzeiten_) [srkbgekv]*"))) ##Create lookup table, match each pattern with the right color and legend value. lut &lt;- c("ausfallzeiten_bg" = "not included", "ausfallzeiten_k" = "on the bench", "ausfallzeiten_s" = "starting eleven", "ausfallzeiten_r" = "absence/injury", "ausfallzeiten_e" = "substituted in") ##Substitute in the newly coded status for each color code extracted previously. df2[] &lt;- apply(df, 2, function(x) lut[]) ##Get data in proper columns and rows... by use of matrix function. Gather players into single column for more string manipulation. players_status &lt;- lapply(df2, function(x) lut[x]) %&gt;% matrix(ncol = 42) %&gt;% data.frame() %&gt;% gather(player, status) ##Get rid of "X" in front of player name to match player id in player_positions data frame. ##Make player id an integer to match other dataframe. players_status$player &lt;- unlist(lapply(status[1], function(x) gsub("X", "", x))) players_status &lt;- players_status %&gt;% mutate(player = as.integer(player)) ##Sigh, more wrangling. Create ID variable for each match. A bit round about but it works. players_status_final &lt;- players_status %&gt;% mutate(one_game = 1) %&gt;% group_by(player) %&gt;% mutate(match_num = 1:sum(one_game)) %&gt;% select(-one_game) %&gt;% unnest(status) ##Let's merge players, positions, and the game status now players_status_final %&gt;% left_join(player_positions, by = c("player" = "player_id")) 
Thank you for this. I will take a look when I am in front of my computer and let you know of ghe results. Really really appreciate your time and effort!
Yeah, run it line for line and it aught to work - i just fixed a kink out of it. I sorta inspected the css manually through trial and error.. yeah. Not sure why I did that, but it is probably boredom and stubbornness, lol. 
Mightily impressive! Your productivity while bored is a sight to behold hahah
I don't see "Add rtools to system PATH" option when I try to download the latest rtools. 
When choosing what family to use, your first concern should be the domain of your response variable. If it's constrained to be positive, you have to use something that's constrained to be positive. Same for integers. If there's overdispersion or a large number of 0s, a poisson is not going to work well so you might have to do a quasipoisson. Using the canonical link is usually a good idea, but you should look at the relationship in your data between the mean and the variance. If it doesn't fit given the link function you're using, you may consider using another. You may run into problems using the canonical link with gammas, by the way.
Hey there what you're asking for is a lot more than can be answered in a single post. I would, however, recommend using this resource: r4ds.had.co.nz to get starting. You'll want to create a new script, copy the commands into your script and run them in the console
Selector Gadget is my favorite tool. It's like magic when you've got to find super deep nested cells within a table like with a baseball box score or some such. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/nyhYQzC.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e3rhq9a) 
Imagine a list X is a train consisting of a bunch of traincars, each holding a passenger. Then X[1] gives the first train car, while X[[1]] gives you the passenger in the first train car.
Great explanation, X[] always returns an element of the type X, X[[]] returns an element of the type that 'X' is holding.
list[1] gives you the subset of the list, which is again subsetable: list[1] is a list i.e. list[1][[1]] works, while list[[1]] gives you the content directly. When you want to call multiple subsets of lists, you have to use list[1:3]. That's actually the only time I use single brackets for lists. It all doesnt make too miuch sense to me.
http://r4ds.had.co.nz/lists.html This page has a great analogy with a pepper shaker the explain the differences
Do you need that final dataframe? or are you going to do some operations on it after that. data.table may have what you want. The code for what you're trying to do is: DT &lt;- data.table(your.data) DT2&lt;- DT[DT,on="category",allow.cartesian =T][,c("xdiff","ydiff") := .(x-i.x,y-i.y)] names(DT2)[4:5] &lt;- c("x1","y1") (I think) 
Sorry, didn't realize it hadn't made it to CRAN yet. As of this week, tidybayes is finally up on CRAN, so it can be installed the usual way with `install.packages("tidybayes")`.
You are in Vegas and the only thing you could think of is coding????? You're my kinda guy (or gal).
I might be able to help, but I'm failing to see how it is possibly infinite rolls. Roll 1 is either loss win or point. roll 2 is either point (win) or loss. How does one get beyond 2 rolls in a single ”go”
&gt; &gt; &gt;Thanks very much &gt; &gt; &gt; &gt;As given that's a Montecarlo simulation, what are these results, in your opinion?
&gt;that's a Montecarlo simulation, what are these results, in your opinion?
&gt;I just use default link function. I mean, I use glm( &gt; &gt;y \~ x + x &gt; &gt;, family=poisson). Why poisson?
A large application of stochastic differential equations is in the study of dynamical systems, such as epidemic and diffusion models.
Ah ok, now I got what do you mean. So, you need to study and understand glm models and errors distributions and their assumptions. You need to choose the best distribution according with your data, mainly your y-variable. Which distribution fit better in your model? That is te main point. You need to do that troght the error analysis. Quasi- is just for overdispertion data. Some data, as population weight, is gaussian. Species count usually is poisson. Binomial data (0, 1)... but you need to check the model error to certify that. 
If you use subset precisely like that, you've passed the already-subsetted df to the function. I think you mean: df2 &lt;- subset(df1, conditions) Subset is good for interactive data exploration, it isn't so good for programming, us [] for that. The non-standard evaluation can come back to bite you. 
Ah, that's a good point... Thanks for the advice :)
Hi there, it appears that gather() doesn't work on my version of R studio. I installed both the tidyverse and tidyr packages in case that was the reason. Running the code up to gather(player, status) provides me with the 38 by 42 matrix filled with NAs (that explains the needing of gather() I suppose) 
Why is it not good for programming?
Hey, sorry about that. The problem was in the 6th and 7th code blocks, I accidentally repeated the same procedure. I applied the look up table (substituting the status in for the color codes) in an individual step, and down below I tried doing it again and passing it to gather() which didn't make any sense - after replacing those items, there's nothing else to replace, so it all comes back as NA. I'm updating the solution I posted in the original post - basically I am looking at this line: df2[] &lt;- apply(df, 2, function(x) lut[]) players_status &lt;- ~~lapply(df2, function(x) lut[x]) %&gt;%~~ df2 %&gt;% matrix(ncol = 42) %&gt;% data.frame() %&gt;% gather(player, status) 
Brain is fried today but I'll give it a shot tomorrow and report back. Thanks!
I think if you use the httr::content() function without the type = 'text' arg, it'll recognize the json and convert to a list. You can then figure out how to extract elements from the list and flatten it into a data frame.
Hey there, I figured out the immediate problem -I had edited my data frame with the lookup table and then accidentally tried repeating the same code on it in the next pipeline, which didn't work since it was already edited. Anyway... I just noticed the table on the site is messed up - look carefully at the game numbers - they're out of order! Therefore that messes up our original game_num column - (I didn't scrape that, I simply did a sequence of 1:nrow(.), gotta be careful with that, if data is clean it works, but in this case, it didn't. Since I'm on a computer with selector gadget it is easy to fix this, i'll simply scraped the game numbers and bound the column in so that they match in the correct order. Also i noticed there were hundreds of NA values - the scraping didn't catch the injured players very well. I manually went in and hard-coded "injury" for their status. There are still 58 NA values - most of them are "not included" (grey colour) but they have a club insignia on them or 3 lines, since i'm not sure what that means I don't feel confident in editing those NA values. The entire post has been edited and it should all work now.. please try it out again!
I have also asked on stackoverflow: [link](https://stackoverflow.com/questions/51765374/read-cvs-faster-than-data-tablefread)
Every algorithm has a fixed part and a scaling part to its runtime complexity. Fread() apparently has more fixed overhead so it's slower on very small files like this, it will slow down less as file size increases. Worrying about what file reading function is fastest is premature optimization though, worry about it when reading in data actually becomes a bottleneck.
Can you give an example in your last sentence? [...worry about it when reading in data actually becomes a bottleneck.] :)
Well for instance when your script spends more than 10% of its runtime reading in data, or reading in data is the slowest single action in your script. Profiling really helps to understand where your optimization efforts are best spent at any given time (though only if the runtime of your code is problematic in the first place!). The lineprof package is great for understanding the time cost of every line in a script.
I’m confused what’s wrong with your current picture? 
This is the single best answer you will get. jsonlite is a no brainer for dealing with jsons. Especially for nested structures it takes away all the pain of flattening this thing.
You have to learn to crawl before you learn to walk...
Alternatively, there is the feather package... https://blog.rstudio.com/2016/03/29/feather/ I'd typically read in a csv initially, then write it as a feather, and from that point on, I would only read in the feather.
Good question. Unfortunately you marked as “accepted” an answer that gives a wrong explanation.
I see, I have checked the correct answer now :)
I use a similar pattern but with RDS, which is part of base R. Last commit on feather was 11 mo ago (CRAN back in November 2016) and the newest version depends on Apache Arrow, which has no good bindings for R. I’m not sure it will be useful in the coming years. 
Wow, I was unaware of that. Thank you for the heads up!
can you provide a sample of the data and an english explanation of what you're trying to do? I'm interested in helping, but want to make sure I get it right... 
Feather is a very simple library though, so there might just not be a need to actively develop it.
Is there anyway to do compression with Feather? I've been using feather too, but wishing I could easily gzip/ungzip the dataframes automagically. 
This suggests they want to but they are limited by lack of Arrow support on R... It seems more complicated than I would have thought until I looked into it today: http://wesmckinney.com/blog/feather-arrow-future/
THANKS! I got it. It's not pretty but it works. There's still a lot more steps to this project like rotating the URL based on my original data sheet but this part is done! I really appreciate it. I was having some issues because the fromJSON command was still returning a list but it was easy enough to fix through base R coercion. Posting the code in case other run into a similar issues in the future: # This package is required for Accessing APIS (HTTP or HTTPS URLS from Web) library(httr) #This package exposes some additional functions to convert json/text to data frame library(rlist) #This package exposes some additional functions to convert json/text to data frame library(jsonlite) #This library is used to manipulate data library(dplyr) #API requests resp &lt;- GET("https://projects.propublica.org/nonprofits/api/v2/organizations/10549867.json") #Status check http_status(resp) http_type(resp) # Raw data which is not structured and readable data.cont &lt;- resp$content #Raw data to character data.char &lt;- rawToChar(resp$content) #Convert to workable list data.flat &lt;- jsonlite::fromJSON(char, flatten = TRUE) #Extract DF for years filed df.filing.with &lt;- as.data.frame(df[["filings_with_data"]]) #Extract DF for year not filed df.filing.without &lt;- as.data.frame(df[["filings_without_data"]])
My go-to for getting out of messy loops is to functionalize them, like in your attempted fix, but you've only done it halfway. Move the teste2$SALDO to a function parameter and generalize the saldo_fn. Avoid the apply family here if you don't know how it works. 
Make (a) function(s) and [https://csgillespie.github.io/efficientR/7-4-the-byte-compiler.html](https://csgillespie.github.io/efficientR/7-4-the-byte-compiler.html) 
Two words: vectorised computation. R is at least an order of magnitude more efficient working with vector operations than loops. Can you give us some sample data to work with, and comment your code so we can roughly see what each block is supposed to do. 
You're redefining saldo_fn NROW(teste2) number of times and then you execute it each time with a j value of NROW(teste2), since that is the last value of j.
tail(vector, n=1)
df[3, length(df)] This will give you third row, last column. 
Is there a way to do this when it isn’t a function? Would make it alot easier to work with
Not that I know of. Try the other answers too in case you like how they work.
here's a quick and rough function illustrating a zero-inflated poisson. The example here draws 100 samples from the distribution I listed below. The mean != variance. set.seed(20180809) rzipois &lt;- function(n, lambda, p){ # n = number of draws # lambda = poisson parameter # p = proportion of zeros zeros &lt;- rep(0,n) lambdas &lt;- rpois(n,lambda) samplematrix &lt;- cbind(zeros,lambdas) results &lt;- samplematrix[cbind(1:n,sample(c(1,2),n,TRUE, prob = c(p,1-p)))] results } draws &lt;- rzipois(100,4,.5) mean(draws) var(draws) mean(draws == 0) You should get a mean of 1.39 and a variance of 4.92 with that seed - very different!
If you want last value in 3rd row then you can use data[3,-1]
This takes the third row and drops the first column.
indeed, my apologies I tried it with a 2 column dataframe haha I guess the best way to do it then would be data[3, ncol(data)] as mentioned by others already
No, it does *not* work. If you tried it then it accidentally worked because your table only had two columns. Try it with a table with more columns to see the effect.
&gt; You should get a mean of 1.39 and a variance of 4.92 with that seed - very different! so if you were fitting this as a poisson, you would note that and that the proportion of zeros is high, so maybe a zero-inflated poisson is better than a straight poisson. And then you fit a model and get the right answer. Thank you, anyway I am new to statistic, so I don't understand what ''0'' are you talking... What are those 0? You said ''the proportion of zeros is high''.. may I tell you where? In the Y ? Where do they came from those zero? 
Careful with that. If you accidentally handle a matrix instead of a data frame length will be wrong. Ncol is generally safer 
Thank you! Also, I was playing with the code to try to figure it out myself and I forgot to switch back the levels. The original code actually had levels = c(1,2). Does that mean it drew a line where it changes from 1 to 2?
Saying what exact levels to draw the contour at - `c(2)` or `c(1,2)` - says what exact levels to draw it at. Actually I think it only draws at the negative gradient (because that's how a contour works - everything inside the contour is &gt;= to the line), so putting `c(1)` won't draw a line.
Ah okay makes sense. Thanks a ton!
I'm not sure what you want, I don't think you know, either. There are a couple of terms that you are using ambiguously. What sort of Monte Carlo simulation would you like to do? Can you phrase it as "I am looking for the empirical distribution of _____"? (Monte Carlo simulations are a huge class of methods, you need to be much more precise - maybe reference a specific part of the Wikipedia page?) What do you mean robust? Robustness generally refers to violations of assumptions, so what assumptions are you looking at? That's not a parameter; at least not the way statisticians usually use them. A parameter is usually a stand in for a number that must be estimated. What would be a reasonable number for your parameter? What range of values can it take? Can your parameter be negative?
I haven't read your entire post, but perhaps what you should do is define the function so that the place within the function where it hits an error you are using try(), then you can have something in order to catch the error and output something like NA on those spots.
Check out safely() from hadley's purrr package. Then enjoy that weekend!! It's exactly what ypu want. 
A has 60%. B has 40%. No conditionals required. Just do the arithmetic from the problem. 
I did not get this. Lets see. At the beginning of the game player a has a probability of 40% of winning, but: * it raises 4% every point in a row * he has 5 points in a row so I believe player a probability by now it 40% \* (1 + 4%)\^5: a composite rate, this gives us 49% of winning. Now player B starts the game with 60% of winning chances, but: * it gets 5% down for each 5 minutes after the first 1h * we have 1:20h of game and nothing so I believe player B probability by now it 60% \* (1 - 5%) \^ (20/5): a composite rate, this gives us 49% of winning. Is the remaining 2% a draw probability? They are stated to be dependent on each other? Let's assume yes for now. The conditional probability would be P(B|A) = P(B) / P(B inter A) In the beginning of the game, they are mutually excludents, so there is no P(B inter A) and now there is a gap between them of 2%. If P(X) is winning then P(B inter A) would be draw, so the 2% fits here. However, both A and B are changed, this does not sound right to me. Maybe they are not conditional at all.
Is this data manually entered?
This data frame represents 3 different forecasts I smushed together. I want to do this analysis so I can quickly find large changes from one forecast period to another.
&gt; so I believe player B probability by now it 60% * (1 - 5%) ^ (20/5): a composite rate, this gives us 49% of winning. &gt; &gt; Is the remaining 2% a draw probability? They are stated to be dependent on each other? Let's assume yes for now. &gt; &gt; The conditional probability would be P(B|A) = P(B) / P(B inter A) &gt; &gt; In the beginning of the game, they are mutually excludents, so there is no P(B inter A) and now there is a gap between them of 2%. If P(X) is winning then P(B inter A) would be draw, so the 2% fits here. However, both A and B are changed, this does not sound right to me. Maybe they are not conditional at all. Thank you so much you have been very explanatory. If you want the two events to be correlated (negativley correlated) I can tell you that they have a negative correlation and Pearson coefficient of -30 (A, B) 
&gt;A has 60%. B has 40%. No conditionals required. Just do the arithmetic from the problem. Thank you. Let's suppose both events they are negativ correlated , Pearson coefficient -30 How to set and to solve the equation to get what would the probabilities for one player at a given chance?
&gt; . &gt; &gt; What sort of Monte Carlo simulation would you like to do? Can you phrase it as "I am looking for the empirical distribution of _____"? (Monte Carlo simulations are a huge class of methods, you need to be much more precise - maybe reference a specific part of the Wikipedia page?) &gt; &gt; What do you mean robust? Robustness generally refers to violations of assumptions, so what assumptions are you looking at? &gt; &gt; That's not a parameter; at least not the way statisticians usually use them. A parameter is usually a stand in for a number that must be estimated. What would be a reasonable number for your parameter? What range of values can it take? Can your parameter be negative? Sorry I am new to statistics. So maybe I can say somenthing worng. Basically I am trying to predict the development price of a stock. So with the montecarlo simulation that should give me a range where probabilistic the stock could be. This distribution will be a gaussian. To run a simple montecarlo simulation is easy. Anyway I want to factor in the Montecarlo, the fact that when a stock had a huge pump in one day (let's say more than a 20%) at the next day it will go down to the pre-pump levels + 10% (the price levels before the pump plus 10%) and this random variable (price drop) has a 60% of probability to happen. I don't know how to write this with R... this is my problem..
So have you written a script that semi-automates the importing of data? For example, save the .csv files in a specific location (You could even automate this in R by using a PowerShell package. I can provide more detail if needed). Then, read_excel(file path) from the "readxl" package". This code can be run on a timer in RStudio to essentially automate the file import. The same timed code execution can be done for statistical modeling and plotting as well. Is this the type of "effective" you're looking for? Or are you looking for "effective" analyses? If it is the latter, more specifics will be needed regarding your objective. What does management (or you) do with these data? 
Thanks, reading up on them now
Yes, the script I currently have reads in 15+ forecasts I received in excel documents and puts them into 1 document that is similar to the example I provided in my original post. I am looking for a more "effective analyses" of this data. Plotting the data is kind of easy, I am trying to find a more effective way to spot large changes in the forecast without going through every line item to do so. I.e. I want to search for any change in the forecast that changed \~20% (could be any number really) from the last forecast. Using the example above, the Toyota Supra Month1 forecast increased 33.33% from the 2/1/2018 forecast of 15 units to the 3/1/2018 forecast of 20 units. Does that make more sense?
Gotcha, but there is still more inquiry!!! I love it. Are you looking to simply describe what is occurring? Are you instead trying predict something? If so, what? Even further, are you trying to determine the cause of fluctuations? Maybe PM me, because this could be a long thread.
I think you are trying to peel back too many layers of the onion haha. I'm just trying to find out how the forecast changed and by how much.
RGL is a really good tool for interactive 3d plots. I'm pretty sure there is a function that can plot line segments using vectors
Plotly
Where* 
[https://support.rstudio.com/hc/en-us/community/posts/200640838-automatic-tab-after-enter](https://support.rstudio.com/hc/en-us/community/posts/200640838-automatic-tab-after-enter) 
Saw this while on holiday, unlike you I decided until I was back at work and slightly bored to try it out library(microbenchmark) craps1 &lt;- function() { Obsnum = 1:10000 roll &lt;- replicate(10000,sample(1:6,1) + sample(1:6,1)) DF &lt;- data.frame(Obsnum,roll) flags&lt;- rep("ROLL",100000) come_out &lt;- 1 # 0 is "point cycle" point &lt;- 0 for(i in 1:length(DF$roll)){ if(come_out){ if (DF$roll[i] == 7 | DF$roll[i] == 11) flags[i] &lt;- "WIN" else if(DF$roll[i] == 2 | DF$roll[i] == 3 | DF$roll[i] == 12) flags[i] &lt;- "LOSE" else{ flags[i] &lt;- "POINT" come_out &lt;- 0 point &lt;- DF$roll[i] } } else{ if(DF$roll[i] == point){ flags[i] &lt;- "WIN" come_out &lt;- 1 point &lt;- 0 } else if(DF$roll[i] == 7){ flags[i] &lt;- "LOSS" come_out &lt;- 1 point &lt;- 0 } } } cbind(DF,flags) } craps2 &lt;- function(n = 10000) { stopifnot(n &gt;= 1) Obsnum &lt;- seq_len(n) roll &lt;- sample(1:6, n, replace = TRUE) + sample(1:6, n, replace = TRUE) flags &lt;- character(n) first_roll &lt;- TRUE point &lt;- numeric(0) get_flag &lt;- function(i) { if (first_roll) { if (roll[i] == 2 || roll[i] == 3 || roll[i] == 12) { return("LOSE") } if (roll[i] == 7 || roll[i] == 11) { return("WIN") } point &lt;&lt;- roll[i] first_roll &lt;&lt;- FALSE return("POINT") } else { if (roll[i] == 7) { first_roll &lt;&lt;- TRUE return("LOSE") } if (roll[i] == point) { first_roll &lt;&lt;- TRUE return("WIN") } return("ROLL") } } for (i in 1:n) { flags[i] &lt;- get_flag(i) } data.frame(Obsnum, roll, flags) } microbenchmark(craps1(), craps2(), times = 50) results1 &lt;- craps1() results2 &lt;- craps2() I have replicated your output structure with code that's about 30x faster. The key observation is that since each roll is independent of the previous, you can just generate al the rolls in a single go, which is significantly faster than doing them one by one. See: f &lt;- function() { sample(1:6, 10000, replace = TRUE) + sample(1:6, 10000, replace = TRUE) } g &lt;- function() { for (i in 1:10000) sample(1:6, 1) + sample(1:6, 1) } microbenchmark(f(), g(), times = 50) In general you should use "vectorised" code, that is any code which can operate on and return whole vectors should be used to return whole vectors rather than using some kind of for-loop. This is often a source of tremendous performance gain as seen above. The other thing is that I'm using a function to compute the flag based on some fancy R scoping rules. When you declare a nested function, that function can use any variables in its parent environment, it can also assign to the parent environment using `&lt;&lt;-` rather than `&lt;-`, this allows me to write the code in such a way that uses `return(...)` rather than `flags[i] &lt;- ...` which distinguishes the important flag setting lines of the code from the less important regular assignments. I've also used actually booleans of `TRUE` and `FALSE` rather than 0 or 1. One other interesting note is that if you want n complete games, you won't be able to figure out how many rolls you want beforehand; it may actually be faster to just simulate large sequences of rolls as done here at a time, then discarding any excess rolls once you reached the desired number of complete games, as opposed to doing one roll at a time and doing the minimum number of rolls.
Since version 3.4 R has a jit compiler for all functions. I wouldn't expect compiling by hand to do much.
Rather than generating the rolls, which I agree is vectorized, please remember that this data was supposed to be ALREADY GENERATED. The only reason that I included the "generation" code was so that we could all work on the same dataset. (Emphasis only because everyone has been talking about probabilities, and rolling in situ). Admittedly, I may not have made it clear in the original post (Updated). My reply to the original post was an attempt to address this. If you have telemetry (or "collected data") how can I make it fast to do complex computations like this (how can I vectorize it)? If your code is doing this, and I'm incorrect in the 
Not to bother you. But your unlist, doesn't work 100% for the problem definition I gave. As I said, when I have ";" i works like a AND. So in my list mylist &lt;- "ac*, be, cd*; daa, efae*, fge*; gefa*, h" when I run the code, I have to atleast have 3 elements as output. If I have had, another list, e.g. mylist2 &lt;- "ac*, be, cd*; daa, efae*, fge*, gefa*, h" then I would have required to of the elements to be as output. Is there a way to make a modification to this, because currently unlist will put them all in the same vector and then is no way for me to know if the list I have any ";", because the output from `unlist`is unlisted_list = "ac*" " be" " cd*" " daa" " efae*" " fge*" " gefa*" " h" Hope you understand my question :)
Hi. I start to answer the questions as they appear. &gt; Why did he set the α1 = 0.6? Based on what? Based on pure luck. There are ways to calculate a_1 based on the time series, but in this case, it's an example to show the concept. &gt; And what is this coefficient? What does it rapresent? (In simple words) It represents how much the last date impacts today. Here, 60% of yesterday are the base value of today and then we add some error to it. &gt; The loop ''for'' from 2 to 100..... what is that? Is it the number of simulations? It is just calculating the formula for the values from 2 to 100, always taking 60% of yesterday and adding the error of today. &gt; My question is WHAT IS THIS ACF graph? How to read it? What does it mean with LAG? What is LAG? The acf shows, how much today is dependent of yesterday, two days ago and so on. The bigger the line, the greater the influence of this "lag". "lag" is the amount of steps to go back in time. &gt; why do we need to find out the ''αi'' parameter if we already knew it is 0,6? You already know it's 0.6, but that's coming from the example data you are creating. You wouldn't know in real life, hence you need to estimate the a_i
tryCatch() is what OP needs. [https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r](https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r)
https://cran.r-project.org/web/packages/scatterplot3d/index.html 
There's no general way to make complex calculations faster, you have to exploit the specific properties of the calculation you're trying to perform and the structure of the data you're working with. The first question to ask is whether you actually need it to be faster, unless the code is actually unreasonably slow then there's no great reason to look for optimisations. If you do need to optimise then look into Rprofvis with RStudio, this will tell you what code is slowing you down. If your calculation is not stateless (i.e. in this case where the result depends on the previous roll) then in general you will not be able to vectorise. If you can break it into independent chunks you may be able to use `mclapply` from the `parallel` library to run functions in parallel.
Thanks, but this isn't quite what I'm looking for. It looks like this would just plot the forecast for a particular item and let me know whether the change in that forecast is random or not.
Ugh, I'm slightly out of touch then... Second try: Rcpp? [https://cran.r-project.org/web/packages/Rcpp/index.html](https://cran.r-project.org/web/packages/Rcpp/index.html)
Time series is a complicated subject that can be followed to into mathematical depths and then eventually by just accepting that everything has a unit root you're test just didnt pick it up for 1 of a dozen reasons (jk...kind of). But a great starting point (that will also help with your R) is https://otexts.org/fpp2/ 
Imagine this same example with 30+ concepts and opportunities. Making a graph for each one and moving through them one at a time would be quite a process. I would like to find the percent change for each one so I can filter on the ones that changed (let's say) ~30% from the most recent forecast (3/1/18) to the one that was received before that (2/1/18).
Totally depends on what you want to do. Munging many streams or spreadsheets of data into a common unified form will be far easier in python. Replicating basic spreadsheet funcions will probably be much easier in python. Visualizing, analyzing, and forecasting is going to be way easier in R. Statisticians and economists work in R. There are multiple task views for [empirical finance](https://cran.r-project.org/web/views/Finance.html) or [econometrics](https://cran.r-project.org/web/views/Econometrics.html) that have specialized tools, graphs, and models for financial data that python simply doesn’t have. Complete set of [task views is here](https://cran.r-project.org/web/views/). R is more powerful than python in many ways, but it is a programming language built by statisticians. python is far simpler to pick up and use right away, as the learning curve for R syntax is much more brutal and steep.
Is it sufficient to simply import/export from/to excel? Both R and Python can do this no problem. Admittedly I haven't had more experience than reading in data from someone's excel spreadsheet, manipulating the data, and exporting the results to a new file or sheet. 
Whoops, to answer the actual question asked, R has several packages that can read excel sheets in their native format. It’s just xml after all. I tend to use CSV as a lowest common denominator exchange format in both languages because it’s easier that futzing around with why importing native excel data won’t work. What exactly do you mean by “integrating” with excel? That can mean [a million different things to different people](https://www.r-bloggers.com/a-million-ways-to-connect-r-and-excel/)
Chiming in to say that I overall agree with what you say aside from Python being easier to learn than R. For many, including myself, R was much easier to learn and use than Python. R is decidedly different than any programming language developed by computer scientists, and as a result it is often easier to pickup by non-computer scientists. In any case, ymmv with learning either language and one isn’t necessarily easier to learn than another. 
Main use would be conversion of spreadsheets containing financial models - so Python/R would need to do a lot of handholding for an end user in terms of their works flows and day-to-day, but still needs to be powerful and robust enough to work like a model that can be trained and fitted to model parameters.
Wow this it. That is the right formula for conditional probability. Mine was inverted. Sorry.
I’ve used R package ‘XLConnect’ to load in Excel sheets. Not really any issues other than you need to make sure the architecture (32-bit/64-bit) matches between Java and R. It loads in the sheet as a data frame.
Some caveats 1. This method treats your data like a time series as defined by R, which changes the date format. 2. Since your opportunities are unique but stay the same, you would just copy and paste this code for each opportunity name. Then your script should be relatively set. 3. Since I assume your data frame simply grows one row and by one column, this method references the last cell in your data frame in the conditional statement. If this isn't the case, let me know. Adjustments can be made. supra &lt;- ts(forecast[grep(pattern = "Supra", x = forecast$Opportunity),]) supra.ts &lt;- supra/lag(supra,-1) - 1 if(supra.ts[nrow(supra.ts),ncol(supra.ts)] &gt; .30){ print(supra.ts[nrow(supra.ts),ncol(supra.ts)]) }
Right, based on your answer to the other guy and this one, definitely take a look at [RExcel](http://rcom.univie.ac.at/download.html). It lets you run code like VBA in excel, or run R code from within VBA in excel. Might be what you are looking for. Other option is using shiny-server/shinyapps.io to build custom analytic dashboards that allow your users to upload different XLS sheets. That’s a heavy lift for one guy without a lot of experience in R. It’s totally doable, but expect it to take months to develop anything useful. Once you get the hang of it though it’s... crazy how fast you can develop interactive modeling and visualization dashboards. If you go the python route, maybe a jupyter notebook [with a set of interactive widgets](http://jupyter.org/widgets) would work. They wouldn’t really be “integrated” with excel though - probably more like your user points a notebook at a standard-format excel file and gets a set of interactive graphs and predetermined analyses. I don’t know a way to integrate python directly into excel off the top of my head, but I’m sure there is one out there and only a few minutes of googling will produce something that works. Excel is the workhorse of science and business for non-programmers.
Oh cool - I'll give XLConnect a try, thanks!
Wow, really? I’m curious - did you learn R using just base R, or with tidyverse/dplyr? I really wish we could get a language with the readability and easy syntax of python with the pure I WANT TO DO STUFF FAST analytic-power-on-steroids that R brings to the table. Although dplyr is bringing us so much closer to that than we were even 5 years ago.
This is some great info - thank you! I'll start reading up on RExcel tomorrow. shiny-server looks interesting but I'd need to get comfortable with R, as you said. I have been consider Jupyter Notebooks (love the literate programming style, similar to R Notebooks), but I'd have to think how to get buy-in from business types who have never used it. Nice to know about it to play around with it though. With Python, I've been using xlwings for the most part. It's been fun, but difficult while trying to do the business-as-usual stuff with no one to bounce off of. Just gotta keep at it and get better, I suppose.
I've found it quite convenient to read and write from Excel with the clipboard (for reading) and CSV files (for writing). Some user named geneorama made some functions for this purpose. https://github.com/geneorama/geneorama/blob/master/R/wtf.R https://github.com/geneorama/geneorama/blob/master/R/clipped.R
My team is currently automating our organization's HR reporting processing, moving from Excel to R. Finance and other corporate functions are also looking into R. R has some great packages for "speaking" with Excel, have a loot at `readxl` for reading and `xlsx` for reading/writing. There are even packages that allow you to highlight, indent or otherwise modify Excel files. More importantly, you can automatically generate reports and (powerpoint) presentations with R markdown! Via Shiny, flexdashboards and many other packages you can create beautiful dashboards in R. R was actually bought by Microsoft recently and thus also integrates quite nicely with PowerBI - a very nice dashboarding tool. For my team, R currently is our swiss army knife.
I started on base R but quickly focused on the tidyverse. More than anything else, tidyverse provides the kind of straightforward, consistent, and as a result intuitive syntax that makes it so easy to progress through an analysis. 
Both could get the job done. Unless you’re building applications that only you will use, I think you should pick the tool that will be most readily understood by your coworkers. If you’re team comes from a (modern) finance background, R might be the better bet as most universities now teach statistical languages (eg STATA, R, Matlab). Otherwise, I think Python would be more readable for semi-technical users. Best of luck, I’d love to hear an update on your final decision!
if you do limitations with python or r in Excel, I've used c# in the past and Microsoft office objects to directly control Excel instances. There are some issues but virtually all functionality available in Excel is possible through this means. 
That’s awesome. Thank you for sharing. 10 years ago, when I was introduced to R, base R was really all there was. The magrittr pipe wasn’t even a thing. Hell, there wasn’t even an IDE - I was taught to write in notepad and copy-paste to the terminal. Watching the effort the R community has put into tidyverse and the growth of RStudio has been amazing and gratifying. And honestly, it’s probably time I change my conception of R and how hard it is to learn. In my head, I guess “learning R is hard” is a thing because.. well, it was. Brutally so. The learning curve isn’t as steep as it used to be. Hell, I use the same packages every day because they are intuitive and produce readable code. Thanks for the wake up call. 
Couple of reasons I prefer munging in python. Python by its nature produces more readable and comprehensible code - readable code being the motivation for the language in the first place - and it has a lot more functionality for manipulating basic data types than does R. Errors and exceptions are typically easier to handle. And oh my lord OOP makes so much more sense. Another commenter in this thread pointed out that R isn’t as syntactically retarded as it used to be thanks to tidyverse - something was a bit of a wake up call for me - but python is still a neater language overall by design. ETL is something that I find is relatively stable over time. Once I get an ETL pipeline in place - say to read a certain type of raw CSV files produced by a sensor, cast the data to types, do some basic QA/QC, map to the fields in a database, and then insert - it tends to stay stable for months. Until suddenly it doesn’t work 9 months later because of a firmware change and I have to go and pull apart hundreds of lines of code and figure out what the hell they do. The enforced readability of python is a big asset there. Second, working with list comprehension, dictionaries, and dictionary comprehension is the fucking kryptonite for ETL work. Using lists and dictionaries in combination gives you lots of options for mapping field names, data types, and functions across many different input files, correcting or skipping problems by element, by row, or by column, as you encounter them. Most of the nested list and dictionary patterns are arbitrarily expandable - or easily extendable to things like json data. Working with json in R is a nightmarish compared to python. Last, string formatting is a bit easier, as are date times, because python has 1 readable set of built in standard methods - not a hodgepodge of packages that you have to hunt through for that one function you need but can’t remebed the name of. And then you forget if this function is called this way, or that way, and NO DAMNIT THIS FUNCTION USES na.omit = T not na.action = “omit” FUCK FUCK FUCK WHY GOD WHY Just my personal opinion but generally everything related to QA/QC and ETL is just a little bit easier in python - and those little bits really add up on those tediously huge ETL jobs. 
Very welcome. Best of luck in your journey
That depends on your working directory (wd - where R is looking right now) , which depends on how you initiated your R session. If you just executed R executable, your wd would probably your home, if you are using an Rstudio project (you should) your wd will be where the project is started. To be able to read the file the "directory_data" either has to be set to the absolute path to the folder where the file lives or to a relative path based on your wd. I'm going to assume you're on windows. An absolute path would be something like "C:/users/yourUserName/desktop/test". A relative path assuming you just started R executable without setting your wd, which means your wd is probably "C:/users/yourUserName/", would be "desktop/test".
Thansk for taking the time for this elaborate answer. 
[https://readxl.tidyverse.org/](https://readxl.tidyverse.org/) readxl for reading (non-java dependent) and apart of the tidyverse [https://github.com/awalker89/openxlsx](https://github.com/awalker89/openxlsx) openxlsx for writing excel files (non-java dependent) [https://github.com/leeper/rio](https://github.com/leeper/rio) is a good package to make importing/ exporting data from R easier (both use the above packages)
Thanks! It currently is a toss up - I'm getting more comfortable with Python and have had some success in delivering automation solutions in my area. Most of my team are non-technical users (with me leading the charge, so the speak) so the decision of picking which of the two relies mostly on what I can make work and whetting the appetite of my area. It's been tough without any peers to code with, but getting info here and on the internet has been quite helpful.
It's encouraging to hear that you've had success transitioning from Excel to R, and that there is enough buy in for other areas to try it out as well. I'll take a look at readxl, Shiny, and R Markdown. I didn't know about R being acquired by MS... Another argument for it, for business folks who swear by Excel. Thanks for the suggestions!
Check out Tabula, it performs automatic table recognition in PDFs and exports the table as a csv. It is also available as an R package, although I’ve had trouble getting it to install. 
&gt;i. &gt; &gt;I start to answer the questions as they appear. &gt; &gt;Why did he set the α1 = 0.6? Based on what? &gt; &gt;Based on pure luck. There are ways to calculate a\_1 based on the time series, but in this case, it's an example to show the concept. &gt; &gt;And what is this coefficient? What does it rapresent? (In simple words) &gt; &gt;It represents how much the last date impacts today. Here, 60% of yesterday are the base value of today and then we add some error to it. &gt; &gt;The loop ''for'' from 2 to 100..... what is that? Is it the number of simulations? &gt; &gt;It is just calculating the formula for the values from 2 to 100, always taking 60% of yesterday and adding the error of today. &gt; &gt;My question is WHAT IS THIS ACF graph? How to read it? What does it mean with LAG? What is LAG? &gt; &gt;The acf shows, how much today is dependent of yesterday, two days ago and so on. The bigger the line, the greater the influence of this "lag". "lag" is the amount of steps to go back in time. &gt; &gt;why do we need to find out the ''αi'' parameter if we already knew it is 0,6? &gt; &gt;You already know it's 0.6, but that's coming from the example data you are creating. You wouldn't know in real life, hence you need to estimate the a\_i Thank you very much you have been so helpful Thank you. So the a1 is an estimate that WE ASSUME (based on our emotions)? IN case of a stock, how can you tell that the price of yesterday influences the price of tomorrow with a 60% of probability? How could you assume that? Is there a method?
Thank you for the tut, great. I am studying MA (moving average. At some point it says: ''For example, we might take a moving average of order 4, and then apply another moving average of order 2 to the results. In the following table, this has been done for the first few years of the Australian quarterly beer production data.'' My question is what is a MA of order 4? And of order 2 or 3? What is the difference?
R can’t be “bought” it’s an open source language. That’s like saying someone bought python or C. What MS did was implement their own version of the R language and then began to hook that version of R into their products. Still a great move by MS, just not quite the one you are describing.
This is the best answer. I’ll add that i regularly use r to read, edit and write excel sheets, word documents, and PowerPoint presentations. The integration is complete and is certainly and option that many companies in finance use. Python is not more general purpose than R when it comes to interacting with Microsoft software. I work with several funds in New York that use R primarily. Matlab is another common choice. Python is much less common the finance world. 
For what it’s worth, I use both python and r and disagree with literally every point. I find r easier than python for every point listed above. As you mentioned. You really should learn about the tidyverse as I think it solves every qualm you mentioned. 
&gt;The acf shows, how much today is dependent of yesterday, two days ago and so on. The bigger the line, the greater the influence of this "lag". "lag" is the amount of steps to go back in time. I have red that they try to do not have bigger lines going up, why? Because biiger lines shows a great autocorrelation and that should be avoided? Look at this correlogram here below: [https://s3.amazonaws.com/quantstart/media/images/qs-tsa-armapq-ar1-plus-six-x-correlogram.png](https://s3.amazonaws.com/quantstart/media/images/qs-tsa-armapq-ar1-plus-six-x-correlogram.png) The first 2 lines are much higer than the other ones, what do they show? Should this thing be avoided? And why?
Completelt true, sorry for the confusion 😅
In my experience, discussion of R's steep learning curve is usually in relation to tools like SPSS, rather than proper programming languages. 
Ahh. That would make sense. Kinda funny too - after nearly two years or so of making R my primary statistical analysis software, I find all the things like SPSS difficult and confusing to use. 
I'm a biologist and this was my first experience with a programming language. It kicked my ass for a long time, but I think it was because I had never programmed anything before. My guess it that people think R is difficult because it attracts people like me that have never programmed before but need R to summarize data and apply statistics. Analyzing data is a common need for alot of professions that wouldn't otherwise encounter a programming language. 
Hey, MadPhatFishKiller, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I definitely feel this. As I said, me learning R didn’t happen overnight. Only in the past year have I matured to the point where I feel it’s appropriate to describe my self as an intermediate to advanced user (as opposed to a beginner).
Yeah. As other commenters have alluded to, it definitely seems that R gets this bad rep because it has become the go to choice for a lot of people who need a better tool than something like SPSS but have no programming experience. 
I come from a similar background to you and have had a similar experience. That being said, I've heard it is very different from most "traditional" computer science languages and it is often harder for comp. sci. people to get used to it due to its little idiosyncrasies. So it might be harder to learn R if you are used to Java/C++ than it would be to learn Python for example. 
It took me a year and a half to get to where I could do all the stuff I wanted. All I wanted was to summarize data, make graphs, and make maps. After that, I've started saying "I use R" instead of "I'm learning R".
I think before Hadley Wickham's contributions and before R Studio, R was a lot less user friendly. Some of the error messages you'd get were also really unhelpful. Comparing that experience to using SAS or Stata (screw SPSS) is what made people (me) say R has a steep learning curve.
I was looking into some of the parallel stuff, and some of the GPU stuff (not for craps, of course) and have found many to not be up to date in CRAN. Is there a community around this kind of parallelization and acceleration? Thanks for your reply!
You really should learn learn how not to sound like a condescending douche nozzle when you post on the internet. I work with tidyverse every day. Its nice, but it only solves about 1/3 of the problems I need to solve for building an effective ETL pipeline. You can lipstick base R with dplyr and tibbles all you want, but when you get down to her core she’s a fugly beast. Its cute that you’ve never had an ETL problem that requires custom classes, methods, and exceptions, graceful error handling, and action logging, among other things. I suppose if all I did was read in other people’s well structured CSV data, I would think tidyverse can do everything, too. 
The basics of R are easy to master. The problem is the skills you gain are often non-transferrable because most of the time you learn to use packages rather than R proper. Python, being more general-purpose, relies on specialized packages somewhat less. For example, for simple web scrapping I don't use fancy data models in python. I process and store my data in a list and then output it to a file with a customized function. In R, I'd probably collect data in a data frame, and then use a package tool for file output.
Since you have asked two questions, I will answer for each accordingly. The acf shows the correlation between Lag x and today, so the correlation between one y[t] and y[t+x]. For Lag 0 the correlation is 1, since there is no difference. Then for lag 1 the correlation is round about 0.6, since we configured our data to rely on the last datapoint by 0.6 ;) The acf can show you indicators for lags that influence today or seasonality.
No, a1 is not an estimate. a1 is set by us to create some example data set. This was stupidly worded by me, my apologies. The author sets the a1 for us to be able to get some data for an easy example. As you can see, you just multiply the last data point by 0.6 and add some random value based on the normal distribution. This is a somewhat perfect example for time series for beginners.
I shouldn’t try to sound like a condescending douchenozzle, huh? You seem really sensitive. Having a bad day? Do you need some water?
I dont know, but as someone who has a programming background who first learned Python , learning R was weird for me and it was due to R having to treat everything as a vector in one form or another. Whereas in Python and most other languages, they have primitive, generic data structures. I only use R for the simple oneliner functions, but if I had to do some complex logic and control flow, I'd rather use Python. R used to be a pain to work with SQL string since it broke up the SQL into a vector delimited by the newline character(a la paste). But now there is a library (glue) that you can use that makes handling SQL string so much easier. Had that library came out sooner, I may have been using R for db work instead. I may revisit using R for database work again.
Remember that technically steep learning curve means that something is easy to learn :) Most people however use this in completely opposite way https://en.wikipedia.org/wiki/Learning_curve
&gt;I shouldn’t try to sound like a condescending douchenozzle, huh? No. I said you should LEARN how to not sound like a condescending douche nozzle. It’s quite evident that sounding like a condescending douche nozzle requires no “trying” on your part. &gt;Do you need some water? No, but thank you. I swore off drinking Summer’s Eve straight from the nozzle years ago.
Oh yeah. Hadley Wickham has been a godsend for the community. And yeah, in regards to Python, breaking the environment has been my problem. Just like you, sometimes I install a library and the whole thing blows up. 
Well what I’m gathering from the comments, it seems it’s non-stat people and people who have never programmed before that make these claims. But yeah, I agree with you. R is relatively much easier than other languages.
most people who struggle with it are using base r instead of properly embracing the true power of tidyverse. code without pipes just isn't R code.
I would say that they are NOT correlated. The correlation between column 1 and 2 is about 6%, which is super low and might as well be zero.
&gt;EDIT: Sorry, didn't read the last part. Load the data from the first image into R (dat &lt;- read.csv("path\\to\\csv")), and then do Cor(dat). &gt; &gt;It will give you a very similar output to the one in the second photo. Thank you, but are they a little bit inverse correlated? (the minus - 0,6%)
Scientist w Perl / SQL here - R feels very natural
I am not sure R is harder but it is definitely different than other OOP languages like Python. I think for those used to OOP R then becomes difficult to pick up. 
r is easier to learn than most programming languages its interpreted and dynamically typed and reflective which makes it easy to use i bet reputation comes from R being an easy language, so many people try to learn it first/ are reccomended to, therefore it has a "steep" learning curve, as you would be learning programming while u are learning r
The 1 is there because the table is comparing the relationship between column 1 and column 1. Since they have the same values, they are going to be 100% correlated, and will have a correlation factor of 1. Same for the other part that is 1 for the correlation of column 2 to column 2. Fyi, the values you're going to see here are between 0 and 1 (or negative 1). 0 being not at all correlated and 1 being very correlated (the negative sign just denotes in which direction the correlation exists). Given this knowledge, your correlation is far closer to 0 than 1.
Thanks, that's not really what I'm looking for either. See what I came up with in some spare time today, it can be used in the example I posted above. # reordering data library(dbplyr) forecast &lt;- forecast %&gt;% arrange(Opportunity, Date) # creating data frames to hold data during for loop # forecast change forecastchange &lt;- data.frame(matrix(ncol = 3)) colnames(forecastchange) &lt;- colnames(forecast)[4:6] # From and To dates dates &lt;- data.frame(matrix(ncol = 2)) colnames(dates) &lt;- c("From", "To") # firing up the for loop for(i in 1:dim(forecast)[1]){ forecastchange[i, 1:3] &lt;- ifelse(test = i == 1, yes = c(0,0,0), no = ifelse(test = forecast$Opportunity[i] == forecast$Opportunity[i-1], yes = (forecast[i, 4:6]-forecast[i-1, 4:6])/forecast[i-1, 4:6], no = c(0,0,0))) dates[i, 1:2] &lt;- ifelse(test = i == 1, yes = paste(x = forecast$Date[i], forecast$Date[i]), no = ifelse(test = forecast$Opportunity[i] == forecast$Opportunity[i-1], yes = paste(forecast$Date[i-1], forecast$Date[i]), no = paste(forecast$Date[i], forecast$Date[i]))) } cbind(dates, forecastchange) If you hate me, that's perfectly fine :P I just need to find a better way to reorganize the from and to dates data frame as it looks messed up, I also need to get the customer and opportunity columns in there so it makes more sense. What this does is checks to make sure that the opportunity at row i is the same as the opportunity from the row above, if so, then it can calculate the percent change from one forecast cycle to another.
 I learned r concurrently with java and python. I found r much easier. 
There's a CRAN task view [here](https://cran.r-project.org/web/views/HighPerformanceComputing.html) with many options to perform parallel programming in R.
I think this is it. R is being compared with other statistical analysis programs like SPSS, Stata, Matlab, etc. Relative to them, R is more difficult, but only because it is a more complete coding language that demands a deeper understanding of writing and maintaining code and functions compared to its competitors. 
I had a sneaking suspicion that this would be the next comment!
Yeah I learned R five years back when he just started to throw out his stuff, and my professor was against using what became the tidyverse at all, thinking it was a fad. 
Post here if you have problems. I am not the best myself, but one thing I noticed is the R community is quick to help.
Oh I’m an OG r user too that but you can’t deny that R wouldnt be what it is today without rstudio, ggplot2, rvest, and Tidyverse. If R had a Michael Jordan it’s him 
Maybe a LeBron since they both contribute to thier field and education. 
Some of it is a holdover from old farts like me. My first real intro to R was about 10 years ago. There was no ggplot2, no magrittr pipe or dplyr, no RStudio. I learned using notepad, the default R console, and the base plot package. It was hellish. Now, things are much easier and stuff like tidyverse are helping to clean up the syntax and make it more accessible to beginners. But base R is rather terse and esoteric. So stuff like `DF[DF$col1==“a” &amp; !DF$col2==‘c’,c(1,5,7)]` tends to be really confusing for beginner because it includes the concepts of index slicing rows and columns, Boolean logic and equality testing, and the concatenation function all at once. Simply selecting the rows and columns you wanted as a novice user became an exercise in frustration and difficult to remember syntax - WHY do I have to call DF every time I reference a column, etc. Now, of course, we just pipe DF through filter() and call it a day. So I think a lot of it is a holdover. If you ever venture off the tidyverse reservation, things can get ugly very quickly. Try using the base plot package sometime. It’s still “old school” in that it has confusing or obtuse parameter names and really strange usage conventions. 
Yeah, you’re right. But there is OOP functionality written into R as well. Like, three different kinds of functionality - S3 objects, S4 objects, and something else I can’t remember right now.
I can't even imagine R without piping, would be hell on earth re-assigning so much. R is already so copy hungry, working with large data sets must have been a real pain 
Use corr.test() (not the same as cor.test()). This function will also give you a statistical significant test for the correlations. Best!!!
That is what I have been doing. It just seems really odd to me that a language as powerful as R requires so much manual work for something as vital and commonplace as this, especially since it has so many other rich and powerful features that simply require a line or two of code.
I've been working on that chunk of code for quite a while now, I was wondering if anyone had a better way to get the same thing accomplished. I'm still not close to done yet.
I actually think being able to draw it exactly how you want it shows how powerful it is. I've thought about trying to make a package to implement this, but there are so many commands at some point the user might as well input it themselves. 
I usually use Illustrator to put in graph annotations because I post-process anyway. But you could try add_pval from the ggpval package and use the annotation argument to input p-values generated from the test you like. It's the closest thing to a unified function that I could find.
I wrote some code to calculate Tukey letters and added them as a column to my data frame. From there I was able to apply the letters to both xtable output and ggplot2 output with some additional coding. Stack Overflow was very helpful.
Good video
The solution seems to be here https://github.com/kassambara/ggpubr/issues/102 
For *programming* - R is no more difficult than any other language I have encountered and beginners don't face any thing as puzzling as Python's list comprehensions, nor the slightly counter intuitive indexing on slices. The syntax is fairly clean and I don't mind brackets. There isn't a huge amount of boilerplate (I'm looking at you Java). So what can people be talking about? Well, I suspect it may be the *huge* number of statistical functions that beginners tend to encounter very early on. Only in MATLAB do I think are beginners routinely thrown so deeply into the deep end - and with similar results. Beginners in both languages often don't learn how to read the standard documentation and therefor have trouble not only using a function but understanding and manipulating the output. But, if you wanted to teach people programming basics without intro ducting data analysis and statistics, then I don't think R would be particularly tough.
Hey, thaisofalexandria, just a quick heads-up: **therefor** is actually spelled **therefore**. You can remember it by **ends with -fore**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
You are looking for [shiny](https://shiny.rstudio.com). It allows you to create webpages. You can also package them in a standalone manner with various methods. See https://github.com/ficonsulting/RInno And https://oddhypothesis.blogspot.com/2014/04/deploying-self-contained-r-apps-to.html I have used rinno and had mixed experiences with it (users report dependencies not getting installed etc.) if you use it make sure you test it in a naive machine The second method I didn't try but should be fairly robust as you prepack all dependencies too (I don't know what this means in terms of your choice of license however. You'll be shipping GPL licensed code) 
Of course there is. But when asking for programming related questions, what your end product needs to look like and how your data is structured is more important than what your data is. From this, it sounds like you can simply turn them into factors ordered from bad to good ``` hypotheticalDataFrame %&lt;&gt;% mutate(sentiment = factor(sentiment, levels = c('bad', 'neutral', 'good')) ``` And ggplot ``` hypotheticalDataFrame %&gt;% ggplot(aes(x = date, y= sentiment) + geom_point() + geom_smooth(method='lm') ``` 
Or simpler: stacked barplot by week or month. A ~2 linger in ggplot2.
&gt; Use corr.test() (not the same as cor.test()). This function will also give you a statistical significant test for the correlations. Best!!! Hello, as seen that I want to see what correlation there's between the matches won and / lost from a tennis player and the number of double faults made from the same player in those matches, would you suggest me somenthing better than a correlation test? What should I use? A logistic linear regression? How to set Y and x? what on X and what on Y? 
https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf
rmarkdown can do some formatting, but it’s really designed for simplicity and speed. If you start to seriously battle with it for aesthetics then it might be worth ditching it and jumping to the lower level knitr/latex combo directly. 
You can take a look at “Rattle” to get some ideas on GUI features:p
R is fully featured in Power BI - python support is in beta and can be used as a preview feature
you can try impact coding or check to see if you can logically roll up your data into fewer categories
This is the code I wrote: mutate(data, x1_delta = lag(x1, order_by = Date)) but the output wasn't right - any suggestions for how I can tweak it?
 a stacked bar plot is the same idea but harder to interpret, unless you're going to create very large bins (like quarterly), as they will be separated by whitespace. and I'm not sure why or how that would be simpler, it would require the exact same data object (you still need to perform the same aggregation and normalizing) except use a factor for the x variable.
First, you need to group by `y` so the software knows when to reset. The delta value is the difference between the current value and the previous value, or `x1 - lag(x1)`. Does that help?
You might not need to read left to right &gt; string &lt;- 'asdasdasd (987REM {CS} [029292920] (123203020200)' &gt; match &lt;- str_extract(string,'(\\([0-9]+\\))') &gt; match [1] "(123203020200)" &gt; gsub('[^0-9]','',match) [1] "123203020200"
data %&gt;% group_by(y) %&gt;% mutate(delta = x1 - lag(x1, order_by=Date)) problem solved!
Thanks for the suggestion! I'll take a look at it!
I am guessing it'll be fine if you release the shiny code you used in there with MIT in a separate repo since its ok to use MIT code in GPL projects. The standalone version can be the GPL one that happens to be using your MIT code. 
unlist(lapply(strsplit(your_string, ''), function(x) paste(x[length(x):1], collapse = '')) Didn't test it but that's an idea for flipping your strings around. Maybe use regex after that. Might be slow.
Is everything exported correctly in the namespace? Are you documenting by hand or using something like roxygen? Do the functions work if you call them directly, e.g. mcccr::coinflips() ? 
Exported in the namespace? Can you clarify? Documenting using devtools::document() with roxygen data (ie the #’ @param and so forth) The functions do not work when called directly. 
I'm far from an expert, but I once managed to install a package into a different library than the ones in my search path. Try checking whether your install location is being searched for the package. 
It’s all in win library 3.5. I maybe should update to 3.5.1 but that probably won’t do anything. 
ggplot2 barplot does all calculations for you, you just need to specify X and Y (I wouldn't be surprised if it does date aggregation as well, but I won't bet on this), this is why I consider it simpler.
stupid question but do you have devtools installed yea? install.packages("devtools") library(devtools)
then try `mcccr:::coinflip()` (with 3 colons). if that does work, the problem is your namespace. I'm on my phone but check out Hadleys book on r packages. you probably just need to add `#' @export` to your functions roxygen (and then redocument), but a basic understanding of package Namespaces is vital when creating packages) 
There are some workable answers in here, but a more general answer is the non-greedy operators. By default, something like `.*` will match as much as it can, so a regex like `\\(.*\\)` will go all the way from the first parenthesis in your example. But if you use a non-greedy operator instead, it will match as little as possible, so it will only match the closest parenthesis `stringr::str_match(my_str, "\\(.*?\\)")`
Thanks a lot for this input
Thanks a lot for this input
Thanks a lot for this input
This worked! Can you explain what it means to use the ::: rather than :: and what it means I should do next to get it to work as typical? thanks so much!
Incidentally, I just tried it on my work computer and it worked regardless of the ::: or namespace issue so whatever the problem is, it’s on my home pc. 
I think Open R is really useful when you want to work on very large datasets (&gt;4 GB) that cant be loaded into your RAM. Otherwise, I guess there is not much difference...
Why would that be the case if you’re using a 64 bit CRAN R?
R Markdown: The Definitive Guide
I WASNT ADDING ‘@export’ IN MY ROXYGEN TAGS Augh! This is on a different page of Hadley’s guide so I totally missed it when doing just the documentation page. 
To add to this, if you manage to be exceeding RAM even with 64bit then you can always use tools like spark. 
As of RInno v0.3.0, all dependencies are shipped automatically and managed in a local app library. We're working on a stand-alone UI built on the NodeJS Electron framework, so it will also be browser independent. DeskopDeployR's use of R-portable is cool, but I don't see the need for multiple R installations. Especially if you install multiple apps. They don't all need separate installations of R and Chrome. It seems a bit wasteful ;). Full disclosure - I'm the author of RInno :)
RInno is moving towards a stand-alone UI and isolated dependency management (like packrat). Check out the master branch on github and let me know if there are other improvements we can make!
up vote for windows bash
Serious question: what would you choose in place of Windows? My Company just upgraded to Win10 (32-bit Office, which is the truly baffling part), but I personally am a MacOS user. Both work fine for medium sized data (&lt; 32 GB) IMO. I guess the only other option I’m aware of is a Linux distro. 
Well to be clear I think Windows is great for using Microsoft Edge, outlook, Microsoft office, and having your computer reboot on you at the most inopportune moments for updates that break your system. No native GCC and issues installing any R packages that need compilation is a deal breaker for any serious compute work
Yes. I use a Linux distribution for my compute work. As do all super computers. Macs have compiler support and as a UNIX system has enough system utilities to be okay for compute work. But the price premium is way too high. The hardware is poor and unserviceable, and there's no centralised package management which means manual dependency resolution. That's like going back to the dark days of Linux as far as I'm concerned. 
&gt; and having your computer reboot on you at the most inopportune moments for updates that break your system oh yes... over multiple days 
Ooooh that’s exciting - and could be very useful for me given I’ve been wanting a way to share shiny apps with colleagues who are much more familiar with Excel. 
&gt; Also, why anyone is using Windows (and Windows 7) as a serious compute machine in 2018 is baffling. Probably because OP isn't actually talking about a compute machine. OP straight up said it was an average desktop. Even by your estimate this is a puny dataset so why on earth would OP even need a dedicated machine? 
Also be sure not to use base R for loading in data etc... there are more efficient packages such as data.table... 5MB is relatively small data. Base R is not great for so many things.
That's interesting....what kinds help are you looking for.
nah just basic R stuff that i wanted to calculate, ive done some stats, and if i just go thru a few tutorials ill figure it out, just wondering if someone could give me the R expression to get what i wanted. but prob better to just learn R and figure it out
http://r4ds.had.co.nz/ https://paulvanderlaken.com/2017/10/18/learn-r/
Learn r. It helps me immensely for analysing markets. I haven't spent time looking at crypto....but would like to. A good start to r is coursera, there is an introductory course. Please ask for help, the r community is incredibly generous.....get on Twitter #rstats. I like working with tidy principles. Best of luck mate.
Lets partner on this?
what did you want to contribute?
The NAMESPACE file is what tells R where to look for your functions, data, and documentation. `roxygen2` automatically generates a namespace from `devtools::document()`, so if you're having trouble accessing functions then the go-to is to double-check all your roxygen notation, delete your `/man` folder and `NAMESPACE` file and run `devtools::document()`. Then, build and reload your package, and restart R. You should read \[Hadley's book\]([http://r-pkgs.had.co.nz/](http://r-pkgs.had.co.nz/)) if you haven't already.
Then why should OP worry about any speed up from using Microsoft Open R? Lets say the speed increase is an order of magnitude (10X faster). Does it really matter that his code now runs in 0.1s instead of 1s? Probably not.
&gt; Then why should OP worry about any speed up from using Microsoft Open R? OP *literally* came here asking if it would give increased performance. In other words, OP didn't know and OP was asking, and OP (probably) wasn't expecting some condescending douchenozzle to go off about dedicated instruments. That aside, [I actually managed to same effectively the same thing yesterday in a neutral way](https://old.reddit.com/r/Rlanguage/comments/983ae5/microsoft_r_open/e4e18sh/). 
Prefixing a field or function with a dot is an indication that it's not meant to be called directly by the end user (in this case, a private field that is accessed via an active binding or public method). If you look in the source code for most packages you will find that convention. It's certainly not required, however. 
Just a pretty face
“An indication” So it doesn’t do anything mechanically like in ggplot outcome ~., ? Or in Dplyr?
It does have an effect, prefixing names with a dot hides then from `ls` by default.
ah...that's now obvious as I think about this from a linux perspective. Cheers
I'm not sure about an R Shiny CSS cheatsheet but it's pretty easy to tweak and play around with things in your browser. In Chrome Ctrl+Shift+I will bring up the inspector, and if you go to the Elements tab, you can navigate to the thing you want to tweak (say a button, for example) and it will show where in the CSS that button is getting its styling.
Ahhhhh yeah! https://www.rstudio.com/resources/cheatsheets/
Oh I didn't even think about this. I'll have to try it out. Although, I need to firsts figure out how to make it work in the browser. Right now it only my app only works when I run it from R Studio. But I think it's because I import the CSV file from a folder instead of uploading it through R Shiny. This is my first time using R Shiny, so I'm still learning the ropes! Thank you for the tip!
Hey thanks for that. I've actually been using the Shiny cheatsheet on there for this project. I was hoping for a more "dumb version" cheat sheet that had something like (background of table = 'whatever I need to search for in css file') But I'm getting more of it with some editing and running the program to see what happens! Thanks for the link, though!
As Holophonist said, use your browser's inspect tool. You can even directly modify the CSS in it (works in all browsers, probably even IE) and immediately see what it does. Here's a video that demonstrates it: https://www.youtube.com/watch?v=Xb6ZIlYj2OY
Thanks. I think I can get this once I can figure out how to make my app work I the browser. Right now it’s just a blank screen. But seems similar to requests/webscraping with python. Now when I edit it, how do I save it so it defaults that way? Or does it do it automatically?
There should be a button that says "run in browser" or something along those lines. I believe it just launches your browser and goes to your localhost at the Port for the app
Got it working! I think I had an error somewhere else where it was just printing a blank screen. I’m honestly not sure how I fixed it. But it works now. /shrug
I haven't actually tried rshiny yet, so I can't really say anything, but the inspector in your browser can also tell you about what's going wrong. The CSS/HTML is just one part, you can also check the Javascript console for errors, and examine XHR (web scraping) requests. Just click around the tabs and look for suspicious suspects.
I believe you're forgetting the print for ( i in scenerange){ TheOfficeLine&lt;- mod_data %&gt;% filter(id ==i)%&gt;% select(line_text_mod) UserDefineText&lt;- "all right Jim so this quarter look good" print(compareSentences(TheOfficeLine$line_text_mod, UserDefineText)) } If you evaluate a function normally, it prints the returned value. If you are evaluating in a loop the return value is not automatically printed.
A possibility is to count() (from tidyverse, dplyr package).
This tidyverse way might work, but if you're new to R it will probably be difficult to understand: library(dplyr) mydf %&gt;% filter(Language %in% c('English', 'French')) %&gt;% group_by(Language) %&gt;% count(Gender) The standard table() function might work too: table(mydf$Language, mydf$gender) 
I generally prefer count() over table(). If your dataframe is called df, then try: count(df, gender, language)
This works great, thank you! One day I might even understand how ;)
This works great and is really straightforward. Thanks for your help! 
also with more columns you can combine any two you want i.e. table(df$gender, df$language) this produces exactly what op wished with sample output
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/ItcBPD5.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Yes perfect what are the steps to do this in excel?
What do you want the model to do? My first thought is that if your independent variable is "time" (months), you're actually looking at some kind of time-series analysis. In that case, this book might be helpful: https://otexts.org/fpp2/ But determining the purpose of the model is very important... "I want to build a model that fits this data so that I can ___"? 
I love how in the title he calls it a "silver bullet" and then just a few paragraphs in says it "is no silver bullet". Its a good explanation otherwise though.
How did I miss this area of the site! Thank you so much!
Let r be a vector of t fitted points: `r &lt;- fitted(Returns)` Then r[t+1] &lt;- 239216.9 + 10705*x1[t+1] + 23.3183*x2[t+1] - 0.7468334*r[t] r[t+2] &lt;- 239216.9 + 10705*x1[t+2] + 23.3183*x2[t+2] - 0.7468334*r[t+1] &amp;#x200B; Or, with a loop: r &lt;- fitted(Returns) for(i in 1:2) { r[t+i] &lt;- 239216.9 + 10705*x1[t+i] + 23.3183*x2[t+i] - 0.7468334*r[t+i-1] } Of course you'll need to account for the new `x1` and `x2` values. But you get the point. &amp;#x200B; You can get more sophisticated by using `predict()` if you have the future values for the other covariates. If you have this, PM me and I'll get you going.
Each table is a single variable but for multiple cities (a column of the variable for each city), so to regress for a city I have to pull the data from two different tables.
&gt;Try using a Generalized Additive Model, it will fit your data well. I prefer the "GAM" package over "mgcv" but that is up to you. You can read more about it here Can I ask you why a Generalized Additive model? Why would it be useful in this case?
&gt;https://i.imgur.com/ItcBPD5.png Sorry why R squared declines after 25? What do this mean?
^[Because](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
Sure, a GAM fits data using functions instead of coefficients. The functions work to fit the data using an iterative spline fitting technique where the goal is to reduce the error by creating a best fit line. In this case the line can fit all types of crazy nonlinear functions. So basically a GAM will pretty much always fit your data well. If there are quick, sharp, curves in your data the GAM by default might try to smooth them out but you can actually force the parameters to fit the data exactly. If you download the GAM package the syntax would be something like "gam(y ~ s(x), data = df)". The "s" designates the data you want to spline fit so you have to wrap the field in it. The example assumes you have your data in a data frame called "df" and your field names are "x" and "y". The other thing I will note is that when you do predictions on the next point the GAM model will use a linear prediction based on the slope of the last few points. I don't understand a decent amount about the GAM and how it makes it's predictions and I know there are different ways to fit a GAM (using loess is another option), but so far it's worked well for me. Especially when predicting time series data that are non stationary.
rmarkdown: a document to be formatted for output - screen or print - that can include r code to be excuted before rendering the final product. notebook: an rmarkdown document that can include code chunks that can be \*executed interactively\*, and not just before it is rendered for output, usually intended as an interactive teaching document. script: a text file containing r code that can be run repeatedly, either the whole script (known as sourcing) or in RStudio, highlighted portions of the script. The output is by default sent to the R console not to a document.
 ?rpois ?rexp
Here's an almost line-by-line translation of a Single-Server, Single-Queue from M. H. MacDougall's book _Simulating Computer Systems, Techniques and Tools (1987) (pg. 13-20) In this particular system, only the exponential distribution is drawn from, (in R, that's `rexp()`), which is used to model the **duration** of service, and the **time until next arrival**. You can use `rpois()` to randomly draw from the poisson distribution but note that it will give you counts over a given period and not **time until the next thing happens**. Note that the exponential and poisson distributions are closely related here. The book can be seen here: https://archive.org/details/simulatingcomput00macdrich Equivalent R code here: Ta &lt;- 200 Ts &lt;- 100 te &lt;- 200000 t1 &lt;- 0 t2 &lt;- 0 simtime &lt;- 0 n &lt;- 0 df &lt;- data.frame(SimulationTime=simtime, NumInSystem=n) while (simtime &lt; te){ if (t1&lt;t2){ # event 1: arrival simtime &lt;- t1 n &lt;- n + 1 t1 &lt;- simtime + rexp(n = 1, rate = 1/Ta) if (n ==1) {t2 &lt;- simtime + rexp(n = 1, rate = 1/Ts)} } else { # event 2: completion simtime &lt;- t2 n &lt;- n - 1 if (n&gt;0) { t2 &lt;- simtime + rexp(n = 1, rate = 1/Ts) } else { t2 &lt;- te } } # accumulate data tmp_df &lt;- data.frame(SimulationTime=simtime, NumInSystem=n) df &lt;- rbind(df, tmp_df) } plot(df)
What are you looking for that those two functions don't provide?
What have you tried? What resources are you drawing from? What's not working? I found this with a quick search... https://www.r-bloggers.com/simulating-a-queue-in-r/
Ok everyone what I am actually trying to do is extrapolate this data out from 36 months to 72 months. The polynomials from scatterplot trendlines are unique to each monthly set of data. I need to find the original equation that made this data. My guess is some sort of exponential function...???
&amp;#x200B;
Booooooooooo
Is this something your going to be regularly doing? Might want to take a quick mooc on CSS and possibly some html do get an idea of how you can optimize your styling. 
I am quite new to R programming, but after playing with it for 1 day i realized i need packages which I have installed now, so now It are working and i can continue playing :)
I’m not sure how much I’ll be doing it. It seems like every time I wanna learn something knew I have to learn something along with it. This was my first Shiny project and I’m glad I did if. I used the above methods and actually learned how to navigate though everything. Created my own css file to use in my project based off reading the elements. It’s actually fairly easy once you get the hang of it! But you’re right. I need to take a small class over css and/or html and just get it over with. At least learn enough to be familiar. I don’t have to be a pro.
Yeah it helped me out. Pretty easy to have a decent basic knowledge. 
Do you recommend any specific tutorials? Or just whatever I can find from googling?
I’m sure code academy is good enough. It’s what I planned on anyway. Thanks for your tip!
try RubyKube https://github.com/rubykube it's an open-source platform for creating exchanges
Wtf man. They were literally all working before..Haha thank you though. Lemme go through them again.
It's working now, thanks! &amp;#x200B;
Thank you for pointing it out. They *should* all be working. I'm still learning my way around Github!
Hmm...can you post your full code? You installed it and typed `library(keras)` at the top of the file? and you're running the entire script?
Here's the full code `library(shiny)` `library(keras)` `# Define UI for application that draws a histogram` `server &lt;- function(input, output) {` `hisdata &lt;- reactive ({matrix(cbind(input$VK_001, input$VK_002, input$VK_003, input$VK_004,input$VK_005,input$VK_006,input$VK_006, input$VK_007, input$VK_008, input$VK_009,input$VK_010,input$VK_011, input$VK_012, input$VK_013,input$VK_015, input$VK_017, input$VK_018, input$VK_021, input$VK_022, input$VK_024, input$VK_025, input$VK_027, input$VK_028, input$VK_030, input$VK_031, input$VK_033, input$VK_034, input$VK_036, input$VK_037, input$VK_039, input$VK_040, input$VK_041))})` `test_data &lt;- hisdata` `model &lt;- load_model_hdf5("my_model_bft_003.h5")` `test_predictions &lt;- model %&gt;% predict(test_data)` `output$oid1 &lt;- renderPrint({x &lt;- test_predictions[ , 1]` `print(x)` `})` `}` `ui &lt;- fluidPage(` `# App title ----` `titlePanel("Shiny + KEras!"),` `# Sidebar layout with input and output definitions ----` `sidebarLayout(` `# Sidebar panel for inputs ----` `sidebarPanel(` `# Input: Slider for the number of bins ----` `numericInput("VK_001", "Value:", 10, min = 1),` `numericInput("VK_002", "Value:", 10, min = 1),` `numericInput("VK_003", "Value:", 10, min = 1),` `numericInput("VK_004", "Value:", 10, min = 1),` `numericInput("VK_005", "Value:", 10, min = 1),` `numericInput("VK_006", "Value:", 10, min = 1),` `numericInput("VK_007", "Value:", 10, min = 1),` `numericInput("VK_008", "Value:", 10, min = 1),` `numericInput("VK_009", "Value:", 10, min = 1),` `numericInput("VK_010", "Value:", 10, min = 1),` `numericInput("VK_011", "Value:", 10, min = 1),` `numericInput("VK_012", "Value:", 10, min = 1),` `numericInput("VK_013", "Value:", 10, min = 1),` `numericInput("VK_015", "Value:", 10, min = 1),` `numericInput("VK_017", "Value:", 10, min = 1),` `numericInput("VK_018", "Value:", 10, min = 1),` `numericInput("VK_021", "Value:", 10, min = 1),` `numericInput("VK_022", "Value:", 10, min = 1),` `numericInput("VK_024", "Value:", 10, min = 1),` `numericInput("VK_025", "Value:", 10, min = 1),` `numericInput("VK_027", "Value:", 10, min = 1),` `numericInput("VK_028", "Value:", 10, min = 1),` `numericInput("VK_030", "Value:", 10, min = 1),` `numericInput("VK_031", "Value:", 10, min = 1),` `numericInput("VK_033", "Value:", 10, min = 1),` `numericInput("VK_034", "Value:", 10, min = 1),` `numericInput("VK_036", "Value:", 10, min = 1),` `numericInput("VK_037", "Value:", 10, min = 1),` `numericInput("VK_039", "Value:", 10, min = 1),` `numericInput("VK_040", "Value:", 10, min = 1),` `numericInput("VK_041", "Value:", 10, min = 1)` `),` `# Main panel for displaying outputs ----` `mainPanel(` `h4('Predicted values'),` `verbatimTextOutput("oid1"))` `)` `)` `shinyApp(ui = ui, server = server)`
Hmm. I'm not getting the error about the keras package not being installed. However, I am getting this error: Error in load_model_hdf5("my_model_bft_003.h5") : The h5py Python package is required to save and load models I also can't figure out how to install this package. Did you have to install this as well? I don't see anything in your code showing you did or importing the library. Where does `model &lt;- load_model_hdf5("my_model_bft_003.h5")` come from? 
library(keras) has this necessary function: [https://keras.rstudio.com/reference/save\_model\_hdf5.html](https://keras.rstudio.com/reference/save_model_hdf5.html) my\_model\_bft\_003.h5 is just a file (pre-saved keras model).
Here are some instructions on keras: [https://blog.rstudio.com/2017/09/05/keras-for-r/](https://blog.rstudio.com/2017/09/05/keras-for-r/) &amp;#x200B;
ok, I think I fixed that (by installing keras python version in my conda env). But now the issue is that no predicted values get printed.
Maybe have it load a default example data set on load if you want people to play around with it. 
Sorry, been busy at work so haven't had a chance to play around. What happens if you change you're reactive({}) function to renderUI({})? Try that and see what happens.
Thanks for that suggestion. I'll try and do some research to see if I can figure out how to do that. 
I think so too! Many things are 'simple' once you know how to do it, but it can be daunting trying to read everything and figure it out yourself. I find using a lot of visuals very helpful so I wanted to show other people how easy it actually is. 
Sorry for the long time with no answer, I was out travelling. Hope you had a good summer too :) Hope it is okay, that I take it from the beginning.. I have mylist = c("ac*, be, cd*; daa, efae*, fge*; gefa*, h") Liste &lt;- strsplit(strsplit(mylist , ";")[[1]], ",") The output of `Liste` is [[1]] [1] "ac*" " be" " cd*" [[2]] [1] " daa" " efae*" " fge*" [[3]] [1] " gefa*" " h" The reason for me, using the nested-forloop was to know, when I had a `;` in `mylist` What I did not write in the code, that I maybe should have, is this for (c in 1:length(Liste)){ for (d in 1:length(Liste[[c]])){ outer_tmp_string &lt;- "" # Extracting last character, matching if it is a wildcard # Liste[[c]][d] Prints all elements from the list # I need the double for-loop for this check inner_tmp_string &lt;- "" if (stri_sub(Liste[[c]][d],-1,-1) == '*'){ inner_tmp_string &lt;- data_var[...] } else { inner_tmp_string &lt;- data_var2[...] } } }
Go ahead and post it. I hope it helps people. I don't have a Twitter, so no need to tag me in it. 
I lot of people come at it from a web background, so they'll say things that assume that you already are a web developer and are new to this weird "R" thing. Lol. Do you think this R thing will catch on? 
I’m not sure. I don’t know enough about the direction of programming right now. I chose to learn R along with python just because they align what things I want to do. I got my degree in biology and would love to write biology and education related programs along with dabbling in data science. From what I’ve read, R is definitely trending up. I think many people are set their ways but as new companies emerge R will continue to increase. I also think it’s great there are ways to make web pages without knowing “web development” and programming languages like R make that very easy. In the end, I have limited knowledge on everything. My R and python knowledge has come from free books online and a few Udemy courses. Everything I know is essentially posted in my Github. So as you can see, I’m not a professional programmer, but I aim to be one eventually. I hope the weird “R” thing catches on more. I would love a larger community with even more resources. Until then, we will have to start creating our own for others.
still nothing :(
&gt;Sure, a GAM fits data using functions instead of coefficients. The functions work to fit the data using an iterative spline fitting technique where the goal is to reduce the error by creating a best fit line. In this case the line can fit all types of crazy nonlinear functions. So basically a GAM will pretty much always fit your data well. If there are quick, sharp, curves in your data the GAM by default might try to smooth them out but you can actually force the parameters to fit the data exactly. If you download the GAM package the syntax would be something like "gam(y ~ s(x), data = df)". The "s" designates the data you want to spline fit so you have to wrap the field in it. The example assumes you have your data in a data frame called "df" and your field names are "x" and "y". The other thing I will note is that when you do predictions on the next point the GAM model will use a linear prediction based on the slope of the last few points. I don't understand a decent amount about the GAM and how it makes it's predictions and I know there are different ways to fit a GAM (using loess is another option), but so far it's worked well for me. Especially when predicting time series data that are non stationary. Thank you so much. But why using GAM instead of just increasing the number in the polinomial regression, for example instead of making a normal linear regression you could run a quadratic or cubic regression to fit the S curve better ... so why using GAM instead of uising non linear regression? 
My man. https://media.giphy.com/media/cYxLgjZI5ezI2lrItX/200w.gif
Maybe you could encapsulate the tabPanels using [shiny-modules](https://shiny.rstudio.com/articles/modules.html). This may come in useful if you want to extend your project later on (e.g. by adding another tabPanel or by adding additional charts).
I would try the rgl library https://cran.r-project.org/web/packages/rgl/vignettes/rgl.html
As some others users said, I think you should plot this using a 3D scatter plot, and then you are done. A scatter plot would the job, and then you can manually ad the lines for the connections. What you you have in mind as a final product btw? maybe you should use another tool language for this...
I'm needing to plot in 2D since it's for printing.
I'm needing to plot in 2D since it's for printing.
Yeah, I'll take a look at that. Just been fiddling with ggplot2 so far.
rgl will at least let you rotate and turn the points and see how they look from different angles (eg projected onto your 2d display). I think you might be able to output the point of perspective that's currently being shown. By design choice I mean you'll have to decide what point you want to use to view the points, and (maybe) how you want to represent the large distances. I'm not sure if you can use the mapping programs to handle this kind of projection. Maybe the rgdal library has something for you? I can't answer this, just sharing thoughts. 
1. you need a dataset with 2 columns (=won/lost; %1st serving points) and 1 row per game. 2. use a logistic regression [glm(y ~ x, family=binomial())], with y = won/lost; x= %1st serving points. 3. Look at the p-value of the coefficient. I think that is what you are looking for...
This looks really interesting. I fall into the category they talk about in the second paragraph. &gt; In the past, we’ve responded rather glibly to these requests: “Just use functions!” Knowing there is an even better solution is great. I'll be looking into and seeing if I can implement it in this project. If not, I'm sure I can use it in a future project. Thank you so much. &amp;#x200B;
&gt; Mostly, we don't look for linear or non linear relationships between variables, we only look for " relationships ", and we later find out. Actually, most of the time we do not really care, in fact we try to make it out so that we get a linear model whose results are very close to the actual relationship. So if you do a linear regression for example you will get a linear model that describes the best your data. If you do a logistic regression you get a non linear model that describes the best your data. &gt; &gt; So we don't know really, but when you compare the two model, the linear and the non linear, if the linear describes the data the best it means the relationship is in most part linear, and vice versa. It could be divided too, like linear at some intervals and not linear at others Thank you, I am starting out Well here I wouldn't need statistic to see that the number of matches won and the number of 1 service point at service are not correlated. Let's assume that I would find the variable which has more weight into determinate the chances of winning or losing the match for a tennis player. So I will have as predictors various variables (dauble faults, percentage of points won at the service, return points) and I want to find oout WHICH variable is determining the most the chances of winning or losing a match for a tennis player. Could you please tell me which would be the ''way to go'' ? A multivariate logistic regression with in Y I will put the (0 for lost matches and 1 for won matches) and the predictors as X? 
&gt; you need a dataset with 2 columns (=won/lost; %1st serving points) and 1 row per game. &gt; use a logistic regression [glm(y ~ x, family=binomial())], with y = won/lost; x= %1st serving points. &gt; Look at the p-value of the coefficient. I think that is what you are looking for... Let's assume that I would find the variable which has more weight into determinate the chances of winning or losing the match for a tennis player. So I will have as predictors various variables (dauble faults, percentage of points won at the service, return points) and I want to find oout WHICH variable is determining the most the chances of winning or losing a match for a tennis player. Could you please tell me which would be the ''way to go'' ? A multivariate logistic regression with in Y I will put the (0 for lost matches and 1 for won matches) and the predictors as X? 
So this is a half terrible answer (sorry) - about 6 months ago in my master's program we were using RStudio, and people that were using it through Anaconda were having an issue that required them to download RStudio separately. I can't for the life of me remember what it was exactly though.
I literally wrote a whole paragraph to answer you and accidentally deleted it. Sorry , I'll try to make this one short. &gt; Let's assume that I would find the variable which has more weight into determinate the chances of winning or losing the match for a tennis player The results of the regression will tell you that, the higher the coefficient of a varibale the heavier weight it has, not always though, you should first check how much of its variance is explained, using a statistical test. &gt; Could you please tell me which would be the ''way to go'' ? A multivariate logistic regression with in Y I will put the (0 for lost matches and 1 for won matches) and the predictors as X? That is correct. Although I would suggest using " win " and " loss " for the game results istead of 1 and 0 which could be wrongly predicted by R into thinking that is a quantitive variable, instead of a factorized one ( don't focus too much on this lol ). I would suggest you look up this book , it is amazing I used it myself to learn a lot, it will definetly help and it actually does some examples of similar things to what you are willing to do: R for Data science by David Wickham
I've been playing around with your code, but I can't seem to figure it out. I'm not sure how your data is supposed to look, but do you need a matrix? Can you use a datatable? There is a renderDataTable() function that might be able to do what you want. [Here](https://shiny.rstudio.com/reference/shiny/latest/renderDataTable.html) is some info on the R Shiny website about it and I also use it in my project on my Github for an example. 
By line break, do you mean that you want the line adjacent to "Education", or would you like to remove the space below "Education" and above the line?
Tensorflow. https://keras.rstudio.com
The most straightforward solution would be to group by sex and status, do a count and join back to the main df (assuming you want a new column with the percentages).
mean(data_frame$status == "D")
prop.table(table(df$status[df$sex=="M"))
&gt; The results of the regression will tell you that, the higher the coefficient of a varibale the heavier weight it has, not always though, you should first check how much of its variance is explained, using a statistical test. Thank you so much When you say ''you should always look first how much of its variance is explained with a statistical test, which can I use for a dataset of more than 20 samples? A t student? And do I need to check the variance ONLY for the variable that has more weight, and ''esclude'' the other ones? How do I calculate the variance as you said? 
 &gt; a &lt;- rnorm(1000,0,1) &gt; colnames(a) &lt;- c("test") Error in `colnames&lt;-`(`*tmp*`, value = "test") : attempt to set 'colnames' on an object with less than two dimensions This example isn't working for me. :-( 
what I want is not to plot the dimension but rather to do something similar to hist percent norm. so just kind of have a gaussian like curve that closely matches the bins. I could get it to be density but I want the curve to be percentage instead of the histograms to be density
I think that you should use seasonal trend decomposition. Your data is a time series, and your description fits in seasonal variation with an increasing trend.
Hey man you don't know how thankful I am that you helped me. I've been trying to get this to work for literally so long you saved me lots of pain :) 
I’m glad I could help… I know exactly how that pain feels… you’re welcome :)
Okay? So would that be using a special function/package in R or can I do it using the base R packages?!
It does if he's running that same bit of code 10,000 times.
I am trying to use the example code provided on this page: [https://www.r-bloggers.com/simulating-queueing-systems-with-simmer/](https://www.r-bloggers.com/simulating-queueing-systems-with-simmer/) &amp;#x200B; But i still get errors while trying to do this in RGui. What is the problem? I have updated all the packages.. &amp;#x200B; Could somebody try this code and tell me where I am wrong(save the file and send to me?) &amp;#x200B; Thank you.
&gt; A t student? You can use the F test , [here](https://en.wikipedia.org/wiki/F-test#F-test_of_the_equality_of_two_variances) is a wikipedia page for you to understand what it is used for. &gt; And do I need to check the variance ONLY for the variable that has more weight, and ''esclude'' the other ones? Not really, think about it logically, for example you have a variable that is explained by two others, one affects it by let's say 60% and the other by 20%, the one that affects with 20% is by no means neglected, it is also important right ? that's why we use multivariate regression, to find in which way many variables affect the same variable. &gt; How do I calculate the variance as you said? Normally you don't, because there are functions that do it all for you and only give you F0 and F. F is the F statistic for your data, F0 is what's called the critical region probability, by comparing the two you can find if the variance explained by the model is enough. But it could be done manually so you would understand how it is done, normally that is explained in statistical courses. I feel like my explanations are a bit out of order, I should give find a book or source for you, but I feel like they are too detailed and will scare you away. Feel free to message me if you need anything. :)
Here you can find a tutorial http://r-statistics.co/Time-Series-Analysis-With-R.html. Best!!!
The (R Studio) \[[https://www.rstudio.com/](https://www.rstudio.com/)\] website is a good beginning. They have a lot of resources on there that is useful. Then (R for Data Science)\[[http://r4ds.had.co.nz/](http://r4ds.had.co.nz/)\] is a very good place to start. (DataCamp)\[[https://www.datacamp.com/](https://www.datacamp.com/)\] also have very good tutorials. There is a lot of other resources but this is a good beginning.
Start here: [https://paulvanderlaken.com/2017/10/18/learn-r/](https://paulvanderlaken.com/2017/10/18/learn-r/)
Thanks Mate for referring me awesome websites, specially Datacamp.
r/learnrstats
Specifically from the R Studio website, their cheat sheets are a great resource for beginners in visualizing the functionality of some packages, including tidyverse. (https://www.rstudio.com/resources/cheatsheets/)
Check out swirl for R. 
Yep. With these kind of analyses, it is not really statistically sound to ask 'which predictor has most weight', but what is commonly done is to interpret the significance level of the p-values of the predictors... 
Can you post some data from your situation? If you want to compare two players, you'll want them in the same data frame (or tibble). How you model it depends on what your data looks like -- which you should post.
Depending on what you are looking for, you might want to look into "Data.table rolling joins"
&gt; And do this for each group. Is there a variable which indicates which group each entry belongs to? 
Sounds like you're new to R. So the first thing you're going to want to do is probably install tidyverse libraries - mainly the the plyr, dplyr, and ggplot2 packages. There is a ton of info on these guys online. For your specific problem, first read the csv and set the delimiter as semi colon. Then you will have a data.frame() object with your data. dplyr is used for efficient manipulation of objects of this type. Does your sense always capture 3 data points per cycle? If so, it's easy to do what you want. Just select those rows by indexing. If not, the next easiest thing is to add a data column that indicates which cycle you're on. Let's call this column 'cycle'. Read up on dplyr and familiarize yourself with %&gt;% (pipe operation), group_by(), select(), mutate(), filter(), and inner_join(). Many different combinations of these functions can do what you want. One option: group_by(cycle), then mutate(minTime = min(time)). This adds a new column with the minimum time from each group (cycle, in this case) at every row in the group. Now you want to filter for rows where time == minTime. Then you're done. You can select() columns you want to keep, then write to a csv and use your data as you wish. Or try some plotting with ggplot()
It can be done for nearly any time period you want to limit it to, but the solution probably isn't for an R beginner. First you'll need to get the time column into a time format using something like as.POSIXct(), and strptime(). Then mutate another column with floor_time(), group_by() chip, and filter() for the min() time. Here's some sample data and code: data (named zdf in the code below): time chip 1 2018-12-31 19:00:03 a 2 2018-12-31 19:00:08 d 3 2018-12-31 19:00:17 a 4 2018-12-31 19:00:21 a 5 2018-12-31 19:00:22 c 6 2018-12-31 19:00:23 a and on for 1000 rows, with 4 unique chips over one hour. To pick just the first time a chip shows up in 30 second intervals: zdf %&gt;% group_by(chip) %&gt;% mutate(time2 = lubridate::floor_date(time, unit = '30 seconds')) %&gt;% group_by(chip, time2) %&gt;% summarise(min(time)) %&gt;% arrange(`min(time)`) Which ends up looking like this: chip time2 `min(time)` &lt;fct&gt; &lt;dttm&gt; &lt;dttm&gt; 1 a 2018-12-31 19:00:00 2018-12-31 19:00:03 2 d 2018-12-31 19:00:00 2018-12-31 19:00:08 3 c 2018-12-31 19:00:00 2018-12-31 19:00:22 4 a 2018-12-31 19:00:30 2018-12-31 19:00:32 5 c 2018-12-31 19:00:30 2018-12-31 19:00:42 6 b 2018-12-31 19:00:30 2018-12-31 19:00:54 7 d 2018-12-31 19:00:30 2018-12-31 19:00:56 8 a 2018-12-31 19:01:00 2018-12-31 19:01:05 9 b 2018-12-31 19:01:00 2018-12-31 19:01:12 10 c 2018-12-31 19:01:00 2018-12-31 19:01:15 # ... with 415 more rows From 1000 data points to 415 data points by rounding the time on 30 second intervals.
Ahhhh, thank you!
A faster alternative to the summarise() is to call slice(1) after group_by(). 
What you want is a cache, specifically a LRU or similar. They come in multiple flavors. Caching database software is the most popular, like memcached and redis, but you can also grab a cache library that does an LRU data type and work with it that way. Writing and reading from a file is one of the slowest ways to do this. It's ideal to keep the cache in memory, but if you need to write out to a file, Redis is probably your best bet. It's worth the trouble to do it right.
Thanks, will look into this. Right now, I am also looking at rewriting the code, adding a std::unstructured\_map with a key of the chip id, and value is the last time it was read. That way, I can easily prevent reading if last time read is less than 30 seconds from the current time.
It doesn't always capture three data points. I currently don't have groups, but when I was thinking of how to add that in the software, I came to the realize another way to solve the problem. A std:unstructured\_map&lt;int,long&gt; with key being the chip id and long being the last time it was read, that way it's easy to compare how long ago the chip was last read, and if it was less than 30 seconds, don't log the read.
I'm not 100% sure but it might be worth while trying to change your column from a factor to a string. `biosub$grade &lt;- as.character(biosub$grade)` before doing anything else. I say this because of the error. If not, could you provide a reproducible example?
I did just try it though, and it works fine. Thanks a million!
Base R is ok, but check out this website [http://r4ds.had.co.nz/transform.html](http://r4ds.had.co.nz/transform.html) and specifically for this question: [http://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate](http://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate) `library(dplyr)` `df = data.frame(student = c(1, 2, 3, 4), grade=c('A', 'B', 'A', 'F'))` `df = data_frame(df)` `head(df)` &amp;#x200B; `df = mutate(df, new_col = case_when(grade == "A" ~ 4,` `grade == 'B' ~ 3,` `grade == 'F' ~ 0))` `head(df)` &amp;#x200B; imo is nicer code. Regardless, that link above is a great resource. 
You can use geom\_segment on a map where start and end points correspond to longitude and latitude but ggplot doesn't natively do network diagrams. There's a lot of different programs but you might look into ggnet2.
Maybe use geom_path with a group variable for each pair?
There is a package that does this. I forget what the package is called but the term you are looking for is isochrones. 
can you put an exemple of your data set?
Sure!
To remove the mg from the number, use the strsplit function to split it on the " " character.
Could you write down the code for that? I think I need to indicate that I wanna do that for only one of the columns, but not sure how...
To piggyback off of this, don't forget to convert the number part into a number using as.numeric afterwards, and if the units are not all the same (some rows have mg, some have g) convert them to have the same units before you do any analysis. Just the common pitfalls. 
First off, here is the best resource to get started with R: r4ds.had.co.nz If you want to use tidyverse, which I highly recommend, something like this would help you: library('tidyverse') # assuming df is your data frame # we need a numeric version of dose df$dose_num = str_split(df$dose, pattern = " ", n = 2)[,1] df$dose_num = as.numeric(df$dose_num) # and if the units are different for different observations df$dose_units = str_split(df$dose, pattern = " ", n = 2)[,2] # you'll need to do some math to get them into the same units from here # this depends on there being a space in all of the doses to separate the number and units # if that's not the case, consider a regular expression # then to calculate a summary statistic you can do something like this min_doses = df %&gt;% group_by(subject) %&gt;% summarize (min_dose = min(dose_num, na.rm = TRUE))
Attempts at converting with as.numeric deliver the factor level, not the actual value, so there’s that.
I really appreciate it, but tidyverse gives me error messages I can’t fix because I don’t have the privileges (it runs R in a virtual machine thats controlled by a company).
Also, what's the function of \[,1\] and \[,2\] after closing the str\_split function?
Are you sure? This seems to work, is there something wrong with it? [https://imgur.com/QaaqeKw](https://imgur.com/QaaqeKw)
&gt; df$dose_units = str_split(df$dose, pattern = " ", n = 2)[,2] Do you read yr data into R using `read.table()`? If so, add `stringsAsFactors = FALSE` into yr `read.table` call.
You generally want to do that for all columns, you hardly need factors nowadays (lm etc interprets your character vector as a factor automatically). Reading in characters as factors is generally a horrible idea as it strips you of so much flexibility. 
I'm using read.csv, but I will try to implement this. Good call!
Here's the easiest solution, using the tidyverse packages (first, run install.packages("tidyverse") and library(tidyverse) if you aren't already using them). The first line is just me creating your example dataframe, so you can ignore it. [https://imgur.com/a/hy2LJoq](https://imgur.com/a/hy2LJoq)
Ok so I changed to read.table but I get the error that the argument `StringsAsFactors = FALSE` is unused.
Function arguments are case senstive! Yrs starts with a capital S instead of a lowercase s. In other words, `stringsAsFactors = FALSE` and not `StringsAsFactors = FALSE` Good luck!
Yah. `read.csv` is a special case of `read.table` :D You can see this if you just type `read.csv`into the R prompt (without the usual function-call parantheses).
If you choose a value for n or use str_split_fixed it returns a matrix where the columns are the 'splits'. In the first example we're picking 1 since we want the string coming before the first space. Bummer about installing. I assume you tried install.packages('tidyverse') ?
I mostly agree but for a lot of plots you want factors. It's also nice if you want to sort in non-alphabetic way
Would a ‘for’ loop not work?
That sounds extremely promising. Do you have general example that could get me started? Thank you so much. 
I was looking into this option, but many have warned against the for loop in R. I'm still intermediate with R, so I'm not entirely sure why. 
Here's a function which generates a data frame from a student name. Yours will obviously be more complex. myfun &lt;- function(name) data.frame(student = name) Now, after defining that function, run the following line and look at the result lapply(c('James', 'Jane', 'Usha'), myfun) The result is a list of 3 data frames, created using the parameters you passed to lapply [[1]] student 1 James [[2]] student 1 Jane [[3]] student 1 Usha
If you are comparing two tibbles and the only difference is that the column in one tibble called “num_customers” is of type integer but the column of the same name in the other tibble is of type double, then the function would return false unless you set convert = TRUE 
Another example: factor to character columns
Thanks, I'll try that.
I receive no message at all. Which is why it’s even harder to figure out why t isn’t working. I was able to get it to work previously, but I did not change the directory so I don’t know what I did to make it work because I didn’t see the zip file be created 
I just tested it on my machine and it works fine. `getwd()` returns "/Users/angel" I created 1.txt and 2.txt with text in it in my home folder. &gt;zip("out.zip", c("1.txt", "2.txt")) &gt; adding: 1.txt (stored 0%) &gt; adding: 2.txt (stored 0%) And out.zip is there, and can be extracted successfully.
setwd("C:/Users/MyName/Desktop/rziptest") zip("[test.zip](https://test.zip)",c("test1.txt","test2.txt")) Am I missing something haha? getwd() gives the right path, and even doing dir() says that the two text documents are in the directory, but when I run it, the zip file is not created.
 zip(zipfile, files, flags = "-r9X", extras = "", zip = Sys.getenv("R_ZIPCMD", "zip")) Arguments zip A character string specifying the external command to be used. Weird it doesn't give any errors. Could be the command line command 'zip' is not found in your path. What does `Sys.getenv("R_ZIPCMD", "zip")` say? For me it's `[1] "/usr/bin/zip"` 
\&gt; Sys.getenv("R\_ZIPCMD", "zip") \[1\] "zip"
Nothing happened when I typed zip into cmd. How do I install zip then? I have the utils package installed.
Hahaha...I have. I’ve looked at every search result that comes up when I’ve tried googling solutions.
You googled how to download zip for command line windows?
&gt; DBA / Barycenter Averaging? &gt; &gt; I'm not sure what parts quantify as "best", so it may not be what you're looking for. Thanks, how does it work the barycenter Averaging? What is its formula?
Try google.
&gt; Are the times on the same scale? On the X there is the time, on the Y there is the performance of the player by the time in form of odds of winning or losing the match
&gt;Depending on what you are looking for, you might want to look into "Data.table rolling joins" Thanks, how does it join the two series? which formula does it use to join them?
Ah I meant, if I typed c("variable", "variableA"),B the the cursor immediately goes from point "A" to point "B" without escaping the brackets and then pressing comma. Or maybe the video was just edited and this cant be done...
Awe shit... You done did it!
Yeah only 375 rows won't tax R's for loop.
See [this post](http://prokulski.net/index.php/2018/03/27/lato-bylo-cieplejsze/#mapki-2017) (in polish, but with R code) or [this one](https://timogrossenbacher.ch/2018/03/categorical-spatial-interpolation-with-r/) in english - maybe it'll help?
No, I haven’t. I’m not exactly sure what you mean by that
Same error still.
Sorry, could you show me the 'X' column? Is it 1:00:00 PM 32 January 2017, 2:00:00 PM 32 January 2017.... for both or is it something like 1 PM, 2PM... or is it like this: 120 minutes before match, 90 minutes before match, 60 minutes before match...
Can you combine the 2 datasets?
You need zip installed to use the zip program.
Can you elaborate?
There is this program called zip. It unzips and zips files. You need it to zip and unzip files. R uses it to zip and unzip files. Install zip so that you can create an archive.
try this: ggplot() + geom_point(data = XYZ, aes(x = X, y = Z, label = SysName)) + geom_point(data = gates, aes(x = X, y = Z, group = inSystem_outSystem)) + geom_text_repel(size=2.5) This works for me, but is not the best practice. I recommend to put everything in one dataframe
GraphPad PRISM has a manual called Fitting Models to Biological data using linear and non-linear regression. It is n excellent practical introduction to non linear regression. You'll find the PDF online with a search.
 myVec &lt;- c("Apple","Banana","Cantaloupe") for(i in myVec){ assign(i, as.character()) }
thanks
No, they're uneven column lengths (more paths than points).
If I were doing it I would probably make a list as long as the vector and name the elements of the list that. Also facilitates iterating over them easier.
&gt; What's the best way to create three empty vectors with the names apple, banana, and cantaloupe The best way is *not to do that* and to instead work with a nested list or some other appropriate data structure. Programmatically creating separate variables isn’t the right thing to do 99% of the time when these questions are asked. After that, it depends on what you want to do: clearly empty vectors aren’t very useful (otherwise, just create a list of `NULL`s using `result = setNames(vector('list', 3L), myVec)`); instead, generated your data directly and *then* assign the names (as shown in the previous code, using `setNames(…, myVec)`). 
You're absolutely right, I didn't think about LOESS or spline interpolation. Those techniques would work just fine I'm sure, but I am a giant dingus and forgot that there is already a formula out there for this function! After a quick search I found an R package, FitWebb, that uses the model laid out in Webb et al. 1974 (photosynthesis vs irradiance in leaf productivity). This package is (thankfully) super easy to use! However, I'm having a hard time adding a trend line, which is super critical to what I'm looking for. If anyone knows general script for fitting a trend line that would be awesome! I've tried abline() but it doesn't seem to give me anything.
Don't declare a data or any aes in ggplot(). Just leave it empty. This will create an empty sort of canvas layer. Then in your geoms, declare your data and variables.
Where do I put the label=XYZ$SysName then?
 lm()
If the datasets aren't too big to crash your R session, you can still join them together, the unevenness will be evened out. You can use dplyr::full\_join, so you retain all data points. Is it possible that the error you are getting is due to them being uneven?
In the geom_point() like the example below. That works for me at leats
I would use a for-loop. 365 iterations of a simple API-call will take a second. No need to speed that up and make life harder than necessary...
make a loop and iterate over the set of numbers?
I just can't figure out how to use GET() and input a different URL each time amid the for loop. 
I'd use the httr package. Inside the loop, create a reqstring&lt;-paste(...). Then, your GET(reqstring, headers).
Use paste0 function or the glue package to construct the urls if that's what you are looking for 
I have not used it myself but you might want to look into the logr library.
I haven't used it but Rmarkdown sounds like it should be able to do what you need.
Thank you! 
Thank you!
2nd the RMarkdown suggestion as a former educator. In addition, it allows you to segment the code into chunks and have separate outputs from each chunk of code which will be very, very helpful in grading in a fair, less pass/fail format. If you get really stuck on anything, feel free to shoot a PM.
This is it. 
Take the Coursara Reprodoucible Research online course. It's all about R markdown.
Good idea to null out the handle too. I had issues on my Mac with http2 requests. 
Have you tried using a vector? 
Does this work? as.logical(student[student %in% c(1, 0)])
I'm sure there is a better way to do it but this is what I got to work: as_logical &lt;- function (x) { if (all(x %in% c(0, 1, "TRUE", "FALSE"))) { as.logical(x) } else { x } } data_frame(var_1 = c(1,1,1,0,0), var_2 = c(0,1,0,1,1), var_3 = c(5,4,3,2,1), var_4 = c(T,F,T,F,T), var_5 = c("TRUE", "FALSE", "TRUE", "FALSE", "FALSE")) %&gt;% mutate_all(as_logical)
The order function is a bit confusing at first! **Here's what it does:** It takes a numerical vector as input and spits out a vector as output. For example - x &lt;- c(3,5,10,1,4) order(x) will return the following vector (note, the function will use increasing order by default): 4, 1, 5, 2, 3 Why? The function looks at the numbers in your vector and then shows you how to reorder those numbers in an increasing fashion. In other words, the order function gives *indices* as output. Here, the output is basically saying "Put the fourth index of x first, the first index of x second, etc." i.e. c(x\[4\], x\[1\], x\[5\], x\[2\], x\[3\]) will result in a vector that is increasing in order &amp;#x200B; So: x\[order(x)\] == c(x\[4\], x\[1\], x\[5\], x\[2\], x\[3\]) &amp;#x200B; **How to order a data frame with order()** Here's an example data frame or matrix Grade Student 63 Travis 91 Clint 94 Griffin 72 Justin &amp;#x200B; If you wanted this data.frame to be ordered by the students' grades, you could write the following code: ind = order(df$Grade) This is what ind would look like: c(1, 4, 2, 3). This is essentially how you'd want to reorder *entire rows* so that the entire data frame is ordered by grade. ordered.df = df\[ind,\] Or written in one line: ordered.df = df\[order(df$Grade),\] &amp;#x200B; Which would return: Grade Student 63 Travis 72 Justin 91 Clint 94 Griffin &amp;#x200B; Hope this helps! It also helps to read the documentation and just mess around with the code until you understand. 
Use the apply function! Assuming your data frame is df: apply(df,2,as.logical) &amp;#x200B; Note: this will convert *all* columns in your data frame to logicals. You can consider subsetting your data then using the apply.
Jumping off of this data-checking approach, here's a set of some `dplyr` solutions: library(dplyr) set.seed(1) n &lt;- 100 fake_data &lt;- data_frame( id = 1:n, a = rnorm(n, 0, 1), logical_1 = rbinom(n, 1, 0.5), logical_2 = rbinom(n, 1, 0.5), logical_3 = rbinom(n, 1, 0.5) ) # listing all variables to be converted in vars() fake_data_retyped_1 &lt;- fake_data %&gt;% mutate_at(vars(logical_1, logical_2, logical_3), as.logical) # or if all your variables to be converted have common naming pattern fake_data_retyped_2 &lt;- fake_data %&gt;% mutate_at(vars(starts_with("logical_")), as.logical) # or check to see if all values are 0/1, then convert fake_data_retyped_3 &lt;- fake_data %&gt;% mutate_if(funs(all(. %in% c(0, 1))), as.logical)
Can you provide an example of your data frame?
Thanks for the advice!
Do a VM do not do a fresh install. Get in the practice of using VMs and you will be ahead of your competition.
A free-tier offering from a cloud provider is a reasonable option here, if you have the time and support to learn how to take advantage of it.
I'd say learn ggplot2 and do something like this library(ggplot2) ggplot(tabela, aes(Salario)) + geom_histogram(binwidth = 5)
You can add x labels in the barplot command: `labs.x &lt;- c("[25,35)","[35,40)","[40,45)","[45,55)","[55,65)","[65,85)","&gt;=85")` `barplot(mulheres,salario, names.arg=labs.x)` With this data, I'd suggest a scatterplot instead. If not, make your bins the same width.
that's working!!!!!!!! thank you. the only problem now is that some of the bars don't have labels under them... like, the following labels for each bar: [25, 35), none, [40,45), none... and so it goes. but that already heloed a lot!! thank you, really.
thanks, i'll check it out!!
ahhhhhh just letting you know i figured out what was wrong, it was the font size. i got to change it. so everything's alright now. thank you so much!!!!
Could you please elaborate on this point? When I run the code library(dplyr) df1 &lt;- data.frame(x = "a") df2 &lt;- data.frame(x = factor("a")) all_equal(df1, df2, convert = FALSE) all_equal(df1, df2, convert = TRUE) I get `TRUE` in both cases. It appears that the function equates factor and character columns no matter what the value of the `covert` parameter is. The documentation did give your example so is there perhaps something more subtle going on that I'm not following?
looks like your date column is stored as factors. do a str(data) where data is your dataframe name, and see what comes out. &amp;#x200B; And as the other commentator stated, look at lubridate. &amp;#x200B;
yes is a factor, but when i tried to transform it in a character i lost all my dates
Just do as as.Date(column_name) then you can do filtering just fine
difficult to say what's wrong, unless you give a minimal reproducible example. some code, some sample data, and perhaps some screenshots would be helpful. &amp;#x200B;
also looking at your date format this seems to be the correct way to format it &amp;#x200B; `as.Date``("01/01/2008","%m/%d/%Y")` &amp;#x200B;
[https://imgur.com/a/7d8QPrY](https://imgur.com/a/7d8QPrY) Here is a section of the Data, the numbers should be in factors and the T&amp;F answers should be Logical, but not all numeric answers should be factors just some of them. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/FfeMqrf.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e5dci49) 
Transformation from factors to numeric yields strange results. Use as.character() on your factors before trying to do other transformations. 
Your date being converted to numbers IS intended, it is not a bug. The way that POSIX compliant (looking at python, which is not in this regard IIRC) programming languages store dates is as the number of seconds before/after a certain date (somewhere in 1970 I think). Convert your required that (01/01/2008) into the same POSIX compliant format, and you will able to do an easy comparison, exactly as your current code wants, to get the desired result
The syntax in your example is wrong. 1. You refer to "Date" as an object in the namespace of your function call, most likely the global namespace. But if it is indeed a column in a data.frame, you need to refer to the object directly when calling it as well when assigning it. You can do so by direct reference like this (assuming your dataset is called dt): dt$dummy &lt;- someFunction(dt$Date) or by contextualizing the function call inside the dataset namespace like this: dt &lt;- with(dt, dummy = someFunction(Date)) Since you got some output as if Date is a factor, it seems to me you also have an object called Date in your global namespace, that gets evaluated before the column in your data.frame. 2. You apply 'as.numeric' to the output of the `&gt;=` operation. However`&gt;=` will always yield a logical, and as.numeric applied to a logical will yield zeroes and ones. If you want to convert a variable x to numeric and compare it to some value z, you need to write as.numeric(x) &gt;= z 3. Independent of what type the column Date is, the operation Date &gt;= 01/01/2008 will not do any kind of meaningful Date comparison, since 01/01/2008 is just dividing 1 by 1 by 2008 (the numbers). If you want to know if a date is larger than a certain other date, you should first convert that variable to a data type that represents time or dates, like POSIXct. If you are not dealing with hours/minutes/seconds, you can use as.Date() in R to convert a character string representing a date into a POSIXct Object. If it is currently a factor, you need to convert it to character first: dt &lt;- with(dt, Date = as.character(Date)) Then you can try something like dt &lt;- with(dt, Date = as.Date(Date,"%m/%d/%Y")) and look at the output from str(dt). The Date column should now be of type POSIXct. If you want to create you dummy variable, you could now compare the dates like this: dt &lt;- with(dt, dummy = Date &gt;= as.Date("01/01/2008","%m/%d/%Y")) (PS: If you want to use markdown for your code, use four spaces instead of the double *) 
For times that don't match, it will either move them forward or backward (your choice) to match a time in whichever series you choose (based on which table you make the left "side") and then you can associate observations with eachother. I don't know if that's totally what you're looking for, but this page explains the concept well: https://gormanalysis.com/r-data-table-rolling-joins/
thanks for your answer and your time!!i solved my problem
Thanks a lot for your time and effort! It still took me a while, but I get! &amp;#x200B; Can finally move on to the next assignment!
Thank you so much!!
Does this help https://twitter.com/hadleywickham/status/643381054758363136
In the analogy, `x[[1]][[1]]` is not necessarily a list of items; it's the contents of `x[[1]]`. If `x[[1]]` is a list containing that vector, `x[[1]][[1]]` is a vector `n` with `k` elements, where each element `k` is a particular grain of pepper. Imagining that pile of pepper is ordered, if you wanted to grab a particular grain of pepper, say the third one, you would write `x[[1]][[1]][3]`. \[, applied to a list, always returns a list. \[\[ returns the contents of that list. That contents can be anything, even another list. Here's an example: # making a nested list &gt; y &lt;- list(list(list(element = 1))) &gt; y # no subsetting [[1]] [[1]][[1]] [[1]][[1]]$`element` [1] 1 &gt; y[[1]][[1]] # two levels deep, we see the named list element $`element` [1] 1 &gt; y[[1]][[1]][1] # returning a list, which is named, along with its contents $`element` [1] 1 &gt; y[[1]][[1]][[1]] # returning the contents only [1] 1 Does that make more sense? &amp;#x200B;
Two main types to worry about in R (yes there are others, but these are the first things to know IMO): 1) Vector 2) List A Vector has one or more values of the SAME type. This is similar to the `numpy.ndarray` type in Python (numpy package). A List has one or more values of different types. Lists can also contain complex types (e.g. vector in a list, list in a list). An R list is like a Python list in that regard. The main difference is that you can use names to index into an R list, like a Python dictionary, though the performance is not the same (e.g., I believe lookups are still O(n) in R). Notably R has *no singleton type*. If you type `a &lt;- 5` then a is a vector of mode 'numeric' with a length of 1. For Vectors, you always use the single-pair of brackets. For Lists, if you want to slice the list, you use a single pair. E.g., if `L` is a list, then `L[m:n]` will give you back a list. Even if you use a single number. `L[3]` returns a list with one element in it (the third element of the original list). On the other hand, if you want to actually get the object that's inside the list, then you use two brackets. `L[[3]]` returns whatever object is in the third position in the list `L`.
I'm not a semantics expert in R, but `[[index]]` can be reserved for slicing list objects, whereas `[index]` is for just about everything else. Most commonly, you will see something like `X[['list_identifier']]` where `X` is a list object. This is the documentation for list objects: https://www.rdocumentation.org/packages/base/versions/3.5.1/topics/list
&gt; If a packet of pepper is a container of a list of items, then why would x[[1]][[1]] give the list of items instead of a single grain of pepper? In this illustration the jar is a list. Each packet is also a list containing one element — a vector of pepper grains, so x[[1]][[1]] gives you the first vector (there is only one) in the first packet (list) of the jar.
'[[' references an element of a list. ie you lose one level of the list. '[' references a slice of whatever. A single value is equivalent to a vector of length one, in this context (and most others). vectors != lists
Okay, I think I understand the actual semantics of R. My question is, why? This seems like a needlessly confusing way of doing things: &gt; y &lt;- list(list(list(foo = "bar", "x", "y", "z", 42, 99, element = 42))) &gt; &gt; y[[1]][[1]][2][1][[1]] [1] "x" &gt; y[[1]][[1]][2][1][1][1][1][[1]] [1] "x" &gt; y[[1]][[1]][2][1][1][1][1][1][1][1][1][[1]] [1] "x" But I realize I'm new, and there's probably a good reason for writing the language this way. What kind of operations does this operator simplify? Or... can anyone think of an example of why this difference would help?
I found out a way, don't know it is an elegant way. `for(col in names(forloeb_var)) set(forloeb_var, i=which(forloeb_var[[col]]==""), j=col, value=NA)`
To add onto this from a "helpful resources" perspective, you might want to take a gander at the "Subsetting operators" section of Hadley's *Advanced R* found [here](http://adv-r.had.co.nz/Subsetting.html).
Excellent answer. This is exactly what I was hoping for. Thanks! I didn't know that `5` is equivalent to `c(5)`.
I see. So L[k] is identical to L[k:k] which is identical to L[c(k)]. Interesting language....
The basic data structure in R is the vector. You have two types of vectors: atomic vectors and lists. The elements of an atomic vector are of the same type. The elements of a list can be of different types. Now, the way in which you deal with atomic vectors and lists is obviously different. Subsetting atomic vectors is the simplest form of subsetting: “[“. Using “[“ on a list will always return a list. But using “[[“ on a list let’s you pull out a component from that list. There are some other details involving subsetting but I don’t want to write them down since it will just turn into a wall of text. 
Further, this is one of the reasons R may not be a good first programming language. While the object is called a list, the interface is more similar to a hash table, mapping, or dictionary, as they are called in other languages.
`[` accesses an element of a vector. An element can have a name, so if you print it, for example, it will print the name with the data in that element. `[[` accesses the data of an element within a vector. If you print it, you will just see the data. `[[` accesses an element within a list. `[` accesses an element within a matrix and a data frame, specifically `[row,column]`.
Are you trolling? "x^2" is the square of x...
NOW you tell me! R is the only language I know. All kidding aside, I do know R is considered a quirky language, I just haven't experienced why that is, **yet**. Once I tackle Python we'll see, I suppose.
If I recall, `tibble()` corrects that `df[, 5]` case so that it returns a data frame, correct?
Yeah - tibbles are great! Corrected some of the design issues with data.frame
 x &lt;- 2 x^2 x^4 x^6 Etc.
You can also use X**2 to get a factor to the exponential power, which I learned in class today! 
thank you for the explanation, that makes a lot more sense. now i’ll know for next time. 
As others have said, the distinction is only important for lists. mylist[stuff] is a list containing a subset of the things in mylist. mylist[[3]] is element 3 in mylist. mylist[["Larry"]] is the element of mylist with name "Larry" 
first install devtools `install.packages("devtools")` then install the woolridge package from github `devtools::install_github("JustinMShea/wooldridge")` &amp;#x200B;
As I've made this mistakes many times when searching for the book in the library system: the name is spelled Wool**d**ridge. My guess is that the spelling was your problem before. 
Note: operators in R *are* functions. Calling them just uses a different syntax (though you *can* call them like normal functions, by making them into a valid R symbol name, i.e. backtick-quoting them: `` `^`(x, 2)`` is the same as `x ^ 2`).
[str_detect](https://stringr.tidyverse.org/reference/str_detect.html) should work.
Programming with dplyr can be challenging. A full explanation of how to do what you want would be very long. I suggest you review this carefully and work through the examples to understand how it works: https://dplyr.tidyverse.org/articles/programming.html
Your basic issue is dplyr/mutate uses bare column names when you're naming the columns. You're giving it the column name as a string. So your code should be: dataset %&gt;% mutate(tfoo = mean(bar)) # Correct dataset %&gt;% mutate("tfoo" = mean(bar)) # Incorrect Think of dplyr as a way to not have to write *dataset$* when selecting/naming/adding columns.
Hi there! I've edited my post above now that I am back at my desk. 
Gotcha. So if you want to name columns with strings, you add an underscore at the end of your call. dataset %&gt;% mutate_(paste0("t", depvar) = mean(bar))
mutate_, thanks for that! I edited the post with the solution that I ended up finding, involving rlang and double-exclamations and :=. 
So though the update you posted works, the *mutate\_* is the normal dplyr way if you want to use strings. For example you can use *select\_* with strings instead of bare column names if you wanted to.
By any chance, are you a (former) Stata user? It sounds like you're trying to replicate the egen command in R. Glad you found a solution!
I think this is a case to use regular expressions &amp;#x200B; e.g. expr &lt;- "XY05.{1}" codes &lt;- c("XY05A", "XY56", "XY555", "AT003", "GA003A", "XY36", "XY100", "XY03") grepl(expr, codes) \[1\] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE &amp;#x200B; details here [https://www.regular-expressions.info/rlanguage.html](https://www.regular-expressions.info/rlanguage.html)
Can you just make the function a little more complicated? &amp;#x200B; `add_colnames &lt;- function(x){` `x$rows = row.names(x)` `x` `}` &amp;#x200B; `df.original[] &lt;- lapply(X = df.original[], FUN = add_colnames)` &amp;#x200B; This assumes you don't have a column named 'rows' in any of the data frames.
!remind me after 3 weeks "read"
Never meant to be rude, sorry
Thank you so much. This did work to add a column at the end of the data frame. But now I have list of 452 [[]]. How can I extract each of these data frame. AAAAAHHHH!!!!! ;) [[1]] X[[i]] time 2018-08-29T19:00:00-06:00 39 2018-08-29T19:00:00-06:00 2018-08-30T11:00:00-06:00 12 2018-08-30T11:00:00-06:00 2018-08-30T12:00:00-06:00 51 2018-08-30T12:00:00-06:00 2018-08-30T13:00:00-06:00 6 2018-08-30T13:00:00-06:00 2018-08-30T14:00:00-06:00 14 2018-08-30T14:00:00-06:00 2018-08-30T23:00:00-06:00 3 2018-08-30T23:00:00-06:00 2018-08-31T14:00:00-06:00 5 2018-08-31T14:00:00-06:00 2018-08-31T15:00:00-06:00 1 2018-08-31T15:00:00-06:00 2018-09-02T18:00:00-06:00 5 2018-09-02T18:00:00-06:00 2018-09-03T15:00:00-06:00 5 2018-09-03T15:00:00-06:00 2018-09-03T16:00:00-06:00 5 2018-09-03T16:00:00-06:00
Thank you so much, this worked. 
unnest() ?
The purrr package can help you apply a function to each element of the list and combine the results. df &lt;- unholy.list %&gt;% map_df(bind_rows) Might do the trick. Without seeing more sample data it's hard to be more specific. 
Though the tidyverse answer is probably the best, does unlist() work?
For some reason that completely dismantles the list into a ton of characters. But thank you for the suggestion.
That is very close! But it does not retain the dataframes individually. Thoughts? and THANK YOU 
names(unholy.list) &lt;- as.character(1:452) first_df &lt;- unholy.list %&gt;% map("1") or first_df &lt;- unholy.list[["1"]] 
test %&gt;% tibble()%&gt;% unnest(.id = "name") Then manage the single combined data frame between long and wide formats 
&gt;A t student? &gt;You can use the F test , here is a wikipedia page for you to understand what it is used for. &gt; And do I need to check the variance ONLY for the variable that has more weight, and ''esclude'' the other ones? &gt;Not really, think about it logically, for example you have a variable that is explained by two others, one affects it by let's say 60% and the other by 20%, the one that affects with 20% is by no means neglected, it is also important right ? that's why we use multivariate regression, to find in which way many variables affect the same variable. How do I calculate the variance as you said? &gt;Normally you don't, because there are functions that do it all for you and only give you F0 and F. F is the F statistic for your data, F0 is what's called the critical region probability, by comparing the two you can find if the variance explained by the model is enough. &gt;But it could be done manually so you would understand how it is done, normally that is explained in statistical courses. &gt;I feel like my explanations are a bit out of order, I should give find a book or source for you, but I feel like they are too detailed and will scare you away. &gt;Feel free to message me if you need anything. :) Sorry for the late response, I am really thankful to your explaination. I am beginning with statistic. you say: &gt;Not really, think about it logically, for example you have a variable that is explained by two others, one affects it by let's say 60% and the other by 20%, the one that affects with 20% is by no means neglected, it is also important right ? that's why we use multivariate regression, to find in which way many variables affect the same variable. Just to be clear: when you say ''a variable is affected by two variables'' tecnically are you saying that Y (dependent variable) is effected by two predictors (X1 and X2) in a multivariate regression? Or with ''a variable is affected by two variables'' are you referring ONLY to the predictors (the X) and not to the Y (the response variable) in a multiple regression? And why do you need to calculate the variance of the variable that explain the most the Y? When I have the F0 the critical region probability what can I assume? Let's make an example: I have made a multivariate linear regression and I got an F0 of 0,43. on one variable which explains the 40% the Y and a F0= 0.80 on a variable which one explains for the 60% the Y. What can I assume with this F test? I mean pratically Sorry for all these question, I am beginning and fascinating about this 
I mean, you could always do it with a for loop if you do not care about effectivity but something that just works. But what do you mean by 'extract'? To where? Are the list entries the same? Do you want to rbind them? Do you want to create a new variables? You didn't provide much info...
Hmmm. Try unholy.df &lt;- do.call(cbind, unholy.list) 
Use rbindlist from data.table
Sorry for the lack of detail. Brain is fried. Thank you for taking the time. I need to isolate these data frames as they represent web actvity for individual users. Other data from a different API call will merge with it, but for now, I just need be able to access these listed data frames and their columns efficiently so I can eventually send them to a Shiny app for analysis of each user. As the data frames are, I cannot manipulate and analyze the data across all users since the data is so nested. Does that help clarify?
I think this will just combine all the data sets in the list into a single one rather than giving out separate lists.
I'm not in front of my machine, but _recursive = FALSE_ might fix this
Yes.... from the description it looked like he had a ton of length 4 vectors and dataframes but as I said I might have misunderstood what he’s trying to do. I thought by “extract” he meant “combine in a df so I can work with it” After reading his comments I really don’t know what he’s trying to do and I don’t think he does either. A list of data frames is probably the most efficient way to store this. Perhaps he wants to pull all 400 items out into variables but that seems... terribly dumb to me.
lapply(mylist, function(x) {y &lt;- x[[1]]$col1 ; doStuffToY() } ??
Did you install it? &amp;#x200B; [https://blog.rstudio.com/2016/09/15/tidyverse-1-0-0/](https://blog.rstudio.com/2016/09/15/tidyverse-1-0-0/)
I've tried but it didn't seem to work. install.packages("tidyverse") Installing package into ‘C:/Users/User/Documents/R/win-library/3.5’ (as ‘lib’ is unspecified) trying URL 'https://cran.rstudio.com/bin/windows/contrib/3.5/tidyverse_1.2.1.zip' Content type 'application/zip' length 92547 bytes (90 KB) downloaded 90 KB package ‘tidyverse’ successfully unpacked and MD5 sums checked The downloaded binary packages are in C:\Users\User\AppData\Local\Temp\RtmpyexpBV\downloaded_packages library(tidyverse) Error: package or namespace load failed for ‘tidyverse’ in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]): there is no package called ‘stringi’
"cathartic exhale"... thank you so much
You can just try to install "stringi"
Could you provide a data sample please? A dummy set would work.
This is really hard to say because of the limited data example and the complexity. Maybe try nesting lapply? Ie, write a function that is designed to take one element of the list and analyze it using lapply. Then use lapply to apply that function to all elements of your list. If there’s no standardization or rhyme/reason as to HOW your 400 elements are nested (some nested at one level, some two levels deep, some as data frames, some as vectors) then any single function isn’t going to work. You’re going to have to write something that will be a mess of rules for handling your different cases.... Best of luck!
has he shared some more analogies like that ?
No problem. I hope it helps out!
noquote(rep(30, '-')) is what I would do. 
You are looking for something like the following: cat(rep("-", 30), sep="") 
dang I guess I was pretty close then, thanks! 
Yea, you can’t use operators like “+” on strings in R.
The dude literally wrote entire books that are freely available online. [It's the second google result.](http://lmgtfy.com/?q=hadley+wickham+indexing+lists) Not trying to be a dick, but you gotta do a little legwork yourself...
haha no problem, good luck!
Might be an interesting watch. [https://resources.rstudio.com/the-essentials-of-data-science/the-grammar-and-graphics-of-data-science-58-51](https://resources.rstudio.com/the-essentials-of-data-science/the-grammar-and-graphics-of-data-science-58-51)
awesome, thanks
That method will yield a line that looks like this: [1] - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - This is because you are still using a `print()` call. Where `cat()` is used more typically used for output in user-defined functions. The one I show below, `cat(rep("-", 30), sep="")` produces an unbroken line without a preceding element index: ------------------------------
Yea, you’re right.
&gt; Just to be clear: when you say ''a variable is affected by two variables'' tecnically are you saying that Y (dependent variable) is effected by two predictors (X1 and X2) in a multivariate regression? Or with ''a variable is affected by two variables'' are you referring ONLY to the predictors (the X) and not to the Y (the response variable) in a multiple regression? Sorry I didn't use technical terms there. I meant this: &gt;Y (dependent variable) is effected by two predictors (X1 and X2). Mm.. &gt; And why do you need to calculate the variance of the variable that explain the most the Y? No, you probably misunderstood it. You don'tneed to calculate the variance of the predictors, you need to calculate how much of the variance of the dependent variable is explained by the predictor(s) &gt; When I have the F0 the critical region probability what can I assume? Let's make an example: I have made a multivariate linear regression and I got an F0 of 0,43. on one variable which explains the 40% the Y and a F0= 0.80 on a variable which one explains for the 60% the Y. Not much, you need the critical region probability and the F statistic of a certain hypothesis, you compare them to draw a conclusion. &gt; Sorry for all these question, I am beginning and fascinating about this I have been there too, in fact I am still learning. You should take some course or read a book that is organised and simple, I took some subjects in college so I don't think I can help with material. &amp;#x200B;
I recommend either modifying it to cran-berry or cran-apple.
Which package are you having problems with? Also yeah just extract the function from the package like this [https://github.com/cran/ggTimeSeries/tree/master/R](https://github.com/cran/ggTimeSeries/tree/master/R) 
It's shinyTree. It's so diletantly and incompetently gobbled up package I seriously wonder how it ended up on cran. There's a github fork that tries to patch some bugs and I had to mold it even further.