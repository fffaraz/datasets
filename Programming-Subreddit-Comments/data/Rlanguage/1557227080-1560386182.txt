Hmm, never knew this could impact on one like this.
I had a similar problem I solved a couple different ways as I learned R. My first attempt was to loop through to the points that could be in the radius and then do: sqrt((x1-x2)^2 + (y1-y2)^2) but that was horribly inefficient. The best method I've found is to use spDistsN1 in the sp library. If df.of.points is the data frame of points you would like the qualify as in the radius and base.lon and base.lat are the center of that circle, then you can add a variable to df.of.points called Distance like this: df.of.points$Distance = spDistsN1(as.matrix(df.of.points[,c("Long","Lat")]), c(base.lon, base.lat), longlat = TRUE) * 0.621371 #convert to miles Then use something like: df.of.points[df.of.points$Distance &lt; 1.0 , ] # distance less than one mile to subset your points inside the radius.
Yep, I think you're good. Unless you are building a super AI using a Dyson Sphere or something.
Hahaha, thank you!! I cannot figure out why my PC crashes when trying to load my dataset then, the dataset itself is perfectly okay and should load. Do you have any suggestion for how to figure out what's wrong or what I'm doing wrong?
Open your task manager and look at your resource management and see if anything is going to 100%. You may actually have the opposite issue where it is utilizing 100% of your ram when it shouldn't use more than 85-90% of it or else Windows has nothing to use.
You were right! It's the "disk" that's hitting 100% when R does something, will look into this - thank you very much
You're right, but only if your screen is wide enough. ...mine is 4:3 ...I need another job...
If it's only your disk then there should be no problem: it's reading the file from disk and loading it into the RAM. Opening other files or programs will be much slower it but should eventually finish. Crashes on the other hand could be because of 100% RAM (or even 100% CPU in very rare cases). If it takes too long it could also mean your hard drive is getting old and slower, but in that case your whole system would suffer, not just R.
It is indeed crashing every time and RAM is reaching 100% as it crashes. I just got a brand new PC though with 32GB RAM so this doesn't make very much sense
- Tab indent to 2 spaces Gotta see all that nesting in those shiny apps - Highlight functions in the syntax It makes R 10 times prettier - Change the theme to Cobalt It makes R *usable* - Change the font if I'm on Linux Srsly why does it use a proportional font for default?? This is for programming!!! (Also install all the dependencies in Linux before installing the R packages) - Change the autocomplete delay to 50 milliseconds. Gotta code fast - Change the pane layout Ultimate layout (for me): Environment | Source Plots | Console Make the right side wider than the left, and the console smaller than source. Now plots are square, environment doesn't take too much valuable space, and since you usually see one command at a time on the console, it's smaller, so most of the space goes to the Source, the thing you're working with 95% of the time.
Are you in Linux? (Build times there are way slower than on Windows)
You can just add a 1 into your sample! mydata[, c(1, sample(1:ncol(mydata), 5000)] Also, just to confirm, you're sampling columns from a dataframe?
Commenting for later :)
The sf package has some useful functions for spatial data. https://r-spatial.github.io/sf/reference/geos_binary_pred.html st_is_within_distance(x, y, dist, sparse = TRUE) The above function should help. I think you will need to project your lat/lon data with a spatial reference system (CRS) to end up with an sf object. A pretty good guide for that can be found [in this stackoverflow thread](https://gis.stackexchange.com/questions/222978/lon-lat-to-simple-features-sfg-and-sfc-in-r).
Affirmative. I find it much nicer dev environment
its from a tibble, but I believe that should work the same right?
Thats worked perfectly, thanks for the help!
Yes! It will convert your tibble to a data.frame, but you can easily convert it back without losing anything.
Related to setting up R on a new PC. I have a question as well. When installing packages (tidyverse etc.) using install.packages(). Is it normal that R throws this on every package install?: WARNING: Rtools is required to build R packages... I'm new to R and RStudio.
How large is your dataset? And, can you read it in other software, like python? Just to test if it's a fault of the R package. Also, try using a different package to read it in R. Maybe that will work.
If I have 16GB RAM and set my max to 32GB will it actually do anything? Also this only applies to Windows - Linux users will get a warning.
Are you using Linux? Mine says Inf but sadly I do occasionally run out of RAM.
I didn't realise you can do that, my bash script has a section to install ruby gems, Atom and LaTeX packages in pure bash so if I did this could I just move the R packages to a section in that rather than calling an R script to do it?
You can't use more than you have... I only mentioned this because the default ram cap is set by RStudio and has caused myself and some other people issues in the past. There is a reasonable chance it set an appropriate value for you and you don't need to do anything though.
It will help us to see the error message - try to include that in the future! However, I think you need to group_by age, gender first. As is, you're making a df of just 1 number. `small.data %&gt;% group_by(age, gender) %&gt;% summarise() ... `
How does it determine what to set the limit at? My work laptop has 8GB and it says 8GB but a few years ago I did have to change it to get it to run some scripts.
This is true, also after the ggplot() function you have ‚Äú%&gt;%‚Äù, where as ‚Äú+‚Äù is used for chaining ggplot commands together.
I have no idea.
ggplot doesn't support piping internally. though you can pipe a dataset to it. &amp;#x200B; so, instead of: &amp;#x200B; `ggplot(aes(x=age, y=avg.height)) %&gt;% geom_point() + facet_wrap(.~gender)` &amp;#x200B; it should be &amp;#x200B; `ggplot(aes(x=age, y=avg.height)) + geom_point() + facet_wrap(.~gender)`
ggplot2 allows you to directly link the data dplyr-style so that portion isn't incorrect. sumwunelse's solution is correct. OP needs to group\_by() before summarize().
subex&lt;- read\_rds("Desktop/finalexam\_19s/examdata/suburbanexpress.rds") subex&lt;- subex %&gt;% rename(date = created\_at) subex &amp;#x200B; here is the code I am trying to knit
i just tested it, and it didn't work. you have to use "+" to join up the ggplot methods.
Is the `RMD` saved in the `C:` top-Level folder? Knitr adds the directory path of the `Rmd` file to the front of any file paths (unless specified from a top-level location, ie ‚ÄúC:/‚Äù).
Not every package is available on conda-forge but otherwise you should be able to just do a standard conda install r-package name. You probably want python installed somewhere so might as well use conda to do these steps.
ah forgot that part! thank you. I'd say that error catches me more often than I'd like to admit.
First, the function does not run as-is,since it refers to an undefined variable (`J`). If what you are trying to do is carry forward the previous value (next obs carried forward), then look at the `na.locf()` function in the `zoo` library. Generally, loops in `R` are not an ideal solution for one of two reasons: it's easy to forget to pre-allocate, so you end up growing vectors. Also, you want to avoid the overhead of a function call for each element if possible.
\&gt;Im not sure why my script doesn't work! Me neither!
https://topepo.github.io/caret/pre-processing.html#corr
Going to need more of an explanation of what you're trying to ask. For some meaning of what you're trying to ask, yeah, this is what GLMs are about.
`ggplot(aes(x=age, y=avg.height)) %&gt;% geom_point` Should be `ggplot(aes(x=age, y=avg.height)) + geom_point`
This looks like it could be a for loop instead of a while loop. And if it's for filling in missing values, see the ```fill()``` function from tidyr.
You could write this in vectorised form pretty easily with ```ifelse``` and ```dplyr::lag```.
Understood on the time issues, thanks. I modified the code to make it shorter when I posted and made a mistake.
You would have call lag several times. Would probably just end up being a loop?
Thanks.
Actually they are useful! Why didn't I say so?? Because I knew someone who thinks they are God's gift coming along and saying something like 'actually, the genetic code is non-overlapping'. You don't know everything mate... In my case I need to split them into overlapping triplets.
I just found that - %m+% lubridate:::months(x) results in Evaluation error: object 'months' not found. But it works if I use %m+% lubridate:::months.numeric(x) &amp;#x200B; This is still irritating
Biggest reason is it removes ambiguity of direction in your assignment. y &lt;- x means I am creating/replacing variable y from variable x but not altering x. However x &lt;- y does not mean the same thing, while an equal sign implies both directions. You can also do the arrow, the opposite direction where x -&gt; y is the same thing as y &lt;- x. Beyond this, there are some historical reasons which aren't super relevant today. It also really helps improve legibility when you have a series of pipes together in compact code as you can tell what is creating a new variable, vs what is acting as arguments in a function without squinting too hard. &amp;#x200B; You can also assign hotkeys in r-studio to type &lt;- easier, I believe "Alt" + "-" is the default.
&gt;In my case I need to split them into overlapping triplets. Then this information belongs in the question.
Thanks for that code, I do appreciate it. But I think maybe you skipped the title and first sentence and went straight into thinking about what was wrong about my post. Reading more carefully would have probably avoided all this unpleasantness
Also, loops are fine. It's just there are normally faster, vectorized solutions. This would be my loop solution: set.seed(1) x &lt;- c(1, sample(c(NA,1,2,3,4), 100, replace = TRUE)) x for (i in 2:length(x)){ if (is.na(x[i])) x[i] = x[i-1] } x
Behind the scenes, the big difference is precedence. See this stack overflow question for a good overview: https://stackoverflow.com/questions/1741820/what-are-the-differences-between-and-in-r
I'll note that many of the answers there are bad or at the very least misleading, including the accepted answer. The only answer that's both correct and complete is, I claim, [my own](https://stackoverflow.com/a/51564252/1968).
&gt; while an equal sign implies both directions Strictly speaking it implies neither. But there's a virtually unanimous convention in programming (influenced by definitions in mathematics) that assignment is performed from right to left. There's really no ambiguity at all. If anything, the fact that R allows left to right assignment causes confusion.
If I understand your question, you could run a binomial GLM on your features using the glmnet package.
With that in mind, as long as one was consistent about always assigning variables right to left, it would seem one could use = without any trouble. Anything I'm missing there? As you note, mathematics typically follows this convention as well, so I really find the bidirectional assignment "feature" of R undesirable, to be frank.
&gt;Anything I'm missing there? Nothing at all. I‚Äôve swapped from `&lt;-` to `=` a few years ago and I never looked back. &gt;so I really find the bidirectional assignment "feature" of R undesirable Yes, I‚Äôve argued the same elsewhere. Left-to-right assignment should simply never be used, *especially* at the end of long pipelines (where it‚Äôs easily overlooked).
OK, thanks. I did run into an issue in a while loop where the condition read values from several data frames and performace was an issue. I wrote it in Rcpp and it was way faster. Rcpp for everyone states: R is weak in some kinds of operations. If you need operations listed below, it is time to consider using Rcpp. ‚Ä¢Loop operations in which later iterations depend on previous iterations. ‚Ä¢Accessing each elements of a vector/matrix.
Did you mean "&lt;-" or "-&gt;" There's no case in which = is interchangeable with -&gt; as a "forward assignment" operator. The only \*reasonable\* use case of -&gt; is to save a long chain of operations in a variable, like this: \`\`\` my\_df %&gt;% group\_by(some\_column) %&gt;% summarize(counts=n()) -&gt; new\_df\_with\_counts \`\`\` (although this makes it harder to find things in code "where do I declare new\_df\_with\_counts ???" )
Hey! If I understand correctly, you question is how to show that logit's systematic component is has a linear relationship between explanatory variables? If so, here is an [article](https://newonlinecourses.science.psu.edu/stat504/node/164/) that briefly discusses it. Hope this helps! Best, [DataSharkie](https://datasharkie.com/)
Even as a cimg, it still spits out an error unfortunately. Is there a way I can throw 3 more channels behind it, even if they contain no data?
Hmm. It's probably expecting 3 channels or something, yeah. My first bet would be to try replicating that one channel three times. I don't have `raster` installed on this machine and don't have anything saved as a raster object, otherwise I'd fiddle around with this myself.
I don't know of **any** best practices that recommend use of `-&gt;` ... but there are many that recommend `&lt;-`. As r/guiper points out, it is possible to program in R and never using `&lt;-` with occasional additional use of parentheses. However, the distinction between which environment is affected by the operator when used for named parameter matching versus variable assignment is what leads most R programmers to use `&lt;-` for assignment... to clarify context. If you plan to work with other R programmers, be sure you know which convention is typical among that specific group (e.g. company style conventions) and choose whether this is important enough to fight over... because it seems to be more divisive than `vi` vs. `emacs`.
&gt;the distinction between which environment is affected by the operator when used for named parameter matching versus variable assignment is what leads most R programmers to use `&lt;-` for assignment... to clarify context. I can‚Äôt let this stand uncorrected, sorry. Two points: 1. **There is no distinction** between which environment is affected by either operator**.** It‚Äôs *always* the current environment. Argument passing (a) does not use the assignment operator (though the syntactic token is the same as `=`\-assignment), and (b) *does not perform assignment at all*, so it affects no environment. Argument passing isn‚Äôt assignment. 2. The reason why *the vast majority* of R programmers use `&lt;-` vs `=` is either for historic reasons or ‚Äúconvention‚Äù, or due to a misunderstanding of point 1 (I used to fall in that camp). I know [you‚Äôve explained your reasoning to me before](https://www.reddit.com/r/Rlanguage/comments/bipho3/how_to_start_a_sustainable_r_project/em5l0o9/?context=3) but I can‚Äôt help but note that you‚Äôve committed the same inaccuracy here. And this matches my experience with other people: It *may* be different for you. But for most people, ‚Äútechnical‚Äù reasons for using `&lt;-` over `=` are pure post hoc rationalisation.
&gt; However, the distinction between which environment is affected by the operator when used for named parameter matching versus variable assignment is what leads most R programmers to use &lt;- for assignment... to clarify context. I'm not certain I know enough about R to understand this; I think I do, but the word "environment" is throwing me off. Are you saying that it makes sense to use both because otherwise things get confusing to read? (that is, the same reason why assignment is = and the equality operator is == in C-type languages)?
&gt;I don't know why the estimate for percent is like that. Isn't it supposed to be from 0 to 1. I mean that's completely arbitrary and dependent on how your data is formatted/scaled. Can't tell you much without seeing the data, or even sample data. Consider the estimate is for a 1 unit increase in x, as well.
Please show a reproducible example.
I'm not sure I've understood your comment. Are you saying that `=` never performs left-to-right assignment? That does not seem to meet with my experience based simply on usage, so I'm assuming either it doesn't, or I'm extremely confused.
 &gt; dir.lat.long$Distance = spDistsN1(as.matrix(dir.lat.long[,c(dir.lat.long$Station_Loc_Lng_NAD27, dir.lat.long$Station_Loc_Lat_NAD27)]), c(dir.lat.long.uwi1$Station_Loc_Lng_NAD27, dir.lat.long.uwi1$Station_Loc_Lat_NAD27), longlat = TRUE) Error in .subset(x, j) : only 0's may be mixed with negative subscripts Thanks for the help. I am getting stuck here because my long's are negative so I think there is some rule that there can only be zero's and positive or zeros and negative in the same subset. Any Ideas on how I could proceed from here?
When does the equal sign perform assignment left to right? Certainly not for e.g. a = 3.
You should be doing `lm(DV~IV)`; it looks like you switched x and y. Does that solve your problem?
Don‚Äôt assume my intentions. If you read my posts here or elsewhere you‚Äôll see that I always attempt to help people and, unless there‚Äôs a good reason, I assume good faith. Given the extensive discussion with me, as well as the other confused answers (both on this question and its duplicate in the other subreddit) it‚Äôs clear that my initial stance was entirely correct: You simply did not provide relevant information to correctly answer your question. Nothing you‚Äôve said has changed this, and rather than accusing me of being rude you need to take a step back and accept constructive feedback.
Edited the post with sample data
Edited the post with sample data
Well, then of course we should expect large estimates. &gt;lm(TAXTRANSFERS~PERCENTWELFARE,data=TEST) That asks: If we had percent welfare == 0.00 and we then changed it to percent welfare == 1.00, what would be the change in TAXTRANSFERS? You can see how the way your data is set up, 1.00 I imagine is 100%? So going from 0 to 100%, we imagine that there is an increase of 2252.33 TAXTRANSFERS.
Thanks to everyone who answered!!! In the end I did a chi squared test, so thanks for reassuring me. Professor said it's a fitting algorithm.
I intend it to be the other way around where increase in TAXTRANSFERS should increase PERCENTWELFARE. I must have switched x and y like what the other commenter noticed.
You should then edit your post with what you learned and the new output and what else you need clarification on.
I'm grateful for your help. I edited the post again.
 &gt; dir.lat.long$Distance &lt;- spDistsN1(as.matrix(dir.lat.long[,c(dir.lat.long$Station_Loc_Lng_NAD27, dir.lat.long$Station_Loc_Lat_NAD27)]), c(dir.lat.long.uwi1$Station_Loc_Lng_NAD27, dir.lat.long.uwi1$Station_Loc_Lat_NAD27), longlat = TRUE) Error in `[.data.frame`(dir.lat.long, , c(dir.lat.long$Station_Loc_Lng_NAD27, : undefined columns selected I am also now getting this undefined columns selected error message. This is after I have absolute valued the negative longitudes.
Tried this out and it is seriously a massive improvement.
It depends on whether built binaries are available for your OS on CRAN. On windows you should almost never get this message.
```base::months()``` is in the default namespace and works just fine (rstudio's linter doesn't like it though).
very interesting. Funny to write it: x %m+% lubridate::years(1) %m+% base::months(1) Thank you very much
I recently did a clean install of Windows 10 Pro and thereby also R and RStudio. And I have the newest of versions of them. So can that be the cause?
Just to be clear, you are running the command install.packages("tidyverse"), written exactly like that? If so you shouldn't be getting that message, have you tried reinstalling R?
Yes exactly like that. I can try that :)
Maybe try in rgui rather than rstudio too.
Yeah you're right. I think a solution using recursion is also possible.
At a glance it does not seem like a useful tool. I am not sure why I would want a point and click interface for R. I am able to manipulate and view data in RStudio fine. I would read chapter 5 of [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/) and other chapters that might be relevant and do work in RStudio.
I used this briefly several years ago before I was comfortable writing R code. It's a nice way to dip your toes into the R world, but you quickly learn that it's best just to learn the language itself rather than using a GUI.
Possible, but kind of strange. Did you set it to always build from source somehow? You could also just install Rtools: https://cran.r-project.org/bin/windows/Rtools/
I think you're getting undefined columns because you're attempting to use the entire vector of Station_Loc_Lng_NAD27 and Station_Loc_Lat_NAD27 as columns. Use c("Station_Loc_Lng_NAD27","Station_Loc_Lat_NAD27") instead.
I don't think so, I don't even know what build from source is actually! üòÅ I have the RStudio Preview version actually, and the only package I installed before I started noticing the problem was devtools.
I get no warnings in the R gui, strange.
I for one have never encountered a programming language for which `=` performs left to right assignment.
I am mystified by this assertion: Argument matching... *does not perform assignment at all*, so it affects no environment. To reiterate: Argument passing isn‚Äôt assignment. AM is different, but it very much does lead to the creation of a variable within the function if it is accessed. And using the term "current environment" very much misses the point that each expression may be interacting with different environments. Assignment is an operator that can be part of an expression, while AM can only be part of a function call. f &lt;- function( x, y ) { x+1 } z &lt;- 1 w &lt;- 2 f( x = z &lt;- 3, y = w &lt;- 5 ) [1] 4 z [1] 3 # side effect of expression w [1] 2 # never evaluated `x` and `y` represent objects within the environment of `f` that are affected by expressions written in the calling environment, yet are not evaluated until they are accessed. So during the execution of `f` (when most people consider its environment to be "current") the first argument is evaluated and changes the variable `z` in the calling environment. But since the second argument `y` is never evaluated no value appears in `f`'s environment, and `w` in the calling environment never gets changed. I think asserting that there is only one environment that is "current" at any time is misleading, and I also think having separate symbols for argument matching and assignment helps me in parsing syntax like this function call. Thinking of argument matching as a differentl kind of assignment is a useful mental model that helps separate the impacts on the two environments being affected within the one line call to `f`, whether you want to be pedantic about terminology or not.
If you do get around to it, let me know; its possible I just haven't tried to insert the channels the correct way as of yet. I'm quite new to R so that is a genuine possibility
Yes, that is my assertion, and numerous others as well, but r/guepier prefers not to rely on that crutch. The term environment has a very specific technical meaning and data structure in R, but loosely it can be thought of as the place where variables that you are using are stored, and for any environment there is often a related "parent" environment (which was "current" when the function associated with an environment was defined), and a "calling" environment (which was active when the currently-active function was called). You may need to read https://adv-r.hadley.nz in order to "get" the example I gave elsewhere in this thread, which illustrates the delayed evaluation involved in passing arguments in R, and how assignment is an expression but argument matching is not.
Presented at the East Bay R User's Meetup in Oakland, California.
&gt;\[argument matching\] is different, but it very much does lead to the creation of a variable within the function if it is accessed. And that may look similar in effect to assignment but it‚Äôs fundamentally different at all levels. I know of *no* language (R included) which treats argument passing as assignment. Beyond the syntactic similarity in R, they are handled differently under the hood. But even at a syntactic level they are different: &gt;each expression may be interacting with different environments Ah, but you‚Äôre making the same mistake again here: Argument passing *is not even an expression*. The argument *itself* is an expression. But the syntactic piece of code `‚Äπname‚Ä∫ = argument` is not an expression. It may look like one but R parses it completely differently (you can check this yourself by inspecting the result of `quote(f(a = 1))`, and trying to find the `=` operator in the parse tree ‚Äî it doesn‚Äôt exist). So much for syntax. Under the hood (and in R in particular), assignment is an *expression* that *calls an operator function*. Argument passing, by contrast, isn‚Äôt a runtime operation at all. Instead, it‚Äôs a hint at the parser as to how the function should be called. Merely the function call itself is a runtime operation, and part of the operation of a function call is to set up a call frame, which contains names that are bound to the arguments. And, again, the effect of this name binding may *look* as if an assignment took place (in the context of the newly created stack frame), but none did. The thing that leads to confusion here, and the only reason we‚Äôre having this discussion, is that R unfortunately decided to use the same symbol for assignment as for named argument passing. If R, like some other languages, had chosen a different symbol (say `:`) this confusion wouldn‚Äôt arise: If we wrote f(a: 1, b: 2) then nobody would get the idea that `a: 1` was an assignment. &gt;I also think having separate symbols for argument matching and assignment helps me in parsing syntax like this function call Well. As I‚Äôve just said I totally agree that it would be best if R used a different symbol here. But using your example code as a justification is disingenuous: Regardless of which operator you use, this is bad code and nobody should write it. You claim that using `&lt;-` for assignment helps you parse that code. Hold on a minute: are you actually claiming that f( x = z &lt;- 3, y = w &lt;- 5 ) is acceptable code to write, and that it‚Äôs less confusing than f(x = (z = 3), y = (w = 5 )) ? In fact, it‚Äôs the opposite: This code becomes vastly less confusing (though still questionable) once you add parentheses, regardless of which assignment operator you use. &gt;Thinking of argument matching as a differentl kind of assignment is a useful mental model Maybe (although I question even that: How exactly does it help?). But it‚Äôs only a useful mental model as long as it doesn‚Äôt lead to confusion. In the case of R, it patently does: It is routinely taught wrong, and it leads tons of people to think that the two assignment operators perform different operations^(1). And that, in turn, is used as an incorrect justification for why `&lt;-` assignment is allegedly technically superior to `=` assignment. ^(1) This claim is based on the fact that many R programmers, from beginners to experienced ones (with prior experience in other programming languages, including people with computer science degrees), have personally told me that they fundamentally misunderstood `=` assignment in R, after I explained it to them. I call that a very unhelpful mental model.
Hm well keep in mind that a lot of the functionality of devtools, like installing from github, requires a compiler, which is in Rtools.
Okay, the reason I installed devtools was for the possibility to install from Github. But I didn't think that devtools would make all future package installations throw a warning. But again I'm not too familiar with R or devtools.
Thank you for the additional information and referral. I can see how you come to the opinion you have; hopefully you will forgive me for understanding and perhaps agreeing with it, but still coming to the opinion that I wish the designer of R had chosen a different character for named parameter matching. For instance, C# uses `:` for precisely the same activity; I think that works well, and is much cleaner than having to use `&lt;-` in place of the typical `=` for an assignment operator.
You are quite right. I would say I am an idiot, but actually, I made the mistake of attempting to response to the comment while absolutely exhausted, and got it all backwards.
https://r4ds.had.co.nz/index.html
This is definitely possible, and using `ggplot2`, you can approach this in any number of ways. Here's an example using the two blue bars in your screenshot. library(ggplot2) df &lt;- data.frame( grp = c("READING", "WRITING"), idx = c(1, 1), idx_lab = c(1.1, 1.1), score = c(45, 39), lo = c(10, 10), hi = c(90, 60), mid = c(50, 35), stringsAsFactors = FALSE ) plt &lt;- ggplot(df, aes(x = idx, y = lo)) + geom_segment(aes(xend = idx, yend = hi), size = 8, colour = "dodgerblue") + # main segment (bar) geom_text(aes(y = lo, label = lo), hjust = 1) + # text labels for segment min value geom_text(aes(y = hi, label = hi), hjust = 0) + # text labels for segment max value geom_point(aes(y = score, x = idx_lab), shape = 25, size = 3, fill = "black") + # score, shape geom_text(aes(y = score, x = idx_lab+0.1, label = score), vjust = 1) + # score, text label geom_point(aes(y = mid), size = 7) + # midpoint shape geom_text(aes(y = mid, label = mid), colour = "white", size = rel(4), vjust = 0.5) + # midpoint text label scale_x_discrete() + facet_wrap(~grp, ncol = 2, scales = "free") + coord_flip() + theme( panel.background = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), strip.background = element_blank(), strip.text = element_text(hjust = 0, face = "bold", size = 14), ) # plot it plt In this example, I then adjust the aspect ratio of the output to get the final result close-ish to what I infer you are looking for: # save as PDF; note file specs tweaked to get alignment as close as possible tmpfile &lt;- paste0(tempfile(), ".pdf") ggsave(tmpfile, plt, height = 3.5, width = 8)
If you don't mind learning via ebook there's a great one called "r for data science" and it's free online.
Language design is a series of compromises, and everyone is a critic. `:` is also used as a sequence operator, and overloading it for argument matching would simply create a different ambiguity than the one being discussed here. I prefer to use the language to get work done and though `&lt;-` has its issues, that was the original design of the language. The use of `=` as an equivalent symbol for assignment was a rather late alteration of the syntax and of the available options I have come to terms with using `&lt;-` for assignment as the most useful to me.
I love RStudio, dunno about others. &amp;#x200B; You can learn through Datacamp/Dataquest...but the point is that you learn to play around, make things work, and understand why they work. I taught myself pretty much all the R I know through google, and a class I took that made me use it on a regular basis. I just took data I had lying around and played around with it, and now I'm pretty handy with ggplot and stats in R.
Thank you. The book is exactly what I was looking for.
You will handicap yourself if you limit yourself to only understanding `tidyverse`-style code. As much as I agree that most day-to-day data manipulation is often easier with the `tidyverse`, that is based on _non-standard evaluation_, while base R uses the more flexible and precise (if verbose) _standard evaluation_. In particular, formulas, lists, and indexing are crucial base R concepts and working with objects is based on those concepts. Also of course the vast majority of contributed analysis packages in R use standard evaluation, so avoiding them will simply make your options much narrower.
Nice! But seriously, who indents their code like this \`\`\`{r} DF1 &lt;- tibble( x = seq( -3, 5, 0.1 ) , y = A\*x\^2 + B\*x + C ) \`\`\` My brain only likes: \`\`\`{r} DF1 &lt;- tibble(x = seq(-3, 5, 0.1), y = A \* x\^2 + B\*x + c) \`\`\`
Interesting that you leave spaces with the ‚ÄúA‚Äù variable but not the ‚ÄúB‚Äù variable
I don't necessarily disagree with this, but you will get most of what you need out of base R when learning the tidyverse. Like, purrr is a huge set of functions in tidyverse that use lists, so you'll learn lists when learning purrr. There are very, very few times when I need a base R solution to something. It could be that this is because of what I'm using R for (academic research), but this has been my experience.
Yep, I like RStudio too, haven't tried the others. There's plenty of free resources for R (I've been using books by Roger Peng and Hadley Wickham). I've been playing around with some datasets from Coursera and my bank statements to learn the process. From what I've learned so far, it's good to read, but unless you actually keep applying it, you really dont internalize it.
Could you please DM me the link or post it here?
[Here.](https://r4ds.had.co.nz/index.html)
Thanks.
This isn't the nicest way to do it I'm sure but this achieves what you want. Hope this has helped! df &lt;- data.frame(Company = c("A","B","C","D","E"), Factor1 = rnorm(5), Factor2 = rnorm(5), FactorN = rnorm(5)) library(dplyr) factor.names &lt;- names(select(df, contains("Factor"))) for(i in 1:nrow(df)){ df$Factorprod[i] &lt;- prod(select(df[i,], factor.names)) }
Thanks! &amp;#x200B; That might have worked, but I ended up finding a solution using the matrixStats package and edited my post.
1) Yes RStudio is the best IDE. 2) I had great success following this article: https://paulvanderlaken.com/2017/10/18/learn-r/amp/ It starts with some base R, then gets to R for Data Science which is great.
Brilliant, thank you. I've mostly used plotly so ggplot is abit foreign to me still.
The solution to this is to install 3.5 3.6 concurrently and toggle between the required versions of R. &amp;#x200B; Details here - [https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop](https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop) &amp;#x200B; (you don't have to be using RStudio, the idea is the same)
As I say below, I shouldn't have responded to your comment while so low on sleep. May I ask how often one sees -&gt; in practice?
I've seen it once from a co-worker. Fixed it, since I expect to find the declared variables on the left, and I was going crazy looking "where does this variable come from!?". Being sincere, I've never seen it on the web or on other data scientist scripts (I've seen scripts from about 15 scientist I know in person). But hey, I love how flexible R is so I don't mind -&gt; ... Or &lt;&lt;- ... Or using = instead of &lt;- ... Or using magritr's %&lt;&gt;% ... Or reassigning `+`=function(x,y){x&lt;-y} and doing x+y to assign
Thanks for the suggestion. I've already tried R 3.5 as I mentioned in the post - the packages aren't available in it. Only 3.4 and below. When I do run 3.4, then the issue of workspaces comes up. It was working fine before the update, so I'm assuming that going back to an earlier workspace might work. Unfortunately I'm not sure how to do that.
Yes, a mixed model could handle this and lme4 is a great R package to use. I'd look into examples online, as they'd be far more helpful and thorough than I can be.
 data$Score &lt;- 0. data$Score[data$Order=="PreN1" &amp; data$Topic=="Gender'"] &lt;- (data$GenderPostN2 - data$GenderPreN1) repeat. same amount of lines, cleaner. imo.
I can't tell from your abbreviated data what calculation you want to perform. does Thk_All_V2b &lt;- spread( Thk_All_V2, Side, Thickness ) help?
I'm not quite sure what exactly you want to do but I will infer. For the first unsuccessful attempt, you're trying to concatenate entire dataframes, given by your indexing taking all rows and each column. If you want to join these dataframes per se, take a look at tidyr::spread. This can take your one race column and turn it into say 3 columns corresponding to each value you want. Holding all other columns the same. If you wanted a dataframe of just the subsetted race column then take out the comma in your index. For your second attempt try DF %&gt;% filter( race %in% c(1,2,6))
Thanks! I'll give it a try tomorrow and let you know.
Thank you for confirming my suspicion. I will have to do some more reading then =)
Maybe try facet wrap ~age_g and have x=sex? Not sure if this is exactly what you mean but give that a whirl
&gt;Ah, that totally worked, thanks! In retrospect, I feel silly for not seeing that solution, R has a way of scrambling my brain.
Thanks for reposting the "after" plot
haha whoops. I blame the reddit markdown editor. This wouldn't happen to me in emacs. edited above post ;-).
Happy to help and glad it worked!
I haven't looked into where exactly is the problem, but the function ifelse is a vectorised function, so keep in mind all the ifelses are returning a vector the length of the columns to the previous ifelses, not exactly one value. How about using a N1 and N2 columns, instead many of TopicNX variables? That's the tidy way of doing this. Look for the gather and substr functions to do this.
I disagree with that article. IMO, R is for people with statistical training, Python for people with CS training (and potentially without statistical training).
R is single threaded. Python scales much better. This is why our organisation moved from R to Python. My advice to everyone would be to use Python instead of R. AWS sagemaker and Google ML engine also have chosen not to support R.
Agree with you on this. R will not be swallowed whole until every R user becomes a python user. R is probably more used for data analysis in the softer sciences. R is more popular in insurance and the society of actuaries will be forcing thousands of people to learn some R for the next few years at least. &amp;#x200B; Tools like dplyr and ggplot are just so easy to learn it is really appealing to the average data analyst. The average R user is not building neural nets in tensorflow, or multithreading for large calculations in spark or something. I can imagine python becoming the go to tool for heavier machine learning type stuff though, but just plain data analytics seems unlikely.
\&gt; R is single threaded. You don't know what you are talking about. It is extremely easy to parallelize in R. **data.table** is now parallelized as well, there is no better ecosystem for table data. Try to analyse table with 20 million rows in pandas.
But can you run machine learning algorithms in multi threads?
Ugh, not apples to apples. I hate it when R is compared to Python, but I do understand the causes for the tendency.
Most ML libraries were parallelized in R long time ago.
Are you talking about revR packages?
Its claims like this that make think R has a future. Do you look at your resource usage when using python? I bet its using a single core unless you are using something like dask. Its pretty easy to get multi core usage in R with things like doMC. Both R and Python have interfaces to Spark and the like (which aren't Python native anyway). &amp;#x200B; Python is a better general language though. No argument there. It's better for NN as most of the code out there is in Python... for now. TF is moving to swift. Python programmers are probably better than R programmers on average and that could explain why it 'scales better', a good CS background helps for that. Where as most R programmers have a stats background.
If only there was as many statistical and forecasting packages in Python as there are in R, I'd be happy.
&gt; ‚ÄúR has issues with scalability,‚Äù Enriko Aryanto, the CTO and a co-founder of the Redwood City, Calif.-based QuanticMind, a data platform for intelligent marketing, told Dice. Here comes the stupid... &gt; ‚ÄúIt‚Äôs a single-threaded language that runs in RAM, so it‚Äôs memory-constrained, while Python has full support for multi-threading and doesn‚Äôt have memory issues. Python isn't a data science language, that's why the numpy, Scikit Learn, and Pandas libraries were developed. R wasn't multi-threaded, that's why the parallel package was developed. Regardless, neither of the languages are blazing fast. That's why anything that's compute intensive in either language calls C/C++/Fortran; e.g. TensorFlow, glmnet, etc. Where does Python store numpy arrays? The hard drive? The cloud? A Mechanical Turk? No. [It's stored in RAM, just like R.](https://ipython-books.github.io/45-understanding-the-internals-of-numpy-to-avoid-unnecessary-array-copying/) &gt; A NumPy array is basically described by metadata... The data is stored in a homogeneous and contiguous block of memory, at a particular address in system memory (Random Access Memory, or RAM). I thought we'd put this ignorance to bed, but here are in 2019 with another CTO embarrasing himself.
Great for people in ops. Not so relevant for people generating insights. Multicore processing is possible in R, and Cpp integration is easy and supports multiple cores as well via openMP. R even has pyhton integration via reticulate, and some companies really like shiny tools. So I find this discussion not very fruitful. Python is more general, sure, and that why I use it for scripts on my raspberry pi.
Completely agree. Thanks for writing it while I was holding my anger at this silly article.
One point I have heard often is that Python is better to 'productionise' models and scripts - however, I have no experience on that, so I have no opinion. Thoughts? &amp;#x200B; Each language has its quirks and is far from perfect, of course. &amp;#x200B; I think the truth is that most of the basic statistics and data science curriculum of introductory courses can easily be taught with either.
That worked! Thank you so much. I wish I had reward currency : (
Python is easy to parallelize, just as R is. Standard operations in Python are single threaded unless you either use vectorized operations (commonly found in `numpy`, `scipy`, and `scikit-learn`) or put in a little effort to use its `multiprocessing` library. So using Python for data analysis is 99% fine and fast since the heavy lifting is being done outside of actual Python (as is the case in R as well). And you can write your own low level code in Python, just like you can in R. They are both very useable for data analysis.
From an academic perspective I think R will stick around. Reviewers have more confidence in statistical methods deployed in packages from cran and bioconductor than they do python modules. This may change, but not for a while.
Hah yes this section makes no sense. I particularly don't understand why R is memory-constrained and Python isn't! (Pandas DataFrames in particular are notoriously memory inefficient).
Don't sweat it! Good luck!
&gt;data$Score &lt;- 0. data$Score\[data$Order=="PreN1" &amp; data$Topic=="Gender'"\] &lt;- (data$GenderPostN2 - data$GenderPreN1) Hmm, I appreciate the help but this doesn't appear to be working. The code runs but when I inspect my new Score variable it is still full of 0s
That would be a terrible representation of the data. A histogram with data binned by "years someone has been on file" would be the plot you're looking for.
mean(df$years &gt;= 1) mean(df$years &gt;= 2) etc.
Yes, your suggestion would be better for various reasons and I intend on providing that also. What I also want to offer is a visual that shows long the entire group stayed on file. So I‚Äôm looking for a visual that shows 100% of the records were here after a year, in year 2 we only had 66% and so on. Not unless the work week has turned my mind into mush at this point, doing a histogram and binning won‚Äôt give me that.
If you can find where the ROC values are stored (the ones that would go in the plot as additional lines) then you can add them to an existing plot using the lines function.
What about a CDF. With the x axis starting at the most years and ending at 0.
Not the most elegant but I'd go: df$new = NA df$new[(df[,2] - df[,1]) &lt;= 2] = "yes" df$new[(df[,2] - df[,1]) &gt; 2] = "no"
I would approach this as two separate problems. First call your 2D neighbor function on each point. Then filter each of those neighbors by some depth range.
Use dplyr df %&gt;% mutate(new_column = case_when(col2 - col1 &lt; 2 ~ ‚ÄúYes‚Äù, TRUE ~ ‚ÄúNo‚Äù)
For the case of two alternatives, `ifelse` is simpler than `case_when`.
I believe that this is true due to the fact that ‚Äúproductionalizing‚Äù in larger organizations means working with IT, and those people most often have never even heard of R. There are many large organizations that have R code in production, [T-Mobile being one](https://blog.revolutionanalytics.com/2018/11/t-mobile-uses-r.html). I think you just face an uphill battle most of the time so python is the path of least resistance.
Finally someone who speaks some sense.
The (subjectively) better version of ifelse is if_else
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstudio] [storing output probability distributions from a 'for' loop](https://www.reddit.com/r/RStudio/comments/bn7t71/storing_output_probability_distributions_from_a/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
case_when might work here
Thanks, I will look that up.
Would not work unfortunately, df is too large with too many vars to adjust. Thanks again though.
Got it in case anyone needed this: df %&gt;% mutate\_all(\~ifelse(.x %in% c(8, 9), NA, .x)) &amp;#x200B; Still need help with the reverse code bit : )
Subtraction operator?
This should help. &amp;#x200B; vector &lt;- sample(1:5, 100, replace = TRUE) # Generate likert-scale data, from 1 to 5 table(vector) # From the table we can see distribution \## vector \## 1 2 3 4 5 \## 16 20 24 22 18 &amp;#x200B; vector\_reversed &lt;- 6 - vector # This line reverses scale table(vector\_reversed) # Which can be seen from this table \## vector\_reversed \## 1 2 3 4 5 \## 18 22 24 20 16
Sweet. I will look at that. I got it working with the following, but now when I add up 3 variables into one scale item, any single NA for a variable equals NA for the sum of those 3. df$var &lt;- recode(df$var, "1 = 4; 2 = 3; 3 = 2; 4 = 1;")
I think it's even objectively better. I'm just not sure that it's *sufficiently* different to warrant having a new version. They said, if you have dplyr already loaded, by all means, use the better version.
That‚Äôs my secret, Captain - I always have dplyr loaded.
You just need to initialize your distributions object as a list: distributions &lt;- vector("list", length(ests)) for(i in 1:length(ests)) { distributions[[i]] &lt;- pnorm(distrange, mean.estimates[i], stdev.estimates[i]) Lists let you store anything in a single object; they elements don‚Äôt have to be the same type or length and don‚Äôt have to be a atomic (you can have a list containing vectors, and data frames, and single numbers all at once). (Sorry if formatting sucks, on mobile.)
Hell ya this, and post looks good friend cheers
I think you should be able to identify unique rows by using the duplicated or unique function, then making a new column based on the outcome of that function and give them all a unique name, for example by ordering or sorting your data based on one of the columns. I wouldn't know exactly how, but I'd suggest reading a bit here: [https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/](https://www.datanovia.com/en/lessons/identify-and-remove-duplicate-data-in-r/) and just playing around a bit.
Maybe using abs(df\[,1\]-df\[,2\]) &lt;2 in your ifelse code will do it for you.
I mean yeah but lets say I have 100 rows and 50 columns. I want to find which 5 columns are unique and use them as a key instead of 50
Could it be that you are trying install a Linux version of windows? Ensure that the package you're trying to install is for windows. "make" is typically part of the compilation subsystem on Linux - for when you want to compile packages yourself (if you want bleeding edge for example), it's also the standard way R packages are installed on Linux these days.
Time to learn how to use lists: distributions = vector("list", length(ests)) for(i in 1:length(ests)){ distributions[[i]] = pnorm(distrange, mean.estimates[i], stdev.estimates[i]) } If you use `[` on a list you will obtain another list. If you use `[[` the index has to refer to a single element but you manipulate the thing inside that list element. To extract a vector of summary information from a list you can use a for loop but `lapply`/`unlist` or `sapply` from base R or the `map_dbl` function from the`purrr` package are more convenient ways to handle them. See https://r4ds.had.co.nz for more ideas about convenient ways to handle things like this.
p3 = 3 while(p3&lt;100000){ p3 = p3 +3 }
Do you have Rtools installed?
Thank you! This gives me an object "p3" with the value of 100002. How do I get a data matrix of all values between 3 and 100002 that are dividable by 3?
I see that you have a if_else() statement below as a solution. Case_when() will work everywhere that if_else() will (such as inside a mutate_all() function. If you give us code to build a small sample dataset that looks a bit like yours, then we can help you find an elegant solution that works for you.
Threes &lt;- seq(3, 100000, by = 3)
Thank you so much. Truly, I appreciate the support. I created a new post regarding scales and NAs that sums up where I am now. I posted a link to sample data too.
x&lt;-1:5 y&lt;-6:10 m&lt;-matrix(c(x,y),nrow=2,byrow=T) rbind(m,x+y) #to make some rows dependent on others apply(m,1,unique) apply(m,2,unique)
Compiling a package requires that you have the compiling tools accessible in the environment where you are building it. It sounds like you may be working in two different environments (e.g. using devtools or OS command line). I normally setup the necessary OS environment variables in the `.Rprofile` file for use in RStudio, and I keep a batch file around that I can execute in order to run `R CMD` from the `cmd` command shell. There is also the `installr` package which I have seen mentioned, but I have never used it. For example, I have
Okay, since I actually really should be folding my laundry and doing my own work, I decided to not do that and figure your thing out. Try this: `a &lt;- sample(c(1:10), 100, replace = TRUE, prob = rep(0.1, 10))` `b &lt;- sample(LETTERS[1:4], 100, replace=TRUE, prob=c(0.1, 0.2, 0.65, 0.05) )` `c &lt;- sample(LETTERS[23:26], 100, replace = TRUE, prob = rep(0.25, 4))` `df &lt;- data.frame(a, b, c)` &amp;#x200B; `df &lt;- transform(df, ID = as.numeric(interaction(df[,1:2], drop=TRUE)))` Instead of df\[,1:2\] you can select whatever column numbers you want. I basically Googled a bit and did this, but instead of putting column names I put df\[,1:2\]. It gives every unique combination of column 1 and 2 the same ID number.
This seems like it should work thanks!! What did you google all I got was how to drop duplicates.
 x1&lt;-c(1,NA,3,4,5) x2&lt;-c(6,7,NA,NA,10) x3&lt;-c(7,NA,NA,13,15) k1&lt;-matrix(c(x1,x2,x3),nrow=5) f2&lt;-function(x){ x\[[is.na](https://is.na)(x)\]&lt;-0 return(sum(x)) } apply(k1,1,f2) #to get row sums apply(k1,2,f2) #to get column sums
"generate number based on combination of column values r"
Worked like a charm, thanks!
Cheers! I'll check it out
3 * (1:100000)
yep. version 35 i think
yeah i was considering switching to linux. Glad it is OS related. I was starting to worry about R as a platform tbh after going through all this trouble
You could also use the `dplyr::distinct()` function to trim your dataframe down to just the rows with distinct values of the selected columns. Then you could add a new column as a key and join this distinct dataframe to the non-distinct one. &amp;#x200B; distinct_df &lt;- dplyr::distinct(my_df, col1:col50) cols_to_join &lt;- names(distinct_df) distinct_df$key &lt;- seq_len(nrow(distinct_df)) my_df_w_key &lt;- inner_join(my_df, distinct_df, by = cols_to_join)
Lol +1 to u/Jordo82 but also check out this hideous thing: n &lt;- 3 result &lt;- c() while (n &lt;= 1e5) { result &lt;- c(result, n) n &lt;- n + 3 } print(result)
Or just something like: irscale &lt;- ifelse(is.na(df$ir1), 0, df$ir1) + ifelse(is.na(df$ir2), 0, df$ir2) + ...
(assuming that your thresholds are 1 (lower) and 100000 (upper), it can be done with a vector manipulation: `(1:100000)[1:100000 %% 3 == 0]` PS Be very careful with the brackets
Thanks, but my problem is that [,2] is not always larger than [,1]. I just need the different, rather than [,2] - [,1] specifically.
You are a legend, thanks so much. This code is ideal for me.
Have you heard of this thing called the Global Interpreter Lock?
The library forcats has ```fct_rev()``` for this and many other nice helper functions for factors.
Create vectors with the states in the regions data %&gt;% mutate(case_when(‚Äûregion1‚Äú ~ state %in% region1vector, ‚Äûregion2 ~ ..., ...))
Could create a little data frame with two columns state and region and then join it to your original.
I'm probably not as elegant as some others, but I would do something like: west = c(some states here) west\_filter = which(datadf$State %in% west) datadf\[west\_filter, "Region"\] = "West" repeat, etc. But certainly less elegant than HillTheBilly's solution!
You are welcome :)
You could put the subtraction formula in abs(), which will return the absolute value
I prefer to use data.table syntax, but this is the way I would handle it: library(data.table) west.region &lt;- c("Washington", "Oregon", "California", "Nevada", "Arizona", "Idaho", "Montana", "Wyoming", "Colorado", "New Mexico", "Utah") south.region &lt;- c("Texas", "Oklahoma", "Arizona", "Louisiana", "Missouri", "Alabama", "Tennessee", "Kentucky", "Georgia", "Florida", "South Carolina", "North Carolina", "Virginia", "West Virginia") midwest.region &lt;- c("Kansas", "Nebraska", "South Dakota", "North Dakota", "Minnesota", "Montana", "Indiana", "Illinois", "Indiana", "Michigan", "Wisconsin", "Ohio") northeast.region &lt;- c("Maine", "New Hampshire", "New York", "Massachusetts", "Rhode Island", "Vermont", "Pennsylvania", "New Jersey", "Connecticut", "Delaware", "Maryland", "Washington DC") datadf[region %in% west.region, region := "West"] datadf[region %in% south.region, region := "South"] datadf[region %in% midwest.region, region := "MidWest"] datadf[region %in% northeast.region, region := "NorthEast"]
Thank you!
Thank you!!
Thanks!
I am not sure if it applies to your specific problem but you could take a look at the country code package (https://github.com/vincentarelbundock/countrycode). Regions should be included there.
This creates the lookup table based on the current data: lapply(Regions, function(x) tibble(State = x))%&gt;% bind_rows(., .id = 'Region') ``` # A tibble: 49 x 2 Region State &lt;chr&gt; &lt;chr&gt; 1 west Washington 2 west Oregon 3 west California 4 west Nevada 5 west Arizona ```
I would just export it to excel and do a vlookup.
The professional organization for actuaries has made it a requirement that actuaries pass an exam in R. This will result in about 2000 exams being issued testing knowledge of R each year. Currently having some issues with viewing my vignette by calling vignette(). Hoping to upload to CRAN soon. If you have some time let me know what you think.
vignettes figured out. exciting
You should use ```dput(df)``` and provide what the final output should look like. This is the approach I would take: df%&gt;% select(starts_with('ir'))%&gt;% mutate_all(~ifelse(.x %in% c(8, 9), NA, .x))%&gt;% mutate_all(~recode(.x, "1 = 4; 2 = 3; 3 = 2; 4 = 1;")%&gt;% mutate(ir_scale = rowSums(.))
Thanks! That helps. I was going with this but was hoping there was a faster way to do it: amer is df, imm is ir amer$ir1 &lt;- recode(amer$ir01, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$ir02 &lt;- recode(amer$ir02, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$ir03 &lt;- recode(amer$ir03, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$t01 &lt;- recode(amer$t01, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$t02 &lt;- recode(amer$t02, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$t03 &lt;- recode(amer$t03, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$t04 &lt;- recode(amer$t04, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$m01 &lt;- recode(amer$m01, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$m02 &lt;- recode(amer$m02, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA") amer$m03 &lt;- recode(amer$m03, "1 = 4; 2 = 3; 3 = 2; 4 = 1; 8 = NA; 9 = NA")
The variables I need to recode are consecutive. I was hoping to plug in 2:20 somewhere to just cover all of them.
Interesting, I've started working with actuarial data, this may well be a very useful package, thanks.
`plot( DF )` You might also look into the `GGally` package.
`pairs()` is what you want.
FSA here who manages an experience studies team at a large life insurer. Glad to see more stuff like this coming out for actuaries. I'm trying to push more R and general programming usage on my team and at my company so it's nice to see more tools being developed. I think this is the way the industry should move in terms of tooling! &amp;#x200B; If I get the time I'll try to contribute to the project on github. I think getting this on CRAN would be very helpful in terms of exposure (no pun intended), and once it's more developed you could probably advertise it by writing up an article for an actuarial magazine or section newsletter or something.
Specifically the ggpairs function in that package
 [http://htmlpreview.github.io/?https://github.com/MatthewCaseres/expstudies/blob/master/vignettes/expstudies\_intro.html](http://htmlpreview.github.io/?https://github.com/MatthewCaseres/expstudies/blob/master/vignettes/expstudies_intro.html) Here is the vignette. I'll get it on CRAN soon I hope. I am going to add 2001 CSO as included data and add a section on A/E analysis. &amp;#x200B; I do believe we can run full mortality/lapse studies using input in the format required by VM-51.
That vignette is pretty cool. Here are a few thoughts I have to make this more useful: - We have policies that have been on the books for decades but we only look at the last X # of years for any given study. It would be nice to be able to specify a start duration for addExposures(). I realize you could run it, then filter on start\_int/end\_int but that is a lot of unnecessary processing to make a ton of exposure records and then just delete most of them. Plus you might run into memory/compute constraints if you have on the order of millions of policies. - Very minor but that 365.25 thing is a bit odd. I'd probably follow the calculations in the SOA paper on experience study calculation (see page 16-17). This could also be a parameter perhaps to give users options. https://www.soa.org/globalassets/assets/files/research/experience-study-calculations.pdf - I'm new enough to package development to have no idea what the C++ code does. Is there anything critical in there? For companies/SOA to use this, my guess is that they'd want it to be very transparent and keeping it in one language might help. My understanding of RCpp is that it is most useful for code that you need to be really fast, which may not be a big constraint here. Just my two cents, feel free to take or leave. Cool project, I'll promote it where I can!
R is really slow.. that's why I don't consider using it really
* You process the start dates beforehand. Suppose we start with a column raw\_start. records$start &lt;- records %&gt;% mutate(start = if\_else(raw\_start&lt;min\_date, date, raw\_start)). The idea is that the user performs a lot of the manipulation in their tool of choice (I use dplyr), and that the package only supplies actuarial functions. That way I am not building this giant thing, I am just providing a couple well implemented and documented functions. * Thanks for the resource. * I probably should add more comments to the c++. It is necessary though. Subsetting dataframes in loops is very slow in R. I tested for large transaction files and it was needed. Trust, I did not want to have to learn it. I really want to build something that is useful. You are the person I imagine as my end user., so I like having your 2 cents very much. I hope to continue receiving feedback as development continues.
I don't think it's on purpose. Some of the questions here are kind of interesting but a lot of people just seem to not know about SO.
Could you pass the way you install it? I cannot find the package on CRAN.
I have a multi-Sub of r/rlanguage, r/rstats, and r/reprogramming. If you find other good subs, I'd like to know too!
For me, learning R and using R is about 90% Google skills...
It‚Äòs indeed slightly annoying that R by default raises a warning for missing packages rather than an error. However, this can be easily fixed by setting the appropriate warning level: R -e "options(warn = 2); install.packages('sdfjsdfsdfs')"
I ran a couple of tests and saw no warnings. Thanks for the options tip though.
My favourite mistake that I like to tell students about was once, I was trying to make a dataframe to automatically record some of the results from my computational model. It was something I‚Äôd done before, so I wasn‚Äôt worried. But I‚Äôd made a mistake in one of my if loops, and basically produced a dataframe that was 100,000 rows long and 100,000 columns wide, with every cell containing the same word. It took like 5 seconds to fix but I was confused for a while before I spotted what was wrong.
Pretty much any time I use a join for anything, I screw it up and use the wrong one at first.
/r/rstudio exists too, though that's really only relevant if you're using it I suppose.
Here's a sneak peek of /r/RStudio using the [top posts](https://np.reddit.com/r/RStudio/top/?sort=top&amp;t=year) of the year! \#1: [We need to unify the R subreddits!](https://np.reddit.com/r/Rlanguage/comments/9h3hrk/we_need_to_unify_the_r_subreddits/) | [4 comments](https://np.reddit.com/r/RStudio/comments/9h3t84/we_need_to_unify_the_r_subreddits/) \#2: [Rstudio_irl](https://i.imgur.com/68Njft4.jpg) | [4 comments](https://np.reddit.com/r/RStudio/comments/8og4vb/rstudio_irl/) \#3: [I just met Hadley Wickham](https://np.reddit.com/r/RStudio/comments/b30mxm/i_just_met_hadley_wickham/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
For which use cases do you find it slow? I switched from Stata to R largely because, with the data.table package, R is significantly *faster* for data wrangling.
dplyr::filter() is not the same as built in filter(). I usually forget to attach the library or explicitly call with the scoping prefix. :-(
I‚Äôm about a month in with R and it‚Äôs nice to hear that experienced users still make mistakes. It‚Äôs all I‚Äôm making these days.
At least 3 different packages have a function called mgsub(). All of them are "multiple gsub", in that they take a vector of patterns and a vector of replacements. They behave differently, and fail with a warning, not an error, so if you're running the script as part of a pipeline you'll never know that your replacement is happening differently than expected. My mistake: knowing the above, I *still* used mgsub() in a script without specifying the package, and deployed the script to a whole lab group who have unknown R environments.
I once found a bug in one of my scripts that messed up dates very badly. Said script called data from an api and added it to an xlsx file. But because excel and fuck formatting dates in a standard I had to format the dates from excel numeric to posixct everytime I added some data. So after about 6 or 7 months running I find some odd dates and began digging. Turned out everytime the script turned the excel numeric to posixct it added one hour. So data I added two calls ago was two hours off and so on. Took me about two days to fix everything. Luckily I found it out myself. Conclusion: xlsx and datetime never again.
Sample() samples from a vector if passed a vector. Cool. So if you give a vector if length 1 you just get back the value right? No, with a scaler sample() samples from 1:n where n is the value of your single item vector. No warning. No error. Things just don't work as expected
I regularly import timestamp data as character and convert it to `POSIXct`... and forget to run `Sys.setenv( TZ="Etc/GMT+8" )` first. This results in a couple of `NA` values in the middle of the night once each year in the spring because my default local time honors daylight savings for which times between 1 and 2am at the transition don't exist. I have learned to check for `NA`s after import to catch this error. Truth is, if I regularly worked with daylight savings time data this would be a blessing... but I don't.
Using integers to refer to a factor. I usually forget that the integer reference is referring to the level, not the label. I make this mistake when the data read comes in as a factor and I forget to typecast it to a numeric.
Agreed. It would be nice if Hadley would actually make this clearer, but I think the `tidy` term appeals to people who don't read references anyway so it carries its own momentum.
While working on a larger project I was using the seq() function with a very small "by = " argument, think 1/108, but then multiplying the resulting vector by a multiple of it, like 432. This is of course identical to just making the steps 432/108 = 4 instead, but that skipped my mind at the time. The resulting vectors of these two methods are not actually identical, they just look identical when visually inspecting them, but the rounding used by specifying a "by = " argument that's very small means that what shows up as a 9 is not exactly a 9; instead it could be 8.99999... for example. If you then later do something like "[vector] %% 3 == 0" with the resulting vector, it won't return TRUE for the element that is (as far as visible) a 9. I remember pulling my hair out over that until I noticed what I had done.
I'm partial to the scatterplotmatrix() function in the "car" package, because it also puts a nice density plot of each variable on the diagonal, plus it has arguments to put simple least-squares lines and/or smooths in each scatterplot.
This is pretty tough to answer without some toy data to understand exactly what is going on here, if you can add that, I suspect the solution to this will be relatively simple.
On it!
Some small things: **In your remaining_pens function:** The only possible answers it will return are either "5 - pen_i" or "5 - pen_i - 1". Just by knowing that, it seems that you are likely using too many if and else statements. A simple alternative would be always just calculating remaining as "5 - pen_i" and then having a single "if" statement that checks whether both "ti == 2" and "tj == 1". If so, just do "remaining = remaining + 1". exact same results as far as I can tell, but a lot more compact. Something you can also experiment with is using input parameters outside of simply checking if they're certain values. Like in the remaining_pens function: Instead of an if statement at all, you could set up a small general formula that contains a term that only applies when both "ti" and "tj" are a certain value. Let's say "ti" had values 0 and 1, and "tj" still had values 1 and 2. Then you could do "5 - pen_i + (ti * tj)". Only when both "ti" and "tj" are 1 does it add 1 to the calculation. **In your diff function:** the input parameter "tj" isn't actually used in the function as far as I can tell. Also suffers from redundant code. Just always calculate: team_1_score = sum( m[1:(pen_i-1),1]) team_2_score = sum( m[1:(pen_i-1),2]) difference = team_1_score - team_2_score Then have an if statement: "if (ti == 2)". If this is true, then the sign of the difference flips, so multiply the difference by -1 and subtract the element m[pen_i, 1]. Short example. let's say pen_i = 4, Team A has scored all 3 up to this point, Team B has scored 1. The difference following the code above is 3 - 1 = 2. Then if ti == 2, this is multiplied by -1 (-1 * 2 = -2) and the 4th penalty that Team A has already taken is added as well (this is why you add m[pen_i, 1]). If they scored, it becomes -2 + 1 = -1, the difference in score from the perspective of the second team. If Team A didn't score their 4th penalty, it's -2 + 0 = -2, and nothing changed. Then just return the difference value. You could also use apply functions instead of separate sum() calls, or use ColSums (very fast as it uses Internal R functions) on a reduced matrix m. I don't have the time to go through your whole code, but from skimming the rest, you seem to overuse conditional statements in general. I'd definitely try making this a lot more compact before presenting this as an example of your ability. I do appreciate your extensive annotation though.
If I were reviewing this for an entry level analyst position, I would put you on my list to follow up with. There are a number of things I could advise on technically, but I can see you understand functions, input checking, and modularity. I can work with someone who demonstrates an understanding of core concepts.
Ok, so how are you wanting to deal with NA values in the date2 column? Without any logic to deal with that, R would essentially ignore any rows with NAs in that column, because the condition "DF1$Date_3 &gt;= DF_2$Date_2" would return NA in those cases.
Thanks for that - &amp;#x200B; \&gt; I don't have the time to go through your whole code &amp;#x200B; no that's fine, I think if you felt the need to go through it with a fine tooth comb it would probably be alright :P And if you have found a few reasonable criticisms without having to comb it then it's not needed anyway :) &amp;#x200B; I have a look at the points you've made, cheers
Good point.. I would count the NA values as today's date.
If it shows potential then that's all I can really hope for from it, so that's reassuring. Thanks for having a look, I'll try and clean it up a bit.
Ok in that case we shouldn't need to do anything as the condition will always be false anyway, unless you have values for Date_3 that are in the future. Here is my stab at your problem: library(data.table) # convert data.frames to data.tables dt1 &lt;- setDT(DF1) dt2 &lt;- setDT(DF_2) ## set up join setkey(dt1, ID) setkey(dt2, ID) # essentially a left join, dt_join should have the same number of rows as dt1 dt_join &lt;- dt2[dt1] # summarize data, group by ID so that there is only 1 row per ID dt_master &lt;- dt_join[Date_1 &lt;= Date_3 &amp; Date_3 &gt;= Date_2, ## will sum the values where date conditions are met for all instance of ID .(count_A1 = sum(Type_A1) ) , by = ID] Hopefully that works. Let me know if you run into any issues.
The three factor and the hierarchical are equivalent models. You expect to get the same fit. &amp;#x200B; Think about the second order level. You've got three variables. You are fitting one factor. If these were regular (measured, not latent) variables, you get perfect fit, because you have three covariances, and you are converting these into three loadings. You're getting out (at the structural level) exactly what your putting in. &amp;#x200B; Your structural model is saturated in both cases.
So I was right - the problem was me! I really appreciate your answer, that makes a lot of sense! I'll make sure to highlight that on the manuscript. Thank you again!
From a post sometime back on this sub: Behavior of 'sample(x)' x = c(2,4,6) sample(x) Returns all 3 enteries but shuffled x = 10 sample(x) will NOT return '10' back. It is equivalent of sample(1:10)
Its just 3NF without all the ceremony around candidate keys and whatnot. Clever branding but helpful. Excerpt from https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data: Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table.
Did you know that in the word ‚Äúlength‚Äù the n comes before the g, not the other way around? I feel like I relearn this on a daily basis
I‚Äôve only used it with a time series, but you could try the ‚Äúdygraphs‚Äù package. You could try it, not sure if it‚Äôll work though
So. Many. Mistakes.
There's something like that in the apply family. I think the behavior of apply isn't the same with margin=1 vs 2, if you get down to one row / column.
I jump between SQL and R frequently and find myself trying to write R in SQL or vice versa. The one that always gets me (only briefly however) is equal to being == as opposed to =.
UPDATE: We've released v4.0.0: [https://twitter.com/oas\_generator/status/1128214723260850177](https://twitter.com/oas_generator/status/1128214723260850177) &amp;#x200B; Please check it out and let us know if you've any feedback.
And when I am testing char columns for unique constraints in R in prep for schema creation using a CI collation, and forget that R is case sensitive.
Thanks!
This is 95% of what I needed, just needed one more step: &gt; library(tidyr) &gt; Thk_All_Ratio &lt;- spread(Thk_All_V2, Side, Thickness) &gt; Thk_All_Ratio$Ratio &lt;- (Thk_All_Ratio$B / Thk_All_Ratio$C) Thank you very much. Your help was greatly appreciated.
you could use sqldf dfFiltered &lt;- sqldf('select \* from df where df$ppid\_trialn in (select df$ppid\_trialn from df where df$YawRateChange &gt;1')) &amp;#x200B; (not checked, no R on this computer)
Filter removes the entire row of data based on the specified condition, so you can simplify your second line of code to: filter(YawRateChange &gt; 1)
Thangs for taking the time on this! I think ill be counting duplicate values if they are joined, no? I'll run to verify once I put out other fires.
Can't you just use base commands? filteredData\[filteredData$YawRateChange&lt;1,\] ? Do you want to drop just the rows with values &gt; 1 or the whole group? Because if the latter I'd do: drop &lt;- unique(filteredData\[filteredData$YawRateChange&gt;1,\]$ppid\_trialn) dfnew &lt;- filteredData\[filteredData$ppid\_trialn %in% drop,\] #here you do need dplyr though. I haven't actually tried this, but based it on this example: [https://stackoverflow.com/questions/15227887/how-can-i-subset-rows-in-a-data-frame-in-r-based-on-a-vector-of-values](https://stackoverflow.com/questions/15227887/how-can-i-subset-rows-in-a-data-frame-in-r-based-on-a-vector-of-values) .
What are you trying to get? What do you mean filter out particular trials? What do you want to do with them? Filter will drop the rows if they don't match your filter, not sure what you're trying to get out?
Try different starting values. Convergence can be really unnervingly dependent on your starting values. Try different methods/algorithms. Look at the return value. If it‚Äôs failing because it ran out of iterations, tweak the control arguments to increase the number of iterations, change the tolerances, etc. If possible, provide explicit gradient functions rather than having optimx approximate them numerically. Or change the step sizes for the finite-difference approximation. Consider scaling: does a small change in a parameter produce a big change in the outcome? Can you rescale your parameters or data so that the relationship is more proportionate?
Yeah it is over counting values. Is there a way to do this without joining tables? Regardless, this method is computationally fast.
 threshold &lt;- 1 set.seed(11) df &lt;- data.frame( grp = rep(c("A", "B", "C", "D"), each = 10), rate = runif(40, 0, 1.2), stringsAsFactors = FALSE ) grps_to_filt &lt;- unique(df[df$rate &gt; threshold, "grp", drop = TRUE]) df[!df$grp %in% grps_to_filt, , drop = FALSE]
Interactions?
What I want overall is if a particular trial has a YawRateChange value that is above a certain number, I want to filter that trial out, leaving me with only trials that YawRateChange values under that number
This filters out the YawRateChange value - what I want to do is to filter out the trial IF a value of YawRateChange is 1. Maybe I was unclear - my bad!
Ah, in that case you could use the NOT operator (!) to reverse the filtering direction from my original suggestion: filteredData &lt;- data %&gt;% group_by(ppid_trialn) %&gt;% filter(!(max(YawRateChange) &gt; 1))
I'm still not understanding ... what that you're not getting from this? What do you mean "filter just the ppid_trialn's" versus dropping the whole row if it means `YawRateChange &gt; [threshold]`? `filteredData &lt;- data %&gt;% filter(YawRateChange &gt; 1)`
You code filters out the YawRateChange value. I want to filter out the whole trial, which is denoted by the ppid\_trialn. Every row in the dataframe isn't equal to a trial; rows are equal to a frame within each trial in this case. Thus your code will just filter out a frame in my trial where YawRateChange is greater than 1 - that's not what I'm looking for...
That's what filter does. Run the code and look at the resulting dataframe. Or do that and then screenshot what's off from what you want.
This answer should be the most efficient, especially when being performed on larger data sets. Base R is pretty optimal for vector operations here to slice data. One thing to note is that in the above code filteredData[filteredData$YawRateChange&lt;1,] that you can select specified columns by name or number after the comma. Adding nothing after the comma will return all columns. ie: filteredData[filteredData$YawRateChange&lt;1,"ppid_trialn"] only returns ppid_trialn filteredData[filteredData$YawRateChange&lt;1,c("ppid_trialn","heading")] returns ppid_trialn as well as heading filteredData[filteredData$YawRateChange&lt;1,c(1,2)] will return the same as well
Yeah you're right! I would've defined a new data frame with the first line of code, because I assume OP wants to do more analyses with it, just without the trials that are above the threshold.
I don't think you understand my dataframe, and that's probably my poor explaining. When I run your code, I am left with only frames of each trial where YawRateChange is above 1 (see attached). This is not what I want.
I think this does what I want despite not quite understanding how this code works... could you explain if possible? But thank you anyway!
Oh I just did that out of habit....all day is spent working through slicing nasty data haha. Was mainly pointing out some syntax on column selection. Just making sure syntax was laid out. Just happy someone else used base R slicing methods. Do time tests on large data sets and you'll see how fricken efficient it really is!
It groups the data frame by ppid_trialn, then removes any group whose maximum YawRateChange is greater than one. The "!" operator selects groups where the defined condition (max(YawRateChange) &gt; 1) is not true.
I see, perfect! Thank you so much for your help!
A user called Jacket24 at the top of this post has given an answer than seems to work for me if you're interested! But thank you for your help anyway!
Ah, got it now. So yea, using \`group\_by()\` firs then tells \`max()\` that you're looking for the max within the group, not within a row or the whole dataframe. Nice.
Oh sure, no harm done! I didn't read your first comment as critique or anything. ;) I'm kinda curious why OP seems to ignore this solution, but whatever. Honestly, I haven't worked in R very much for very long so I'm just getting the hang of it. And only just the last few years on my own datasets, which I think are pretty large, but I'm an ecologist so a sample size of 25 is quite good, and anything over a 100 is large. :p I did learn R ~10 years ago as an undergrad and we weren't allowed to use RStudio then, so while I know dplyr etc. exists, I'm still more drawn to the base functions if they're there.
Hey sorry! Tried the idea in your original comment. You're right, I want to drop a whole trial if the YawRateChange is above a certain value. However I'm not sure your suggestion works for what I want... if you seem the comment at the top of this post, that seems to work. Is yours doing something similar because I'm not getting similar dataframes out...
Sorry, whipped this up while at work. Let me know if you have any questions. You could also always try ifelse statements to create a couple of flags or do a single nested ifelse(this is a vectorized function). Id match, date above, date below, and master flag for all 3. df2$masterflag=ifelse(df1$id==df2$id,ifelse(df2$date1&gt;df1$date3, ifelse(df2$date2&lt;df1$date3,1,0),0),0) Then df2$count=ave(df2$masterflag, df2$id, FUN=function(x){sum(x)}). Then df=df[!duplicated(df$id),] And left join, df1,df2. Or df1$Type_A=ifelse(df1$id=df2$id,df2$count,0) Also, you need to put in a dummy variable where no date exists as shown in your sample data. df$date=ifelse(is.na(df$date),dummyvar,df$date) Should run in a few seconds. Sloppy pseudocode ftw!
Ahh, my last job was ~100million rows of data across different asset classes. Think minute data on equities, crypto, commodities,etc. Base functions are extremely efficient at what they do. If it works it is most likely the most efficient function.
To make sure you're doing this. The syntax is: filtered=data[data$YawRateChange&lt;1,] Was this what you used? That will slice only data meeting that criteria into a new data frame.
&gt;`drop &lt;- unique(filteredData[filteredData$YawRateChange&gt;1,]$ppid_trialn)` &gt; &gt;`dfnew &lt;- filteredData[filteredData$ppid_trialn %in% drop,]` Sorry, I was using this code so that I drop a group. However it seems to remove far too many trials... The code below seems to work for me. Is your suggestion doing something similar? `filteredData &lt;- data %&gt;%` `group_by(ppid_trialn) %&gt;%` `filter(!(max(YawRateChange) &gt; 1))`
1. code looks like this: control\_mod &lt;- lm(y \~ x1 +x2) inter\_mod &lt;- lm(y \~ x1 \* x2) this includes x1+ x2 + x1\* x2 2. see if the interaction is significant 3. sim\_slopes(inter\_mod, pred = x1, modx = x2, jnplot = TRUE) and probe\_interaction(inter\_mod, pred = x1, modx = x2, interval = T, jnplot = T, control.fdr = T)
You shouldn't name things **c** in R since it's one of the most commonly used base functions. `sweep(A, 2, colMeans(A), '-')` R is a column major language, otherwise you could just subtract directly and it would recycle the shorter vector. What you can do instead is: `A - rep(colMeans (A), each = nrow(A))` Or you could take advantage of the recycling and do this: `t(t(A) - colMeans (A))` See, there's lots of ways to do it in base R, though the first I posted will almost certainly be preferred.
A general solution pattern apply(A, 2, function(x) {x-mean(x)}) You can usually just put the for loop logic in the function part. But honestly your method probably performs as well using apply.
 sapply(listofmatrices,function(x){eigen(x,only.values=TRUE)$values}) You could substitute sapply for lapply if you want to preserve the list structure.
Or better... ```library(parallel) ```mclapply(...)
Or better: ``` library(parallel) mclapply(...) ```
Have to agree with that - using `for` loops in R is very bad performance wise (I mean, in OPs case it wouldn't really matter but if you need to handle a large dataset). From my experience, using `apply` is quicker and consumes less memory. I always suggest people to stay away from `for` loops , unless they are absolutely unavoidable.
Having some issues getting this running on my Manjaro box at home: Installation seems to work and I can connect to the server, but when I attempt to start Rstudio or VSCode, I get the error "Rstudio Initialization Error Unable to connect to service" or "Error Status Code: 500 Message: Container did not respond in time". Any ideas what I might be doing wrong?
You're doing the opposite in both examples you give. I'm sorry, that's my fault because initially I wanted to drop whatever was in the vector "drop" but then went for subsetting. If you change &gt; for &lt; in my example it should work.
I'm honestly already lost by half of the words you use to describe your data. :p I really like that about R though, it's very powerful for so many different kinds of datasets! I actually still have a completely different dataset that I'm not sure how to proceed with. It's a side project but I just never worked with data in that structure before. I might ask about it here some time... It's basically repeated measurements for each ID with different setting each time, and then that repeated again (i.e. each sample is measured 100 times at values 1, 2, 3 etc.).
`scale(A, scale = FALSE)`
Sorry I didn't see your message! Just had a play around with this, thank you!
No worries, thank you for your help!
I have do use dplyr::recode quite often (sometimes I use case\_when) on a column in a dataframe . And I some how always forget that dplyr::recode takes vectors and I have to use mutate instead of piping straight into it. &amp;#x200B; I am confusing the workflow between dplyr::recode and dplyr::rename (dplyr) functions.
Could you tell us what is your R (not Rstudio!) version?
When you restart R, do you load any packages? You could also try uninstalling dplyr completely a d reinstalling it. Another option is to try to install those other packages individually that you get messages for
oh sorry completely slipped my mind that R version would be separate! I checked and it was version.string R version 3.3.2 (2016-10-31) I guess I will update my R and try again!
I did not manually load any packages, these seem to automatically load for some reason. Though the problem seems to have been resolved thanks to devnullumaes, looks like my base R version was old lol *facepalm*
Thank you for the suggestion to check my base R version. The problem has been resolved. This will be one rookie error I won't make ever again lol, thank you for the lesson.
Your loop is only slightly slower than the ```sweep``` solution. Loops aren't necessarily bad. The best looking solution - ```apply```, is the slowest. Here's a microbenchmark of 1,000,000 rows with 10 columns. Unit: milliseconds expr min lq mean median uq max neval f_sweep() 120.4074 126.6669 153.6262 143.9103 169.4261 228.2519 20 f_apply() 239.9807 249.1791 285.2853 290.8615 317.1144 328.6278 20 f_loop() 145.5745 165.1811 193.4595 171.6225 240.1194 268.4426 20 library(microbenchmark) sims &lt;- 1000000 cols &lt;- 10 A_Big &lt;- matrix(rnorm(sims * cols), ncol = cols) f_sweep &lt;- function(){ sweep(A_Big, 2, colMeans(A_Big), '-') } f_apply &lt;- function() { apply(A_Big, 2, function(x) {x-mean(x)}) } f_loop &lt;- function () { c_mean = colMeans(A_Big) for(i in 1:ncol(A_Big)){ A_Big[,i] = A_Big[,i] - c_mean[i] } A_Big } microbenchmark(f_sweep(), f_apply(), f_loop(), times = 20)
Glad you found the solution. I'll make special note of this in the setup doc.
I don't know your workflow with leaflet, but here ([http://ifnapp.creaf.cat/nfi\_app/](http://ifnapp.creaf.cat/nfi_app/)) if you select "parcel¬∑les" and "apicar" you will see that I'm plotting between 5000 and 13000 points, depending on starting data (\~5100 for IFN4, but \~11000 for IFN3) and is really smooth (is only slow when drawing the points, not in zooming or panning), even when I'm also loading polygons and baseTiles. In my case, the trick was converting the points data frame in an sf object ([https://github.com/r-spatial/sf](https://github.com/r-spatial/sf)) as it is really faster than using a normal dataframe
I don't know why that specific instance would freeze RStudio, but I've had issues with ggplot freezing RStudio at randomish times. I updated everything and the problem was fixed.
You can color the vertical lines by specifying `color =` outside of `aes()` in `geom_vline()`. I can't figure out how to display a legend for something that is not used in an `aes()`. I have a work around using `geom_label()`, but maybe someone else has a better answer. I'd love to know it! ggplot(data = mtcars, aes(x = disp, y = mpg, color = factor(vs))) + geom_line(show.legend = FALSE) + geom_vline(aes(xintercept = mean(disp)), color = "red") + geom_vline(aes(xintercept = median(disp)), color = "blue") + geom_label(aes(mean(disp), 4, label = "mean"), show.legend = FALSE) + geom_label(aes(median(disp), 6, label = "median"), show.legend = FALSE) + facet_wrap(~ factor(am))
Interesting, cheers for letting me know!
What if you put the x-coordinates of the median and the mean in one data frame, with a second column indicating which is which (so 1 for mean, 2 for median, or any two different number, doesn't really matter). If you do this, you can use the "group = " argument within aes() within geom_vline. I believe that would color the lines differently and provide a legend.
Feel free to drop me a message anytime if you need help. From the sounds of it you just want everything in a single data frame with a column as an identifier for each iteration (values 1,2,3,etc). Perform vector functions on them. If you're doing anything in an "on" "by" approach (ie. sum of y, by id=x), look into ave() and it's infinite possibilities due to defining the function yourself. FUN=function(x){f(x)} where you repalce f(x) with wanted function (ie count(x), sum(x), nrow(x),rank(x,ties.method=""))
https://www.rplumber.io/ is probably the goto for this type of stuff.
I've briefly read up on it, but does it handle storing data as well?
No, I think you handle that part :)
&gt; Your loop is only slightly slower than thanks, I should have been clear that i wasn't really thinking about speed here as much as aesthetics, or the logic perhaps. In that I got the feeling I was using a lazy pattern or something. I don't really know all the proper words to address this, a "code smell" ? thanks for the examples though
Here's a workaround for the time being. data$Score=0 data$Score=as.numeric(ifelse((data$Order == "PreN1" &amp; data$Topic == "Gender"), data$GenderPostN2-data$GenderPreN1, data$Score)) repeat for all scenarios at end data$Score=ifelse(data$Score=0, "Error", data$Score)
I might! Thanks for offering! I'll look into your suggestions. :) Right now I'm wrapping up a paper though, so I don't know when I'd have time to look into this.
Depends what you're deploying. Eg, Packrat + Docker.
I think the choice comes down to the number of users- plumbr might not be able to handle high volume of requests. You might look at deploying shiny as an alternative
It's not much I don't think. It'd be about 1,000 daily requests.
Sincere thanks.
Welldone! Do you prefer to publish at datascienceplus.com and consequently to R-Bloggers?
This is an improper way to test the relative speed of things. R has a JIT (just-in-time) compiler which kicks in after the first time you run a function, so if you want to really test the performance or different bits of code, you need to take that into account. &amp;#x200B; Also, wrapping things into a function has the dual side-effect of adding function call overhead while forcing compilation. &amp;#x200B; You can see that in the attached document where, with compilation turned on even for OP's tiny data, the bare for loop takes over 2100 microseconds on average to sweep through the data. That's because with the default JIT level of 3, all top level loops will be compiled before running. So, when we benchmark it, it needs to include the compile time, hence the bare for loop is faster with the JIT compiler turned off. On the other hand, when we put the bare for loop inside of a function call, `forsweep` R compiles it after the first time it is ran rather than every time it is called, so the function version gets results closer to to the uncompiled times because it only suffers the hit for needing to compile once. [Some benchmark results for various sweep functions.](https://drive.google.com/file/d/1BQb5XrdIBGx_rqHniL0vBkXzJkurEzkN/view?usp=sharing)
nice ill try that out
MySQL... Lol
Simple base R syntax: your_data$anemic &lt;- ifelse(your_data$hemoglobin &lt; 10, "anemic", "non-anemic")
thank you! It seems to have taken but it doesn't appear on my dataframe. Is there any way I can make it do so?
Also I'm trying to create a linear model subsequently relating anemic to age or something along those lines, but that's not working.
It means that at least one of your values in column y is not a usable value. To resolve the error, remove the value(s).
Because MariaDB is newer, or why?
I will check out datascienceplus!
Thanks!
Also [BBC](https://blog.revolutionanalytics.com/2019/02/bbc-r-cookbook.html)
too busy to fully reply, but ifelse() will most likely give you the best &amp; fastest solution https://www.datamentor.io/r-programming/ifelse-function/
Thanks, but ifelse() doesn't really apply here sadly!
What data do you need parsing and why do you need it to be done in R?
Could also just create pseudo variable and a flag. This might work better. paste(col1,col2) ifelse(var1=paste(col3,col4),1,0) #create a flag count=ave(flag,var1,FUN=function(x){sum(x)}) This way you can return a count of all fruit_togethers, and if you want to simplify do a df=df[!duplicated(df$fruit_together),]
Thank you, I do really appreciate the help. But I am not sure if I explained it right. In fruit columns 1 and 2 there are numbers in each row, which I want to correspond to columns of fruitnames. For every row in fruit (lets say row 1 is "3" (column 1) "4" (column 2), I want it to read the corresponding columns (3 and 4) in fruitnames together (which might give "apple orange" "apple banana" "banana pear"... where column 3 is apple,apple,banana... and column 4 is orange, banana,pear...). Then count the number of a combination I specify e.g. "apple banana" which in this scenario is "1". I will extend this (manually) so there will be a new column for every combination of two fruits, which is order specific (e.g. "orange banana" and "banana orange" will give different counts. For each row in columns 1 and 2 in fruit there will be different numbers, corresponding to different columns in fruitnames.
That explains a lot better. I'll work on something in a few minutes after I send out some e-mails....memos that wont matter anyways woohoo.
The part before the : is telling you where the error was, the part after the : is telling you what the error was. The error is that y contains NA, NaN, or Inf. These are values and the function doesn't support those values. You need to get rid of them. What is y? That's explained in the first half, before the :.
I think you could actually be able to use my previous solution but just take it a step further and make some edits. Also, please correct me, but it would seem to be a bit redundant to have number IDs and fruit names in the same data frame. ie(keys in a database youd have an entity with ids corresponding to names), and the other has just numbers and can reference the entity with keys. This is similar to setting up data for best functionality in R where you may want a reference data frame of ID to Fruit mappings(quicker to reference if looping anything than using unique etc). My thoughts, for efficiency, are to: First create a data frame with mappings for id to fruit if simple enough. Second loop through mappings to create a combinations dataframe. for(i in 1:nrow()){ for(j in 1:nrow()){ df$col1=paste0(i,j) #from reference df df$col2=paste0(fruit1," ",fruit2) #from reference df using i,j } } Third, add combined ids in df with data, id_comb=paste0(id1,id2) Create columns looping through combinations df to create columns in dataset using ifelse() to create a flag, then ifelse() to sum and leave zeros when not id_comb. And rename to reference id_comb name. This is best in practice data organization. Also doesn't require manually extending and any data can be pulled by reference your mapping dataframe to pull columns etc.
Wow... That is so much better than the shit code I've been writing. I will give it a go but I am no expert and got a bit lost near the end haha. Truly, I appreciate this. Just getting to grips with R and it has been a steep learning curve
Just pasting another description of my problem, I'm still unsure whether I explained it right: In fruitcols dataframe columns 1 and 2 there are (randomly generated) numbers, which I want to refer to the column number in another dataframe of randomised fruit names (df called 'fruit'). There are 5 types of fruit, and for each combination (where order counts i.e. "banana apple" is not the same as "apple banana") I want a new column in fruitcols with the count of co-occurence in the two columns of 'fruit' as specified by entries in cols1&amp;2 of 'fruitcols'. If in row 1 of dataframe 'fruitcols', col1 and col2 are "3" and "4" respectively I'd like it to read columns 3 and 4 in dataframe 'fruit', but with rows together i.e. column 3 may be apples, bananas, pears (in descending order) and column 4 may be pears, bananas, oranges, but would be read "pears apples" "bananas bananas" "pears oranges". So in a new column called "pears oranges" for this row would specify "1". But if in row 2 dataframe 'fruitcols' the entries for cols 1 and 2 are 6 and 7, it would do the same for columns 6 and 7 of dataframe 'fruit'.
No problem! It's definitely a steeper curve to start when you're trying to understand syntax and optimization around vectorized processes. Once you start thinking along that thought line and optimization for layout going forward it picks up exponentially. Also remember loops aren't always slower compared to a vector process (ie dplyr). The only time to avoid loops is when wanting to do things like rbind continually in a loop(many rows) where you're calling a dataframe, concatenating, and then calling the same dataframe and concatenating again. This would be very overhead intensive &amp; in short, suck dick to deal with.
I think I understand and the above solution should work. When I'm off work in a couple of hours(pending i dont go to some social event since my boss is in town) I'll do a solution up in R and paste it and my results.
OK, I think I just don't know exactly what 'keys', 'entities', 'mappings' etc are! I was just going to create columns 'freq_apples_apples', 'freq_apples_oranges' for all combinations of the five fruits in the 'fruitcols' dataframe, each with it's own specific code. It would still be useful for the columns with counts to be within the 'fruitcols' dataframe, so I can reference the cols1&amp;2 with new columns (freq_apples_apples etc...) in future steps.
By the way, are you a real actuary? That is damn cool.
Are you sure you want `renderTable()` for this type of output? This is only a character string and usually you use `renderTable()` on data.frames. Maybe you want to use `renderPrint()` or `renderText()`?
Yessir/Yes'm! I am. Before this I worked at a fintech startup. That's where I really polished my R and Python skillset. If you ever want to see some shitty research I did look at the whitepapers for THETIE.io (The data came from my company I worked for).
Oh those were references to data management in a database setting. Just a logic I like to follow when working with datasets for clarity on my own thought process for solutions. Leaving work soon and I'll do up a quick solution once home for ya.
Wow, must've taken some training! Do you often publish in your job?
Ah OK, I will have to get clued up on these phrases eventually... Where is best to learn R? Teaching myself, making up problems and reverse-engineering code works for me but it is slow. Thank you so much!!
That was a lot. Thanks. I don't fully follow but with 1 million rows, wouldn't some of the potential bad side effects of the function call be insignificant? I mainly did it as a function to make the summary simpler. And for people in base R, sure sweep might be great. But the apply solution seems more intuitive and even the loop really isn't that bad on the eyes.
What's the proper way to time? I like having the function call because it simplifies the summary. Would turning off the compiler be the way to do it?
&gt;What's the proper way to time? I like having the function call because it simplifies the summary. Would turning off the compiler be the way to do it? I believe, even with enableJIT(0) R would still compile functions after the first call. If you are writing a function, then, of course it should be a function call, but if you're doing something which will live outside of a function, forever and always, you're better off timing it as is to get a true measure of it's computational cost. Again, at least from my understanding.
Rstudio's automatic restart sometimes doesn't work. I recommend using session -&gt; restart R from the menu instead.
I will try it this afternoon and report back. Thank you!
Sorry got super busy, should have time today!
In my last job I had a lot of pseudo-publishes work. Most data sided at my current job is internal. To take anything to state boards and other organizations it needs to be simplified down quite a bit for approval. A quick example is in the P&amp;C industry, for self-driving cars there's not a sufficient time series of data to use standard methods to create all assumptions for efficient pricing. This is where boosting machines come in like a gradient boosting machine (GBM) to tinker with the decision tree and pull assumptions back to a more broad generalized linear model(GLM). In short, everything tends to be internal or for my own personal projects currently (sold a mobile game ~ 2 weeks ago that I coded with a friend)
If finance data is outside of your realm or you have zero interest at all. https://www.kaggle.com/datasets I'd suggest sports data like below if anything since it's easily understandable. https://www.kaggle.com/karangadiya/fifa19
Thank you, let me know if you crack it! I've tried chipping away at it today but with no luck. Annoying as it seems relatively simple... Time series, I've done some stuff with time series before, not found it too terrible. I'll need to find out what those business-y words mean first, I'm half tempted to work with climate datasets, they seem pretty damn interesting too and relevant given climate change. Does colClasses = "character" convert all column entries into characters?
Thanks, I think I'll use these on my next R challenge. The second one looks most promising!
Pseudo-publish as in publishing under a different name? :P wow, a mobile game!! Good money?
I assume it has to do with how Excel saves csv format. Instead of opening in Excel why don't you just remove the point in r? Or in a text editor like notepad where it won't try to be clever about headers.
You could do something like: names(df)[names(df)==`"Load (lb)"`] &lt;- `Load (lb)` for the columns that get changed. If the structure doesn't change, you can just feed `names` a vector of the desired column names.
I did go through and do it in a simple text editor to get it to work, but removing the data point in r is not an option, as it is part of a much larger comparative study. None of the dozen or so files previously had thrown a fit at me about some small jitter. The particular data point is unimportant, but modifying the script for this single file would be messy, particularly since I've used a for loop.
This is certainly a solution. I fixed my issues using a text editor to modify the file instead, but just changing all the column names for every new analysis may be the safest option.
Are you using the `readr` package to import the data and saving it as a `tibble`?
Nope. I was just modifying an older file of mine where I just used read_csv.
`read_csv` is a member function of the `readr` package. You're loading it somewhere. It returns a `tibble`, which can support exotic characters in column names. You can check by running `typeof(YourDataObjectName)`. Although tibbles can handle exotic characters, such as spaces, in column names, the best approach is to use column names that a standard R `data.frame` can handle, such as letters, numbers and the underscore character. However, I think you also have an additional problem. By default, when Excel exports a CSV, it automatically *double quotes* all strings. Then, when you import the CSV into R, `read_csv()` keeps your filenames as is, including with the double quotes. You can disable this behavior in Excel when you export the CSV by unchecking the appropriate dialog box.
You can tell the `readr::read_csv()` function to not use the header row of the csv but instead to use your own column names since you're expecting all of the csv files to be of the same structure and column names in your for loop. I would have guessed that the csv's with the column names in the 2nd example are due to maybe some extra whitespace in the raw csv files?
You can tell the `readr::read_csv()` function to not use the header row of the csv but instead to use your own column names since you're expecting all of the csv files to be of the same structure and column names in your for loop. I would have guessed that the csv's with the column names in the 2nd example are due to maybe some extra whitespace in the raw csv files?
You can tell the `readr::read_csv()` function to not use the header row of the csv but instead to use your own column names since you're expecting all of the csv files to be of the same structure and column names in your for loop. I would have guessed that the csv's with the column names in the 2nd example are due to maybe some extra whitespace in the raw csv files?
Do it in a separate R script then.
What have you tried &amp; what part are you stuck at?
This sounds like an assignment question. What do you need help with?
Hey, thanks for replying I am super stuck. &amp;#x200B; I am unsure how to get the or\_glm working &amp;#x200B; I know the format is \*\*or\_glm(data, model, incr, CI = 0.95)\*\* &amp;#x200B; I guess I am having issues knowing what to plug in as I am unfamiliar with the function for the data I can put my info which is under "loan". &amp;#x200B; For the incr: list(Nowork=10,GrAppv=1000000) &amp;#x200B; I am not sure what to put for model though, like I do not know what is normally under that variable, do you know anything about this?
Yeah, it is! I just responded to u/mtl_economics but essentially I have never successfully run one of these and I am struggling to figure out what to put for the model variable of the or\_glm
Yeah, it is! I just responded to [u/mtl\_economics](https://www.reddit.com/u/mtl_economics) but essentially I have never successfully run one of these and I am struggling to figure out what to put for the model variable of the or\_glm
&gt;I am not sure what to put for model though &amp;#x200B; Just your fitted logit GLM of the form response\~predictor1+predictor2+predictor3 See [here](https://www.rdocumentation.org/packages/oddsratio/versions/1.0.3/topics/or_glm).
Do you mean you cant use RStudio anymore? R and Rstudio are not the same. If you have R installed you can use it without RStudio.
I need RStudio because my workflow has RStudio in it.
I am mystified by your complaint. Fortunately I am just another reddit crawler, but this makes no sense since `R` is a different tool than RStudio... they don't link to each other, so should be able to use different versions of any linked library. Even if you cannot untangle them in your configuration then there are still other ways to work with R than through RStudio.
Are you using a release version or an experimental version? What operating system? What version of R?
try this: library(dplyr) newdf &lt;- left_join(ucpairs, upairs, by = "common_column_name")
The following reprex works fine for me: x &lt;- data.frame(a = 1:20, b = rep('b', 20)) # 20 rows y &lt;- data.frame(a = 1:30, c = rep('c', 30)) # 30 rows merge(x, y, all.x = TRUE) # merges on common column 'a', returns 20 rows of 3 columns This might be more of a data quality issue than a programming one. Do your merge columns contain the data you expect? Are you also sure that the data frame you're passing as `x` is, in fact, the one with 5,330 rows, and *not* 12,475 instead?
You say that but someone at work was writing scripts that relied on the rstudioapi package. The idea that code relies on a specific IDE being installed is idiotic.
for some reason this does not work. Super weird. Any idea? my data looks like this head(ucpairs) acq\_tar inFlow freq &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; 1 AD2015\_ES2015 20775 1 2 AE2008\_AM2008 20000 1 3 AE2008\_AT2008 25 1 4 AE2008\_EG2008 393 1 5 AE2008\_GB2008 68752 2 6 AE2008\_GE2008 80 1 &amp;#x200B; head(upairs) acq\_tar id\_a id\_t iso2\_a iso2\_t nDealsIn value date\_a date\_c 1 AD2015\_ES2015 AD2015 ES2015 AD ES 1 20775 2013 2015 2 AE2008\_AM2008 AE2008 AM2008 AE AM 1 20000 2008 2008 3 AE2008\_AT2008 AE2008 AT2008 AE AT 1 25 2008 2008 4 AE2008\_EG2008 AE2008 EG2008 AE EG 1 393 2008 2008 5 AE2008\_GB2008 AE2008 GB2008 AE GB 2 8896 2008 2008 6 AE2008\_GB2008 AE2008 GB2008 AE GB 2 59856 2008 2008
see my reply above
what do you mean it "does not work"
newdf &lt;‚Äì merge( x = ucpairs, y = upairs, all.x = True, by = ("common column name")) &amp;#x200B; &amp;#x200B; that should work
Sorry. It ends up with a dataframe with 12,475 entries not 5330
Are there duplicate acq_tar values in the upairs dataframe?
So weird, I still end up with 12,475 entries
yes
then there's no way to join these dataframes together without increasing the number of rows. You'll first have to rectify the duplicates somehow If I have the following: df1 &lt;- data.frame(id = c("a", "b", "c"), values = 1:3) df2 &lt;- data.frame(id = c("a", "a", "b", "b", "c", c", values = 1:6) there's no way to merge these together by 'id' and only get three rows unless I'm willing to drop some rows from df2
that makes a lot of sense. Thank you for your help!
The latest release of RStudio v1.2.1335 (see here https://github.com/rstudio/rstudio/releases) isn't compatible with the latest release of boost 1.70.0 (see here https://www.boost.org/) Operating system FreeBSD, R version 3.6.0
No, people run scientific R scripts from RStudio, and see and save the graphs from RStudio. It is much more convenient to do this kind of work in IDE. Should we begin to migrate to something else just because RStudio devs didn't get around to update for the latest boost?
GRANT_log = log(GRANT)
I'm not sure what your issue is, you can do all that without an IDE, or any of the other perfectly capable IDEs out there. Just because RStudio broke their latest build doesn't mean you can't use R.
gplots::heatmap.2 allows you to specify dendrogram ordering via the "Rowv" argument
What does it mean to you when you say "get R to read flow1 as the direction from import2 --&gt; import1"? The fact that *you* know how to interpret the meaning of flow1 and flow2 seems like enough. Could you provide more info? If you wanted to be explicit, maybe to explain to someone unfamiliar with the data, you could create a string that reads "`import1` imports `flow1` from `import2`". data$flow1_str &lt;- paste(data$import1, "imports", data$flow1, "from", data$import2) data$flow2_str &lt;- paste(data$import2, "imports", data$flow2, "from", data$import1)
why do you want to log both variables? are you going for a log-log model? otherwise the first post is the answer to your question.
Don‚Äôt forget to add a small number inside of your log function to handle zeros.
Yes, it's possible in other environments, but it is massively inconvenient if you're used to developing in an environment and you suddenly can't use it. What, I have to learn how to use emacs now? Do I know, off the top of my head, how to spit out an Rmd template to start a new R Markdown file? Should I *have to* know? Should I know how to compile, from the command line, an Rmd document to pdf? Still, though, this is a good "opportunity" (the old acronym AFLO: another 'fun' learning opportunity) for the OP to refactor things so that analysis scripts can be run from the command line, `ggsave()` in the script can be used to save images if they're using `ggplot2`, etc so their workflow is more reproducible and less dependent on human interaction. Or, as alluded to above, to turn things into documents that they can run and get the results from.
Are older versions compatible? You might be able to use a downgrade
I want to expand @2strokes4lyfes comment. I think the proper formula should look something like this. df$grant\_log &lt;- log(grant + 0.00101). The 0.00101 eliminates the chance of any zeros. (unless the number, -0.00899 is in your data set. This will help.
Or if it's a dataFrame: df$GRANT_log &lt;- log(df$GRANT) Or you can use the = instead of the &lt;- , be yourself
A proper heatmap would be some sort of rectangular grid, which you could potentially achieve by binning the x &amp; y values. How to do this depends on what the values are to begin with. You can either multiply or divide by some number to get the number of bins that you want in each direction, then floor those numbers to remove the precision.
I think [geom_density_2d](https://ggplot2.tidyverse.org/reference/geom_density_2d.html) may be what you're looking for.
Only certain row orderings will be consistent with certain dendrograms. Plotting a dendrogram where nodes on the same branch are at opposite ends of the ordering would have to involve a lot of mess and probably wouldn't be an interpretable picture. Do you perhaps just want to plot a heatmap? There are some packages, like seriation, that attempt to find the best ordering of nodes based on a given dendrogram. You could reorder to be as close to your optimal order as possible, given a dendrogram, but you can't really choose any order you want and then do a dendrogram that fits it.
You‚Äôre looking for a hexbin plot. [sauce](https://ggplot2.tidyverse.org/reference/geom_hex.html)
Or simply alpha=.3 in geom_point.
Alternatively just playing around with ``
Firstly, make sure that they are all positive (the log of zero to any base is minus infinity, and not defined for negative numbers), otherwise you are at risk of getting `-Inf`s and `NA`s. &amp;#x200B; Also, it is a good idea to alter the column names to reflect the fact that the data has been transformed. A shortcut is `colnames(df_log) &lt;- paste('log',colnames(df),sep='_')` where `df` is the original dataframe and `log_df` is the log-transformed dataframe (`log_df &lt;- log(df)`). &amp;#x200B; Finally, remember to similarly transform any other data that you feed in for testing, so that you are always comparing like with like. Consistency within the dataset is the key.
Why not both semilog and log-log models? The additional computational cost is pretty low if done at the same time, although it is important to make sure that the output `y_hat` is in the same form as the observed variable that it is being compared to.
Yes I am. I'm planning to regress both with SOCIALSPENDING as DV. How should I log SOCIALSPENDING? Should I log it similarly as I would log GRANT as others have demonstrated?
Are all the values in the common column unique?
I already played with the sizes of the dots, but using ‚Äò‚Äôalpha‚Äô‚Äô to down-weight the shots that are less importants could be really interesting and I didn‚Äôt think of that thanks !
Thats a very good solution and would totally solve my problems ! Thanks a lot !
That was exactly what I was looking for, thanks a lot for that !
Is it possible for you to increase the size of the intervals of size of the X and Y coordinates?. You could try increasing the resolution of the image
No, just turn down the alpha of all the shots. Then areas with more shots will show as more intense colour.
use jupyter as your IDE
yes but if you don't know what the best model form is, you could even try out lin-log, log-lin, log-log and see what's best, though i'm not sure why you would want to have that? doesn't seem like a probit/logit would make much sense here.. your dependent variable y is not binary response. i'd suggest you to try out a normal linear regression and lin-log before trying out more. and in general analyse your data's distribution and not just randomly picking a model.
i don't get your point, could you elaborate a bit more? ofc you could do two models but in the end compare adj. R^2 or information criteria and pick the better one right?
Once you have the log-transformed variables in one dataframe and the non-transformed in another, it is simply a matter of either making a third dataframe with transformed predictors and non-transformed output or passing the transformed input along with the non-transformed output to make the semilog model. As for the comparison criterion, that depends on whether the model is intended to be descriptive ($R^2$) or predictive (F score). Also check the distribution of the predicted variable and/or the residual, that the residual (actual dependent variable - predicted) from the model output itself is normally distributed.
Here's one I've heard good things about: https://www.tidytextmining.com/
Thanks.
I did a normal regression and my professor suggested to log both variables and run the estimates again. Can you enlighten me what it does?
Start writing your code in R that you would have written in Python.
Check this Python vs R cheat sheet: http://mathesaurus.sourceforge.net/matlab-python-xref.pdf
Dplyr Package! Know this syntax and you will make the transition really fast. Basically SQL for R, with so much more.
I find translating files I have already made and know will work well in the target language is the best way to learn.
Thanks !
R is a functional language, and you may feel frustrated trying to apply techniques you know from Python in R. [*The R Inferno*](https://www.burns-stat.com/documents/books/the-r-inferno/) should help you learn the philosophy of the language, once you have learned the basics.
If your goal is to use R for stats, I would recommend going through [R For Data Science](https://r4ds.had.co.nz/). It'll teach you R from within the tidyverse environment which would be, in my opinion, more beneficial to you in the long run. It's a fairly easy read to blitz through (1-2 weeks) but it sets a good foundation IMO.
I recommend learning the tidyverse. You don't need it to learn R, but it is much more consistent for the most common tasks, but it'll be fairly different from most of your python code. There's also a lot of growing support for it, which you'll start to see when you look up how to do things.
You can do anything, you can even program your own IDE in C++, it just takes too much time and is not a practical advise. &amp;#x200B; Next week RStudio might get fixed, and then we should be migrating back? This just causes a disruption. The point is RStudio needs to be fixed.
wtf is tidyverse?
wtf is tidyverse?
[tidyverse](https://www.tidyverse.org) is a set of modules/packages that help with a variety of tasks including attractive graphs, data manipulation, loading data, etc. Things are a lot more consistent and streamlined in tidyverse than they are in base R.
A [collection of packages](https://www.tidyverse.org/packages/) that make everything from data wrangling to data viz much more efficient &amp; powerful than what you get out of base R.
https://www.tidyverse.org/
https://www.tidyverse.org/
If you're paying for RStudio then you should be complaining loudly to them, if not then feel free to use an alternative or fix it yourself. Im not sure why you feel its such a big deal to 'migrate', this isnt C++ or the like with some complicated build process.
I am trying to have it fixed, and I am complaining loudly here because they also read this forum.
You can tell the `readr::read_csv()` function to not use the header row of the csv but instead to use your own column names since you're expecting all of the csv files to be of the same structure and column names in your for loop. I would have guessed that the csv's with the column names in the 2nd example are due to maybe some extra whitespace in the raw csv files?
One way to handle this problem is to use logistic regression. In most cases this will do. Interpretation wise you will odds that someone voted YES. If you want to use it for prediciton you have to keep in mind that errors in classicication (false-negative/false-postive) occur.
You can include a dummy variable (binary response) just like a metric variable. The coefficient represents the difference between the two groups (e.g. the effect). In a simple regression (only one independent variable): * the intercept represents the average for the 0-group * the coefficient represents the difference between the two groups * intercept plus coefficient represents the average for the 1-group
 election &lt;- c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0) expenditure &lt;- c(20.74, 15.06, 16.86, 7.48, 35.99, 67.06, 55.68, 161.1, 323.0, 39.19) df &lt;- data.frame(election, expenditure) plot(election, expenditure, pch = 16, cex = 1.3, col = "blue", main = "Expenditure against Elections", xlab = "Election", ylab = "Expenditure") abline(lm(expenditure ~ election)) library(cowplot) a1 &lt;- ggplot(df, aes(x=election, y=expenditure)) + geom_point(shape=1) + # Use hollow circles geom_smooth(method=lm,color="black", # Add linear regression line se=FALSE) a1 ggsave("ex.png")
&gt;Dependent Var That would help if the *dependent variable* is binary.
 as.POSIXct( paste0( df$DateTime, "m" ), format ="%a %b %d %Y %I:%M%p" )
It's typically done to address heteroskedasticity in the data. Roughly speaking, the end result is that the regression coefficients will represent percentage instead of level changes.
Assume you have an object in the R session called `Data` that has the IV and DV you have here. The linear model is rather simple. M1 &lt;- lm(EXPENDITURE \~ ELECTIONS, data=Data) At least, I'm assuming you mean you want to regress the expenditure column on the elections column from the MWE you provide here. It doesn't matter that the IV (elections) is binary since the coefficient will give you an estimate of the effect of elections on expenditures when the elections value is "YES" (relative to "NO").
You will almost certainly need more independent variables. One binary independent variable won‚Äôt be able to effectively predict a continuous dependent variable
A point biserial regression may help.
Yep, you need it to read "pm" not just "p".
https://ggplot2.tidyverse.org/reference/scale_continuous.html https://ggplot2.tidyverse.org/reference/theme.html ctrl + f tick
Is geom_histogram what you're looking for?
You could trim down your example a bit. Provide the head of the resulting data frame, wf, in this case. You can use dput(head(wf)) to get copy pasteable code to reproduce the data structure for an example, or use tibble to create a more readable version.
Haven‚Äôt tried but try the xslx package?
Try adding braces around your if/else statement, and also adding an "if" after the else if you want another condition. if () { } else if () { } or if () { } else { } Braces may not be necessary.
why not just create the report entirely in R? or entirely in vba? trying to recreate excel functions in R doesnt make sense
tried both; still getting this: Error in solve.default(oout$hessian) : Lapack routine dgesv: system is exactly singular: U[2,2] = 0 In addition: There were 50 or more warnings (use warnings() to see the first 50)
and the warning is In if (x$choice == 1) { ... : the condition has length &gt; 1 and only the first element will be used
Report automation. Output needs to be a pivot table. Don‚Äôt know any vba. Was hoping an r package had a function to do it but haven‚Äôt found any yet
you should try to look at MS Access - it is very easy to learn. Trying to do this in R would be way more complex
That's a completely different type of error that's not related to the code you showed here. At some point you are trying to invert a matrix that doesn't have an inverse.
x$choice is a vector, but you are using it as if it was a single value. Some x$choice might be 1, other x$choice might be 2, you haven't told it what to do, so it's just checking the first value in x$choice. What are you trying to do?
Based on x$choice value, calculate softmax according to one or the other formula.
There is no x$choice value. There are x$choice value**s**.
Right, and itertating through rows, I want to the value of choice for a given row to determine the way to calculate probability (softmax).
You aren't doing any iteration there. One way would be to make a loop, but the (probably) better way is to use the ifelse function which is vectorized. Look at ?ifelse
I'd suggest reading [this post on the package.](https://www.statmethods.net/stats/power.html) I think it's fairly concise. To quote the post: &gt; For each of these functions, you enter three of the four quantities (effect size, sample size, significance level, power) and the fourth is calculated. So, in your case, you need to know (1) effect size, (2) significance level, and (3) power. Once you know these 3, you can calculate (4) the samples size needed. Rules of thumb: (1) significance of 0.05 is fairly standard (2) power of at least 0.8 is generally desirable. These are obviously subject to change based on your exact problem.
I do this by writing writing the data fro r into an existing spreadsheet template , with the pivot table set up to reference it - once you save the updated info you can just refresh the pivot table.
Can you create a master pivot table and then just have R write to the data tab and save as a unique name (like exceldata_20190519.xlsx). Then in the pivot table options you can set it to refresh on workbook open.
Thanks! I have read this but I have no clue which one to use. I am trying to figure out, say, if I have a population of 100 how many I need to sample to have a effect size of .5, power of .8, and significance level of .05. However, when I use each of these it gives me a number that is not based on the actual size of the population because there is no where for me to input that.
This is because the sample required is not related to the size of the population.
To expand on this a bit for u/AdventureintheCosmos, for power analysis like this, either the populations are big enough or they are not. For example, say you have some set of parameters that would require a sample of 200. The population of 10 is not enough to meet those parameters, whereas the population of 1000 is more than enough. The calculation you're running is independent of group size because it's *telling you* what the group size needs to be. To use an example I'm more familiar with: You want to see if overexpression of Gene X causes bigger tumors in mice. How many mice do you actually need in each group to do this? You know you want alpha = 0.05, a power of 0.8, and a "medium" effect size of 0.5. Run the numbers (I'm on mobile, so I actually can't), and you'll get your answer. Order *at least* that many mice (possibly more to account for mice that don't form tumor or die for unknown reasons during the experiment).
second this approach. &amp;#x200B; you can automate the pivot refresh with a tiny bit of VBA [https://www.excelcampus.com/vba/refresh-pivot-tables-automatically/](https://www.excelcampus.com/vba/refresh-pivot-tables-automatically/)
Have you looked at solutions in Python? The VBA for what you are trying to do should be quite straightforward but Excel will do whatever it can to resist when you try to schedule it.
Yes - doing this on the workbook open event might be a good fit here?
If you can write R, you can learn enough simple VBA to do what you need to do. &amp;#x200B; You might not even need to edit the VBA at all. Import the data into excel and then record a macro while you format the data into a pivot table. Then you should be able to re-run the macro with fresh data and get back to your desired pivot table. Will likely require some trial and error but should be a LOT simpler than trying to send an editable pivot table from R to excel.
The dots are used to distinguish calculated from (in the dataframe) defined variables. It seems they're just called "special variables" or "computed variables"/"calculated variables" in ggplot. &amp;#x200B; [Source](https://stackoverflow.com/questions/17502808/double-dots-in-a-ggplot)
Yep, calculated. https://github.com/tidyverse/ggplot2/blob/92099706e0e009fbd58bfa83f7fc8b41deec8877/R/aes-calculated.r
Thank you for the explanation. For the "gene X" example, which specific function do you use from the list? Power.prop.test? In my situation, I have ~200 populations that may range in size from 1 individual to 1000 individuals. When plugging in the numbers you specified, I get that I need a sample of 300 individuals. So do I merely select 3 individuals from each population? Or do I select 3 individuals from most populations, but then select a greater number (say, 10 or 20) from the largest populations? How would I know this? I can't simply leave out a population because there are less than 300 individuals and only focus on large populations because that would not be true to what is actually occuring.
I guess I'm sort of confused as to what exactly you're trying to do. If you're just trying to compare populations to one another, you would take 300 from Pop 1 and 300 from Pop 2 and compare them. If they don't have 300 people you can't use them at the power you're wanting with the effect size you've estimated. Is there a particular reason you're trying to sample people from each distinct population?
Is there a reason you're trying to create one "super" population? Usually you just compressed existing populations to one another, and if they're not large enough then they're not large enough, and you either have to drop power or not include them. You might try googling around for how to subsample several populations to estimate a unified larger one, but I'm not sure if that's really done. It's getting a bit outside my scope here.
Here's a good read: [http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels](http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels) &amp;#x200B; I think you need `scale_x_discrete(breaks=c(30,40,50,60,70,80,90)`
I usually write a text file and load local for MySQL. You just send the statement that references the local file path. I assume there is a way to do the same in db2? In MySQL, this is also more performant for bulk inserts.
zzzzzzzzzzzzzzz...
https://rdrr.io/cran/DBI/man/dbWriteTable.html
Thank you!
So there are two answers to this question. "How do I build this graph?" and "How do I do things like this graph?" This graph is totally custom-coded js - you can inspect the code yourself and how it works, but it's defined in [https://www.washingtonpost.com/graphics/sports/olympics/the-1000-medals-of-the-united-states/js/base.js](https://www.washingtonpost.com/graphics/sports/olympics/the-1000-medals-of-the-united-states/js/base.js) \- this is all custom code written by TWP's front-end teams. It's pretty crazy the amount of work they had front-end guys do to enable this relatively simple visualisation. So, you can't make exactly this graph using R, because they made this graph using custom JS rather than an off-the-shelf framework. If you wanted to get close to this using an off-the-shelf framework, though, there are a number of packages that have bindings to frameworks like chartly or directly to d3.js that can do things like this. But you could never get *exactly* this visualisation without custom front-end work.
I see... Thank you so much for the answer! :)
note- I tried without typing expstudies:: and it didn't work so that is me trying to troubleshoot. &amp;#x200B; I posted about an R package for actuaries a while ago. I'm trying to get continuous integration and testthat up and running right now.
Is this the only failing test? Am trying to understand if you have an issue with this specific test, or if this is a more general testthat matter.
It's kind of the basis of my whole project. Thanks for the help! I was asking here because I have to give a presentation in class tomorrow and didn't have time to ask my professors. One professor told me "maybe 12" but I have no clue how he came to that number. I think I'll just leave out the number I plan to sample and hope no one asks. Thanks!
How about a heatmap for the visualization part. Also are these correlations?
They're not that specific to just these types of matrices, but: symmetric square matrix and unsymmetric square matrix. Could possibly further specify that the main diagonal is always missing, but there's other missing values largely at random as well. If you want to format it differently, I'd start by creating a data frame with all the pairs of indexes you want, e.g.: letters &lt;- colnames(my.table) df2 &lt;- expand.grid(letters, letters) # Contains self-self entries df2 &lt;- df2[df2$Var1 != df2$Var2,] # To remove self-self entries Then maybe do an apply across rows: df2$Relationship = apply(df2, 1, function (x) my.table[x[1], x[2]]) Probably prettier ways, but I think this works.
They are not statistical correlations. The numbers are just a quantity that represents how much each group feels towards each other: like (positive up to 10), dislike (negative down to -10), impartial (0), unset (N/A). &amp;#x200B; A heatmap would work for a single table, but not for plotting the relationships over time.
OK they all threw errors. I was in the process of adding a "PYCY" option to the add exposures function and figured I may as well test it. I appreciate that you would take the time to go and investigate on github. I pushed out your recommended tests to github and also the most recent version of the addExposures function. Check the version 0.0.2 branch if you are still interested. [https://github.com/ActuarialAnalyst/expstudies/tree/Version0.0.2](https://github.com/ActuarialAnalyst/expstudies/tree/Version0.0.2)
It looks like `addExposures` has some issues with namespaces, in particular for the dplyr functions `group_by`, `ungroup`, and `filter`. This should work if you use fully qualified calls for all instances of these functions in `addExposures`. As-is, the tests are all passing (which of course is not the expected outcome for the negative controls) because an error is being thrown further upstream. So, everything is technically an error, just not for the reason that is being tested for. In other words, add this simple test: test_that("function works", { expect_success(addExposures(record_mid, type = "PY")) } ) ...which should fail. You should now have five tests, one of which fails. &amp;#x200B; Then, modify the existing chunks in `addExposures`: `bad_count &lt;- records %&gt;% filter(start &gt; end) %&gt;% nrow()` #Calculate the interval ends, exposures, and durations formatPY &lt;- function(){ mod_exposures &lt;- addPY() %&gt;% group_by(key) %&gt;% dplyr::mutate(end_int = dplyr::lead(start_int) - 1, end_int = dplyr::if_else(is.na(end_int), end, end_int), exposure = as.integer(end_int - start_int + 1)/365.25, duration = yrs_past_start + 1) %&gt;% ungroup() mod_exposures %&gt;% dplyr::select(key, duration, start_int, end_int, exposure) } ...to these: `bad_count &lt;- records %&gt;% dplyr::filter(start &gt; end) %&gt;% nrow()` formatPY &lt;- function(){ mod_exposures &lt;- addPY() %&gt;% dplyr::group_by(key) %&gt;% dplyr::mutate(end_int = dplyr::lead(start_int) - 1, end_int = dplyr::if_else(is.na(end_int), end, end_int), exposure = as.integer(end_int - start_int + 1)/365.25, duration = yrs_past_start + 1) %&gt;% dplyr::ungroup() mod_exposures %&gt;% dplyr::select(key, duration, start_int, end_int, exposure) } You should now have two failing tests.
I'll fix the namespaces. I really appreciate your help, it would have taken a very long time for me to figure this out.
Or plotly
You said the numbers represent feelings of groups to each other. This looks like a case where you potentially could use Multidimensional Scaling (with its various algorithms). It often provides nice visualizations in e.g. a 2-dimensional space. An (older) example would be [Myron Wish (1970)](https://psycnet.apa.org/record/1971-04086-001)
Here's how I would investigate this: &gt; xts::as.xts function (x, ...) { UseMethod("as.xts") } &lt;bytecode: 0x000000000c056cd8&gt; &lt;environment: namespace:xts&gt; &gt; xts:::as.xts.zoo function (x, order.by = index(x), frequency = NULL, ..., .RECLASS = FALSE) { if (.RECLASS) { xx &lt;- xts(coredata(x), order.by = order.by, frequency = frequency, .CLASS = "zoo", ...) } else { xx &lt;- xts(coredata(x), order.by = order.by, frequency = frequency, ...) } xx } &lt;bytecode: 0x000000000d402e18&gt; &lt;environment: namespace:xts&gt; &gt; xts::xts function (x = NULL, order.by = index(x), frequency = NULL, unique = TRUE, tzone = Sys.getenv("TZ"), ...) { if (is.null(x) &amp;&amp; missing(order.by)) return(structure(.xts(, 0), index = integer())) if (!timeBased(order.by)) stop("order.by requires an appropriate time-based object") if (inherits(order.by, .classesWithoutTZ)) { if (!missing(tzone)) warning(paste(sQuote("tzone"), "setting ignored for ", paste(class(order.by), collapse = ", "), " indexes")) tzone &lt;- "UTC" } if (NROW(x) &gt; 0 &amp;&amp; NROW(x) != length(order.by)) stop("NROW(x) must match length(order.by)") orderBy &lt;- class(order.by) if (inherits(order.by, "Date")) { order.by &lt;- .POSIXct(unclass(order.by) * 86400, tz = tzone) } if (!isOrdered(order.by, strictly = !unique)) { indx &lt;- order(order.by) if (!is.null(x)) { if (NCOL(x) &gt; 1 || is.matrix(x) || is.data.frame(x)) { x &lt;- x[indx, , drop = FALSE] } else x &lt;- x[indx] } order.by &lt;- order.by[indx] } if (!is.null(x) || length(x) != 0) { x &lt;- as.matrix(x) } else x &lt;- numeric(0) if (orderBy[1L] == "timeDate" &amp;&amp; missing(tzone)) { tzone &lt;- order.by@FinCenter } else if (!is.null(attr(order.by, "tzone")) &amp;&amp; missing(tzone)) tzone &lt;- attr(order.by, "tzone") if (inherits(order.by, "dates")) index &lt;- as.numeric(as.POSIXct(strptime(as.character(order.by), "(%m/%d/%y %H:%M:%S)"))) else index &lt;- as.numeric(as.POSIXct(order.by)) if (any(!is.finite(index))) stop("'order.by' cannot contain 'NA', 'NaN', or 'Inf'") tzone &lt;- tzone[1L] x &lt;- structure(.Data = x, index = structure(index, tzone = tzone, tclass = orderBy), class = c("xts", "zoo"), .indexCLASS = orderBy, tclass = orderBy, .indexTZ = tzone, tzone = tzone, ...) if (!is.null(attributes(x)$dimnames[[1]])) dimnames(x) &lt;- dimnames(x) x } &lt;bytecode: 0x000000000bd2a1a0&gt; &lt;environment: namespace:xts&gt; What I gather from this is that basically ```xts``` checks for some constraints on its input, then converts the input to its own structure, while ensuring that it has an ordered index that is time-based and has no missing values. The class of the returned object is xts, which allows for different methods to be applied to it. ```?xts``` also has some good details on the xts format.
Honestly, I think the first thing you need to do is markup your code. I would go through every line and figure out what you are doing, but I have no starting points. Before you do anything else you should markup your code and explain what every few chunks are doing. I wish I could help more.
How can I do something similar using Plotly?
&gt;Multidimensional Scaling The reading is interesting but it looks like both classical and nonmetric MDS can't handle NAs. In my data NAs are necessary because 0 is when a group intentionally sets another to neutral vs. NA when there is no relationship set.
Simply put, xts adds attributes to the time series. So you can tag it with location data, for example. There are also some enhanced commands Why do you ask? What are you trying to accomplish?
BTW... xts and zoo are for time series data. Forgive me if I'm already telling you something you know, but understanding time series is essential to understanding the difference between them. While you're at it, look at ts(), tsibble, and tibbletime
I see. I'm not trying to accomplish anything. I'm just learning about it. Thanks
`diff` is using the difference filter, where `diff(x, difference = 3) = diff(diff(x, difference = 2))` iirc. Lag is just the amount of shift, so `t = diff(x, lag=3, difference=1)` is the same as `t = x[(1+3):n] - x[1:(n-3)]` whereas `diff(x, difference=3)` is more like `d = (t[2:n]-t[1:(n-1)])[2:n]-(t[2:n]-t[1:(n-1)])[1:(n-1)]` with `t` from above.
Got it! Excellent reason. I just completed recording a course on this for LinkedIn learning. It should be available in about a month
I just want to say that in case of problems like this, where you don't know what some parameters do. First of all you should read documentation of function. You can access it by typing "?name_of_function" in console. If you still don't understand what is going on, try to use this function with different parameters on simple data like: diff (1:10, lag = 1, difference = 1) diff (1:10, lag = 1, difference = 3) etc. Then look at the results and probably you will understand what it does. This is the best way to understand and learn programming in my opinion. P.S. sorry if I made mistakes, I'm not native English speaker.
I agree. Formatting the code will also help.
This is true. Please try to format the code next time for legibility.
This digest is great. Thank you üòÄ
The main drawback is that notebooks/R Markdown documents are meant to be run interactively, and that may not be the ultimately desired workflow for you. However, using `knitr::purl()` with the argument `documentation=2L`, you can easily turn an R Markdown document into a plain old R script, retaining all of the text between code chunks as nicely formatted comments. I often do this as an intermediate step between an exploratory analysis and a reusable script, function, or package.
Oh my goodness! I never knew purl() existed. I used to do a Save As which of course is a royal pain because of all the comments. Gotta try this at home. Thanks for the tip. It never ceases to amaze me how R often (usually) has a function to do exactly what you need.
Awesome, I will use that going forward. I'm still not sure I understand why it can't be used as a script. As I understand it, you can run Rnotebooks like they were scripts. So even if I produced a script for an automated process, turned it into an interactive RNotebook and then set a rule to prevent the Rnotebook from printing inlines: then it should not be a problem. That to me means I can produce all my scripts as clear, comprehensible, interactive formats for any newbies who want to use and understand my script, and also have it work just a like a script should for automated works.
That's because the data are being loaded using JavaScript. You'll notice that when you first open the page, you see some brief "loading" text. This is the data loading into/from a JavaScript app, so the data itself never touches HTML. You could try a package (e.g. `RSelenium`) that loads the JavaScript and then you could scrape using `rvest`. &amp;#x200B; Here is a better explanation than I can give: [https://community.rstudio.com/t/scraping-table-weirdness-with-rvest-undesired-xml-nodeset-0/6798/5](https://community.rstudio.com/t/scraping-table-weirdness-with-rvest-undesired-xml-nodeset-0/6798/5). It also has some next steps you can try.
run lots of production solutions in R, portfolio management and optimization, demand forecasting, pricing optimization, inventory allocation, next best action
If you are just doing a 2d scatter you can plot with ggplot2 first and then call ggplotly() on that. I've found modifying ggplot2 legends far easier than plot_ly
This is great. Thanks!
Truly, the work that R devs do is truly amazing!
I made a comparison tool with 20 of the most popular cards in November 2018.. see if this is something close to what you‚Äôre looking for: https://supplychenmanagement.com/2018/11/06/november-2018-credit-card-comparison-tool/
r code &amp;#x200B; model = lm(y \~., data = dataset) &amp;#x200B; summary(model)
So you pay for a license and support from RStudio and you come to reddit to get that support?
Where in the summary does it list the accuracy?
You should look into the `caret` package if you want something that is more similar to how `sklearn` works. I'm not very familiar with `caret` so this is what I would do. mod &lt;- glm(y ~ ., data = dataset, family = "binomial") pred &lt;- as.numeric(predict(mod, newdata = dataset, type = "response") &gt; 0.5) # with a cutoff of 0.5 table(dataset$y, pred, dnn = c("actual", "predicted")) # confusion matrix mean(dataset$y == pred) # accuracy
What do you mean by "just not working"? Can you post the output when you run `coefficientalpha::omega(data, varphi=.1)` and `psych::omega(m = data)` ? (fyi the xyz::f() notation means "function f, from the xyz package"). &gt;How can I specify which items from the data set I want to be included in the omega coefficient calculation? It looks like `coefficientalpha::omega(data, varphi=.1, complete = T)` will evaluate only complete cases (i.e. entries with no missing data). Might be as simple as that, otherwise you'll need to wrangle your data frame into shape before passing it to `omega()`. Check out the dplyr package for that. For example... library(dplyr) # run "install.packages('dplyr')" first if needed data &lt;- data %&gt;% select(-uselessColumn) %&gt;% filter(!is.na(score)) ... will take your data, drop the column "uselessColumn", then drop any rows for which the "score" column is missing data. edit: I'm using \`varphi = .1\` because you did above. No idea if that's appropriate here.
Hi there, take a look at stringr package: stringr::str_replace_all('x-X25-X_11-X-5', pattern = '([\\D])([\\d]+)', replacement = '\\10\\2') &gt; "x-X025-X_011-X-05" will do exactly what you want to the names of the files: Explanation: pattern is a regular expression indicating "match me any non digit character `([\\D])` followed by one or more digits `([\\d])` ". replacement uses \\\\1 to replicate the first part of the previuos match (the non digit part), put a 0 and \\\\2 to replicate the digit part, so '\\\\10\\\\2' must return the same match but including a zero between the non digit and digit part. See the function help for more info &amp;#x200B; Cheers!
Here you go: newdata &lt;- c("var1", "var2", "var3".... "varx") omegadata &lt;- datafilename[newdata] omega(omegadata) That plots omega for me. The other output I get is: Call: omega(m = omegadata) Alpha: 0.79 G.6: 0.81 Omega Hierarchical: 0.61 Omega H asymptotic: 0.74 Omega Total 0.83
Thanks for your help! The scale has 18 items - here is what it is giving me: \&gt; newdata &lt;- c("DC\_03", "DC\_14", "DC\_22", "DC\_09", "DC\_19", "DC\_06", "UC\_15", "UC\_14", "UC\_13", "UC\_20", "UC\_12", "UC\_23", "LC\_01", "LC\_06", "IC\_07", "LC\_15", "IC\_15", "IC\_19") \&gt; omegadata &lt;- newdata \&gt; omega(omegadata) Error in 1:n : argument of length 0 &amp;#x200B; Again, I appreciate any help - I am only now learning R, so I am pretty new to this. Thanks!
Thanks, I appreciate the help! Here is the output: &amp;#x200B; coefficientalpha::omega(data, varphi=.1, complete = T) \&gt; omega(CFA\_PCQ, varphi=.1, complete = T) Error in xi - mu0 : non-numeric argument to binary operator In addition: Warning message: Setting row names on a tibble is deprecated.
Where you have omegadata &lt;- newdata It should read omegadata &lt;- thenameofyourdatasetwiththevariablesinit[newdata] Good?
In the browser that opens through RSelenium do you actually observe the output from the tutorial? That is, does the website actually look like the screenshot they post? When I just tried it I had to click twice so that wouldn't work given the code you have.
Worked perfectly! Thank you so much!! I appreciate the help - learning R has been a bit harder than anticipated.
I want to 2nd the stringr package. For more information on stringr read [Chapter 14 - "strings" from R for data science](https://r4ds.had.co.nz/strings.html).
Yes it works up until the point where it opens a new chrome browser, maximises window size (or else it won't find the search button), types in the incomplete street address, clicks search, and displays a full address on the webpage. However, instead of returning the postal code in "output"it is returning an empty string.
This is a simple one but it took us longer than it should have to figure this out because it was buried in our code and seemingly innocent. The mean() function is looking for a vector! &amp;#x200B; mean(c(10,20,30)) correctly returns 20. mean(10,20,30) returns 10. No error, no warning. &amp;#x200B; We thought we were all going crazy when we saw the mean function "not working."
I haven't looked at your implementation yet (post it as text in code blocks rather than an image), but the last part of your post makes me want to point this out from the documentation of ?optim: &gt; By default optim performs minimization, but it will maximize if control$fnscale is negative.
Speculating and it's not my field but in [this](http://www.sthda.com/english/wiki/survminer-r-package-survival-data-analysis-and-visualization) blog post where they announce the package they use it together with the `survival` package. They probably wanted this one to sound similar and stretched the definition of "data mining" to use it with their plotting package.
You may find the future package useful. [https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/bigdata] [Run kmeans many times on several CPU cores](https://www.reddit.com/r/bigdata/comments/bupust/run_kmeans_many_times_on_several_cpu_cores/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I use the parallel package, its easier on Mac or Linux, I can't remember how I did it on Windows. The apply functions work the same way but have mc in front of the names (so apply becomes mcapply etc) and there is a num.cores argument.
In your *foreach* function, write the results to file in each loop, not at the end. What happens in the *foreach* loop for storing variables doesn't always transfer outside of the loop based on how the parallelism works. After the loop is done, compare the results in the file to find the best result.
Depending on how exactly you are storing vectors and computing distances, you'll probably find that kmeans, along with most ML algorithms - is primarily memory bound not CPU bound. So if you have 6 CPUs, don't expect anywhere near a 6X speed up. If you need to do kmeans really fast, I'd consider using tensorflow instead so you could run it on your GPU.
kxgq, thank you for your answer. I have edited my post. I tried doing as you said, but my results were persistently weird and nothing close to the w1 and w2 values that were found by Judd. I'm checking around if I have defined the problem correctly, perhaps this is the actual problem. Thank you once again.
Thanks for posting!! I'll be looking in to this.
Small typo, last part should be: m.lst = lapply(model, function(x) { as.data.frame(lstrends(x, "Tissue", var="GeneX"))})
If somebody has compiled it for your architecture and they've shipped you the compiled package etc it should work for you and you don't have to compile it yourself - if it's being called correctly. This is, after all, how most Windows users operate when it comes to R packages that use compiled source - they download binaries from CRAN and don't install from source. EDIT: and don't even have the capability of installing from source.
absolute hero!
Official releases of R have specific compilers that all compiled code must be compiled with. In Linux distributions this is usually the default compiler for that distribution, but on Mac and Windows the requirements can be much more specific.
In ggplot, the x/y data, and their categories, are called aesthetics (aes). If you want something to be collored by an aesthetic, for example by each Analyte, you need to state that in the aesthetic. If you assign color outside the aesthetic, it assumes you want it to be universal. In your case, you would want to do geom_point(aes(color = Analyte))
Here's a reproducible example using the built-in mtcars data: ggplot(mtcars, aes(y=mpg, x=cyl, group=cyl)) + geom_boxplot(fill="gray") + geom_point(color="blue") + coord_flip() Does this work if you sub in your data?
Thank you so much! That worked perfectly. I was missing the aes in there. Would you happen to also know how to add an y axis to the right hand side of the plot that corresponds with the number of analytes?
Thank you! I got it to work!
You need to make sure Bioconductor is installed first. https://bioconductor.org/install/ Then follow the installation instructions for limma. https://bioconductor.org/packages/release/bioc/html/limma.html
Hey there, unfortunately I'm not familiar with the packages that you mentioned, but in general, you can always install any missing packages. If you are using RStudio, then try googling "installing packages rstudio". RStudio has a GUI to manage packages. Alternatively, you can use the function install.packages(), e.g. install packages ("limma"). And if my memory serves me rigt, that should install by default all the dependencies as well, and if it doesn't then use the function again. Once successfully installed, you will need to load it using the library() function, e.g. library(limma). As a side note, sometimes in R, different packages can have functions that have the same name. For example, assume that the function foo() exits in both packages bar1 and bar2. If you load bar1 first and then bar2 package, the foo() function in bar1 will be masked by foo() in bar2 and eveytime, you call foo(), the one from bar2 will be used instead of bar1. But, say you want to use foo() from bar1. You can do it, using the syntax bar1::foo(). This way you are saying that you want foo() specifically from bar1. The bar1::foo() in fact would work even without the need of loading bar1 package. Hope I did not confuse you with this extra info. Ignore it if it doesn't make any sense.
Hello! I was reading a lot about the nullabor package, a very convenient package when sometimes we perceive something in a visualization, some pattern or logic, and we come to conclusions, which may or may not be sustained using a more specific technique. This is a small summary, in any case, your comments are sincerely welcome.
There are hacky ways to do it but the creator of ggplot (Hadley Wickham) is generally against plots with two y-axes, so didn't include that functionality. I tend to agree with his reasoning.
That's interesting. So it's basically using a human as the inference method due to the utter lack of information on the underlying distributions?
As /u/jgbradley1 said, you need to install through Bioconductor. biocLite was installer for Bioconductor, which is now deprecated and replaced by BiocManager. As far as R solutions are concerned, limma is the gold standard for gene expression data. It actually got me into R in the first place, back in 2008 when I was working with classical two color microarrays. Limma has also one of the best documentations of all R packages I used, just go to the link provided by /u/jgbradley1 and download the usermanual.pdf (or launch it with browseVignettes("limma") from within your R session). You will find detailed description of the procedure and several examples of most common experimental designs. If your data are RNAseq, you'll probably need to use some other R package for pre-processing and normalization. WGCNA is independent package to limma. Limma will give you classical list of significant genes in pre-defined comparisons, with corrections for multiple testing. One problem with this approach is you'll end up with list of hundreds to thousands significant genes and now what? [WGCNA](https://horvath.genetics.ucla.edu/html/CoexpressionNetwork/Rpackages/WGCNA/) (weighted correlation network analysis) tries to go around this problem by identifying clusters of co-expressed genes (genes with similar expression profiles across the conditions of your experiment) and correlate them to your sample traits (phenotypes, time points whatever). This might help you to identify e.g. regulated pathways and interpret your data better. They also have some tutorials and methodological papers, but I found it harder to get going. It's also significantly more computationally demanding, so you better run it on beefier machine (e.g affy experiment with 20 000 probes and 12 arrays will run in minutes even on my quad-core laptop, while WGCNA will take up to several hours).
Cute idea, but would it fly past reviewers? It‚Äôs essentially an n=1 visual p value test. Why not use the general metrics given for a regression?
Here's my extemporaneous comments after a cursory reading of your post and the `nullabor`[vignette](https://cran.r-project.org/web/packages/nullabor/vignettes/nullabor.html). I'd never heard of the nullabor package previously. Although I find the concept interesting, I worry that it could be horribly misused. Long before I used R, I would *visually examine* my multivariate data sets using Spotfire DecisionSite (now just called Spotfire) to look for trends that might be interesting. # Classical Reductionist Methodology vs Modern Hypothesis Generating (Systems) Methodology ## Background Most science training teaches a *reductionist* methodology, which follows the well known paradigm: 1) formulate a narrow hypothesis, 2) design experiment to test the hypothesis, 3) conduct the experiment, 4) reject or don't reject the hypothesis based on the experimental data. With modern scientific techniques like genomics, the classical reductionist approach is somewhat outdated. Instead, in the modern *systems biology* paradigm, we typically start with a very broad hypothesis, then conduct an experiment to develop one or more narrow hypotheses, which are then tested. As an example, with the classical *reduction* approach, a neuroscientist might hypothesize that beta-amyloid plaques are the cause of Alzheimer's Disease (AD). Then, they could test their hypothesis in a number of ways, such as developing an animal model for classic experimental design or using observational data with humans to determine if there is an association between beta-amyloid plaque and AD. ## Strengths and Weaknesses of Each Methodology However, a modern *systems* approach could look as follows. Recruit disease and healthy (control) cohorts, perform a genomics study, then identify associations between genes and disease severity. This modern *systems* approach yields far more data, and it has the possibility to help find associations that a researcher might not think to look for with a classical reductionist approach. However, it also has a much greater potential for *false discovery* because so many hypotheses are tested simultaneously, and typically, the number of experimental variables measured per subject often far exceeds the total number of subjects studied. ## What does all of this mean for a package like nullabor? Nullabor seems to be perfect for people who want to visually interrogate large data sets quickly. That's a positive. However, it comes with the risk that people who lack the appropriate statistical training will find false associations, especially with modern high dimensional data sets. The [Spurious Correlations](http://tylervigen.com/spurious-correlations) website has so many great examples of false associations that result when thousands (or more) correlations are tested. # Feedback on Your Post First, I really like the graphic design aspects of your post. The text is very easy to read because of choices for font-family, line spacing, element layout, etc. I like it so much that I'm going to use it as a guide for some of my own future blogging. Second, I found the sentence construction to be distracting. If English is not your first language, then I understand the reason. Either way, you should have someone proofread it for you and give you specific editorial recommendations. Third, I think that the original nullabor vignette has a good overview of nullabor, and truthfully, your post covers some of it, but not as well. You do introduce an additional example, but I didn't find the added example to be particularly informative. In closing, I think you have the beginnings of a really great post. If you'll address the grammar issue I raised and give more of an overview of the value of a package like nullabor along with an explanation of the limitations, you'll have a great post. Good luck!
I don't see why not. The problem likely comes from *when* it's applicable. You're testing a really specific null with this technique. If you can reject that at the 95% level, great, but if you do, you might not be rejecting the hypothesis you really wanted to reject. The other advantage or disadvantage seems to be power. If there's a pattern like spoken language that people are way better at detecting than state-of-the-art methods, then this could plausibly increase your power. If there's a pattern like a small difference in means, where we have methods that detect better than people, this method would decrease your power.
I don't know where the R part of your script comes in but guessing it's the string inside this command `grviz(R-part)` you could: grviz(paste0("digraph....n=",txpl_2005_count,"...")) or you could use `sprintf` but I think the above is easier when you're not familiar with the language.
Sounds like string interpolation. I usually use `paste0()`.
sprintf('%s%s', a, b)
This isn't particularly clean or elegant, but it will get the job done. Full disclaimer, there are probably more memory efficient ways: counts = replicate(nrow(data)-1, 0) for(i in 1:(nrow(data)-1)){ counts.i = replicate(nrow(data)-i, 0) for(j in (i+1):nrow(data)){ mtc = sum(data\[i, \] == data\[j, \]) counts.i\[j-i\] = mtc } counts\[i\] = sum(counts.i) } I didn't want to go through and do a lot of appending in a loop, that tends to be misery. I'm sure others can chime in with something better.
glue::glue() is a more modern take on this.
Rather late than never--sorry for the delay!! Here are a couple basic tips that are somewhat related to SPSS. I could easily lose focus with this list, so if you have R questions or questions about any items I can get into more detail. 1. Use Rstudio for your IDE and use the Haven package when importing/exporting data with SPSS 2. Be sure to take advantage of ggplot: a huge perk to working in R (especially opposed to the SPSS graphics) 3. Use broom and broom.mixed (respectively) to make working with statistical models a little easier 4. Using kable / kableExtra with R Markdown makes for nice presentation of results 5. Force yourself to follow good coding style (e.g. [tidyverse style](https://style.tidyverse.org) or [google style](https://google.github.io/styleguide/Rguide.xml)) while writing code. You will thank yourself in the future.
That's awesome!
Thank you for everyone who answered! it seems for now that I don't even need this package lol, but I still wanted to install it (just in case), and [jgbradley1](https://www.reddit.com/user/jgbradley1/), your installation guides helped me. Also [neutral\_mutation](https://www.reddit.com/user/neutral_mutation/), I need to fix correlation between some of the predictors in my DataSet, for now I'm good in this aspect, but I mght end up using WGCNA, since you said that it helps. So as you can see your answers helped me so thanks again!
Hey! OP-- this is great. Though I feel there is a critical typo in both your description and the article. You are using the package title \`rvast\` is this a mistake? Shouldn't it be the popular web-scraping wrapper \`rvest\`?
Oh yes, that's right! thanks, I am going to fix it right away! :)
Try hist(as.numeric(Life_sub$`Life expectancy`)) Output that would help people trying to help you in general: class(Life_sub) head(Life_sub) # If it's not too wide etc.
This is assuming that you do have a data frame named Life_sub defined. But Since as.numeric isn‚Äôt working, I would run ‚Äònames(Life_sub)‚Äò to see if there‚Äôs a blank character at the end of any of the variables‚Äô names. Would also recommend running ‚Äòstr(Life_sub)‚Äò to check the variable types within the data frame.
Hey there, it would be helpful if you would show us the rest of your code as well, as otherwise what you posted looks ok. I did go ahead and tried to plot the histogram as well. It worked for me. Following is my code: `Life_sub &lt;- read.csv("Life Expectancy Data.csv", header = TRUE)` `str(Life_sub)` `&gt;'data.frame': 2938 obs. of 22 variables:` `...` `&gt; $ Life.expectancy : num 65 59.9 59.9 59.5 59.2 58.8 58.6 58.1 57.5 57.3 ...` Note that when I read the file in, the column is named "Life**.**expectancy". The space has been automatically replaced with a dot. The following works: `hist(Life_sub$Life.expectancy)` When I run your code, I get the same error `&gt; hist(Life_sub$\`Life expectancy\`) Error in hist.default(Life_sub$\`Life expectancy\`) : 'x' must be numeric` So can it be that your are simply mistyping the name of your column? You can always use the `str` or `colnames` to check the name of your columns.
Thank you so much, this worked (although I'm not sure why the others didn't lol I'm going to look into that now).
Thank you so much this also worked :) What a great community.
¬Ø\\\_(„ÉÑ)_/¬Ø `sprintf` is in base package
I think you need to add the breaks argument in the scale_colour_manual function. Breaks would basically be your line names, so maybe try breaks instead of labels?
Unfortunately it did not change a thing. Chart looks exactly the same now.
You might need to specify the exact location of the legend, instead of"right".
 Rotterdam area Netherlands
It did not change anything :/
Why can you not add the color to ```aes()```? Then you would get rid of the loop. Also, do you want to have a legend with 28 lines? You should see if you can have a fill aes.
I have color it in aes, but instead of bein aes it is aes\_string so I can loop through it. I have no idea of how to do it better, since my data frame consists of 28 series, and I do not want to enter all of these manually. Do you have a better solution? &amp;#x200B; Regarding `You should see if you can have a fill aes:` what do you mean?
Okey solved by adding changing for block into: &gt;^(for (i in 1:(dim(data)\[2\]/2)) {) &gt; &gt; ^(assign(paste('a',i,sep=''), cols\[i\])) &gt; &gt; ^(g &lt;- g + geom\_line(aes\_string(x = paste('x',i,sep=''), y = paste('y',i,sep=''), color =) [^(as.name)](https://as.name)^((paste('a',i,sep=''))))) &gt; &gt;^(})
Have you checked whether `tissue.edata.qn` or `timeCharacter` contains NAs? `lm` will remove samples containing NAs unless another a different `na.action`-argument is given.
I've not looked at it in detail but I would assume if you have a difference of 1 it's due to an intercept term being fitted in the linear model. If you don't want it either filter it out of your final data set or (more risky) ask the model not to fit an intercept term
Thank you so much for this. I will give a try immediately when I am work tomorrow. Thanks again. I will let you know how it works out.
Oh, I should mention I stripped out your categories column when I ran that. Not that it should change anything, but just to be complete, I put the code you have there in and then started with: data$category = NULL
Take a look at the source for `print.summary.lm` (you can use `getAnywhere`). This is what produces the text output from a call to `summary`. You can then make a copy and exclude the chunks you don't want to keep, and also route the text into an object so it's a bit easier to play with in the context of an Rmd.
Depending on what part you need, [`broom`](https://broom.tidyverse.org/) should make it easy to extract. For instance: ``` &gt; library(broom) &gt; &gt; fit &lt;- lm(Sepal.Width ~ Petal.Length + Petal.Width, iris) &gt; glance(fit)$p.value [1] 2.238465e-08 &gt; ```
Markdown has a few chuck options like echo, include, results, etc that you can set to True it False. https://rmarkdown.rstudio.com/lesson-3.html Hope it helps! Good luck!
```ggplot()``` works best with tidy data. That is, instead of 28 columns, you should have 3 columns: x, y, and which group. In the following example, I generate 5 columns of x and y data. Then I show a base R approach (```matplot()```). Everything up until ```library(ggplot2)``` is base R. # Data constants and info ------------------------------------------------- n_cols &lt;- 5 n_rows &lt;- 20 colx &lt;- paste0('x', 1:n_cols) coly &lt;- paste0('y', 1:n_cols) # Generate Data ----------------------------------------------------------- set.seed(1) x_data &lt;- matrix(rep(seq(1250, 1400, length.out = n_rows), n_cols), ncol = n_cols, dimnames = list(NULL, colx)) y_data &lt;- matrix(runif(n_rows* n_cols, -100, -60), ncol = n_cols, dimnames = list(NULL, coly)) # base r approach --------------------------------------------------------- matplot(x_data, y_data , type = 'l' , col = 1:n_cols) legend('bottomright', legend = coly, lty = 1:n_cols, col = 1:n_cols) # ggplot2 approach - you need tidy data ----------------------------------- data &lt;- as.data.frame(cbind(x_data, y_data)) data_melt &lt;- reshape(data = data , v.names = c('x','y') , varying = list(colx, coly) , timevar = 'group' ,direction = 'long') library(ggplot2) ggplot(data_melt, aes(x = x, y = y, group = group, colour = group))+ geom_line()
An alternative method using only base functions would be to save the summary in a variable like `reg &lt;- lm(Sepal.Width ~ Petal.Length + Petal.Width, data = iris)` `reg_summary &lt;- summary(reg)` reg\_summary is a list and you can extract the desired element using `$`, like `reg_summary$r.squared` Alternatively, you can also skip the intermediate variable and use something like `summary(reg)$r.squared`. There are also base functions like `coef(reg)` and `coefficients(reg)` that return simply the regression coefficients, you can also do `summary(reg)$coefficients`.
Thank you for your feedback. About the points that you mentioned: 1) I glad that you like the graphic design of the post. 2) You are right, English is not my first language. I will consider your advice about to have someone who reads my posts to address specific recommendations and corrects my mistakes. 3) Thank you for your recommendations. I have already included some of them. I am still thinking of how to improve the included examples in the post.
Remember reg will be a list which you can easily extract components from (using the `$` operator or `[[`). For example, reg$coef or reg[["coef"]]
Hey there, I can currently think of expand.grid() x &lt;- c(1,2) y&lt;- c(3,4,5) comb &lt;- expand.grid(x,y) comb[[1]] - comb[[2]] Note that expand.grid creates a data.frame.
Do you care about the order of elements in your new column? The classical way to think about this is to produce a table/grid called X with N rows and M columns (where N and M are the length of your two lists A &amp; B respectively). Then in each cell (i,j) of the matrix X[i,j] = A[i] - B[j]. Then all you have to do is convert the matrix into a one-dimensional vector rather than a two-dimensional table.
You can use `permutations()` from the `gtools` package: library(gtools) a &lt;- c(1, 2) b &lt;- c(3, 4, 5) c &lt;- permutations(length(a) + length(b), length(a), c(a, b)) `c` contains a matrix with all possible permutations, which you can turn into a dataframe as you see fit:
You can use `outer()`: a &lt;- c(1, 2) b &lt;- c(3, 4, 5) c &lt;- outer(a, b, `-`) Note that `c` contains a matrix. If you want the results as a vector, use: c &lt;- c(outer(a, b, `-`))
Awesome, this works. Thanks!
I think the line `data_available = df[df$PRODUCT == input$OWNER, "PRODUCT"]` is what is giving that error. I don't think you can use `input$OWNER` outside a reactive/observe expression.
You were right, that solved the initial error, but now I'm getting "Warning: Error in $.shinyoutput: Reading objects from shinyoutput object not allowed." and the map doesn't show. Thoughts?
I would remove the lines output$OWNER &lt;- reactive({ as.character(input$OWNER) }) output$PRODUCT &lt;- reactive({ as.character(input$PRODUCT) }) and see if that fixes anything. Then replace anytime where you use these with just `input$OWNER` and `input$PRODUCT`. `input$&lt;inputId&gt;` variables are already reactive so elements that use them will update when the value changes. You usually pair `output$&lt;outputId&gt;` with a `render&lt;type&gt;()` function like you have done with `renderPlot()`.
my observations: \- you don't seem to be loading the data. you refer to `df` but i dont see it being loaded anywhere \- data available should be a reactive element e.g. `data_available &lt;- reactive({` `data_available = df[df$PRODUCT == input$OWNER, "PRODUCT"]` `})` now `input$owner` should work. your prior example didn't work because if wasn't in a reactive element. &amp;#x200B; and when you call `data_available` in another function refer to it as `data_available()` &amp;#x200B; also, in your example, i assume you are loading `df` with something like `df &lt;- read_csv("blah_blah")` assuming its a csv file. this should before your shinyserver(funtion{input,output}) block. in this case you can get to the data simply by using `df` variable name.
While that did clean up my code, it didn't resolve the error.
I have this code before defining ui: `pack &lt;- c('tidyverse', 'highcharter', 'htmltools', 'rio', 'DT', 'shiny', 'shinyjs', 'shinycssloaders', 'ggplot2','dplyr','tidyr','NMF','RColorBrewer','highcharter','openintro','ggrepel','tidyquant', 'httr','jsonlite','dygraphs','maps','ggmap','mapproj','viridisLite','d3heatmap','flexdashboard', 'plotly','plyr','qdapRegex','readxl','htmlwidgets') pack_new &lt;- pack[!(pack %in% installed.packages()[,"Package"])] if(length(pack_new)) install.packages(pack_new, dependencies = TRUE) lapply(X = pack, FUN = library, character.only = TRUE)` `rm(list = ls()) df &lt;- read_excel("df.xlsx")`
Same issue for the line inside your `renderPlot()` as above, you don't need: output$DF1 &lt;- reactive({ df %&gt;% filter(...) }) Just refer to `df` or `data_available()`, whichever you intend to use for the map.
as others have stated, you preface function names with OUTPUT only for items that will make it through to the interface \[i am sure i am missing out on some details here\]. &amp;#x200B; any data element should only be a reactive like such &amp;#x200B; `df &lt;- reactive({do something})` &amp;#x200B; then you call it from other functions by using the `()` with it e.g. `df()`
You forgot to include the relevant code you are using to compute the distances, so you get a suggestion rather than an example. See `?outer`.
I don‚Äôt know because when I put R on my resume what I am really saying is that ‚Äú I can find answers on stack exchange and implement them ‚Äú...
try to duplicate a project or similar one youve done in Python but in R ?
It's not a bad idea if it works! I like your honesty tho xD
My skills in python are far better than R.. I dont know if that s reachable
I cant speak for all fields, but in biology/ecology we do ALOT of analysis of variance. Learning how to do one/two way anovas, manovas, ancovas, and repeated measure models would make you really employable in a biological field. Being comfortable with non-linear regressions is also an asset.
depend what you wanna do, try think of a small project and implement it my best suggestion
Try making data visualisation projects. Look up Shiny for making dashboards and interactive data stuff. You can work on some free data sources and share them for free on shiny.io
You could use two nested for loops. One of them will run through each row of the column, and the other will run through every colum (for every row). Make a square matrix as big as your list of coordinates (say, A), then for (x in coordinates.list) { for (y in coordinates.list { A[x,y] = distance(x,y) } } It's a symmetrical matrix so you only need to calculate half of it, and the diagonals will be 0, but for just a few coordinates that'll do it. I think that's what you're looking for.
Hey there, at first glance, your code looks alright to me. Maybe I'm overlooking something as well. Does the problem occur during training in `df[,1:dd,drop=F]`? If yes, one alternative might be to use the whole data.frame and instead change the formula inside the loop using the `ff &lt;- formula(...)` function and then passing it to train like `..$train(ff, data = df, ...)` . Maybe you can try to comment out some part, create new variables, etc. to somehow narrow down the source. If you know how to debug, then that might be helpful, otherwise try printing some metrics, dimension, column names, etc.
It's pretty simple to create shareable packages for R. You can create packages that have an impressive readme and put them on github. I have a like to a package I created on my resume.
I'll look into it, thank you
I like this approach thank you
Thanks for the tip :)
Flex life xD Thanks
I replied with that answer when asked "what I'd do if i was stuck with something and your supervisor isnt around to help".. suffice to say, i got the job!
Orrr you could do `a = dist(x, y)`. üòâ
Files don‚Äôt define chapters, markdown headers do. So you want to start the subtitle with ‚Äú## ‚Äú
But that will only create a subsection within the same file. I want to be able to make: **1.** Main |\_\_\_ 1.1 Abstract |\_\_\_ 1.2 Overview &amp;#x200B; Where Abstract and Overview are not within the same HTML file as Main. But are each their own separate HTML files that open as a new page.
I can‚Äôt find those details from mobile, but the files themselves don‚Äôt matter. For all intents and purposes, bookdown concatenates all the .Rmd files into a single Rmd file and runs the whole book from there. The separate Rmd files are for your convenience not book downs. So separate Rmd files vs 1 big Rmd result in the same book. I think what you‚Äôre looking for is in a yaml option or other option
Thanks. I appreciate your response!
You need to better explain the desired output. From what I understand, the output won't be a table since buildings may have different parts presumably and they don't all have the same materials.
I should have also explained that because of the potential for two (the dataset's max amount) of the same building feature, I would be including "Wall2", "IntWall2", and "Floor2". I was thinking of something along the lines: "if Building Feature is 1, and Percentage &gt; 50, then Wall1 = Description. Percentage &lt; 50 then Wall2 = Description". Ignoring the 50/50 percentage case at this moment. So once all of those appropriate assignments of variables are completed and stored on one row of the first instance of the Building ID, then I will remove the duplicate rows which should be easy enough. &amp;#x200B; I will check out lapply tomorrow, thank you.
Finally figure it out. Replying in case anyone is searching for this answer. &amp;#x200B; In your "index.Rmd" file, there will be a section of code in the beginning that will begin and end with "---". Within this section of code, there will be things like "title:" and "date:" and "author:". In order to create separate HTML files, instead of one long HTML file, you must include the following into that section of code, where each indent is incremented by 2 spaces: `output:` `bookdown::gitbook:` `split_by: "section"` `split_bib: no`
If you gather your data to long format you can easily do something like: ggplot(mydata, aes(x = risk, y = time, scale = occurances)) + geom_point()
ggplot can do this out of the box, you just need to reshape the data into long form as usual, and then use `geom_point(aes(size = Value))`, where `Value` is your value column.
I can't give actual data due to an NDA, but basically it's harbour info, where we're looking for the relation between the time at the dock and the waterlevel. It's been categorized into 5 categories each and we want to show a simple visualization of the amount of occurrences of each. It's just 2 columns we're trying to plot out, this is what the relevant columns look like. Docking time category | waterlevel category ---------------------|------------------- A | B A | C A | D B | E B | B A | C One column is the time the ship was docked during it's visit, the other was the water level during the visit. Each row is one visit.
Recommend downloading Gephi (download link below) which is a free visualization program for R code. You just export your data as a graph.ml and then open that file up in Gephi. Watch a youtube tutorial on how to use the program to get the basics, in there you can select Node, size and due it based on degree or possibly weight in your case. Download Gephi: https://www.softpedia.com/get/Science-CAD/Gephi.shtml
URL is broken!
Really? It works for me
&gt; Oops! Thats not right. &gt; Something went wrong. It may be... &gt; We have an influx of traffic or are experiencing network connectivity issues. &gt; &gt; You tried to use HTML or other restricted code. Try again with simple text only. &gt; &gt; You used too many smilies or other html tags.
[2010 page](https://247sports.com/Season/2010-Football/CompositeRecruitRankings/?ViewPath=~/Views/SkyNet/PlayerSportRanking/_SimpleSetForSeason.ascx&amp;InstitutionGroup=HighSchool&amp;Page=1&amp;utm_source=share&amp;utm_medium=ios_app&amp;utm_name=ios_share_flow_optimization&amp;utm_term=control_1)
Ah, I see. You've got a more complicated scraping problem here, though it is definitely solvable. See how you have to click on the "load more" link at the bottom each time? This means it's loaded in through javascript making a request to the backend. However, if you inspect that link you can see that there is a page GET value for each new page, which is nice. So you have two options: 1) Use RSelenium to scroll down and click on that link until that link doesn't show up again. 2) Loop through a sequence of page numbers until you load a page with no content on it (or an error) page. Both are doable but will require some interesting ifelse stuff.
Thanks for the help and looking into it. Yes, I noticed the URL has the pages associated to it. If I create a vector for a loop to run through each page and the loop hits an error, will it break and not scrape the data before the error?
There are a couple ways of doing this, but the easiest is probably something like creating a blank list, each element of the list is a dataframe scraped from each individual page, and then at the end of the loop you stack all the dataframes within the list together, like: player_list &lt;- list() for(i in 1:50) { df &lt;- # scrape stuff player_list[[i]] &lt;- df } player_df &lt;- bind_rows(player_list)
Ok thank you. I think this should help me. I‚Äôll have to try it out later. I really appreciate you taking the time.
This is definitely one of those scraping exercises that will take some trial and error. It's a good learning experience though
Yeah, I feel like I‚Äôve had to learn so many different functions from so many different packages already haha
The color goes inside aes()
Oh shit. It's ALWAYS something dumb lol. Thanks dude!
`geneNameAndMean` is getting created each time the `for` loop runs.
Oof, thanks
No worries, we've all been there!
Now it's saying object "trt" not found. So dang. :(
You have this trt as a column in your dB? I don't know if this is important but it is a class char?
It sounds like you would have to create 5 new variables (Level 1, ..., Level 5). Is there a reason that your days between variable is a factor? Is it actually categories or numbers? If it were a numeric variable you could define these easily: `level1 &lt;- 1 &lt;= days_between &amp; days_between &lt;= 3`.
I made it a factor just because I preferred the output in a logistic regression . I can easily turn it into numeric I guess . So then level 1 would still be a factor right ? So my interpretation would be within 3 days compared to not within 3 days . I think I am having a hard time wording my output in a supple way for c suite - that is why I went with that categorical before. .
As I wrote it above, `level1` would be a logical, but you could make it numeric by wrapping it in `as.numeric()` or a factor with `factor()`. For your own interpretation, a factor of 1's (yes, within 3 days) and 0's (no, not within 3 days) might be preferred: `level1 &lt;- factor(as.numeric(1 &lt;= days_between &amp; days_between &lt;= 3))`
Do you want to install updates or develop the package? You'd have to get in touch with the author. What package is it?
I suppose the best way to put it is that I want to install updates to the packages, so that I can use it myself. I already emailed the author, and haven't gotten a response. [playitbyr](https://playitbyr.org) is the package I'm wanting to use.
If you just want to use it yourself you can load it from github using devtools?
I FIGURED IT OUT lol
Nice, congrats.
It looks like this thread from Stack Overflow should help: https://stackoverflow.com/questions/28056805/find-discrepancies-between-two-tables I primarily use PROC COMPARE in SAS so I'm interested to see what others have to say about this procedure in R.
I'll have to check that package out, thanks!
I‚Äôm sure there‚Äôs a better solution out there but I would combine all the columns into 1 new column and then get counts of each unique row in the other so. A count of 0 means its missing in the other df
Anti-join from dplyr
1. Assign a Record Id 2. For each dataframe, tranpose into one column, grouping on record ID (i.e., Record ID will not be unique). You should now have two new dataframes, each with one column. 3. Use Anti-Join from Dplyer as mentioned
This.
The `symbols` function in base R will also do the trick.
Hi Th3Actuary, I don't suppose you've had a look at my problemo yet? No worries if not
i was very excited.
Feels good when we start to correct our own code.
Look up dplyr and tidyverse, specifically the mutate function paired with ifelse as an alternative solution.
It sure freaking does
1. What package is that function in? The answer will vary because we don't know what that function does or outputs. 2. Why do you need to *replace* those items?
Does this give you want you're trying to do? aggregate(ct ~ gene, data = flies, FUN = mean)
See ```tidyr::spread()```. You want to reshape from long to wide.
If base I think standardize converts to Z. If so, do you still have the original mean and SD for the data? You can always turn it back into the original score.
Agreed on both questions. As for #2, why not create a new variable called ItemA\_1\_std (or similar)?
Thanks for the feedback the loop finally worked after following your advice of passing it to the train function.
My apologies - standardize is in the psycho package
That would work too - it's .not really important to me that I overwrite the old variable. I'm just now stuck with one column that has two variables, and I need to cut it back in half into its two respective variables.
Why are you standardizing all three together rather than individually?
Well, it is. I appreciate your help but I'm here for R help, not stats help, ya know?
You have a couple options, I'll rate them in order of ease + usefulness since I think you should probably be working in data frames rather than a collection of vectors unless they're all the same length and some of these suggestions are easier to put in that context: 1. that standardize function for vectors is just a wrapper around `scale`, which returns the mean and sd as well, which is all you need to then standardize it yourself. 2. compute mean manually, compute sd manually, then standarize it yourself. 3. split the vector into the thirds, one way of doing this is `matrix(output, ncol = 3, byrow = F)`. Now you have your stuff as three column vectors. Only if they're all the same length.
If that's what you really want, you can split the output vector using `split(values, grouping)` or just use brackets: ItemA_1 &lt;- StandardizedItemA[1:length(ItemA_1)] StandardizedItemA &lt;- StandardizedItemA[length(ItemA_1):length(StandardizedItemA)] # etc for 2 and 3
dude find a guide for dplyr and learn it! There is a half dozen of functions that will make super easy all these data-wrangling types of things.
Have you installed transformr ? That package is needed to animate lines.
Also I think you should change variable to time. On my phone now so I can't test the code.
The website has installation instructions. The same instructions are also usable to upgrade the package.
Yes I have installed and loaded the loaded the library as well
Okay, let me know when you get a chance to try:). The exact same code works with geom_point
Hi Path, any result of that experience with R?
As folks already suggested, anti-join might be your best bet here. There are also a few packages out there that might help if you want more of a record of what has changed/where the differences are. I've used the package `compare` for a quick overview of general differences between two datasets. I also saw `compareDF` mentioned recently but haven't had a chance to experiment with it yet. ([Here's an overview of compareDF](https://www.r-bloggers.com/comparing-dataframes-in-r-using-comparedf/).)
Is this what you are looking for? If I remove the enter_fade() line the function seems to run fine. library(gganimate) library(gifski) library(av) library(tidyverse) plot.curves&lt;-function(type){ time &lt;- seq(0,15, length=100) sd&lt;-seq(1,3,length.out = 4) b&lt;-list() for(i in 1:4){ b[[i]]&lt;-switch(type, "c"=dnorm( time,3+i,sd[1]), "d"= dnorm( time,3,sd[i]), "c+d"=dnorm( time,3+i,sd[i]) ) } df&lt;-data.frame(time, b) names(df)&lt;-c("time","one","two","three","four") df_melt&lt;- reshape2::melt(df,id="time") q&lt;-ggplot(df_melt,aes(time,value,color=variable))+ geom_line(size=2, na.rm = FALSE)+ scale_color_manual(values= rev(RColorBrewer::brewer.pal(9,"PuRd"))[seq(1,7,length.out=4)])+ theme(legend.position = "none",axis.ticks=element_blank(),axis.title.y =element_blank(),axis.ticks.y = element_blank(),axis.title.x = element_text(face = "bold", color = "black", size = 16))+ transition_states(variable, transition_length = 2, state_length = 1)+ shadow_mark() #enter_fade() animate(q,fps = 30,renderer = av_renderer()) } plot.curves("d")
Ye, it works when enter_fade() is removed. But I would like to have each of the curves appear as a fade. Not sure what the error means. i raised a issue in gganimate github too
Copy of my answer on your r/rtats copy of this Q: So, after much messing around, I think the issue is that the way transition_states works is that it knows what is turning into what based on things in the same `"group"`. In `ggplot` by giving a line a colour you group it by that variable. So what it happening is that your 4 lines are not representing data belonging to the same group. The solution is to `"ungroup"` the data. I'd never actually tried it before but it turns out that if you change your geom_line call to `geom_line(aes(group=NA), size=2)` then you get what I suspect you want.
Task scheduler on windows or crontab on linux.
Thanks for your reply! I've tried it, but it doesn't show any mesage on the console that confirms it is working properly. I used "on idle", which I assume it executes the script whenever the computer is "inactive".
Best to configure it at predefined times. Have the script create logs if you want to be sure. TS also has some metrics such as went it last ran
I have such a nerd crush on this guy.
Hopefully I can break away from work to watch.
He's so handsome and talented üòç
He is! It would be annoying if he didn't put out so much code that makes my life easier.
Excellent! Thank you so much. One thing still doesnt work - that is enter fade and exit shrink, but this is much better than the original one I had
I am also at a loss to that error message. If you get an answer - will you post it here?
What do you mean it doesn‚Äôt work? The code runs with it. Were you expecting some sort of different effect?
Does anyone know if this will be recorded and available on demand after the event?
It will. If you register you'll receive a notification and link when it is available.
Thanks for the reply. Will I get the link even if I am a non-member?
Did you already do swirl within R?
Hi try [https://unibit.ai/docs#Api\_Content](https://unibit.ai/docs#Api_Content) You should get 500k free credits for just creating an account
Yes. It is open to all.
https://r4ds.had.co.nz/
Independent practice
I think those are two different things that you should probably learn independently of one another. If you want to learn statistics, you should look for textbooks on the subject. Once you understand statistics, R is basically trivial to use as a statistical tool. If you're interested in less statistics and more data analysis, then u/paulusj8's suggestion of R for Data Science is a great place to start.
This one
I swirl in R and would like to up my stats game. Suggestions?
Datacamp has a beginners R course, gotta pay for more lesson but it‚Äôs a decent start
Work through this free book http://www-bcf.usc.edu/~gareth/ISL/
thanks, good advice! I just finished a course on statistics at my university, and im starting my masters on innovation sciences in sept. i figured it would be a very nice skill to know how to analyse data in R.
very lovely, thanks!!
The above answer by user - https://www.reddit.com/user/DatchPenguin works. Credits to him
Look for courses on www.edx.com
Agree, I read this book. Best place to learn from scratch.
Why not just do a for loop? It's not like you have thousands of elections, right? You could cook up something using apply() and aggegate(), but frankly I don't even think it's worth it.
If you have this data in a dataframe, then one approach is to make each calculation "step" a new column, adding columns as you go, until you can get your final result. As mentioned in another comment, there's no particular reason to avoid a for-loop... you have a small amount of data, so it won't matter. For calculating your means, you might look to the `aggregate` function, or take a look at `dplyr`. Just make sure as you calculate means that you're including what you want (e.g. don't mix years, etc.).
Thanks for this. I'm still fairly new to R so I don't have the right instincts yet (as you can see from my resort to Excel). I'll have a crack at a for loop.
Thank you for your response. I'll give the for loop a crack. I'm definitely not familiar with aggregate so I will take a look as well. Thank you again.
As a general rule of thumb: "classical" programming concepts like if, for etc generate easy to read and reusable code. R-specific techniques liky apply, vectorization and such generate faster programs, but can be hard for a beginner to construct and to debug. But with the amount of data we're talking about here, you will not be able to see the speed-up anyway.
So I've had a Google of this and I am completely out of my depth. My knowledge of R is far too rudimentary for this at the present time. &amp;#x200B; Out of curiosity, how long would it take you or a competent R user to create the script necessary to do this? (I am trying to work out how much it would cost to pay someone to do it).
If you have the data ready in a decent format (like csv), I guess 10 minutes or so. &amp;#x200B; But I strongly recommend you do this yourself. Grab the standard manual from the R project homepage and go through the first 1-2 chapters. Lust learn R. It's worth it and you will not want to do things in excel ever again, even when they're perfectly suited for excel.
The problem is that I am on a rather tight schedule research wise. &amp;#x200B; How long would it take an intelligent lay person to do this, in your view? I am trying to work out whether it is worth doing now (or later when I have more time).
Do you mean Introduction to R found on the [https://cran.r-project.org/](https://cran.r-project.org/) website?
This bit of code tackles the first part of your problem, calculating raw nicheness score for each party within each year, and demonstrates how to do some key steps you'll need. The key ideas here are: * Define a function that calculates the score for a single party when it is given the data for 1 year. * Using `split()` to group your data by year. There are multiple ways to do this, others suggested `aggregate()` and looking into `dplyr`. * Applying that function across your split data. Again, there are multiple ways to do this, I used `sapply()` and `lapply()`, but each of these could be replaced with a `for` loop as others have suggested. # make some fake data parties &lt;- c("Left Party", "Social Democratic Labour Party", "Liberal People's Party", "Christian Democratic Community Party", "Moderate Coalition Party", "Centre Party", "Green Ecology Party") d &lt;- expand.grid(party = parties, year = seq(1994, 2006, by = 2)) set.seed(0) d$vote_pct &lt;- runif(nrow(d)) set.seed(0) d$dimensional_emph &lt;- runif(nrow(d), 0, 20) # given 1 years worth of data and a party name what is that party's raw nicheness score? raw_nichness_sc &lt;- function(year_df, party_name) { i &lt;- which(year_df$party == party_name) dim_by_vote &lt;- (year_df$dimensional_emph * year_df$vote_pct)[-i] total_vote &lt;- sum(year_df$vote_pct[-i]) sqrt( ( year_df$dimensional_emph[i] - (sum(dim_by_vote) / total_vote) ) ^ 2 ) } # how it works for a data frame of 1 years worth year_df &lt;- d[d$year == 1994, ] sapply(year_df$party, function(party_name) raw_nichness_sc(year_df, party_name)) # split the data by year d_split_by_year &lt;- split(d, d$year) # apply across the split data ds_with_rns &lt;- lapply(d_split_by_year, function(year_df) { year_df$rns &lt;- sapply(year_df$party, function(party_name) raw_nichness_sc(year_df, party_name)) year_df }) # stack (row bind) all of these data frames together combined_d &lt;- do.call("rbind", ds_with_rns) head(combined_d) Let me know if you have any questions on the code and I can try to explain what it's doing in more detail. I would suggest running each line yourself and examining the objects each line creates. With this to work off of you might be able to extend some of this code to complete the rest of your calculations. If not, let me know and I can assist further. :)
yes, that's what I started with back in the day.
I'll give it a read through. Seeing the code provided by [RWeThereYet17](https://www.reddit.com/user/RWeThereYet17/) has shown me that my knowledge of R is a lot weaker/poorer than I thought (and it was bad to begin with). I've also managed to get a copy of R In a Nutshell, which does something similar, with plenty of examples, so I'll have a crack at that also. &amp;#x200B; Thank you.
Bit of advice (correct me if I'm off base, though): if you're going to `for` loop through some data and append it to a data frame, it's generally better to preallocate size by first creating an empty df and putting the results of each loop into that.
I want to give a strong recommendation for `data.table`. You can `merge` A and B with `data.table`. You want to join on columns that can be keys and what you're left with are matches and non-matches. For example: merge(A, B, by = c("Column1", "Column2"), all = T) The argument `all = T` is the equivalent of a FULL OUTER JOIN. [Here is the merge documentation for data.table](https://www.rdocumentation.org/packages/data.table/versions/1.12.2/topics/merge)
Thank you so much for providing this. What it has demonstrated to me is that my knowledge is far too weak to be using the software for any research purpose and that needs to be corrected. I am taking [24benson](https://www.reddit.com/user/24benson/) up on their suggestion to spend a little time before hand learning the ropes - I've got a copy of an introduction to r textbook and will work through that before revisiting what you have thoughtfully provided here. Hopefully then I will understand how to apply that which you have provided. Thank you again.
The xlsx library can manipulate individual cells in existing Excel worksheets, xlsx imports Excel sheets as R data.frames so once the worksheet is imported just manipulate is as you would any data.frame in R.
Xlsx sucks as it requires java. Try openxlsx or readxl+writexl.
I said xlsx because that is what op said they use. What ever library op chooses to use they all import Excel worksheets into R as a data.frame and from that point on they are all pretty much the same allowing the user to manipulate individual cells of a Excel worksheets.
You are right. Missed that.
Aren't the plots the big problem? OP says the sheets have graphs on them. Do either `openxlsx` or `readxl`/`writexl` work with the plots?
No problem, I was not suggesting which library op uses to import in Excel files because to me they are all pretty much the same. For Windows users a lot of windows users already have java installed. Some of the other libraries require perl which very few Windows users have installed. So instead of suggesting which library to use I just went with the one op already stated that they use.
I know I can import individual cells or sheets, but can I then export it back and modify only that cell/sheet in Excel? &amp;#x200B; I mean, I want my final product to be that Excel sheet. &amp;#x200B; I don't know if I'm making myself clear.
Yes, fortunately I already have java installed.
I might not be understanding what you are asking in R you can manipulate any cell that had been imported since it is just a data.frame, are you asking how to export the file back to Excel?
Yesss, that's exactly what I mean. I didn't have any trouble importing data from Excel into R, but what I'm not being able to do is exporting it back into Excel. Modifying only that particular cell or sheet.
After you import your Excel worksheets into R and make you desired changes you can export that file back by using the function xlsx::write.xlsx() if you are using the xlsx library.
Also if what you are asking is to export that data but not overwrite other sheets then make sure that you specify a sheetName and use the parameter append=T. See the documentation for more info.
I have Java installed but I still get problems reading big files. If it works then its fine, but I try to use other packages which don't fail.
For a large amount of data, that's a great idea. But the OP has 700 rows and it looks like they're adding a few columns with each calculation. If the rows don't have to interact, then they might not even need to loop through the rows. But you're right in general that if you have a large amount of data, it's better to pre-allocate, or use another structure like `data.table`.
Couldn‚Äôt you just use mutate() (I believe from dplyr package) and just create a data frame with a mutate() column with the function ifelse(as.numeric(as.logical(grepl(one string), 1,0) | grepl(next string), 1,0) | grepl(last string),1,0)))? Possibly would require some =TRUEs in there. A bit lit but my point is that you should be able to easily make a new dataframe that ‚Äúcounts‚Äù the instance you‚Äôre looking for. Then if you want the study names then all you need is to filter for 1 or true and return that column.
Thank you for the reply any direction helps. This is my first week learning R.
Have you been using SAS beforehand? Looks very SASsy
Makes sense, the files I'm reading aren't really that big.
I think using your code you could just filter `data_dlt` using `counter_dlt` like this `data_dlt[counter_dlt &gt; 0, ]`. This should display the rows of `data_dlt` that have the key words. I wouldn't be a true R user if I didn't point out that functions in R are vectorized so rather than using a `for` loop you could do something like this: substrs &lt;- c("dlt", "dose limiting toxicity", "dose-limiting toxicity") regex &lt;- paste0("(", paste0(substrs, collapse = "|"), ")") title_detect &lt;- str_detect(tolower(data_dlt$Title), regex) outcome_measure_detect &lt;- str_detect(tolower(data_dlt$Outcome.Measures), regex) either_detect &lt;- title_detect | outcome_measure_detect data_dlt[either_detect, ] But since you mentioned you just started using R I wouldn't worry about it,`for` loops get the job done too.
A reproducible example is great - it allows more people to tinker with the problem. If we aggregate by year without rolling-up, we can append the total vote and dimensional emph by year to the original data frame. The ```ave``` function does this. d$total_vote_year &lt;- ave(d$vote_pct, d$year, FUN = sum) d$total_dim_year &lt;- ave(d$dimensional_emph * d$vote_pct, d$year, FUN = sum) Then, we can use @RWeThereYet's ```sqrt``` line but subtract out the party's contribution: d$total_richness &lt;- sqrt((d$dimensional_emph - ((d$total_dim_year - (d$dimensional_emph * d$vote_pct)) / (d$total_vote_year-d$vote_pct)))^2) With 3 lines, we did it! Alternatively, here's the dplyr way: library(dplyr) d%&gt;% group_by(year)%&gt;% mutate(total_vote_year = sum(vote_pct) , total_dim_year = sum(dimensional_emph * vote_pct))%&gt;% ungroup()%&gt;% mutate(richness = sqrt((dimensional_emph - ((total_dim_year - (dimensional_emph * vote_pct))/ (total_vote_year-vote_pct)))^2)) And here's the data.table way: library(data.table) dt&lt;- as.data.table(d) dt[, `:=`(total_vote_year = sum(vote_pct) , total_dim_year = sum(dimensional_emph * vote_pct)) , by = .(year)] dt[, richness := sqrt((dimensional_emph - ((total_dim_year - (dimensional_emph * vote_pct))/ (total_vote_year-vote_pct)))^2) ]
I agree on the ```apply()``` looking complicated at first, but vectorization makes it easier to transition from Excel. Meaning, for two vectors ```a``` and ```b```, this is easier on the eyes c = a + b Than for (i in 1:nrow(a)){ c[i] = a[I] + b[I] } Also see my vectorised solution using ```ave()``` towards the bottom. A ```for``` loop might look somewhat intimidating to account for grouping by election year.
Think you have the wrong sub. This is for R language, the programming language
Oof thanks. I‚Äôll move üòÇ
I'm assuming you mean 90% `NA` rather than `NULL`? Usually `NULL` in a vector/data frame column isn't possible (someone correct me if I'm wrong). Anyway, you could `sapply()` along the columns of your data frame (or write a `for` loop if you prefer that) and check for the `mean(is.na(column))` being greater than 0.9. Does that give enough of a hint?
Hello there, thank you for your answer. I am just a starter, to be honest, I am sure if I know the difference between null and NA. I have a data attributes such as \`comments\` section which could not be identified by computer. Most of the instances are leaving it empty. Thus, I think it was a null value. Thank you for your answer I am trying it now
I would suspect your "missing" data in your `comments` attribute is either represented as an `NA` or `""` (blank string). To test which it is I would suggest running: sum(is.na(data$comments)) # number of NA's in the comments column sum(data$comments == "") # number of blank strings in the comments column sum(is.null(data$comments)) # is suspect this will be 0, but I could be wrong Potentially both of these are greater than 0.
It looks like NA. Thank you for your help! [https://osmihelp.org/research](https://osmihelp.org/research) This is the data I am working with 2018 version is my dataset there is 123 attribute all of them are questions. I am not sure how to deal with that attribute names. I am trying to eliminate attributes with high NA values
\`\`\`for (i in c(1:123)){ # for loop for beanplot att &lt;- data\[,i\] if (mean([is.na](https://is.na)(att))&gt;0.9) { data.rNA &lt;- data\[,-\[i\]\] } }\`\`\` This is what I tried but, it is not working
Awesome, you'll always get better answers if you provide a way for others to replicate your results. Here is how I would handle it (I read in the csv as a data frame named `d`): na90_cols &lt;- sapply(d, function(column) mean(is.na(column)) &gt; 0.9) names(na90_cols)[na90_cols] # names of columns with over 90% NAs Does this code make sense, particularly the `sapply()`? Then from here you could use `na90_cols` to filter out the columns of `d`. Does that make sense?
I am trying to remove attributes with high NA values from the dataset. By using the first method suggested by you: for (i in c(1:123)){ # loop for 123 attributes att &lt;- data[,i] # assign variable to current attribute if (mean(is.na(att))&gt;0.9) { # test if attribute has NA value more than 90% data[,i] = NULL # if true remove attribute from the dataset } } But this is not working. I am getting this error Error in `[.data.frame`(data, , i) : undefined columns selected
The reason this isn't working is because you are reducing the number of columns on each iteration of the loop. So by the time i = 102 there are no longer 102 columns left to remove.
I guess for loop is a weak method in such case. Thank you for your help! I used the \`sapply\` method to eliminate NA values even though I do not understand that completely
Here is how to write the `sapply()` method in a `for` loop: na90_cols &lt;- c() for (i in 1:ncol(data)) { na90_cols &lt;- c(na90_cols, mean(is.na(data[, i])) &gt; 0.9) } colnames(data)[na90_cols] Does that clear it up?
Oh I just noticed there is misunderstanding. I am trying to split up the dataset into two pieces [dataset.NA](https://dataset.NA) = NA&gt;=0.9 dataset.notNA = NA&lt;0.9
I'm assuming we are creating these two data sets by selecting **columns** and not rows. dataset.NA &lt;- data[, na90_cols] dataset.notNA &lt;- data[, !na90_cols] So `dataset.NA` contains the **columns** from `data` that have over 90% `NA`'s and `dataset.notNA` contains the **columns** from `data` that have 90% or less `NA`'s.
That was exactly what was looking for. Thank you for your tireless help. Irrelevant from the question starting question. I understand if you do not answer. This action did not reduce the dimensions as I expected how can I start to analyze this data? or plot descriptive statistics, This is much more different than I worked before
No problem! I'm glad we got it! I guess this depends on what questions you are interested in or are trying to answer with this data. Think about things like why did you pick this data set in the first place? Who are the people that answered this survey (what's the distribution of gender, age, race, company size)? Do these demographics vary across tech vs not-tech companies? Do these demographics vary across self-employed vs not? Across company size? There are lots of questions you could ask, it's just a matter of what your goals are with the data and what interests you. For dealing with the columns of the data set that contain comments you could look into doing a sentiment analysis to categorize the comments down to something like "person thinks employer has positive attitude toward mental health" vs not. This [book](https://www.tidytextmining.com/index.html) looks very promising, try chapter 2. Some important columns that stick out to me are "Do you currently have a mental health disorder?" and "Have you ever been diagnosed with a mental health disorder?".
Until now, I just interpreted data. Looks like I must render some research questions before extracting information from that which I knew already, but when I saw a number of attributes I intuitively tried to reduce dimensions before questions. It looks like, I have to be picky about the parameters, by justification depending on the questions. This is an assignment for a data science course in school. I will read more before taking actions. Thank you for everything! and book recommendation \^\^
The loop can work fine, just go the other way `for i in c(123:1)` would make it so if you delete the 123rd column, the others still stay in place.
I mean if he wants you to learn R, don‚Äôt learn Python....
As to learning another language first to learn about computers... I would advocate doing that later when you don't have this deadline. Any time you spend now with other languages will simply make learning R more difficult. In particular you want to learn when not to use for loops, and most other languages won't help you do that. You can learn the basics of R in a few hours... and depending on your dedication become proficient in a a few weeks. However, there are techniques and math that may prevent you from really getting good with it until you are at least halfway through college (linear algebra, calculus). Although I think studying base R is very useful, working your way through the first half of R for Data Science will make you productive sooner. https://rstudio-education.github.io/hopr/ https://r4ds.had.co.nz/introduction.html
Look up DataCamp. You‚Äôll be hard-pressed to become proficient in two weeks... but that‚Äôs the best bet you have. Don‚Äôt learn Python if they didn‚Äôt ask you to. There‚Äôs also a 99% chance you mean JavaScript, which is very different than Java (and neither will accomplish what your professor wants you to use R for in a reasonable manner).
Thank you for that, I actually laughed out loud.
I had good success with the recommendations in this link: https://paulvanderlaken.com/2017/10/18/learn-r/ Get yourself as quickly as you feel comfortable though Step 5 here and you‚Äôll be golden.
[http://aejaffe.com/teaching.html](http://aejaffe.com/teaching.html)
How is calculus beneficial toward understanding R?
Forget learning about computers. R is made especially for your position. R is a language that doesn't make you worry about operating systems and all the other components full languages like Python and Java offer. R allows you to focus on transforming data into a meaningful insight. 1. Download RStudio and R (both free). 2. Go through basic R tutorial - goal: understand basic syntax of R. 3. Load a csv file into R. Transform into a ggplot chart. 4. More R tutorials. The rest will come with time. Good on you for trying to prepare. Good luck!
Take a look at case_when() from dplyr package
Store all objects in a list and loop trough it using lapply()
A lot of the statistical methods used in R are based on ideas from calculus. If you don't understand how the math behind them works then you will just be playing with black boxes, and you may just dig yourself into a computationally inefficient or inaccurate hole if you mis-use them.
Just worked on a similar problem this week, I'm far from a pro but here's how I addressed it: &amp;#x200B; \-Create a helper dataframe that has a column of values you want to procedurally step through (mine was a series of dates, but it sounds like yours would just be a column of numbers 1:25) &amp;#x200B; \- Call your helper dataframe to adjust which dataframe you are calling within your for loop by using sprintf(): `sprintf("time_period_%s", helperdf$helpercol[i]) &lt;- simulation(data[2,i]` &amp;#x200B; \- If you want, you can then also use that helper dataframe for record keeping of the process, by writing out a check for each step of the loop. &amp;#x200B; &amp;#x200B; It sounds like you might need to nest the for loops to do all your summing and graphing, but the concept should be about the same. &amp;#x200B; Hopefully that makes sense, if you think it would help I can provide my code. Mine used the helper dataframe to procedurally hit a website to download some datasets, then wrangle and save them for a project I'm working on.
Why don‚Äôt you just use some if-else blocks? This is probably a control flow issue that you can solve by just being patient and writing it out long form
Using plumber to build api used by mobile app?
Waiting to learn anything until you know all prerequisites is dumb. It's like saying you shouldn't pick up an instrument before taking a full year of music theory. No, you pick up that guitar, learn to play Wonderwall and annoy people at the beach. Later on you can understand harmonic scales and all that fun stuff. Programming in general is about working with loads of black boxes and just accepting things work. So pick up R, push some numbers into it and go wild. Who cares you can't explain all the proofs of why the function work.
Plumber is used for making Web API , can be used for mobile app ?
Mobile app can use this api
Ok üëç
That is a brilliant solution. Thanks for sharing
install.packages("swirl") library(swirl) swirl()
If you have no experience in programming. R isn't that difficult to learn at all. Don't waste your time on learning other languages when you are limited, just learn R. ggplot2/tidyverse most important libraries. The first link will definitely get you up to grips. https://r4ds.had.co.nz/ https://r-pkgs.org/
You could use arrays. time_periods &lt;- array(0, dim = c(10000, 300, 25) for (t in 1:25) { for (i in 1:300) { time_periods[, i, t] &lt;- simulation(data[t, i]) } }
Any good example pr sources you can direct us to? Or some gallery. Been working with R but in future would like to integrate it with mobile. Thanks in advance.
Thanks, but when I used this method and viewed the nested matrices their structure was 1x3,000,000 instead of 10,000 x 300 as I needed. Also how do you call a specific cell from a nested list? &amp;#x200B; I tried doing something like: list\[\[1\]\]\[1,2\] to access the cell in row 1, column 2 in the the first matrix.
Thank you for the thorough reply, I truly appreciate it. I am going to go ahead and try this andreport back.
Thanks, going to go educate myself on the topic of arrays.
r/learnrstats
An array with 3 dimensions is pretty much a bunch of matrices stacked next to each other. It's cool because it's easier to get slices in any dimension. The following will all return a matrix: Get the first row for all time periods and columns time_periods[1, , ] Get the all the time periods and rows for the 3rd field time_period[, 3, ] Get the 10th time period: time_period[, ,10] To me, this data set better models what you're doing - seeing what the data does across multiple time periods. Sure, with 25 matrices you can get the same information, but 1 array allows you to get there faster. This time the array is created with more information so that it's easier to follow. n_rows &lt;- 10000 n_cols &lt;- 300 n_periods &lt;- 25 time_periods &lt;- array(0 , dim = c(n_rows, n_cols, n_periods) , dimnames = list(NULL, paste0('field_', 1:n_cols), paste0("time_", 1:n_periods)) )
It would be great if you could share your code, it appears as if you cannot have a for-loop loop through dataframes. I have tried the following variations: for (i in 1:25){ for (j in 1:300){ eval(parse(paste0(sprintf("time_period_%s", i), sprintf("[,%s]", j)))) &lt;- simulation(data[i,j]) } } # I also tried: for (i in 1:25){ for (j in 1:300){ get(sprintf("time_period_%s", i))[,j] &lt;- simulation(data[i,j]) } } No matter what variations of that I do, I get the following error: "Target of assignment, expands to non-language object." I am glad I went through this because now I intuitively realize to use an array as suggested below.
Thanks a lot, trying it out now. Quick question about the last line of code, what is the purpose of NULL and the second paste? Also why are you only naming one of the dimensions of the array? Thanks!
You're going to do great! If you're a HS student with no prior experience, the professor you're working with probably does not have tremendously large expectations for your coding skills. It would have been nice if he had been more specific about what he meant be "familiarize yourself with R," but you're going to be fine. Here's what I would suggest: 1. Install R and R Studio 2. Work through some of swirl--an interactive course that happens inside R Studio--which you can do by typing this into RStudio once you've installed it: 1. install.packages("swirl") library("swirl") 3. Take a look at [this book](https://bookdown.org/ndphillips/YaRrr/installing-base-r-and-rstudio.html). You can probably get through that in 15-20 hours, which should be **more** than enough familiarity. Good luck!
[removed]
Honestly there are a few quick and easy (and cheap) Udemy courses on R as well. They're ultimately probably not as good as the books below, in particular Hadley's, but if you're more of a visual learner it can be easier to follow the videos. They are mostly based on quick hits, so you should be able to power through them. That being said, R is fantastic and deeper than it seems, so all the solutions below are worth your time as well, especially once you get an intro.
Okay this worked beautifully, thank you for the solution!
Not sure if the Swirl package is still being maintained, but I recommended it to a lot of people that had great things to say about it. I wish it had been around when I was learning r.
FWIW I did not advocate waiting. I do recommend patience.
The NULL is saying I don't want to name the first dimension. That is, the rownames are unnamed. I am naming two dimensions, the second and third dimensions. I just did it so that when you subsetted, it would be more descriptive. As an aside, I bet you could modify your ```simulation``| function to return a matrix. That would speed things up and eliminate the ```for (I in 1:300)``` loop. But it sounds like it doesn't take too long to calculate
Unpivoting the data (i.e., ```gather()```) and then a join condition is likely what you are looking for. If you provide a reproducible example that is simplified you might get more specific responses.
I learned R on my own too, and it wasn't too long ago ( less than a year ago ). I wouldn't say I am an expert at it, but I can look at pretty much any code out there, understand it and apply it if needed ( or find a cleaner one ). If you want a quick launch just go for [Hadley Wickham's book R for Data Science](https://r4ds.had.co.nz/). If will get you through all the basics, the mostly used packages, ways to approach problems, differences between methods and more.. Just make sure to have Rstudio open and play around with the code yourself.
A while ago someone made a shiny app that is accessed through an app that is essentially a browser. Currently you can install r to your phone and play around with it but not do much more than that. It should be theoretically possible to ship R with an app, write the gui with shiny and make the whole thing work but it's too much work that no one had tried yet
This means: you program your Android app with the same language as anyone else: Java, C++, etc. Then that app would make a request to some web server that runs R on the background, using plumber. If you want to run R on your phone, you might need more work. I've seen rooted Android devices running R on a terminal, so you probably can run R scripts on ARM processors, but I don't know the details. You can look that up.
I like arrays better for this, but this is how I would use ```lapply()```. Here you would generate a list of 25 matrices based on different time periods. I pre-allocate a matrix once in the dummy call. Due to how environments work, the ```dummy``` matrix doesn't actually change, that's why I can re-use it in the ```lapply``` without any side effects. dummy &lt;- matrix(, 10000, 300) time_periods &lt;- lapply(seq_len(25) , function (x) { for (i in 1:300) { dummy[, i] &lt;- simulation([x, i]) } } )
I don't know of how to do this with packages other than `xlsx`. But if you have experience with VBA/VBS, you could write a VBS script to make arbitrary edits to an Excel sheet and call that script from R. You can [pass data to the VBS call](https://stackoverflow.com/questions/45622497/how-to-run-a-vbs-script-from-r-while-passing-arguments-from-r-to-vbs) from R by using named arguments.
After you filter, pass the did to tally() and it will give you the count of your filtered dataframe
Sounds like you want to do a group by the categories/date and then do some type of aggregations (sum, count, mean, etc.) Something like this might do: https://datascience.stackexchange.com/questions/12078/how-to-group-by-multiple-columns-in-dataframe-using-r-and-do-aggregate-function
Here's my code, though now that I look at it I'm not positive it will resolve your issue. Hopefully it will, or at least get you a bit closer! &amp;#x200B; `for(i in 21:28) {` `## FILE DOWNLOAD` `# Download Location` `target &lt;- sprintf("`[`https://medicare.gov/download/NursingHomeCompare/%s/%s/QualityMsrMDS_%s.zip`](https://medicare.gov/download/NursingHomeCompare/%s/%s/QualityMsrMDS_%s.zip)`", date.record$Year[i], date.record$Month[i], date.record$Date[i])` `# Download File` `setwd("~/CMS Data")` `download.file(target, '`[`MDS-DL.zip`](https://MDS-DL.zip)`')` `# Unzip File` `unzip('`[`MDS-DL.zip`](https://MDS-DL.zip)`', exdir = 'MDS')` `# Delete Unnecessary File` `file.remove('`[`MDS-DL.zip`](https://MDS-DL.zip)`')` `# Bring File into Environment` `setwd("~/CMS Data/MDS")` `m.y &lt;- read.csv(sprintf("QualityMsrMDS_%s.csv", date.record$Date[i]))` `## RECORD KEEPING` `# Pare Down File Size` `m.y &lt;- m.y %&gt;% filter(STATE %in% c("OH", "TX", "CA") &amp; STAY_TYPE == "Short Stay")` `# Save Smaller File For Records and Future Use Cases` `setwd("~/CMS Data/MDS/Records")` `write.csv(m.y, file = sprintf("%s - MDS Record.csv", date.record$Date[i]))` `# Delete Unnecessary File` `setwd("~/CMS Data/MDS")` `file.remove(sprintf("QualityMsrMDS_%s.csv", date.record$Date[i]))` `## DATA PREP (probably can't share this part, and probably not relevant anyway)` `## DATA SAVE` `setwd("~/CMS Data/MDS")` `write.csv(MDS, file = sprintf("%s - MDS.csv", date.record$Date[i]))` `## WRITE RECORD` `date.record$MDS[i] &lt;- ymd(`[`Sys.Date`](https://Sys.Date)`())` `date.record$MDS_nrow[i] &lt;- nrow(MDS)` `setwd("~/CMS Data")` `write.csv(date.record, file = "daterecord.csv")` `}`
So to make your life simple. What you are returning with that conditional is a shortened data frame. So the number of rows of that data frame is the number of observations you see. So for the first problem, you could do the following. &amp;#x200B; \&gt;**x &lt;-** Bouldering\_raw.df %&gt;% filter(Date=="1/1" &amp; Vgrade=="B" &amp; Sent=="Y" &amp; Location=="Indoors" &amp; activity=1)' \&gt;nrow(x) &amp;#x200B; There are much more efficient and better ways to make this reproducible, but for a one-off ad-hock type, analysis this will work.
I should also mention that my helper table was structured like this: &amp;#x200B; |Date|Month|Year|MDS|MDS\_nrow| |:-|:-|:-|:-|:-| |February2017|February|2017|Date DL performed|Rows of data (check for data quality issues)|
I think you might be experiencing what I see happen to a lot of spreadsheet users. It's hard to move from using a spreadsheet to manipulate data into a code based approach. I think it has to do with 'seeing' the data in a spreadsheet and how you have to aggregate by a single row or column at a time. Basic code aggregations are closer to a pivot table, and piping is like making a pivot table from a pivot table from another pivot table... And so on. From a practical perspective, if you've ever tried to recreate someone else's spreadsheet, you know know how hard figuring out multi tab formulas and multiple pivot tables can be. Coded solutions are more transparent and reproducible, and that's not even considering all the automation and ML libraries available.
I‚Äôm assuming it‚Äôs not too difficult to then assign that output into a new variable, correct? And if I wanted to fill out that variable with the rest of the observations ranging from all dates, is there a way to do that easily? Thanks!
Thanks! I will check this out!
So I suggest that you play around by adding things step by step. What you will ultimately probably want is to use group\_by followed by tally() and then assign that output as a new dataframe. Or you could use summarize and pass in a function to a grouped dataframe. &amp;#x200B; Afterwards you can pass the entire thing to a new variable using the standard &lt;-
Also a bit of a dumb question, but when you say pass that output into tally() Would it just look like ... %&gt;% tally() I don‚Äôt think I‚Äôve yet encountered a situation where the contents within a function is left blank, but if piping does what I think it does, it makes intuitive sense for it to be blank. I‚Äôd test it out myself if I wasn‚Äôt away from my computer, so I appreciate the response. I think I had a mini eureka moment. Are you saying that rather than specifying a specific date such as 1/1 and repeating the same function for 1/2, 1/3, 1/4, etc, I can use group_by to get multiple outputs for the range of Dates 1/1-12/31? If so, that makes a lot of sense.
I agree with you wholeheartedly. I also think the formulas I use in the spreadsheet just make more intuitive sense because I can directly see what is being referenced and what function is being done. But, I think that‚Äôs partly just because of familiarity. Nested formulas are still a pain to interpret and the sooner I learn to use pipes, the better I think I‚Äôll be.
Correct on both counts. Pipe takes the output of one function and passes it as the input to the next function. Group-by essentially splits your dataset into multiple smaller data frames for each operation. If you group by date you will get tallies for each date. try something simple like df %&gt;% group_by(date) %&gt;% tally() and just look at the results.
Sweet! That helps a bunch. I‚Äôll have to play around with it once I‚Äôm in front of my computer. If I were to try and apply an operation to the output of two different piped arguments, say, dividing the output of one by that of the other, would I have to assign each to a local variable or is it possible to do it all in one argument? (I hope my usage of ‚Äúargument‚Äù is right here) A &lt;- argument with pipes B &lt;- other argument with pipes Then do A/B?
 my_list &lt;- lapply(seq_along(my_list), function(i) { my_list[[i]]$record_id &lt;- i return(my_list) })
I think `gather()` from the `tidyr` package is the easiest way to do this.
I think to make `separate_rows()` work you would have to paste on your own separator to each column. d &lt;- data.frame( Area_1 = rep("Apple Dog", 5), Area_2 = "Bee Giraffe", Area_3 = "Cricket Ant", Data = 0.05 ) d$Area_1 &lt;- paste0(d$Area_1, "#") d$Area_2 &lt;- paste0(d$Area_2, "#") d$Area_3 &lt;- paste0("#", d$Area_3) separate_rows(d, -Data, sep = "#")
I couldn't get `gather()` to do exactly what I needed, but `melt()` from the `reshape` package worked: #transpose and group by ID (df = original dataset) df1 &lt;-melt(df,id=c('ID')) #order by ID df2 &lt;- df1[order(df1$ID),]
That works as well but, just for reference, the gather approach should have gone something like: gather(df, key = "Name", value = "Value", -RecID)
awesome, thanks.
Honestly, if performance concerns don't matter, I would just do a for loop, perform the operation you need, and rbind the results into a data frame. There are more efficient ways using apply functions, but if you don't need it then I wouldn't
I'm sorry if I'm misunderstanding what you're saying, but it doesn't sound like you want separate rows at all. It sounds like you just want to create a df that is rows 1 to 6 of the original and another that is rows 7 and down?
I would say that, while not official, the melt method is now socially deprecated for the Tidyverse gather()/spread() alternatives.
I never got the melt thing, buy the tidyverse option has been easy to use. ++
Honest question - Why do you want to do this? Would love to know.
It's super kludgy - lot of LISP-y things thrown in on top of some other stuff.
It's a bit kludgy and not fully-baked but quite powerful when you add all the libraries. It's a lot like Python in that respect. It particularly shines in how well it does statistical analysis, and often, the latest techniques will be made available in R libraries before you'll see them anywhere else. This is because the statistician/researchers developing those methods are using R.
Yeah I also think it's very kludgy... on the other hand for specific tasks the tidyverse allows a kind of expressiveness and readability that I haven't found in many other languages (mainly for SQL-like analytics and visualization). I personally avoid all 'general purpose' programming in R and only use it for statistics and data exploration.
It is, but the package ecosystem makes everything so much nicer.
It's kludgy++ (I'll get my coat.)
This might be due to my inexperience with R but I've had to write a lot of messy code to convert data types from one to another, because none of the libraries I was using didn't accept data in even close to the same format. If types were a bit more standardized I wonder if the kludginess would minimize.
Instead of a loop, vectorize. Try: map_dfr(your_data_vector, get_stats) If you're tied to a loop, try: df &lt;- data.frame(a=numeric(), b=numeric(), c=character(), stringsAsFactors = F) # or whatever the column names and data types are for(i in 1:nrow(your_data_vector)) { df[i,] &lt;- get_stats(your_data_vector[i]) }
Base R is a bit awkward in my experience but I'll back up what others are saying that if you use the tidyverse it is way nicer.
What package does map_dfr require? The documentation says dplyr but even after installing and calling the package, I get an error which says could not find map_dfr I tried the second method, and that gives me an error which says "Must specify a video ID" I'm attaching my code below: ``YT_Info &lt;- data.frame(index=integer(), id=character(), viewCount=integer(), likeCount=integer(), dislikeCount=integer(), favouriteCount=integer(), commentCount=integer()) for(i in 1:nrow(VID)) { YT_info[i,] &lt;- get_stats(video_id=VID[i]) }`` In this case, VID is the data vector where I have all video Ids,
It requires the purrr-package.
It requires the purrr-package.
 [https://www.rdocumentation.org/packages/purrr/versions/0.2.5/topics/map](https://www.rdocumentation.org/packages/purrr/versions/0.2.5/topics/map) &amp;#x200B; it states that dplyr is dependency. but the function itself is from purrr.
If you've installed it in the past, you may be running into this issue. https://stackoverflow.com/questions/32932354/how-to-install-the-libraryreadr If you have never installed it before, I seem to recall that might be a package that requires you to download Rtools online before you can use. I could be wrong about that though as it has been years since I've dealt with it.
This was helpful https://www.johndcook.com/blog/r_language_for_programmers/
Op is running openBSD so they don't need Rtools. Rtools is only needed for windows installs.
Op, do you have mutiple versions of R installed? I'm not sure if openBSD allows multiple/slotted versions of R to be installed simultaneously.
Good call
If you use R as a conventional imperative programming language it is gonna *suuuuck*. If you use it as a functional programming language with metaprogramming/macros it's half decent (if we ignore the lack of a proper module system,which packages aren't). R is essentially an alternative syntax for Scheme with an expansive pack of builtin functions for data processing and vectorised operators. This mind set is best taught in [*R for data science*](https://r4ds.had.co.nz/) by Grolemund &amp; Wickham, and in [*Advanced R (2nd ed)*](https://adv-r.hadley.nz/) by Wickham. Forget most other R books, they teach a clunky subset of R that's essentially a bad C.
Ok - what does that mean for a lay user like me?
I'm not very proficient at using purrr functions but why do you have that tildae before the call to readxl?
I will look into it.
No, I don't have multiple versions of R installed. I only installed R 3.5.3 on my machine.
In `purrr`'s `map()` functions that is a shortened syntax for writing an anonymous function. The line could be rewritten to be `test &lt;- files %&gt;% map_df(function(file) read_xls(file))`.
I learned something new
Are the files .xlsx or .xls files? If the files are .xlsx this could be the reason why `read_excel()` in your `for` loop works and `read_xls()` in your `map_df()` fails.
Depending on the structure of your `all_files` object you might be able to use `do.call("rbind", all_files)`. You might have to change `all_files` to be something like the below code to get the exact results you want: all_files &lt;- lapply(file_list, function(file) { txt &lt;- pdf_text(file) data.frame(File = file, Page = seq_along(txt), Text = txt) }) result &lt;- do.call("rbind", all_files)
 Do.call(rbind,lapply(filelist,read.xls)) Is another way.
It's hard to know exactly what might be causing the problem, but one weird thing that sticks out to me is that `Q` inside of `renderHighchart()` is reactive when it doesn't need to be. I would try making `Q` non-reactive and then using it with just `Q` rather than `Q()` within `hcmap()`.
test &lt;- files %&gt;% map\_df(\~read\_xls(.x)) should work I think
If I remove the parentheses from Q() within hcmap(), then I get the error: no applicable method for 'mutate\_' applied to an object of class "c('reactiveExpr', 'reactive')" &amp;#x200B; If I remove it completely from the reactive formula, the shiny app breaks completely.
Does it look like this? I would expect this to still work, but I could be wrong. output$plot1 = renderHighchart({ #creates the plot to go in the mainPanel ## filter data frame for use in plot Q &lt;- filter(data, OWNER == req(input$OWNER), PRODUCT == req(input$PRODUCT), BRAND == req(input$BRAND)) hcmap("countries/us/us-all-all", data = Q, value = "value", joinBy = c("hc-key"), allAreas = FALSE, name = "Owner Territory", dataLabels = list(enabled = TRUE, format = "{point.COUNTY}"), tooltip = list(valueDecimals = 1, valueSuffix = "%"), pointFormat = "County: {point.NAME}&lt;br/&gt;{point.SHARE}") %&gt;% hc_colorAxis(minColor = '#F7FCF5', maxColor = '#005A32') %&gt;% hc_mapNavigation(enabled = TRUE) %&gt;% hc_title(text = "Owner Territory") })
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rstudio] ["Error: Evaluation Error" message when shiny app loading?](https://www.reddit.com/r/RStudio/comments/bzd0f8/error_evaluation_error_message_when_shiny_app/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
&gt;filter(data, OWNER == req(input$OWNER), PRODUCT == req(input$PRODUCT), BRAND == req(input$BRAND)) &amp;#x200B; Ah, so this does work, however I'm still getting the error message.
Darn, I guess I'm not sure what's causing it, sorry. My only other suggestion would be to move all of your `selectInput()` elements up into your `ui`. It doesn't look like they need to rendered by the server. This could solve it, but I'm not positive. Debugging a shiny app is difficult without a reprex.
Oh how the apply functions have overcome for loops for me! Especially since i learned for loops first!
Say I have a data frame with 20 columns. I need to apply a specific rule to each column and see if it passes or not. For example, I need to test than none of the columns have non-alphanumeric characters or that they meet certain length requirements. It's easier to test one column than 20 columns.
Ya the code runs, The fade and shrink didnt have the same effect as geom_point. Not sure why
The same exact thing but with `geom_point()`? Because when I do that they look the same - I see no difference in either with or without the fade/shrink
Sorry I cant provide code at the moment since I dont have access to it from my phone. I have done something like this using the purrr package and using the map function. I also found these links: https://stackoverflow.com/q/16944956/6108905 https://stackoverflow.com/q/25258562/6108905
To make it work the way you have it I think the apply line would have to be `df$pval &lt;- apply(df, 1, function(row) give.pvalue(row[1], row[2], row[3], row[4]))`.
Ok, I found an example of what you mean. I've had a good play around and can't work out the exact problem, although it seems to be related to the `group` aesthetic. I also found the github tracker issue you created for this problem, and although it occurs when using `gganimate`, the actual code is in the `tweenr` package, it may be worth noting that in your open issue. I may try and debug the issue a bit further but I'm afraid digging into the source code of the packages is a bit beyond my `R` capabilities
What have you tried? Are your dates in Date format?
I don't know any VBA, but I'll give it a look. Thanks!
I tried making a ggplot but because The Data goes from JAN2008-MAY2019 it just looks terrible. I wanted to hopefully get the data points plotted and the y-axis have a range but that didnt work. I'll attach a image of what my graph looks like. [https://imgur.com/XBfje2f](https://imgur.com/XBfje2f) My dates are not in Date Format, since I am using the notation that is required i.e. JAN2008, FEB2008, etc.
Can you share the code you used and the first few lines of your dataframe? It's helpful to know what you did exactly.
Thanks a lot for your help man, really appreciate it. You dont have to go through the pain of going to source code and debugging :)
thank you! is it easy to put into words why my syntax was making it barf? I was hoping not to have to hardcode numerical indices.
When you apply across the rows of a matrix. The "x" that gets passed into the function is a vector that is the current row. So `apply(df, 1, FUN, ...)` is essentially doing: for (i in 1:nrow(df)) { FUN(df[i, ], ...) # dots are further parameters to FUN } So your syntax is sort of like doing: for (i in 1:nrow(df)) { give.pvalue(df[i, ], x1 = "x_1", x2 = "x_2", n1 = "n_1", n2 = "n_2") } This is why you get the unused argument error. If you don't want to hardcode indices you could do `df$pval &lt;- apply(df, 1, function(row) give.pvalue(row["x_1"], row["x_2"], row["n_1"], row["n_2"]))`. This works because `row` is a named vector. Alternatively, this solution might make more sense to you, `df$pval &lt;- mapply(give.pvalue, df$x_1, df$x_2, df$n_1, df$n_2)`.
this was my code: VMT\_2008\_to\_Present &lt;- read\_csv("C:/Users/nzouine/Desktop/VMT\_2008\_to\_Present.csv") View(VMT\_2008\_to\_Present) &amp;#x200B; year &lt;- VMT\_2008\_to\_Present$YEAR #assigns variable year to column YEAR totalInterstate &lt;- VMT\_2008\_to\_Present$TOTAL\_INTERSTATE #assigns variable totalInterstate to TOTAL\_INTERSATE totalPrimary &lt;- VMT\_2008\_to\_Present$TOTAL\_PRIMARY ruralInterstate &lt;- VMT\_2008\_to\_Present$RUR\_INT ruralPrimary &lt;- VMT\_2008\_to\_Present$RUR\_PRI ruralSecondary &lt;- VMT\_2008\_to\_Present$RUR\_SEC municipalInterstate &lt;- VMT\_2008\_to\_Present$MUNI\_INT municipalPrimary &lt;- VMT\_2008\_to\_Present$MUNI\_PRI municipalStreet &lt;- VMT\_2008\_to\_Present$MUNI\_STR &amp;#x200B; &amp;#x200B; p1 &lt;- ggplot() + geom\_line(aes(y = totalInterstate, x = year), data = VMT\_2008\_to\_Present)
The last price for Micron Technology, Inc. (Nasdaq: MU) was **$34.85** (as of 02:35 PM EST on Jun 11, 2019) The 52 week high is **$61.85** and 52 week low is **$28.39** Price action (weekly and monthly): **Weekly:** MU made a weekly high of **$35.40** and a low of **$32.47** (for the week ending on Jun 07, 2019) **Monthly:** MU made a monthly high of **$43.38** and a low of **$32.17** (for the month of May 2019) ^^I ^^am ^^a ^^new ^^bot ^^and ^^I'm ^^still ^^improving, ^^you ^^can ^^provide ^^feedback ^^by ^^DMing ^^me ^^your ^^suggestions!
So you don't need to assign each of your columns to a separate variable. You simply give ggplot your data, and tell it all the aesthetics (for example, which columns should it plot on each axis). geom\_line() tells ggplot what type of plots you want it to plot. This is called grammar of graphics and imo it's a very neat system for plotting. I highly recommend you do some reading on it as you use R and ggplot. &amp;#x200B; Instead of your code, you need something like this: p1 &lt;- ggplot(data = VMT_2008_to_Present) + geom_line(aes(y = TOTAL_INTERSTATE, x = YEAR)) Now it's likely that this still won't work. This is because your YEAR column has dates in a format that ggplot can't recognize. If you use `str(VMT_2008_to_Present)` you can see the data format for each of your column. For ggplot to be able to plot date automatically, that column needs to be in Date format. [`as.Date`](https://as.Date)`()` is the way to convert a string formatted like a date to an actual Date format in R. However, this function requires that your date has year, month, and day components. Since the dates in your data do not have days, you have basically two options: 1. Use `as.yearmon()` from `zoo` package to convert your "JAN2008" string to a yearmon class and then use [`as.Date`](https://as.Date)`()` to convert yearmon class to a Date format recognizable by ggplot. 2. Paste an arbitrary day (say, 01) to the beginning of your date strings (make it like "01JAN2008") and then use [`as.Date`](https://as.Date)`(string,format("%d%b%Y"))` to convert it to Date format. You may have to change your locale. See help document for more information. &amp;#x200B; So evetually you need something like this: # Format conversion library(zoo) VMT_2008_to_Present$YEAR(as.Date(as.yearmon(VMT_2008_to_Present$YEAR))) # Plotting p1 &lt;- ggplot(data = VMT_2008_to_Present) + geom_line(aes(y = TOTAL_INTERSTATE, x = YEAR)) # Add additional formatting to only include year and month on x axis: p1 &lt;- p1 + scale_x_date(date_labels = "%b%y", date_breaks = "1 month") You can change "%b%y" for date\_labels to use different date formatting.
Wow, thank you so much. I‚Äôm a beginner to R, so this helps so much! Thank you.
Eh no worries. I'm kind of intrigued as to why/how to make this work now, but after tracing the root in the problem all I can figure is something odd is happening with the internal frame-by-frame dataframes that `gganimate` is using. Hopefully they get back to your github issue at some point. I see someone already helped you crack the Greek symbols.
I'm not sure if it's a typo, but you shouldn't be defining empty aes in ggplot call, when you're defining it in your geom. So there should be an ending parentheses after the ggplot call, before the + to the geom.
As others said, `map_dfr` is in the purrr package. Your index is capitalized in the get\_stats function (I instead of i). Does it work if you make it lower case?
I would put the group_by above the calculatuon and then use a summarize function to find the means of each group. I assume you are using dplyr.
I think this is what you're looking for: db &lt;- dB %&gt;% group_by(home_team) %&gt;% summarize(avg_angle = mean(launch_angle))
That worked! Except apparently every stadium has an average launch angle of NA, so time to drop those rows. Thank you so much!
Sure! FYI If you include na.rm = TRUE in the mean() funcion, it'll drop those rows for you on the fly.
Awesome!! Thank you so much, kind stranger.
I'm not sure what you need it for (homework?) But this walks you through the process of making annual rainfall plots. https://wcrice.github.io/2019-06-06-water-year-plots/
I would like to add another column to the files before they get appended, is there any way to accomplish this with your line of code?
Is there a specific library I need loaded? I can't seem to get this to work on my machine.
This works well also, is there any way to manipulate the files before they are appended to each other? I would like to add another column before they all get appended.
Both read\_xls and read\_excel are capable of reading in xls files.
Sure, are you familiar with the apply family of functions? I‚Äôm on my phone so this might have some errors, but you can break it down a little more... Lapply takes a list, and then runs a function for each element. So, while I put just ‚Äúread.xls‚Äù, you can expand this more. Let‚Äôs say you want to add a column that lists the file that data came from... lapply(filelist, function(x){ Df &lt;- read.xls(x) Df$original.file &lt;- x Df } Now if you just run that, it‚Äôll go through, run everything, but only give you the last result. I‚Äôm not a huge expert on Do.call, but it can be very useful here; essentially it is acting like apply, but with more complicated objects. It is saying ‚Äútake the outputs of this lapply function and apply rbind‚Äù. I can break that down more when I am at my computer; does this help though?
I think that was just a typo here, it doesn‚Äôt work
Sigh. Always preallocate if you want to avoid performance hits. The easy way to do this is to use functional approaches such as `map_dfr` or `do.call( rbind, lapply( your_data_vector, get_stats ) )` but if you are going to roll your own for loop then a minor change in setting up DF will make a big difference: DF &lt;- data.frame(a=numeric(length(your_data_vector)), b=numeric(length(your_data_vector)), c=character(length(your_data_vector)), stringsAsFactors = FALSE) # or whatever the column names and data types are for(i in seq_along(your_data_vector)) { DF[i,] &lt;- get_stats(your_data_vector[i]) }
Cool, this allows me to add/change stuff but does not seem to bind the rows from the different files together like your first one. Thanks for the explanation of the lappy functions, it's definitely something I need to start looking into.
Something like this would probably work. test &lt;- files %&gt;% map_df(function(file) { df &lt;- read_xls(file) df$new_col &lt;- 1 # add new columns df # return df })
Yes if it isn‚Äôt wrapped in do.call() with rbind, it won‚Äôt compile it all into the same data frame. If you have different columns, that may interfere as well!
Maybe length instead of nrow? I was using a data frame but vectors don't have an nrow property. Is it throwing an error?
You should check to make sure you are not conducting "multiple testing" without adjustment. If not, the following code is an example of how to conduct an independent Hypothesis Test for a difference in Proportions (assuming you are testing for a difference between x\_1/n\_1 and x\_2/n\_2). Instead of using pmap to map 4 inputs into this function, I nested into list-columns to make it more clear what is happening. "Test" is the name of each test where each row is an independent test. "x" is a vector of length 2 indicating the counts of successes for two different proportions and "n" is also a vector of length 2 indicating the counts of trials corresponding to "x". I save a column called "Results" with the output object of prop.test() and then save a column called "pval" with the p value for that test. Finally, I show how easily it scales up from two independent tests to 100 independent tests. &amp;#x200B; library(tidyverse) library(broom) tibble(Test = c("Test 1", "Test 2"), x\_1 = c(546,557), x\_2 = c(599,1204), n\_1 = c(5014,5128), n\_2 = c(5015,8123) ) %&gt;% nest(-Test) %&gt;% transmute(Test, x = map(data, \~ c(.$x\_1, .$x\_2)), n = map(data, \~ c(.$n\_1, .$n\_2)), Results = map2(.x = x, .y = n, .f = \~ prop.test(.x, .y)), pval = map\_dbl(Results, \~.x$p.value %&gt;% round(11)) ) \# Scaled up to many tests tibble(Test = paste0("Test ", 1:100), x\_1 = rnorm(100, 500, 15), x\_2 = rnorm(100, 1000, 30) + rnorm(100, 50, 5), n\_1 = rnorm(100, 5000, 150), n\_2 = rnorm(100, 8000, 200) ) %&gt;% nest(-Test) %&gt;% transmute(Test, x = map(data, \~ c(.$x\_1, .$x\_2)), n = map(data, \~ c(.$n\_1, .$n\_2)), Results = map2(.x = x, .y = n, .f = \~ prop.test(.x, .y)), pval = map\_dbl(Results, \~.x$p.value %&gt;% round(11)) )
did you recieve this error message when you installed/compiled readr or when you tried to load it?
Hey, phatboye, just a quick heads-up: **recieve** is actually spelled **receive**. You can remember it by **e before i**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
I know this is a bot but isn't "receive" what I typed? I didn't edit my post so why am I getting this bot spam?
Programming in R always makes me feel like a complete idiot right up until it works, and then I feel like a genius.
Try to avoid custom operators when you can. They impede readability.
As an aside, you don't need to create a new column in a separate statement, it's redundant: The subsequent assignment creates the column.
For this, do the excel files need to have the same columnar structure?
What is that thing on the left?
Yes. I don‚Äôt think you technically need the same order of columns, but they need the same names / same number, otherwise rbind can run into problems. If you know they may have different structures, and you also know what all those cases are, you could write an extended function inside that lapply function which would parse and normalize your files to get the data frame you need.
LUL I interviewed at a job they only used Excel, spoke about my knowledge of R. They said what i do is great as i showed them some stuff. Didn't get the job... likely cause they dont use R
it's a russian package of office softwares (1C) https://1c.ru/eng/title.htm (I needed to google it by image search to know this lol)
laughs in Python
Python is god awful for data wrangling (at least compared to data.table), but much better if you need to build a pipeline / deep learning.
This chart is screwy. R is not an evolution of SQL or even Excel really. It's used for different things. Imagine giving the office secretary an R installation to keep track of the supplies for the office party! Same thing with Power BI Really... I don't like this chart and I disagree with the attitude that comes with it.
Laughs in Julia.
sorry edited with libraries needed
Is skipping Power BI allowed?
Exactly. I use GGPLOT2 for custom visuals in PBI, but still apples to oranges. These aren't progressions.
They all have different use cases. Not necessarily an evolution. Not sure about the one to the left though.
Also has a high chance of being an example of the [blub paradox](http://www.paulgraham.com/avg.html). Who knows what lies even further up the chain.
I like it but they updated it and it seems it breaks some of my old projects so now I'm second guessing my decision to use it. Are all your scripts for the same project? You could also consider writing your own R package. Edit: see here http://rmflight.github.io/posts/2014/07/analyses_as_packages.html
Maybe your "Gesamt" column is not numeric, but is a factor? You could check this by doing `class(Bundeslander$Gesamt)`.
data.table cryptic syntax is awful. Nothing beats SQL in data wrangling, period.
Hey, thanks for your reply, but the reply is numeric :/ Do you have any other ideas?
What if you try `Bundeslander$Gesamt[1] + Bundeslander$Gesamt[2]`. Does that give the number you expect?
Hmm, I get "NA" if I try it
I guess that's the answer as to why the plot isn't quite right. I'm not sure how to fix it though. It's weird to me that multiple "." are allowed in a numeric column. At first I assumed that maybe that had to do with some way you configured R to display numbers over 999, but maybe that is the issue.
Yeah, that might be it. Thanks alot for your help :)
Hey, DariusNaharis, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Oh well, I just removed the "." in the numbers and plotted it again and it worked :)
Awesome! You're welcome! Also, if you don't already know about it, the [RStudio IDE](https://www.rstudio.com/products/RStudio/) is great for coding in R. I definitely prefer it to the base R GUI.
Looks like you're using the dplyr and magrittr libraries already, so the full solution might look like: library(dplyr);library(magrittr);library(ggplot2) db &lt;- data.frame( home_team =rep(c("A","B"),5), launch_angle = 1:10) db %&lt;&gt;% group_by(home_team) %&gt;% summarise (Mean = mean(launch_angle, na.rm = TRUE)) ggplot(data = db, aes(x = home_team, y = Mean)) + geom_col() + labs(title = "Mean launch angle", x = "Home Team")
R‚Äôs syntax is confusing and non standardized. Python is at least object oriented. And you can do everything in Python you can do in R, plus anything else you‚Äôd use a high level language for - including full stack development.
Sigh. So sick of these tired old arguments. &gt; R‚Äôs syntax is confusing and non standardized...And you can do everything in Python you can do in R R is a tool for data analysis and data science and python is quite simply outclassed by R in every regard in that arena (data processing, visualization, manipulation). Python definitely can do all those things, it‚Äôs just objectively worse at all of them. &gt; Python is at least object oriented. R code itself is not OOP, but if you are trying to develop code, the underlying S3 is, and even better, you can just write C++ code with RCPP. &gt; full stack development This is essentially irrelevant when it comes to data analysis. I understand R is confusing/borderline heretical for all you people with a background in CS, but at least stick with comparisons that are relevant to the intended purpose of the software.
Comparing R and SQL really grinds my fucking gears. Along with other things, but that specifically pains me greatly.
What's the python equivalent of bioconductor?
&gt; R is a tool for data analysis and data science and python is quite simply outclassed by R in every regard in that arena (data processing, visualization, manipulation). Python definitely can do all those things, it‚Äôs just objectively worse at all of them. It looks like the Data Science community says otherwise: [https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db](https://towardsdatascience.com/the-most-in-demand-skills-for-data-scientists-4a4a8db896db) [https://dzone.com/articles/r-or-python-data-scientists-delight](https://dzone.com/articles/r-or-python-data-scientists-delight) [https://www.kdnuggets.com/2017/01/most-popular-language-machine-learning-data-science.html](https://www.kdnuggets.com/2017/01/most-popular-language-machine-learning-data-science.html) [https://bigdata-madesimple.com/top-8-programming-languages-every-data-scientist-should-master-in-2019/](https://bigdata-madesimple.com/top-8-programming-languages-every-data-scientist-should-master-in-2019/) [https://becominghuman.ai/top-programming-languages-a-data-scientist-must-master-in-2019-7101a8bc8e16](https://becominghuman.ai/top-programming-languages-a-data-scientist-must-master-in-2019-7101a8bc8e16)
More popular =! Better. Unless you think Budweiser is the best beer in the world. Data science is a very popular field for CS burnouts, who will naturally gravitate to python for all the reasons discussed above. That doesn‚Äôt make it a better tool for the job.
Well said
This is like comparing an F1 car to a pickup truck and a family hatchback, and saying it is the best car because it has the fastest acceleration. They are all cars but they out perform each other depending on the use case.
You didn't want that job, you'd be tearing your hair out trying to get anything done. I worked at places in the past where just getting R and packages approved for installation took months.
Saying "nothing beats SQL" is like saying "nothing beats using tools". It's not enough information.