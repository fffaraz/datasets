Thank you for the instructive remarks. I can't wait to see more of your works. Thank you so much for sharing them!
Nice. Note that random is not defined over ratios. Only reals/integers. I've converted your drawing code to LispWorks... the use of a macrolet shortens it a bit... (defun display-bg (pane xp yp w h) (graphics-ports:draw-rectangle pane xp yp w h :filled t :foreground (color:make-rgb 0 0 0 1)) (loop for s from (/ h 128) below (/ h 16) by (/ h 128) do (macrolet ((background (x y xa ya a) `(loop for ,x from 0 below w by s do (loop for ,y from 0 below h by s do (gp:with-graphics-state (pane :thickness (random (float (/ 512 128))) :foreground (color:make-rgb 0.5 0.5 0.5 (random ,a))) (when (zerop (random 32)) (let ((radius (1+ (random (float s))))) (gp:draw-arc pane (- ,x radius) (- ,y radius) (* 2 radius) (* 2 radius) 0 (* 2 pi)))) (gp:draw-line pane ,x ,y ,xa ,ya)))))) (background x y x (+ y s) 0.25) (background x y (+ x s) y 0.2))) (flet ((circle (x y radius &amp;optional (over 1) (rot 0)) (gp:draw-arc pane (- x radius) (- y radius) (* 2 radius) (* 2 radius) 0 (* 2 pi over) :foreground (color:make-rgb 1 1 1 (random 1.0)) :thickness (random (float (/ h 32))) :transform (gp:apply-rotation-around-point (gp:make-transform) rot x y )))) (loop for r from 0.0 below 1.0 by (random .05) do (circle (/ w 2) (/ h 2) (+ 1 (random (/ h 2.24))) r (random pi))))) 
Disclaimer: I'm writing this as someone who finds it hard to understand why people would continue to use sources for CL or anything else which are known to be wrong when better ones are available: in other words I can't really understand why people continue to read CLtL2 as reference material. A best selling textbook sells a bunch more than 3,000 copies, although they are rare: look at something like 'Numerical recipes in (Fortran|C|...)': I don't know how many copies that has sold but a great many. I worked for an academic publisher a long time ago and the print run for a monograph was typically 1,000, of which probably not all were sold, but if something got adopted as a first-year text in a bunch of places then it can sell tens of times that. *However* no-one is going to write a best-selling book on CL I should think (it would be interesting to know how many copies the Graham books sold). I think it's entertaining to compare CLtL2 with the spec from the point of view of OP's question. - CLtL2 has the same six functions in one entry, with, I think, rather less useful argument descriptions (does not show results); - it also tangles up the following text in the same way; - but there are significant errors and omissions in the following text. Combining the text as both the spec and CLtL2 do (and CLtL did before it) can not be avoided for a book which might physically exist, as flattening it would cause the page count to explode. The errors and omissions can, of course, be fixed, though it would be a labour of love doing so. And it would be a waste of time, I think: we *have* a spec, why duplicate it by hand? Wouldn't it be better to write a text which had *useful* stuff such as decriptions of style, libraries and so on and contained pointers to the spec? if what is wanted (which OP seemed to want) was a flattened spec, then surely starting *from the spec* would be better: that's what KMP did to make the hyperspec, after all. There are copyright issues but they were overcome before and can be again.
I started a new one last night, so I'll post a link when it's finished.
You might want something extremely minimal and lightweight, in which case neither clojure nor ABCL is really appropriate, and even some of the Schemes, which have to go to great lengths to support features like tail calls, might be heavy. Picolisp's strength is that it is _extremely simple_ and _extremely small_ even compared to R5RS, say. Certainly compared to ABCL or Clojure. There are some cases where that is what you want. 
The problem with this is that categorizing generally implies that you have to choose from a fixed list of categories, which tagging generally implies just a list of strings. A tagging system would suffer from immense repetition and tags that would be mostly project specific. A categorization system would require careful planning but would probably give the best results.
http://libgen.in 
502 Bad Gateway EDIT: Got a good link.
http://burtonsamograd.deviantart.com/art/X-2-512019341
Interesting approach. You are trying to redefine truth values. Cool.
If you want to do it recursively, think this way. if the list is null or length 1, then true. Otherwise Check if the first element is equal to the second, if not then false, if so recurse on the the cdr. [spoiler](/s "(defun constant-list-p (l) (or (null l) (and (consp l) (null (cdr l))) (and (equal (car l) (cadr l)) (constant-list-p (cdr l)))))" )
Very nice, deep, introspective work. Thank you for sharing!
I'm tempted by the shift parenthesis hack, but I have the habit of hitting the shift key to keep screen savers/locks at bay, as a key that doesn't otherwise do anything. On a related note, would anyone know whether a [Griffin iMate ADB-USB adapter](http://www.amazon.com/Griffin-2001-ADB-iMate-Universal-adapter/dp/B000067V8L) would allow me to use a Symbolics space cadet keyboard on a recent iMac? I have the Symbolics keyboard and the Symbolics-to-ADB adapter already, but would like to have some confidence that it could work before acquiring another adapter and messing about with configuration files for hours.
Well, you could always paypal me :-) You can get my email from my git repos, if you want to chat about this ...
This is what I choose to believe.
I don't completely understand the binaryess of EQUAL/equal? It seems to me that even when CL was being standardised it would have been reasonable to assume that a compiler could spot the cases with known and fixed numbers of arguments and optimise them, while falling back to the general case when the number of arguments was unknown at compile time. Indeed you can implement this in the language, for instance: (defun equal* (&amp;rest args) (if (null (cdr args)) t (let ((h (first args))) (every #'(lambda (e) (equal h e)) (rest args))))) (define-compiler-macro equal* (&amp;rest args) (let ((al (length args))) (if (&lt; al 2) 't (let* ((bindings (loop for e in args collect `(,(make-symbol "E") ,e))) (fn (car (first bindings)))) `(let ,bindings (and ,@(loop for b in (rest bindings) collect `(equal ,fn ,(cadr b))))))))) (I may not have thought hard enough about this but something like this will work I think.) However that may have been regarded as requiring unnecessary heroism from implementors (remembering that there are lots of functions which might want this behaviour). It may also be that no-one thought it would be useful. The call arguments limit (which isn't a limit on list length, it's a limit on how big stack frames can be, really) is more understandable: if you want to compile efficiently that kind of limit is, or was, something you might want to allow implementations to impose. Indeed C11 imposes just this kind of limit for instance (127 arguments in a function call). Languages like Scheme and so on which care more about correctness than efficiency tend not to impose limits like this, assuming that either being a bit slow is fine or that heroic compilers will be written. Those are reasonable tradeoffs: they are just not the only ones.
* (defun c(l) (or (endp (cdr l)) (and (equalp (first l) (second l)) (c (cdr l))))) 
A hyperlinked PDF version of CLTL2 is at http://daly.axiom-developer.org/clm.pdf This is derived from the sources at http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/lang/lisp/doc/cltl/0.html The only major changes are (0) the code has been rewritten for LaTeX2e (1) the table of contents includes links to functions/macros/etc (2) the obsolete sections were deleted The index for functions/macros/etc is complete. The full index is partially complete. 
yes, that's exactly what I meant.
Both of those libraries look like what I'm searching for, thank you!
All the back ends are compilers AFAIK.
I think this sort of misses the point. Very few people miss the hardware microcode support. We can live with writing a little bit of C or assembly for the Lisp runtime support and Intel engineers have given us processors with multiple cores and enormous caches that can do things pretty damn fast. What people miss is the fully integrated programming environment from the network drivers up, where the virtual memory system and the garbage collector were best buddies, where the compiler could tell the editor in which file every function was defined, and the window system could tell you what object was behind a displayed result. That's a major software problem, not a hardware tradeoff made obsolete by the progress of time. EDIT: the reason someone might be interested in the hardware is to reproduce something close enough that the original environment can be ported with minimal effort (ignoring intellectual property rights in the process, but we are just having fun here.) But then you realize a TCP stack from the 1990s and no SSH or SSL or modern distributed version control and no applications written in the past 20 years is missing a bit much.
Apply is just a function, not a special operator. You can define that function in lots of different ways, with more or less magic.
&gt; The concept of time sharing and interactivity lead to the development of more dynamic programming environments including the Read Eval Print Loop. I thought that was Peter L Deutsch's Lisp implementation, the second Lisp implementation, which had interactive input with read eval and print. Peter was very young (12-15) when he wrote that in the early 60s. It had nothing to do with 'time sharing'. The LISP REPL I/O loop was discovered independent of time sharing. See the description of PDP-1 Lisp from 1964: http://archive.computerhistory.org/resources/text/DEC/pdp-1/DEC.pdp_1.1964.102650371.pdf &gt; Turn up sense switch 5 for typewriter input; press CONTINUE; and the system enters a waiting loop which causes lamps to light in the program counter, looking like 1335. At this point , the LISP systerm is ready for manual typewriter input, as soon as the operator types, for example: (CAR (QUOTE (A B C D))) together with a final space at the end of the last right parenthesis, the computer takes control of the typewriter, impulses a carriage return, and then types out: A 
This is GREAT! I love how CEPL is coming along, I think is going to be great for people developing, but I can even see a greater potential for people learning OpenGL.
This one looks interesting. It comes with a good documentation.
The problem is the amount of magic you mention. For example, it seems that SBCL is implementing apply in terms of `multiple-value-call`. The Lisp I'm writing currently has first, rest, typeof, quote, if, eq and prepend operators, plus a method of defining anonymouse functions and macros and calling those. I don't think apply can be implemented in terms of those operators. Can it?
Wonderful, thank you. I saw you also share the generative code. Thanks! PS: Would you consider submitting upstream your optimisations to the `vecto` and `cl-vectors` llibraries? I am sure many people would benefit.
Very kind of you to say. However when I looked into the possibility I quickly run into issues. Firstly in my own knowledge but mainly is making it cleaner than a combination of cffi and [cl-autowraps' utilities](https://github.com/rpav/cl-autowrap/tree/master) (like plus-c). By sticking with the gl feature-set and somewhat ignoring structures with pointers I gave myself an easier job! The dream would be something that could take a solid cffi library like classimp and let me explore the c data as fludily as lisp data. Certainly possible and a great project, just not one I can put time into yet.
Is there an easy-ish way of getting the libs in a place where windows will use them? I started reading how to register them in \system32 | \system64 and ... ugh... windows. [this SO question in particular made me "ummm yea, i'm not doing all of that".](http://stackoverflow.com/questions/4897685/how-do-i-register-a-dll-file-on-windows-7-64-bit)
I'm pretty sure they only need to be in a place where cffi can find them, read the cffi docs 
This is an icky answer but...next to the sbcl.exe works :) [EDIT] There is certainly a good answer for this but I hate fighting windows so I previously have been doing this. I'm doing most of the development on linux anyway Also if anyone gets it running smoothly under sbcl on osx I will kiss their beautiful face!
&gt; I'm doing most of the development on linux As do I, normally. I have [lispstick](http://www.iqool.de/lispstick.html) for windows when I just can't be bothered to reboot over to linux. I do have a macbook pro all set up and configured, maybe I'll try that. Maybe a homebrew recipe for the deps might be a good thing to contribute, provided I get it working predictably. 
How long does it take? The LispWorks variant renders using the code I've posted in one second on my Mac mini. Display is on the screen in a 2500x1500 image.
It takes around 2.5 seconds on my Mac with LispWorks (using the code I posted in another comment) to render the graphics and to create a 327 MB tiff file 10800x7800. It takes the same time for a 6.8 MB jpg file. It uses the native platform graphics substrate for drawing and provides a cross-platform portable Lisp interface.
Sounds like the lispworks rendering is a lot faster than the vecto/cl-vectors. I've heard from the author of cl-vectors that some of it is suboptimal with regards to rendering and that algorithm changes might be needed to speed it up to the level of the antigrain rendering. That first one wasn't very complex though.
Clojure has `syntax-quote` which behaves similarly and supports unquoting and splicing. Also, in Clojure vectors, lists and hash-maps all implement the sequence abstraction so you know how they will be treated by the unquote-splicing. e.g. (let [a '(a b c) b '[1 2 3]] `(~@a * ~@b)) ;=&gt; (a b c * 1 2 3) 
I wonder why the tutorials don't use this (or at least the one tutorial I read). 
I learned C first and programmed a lot in it. Then I discovered CL. It did not seem too weird – in fact I was blown away by it's simple and extremely aesthetic syntax.
We are all just one mass of resonating particles. Swaying. Writhing. Seeking.
One of those cases where you don't get the pun in the software name until you recall that English speakers have weird ways of pronouncing foreign words.
And when your program is well-tested you can turn off run-time checking in performance critical sections.
Yes. KLambda is a simpler and smaller language than Scheme, and implementing a VM or even a native code compiler for it is less work (also easier to optimise).
Ah I finally got the point. Riegirlll, RieOldmann, RieYoungMann, RieMommm, RieDadddd, RieBabbby ... and Ryeboy? Riemann pronounces as /ˈʁiːman/ not /rάɪman/ (even in japanese katakana).
Riemann is an extremely useful and flexible tool. The API is consistent, well documented and simply a joy to use.
You might like https://github.com/Ralt/lxc-wrapper too then :)
Can anyone recommend an Alpha emulator for OS X that can run this?
I'm pretty sure he's running it on an actual Alpha over X11 and using OSX only as display... There are Alpha emulators available, but I haven't heard of anyone being able to actually run that. But there is Linux (amd64) version of OpenGenera available that you should easily be able to run, some hints here: https://github.com/ynniv/opengenera
I like plist for the feedback they give on the REPL, as the result of a function call for example. They're a little easier to read than an alist, their values are also easier to access than an alist. You need more manual steps to look into a hash table. &gt; (funcall-with-plist-result) ((:NAME "George" :AGE 29 :ALIVE T) (:NAME "Tina" :AGE 97 :ALIVE NIL)) vs &gt; (funcall-with-alist-result) (((NAME . "George") (AGE . 29) (ALIVE . T)) ((NAME . "Tina") (AGE . 97) (ALIVE . NIL))) vs &gt; (funcall-with-hash-table-result) #&lt;HASH-TABLE :TEST EQUAL :COUNT 2 {1004743383}&gt; &gt; (gethash "George" *) (:AGE 29 :ALIVE T) &gt; (maphash (lambda (k v) (format t "~S: ~S~%" k v)) **) "George": (:AGE 29 :ALIVE T) "Tina": (:AGE 97 :ALIVE NIL) ;-)
When I tested alists vs hash-tables, alists were faster then hash-tables for up til about 700-1000 elements. 
An alist is a cons-based dictionary; a plist is a cons-based object.
CL21 is an experimental project redesigning Common Lisp.
One of the original threads from a year ago: http://www.reddit.com/r/lisp/comments/1vtueu/cl21_common_lisp_in_the_21st_century/ Some good comments. Also, Eitaro (author of this) has some awesome projects written in Common Lisp (see his github for more).
I have tremendous respect for Eitaro. I think the goal is admirable. Common Lisp does need to evolve. Evolve with a sense of direction. For this to be successful the community needs to back this or another effort. Though it needs to keep it's roots but grow it's branches to the future.
Conclusion: «We should consider the future of the language, not only for ourselves but for the next generation.» I find those statements funny, as if programming languages can save the world :p
Eitaro is his first name - it's the same guy. Sorry should have used his github handle ;) well, I think it's his first (Japanese reverses the order maybe )
Use `read-from-string`. http://www.lispworks.com/documentation/HyperSpec/Body/f_rd_fro.htm
One function: (read-from-string "(+ 1 2)") Two functions: (read (make-string-input-stream "(+ 1 2)")) 
&gt; For this to be successful the community needs to back this or another effort. The community doesnt have the funding and manpower to even build a competitive library ecosystem. Or write some basic documentation. Or get a single implementation working across platforms. Stopping badly needed regular boring infrastructure work to do an "exciting" complete language vanity-rewrite sounds as unnecessary and suicidal as "lets rewrite netscape" or "i dont like scheme implementation No. 748, lets write No. 749". 
Why re-posting?
When you say "features from Lisp", is there a particular dialect/implementation that you have in mind? If you consider "Lisp" as a language family, I guess Clojure just sees itself as part of the Lisp-1 continuity, with all that implicit history. If you look up a random Scheme dialect, how much "mentioning [of] where these features are coming from, [...] discussion of prior art, [...] discussions about design differences" do you see there? 
I'm talking about this page and 'reader conditionals'. This is FEATURE-CASE in Lisp: https://github.com/slyrus/mcclim/blob/master/Apps/Scigraph/dwim/feature-case.lisp
You mean: this is FEATURE-CASE in McCLIM ("an implementation of the "Common Lisp Interface Manager CLIM II Specification") where it's defined inside Apps/Scigraph ("BBN's graphing package, currently not quite working"). 
I'm not sure what the problem is here. The subpage I linked specifically cites the Common Lisp approach to this problem under References. Are you suggesting that every page related to any idea from another Lisp be cited explicitly? Also, your original post was certainly not focused on reader conditionals. They were presented as an example of some trend you think you're seeing. &gt; continues to take and change features from Lisp I provided examples of Hickey and others citing other Lisps and participating in the Lisp community in an effort to show your broad claim was false.
What about ASSOC?
Genera is written in a mix of ZetaLisp and Symbolics Common Lisp. It also provides CLtL and ANSI CL compatibility. All inside one system. The ANSI CL compatibility is not very advanced. Remember: development of Open Genera stopped in the early 90s. Symbolics Common Lisp is huge and integrates with a lot of old stuff (like 'New Flavors').
There has been some recent discussion on why building a modern Lisp Machine is very likely to not yield benefits commensurate with the effort. http://yosefk.com/blog/the-high-level-cpu-challenge.html http://yosefk.com/blog/high-level-cpu-follow-up.html https://news.ycombinator.com/item?id=8859918 http://arrdem.com/2014/11/28/the_future_of_the_lispm/ The "turtles all the way down" aesthetic **_is_** highly appealing, but it hasn't been that way even for the "dominant" C-paradigm for a long time now.
Disclaimer: I'm heavily opinionated as I'm building my own Lisp-machine at the moment, just for fun, as a playground for my HDL infrastructure. Lisp may benefit from pretty much the same hardware support as would any other garbage-collected environment do. So, the main thing should be a hardware-assisted GC. Depending on what your preferred power vs. area vs. performance balance is, you may either end up implementing no more than hardware read and write barriers, or, at another extreme, a full dedicated GC core. In my current design there is a dedicated tiny marking core, a sweeping core and a large empty cell FIFO, so I can have 1 cons cell allocated per clock cycle. The main core should not be any different from the typical RISC, no reason to use any Lisp-specific instructions besides those needed for GC and allocation. Of course, having a tagged memory and having a dedicated set of registers for pointers and not allowing to store pointers in the general purpose registers should really help. The interesting part is cache. With a tagged memory and some statically placed hinting it is possible to smartly pre-fetch cons cells.
You mean the [chapter](http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-30.html#%_chap_5) on register machines ?
Yeah, I seem to recall that the die in http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-34.html was the result of a fully automated silicon compiler in Scheme, but I might be confabulating.
I love this story, really inspiring way of thinking. The ability to bend solutions to suit your situation is such a beautiful skill. We're so used to legacy mental frames (32/64bits), he uses 18.. old machines had different bit-width for many parts. We're not, at least I wasn't, taught to think correctly.
Check out reader macros. 
The Symbolics keyboard put ( and ) where [ and ] are usually found on a U.S. keyboard. You press shift to get [ and ]. Then { and } are shifted \ and | respectively. [Here’s a site](http://deskthority.net/keyboards-f2/the-lisp-keyboards-t98.html) with images of several different LispM keyboards.
I tried to write you a short response...and failed... so it became a blog post :| http://techsnuffle.com/2015/02/24/an-out-of-control-reddit-post/ But keep up the good hack!
This post is beautiful...
I saw that you forked OMeta! Cool system, huh?
I do use the numpad for most numerical entry, yep. The way I've got my ergodox set up is [like so](http://i.imgur.com/9j2ZkMn.png). While I hold down the orange key there, the right side of the keyboard becomes a numpad. On regular keyboards I did still use the numpad as well. A bit of an adjustment, but after a year I think it was worth it :)
You better tell us how to buy one of those ergodox keyboards, I've wanted one for a while and didn't get into that mass drop buy unfortunately.
Really seems it though I'm not sure which ometa for CL I should be using. I forked it to make it quicklisp loadable but I was over optimistic about my free time, guess I'll play with it eventually. Have you played with it much? I would love to see someone blog about this stuff so I have some code to steal!
What it means is that you don't have to wait for new compiler support in order to add new control flow constructs. Think of how Java just recently added multi-catch for exceptions. In Lisp, you could have written an alternative exception handler yourself. No need to wait!
I find the reduced accessibility of the number keys to be a good check on my coding style. If I'm typing too many numeric literals, it helps me realize that I really should be stashing them away in constants or local variables instead. Still, the permutation of the positions of the number keys is a bit gratuitous (with caps lock enabled, the numbers row goes like 7531902468). I've gotten used to it anyway.
No, that is defining functions, not a language. Are you confused by syntax vs. form?
Yeah, I could see that being ok with the numpad as an overlay.
http://www.xach.com/naggum/articles/3244633165773164@naggum.no.html also google groups have been nerfed for a while now. if you're doing naggum searches just use http://xach.com/naggum/articles/. i only wish it also included his .emacs articles.
i think his description is pretty exhaustive, you should just try building one yourself! there's a very similar layout https://bitbucket.org/cheater/us_split, i think their definition is in xkbd, though using xmodmap seems a lot more robust. fwiw i built his layout on mac using http://scripts.sil.org/cms/scripts/page.php?site_id=nrsi&amp;id=ukelele from the description in that post, used it for a bit and then reverted to minor symbolics style changes (there are links to photos of symbolics keyboards elsewhere in this thread)
memo-proc take a procedure. big-num is a non-function value, and it's not even well defined. You can't define a value recursively like that, for exactly the reason you point out.
My previous comment was made off the cuff; I didn't bother looking up the semantics of cons-stream before posting. As such, it is wrong. Scheme has a greedy evaluation strategy, meaning that all arguments to a function are evaluated before the function is called. If cons-stream were a function, the definition of big-num given would not work, as the value of big-num is not available in its definition. (Try evaluating (define ones (cons 1 ones)) and see what error you get). However, cons-stream is a special form which does not evaluate its arguments. It could be defined as a macro, which means that (cons-stream a b) gets expanded to (cons a (delay b)) before the evaluator ever sees it. delay is also a special form which one could define as a macro, so that (delay x) expands to (lambda () x) This makes your definition (define big-num (cons-stream (* 10000 10000) big-num)) equivalent to (define big-num (cons (* 10000 10000) (lambda () big-num))) Since the reference to big-num is inside a lambda form, this is a well defined definition. With the definition given above, my scheme environment (guile) produces the following printed representation: scheme@(guile-user)&gt; big-num $3 = (100000000 . #&lt;procedure 86e4000 at &lt;current input&gt;:28:38 ()&gt;) I think your confusion is due to a lack of understanding of the different evaluation strategies between functions and special forms. I won't pretend this is the best explanation, so please respond with any other questions, or requests for clarification where you feel this explanation is unclear.
Neat, though it is apparently hard-wired to emit code only for the [PIC12F683 chip](http://www.microchip.com/wwwproducts/Devices.aspx?dDocName=en010115).
i am venturing into micro controllers and this is one thought that has been on my mind on the feasibility of doing something similar. I know of the [6502 emulator](https://github.com/redline6561/cl-6502) but are there other projects that are doing something similar and in generalized manner? Also, would ECL give you any advantage above other implementations here?
I would suppose it more likely that the confusion is regarding a (e)dsl, vs an independent implementation?
Check out [zwizwa.be](http://zwizwa.be). Tom Schouten publishes some interesting stuff about his work with embedded systems using Racket, Concatenative languages, C, and Haskell (as I recall).
See also mailing list regarding new release candidate (in March).
&gt; Why not just quote the text like this?
No karma for self-posts? /cynicism... Love for all! :)
There's tons of info about delimited continuations [here](http://okmij.org/ftp/continuations/).
It's been done. See [this YouTube](https://www.youtube.com/watch?v=8SkdfdXWYaI)
I know, can't be arsed to explain more than that sry
Why not evil-mode?
Might be better as a self post if you want pointers.
emacs+evil-mode did not work for me. After using vim many many years my .vimrc is 88 lines long. Now including slimv setup. When I tried to configure emacs+evil-mode+slime to meet my expectations I ended up with 577 lines .emacs and still was not satisfied. There is so much in "default emacs" that needs to be changed to make it feel like right editor (vi). Yes, vimscript sucks, but you don't have to change much.
Data existed before code. Numbers. Columns and tables of numbers. Statistics, maybe. Measurements. Generated and processed by humans. If you're telling me that's code... that's a stretch. *Data* (as a word/concept) dates back to the 1600s, according to Merriam-Webster. *Code*... pretty recent.
In physics there is something known as the [three-body problem](http://en.wikipedia.org/wiki/Three-body_problem) which is relevant here because it has no algebraic solution: There is only an incremental numerical method i.e. only a "code" method of solving it. That is to say that the *universe itself* is not a big database, but the variable to a big function called as `for(z=0;;z=doit(z));` All that data humans have been collected? A few mere bits of that big function's output.
&gt; here is so much in "default emacs" that needs to be changed to make it feel like right editor (vi) You may want to look at [spacemacs](https://github.com/syl20bnr/spacemacs) that already did it for you.
&gt; I think that what I want is something that makes the REPL have an editor Emacs Lisp is exactly this way, and Emacs Lisp has Common Lisp extension that is suitable for CL beginner to learn the semantics of the language. Other Lisps come close with the integration. Vim and its minimal command line REPL is not a proper way to give REPL an editor.
You're calling an imperative function `doit` for its *side-effect* on a (global?) variable `z` (the universe)? In /r/lisp? Tsk tsk. At least give us *functional* style pseudo-code! ;-)
If you're only thinking about computers, sure. But what about DNA?
That's an interesting question. I don't find myself re-using a lot of code between projects -- or rather, the re-used code comes from libraries. I've been coding long enough to realize that in most cases someone else has done a better job of building a utility than I could. Occasionally I find myself adding a function or two, or doing a specialization -- e.g., I have a version of a matrix library that incorporates typing information that makes it run much faster under SBCL. Within a project, I find I tend to create a "semantic" layer between the data structures and the functionality, and that code ends up getting use a lot. So for example, in my code to [predict NCAA basketball games](http://netprophetblog.blogspot.com/), there's a layer for teams, games, etc., that's really just a thin interface to standard data representations. So I can do something like (home-team game).
An optimizing pattern matching macro i wrote is the one thing that I seem to consistently use over and over again.
&gt; Do you think God would ever use a C-derived language to create the universe? Certainly not :P
Do share!
Punch them.
If you didn't get it from the previous responses. This subreddit is about the family of programming languages called "Lisp", not the speech impediment. For what it's worth, try this response: "Yeah, I have a lisp, but you're the type of asshole that needs to mock otner people to feel better about yourself. Your disability is greater than mine."
I'm struggling to come up with cons. We have now four clients running production sites with sbcl behind nginx and have yet to have a problem. The application servers run quite nicely on 1g vps instances. $10 a month per app server and $20 for the database server is hard to beat. What we haven't done though is have any of these sites get any sort of 'real' traffic. They're almost all for small companies and are more b2b than e-commerce or the like. We don't ship binaries of the site though; we just pull updates and from git and restart the servers one by one. 
&gt;Vim and its minimal command line REPL is not a proper way to give REPL an editor. JFYI. These days slimv has pretty much the same capabilities as slime.
Well, by definition it's going to be something inside Emacs, as I use Emacs all day, every day, and other lisps only rarely (I'm sorry to say). I'm pretty certain that it's a little bit of code I wrote to run unit test for a particular function, module or package in the code I work in for my day job. I've got that bound to a few key bindings in emacs and I run it probably of hundreds of times a day. It's not big, or clever, but it's there, it's useful and it's used. 
I remember, it was a very nice, inspiring site. From your words it seems that it is gone? On a different note, if I may ask, what kind of framework is http://wigflip.com/ based on?
Thanks for the link, I may try it someday.
I agree. I used to have a crappy logging function. But now there are libraries. I used to have a printhash. But now I just use alexandria's hash-table-alist. This is only utility function I have that is actually useful and is used a fair amount: (defun singlep (l) (and (consp l) (null (rest l)))) 
Matching patterns on an object may be defined as a technique to reach into the constituents of the object (elements of a list, fields of a struct etc., and maybe arbitrary computed properties) as if with accessors, but using syntax that resembles construction of such an object. This allows: - to dispatch on the type or value of the object or its constituents AND bind variables to constituents, in one and the same expression - to dismiss special syntax for property accesses (e.g. dot operator, slot-value, head) and simplify (in some sense of simplicity) definition of languages where pattern matching is the only primitive method to decompose objects You can find examples with explanations in the README of [let-plus](https://github.com/tpapp/let-plus). Paul Graham described and implemented pattern matching in [On Lisp](http://www.paulgraham.com/onlisptext.html), section 18.4.
Can you tell me more about the desktop app? Sounds neat.
I sold it last year. wigflip.com uses tbnl (the precursor to hunchentoot). It doesn't use a framework, just templates and some really simple routing and a bunch of convenience functions.
I'm looking for a good name for the "library" concept of the dependency manager I'm developing: https://github.com/cldm/cldm. I'm using "library" and "deflibrary" for definition at the moment, but I'm looking for a better name. Poll here: http://goo.gl/A5AfEM
Microstore as in below ISA microcode ? how fine grained is it ?
Not fair :-)
Interesting. Compare http://www.reddit.com/r/lisp/comments/2xkrs2/whats_your_mostused_function_that_you_wrote/cp18ujh
Not completely related, but I remember running into this a while ago and thinking it was an interesting project. Particularly interesting was the idea of defining all versions of a library in the library file, associating a version number to a commit hash.
I've started to investigate qlot for this purpose. So far it seems promising.
Here's an example from Racket. It's based on Scheme, which provides no looping constructs because you're supposed to do all this extra typing so you can do it all with recursion, right? But Racket has macros that are *almost* as powerful as Common Lisp's macros. So I did a couple of days' worth of programming, and now you can install this package I made just by adding this to the top of your program: #lang racket (require (planet jphelps/loop)) ...and all of a sudden, Racket has a loop construct with the same syntax and functionality as Common Lisp's loop facility. A change as big as that in any other language requires a change to the compiler, but in Lisp you can implement it as a library. 
Invest the time and (possibly... see below) money in a speech therapist. They'll almost magically clear it right up. If you're still in school, live in the US, and depending on the state you live in, your school has a directory of therapists for these sorts of things and in a lot of cases the school will pay for it. Never thought being a lisp coder and having had a lisp as a kid would ever be useful, but here we are. 
Yes. Looks simpler, and maybe more effective than CLDM. But maybe CLDM could be useful too for some project, I don't know.
I used to run TBRSS on SBCL (on Linode) and had endless problems with the garbage collector. It kept running out of memory; when I started requesting GCs manually, it deadlocked; by the end, it was crashing once an hour with "GC invariant lost" errors. Since switching to Clozure everything has worked perfectly. 
Not necessarily Lisp-related, but yet, the pattern matching chapter in this book is really good: http://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/slpj-book-1987.pdf
It is a *code* indeed (as in *encoding*). And the algorithmic information theory got a lot to say on what the data in general and information in particular really is. 
http://qiita.com/m2ym/items/a2e00c79984750958e8a This is the article introducing optima written by its author. though written in japanese, it is a very good document that describes the concept. pattern matching is **not** merely a conditional expression + binding. simply put, according to the author, it is a way to integrate constructor syntax and desctructor syntax.
I wrote about this a while ago on comp.lang.lisp and others chimed in. https://groups.google.com/forum/#!topic/comp.lang.lisp/oSslA8mJdho Sorry for the google groups link - I wish there was a better archive of Usenet to use instead, but I haven't found one yet.
Lisp was always hidden not far in my life. - In high-school a friend got me to buy a HP-48G [RPL, symbolic diff] - I was deep down in CGI, very fond of a software called Mirai [Adv. Geom., natural UX] - College got me playing with Emacs [actual syntax understanding, programmability] At that point I didn't really knew what Lisp was, 4th year of college, fed with Java/UML and such I clicked at that Sexp thing in a class. I realized this was a common thread in all the things listed that I liked so much. For more programming related features, I never ever liked the expressions/statements split when learning about compilers for imperative languages. Nor the absurd syntax corner cases slowing you down some times. Having a mechanically sympathetic syntax with a trivial everything has a value hit all the strings. Later recursive logic and lambda calculus roots made it even worse.
That comp.lang.lisp thread is amazing.
Did you try to troubleshoot it with the SBCL devs? From what I recall you also had to stop using ironclad right? If so, maybe it was related? 
I have been awe-struck to learn * that data is code. * that conditions can be handled without unwinding the stack. * that CLOS is implemented in CLOS. * that Emacs is a (rather poor) implementation of a Lisp Machine.
I think I'm sticking to "library/deflibrary" for the moment as it seems to be acceptable. Thanks for the input!
In retrospect I probably should have. I never did get a clear idea of exactly what was going wrong, or where. I suppose it was a great education in low-level debugging, but at the time it was extremely frustrating. I was never able to reproduce the problems on a physical machine, so I suspect the problem was with SBCL on a virtual server rather than SBCL per se. When I switched to Clozure the only thing that had to change was using Ironclad for password hashing -- it was much too slow. But I still use Ironclad in other places, so I doubt the problem was with Ironclad either. For the record, I'm not saying "SBCL sucks"; I'm saying, "SBCL is great, but if you have problems with it, consider Clozure instead." I read somewhere that Reddit originally ran on SBCL, but they switched to Python because SBCL kept crashing. If someone had mentioned Clozure to them, perhaps things would have turned out differently...
fwiw, I didn't interpret your comment as SBCL sucks. I've had to switch from pypy back to cpython due to a gc bug on the postgresql binding (psycopg2) due to a year old gc bug that only manifested on the production machine. Shit happens, software is complicated.
Is he back or was it a long time ago ?
Well said.
Perhaps it would be nice to propagate comp.lang.lisp to Gmane? For example, this LispWorks list is also there: http://dir.gmane.org/gmane.lisp.lispworks.general (Not to be confrontational nor negative, but I know a few people that would prefer not to deal with Google in any way.)
Nice. Appreciate the effort, but just like the pseudo-C, this code is relying on a *side-effect* of a *global variable* change every moment.
 (defun iterate-history (history) "Iterate the next history from current one. If history is nil, create one." (if (null history) (iterate-history (make-big-bang)) (let ((current-moment (first history))) (iterate-history (cons (make-next-moment :from current-moment) history))))) Better now?
I was programming a GUI using OpenGL in Common Lisp. I had a DSP-programmer looking over my shoulder and another Lisper/polyglot there, too. The Lisper said, "I think these buttons need to be over a little bit more this way." I found the line in Emacs, changed it, hit the key binding to evaluate that top-level block, and "Blammo." The button moved while the GUI was still happily displaying it's running histogram. The DSP-guy almost fell over. "You do not have to restart?" My own similar awe-struck moment with incremental compilation was several months before that, but this was a particularly vivid example. I had a program displaying a chart that was updating 20 times a second. The program had buttons and other controls on the screen. I could move those around, get rid of them, change their labels, etc. without interrupting the chart at all.
The single biggest moment was when I wrote a chip-8 emulator and realized that I could pre-calculate most of the work in a given chip-8 program and 'compile' it to lisp code in advance. (it ended up not working too well because of indirect jumps)
Alas, it was a nice thought.
Had to move away from sbcl for jsonip.org as it would stop responding every few weeks. Clozure was faster and stays up forever without issue. Simple case though.
What are you trying to say?
That could be a concise definition of original LISP: "LISP" ((((I S) L I) (S T) P R) ((O C) E S) (S I) N G)
I thought it was a simplification of syntax ie (caar '((a b) (c d))) = (car (car '((a b) (c d)))) -&gt; a I hope I counted the parentese right . Lisp sucks without emacs. 
Cheater :)
https://github.com/orthecreedence/carrier =] HTTP client on fast-http, very similar. EDIT: Just realized Dexador is sync/threaded, so definitely a good alternative to Drakma.
 #1=(#1# . #1#)
seems useful in writing a test code.
Very nice, thank you, and thank you for the Dexador wiki page :) Does it support the same HTTP features as Drakma?
If you want an in-depth explanation, read Paradigms of Artificial Intelligence Programming by Norvig. In fact, read it anyway - it's great. It spends a fair amount of time getting you to build a pattern-matcher from the ground up, as you use it, abstract it out, and then morph it into part of a Prolog system. I've not found a better example of both the 'how' and the 'why'.
Sweet, I've been wanting something like this for a while. Keep up the great work.
Me, too. So, when I saw this on GitHub, I thought I'd share it here.
I think he means that ASSOC returns the cons containing the value, thus not as direct as plists. It's more convenient to write: (getf plist key) than: (cdr (assoc key alist)) 
See [CDR Coding](http://en.m.wikipedia.org/wiki/CDR_coding).
[**@ID_AA_Carmack**](https://twitter.com/ID_AA_Carmack): &gt;[2015-03-10 16:58:32 UTC](https://twitter.com/ID_AA_Carmack/status/575339970380062721) &gt;I am seriously considering pushing s\-expression data and an embedded Scheme for a VR experience file format. ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/2ykxj3%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://np.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
Years ago he tweet "Scheme has the challenge of getting your program to work." I'm glad he find the way for it if he's seriously considering so. ;-D
I'm on mobile but there's a relevant xkcd for that. 3-4 xkcds pertain directly to lisp, in fact
This is a pretty late response, but as a person who really likes common lisp but feels forced to learn/use clojure instead the problem doesn't seem to be lack of funding/manpower but rather lack of focus. The generally stated reason for not using common lisp is lack of a strong, unified community. There seems to be a rather large prevalence of not invented here (or perhaps not invented by me) syndrome. This combined with the ease of making your own dsls seems to lead to very little community focus on improving existing libraries. Idk how to fix this, or if it is possible, but it appears to be the issue.
[Image](http://imgs.xkcd.com/comics/lisp.jpg) **Title:** Lisp **Title-text:** We lost the documentation on quantum mechanics. You'll have to decode the regexes yourself. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/224#Explanation) **Stats:** This comic has been referenced 52 times, representing 0.0944% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_cpb8he9)
[Almost Greenspun's 10th rule!](http://c2.com/cgi/wiki?GreenspunsTenthRuleOfProgramming) &gt; Any sufficiently complicated C or Fortran program contains an ad-hoc, informally-specified, bug-ridden, slow implementation of half of CommonLisp. 
If you use asdf (which you should), (defun path (filename) (asdf:system-relative-pathname :your-system filename)) 
Ha. Can't please everyone.
Try `(defvar *base-pathname* #.(or *compile-file-truename* *load-truename*))` instead, as per Stassats' comment on Xach's blog. Explanation: in the version Xach posted above, if you place the `defvar` in a file that gets compiled to a fasl the `(or *load-truename* *compile-file-truename*)` isn't executed until the fasl is loaded, at which point `*compile-file-truename*` is nil and`*load-truename*` points to the location of the fasl. By executing at read time using `#.` Stassats' formulation executes while the `*compile-file-truename*` points to the source file and stores that path in the fasl. The `*load-truename*` is included for the case when the file is loaded directly. 
Graphics engine and all? Amazing.
My SBCL-backed [website](http://lisperator.net/) was on HN and r/programming for a few hours, and it's still on r/javascript. According to Google Analytics, the peak was 1400 requests in an hour. That probably doesn't count as serious traffic, but still I've seen plenty of PHP/Ruby/Python/Java websites crash under traffic from reddit/HN, so I'm quite happy about it, also given that all pages are dynamically served (no cache) and it uses a custom [template engine](http://lisperator.net/sytes/) which is interpreted. It was very responsive all the time, with CPU load under 5% and constant RAM usage (around 140M).
Very reminiscent of : &gt; “Lisp has jokingly been called "the most intelligent way to misuse a &gt; computer". I think that description is a great compliment because it &gt; transmits the full flavor of liberation: it has assisted a number of &gt; our most gifted fellow humans in thinking previously impossible &gt; thoughts.” — Edsger Dijkstra, CACM, 15:10 Brings this question though, do you think he could have done it in another language ? Maybe he reached the level where language/paradigms, Lisp or COBOL, don't really matter, or maybe lisp really brings a nice set of ideas that remove friction found elsewhere.
Screenshots of a video showing developpers IDE with a bit of racket/lisp: http://imgur.com/a/vm72A Can't find the URL, if I do I'll post it here. update: http://www.youtube.com/watch?v=LHvUjmlWRAI @ 10:30 -- The Making of Jak &amp; Daxter
I believe they emit both racket definitions and c++ code in the generated source files for tracking/debugging purposes.
I'm also having problems with the gitorious migration. The git urls just plain stopped working this past week.
I see your point ... but on the other end of the spectrum what would the ideal language be? I'd say it world be a DSL created specifically to describe your game. So maybe the question should be what's the best language for making DSL's? 
&gt; Andy Gavin can use Lisp to make games More like, "Andy Gavin can write a Lisp to use to make games while making games." You've got to be a unique talent to do that, but I don't think you've got to be on that level just to use a Lisp if you already had one designed for the purpose.
I think people with his skills make an excellent case for learning Lisp implementation in depth, through what they accomplish with Lisp. Andy Gavin obviously knows Lisp down to the bits.
that's probably racket, but performance might suffer if that's the case. what if lisp/scheme was just for game scripting, ai, etc. I forget the game company, but that was the case.
I was under the impression that the team behind Jak &amp; Daxter Collection gave up either retargeting or transpiling GOAL (I forget which) and ended writing a dynarec for it. Am I mistaken?
I must have skimmed too much through the video, did they say anything about that explicitely ?
From what I remember in either GOOL or GOAL the game was divided in an Engine/Core which which seldomly needed recompilation and the Game Logic, levels, etc on a Lisp which could be reloaded on the fly. This short feedback cycle was considered a crucial asset. You could mimic this entirely in C as this blog post shows[0]. Don't know much time you would spend fighting the language though. [0]: http://nullprogram.com/blog/2014/12/23/
I think you're right, reading though the yc post and someone mentioned uncharted which is what I think I was thinking
I think the custom for "Foo et al." citation is to give the last name of the first author, or (if you prefer) the full name of the first author. So you should use "Le Fessant et al." and "Balland et al." rather than "Fabrice et al." and "Émilie et al.".
thanks for pointing it out. I'm still not used to european naming convention and i make mistake, for the family name being the first name in japanese. things get more complecated when the name is in french, just as most people wont tell if asai is my family name or my given name. anyways, this is embarrassing, it seems i have to rename the repo. normally i am helped by bibtex a lot because it automatically abbreviate the citation by itself, but apparently it was not the case with a plain text...
&gt; it seems i have to rename the repo. If you are referring to the use of "emilie" in "emilie2006", I don't think you need to change this. Naming things after first names (or nickname, whatever) of other people is original but a long-recognized practice (eg. Haskell, Ada), and you may name things as you wish. My remark was only on the bibliographic reference, where I think respecting a standard format makes sense.
What's up with the source code in package files?
Agree, I'll separate them soon, for SBCL complains "package ... also exports following symbols" when reloading the file. 
Emulating the ps2 at full-speed will probably require a dynamic recompiler :-)
Original author here. I'm working on part 2 after taking a break for a while; needed to get some features added to MicroScheme (like apply) before I could proceed. Happy to answer any questions here, about the firmware or about microscheme or about the keyboard.
Maybe not directly related to Atreus Keyboard but to keyboard construction generally: I'm curious, are keyboard designs/layouts patented? Or to be specific, why no one tried to do remake of [Space-cadet](http://en.wikipedia.org/wiki/Space-cadet_keyboard) (or Lisp Machine) keyboard? I would really like to get hands on that, but with softer keys. I'm going to play with MicroScheme on Arduino but noticed there isn't support for macros. Any plans to add it, at least 'define-macro' should not be that hard, hopefully? Also, why directly go and generate assembly instead C, as gcc will be able to squeeze the size even more?
It should be defined by the defconst in the code. If you mean cl-format-fontify-defforms-alist, you have to load cl-format first.
MicroScheme code cannot run on PCs, so a macroexpander for macros written in MicroScheme is infeasible. However, it would be possible to use a macroexpander written in another Scheme implementation. This has been discussed here: https://github.com/ryansuchocki/microscheme/issues/8 A primitive proof-of-concept is demonstrated in the comments there, but an implementation that preserved line numbers would make debugging much nicer. The MicroScheme maintainer is very responsive; I bet if you wanted to add this via Guile or some such he would be happy to take a contribution to the tooling or documentation around MicroScheme (rather than to the MicroScheme compiler itself).
Sure thing; it's my job. (literally)
Note, I've pulled these projects down onto my laptop; if any of these software bits are orphaned, I would be *happy* to put them up on a github/bitbucket/etc and share them out.
I don't sell the PCB by itself, but I can sell a partial kit that leaves out switches (-$25) and/or caps (-$20). Just make a note in the bottom of the shipping field if you place an order.
hmm you only sell the wooden case right? if i wanted all acrylic I would have to source my own?
What's up with the dumb title on this submission?
The python source is available - was the lisp source ever made publicly available? 
Ah OK thanks. So it looks like ~100 with the wooden case and the pcb. Thanks.
Not that I know of, but there were a lot of CL reddit clones (linkit started that way) from around that time. Maybe interesting to you: http://web.archive.org/web/20060206133929/http://reddit.com/blog/2005/12/night-of-living-python.html
First link didn't work for me; Here: https://wukix.com/mocl
OpenMCL is hosted on Google Code, and hasn't had much activity the past few years. Is there anyone to migrate it?
Hi! Hey, thanks for the info. Sorry for not replying, been trying to stay off of reddit. I really appreciate it.
Is there a way to edit cliki from lisp? Fixing up deadlinks and keeping content up to date will be much easier if some of it can be automated.
sorry, i'll have to fix that. thanks.
What would make a _good_ page then? What about the list(s) of libraries and such? What would you suggest to make a better resource for people looking to start with CL?
&gt; and start updating content. With what?
Start by reading the content over on http://cliki.net/ They have a great page about libraries (that are pretty much all available in Quicklisp): http://cliki.net/current%20recommended%20libraries
Cheers for the link, I'll check it out. For me it's not about solving it with code but rather assisting myself with code. I can reduce the busywork for myself then it's less of a chore am I'm more likely to do it in my free time.
I'll look into those thanks!
Not really Quicklisp but I always advise people to develop in SLIME. It is one of the best development environments you'll ever use (Smalltalk's the other). Once you experience the joy of incremental development and Lisp restarts, you'll forever curse nearly everything that's come after it.
Cliki used to be better maintained but after Quicklisp replaced ASDF-INSTALL, it kind of fell into disarray, IMHO. In other words, it's all xach's fault! :-)
I always prized Cliki for its categorization... it's groupings of similar libraries (e.g. http://cliki.net/Cryptography http://cliki.net/Networking ). Since Quicklisp obsoleted ASDF-INSTALL, it's been sad that Cliki hasn't been kept up-to-date. It should really be automated though. Something like the proposal here: http://www.reddit.com/r/lisp/comments/2uun0n/quicklisp_categories/cobwcvj Or, even a quicklisp function that could be invoked in an ASDF file to associate categories with a system: (ql:set-system-categories :my-system :my-cat-1 :my-cat-2 ...) Right now, I end up doing a whole lot of `(ql:system-apropos "...")` (or searching quickdocs.org) and missing a whole bunch of things.
I haven't used Python but that sounds similar and Python is vaguely Lisp-like (Ruby's much closer I think). To be fair, modern debuggers are a lot better than they used to be. But it's still very cool to be able to tweak something in a traceback, peel back a few layers and restart the code. I just think you get a whole lot more out of debugging sessions and you're not sitting around waiting for things to recompile all the time.
Well, your post is really confusing. At first it implies that you don't know how to edit pages and that links are broken. As far as that is concerned, I am happy to create an account and start editing links to things. However, you seem to be really asking 'what makes a good wiki' and 'what types of content should be on there' as well. To that I do not know the answer. Some possibilities: * Add a hierarchical menu widget to the wiki so we can browser the wiki like a book. * Add a versioning mechanism to the whole book / chapters so that we can instantiate the entire wiki and work on an updated version whenever there is a majour release of something like quicklisp. * Add a section / chapter / category in the wiki with some integration with quicklisp so that whenever xach publishes a new release the sections gets automatically updated with basic information like package name, date of release, url to git repo, that kind of thing. As for me, I am just going to start editing bad links. Also, I am going to start editing out all the strikeouts by that lazy fucker that is too chickenshit to change the fucking link/text and instead strikes text out and replaces it with something sutpid like "link now 404". If I ever find you, hulk smash. You jackass.
This is a really intersting insight. It might be helpful to have Quicklisp publish a bunch of info every time Xach pushes out a new release. It might be interesting to have a live list of packages on the cliki.
That's not really a good answer. Serious long-term use (e.g., over a product release lifecycle) reveals things a one-month demo won't.
Interesting, thanks! I guess python can't be changed code wise while running like lisp can.
Yes, it is much better. One of the pain points of Python for interactive development is that once I've imported a module one has to go out of their way for changes to that module to register in the current session. Also as other people have mention if I redefine a class its instances are not 'updated'. Another thing that Slime provides is integration of the editor with the REPL, which is not just sending code to the REPL but it tags the objects it returns with information to associate the text shown in emacs with a live object. You don't have to use SLIME to use CL, but it is the most pleasant experience I've found to use CL (And pretty much any language).
It can, up to but not including file reloading. So library editing on the fly is so sad in Python.
I see you are back into programming games again dto! Looking forward to it. :-)
Ooh, haven't seen this! Cool! Good job dto.
Quicklisp will use the ASDF provided by implementation.
I like the game, I've enjoyed 2x0ng, never downloaded 3x0ng but thought it was a solid idea. This one looks fun. I think one way to improve it would be to get "powerups" trough the differnt levels, things like temporary "free tail electrification", or "velocity increase", or "tail length". Even have some "powerdowns" as obstacles you have to avoid. Of course those are just suggestions. 
&gt; "Lisp calls free() on your mind" Does that mean I will lose my mind? ;)
So I just need to stick to ASDF3 while writing my code?
Is this built on blocky? If so, the link from https://github.com/dto/blocky-old to http://blocky.io/ is dead. :-(
I've simply used Firefox's new "Print Edit" feature to remove all the surrounding junk and to dump the whole thing into a PDF.
Except that CL doesn't seem to be a "not small language" anymore.
Well, I didn't claim that! But it comes in handy sometimes.
It looks like the type inference is defaulting to single-float. You can coerce all of your numbers to double-float before adding and overflowing. (apply #'+ (mapcar (lambda (nr) (coerce nr 'double-float)) '(2457104 0.16666667 104.0 0.020833334 0.0))) I should add that you can add expected types and return types to functions but hopefully xach or one of those smart guys can explain how that would work.
This is a common problem in any programming language. It stems from floating point arithmetic being inexact. For single-precision floats (the default), there are about seven or eight significant decimal digits. You are experiencing what happens when you exceed that limit. All you have to do to fix this is use double-precision floats instead. They have about 15 or 16 significant decimal digits. Here is the code to change the default from single-precision to double-precision. (setf *read-default-float-format* 'double-float) You just have to run this before compiling the make-date-time procedure.
yes, definitely needs fleshing out a bit more, now that i've got the main mechanics working well. trail length increaser could be cool and is def on the list. Also i'm considering making this a multi-level thing where you explore a grid map of a malfunctioning nuclear base and need to defeat screens of robots before even reaching the reactor.
An alternative to `*r-d-f-f*` is to use the syntax for double floats, e.g. [`24d0`](http://www.lispworks.com/documentation/lw60/CLHS/Body/02_cbb.htm). You also have the option to not use floats. When you divide two integers in lisp you get a rational. Use `~f` instead of `~a` in the format string to print them like floats. Rationals should give the most precise result.
It's the reader's default settings, not type inference, that result in single-float computations.
Well, Common Lisp has even [facilities](http://www.lispworks.com/documentation/HyperSpec/Body/f_upda_1.htm) for upgrading data types/classes of a running application. I think you can redefine functions and methods in Python at runtime, but considering how classes are handled in Python (basically hash tables), automatically restructuring already existing data seamlessly while working interactively seems more difficult in Python. Systems like AllegroCache which implement a persistent database presumably use this to great effect, similar to MVCC in PostgreSQL/MGA in Firebird - I assume that in all three cases, these things can be done lazily.
I'm not sure that would work since the floats are read in at read time which is before compile time and load time. Instead you should use the following. #.(setf cl:*read-default-float-format* 'double-float) The #. reader macro causes code to be executed at read time.
stick to a quoted version + function. if you really want to avoid the quotation, use strings or keyword symbols, because they are self-evaluating and don't need to be quoted. However, there IS a direct, dirty solution w/o quoting. Make the function internal, e.g., (%is-a 'seat1 'seat), and write a thin wrapper macro which quotes its arguments and pass it to %is-a, e.g., (defmacro is-a (&amp;rest args) `(%is-a ,@(mapcar (lambda (x) `(quote ,x)) args)))) However, this would make your `is-a' no longer be able to be funcall'ed, mapcar'ed, applied, and so on. in such a case, you have to use %is-a. Also note that your implementation of is-a is not functional. For example, your `is-a` function is not creating an object, it is just setting the value of `(get type 'name)`. I see a design failure in your interface overall. First, `(is-a seat1 seat)` is just a data and need not be even a function. Just a quoted `'(is-a seat1 seat)` suffices, since it is a meaningful data. Here, quoting itself is creating a list representing the knowledge, since it is equivalent to `(list 'is-a 'seat1 'seat)`. In contrast `fact' can be seen as a construct that adds a new knowledge into the database, and therefore written in somewhat procedual manner.
The `(FACT (IS-A SEAT1 SEAT))` input should not be treated as code, but as data. Certain parts are syntax symbols (FACT and IS-A) and certain parts are data (SEAT, LEG, SEAT1, L1, L2, L3). You can read a form at a time, and pick it apart with FIRST, SECOND, REST, DESTRUCTURING-BIND, etc., look at the operator with CASE or COND, then do something with the data as symbol objects, rather than as variables in code to be evaluated. Ultimately, the code could have something like this inside: (case operator ... (is-a (destructuring-bind (name type) data &lt;save the name/type relationship&gt;)) ...) 
My main issue with using the quote on that input is that the sample input doesn't have them and I have no way (until tomorrow) to confirm if that would be an option. Edit: Let me just see if I'm following what you've said in your edit. Yes, I should specify that it's not really an object. I'm following my examples of property lists from a previous course. So, (is-a seat1 seat) itself can be treated as data, where fact would be the actual function. That still runs me into the issue of not having a quote there. I'm currently breaking down the (defmacro ) that you put above. Unfortunately, that's never something I was introduced to in the previous course. Definitely the most straightforward way I see to do it at this point is just treating fact as a function and the rest as data, but it's a matter of reading that in if I'm not expecting a quote there. But if I'm following this right, that would make the query section rather straightforward since you'd just be looking for that data. (query (connected l1 l2)) (query (connected l1 seat1)) (query value (is-a value seat)) 
What you need is Prolog. Fortunately, quicklisp has a package called "gambol" that will provide exactly that: (ql:quickload :gambol) Gambol will do exactly what you want once you understand its syntax. It provides two macros, *- and ??-, which act like FACT and QUERY. So you can fill out your rulebase as follows: (*- (is-a seat1 seat)) (*- (is-a l1 leg)) (*- (is-a l2 leg)) (*- (is-a l3 leg)) (*- (connected l1 seat1)) (*- (connected l2 seat1)) (*- (connected l3 seat1)) After which you can make your queries as follows: (??- (is-a ?value seat)) finds all correct values of ?value that match the rulebase: ?VALUE = SEAT1 NO AND relationships are implicit--just use the same variable (?value) in multiple forms: (??- (is-a ?value leg) (connected ?value l1) (connected ?value seat2)) has no correct answer for ?value, so it just returns NO But (??- (is-a ?value leg) (connected ?value seat1)) will find all solutions for ?value: ?VALUE = L1 ?VALUE = L2 ?VALUE = L3 NO To find just the first solution (L1), use solve-one (?-) instead: (?- (is-a ?value leg) (connected ?value seat1)) Gambol is a toy Prolog, though, and really isn't well-suited to a large semantic network (although it [has been used as one](https://github.com/crossbowerbt/prolog-talk)). If you want to learn more about constructing an efficient compiled Prolog in Common Lisp, check out Norvig's Paradigms of Artificial Intelligence Programming, which contains a chapter on it.
You do not need appeal to Prolog. What you are looking for is something called [fexpr](https://en.wikipedia.org/wiki/Fexpr). However, fexprs have a bad reputation for inducing a [trivial equational theory](http://www.ccs.neu.edu/home/wand/papers/fexprs.ps) of the language. For this reason, the Lisp community has ditched fexprs. Most Lisp dialects nowadays do not support it. There are two which do, namely, [newLisp](https://en.wikipedia.org/wiki/NewLISP) and [PicoLisp](https://en.wikipedia.org/wiki/Picolisp). But common opinion is that you should turn to macros or DSLs (Domain Specific Languages) as suggested by xach.
You can make `FACT` be a macro which quotes is argument and calls `FACT*` or `%FACT` or `FACT-FN` with it.
It's not an answer to your question, but I'm happy to take any excuse to link to the [Long, Painful History of Time](http://naggum.no/lugm-time.html). Worth a read for anyone implementing anything to do with time, especially in Common Lisp.
Sort of x-post from r/retrobattlestations... http://www.reddit.com/r/retrobattlestations/comments/301lhq/my_symbolics_lisp_machine_an_ai_workstation_from/
I am doing one hell of a lot of preventative maintenance to avoid such a thing happening. Put it this way, If I get my goal none of us will be around when that happens. This includes aggressive power supply overhauls and additional cooling in order to prevent issues. I also have a reasonable understanding of how to replace any of the parts that are not custom logic, but I'm on the lookout for any extra parts. Currently backing up all the PALs in the machine. Owning one of these is definitely not for the faint of heart or hardware challenged...
Sadly, one zillion times slower than an emulator on a run-of-the-mill 2015 laptop.
True, but the emulator does not emulate everything. There is a bunch of nice software which does not run on the emulator. For example not every graphics software runs over X11 - many won't - it might want to have a local frame buffer. What helps a bit is to use the emulator as a server. Getting stuff over the network from an emulator will be much faster than getting it locally from the slow disks of the 3620. I/O to those local disks is really slow...
I really hope this doesn't come off as discouraging or pretentious, but if there's one thing I'm good at it's keeping old computers running. There are enough nuances to keeping the 36xx series, and they are scarce enough that unless you have extensive experience restoring things like vintage minicomputers, and doing heavy logic-level troubleshooting when something goes wrong, I would not recommend owning a 36XX series machine. You need to be very comfortable working with hardware or the whole thing is doomed. I'd look out for an Ivory or Symbolics XL system, they are far less trouble. In order to get this one to last long-term I will have to figure out how to emulate an ESDI disk- this is not trivial and requires a lot of engineering. These ESDI disks cannot be copied (a la dd) by non-Symbolics controllers. The software to write a disk label on the 36XX is not easily available, leaving you no way to restore from a disk crash. I am working on ways around this but it isn't easy. The 3620 is a cool machine, but please understand it is an extreme undertaking to maintain, and rare enough that such a thing should not be taken lightly. Reverse engineering is practically required to keep it up, there is not much service information available publicly. They're really complex and densely packed with irreplaceable ASICs that run hot. You will have to at minimum replace every electrolytic capacitor in it as preventative maintenance shortly after acquiring the machine. The MacIvory is orders of magnitude more sustainable and great fun. They pop up on eBay every so often and are faster than the 36XX, but otherwise run the full Symbolics experience. They are a true lisp machine implementation, just on a card. They'll do networking, and run Genera just like their larger siblings. At the end of the day, they have fewer parts, run cooler, can use any SCSI hard drive, and install off a CD. Snag one of those and ping me if you want help setting it up. :) If a power supply dies in the ivory, you can actually find a spare on eBay any day. If one in the 36XX croaks, you're doing real repair. If you find yourself in Seattle, come have a go on it. :)
I will get on documenting, photographing, and sharing all I can shortly. :) I may consider giving out some remote logins as well if there is interest. I would highly advise against a 36XX for your first lisp system for reasons stated above. It's more work than I imagine raising a child would be, and it'd be wrong to give up after you get started. Look out for an XL series or a MacIvory and see my reasoning above. I collect and restore tons of vintage computers with the goal of preserving them for the future, and the 36xx is by far the most challenging machine I've found to ensure a future for. It's not something you can just keep to fire up every now and then, sadly. It's a commitment.
Well that rules me out, but as someone fascinated by this stuff do you think you will ever make videos of the hardware work you are doing? It's kinda rare to find people who have active experience of the nuances of these machines and having a conversational record of this will live even longer than your excellent efforts are going to let this machine last. [edit] Ah I hadn't read the comment further down. Good to hear there will be records and photos. A 'fly on the wall' video would still be exciting though :)
For example (&gt; (get test 'activation) max) If the GET form returns NIL, you'll have a problem. Personally I would recommend using Clozure CL on a Mac, SBCL with SLIME/Emacs, or similar. Compiling your code you will find a few more problems... The editor will also help indent your code -&gt; it will look much better... (defun choosewinner (list) (let ((max 0) (current (first list))) (mapc #'(lambda (test) (if (&gt; (get test 'activation) max) (setf current test max (get test 'activation)))) list) (activate current) (decay namelist) current)) This is ugly code: ;;Used to determine which function to call ;;Passes rest of list to appropriate function (defun fact (list) (cond ((equal (first list) 'is-a) (is (rest list))) ((equal (first list) 'connected) (connect (rest list))) ) ) Introduce accessors: (defun head (clause) (first clause)) (defun args (clause) (rest clause)) Use the accessors and replace the COND with a CASE: (defun fact (clause) (case (head list) (is-a (is (args clause))) (connected (connect (args clause))))) Alternatively use a macro to destructure the list: (defmacro with-clause (clause (head args) &amp;body body) `(destructuring-bind (,head &amp;rest ,args) ,clause ,@body)) (defun fact (clause) (with-clause clause (head args) (case head (is-a (is args)) (connected (connect args))))) 
If the security fuses aren't set, absolutely. You can dump and clone the device onto identical unprogrammed programmed stock, which is a nice safety blanket.
Yeah. In truth a friend and I are trying to make a museum and release content about all these machines to do exactly what you said- provide detailed visual, aesthetic, and conversational records that can help preserve a machine longer than the logic can possibly function. As enthusiasts, we have been frustrated by the lack of online content for many computers. The systems are also available by appointment, but free of course, in Seattle. I haven't emphasized the 'museum' aspect because I don't like counting chickens before they are hatched, but this is the plan. I'm an experienced photographer so sometime we will absolutely shoot this stuff with better quality than camera phone photos, and get some 1080p video walk thrus. 
I'm interested about certain keys on the keyboard, specifically the `select`, `local`, `network`, `function`, `escape`, `refresh`, `clear input`, `suspend`, `resume`, `abort`, and `complete`. I would appreciate if you could explain their function.
Select - This key is generally used for some kind of mode selection. For example, tapping select and pressing "L" goes to the lisp listener. Tapping select and pressing "D" goes to the document examiner. Clear Input - this clears the current line of input in it's entirety. Useful for if you made a typo or entered the wrong command. Abort - This key gets you out of trouble if you've opened the wrong message box or started doing the wrong task. It puts you back to the point from which the last command was launched without saving state. Refresh - This redraws the console, among other things. I'll do a video to get to the others shortly... 
Thanks, I'm looking forward to it!
Sounds wonderful, keep up the great work and I hope to come see it someday
That sounds pretty amazing. Definitely keep us posted!
In SBCL 1.2.8, this definitely works: (eval-when (:compile-toplevel :load-toplevel :execute) (setf cl:*read-default-float-format* 'double-float)) Strangely I've been unable to get your suggestion to work: #.(setf cl:*read-default-float-format* 'double-float) I either get "The variable DOUBLE-FLOAT is unbound.", or if I add another quote to make it ''double-float: Value of ''DOUBLE-FLOAT in (THE (MEMBER SHORT-FLOAT SINGLE-FLOAT DOUBLE-FLOAT LONG-FLOAT) ''DOUBLE-FLOAT) is 'DOUBLE-FLOAT, not a (MEMBER SHORT-FLOAT SINGLE-FLOAT DOUBLE-FLOAT LONG-FLOAT). Edit: I've tried qualifying the symbol as 'cl::double-float and adding (in-package :cl-user) before it but that makes no difference. Edit: Clisp, ecl and lispworks concur but just word it differently. I wonder what the trick is to specify a value that is acceptable in the reader macro.
ha ? what link did you post ? I saw this on twitter (reddit found neither the tweet nor the instagram account, otherwise I wouldn't have reposted it)
This one http://www.reddit.com/r/retrobattlestations/comments/301lhq/my_symbolics_lisp_machine_an_ai_workstation_from/
Ha !
Nice. That dude is collecting cool stuff. Love the 1989 PocketPC.
Hey man I'm flattered ether way! Thanks :)
That's a good puzzler. It is the return value of the `setf` that becomes an unbound variable. You could do, e.g. `'#.(setf cl:*read-default-float-format* 'double-float)`. Unlike the `eval-when`, I don't think that the `setf` in a file will change the `*r-d-f-f*` value if a cached fasl of the file is loaded. Lisp's separation of environments and evaluation times allows for some neat techniques but can be tricky.
I should look into doing this. I am currently using the MacIvory as a server but I could use VLM on Linux. Maybe I should just get an alpha box to do this.
Of course; thanks for figuring that one out. Though I don't need this at the moment, it's good to grok in case it bites me down the track.
University of Louisiana at Lafayette. Grad school is always open and there seems to be a reasonable amount of funding
Damn that was fast :) thanks i'll get reading
A bit of trivia: &gt; (You'll note that the x3j13 cleanup proposal that gave you the fact &gt; that \*READTABLE\* is bound by LOAD and COMPILE-FILE is called IN-SYNTAX. &gt; This might give you a clue about what I originally had asked the &gt; committee for and what they bargained me down to. Sigh...) -- Kent Pitman [[quote from c.l.l](https://groups.google.com/forum/#!original/comp.lang.lisp/HEYt5_FglhI/givbSb0IetkJ)], [[the cleanup issue](http://www.lispworks.com/documentation/HyperSpec/Issues/iss196_w.htm)]
http://xach.com/naggum/articles/notes.html
A few years ago (well, 2009), Ron Garret posted a full archive of comp.lang.lisp, and I saved it and have used it to generate the Erik Naggum archive and the Rob Warnock archive. I don't know of any source to get articles after his snapshot, though. I'd love to start maintaining a comp.lang.lisp archive. The roadblock for me is finding time to write/configure an NNTP client to periodically grab stuff. If you want, I can send you a link to the comp.lang.lisp archive file. It's about 200MB compressed.
That doesn't mention comp.lang.lisp specifically. Do you know, before I download an 8GB archive, if it's included?
&gt; Oh, can I please? It doesn't seem likely.
Come again? If I understand your tone correctly, you are suggesting that I — someone you haven't got a clue of who I am — aren't capable of something, is that a correct assumption?
I am open to being pleasantly surprised.
Aren't we all. Never expected such condescending tone from you though. Whatever, tis the interwebz.
Thank you, I would love to have that comp.lang.lisp archive file. If you would maintain a comp.lang.lisp archive, how useful it would be, not only as a deeper source of Lisp lore and wisdom. It would be kind of a beacon of continuity. It would benefit so many people, and not only from the younger generations.
These guys found a way https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;uact=8&amp;ved=0CDQQFjAD&amp;url=https%3A%2F%2Fwww.reddit.com%2Fr%2Flisp%2Fcomments%2F30eqne%2Fis_there_a_complanglisp_downloadable_archive%2F&amp;ei=n0kVVYjJHdjwaJ7PgoAP&amp;usg=AFQjCNHMVIERp_mU2-7VzrBEQZ1eMp2jng&amp;sig2=DAkkGOsCBu2eH5SuikltVQ&amp;bvm=bv.89381419,d.d2s
Excellent, thank you.
Thanks for the offer! I actually signed up with giganews for this project, but found that I didn't have the time to actually fetch and process the NNTP feed. Maybe someday!
I'm going, hope to see you there!
Yes, two: Dale (C++ and LLVM toolchain used for a Lisp systems lang with manual memory management): https://www.reddit.com/r/programming/comments/2ziscs/dale_a_gcless_sexpression_system_programming/ BitC: https://www.bitc-lang.org/ I have tried to compile the former recently on my Arch laptop, and I got an error and have you to debug open up a Github ticket. As for BitC, the website is bad and I am having trouble finding detail/source/code examples. I know it started out as an academic project, so hard to tell if it is being used. Maybe less up your alley, but still interesting to me: Rhine, another LLVM Lisp, but more dynamic and has garbage collection https://github.com/artagnon/rhine Pixie-lang, using PyPy's rpython subset of Python to write their own Clojuresque VM and stack, with an easier to use C FFI/Interop than Clojure/Java-JNI. https://github.com/pixie-lang/pixie I got the latter to compile on my laptop but have not had much time to use it yet. 
Interesting, although this doesn't look very homoiconic: (decl char |s[10]| "") I would have preferred: (decl s ([] char 10) "")
More-and-more hooks for compile time optimization. https://github.com/cl21/cl21/issues/64 Currently there is no standardized way to achieve this (other than writing a compiler macro by hands). Is there any SBCL internal hacks available?
Any opinion about, you know, the actual question about the mutated arguments labeling?
Sc does this: http://super.para.media.kyoto-u.ac.jp/~tasuku/pdfs/ilc2005.pdf 
Well to directly answer I would prefer (! arg) or possibly #!thing so it looks less like it's part of the name. The feel of it is weird though. It seems more like it belongs in a broader solution e.g. f#'s explicit 'let mutable'. Also you asked this question 4 hours ago so chill a bit, people will answer if/when they like.
L++ author here. You can do: (decl char (at s 10) "")
http://con.racket-lang.org/2013/ Naughty dog used Racket to generate c++ code for PS3 games.
Nice site. Also, Dynarchlib was seriously amazing at both functionality and performance. Those were days when JS engines weren't so fast and yet your framework was the smoothest library I had tried(and I had tried pretty much every major library out there).
I'm amazed by how well the SBCL team is run. This build has a fix for building sb-thread on OpenBSD which I reported about two weeks ago. Three days after reporting the bug a fix was in the trunk, followed by a patch a day later by the OpenBSD port manager to fix compiling the single threaded binary. Here we are less than a few weeks later and it's wrapped in an release, all the time with the original bug ticket being mindfully updated. Talk about a stellar first experience with the SBCL support community.
https://github.com/deplinenoise/c-amplify
Well, I submitted a bug (and a possible way to fix it) [more than a year ago](https://bugs.launchpad.net/sbcl/+bug/1267540) but it's still there... Although Windows support is experimental so it's understandable it's not top priority.
I'd like to give a lightning talk on it yeah. I think lightning talks are registered on the day so I haven't planned this with any organisers or anything. I've got a couple of things I deliberately haven't put on youtube so I can show something new :)
Like @Baggers below, I'd prefer the `!arg` to `arg!`, but either seems fine to me. Another approach might be: (deftype mutated (base-type) "An item of type BASE-TYPE which could be mutated." base-type) (declaim (ftype (function (t (mutated list)) list) delete)) (declaim (ftype (function (t list) list) remove)) But, unless your compiler's implementation of #'DESCRIBE for a function tells you about the types, that wouldn't really help.
telecommute?
I manage the Seattle Clojure users group (seajure) meetups, which are held at SecureOne, and I'd like to offer my endorsement, for whatever that's worth. A lot of the clojurians at SecureOne have been in the community for a long time and I am jealous of anyone that gets to work with them. They are too hush hush to tell me what exactly they are doing, but it sounds really exciting. Not sure what kind of spaceships, but I think we're definitely talking interstellar.
How do I upgrade SBCL? 
You say you don't want to use Java interop, then say you want great library support for various databases. Despite being on the JVM, Clojure really can look nothing like it. I program in Clojure for $DAYJOB and I find that being on the JVM is an absolutely amazing win. The libraries available are just superb. You can easily write small shim libraries which present Java with a functional interface. If you're still convincing yourself away from Clojure, choose Common Lisp/SBCL since it has a really good set of libraries available (Quicklisp) and a very decent implementation.
ASDF (which quicklisp uses) maintains a separate cache of compiled (fasl) files for each os/implementation/version $ ls ~/.cache/common-lisp ccl-1.10-f96-linux-x64 ccl-1.8-f95-linux-x64 ccl-1.9-f96-linux-x64 clisp-2.49-unix clisp-2.49-unix-x64 ecl-12.7.1-unknown-linux-x64 ecl-13.5.1-unknown-linux-x64 sbcl-1.1.13-linux-x64 sbcl-1.1.16-linux-x64 sbcl-1.2.1-linux-x64 sbcl-1.2.4-linux-x64 So ASDF will recompile quicklisp libraries and your code when you load/quickload a library in the new version. You can even have two different versions of SBCL running at the same time.
There's also the [Joxa language](http://joxa.org/), another lisp dialect built off of the Erlang VM / BEAM. Joxa and LFE made some different design decisions, for example Joxa is a lisp-1 and LFE is a lisp-2. [This post](http://blog.ericbmerritt.com/2012/02/21/differences-between-joxa-and-lfe.html) highlights the main differences between the two of them, so you can evaluate for yourself which appeals to you more.
Dammit man, I'm nervous enough about giving this talk already :D
The 'parsing' looks ugly. Mixes a lot of string operations into the code. It would be more 'lispy' if it separates the read phase from the eval phase.
Testify! the only other lisper I know uses it for work but isn’t interested in nerding out with it. I was surprised to see a couple of common lisp folks coming to the talk though. Also are you madsy on pouet? if so LISP DEMO! Make it happen :D
CLSQL supports ODBC (I am one of the maintainers). I have never tried running it against oracle (I dont use oracle generally), but would be happy to accept patches making it work better if you're up for jumping in the deep end ;)
My company sprung for a copy of this back in the 90's. I remember bouncing [Screamer](http://www.cliki.net/screamer) off it several times as an exercise.
So, was this meant to solicit comments, critiques, suggestions? Either way, welcome to Lisp.
In Common Lisp, a "byte" is adjacent bits within an integer. It need not be 8 of them. I like to use _octet_ when I mean an 8-bit byte. BOOLE is not commonly used any more. LOGXOR and LOGAND make it easier to define mapxor mapand, because you can do (mapcar #'logxor byte-a byte-b) and (mapcar #'logxor byte-a byte-b) instead of using a lambda.
Yes, those three. That's what I was after, haha. Sorry, I got so excited that I just posted without saying why. 
welcome. Try exercism.io lisp problems, or google's lisp koans, for a test oriented approach to getting you up to speed.
Probably because he's used Eclipse.
Probably because it's raining outside, and he didn't want to get wet? My biggest concern is that the short name for it will be ECL, which is already used…
Thanks for the kind reply! I'll get there eventually. The thing that pains me the most about Lisp is it is so beautiful, elegant, powerful, and simple, yet in the modern age its hard to get enough people to write code to really have a complete library set. Fragmentation of implementations and no easy to use text editor outside EMACS hurts. Thanks for maintaining a library that a lot of people need!
CitrusLizard has it right. I've used Eclipse. 
Glad to hear! 
CLOS does not even have direct support for 'protocols'. Where the earlier Flavors had support for class-based protocols: Moon/Weinreb: &gt; A base flavor is a flavor that defines a whole family of related flavors, all of which will have that base flavor as one of their components. Typically the base flavor includes things relevant to the whole family, such as instance variables, :required-methods and :required-instance-variables declarations, default methods for certain messages, :method-combination declarations, and documentation on the general protocols and conventions of the family. Some base flavors are complete and can be instantiated, but most are not instantiatable and merely serve as a base upon which to build other flavors. The base flavor for the foo family is often named basic-foo. There you got somehow explicit class-based protocols in a Lisp OO extension without generic functions... I would have wished for CLOS to have similar support. I think one of the big things missing for CLOS are explicit first-class protocols. It's nice to think in terms of protocols or document them, but I fear that is not enough. If I implement a protocol, I'm still on my own when it comes to checking it for correctness, completeness, etc. The original CLIM implementation came with some, but it was sketchy and undocumented.
Where does this fit in the Common Lisp taxonomy?
What if instances are a part of the *protocol*?
&gt; Now it turns out, macros don't get expanded within a defclass form. Wait, what? Can you be more specific here? With an example?
It sounds like 'defclass' is getting expanded before your macro gets expanded, leading to problems for you. What you might look into doing is creating a replacement for 'defclass' that first expands instances of your macros, and then invokes the original 'defclass' expander.
I personally like Paul Graham's approach to defining Lisp. Programming, axiomatized. Check out [his paper][pgd] for more information, it's quite interesting. [pgd]: http://www.paulgraham.com/diff.html
People are divided on the Scheme question. It certainly makes for an interesting case study. Traditionally, Lisps had separate namespaces for functions and variables. Schemers wanted to have only one namespace, it simplifies the language and looks better in print. But in short time, they realized that *name capture* problems inherent in the namespace unification made writing macros a lot more brittle in Scheme. Hygienic macro systems were invented to make Scheme macros robust again. To make these hygienic macro systems as capable and efficient as regular *defmacro* systems, syntax came to be represented by special syntax objects instead of nested-lists-of-symbols-and-literals. I think this was the point at which some stopped thinking of Scheme as a Lisp. 
I hadn't thought about being able to run arbitrary lisp programs, but that would definitely be a good feature. 
How does clojure's macro system work? AFAIK it's a Lisp-1
A reader hack. Symbols inside a backquote are treated differently from symbols outside backquote, such that your "manual" binding of a variable actually uses a symbol that is different from the one the macro expander inserted using backquote, even though at first glance they seemed to be the same symbol,
&gt; but obviously it is also proof-by-existence that there can be modern lisps that don't fullfill this list LOL
&gt; `scheme-eval` doesn't evaluate scheme, only some subset of scheme! If you are talking about core scheme (without macros), I'd like to hear what is missing in such an implementation.
I think this was miscommunication on my part - what I mean is that I plan on essentially writing a lisp macro (a rather large one) that takes a source file in my "new" language and writes a C source file which is then compiled into a static binary that runs in the target environment. What I mean to ask is how is this (what I just described) different from a traditional lisp compiler (which I'm assuming skips the C generation and spits out machine code itself)? My plan of attack on this project was to create a thin s-expression to C translator macro at first, which would be pretty close to one-to-one mapping from s-expression constructs to C constructs (because access to efficient low level constructs when necessary is a hard requirement of my language), and then build up abstractions from this base until I approach the desired features of lisp. This is probably pretty naive (I'm sure it won't be that simple), but this is just a rough plan for how I'm going to iterate through development. Since, in my mind, even the thin wrapper around C syntax is an improvement (though a fairly small one).
I very much disagree. CL is the same. There is no portable way to construct this object without reading: (read-from-string "`(foo ,bar)") and THIS IS A GOOD THING. Turns out that it is a good thing to be able to represent that as eg. (BACKQUOTE (FOO #&lt;QUOTE BAR&gt;)), and if the CL evaluation rule did not prohibit it, representing that as #&lt;BACKQUOTE (FOO #&lt;QUOTE BAR&gt;)) might be even nicer for the implementation -- and users, given the appropriate interfaces. Sane code walking might be found in this direction... I've heard this argument several times, and I've made it myself, and while I see the aesthetic point of it, I've come to see it as pointlessly divisive and a symptom of divided interests of different communities. CL-peeps are trying to squat on the term "lisp", and schemers could not care less because they're off doing their own thing. Saying that to make a Scheme you have to write the whole Scheme is just silly. Nothing forces you to implement one of the newer macro systems from the Scheme community in order to implement a Scheme -- plenty of Schemes were made with DEFMACRO-type macros, and plenty of them still come with them. 
I think it's even more human than that. The CL community felt that they were being told: "You should not use DEFMACRO, it's unsafe and wrong. Use SYNTAX-CASE instead."; to which the response was "I like my DEFMACRO just fine, thank you, and haven't cut off any fingers. By the way, your language isn't a lisp anymore, and we're going to stop talking." I really doubt anyone took any issue with lisp1 being a lisp prior to the macro-debacle.
There are several Lisp programs which have been moved through the decades. Even more complex ones than Maxima. Axiom is an example. We have seen Common Lisp versions of Reduce, which was written in Standard Lisp and then in Portable Standard Lisp. Many programs written in Maclisp, Franz Lisp or Zetalisp have been ported to Common Lisp. Even software from Interlisp ended up in Common Lisp. Object systems had been written for several Lisp dialects. The LOOP macro had one source file for several years for Maclisp, Zetalisp, NIL, and Common Lisp. Spice Lisp was coming from Maclisp similarity and evolved into Common Lisp ... CMUCL... With very little support, we can even run Lisp 1.5 code from McCarty's publications in Common Lisp. For me Lisp is like 'German'. http://en.wikipedia.org/wiki/German_language There is a group who share a language and its texts. It is not 'Germanic': http://en.wikipedia.org/wiki/Germanic_languages Both English and German are Germanic languages. Practically they don't share language and texts, even though some principles and even some vocabulary look similar. That's *Lispy*. But not Lisp.
Ok, fair enough. My "lisp" is your "lisplike", and your "lisp" is my "Maclisp heritage". :) I'm still interested in your definition of "Lisp code", though. Seriously. ...because I think (and hope) than we're seeing a start of a new period of divergence in lisplike/lisp languages, and even a limited source compatibility would be a massive enabler there -- so it's IMO worth thinking and talking about. 
I have been wrapping my head around that for the last few days - its insane that that's all it is, but of course the program you generate can be arbitrarily complex. Is a runtime, then, the part of the program that you don't specifically write in the source file, but the compiler sticks in all generated code? If I can get the assurances that I need with a lisp that already exists, I should probably use that (though creating a lisp sounds like a lot of fun). Is there already a compiler which: * targets something gcc understands (I don't want to have to drop it because I can't run it on some other microprocessor) * gives me access to low level constructs when I need them. For instance, the ability to be specific about how much memory will be used (I never thought I would care this much about performance, since I'm usually all about abstraction, but I really only have 16KB here, and that's not including whatever nordic needs to run its softdevice), etc. Or at least would be efficient enough to run in extremely small memory spaces. * as many of the higher level features I can get with the aforementioned concerns. Actor model libraries and aspect oriented programming libraries would be a plus, but I can write those if need be
&gt;Don't stress too much over the chapter on denotational semantics, though -- it is very dense, much much more so than the rest of the book. This is actually where I got stuck
Definition of runtime varies a bit, but basically yes. Stuff like signal handling (to the degree that it happens behind users back), GC, initializing the state on program startup, etc are typically considered part of the runtime. The runtime might a separate "loader" program that maps the actual program into memory, or it might be just a library linked into the actual program. Options abound. Have a look at eg. https://github.com/sbp/hedgehog ThinLisp is an older thing that's pretty interesting. There are probably others as well. ...at the same time, be aware that the itch to write a compiler doesn't typically go away until you write one -- even a toy one. :)
This does look very good - I'm forking it so I can mess around with it. My only concern is that it doesn't look like it's been active in 2 years. I get the feeling that you're right - a compiler seems like such a satisfying problem to solve. As I'm thinking about the problem space, they strike me as similar to kernels, in that you have to manage the tasks the user wants to execute, but you get to be a little freer in how you manage the way data is laid out in memory, for instance. I have always visualized compilation as a one to one mapping from some instruction in the source code to some complex instruction in the target machine code, and interpreters as machines which took in the program's source code as input and executed it. It seems to me like a compiler is really somewhere in the middle of these two - you can't get certain language features by just mapping an instruction to an arbitrarily complex instruction in machine code (well, I suppose arbitrarily complex means that what I'm talking about would be included), you have to generate a program that treats the code to be ran as data, and operates on that data.
Before R4RS, scheme didn't even have macros, however: I brought up `(macroexpand)` not because `defmacro` is better than `define-syntax` but because there's no corresponding `expand-syntax`. Scheme programs aren't data and to me that's still dealbreaker.
Scheme is a language, not an implementation. Some scheme *implementations* are more lispy than the standards, but I'm talking about scheme *the language*. Some fun facts: R3RS didn't have macros. It didn't have eval either. R4RS had macros, but no eval. R5RS has macros and eval, but given that `interaction-environment` is optional means that it may be impossible to find an `x` where you can `(eval '(define foo 69) x)`. and so on...
The entire book is good, but the chapter on the testing system blew my mind. Made me long for a macro system in my non-Lisp projects to get rid of boilerplate code.
Macros are actually possible without homoiconicity, they just suck more. Haskell, python, and particularly rust all let you manipulate the AST, it's just that that only vaguely resembles the code you write; the more complicated parsing makes it harder to add things that "look native" too, as adding new infix operators is not really possible in python nor rust (lisp's lack of infix operators gives it a leg-up here).
I understand you are talking about the language. Me too. Since you said &gt; you can not actually write `eval` in Scheme" I doubted it is the case for core Scheme _without_ macros hence the inquiry. When you said &gt; Scheme `eval` doesn't evaluate scheme, only some subset of Scheme"! I suppose you disignated Scheme _with_ macros. Otherwise, I will be pleased to see an example core language construct that can not be implemented in core Scheme.
Presumably Clojure has idiomatically Lisp-y wrappers for the awfulness that those Java APIs are, right?
There's not actually a circular list here, right?
Yes I mentioned exactly this in my post. &gt; You can easily write small shim libraries which present Java with a functional interface. 
From what I've gathered it's a question of manpower when it comes to the Windows release. Hopefully, the great work they are doing presently will attract more developers that can develop SBCL for the platform. 
I must have looked right over that!
[**@peterseibel**](https://twitter.com/peterseibel/) &gt; [2015-03-23 14:33 UTC](https://twitter.com/peterseibel/status/580014527242092544) &gt; Gotten rich using Lisp after reading Practical Common Lisp? Good news! Both of you can http://cash.me/$gigamonkey. ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
It's wrong about some of the features of LFE, especially in the lisp-1 vs lisp-2 discussion where the description of (: ... ) in LFE is bonkers.
Gross... Apparently bots can spam Reddit? Regardless, I put that post there out of appreciation toward Peter for writing such a fabulous book.
I've got a Black-Scholes implementation in Emacs Lisp lying around somewhere that I did for a final assignment in a course on finance.
Hmm, I'd do it differently: make an image with all useful libraries pre-loaded. Compiling scripts themselves into this image seems to be an unnecessary complication.
Yikes, wall of text. CL is an acceptable scripting language only if there's a reasonable way to execute shell commands, direct the stdout and stderr to different places and parse the results easily. Everything else is sugar.
What's funny about Scheme is that they invented macro hygiene to solve the problem of unintended captures, but then mandated in R6RS that it be made possible to use lexical scope to capture macro keywords. For example: (let ((else #f) (val 'none-of-the-above)) (case val ((a) 'not-this) ((b) 'not-this-either) (else 'wont-return-this-either))) The above would return the "unspecified" value, or `(void)` in Racket, because the word `else` as used within the `case` form is interpreted not as the literal `else` that the macro would recognize in that context, but as the local variable `else`, whose value is `#f`. The CL equivalent would be having the ability to shadow `otherwise` so that the `case` macro couldn't recognize it. I brought this up in #racket and the devs said that it wouldn't even make sense for words like `else` in a program to be anything other than a binding. 
I am not sure what you are getting at &gt; map, fold, foreach, all are essentially scoped macros. These are functions not macros. Actually macros are functions, they just run at compile time. Treating code as input data to a function is what makes that function a macro (thus somewhat essential to it being a macro). It sound like you are actually saying: * You like lexical scoping - Yes its great * Or you like namespaces / packages - Yes they are great Then you throw in something about primitives at the end? I dont understand? Macros have something of an odd scoping in general. They have no access to the runtime so to say they are scoped to some runtime block is a bit odd. I think you might be better saying: "I like how in lisp macros are just functions that accept code and return other code and are part of the same language I am already writing."
Last time had really good recordings
Oh wow hadnt seen that. Found 'em http://medias.ircam.fr/search/?q=lisp
&gt; Actually macros are functions, they just run at compile time. Compile time functions or run-time functions is a fundamental difference: one operates on run-time inputs (conventional data), one operates on code as data. Of course you can call both as functions, then you do not really know what you are talking about when referring to their differences. Here I call one macro and the other function (except I ran out of words when I want to describe a general *function* referring to the general idea of *purpose* that we are trying to achieve). I have been developing and using a meta-layer programming system called MyDef for almost 10 years. It works great. But I only recently realize it is essentially just a scoped macro system. That is the background of my post. &gt; Then you throw in something about primitives at the end? I dont understand? Lisp's primitives are s-expressions -- deeply nested lists. It is great to implement, but it is difficult to comprehend -- at least I always have trouble whenever trying to get a high level overview of a deeply nested structure. If we treat general code as text data -- largely flat with shallow hierarchy, I find it better models how our mind works. To illustrate, think about how we would peruse a book, we will go through quickly and picking out names and verbs. We are treating data as flat text with shallow structures. Now if the text is formatted as a tree, perusing will be a much more difficult tasks. That is what I mean that the s-expression primitives are limiting. (In another example that may be more relevant to programming, we often find using "innerHTML" much easier than manipulating the nodes -- it is not just the syntax, it is the way we think.) &gt; Macros have something of an odd scoping in general. They have no access to the runtime so to say they are scoped to some runtime block is a bit odd. By scope, I am not referring to a run-time scope such as function scope or switch scope. Rather, I am describing a general context. For example, we can define "it" in a very narrow context. The narrow context is important so we can use short names such as *a, b, c* or *it* or any simple words, also defining the macro become much easier-- unlike in C we often need consider whether to surround the definition with parentheses or block braces or potential name collisions. I am not thinking that the macro scope has to be bounded by run-time scope. In practice, macro scope often observes run-time scopes, but that is because it is natural. In general, run-time scopes are from computer run-time units; macro scope are contexts of our thinking. In case when we are thinking over a collection of functions, which is not unusual, the macro scope can and should span over the collection. 
this OP seems to be talking not only about lisp but about functional languages in general, as can be seen in `fold` s/he mentioned, which rarely appear in common lisp code (in which we more commonly use `reduce`). But I think I got some point in the post. What he really means by "scoped macro" or "natural meaning" or whatever is actually the normal order evaluation e.g. ((lambda (x) (+ x 2)) 3) -&gt; (+ 3 2). In this sense, yes, "compile-time partial evaluation" and "runtime evaluation" are both evaluations. They are merely given the different names as "macro" and "function". Think about haskell. haskell uses [normal order evaluation](http://people.cs.aau.dk/~normark/prog3-03/html/notes/eval-order_themes-reduction-section.html#eval-order_normal-appl-ex_title_1) (evaluate from the outermost parens) and CL is believed to use applicative evaluation (evaluate from the innermost parens). Not actually. Evaluation is all about rewriting and transformation. Then remember that CL macros expands from the outermost parens. Thus, it can be said that CL supports normal order evaluation in a form of macro, which runs in compile-time only, while in runtime it uses applicable eval with functions &amp; special operators. Haskell, on the other hand, also use normal order eval in compile time, but hides every runtime clutters inside the (fog of) monads and lazy evaluation. The difference between macros and functions are not so important in this context. Indeed, CL macros are believed to run only in compile-time but that is not true. ANSI allows interpreter-only CL implementation as far as I remember, and interpreters may expand macros in runtime. Therefore, the only fundamental difference between macros, functions (and special ops) are their rewrite rules, or their evaluation model. Still, I don't agree with OP due to two reasons. First, I don't use haskell which forces lazy normal-order evaluation. Second, as said above, CL already supports normal-order evaluation. Finally, in the final paragraph OP seems to refer to the shift from "code as data" to "data is code" paradigm but they are just the same. It is just the matter of the timing of the evaluation, i.e., compile-time partial evaluation versus runtime dynamic programming.
&gt; What he really means by "scoped macro" or "natural meaning" or whatever is actually the normal order evaluation e.g. ((lambda (x) (+ x 2)) 3) -&gt; (+ 3 2). In this sense, yes, "compile-time partial evaluation" and "runtime evaluation" are both evaluations. They are merely given the different names as "macro" and "function". No, that is not what I meant. I am comparing the macros in lisp and macros in C and the classes of macro systems represented by each. The order of evaluation is a run-time concept, IMHO.
&gt; Lisp's primitives are s-expressions -- deeply nested lists. Two of lisps primitives are lists and symbols. When we use lists to order symbols that make evaluable statements we call them s-expressions. There are other primitives too (numbers, quote etc). Also sexps are only as nested as you make them, they needn't be nested more than any other language. Some common idioms do cause a touch more nesting, but then we can always macro away common idioms into less nested forms
&gt; Also sexps are only as nested as you make them, they needn't be nested more than any other language. I assume you are a veteran lisper, what would be a ballpark average nested levels for typical lisp code? 
Its really almost exactly equivalent to every other language. Deeply nested expressions are hard, as you say, so we tend to avoid them. It happens that sometimes you end up more deeply nested, but its usually very obvious from context whats happening. Again, in that file 3-6 levels of nesting on average. I have no problem seeing the `if` expression inside the `defun` expression any more than in other languages. When I first started the parens were a bit much, but they are just background once youve done it for a while.
C actually has one of the worst macro system I have ever used. Even the average macro-assembler has a better macro system, and m4, while very warty leaves the C preprocessor in the dust.
&gt; I have been developing and using a meta-layer programming system called MyDef for almost 10 years. It works great. But I only recently realize it is essentially just a scoped macro system. That is the background of my post. [MyDef](https://github.com/hzhou/MyDef) appears to be a text-substitution engine. This evident from the text of [the manual](http://huizhou.gitbooks.io/programming-with-mydef/content/intro_quick_tour.html), where you go into depth about how MyDef adds semicolons and "return 0;" for you. Lisp macros operate on the already-parsed abstract syntax tree (AST) of your code, which eliminates any need to think about parentheses. Not only doesn't your code need to do anything with parens, but the macro system itself doesn't, either. &gt;If we treat general code as text data -- largely flat with shallow hierarchy, I find it better models how our mind works. Unfortunately, it's more difficult to write an algorithm that manipulates the text of the code instead of the AST. That's because you have to write code that parses the text, so you end up having to pay attention to every space, comma, parenthesis, brace, and bracket and write code that handles each one. &gt;To illustrate, think about how we would peruse a book, we will go through quickly and picking out names and verbs. We are treating data as flat text with shallow structures. Now if the text is formatted as a tree, perusing will be a much more difficult tasks. That is what I mean that the s-expression primitives are limiting. (In another example that may be more relevant to programming, we often find using "innerHTML" much easier than manipulating the nodes -- it is not just the syntax, it is the way we think.) `innerHTML` is only needed because the only way to literally represent HTML in JavaScript is with strings. In Lisp, the equivalent would be using quasi-quoted S-expressions, which are still hierarchically-structured lists even though they look like text in the source code. Unlike strings, however, quasiquoted S-expressions are converted into cons cells as soon as Lisp reads your source code, so by the time your code does anything with it, it's already parsed. As a matter of fact, Lisp programmers prefer to represent HTML as S-expressions, and there are libraries that help them convert between S-expressions and HTML. As an example of why it's easier to work with the S-expressions instead of a string, imagine you're writing a macro that accepts a `cond`-like syntax. `cond` has the following syntax: (cond (boolean-expr form1 form2 ... formn) (boolean-expr2 form1 form2 ... formn) ... (boolean-exprn form1 form2 ... formn)) It's equivalent to writing the following nest of `if`s: (if boolean-expr1 (progn form1 form2 ... formn) (if boolean-expr2 (progn form1 form2 ... formn) ...)) Writing a macro like this using S-expressions as both argument and return type, you can write this: (defmacro my-cond (&amp;rest clauses) (let ((result nil)) (loop for (bool . body) in (reverse clauses) do (setf result `(if ,bool (progn . ,body) ,result))) result)) The part that says \``(if ,bool (progn . ,body) ,result)` is an example of a quasi-quoted S-expression. Even though I just typed some text into the source, the type of that expression is `cons`. You can check for that at the REPL: CL-USER&gt; (type-of `(if bool (progn . body) result)) CONS (I didn't unquote anything here because I don't have the variables `bool`, `body`, or `result` defined). However, if the `clauses` were just a flat string, you'd have to parse each clause out of the string before you could use it, instead of being able to just loop over the clauses and pattern-match the components of each clause. A string-based version of `my-cond` would look like this: (defmacro my-cond/string (string-clauses) (with-input-from-string (*standard-input* string-clauses) (let ((clauses nil)) (handler-case (loop for clause = (read) do (push clause clauses)) (end-of-file () nil)) (let ((result nil)) (loop for (bool . body) in clauses do (setf result `(if ,bool (progn . ,body) ,result))) result)))) So basically, it's the same macro, but I had to parse through the string first, which required me to add a parsing loop complete with an exception handler. The code has *way* more lines than the S-expression version. It would be *even more* difficult if the Lisp parser, `read`, wasn't available to Lisp application code and had to be written from scratch, which is the case for C and JavaScript programs, among others. 
&gt;From a text point of view, it is just: &gt; &gt; &amp;call cond &gt; cond1, text1 &gt; cond2, text2 &gt; ... &gt; condn, textn But you see how you had to translate the form? That's because MyDef is a separate minilanguage for generating text. Try calling a Lisp function from MyDef. You can't. Try using your `cond` MyDef pattern as if it was part of the Lisp language. You can't do that, either. On the other hand, Lisp macros are written in Lisp. They create language constructs that seamlessly integrate with the rest of the Lisp language. That's what they call homoiconicity. You can call functions from your Lisp program in your Lisp macro at compile-time. The lack of being able to do real programming in MyDef makes it less powerful than Lisp macros. Consider this macro: (defmacro destructure (pattern value-expression &amp;body body) (let ((value (gensym))) `(let ((,value ,value-expression)) (declare (ignorable ,value)) ,(cond ((eq pattern '_) `(progn . ,body)) ((null pattern) `(progn . ,body)) ((symbolp pattern) `(let ((,pattern ,value)) (declare (ignorable ,pattern)) ,@body)) ((consp pattern) `(destructure ,(car pattern) (car ,value) (destructure ,(cdr pattern) (cdr ,value) ,@body))))))) It calls the `car` and `cdr` list-manipulating functions both at compile time (to use on `pattern`) and at runtime (to use on `value`). The result allows you to pattern-match into a list, binding the variables from the pattern to the values in the corresponding location in the value-expression, similar to Common Lisp's own `destructuring-bind`: (defparameter my-list '(1 2 (3 (4) 5) 6 7)) (destructure (a b (c (d) e) f g) my-list (+ g c)) The result is 10. You could *almost* implement this in MyDef, perhaps with something like this: subcode: DESTRUCTURE(pattern, value_expression, body) &lt;something&gt; But there's no way in MyDef to pick apart the `pattern`, so you can't go too much further than this. But that's a simple example. The `LOOP` macro is an example of something that is *way* outside of the realm of possibility for MyDef. It defines its own sublanguage. The arguments it can take are endlessly flexible: (loop for n from 1 to 100 for a in (loop repeat 1000 collect 'a) when (evenp n) collect a else collect n) The loop itself is an expression, not a statement, and consequently, it has a value. The above loop expression's value is a list from 1 to 100, with all the even numbers replaced with the symbol `a`. There's another `loop` form within that loop form, with not even remotely the same argument pattern. The inner loop generates a list of 1000 `a` symbols. 
&gt; What's wrong with macro programs that do text replacements? Nothing. It's just that Lisp macros, unlike C macros, are not *only* capable of doing text replacements - as lispm said, Lisp macros can be written in Lisp itself, with all the power that entails, rather than in a separate and much more limited language. Cf. [redditsuxass' comment](https://www.reddit.com/r/lisp/comments/32ol2h/the_significance_of_scoped_macros/cqdylbi).
Fair enough. Edit: I want to add a point I was just thinking: the reason to separate run time from compile time is, run time are complex in nature -- it depends on run-time input which is unpredictable and it lives from start to end, which is complex; on the other hand, compile time input is fixed to the text we type in, and with scoped macros, the compile time context is limited to the part of text we can hold in our head. Mixing compile time with run time, on one hand, we are giving compile time an all powerful knife that is good for run time as well, on the other hand, it makes the situation more complex than necessary (and may encourage users to do more than necessary). My original post is to emphasize the benefit of scope, where we can even narrow down the context of macros to make it as simple as it should, and I meant for a lexical (or actually a text context that could go beyond lexical scope). Separate, so that simple makes easy. In addition, depend on implementations, some of the compile time cost may unnecessarily become run time cost, is that true?
The C macro language does not only have 'certainly limits'. It's primitive and not a programming language.
&gt; The purpose of macros are text replacements, straight and simple. In C. Not in Lisp.
I want --dump to be enabled by default, and use-package option. Also, with dumping options added (which I assume will speed up the bootstrapping), it still takes &gt;2 seconds to load the whole image. (It might be because of my slow, fragmented HDD.) ``` time ./test2.lisp "SBCL" 0 1 2 3 4 hello! real 0m2.716s user 0m2.478s sys 0m0.230s ``` Regarding shell integration, there are ScriptL, cim, roswell, shelly, and trivial-dump-core. I wish someone does the survey and comparison.... 
&gt;&gt;Try calling a Lisp function from MyDef. You can't. &gt;Why do you want that? Macro system is treating code (with macros) as data and output (lisp) code at compile time, why do you have to call run-time functions at compile time? Lisp programs manipulate lists. Lisp programs *are* lists. It is sometimes useful to have list-manipulating functions that you wrote to use at runtime also available to use at compile-time, because those functions can often be used to manipulate your code. &gt;If you have to write all code in lisp, certainly you could implement MyDef (with some other name perhaps) in lisp then you'll have a lisp engine for macros (but still separate from run-time lisp). We already have a "MyDef in Lisp." It's built-in. There'd never be any reason to implement a preprocessor for Lisp. &gt;I know the current way of lisp is mix compile time and run time and call it homoiconicity. But why that is necessary? *Homoiconicity* refers to being able to use the same functions to manipulate your code that you use to manipulate the rest of your data, not to being able to mix compile and run times. That said, mixing compile and runtime is useful for doing things that the language would otherwise not allow. It maybe useful, for example, to have a hash table that gets updated every time a certain type of function is defined. You could do that in a macro, and the hash table would still be there at runtime. Sometimes it's useful to have an interpreter in your program. There have been many interpreters implemented from scratch because the program that needed it was written in a less powerful language than Lisp. For example, JavaScript is the scripting language of the Web solely because Netscape was written in C++ instead of Lisp. &gt;&gt; (defparameter my-list '(1 2 (3 (4) 5) 6 7)) &gt;&gt; (destructure (a b (c (d) e) f g) my-list (+ g c)) &gt;&gt; The result is 10. &gt;You are essentially asking to implement a variation of lisp interpreter. Not that it can't be done (Greenspun's tenth rule), but why? I'm not asking you to implement a Lisp interpreter, I'm demonstrating that a simple Lisp macro can change Lisp's syntax so much that you can implement [pattern matching](http://en.wikipedia.org/wiki/Pattern_matching) as a macro, whereas to get structural pattern matching of any kind into C++ using MyDef it would be a lot more difficult. If you'd have to implement a Lisp interpreter just to be able to do that in MyDef, that just demonstrates the power of the Lisp approach to macros. &gt;The definition can be: &gt; &gt; perlcode: compute &gt; $if $param=~/.* and (\d+).* finally (\d+)/ &gt; push @$out, "(+ $1 $2)" This isn't in the manual, but you're only proving my original point about why it's advantageous to be able to write macros in terms of the abstract syntax tree instead of raw, unparsed text. If you really like to parse raw text, though, Lisp can do that via reader macros. Also, your macro above doesn't implement [structural pattern matching](http://en.wikipedia.org/wiki/Pattern_matching). The `destructure` macro I showed you does implement pattern matching (for lists, anyway), and only took half an hour to write. &gt;Of course if your intention is list comprehension, then I have to program a list comprehension function in, and unless you are asking for a common lisp, it doesn't have to be complicated. My intention is only to demonstrate that there's a lot more to Lisp macros than what you're referring to as "narrow scope". 
You are explaining why lisp is lisp and why MyDef is not lisp. In the process of explaining what MyDef is, I often got response either it is doing nothing useful, or it is doing something lisp already do, the latter often come from persons who are not really familiar with lisp. Your effort (despite being cross talking) did help me understand why MyDef is fundamentally different from lisp, thanks.
There is nothing wrong with such a macro system - see Tcl, the whole language semantics is built upon text substitutions. But this kind of macros is not nearly as convenient to use as AST-based macros. For example, you may want to extract all the free variables inside a code in your macro argument. In an AST-based macro system it's trivial. In a text-based one you'd have to *parse* the argument, making all the possibly incorrect assumptions about your context.
&gt; The purpose of macros are text replacements, straight and simple. No. Purpose of a macro is to *transform* a new syntax in a number of steps into the existing one. You can do it as a text replacement, in some of the most trivial cases, but in most of the cases it's nearly impossible.
Could you raise an example (of the nearly impossible but most of the cases)? &gt; Code is a data. It's a text (or a binary) source, it's an AST, which can be pretty much anything, but this "anything" is always (not very efficiently) representable as S-expressions. Text is more general. AST is more specific form of text (let's say the serialized form). S-expressions are even more specific. So a macro only works on s-expression is more narrow. I understand as lisper, you are contend with s-expression, that is OK, but it is not a base of argument. EDIT: to illustrate, MyDef can work with s-expression at pseudo AST level. It is currently in Perl, so all I need is a routine `$ast = parse_s_expr($str)` (just parse the parentheses, right?), then we can write macros manipulate that `$ast` -- in Perl as now, but as an idea, it can be in any script language (but Perl is great with text): perlcode: macro_on_ast $ast = parse_s_expr($param) # manipulate $ast push @$out, output_s_expr($ast) To use: $call macro_on_ast, (your s expression) As you see, it is still from text to text ($param --&gt; output), but more specific. You may not like the `$call syntax` (imperative-ish), but that is just syntax. How about: (macro_on_ast your s expression form) Here, we can let MyDef to intercept all action names with pattern "macro_XXX" and do text replacement.
&gt; I need an example to understand what you mean. I already gave you a trivial example: tracking the source location. You'd want to preserve it to keep your debugger happy. &gt; AST is complex. It does not need to be complex. &gt; I am not sure asking programmer to be conscious of that AST is a good idea. You don't need to. This is what quasiquotation and pattern matching are for. See how I'm doing it for C: https://github.com/combinatorylogic/clike/blob/master/tests/syntax.c - macros do not need to know the AST structure, it's all hidden behind the quasiquotation syntax sugar. &gt; I am not sure what you mean here. http://en.wikipedia.org/wiki/Hygienic_macro 
&gt; Having the meta-layer (macro and syntax) separate from the language results in two processes: the meta-layer compilation and the binary compilation. That's exactly the reason why Lisp approach is simpler and more powerful: you do not separate phases and they can exchange information (e.g., types). &gt; I am not sure what I am reading there. Could explain your purpose there? It's, basically, a C compiler with Lisp-like macros and extensible syntax, quasiquotation and all that. For example, if you want to generate an AST for applying function `fn` to arguments `a` and `b`, you simply quote it as '\fn\ (\a\,\b\ )'. I.e., it's not a text substitution, as in C preprocessor, but a proper AST transform, with all the bells and whistles. That particular example shows how to define a string interpolation syntax using this sort of macros. Can you implement a type-aware string interpolation for C using MyDef? &gt; However, I don't think it is critical. It's critical when your macros introduce new bindings. 
&gt; That's exactly the reason why Lisp approach is simpler and more powerful: you do not separate phases and they can exchange information (e.g., types). Could you give some examples to illustrate your point. Complex means interweave. Having two separate process mixed together is not necessarily good. &gt; fn to arguments a and b, you simply quote it as '\fn\ (\a\,\b\ )'. Why not just insert text: `fn(a,b)`? &gt; That particular example shows how to define a string interpolation syntax using this sort of macros. I see. Do you think this is better than write a proper program to do the translation? For me, I don't need invent nor learn a new set of special syntax. Anything I know how to do (in my case, in Perl), I code it, straight forward. &gt; It's critical when your macros introduce new bindings. I see, you are thinking in the line of making new language. All programming languages are limited by its context-free nature. All the concepts are out there: strong types, mutability, concurrency, ... why we are still making new languages? Because ultimately language is about making balances -- there is no perfect language. Look at our natural language: is Chinese or English a better language? There is culture difference, but there is no fundamental difference. However, both are context dependent. For example, I say `get me that`, you ask, `milk?`, I say, `no, salt!`. See, we never bother with strict bindings, we use context, establish conventions and use macros. Unlike the language used in contracts or law, they are ambiguous. When misunderstanding arises, we simply repeat in a different, more detailed sentence and move on (or correct it again). With current programming language, it is one go. We write source, then it become binary code. If we write ambiguous source code, there is no easy feedback loops for us to repeat and clarify. The only feedback we will have is runtime. But runtime is complex. We can do unit tests, but tests never can cover all runtime. We are forced to write un-ambigous source code all the time, that is difficult. Types, mutability, contexts, they are elements of complexity and to be unambigous, these complexity has to be there one way or another. Therefore, programming is difficult. Now add a meta-layer, suddenly, we can establish conventions and write ambiguous source code. Now programming become easy. We do not need hold all elements of complexity at the surface all the time. We can rely on conventions. When it go wrong, we get feedback from compiler, then we read the intermediate code (c) to see what wrong (just like you ask `milk?`), then we re-write our meta-code (or c code directly) to clarify, and move on. Now types, mutabilities become not so critical in the meta-layer. Let's hope the underlying language compiler will give us enough feed back in case we mess up -- isn't this safety net what current language research is all about? Currently, we only have one layer, so we can't have all our language knowledges put into one language and ask the programmer to deal with it -- it is simply too complex/tedious to deal. So different languages make different choices between easy and power as well as a gap of ambiguity. Its a balance, there is no solution. But if we have the programming precess separate into meta-layer and rigorous layer, it become possible to put all our strict knowledge into the language layer and it is not too complex or too tedious, because most of them is covered by conventions -- macros, which is ok to spend time work out initially and rarely need to be repeated. Actually I don't think we will have more errors with convention and ambiguous code. With current programming language with every elements of complexity at surface, we make mistakes, and we often make *same type of mistakes* again and again. In reality, we will correct these mistake pattern *in conventions*. We still make mistakes, but with plenty of conventions and habits, we make mistakes that worth our time to investigate.
&gt;&gt; I don't see why run time should get involved &gt; You have your full runtime in compilation time in Lisp. I think you are confusing run-time with compile time passes. Macro expansion can be implemented as a first compile pass, but you insist to have macros expanded as a compiler plugin (so essentially only have one pass, but still at compile time nevertheless). I think you are arguing something along the line of `du -s |sort -g` vs `dus` (separate processes vs single inter-weaved process). &gt; The values passed to a quasiquotation expression are ASTs, with all the metadata attached. a, b and fn came straight from parser, so they carry the location info from the place where this macro (or a syntax extension) was used. So even when the macros only need simple text translation, you would insist the macros to be implemented in a *context-free* fashion? Can you do this: $(for:z in x,y) $(z)_0 = $(z)_1 + $(z)_2 
&gt; Of course if you have input in a tree structure already, it may make sense to use an AST based approach. But even in that case we routinely serialize the structure and treat it as flat text. It all depends on the actual situations. As far as I see, text based approach is more common and more straight forward. Look at the Javascript, .innerHTML is used way more often than DOM. The DOM is a level deeper than .innerHTML, would you say it is more powerful? Or compare with assembly, which is as deep as you can go in programming, is it more powerful -- I guess it depend on your viewpoint. In term of language semantic, AST is the most precise representation for human to manipulate. Assembly is unrelated since it represents operational semantic of the computer. For example, suppose that you want to generate a `car` form for specific function and a cdr form for specific functions when it is called: (defmacro gen-car-or-cdr (name list) (cond ((member name '(test defun)) `(car ,list)) ((eq name 'test2) `(cdr ,list)) (t nil))) Then, try `macroexpand`: (macroexpand '(gen-car-or-cdr test '(a b c))) ;; =&gt; (CAR '(A B C)) (macroexpand '(gen-car-or-cdr test2 '(a b c))) ;; =&gt; (CDR '(A B C)) (macroexpand '(gen-car-or-cdr "" '(a b c))) ;; =&gt; nil Because the code you write is your data, in tree structure, you can write Lisp to manipulate it i.e. you can directly read a Lisp source file into sexp form and manipulate it. This is unlike other macro systems where the language to generate code is different from the language you use to write your program and this greatly increase complexity, and the macro language is not as convenient and elegant as your main language to use.
&gt; I think you are confusing run-time with compile time passes No, I'm deliberately mixing runtime into compile time. I can use any function or a variable I defined previously in my macro. So, a typical workflow I'm using is to implement the whole compiler as usual and then add a tiny little step, a single, one-line macro which adds the whole language I just built as an eDSL into your host language. For example, see how it's done for adding a full Prolog as both an embedded and standalone, compiled and iterpreted, all from the same single source: https://github.com/combinatorylogic/mbase/blob/master/src/l/lib/wam/prolog_backend.hl And since the parser is based on PEG (i.e., lexerless), it's trivial to mix any language into your host language, you don't have a fixed set of tokens. &gt; Can you do this: Of course. I can transform identifier AST nodes, while still keeping all the metadata around (and it's nice to know that your x_1 and y_2 came from exactly that "x" character from file "helloworld.blub", line 15, col 40).
&gt; mbase In the other comment you agreed that DSLs need be user maintainable. It appears the mbase approach requires the user to be familiar with compiler details as well as master of the pfront syntax, and even after that, I am not convinced that it is easy to use. Let's look at an example, the other day I was doing some geometry algorithm and wrote a lot of statements like: tf_x0=tf_x1 tf_y0=tf_y1 as if every statement need to be duplicated. So I wrote a macro and I instead wrote: $call zexpr, tf_z0 = tf_z1 # above is the macro applied to single statement &amp;call zexpr, tf_z = (tn_z - tn_zmin)*f_px $if b_start tf_z_save = tf_z # above is the macro applied to a block of statements The macro zexpr is defined as: perlcode: zexpr &amp;call filter_codelist $call filter, $t subcode: filter(v) $if $(v)=~/_z/ $(for:z in x,y) my $t2=$(v) $t2=~s/_z/_$(z)/g push @source, $t2 $else push @source, $(v) subcode: direct $call filter, $param (if you know perl, it will appear straight forward: for each line to be filtered, I search for '_z' in the text and replace with _x and _y. The main code filters block of code line by line (in $t), the direct branch filters single line. I used filter macro so I don't have to repeat the same code.) It uses quite a few conventions (therefore the code is ambiguous in nature) but within the narrow context, it is easy to define, and makes the code more readable (and writeable). In a sense, the DSL is so narrow that only makes sense in a user maintainable context. A designer will never come out such solutions (as it is not general enough). Is it possible to show me the example how to have a similar solution with clike? ( I assume it would be challenging since here the statement may be in arbitrary form). 
&gt; AST is the most precise representation for human to manipulate. Since you emphasize *human*, let's look at an example. Let's say I want you to pass me the salt, here are the ways to say it: Could you pass me the salt? Have you done with the salt? Can I have the salt? Salt, please. Is that salt? Each can be grammatically decomposed into a syntax tree, which version do you think is the most precise? (For this example, google/apple will just pick the keywords, maybe includes the order, feed into a neural network. I don't think it is even precise for machine to manipulate the meaning at ast level). In fact, in our daily usage of language, how often do you work with AST (or grammar)? When we go down to programming, yes, we are bound into one of the rigid programming language where you do not have freedom to even omit a semicolon or parentheses, and we have been trained to accept that as the only way, is it (the only way)? 
&gt; I search for '_z' in the text and replace with _x and _y What if there is a string containing "_z"? Or an identifier `foo_zbar`? &gt; Is it possible to show me the example how to have a similar solution with clike? Something like this: http://pastebin.com/CMpAVu81 It will only duplicate statements if they contain _z variables as expressions or lvalues. E.g., a function call `foo_z(...)` will be ignored. It will also handle correctly multi-line statements (which won't work in your case).
My single-threaded-ccl library will allow you to fork with CCL. SIGCHLD handling is an issue with CLISP, but also an issue with SBCL. Alastair Bridgewater had ideas on how to give users more control on signal handling on SBCL, so if you want to do things right you can pick his brain. 
That's not how --dump works. You use --dump to dump an executable. Then, you run the executable instead of the script. Since loading ASDF, searching the source registry and checking timestamps for all components is taking a sizeable part of the slow startup, it's no use having a dump image to cache only the latter parts of the build. 
Nice to have you follow up! &gt; Reading a language as a human, and writing code as a human to instruct a computer to manipulate a language. Do you read your code? I often spend way more time reading my own code than I write it. &gt; As an example, consider the following Perl program that attempts to get the second element of a Lisp list I am not sure how your example is related to macros. For run time inputs, we are making assumptions (because we can't really control run-time inputs. For macros, the input is/should always be input by the coder himself so he should know what to expect. If list pattern is expected, all he need is a proper_split (which happen to be in my personal library, which not only deal with parenthesis, also handles quotes, brackets, braces.): my $tlist = proper_split($line) print $tlist-&gt;[1] Not only it can deal with lists, even when you expect JSON, you can write macros to deal with JSON pattern as long as you have a parse_json in your library (which is not that difficult to code it your self in one afternoon). As in comparison, could you write lisp macros that deal with JSON patterns? (The answer probably is yes, because lisp macros is a lisp program on its own -- I am asking it so you may put your example in proper perspectives). &gt; This allows you to concentrate on writing the macro, instead of spending time and adding more complexity to your macro. It is about having the right tool for right problem. When grammar is needed, regexp may fall short. But with a proper parse function, the macro is not more complex.
&gt;Do you read your code? &gt; &gt;I often spend way more time reading my own code than I write it. I find that the less code I had to write, the less I have to read later. So I appreciate not having to put parser code in every macro. &gt;For run time inputs, we are making assumptions (because we can't really control run-time inputs. For macros, the input is/should always be input by the coder himself so he should know what to expect. A robust macro should work in a wide variety of situations. For example, it might be valid for an argument to be either a string literal or a variable whose value is a string. Or, if the argument is a number, it could be a numeric literal, a variable, or even a complicated expression whose value will be a number after evaluation, such as `(* pi (expt r 2))`. Often times, *any* kind of expression is allowed. When working with an AST, you don't have to worry about whether an identifier or an expression was passed because the parser has already taken care of it for you. But when working with strings, you have to write extra code to take care of the case where a literal was passed, or the case where an expression was passed. &gt; If list pattern is expected, all he need is a proper_split (which happen to be in my personal library, which not only deal with parenthesis, also handles quotes, brackets, braces.): That would be a parser. Well done. So you do see *some* of the need for parsers, otherwise you wouldn't have written `proper_split`. You would instead have tried to muddle through with regexps. Lisp's parser is called `read`, and it's a Lisp function. It works like proper_split, except it can parse the whole Lisp language, not just certain parts of it. And `read` is called automatically by Lisp, except when you write a reader macro. Reader macros are called *before* parsing instead of after, and are even more powerful than ordinary macros. One thing you could do with a reader macro is make it parse JSON, which would enable you to have JSON literals in Lisp. The literals would work even with pre-existing macros that weren't written with JSON in mind. &gt;As in comparison, could you write lisp macros that deal with JSON patterns? Here's some code that implements a JSON reader macro using a pre-existing library: (ql:quickload :cl-json) (use-package :json) (set-dispatch-macro-character #\# #\J (lambda (s c n) (list 'quote (decode-json s)))) And here's a JSON literal: #J{ "foo" : 3, "bar" : [1,2,3] } It parses as if you wrote this: '((:FOO . 3) (:BAR 1 2 3)) You can even use this macro with pre-existing macros that weren't written with JSON in mind, and it'll still work. The LOOP macro is part of the Common Lisp library, and its authors had no idea I'd try to use their macro with a JSON literal. In a world where Lisp macros dealt in strings and everybody wrote their own parsers, giving JSON to a macro that wasn't expecting it would cause it to break. And yet: (loop for (key . value) in #J{ "numberOfThings" : 30, "nameOfCollection" : "my_stuff", "id" : 39284, "gratuitousArray" : [23,40,30] } do (format t "The key is ~a, and the value is ~a~%" key value)) The above code works, because Lisp parses everything before the LOOP macro starts to expand. The output when you run it: The key is NUMBER-OF-THINGS, and the value is 30 The key is NAME-OF-COLLECTION, and the value is my_stuff The key is ID, and the value is 39284 The key is GRATUITOUS-ARRAY, and the value is (23 40 30) The only other language where you can write JSON literals is JavaScript.
&gt; I did emphasize it works for blocks of code I understood "blocks" as sequences of statements (i.e., { ... } in C). &gt; I don't see why function call and string contents should be excluded for my intention That's how I understood your problem specification. It's easy to include functions if you really want to, but yet I'd prefer to introduce a special syntax for the mutable identifiers instead of introspecting their textual representation, just for robustness. &gt; But in your case, seems he has to learn your regex syntax, in fact, every line in your macro definition appears to be a special syntax that the user need to master before know what to do. Of course the user must understand this language, but it's much, much smaller than Perl (after all, it's nothing but a tiny DSL designed for implementing compilers and nothing else). &gt; is you macro definition limited to file scope Yes, it's valid from where it's defined down to the end of the file scope. There are ways of defining more fine-grained locally scoped macros (e.g., this is used to pass a context-sensitive data to a macro, like the function being currently compiled, etc.) &gt; Could you explain the work flow/tool chain (to run your example)? This is a standalone C compiler, taking a source file as an input and producing an LLVM bitcode file or a target platform object file as an output. Macro expansion is done inside the compiler, on various compilation phases (e.g., during parsing, just after post-parsing lowering, after typing pass, etc.). It can also use a JIT compiler in compile time if a macro needs to run a previously defined C function. Of course, you need `mono` and LLVM libraries to run the compiler, as it runs on top of .NET. &gt; In your case, do you have a text version to debug, or do you have to rely on gdb? You can always pretty-print a resulting AST and inspect it. Same as in any Lisp.
&gt; In your case, the idea of yours is bundled with particular choice of compiler as well as the particular set of syntax, bells and whistles etc. The very same idea is bundled with *any* Lisp, or any other meta-language (including Template Haskell, MetaOCaml, MetaLua, Nemerle, Converge, and many more). There are even JavaScript implementations of the very same idea: http://sweetjs.org/ So, it's pretty ubiquitous in the powerful enough languages, you don't have to stick to any particular one in order to be able to use this approach. &gt; Yes, I am implying that AST is not the only way or necessary chain in compiler, it is merely one particular algorithm choice some early pioneers have chosen. We've been there already, in late 60s-early 70s. It really sucks without an AST. 
&gt; it's pretty ubiquitous ... With which I see as a problem -- there is little room for new ideas. &gt; We've been there already, in late 60s-early 70s. It really sucks without an AST. What kind of computer do they use then? What kind of computer do we use today and in the future? It sucks then does not necessarily mean it will suck today. Also, it is highly dependent on the problem context. When computer resource are scarce, you want your solution be general, which may lead to one logic. When the computer resource are plenty, it affords localized solutions for diverse problems, where one general logic may not apply to all. Case in point, our brain do not use AST, so I am sure in the far future, AST is not a good implementation for compilers. I am not so sure about how far is this far future, and I am no so sure about what today we stand, but I believe it is worthwhile to revisit these assertions from time to time. What I know is back in the 60s and 70s, researchers are investigating and debating on the idea. Today we are not.
For simple language, such as MyDef, a switch on the leading keyword and each case pass on to another simple parser will do. This is a general top-down approach, and it handles much of real world problems. For complex ambiguous language such as English, a neural networks where words as well as order and punctuation are taken as neural input may work. It may work but just as our brain, we don't really know how it works. Automatic optimization is a logic fallacy (when the problem context are general). Of course you will object to my opinion, that is OK. Just a few days ago I read a presentation by djb on this with a similar thinking, so I don't think I am alone. Having too much *must* in your logic chain is a warning sign.
&gt; We need read the code as human more often than we write the code as human *regardless how much code.* A 50,000 line program with 200 source files takes far more effort to read and understand than does a 5,000 line program with 20 files. Most programmers understand this. &gt;Regarding your comment, calling a parser routine is not the same as putting parser code in every macro. Most of your MyDef macros have custom parsers in them. Many of these parsers are broken in subtle ways. &gt;But in your sense, lisp is putting a lisp compiler in every macro. All macros, including MyDef macros, can be thought of as being little compilers. &gt; Please do not fall into the trap of defending lisp. I am not attacking lisp. This is a thread about macros. I've been trying to get you to understand that there are certain advantages to writing macros that deal with an AST. Each code example I've given you has been meant to illustrate various areas where operating on the AST has strong benefits. You keep coming back and saying that there's no case where AST macros have benefits over string-based macros, but on the contrary, you claim that a naively-written regexp is usually better than having to learn how to manipulate a data structure, so clearly you don't understand. It's a wonder you were able to write something that passes for a Lisp compiler. I'd bet your compiler doesn't support macros, though, because that would require your Lisp parser to produce an AST. 
what are you talking about? That object could legitimately be read as: `(append '(foo) (list foo))` . It's not *required* to be read in as that, but however it is read in must evaluate to something equivalent. You could create a backquote macro that reads in to that form and be 100% compliant
&gt; Because there are no better ways. I cannot possibly persuade or even illuminate you to any other way when you are holding such opinions. With such opinions, every time a new way being suggested, you'll simply dismiss it -- maybe until all your friends has switched their ways -- there is no way to be convinced, there is only way to follow what is established. What I have is history -- in anytime of history, there is always rule, way, theory, method, to be believed by overwhelming majority to be the only, best, no other way. Put into their shoes, there would be no way to see otherwise. Only history says differently -- sometime, hundreds of, thousands of years later. How long a history of your current view is holding? Logically, to prove it is absolute, most, only, you have to exclude every other -- an impossible event -- therefore, logically, the only reasonable sure thing is to believe there is some other better way. You don't have to find that way, but you'll benefit having an open mind -- at least when you hear a heresy, you will not feel anxiety to put him on fire. &gt; Flat sequences are not natural anyway, people think in graphs. If you plot every elements and every connection on a piece of giant paper, it is a complicated graph. However, having every information inter-weaved together is by definition, complex. The natural way (for me at least) is to pick 5-7 items at a time (from a top-down, or bottom up, or any hierarchy), and lay them flat (not necessary sequential) in my head (working memory) and work on them. When we do this, we are not limited to any fixed connection, we are open to any connections that previously not obvious, such as hidden connection, reverse dependency, etc. Putting them into a tree prevents you to see any other way of laying out the connection, especially when they are a graph. So I am not denying that the entire connections make graphs, but I don't think the best way is to have them processed at one time. I believe we should process them one scope at a time, one statement at a time, and when we reach the whole file, we are dealing with data with limited depth (with branches and leaves resolved earlier). &gt; I've been dealing with all kinds of weird and unorthodox compilation techniques. Out of interest, what do you do (if you don't mind)? 
&gt; A 50,000 line program with 200 source files takes far more effort to read and understand So we give up reading the code, which is the problem. &gt; Most of your MyDef macros have custom parsers in them. I don't think you are speaking from honesty (how much of *my MyDef* did you read?). &gt; I've been trying to get you to understand that there are certain advantages to writing macros that deal with an AST. &gt; You keep coming back and saying that there's no case where AST macros have benefits over string-based macros You sounds like about to put this heretic (me) on fire. Please do not treat your opinion as religion. I never said "there is *no* case ...". I am just stating some of my opinions and some of my experiences. &gt; It's a wonder you were able to ..., &gt; I'd bet your compiler doesn't support ... In fair discussion, we are putting each other's sanity and ability at equal footing. To correct the record, I just stated I have wrote lisp compiler as exercise before, and it was producing AST. And I stated that to just say I understand the basics of LISP. I did not and am not dismissing LISP. I am merely saying lisp way may not be the best way. And in my experience, it is not the best way. Please try to interpret my statements in the right context.
&gt; I cannot possibly persuade or even illuminate you to any other way when you are holding such opinions. Ok, let me rephrase it: there are no better ways I'm aware about, and I was digging really deep. &gt; every time a new way being suggested, you'll simply dismiss it Suggest a better way. I'm open to accept any unorthodox ways of compilation, as long as they work better than the existing ones. It's really, really easy to prove that there is a better way: show one. Simply speculating that there must be one just because it's logical to always expect a better option is not constructive at all. &gt; The natural way (for me at least) is to pick 5-7 items at a time (from a top-down, or bottom up, or any hierarchy), and lay them flat (not necessary sequential) in my head Are you always working with an unindented flat code, in a single long wrapping line? Because as soon as you format your code in 2D it becomes a tree or a graph in your mental model. &gt; What do you do (if you don't mind)? Compilers, for all the parts of the language spectrum: from HDLs and low-level languages for high-performance bit and number crunching to the very high level languages (functional/logic/dataflow/arbitrary declarative DSLs, etc.). And, yes, there is quite a wide segment where flatness is embraced: Forth and its derivatives. It's all nice and clean, but only as long as you stay on pretty much a flat abstraction level, and only as long as performance does not matter.
&gt; It's really, really easy to prove that there is a better way: show one. Simply speculating that there must be one just because it's logical to always expect a better option is not constructive at all. I assume you are having llvm (clang) in mind, and with the question, you want me to show you an alternate compiler that beats clang. Such effort takes time (and endeavor), be patient (or take part in). Even I show you an compiler, how are you going to assess it? How do you assess llvm (against gcc for example)? Perhaps by spend quite some effort in learning to use it. I am not sure you are ready to spend such effort on an unconvinced compiler any way. So you are fooling yourself to believe you'll be easily convinced (by seeing an example). Plenty of examples in history. You are saying that because you are not ready to be convinced and a sign of close mind. EDIT: on the other hand, I do have MyDef (not a compiler, but a different way of doing macros). I don't see any sign that you are even willing to read. That is OK, &gt; Are you always working with an unindented flat code, in a single long wrapping line? Because as soon as you format your code in 2D it becomes a tree or a graph in your mental model. You are twisting my word. I am not denying there are trees or graph in the information or structure. I am saying I do not work with the whole tree or whole graph at a time (sometime I do when the tree or graph is simple). And for 5-7 items, I like to lay them flat -- such as 5-7 elements in one statement and 5-7 statement in one block. I am not saying having flatness through out either. 
&gt; Compilers, for all the parts of the language spectrum: from HDLs and low-level languages for high-performance bit and number crunching to the very high level languages (functional/logic/dataflow/arbitrary declarative DSLs, etc.). It is such a broad answer I still have no idea to paint the picture :) What is your specialty (interest), high level end or the low level end? (because I do believe those two interests are in conflict in nature). 
There was a recording. With luck, it will be released. Not soon.
Yes, or it could be legitimately read as (backquote foo #&lt;comma&gt; bar) or (backquote (foo #&lt;unquote bar&gt;)) or anything else. The only thing you can rely on are the semantics of the form after you've macroexpanded the read-result in the correct environment. My point is that an implementation can quite legitimately already create forms that don't conform to a simplistic "lists and atoms" model, but can nest eval-able expressions inside opaque structures -- as long as those structures are wrapped by a macro that knows how to deal with them. ...and many schemes provide an equivalent of macroexpand, so are those Schemes lisps, then? I get that some people see this as the big difference, but I don't. I also care less and less about the fineness of the line between "a lisp" and "a lisplike language". Maybe I'm just becoming blind.
&gt; Exactly the reason why I always use literate programming. E.g., CLike we've been discussing previously is a literate code [1]. And it goes really well with DSLs. Btw., another reason to use AST-based syntax extensions for implementing DSLs - you've got nice syntax highlighting (in TeX and in IDEs) for free, it's something you won't get with text-based preprocessing. I have Knuth's TeX and MetaFont on my shelf and I do read them every once a while. I think the key idea of literate programming (the reason Knuth still use it) is the ability to re-arrange code. Not so much with the documentation part. In fact, unlike the TeX book or the Metafont book, Knuth's program book is very un-readable -- dragged by the details of his actual code (which still bound by the programming language). So his literate part is filled with language details or nuisances. Today, I think I can work out a version of TeX much easier with his "TeX book" alone rather than his literate source "TeX program". Typical literate program is not writing the code as one would write the book. Often it is writing the book as if writing the code. Imagine me, who is not familiar with your clike code and you are going to write a book (not necessarily the code), would you write your book the same way as the documentation you linked? As in a book, the readers are typically presented with high level ideas, and then expanded with low-level ideas. And for experienced readers, they may skip the low-level texts when he thinks he understands the idea. With code, such low-level/high level details are not well separated. The TeX program contains dozens of links on every page and the entire book is a weaved web -- that is not easy to comprehend. I am working on a direction of literate programming with minimum documentation. If one (given sufficient background knowledge) cannot understand the code easily , then the code is the problem. Having text annotating the problematic code does not really help. Yes, I would agree to understand any thing, it is necessary to spend time in it. The problem with current programming is the low level details are inter-weaved with the high level intention. Reading TeX: the program, at no point I felt that I can skip part of the details, swap in my own strings, hash tables, data structures, it is more or less one or nothing. Ideally, I would like the program have several layers. At the top layer, omitting all the lower layer, the code would compile and run -- with basic functions, missing most features, running on basic defaults that are built into common conventions. Then one can include some of the underneath layers, adding features, annotations, or even modify the top layer default behavior -- without touching any of the top-layer code. Each sub layer can have its own sub layers, all the way down until CPU specific tweaks. That is my goal with MyDef. 
Looking forward to library bundles. That is going to be sweet!
Ufasoft Common Lisp is fork of GNU CLISP implementation. * http://ufasoft.com/files/ `ufasoft_lisp_4.37.exe 22-Apr-2015 11:45 488448`
Great work, qtools is a big relief.
C-c C-p will evaluate and pretty-print; not sure if it's what you want
By "another frame" you mean the frame with the REPL? Or a third frame. Does `M-x slime-eval-last-expression-in-repl` do what you want? 
E: Yup, thank you, that's exactly what I was looking for. You rock. ~~I believe that might be it, I will try (in process of rebuilding emacs right now, takes a few minutes). Thanks.~~ ~~(if it answers your question, here's what I'd like to see: http://www.reddit.com/r/lisp/comments/33n7ln/a_slime_question/cqmjlg0 if it makes any sense.)~~
I think, most important thing for an organization like this to have is the video record of the talks. Unfortunately I could not find any for this event. Except this one https://www.youtube.com/watch?v=rzOx1cexEgE&amp;list=PLkDl6Irujx9PL5LfhvvkkeMiCiDij8jzM&amp;index=1 Shinmera thanks for the videos man. You are my hero. 
That's wonderful news, thank you. I just found a cygwin package GNU CLISP 2.49+
If you are open to other Lisp implementations then there is always Racket. I know it has and extended REPL that lets you launch your favorite editor from within the REPL. It also has DrRacket (which looks similar to lispbox), but isn't emacs and is pretty nice to use. You can check out their documentation on [XREPL](http://docs.racket-lang.org/xrepl/index.html) and see if maybe that is something that fits your bill. I've never used it (just started learning Lisp myself), so probably can't answer any questions on it.
Thank you! I think I would prefer to stick with common lisp, but I'll check these out.
The most popular ones are SBCL and Clozure CL, but I think clisp is fine too.
Great, thank you for all your help :)
Clozure CL has a full-blown IDE of its own if you're on Mac OS. http://itunes.apple.com/us/app/clozure-cl/id489900618?mt=12 Of course, the default editor in it, I think, is Hemlock which is an Emacs clone. I think that environment though feels significantly less Emacs-y.
I don't happen to be on OSX, but thank you very much for your response!
CCL works better in Windows than SBCL.
I'd really recommend against notepad++ and a simple command-line REPL. Not being able to send text from your buffer to the running lisp image sounds like a pain. You can use emacs and enable cua-mode for more familiar keybindings (Like C-c/C-v being copy and paste) Or you could try vi's slimv or [SublimeREPL](https://github.com/fukamachi/SublimeREPL) As for implementation choice, in windows Clozure Common Lisp has better support than SBCL.
I would strongly recommend using emacs and slime as your IDE even if you will be writing your code in notepad++. Just think of it as e.g. visual studio or eclipse for C++ or Java respectively. You can get a fair amount done with just using menu items, and the debugging experience is much better. For a quick install of both, check here: http://www.iqool.de/lispstick.html Note also that nodepad++ will have some issues, as I'm *guessing* though that it has horrible lisp indentation support, since of about a dozen editors I tried only vim and emacs had support for indenting lisp out-of-the-box and sublime was the only one with a decent plugin (I didn't try notepad++ since I don't have a working windows machine currently).
When run from the command line, clisp has a nicer repl than sbcl or ccl because it has gnu's readline built-in (command history, emacs-like editing commands). If you use sbcl from the command-line, I suggest you use [linedit](https://common-lisp.net/project/linedit/). supposedly linedit works with ccl too, but I have never tried it with ccl.
Another option is [rlwrap](http://utopia.knoware.nl/~hlub/uck/rlwrap/), which is compilable in cygwin. I use this a lot when running ccl from the command line.
You may want to give [Spacemacs](https://github.com/syl20bnr/spacemacs) a try. It has a SLIME layer there and editing features for Lisp already setup. It supports both Vim and Emacs key bindings that you can switch back and forth or just use one you prefer. Also check [this answer](http://stackoverflow.com/a/1101605/496700) on SO to quickly learn how to use SLIME in 15 minutes before reading the manual. Even just recompiling the current buffer and send code to SLIME REPL, it's miles more productive than using Notepad++ and primitive command line REPL. You can read the manual to leanr more fancy features after you feel comfortable basic Lisp workflow (other Lisps also has similar way of working in Emacs). You can ignore the setup code if you use Spacemacs.
&gt; write it yourself in Emacs Lisp Of course, though, that requires time to learn Emacs Lisp, and time isn't always available.
You could try the free personal version of Lispworks: http://www.lispworks.com/downloads/index.html It does have restrictions that will prevent you from building and distributing apps with it, but they don't really affect your ability to learn Lisp with it, and it's a pleasant alternative if you don't want to have to learn Emacs.
Link to the presentation videos: https://www.youtube.com/watch?v=rzOx1cexEgE&amp;list=PLkDl6Irujx9PL5LfhvvkkeMiCiDij8jzM&amp;index=1
Not to be _that guy_, I'm just asking out of curiosity — why not emacs?
Thanks. Do you use quicklisp for deployment? I think the biggest concern I have right now is how to make a homogenous environment; I have extensive experience with other tools, like the GNU toolchain, cmake to an extent, some commercial products, for organizing development and maintenance cycles, I would like to create (or use) something with CL, so that the process is streamlined as it can be done with other tools. Another question, for you specifically, do you have any recommendations on how to use buildapp and quicklisp together to streamline aforementioned process? And just so I'm clear — I'm fairly new to Lisp, so if I'm asking something retarded — please, bear with me.
I'm sorry I answered
I use emacs + prelude + slime + SBCL. If you're going to use CL for 'shell-related and operations activities,' you might wish to take a look at scsh and consider how to port its ideas to CL
There's also [shelly](https://github.com/fukamachi/shelly) and CL-launch discussed [here](http://fare.livejournal.com/184127.html)
rlwrap (and others? Non CL nor SLIME) Use a fixed completion file, rather than introspect-ing the in-, use-, -package, *package*
Sorry this post isn't user friendly at all, I can't extract anything from it !
I just love it if every one who was there would post an account of their experience.
Great write up! 
So, on OS X 10.10.3: $ sh make.sh --prefix=/q/cl/sbcl --fancy and I got: Finished running tests. Status: Unexpected success: threads.pure.lisp / (SEMAPHORE-NOTIFICATION WAIT-ON-SEMAPHORE LP-1038034) Skipped (broken): debug.impure.lisp / (TRACE ENCAPSULATE NIL) Skipped (broken): debug.impure.lisp / (TRACE-RECURSIVE ENCAPSULATE NIL) Expected failure: packages.impure.lisp / USE-PACKAGE-CONFLICT-SET Expected failure: packages.impure.lisp / IMPORT-SINGLE-CONFLICT Failure: threads.impure.lisp / FP-MODE-INHERITANCE-THREADS (4 tests skipped for this combination of platform and features) test failed, expected 104 return code, got 1 
[sbcl.org seems down](http://downforeveryoneorjustme.com/sbcl.org) - surely not the reddit effect, we're too small a community for that
I hope it will be available in the PDF format.
pre-order done! :) - his web-framework TBNL (later Hunchentoot) got me into using Common Lisp in realworld commercial projects. Not to mention all the other libraries he contributed. Thank you Edi!
Put it in my cart, but I doubt I'm buying a $70 Lisp book unless it turns out *amazing*.
BTW, every single really valuable thing in our Life is PDF (Pretty Daring Free).
preorderd.
Reminds me of wise Erik's words about achieving anything in C++ demanded so much effort that you cherished it and wanted to reuse as much code as possible as opposed to CL. From another perspective, too many libraries seems like a nice to have problem. Much better than no libraries. Lets hope the next problem that arises is "the documentation is too long" \^_\^ Also kudos to Mark, it appears he really did a proper survey of the field before starting to write his own. 
True. If all the libraries are half baked it would be an illusion of solutions. (Not that that is the case, at least with yason)
I am avidly awaiting this, and hoping it will not be all web-based.
With deep shame.
Redundancy is also a result of inaccessibility. I hope your quicklisp fulltext search system improves this! 
It went well. Drinks were sponsored in part by Clozure. Laid back chat. People seemed to like the talk. I'll post slides soon. Definitely a very pleasant meetup. 
Redundancy is also a result of many of the libraries being half baked, undocumented, and generally sloppy at this time, so people wont risk commiting to something that feels like "some random guy hacked together last weekend" and instead go DIY, and then end up publishing their own "I hacked this together last weekend" as just another library. You dont see people wasting time on duplicating redundant clones of alexandria, cl-ppcre or cffi. The only justified risk on working on a redundant clone is when the best current offering is in such a bad state that you think that you can do better.
It's actually been fairly common for people to write general purpose utility libraries like Alexandria, and there are a couple regular expression libraries too. For the former case, I think it's sometimes because people started in isolation and built up their own little ecosystem with a utility library at the core, and they don't want to "port" it to something like alexandria, or alexandria is missing something they consider essential. CFFI was "redundant" compared to UFFI, and uffi is pretty good, and well-documented. CFFI took a different implementation approach and has achieved excellent success and quality.
But how would I contact you offline? Did you mean PM?
You can find my contact info on HN (same user name)
Cool deal, thanks. Should've thought of it...
&gt;Describe the goal, not the step If you are trying to find out how to do something (as opposed to reporting a bug), begin by describing the goal. Only then describe the particular step towards it that you are blocked on. Often, people who need technical help have a high-level goal in mind and get stuck on what they think is one particular path towards the goal. They come for help with the step, but don't realize that the path is wrong. It can take substantial effort to get past this. Stupid: How do I get the color-picker on the FooDraw program to take a hexadecimal RGB value? Smart: I'm trying to replace the color table on an image with values of my choosing. Right now the only way I can see to do this is by editing each table slot, but I can't get FooDraw's color picker to take a hexadecimal RGB value. The second version of the question is smart. It allows an answer that suggests a tool better suited to the task.
I believe you, except for the bit where you said that it works quite well. I have wasted I-don't-know-how-many hours trying to get it to load.
I looked at the linked page, the link to "erudite" in the page, and at the github. Couldn't find an example of the erudite syntax.
From the last line of the article: https://github.com/mmontone/erudite/blob/master/erudite.lisp
It's great that interactive development is still possible with erudite. What I fail to see the benefit of, and this is a general comment about LP, is generating a separate document containing internals. It's easier to make the case for user documentation that it needs to be presented as a pdf or web page. Still, even for user docs I find that docstrings coupled with M-. in Slime are great and I aim at one thing only *make the library explorable by adding docstrings/mgl-pax:defsection forms*. I found that by making the source read well as a user manual the design is improved. Documentation can be generated too, but it's an afterthought.
In our life?
could you also post your actual ssl certificate, signed with your pgp release key? afaiu one of the next goals is to provide a robust validation mechanism based on gpg. if that's the case, my two cent request is to allow, for cases where there's already gpg on the system with your distribution key in trustdb, to still interact with quicklisp hosts over plain http. ssl has a lot of well documented problems, while it makes some handy hacking activities needlessly complicated. right now it's pretty easy to change http.lisp and network.lisp to tunnel connections over ssh. it's also easy to debug problems using wireshark. ones you have ssl everywhere that goes out of the window. 
Woo!
[Woo](https://github.com/fukamachi/woo)?
 lang | users --------|------- php | 36k js | 60k python | 90k perl | 9k Not bad, let's keep growing.
I think so, old mailing lists and boards are full of quality help, reddit seems unnecessary for perlists. It's not surprising, to my understanding, Perl was always off the mainstream, they had everything set up their own way (packages, OOP, etc, etc) but noone knew.
Good beginning. What are the main benefits? And what about tail recursion?
Some of this reminds me of my thesis, on Boxer, a visual programming language. http://klotz.me/thesis.pdf
The example I like to give when talking about the power of lisp macros to non-lisp programmers is as follows: Imagine for a moment a program using functions returning numeric (integer) values to drive core logic, where which branch executes depends on the return value being positive, negative, or zero. This is easily done with two if-else statements (curly braces indicate code clauses not important to the example, syntax is in my personal pseudo-code format): x = somefunction(); if x &gt; 0: {positive-things} else if x &lt; 0: {negative-things} else {was-zero-things}; However, the program would perhaps be easier to read (using fewer temporary variables, at least) in a language which supported a "sign conditional" instead: signof somefunction(): positive: {positive-things} negative: {negative-things} zero: {was-zero-things}; If you were to implement this yourself, not only must *only* the required clause be executed, you would like to try and have your new statement's grammar match the rest of the language's general syntax. Also, each of the clauses are optional (as long as there are at least one). Can it be done *with a single macro*? (Yes, with lisp macros. No, for most other languages.)
Cool idea, although the animations a bit too fast for me. Also, shift + 0 is not unambiguously ), (for example, I have ( and ] swapped). Does anyone use all these? For the record, 7 commands are not in my current roster. I see a use for 4-6 of them. 
I wonder if adding Japanese support to the editor was prompted by consumer demand.
With LispWorks you can implement something like Mathematica completely in Lisp. With Mathematica you can't implement LispWorks. Mathematica is a relatively specialized high-level language for math related stuff with an environment for that. The implementation technology of its language is relatively primitive. Thus much of the Mathematica environment is implemented in C++. if you look at the manuals of LispWorks, you will detect that it has an extremely wide range of capabilities. It's IMHO the best implementation of a dynamic language. As such you can implement a lot of things directly in Lisp, where one usually would need C++ or maybe Java.
The editor already supported Japanese, IIRC. It's just that they now support a wider range of Unicode than before.
Go with ClozureCL - it has native threads support on RPi, and SBCL currently does not. The speed difference is noticeable. [I wrote a blog post recently](http://temporal.pr0.pl/devblog/2015/03/07/lisp-on-raspberry-pi-2-part-1-setting-up/) on my experiences with setting up CL on RPi2.
Hey LispWorks, $100 is what I'd pay right away. I know other people who feel the same. $500 is a bit too much.
Then why call it "Hobbyist Edition"?
Because it is for hobbyists?
I consider myself a hobbyist, and I can't afford it. Neither can any of my fellow hobbyist I could remember. That is not a LispWorks problem, of course, but I'm not sure who is the market for this product. Look, if I owned a startup or say my employer needed a lisp this price tag would not be a problem. But when you say "hobbyist", it all of a sudden turns into an obstacle. It's not justifiable. Not to me, at least, sorry :-) 
mocl is basically a delivery tool for a CL subset, creating C code plus a runtime. Great for what it does, but very different from LispWorks.
There are lots of people who can afford that. An iPhone costs more. There are more expensive sneakers. Many bicycles cost more. There will always be people who can afford it (or will be able save money for it) and some who don't. But, hey, Common Lisp has other implementations which don't cost anything.
The hobbyist edition also has no heap size limit, it can save and start images of the Lisp state, can load patches and loads init files upon start. None of that above is in the Personal Edition, which also only runs for five our sessions.
Also, Hobbyist comes in 64 bit, and Personal is a version behind. 
Very happy they did this. I've been hoping they would do this for years and the response I always got on various forii (and on here) that thousands of dollars was just your typical hobby purchase and gut up. Well perhaps for Moneybags McCarthy Jr, Hobby Lisper Extraordinaire, but certainly not for me. $750 isn't cheap but its within the realm of reason for me at least.
sbcl ftw
Enhanced unicode, ARM support, Android and IOS targets. Sounds like a pretty substantial move forward. Nice to see that the mobile targets are at no additional cost.
A hint at what exactly? That charging an arm and a leg for something that is used by a fraction of a percent of the market isn't a good business model?
Charging an arm and a leg for something that is used by a fraction of a percent of the market is the only possible sustainable business model for it.
I recently had some weirdness with slurping forward: it never seemed to want to do it. This confirms that I was doing it right. :/
Does Racket actually work on iOS?
Very cool. I was faced with rolling my own and putting off a project because of it. Thanks for saving me the trouble!
LispWorks 7
I'm interested in seeing how the new UDP socket support is. My experience of UDP networking with LispWorks on Windows has not been great, but I guess that's partly because the usocket devs have had to roll their own LispWorks backend. 
64bit Hobbyist costs 1.5 times the 32bit...
No, it is not.
How would you propose that they fund continued development of their product?
not as pretty but smaller learning curve
&gt; ahh, can't think of any I'm sure this comment doesn't carry any weight, and will get deleted by the almighty "lispm": with this level of ability to apply logic you have no business being in business. Please PM me your info so I make sure to never hire you in the future. E: lispm who clearly has a chip on his/hers shoulders: https://news.ycombinator.com/threads?id=lispm and who obviously works for LispWorks. What's your agenda, son? I'm going back to work on my boat and to enjoy my retirement — you go fight your inexistent fights.
Are you familiar with logic?
Who is the person and what is the product? *edit* Oh, I think you're complaining because lispm uses and enjoys LispWorks. I do too. I also enjoy SBCL, Clozure CL, and other Common Lisp implementations. Rainer Joswig has used all kinds of Lisp implementations for many, many years. I don't always agree with his opinions but I think he is a good moderator for /r/lisp. I think it's a little silly for an anonymous, one-day user to attack him for exercising moderation power. 
He is unhappy because I've deleted two of his messages (not to me). The first: 'It's a pointless conversation. You aren't going to see the other side, and I'm too old, too tired, and too busy to argue with know-it-all 25-year-olds.' The second: 'Surely you jest, kid.' Then: &gt; I'm sure this comment doesn't carry any weight, and will get deleted by the almighty "lispm": with &gt; this level of ability to apply logic you have no business being in business. Please PM me your info so I make sure to never hire you in the future. &gt; E: lispm who clearly has a chip on his/hers shoulders: https://news.ycombinator.com/threads?id=lispm and who obviously works for LispWorks. &gt; What's your agenda, son? &gt; I'm going back to work on my boat and to enjoy my retirement — you go fight your inexistent fights. I've asked him to stay friendly and polite. Response: &gt; '"Polite and friendly" isn't the same as "someone who agrees with me and the product I'm peddling", hon.' 'aol-zombie' is an account active for one day. Personally I have enough of trolls, spammers, ... and I prefer not to see this turned into another comp.lang.lisp . 
What a maroon.
&gt; ... and then it becomes a cognitive burden on the programmer to figure out the right delimiter to put in depending on the context. I kind of agree that s-expr is kind of minimal elements to represent hierarchy structures. However, having a pile of elements does not make comprehension easy. For typical lisp programers, they call upon indentations to help sort the elements. The reason I emphasize indentation is because they are important. Lisp programs without indentations are impossible to work with. Now if you use indentation, yet you still insist on parentheses, then you end up with duplicated functions and (typical) lisp is no longer so minimal any more, right? How about let programmer skip a couple level of parentheses where indentation are used? Of course, if we allow that, indentation become another set of delimiters and lisp is no longer so elementary anymore. Back to OP's quote, try assess the real cognitive burden by removing all your indentations. What I point out is the fact that lisp is workable purely due to the fact of employing multiple set of delimiters (paren and indentation), indentation not being in the official syntax just make that not obvious. To assess cognitive burden, we should differentiate between short range and long range. For short range, when all the relative items fits in our working memory -- 7 +/- 2 items -- it is often easier to work with multiple delimiters, such as 3 + 5 * x + 6 * x^2 Turn it into s-expr equivalents, the parentheses become noises and add into cognitive burden and over-flow our work memory capacity. So at short range, syntax reduces cognitive burden. Currently the way we design programming language always insist global context free syntax. So all short range syntax spread to long range, that gives lisper the argument that lisp is easier to track. I think in principle, short range syntax should be always limited to short range (7+/-2) items, and beyond that, uniform indentation; which will allow unlimited DSLs in a single program (yet with uniform simplistic long range syntax). At long range, we don't see syntaxes, all we see are semantic elements organized by containers. EDIT: e.g. the quotation example in OP's post, if we limit the syntax to short range, then we can safely omit quotes in most situations where sorting out the semantic meaning is easy (even at compiler point of view). But if we insist on global syntax, then we need different quotation marks to ensure global safety, which forces the syntax onto long range cognition burden. For long range cognitive tasks, we can't track all items any way, so we need hierarchy containers. But we still need track the delimiters at all time so the minimal the delimiter syntax, the better. So either uniform parentheses/braces or indentation. I prefer indentation, which is one element to track. All brackets/parentheses/braces have two elements to track and at long range is impossible for human to track. Even lispers agree which is why they rely on indentation to reduce long range cognitive burden. [ oh the OP's quote is also misleading, even in Java, there is only one set of long range delimiters, curly braces. I don't remember any other. EDIT: but if we consider syntax beyond symbols, Java has classes and methods which can be argued as two sets of long range syntax (, which I agree are absolute cognitive burden). ] EDIT: I am not arguing against lisp, but I am arguing against OP's view that it is the uniform syntax made lisp. I think it is the ability to create macro made lisp. If we think syntax beyond symbols but semantic meanings between words, then macros are essentially local syntax that user creates. So I think it is the ability for user to create proliferation of localized syntax made lisp.
I hope that someone is up to the task. Clisp is the one implementation that's installed/installable *everywhere* (usually by default) and is also probably the easiest to pick up using from the command line. It's not particularly fast but for many scripting or learning purposes, that doesn't matter at all. That's not to say it can't do real work as well. I've successfully run a lot of projects in clisp quite well that were built in CCL/SBCL. Yes, it's a bit slower, but if you can say "This app runs in clisp" then you've expanded your potential user base quite a bit instantly. It's important this implementation sees progress because it's Common Lisp's hook into almost everybody's machine. From a strategic grow-the-community standpoint, it's a great trojan horse that we're just not leveraging.
I just tried building clisp 2.49 on Debian 7, and it died on libsigsegv, libffcall, libreadline, and libncurses, respectively, without any warnings from ./configure about potential issues. clisp might have been easy to build at one time, but that reputation doesn't match up to reality for me much of the time.
I've got a bit of LISP and decent C chops, but I'm mostly a JS dev. How can I get involved with maintenance?
Precedence based expression parser is not particularly difficult. Imagine in the macro facility you have a function ast = parse_expression(op_precedence_table, expr) then you just had the ast type of macro power without s-expr. In practice, I rarely create DSL syntax that require many precedence, often only need observe brackets and quotes, which I have tlist = proper_split(line_of_code) Even more practical, for most macros, regular expression is sufficient. If I need syntax spread over entire program, that is difficult to design. But for DSLs that lead by functor-like keyword (not much different from s-expr), arbitrary syntax with regex, proper_split, parse_expression covers all my needs.
&gt; interpreted languages There is no such thing and most Common Lisp Implementations are compiled. Don't know about CLISP though.
Not a full solution, but having a "transitional maintainer" just to move the project to a GitHub, create a GitHub organization, clean up the old website a little, would help future contributors a lot.
I think vendoring in a copy of the library would be reasonable, however, since it's used by so many other languages, chances are the user already has a copy (on Linux or OS X) and it can be dynamically linked. I guess it's a question of how large the library is.
Too few people share your view of how it should be done, apparently, because no Common Lisp that I know of has taken that approach. CLISP is somewhat unusual by doing a large part in a C-like language and linking to a bunch of shared libraries for certain features. It's more common to provide GC, FFI, and some other stuff via a smallish C runtime, and to provide almost everything else (like the debugger, compiler, standard library, etc) via Common Lisp code. 
Nah, "fast enough" is misuse of CL. CL is ultimate compiler and best tool for almost everything and to squeeze perf in particular. People who look at it from different angle don't see the point. Edit. The situation is worse than I thought :)
Plot look interesting and I will take a serious look at it.
Something about lisp that I think has to do with the parenthesis and I haven't experienced with any other language is that expressions between parenthesis are like **self contained building blocks** that I can compose, move around, refactor and evaluate. It is very easy to compose those building blocks because everything is an expression, and also lisp supports particularly well the concept of locality, both for variables (lexical variables), macros and functions (local functions) (let, flet, macrolet, ...). That helps with the "self containment" part. I imagine that without the parenthesis it would not be as easy as it is not obvious where an expression ends; with the parenthesis, that is explicit. Also, I agree with the author of the article that having parenthesis and only parenthesis reduces the cognitive burden. At least, it works like that for me.
I read Ron's blog post as being about how the classic Lisp style is about an interactive style of coding in which the programmer communicates in real time with a running program, teaching it how to do new things. His remarks about s-expressions explain that they've survived because they have some useful properties for that style of coding. It seems like most commenters here have missed or glossed over the central point--the one about the interactive style of programming--and fixated on the same peripheral discussion of syntax that talk about Lisp so often seems to degenerate into. Yes, it's true that s-expressions were never intended to be Lisp's actual surface syntax. It's also true that, once someone implemented them, they proved to be sticky because they are actually handy. They work well in a language designed for interactive programming. Partly that's because of the points Ron made in his post. Partly it's because of related points mmontone raised here--that s-expressions are conveniently self-delimiting chunks of meaning that are easy to work with in text editors. These are peripheral matters, though. The central point is that traditional Lisp (and Smalltalk) coding is livecoding. The programmer is in a real-time conversation with the program, teaching it new skills and watching it perform them immediately, instantly. Although there are vastly more programming languages to choose from than ever before, and although many of them are exploring interesting new ground in lots of areas, it seems like languages that embody that livecoding spirit are a smaller and smaller fraction of languages in use. That's too bad for those of us who prefer live conversation with our programs. I can't help but feel we're in danger of losing something valuable. It's a spirit that I feel like is missing or attenuated even in some newer Lisps, and it seems like a crying shame that even Lisp itself would begin to lose this essential Lispiness. I have a friend who is an accomplished programmer who has recently become interested in learning Lisp. He reports that what really switched on the light bulb for him was when I was describing the standard Common Lisp functions that enable you to change the definition of a class while a program is running, and have the reasonable expectation that the program--including all the existing instances of the class--will continue to work. More specifically, the lightbulb moment was when I said to him: of *course* the programmer will sometimes want to change the class definitions; and of *course* he or she won't want to have to kill the program and restart it just because of that; and of *course* everything should be able to just keep working. Well, and of *course* Common Lisp recognizes that and is designed to handle all of it gracefully, because it's a language for people who want to work that way. I really like a bunch of other languages, but I keep coming back to Lisp and Smalltalk because of this. 
A github repo would make it easier for potential contributors.
One of the biggest strengths of CLISP is its small executables. Not as important now as it used to be, but it's a pretty useful property.
" Although I put a lot of effort into making the output legible, it really isn't aimed at producing publishable scores. If you need beautiful output, use Lilypond, Score or Finale." -- https://ccrma.stanford.edu/software/cmn/cmn/cmn.html
I'm not sure what you mean by "visual" but a structured editor for "strongly typed documents" as a source code editor might work great for smartphones and tablets, simply on the basis of not forcing the user to hunt for pixels to position the cursor and by allowing the use of a "contextual keyboard". Every editing operation would preserve correct "syntax" by nature. Some of the things I've been recently thinking about was 1) how to model the language structure abstractly (strong typing of the AST basically replaces the language syntax/grammar, but requires some kind of type system as a metagrammar to describe how to describe the languages themselves, plus visual presentation is an additional thing to describe), 2) how to express the language typing rules (a Shen-like programmable type checker, perhaps?), 3) how to express the translation/compilation process, 4) how to generalize the lower layers over all possible programs (what kind of lowest possible code representation is desirable?), 5) how the make the whole thing maximally modular to minimize the amount of "source code" (which would be a data structure, since the system, naturally, deserves to be circular).
Yes, I did read that line about the final output. I was thinking about any other comparisons - ease of use, learning curve, was there something that the author was trying to achieve that caused him to recreate the wheel and was that achieved?
What about doing a macro system with something that looks like conventional syntax but isn't? Like a graphical presentation of the syntax tree?
Today? That train has already left the station... Smalltalk and Lisp companies have killed their respective market prospects a loooong time ago. But that's entirely their fault.
Please do, I'm very interested.
The one big thing about Haskell that puts me off from getting started with it is the syntax. The core language ideas are nice and interesting, but I just have to get over the syntax before I can make progress with the language.
The syntax is really tricky at first. I blame it on infix operators.
I wrote it more than 20 years ago. At that time there weren't any decent free music notation programs. 
maybe; somewhere in my infinite TODO list is the line "translate cmn to scheme"; another is "make it possible to select a section and move it, with everything else adjusting at the same time". When I wrote it, we had 68040-based NeXT's with ACL (which came for free on that machine); a batch-process was about all you could hope for. I made one big mistake (I realize in retrospect): heavy use of CLOS.
The standard version of Mathematica costs $2495 or $995 per year. 8 core. 'Enterprise' $6995. 16 core. Can I develop an application with it and ship that? &gt; 'Wolfram products are usually licensed to be installed on a specific machine—moving the software to a new computer requires an update to your license.' &gt; 'undertake a System Transfer only if Your Mathematica service level permits the transfer; You contact WRI Customer Service and, at its direction, complete and submit a System Transfer Form; and You pay any license cost difference or applicable transfer fee;' Oh oh... I have to transfer the license, if I want to use it on another machine... LispWorks is a pretty good deal compared to that. Not licensed for a particular machine. No core limit. Free application delivery with the HobbyistDV, Professional and Enterprise version... That means you can develop an application with LispWorks and use it everywhere without paying royalties. With the Professional and Enterprise version you can sell LispWorks applications - again without paying royalties and without the need for a LispWorks license for the users. Royalty-free application delivery is not possible with Mathematica. LispWorks is cheaper for commercial users than Mathematica and has better licensing terms.
Have you tried creating a [bundle](http://www.quicklisp.org/beta/bundles.html), with only what you need, it might be better than loading Quicklisp, I think, if I understand well how bundles work.
Could you expand on why do you consider heavy use of CLOS a mistake?
Just to not produce misinformation: I didn't have any problems transferring Mathematica Home license between 3 of my laptops. First time I did it 4 years ago and I asked to transfer the license in support, they helped me out. The old one was still working unless I ran 2 instances on 2 laptops simultaneously. Next time I did it few weeks ago and the matter was just to login to my account on their page and download/install the app. Again the old version is still working. So it is not true what the transfer of Mathematica to another machine is a pain. In contrast, I can't see the customer portal or something like this on LispWorks site. What if I loose my downloaded and paid version? Also I would like to compare home/hobbyist licenses, the enterprise could cost whatever they want since it is not an ordinary people paying for them the hard-earned buck out of their pocket. The Hobbyist version has 4 different price tags: 1) Hobbyist 32bit without application delivery - 400EUR. 2) Hobbyist 64bit without application delivery - 600EUR. 3) HobbyistDV 32bit with application delivery - 800EUR(!!!) 4) HobbyistDV 64bit with application delivery - 1200EUR(!!!) The direct comparison could be made only between Hobbyist without application delivery and Mathematica Home, since it doesn't have application delivery as well. And therefore the advantage you are talking about the application delivery is vanished at this price level. For anything with the possibility to make a stand-alone executable and give it to your friends(which is the decisive advantage over Mathematica Home) you obliged to pay at least 800EUR P.S. I'm not trying to be against this product, in contrast I'm thinking about making it my primary platform for hobby projects, but for me the ability to make stand-alone apps with UI is one of the decisive factors, and I need to justify the spending to the family, which is hard :) P.S. Edit: corrected price for 64bit DV version, see original in the quotation below.
&gt; I didn't have any problems transferring Mathematica Home license between 3 of my laptops I didn't say you have problems. I did say that with LispWorks it is unnecessary. Completely. &gt; First time I did it 4 years ago and I asked to transfer the license in support, they helped me out. I don't want to talk to customer support, because I want to install it somewhere else. Not really. &gt; In contrast, I can't see the customer portal or something like this on LispWorks site. What if I loose my downloaded and paid version? Just download it again. You'll get a link with buying LispWorks. &gt; The direct comparison could be made only between Hobbyist without application delivery and Mathematica Home, since it doesn't have application delivery as well &gt; 4) HobbyistDV 32bit with application delivery - 1200EUR(!!!) 4) HobbyistDV 64bit with application delivery - 1200EUR With Mathematica I have no choice... Mathematica has no equivalent of a HobbyistDV version. It also has no equivalent of the Professional Version and the Enterprise version. I don't find it strange that someone wants to give an application she wrote to her friends. Mathematica can't generate freely distributable applications. Mathematica has some 'Player' license option, which costs almost $1000 for 5 instances... For a 'Player' for Mathematica Notebooks.
I hesitate to say anything; it was years ago, and the CL community has become much less collegial. I'd say that CLOS is over-engineered; it has a medieval obsession with heirarchy and names that leads to rigid, opaque code; whenever I look at that code, my heart sinks. I'm trying a slightly different approach in s7, and translating cmn would be a great test ot it. If only I had two lives. 
how about org-mode ?? the following are fake org-mode html pages generated by emacs' htmlize.el http://the-little-language-designer.github.io/cicada-nymph/core/show-all.html http://the-little-language-designer.github.io/cicada-nymph/show-all.html
I did my thesis coding in common lisp, and I prototype most of my hobby coding in Racket (scheme). Mostly I'm a hobbyist programmer keeping my skills fresh by dipping through Project Euler and the occasional work-related text processing script. My roadblocks with Haskell were: 1. I could never figure out how to operate on unsafe data (such as file processing) using a monad. 2. Every now and then, I'd hit a problem where the simplest solution involved iteration over mutable data, and had difficulty figuring out the recursive, immutable, and lazy solution. 3. Evaluating a form that returns the wrong data type (sometimes intentionally) can be a useful debugging tool. 
i have ~7 years of CL programming experience, but only ~3 months of Haskell, so this might be not a fair comparison, but still... What I like about Haskell is its type system and type inference. Compile-time type checking helps one to organize thoughts and to avoid mess; and thanks to type inference, it doesn't get in a way. Also Haskell's syntax is great for functional programming style, and so I'd prefer it for projects which are complex and can be naturally written in functional style. But I noticed that when one needs to combine several monads, Haskell code can get ugly, as one needs to sprinkle `lift*`, `do` and `return` all over the code. Also I noticed that there is a plenty of language extensions which are commonly used (which implies that without these extensions core language is kinda deficient) and there is a "DLL hell" sort of a problem: to compile a latest version of some app or library you also need recent versions of other libraries, which usually require a recent version of GHC. E.g. I have Debian 7, which was released only 2 years ago, and it comes with GHC 7.4. That's really old by Haskell's standards, I can't run modern stuff. And the Haskell Platform which haskell.org recommends doesn't run on Debian 7 either. (OK I just checked it and it looks like Haskell Platform recently got out of fashion, good.) In this respect I'm much more comfortable with Common Lisp which didn't change for the last ~25 years. So anyway, in my opinion, neither Lisp nor Haskell is perfect. I'd like to see a language which combines their features: static typing, algebraic data types, expressive syntax for functional programming, but also an easier way to do IO and state, and macros.
Yes, the meme is well known and your lisp enthusiam well-noted. Quite frankly, such fanboy, spammy comments reflect badly on the helpfulness and maturity of the lisp community as a whole, and I am sure there are serious, intelligent lispers here who would not wish to be cast in such a light. Please reread the original post and if you have something serious and valuable to share, I would be much obliged to hear your input. Thank you.
This sentiment seems to be non-singular which I find surprising and sort of disappointing. I thought the lisp community as a whole would consist of more folks able to look deeper and appreciate sharing *concepts* instead of just reiterating what seems alarmingly akin to religious tokens. "I tried traveling to Morocco once, but their fashion was just absolutely *atrocious*. Can't go there until I get over it." Seriously, guys?? Lisp has *so much more* to offer than simply some abstactly beautiful syntax (which it *does* have)! Homoiconicity is really a big deal and changes how we code. Let's discuss these kinds of issues in a way that really relates to Getting Things Done^(TM). How does working in a homoiconic language really affect your day to day work flow? When everything is an sexp, how is long term maintainability of code affected? *Et hoc genus omne*. Please, for the sake of the community as a whole, let's engage in discussion of real issues?
This response is of higher quality than most comments here. You honestly and straightforwardly sharing your experience is much appreciated. Would love to hear more in a top level comment if you have the time.
I did a lot of different things in CL, ranging from web programming to machine learning and signal processing research. As for Haskell, my experience is rather limited. The biggest one I did was a strange kind of statistical analysis/prediction, which involved generating huge-ass Excel files. Currently I'm using Haskell for cryptocurrency research, our goal is to describe non-trivial cryptocurrency logic in Haskell and plug it into an existing wallet implemented in JS, so that it will work on all platforms.
"We can have as many lives as we want, as long as we can" Anyhow thanks for your honest response.
Thank you. I actually find it difficult to give comprehensive responses on an iPad which is what I usually use for Reddit browsing for personal enjoyment. That said, pharpend above gave a bunch of good comments as well. I'd say his points 2, 3 and 6 particularly are apt. There is no Haskell equivalent to the fantastic IntelliJ however. I have found these comprehensive IDEs to be a huge help as I have aged and taken on more architecture and management responsibilities and have less time to devote to the lowest layers of software engineering. There really is something magical in the "if it compiles it probably works" meme of Haskell. Likewise, there is something awesome in finding a perfect library function just by searching for ones matching your desired signature. Finally, forcing yourself to think in terms of pure functions all the time by default really helps all your software engineering. Getting good at Haskell will stretch your mind and improve all your engineering, even if you never use it in production. It will also frustrate the heck out of you from time to time as you adjust...
Thanks. Sounds like you have been around the block a few times with CL.
I fear you're being a bit idealistic. Sure, as an academic, the points that you have listed out make perfect sense. Most of us who are keen to try out new languages are working in the industry with whatever brand of poison we are lucky enough to be banded with. And learning a new language with the aim of applying it at work is quite a big undertaking. The smallest barrier to picking up a new language is the syntax, not the actual paradigms of the language. That's how we move forward and become competent in the core concepts of the language - a process that takes years. Syntax does matter in my opinion. A few years back when I was evaluating dynamic languages, I found Python much easier for me to pick up instantly than Ruby (even though they're both very similar). It just made things easier for me. So we shouldn't *not* discuss syntax just because the language has so much more to offer. It's definitely subjective, and I merely listed out what works and doesn't work for me! I think you should step down a bit from your slightly disturbing harangue, and realise that for a lot of people, practicality does trump idealism.
Sorry. I'm not a native speaker. And thanks for the info. Is there a way to correct it?
https://uxul.wordpress.com/2008/06/02/ein-paar-wege-den-pc-zum-piepen-zu-bringen/
This is a community, we aren't necessarily only responding to you.
You are entirely correct. And as such it is important we recognize that throwing out stock comments about lisps greatness and such create little but a sense of hostility and unhelpfulness. Check out the x-post over in /r/haskell. There are many content-filled comments not directly related to my original question. However, nowhere does anyone spam with completely tangential and unfounded remarks about Haskell's supposed superiority. Much less top level comments. This makes Haskell look considerably polished, attractive and friendly as a community. I really like lisp as a language from my limited exposure, and the fact that the lisp community doesn't emulate such behaviour is saddening. Don't get me wrong; there are several people here in /r/lisp that have contributed generously and been helpful, and I am grateful for their input. Let's have more of these intelligent, content-filled and inviting contributions!
Does this really require a brand new Lisp interpreter? 
No it doesn't. It's not an easy task and it would be much simpler to redirect requests to an existing interpreter. But I simply like to write one. Maybe I offer to choose beween interpreters in the future.
You forgot to add 25% of VAT to these prices :( However I think I'll give a Hobbyist edition a try anyway. Pay once cry once...
I successfully registered yesterday. Have you seen that there are two anti-spambot fields?
No, see Hy, the lispy python (or pythonic lisp) ! http://docs.hylang.org/en/latest/
I found the [Scope and Extent](https://www.cs.cmu.edu/Groups/AI/html/cltl/clm/node43.html) chapter of CLtL2 helped me understand it. It provides definitions and examples. A lot of practice will help you master it.
Mostly you want lexicals. Mutable (changeable) variables are tricky if you don't know their scope - i.e. which bits of the program can update them. Lexical variables have their scope laid out directly in the program source. You can see where the parens which close their scope is. The scope of dynamic variables depends on which functions call which other functions. They are therefore, in general, harder to think about. All that said - dynamic variables have their uses. They are essentially "context". Linguistically, you and I might realise that we are in a conversation about the president of the US, so when we say "he" we understand who we're talking about. The context is important. A dynamic variable is like saying "he" - the same sentence might mean two different things depending on what the 'current' meaning of 'he' is.
dynamically scoped variables (special variables) in common lisp have global scope. They are available everywhere. lexically scoped variables (normal variables) in common lisp have lexical scope - they are available locally within the structures that defined them. The vast majority of variables you will use in lisp are lexically scoped. That's the default. 
Here's two *very simple* examples: ;; lexical (let ((x 8)) (funcall (let ((x 5)) (lambda () x)))) ; =&gt; 5 ;; dynamic (let ((x 8)) (declare (special x)) (funcall (let ((x 5)) (declare (special x)) (lambda () x)))) ; =&gt; 8 In the lexical example, the variable `x` retains the value from the closest enclosing `let` *in its definition* (which is 5). Even though x is redefined later on, it is still 5 because the `x` outside of the function definition is a different `x` to the compiler. In the dynamic example, the function returned from the inner `let` looks for the variable `x` *as it exists in the current call stack*. So lexical scoping takes values from the nearest point from where they're defined, dynamic scoping takes the nearest value from where they are being run. Both have their uses, but lexical is the default (and for good reason). You can do cool things with lexical scoping you can't do with dynamic: (defun make-counter () (let ((x 0)) (lambda () (incf x)))) (let ((my-counter (make-counter))) (funcall my-counter) ; =&gt; 1 (funcall my-counter) ; =&gt; 2 (funcall my-counter)) ; =&gt; 3 ... So notice that the function returned from `make-counter` retains its reference to `x` even after the stack has unwound outside of `make-counter` and the lambda is returned. Congratulations, you just made a rudimentary class with a private member and a public interface function =]. Only when `my-counter` is no longer referenced will both it and `x` be garbage collected. Hope that helps. As an aside, if you're familiar with Javascript, that is also lexically scoped so you can expect many of the same behaviors when using CL. 
What is "adujucational" mean?
Dynamic variables mean that if the variables are not found in current function stack frame, it searches the upper function stack frame until top level and if none found, error is thrown. For example, if you have a `func-a` that has let a let with variables and calls `func-b`, then `func-b` can happily use all the variables in the let defined by `func-a`, even if no variable is defined in `func-b`. On the other hand, lexical scope does not allow this and simply throws error if it cannot find a variable in its current scope.
I meant educational. It isn't possible to correct it afterwards. 
New version is online. - Added functions '/=', '&lt;', '&gt;', '&lt;=', '&gt;=', 'ftruncate', 'fceiling', 'ffloor', 'round' and 'fround'. - The type check of parameters was not working properly for optional arguments. - The type-checking-error-message now highlights optional and variable length arguments. - Increased version number to 0.2 Please tell me if something isn't working properly. Thanks!
It is very incomplete, yet. That's why it isn't working properly. :-). Let's see what is looks like in 10 Months. Maybe it is a bit more useable then.
If this confuses you more, completely ignore what I'm saying; there's more than one way to reason about dynamic bindings. Lexical bindings mean that the value is bound for anywhere within that block of *code*: (funcall (let ((x 1)) (lambda () x))) =&gt; 1 Dynamic bindings could be 100% emulated by global variables and unwind-protect (the standard doesn't mention multi-threading). It acts as if the value were globally set on entry to the block, and unset on exit from the block: (defvar *x* 0) (funcall (let ((*x* 1)) (lambda () *x*))) =&gt; 0 This is functionally identical to (but probably isn't implemented as): (defvar *x* 0) (funcall (unwind-protect (let* ((was-bound (boundp *x*)) (old-value (and was-bound *x*))) (setf *x* 1) (lambda () *x*)) (if was-bound (setf *x* old-value) (makunbound *x*)))) Note that some implementations do not work this way on multi-threaded; the standard is silent about threads, so each implementation can have dynamic bindings work differently on different implementations. Note also that this means that dynamic bindings can interfere with tall-call-elimination, since what appears to be a tail-call might actually not be.
I think it is Lisp flavoured C++, not C. See also: L++ | https://bitbucket.org/ktg/l L++ is a programming language that transcompiles to C++. It uses Lisp-like syntax. 
&gt; It is very imcomplete, yet So, it is full of typos as well? 
&gt; I'm not familiar enough with Lisp Let's change the syntax first, before we get more familiar with Lisp. &gt; It doesn't look too different--I only removed a few layers of parens--but it's enough that the blocks are more clearly delimited, How so? Delimiters have been removed and things are now more clearly delimited? &gt; without having to count parentheses. (Yes, an editor should do that for you Nobody 'counts' parentheses. Maybe you should get familiar with actual use of Lisp first. The parentheses-using s-expression syntax is there, because of the code is data is code quality of Lisp. It makes the tree (or graph) like code structure explicit and allows easier manipulation in the editor. Sometimes large code structures get generated, which can both easily be manipulated on the text and data levels. Stuff like that gets more important in a language which tries to provide relatively easy computation with code. 
In defense of the poster, this is roughly how I write Lisp on the rare occasion I have to write it by hand.
As I understand it, both lists and vectors are seqs. A seq supports the Clojure equivalent of car and cdr (first and rest), so if you program at the level of mapping over a series of seqs then it is irrelevant if the underlying code is represented as a vector or a list.
I read his comment, your comment, your websites, projects, etc. Sadly, same feeling. I mean, in your web sites. For example, in http://marty.alain.free.fr/confs/ , I saw seemingly random changes of font color, style and size. Mixed use of Serif vs Sans-serif. It's nothing about lisp, it's about web design.
&gt; I'd like to see a language which combines their features: static typing, algebraic data types, expressive syntax for functional programming, but also an easier way to do IO and state, and macros. how do you think F# holds up to these? to me, it is the most real-world multi-paradigm language available.
[Have you read this?](http://www.cliki.net/Getting%20Started) I think it should help. If it doesn't help, feel free to ask. Other than Emacs, you also need a CL implementation installed as well. There are many of those, like SBCL, CCL, CLISP, ECL etc.. See the sidebar for links. SBCL is my personal favorite. Another thing you need is [Quicklisp](https://www.quicklisp.org/beta/), a software manager for CL, and Slime, a plugin for Emacs, installable via Quicklisp.
Crawl before you run. I'd suggest you download a CL implementation and just type in the REPL the new functions your learning (note some will need rlwrap in order to be usable). Several are available from apt-get; CLISP is probably the friendliest.
[CLiki](http://www.cliki.net/Getting%20Started) seems like the best place to start. 
I got my start on emacs because it's the best way I found to work with lisp. To add to the above: * `M-&lt;key&gt;`: M (Meta) is usually bound to &lt;Alt&gt;, so that's Alt+key. * `C-&lt;key&gt;`: Ctrl+key * `C-M-&lt;key&gt;`: Ctrl+Alt+key * `C-&lt;key&gt; C-&lt;otherkey&gt;`: Ctrl+key, then Ctrl+otherkey * `C-x C-f`: Find (open) file. Will prompt for filename (does tab expansion). Will create a new file if needed. * `C-x C-s`: Save the currently-open file * `C-x 3`: Split the buffer left/right * `C-x b`: Change current buffer; use this to change to `*slime-repl sbcl*` after doing `C-x 3` to have both the source file and the REPL visible; this is my preferred layout. * `C-x o`: Switch to other window. * `C-M-x`: Evaluate the expression the point (caret/text cursor) is on. This requires that you're in a .lisp file, and you've already started slime with `M-x slime`. Useful when you're incrementally working on a source file; I usually do this after every side-effecting s-exp (`defun`, `defparameter`, `defvar`, `setf`, etc) * `C-c C-r`: Evaluate region (highlighted area, or entire buffer if nothing highlighted). Good for starting up a new session with an existing file. This is all you need to get started, and it's less than it might seem; I got used to two-thirds of these bindings after a day, and the rest took less than a week.
Most lisp tutorials assume you are in REPL all the time, but simple command line script examples are missing. Consider this: If I want to distribute my Python script I only have to instruct user to install Python (click NEXT, NEXT, NEXT...), fix PATH and then run python &lt;script.py&gt;. Anyways. DrRacket got this right.
I definitely recommend this approach before getting started a more fancy development environment. Here's some basics commands that help: * (describe 'describe) COMMON-LISP:DESCRIBE [symbol] DESCRIBE names a compiled function: Lambda-list: (OBJECT &amp;OPTIONAL (STREAM-DESIGNATOR *STANDARD-OUTPUT*)) Documentation: Print a description of OBJECT to STREAM-DESIGNATOR. Known attributes: call, unwind, any Source file: SYS:SRC;CODE;DESCRIBE.LISP.NEWEST * (describe 'inspect) COMMON-LISP:INSPECT [symbol] INSPECT names a compiled function: Lambda-list: (OBJECT) Known attributes: call, unwind, any Source file: SYS:SRC;CODE;INSPECT.LISP.NEWEST * (describe 'apropos) COMMON-LISP:APROPOS [symbol] APROPOS names a compiled function: Lambda-list: (STRING-DESIGNATOR &amp;OPTIONAL PACKAGE EXTERNAL-ONLY) Documentation: Briefly describe all symbols which contain the specified STRING. If PACKAGE is supplied then only describe symbols present in that package. If EXTERNAL-ONLY then only describe external symbols in the specified package. Known attributes: call, unwind, any Source file: SYS:SRC;CODE;TARGET-PACKAGE.LISP * (describe 'load) COMMON-LISP:LOAD [symbol] LOAD names a compiled function: Lambda-list: (PATHSPEC &amp;KEY (VERBOSE *LOAD-VERBOSE*) (PRINT *LOAD-PRINT*) (IF-DOES-NOT-EXIST T) (EXTERNAL-FORMAT DEFAULT)) Documentation: Load the file given by FILESPEC into the Lisp environment, returning T on success. Known attributes: call, unwind, any Source file: SYS:SRC;CODE;TARGET-LOAD.LISP 
I second this, also this course on edX https://www.edx.org/course/systematic-program-design-part-1-core-ubcx-spd1x uses Racket, and it's very good, and follows the Design programs book. It's starting in a couple of weeks.
Lisp is definitely worth learning and using in 2015. I honestly don't know if there're any good tutorials out there. Part of the issue is that Lispers tend to customise their environments so much that what applies to me doesn't necessarily apply to you. There's also the issue that some aspects of getting setup can be fractally complex. For working through Practical Common Lisp, you don't _need_ emacs+SLIME, but it sure helps. If you're using emacs, you might wish to take a look at [prelude](http://batsov.com/prelude/), which is a nice config setup. But of course that's one more thing to use (remember what I said about fractal complexity?). Download &amp; install sbcl. Configure SLIME to use sbcl, setup SLIME with all the fancy contribus, then type M-x slime in emacs, and start working at the REPL. You can open files ending in .lisp and use SLIME commands like C-x C-e to evaluate expressions in them, and use other keys to load them into your running SLIME session.
I enjoyed practical common lisp and land of lisp. Understand that a cons cell is a structure of two pointers (car and cdr). Implement [McCarthy's original lisp](http://ep.yimg.com/ty/cdn/paulgraham/jmc.lisp) in C. Your understanding will be transformed. Also, try the little schemer. Scheme is a simpler lisp, more like C in that aspect. Scheme is lisp, but common lisp includes the kitchen sink. But understand, there is no lisp for dummies. It will blow your mind. Like when you learned to compose programs in unix? Or when you read an implemention of Forth. When you read the paradigms of artificial intelligence programming and implement prolog. Land of lisp is worth it just for building a web server in a single chapter. Strap yourself in and enjoy the ride.
I started out like you a couple of years ago. I tried to learn Racket and Emacs at the same time and ended up abandoning Emacs (for the time being) and using Sublime (my usual text editor). Try learning one thing at a time would be my suggestion. I second the suggestion by /u/TechnoSingularity to take a look at Racket and read Realm of Racket. Also take a look at the tutorials on their site: http://racket-lang.org/ Once you've got Racket under your belt, then I'd recommend taking a look at Practical Common Lisp (I'd recommend using whatever text editor you already know). Finally, I'd say that a Lisp is definitely worth learning. For me, the biggest insight has been: 1) learning to think functionally 2) and getting into the habit of more/smaller functions I do mostly do Python for my job and my Python code has gotten *much* cleaner, more reliable and better tested since learning Racket. I hope this helps!
I hope this short guide may help you use Emacs and SLIME to play with Common Lisp: - Install Emacs: `sudo apt-get install emacs` - Install SBCL, the compiler: `sudo apt-get install sbcl` - Install `gnome-tweak-tool`: sudo apt-get install gnome-tweak-tool Then [swap Caps Lock and Control before using Emacs](http://www.emacswiki.org/emacs/MovingTheCtrlKey#toc3). - Install [Spacemacs](https://github.com/syl20bnr/spacemacs): git clone --recursive http://github.com/syl20bnr/spacemacs ~/.emacs.d git checkout develop - Start Emacs. Spacemacs will ask you which editing style you want to use; choose `Emacs` (using the arrow keys) and wait for the packages to install. - After every package is downloaded, open the `~/.spacemacs` file with `Alt-m f e d` : it menas, you press `Alt` and while holding `Alt`, press `m`, then press `f` then `e` then `d` sequentially to open the file. Or, alternatively, you can press `Alt-m f f` and type `~/.spacemacs`. From now on, when I say `M-m` it means `Alt-m` or `C-m`, it means `Ctrl-m`. In Emacs, `M-` means `Alt-` and `C-` means `Ctrl-`. If you use Vim editing style, you can press `SPC` instead of `M-m`. - Press `C-s` to search for this string `dotspacemacs-configuration-layers`. In there, you see a list of configuration layers. A configuration layer is a collection of packages setup toward a feature. For example, you can see `auto-completion` layer there; it is a collection of packages related to `auto-completion` Spacemacs already created for you. You don't have to manually setup all the packages in there to get this features. Originally, `dotspacemacs-configuration-layers` looks like this: dotspacemacs-configuration-layers '( ;; ---------------------------------------------------------------- ;; Example of useful layers you may want to use right away. ;; Uncomment some layer names and press &lt;SPC f e R&gt; (Vim style) or ;; &lt;M-m f e R&gt; (Emacs style) to install them. ;; ---------------------------------------------------------------- ;; auto-completion ;; better-defaults emacs-lisp ;; (git :variables ;; git-gutter-use-fringe t) ;; markdown ;; org ;; syntax-checking ) Change it to looks like this: dotspacemacs-configuration-layers '( ;; ---------------------------------------------------------------- ;; Example of useful layers you may want to use right away. ;; Uncomment some layer names and press &lt;SPC f e R&gt; (Vim style) or ;; &lt;M-m f e R&gt; (Emacs style) to install them. ;; ---------------------------------------------------------------- auto-completion better-defaults emacs-lisp slime ;; (git :variables ;; git-gutter-use-fringe t) ;; markdown ;; org ;; syntax-checking ) Adding `slime` is important. It's your Common Lisp development environment. Then restart Emacs and wait for everything to be installed. After the installation is finished, you can start playing with Common Lisp. `M-m f f` and create a new file named `test.lisp` anywhere; you can use `TAB` to enter a directory and `C-l` to get out of a directory. Then press `M-x` then type `slime` and press Enter key to create a connection to `sbcl`. A REPL is opened to the right side of your buffer. Now you can either enter Common Lisp code in the REPL, or in your buffer then: - `C-x C-e` to evaluate individual expression. That is, you move your cursor to the end of any closing parentheses and press `C-x C-e`. - `C-c C-k` to compile and reload the whole file in your REPL. These two key bindings along with the REPL should be enough to give you a nice environment with Common Lisp. You can switch between buffers with `C-x b`. What is Spacemacs? Spacemacs is a user-friendly starter configuration. It includes common packages for writing Lisp. If you have any problem, you can always ask on the [Spacemacs chatroom](https://gitter.im/syl20bnr/spacemacs). When you first start Spacemacs, you will see `[?]` button for the quick help. In there it contains basic key bindings for newbies as well as Evil (Vim) and Emacs tutorial. Pick one according to your style and spend an hour to learn it, then you will feel comfortable with Emacs.
Yeah, even nowadays I sometimes code CL using Vim (or any other text editor) and reloading the updated files. It's no Emacs+Slime but you will get a taste of the possibilities.
This. This is what I'm doing.
http://www.buildyourownlisp.com/
http://weitz.de/html-template/#mail lists one option. I'm a little surprised html-template is not on https://github.com/edicl/ though.
I see the links to the mailing lists, but they are broken. I tried looking for the mailing lists on [common-lisp.net](https://mailman.common-lisp.net/listinfo) but couldn't find them there either. Am I just missing something painfully obvious?
I think that now is quite a good time to learn LISP. The language is getting more popular again and has an active community. With quicklisp, there are many libraries available and installing them is no issue anymore, Common Lisp has become "batteries included". Once you have an REPL set up, it's really easy to try things out and if you don't know how to do something, you google it. That's how I did it. It's not something that will get you a job, but it's fun. I had used emacs before I started learning LISP and had experience with other programming languages (mainly C++ and python). Because of that, installing slime for emacs was pretty easy for me and the introduction in "Practical Common Lisp" (the "gigamonkeys book") was sufficient for me, but if you are not as experienced with programming and Linux/Unix, you might be baffled at first. The target group of that book are people who have programmed before. But as was already mentioned, there are other ressources in the internet as well. One option might be Clojure. It's a really new lisp dialect and has grown to be really popular. Because it runs in the Java Virtual Machine (JVM), its code is easily portable and you has access to all Java libraries. As it's so new and popular, there are many great resources for total novices, just google it. It stresses functional programming and uses immutable data structures, you might or might not like this. And it's not for you if you want fast, compiled code.
Or just go download http://www.lispworks.com/downloads/ for free, and save yoruself a lot of time to just get down to writing Lisp examples. Avoid #lisp on Freenode as it can be very anti-noob, and will just frustrate you to no end. #emacs is a great resource however if you choose to go that route. This version of lispworks is free, and has a full gui with all the knobs without requiring all of the above to just write code. Then, once you feel you like it, there is always emacs+slime
I have spoken with Kent Pitman about the TeX sources; I've been interested in making a printed version sometime. In his view, the dpans3 TeX sources are fair game for anyone to create derived works, for free or for pay, without limitation. I haven't talked to him about the CLHS, but I suspect that the program that produces it will never be freely available and that the license on the output will never be changed. If you want something prettier, I think the dpans3 sources are the best place to start. I know of at least ~~two~~ ~~three~~ four people currently working on parsing and processing it. https://github.com/robert-strandh/dpANS-parser is one example. https://github.com/lokedhs/parse-lisp-spec is another. 
I don't see why the five hour time limit is a real deal-breaker. You save your files from time to time and just have to reload every so often. In exchange, you get a graphical environment that understands Lisp code and mouse clicks without having to grok GNU Emacs, which appears to be something new to the OP.
The problem is that there is no single copyright holder who can give the permission. People more or less involved give only opinions. E.g. I have contacted PARC a couple of years ago about dpans files on their FTP. The question is who is the injured party if one produces the derivative work without permission. The scenario might be following. Make the public derivative work and see who will try to claim the copyright violation and then negotiate. Of course the problem is that their lawers kung fu might be much stronger than of that guy. He should take care of that or be some other country resident for example.
I was told that Digital Press had released the old latex source for CLTL2 and allows anyone to use them as a basis for further work. Since I've worn out my second paper copy of CLTL2 I needed something online. I did minimal changes to make it Latex2e compatible. I added hyperlinks, most of the index, and lists of functions. I added hyperlinks to the functions in the table of contents. Try http://daly.axiom-developer.org/clm.pdf The sources are at http://daly.axiom-developer.org/cltl2_latex2e.tgz See the shell script 'm' for building. There are build comments at the top of the clm.tex 
Don't use CLtL2. It has many differences to ANSI CL. Some are hard to find. It's also a very confusing document, since it also includes the full content of CLtL1. It's a great piece of work, but it's more confusing than helpful.
I think it's pretty nicely formatted. It's a [motherfucking website](http://motherfuckingwebsite.com/), it doesn't need to look fancy.
I suspect dr_jumba is talking about the TeX sources from which the HyperSpec is derived.
Ah, I had assumed the TeX sources weren't "Hyper" which I was assuming the OP considered a necessary feature.
Unlikely, but I think that's moot. HTML is horrible to work with and the original XML source (done by a company called "Schema" in Germany) is lost. At least, when I tried to get it from them they ignored me.
I work on Allegro. You know me as [@envoy510](https://twitter.com/envoy510). I wish I knew more. The person that did the deal (in the 90's) is no longer at the company.
It was done at SCHEMA GmbH. They had a Lisp/CLOS-based content management system as a product. The contact was Marcus Kesseler... he is still in company management at SCHEMA GmbH... But that is all long ago.
Maybe we can organize some sort of kickstarter or bountysource to get this work done? To convert dpans3 TeX sources to XML or something else which could be used as a basis to generate the documentation in any form. 
There are several people working on processing the spec right now. I think it's worth waiting a few months to see how it pans out.
AllegroGraph is the only semantic database that can scale to trillions of RDF triples and still perform query in almost real-time. No one else come even close.
&gt; To convert dpans3 TeX sources to XML or something else... Are there existing XML schemas that would be appropriate for the task?
Dylan itself used to be Lisp flavoured. It's now called "Prefix Dylan", but was ditched in favour of the new "Infix Dylan" syntax. 
I believe DocBook5 will do the job, but probably something else is around?
Back up.
The gigamonkeys book does the "download emacs, figure and out, and let's do some lisp" on chapter 3, but on the next chapters it explains things better. I'd recommend skipping chapter 3 if you prefer - I'd say it's more of a chapter showing lisp's capabilities
Using the condition system for backtracking is wasteful. All you need is a single special variable and CATCH/THROW: the condition system provides all sorts of bells and whistles related to, you know, handling conditions, which are not needed here. See: https://gist.github.com/nikodemus/b461ab9146a3397dd93e for comparison. It swaps out the guts of the linked backtracking implementation with CATCH/THROW and gets 10 x the performance with less code complexity. 
This makes me wonder whether there might be a lot of other common usage patterns for the condition system whose performance could be drastically improved with catch/throw. Right now I'm toying around with building a system where the object-oriented nature of conditions is pretty useful, so that by signaling a condition of a more specific class I can handle it closer to the point where it was signaled, but in principle I can't see any reason why I wouldn't be able to use a few macros to wrap a similar mechanism around catch/throw instead.
You also need to take into account that using the condition system to effect a transfer of control is a two-step process: 1. Find and invoke the handler. 2. Handler runs (possibly transferring control.)
&gt; The open source versions don’t have everything you really need, e.g. multithreading. [...] &gt; Common Lisp is big, but doesn’t include as a standard libraries that are important for real applications — multithreading, networking, user interface. Commercial Lisp implementations have their own versions of these libraries, but they’re all different. SBCL, ClozureCL, ECL, and many other open-source implementations have multithreading and networking. Portability libraries also exist for them. Is the author not aware of those?
I'd guess he stopped using Lisp twenty five years ago or so. If he worked on TI Explorers, those were cancelled by TI in 1991.
Putting multithreading functionality in the standard for a language is actually a bad idea. Most libraries are very ad hoc. You can't really have a concurrency framework in a standard without a formal memory model. Defining a memory model for a language clashes with both compiler in place and platform you run on. So we stay ad hoc.
But how can you have a usable language with concurrency and side effects without some sort of memory model?
Python has a single definitive implementation with a massive standard library that includes batteries and the kitchen sink. And if you want additional functionality, there is usually at least one clear choice for a 3rd party library. This is tremendously helpful. I've read a lot about lisp, but this issue of libraries is a major stumbling block in actually writing any code. The first program I tried to write was to make requests for json data from a rest API. So I first started looking for a way to make http requests, and found myself having to [research http libraries.](http://www.cliki.net/HTTP%20client) *You would think sending a simple http request sould be a stupidly simple enough task that I wouldn't need to spend 15 minutes trying to choose a library!* When I finally settled on one, I then started looking for ways to handle the returned json object. Once again, I have to spend 15+ minutes perusing a ridiculous [comparison of json libraries,](https://sites.google.com/site/sabraonthehill/home/json-libraries) instead of, you know, actually programming. I eventually gave up, and went back to Python, where I don't have to spend stupid amounts of time trying to figure out how to do insanely basic things. I love the ideas of lisp, which have really changed the way I think about programming. But for writing what would be a very straightforward task in Python is a catastrophic mess in CL. 
So "too many libraries" is a problem now? I always thought the problem is that there aren't libraries for Lisp... /s
And then you just write `(jsown:parse (drakma:http-request url))` when you know the bunch of libraries you need.
Funny you chose http libraries as an example when Python's std library choice, urllib2, sucks so hard people use requests instead. And encodin JSON with the standard library is surprisingly unpythonic. When you want to teach the encoder about a new class, it doesn't work with a \_\_json\_\_ method as one would expect but have to subclass the encoder iirc. Also you mention the advantages of a blessed single implementation, but not its disadvantages. PyPy has to copy its functionality even when it doesn't make sense at is clearly an implementation detail just for compatibility, ie True being a monkey patched instance of 1, False for 0. Btw having CPython being the de facto standard wouldn't be so bad if it didn't sucked as much.
Hi! I made this library because I often get projects where all I need is a CRUD API + custom actions... and rewriting the CRUD part every time is boring. Feedback appreciated! Thanks
He is saying without an standardized memory model. Different implementations define their own.
Hah, I didn't actually know about that. Thanks. I'll try to add that through the `Link` response header. Edit: hm.. I'm not sure, actually. HAL seems to be the standard. But it's also (imho) ugly. The `link` response header seems to be the de-facto standard for APIs like Github's... not sure. I'll have to think about it.
there's always racket, which has a single community and implementation, and is actively working on ways of integrating the "too many definitions of better" into a coherent whole
&gt; My favorite language is Lisp. What happened? He stopped using it? He can't find a company that will let him use it? 
I don't think he was saying that as a bad thing, but as a reason why no one is using LISP in comparison to python, java, etc.
I was actually going to remove any mention of "REST" in the mean time, yeah. Thanks for your comments, but it sure sounds like HAL with less formalism.
While you can argue as much as you want about the quality of python's libraries, the simple fact is that the libraries are there, and for somebody new to the language not having to go in a quest to find them really encourages adoption. When learning a new language you usually want to do something simple but somewhat useful, so the http+json example is valid; and we have to recognize that is way easier to do that in python than in CL, sadly. Once you are engaged enough to find out the flaws searching for something better *looks* easier.
Oh smart, indeed! I was looking for a way to DRY up this bit of code, this is nice. Thanks!
And you think the lisp example is worse? I don't see your point.
No, my point is "Once you find a library, it's really simple" is not a benefit exclusive to lisp in this example.
It would be excellent if that list had the URLs at the top-level. I think I've found all of my packages that are on the list, but I'm not sure. If I could search for 'nklein', that'd rock.
I will do the ones I have access to now.
It's also hard to tell which are private and which are not. It would be easy if every system had metadata anyway, but I understand the reluctance to go to that trouble.
That's a fair point about cliki, I actually didn't know about it's recommended libraries list. I agree with /u/sionescu too, but I'm not sure we could expect somebody "new" to have due diligence. Again, I'm assuming we talk about "new" people that wants to get a try of this weird language called lisp (not even differentiating between the myriad of lisp dialects) and wants to hack something to get a taste of it. Most of the programmers I know are like that and that's were my http+json reference comes from. To truly appreciate a language you have to use it somewhat regularly and if you don't have a itch to scratch is hard to be motivated by going through a list of exercises from a book. And most day to day task require you to use a library to do some of the work. Devising a clever algorithm for a task or implementing a data structure is exciting, at least for some of us, but daily tasks usually are just gluing together APIs.
all systems in my part is fixed.
If clojure wasn't tied to JVM/Java (or worse javascript) it might have been worth spending some time with. As it stands right now, it's garbage of the worse sort for anyone that despises the Java ecosystem (which should be anyone sane really). Common Lisp doesn't compromise in this fashion.
Maybe in a few months we'll be able to move on to `:homepage`, `:bug-tracker` and `:source-control` :)
Looks really cool/useful For the lazy: https://github.com/Shinmera/chirp 
Subordinating systems in a single asd file to the system that shares its name with the asd file and reusing its author and license seems fairly foolproof.
The pattern I always use is to have a file foo.asd which contains defsystem forms for foo and foo-test. System foo can always be loaded with (ql:quickload "foo"), but (ql:quickload "foo-test") won't find foo-test unless foo has already been loaded. Clearly the system that has the same name as the .asd file is already considered special in some sense, so I'd expect we could use this distinction to automatically tag the others as subordinate.
I tried to install by cloning cepl and varjo into quicklisp/local-projects and doing (ql:quickload :cepl), but I get an error: System "fn" not found. Seems to be related to yesterday's commit?
fn &amp; temporal-functions are both other public repos on my github page. I need to a cleanup of cepl to remove these dependencies but I really like the lambda shorthand from `fn` so I keep avoiding doing it :D. I have also started the process to get those two &amp; varjo into quicklisp. But that, necessarily, takes time. Thanks for checking it out, I'm looking forward to making the process much easier [EDIT] I bit the bullet and remove `fn` so that's one less thing to clone.
I'd like to try that. How do you run it? 
Even when user macroes are banned, some builtin macroes are horror. Common Lisp loop contains suddenly flat list of items which are not definitively Lisp (or any other language), just list of symbols which may or may not be valid, it is all in the macromaker's whim. This is totally valid loop. As it expresses exactly what I want to: *(loop for x in abba for y from 1 to do dodo unless I do not if &gt; y 1 do format t " ~A x" else do do do do pili loop)*. If macro does not want it, they should fix the bloody macro. And of course it is totally my right and duty to fix this faulty product by myself. 
Great stuff. Keep up the good work!
Has the above text been generated by a markov bot?
Except for the development environment. Yes, I know there's options there, from Emacs, to Eclipse, to environments similar to IPython, but nothing on the level of something like the Lisp Machines, or even better Symbolics. 
Are you speaking in sarcastic voice or do you actually mean it? 
Bind that function to a key?
Eh whatever, macroexpand it or read the docs. I think the real problem with lisp was that nobody documented their shit. They just wrote it for themselves and expected anyone else to "figure it out." This is changing a lot lately though, people are actually starting to take some pride in their work. I think github (and others like it) are helping by making it really easy to publish a lib and then be accountable for it via issues. And slamming out a README.md is stupid easy too.
I use sbcl on windows so I can't help there I'm afraid. I'll be uploading the 'getting set up' video in a minute but it's really a glorified version of the readme :). Do comment if anything is unclear as I'm happy to add annotation to the video once it's up
Did the WJ bot learn to use Reddit? Should we be concerned?
Great work guys!
Ok here's the first one. Just a quick one today. https://youtu.be/6EN2OVmVz-c
Seems like you have all the pieces to make it into an Elisp function which *does* do what you need though?
I strongly agree. Could xach do it?
I disagree with a lot of these posts. LISP isn't cursed or disadvantaged by its power at all, the majority of programming languages have unique features or rules that can throw off beginners and, if we're being honest, most beginners are focusing on hello world and simple println() statements. I'd argue that LISP's (or common LISP's) problem is in community and exposure. Essentially LISP has terrible exposure, a lack of tutorials and out of date resources for people wanting to learn the language, when the top hello world tutorial on google is from 1996 it'll put many people off, it doesn't help that much of the documentation and learning material looks like it belongs in 1996 as well. I think LISP needs to modernise and get away from the image of a solo, super-knowledgeable hacker and move towards a more community driven method of expressing itself if it wants to grow, part of that would come from modernising, because when I compare [LISP's](http://www.lispworks.com/documentation/HyperSpec/Front/) methods of presenting itself to new users and [Python's](https://docs.python.org/3/), I can see why people don't come to LISP. 
&gt;Now make this thought experiment interesting: Imagine adding object orientation to the C and Scheme programming languages. Making Scheme object-oriented is a sophomore homework assignment. On the other hand, adding object orientation to C requires the programming chops of Bjarne Stroustrup. No, it doesn't. Making C object oriented requires you knowing how to debug preprocessor macros and a little bit of patience. &gt;Consider the case of Scheme, again. Since making Scheme object-oriented is so easy, many Scheme hackers have done so. More to the point, many individual Scheme hackers have done so. In the 1990s, this led to a veritable warehouse inventory list of object-oriented packages for the language. The Paradox of Choice, alone, guaranteed that none of them would become standard. Now that some Scheme implementations have their own object orientation facilities, it's not so bad. Nevertheless, the fact that many of these packages were the work of lone individuals led to problems which Olin Shivers wrote about in documenting the Scheme Shell, scsh. This is what happens when you use a tiny language for big work. No scheme object system comes close to CLOS for power, except maybe GOOPS. But writing in Guile is like writing in a worse Common Lisp. And CLOS is a standard. So this holds for Scheme, not CL. &gt;Programs written by individual hackers tend to follow the scratch-an-itch model. These programs will solve the problem that the hacker, himself, is having without necessarily handling related parts of the problem which would make the program more useful to others. Furthermore, the program is sure to work on that lone hacker's own setup, but may not be portable to other Scheme implementations or to the same Scheme implementation on other platforms. Documentation may be lacking. Being essentially a project done in the hacker's copious free time, the program is liable to suffer should real-life responsibilities intrude on the hacker. As Olin Shivers noted, this means that these one-man-band projects tend to solve eighty-percent of the problem. This builds off the previous point, and having addressed that point, I don't feel the need to address this one. &gt;Therefore, those who already know C don't ask "What object system should I learn?" Instead, they use C++ or Objective-C depending on what their colleagues are using, then move on to "How do I use object-oriented feature X?" Answer: "Goog it and ye shall find." C hackers are incredibly opinonated. Most don't like C++, and they could opt for GObject, structs with function pointers, whatever, instead of C++ or Objective-C. &gt;Real Hackers, of course, have long known that object-oriented programming is not the panacea that its partisans have claimed. Real Hackers have moved on to more advanced concepts such as immutable data structures, type inferencing, lazy evaluation, monads, arrows, pattern matching, constraint-based programming, and so forth. Real Hackers have also known, for a while, that C and C++ are not appropriate for most programs that don't need to do arbitrary bit-fiddling. Nevertheless, the Lisp Curse still holds. Translation: Real Hackers are trendy automatas that base technical decisions based on what's on the HN front page. I'm not analyzing any further. This is sloppy work of the sort people can't help themselves to write about Lisp. To any readers out there, waxing philosophically about Lisp doesn't make you cool, and it won't make Yahoo buy your startup. The next time you want to write a blog post lamenting Lisp, write a library instead.
That's my plan if no one already had a solution. I can probably just copy-paste the source of SLIME-COMPLETE-FORM w/o the insert paren bit, but I'm usually hesitant to build functions off of libs especially if they have deprecated docs. It's no big deal, but that's why I asked first.
I found that the Quicklisp version of cells claims that it's incompatible with CCL, but there was a checkin a couple weeks ago to fix this in the github repo. I've been using CCL because I got scared off by the ominous warning SBCL prints on startup about it being an experimental port on Windows, heh. :)
I think that warning was much more true a couple of years ago than now. It seems that windows is being kept fairly up to date and the old 'kitten of death' error doesnt seem to be a thing anymore. Cells is not ccl compatible? bummer. We may need to use something else in the future then, I just really liked the glitch free propagation of data
Try to take a simple app you've made and implement its features in LISP. Solving a sudoku puzzle in LISP would be interesting, as there are many ways of approaching the problem.
I'm currently attempting to implement a telnet based pen and paper RPG server. When finished, it will allow for console chat, dice rolling, personal messaging and a GM console. I just wish I had the time to take a real good swing at it.
Off the top of my head: a web crawler, load testing framework, a bitcoin price alert system (with moving average tracking), a static site generator (there are millions already, what's one more?!), a game of life implementation (either text based or OpenGL), a database driver... Hard to suggest things without knowing some of your overall interests.
Hmm, a game of life implementation with OpenGL doesn't sound so bad or the bitcoin idea since I know nothing about bitcoin and maybe this way I'll learn something about it.
the only downside with the opengl idea is that it can feel like you are writing c in lisp. Some of us \&lt;cough [shameless plug](https://youtu.be/iL9_p3I0Pdg)\&gt; are working on the issue of making it more lispy but it is wip. Do check out cl-opengl and cl-sdl2 though. One thing that gl coding in lisp can benefit from is macros as you can automate the writing of the ugly stuff (VAOs for example) If you go for the bitcoin option (or even if you dont) you may be interested in the [optima pattern matching library](https://github.com/m2ym/optima). [EDIT] my bad, I thought it was bittorrent not bitcoin and thought the optima library would be useful for the bencode parser.
I think it is compatible, they just needed to add #+ccl in a few places. ([see here](https://github.com/kennytilton/cells/commit/385720fcee3a170c982e0e79e7d27306e8d004c0))
Protip: it's going to suck ass. Telnet protocol is a steaming heap of ineptitude and hacks.
Sorr but you cannot have an object system without somehow emulating virtual methods. How do macros do that for in C? I mean can you get the animal to speak correctly without knowing if the animal is a cat or dog? 
Tackle some Project Euler problems.
It'd be cool if something like Paramiko (a python SSH client and server implementation) but written in CL. I've done a quick search, but can't find anything comparable.
First hundred problems or so are fun, but don't blame yourself for not solving most of them, after all: (eql 'math 'programming) NIL I solved 166, then gave up) 
Lisp really is made to build new languages. A brainfuck interpreter can be done in about 40L (no pressure) and is an interesting way to learn. I support this idea.
SICP is available as a free online book and contains many interesting exercises from basic arithmetic to building whole new languages and even a virtual machine. If you're using Common Lisp, know that it uses scheme so some translation may be necessary. (Also the book in itself is damn good)
It is available here as an eBook or PDF with re-rendered formulae and figures: https://sicpebook.wordpress.com
20 years overdue!
I can certainly agree with 2 and 3. Regarding 1, I politely disagree: OCaml does just fine with only one implementation. Other languages usually have only one or two main implementations Haskell (GHC), C/C++ (GCC and Clang), Go (GC and GCCGO), Lua (Lua and LuaJIT). I'm not against multiple implementations - I agree they are a good thing. But I worry that we have too many (see http://en.wikipedia.org/wiki/Common_Lisp#List_of_implementations for a list)
"I politely disagree" ... have I mentioned that one thing I really appreciate is the quality of discourse surrounding Common Lisp? :) W.r.t. OCaml, though, it's not a specification-driven language. Unless I'm mistaken, there is no "OCaml spec"; the language is essentially defined by the reference implementation in the same way that Ruby is defined as "what MRI does". I'd be very surprised to find more than one or two non-toy implementations of such a language. C is spec-driven (to an extent) and there are many C implementations - I've used several of varying power, quality and usefulness (PCC, SDCC, GCC, Clang, Microsoft) over the years. See http://en.wikipedia.org/wiki/List_of_compilers#C_compilers for many (many! I had no idea) more. Possibly we do have too many CL implementations. But I still maintain that's less of a worrying sign for the long-term success of CL than if there were only one or two implementations of the specification.
Low hanging fruit first. The standard can be updated after libraries people want are written. And I think you're laying it on a little thick. Common Lisp standardized a metaprogramming environment. It's ability to adapt is baked in. I'd even be fine with just extensive documentation for popular libraries.
The burden of history can be removed by ____ ?
Ruby [has multiple implementations](https://github.com/cogitator/ruby-implementations/wiki/List-of-Ruby-implementations), and it's doing well. I don't see we have abnormal numbers of CL implementations. Some are unmaintained (CLISP, GCJ). Somet CL implementations serve different purposes i.e. ECL is for embedding to your C/C++ code or compile to executable, ABCL is a CL on JVM.
You might want to look at [SICL](https://github.com/robert-strandh/SICL) whose goal is a modular common lisp implementation. The CLOS module is under active development.
I have common lisp code from 30 years ago that still runs without change. The fact that common lisp has a standard is vital. I've written python (2.7) that no longer runs. I've written Java that no longer runs. I strongly oppose "breaking changes". It would be useful to have a library of code (quicklisp is the right idea although I really wish it was git-based). For instance, I'd like to use a trie and I can find such code online but it would be nice to be able to search/fetch it. I'd also like to see immutable versions of data structures that can be searched/fetched. Zach has the right idea. Perhaps Edi Weitz's book could coordinate with Zach to provide a nice (literate?) documentation of a Zach quicklisp "standard library". As for multiple versions of lisp... The beauty of common lisp is that everything I write runs everywhere (modulo extensions, which I try hard to avoid). The various versions have different features that are important for different reasons. CLISP interprets, SBCL always compiles, GCL generates C code, etc. I use them all for different projects and different reasons. As to the question of "stays relevant in the 21st century", a LOT of the current crop of wonderful new languages will die. Scala/Rust/Python/Ruby/etc are all going to be legacy code at some point. I know you don't believe me but I've been around long enough to remember other "wonderful new languages". Remember PASCAL? It was hot. You HAD to know it. It was going to take over the world. It was strongly typed. Of course, there was a debate about the non-string (academic) vs the string (industrial) version but everybody knew that PASCAL was the language that was clean, elegant, fast, and nearly error-free to write. Sound familiar? When was the last time you wrote a PASCAL program? Lisp endures because it lets you think. Every programming language constrains the way you think (try writing functiional code in Java). Lisp is unique in language land. Do you want object-oriented? Use CLOS. Functional? Qi? Don't like the syntax? Reader macros. Don't like the set of special forms? Macros. Want really low-level code banging? Rplaca. Want to try something REALLY new? MOP. As for becoming "the next C++"... you'll notice that it is converging on lisp ideas. They recently introduced anonymous lambda expressions. How novel. I would love to see some ideas standardized, such as the foreign function interface, but I think it should only become standard after it works everywhere and has been "time tested". I would love to see common lisp replace javascript in the browser, making the browser into a REPL-based program. There are a lot of ideas that could be suggested but programming will be with us for a long time so there is no hurry to force a new standard. 
+1! I've written brainfuck interpreters in many langs and it's always a nice way to learn! The "downside" is that bf is so simple you'll get it done in a day or two, so I don't think it fits into the middle-sized project requirement...
The standard was the result of a long and expensive process. Nobody would say its perfect but most agree it is pragmatic. The risk of going back to the standardisation process seems to be too great when the language is so easily extensible for most practical concerns. If we want sockets, threads, coroutines, contexts etc there are libraries for those. I'd like to see a growing set of common language extensions added to each implementation. Our community is small and sharing these marginal features makes sense. It would be nice if the CDR was more alive, but its not dead either: https://common-lisp.net/project/cdr/
More layers, more abstraction, more programmers with an ever decreasing understanding of what is happening within those layers and abstractions.
You know, half the Lisp community's reputation for crotchetyness to newcomers probably stems from the need to fend off newcomers regurgitating the same thoughts bemoaning the state of Common Lisp every few weeks, like clockwork. It wears you down.
You would have to be to not recognize this as a retread of a conversation that's been had many, many times, here and elsewhere.
Ok, guys, thanks for all the feedback. I think I understand now that you guys firmly support the language's overall suckiness. I'll enjoy watching it turn to dust.
I'm going to dissent here and although I agree the commenter you are responding to didn't take the time to research what I'd posted, I'd prefer it if we didn't make fun of someone trying to speak in a different language other than their own. I could understand them just fine, and critiquing grammar, especially of a non-native speaker, is a low blow and beneath the level of good discussion.
I know, but I also wonder if there's a lisp with a c like syntax, like: define (sum-of-all-numbers n) { * (/ n 2) (+ n 1) }
Dylan is the first that comes to mind, and then of course the M-expressions: http://en.wikipedia.org/wiki/M-expression [edit] And I forgot about Logo
To add some context for those not inclined to blindly clicking links, SRFI-49 introduces an alternative, indentation-sensitive syntax for Scheme (called "I-expressions"). The neat thing about this solution, is that it can be implemented in vanilla Scheme, no alterations to the compiler needed. Yet another great example of the power in the simplicity of Scheme.
It reminds me more than a bit of Ruby, but many people have made the argument that Ruby is at its core a functional language despite presenting itself as an object-oriented one.
I've tried some of the various components out (not afraid to get my hands dirty), but there is a pretty long list. Just want to see what other people are using and what works so I don't miss any gems I should know about.
David Moon is one of my heroes, it's sort of sad watching him waste his valuable time with something like this. The absence of s-expressions makes PLOT infinitely worse than Lisp, no matter what else it claims to improve on.
What's wrong with having a hobby?
I misunderstood. My bad.
In the latest caveman they use djula by default as well (used to be cl-emb)
Do you have a specific request? I could try to find some time to do another one.
Wow, really? I tried caveman a few months ago and thought cl-emb is rather clunky. djula certainly looks much better (but now I need to port my app to it...). I also thought the handling of forms is lacking in caveman as compared to e.g. Django, so I made [my own forms library](https://github.com/tshatrov/1forms) which is quite unfinished at the moment. It's very extensible though...
You can be specific about the path though: ;; something like this (defparameter +my-template+ (djula:compile-template* #p"...")) (djula:render* +my-template+ ...) Even if the path is not inside Djula's known directories. Still, I see how it might cause problems when you're using libraries that define their own templates and there might be a collision.
Interesting site you got there! I'm currently learning Japanese, and your site looks like a nice resource.
I've filed two bug reports on there, none of which I can see if I'm not logged in. There's an especially bad one involving `defpackage`, which is surprising considering this thing used to cost money. Are the project's owners able to see my (apparently spam-filtered) bug reports? 
Thanks. Do I need a webserver to use CGI? I assumed I just put the compiled program in Apache or something. Maybe I still don't understand how it works.
With CGI, you don't need a web server, only an executable, which web server will execute, passing all the request related information via CGI variables, and content body via stdin. HTH
That's what I thought, thanks.
Nobody has mention it, and I think you did not ask about it, but you could add [ParenScript](https://common-lisp.net/project/parenscript/), to use instead of javascript. You can write anything you would write in javascript using ParenScript and it integrates better with the LISP stack.
With CGI, for each request your web server will create a new process that executes your lisp program. But, as your web server is not a Lisp environment, your Lisp executable will have to load a complete Lisp environment prior to handling the request. This is a huge waste of resources that you should maybe try to avoid. The usual solution is to use a Lisp web server behind a lightweight C web server (that will simply filter the requests and provide anti-DDoS/security services). It’s not much more complicated, by the way. Most Scheme implementations provide such web server, you may therefore choose your favorite. For instance, [Guile](http://www.gnu.org/software/guile/manual/html_node/Web.html#Web).
Hm, that's what I suspected when I had the idea to implement something similar myself. My problem with using a pure Lisp server is, as I've said, if I can't find the library I need for one of the site's functions, it's a huge delay in the project. I'm interested in CGI because it seems like I can write different pages in different languages if necessary. Although, it looks like Racket has a lot of good standard libraries. Will it help if I use compiled executables instead of scripts?
As I said in my OP, the reason I don't want to go pure lisp is because I sometimes have trouble finding and using the libraries I need. CGI would give me the option of using other languages as needed.
Lisp can be interpreted, compiled to machine code, or something in between. What you call a Lisp “executable” is just the shipping of the Lisp environment with your program. These might be compiled, or not, it depends on your implementation. The major free implementations of Scheme and Common Lisp will only add the needed parts of the Lisp environment to your byte-compiled program (your program compiled to an intermediate language between Lisp and your machine language). When launching a Lisp “executable”, your computer will probably load some kind of interpreter for byte-compiled lisp, and then use it to run your program. But some implementations might allow you to compile your Lisp program to machine code. For instance, Chicken, Bigloo and ECL will compile your Lisp program to an intermediate C program that can be compiled to machine code.
So will it help if I compile to machine code? Can you suggest any alternative solutions to my problem? I want to work in lisp, but I also want the option to switch to another language in case I can't find a good lisp library to do what I want.
Erf, sorry, nested links are dead, you may use this [archive](http://web.archive.org/web/20060622183642/http://moonbase.rydia.net/mental/blog/programming/misp-is-a-lisp.html) instead.
Please, re-read carefully my previous answers, as you might have misunderstood. For web, I know only two possibilities: either you use CGI, either your web application is your web server. As the second only won’t allow you to mix different programming languages, you might wanna fall back to CGI; but remember, performances will be very, very, *very* bad.
&gt;By the way, almost every Common Lisp and Scheme big implementation (Guile, Racket, Chicken, etc.) has the libraries you’re looking for (SQL &amp; JSON). Yes, those are the libraries I'd need for almost every site function. The libraries I'd need for more specific site functions might be a little harder to find.
Yup, it fixes event handling. I've pasted together some code from a previous life to better illustrate what I was trying to describe above. I've put it here. http://pastebin.com/TQUsB89P HTH
&gt;One of my biggest and most recurring frustrations with a major long term personal project I've been working on a LONG time is having to choose between a language that I prefer (Lisp) and one that has good, well-maintained, and well-documented libraries for what I need to do. I like lisp, but sometimes I can't find a library that does what I need, or I can't get the libraries I find to work. Anyways, I think I've found a solution in SCGI. Any thoughts on that?
Have a look at FastCGI. The primary difference between CGI and FastCGI is that the web app process becomes persistent. Rather than servicing a request and then terminating, the web app keeps running and waits for another request to come in. I found sb-fastcgi worked quite well for me.
Great stuff, I havent played with ccl yet so this will give me a huge head-start when I do. I am using cl-sdl2 rather than lispbuildersdl these days and the excellent rpav helped me get set up on osx. So for those interested I have put up a [follow up video showing how to get cepl on osx](https://youtu.be/uq5_4cKDJSQ). My current technique occasionally doesn't feel as good as the experience on other platforms so am going to trawl this for ideas. Thanks for writing all this up!
Is there any relationship between the Roger Corman who is the author of Corman Lisp and the film producer Roger Corman? 
hmm does sound like it will be possible to minimize the magic on ccl. That sounds nice, I want to get ccl support for cepl so I guess I will need to do that at some point. Thanks again for all the info!
[Previously](http://www.reddit.com/r/lisp/comments/86wz9/ilc2009_david_a_moon_programming_language_for_old/)
Fernando, thanks for the awesome writeup. I've never really put in the time to figure out how travis works, but this really makes it simple to understand, especially in the context of CL. Thanks for putting in the time.
ASDF is a way of defining your systems (what other languages might call "packages"); it stands for "Another System Definition Facility" Quicklisp is a client, plus a database of systems, that will let you just ask to load a system, and it will download the system, plus any dependencies automatically for you. It relies on ASDF for all of the system definitions and loading. If you have an ASDF system that is available under a permissive license, you can ask Zach Beane to add it to the quicklisp database, so that other's can use it.
If you don't see the issues on github, they didn't make it into the system. I didn't see them anywhere. If you like, you can try again here: https://github.com/sharplispers/cormanlisp/issues 
Can you log in and give me the URL of one of the issues?
I'm going to go ahead and post the bug reports here, since it's obvious that GitHub is blocking me, and Reddit supports markdown. This account is rate-limited, however, so the reports will come slowly. Title: **Output to `*standard-output*` doesn't work in console executables created with SAVE-APPLICATION** Try this at the REPL (either clconsole.exe or CormanLisp.exe, it doesn't seem to matter): (defun hello () (format t "Hello, world!~%") (lisp-shutdown "")) (save-application "hello.exe" #'hello :console t) The resulting `hello.exe` prints nothing when launched. However, the following program works fine (sometimes): (defun hello () ;; Explicitly open the console. (with-open-file (*standard-output* "CON" :direction :output :if-exists :append) (format t "Hello, world!~%")) (lisp-shutdown "")) 
https://github.com/sharplispers/cormanlisp/issues/10 https://github.com/sharplispers/cormanlisp/issues/11 https://github.com/sharplispers/cormanlisp/issues/12 
Remember that, if you want a easy way to make an asdf compatible project, one of the best solutions is to use quickproject. ```(ql:quickload :quickproject) (quickproject:make-project "the-project-name")``` Done! 
Hemlock and slightly better Slime-support. Probably more. 
Thank you, that sounds the wise thing to do.
Well, FastCGI/SCGI seem to solve the problem for you. I did not know about these.
Someone tweeted this: https://twitter.com/didierverna/status/607889552540364800
[**@didierverna**](https://twitter.com/didierverna/) &gt; [2015-06-08 12:38 UTC](https://twitter.com/didierverna/status/607889552540364800) &gt; Celebrating Christian Queinnec's work at UPMC. \#lisp [[Attached pic]](http://pbs.twimg.com/media/CG-oUB5WgAEZu_l.jpg) [[Imgur rehost]](http://i.imgur.com/zKYowxc.jpg) ---- ^This ^message ^was ^created ^by ^a ^bot [^[Contact ^creator]](http://www.np.reddit.com/message/compose/?to=jasie3k&amp;amp;subject=TweetsInCommentsBot)[^[Source ^code]](https://github.com/janpetryk/reddit-bot) 
You must be aware of http://letoverlambda.com . How is your take on macros different? Do you think it is the *sine qua non* of good lisp style? Or are you more moderate in your affection?
I would say I have a similar focus as Let Over Lambda. The stuff I write will just be a step or two down from the kind of things in Let Over Lambda. As for macros, I believe they are one of the most important features of Lisp. They separate Lisp from pretty much every other language by making it easy to add near arbitrary extensions to the core language. The other features I find important are the debugging capabilities (restarts + inspection + redefining functions at runtime), and the ability to use s-expressions for symbolic manipulation (recently I wrote a program which solved constraint satisfaction problems. I found it extremely helpful to represent the constraints as arbitrary Lisp expressions).
Is Hemlock actually being used? I tried it once but it felt very incomplete.
I would really like to use any Lisp in these things: [esp8266](http://espressif.com/en/products/esp8266/). Lua is already available there and uPython IIRC, is a well advanced WIP.
Could be wrong (proclaim/declaim/declare have always been somewhat unclear to me) but I think you can use [proclaim](http://www.lispworks.com/documentation/lw51/CLHS/Body/f_procla.htm): &gt; Establishes the declaration specified by declaration-specifier in the global environment. So as /u/ruricolist said, I think you'd put that in your init file. It would be nice if there was a package-wide version of declaim/proclaim so you could run declarations within an entire package without affecting the rest of the global space. Or maybe there is a way...anyone know more about this?
ASDF's :around-compile would get you at least some of the way there.
Figured ASDF might have a baked-in solution. Thanks for the pointer.
I would strongly rethink using SAFETY 0 in production.
Zero runtime checks. It also may change semantics. Modify an array beyond it's size? Boom! Add an integer to a string. Boom! The boom! might come at a later point, for example when the GC sees a memory corruption.
QT in general has better APIs and documentations than GTK+, as well as better cross-platform supports. Not to mention that most GTK+ bindings for Common Lisp seem to have been abandon.
Ah, fun stuff indeed. I'll just keep it to 1 then.
Ever tried Forth? It's essentially built for that type of thing, available on almost everything, and not that hard to implement a simple version if it's not. It's also worth learning because stack-based programming is odd. 
What others said. (I had a problem with safety 0 which was actually inside SBCL which caused my bad code to silently corrupt memory, spend many months trying to reproduce and catch it.) If you need to use it, use only locally inside functions which you know are correct, even then the impact is often negligible, so checking that it actually helps is a good idea as well. 
The Qt API is huge and of high quality, not just GUI stuff. 
Here is a relatively recent GTK+ lib for CL: https://github.com/crategus/cl-cffi-gtk Both LispWorks and Allegro CL use GTK+ for their GUI libs (CAPI and Common Windows) and their IDE on Linux. http://franz.com/support/documentation/current/doc/cggtk-relnotes.html http://www.lispworks.com/documentation/lw70/RNIG/html/readme-24.htm#pgfId-892516 
Prior this post I started to prepare my interpreter for the parsing engine. It still takes few days. But I can give an example of how it will work. Say you've got a context-free grammar with attributes, such as this: expr -&gt; expr1 expr1 -&gt; :add expr1 '+' expr2 | expr2 expr2 -&gt; :mul expr2 '*' term | term term -&gt; :int number term -&gt; '(' expr ')' You may make an interpreter that corresponds to it: (define (post_add env lhs rhs) (+ lhs rhs)) (define (post_mul env lhs rhs) (* lhs rhs)) (define (post_int env digits) (int digits)) Then you have something written in this language eg. `15 * (5 + 2)` Now if you've got a parsing engine, you may feed this grammar and interpreter to get yourself an interpreter for this language. Then you may feed it the input string and request result. The parser internally determines whether the input it gets is unambiguous against the grammar while it traverses through the parse using your interpreter and finally returns the result. [pyllisp](https://github.com/cheery/pyllisp/) is the interpreter I'm customizing for this kind of use. I intend to push early results into the repository this weekend. The early parser is called from outside the implementation. After I have the parsing engine available from within the language, it will become capable of doing the feat described above.
Qt works pretty much as-is on Linux/OSX/Win32, it's a remarkably well done compatibility layer, whereas Gtk+ is only practically useable on Linux. CAPI is more like Qt itself, it has 3 separate implementations.
Have you looked at all into Racket?
So basically, grammar spec + one function per attribute =&gt; magic box =&gt; full interpreter? I thought things like that already exist, no? What do you mean by mutable grammar? Do you mean - for example - the user of the compiler can provide this override file: expr2 -&gt; :mul expr2 'x' term | term | expr3 //Override rule expr3 -&gt; :neg '-' term //New rule (define (post_neg env term) (* -1 term)) Then instead of `15 * (5 + 2)`, you can write `15 x (5 + -2)`?
Take a look at https://github.com/combinatorylogic/mbase - a PEG-based layer over Lisp.
&gt; Jeffrey Kegler's take on PEG In my book, PEG is a nice DSL to represent a recursive descent parser - which would otherwise be hardcoded. So it is indeed an improvement over a hardcoded implementation, but yet allows to retain all of the advantages of an ad hoc way, including arbitrary complex error reporting and recovery and all that. &gt; The distinction is less severe if you work with PEG as if it you worked with hand-written parsers. Exactly. And before PEG, all the professional parsers were hand-written, nobody bothered using parser generators. Now it's possible. Quite a progress. &gt; That is design the whole language at once and never change it again. I would not agree. I implemented dozens of arbitrarily complex PEG-based parsers, many of them heavily evolved. Never had any problems. For most of that languages it would not be possible at all to implement them any other way (probably GLR would have worked in some of the simplest cases). &gt; But if you allow users to extend or modify the language, then minor changes into the PEG grammar can change the language in unexpected ways. I do allow users to extend and modify the language, in the very carefully chosen extension points. No problems so far. Take a look, here's a bunch of language extensions: https://github.com/combinatorylogic/mbase/blob/master/src/l/lib/pfront/extensions.hl And a similar thing on top of C grammar: https://github.com/combinatorylogic/clike/blob/master/tests/syntax.c &gt; For example, adding a new rule that uses '+' -mark for something may override other use of '+' and change the meaning of existing programs without a warning if you use PEG. I cannot see why it's a bad thing. After all, overloading a '+' operator in C++ would have similarly destructive effect. Just do not override the existing rules unless you know what you're doing. &gt; With context free grammars the parser will refuse to parse because it notes the ambiguity, providing more trust to what you can do with them. With the context free grammars you won't be able to extend the language in the first place. Ambiguity is good. It allows to mix very different languages any way you like. Mind fusing together Verilog and C, for example, using a context free grammar? With PEG it's trivial: https://github.com/combinatorylogic/soc/blob/master/backends/small1/sw/long_hdltests/test1.c P.S. And, of course, it's always possible to do things the old, context-free way, this is one layer below PEG in that system: https://github.com/combinatorylogic/mbase/blob/master/src/l/lib/parsing/peg.al
I am implementing a language where every syntax is implemented by reader-macro http://the-little-language-designer.github.io/cicada-nymph/core/tangled/show-all.html
Hey, cool! It's always nice when I find a library with good SW engineering practices demonstrated. This makes that lots easier. I should hook my QL systems up to this. 
&gt; I've used coffeescript and it's PEG parsed language. Did not know it's using PEG, thanks. I'll take a look. &gt; If I don't want the rule to be applied then I discard it from the grammar. It would be interesting to see such an approach in action. &gt; if you don't need better than that. What I really need is a very thin layer on top of an ad hoc, handwritten recursive descent. I doubt it is possible to have a quality parser with something more declarative. Would be delighted to be wrong here. 
Perl6 allows the grammar of the language to be extended. Though this is an old post, its a core aim of the language and the status of perl6 is closer to the fabled version 1.0 than ever: http://perlgeek.de/en/article/mutable-grammar-for-perl-6
Forth -like languages can be also made to evolve over needs. But I've always appreciated it more for how barely anything it is in the first place while still providing lot of the convenience you can get by having a language. Forth evaluation loop requires you to get the next word and invoke it from a table. You need to have the table populated with few primitive commands, including the commands to modify the table and commands to create new commands. Other than that, it's the fastest way to get yourself an interactive programming environment from nothing at all. Haven't seen one implemented with reader macros before, though.
So the idea of letting user to change the grammar is at least or older than perl6. I keep wondering what are the implications. The cost of parsing complex grammars is still there even if the parser would manage to take care of popular deterministic grammars in linear time. It remains to be observed how much this really affects the usability in practice. I just got myself a bytecode format into the interpreter, so I wouldn't have to bother with low level representations of the parser. The approach allows me to construct the parser and bytecode compiler in another high level language than my own and make it driven by an attribute grammar. When it's sufficiently developed I can implement the parser engine and the compiler in my language. The bootstrap parser remains functioning with less effort because part of the parsing is driven by the grammar, which can be shared by the implementations. I also made sure the bytecode allows encoding new instructions without needing changes into format. This is feasible because due to JIT compiling the bytecode is no longer very performance critical portion of the implementation.
Nice work!
This does not answer the question: CLM is not part of Quicklisp and, thus, it is not easy to install.
You have a pull request that I made while commuting. I've an unfinished change that writes to streams so you can write to files or strings at your convenience.
Here's some idea of what I'm getting out of this: http://boxbase.org/entries/2015/jun/15/marpa-parsing-revolution/
Just merged. Thank you for contributing :)
Just a heads up, you may want to pull at some point because I made a little change (the call to `reset-state` was a little early, before the program was done with the variables). Thanks for helping out :)
Good catch. I found time at lunch to implement writing to supplied streams and made another PR.
Weblocks?
Found it: [hh-web](https://github.com/hargettp/hh-web/wiki)
That would be awesome.
When I was hacking on the code I ran into something that didn't feel right. Converting example into example-files and example-strings, I wanted to create a variable with the generated DOM once, so the same content could be passed to write-files and write-strings. There wasn't a nice way to do this though, because the functions rows and cols have side-effects on the global variables, particularly *running-css*. The design decision to make the html content return in a value and the css go out of band into a variable was starting to bite. I think a better approach would be to have all these functions return (values html css) and you could later add js as the third value. Or encapsulate these 'streams' into objects that are passed as a single but compound value. This way, your API can be relatively functionally pure and without side effects. Coping with the ids is another challenge, but they could be left implicit until resolved to concrete values at the output phase.
You bring up a valid point. I chose to have state, instead of keeping the functions stateless (and thus functionally pure, as you put it), and it seems that that design decision is causing problems now. Consider it a lesson learned... I think it's a good idea to encapsulate the streams as an object that can be passed around (using CLOS). We could perhaps turn each div into an object as well, and have the constructor assign a unique identifier to each div (this is possible with CLOS, yes?) My other idea was assigning divs values based on list indices, since the nested divs are inside of lists, we could assign each one a value that is based on their location in the list, and this would be guaranteed to be unique... (does that make sense?)
Introducing objects and generic functions will impose a performance cost, but should lead to a more flexible design, should still be faster than PHP and the web is all about caching anyway. Positional div ids could be a default behaviour that can be overridden by defining methods or setting slots on the object. 
You should see https://www.youtube.com/watch?v=ZoFX2QYJhbY In this video, it shows how to use S-Geometry to make and modify 3D objects. By "refer" they use menu to select objects; by "manipulate" they right click on the "refer"-ed object and pops up a context aware menu. For the part "associate text output with an object", they are almost everywhere in Genera's non-image-related subsystems. For example, Symbolics Concordia https://www.youtube.com/watch?v=NOysrxexTXg And there are many other Genera videos. In Genera, click anything you can click, and you get the associate text output. About description of this system, it's certainly in Symbolics volumes of manuals somewhere. But of course, the heavy work is done by the underlying system, Dynamic Windows (I think). There is a tutorial on how to build a Game of Life demo: https://archive.org/details/bitsavers_symbolicssamDevelopmentTutorial_473824
This is one of the most impressive bodies of work I've ever seen. He wrote his own common lisp dialect that targets the LLVM that allows for C++ libraries to be utilized and designed a framework for creating biological macromolecules in silica that is backed by experimental work. This isn't just some pie-in-the-sky, "oh jeez isn't that nice" kind of simulation that comes along once in awhile. It is a simulation that makes accurate predictions in the real world! 
I have immense respect for him having talked with him about a few of the llvm-related problems he was working on. Exception handling is *hard*, and this guy just picks it up after a week or two of fussing with it.
Thanks for the links - watching...
This is really fascinating. I read the intro pdf for McCLIM. I'm always blown away by how these people "got it right" in the early 80's. They immediately saw the weakness with the more traditional event based approach. This is so much more advanced than anything I've every used. 
 (reduce #'+ "two hundred and fifty nine" :key (lambda (char) (let ((delta (- (char-code char) (1- (char-code #\a))))) (if (&lt;= 1 delta 26) delta 0))))
Fun idea. I guess you could write a bruteforcer. IMHO the most difficult part would be to "spell" a number: 1 -&gt; "one" 2 -&gt; "two" ... 259 -&gt; "two hundred and fifty nine" I think it's difficult because there are many irregularities: how would your function know if 318 is "three hundred eighteen" or "three hundred and eighteen"?
Can't format print long pronunciations of numbers via a certain control string?
I found it useful, since DDG is my default SE now. If I do not find what I am looking for, I can always use the google bang, !g, ie [loop on Google](https://duckduckgo.com/?q=!g lisp loop)
The spelling part is easy: (format nil "~R" 259) =&gt; "two hundred fifty nine") The irregularities could be handled by adding "and" to every number greater than 99 and not evenly divisible by 100.
picolisp https://bitbucket.org/mihailp/tankfeeder/src/d9c33c9fa75980a805a13dc51442b27d9dec5ece/259.l?at=default
how much do the DSP slices and memories help
the other question I have is, could a non-pointer based language be implemented for a DMA based system like the parallella(or sony CELL) - given data is logically shifted around the system , compacting garbage collection might make sense (data must be explicitely loaded &amp; saved back by DMA, instead of data going in &amp; out of the caches whilst logically staying static - so perhaps compaction could be done at that time.... however.. CELL basically died because no programming models appreard that could leverage it. they talk about erlang as a possibility for the parallella)
I remember that referring to a different site so I checked ddg's docs. !l1sp takes you to Xach's very nice redirect service. !clhs takes you to a LispWorks search that I haven't seen before. Three keywords, that's some pretty nice lisp support:)
You're right about other languages. My ethnocentrism blinded me to that possibility.
in Clojure (reduce #(+ % (- (int %2) 96)) 0 "twohundredandfiftynine")
It's definitely good book, if you are studying lisp and it's not your first lisp book. I was impressed when found some of solutions from this book in the clojure.
Which one is PAIP?
as a beginner, following this kind of bit me, but then again I learned a valuable lesson from it. (defvar special) (defun make-closure () (let ((special nil)) ;; I assumed special is lexically bound. (lambda () ;; Here I _expected_ special to be bound to the lexically bound special (setf special t)))) (funcall (make-closure)) ; =&gt; T ;; Now I still expected special to be unbound. special ; =&gt; T In my particular example I was using a special named PAUSED?, and was hoping to set a lexical PAUSED? to T, but ended up pausing the whole system :-P.
There are few people (very famous in the lisp and scheme word) who basically think that Let Over Lambda is **"just an overall bad piece of text"**. See [this discussion](https://groups.google.com/forum/#!topic/plt-scheme/5cMVPkGsueo). My opinion is that those people are pretty weird hygienic fanatics who, more than anything, likes to write numerous [useless dull academic papers about shortcomings of stateless web that leads to booking on the wrong flight!](http://cs.brown.edu/~sk/Publications/Papers/Published/khmgpf-impl-use-plt-web-server-journal/paper.pdf) 
Is this lisp-ish?
Ah, Racket people: worst community ever, full of arrogance and deep shit!
That's some deep and entirely needless hostility there.
More like "Lambda over Lambda." You know that you can express **all** lets as lambdas? Consider the following example: (defmacro my-let (bindings &amp;rest exprs) "Our own version of let." (if (or (not (listp bindings)) (member nil (map 'list #'listp bindings))) (format t "Invalid variable binding(s) inside of \"~A\"~%" bindings) `((lambda ,(map 'list #'car bindings) ,@exprs) ,@(map 'list #'cadr bindings)))) 
The LW IDE has a few features Slime doesn't have such as graphical browsers for systems and classes and a debugger that lets you set breakpoints without editing the code, due no doubt to privileged access to the Lispworks source. Version 7 has a new hobbyist category and adjusted pricing but its still not cheap. That said, I don't personally own a license - I'm an SBCL user at home, and my use of Lispworks is 99% via Slime.
Can someone explain to me the reasoning behind using vectors instead of lists for bindings? This appears to have appeared in clojure, and many lisp frontends for other languages appear to have adopted it.
I guess it's because some think that mixing parens and brackets improves visibility.
some ignored, testing against built-in functions, a lot of recursions, comments and patches are welcome. 
How does PSL relate to CL?
See also: http://www.softwarepreservation.org/projects/LISP/standard_lisp_family/
I don't know. You should open a thread and ask for advice.
Link?
Sorry I am being a bit dense, if you meant link to the fact he is using the old api then sorry I don't have one, I just recognised the code. For a link for learning the new api I like learnopengl.com
Damn, I don't know. cl-opengl is a fairly thin wrapper so pretty much everything translates 1 to 1 (except the enums change from GL_EXAMPLE -&gt; :example, any nothing is prefixed with `gl). @yokhahn's post suggests that z27 may have some good info for you soon.
I use stumpwm on my Linux machine, and it feels really good that I'm in a live lisp environment. Like a WM version of Emacs.
Maybe not what you seek for (except for stealing some idea), but [here](http://gitlab.com/eql/eql/tree/master/examples/M-modules/webkit/) are some examples using ECL and WebKit for a GUI. They use a QtWebkit/Lisp bridge, which is easy to implement using ECL. It also shows how to use Qt plugin widgets. The Tic-Tac-Toe example doesn't require any Qt knowledge.
Is it a robust window manager? Does it generate a lot of load on the CPU and RAM?
It's pretty robust. Haven't seen any significant issues for several years. There are some minor glitches but I can bear with them. But beware that it's a tiling window manager and it may not be for you.
I'd say that reddit *cared* (past tense) about lisp. They eventually ended up rewriting it in Python. Side note: I wish they'd open source the lisp version of the code.
Do you have a blog or anything for your project that I can check on? I was thinking of doing something similar, but after doing the research I decided it was a little over my head at this point considering I have no experience with embedding WebKit into a program. I've been trying to find out how to use HTML/JS as a UI for a C++ program since it's cross platform, I'm familiar with it, and I don't care about a native feel. I was looking into Chrome Embedded Framework but it looked like it was more trouble than it's worth and might not be exactly what I'm going for. It seems like my best bet might be to have two processes with the C++ running an HTTP server or maybe communicating between the UI and C++ with WebSockets. That feels like a lot of indirection/overhead though. Sorry for the rambly noob post, i'm hoping someone might be able to point me in a good direction. If I was doing a typical desktop app I'd probably just go for Electron, but I'm not a fan of JS for larger projects and I'm hoping to do some work with real-time audio.
Hi, Andrew here. I found that it was much, much easier to embed lisp into an existing browser than it was to embed a web engine into lisp. Your two main choices are firefox and node-webkit. I wrote up some of my adventures [embedding ECL into node-webkit](http://turtlapp.tumblr.com/post/83184613020/embedding-lisp-in-node-webkit-part-1). I also ended up doing this with firefox as well, using [js-ctypes](https://developer.mozilla.org/en-US/docs/Mozilla/js-ctypes/Using_js-ctypes) (basically CFFI for spidermonkey). You'll want a messaging layer so the browser and the lisp engine can talk to each other, [nanomsg](http://nanomsg.org/) worked great for this (and was the inspiration for [cl-nanomsg](https://github.com/orthecreedence/cl-nanomsg)). It was definitely a learning experience (a long one), but in the end I was able to integrate a large portion of the app into lisp and have the browser and lisp messaging/eventing over nanomsg pretty seamlessly. Fun stuff. I'd say in the end it was probably much easier using js-ctypes in firefox than it was to build a node module in C++ that plugged into nanomsg/ECL. It might be easier if you're just compiling for nix, but dealing with gyp and the node API on windows is a nightmare. The repo for the project is here: [turtl-core](https://github.com/turtl/core) and the desktop app that used it: [desktop-ff](https://github.com/turtl/desktop-ff). Hope this helps.
Hey, have you used node-webkit? I'm curious how Electron compares to it. NW has some warts and I'm wondering if Electron is more updated.
Never actually used NW, but I have used Electron successfully.
What is it for?
I don't remember that there was much work in it. I doubt it is worth it. When it happened, there was a *sport* for a short period of time to reimplement Reddit (or what it was in the early days) in Lisp as quick as possible.
Great that you are still moving things forward!
Not sure I understand what you want to do.Your parser should convert a string like "(1 2 3)" into '(1 2 3)? Then "(+ 1 (* 2 (- 3 4)))" becomes '(+ 1 (* 2 (- 3 4))) ? 
Maybe this shouldn't be on this subreddit.. I'm writing it in C, not lisp
Check out [Planet Lisp](http://planet.lisp.org/).
I'm looking forward to #3 - being able to find a package without relying on a combo of googling and hitting old/non-quicklisp libraries. Since a large portion of the available packages are on github nowadays, maybe the rating system could be tied to #'system-apropos and the data supplied from the github star rating that currently exists? Using an API call like: curl 'https://api.github.com/repos/quicklisp/quicklisp-client' For instance returns a key, 'stargazers_count' of 120 for quicklisp-client.
Hello gavino.
Is there anything we can do to help things move forward?
When I wrote [mine](https://github.com/fourier/libsexp) I used cons cells to represent lists. Actually all you other requirements just depends on how do you want to use it. I used it as a parser for the input data for my other libraries(instead of say XML); therefore my requirements probably differ.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 8. [**"Steep learning curve"**](https://en.wikipedia.org/wiki/Learning_curve#.22Steep_learning_curve.22) of article [**Learning curve**](https://en.wikipedia.org/wiki/Learning%20curve): [](#sfw) --- &gt; &gt;The expression ___steep learning curve___ is used with opposite meanings. The term is often used in common English with the meaning of a difficult initial learning process. Nevertheless, the Oxford English Dictionary, The American Heritage Dictionary of the English Language, and Merriam-Webster’s Collegiate Dictionary define a learning curve as the rate at which skill is acquired, so a steep increase would mean a quick increment of skill. &gt;Arguably, the common English use is due to metaphorical interpretation of the curve as a hill to climb. (A steeper hill is initially hard, while a gentle slope is less strainful, though sometimes rather tedious. Accordingly, the shape of the curve (hill) may not indicate the total amount of [work](https://en.wikipedia.org/wiki/Work_(physics\)) required. Instead, it can be understood as a matter of preference related to ambition, personality and learning style.) &gt;The term *learning curve* with meanings of *easy* and *difficult* can be described with adjectives like *short* and *long* rather than *steep* and *shallow*. If two products have similar functionality then the one with a "steep" curve is probably better, because it can be learned in a shorter time. (Fig 9) On the other hand, if two products have different functionality, then one with a *short* curve (a short time to learn) and limited functionality may not be as good as one with a *long* curve (a long time to learn) and greater functionality. (Fig 10) &gt; --- ^Relevant: [^Learning ^Curve ^\(Star ^Trek: ^Voyager)](https://en.wikipedia.org/wiki/Learning_Curve_\(Star_Trek:_Voyager\)) ^| [^Learning ^Curve ^\(Babylon ^5)](https://en.wikipedia.org/wiki/Learning_Curve_\(Babylon_5\)) ^| [^The ^Learning ^Curve](https://en.wikipedia.org/wiki/The_Learning_Curve) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cspqnor) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cspqnor)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](/r/autowikibot/wiki/index) ^| [^Mods](/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Call ^Me](/r/autowikibot/comments/1ux484/ask_wikibot/)
I think the book ANSI Common Lisp by Paul Graham has you code a html server
I'm slurping a lot of data from github right now for just that purpose. I'm also using download stats and VCS commit activity. I hope it helps.
I'm working on something along these lines, stay tuned.
http://subvert-the-dominant-paradigm.net/blog/?p=23 ometa in cl
Check out http://www.pvk.ca/Blog/archives/
I'd like to see a CL implementation of Marpa: https://jeffreykegler.github.io/Marpa-web-site/ Got an ambiguous grammar, get an abstract syntax forest.
Lists to cons cells makes sense. As for data types, that depends entirely on what your aim is. If you're just translating to/from xml, strings would do. If you're building a lisp, you'll want numbers at the very least and probably symbols as distinct from strings.
It appears misleading is a harsher tearm that I thought. What I meant is that when one uses the interpreter one uses define-ometa while on in the compiler one uses defgrammar and defrule which isn't documented in the readme. Yes the tests are the best documentation but because I found a readme I didn't went for the tests at first.
Thanks for all of your hard works, xarch. I've been using Quicklisp, Quickproject, and Buildapp to manage my projects recently, and they have make my life a whole lot easier. 
How are symbols different from strings?
String literals must appear within quotation marks. Like numbers, they are primitive values that evaluate to themselves. Symbols are like variable names in other languages, and they can be bound to (assigned) a value that they evaluate to. It's possible in writing a lisp to forego strings as a separate data type and use symbols for this purpose, I think Picolisp does that.
A string is a sequence of characters. When the interpreter encounters a literal string in the source code, it returns the string itself as the value; a string is thus *self-evaluating*. A symbol names something (a variable, a function, a class, or whatever kind of data your program handles). When the interpreter encounters one, it looks up the object that the symbol names. In the form `(+ 3 x)` the first symbol `+` names a function and `x` names a variable, and the interpreter uses the function and the variable to compute the value of the form.
If it's just a matter of hardware, it wouldn't cost much for someone to donate a Raspberry Pi for Linux ARM hard-float Linux support, or an Intel Edison for 32-bit Intel/AMD Linux support. (Some people can't donate effort without significant red tape, if at all.)
Hi there, I'm the noob who talked about it. I just wrote a small article with a few examples I used during the event (it was not recorded). https://ehret.me/and-keep-calm-hack-lisp/
I'm unable to really grok this right now so can anyone (author) speak to the state of the framework? Beta, alpha, stable?
How does this differ from caveman2? (the web framework clack's author put together) It appears to use the same cl-annot for route definitions (although a #'defview call instead of a #'defun call for the code tied to the route). Both also use djula for the template engine. Can you give a quick summary/checklist of what is and is not the same as caveman2?
I second this question. Seems like "Yet Another Clack Framework"... :-)
Additionally: Caveman originally used cl-emb, and Lucerne used my own template engine, until Djula came along and we both (independently) decided it would be a much better option.
What kind of time? I have a Raspberry Pi2 I could use for nightly builds.
do you have a blog I can follow?
Thanks for the awesome reply! I think I'm going to have to cook something up with this, sometime soon. Thanks for your hard work.
Is there a spec for the full lisp grammar somewhere I looked and couldn't find it?
THIS IS AWESOME, I'm gonna go and build the coolest lisp in the WORRRLLDDDD
You could try the [hyper spec](http://www.lispworks.com/documentation/HyperSpec/Front/). 
An excellent post. Thanks Michael!
This looks really great. I've been annoyed trying to debug Lisp and I use slime + sbcl, so it's perfect. That said, one of the annoyances has been that "insert a statement" isn't so easy in a big nested sexp. That means a lot of navigation or breaking it apart into more imperative style before I can even start debugging. A lot of this is learning when to use what style and learning the sexp navigation commands, I guess.
Reread the first sentence.
Thanks! What types of things have you written in it?
[Reader Algorithm](http://www.lispworks.com/documentation/lw70/CLHS/Body/02_b.htm)
For whatever reason I can no longer find it in the Web. The only thing that is left of it is [Episode 2](http://vk.com/video-88124_83068938) in poor quality and without sound...
I wonder what &gt; Now to get up to 50 characters. means? (Also, I wonder if you have contacted PG about the posting? Just as a courtesy.)
I sent him an email once a few years ago when Lulu had problems and never heard anything. But since the PDF is [here](http://www.paulgraham.com/onlisp.html), I can't imagine he minds if people print it.
I find it rather depressing that there is almost a total lack of introductory books on Lisp programming that have a modern look-and-feel. Practical Common Lisp is a great book, but it was published long ago. There are other great books on Lisp programming, but they were published long ago and are hardly accessible/suitable for modern book-reading gadgets (pads, smartphones, e-Book readers). Having a book in its .tex form probably will allow to fill the gap a little bit...
I hear you. But I offer a couple of books as PDF's free for download and in the past I have appreciated an email (for a number of reasons that don't bear elaboration here). And, just in general, I'll note that imagining anything about other people has often got me in trouble! Anyway, thanks for the book.
OK -- the issue was actually with my sbcl version. Ubuntu latest is still at 1.1.1.14; jonathan compiles correctly (and :caveman2 quickloads correctly) with SBCL 1.2.13.x.
I don't think so. Since Common Lisp provides break, there doesn't seem to be any reason for emacs to implement an equivalent feature where the only difference is that a call to break isn't inserted into the code.
Ah version hell, gotta love it (not!) =]
PSA: I noticed a link to a Lulu book in the comments; don't waste your money buying that print version. If you download the PDF and cover art from http://www.lurklurk.org/onlisp/onlisp.html you can make your own 6x9 paperback version that will be a lot cheaper. I created my own private Lulu project using the aforementioned web site's instructions and purchased a printing of the book for a grand total of $12.31 USD (I used a 15% off coupon I found on RetailMeNot.com).
Shen is a lisp with optional static typing that is highly portable (it's implemented in a small lisp with 46 primitive functions/special forms). Check /r/shenlanguage on the sidebar. Racket has the typed-racket module if you like racket.
Do either have anything like quicklisp or emacs slime support?
It's strange how /r/lisp is just above /r/cpp on that same metric. I didn't see that one coming. 
&gt;I've toyed with racket very little, so I don't know to what extent it manages libraries automatically. Racket's development team has deprecated its automatic library system, known as PLaneT, in favor of a new command-line tool called 'pkgs' (installed as a subcommand of the pre-existing `raco` command-line compiler) that resembles Ruby's `gem` command-line tool. 
Maybe because Lisp Machine is discussed a lot here?
Built in functions will warn. Using a user defined such as mod! Will not warn 
Ah yea the check types in the macro are to prevent a bad runtime call before it gets into the main function body 
The C guys are too constrained in there language so it makes them grumpy. I don't know if its just me, but it seemed kinda like the more expressive languages (like LISP!) were happier and had more fun talking about their language! The funner languages will have funner people, it seems.
I have been searching for more information on concurrency and threading, that chapter is very instructive and wonderfully clear. I hope very much one day Nick Levine will complete The Lisp Book, he is an excellent writer.
I use it with a single core pentium M (1.6 Ghz) and its super snappy and responsive (my machine also only has 1.5G ram and stumpwm + vimprobable2, the browser I'm posting on, is only using 300M RAM).
Thank you for sharing. What does it offer differently from Zach Beane's quickproject + buildapp? http://www.xach.com/lisp/quickproject/ http://www.xach.com/lisp/buildapp/ 
I used those tools to make a project the other month and I wanted to make a standalone project, but setting it up so that buildapp would properly load everything from quicklisp was a hassle. This tool just automates creating a project that can use a makefile (using buildapp underneath) to create a standalone executable that integrates libraries from quicklisp.
Thank you, that is very useful indeed! PS: Just for completeness, let me add here as reference also cl-project: https://github.com/fukamachi/cl-project
Hmm, I may have to adopt the modern cl style for my next release.
Simply amazing, and look at all those documentations! I'm currently looking into building web applications in Common Lisp, so this will be a great addition. Man, I'm enjoying living in the current Lisp renaissance.
Are you sure your Quicklisp dist isn't outdated? Both jonathan and proc-parse were added in the [April dist.](http://blog.quicklisp.org/2015/04/april-2015-quicklisp-dist-update-now.html)
Not sure when the last QL dist was released, but jonathan has had some pretty big bugfixes since April.
Example: http://eudoxia.me/lucerne/docs/example--a-twitter-clone.html
No, using emacs and slime is really not a valid option. I had to actually make a custom package to install everything! Emacs, then slime, then a bunch of lines in .emacs, then quicklisp. Granted, quicklisp is not required. But the current state of Lisp IDEs is a sad one. Many people don't like emacs and just want a simple one click install that has everything, e.g. a sublime package.
Ooooh. Huh. I've never had to do bigger scale application, so I've never thought about the overhead problems for CGI processes. So is it like one big event loop? Or is there something even more magical going on?
This sounds like those people that won't try lisp because of the parentheses. Try it for a while before you dismiss it--maybe it works that way because that way works best.
As an answer says, I think using a swank client is the best option. Slime is just that! Slime from the CLI would be weird though.
It looks to be an improvement, but it's still using emacs. It's far to invested in sequential hotkeys for my taste.
&gt; So is it like one big event loop? What do you mean by that? &gt; Or is there something even more magical going on? Well, first you have to define what you mean by an event loop for stateless HTTP servers. If that is as magical is it sounds, then I'll have some idea how to respond. 
What's the license? Its not obvious from the site.
I saw this link a while back about writing a simple socket repl: http://sourceforge.net/p/sbcl/mailman/message/20182792/
There's no way to hook up a file to it. :(
&gt; stateless HTTP servers Had to google. That makes sense.
I know Xach had some plans for new Quicklisp features as far as tagging/popularity etc. goes, but for now I had an itch to get something in a similar vein going. It's very early/prototype stages, but if anyone has ideas they'd like to see in it, I'm all ears. About 75% of the repositories will work when their link is clicked (it checks github for a README.md file). About 25% are not hosted on github, or don't have the file. I'll work on adding robustness to cover bitbucket and other miscellaneous sites next.
I didn't get any feedback other than an automated mail saying the site admin personally reads all emails. I submitted 3 (2 common lisp, 1 other). You can find them on http://ahungry.com/blog (its just the ones I added to /r/lisp in the last few days).
This entry was even better than the last one. I use both these techniques all the time, but in other languages and without assistance from a debugger. Modify a single part at a single point in a program...by hardcoding an assignment in a strategic place while debugging. Examine values in a trace...by adding print statements and running through it. Having it all pre-made for me would be great. And then running live in addition? Could be amazing. Unfortunately, I don't have any opportunity to build any big lisp system to try these things out.
I still have to do some minor cleanup (such as 'archive' also being pointless). I generated the list from the quicklisp-projects repository, as that contained the sources.txt which let me easily find the readmes. I didn't see where I could get the original source urls within quicklisp-client or I would have used the provided ql functions for getting a list of projects. Edit: Although I guess I could build up the project list from the ql function for listing projects and then pull out the remote project location as I currently am when passed in the project/package name.
There's no guarantee that I'd accept it. I am very glad when people do things on their own. They can follow their own vision without worrying about my personal tastes.
Hm, there is a package named archive on quicklisp/github, and it's a useful one. Is something wrong with it?
Turns out it was my mistake, I had thought archive was related to a quicklisp-project thing, but its a legimate package.
Sometime ago as a response to a similar question I wrote eval-in-swank.sh http://paste.lisp.org/display/138900
I have a local copy from 2014, it's a 500K zip file (which should be identical to the current one). What shall we do now? Can you provide a place to upload it?
Thanks, not sure how I didn't find that in my searching. It looks to be quite old, but I don't think ECL has changed its interfaces significantly in the past decade. It can probably hold me over until sourceforge get their act together.
Me too. I've just recently discovered the incredibly useful "presentation" feature of SLIME, but didn't knew that it could be used in conjunction with a debugger in such a powerful way. Looking forward to the rest of the articles in the series.
I could send it, but at this point it's maybe easier to just: wget -r --no-check-certificate http://common-lisp.net/project/ecl/manual/
shen-mode is up on ELPA, but it's pretty bare bones. I'm thinking of retooling it, one of these days. Supposing I get around to learning Emacs Lisp.
I was looking around to see if there were some alternatives to using emacs, which is not something I'm going to try myself, but I found these: [lispide](http://daansystems.com/lispide/) [able](https://common-lisp.net/project/able/) of these two lispide seems to be easier to install if you never worked with common lisp before. I never used any of the two so I cannot recommend them. There also seems to be an Eclipse plug-in called [Dandelion](http://www.cliki.net/Dandelion) but I was not able to check if the link is active since sourceforge seems to be down. 
Next year's ELS (2016) location has been announced and it's Kraków (Cracow), Poland. Exact dates and location are yet to be announced, but in general the ELS is going to be hosted by the AGH University.
Maybe it's time for sbcl to move out of sourceforge?
SBCL mirror on Github: https://github.com/sbcl/sbcl
Wow, that's almost close enough for me to seriously consider attending.
Doing God's work there, son. 
Everything should move out of SF at this point. Just tried opening CLISP manual to find specific equivalent to certain socket functionality. It's down for maintenance. Soo... any kind soul wanna return the favor and give link to CLISP manual pdf?
http://marty.alain.free.fr/confs/ among others is NOT about lisp, it's a document produced using a tool I developped for students in architecture. It's a work in progress with random choices. What is your problem with Mixed use of Serif vs Sans-serif? Do you really think that http://epsilonwiki.free.fr/alphawiki_2/?view=lambdatalk_slides_5 is nothing but (bad) web design? You'd be nice to show a minimum of empathy before giving your opinion! 
I was more speaking to the fact that a vast amount of quicklisp libraries have problems on windows and most implementations don't bother fully supporting windows(last I checked).
The original function NEIGHBORS is defined on top. Then the modification: (let ((old-neighbors (symbol-function 'neighbors)) (previous (make-hash-table))) (defun neighbors (pos) (or (gethash pos previous) (setf (gethash pos previous) (funcall old-neighbors pos))))) The next form stores the function object in a variable OLD-NEIGHBORS and stores an empty hash-table in PREVIOUS. The the original function gets redefined. The new function has access to the two new local variables (above). If there are neighbors stored in the hash-table for the position, then the value is returned. If not, the old function gets called, the result gets stored in the hash-table and the result gets returned. 
Aha! Now I start to understand. So when (attacking-moves) calls (neighbors), it redirects to (symbol-function 'neighbors). Am I on right track here? It's still quite blurry. For example, why is nested symbol-function call higher on priority list than the plain function? Not intuitive at all.
That's why we have symbols in Lisp. They are a name, but actually they are a real data object with value, function, properties, etc. Thus we can refer to things via symbols. Example: CL-USER 25 &gt; (bar) Error: Undefined operator BAR in form (BAR). 1 (continue) Try invoking BAR again. 2 Return some values from the form (BAR). 3 Try invoking something other than BAR with the same arguments. 4 Set the symbol-function of BAR to another function. 5 Set the macro-function of BAR to another function. 6 (abort) Return to level 0. 7 Return to top loop level 0. Type :b for backtrace or :c &lt;option number&gt; to proceed. Type :bug-form "&lt;subject&gt;" for a bug report template or :? for other options. CL-USER 26 : 1 &gt; :top CL-USER 27 &gt; (setf (symbol-function 'bar) (lambda () (* pi 2))) #&lt;anonymous interpreted function 406000D594&gt; CL-USER 28 &gt; (bar) 6.283185307179586D0 CL-USER 29 &gt; (symbol-function 'bar) #&lt;anonymous interpreted function 406000D594&gt; CL-USER 30 &gt; (setf *stored* (symbol-function 'bar)) #&lt;anonymous interpreted function 406000D594&gt; CL-USER 31 &gt; (setf (symbol-function 'bar) (lambda () (+ 1000 (funcall *stored*)))) #&lt;anonymous interpreted function 406001ACA4&gt; CL-USER 32 &gt; (bar) 1006.2831853071796D0 
I've added a short 'late binding' example for Lisp to the corresponding Wikipedia page: https://en.wikipedia.org/wiki/Late_binding#Late_binding_in_Lisp
 wget -r Wow, thanks. Wish I'd found out about this, oh, 15 years ago.
"Maintenance" is putting it mildly. Their entire storage cluster seems to have evaporated.
I use sed 's/^/ /' thecode.lisp and then cut and paste it. The point of memoization is that you don't call that explicitly but it's part of the function you want to memoize. That function has a part which looks up the arguments in a cache and if found doesn't recalculate, here is a generic version of it to help explain; (defun memoize (fn) "Returns a function which memoizes results" (let ((cache (make-hash-table :test #'equal))) #'(lambda (&amp;rest args) (multiple-value-bind (result exists) (gethash args cache) (if exists result (setf (gethash args cache) (apply fn args))))))) (defun memoize-rec (fn) "Memoize function associated with Function-Name. Simplified version" (setf (symbol-function fn) (memoize (symbol-function fn)))) ;;; (defvar memoize-+ (memoize #'+)) ;;; (funcall memoize-+ 1 2 3) Personally i think in scheme it's more friendly; (define (memoize-hash function) (let ((cache (make-hash-table))) (lambda args (apply values (cond ((hash-table-exists? cache args) (hash-table-ref cache args)) (else (call-with-values (lambda () (apply function args)) (lambda results (hash-table-set! cache args results) results)))))))) (define memoize memoize-hash) ;;; (define memoize-+ (memoize +)) Hope this helps a bit. 
What is wrong with CL:SORT? Are you simply trying new things to learn? If so, then I have a number of other questions. First off, do you care about other sequences besides a CL:LIST? Being able to SORT an array or a string has helped me numerous times. For all sequences, SLICE can be replaced with CL:SUBSEQ. (slice list 2 4) = (subseq list 2 4)) Please note the the performance of CL:SUBSEQ is significantly better, it is only travels down the list once at most (only for the START length actually), rather than your approach which does the entire list once, then the slice - 1, then the slice - 2. Your implementation of SLICE is quite poor for performance... Have you never used a linked list before? If you just want lists, CL:NTHCDR is precisely what you want for SLICE. For your MOVE function, you are again travelling trough the list many many times. This is an extremely poor approach to sorting as the longer the list, the more times you need to go through it over and over and over again, which means it would be faster to use an unsorted list and simply go over it once (or us CL:SORT, which is destructive) sorting it along the way. So, if you want to improve this code, it does not matter what language you are using ... it is a very poor algorithm and will be poor in whatever language. You need to look at the code and try to reduce the number of times you go through the list, from (LENGTH LIST) + (* (LENGTH LIST) 3) to 1 . Make sense? Google might be a help here, and TAOCP or similar (quicksort, bubblesort )... there are a bunch of algo's that work are are fairly easy to implement. For example : (defun bubble-sort-list (list predicate) (do ((swapped t)) ((not swapped) list) (setf swapped nil) (do ((list list (rest list))) ((endp (rest list))) (when (funcall predicate (second list) (first list)) (rotatef (first list) (second list)) (setf swapped t))))) Notice how it can only travel the list once, testing FIRST item with SECOND and then moving where FIRST is. So, in conclusion. CL:SORT, CL:NTH. CL:NTHCDR, CL:SUBSEQ can take away 95% to all of your code, which is very inefficient and needs a better design and algorithm no matter what language you are programming in. Even using NCONC over APPEND would help, and that is a simple and trivial step. Beyond that, the syntax and indentation are looking good, and learning CL and programming at the same time is usually a good thing as well. Enjoy Coding! 
I see that a main issue was traversing lists more than necessary. I'm guessing that using the loop macro would be the best way to get down to single traversal in certain areas. I'm not familiar with any data structures besides lists yet in Lisp, so I'll be sure to get to work on learning about arrays in Lisp. For the traversal, I knew I could have implemented the recursive functions using the loop macro, but chose to recurse without thinking too much of performance. Would using loop be the best substitute for recursion when improving performance of recursive (or possibly recursive) functions. I completely get that it wasn't necessary to make slice. I'm gonna keep learning. Thanks for the great advice u/drewc!
Ok I understand. Because a list in lisp is a linked list, random access takes O(n) time instead of constant time that it would for an array. Kind of funny I didn't assess that, be it it's *named* "nth." I'll check the hyperspec for the functions that are better suited. Thanks for the help u/lispm!
I've been programming in other languages for a while, so I'm aware of various data structures in general. I stress my naivety in my lisp knowledge, so when I said I wasn't acquainted with different structures in Lisp, I really do mean in Lisp. I get what you're saying about the recursion, and I'll continue to learn Lisp and comp sci in general. Your help is greatly appreciated. 
&gt; I get what you're saying about the recursion, and I'll continue to learn Lisp and comp sci in general. Note that some CL-like languages (like, for example many CL implementations and the scheme language and a lot of other languages not related to lisp for that matter) have Tail-call [optimization | elimination] (TCO/TCE) which can, when it works and is available, optimise/eliminate the stack frames. This makes recursion and iteration equivalent as far as such things go. And, just for fun, since you seem to know JavaScript/JS/ECMAScript somewhat ... it was originally implemented in CL : http://mxr.mozilla.org/mozilla/source/js2/semantics/ Lots of code, lots of fun :D 
Whoa 
I never really understood what logic programming was for. I understand this better now. I see it as "logic programming can be helpful when you need to find a solution given some constraints". That brick wall example was pretty cool!
Here's an example of memoization that I've seen used in production: http://beta.quicklisp.org/orphans/tfeb/memoize.lisp
/u/EnigmaticSynergy I'd like to point out technee's use of a string following the function's lambda list, instead of a comment preceding the function. In CL this is the syntax for documentation that can be programmatically queried, e.g. by your development environment or for documentation generation, whereas comments are typically discarded.
I can't say I've been looking much. It's certainly one of those things I wouldn't mind implementing myself, but I fear that I would just be adding to the confusion if I did.
I remember how surprised I was when I realized that a consequence is that just visiting every element using `nth` is O(n^2) -- and therefore by itself is already worse than an algorithmically ideal sort. {Although insertion sort is O(n^2) worst case anyway.} Sometimes you can change an algorithm to walk the list (e.g. with `cdr`) to avoid `nth`. Wikipedia describes an interesting such transformation for insertion sort: take successive elements from the front of the original list, and insert them into a new list that you eventually return. You might find it an interesting exercise to compare the efficiency of your sort efforts to the built in `sort` using `time`.
Author here: thank you for your feedback. It's a pleasure to hear this. The reason for project Euler in this case study is that it shows how logic programming can be applied to imperative problems, not merely declarative ones. 
Yeah GitHub's language detection (and highlighting) is pretty borked.
Hahaha... oh, man. I can see the CL crowd readying their torches and pitchforks from here.
How about BLAS/LAPACK wrappers/bindings. Just googled https://github.com/tpapp/lla I doubt you will find anything significantly better than BLAS/LAPACK.
You can find a few here: http://cliki.net/Mathematics CLEM is quite good: http://cliki.net/clem In general, the cliki site is a good resource to browse looking for good packages.
With respect to vectorization you might want to check MGL-MAT (https://github.com/melisgl/mgl-mat) Which includes CUDA operations on arrays. Also [shameless plug, CLML project maintainer here] you might want to check out CLML (https://github.com/mmaul/clml) which provides a Matrix and Vector operations a full BLASS and LAPACK interface as well as statistics and machine learning .
Check out sb-cga if you're doing things with matrices. I haven't used it in a bit, but it's by one of the sbcl developers and optimizes different matrix operations. I also have a pet project I haven't touched in a year or two at https://github.com/fisxoj/map that does linear algebra things, though it's by no means well vetted. It was an attempt at doing scientific computing in lisp.
Only useful for graphics, really. Restricted to matrix size, and matrix multiplication has not been vectorized - though pretty much all vector/matrix operations have been.
How about maxima (http://sourceforge.net/projects/maxima/) ?
Nice article! I'm not a fan of using special variables as much as you do, but practical examples like this need to be explained for people to understand that Common Lisp is a very practical language with a rich set of libraries. (Thanks Zach for quicklisp!)
One man's evolutionary dead end is another man's hundred-year language. I think fixed standardization is great if you don't want to rewrite all the code you've written every few years. And, come on, dearth of libraries? What in particular do you need? We'll point you to an existing library that does that.
I requested a feature that would allow repository owners to disable syntax highlighting... I don't think they will implement it...
Nice write up.
I'd suggest Shen but it sounds like you want a large community. Haskell but its not that lispy and you want dynamic typing. Python/Perl/Ruby, hell even Javascript are found by some people to be "acceptable Lisps" but you see them as inferior. I don't think what you're after exists so maybe this is of no help, but it could be built. The problems with CL you've cited are all to do with the size and nature of the culture and community. You could help work on that, or on some other language/community that you feel is a better starting point.
I am not sure how long ago your impression of the community around Common Lisp was formed, but I have had the opposite impression over the last decade. The maturing of Quicklisp has meant that there is a fairly comprehensive collection of libraries that have a standard access mechanism. The existence of various projects to provide documentation and reference locations of these libraries means that evaluating and incorporating a given library into a project is a more or less trivial task. For development of a medical web application over the last year and a half, I have not had a problem finding diverse libraries ranging from the construction of QR codes (cl-qrencode) to parallel processing frameworks (lparallel). Abandoned libraries are no more likely to exist in Common Lisp than any other language. When I use such libraries, the existence of source hosting services has meant that forking them, providing my patches for future users is considerably easier than it was a decade ago. Sometimes, one has to read code to understand how to use a library. This is not more or less a problem in Common Lisp than other languages as far as I can tell. Statistically, I would argue that there are far more undocumented Java code bases "in the wild" than Common Lisp based ones. Having multiple libraries to address a common need is a sign of maturity rather than a problem: I'd rather have choices than none. And I categorically reject the statement that "community does not make it easy to put together apps using multiple libraries": my project from the last eighteen months uses over forty Quicklisp libraries together without any problems. Oh, and we wrote the medical application on ABCL.
If the CL community talked to each other the way you talk to people on reddit, then yes, I would agree the community would be toxic.
DAYM, OP got told
how do you define a matrix in LLA? 
If you give up on Lisp it's only because you gave up on yourself. That said: racket is a scheme that's pretty good for day to day stuff. That said quicklisp + slime + emacs + Common Lisp (sbcl,clozure,abcl,ecl,lispworks,allegro,corman -- you are basically covered pick whichever works for you.) is the most amazing setup to grace this earth. You should just stick with it and if you need to implement your own libraries for some stuff so be it. In the end this will help you become a better programmer with mainstream languages anyway. With the introduction of ASDF3 common lisp as a scripting language has become a reality. (don't worry I hate Clojure too.)
if you think lparallel is amazing check out lfarm for distributed computing using lparallel(name might be off) made by the same people. I'm running a 2500 machine cluster right now and it's sick.
TBH, I can not find an easy answer right now, so I've [opened a ticket](https://github.com/tpapp/lla/issues/17).
"Why do you see the speck in your brother's eye but fail to notice the beam in your own eye?" ~ Matthew 7:3 
Also I doubt that it performs vectorization of arithmetic operations.
I got really excited and then realized it was only my stuff :D Thanks for checking it out though!
I'm in. We can call ourselves the 'Travis Triumvirate'.
Hi, can we move this conversation to the mailing list ?