If you're using `Future` you're not controlling side-effects, so you might as well do your logging Java-style inside the body of your future because it's a side-effecting program already. You could also construct futures that do logging and compose them with the "real work" future using `flatMap`, which will ensure that log statements happen in the right order. If you want to do this functionally you need an effect type like `Task` or `IO` that understands side-effects, in which case you can do `Task.delay(config.log.info(...))` (surely the log is part of your config!) and compose that operation with others using `flatMap`. 
I'm looking to start building some new backend microservices in a team that is used to a Java + Spring Boot stack. What's a good web framework to use? (e.g. akka-http, scalatra, finagle)
I would start by using the same frameworks you've previously used, rather than changing everything at the same time. Spring Boot isn't so bad. If you do want to go further I'd pick akka-http - I think it shows off the power of Scala in terms of "everything is plain old code that can be refactored according to the normal rules of code", and the DSL is nice. I think the purist communicy largely prefers http4s but I dislike its reliance on unsafe-looking `case` constructs.
Removed because this has already been posted (more than once if I remember correctly), and it is in fact already in the sidebar.
Also you can treat `Future` in a purely functional way, you just have to combine it with another monad that handles state using a monad transformer (or something like eff) Strict/Lazy are orthogonal concerns in regards to purity
&gt; FWIW I don't think Java's GC has much of an impact on startup time, if at all. Initializing the GC, the JIT and loading the classes have their own costs. I don't know the numbers. &gt; Are there links/benchmarks that show the contrary? How would you benchmark that? Take out the GC? &gt; Scala native and similar technologies can have better startup time because they load native code, not bytecode. There are many byte-code-based languages with faster startup times just like there are many native languages with fast startups and tracing GCs. Allocating and fragmenting heaps are not cheap.
Even in Haskell, I found that purely functional logging was a pain in the ass.
I accept that, but my assertion is that it's useful to give up that absolute certainty in exchange for somewhat simplified types. Anyway, using Futures doesn't mean exceptions become completely unpredictable--there is still a logic to them. It's just not in the type system.
The template thing is a shortcut to create simple macros. You may find that the type-checking rule is similar to a whitebox macro.
&gt; I think the stronger premise in scala-native is the ability to plug and play different GC's depending on your case (and you can always use JVM if you need to) Agreed. There are all sorts of interesting GC's that might be applicable to scala-native, e.g., [this one](http://www.pllab.riec.tohoku.ac.jp/papers/icfp2011UenoOhoriOtomoAuthorVersion.pdf) and it would be interesting to try many of them. I also hope that a future JVM (Java 10?) gives us better value classes in Scala, so that scala-native stack allocated structs can be done mostly with vanilla Scala. scala-native is certainly promising new technology. 
If you don't treat logging as an effect in pure code it becomes hard to know *what* you are logging. Are you logging the construction of your program, its interpretation, or its execution? All might be useful but you need to know!
&gt; The committee started to work on a new low-pause time gc but it's in a distant future They say it's designed for "a heap of 20GB" or larger. WOT :-O
I would love to see some evidence of that. It's fairly easy on the other hand for me to produce my own evidence that startup for Java has a fixed cost of X (~100 ms) and a variable cost that increases with the amount of code loaded on startup. Also, what other bytecode-based langauges are you referring to that start up that much faster? Long ago when I last measured, Node.js was almost as slow as java in terms of variable costs loading javascript code, and Python, while faster than Java, was still very slow to start compared to native code. For all three of these languages there was a relatively small fixed runtime startup cost and a huge variable cost associated with application size. If you want to test this yourself, measure hello world programs in all three runtimes. Then measure a program that loads a huge library, like for instance the AWS SDK, and does something simple like query an AWS endpoint for some piece of data. 
&gt; I would love to see some evidence of that. Search for it. &gt; It's fairly easy on the other hand for me to produce my own evidence that startup for Java has a fixed cost of X (~100 ms) and a variable cost that increases with the amount of code loaded on startup. If it's "easy" then you can share it. &gt; Long ago when I last measured, Node.js was almost as slow as java in terms of variable costs loading javascript code, and Python, while faster than Java, was still very slow to start compared to native code. All of them you've mentioned have a faster startup time. .net and ruby too. The slow st of the JVM is well-known. &gt; If you want to test this yourself, measure hello world programs in all three runtimes. Quality measurement :D I don't really care we can see how heavy the JVM is. Performance is maybe better then on the other toy platforms but it's still has a long way to go.
I'm not interested enough in convincing you to write a long blog post with scientific-quality documentation on my reproducible measurements, and apparently you aren't either, as you continue to throw around your incorrect opinions like they are obvious fact.
http://scalatimes.com/
One option I didn't see mentioned in the SO post is: import scala.language.higherKinds trait BaseTrait[A] { type T[_] &gt;: this.type &lt;: BaseTrait[_] def someMethod[B](f: A =&gt; T[B]): T[B] } class ConcreteClass[A] extends BaseTrait[A] { override type T[_] = ConcreteClass[_] override def someMethod[B](f: A =&gt; T[B]): T[B] = new ConcreteClass[B] } Because of the `this.type` lower bound, you at least can't specify `T[_]` to be anything outside the hierarchy of the implementing class. Consequently you don't have quite the same issue as `type Self[X] &lt;: BaseTrait[X]` where the implementing class could specify something complete separate from itself, nor do you get the proliferation of type parameters as with `BaseTrait[A, T[X] &lt;: BaseTrait[X, T]`. The latter is important because it cleans up method signatures; you don't need stuff like `def foo[T[_] &lt;: BaseTrait[_, T]]` (or whatever). For a shallow hierarchy like `final case class Foo extends BaseTrait` I believe it works exactly like you'd want. The closest I've been able to abuse this setup is to specify `T[_]` as some super type instead of the concrete implementing type and/or to return an instance of the supertype rather than the bottom-most type. I haven't been able to override `T[_]` more than once in a hierarchy. Still, typeclasses are probably the best option.
Interesting, didn't know that. 
Good question, I need to check it. You can always monitor actors, that are used by the stream, described here: https://softwaremill.com/akka-monitoring-with-kamon-part-3/
Wasn't value classes meant to be for Java 9?
Oh nice! :D I'm holding my breath then..!
&gt; because L is a type alias and not an abstract type, so there's no possible type soundness issue That actually makes sense, thank you! I got it wrong since Martin said they were going to be eliminated in his Copenhagen keynote [here](https://youtu.be/9lWrt6H6UdE?t=25m49s) In any case, I think I'm going to stick with proper type lambdas when we have them ;)
SECTION | CONTENT :--|:-- Title | Keynote - What's Different In Dotty by Martin Odersky Description | This video was recorded at Scala Days Copenhagen 2017 Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org Abstract: Dotty is the project name for the next iteration of the Scala language. As we are nearing a first developer preview, this talk will give a summary of the major changes and innovations as they are currently implemented. I will show with many examples how you can increase the legibility and safety of your Scala programs using the new feat... Length | 1:01:07 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
So after some digging, tracing in Akka Streams is not supported: https://github.com/kamon-io/Kamon/issues/206 Although, you can generate some metrics manually: https://github.com/svezfaz/akka-backpressure-scala-central-talk/blob/master/demo/src/main/scala/eu/svez/backpressuredemo/A_local/HelloWorldBackpressured.scala https://github.com/miciek/monitoring-akka-prometheus-kamon/blob/master/src/main/scala/com/michalplachta/shoesorter/Decisions.scala depends on what you really want to achieve. 
Unfortunately not. [This](http://cr.openjdk.java.net/~jrose/values/shady-values.html) is the last thing I read about it. I don't even know if it's guaranteed for Java 10. scala-native has its own way of creating stack allocated structs, but it would be great if restrictions on value classes were lifted in the language proper.
Is there any support for Ammonite scripts in Intellij yet? What about step-by-step debugging scripts?
How would one use type classes here? I've used them in other cases, but I'm not sure how to apply them here. Can you give a quick example?
Is it finally fixed under Windows? Last three times I tried it (several months between each attempt) it was crashing even though on github it was marked as fixed :-/.
Usually this happens when you didn't do import project as an sbt project. The better solution is to close the project, then do import project, select the directory, and make sure to choose External Model: SBT. No need for `sbt-idea`.
Can you be more specific? Scala has had some form of macros for years
You've already got one answer so I'll go a little more step by step to show one way you can get from where you are to a typeclass implementation. Starting from: trait BaseTrait[A] { def someMethod[B](f: A =&gt; T[B]): T[B] } Put the parameter you wanted to be a specific type (`T[_]`) into the type parameters of the base trait so it's actually available for use. trait BaseTrait[T[_], A] { def someMethod[B](f: A =&gt; T[B]): T[B] } This parameter will eventually be the concrete class you had as a subtype. Instead of implementing the behavior as part of the subtype, you will remove the inheritance and _associate_ the behavior with the standalone class. It's basically changing from an inherited method to a free function. So instead of class A { def method: B } where you have an `A` internally generate a `B`, you have def function(a: A): B where you generate a `B` externally or an `A`. Applying to your case, instead of `BaseTrait[A].someMethod[B](...): T[B]`, where `someMethod` delegates via inheritance to a subclass `T[A] &lt;: BaseTrait[A]`, you pass in the `T[A]` directly. trait BaseTrait[T[_], A] { def someMethod[B](ta: T[A])(f: A =&gt; T[B]): T[B] } Here you choose wether you want to support one implementation per `T[_]` or one implementation per `T[A]`. For instance, do you want one behavior for any `List` or do you want to treat `List[Int]` separately from `List[String]`. The main difference is where you put the generic `A`. Putting it on the trait's method gets you the more generic behavior, putting it in the trait's type parameters gives you more specific control. You can convert from one to the other so for now I'll assume you want generic behavior. To get that, move the type parameter to the method. trait BaseTrait[T[_]] { def someMethod[A, B](ta: T[A])(f: A =&gt; T[B]): T[B] } We're not going to have anything extend this class, exactly, so rename it according to the behavior it represents. trait SomeMethod[T[_]] { def someMethod[A, B](ta: T[A])(f: A =&gt; T[B]): T[B] } This is your typeclass. You create an instance of the typeclass for any type you want behavior for. val SomeConcreteClassMethod: SomeMethod[ConcreteClass] = new SomeMethod[ConcreteClass] { override def someMethod[A, B](ta: ConcreteClass[A])(f: A =&gt; ConcreteClass[B]) = ??? } Then any method which used to accept `BaseTrait[A]` as a parameter, now needs to accept the concrete type _and_ the typeclass instance, then forward the concrete type to the typeclass. For conciseness, the typeclass instance is usually specified implicitly. So say you had a function which accepts anything extending `BaseTrait[A]` def foo[A &lt;: BaseTrait[A]](a: A) = { a.someMethod(???) } you would change to accepting any `A` that has a typeclass instance. def foo[A](a: A)(implicit ev: SomeMethod[A]) = { ev.someMethod(a)(???) } And that's pretty much it! Now you have precisely guaranteed that given a `T[A]` and an `A =&gt; T[B]` you will get a `T[B]` such that the `T` is the same class for which the behavior is defined; given a `ConcreteClass[A]`, you can only get a `ConcreteClass[B]`.
You *might* need to delete your .idea dir in the project too.
`http4s` doesn't actually _rely_ on `case` though. `http4s-dsl` does, but that's just one of many DSL you can define for `http4s`. Have you had a look at [rho](https://github.com/http4s/rho)?
Thank you!
Sometimes it imports funny and you have two overlapping modules. Another thing to try is to go into the project module settings and delete all the modules, then it will show an option to import.
Nicely done!
Great job. I like where Scala is heading.
2.6 is almost there! Albeit nearly 1 year after the fact...
Don't worry, it'll be full of breaking API changes.
https://twitter.com/simon_yann/status/874971964942712832 EDIT also https://twitter.com/ScalaWilliam/status/872964635271376896
I just looked at the milestone page and it still looks pretty far away to me. 
Think they're shooting for GA in July, so not at all far off. Haven't been following too closely as I've been wrapped up with Scala.js frontend but I've believe they're on RC1 or RC2 now.
It's not about strictness or laziness. Future is not referentially transparent. val x = Future { foo() } val y = Future { foo() } is a different program from val x = Future { foo() } val y = x 
/u/saosebastiao, which milestone page? We are very close to release 2.6.0 (next couple weeks). 
https://github.com/playframework/playframework/milestone/37
Yup this is our biggest hurdle at the moment. After trying out the http client side of akka-http I started to look elsewhere but just haven't found anything half as good as the spray client.
Isn't Akka http pretty much the continuity of spray?
I've never liked Scala-IDE: it feels slow and has a weird interface. It also has a problem with SBT projects. Try IntelliJ Idea Community Edition or pimp your favourite editor. I use [neovim](https://neovim.io/) without plugins.
When you're publishing a library, do you include the project name in the org name? It doesn't make sense to me why you would do that, but I see it done either way. To clarify, is it proper style if your library is imported as libraryDependencies += "orgname" %% "projname" % "0.1.0" or libraryDependencies += "orgname.projname" %% "projname" % "0.1.0" ? And then say if you want to publish a separate package that contains test utils for your projname, what would you call it?
Plus every time you update the build config you need to regenerate the ensime config and restart the server and if you forgot to stop the server before closing your editor your ensime config will corrupt. Also, ensime only works some way in emacs - other editors are barely supported.
Emacs in Evil mode does it for me. Then I just use SBT from command line.
I downloaded the bundled version from here http://scala-ide.org/download/sdk.html a few days ago. The platform-specific download link worked just now, too. I installed the new release by unzipping the tarball and updating my `/opt/eclipse` symlink to point to the unzipped dir, and that was that. I heard from others that the update site link (used to add or update plugins to/in an existing Eclipse install) wasn't working last week, though.
&gt; But, if you don't need to have a UI, going with Akka HTTP is the best choice. Akka HTTP is a library. Play is a framework. Play gives you a hot-reload experience, where you can keep your application running, change the code and have Play reload everything (i.e. database, etc) for you. Akka HTTP does not have that. &gt; Unlike Play, it is not a full stack web framework. Play comes as a series of modules, so you never need to load the modules you're not using. Play is very often used as a REST API, and there's even a guide written up telling you how to do it: https://developer.lightbend.com/guides/play-rest-api/ There is minimal performance difference and the artifact size is the same, so using Play or Akka HTTP ultimately comes down to your preferred style of routing and whether you like Play's dependency injection options, be it runtime or compile time. Appealing to Play's "size" or "full features" makes no sense at all -- it's like saying Akka HTTP is too big because Akka Clustering and Distributed Data are modules in Akka. 
I doubt author really understand the topic. No offense but text looks like allegations without any practical value. P.S.: and I have strong feeling author down votes criticism.
&gt; body parser's static parse is gone, now need to use injected parsers No, it's still there. It's deprecated. &gt; multipart form data file's ref property is now protected (can't call finalize on it) You're not supposed to, the TemporaryFileReaper does that. &gt; http filters enabled by default Migration guide details how that works, it's a one line tweak in configuration if you want to disable it. &gt; joda time gone, replaced with java time Dunno about this one, I think that just got moved out to play-forms-joda &gt; now need to pass around DI injected ControllerComponents Controller is still around, you can also use InjectedController &gt; now need MessagesApi in place of Messages Messages is the intersection of MessagesApi and Lang now, there was not a clear distinction before. Also, check out MessagesBaseController: https://github.com/playframework/playframework/blob/2.6.0-RC2/framework/src/play/src/main/scala/play/api/mvc/MessagesRequest.scala#L120
I think, the formatting is mangled. There are `&amp;quot;` where I would have expected ".
As with any homework. Break it down to smallest problems you can solve while having big picture in mind. Use FP so that it's easy to compose &amp; extend down the line. Keep piling on requirements as needed.
If your organization releases a lot of projects and your projects have a lot of modules then it's probably worth including the project in the groupId; if not, not. Use as much nesting as you need to, but not more. I'd try to avoid having a "test utils" concept, but if that's really the best name you can come up with then I'd call the module `projname-testutils`.
&gt; EDIT: Since the IntelliJ suggestion comes up quite often, I'll address it here: I'd be looking for a Scala IDE, for that IntelliJ doesn't qualify, not even close. It implements something that resembles Scala but isn't, not even close. It'll only work with your Scala code if it's practically Java. Might as well do Kotlin instead, and oh hey, who happens to be the patrons of Kotlin I wonder. What are you on about? It's normal Scala as everywhere else...
&gt; List.head is quite heavily used here... Looks like you have some bugs to fix!
Great work :) Looking forward to the day the GCs are written in Scala.native
&gt; ...but this also means literally no IDE, but hacks Hacks?! Well, yes but it works... &gt; 14 years of scala, and still no IDE IntelliJ IC has an excellent Scala extension as I've said it before. Scala IDE is "dead" because barely a few people use it.
Astrologists proclaim week of the Ammonite. Downloads doubles! (Heroes of Might and Magic ref :)
As another commenter said, don't use gen-idea. Their external model support is better. That tipped over a couple years ago. Also don't use `sudo` for stuff like this.
Nope, that is not what will happen at all. Your `doMath(matrix)` will be performed on a single thread in your passed `ExecutionContext`s thread pool. Hard to say how to parallelize your `doMath(matrix)` without knowing what it does, but safe to say this is still single-threaded (albeit, on a different thread than the one that `run` is running in).
Thanks for the reply. I'm not that fond of eclipse myself to join the gitter channel. In general I wanted the big picture of where Scala IDE is at, and I think I get it now.
Let the fun begin x)
Wow, such fast progress! Keep it up!
I've recently had to adopt Scala-IDE because I've had to adopt Eclipse for entirely un-Scala-related reasons. The one thing I've always found necessary with Scala-IDE—which is a good idea with Eclipse anyway—is to edit my eclipse.ini and bump the mx setting to something realistic, e.g. 2G. If I forget to do that, I see anything from GC thrashing to hanging. So you might want to try that. As for Eclipse vs. IntelliJ, as a couple of other people have pointed out, IntelliJ uses its own parser and doesn't actually support the Scala language. For me, that's a non-starter.
Thanks for the suggestion.
I think you may be confusing what the editor supports as highlighting/"inteli-sense" and general compiler support. You make it sound as if the Intellij cannot compile some code. This is absolutely not true. Intellij uses normal sbt, hence normal scalac just like any other IDE. Can you give an example of a library Intellij **doesn't work with**? edit 1: As for &gt; It doesn't even get overloaded methods right (try and do a type with more than 1 map definition and use it in a for comprehension) You're comment is very unclear, but if you mean something like this: https://i.imgur.com/BjnYX36.png https://i.imgur.com/rXR8qi9.png Then it seems to work perfectly fine for me? edit 2: I trying googling "infer-argument-types" but nobody's seems to be using it and as far as I can see it's just a compiler flag hence Intellij cannot *not* support a compiler flag. Intellij doesn't do compilation itself hence it will just work with whatever Scala version you put in your sbt file. I found the definition of this flag to be "Infer types for arguments of overridden methods." but once again I'm not sure what this means. Since when does Scala infer the argument types for methods parameters? I couldn't find any more information about this flag hence I couldn't test it myself. If you could give an example I'll gladly test it on the current version of Intellij.
&gt; Oof… isn’t it a big list BTW, we also have additional rules in [wartremover-contrib](https://github.com/wartremover/wartremover-contrib).
What's the preferred way to connect some outside cache with stream? (I use Akka Streams if it matters) I have a stream of events and I do some calculation on them depending on my cache. I only read from it but it may change in the meantime and when that happens the stream is notified and somehow receives updated cache. I am not convinced of emitting it in the stream together with my events because this "cache" might be a HashMap with few thousand keys so I am worried about potential performance overhead (didn't benchmark it yet tho) but I would like to avoid storing it in outside 'var'. Are there other solutions I could try?
a bunch of tables but no charts? not the best way to visualize such vast array of numbers
I would tell you that of course it can, if there is a macro to teach it.
Its a very nice coat of paint on that bike shed, but its just a coat of paint on a fucking bikeshed... why the hell is this the top post in this subreddit ? Also, this just smells of anti-pattern.... it basically repeating your schema in your code, suddenly your code has to be aware of every little schema update... Why are the queries hard coded in the first place rather than loaded from files which can be modified without having to recompile your whole application ? Surely this is a must when your database schema are so complex that you could imagine needing compiler checks for your queries.
I don't agree with you. We're using slick in production, there was indeed some problem with 3.0.0 (regarding transactional queries), but we spotted it with load testing before deploying the first version and upgraded to the last version of slick. It's actually running fine and can support load up to 50k requests per minutes on a single aws m3.medium as a server and T2.micro as database. More when multiple m3.medium works together. The key is load testing and optimisation. I agree that the parts about thread pools isn't explained clearly in the documentation, and it should be because it's a very important point. 
The way its describe here it seems like the schema is manually maintained. Even if that was not the case having to run compilation an a machine with access to the database and integrate the script that does the show schema select in the compilation process seems a bit of overkill, considering the alternative is just running a query that describes the table an manually looking for changes... after all table schemas tend to be quite static in nature.
&gt; The short answer is that A with T is a new class So whenever I use `A with T` it is essentially the same as manueally writing `class AnonymousAwithT extends A with T` and replacing every `A with T` with `AnonymousAwithT`? Also, you are saying that `asInstanceOf ` without `isInstanceOf ` in "unsafe", however I suppose that traits without any implementation (as `T` in my example) are an exception to this rule? Still, I do not understand why `aAsInstanceOf.isInstanceOf[T]` is false but `aAsInstanceOf.isInstanceOf[A with T]` is true. I would have expected both to be false because `aAsInstanceOf` is neither `T` nor `A with T` at runtime. Or so I thought.
First, thanks for confirming the `AnonymousAwithT` thing! I think I understand that part now. However, I am sorry, but what I still don't understand is `isInstanceOf`. I thought this was checking "runtime-types" only. So why does `aAsInstanceOf.isInstanceOf[A with T]` return true *BUT* calling your method `af(aAsInstanceOf)` will return `IS A: true IS T: false IS A WITH T: false` so within that method, suddenly `isInstanceOf[A with T]` returns false. That puzzles me. This means, when I tell the compiler "hey, believe me, this is an `A with T`" then `isInstanceOf[A with T]` will return true. However, when I don't do that and the compiler has no knowledge about the type (rather than beeing `Any`) then `isInstanceOf[A with T]` behaves different. How can it be that "static-compile-time-types", which I thought to even be lost at runtime, can change the behaviour of such a runtime-only check...
I've built a few REST APIs using Http4s and despite minimal experience with Scalaz/Cats found it to be pretty neat. To be honest, I was happy to get away from Play. I find the article rather funny, as I preferred Http4s simple documentation (and the same goes for Finch) to that of Akka Http. I found the Akka Http documentation to be overbearing when I just wanted to get started.
if you want to dynamicly mixin trait to object you need to use such macro like this: https://github.com/ThoughtWorksInc/Constructor.scala/blob/master/Mixin/src/test/scala/com/thoughtworks/MixinSpec.scala asInstanceOf doesn't change object
I'm guessing there's some rule that says anything with type `A with T` is an instance of `A with T`.
I see. Thank you (and also /u/yespunintended) for uncovering this. Also good to know this compilerflag, I will use it the next time before I ask :) Should I create an issue and ask if that is desired behaviour?
Sure, it's worth clarifying I think.
Was it [The High Cost of AnyVal subclasses...](https://failex.blogspot.jp/2017/04/the-high-cost-of-anyval-subclasses.html) by any chance? The same topic is also covered in [Scala High Performance Programming](https://www.amazon.com/Scala-Performance-Programming-Vincent-Theron/dp/178646604X/), which I happened to read after the article. 
Ha, the first post is the one I searched for. Thank you! Probably should have searched for "AnyVal" too...
Yes, `vwithfield` is a field-at-a-time version of a case class's `copy`. Sadly, due to BC requirements, I suspect Scala could not transparently make case classes value classes overnight (though I would love to be wrong here). And current extensions of `AnyVal` only have a single delegating struct field. So I would assume a close-but-not-exactly-the-same alternative to case classes would emerge to leverage this. As for boxing/unboxing, not sure this helps. The boxing from primitives to their wrappers would still exist (assuming that `java.lang.Integer` remains a ref type for BC reasons). The boxing that occurs by javac (and I presume scalac) use the `valueOf` static methods on the primitive wrappers. Not sure how that could change.
This is cool I wonder if it would make sense to add this a library available to Scala Fiddle?
I've personally had a great time using Twitter's Finatra (built off of Finagle). Easy to set up and very performant. I dislike akka-http because at least to me, it has a much more obtuse interface and is harder to get started with.
Bunches of stuff, but it's all falling apart. My template project, https://github.com/KyleU/boilerplay, is doing fine, but upgrading to Play 2.6 is becoming a slog. I made a solitaire game, https://solitaire.gg, but Amazon just killed my server so I had to push an unfinished beta from another machine. There's ~600 Scala.js projects available at https://definitelyscala.com, but I have to fix a some stuff before they're usable (thanks /u/sjrd). I've got an awesome database client webapp that I haven't released yet, but I can't find the motivation to market it or a partner to make it salable so I'm just going to open source it once I finish those others. The current plan is to catch up on these, stop making bespoke Scala projects that need constant support, and just start helping other folks build cool shit.
Probably the most interesting thing to folks here would be that I've been working on a [custom scaladoc doclet](https://github.com/fulcrumgenomics/fgbio/pull/251/files#diff-57d64dec0e93b0c8d2d00bb6de489020). It's been a frustrating process as there are very few examples around and most of those are many years old. The doclet in question is not just a minor tweak to the standard doclet, but is instead intended to extract documentation of a small number of classes as a MarkDown document. It's part of a toolkit of bioinformatics software which produces tabular output files via case-classes, and the goal is to create user (not developer) documentation of the output formats.
Hi lihaoyi, your project seems to have disappeared from the surface of Earth (website and github account gone). What happened? That was super useful :| 
Because it's the base type only to be instantiated as Some or None (which are case classes).
They shouldn't, but surely they can.
Thanks, that makes sense! Why not a trait then?
try it case class A() case class B() extends A
My bad. I though case class extensions was exactly the reason wart remover want you to make all case classes final.
Maybe try merging these into scalaz-outlaws?
Seems like the import line in the readme for the covariant version is wrong.
Indeed, I'll fix it.
Your link is broken (404) by the way
I started this project from https://github.com/ThoughtWorksInc/Constructor.scala and https://github.com/ThoughtWorksInc/Caller.scala . In this 2.0 version of feature.scala, I merged the two projects, and added some other utilities to operate on named arguments and mixins.
Great comment, and I largely agree. I think the Church encoding is an OO-style, but there is another OO-style that are closer to the reified FP style. That is, simply unsealing the trait turns it into OO-style code. Now I can't say which is more OO---that term isn't well defined. The point about decomposable representations is investigated in depth in the "Deep and Shallow" embeddings paper---well worth a look if you haven't read it. As for multimethods or extensible records, I know at least for multimethods there are issues with modularity and probably static reasoning. I agree they solve the extensible problem but I think they throw away some of the ability to statically reason about code and I expect that's not an acceptable tradeoff for FP people. I'm not very familiar with extensible records. I guess the type system becomes more complicated if you include them.
Ahhh yep, nice catch.
This **feature.scala** project, along with [example.scala](https://static.javadoc.io/com.thoughtworks.example/unidoc_2.12/1.0.1/com/thoughtworks/example.html), [tryt.scala](https://github.com/ThoughtWorksInc/TryT.scala), [RAII.scala](https://github.com/ThoughtWorksInc/RAII.scala/) and other popular libraries including Scalaz and Shapeless, are the foundation of [DeepLearning.scala](https://github.com/ThoughtWorksInc/DeepLearning.scala/) 2.0, which is a project sponsored by ThoughtWorks. DeepLearning.scala 2.0's release candidate version will be published in this week.
&gt; Have you considered forking the project and just making that inversion in the fork? No, because I am not using ammonite currently. If someone does fork and remove the tracking feature, I will follow the fork gladly!
wow. I didn't know about that. Uninstall in progress. It's so sad these days to create a dozen of env variables or stuff to remove analytics.
Link to where we can donate for Lihaoyi?
This looks to have a bit of a learning curve. Why would you use this instead of just learning scalaz?
I would say the scala ecosystem is build on very few people, and lihaoyi is one of these. He is also one of the few who is not paid to work on this stuff (as far as I know) a lot of people use his stuff, but since very few contribute it's okay imo that he would like some money in return. you always have the option to fork and continue his work, I just did that with autowire since he didnt react to any pullrequests there, I asked him privately if it would be okay if I try to take over and he gladly accepted. Imo what he is doing is very reasonable, he can't work on 10 libraries at once in his freetime as to ammonite, I'm not sure for what I would use that, I just use the scala worksheet in intellij. I also don't think collecting user data is bad practice, specially in an open-source project where you can check exactly what is collected btw. intellij idea also does this 
https://www.patreon.com/lihaoyi A one-time donation can be made by becoming a patron, then canceling after one month (which is pretty inconvenient imho).
What's rubbing people wrong is the unfortunate reasoning behind logging. First it's explained it is to understand usage and improve the program where needed, that's ok (though there should a shitload of anonymity guarantees and only general stats not actual third party source code being logged, a there it's a shame it's using google api's). Later it's said that if people will pay than logging can go. But if that's the case logging is not to understand usage and improve the program, it's for monetization (which i highly doubt it actually is, i never heard of a relatively low profile program like ammonite monetizing using google analytics). I sincerely hope it's just a misunderstanding or a clash of cultures in action. Don't think anyone begrudges lihaoyi to generate some income out of his projects.
yeah I agree totaly with you, I wouldn't endorse it either. I guess what I'm trying to say is, that I understand lihaoyi point of view. 
maybe he's using the usage data to create an enterprise solution, I don't know.. 
Has anyone ever read my explanation for why I added logging, and why monetary contribution will obliviate the need for it? It's all there. I've given an explanation; if people want to feel outraged without trying to find an answer, those are not users I want anyway
For all of you complaining that software you get for free, don't need to install, and don't contribute to at all doesn't do exactly what you want: it would be trivial to create a wrapper project that embeds ammonite, and just disables the remote logging. Apart from bumping the ammonite version, you wouldn't even need to update between versions. 
His tone could use work, but him wanting to make money from his craft is not a shun worthy offense. The ideas that he doesn't have the right to make his creation any way he likes, or that he should never try to make a living with it, are equally tone deaf in my opinion. 
Not how I read it. The program is how he made it. If you want custom changes or creative control then he would like financial support. There are lots of projects that wouldnt just make any change I wanted. And many of those would make the change if I were paying the bills. 
So you admit you're directly making money by monetizing the usage patterns of your end users? If so, that's perfectly fine, but don't lie to us about it. If you're not actually monetizing this data, you're just including a user-hostile feature to essentially annoy or displease your users enough such that they will pay you for it to go away. So, which one is it - are you a liar or an extortionist?
Yes, this is the correct answer. You can reach this answer with a bit of thought, a bit of empathy, and a bit of https://en.wikipedia.org/wiki/Principle_of_charity.
**Principle of charity** In philosophy and rhetoric, the principle of charity requires interpreting a speaker's statements to be rational and, in the case of any argument, considering its best, strongest possible interpretation. In its narrowest sense, the goal of this methodological principle is to avoid attributing irrationality, logical fallacies or falsehoods to the others' statements, when a coherent, rational interpretation of the statements is available. According to Simon Blackburn "it constrains the interpreter to maximize the truth or rationality in the subject's sayings." Neil L. Wilson gave the principle its name in 1958–59. Its main area of application, by his lights, is determining the referent of a proper name: How should we set about discovering the significance which a person attaches to a given name? *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot/) ^] ^Downvote ^to ^remove ^| ^v0.22
You say you will be willing to pay money, but if you actually were willing you could have paid already. Several people have, and not just through Patreon. Words are cheap; if you're really willing to pay me 50 bucks, do it. And I'll give you a way to disable your ammonite tracking forever (`--no-remote-logging`, which already exists)
The principle of charity ceases to apply in a rational discussion when one party asserts something self-contradictory. There is no way to ascribe positive motivation to a person who says one thing, and then follows it with another contradictory statement describing why they are doing the thing they just said they aren't doing. "I need to have access to your home to ensure that my product is of the highest quality, and I assure you that I'm not actually selling info I learn about you from that. I am, however, happy to waive this requirement if you pay me." So, theoretically, if I pay you instead of letting you into my home, my services actually become worse, because you no longer have access to the data that you need, by your own admission, to ensure you do a good job. If this is true, you present me with three choices: - Allow you unfettered and opaque access to my home, in return for a functional version of your product - Retain my privacy and security, while paying you for a version of your product that is by your own admission inferior, due to the lack of tracking info used to improve it. - Do not use your product, and retain both my money and my privacy. Of course, the rational decision is the third one. The second one might be rational as well, except that there are significant current structural problems with it. You haven't gone out of your way to provide mechanisms for people to purchase your product. If this were to change and you were to run a business that sold licenses, then, of course, the second option is superior - it allows for a mutual and consenting exchange of value between parties. Note: the above options assume that you're not a liar, but rather a person who is good-natured yet not terribly skilled at product marketing. If this is not true, and you are a liar, then there is only one rational choice: avoid usage of your product. Do you see how your own actions and statements make it nearly impossible for anyone to come to a common ground with you, no matter what they think your motivations are? 
&gt; Either way, the popularity of your tools, and I'd say the popularity of Scala itself, hasn't reached critical mass, so you can't get away with this sort of stunt without it resulting in a large setback to the growth of the community around them. That's my biggest worry. Enough Scala drama already. 
Your rather hostile condescending reactions only enable that :/ It seems you feel your intentions are misunderstood, and you're right in that they are. While blaming others for uncharitable reasoning and whatnot might be justified, it's not a solution. The solution is to not be misunderstood. In that way having a presence on the internet is like politics, your words and intentions can be twisted, it's up to you to avoid it. Excuse me these harsh words, but my take away from this topic is "double check if i use any lihaoyi projects and make a backup if i do cause the author ~~seems unstable~~ can be erratic" is that really the reaction you want people to have?
&gt; You say there'll be a hostile fork and disgruntled collaborators, but all I see are hostile comments by disgruntled non-users. You may be well right. I guess I should stop commenting and wish you the best.
You continue to make an unfortunate, but perfectly understandable, mistake in the way you are handling this. You need to understand that when it comes to public perception of a product and brand, there is no right or wrong answer, and to some extent, your own personal opinion is utterly immaterial. The phrase "the customer is always right" does not mean what you think it does - it's not that the demands of an individual customer must always be satisfied. In fact, sometimes the right thing for both parties is to politely decline the request from an individual dissatisfied customer. What it does mean, however, is that the customer *in aggregate* is always right, because by definition, that's the market speaking to you. If you want to be successful by providing a product or service, it's important to remember that your own personal opinion does not matter at all, and in fact, right or wrong, if you discount, dismiss or ignore large amounts of feedback, you're going to fail. Either you'll have a competitor step in and do it better than you, or your business will slowly erode. You may not think you're running a business, but any public offering of a product or service is in some sense a business. You have two choices, both of which are perfectly acceptable, but either path has consequences. - listen to your customers, and provide what they are asking for despite the fact that you disagree. You are there to assist them, not yourself. Of course, this means you have to subjugate your own desires to some extent, and the project is no longer exclusively fueled by passion or personal desire. - continue to ignore or dismiss your customers desires. This is also perfectly fine, but you have to accept the fact that people have no obligation to agree with you or use your product. You get to keep it as a personal passion project, but you cannot then expect everybody else to walk in lockstep with you. I see most of the criticism here being motivated by the commenters attempting (either purposefully or subconsciously) to save you from yourself. You have to realize that most people only form strong negative or positive opinions on something that they feel strongly about. The good news for you is that there is a large community of people who feel passionate about your product - if they didn't care, you wouldn't be hearing anything. The danger to you is that if you continue with the communication strategy you're using, that community may evaporate.
I was same, but I keep getting convinced, by some militant dynamic programmers-friends, that it's just different workflow. At the end of the day, those guys are smart and productive, so there must be something to it that I'm missing I suppose? I also consider Rich Hickey pretty smart, so...I think if there's a place where dynamic languages are of least pain, is must be in some dumbed down language, i.e. dialect of LISP... I still dont believe any workflow can substitude lack of types, and it will be pain, but there must be a bit of mental leap / different workflow in that world.
He just didn't express himself well in the ticket. The guy contributes a lot to the Scala world. Give him a break. 
To set out the facts, without snark: - The reason I have logging is so I can decide if Ammonite is being used and thus worth maintaining. If nobody is using it, I will stop maintaining it and do something else with my life. If someone is willing to pay to support it, that is an indication that it is being used, and thus I will no longer need logging to validate it's usefulness with usage data. That is all. - I'll happily make bespoke changes to Ammonite, including changing the default, in exchange for cash. In this world, people pay for food and shelter with money, which they get in exchange for doing things other people want. If someone wants me to do something, and are not willing to pay anything for it, forgive me if I say no. - The logging is easy to disable, and you can write a trivial wrapper around Ammonite yourself to make it always-off. All the talk about separate jars for logging/no-logging versions is rubbish: you can create an alias yourself in 10 seconds, why need a separate jar? - It's on by default. If you don't like that, turn it off. Or stop using Ammonite if you want to make a philosophical statement. I don't make any money off any of you, so I won't be particularly sad to see anyone go. - Ammonite is MIT licensed. Feel free to fork it and publish your fork with a different default value. I do not expect this to happen, for reasons already stated, but you are free to do so and I'll be happy for you to do so - If you want to support Ammonite monetarily, feel free to do so. Many have, including universities using grant money, for no reason but to express their thanks, without public grandstanding. If you want to, do it. If not, there's no point telling the world how much you want to provide monetary support if you're not willing to put in effort and click a few buttons to actually do it. - My priority for supporting Ammonite is Myself, Paying Supporters, Contributors, Friends then Users. As I expected, the people complaining the loudest are the ones who aren't using Ammonite at all, so please don't feel offended if I don't prioritize your opinions over others
You're correct, but that's not at all the point of the conversation. Responding to users who say "I wish that this wasn't a feature" by responding with "It's a shame that you don't want me to eat or have an apartment" is utterly inappropriate and makes me think twice about supporting the author. On top of that, he's made contradictory statements about what he's doing with the invisibly and opaquely collected data - is he using it exclusively personally to improve the product, or is he selling it? You cannot expect users to react positively to somebody who essentially berates them for bringing up a concern, states that criticism must come from people with ill intentions, and also makes almost no effort to provide a channel for customers to support him despite the fact that he's said he wants them to. It can be summarized thusly - "How dare you attempt to rob me of a living by bringing up a concern about my product. Yes, I collect data on your usage of it, but it's only to make the product better. I wouldn't have to collect the data that I'm not selling to a third party if you would just pay me directly. The only way to pay me is through a cumbersome and annoying recurring donation system - I don't want to set up a more traditional payment mechanism. Your payments, by the way, don't actually entitle you to anything. I don't sell licenses or even have a way to recognize that an individual person has paid me. Oh, and that payment doesn't mean my capture of your private data ends - it will end at some indeterminate point in the future when I've judged that enough people have submitted enough Danegeld for me to disable the user hostile features." That may not be what he *thinks* he's saying, but it's how nearly everybody who reads it interprets it as.
We really should start thinking about developer ethics. Anonymous statistics might be acceptable as opt-in, but it should not be acceptable to "silently" add it to an already used product. Maybe it would be even acceptable to enable it as default, but only if its enabled state would be clearly visible for its user.
&gt; Responding to users who say "I wish that this wasn't a feature" by responding with "It's a shame that you don't want me to eat or have an apartment" is utterly inappropriate and makes me think twice about supporting the author. On top of that, he's made contradictory statements about what he's doing with the invisibly and opaquely collected data - is he using it exclusively personally to improve the product, or is he selling it? No its not. He is not getting paid for his work, its taking a lot of his time and as he stated, he added the logging feature to actually know if people are using his product and what features in the product (so that he knows if its worth supporting or not). Then people such as yourself grandstand, so its a perfectly appropriate response &gt; You cannot expect users to react positively to somebody who essentially berates them for bringing up a concern, states that criticism must come from people with ill intentions, and also makes almost no effort to provide a channel for customers to support him despite the fact that he's said he wants them to. If they are getting the product for free, they have no right to complain (doesn't stop them from doing so however). I am not paying for Googles services, and I know they collect all sought of information tracking (and sometimes too much as well!). However I accept this, because I don't pay anything to use their products, and they are incredibly useful. People complain all the time. doesn't mean their complaints are valid. We have had people complain (and even worse sue) for completely idiotic reasons.
I found the "I want to know that someone is still using it before I maintain it forever for free" argument in one of the comments above pretty reasonable. 
&gt; If the tracking is there to measure support for his product, why does it need to use a third party system? Because its a lot less work, which means its a lot less time. Do you honestly want him to recreate the equivalent of Google Analytics? &gt; The stated point of tracking is to determine if it's even worthwhile to maintain the project. If the author needs external validation to maintain a product, it would stand to reason that it's no longer motivated by personal passion, but rather by a desire to sell it for money. Or it could be a number of other reasons, such as "I don't have a lot of time" or "I am changing jobs" or any number of other reasons. &gt; The stated point of tracking is to determine if it's even worthwhile to maintain the project. If the author needs external validation to maintain a product, it would stand to reason that it's no longer motivated by personal passion, but rather by a desire to sell it for money. And this is wrong, by far the best way to determine popularity (i.e. how much a product is being used) is to have user statistics on, by default, because (almost) every user is going to send stats, so he will know exactly how popular it is. Statistics on how often people buy your product tell you how valuable the product is, not how popular it is, they are two different things. &gt; His reaction to feedback is fundamentally hostile and underhanded. Asserting that people who have problems with a feature are motivated by a desire to see you deprived of the ability to make a living seems somewhat suspect to me. You can disable the feature, and the project is open source (so you can even fork it if you want, although if you do I suspect that basically almost no one would bother to use the forked version if it just turns off this feature). Also you are throwing around a lot of assumptions which are baseless, you can suspect as much as you want, but at the end of the day, they are suspicions BTW, people do this stuff all the time (i.e. abandon projects because of time/money, usually because it takes too much time and they don't get money for it) and the reasons are often exactly the same as lihoyai's. The difference is that he is being honest and open about it, as well as giving the project a chance to live on
I haven't verified it myself, but in this thread there are mentions of a Patreon account. Edit: https://www.reddit.com/r/scala/comments/6irnix/comment/dj8mpvw?st=J48HU56U&amp;sh=d8672ee8
&gt; To be honest, I was happy to get away from Play. Could you elaborate on why that is?
The logging would be useless if it would be opt-in. As he stated, he is trying to figure out how popular Ammonite is (and some of its features) to see if its worth maintaining. Any kind of statistics from opt-in logs would be completely meaningless/deceiving in this regard
We are talking in circles, I will try one more time Customer: "Gee, I don't like opaque anonymous tracking built into a REPL or a shell. Can you disable it by default?" Author: "I guess you begrudge me for attempting to make a living, all you want to do is complain." Customer: "Well, gosh - actually, I think you should get paid for working on this. Are you selling the tracking data?" Author: "Absolutely not! I just need to see if it's worthwhile to keep maintaining it." Customer: "Well, I guess that makes sense, I'd really just prefer it if the tracking was disabled. If you set up a non-recurring, non-inconvenient way to directly purchase your product, I'd be happy to pay." Author: "People are just mobbing on and resenting my attempts to keep food on the table and a roof over my head." Customer: "Well, that's not really what I was going for - I guess it just doesn't add up to me. You say you need money, but you're not selling the tracking info. Also, I'd be happy to pay you if you made it a possibility, but you're interpreting that as an attack. Good luck to you, I'm going to look elsewhere." Author: "People are always just looking for a way to bring you down."
As an occasional open-source maintainer myself, I wouldn't find it convincing even if it had been his consistent position from the start. Yes, it would be nice to know how much my projects were being used, but non-visible tracking by default would be an unreasonable thing to impose on my users no matter how valuable it is to me.
It's a lot more aboveboard, and frankly more lucrative, to judge popularity by charging for something than introducing opt-out google analytics, especially if the point is to determine if something is worth maintaining. 
Serious question (i think amount of ur work is impressive) - is it really necessary basically to roll your own scala dialect? Is scala not expressive enough?
Because of people like you, cool people stop doing cool stuff. 
Scala dialect? Not really, I always try my most effort to reuse existing infrastructure in Scala community. Unlike scalaz, cats or shapeless, libraries created by me are all very tiny, and extremely modularized. For example, this RAII.scala project consists of four libraries. Each library provides only one monad transformer or one monadic data type. Also these libraries are optional. You choose the syntax, either `for`-comprehension, or higher kinded functions, or even ThoughtWorks Each, as you wish. Unfortunately some of libraries created by me still depend on the heavy-weight libraries. It's not my fault. I suggested the one-source-file-per-library-policy https://github.com/milessabin/shapeless/issues/662 , though other libraries have not been modularized at the moment.
"cause the author seems unstable" He's as stable as anyone I've encountered in the Scala world. I'm not willing to brand him erratic/malicious over this. Valid points in your comment otherwise!
In the end, it's all a matter of opinion, but to me you come off as a bit entitled. He's simply not your subordinate. 
You're right, unstable has a way more negative connotation than i thought of when posting it.
I fail to see why this is really an issue. This was posted on Reddit, which most *definitely* is tracking your every move. I certainly don't give a flying fuck if my `val x = "123"` is sent to Google Analytics. 
Much ado about nothing. The author - Provides us his (non-trivial) work for free - Tells us there is a tracking feature, what is does, and a good reason why it is there - Provides a 19-character flag for users who don't want it And still some find the time to complain about it akin to a betrayal of trust (despite the author not hiding the fact). Point is, monetary support is an alternative to the tracking feature. That's the reason why it exists in the first place -- to gauge the interest and potential of the project. Why clamor about having to "opt-out" of something when you already have the "opt-in" choice of installing the program, or using the flag in the first place?
This^ ! /u/lihaoyi I think you've presented plenty of justification for why you're collecting the stats, and this is such an easy solution to the "sneakiness" that's upset so many people here. This is one of my favorite Scala projects, and I would hate for people to sour on it when a single line of output upon startup could satisfy the worst of their concerns.
I wonder how many of you all deleting Ammonite are Homebrew users.
Wow, it's really true the people lose their shit when privacy is concerned, where it doesn't really matter if you think about it. 1. It won't upload any sensitive info about you, and the fact that the project is open-sourced gives you/community the power to prove/correct its wrong-doing if there's any 2. You can turn it off 3. It's a free software So I don't really see the reason to panic.
I was happy to get away for a number of reasons: * Upgrades have been painful * Not a fan of the routes files * IntelliJ getting lost in ReverseRoutes and other precompiled parts (really hurts clickthough and searches) * I want to explore the functional side of Scala and it's easier to pickup FP concepts when using a functional framework
Closer than I expected at first, but there is no threat.
It's definitely useful in the sense that it shows a clear lower bound of active users.
I've asked about the ETA in this ticket and they removed the 2.3.0 milestone, so the official release for 2.12 is probably not coming anytime soon.
I don't use Clojure, but there is an optional typing project: https://github.com/clojure/core.typed
&gt; Akka http has a routing DSL. How is this different from Play? Play uses a config file for routing. This means its routing config has its own syntax and the way you refactor routes is different, and it's not as easy to pull out a piece of common routing logic and avoid repeating it. E.g. under Spray (the predecessor of akka-http) I wrote a custom authorization directive in a couple of lines by using `for`/`yield` on a couple of existing directives (all the existing directives are just Scala code so you can click straight through to their definitions and read them), and then my custom directive was immediately a first-class directive that I could use in all my code. On the positive side a config file probably means better error messages when you make mistakes, and in theory ought to make the very simple common cases simpler. &gt; Since play ws is a separate independent component, is it more preferable to use play ws instead of akka http? I don't understand how these clauses relate to each other? &gt; This is speculation, but is play ws easier to learn when compared to akka http since it is a higher level abstraction on top of akka http. In theory it should be, but only if the abstraction is good. I find play is usually more confusing than helpful - in trying to "simplify" they end up obscuring important differences.
Just how really bad is blocking code in Play's main dispatcher. And even akka's dispatcher? I mean ,like making a sync db request, or http requests to other APIs? And I mean truly blocking, io-related one. I see it too much in too many codebases, and I wonder, just how drastically it hurts the app?
That rather depends how long it blocks for, and on how many threads at once. Reading from a local disk is ^actually ^totally ^fine in almost all use cases, because we're talking microseconds or single-digit milliseconds at worst. Making an indexed query to a database in the local datacenter is ^probably ^totally ^fine - again milliseconds at worst, and if your database is unresponsive then your site is probably entirely down in practice anyway. Even making a call over the internet to your client's API could be fine if they're always going to respond or fail quickly or if you'll only ever make one call to them at a time, but if you're calling them in a way that has a potential to use all threads you've now made all their latency spikes your latency spikes. Which still might be fine, depending on your business circumstances.
&gt; The documentation is pitched at the level of someone building a web app, not someone integrating HTTP into an Akka project This is important for me. Because I am not dealing with HTTP at a basic level and just REST APIs. Thanks for your input.
Sounds like you want a monad transformer e.g. `EitherT`, so that you can use the either effect to handle the early return. At the most basic level you can do something like: (for { user &lt;- EitherT.right[Future, Response, Option[User]](findUser(userId)) address &lt;- user match { case Some(user) =&gt; EitherT.right[...](findAddress(user.addressId)) case None =&gt; Left(NotFound("No user with given id")) } city &lt;- address match ... } yield Ok).run.map(_.merge) Obviously you could pimp on a helper method for turning `Future[Option[...]]` into `EitherT[Future, Response, ...]`, but it's probably better to have `findUser` etc. return the `EitherT` (or maybe have them return `OptionT` and pimp on a `toRight`-like method - I'm amazed Cats doesn't have this already). (The fancy sophisticated thing here is final tagless encoding but I don't think you need that yet)
thanks for the library reference. Yes his answer looks like covering error case as well
thanks i will try this as well, but as i understand it allows me to return the object with the same type as the option's value, not like a Result or custom ApiError class, is that right?
You can use map on FutureOption and change the type of the value. 
https://www.youtube.com/watch?v=hGMndafDcc8 here is a talk with a very practical solution, I use it myself like this: https://github.com/Daxten/bay-scalajs.g8#structure Source: https://github.com/Daxten/bay-scalajs.g8/blob/master/server/app/controllers/ExtendedController.scala 
It depends on what you want to do. Jetty is a traditional Servlet engine, and while it can do asynchronous operations, it's default simple mode is to do thread per request. If you're wanting to practice Reactive principles, you'll want to use play or akka http. Both are not the thread per request model, and will help you learn to write reactive software.
I don't know if you refer to [this](https://www.playframework.com/documentation/2.5.x/ScalaWS) but this play-ws lib is only an http client library. You can use Play to build REST APIs but it will use the whole Play stack. Play adds more features over Akka Http, like route config file, easy configuration, templates, JSON support, i18N etc. Even if Akka is a bit lighter I still use Play to write Json APIs as it's still quite light, and really developer friendly. 
Akka HTTP is way, way better than Jetty for performance.
isnt this executes all the tasks even if any of them will be a failure and always give the last error reason?
Wrote about this here: https://tersesystems.com/2014/07/10/composing-dependent-futures/
&gt; I see play as just a wrapper around lower level akka http routines. That's about right. There's a bunch of packaging together with different components on top of that, which means application lifecycle management and dependency injection so you have a "hot reload" experience of your application when you change code.
You definitely want OptionT from Scalaz: https://www.47deg.com/blog/fp-for-the-average-joe-part-2-scalaz-monad-transformers/
&gt;Play uses a config file for routing. This means its routing config has its own syntax and the way you refactor routes is different, and it's not as easy to pull out a piece of common routing logic and avoid repeating it. E.g. under Spray (the predecessor of akka-http) I wrote a custom authorization directive in a couple of lines by using for/yield on a couple of existing directives (all the existing directives are just Scala code so you can click straight through to their definitions and read them), and then my custom directive was immediately a first-class directive that I could use in all my code. &gt; On the positive side a config file probably means better error messages when you make mistakes, and in theory ought to make the very simple common cases simpler. The benefits of Akka-Http's DSL is flexibility like you said. The config based is great for simplicity. It's easy for to know what URL pattern goes to which controller. With akka-http, you would have to follow the routes. Also, the way you order the directives can cause unintended errors. That said, for flexibility, Play also has its own DSL for constructing routes. 
What you wrote is accurate. I would choose Play. It would make things smoother IMO. We began using spray (what akka-http was based off of), and we transitioned to Play due to certain quirks of spray and found Play easier to work with. It helped when working with guys transitioning from Java. We are not creating web applications either. We are developing REST APIs. 
core.typed is unmaintaned and the community doesn't care about it anymore - except when an outsider brings up the drawbacks of dynamic typing.
this library finds place on some of the blogs and videos suggested here as well, definetely will give it a try. Thanks
looks very detailed, i am diving. Thanks
Well, I had a look at some Spark source code a while back. It looked pretty bad, to be honest. I wouldn't be surprised if the Scala 2.12 block has to do with chaotic code base produced by people who think they can literally translate Java to Scala.
Another option is http://monadless.io/. It supports more constructs and is generic to any monad.
Finally?
Awesome, let's hope 3.0 release lands somewhere near that of Scala 2.13/2.14 (since 9 months post-Scala 2.12 release is way off the mark, pretty frustrating as a Play user since 2.0).
Can't they backport play 2.5 to be scala2_12? What's the problem ? edit: I'm getting downvoted? I didnt mean to be rude, I just wonder what are technical difficulties, since they're scala 2_11 and 2_12 is pretty much source code compatible I thought...
While I love Scala for its flexibility when I am doing things myself, it is in my opinion the biggest drawback when working on it as part of a team. I have a codebase I work with fairly frequently and it is a mish-mash of what might as well be java, almost zealous functional approach and a bunch of things that lie in between. It's a pain to work on. Scala seems to have developed over time as something that means different things to different people. Kotlin has it beat on the Java-but-better thing imo and I'd rather Scala focused on being THE functional jvm language. There are doubt people who disagree with me though and that's kind of what's great about Scala but also what holds it back. 
&gt; I'd rather Scala focused on being THE functional jvm language. There are doubt people who disagree with me though and that's kind of what's great about Scala but also what holds it back. Except that Scala is not THE functional language (assuming you are talking about Pure Functional Programming). Its not even the design of the language. Scala has closer design roots to ML and Caml, which are both hybrid OOP/Functional languages. If you are looking for "the functional programming language" on the JVM, its either Eta or Frege, although good luck with performance/stack overflow due to JVM not having full TCO (either that or you have to give up JVM interopt, in which case there is little point in running on the JVM)
Thanks! JOL is new to me and have started defaulting to Arrays where possible.
Is it possible to make latest Typelevel Scala work with Scala.JS? Following the instruction I get a dependency resolution error, probably due to new TLS version number structure. org.scala-js#scalajs-compiler_2.12.2-bin-typelevel-4;0.6.17: not found **EDIT:** nvm, found fix [here](https://github.com/scala-js/scala-js/pull/2954) // Remove the dependency on the scalajs-compiler libraryDependencies := libraryDependencies.value.filterNot(_.name == "scalajs-compiler") // And add a custom one addCompilerPlugin("org.scala-js" % "scalajs-compiler" % scalaJSVersion cross CrossVersion.patch) 
Yes! Finally Scala 2.12! But it's gonna be hard work migrating. Especially since they're pushing `@Inject ()` in their Controllers these days. We have a pretty huge project where most of our controllers are `object`, which our test code also takes advantage of. That's gonna require a lot of rewrite. Also this execution context thing is chaos for us. I think we would be better of with `cats.effects.IO` or `monix.Task` and not have to worry about ExecutionContexts everywhere.
I think the Scala philosophy is to use the right abstraction for the task at hand. Clearly, the demand for the user of the compiler is to maximise performance, whereas for the authors of the compiler it is also to keep it maintainable. So carrying around that context is perhaps the best choice to satisfy these two goals. The `fun` signature reminds me of an STM-based system I'm developing, where it would be `def fun(arg1, arg2, ...)(implicit tx: Txn): Result`, so syntactically very similar, although in my case state is still encapsulated with the objects as transactional references. That's also not very "fp" in the sense that return type is often `Unit`, but it safely manages state, too.
What about functional and reactive domain modeling. This brings together Domain driven design, fp, reactive (message / event driven). 
Is there any tooling that you use / that exist that would allow you to document your akka architecture? Something to assist in seeing which actor speaks to which, etc... something that would give u capability to draw things like http://derekwyatt.org/images/Streams.png or like http://68.media.tumblr.com/tumblr_m9b6ytNf6J1r4vwx1.png What do you use to document these? Is there anything that would give you "compile time safety" in regards to these? That if you change your actors you invalidate your diagrams and need to rewrite them, or something like that? It seems to be it would be kind of useful.
It's like you can't comprehend how people can have more than one motivation for something in life.
Does the flag allow one to express the same level of self-righteous indignation on the Internet?
&gt; Not so easily. rm requires you to opt-in to dangerous behavior with the -rf flags. Right, but if you actually think about my point instead of being pedantically obtuse, it's obvious that my point still stands: it's easy to misuse things when you don't understand how they work. So complaining that there's a contrived scenario where things could go wrong is just silly, because similar things could happen with almost any software/environment you don't understand. &gt; I agree.. but why do you think I would use software that I don't understand? Because that's exactly the scenario that you are concocting here. &gt; Being inflammatory was not my intention. I genuinely believe what I typed. It's clearly inflammatory and untrue, regardless of whatever your real intention was. You are free to make up whatever excuses you like, but it was still not a threat. 
&gt; It's an interesting question whether we should split up contexts to make dependencies more finegrained. We currently do not do it because it would increase boilerplate. It's bad enough to have to pass around one implicit everywhere, but passing up to (say) 20 different dependencies becomes unmanageable. With implicit function types that overhead would become more easy to deal with. But the other reason against splitting contexts is that it would increase parameter lists which would negatively affect performance. Could you make methods depend on specific traits that make up the `Context`, rather than the `Context` themselves? e.g. if we have a trait Context extends Foo with Bar with Qux Some methods could take `implicit ctx: Foo with Bar`, others just `implicit ctx: Foo`, others `implicit ctx: Bar with Qux`. That would - Ensure that a method "only depends on" the parts it truly needs; assuming people don't down-cast things, there's no way to access stuff outside the narrowed type of the implicit param. - This provides all the benefits fine-grained dependencies have: e.g. if you want a mock for testing, you just need to create a new `Foo with Bar` with the stuff your method needs, rather than an entire `Context` containing things the method doesn't care about - Avoid boilerplate of multiple implicit args; common combinations could be given type aliases e.g. `type BQ = Bar with Qux` and methods could start taking `implicit ctx: BQ` - Avoid runtime cost of passing lots of functions, since we're still passing the same `Context` object around everywhere, just assigned to different static types Not to say implicit function types aren't a good/bad idea, just that this scheme seems like it would satisfy your boilerplate/perf/fine-grained-ness requirements without needing any language changes at all
The Play team doesn't control when new Scala versions come out, and the major releases have been on a yearly basis for quite a while now. * 2.6.0 June 2017 * 2.5.0 March 2016 * 2.4.0 May 2015 * 2.3.0 May 2014 * 2.2.0 September 2013 Edit: Here's [the roadmap doc](https://docs.google.com/document/d/11sVi1-REAIDFVHvwBrfRt1uXkBzROHQYgmcZNGJtDnA/edit#) showing the ETA for Play 3.0 and the feature list, discussed on the [dev mailing list](https://groups.google.com/forum/#!topic/play-framework-dev/ny5CBvsEtzA). What some companies have done when things are a priority is to sponsor development in a particular area, i.e. Linkedin's sponsoring of cached resolution: https://www.lightbend.com/blog/improved-dependency-management-with-sbt-0137 If this is something your company can do, I'm sure Lightbend would be happy to chat. 
&gt; People should be able to make critiques about an open source product without being told that they have to implement the changes themselves There's no insinuation here -- I think it's disingenuous to tell people that money, people and time are not limiting factors to getting things done, because they totally are.
This was nice, migration took about an 1.5hr for a medium sized app! The main hiccup was getting rid of my use of GlobalSettings and making it into a Controller, setting it as a singleton, and eagerly launching it at startup. Otherwise, I made all controllers extend BaseController instead of Controller, added a ControllerComponents, altered dependencies a little bit, and that was that! Thanks!
&gt; Especially since they're pushing @Inject () in their Controllers these days. Specifically, since 2.4 (released over 2 years ago); certainly not their fault the deprecation warnings have gone ignored. And yes, the migration will be a lot of work, been there, done that. 
I read the docs for the libs I'm directly using. But I don't deploy Ammonite, and it wouldn't be sending info I really cared about anyway, and if I did care I'd probably firewall or modify it to prevent that. Here's a fantastic example: some web libraries use a 1x1 transparent pixel for the same purpose. They also are open about it, and I also bet most people either don't care about that, or replace the pixel/url like the instructions say.
Thanks for response. Unfortunately for us it's very hard to migrate. We just migrated from 2.4 to 2.5, while still depending on LOTS of deprecated stuff (which I assume is dropped in 2.6, like crypto). It still cost us lots of money to upgrade.
Sorry to be snarky, but how many days have you gone without food? If the answer is zero, can you make a list of all the open source products you have used all your life, including the scala compiler, IDEs etc and the $ amounts you have personally contributed to each product? If you are not actually starving, all of this posturing is unintentionally hilarious. Feast on your stock options earnings from Dropbox, sure you earned it ; but this whole starving artist act is incredulous. 
&gt; If the answer is zero, can you make a list of all the open source products you have used all your life, including the scala compiler, IDEs etc and the $ amounts you have personally contributed to each product? Scala Compiler: I spent about 10,000$US on travel and other expenditure promoting the Scala.js backend. Also probably spent about 1,500$US over a few years hosting scalafiddle.io, which is used by thousands of people around the world IDEs: probably at least 1,000$US on IntelliJ so far Those are all out of pocket. How much have you spent contributing to the community? What I haven't done, is asked someone else to make a change in their open source project I have never used, refused to pay any money, and then started a flame war with lies/false-accusations/alternative-facts on Reddit when I didn't get what I wanted. Or suggested that it is "incredulous" for someone to ask for pay in exchange for work, because they had a job some time in the past.
&gt; Here's a fantastic example: some web libraries use a 1x1 transparent pixel for the same purpose. Wow, that's crazy, thanks.
Yeah I know, I guess 2 years is what I meant by "these days" :P. Thing is though, I still don't see any motivation at all for doing DI "the play way". Really , if I want to mock a dependency, I just pass it as a parameter wherever I need that. I didn't want to mess about with Guice ApplicationBuilder binding stuff - that seem overly complicated and I feel we're making hard compromises for Java-compability's sake. *Runtime* dependency injection? No way! :p I guess now that play is more neutral about Guice vs other things (like macwire (much better!)), we'll have to reconsider. I really want to be on 2.12 after all. I just wish I could keep using the "static routing" config from 2.5 that allowed us the keep using "object controller"
&gt; How much have you spent contributing to the community? I will give you some fodder to beat me up. I have some open source contributions and in terms of $ contributed absolutely nothing, unless you want to count contributions to the EFF/ACLU/github/dropbox subscriptions - a company that supports open source programmers :). These are the negatives. However, on the plus side, I have never pretended to be a starving artist. I have never gone without food any day of my life and always had a roof over my head, true story. I also know how to communicate online without coming across like a dick, when using my real identity :) Bottom line is, no matter how much you have contributed to an open source community, you are always withdrawing more than you have deposited. You are always standing on the shoulder of giants. Its great that you are trying to make more money than you already have. But no matter how much you twist words to pretend that people don't want you to make money, the real complaint is that users don't expect a command line tool to connect to a remote server and if it does so it must display a prominent message on launch. Also, while the starving artist act might have felt like an awesome idea when you started your argument it is the weakest part of your online rhetoric, if you can call it that. 
&gt; But if that's the case logging is not to understand usage and improve the program, it's for monetization (which i highly doubt it actually is, i never heard of a relatively low profile program like ammonite monetizing using google analytics). My wild guess, considering lihaoyis status as a $ millionaire thanks to Dropbox. He is possibly seeking funding from investors to launch a startup, possibly based on his most successful open source product ammonite. The investors might want to see usage numbers before they commit. A regular job would be the last option a $ millionaire is looking for. If any investors are reading this comment, go ahead and invest in this guy - he will probably produce something great. But yeah, teach him how to communicate online without coming across like a dick.
Add will happen on "that" in this example. The "this" refers to the object the method is defined on.
What do you mean by cheap? Have you had a problem before using RuntimeException alone? What's your motivation for using NoStackTrace. I have an understanding, but curious of yours.
I recommend using the IntelliJ IDE with the scala plugin. They have a nice getting started page [here](https://www.jetbrains.com/help/idea/creating-and-running-your-scala-application.html) 
&gt; And frankly your insinuation sucks. People should be able to make critiques about an open source product without being told that they have to implement the changes themselves. well if you can make critiques about open source, why shouldn't he can critize you for not helping out? P.S.: I helped with getting play to 2.12 and I'm not a Lightbend employee. 
Dumb question, but would it be worth it to use the Play framework to build small web applications? Would the overhead of this framework slow down smaller web apps?
Scala is a pretty complicated language... You should try the course offered by EPFL on coursera. It's taught by Martin Odersky the creator of scala and is pretty good. There is a follow up course as well. It will still take about 6mos to a year to get to the point of writing a large scala program and maybe a little longer to learn best practices so that your code is of a professional standard. That being said it is worth it. They have a lot of cool language features (more than in many others and probably more than you need) which if you learn will translate to other modern languages Scala is its own language but it runs on the Java VM and is interoperable with Java and other JVM languages. Kotlin is another JVM language with some modern features but now as crazy as scala. However I think mathematicians will probably like scala.
Can you elaborate a bit on what concretely is the security concern? Is it just a vague feeling? Do you avoid websites that have google analytics? (As you can see from the source code, all it's doing is sending 'Boot' and 'Action' "page views" to GA.) 
Don't know anything about it but I assume https://github.com/densh/scala-offheap can help?
I am also learning Python. I also know how hard scala is, I've worked with Java before and I'm still struggling. But I want to get into analytics and operations research as a career, and data science is the only job I can get now, as a chemical engineering graduate. I'm making this jump, so I gotta struggle through this, haha. As a future project I also plan to learn h2o, the deep learning software. Most such companies will only hire me if I know R and Scala at the very least, and have a knowledge of statistics and mathematics, which I do, to a good degree.
thanks for the advice, I was looking for some MOOCs, but I wasn't sure which one was right for me, I will check that one out. For learning the basics of R, I used swirl, which really helped. Anything like that which can help me with the basics of Scala?
I am doing this, I have been using the book "Scala in Action"
I don't know if there is anything like swirl in Scala. However, the course is really good assuming you do the HW problems. With regards to a good IDE with language support as per the previous commenter, IntelliJ is a really great product. Eclipse does also have a good scala plugin which is what the course uses.
In a normal program this would be good advice. But compilers are not normal programs in that they are recursive to the extreme. Everything might call everything else In some scenario and therefore everything might depend on everything. So in the end I believe the scope of restricting contexts to parent traits would be very limited. But that's specific to (Scala?) compilers, I am sure for most other programs it would be a good idea.
Scala is not the first box you need to check. R, Python, And SQL are. I do this for a living and didnt touch Scala until several years in when I was on hadoop projects. It is not a good first language to learn.
okay, I was also suggested SQL, so I shall do that then.
If you skip Akka, ScalaZ, Shapeless and libraries like those initially, then Scala is not more complicated than Kotlin, or even Python. If you just learn map, filter and other useful string and list manipulation functions, that's pretty good for data analytics I guess, however these are also quite similar in Kotlin, except it doesn't have something like the ammonite repl(I might be wrong here, I didn't check), which can be nicer than the basic one. I can't disagree with learning python which has a lot of useful data science libraries. Not sure how javascript would help though. Maybe if you want to make web page based visualisations. Anyway whichever language you choose OP, I can recommend doing some puzzles on pages like codingame, because you can see how other people solved the same problem and you can learn some interesting things from that and it's also good if you don't have any specific idea what to develop, so you can get familiar with the syntax and basic libraries. 
This is cool!
I wouldn't expect Dotty to be an example of good Scala design. Odersky has a... particular perspective on certain tradeoffs that I don't agree with. My experience is that what you call Haskell-style leads to both more understandable code and higher performance in practical-sized apps. A lot of inelegant design in scalac/dotty is defended on grounds of "performance", but the end result is probably the slowest mainstream compiler going, so I take that with a large grain of salt.
Do you know open source Scala applications following that design? Not libraries but HTTP APIs with database backend, web apps, GUIs and so on.
This is so awesome, it makes using scala.meta so much easier.
Your code example doesn't seem to include the important parts - it would be clearer to be able to see both objects and maybe have the classes. But within a call to `that.addANumber(5)`, `this` will be `that` - `this` is always the method receiver i.e. the value that the current method was invoked on. (It's the same way as any other language, except JavaScript which does something totally bizarre and crazy).
What benefits "haskell style" would give to dotty? There is very little IO in dotty and IMHO it is obvious that "haskell style" is not performant for algorithmic code.
Pretty wild, and potentially very useful. Thanks.
Thanks. Interesting!
I recently had an insane number of `java.lang.Long`s in memory originating from Slick's query builder. These were generated from queries using `inSetBind` with a hefty `Seq[Long]`. Many examples I see of both table mapping and queries use lengthy tuples and pattern matching as well. Am I wrong in thinking Slick is doing nothing to avoid boxing in many cases? Is there anything I can do to avoid it, e.g. specialization? In some cases, I wish I could just use a case class directly, but it doesn't always seem possible. For example, I'm not sure how to run the below query without the intermediary tuple. for { a &lt;- addresses u &lt;- address.user } yield (u.id, u.name, a.value) .mapTo[UserWithAddress]
I don't really know slick, but I'd look at what the `map`/`flatMap`/`mapTo` are doing underneath. You might be able to provide your own specialized typeclass instance for how to combine `User` and `Address` that would take priority over the default of using a tuple, or something.
I'm not aware of any official documentation/rationale. It's [widely acknowledged as a wart](http://www.lihaoyi.com/post/WartsoftheScalaProgrammingLanguage.html#conflating-total-destructuring-with-partial-pattern-matching). [The typelevel fork of the compiler has a flag to disable it](https://github.com/typelevel/scala/pull/1).
I wouldn't recommend Javascript - too many weird/inconsistent things about it - or Kotlin. Python is a good starting point just for ease of libraries; I'd suggest Standard ML is probably the best learning experience on the typed side.
Both Javascript and Python are fine languages for beginners. Python is a lot more of a mish mash of concepts, though, mixing imperative, declarative, functional and oddities like the `self` variable and the fact that functions with n parameters need to be invoked with n-1 parameters. Javascript is quirky but more consistent than Python from a design standpoint. Also, the fact that it's tied to the browser and as such, present with a superb dev environment on pretty much any computer on the planet is a huge plus. And of course, showing beginners how they can manipulate the DOM and modify web pages in front of their very eyes is very powerful. 
&gt; mixing imperative, declarative, functional I see it as more that Python doesn't tie you to any one. In any case, the Javascript ecosystem does all of those and also forces you to deal with "prototypal" which the programming world has rightly abandoned. &gt; self I find `self` clarifies what's going on a lot - if anything it's a better way for beginners to learn OO. Conversely Javascript's `this` is just awful, actively harmful to understanding anything. It's possibly the last major language with non-lexical scope. &gt; the fact that it's tied to the browser and as such, present with a superb dev environment on pretty much any computer on the planet is a huge plus. A raw browser isn't a good development environment - to do serious work in JS takes just as much setting up as any other language. &gt; showing beginners how they can manipulate the DOM and modify web pages in front of their very eyes is very powerful. Agreed, and that counts for a lot - maybe even enough to outweigh the language's deficiencies. Considered as a language it really is bad though.
I recommend Jupyter Scala, considering your statistics/data background.
I recently spent some time solving this problem and wrote two blog posts about using Scalaz, Cats and Hamsters (with some demos using Scala Fiddle so you can try it out on the page) [Future Either with Cats](http://justinhj.github.io/2017/06/18/future-either-with-cats.html) and [Future Either with scalaz and Hamsters](http://justinhj.github.io/2017/06/02/future-either-and-monad-transformers.html) My goal was to return a type: Future[Either[YourErrorType, YourResultType] to the user which enables you to return programming or errors in your business logic in the Either left side, a correct result in the right side, and then you use the exception handling of Future to handle errors. There are examples of each kind of success and failure in the post. Hope this helps! 
Better readability? Well, you just leave out important information, so you have less to read but what you read is incomplete (and thus wrong). Don't use `Future.failed` for something like that.
That's why I expected a call to map in both cases. But the collection I used has printlns in the map and foreach method. The first listing prints map, the second one prints foreach.
Thank you so much! 
You're right, my mistake. 
Can you give a working example? Are you sure you didn't miss the `yield`? `for` without `yield` desugars to `foreach`.
The following code prints foreach map for me: import scala.collection.generic.CanBuildFrom case class Item(n: Int) class Collection[A](private val elements: Seq[A]) extends Seq[A] { override def map[B, That](f: (A) =&gt; B)( implicit bf: CanBuildFrom[Seq[A], B, That]): That = { println("map") elements.map(f) } override def foreach[U](f: (A) =&gt; U): Unit = { println("foreach") elements.foreach(f) } override def length: Int = elements.length override def apply(idx: Int): A = elements.apply(idx) override def iterator: Iterator[A] = elements.iterator } object Collection extends App { val col = new Collection(Seq(Item(1), Item(2), Item(3))) val res = for { Item(x) &lt;- col } yield { Item(x) } val res2 = for { x &lt;- col } yield { x } } Edit: Interestingly, if I change the class definition to class Collection[A](private val elements: Seq[A]){ def map[B](f: A =&gt; B): Collection[B] = { println("map") new Collection(elements.map(f)) } def foreach(f: A =&gt; Unit): Unit = { println("foreach") elements.foreach(f) } def withFilter(p: A =&gt; Boolean): Collection[A] = { println("withFilter") new Collection(elements.filter(p)) } } map is called for both for-comprehensions. withFilter is also called. The output is then: withFilter map map 
Just a random question, apologies if this isn't the right place. For developers with jobs using Scala, did you know Scala before you started working on it? Did you learn on the job? Did your company adopt the language and you had to learn? I'm about to graduate in December, and I'm mostly a java/javascript developer. However, I've been dabbling with Scala and I really like it.. but it's a pretty dense language with a lot of features, so I was curious how much someones is expected to know for a Scala based job? Of course, I'm not planning on going out for a "Junior Scala" job, but I'm thinking down the road, how to possibly prepare myself. As a junior java developer, its hard for me to justify ever switching to Scala for something, when I feel like I should just implement it in Java to strengthen my java skills. 
In the original question, don't you mean that the first code example results in a call to your custom collection's `foreach`, while the second code example results in a call to `map`, which is the reverse of what is written?
Sub-typing the standard library collections should be done with great care, and the documentation regarding what must be overridden and how should be followed fully. Have you read the documentation for it? I imagine the results you are getting are due to how the default implementation of `withFilter` is written. In the first code piece, you are not simply mapping, but pattern matching; the `Item(x) &lt;- col` is actually a pattern match which will (if I recall and understand correctly) filter out the patterns that do not match. So, if you for instance do this: val xs = Seq(None, None, None, Some(4)) val ys = for { Some(i) &lt;- xs } yield i `ys` will be `List(4)`. Therefore, the first code piece will filter out elements. I think you then in the other comment you made might be making a mistake in regards to implementing `withFilter`, see the [Scaladoc here](http://www.scala-lang.org/api/current/scala/collection/Seq.html). The answer then comes from the default implementation, one implementation is found [here](https://github.com/scala/scala/blob/v2.12.2/src/library/scala/collection/TraversableLike.scala#L1). Note especially the inner class `WithFilter`. But, again, be careful with obeying the interfaces and implementing the API properly, which can be difficult in certain cases. A good practice when making your own classes or traits that might be extended and that are part of your API is to document how they can be extended (unless it is somewhat obvious how they should be extended from just the API documentation of the class/trait and its fields and methods (and maybe type members)). EDIT: See also the [documentation for the standard library collections here](http://docs.scala-lang.org/overviews/collections/introduction).
&gt; But, again, be careful with obeying the interfaces and implementing the API properly, which can be difficult in certain cases. I find the OPs example a good showcase for why inheritance is bad and you can see there how easy it can be to break encapsulation when overriding methods. So, don't document how your API classes can be extended but rather make them final so that they cannot be extended/overwritten. :) There are better ways!
A tip: crypto is a small part of Play so you can copy the 2.5 crypto source into your project if you want to continue using it.
The larger issue is that Crypto.encryptAES is AES-CTR: as long as you're using an HMAC and not reusing IVs you should be fine, but it'd be far preferable to move to libsodium / kalium as demoed here: https://github.com/playframework/play-scala-secure-session-example
I began learning Scala in my spare time about five years ago, while working as a C# developer. While C# was nice I didn't learn nearly as much as I did using Scala. Scala is not just a modern language on the JVM, it's a happy marriage between several distinct concepts like immutability, purely functional architecture, advanced (I dare say one of the most well rounded) OOP-functionality: All optional to some degree. I am much more confident in my Java-code (whenever I write Java these days) since picking up Scala. People often say that Scala is a complex language, but I actually believe that it is the *concepts* that Scala readily puts at your disposal that are complex. Or rather, they are foreign to many, as they were to me. I believe that I am a better developer today than I'd ever be had I nerver ordered Programming in Scala. The learning experience, while frustrating at times, have left me with a greater box of tools at my disposal.
Interesting...had no idea that GWT was still alive and kicking.
"It seems like anything that you can do with Scala, you can do with Java 8 and vice versa.". Cute.
Eh, to be fair, to a java developer flirting with scala this isn't a hard conclusion to draw. Especially if you're using java libraries that bring you immutable objects, collections, monads, that sort of thing. Of course the set of language features scala has over java goes way beyond those libraries and some syntactic sugar. But with a few exceptions like structural pattern matching, a lot of the really nice things in scala take more than a cursory glance (which is all the author has taken) to pick up.
The `java.lang.Long` thing was under relatively extreme circumstances, and I ended up discovering it in a heap dump. I'll see if I can reproduce some boxing scenarios in a simple project. 
Better in terms of type safety and code size. Returning different errors as future.successful will force you to use common supertype for result object and error object (usually it is Any). Also, future is designed to cancel further computation on failure which is the behavior wanted even on recoverable error. Sure, you can use Future[Either[..., ...]] but that is quite verbose to handle. As for recoverability, typical solution is exception hierarchy: catch { case ex: RecoverableException =&gt; //try again, return 500 or rollback JMS transaction case ex: UnrecoverableException =&gt; //fail, return 400 or commit JMS transaction replying that message is invalid case ex: Exception =&gt; //probably bug in application code, return 500, trigger alert, commiting JMS transaction and putting message to deadletter queue }
The build fails, it's not a mystery where the errors are. You still have to resolve the errors, which inevitably requires *doing something*; in the case of Play 2.4, 2.5, and 2.6, the migrations have all entailed significant amounts of work.
&gt; Returning different errors as future.successful will force you to use common supertype for result object and error object (usually it is Any) No. They can and should not use `Any` but a type that describes what the result looks like. E.g. `Option` if something can't be found, or `Either[ErrorType, Result]` if there are multiple errors possible (where `ErrorType` is a sealed trait) or `Validated`/`ValidatedNel` and so on. It can be a little bit more verbose to handle, but with monad transformers it is not as bad.
&gt; though there can be cases where inheritance makes sense Inheritance (and overriding methods) breaks encapsulation, as simple as that. I would like to see an example where you think it makes sense because I can see none. My favourite counterexample is a stack: class Stack[E](...) { def push(elem: E): Unit = ??? def pushAll(elems: List[E]): Unit = ??? def pop(): E = ??? } It is impossible to correctly build a countable stack (that counts how many elements are on the stack) by extend the given stack class and overwriting the methods without knowing their implementations.
Added Kinesis support to the aws-wrap library: https://github.com/mingchuno/aws-wrap/pull/13
For me, practical design in Scala (I won't say functional, because to me Scala design is an object-functional hybrid) is based on a few pillars: * Case classes for immutable data * Case class methods for data transformations attached directly to the data * Typeclasses for general behaviours over various data types * As much data and behaviour as possible inside companion objects--keeping them static simplifies usage and testing * Don't make users import anything more than type names to work with my libraries--that's a design failure; behaviours should automatically be supplied by typeclass instances in companion objects that come with importing the type names.
I just don't like Future[Either[_, _]] for verbosity and for-comprehensions for limited expressiveness. Either is also costly in terms of performance. Definitely, sealed error trait is Good Idea but no Either please. Personally I use solution that may look strange: Tuple2[Error, Result] with nulls. Wrapped to AnyVal monad, it is both efficient and can be unapplied to variable pair: case class Result[Err, Ok](tuple: (Err, Ok)) extends AnyVal { map, flatMap } val (err, ok) = r.tuple if (err != null) return //exit from function eagerly w/o building up nested braces
Do your companion objects call functions from other companion objects? Doing so makes them impossible to stub/mock. 
Thanks, that makes sense.
Thanks for your explanation, I think I understand now. Due to the call to `withFilter`, the subsequent `map` is not invoked on an instance of my class, but rather an instance of that inner WithFilter class. Btw, I am aware that the example code is not how a collection class should be defined. I was just trying to build a MWE as quickly as possible. However, my initial problem actually resulted from trying to subclass some of the collection classes/traits from the standard library. Time to read the docs :-)
You're right. I edited the question.
I'm not sure I understand. Does the following not work? class CountStack[E] extends Stack[E] { private var count = 0 override def push(elem: E): Unit = { count += 1 super.push(elem) } override def pushAll(elem: List[E]): Unit = { count += elem.size super.pushAll(elem) } override def pop(): E = { count -= 1 super.pop() } }
Well, IO in general is difficult to test properly if you don't like mocking and stubbing all over the place--which I certainly don't. My preferred approach is usually a monadic DSL for IO.
using `.get` or `.getUninterruptible` will block your Future. This is actually worse than just doing `.getUninterruptible` and returning the value directly as wrapping a blocking method in Future will actually block two threads — the one carrying the ListenableFuture and one from the ExecutionContext you pass to Future.apply. It should actually be pretty straight forward to convert your ResultSetFuture into a Scala Future without blocking: import com.google.common.util.concurrent._ import scala.concurrent.{Future, Promise} def convertResultSetFuture(rsf: ListenableFuture[ResultSet]): Future[ResultSet] = { val promise = Promise[ResultSet] val converter = new FutureCallback[ResultSet] { def onSuccess(resultSet: ResultSet): Unit = promise.success(resultSet) def onFailure(throwable: Throwable): Unit = promise.failure(throwable) } Futures.addCallback(rsf, converter, MoreExecutors.directExecutor) promise.future } edit: code fixes
Anything you can do with Scala, you can do with machine instructions.
I wouldn't really consider using Scala as "switching" to it. Rather, you're just adding to your resume's skillset.
It's like someone who uses a notepad tried an ipad and they were impressed because it provided a nice flat and reasonably light resting pad for paper and pencil. That's cute, but if they figured out how to switch it on they might get a different perspective. Also, that's the first time I've ever seen anybody recommending scala based on its fast compile times. Which deep circle of hell must you be living in to recommend scala based on how *fast* the compile times are. --------- gosh: thanks for the gold, what a pleasant surprise
That is like saying anything which can be done in scala can also be done in assembly programming. Java 8 is nowhere near scala's elegance.
I think the fact `withFilter` delegates to `foreach` is even more confusing than the fact that there's a `withFilter` in the first place.
It doesn't exist. The author got confused. They answered [in this tweet](https://twitter.com/gwidgets_/status/879612050405810176) that they meant *dynamic typing*. That's even more confusing, though ...
GWT's compilation times are truly hellish. Scala is only awful. By comparison, it's fast :-)
&gt;although good luck with performance/stack overflow due to JVM not having full TCO I thought Eta solved this by implicitly running things in a trampoline?
Must feel pretty good to write a to-javascript-compiler on top of a slow compiler that is faster than the one Google wrote on top of a fast compiler.
I think your instinct is correct to get better at Java first. It's a much more widely used language and a perfectly respectable one. At my job we use Scala for building servers and we usually hire people that have developed an interest in functional programming and can demonstrate some level of that via courses, private projects or through experience at work. After you have a good grasp of Java you should learn about Scala and see if you like it.
You can install IntelliJ + Scala plugin on Windows, yes.
@teknocide Does this have the drawback of creating one additional context switch? Context switch 1: Run the java future callback. Context switch 2: Run the scala promise listeners The wrapper class included (which, honestly, I more or less lifted from somewhere) was intended to reduce the number of context switches.
This is precisely the point which /u/m50d is forgetting. The compiler is already optimized for this case, this is very trivial optimization and is probably one of the only cases where PFP can optimize something competitively compared to non functional programming. Anything else that is non trivial, you basically have to resort to using mutable data structures along with imperative code if you want to max out your performance. DarkDimius recently said in a talk that another reason for using this global context object is for caching because doing a performant cache in a purely functional manner (also taking into account cache eviction and ttl) is incredibly hard (he even joked for the audience to show him an example). Writing compilers is like writing AAA games without a UI, they are very hard because not only do you need very good performance (for differing reasons), but you also need to care about correctness and your problem space is not trivial. So far the best way to manage the extremes of both correctness and performance is to have a functional interface to hand written locally optimized lower level code (i.e. imperative/mutable)
Hmm, how much overhead does trampolining really bring? Especially if you do it optimally, by determining the JVM's maximum stack size and only jumping on the trampoline once you're about to reach it? I know that trampolining can lead to performance improvements for async operations (since it allows you to stay on a single thread instead of scheduling each Runnable separately). I also know that the high performance concurrency library Monix trampolines its Task type. Is it really the case that it kills performance? Are there any JMH benchmarks demonstrating that fact?
Have you seen Kamino? https://kamino.io/
&gt; I just can't wrap my mind why would someone choose to develop their frontend in scala.js. There are some people who are well versed with the whole stack. For moderately sized projects, having fewer engineers and unified tools is a bonus.
Contact info?
Where I work we hire mostly people who don't already know Scala for the back end. I would assume frontend could be similar.
&gt; without requiring a PhD in Computer Science to be used proficiently You really don't need a PhD in computer science to do functional programming. It is no more complicated that learning all the OO patterns, the difficultly happens because it a different way of thinking. If you started with FP, then I'm sure OO would seem weird. 
I knew a bit of Scala before starting, did some personal projects in it etc. But I learned most on job, and I still am learning. I'd recommend go with what you like. If you enjoy Scala, do Scala. If you enjoy Java, Scala is probably not right for you (just kidding, but...). I hated every second of commercial programming before I discovered Scala (or to be more specific, before being introduced to functional programming paradigm)
Challenge acce... ah hell I have better things to do with my time :)
Did anyone finish the article? Me neither. What a loads of bandwagoning bullshits. 
&gt; They aim to solve the same problem: providing a better Java Can't take this seriously. &gt; the managament of null-safety is less efficient and natural than in Kotlin. Can't take this neither seriously
Oh really? That's awesome. That will make migration so much easier for us! Because honestly, we've found a way that works well for us (in term of dependency injection), and thus don't really see much purpose in rewriting to use play-aided DI. That said, I can see that newer projects can benefit from a strong DI pattern recommendation though. But it's just not for us (at this current moment at least).
It is for a business, but I think the use-case is simple enough that it's worth exploring. We're rolling out a UI for some of our analytics and the results are not stellar. The front-end needs the same transformations we're doing in our API layer except that the front-end shouldn't request the same data sliced in multiple ways. If we can stick Scala.js in an iframe with DS3 and our data model layer, it'll be much easier for us to manage evolving that portion of the UI, and the UI only needs to make a single data call. Really simple stuff, we're not the UI team, so no one would want it to grow any larger than that.
I agree with you, yet I would agree with a part of that last quote: "the managament of null-safety is less efficient [...] than in Kotlin." I definitely think Scala is more natural (e.g. options are collections instead of having `List::mapNotNull` in addition to `List::flatMap`) but Scala's approach is less efficient than a compile-time check. Having written a lot of both, I can say that the null-safety and smart casting guarantees become cumbersome any deeper than local variables.
&gt;There is a good paper here http://blog.higher-order.com/assets/trampolines.pdf by Runar Oli Bjarnason. Thanks, but I don't see a discussion of performance in that one. Am I just missing it? &gt;Trampolining is definitely not free or inexpensive due to how the JVM is designed. Oh, it's definitely not free. I'm just trying to get a sense of how expensive it would really be, especially if implemented in the efficient version with the trampoline only bouncing once you're near max stack depth. &gt;When I was talking about killing performance, I am talking about direct function calls (i.e. no async involved). In this case, trampoline does introduce a lot of overhead. Hmm. But for argument's sake, won't most Eta programs already be async? Not sure how `IO` will end up being implemented, but I could totally see the designers having it backed by `ForkJoinPool.commonPool` by default. Or am I confusing two different issues?
&gt; Thanks, but I don't see a discussion of performance in that one. Am I just missing it? It doesn't give specific benchmarks. There were available somewhere on the web, but I can't find where &gt; Oh, it's definitely not free. I'm just trying to get a sense of how expensive it would really be, especially if implemented in the efficient version with the trampoline only bouncing once you're near max stack depth. It it even possible to get actually correctly obtain the correct max stack depth on the JVM (curious here) &gt; Hmm. But for argument's sake, won't most Eta programs already be async? Not sure how IO will end up being implemented, but I could totally see the designers having it backed by ForkJoinPool.commonPool by default. Well if you have a pure algorithm with no IO that uses recursion, it is going to be a direct function call. Afaik, ETA is just an implementation of Haskell on JVM, so there are plenty of scenarios where its just functions calling other functions without IO. 
This call is intended. I could also have set up a timer which calls it every 200ms or so. I think it works (at least it did in my tests, which says not much of course) but even if I made a mistake, that should be easy to correct while still maintaining the async behaviour. It is even not a very "fancy" thing to do. Image a datastructure where `pushAll` takes very long (and blocks the thread) and no one cares about the result for a very long time until some other request to that datastructure is executed. So `pushAll` can just return immediately, the work is done in the background and when the next request comes, the result is ready *or* the request hast to wait (but overall it will not take longer because otherwise `pushAll` would have had just blocked the thread). So you can gain performance without having to touch the rest of the code (which might not even be under your control). Except if your class isn't final because if it isn't, you might break things.
Also, [this nine year old post on CodeRanch](https://coderanch.com/t/408771/java/retrieve-jvm-current-stack-size#1797447) describes how you can check for a manually set stack size, and then fall back to a table of specific JVM's default stack sizes if it's not set OH MY GOD WHAT AM I DOING THIS IS HORRIBLE
Yup, in my Kotlin progs that I either trust my Java usage or I stick strictly to Kotlin, I pass `-Xno-param-assertions`. They take a "no matter what, we promise it's not null" approach for non-interop which can be a bit annoying. For instance (IIRC), you might have `if (foo.bar != null) callFn(foo.bar, 5)` and get an error saying something like "You may have checked the nullness of this field before, but technically it could be modified in another thread between the conditional and its use so you have to find another way" (which usually means assigning to local var, e.g. `foo.bar?.let { callFn(it, 5) }`). It's an annoying way to program and causes undue verbosity IMO. My example doesn't show the burden because it's a simple conditional, but it comes up in larger contexts.
hello /r/scala, i'm still running into a bit of high memory usage problem, and i was thinking about taking some of the classes written in the style of EagerDataCollection and reimplementing them as LazyDataCollection and i'm still wondering whether it will effect my program's memory usage. class EagerDataCollection(allData: List[Data]){ def relevantData: List[Data] = { // here will be some code that filter allData //and returns only relevant data that we want to keep } } class LazyDataCollecion(allData: =&gt; List[Data]){ def relevantData: List[Data] = { // here will be some code that filter allData //and returns only relevant data that we want to keep } } if i'm not mistaken i think that LazyDataCollection will have less memory footprint and my reasoning is, EagerDataCollection will keep a reference to AllData, so that huge list of data will never be garbage collected as long as there is a refrence to an instance of EagerDataCollectio. while the lazy evaluation only holds a refrence to a function object that will return List[Data], and the the reference to the real List[Data] will be inside the relevantData function and after it terminates the garbage collection will be able to reclaim the memory used by the instanes of Data that didn't make it to the returned list of the relevantData function. Thank you in advance! (:
Kotlin is the new toy language: ppl are comparing it against everything without sufficient knowledge about the "targets". Ignorance creates immature assumptions. Immature assumptions create terrible architectural design.
Depends on how the class is instantiated. If the the `List` it's given in the constructor has a reference held outside the class too, then it won't matter if you use a by-name parameter or not. If the `List` is created by `allData`, then you will create a list every time you use `allData`, and each will be eligible for GC. For example: def returnsHugeList: List[Data] = { List.empty } // list sticks around as long as `data` regardless of `LazyDataCollection` val data = returnsHugeList val ldc = new LazyDataCollection(data) // will create a new huge list every place `allData` appears in the implementation // each can be GC'd provided references are not leaked val ldc = new LazyDataCollection(returnsHugeList)
One really, really has to be careful with mutable shared-memory concurrency. On the JVM at least, you can get some really funky behaviour (Java has a proper memory model, the JMM, which describes how to avoid memory issues and which guarantees are given). A basic example is this (taken from Java: Concurrency in Practice, an old but good book, see [3.15: Class at risk of failure if not properly published](http://jcip.net/listings.html)): package net.jcip.examples; /** * Holder * &lt;p/&gt; * Class at risk of failure if not properly published * * @author Brian Goetz and Tim Peierls */ public class Holder { private int n; public Holder(int n) { this.n = n; } public void assertSanity() { if (n != n) throw new AssertionError("This statement is false."); } } The above code can end up throwing the above `AssertionError` if the object is not published properly.
My path to Scala.js started with a desperate need for a better language than JS for frontend dev. Building complex applications with such a limited, dynamically typed language is very expensive in the long run. Once I've tried a few compile-to-JS languages I realized that they all have significant disadvantages in practice. For example, Typescript is just plain unsound in very tangible ways. The disadvantages of Scala.js that I saw were the most ephemeral ones – it is not as popular as Typescript, and requires more skill than Typescript. However, Scala.js is a much stronger foundation to build an application on. Scala is a solid, featureful language, with an ecosystem that is increasingly becoming JVM-free. I do appreciate that it's hard to hire or train devs for Scala.js. I'm personally not in a position to hire anyone just yet, so perhaps my advice is not that valuable. However, I have a very sour taste in my mouth from working with codebases built by people who are easy to hire. The amount of maintenance needed on such code is immense. My personal strategy is to spend the effort to become very good with complex tools like Scala and Scala.js, and then be extremely productive at writing rock solid code when using those tools. Other people prefer to hack things together quickly using more approachable tools like JS. They can do a PoC quicker than me, but my approach would produce code that needs much less maintenance over time, and I think in most production cases would be less expensive overall.
It depends on the complexity of what you're doing. Why is your back-end written in scala instead of python or php for instance ? It's a lot easier to find php developers than scala developers, so why do you use scala not php. It's probably because if your back end code is doing anything remotely complex its not worth the trade-off. You care about reliability. You care about type safety. You like the fact that 95% of the mistakes your developers make are discovered before the code will even compile. You don't even notice these mistakes, they don't count, they never get committed. If your back-end is in scala and your front-end is too, and you have autowire set up, you get end to end type safety. You change the api on back-end, your front-end no longer compiles. You can refactor the entire application with relative safety. If your client side code is simple, sure, use javascript. If you're doing something harder that will be developed over time, the benefits of a grown-up programming language apply on the client side too.
&gt; why they rather not use lisp/clojure coz it's 2017 ;)
Just adding some context here, ScalaJSON AST now has its organization and package name finalized, so this milestone release (`1.0.0-M1`) signifies the release before the final one proper release (`1.0.0`). This is mainly to see if there are any missed details before the final release and any last minute issues!
Probably because there is no real ecosystem around it in lisp/clojure (especially lisp, at least clojure can piggy back off of Java). Scala has a lot of AI/big data libraries (like spark) which have a lot of adoption and use in companies
&gt; Good point. But I gather that in a Real World Application(TM) like a standard RESTful web server, the inherently async nature of serving HTTP requests would help mitigate the overhead of the trampoline? Well remember, we are dealing with purely functional programming here, which means no loops. This issue doesn't come up in async programming, but if you are doing any sought of algorithm or even business logic that has outer level recursion (which your Real World Application(TM) will either directly or indirectly use), then its a problem. I mean at least in Haskell (which Eta is, just on JVM with a prefixed GHC version), this is very common in non async scenarios mainly due to the language not providing other alternatives as idiomatic (loops for FSM's)
Yeah I wouldn't go here, sounds incredibly brittle
I trust it's out of scope to provide macro-based case class serialization ala circe or will that be added in the future?
Hmm, I see what you mean. I guess this explains why Scala includes a while loop construct - it allows a looping implementation of normally recursive operations like reduce, so you can have a functional API but an imperative, non-trampolined core. Thanks for the conversation, I learned a lot.
How is that sparks problem you want to use a different version of a library? They can't possibly account for all the possible projects their library would be included in. They used to shade a bunch of libraries in their project but they did away with most of it since it led to a lot of management problems. It's also very easy to exclude their logging library if you don't want it. I doubt it's fair to call their entire library bad because they don't use bleeding edge library versions. 
19 characters is way too long, I can't be bothered to use any feature that isn't 5.6 characters or less. 
What do you expect of a site called "superkotlin.com". Kotlin is a language that promotes it self by trashtalking Scala.
No, pretty much everyone. And yes, you do wrap every nullable value coming from a Java API in `Option`. This is absolutely standard. We don't use `null` in Scala.
So whats the consensus behind this? Will it be utilized as backend among all other jsons libs? Or is goal batteries-included scala json?
In every commercial project I've worked on, some with teams of 25 devs, people do just use Option, or Either, or Scalaz's disjunction or Xor, or Validation. Never null.
But why is that usefull? Is there any advantage in comparison to use composition and just delegate the calls to the underlying (final) class?
If you think spray is 'behemoth', then you are in for a surprise when you look elsewhere. Spray is less of a framework than it is a library - and that's a good thing. My advice is if you want to support both grpc and HTTP endpoints from the same server, use a scala grpc adapter like scalapb for the grpc side, and run your REST API on a different port and build it with akka-http. Grpc uses http/2 as a transport, but that's really more incidental than anything else - I wouldn't want to build or use a regular HTTP server on the same port.
Sorry. i fixed the typo now.
SECTION | CONTENT :--|:-- Title | Introduction to gRPC with ScalaPB by Petra Bierleutgeb Description | This video was recorded at Scala Days Copenhagen 2017 Follow us on Twitter @ScalaDays or visit our website for more information http://scaladays.org Abstract: This talk will introduce the audience to writing gRPC applications in Scala using ScalaPB. gRPC is a modern, high-performance, open-source RPC framework focused on the requirements of connecting polyglot services in microservice-oriented systems. Its support of HTTP/2, protocol buffers, bi-directional streaming and type-safety make it a... Length | 0:44:18 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
That loop barely registers on my rig. The java.exe process uses like 3% of my total CPU resources, and the conhost that manages the console window itself (I guess) uses maybe twice that. I have 4 physical cores / 8 logical cores; it's an i7 from several generations back. This is against Scala 2.12.1. Also Win10; creator update I think.
&gt; It saves you from having to write the convenience method with the delegation inside them Yes. Now, what if the language would support delegation native (like Groovy does a little bit) and allows you to say `hey, automatically delegate everything except what I explicitly implement`? &gt; (though using something like a type class is likely preferable in this specific case) That comes on top and I would remove `in this specific case` unless we consider performance, compatibility and other such things.
&gt; Would you recommend Maven over SBT for Scala development? I would, but I feel obliged to point out most of the community disagrees (I've never understood why though). In any case, multi-module projects are pretty normal and well-supported in both.
You must have a really, really slow console, then. What's the output going? Windows terminal?
[removed]
It looks nicer in small examples. It can perform better. But I suspect the main reason is that Odersky views implicit parameters as much less of a problem than most Scala users. Last discussion I understood him to say that they're not a source of bugs for him, though I can't imagine how that works.
Is there anything I can do? Should I manage my own threads instead using ExecutorContext and Futures? Should I change GC?
I tried changing the loop to: for (i &lt;- (0 until 1000000000)) { val x = 2 * i; val y = Math.sqrt(x) / x * i / 2.0; val z = 3.54 + i; } It always hits all 4 cores equally :(
As far as I remember, there were no examples of anyone doing the fs2/monix style when the future api was being developed in Akka. If someone had come along 6 years ago with this suggestion the api might be different today.
I'm 99% sure that scala Future API was based on java.util.concurrent.Future, which was introduced back in JDK5. The scala API just makes the java API easier to work with in scala.
What on earth are you doing that saturates a modern CPU core for 30 seconds? Are you waiting on IO or some other synchronous operation inside your Future?
Some follow-up info: https://www.lightbend.com/blog/as-data-centric-applications-go-streaming-ibm-and-lightbend-team-up
Is it really a big source of bugs for you (or other users)? I've never really had a problem with implicit params.
I think it is a widespread anti-pattern – not `ExecutionContext` specifically, but the whole set of APIs that try to stuff a "magic" cookie into the standard method signatures of `map`, `filter`, `flatMap`. See collections' `CanBuildFrom` for another example. It has immense costs in terms of code reuse and forces implementations into some very unfortunate corners. It is wrong and broken, but unlikely to change. There are of course advantages and disadvantages to every approach, but taking everything into account, this approach is worse than the alternatives.
I'm using the windows process manager to show logical cores. When I run the code above I see all cores hit at the same time, at about 30% utilization. A small spike on all four graphs.
Try switching your `sqrt` to `cos` and see what the long-term utilization looks like. Also keep in mind that the graphs in Process Manager are averaging usage over time. Process Explorer lets you actually track cycles used per thread, which can give you a better idea of how many threads are being actively run. Like I said, it seemed like after the startup spike, all the CPU utilization was localized to a single thread (with very minor use in other threads presumably for things like GC). That's the behavior I'd expect from your code.
We have HTTPS and JWT for authentication.
I'm at work (win7) and confirm what you're seeing. I was getting confused by the startup spike! 
I think there are some good reasons why `Future`s should take `ExecutionContext`s in all their operations. It's possible the tradeoff isn't worth it (v.s. allowing clean interfaces, purity, laziness, ...) but there's definitely a tradeoff, and it's worth discussing what it is even if there are better APIs possible The basic question is, if I call `myFuture.map(func)`, who ends up running `func`? There are basically three options: - Whoever runs `myFuture` (e.g. if it's a `Promise`, completing it via `Promise.success` ends up running `func` as well - Nobody runs `func` yet; the whole expression is lazy/pure, and whoever forces it via `.run` (or whatever) ends up running `func` - Whoever called `.map` ends up running `func` The first case is problematic, because often people want to isolate things in thread pools so long running computations on pool B doesn't end up hogging pool A. If I have a high-concurrency websocket server handling lots of small packets and generating `Future`s, and I call `.map` on one `Future` to perform some heavy computation, that computation will block all packet processing until it's complete. The second case could work, but it kind of kicks the problem down the road: when someone ends up calling `.run`, which threadpool ends up running all the operations? The simplest solution would be "the current thread", but often in reality you want different threadpools running different parts of it, e.g. in the websocket-server example above. To allow a `.run` on a totally-lazy `Future` to run different parts on different thread pools, you could add some kind of `runOnPool[T](f: Future[T], p: Pool)` function that would let you swap pools at those points. Then you'd need to pass the `Pool` around everywhere. If you're already passing `Pool`s around everywhere, and their sole purpose is to let you call `runOnPool` at various points, it's not a huge stretch to make `runOnPool` automatically take place so every operation uses the implicit `Pool` to run itself. Maybe not my first approach, but not totally unreasonable. This is the third option: "Whoever called `.map` ends up running `func`", where `Pool` is spelled `ExecutionContext` and "whoever" is defined by whichever `ExecutionContext` is in scope. That means if - user A's piece of code is generating `Future`s on some threadpool A - a downstream user B is receiving those `Future`s - user B can `map` on user A's futures all he wants without clogging up threadpool A (but he'll need his own threadpool B) This is a lot like how Actors work, with many of the same benefits: user/threadpool A and user/threadpool B are basically separate actors that cannot block each other. "what actor I'm running on" is basically defined by the `ExecutionContext` you have in scope. Give how Scala people had traditionally been actor-crazy, it's not surprising they settled on this solution, making `Future`s behave in an actor-like way. While I think actors as a whole are over-hyped and over-sold, there are some non-hype benefits of the design of `scala.concurrent.Future`. Clearly there are other ways to design it with different trade-offs, but I just wanted to say why I think the current design is not completely-without-merit, as is often suggested
Awesome! 
I totally agree that actors are overhyped and oversold. You should blog about it - as you are big person in community, it would make people at least think before committing to 100% akka everywhere.
Yeah. The more general the type of the parameter is, the worse. And ExecutionContext ist quite general. Also it is very tedious to pass it around everywhere in the business logic even though I don't care about it there.
Thanks a lot! Another note - the level of concurrency in this app is at the user-level. Each user gets a "context" and its not thread-safe. Therefore , most all of my futures also have a synchronized{} block in them. I'm guessing this isn't a good thing. I wonder if each user should get their own execution context? So then each context can run concurrently with each other, but the individual jobs are sequential - per user?
But even in Monix, you have to pass around a Scheduler, which is implicit when you create a Task. I usually get around the implicits issue by passing the scheduler explicitly, which removes ambiguity. 
Don't know why you are so downvoted. I agree with what you say. I'm not so sure about `CanBuildFrom` but I feel I tend to agree here too, because it just feels wrong to do something like that for convenience/"type inference". 
I agree! /u/lihayoi a blog post would be really great; your posts read very good and they tend to be little evangelistic and neutral so it could even influence people who blindly use actors for everything.
An actor cell by itself is about 300 bytes so there isn't that much overhead -- depends on the state you're storing. You typically want to have a parent actor supervising -- create the parent actor, bind that actor in guice, use the service to send messages to the parent actor, and let it handle the lifecycle of the workers as children by creating and forwarding messages. * http://doc.akka.io/docs/akka/current/scala/general/actors.html#child-actors If you want an child actor to turn itself off if it has not received a message after a certain period of time, use a receive timeout and send a context.stop(self): * http://doc.akka.io/docs/akka/current/scala/actors.html#receive-timeout * http://doc.akka.io/docs/akka/current/scala/actors.html#stopping-actors Using a parent / child actor relationship is also useful because the parent can summarily kill child actors that have hung and aren't completing their work in time, and can report exceptions thrown from the workers. Using actors can be pretty simple in practice. Here's an example of a session service that stores session ids in a distributed key/value store, for example: * https://github.com/playframework/play-scala-secure-session-example/blob/2.6.x/app/services/session/SessionService.scala And here's the canonical "quickstart" guide: * http://developer.lightbend.com/guides/akka-quickstart-scala/
Wow, thanks! I'll give it a shot!
I probably wouldn't go so far as calling it an anti pattern, but I do agree that these things should be explicit. I prefer Java stream style collectors over implicit CanBuildFrom, and the same with Future now too.
I keep seeing this, but they are mostly based off of Akka's Future api. I'm pretty sure Twitter still has their own implementation, although I haven't checked if they have removed it recently.
Better to use OS random generator. API or /dev/urandom Sure it's the example, but i want to remind that there where a lot of issues with predictable random used by hackers. Just to be sure :) Just 86400 ^ 365 tries to find your random )) PS thanks for the article. Now i want to invent some bicycle :)
You should estimate the performance level you need to achieve ("better" is always just a tiny bit further away), and should use that guideline to know when to leave "good enough" alone and when to consider radical solutions to get there. FWIW, I would consider C or C++ a radical, but occasionally necessary solution. Everything is always a trade off when it comes to engineering. Don't manage memory if you can avoid it, always know why you chose a language to begin with.
Apologies, I meant Akka Future API, I got confused.
Considering its asynchronous, they are actually quite useful, at least a lot more useful then `Task` due to its lazy nature and how its executed
Never had this problem, and I use `Future`'s all the time. You honestly should not be importing the default execution context singleton when working with the API. If you are working with `Future`, your functions should have an `implicit ec: ExecutionContext`. Only when you run the execute your root `Future` def should you supply an `ExecutionContext`. If you want to explicitly supply an `ExecutionContext` halfway through an async execution, it should be via a `map`/`flatMap` operation (i.e. typical use case scenario is executing something on the UI thread)
`CanBuildFrom` you can say it was abused, but at the time it was the only known way to implement the Scala collections the way they were (i.e. making it work with datastructures like `Array` which require implicit manifests). With `Future` its not an anti-pattern, you need an `ExecutionContext` to do operations with `Future`. So sure, you could make it explicit, but then you would need to pass around the same `ExecutionContext` all the time which is one of the problems that implicit parameters solve
The audio sucks. I can't hear a thing!
Given the Scala Future's approach that uses separate threads to run each step of the computation, the computation's stack trace isn't preserved and are pretty much useless for debugging. Note that this is actually a particularity of the Scala Future, most other Future implementations don't schedule each step of the composition on separate thread pools given the prohibitive performance cost.
Ok finally found it myself. fastOptJS::startWebpackDevServer for the server, and in a seperate SBT, ~fastOptJS for hot compiles
We will be soon. I'll let you know how it goes!
That's a bit stronger language then I'd like. Scala 2.8 collections were great, but with what we have learned now I think we can do better. With Future, I'd prefer delaying execution until an ExecutionContext is supplied. I still think the current Futures are great (I helped design them) but with what we have learned since I think we could do better if we had the chance to rewrite them (which is not likely due to the amount of breakage).
I'm guessing it's infeasible to not use excel / Apache POI
Somewhat relevant, the Typelevel blog on the new cats IO monad gives a good explanation on thread pool shifting and lazy evaluation vs future's eager evaluation: http://typelevel.org/blog/2017/05/02/io-monad-for-cats.html
Scala's `MonadPlus` comprehensions usually "just work" with standard containers due to `CanBuildFrom`, so I can understand why people would want to avoid effect stacks altogether--avoiding cases where monads don't compose out of the box, avoiding additional boilerplate (and potentially additional dependencies) needed to compose them. They do make it far easier to distinguish between exceptions and expected errors, though, which I agree is important and worth the overhead. I just figured it was worth mentioning that monad transformers aren't the only to do this (to flatten two different monads). The example in your top-level comment can easily be changed to construct `Eff` instances (with `fromFuture` and `fromEither`) instead of transformer instances, and it would do the same thing with some additional boilerplate. 
&gt; Given the Scala Future's approach that uses separate threads to run each step of the computation, the computation's stack trace isn't preserved and are pretty much useless for debugging. [Not anymore!](https://blog.jetbrains.com/idea/2017/02/intellij-idea-2017-1-eap-extends-debugger-with-async-stacktraces/) 
&gt; In theory I like the idea Why? To me, a big selling point of Scala is easy, sane, non-potentially-system-breaking (and easily-removable) dependency management, even in REPLs. And there are many reasonable JSON libraries out there. &gt; I am disappointed by the decision to make this throw exceptions. I like this because it's new. High performance and low memory usage aren't the highest priority in the other Scala JSON libraries I've seen (I don't mean that they're not given careful attention by library authors, just that other priorities take precedence); this library is relatively unique in that regard.
The slides are good but with this audio it isn't usable I fear
hey OP, can we have a link to slides?
I haven't used Akka in a long time, but isn't the new Akka typed (not the older approaches) seen as a proper solution? There also seems to be an immutable flavour of it.
You don't have to learn new language to do backends. Single build tool. Single IDE. Same experience.
&gt; *B.* Is software development still mainly a team effort ? Do the new technologies allow enough leverage for one person to make something over a period of time ? Let me focus on this, because this is where I think the revolution is. My answer is "yes, and yes." Far too much attention is still paid to "Silicon Valley unicorns," made-up valuations, and who's getting money from Sand Hill Road. Meanwhile, it has become all but trivial for a single developer to: 1. Pick a language that has [robust compilation to JavaScript](http://www.scala-js.org/). 2. Pick a [cross-platform UI framework](https://github.com/scalajs-react-interface/sri#sri). 3. Pick a [PaaS with robust container support and hosted or on-prem options](https://www.openshift.org/). 4. Pick one of a dozen developer-focused cloud providers, such as [Packet.net](https://www.packet.net/). Develop on your laptop when not on your day job. Collaborate with a friend who knows design, UX, and CSS well. Concentrate on building a really good CI/CD pipeline. Spend some $ on some autoscaling spot instances (if your provider supports them) to briefly scale up for load-testing, etc. Launch to the world. Then either run as a going business or sell to someone else who will. This is _100% feasible, today_, and I linked to what I linked to because it's what I'm using. :-)
While now I at least see some point in Futures, I still don't buy it at all. Task can still do all this stuff, providing at each step new execution context if necessary. At the same time I assume that times you have to change execution context are nowhere near you do maps and flatMaps, which most of the times are cheap and can be executed on any execution context really. Also, that's probably for the lack of experience, but I didn't see smth like 50 entities with 50 different execution contexts, often times it's just smth like default and io execution contexts and you can easily control which context runs what, not having any headaches about implicits.
Yeah, but it's so much easier, now you have to care about execution context only on task creation and changing context, rather than in each function which uses future at all. And these cases are so small, that you could actually pass this contexts explicitly
Who funds scalafiddle? Who makes sure it doesnt go black? I'd be happy to contribute some of my hard earned $$$ towards good cause.
Could you help me understand point 3 and 4? It seems like you are building docker images and deploying them. I am used to deploying docker on ECS but unfamiliar with the tech you mentioned in pt 3/4 and how they play with each other. 
Sorry, I was oversimplifying. The akka Future implementation delegates to java.util.concurrent under the hood, that's all I was trying to convey. Moreover java.util.concurrent was published in J2SE5 in 2004. The first actor implementation in Scala was published in 2005. Yes, this is further history than might be useful (and maybe you can trace java.util.concurrent back to pthreads or something like that) but I still think that understanding the java API is critical to understanding any of the scala APIs, because none of them run native code. Then again, that might just be me speaking as an old dude that entered the workforce soon before java.util.concurrent was published.
They're not stable. They're nothing more than toys. Drools is the best rule engine in JVM land. It's clunky, has a weird API, is incredibly hard to test, and requires a lot of knowledge about how it works in order to use it well...but it's also fast, very memory efficient (as far as RETE implementations go), stable, and bug free. 
You need to provide an ExecutionContext a) at execution boundaries to pick the right threadpool and b) at the point where you run it. None of that requires passing it on every method when you define the computation itself. The nonexisting separation between defining a computation and executing a computation is the core issues that is responsible for most of the problems Scala has in this context.
Mario Fusco, the lead developer of drools is/was a scala enthusiast. If a scala api for drools doesn't exists, I'm led to believe he considered that a bad idea. You could write your own, maybe 
`%%%` indeed came from Scala.js, initially co-designed by @gzm0 and myself. It's a very small contribution to Scala Native, though ;)
http://www.tomcatexpert.com/blog/2011/04/25/session-fixation-protection
That was a terrific blog post. Scala Native is the best thing to happen with Scala in a long time. While it was and still is important to be hosted on the JVM, being only on JVM precludes it from really being a scalable language, one that I can write scripts and C++ level performant code in. The experience is still very clunky (compare this to writing the same in D with dmd, where things are much zippier) but it can only get better. Getting a native compiled Spark or similar would be very interesting... 
It seems that scala-native could compile itself in the future [0][1] which is probably a good sign for sbt as well. [0] https://github.com/cvogt/cbt/issues/129#issuecomment-297011764 [1] https://news.ycombinator.com/item?id=14567883
What sub is this?
On the right: "Information regarding Scala" check the links. Shortly: A computing language with 'Object-Oriented Meets Functional' in the Java (another language) ecosystem. r/DesignPatterns/ - Object-Oriented r/functional https://fr.wikipedia.org/wiki/Scala_(langage) 
**Here's a sneak peek of /r/functional using the [top posts](https://np.reddit.com/r/functional/top/?sort=top&amp;t=year) of the year!** \#1: [So, monads....](https://np.reddit.com/r/functional/comments/554sqf/so_monads/) \#2: [What is the Difference between Imperative and Functional Programming?](https://magneticcoredump.squarespace.com/blog/what-is-the-difference-between-functional-and-imperative-programming) | [3 comments](https://np.reddit.com/r/functional/comments/5v7t4f/what_is_the_difference_between_imperative_and/) \#3: [CQRS, Fonctionel, Event Sourcing &amp; Domain Driven Design](https://speakerdeck.com/lilobase/cqrs-fonctionel-event-sourcing-and-domain-driven-design-mug-lyon-2017) | [4 comments](https://np.reddit.com/r/functional/comments/5wgq40/cqrs_fonctionel_event_sourcing_domain_driven/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/5lveo6/blacklist/)
Credit Karma is IIRC
What about the language itself? How are the new set of languages better? To give u an example, I studied C and C++ before I learnt Java. The lucidity of Java was immediately visible especially in contrast to my frustrations with C++. I'm seeing something similar in new languages like Ruby and Scala where its a fresh approach. Given that the differences in Scala and Java are much greater than those between Java and C++(both being OOP at their core), how does it improve programmer productivity besides the obvious simplicity. Is there a fundamental leap(in the software practice) in using a multi paradigm modern language like scala? PS:A simple Yes/No will do.. 
Definitely agree, and thanks for pointing it out. Most of my experience is with dynamic languages, so I appreciate tips on where I can improve with types!
If you're just passing a single `ExecutionContext` through the whole call stack you don't have a problem - but you're also not getting any value out of it being a parameter at all. The case where it matters is the case where you have a UI thread or separate threadpools for separate purposes - but in that case it's very easy to accidentally pass the wrong one, IME. I don't really understand the point about `map`/`flatMap` - just having multiple `implicit` `ExecutionContext`s around is a recipe for trouble however you're using them (unless it's not important which one a given `Future` runs on - but again in that case it's not providing any value), but if you do have them you'll have them be `implicit` so that they can be used in the idiomatic way - or even if you go down the route of having the dedicated-purpose `ExecutionContext`s not be `implicit`, it's still very easy to mess up and accidentally not pass one and so default to your `implicit` one.
&gt; The example in your top-level comment can easily be changed to construct Eff instances (with fromFuture and fromEither) instead of transformer instances, and it would do the same thing with some additional boilerplate. I think it's incumbent on you to actually do this and show what it looks like, if you think the `Eff` approach is production-ready and something the original questioner should be considering.
Hmm, I believe it doesn't give you the same kind of semantics as the untyped actors, though I could be wrong. I haven't really looked into them.
I am a PhD student working on [ingraph](http://docs.inf.mit.bme.hu/ingraph/), a distributed Rete engine prototype that supports [openCypher](http://www.opencypher.org/) graph queries. We evaluate the queries with the [IRE (Incremental Relational Engine)](https://github.com/FTSRG/ingraph/tree/c191d4fbde71a05695ccca0bdb1366f57e611613/ingraph-engine/ingraph-engine-ire) component, which is written in Scala and is built on Akka. It has no dependencies inside the project, so it can be extracted very easily. It is also quite well tested. The other parts of the system - as you'd expect from a research prototype written by MSc/PhD students -, are a bit flaky. Our main research and development goal for the rest of 2017 is to support as much of openCypher as possible, so you can expect significant improvements in the code. We have some papers on openCypher and incremental query evaluation, listed on the ingraph web page and my [university homepage](https://inf.mit.bme.hu/user/1385/biblio). I also gave some talks on the topic, the latest one at [GraphConnect London](https://www.youtube.com/watch?v=uLu2w8JxMKo). Talking about other tools, I worked with Drools a few years ago and included it in my [incremental query benchmark framework](https://github.com/FTSRG/trainbenchmark), so the benchmark code shows an example on how to use it. I think it has quite a steep learning curve and an unintuitive API but is quite robust. There is also [i3QL](https://github.com/seba--/i3QL), but it seems abanoned.
SECTION | CONTENT :--|:-- Title | ingraph: Live Queries on Graphs — Gábor Szárnyas, Budapest University of Technology and Economics Description | What is the common challenge in detecting frauds in financial transactions, analyzing source code repositories and performing runtime verification on cyber-physical systems? These applications operate on large, continuously changing graphs and use complex queries, but also require quick response times. However, these queries are usually known in advance, so a smart query engine can rely on previous results and only calculate the differences of the last change. ingraph is a query engine for con... Length | 0:11:58 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
SECTION | CONTENT :--|:-- Title | ingraph: Live Queries on Graphs — Gábor Szárnyas, Budapest University of Technology and Economics Description | What is the common challenge in detecting frauds in financial transactions, analyzing source code repositories and performing runtime verification on cyber-physical systems? These applications operate on large, continuously changing graphs and use complex queries, but also require quick response times. However, these queries are usually known in advance, so a smart query engine can rely on previous results and only calculate the differences of the last change. ingraph is a query engine for con... Length | 0:11:58 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptbotbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptbotbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
Short answer, then: yes, the language matters. I use Scala in a purely-functional way, with the [scalaz](https://github.com/scalaz/scalaz) ecosystem. Slightly longer answer: since I'm talking about the possibility of a one-person project, the biggest success factor is almost certainly just choosing tools you like to work with, for whatever reasons you like to work with them. I should confess here that I'm using Scala on top of some pretty traditional Java-based technology, building with Maven, etc. not because it's my strongest preference, but because I intend to sell what I create, so some judicious compromises are called for. Left to my own devices, if I intended to continue to develop and maintain my system, I'd probably build it with [MirageOS](https://mirage.io/).
Briefly, there are a whole bunch of concerns that come up pretty quickly when you want to build and scale a distributed system with several moving parts. Containerization is a great start, but a realistic system is going to consist of several containers, e.g. front-end nodes, back-end nodes, persistence nodes, etc. and it may even make sense to have distinct, e.g. full-text indexing nodes with ElasticSearch, monitoring nodes with Prometheus, etc. depending on your needs. This is why tools like [Kubernetes](https://kubernetes.io/) and [OpenShift](https://www.openshift.org/) exist: basically to operationalize _clusters_ of containers. OpenShift, which builds on Kubernetes, adds some very nice (IMO) DevOps and application lifecycle support features in addition to the very nice orchestration features provided by Kubernetes out of the box.
Type safety is one of the things that makes Scala so great.
True, however drools moved onwards to PHREAK as the internal engine (one of the reasons was to support backward chaining) 
mirageOS... mind=blown. This! is the roller coaster I've been in recently. Each new thing I come across just blows my mind away. Gone are the days when academia could keep pace with what's out there. I had come across OCaml before as an industrial strength language, but people are really pushing the boundaries in programming innovation! Unikernels seem like the final frontier! (or not? probably not :( )
As someone that has been involved in hiring scala devs, I have been happy to hire people that had no commercial experience with scala so long as they were open-minded and willing to learn. Everybody is constantly learning so we are all in the same boat so to speak, some have just sailed further. I encourage you to keep learning scala, it will broaden your understanding of languages and improve your Java too. But be warned, the more you use Scala the less you'll want to use Java ;)
The point of passing implicits through your call stack is that you have a shared context, but you can override it at the top level. It is the benefit of globals, but without the architectural inflexibility and risk. Having multiple implicits not a problem, since the compiler will generate an error if there is an ambiguity.
I suspect sbt's slow startup time may be caused many factors unrelated to the JVM.
Would scala.js not be an appropriate target for sbt? 
For a bit over two years, I did, and it costed about 70$US a month, running on a single non-reserved (expensive) m4.large EC2 box (2 vCPUs, 8gb RAM) Now /u/ochrons is running it. My understanding is that he has more servers up and running (e.g. a database, a web front-end, and compiler-workers) but he probably found cheaper hosting than I did. You'll have to ask him what the server cost is like nowadays.
My intent is not to promote eff, and I'm not aware of who is using it in production. Nevertheless, import org.atnos.eff._ import org.atnos.eff.all._ import org.atnos.eff.future._ import org.atnos.eff.syntax.all._ import org.atnos.eff.syntax.future._ import scala.concurrent._, duration._ import scala.concurrent.ExecutionContext.Implicits.global implicit val scheduler = ExecutorServices.schedulerFromGlobalExecutionContext case class User(id: String, addressId: String) case class Address(id: String, cityId: String) case class City(id: String) case class NotFound(msg: String) def findUser(id: String): Future[Option[User]] = Future(Some(User(id, "addr"))) def findAddress(id: String): Future[Option[Address]] = Future(Some(Address(id, "city"))) def findCity(id: String): Future[Option[City]] = Future(Some(City(id))) // we can use kind projector instead of the type lambda; I'm not, here type R = Fx.fx2[TimedFuture, ({type l[A] = Either[NotFound, A]})#l] val action: Eff[R, City] = for { userOrNone &lt;- fromFuture[R, Option[User]](findUser("bob")) someAddressOrNotFound = userOrNone.fold[Either[NotFound, Future[Option[Address]]]](Left(NotFound("No user with given id")))(u =&gt; Right(findAddress(u.id))) addressOrNoneFuture &lt;- fromEither[R, NotFound, Future[Option[Address]]](someAddressOrNotFound) addressOrNone &lt;- fromFuture[R, Option[Address]](addressOrNoneFuture) someCityOrNotFound = addressOrNone.fold[Either[NotFound, Future[Option[City]]]](Left(NotFound("No address with given id")))(a =&gt; Right(findCity(a.id))) cityOrNoneFuture &lt;- fromEither[R, NotFound, Future[Option[City]]](someCityOrNotFound) cityOrNone &lt;- fromFuture[R, Option[City]](cityOrNoneFuture) city &lt;- fromEither[R, NotFound, City](cityOrNone.fold[Either[NotFound, City]](Left(NotFound("No City with given id")))(c =&gt; Right(c))) } yield city Await.result(action.runEither.runSequential, Duration.Inf) 
&gt;app management services, Watson, data analytics, IoT, DevOps, and cognitive intelligence solutions They seem to have decided to just make a list of all the buzzwords instead of spreading them out in the article
Well...sort of. That's true the only candidates are values. But I've definitely had situations where the wrong implicit is chosen, particularly when dealing with chains of implicits and higher-kinded types.
How does Rete differ from unification, and a tool like Minikanran?
I recorded new audio which should be really better: https://youtu.be/tIwr9AhCQYs
SECTION | CONTENT :--|:-- Title | A practical Scala introduction - Alexis Hernandez Length | 1:03:30 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
Here are the slides (while I consider can be useless due to the lack of text): https://drive.google.com/file/d/0Bw-SsYVOTRT9bG12VlhkdjdwUG8/view?usp=sharing
A bit off topic, but relative to this single comment: https://app.updateimpact.com/treeof/org.springframework.boot/spring-boot-starter-web/1.5.4.RELEASE Spring boot starter web has less dependencies than Scala Play, by like 9. That was unexpected.
I appreciate the time that all of you took with this, I recorded new audio that should be really better, thanks. https://youtu.be/tIwr9AhCQYs
SECTION | CONTENT :--|:-- Title | A practical Scala introduction - Alexis Hernandez Length | 1:03:30 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
I recorded the audio again, the audio should be good enough now, thanks.
&gt; You need to provide an ExecutionContext a) at execution boundaries to pick the right threadpool and b) at the point where you run it. None of that requires passing it on every method when you define the computation itself. Except when you need to fork a current async computation on a different UI thread, which is half of the point of `Future` (or when you need to seperate executions in different thread pools for priority/performance reasons) &gt; The nonexisting separation between defining a computation and executing a computation is the core issues that is responsible for most of the problems Scala has in this context. Actually its not, and Scala is not designed to be a lazy by default language as Haskell. Each design has its own tradeoffs.
&gt; That's a bit stronger language then I'd like. Scala 2.8 collections were great, but with what we have learned now I think we can do better. Hence the redesign of collections in 2.13.x. However we need to be careful about arguing this stuff in a retroactively select manner. For the problems that scala collections was trying to solve, there wasn't another widely known solution. `CanBuildFrom`was the solution for defining context overrides for specific collections, which was needed. &gt; With Future, I'd prefer delaying execution until an ExecutionContext is supplied. I still think the current Futures are great (I helped design them) but with what we have learned since I think we could do better if we had the chance to rewrite them (which is not likely due to the amount of breakage). Thats not a `Future`, thats a `Task`. `Future` is defined to be strict. They both have their merits
I am talking more from the PoV of instrumentation code, i.e. if you read http://www.schibsted.pl/blog/tracing-back-scala-future-chains/ there are trips/tricks or ways to preserve stack trace in `Future`. There are also tools like Kamon that can help in this area. afaik, doing this kind of stuff is really hard to do with Task (or even laziness in general)
&gt; Except when you need to fork a current async computation on a different UI thread, which is half of the point of Future (or when you need to seperate executions in different thread pools for priority/performance reasons) Which is exactly what I have written. &gt; Actually its not, and Scala is not designed to be a lazy by default language as Haskell. This is completely unrelated to laziness. &gt; Each design has its own tradeoffs. Yes. And as mentioned, this one is worse than alternatives.
&gt; This is completely unrelated to laziness. ? How, if your `.map`/`.flatMap` functions don't have a context of which scheduler/executionContext/whateveryouwanttocallit to run on, either its going to block the thread (which defeats the purpose of `Future`) or you have to delay the computation until you run it (i.e. lazy) Either that or I am missing something
And many related, i.e. current stable SBT release (0.13.x) uses the old `java.io` because that series of SBT needs to run on Java 6 (this is JVM related) which causes a huge delay in SBT load because the file handling in `java.io` has real performance issues. Also suspect a lot of the startup loading is class loading issues which is related to JVM (although it would be interesting to see how native would fix this as it doesn't even have these dynamic loading of class fields capability) 
&gt; delay the computation until you run it (i.e. lazy) There is a big difference between Haskell-style laziness you allude to and having a clear separation between describing a computation vs. running it. Is SQL "lazy" because `SELECT foo FROM bar GROUPBY baz` doesn't hit the the database three times? Is Slick "lazy" because `bars.map(_.foo).grouBy(_.baz)` doesn't do it either?
&gt; There is a big difference between Haskell-style laziness you allude to and having a clear separation between describing a computation vs. running it. These aren't mutually exclusive definitions. &gt; Is SQL "lazy" because SELECT foo FROM bar GROUPBY baz doesn't hit the the database three times Depending on what your definition of lazy is, yes it is (although it becomes less clear when you talk about transactions and commits). I am not sure what we are arguing about honestly, it seems like you are advocating for a `Task`, which is fine. But the point of `Future` is to execute immediately (i.e. its strict) and its following Scala's default semantics here (which is that there isn't a difference between describing a computation and running it unless you put it explicitly in a `def`, and this also follows for `Future`)
True, but this doesn't happen in the context of `ExecutionContext`/`Future`
&gt; These aren't mutually exclusive definitions. Scala's style of evaluation (strict) and Haskell's (lazy) share the attribute that they are both implicit (the underlying runtime deals with it, invisibly) and happen automatically (without requiring the user to demand it explicitly). In this regard they are both much closer to each other than to an approach where evaluation happens on _explicit_ request. Both languages support this approach (evidenced by all the libraries which make use of this), although in Haskell the need appears to be less pressing superficially due to laziness, rewrite rules, and having control of their own runtime. The key point is that running things automatically vs. running things on demand is largely orthogonal to strict. vs. lazy. &gt; Depending on what your definition of lazy is, yes it is [...] You brought up Haskell's definition. I'm showing how the approach I'm describing is not "lazy" according to your definition. &gt; I am not sure what we are arguing about honestly [...] I agree that it is ill-advised to argue against potential solutions without having understood the problem. &gt; [...] its following Scala's default semantics here (which is that there isn't a difference between describing a computation and running it [...] The difference between ideology and reason is that the latter demands that assumptions need to be falsifiable. If half a decade and a dozen of implementations doesn't present clear evidence that the approach taken by collections, futures etc. is fundamentally flawed, then I think I can not offer anything that can impact this stance. Then it is ideology, not reason, and that's not a fight I care to enter.
Here's something similar, though as a Flow rather than Source. https://github.com/n0rb3rt/akka-streamutils/blob/master/CassandraStreaming.scala I would probably, however, defer to the implementation in Alpakka since it's written by folks at Lightbend. https://github.com/akka/alpakka/blob/master/cassandra/src/main/scala/akka/stream/alpakka/cassandra/ Though I often just get away simply with Source.fromIterator(() =&gt; resultset.iterator()). But I too would be interested in the trade offs of these approaches.
I am not sure about Minikanran, I am checking out their website. Rete is a matching algorithm: http://www.drdobbs.com/architecture-and-design/the-rete-matching-algorithm/184405218
Thank you for sharing. This looks interesting. Do you plan to provide a relational backend or just neo4j? I was just reading a presentation and it mentioned ingraph is "Independent from Cypher and property graphs". Could it be used as a replacement for a rule system eventually? Where "facts" are inserted and the continuous queries are executed?
I don't like the idea that we can't incorporate improvements from other implementations just because they work slightly differently. The point I am trying to get across to the original poster is that if I could go back 6 years and redesign Akka's Future API, I would not make the same decisions I made.
I remember Mimshot from this sub, he's most probably a scalaist. But your post is about java and has no place in this sub - this is why he asked "What sub is this". 
Currently it's running on a single dedicated server in Hetzner Online (Germany). The server has an Intel Xeon E3-1271V3 with 32GB of RAM and costs about 35EUR/month. So the cost is not really an issue here :) As for stability, the software itself has all kinds of watchdogs monitoring its health and since it's running on Docker, it simply restarts containers if something goes wrong. I'm planning introducing an external check as well, to make sure the whole thing is operational, but so far it seems to be very stable. 
[removed]
STOP
X-Post referenced from [/r/programming](http://np.reddit.com/r/programming) by /u/tomer-ben-david [Scala 5 Concepts, Syntactical Pitfalls](http://np.reddit.com/r/programming/comments/6ksfkx/scala_5_concepts_syntactical_pitfalls/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Scala 5 already? And here I am, still stuck with Scala 2.
They pulled an elastic
The IRE component is just an incremental relational engine, so it could be used as a (very low-level) rule engine. The facts - which are just tuples for the engine - are inserted by manually sending messages to the corresponding Akka actors. If your rules can be expressed as openCypher update operations (CREATE, DELETE, SET, ...), than it could eventually work as a rules engine. However, due to the specification of the openCypher language, it will not have very sophisticated firing semantics - for example, if a rule fires and the changes cause new activations of the rule, it will not fire again. To implement "classic" rule engine firing semantics, you'd have to implement your own layer on top of IRE.
Was there anything useful in that video? I'm also going to quibble, you are not "required" to use an anonymous function to map over a sequence of tuples. It is a convenient syntax to destructure the tuples. You can also write: Seq((1, 2), (2, 3), (4, 5)).map(t =&gt; t._1 + t._2)
Which one do you use for newtyping? Value classes, or some form of tagging (shapeless)? General recommodations?
I use value classes or even just plain case classes since they're built-in and easy to reason about and I've never had performance be tight enough to start worrying about the low-level details.
Cool. I just wondered if there are any other non-performance related implication.
I know how to do type-level logic in Scala but I don't do videos. Can you show what you've got, what you're trying to do and what you're struggling with?
I’ll put something together for the problem. I don’t have anything yet as I’m not really sure how to proceed at all.
I've updated the original question with the problem. Is that enough?
At what level are you looking to encode the reasoning? Are you happy to ask: "if A were a Knight what would C be?" and "if A were a Knave what would C be?" or do you need to abstract over those too? Do you have a transcript of what the Coq approach looks like?
I think that your suggestion would be a start. I've put the COQ that Paul used in the question, is that useful?
I think you should dive into scala, however I don't know how beginner friendly it will be. Play is pretty simple, I think it would be easy to pick up if you know how to program and know what you want to achieve. 
I think that Typesafe is the most affordable stack for a beginner. The other alternative is Typelevel.org stack, but that's very functional, you have to invest some time if you're not familiar with FP. IMO - no sense to practice Java - AFAIK most of its frameworks are different from Play, so dive straight into Play. Also I often saw opinion that Scala is difficult language - it's not true unless you start to mess with FP and monads, which are totally optional while developing with Play, Slick, etc.
Play is likely your best bet, just bang out a Hello World, and then go from there -- a lot to learn, both in terms of language and ecosystem, but you'll get up-to-speed fairly quickly given your Java background (i.e. familiarity with the JVM and Java gives you a head start over a developer coming from C/Ruby/Python, etc.). Gitter community links: [Scala](https://gitter.im/scala/scala), [Play](https://gitter.im/playframework/playframework)
Stick with Play / Slick. Try to avoid doing akka &amp; spark as you most likely don't need it. Applies even more for when you're starting.
I have started with 'Programming in Scala' book. So I'm planning to start with Play framework once I learn basic Scala programming. 
Ok, so what you want to do is have types for all the statements, implicits for all the implications, and then see whether you can resolve an implicit that says `C` is a knight. What I can't test at the moment is how to avoid having diverging implicits, so I'm not sure doing the bidirectional implications will work - it might be easiest to split the two hypotheses into their separate directions. We also don't have native support for negation. But something along the lines of: sealed trait Not[Prop] sealed trait A sealed trait B sealed trait C sealed trait Knight[Native] sealed trait Says[Native, Prop] // we don't actually use the values, we just want to use the implicit resolution mechanism // so we cheat and use "null" implicit def doubleNegationIsTrue[Prop](implicit doubleNeg: Not[Not[Prop]]): Prop = null implicit def whatKnightSaysIsTrue[Native, Prop](implicit isKnight: Knight[Native], saysProp: Native Says Prop): Prop = null implicit def trueStatementMeansKnight[Native, Prop](implicit prop: prop, saysProp: Native Says Prop): Knight[Native] = null implicit val bSays: B Says (A Says (Not[Knight[A]])) implicit val cSays: C Says Not[Knight[B]] implicitly[Knight[C]] // should compile only if C is a Knight I can't test now - I don't think this will work as written both because of the diverging implicits issue I mentioned, and because the system can't handle negation for us. So I would guess that `doubleNegationIsTrue` won't work alone and we probably need to explicitly define the contrapositive versions of `whatKnightSaysIsTrue` and `trueStatementMeansKnight`. If we work through by hand we can see what needs to happen - the implicit resolution mechanism trying to find that `C` is a knight should be able to figure out that it can get a `Knight[C]` from a `Not[Knight[B]]` and a `C Says Not[Knight[B]]` (using `trueStatementMeansKnight`); it has the latter as `cSays` so it should recursively attempt to resolve the former. So assuming we defined a `falseStatementMeansNotKnight` in the obvious way it should be able to figure out that it can use that and `bSays` assuming it can get a `Not[A Says (Not[Knight[A]])`. The part that Scala would strugle with is figuring out that `A Says (Not[Knight[A]])` is inherently false - to figure that out you have to go by cases and realise that both cases result in contradiction. I think the only way to do it is add `A Or Not[A]` premise and implement Boolean logic. But I'm not sure we could encode that in a way that the implicit resolution mechanism would actually follow, except maybe by very careful prioritization of our implicits (I can see the Coq code includes a "level" that I assume is some means of tactic prioritization, but I don't know what that corresponds to) . So I don't have a full solution, but hopefully that gives an outline of how we do type-level logic in Scala and the limitations of what Scala has. You might like to look at Idris - it offers more Coq-like functionality but in a more traditional-functional-programming-like style. 
&gt; but you'll get up-to-speed fairly quickly given your Java background (i.e. familiarity with the JVM and Java gives you a head start over a developer coming from C/Ruby/Python, etc.) But I watched [this video] (https://www.youtube.com/watch?v=EJl9mQ0051g) where the speaker says some concepts like routing is familiar to Ruby on Rails programmers. 
SECTION | CONTENT :--|:-- Title | Deep Dive into the Typesafe Reactive Platform - Activator and Play - with Kevin Webber Length | 1:03:01 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
Awesome thanks. I’ll play around with this. And if all else fails I’ll get Miles to glare at it until the compiler yields.
You can check out my sample project with Scala API and React frontend: https://github.com/alleycat-at-git/financial_tracker While it isn't super polished, since I had time constraints while implementing it (integrations tests are especially ugly), I think you could check out some ideas used there
Ok. I'll keep that in mind. 
There are examples i.e.: https://github.com/playframework/play-scala-slick-example
Yeah but spark is super fun. Who doesn't love throwing dozens of machines at processes that used to take days on one?
Well to be honest I don't find it particularly fun :D
I tend to find productioninzing Spark applications is an awful experience. Especially considering how new it is and how it is sincerely behind in scala versioning it has become. It needs a couple more years to gain momentum behind open source cluster management solutions and some maturity in the relational DB support (to match its excellent support for NoSQL solutions). The JDBC connector is very disappointing to say the least, and it limits its use in legacy systems. Developing them though, that's fun for me at least. I love watching huge files get hundreds of cores thrown at them. I HAVE THE POWER. 
How do you avoid monads if you're going to use play-json and Slick? For better or for worse, you kind of have to embrace the monad. But people shouldn't let that scare them away. If you can use promises in your front-end code, `.then` is the same as `.flatMap` and `map` in the more generic monad world. If you know how to use `async` functions in Javascript, Scala's `for`-comprehensions are just a more generic syntax. Also, I'd note that Play *is* a Java framework :), but yes, using Play Scala will feel fairly different from JSP. Agreed though, further investment in Java education won't be very helpful.
I just want to add to the other posts here. I'm a new user to Play, and I think it's a fantastic framework to use for beginners and advanced users. Still... I'd encourage you to also write some basic applications outside of the framework (or at least leverage the default SBT layout design that Play offers: https://www.playframework.com/documentation/2.6.x/Anatomy#default-sbt-layout). The Play framework encourages the MVC model, so it messes with your basic application layout directory structure and main function declarations. A Play application will likely look a heck of a lot different than a general Scala application.
I've begun looking into tagged types more recently for replacing dumb wrappers that only take one value and make The Naming of Things™ less-than-ideal. Things like `case class SocialSecurityNumber(ssn: String)` really bugs my sense of aesthetics, especially once you want to get at the actual value like with `def toJson(ssn: SocialSecurityNumber) = something(ssn.ssn)` This also goes in stark contrast with Getting Stuff Done and Damn It Not Yet Another Scala Concept, both well known phenomenons at my work place. Still makes for interesting conversation :)
I've never heard of staging, can you elaborate more on what it is? I browsed the link and it sounds like it's emitting custom bytecode from higher level scala abstractions to inline optimize some stuff? If that's the case I'm not sure why that's useful given the compiler already does this, unless it's a matter of optimizing custom code given runtime input?
By splitting the program into statically and dynamically known pieces it is possible to aggressively optimize the program by running it at compile time. Inlining higher order functions, deforesting data structures, specializing the program to known data are some of the uses. My own language does type checking as well using it. As it is an optimization technique that makes the language incredibly efficient. As you said, it is true that compilers do it internally using heuristics, but the issue with the technique is termination. Without annotations to split the variables into static and dynamic as LMS does it, the evaluator can diverge. By taking responsibility for dealing with the halting problem, it is possible to make the technique a lot more effective. It is called staging because a piece of a program is staged for later. When it is done automatically, as in a compiler, staging is also called [partial evaluation](https://www.youtube.com/watch?v=n_k6O50Nd-4). [How types can turn an SQL interpreter into a compiler](https://www.youtube.com/watch?v=cBj8EcX9Id0) by Rompf is a decent intro to what LMS does. 
SECTION | CONTENT :--|:-- Title | Ruby Conf 2013 - Compilers For Free by Tom Stuart Description | Partial evaluation is a powerful tool for timeshifting some aspects of a program's execution from the future into the present. Among other things, it gives us an automatic way to turn a general, abstract program into a faster, more specialized one. This math-free talk uses Ruby to explain how partial evaluation works, how it can be used to make programs go faster, and how it compares to ideas like currying and partial application from the world of functional programming. It then investigates wh... Length | 0:41:33 SECTION | CONTENT :--|:-- Title | Tiark Rompf - How types can turn a SQL interpreter into a SQL compiler Description | Abstract Commercial and open source database systems consist of millions of lines of highly optimized C code. Yet, their performance on individual queries falls 10x or 100x short of what a hand-written, specialized, implementation of the same query can achieve. In this talk we will present Flare, a SQL query engine implemented entirely in Scala. Where other systems interpret query plans, operator by operator, Flare generates and compiles low-level C code for whole queries at runtime, using the L... Length | 0:45:59 **** ^(I am a bot, this is an auto-generated reply | )^[Info](https://www.reddit.com/u/video_descriptionbot) ^| ^[Feedback](https://www.reddit.com/message/compose/?to=video_descriptionbot&amp;subject=Feedback) ^| ^(Reply STOP to opt out permanently)
It looks very nice! I have not spotted this project before so I would like to ask: have the version 1.x gathered some user-base or is it still rather "incubating"? 
LOL, that code kills IntelliJ - bug reported https://youtrack.jetbrains.com/issue/SCL-12189
I find that any serious use of types kills IntelliJ, unfortunately. I stick to Eclipse/Scala-IDE - it's clunkier but more reliable.
If you are applying mature algorithms on a big dataset in business scenario, TensorFlow/Keras is a better choice for now. Obviously they have a larger user-base and ecosystem. DeepLearning.scala ships with a tiny kernel with extensible plugins. I would recommend it if you are a researcher to create special purpose model or optimization algorithms, or if you are a learner to reproduce existing machine learning algorithms. Type-checking and dynamic neural network in DeepLearning.scala will help you in those scenarios, in comparison to other Lua or Python deep learning frameworks.
Very interesting library. How would you compare this to [chainer](https://chainer.org)? It seems to me that both are built on the principle of 'tapes', 'wengert lists' or 'computation history', which does not require a static computation graph. Chainer has less of a 'functional programming' feel, but judging from the documentation the differences seem minimal. Or would you say that DeepLearning.scala is still lacking in features/documentation, so such a comparison would be unfair? --- &gt; res20: INDArray = 2.20 &gt; &gt; The result should [be] close to 12. That's not what I would call 'close'...
At first they say &gt; They aim to solve the same problem: providing a better Java and then suddenly &gt; Scala Is More Powerful Than Java &gt; &gt; Scala never tried to do any of that that. It was designed to be more powerful than Java. Or more generally, a better language than Java. Scala was designed to do things that Java could not. 
Yes, it's similar to Chainer or PyTorch. The differences are: 1. The underlying data structure [`Do`](https://github.com/ThoughtWorksInc/RAII.scala/) runs in parallel, avoiding GPU starving. Also I suggest training more than one mini-batch in parallel for the same reason. 2. Static type checking, which means the signature of a neural network in DeepLearning.scala could be more meaningful. 3. Plugins. In DeepLearning.scala 2.0, some features become non-built-in any more. For example, CNN or Adagrad only [exists in tests](https://github.com/ThoughtWorksInc/DeepLearning.scala/blob/2.0.x/plugins-INDArrayLayers/src/test/scala-2.11/com/thoughtworks/deeplearning/plugins/INDArrayLayersSpec.scala#L22) for now. When DeepLearning.scala 2.0.0 is released, we will turn those features into Gists, reused by [magic imports](https://github.com/ThoughtWorksInc/import.scala) instead of library dependencies. In fact, DeepLearning.scala 2.0 itself will be **extremely lacking in features**, while a contributor can share his special algorithm by creating a Gist in 30 lines of code in 30 minutes. --- &gt; That's not what I would call 'close'... Fixed now. The test input was `1.30, 1.60, 1.90`, which is inconsistent from the comment.
A cheat sheet...in the form of a twenty-minute video? Doesn't that defeat the purpose?
Sounds like what I had in mind. Thanks.
X-Post referenced from [/r/akka](http://np.reddit.com/r/akka) by /u/sahil_sawhney [Monitoring Akka based applications with Cinnamon and DataDog](http://np.reddit.com/r/Akka/comments/6kx5zq/monitoring_akka_based_applications_with_cinnamon/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Does it work with akka stream?
Hi, Cinnamon is used to monitor actors, so all the actors that would be a part of Akka streaming could be monitored. I don’t think the behaviour of streaming on the whole i.e. behaviour of source and sink could be monitored. Moreover the number of [available matrices](https://app.datadoghq.com/account/settings#integrations/lightbendrp)(search under metrics tab) for DataDogs **Lightbend Reactive Platform Integration** do not show any metric for the same. In case I get around something regarding it, I would mention in comments. Thanks for the question.
I agree, that my statement is not 100% accurate - the notion of monad itself is super easy when you do just maps / flatMaps (the definition of monad) and you most likely won't avoid it even in the simple real world app on Typesafe stack. Things however gets more complex when you start to mess with Monad transformers / Free monads and Cats / ScalaZ, this is the real complex part of the language and it could be avoided by using some declarative style programming at first.
How is scala growing in specific to the AI/ML/Deep learning community? I can see libraries choosing C++ for native matrix math and CUDA(deeplearning4j), even though it is java based. Can scala native native have an impact for performance sensitive areas like these?
Sure thing, the point I was just making is that they are different designs, its like mutable vs immutable (they both have their advantages and disadvantages, and we usually pick a default which is what should typically be used) `Future` follows the idiomatic scala design where things are strictly evaluated by default unless you put it inside a thunk `Task` doesn't follow the idiomatic scala design (lazy by default), but it also has advantages due to this What I meant earlier in my comment is that the general understanding of `Future` is that its meant to be strict, so from this standpoint I don't see this as a improvement, rather a different design.
Ah, for sure. And I agree that it's completely possible to avoid those complications.
That sounds like a problem with SBT. There's a [scala-seed template](https://github.com/scala/scala-seed.g8) you can download that should set things up for you. It'd help to know what you're trying to do. Scala doesn't have to be complicated, but the tendency to be functional and use FP terminology can make it opaque. My advice: keep it simple, and use IntelliJ IDEA to give you intentions and hints. You can wade into the deep end of the pool when you are ready, but the books (Scala for Impatient, Programming Scala) were a big help for me starting out.
Thanks very much!
Believe it or not, non-verbosity is one of Scala's strong suits. Scala's all about the virtues of a strongly typed language with the presentation and ease of use of a weakly typed language. Obviously it doesn't get all the way there all the time. It's no JS when it comes to *just figuring it out*, but I think you'll find that the language has considerably fewer warts by virtue of this. To be fair, the language has its pain points. Off the top of my head: Enums and Futures feel weak... and it can often feel like a huge pain when these great new language features come out, but you can't use them since the new release isn't binary compatible! These are symptoms of a still growing language with maintainers who are more big-picture than little-picture thinkers. It means a lot of the day-to-day creature comforts can go by the wayside, even though I think this mode of thought is good for us as programmers in the long run. Criticisms aside, I think Scala is here to stay. Scala's problems are frequently overcome by amazing and well maintained libraries, notably Akka and Cats (yes, I know I'm inviting contention by saying these libraries "solve" Scala's problems). Moreso, people really do *love* Scala, so much that they're porting it everywhere! Scala.JS is nearly at v1.0 release now, and Scala Native, while still young, is already usable. Hell, Ammonite even brings Scala to a shell environment. If anything, Scala is a great language to learn because it has so many possible applications now.
Thanks. I have never started to experiment with NNs because most of the tools are in python and I cannot be productive without a type system. Because of that, I'm even more happy to see something implemented in Scala :) Probably I will have to find some time to experiment with it soon.
&gt; SBT kept failing... Proxy issues or just wrong config? I recommend templates until you understand sbt. But you don't really need to understand sbt since it's just plain settings in a Scala dsl. &gt; the syntax at times looked ridiculously verbose Wrong language, I presume. Compared to node and golang, scala is the opposite of verbose. Elixir may seem less verbose in the surface but it has its limitations in terms of expressivity. &gt; Are you guys enjoying Scala? The reason I consider scala a good tool is because it lets/forces me to write good code and I can use it for a wide variety of domains. When I write something in Scala I expect it to work but when I use others I just hope. Once you start grokking Scala I think you'll feel the same.
Given that you're a front end dev working on React and REST APIs, something like https://github.com/Daxten/bay-scalajs.g8 or https://github.com/KyleU/boilerplay may be a good fit -- I don't work on Scala.js myself but I hear it's very popular.
I really enjoy it, I started learning it around autumn. I don't have problems with SBT either, it works for me. Now it's a pain to look at imperative code, especially when it could have been functional too, which is usually more concise as well. At this point it's hard to understand how most people are still stuck with for loops, when we have map, filter, etc.. I don't want to write stuff over and over again. (The other part of my team writes these for loop forests) The other thing which I noticed is after working with Either and Option I'm better at being aware where stuff can go wrong even if I don't use an environment where these are available. So it's a really good experience so far. Not strictly just Scala, but FP in general. Compared to the languages I used before it's also not more verbose (C++, Java, Python, Perl). I would be interested to know what you found ridiculously verbose. Obviously you have to define some classes for data, which you don't have to do with dynamic languages, but when somebody else looks at it later it's not necessary to run the code to figure out what's going on. 
&gt; Scala's problems are frequently overcome by amazing and well maintained libraries For relief from pain caused by weak Enums look to: [Enumeratum](https://github.com/lloydmeta/enumeratum): "a type-safe, reflection-free, powerful enumeration implementation for Scala with exhaustive pattern match warnings and helpful integrations".
I think compared to the rest of the Scala ecosystem, SBT is one of the harder (but necessary) parts to learn. I'm really enjoying the language and its been great seeing how well typed FP works for both big and small teams. The IDE situation still isn't great but a lot of us just use Vim/Sublime/Atom/Emacs with a REPL and do just fine. The language is powerful and has a lot to offer, but this means frameworks are much less necessary. Our team at Verizon writes web services, network servers, and batch processing, and they all pretty much use some combination of http4s, monocle, Scalaz, and a few custom ones. 
I would guess that the source of the verbosity you encountered may have been due to the approach to dynamic/static typing that Scala has, as well as its medium amount of type inference (Haskell has more type inference than Scala due to different constraints and trade-offs). Scala is very much a statically typed language, and it has a very extensive/expressive type system compared to some statically typed languages such as Go which has a more limited type system. This means among other things that Scala in some ways will take more time to learn, and that the maximum possible abstraction level while staying inside the language is higher[1]. When all else is equal, using a lower abstraction level is preferable. It is therefore important when writing Scala to be conscious of being careful with using abstractions beyond what makes sense, see also [Li Haoyi's blog post Principle of Least Power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) (though also don't shy away from abstraction when it makes sense to use it, there is for instance the [scalaz](https://github.com/scalaz/scalaz) functional library). One of Scala's focuses is to seek to keep language features and functionality as libraries, and avoid special cases. Compared to Java, this means that arrays are just another class type, and that there are no primitive types; compared to Go, it means that there are no built-in collections that have generics, instead the generic collections are just provided as libraries, for instance [Seq](http://www.scala-lang.org/api/current/scala/collection/Seq.html) from the standard library. In general, some of the pain points that people discuss are off the top of my head compilation times (these have been worked on and have improved considerably over time, and are planned to improve further with [Dotty](http://dotty.epfl.ch/), the next major version of Scala - Dotty offers many considerable improvements and additions, as well as removals of deprecated features, and full automatic conversion tools are planned), compatibility between versions, considerable learning curve, and multiple others (EDIT2: I don't think it would be fair not to mention other discussed pain points. There has been considerable and at least in certain cases at least partially valid criticism of Scala over the years, and some of the more recent and considerable criticisms came from Simon O., a major contributor over the years, see for instance [one of his github.io sites](https://soc.github.io), [an r/scala thread](https://www.reddit.com/r/scala/comments/6bhrxo/six_years_of_scala_development_departure/) and [an r/programming thread](https://www.reddit.com/r/programming/comments/6bh8xv/leaving_scala_after_six_years_of_development/), as well as more recent commentary in other threads). Of the newer developments, Scala seemingly continues to grow ([indeed.com](https://www.indeed.com/jobtrends/q-Scala-q-java.html) and [redmonk.com](http://redmonk.com/sogrady/2017/06/08/language-rankings-6-17/)), [Scala.js](http://www.scala-js.org/) is going strong and is seemingly one of the more popular compile-to-Javascript projects out there, Scala Native has relatively recently been started, and the before-mentioned Dotty is under heavy development. As a final note and as you note, the somewhat natural and expected bias of the programming language sub-reddit has to be remembered of course. [1]: The reason why I mention "[...] while staying inside the language [...]" is because that the more limited a language's type system is, the more cases there are that it is difficult or not feasible to find a solution that stays inside the language's type system, and where direct casting may not be desired. In such cases, common approaches are to use code generation or similar techniques, whether through macros, special compiler additions, annotations, bytecode manipulation/generation, generated code from XML-files, reflection as in Java, and the like. These approaches effectively extend the language. While these options are good to have, it is often preferable to avoid them when possible and stay inside the language, since they frequently tend to be costly in regards to issues such as maintenance. This is also one of the arguments in favour of languages with more extensive type systems. But there are significant trade-offs in general for a language between dynamic typing, static typing with a limited type system, and static typing with an extensive type system. EDIT: Minor fixes. EDIT2: Added more pain points.
How does scala force you to write good code? Don't misunderstand me I love scala but you can really hack some crazy shit together in it. 
Agree , except for the part about IDE situation. Scala in IntelliJ is amazing! You get to see the type of everything simply by selecting it or clicking on it. You can also goto all usages of a particular function. It makes refactoring in Scala amazing! (compare that to something dynamic like js where refactoring is Russian roulette)
I haven't tried IntelliJ with Scala myself, but my impression was that it wasn't great, as discussed a little bit in this thread: https://www.reddit.com/r/scala/comments/5eu9if/intellij_vs_ensime/.
Java comes with 4 garbage collectors (http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html). The default is the ParallelGC, which is spinning up those other cores. You can call java with -XX:+UseSerialGC to use only a single vcpu, but you'll pay a performance penalty if the application is longer lived.
On the whole, it's pretty amazing, and constantly improving.
I guess verbose is relative. Ain't Java at least. But the syntax is one thing at least, that is unlikely to change in a major way. (Although there is proposal by Martin Odersky to remove braces.)
1) Variables cannot be declared and left un-allocated 2) Encouraging immutability i.e val vs var 3) Case classes prevent silly mistakes - https://www.youtube.com/watch?v=P0mR4i9u-RQ 4) No static keyword and object instead. Cleaner separation of static vs instance 5) String interpolation 6) Good separation of OOP and FP concepts Just off the top of my head. There are many others. 
SBT is awful and at this point I assume it always will be. I just ignore it and use Maven. Scala is still a lot of fun. I generally find it very concise - I can write code that looks like Python but have type safety. It's got a lot of power and while I was productive from day 1 it very much rewards learning - I've been doing Scala for 7 years now and I'm still finding new stuff. It's only very recently that I've started to struggle with the limits of the language.
Guys, /u/sjrd and other maintainers and contributors, thank you for fantastic work and absolutely awesome library!
it has defeated! removed the cheatsheet notation from the next presentation!
Note: Added some more pain points.
1) Yeah they can. trait Foo { val foo: Int println(foo) } class A extends Foo { override val foo = 42 } new A() This prints 1 (whether foo is a var or a val). foo is declared, not allocated, used by the println statement, then set to 42. Or, if you'd rather: foo is allocated twice, once to the type's default value, once to the desired value. And so you have an immutable value that mutates.
I am not an expert in scala to any length, but you cannot instantiate a trait right? The main reason for not allowing undeclared variables is to avoid nullpointer exceptions. But that doesn't prevent you from creating an NPE. It just mitigates it. &gt; This prints 1 (whether foo is a var or a val). This prints zero for me? Isn't zero the default value for an integer in JVM? In this way you can easily overcome. A more better example would be with a string. trait Foo { val foo: String println(foo.length()) } class A extends Foo { override val foo = "Testing" } new A() This gives a Null Pointer Exception. As I said, these are only ways to mitigate and not completely restrict users. &gt; Or, if you'd rather: foo is allocated twice, once to the type's default value, once to the desired value. And so you have an immutable value that mutates. I don't quite seem to understand this. `val` is a compile time restriction and not run time, so there is no problem of immutability here. Would be helpful if you can explain on how this breaks immutability for `val` 
So I agree with you, but I think the overarching idea from what you wrote is that immutability is much easier in Scala than it is in a lot of other languages. Immutability is, in my opinion, an easier concept to design software around than a system that mutates state. It has less side effects, and overall produces more consistently valid and predictable software. If you combine it with some FP ideas and keep functions pure you can (and I'm not an expert) remove almost all side effects from code leading to very predictable software, which everyone loves.
First, you're absolutely right, it prints 0, not 1 - I modified my example to make it simpler and failed to update my comment. Apologies. Second, it does break immutability for vals, at least a bit: trait Foo { val foo: String println(s"from foo: $foo") } class A extends Foo { override val foo = "Testing" println(s"from A: $foo") } new A() This prints: from foo: null from A: Testing (And this time I'm sure, see [here](https://scastie.scala-lang.org/nrinaudo/0MkrZauKRb6jQnYSZaK1cg)) `foo` is either two different values, which is not at all intuitive, or a value that mutated (exactly once, but still).
I understand now. I think this problem is due to the runtime i.e JVM. I am not sure if this was already discussed or perhaps a [SIP](http://docs.scala-lang.org/sips/) would be better? 
It's not just immutability. To be able to write practical systems with pure functions you need monads, you need HKT, you need a bunch of higher-order functions, you probably want some form of dependent typing.
Sorry, but I think this is terrible advice. After using sbt for a while I was forced to use Maven again on the last project, and it's just so much worse. I think sbt's subprojects are superior to Maven's submodules. sbt feels much faster, and much more optimized for Scala (e.g. cross building).
I find Maven modules much better to work with because there's a lot more consistency between a multi-module project and a single-module project. SBT feels slower to me, because its ivy-based dependency resolution tends to run a lot slower, and it takes a while to start itself. It's probably faster than Maven for a `~goal` workflow but I've never found that workflow very useful (for the edit-test loop I just use my IDE, I only use Maven or SBT when doing a full build or release so I always want a "cold start"). It does allow goal-level parallelism rather than module-level parallelism, but I prefer to structure my project with the dependencies at the module level (because again it's more consistent - if you have a bunch of different goals in a bunch of different projects then you have two different mechanisms for doing the same thing). Agree that if you need to cross-build SBT has better support for it (though only at the scala-version level - if you want to cross-build against multiple versions of some library or against both cats and scalaz you're back to the same kind of horrible hacks you'd use under maven). That's the only case I use SBT for. I find it's pretty rare to actually need that functionality though - for applications or small-scale libraries you just use a single version, and for anything internal to a single organisation you do your Scala version migration all in one go, so the only case where you really need cross-building is big general-purpose libraries that you publish for use by people outside your organization.
Oh sorry, I didn't mean to imply I had found a new issue! This is well known and has to do with initialisation order - super classes are initialised first. You have ways to work around it and it's not as huge a problem as it might seem, I was just trying to say that _variables cannot be declared and left unallocated_ is not strictly true, even if it's mostly true.
I believe this is a known issue, it is discussed a bit in this [FAQ](http://docs.scala-lang.org/tutorials/FAQ/initialization-order.html). There is a compiler flag `-Xcheckinit` that throws an exception at runtime when this is encountered, but its usage is discouraged for various reasons. I believe that this is a not so nice corner of Scala. I am uncertain about the best approach; the FAQ I linked to mentions some approaches. I rarely write a lot of body initialization code myself, so I don't experience it myself as much.
Thanks for pointing this out. Yeah there are a few cases. I am interested to see how much of this goes away with scala native.
I don't think scala native can solve this - initialization order is specified to happen in a particular way and there are patterns that rely on that (e.g. early initialization as used in the cake pattern), so to support existing scala code scala native has to follow the same order.
I learnt scala from scratch with a little bit of java background. I was motivated from odersky's talk "Working hard to keep it simple". My point is even though you might not directly benefit from scala i.e using it in current project or getting a job, it can help you learn certain programming concepts which can definitely make you a better programmer. Another thing to note that you can use the imperative side of scala pretty easily. What I generally do is re-write java to imperative scala and then slowly bring in FP concepts and re-factor. I am not sure if this is a good approach, but it works for me. There is no better language to learn FP according to me. The JVM is a very mature platform to develop on and move to production. Proving it in production is in my opinion the best way to convince people. Tooling is improving with each release, but not at the speed one would typically expect. There are always certain minor gripes here and there, but overall it just works fine. For SBT : Make sure you don't click the sources checkbox in Intellij. This imports the source code of every library which takes a huge amount of time and I dont know why. It is always easier to look at the source online if situation comes. 
Yeah, not this one in particular. In general, certain things like native library integration and such. There is a lot more freedom in terms of the runtime in scala native. So anything related to that can probably see improvement.
Trait parameters in Scala 3 cannot come soon enough :-)
Yep. Definitely amazing overall. But the cons mentioned in that thread is valid though. It does sometimes deviate from scalac which is annoying. Especially if you do very hardcore type-stuff. Annoying, but it's still better to have 99% analyzed correctly (and 1% wrongly) than to have no analysis in your IDE/editor at all. imo. you'll just have to ignore the red wiggly lines that one place (and maybe file a bug to jetbrains). As for the cpu-usage/slowness, that's true when working with large files with heavy usage of implicits (it seems to me). very annoying, but perhaps it can be worked around by not making files that are thousands of lines + stay away from too much use of implicit convertions &amp; implicit classes. Or you can just disable type checking when working in a slow file of course (IntelliJ is pretty good as a editor as well (without any of it's psychic IDE-capabilities.). Also the reason it slows down might be that the problem it solves (recurisve implicit resolution) is a really demanding problem I'm general. I catch scalac also using several seconds or sometimes minutes compiling the same files, so maybe it's Scala(c)s fault, and not IntelliJs?
&gt; How does scala force you to write good code? As others have said it's all in the declarative+functional style - it makes your code elegant. When I write inelegant/bad code in Scala it's like spilling coffee on my shirt :D. And there's more: you can go further with compiler flags and linters like [this](https://gitlab.com/stevendobay/rc/blob/master/scala/build.sbt). &gt; I love scala but you can really hack some crazy shit together in it. Is there a turing complete and practical language where you can't? I mean there's a golden mean between having a restrictive/dumb language and one that let's you do mad stuff(lisp with dynamic typing and non-hygienic macros...) if you want good balance between safety and flexibility.
Option is there to stay, since it works different then A | null Since Either is now right-biased that will stay also (since it's different to A | B)
&gt; but with Option the compiler forces you to deal with error situations. Same with `A | null` in TypeScript - if something is nullable, the compiler forces you to handle it. &gt; When using Option/Either/Try or something you should never have a null, not using them to deal with nulls, ideally your code should not have nulls at all. Is I understand it, Option is a safer/more ergonomic abstraction over null. Either and Try should not have nulls.
Could you elaborate on both of those points?
for example, both are monads (see monad laws) so they are completly different to the union types
As far as I understand the unions coming in dotty are not disjoint, and therefore aliases for `A | null` or `A | B` would not be a replacement for `Option` or `Either`.
I think writing typed JS (Flow or TS) in VS Code is a much better experience than writing Scala in IntelliJ.
The problem is that a thing may be null. One solution is to wrap that thing in an Option, another solution is to use a disjoint union of that thing's type and null. I'm not sure I understand why options would necessarily stay- is it because the companion methods are convenient, and a union wouldn't have those same methods available?
&gt; The problem is that a thing may be null. One solution is to wrap that thing in an Option, another solution is to use a disjoint union of that thing's type and null. That is were your are misleaded. The problem is not only that things may be null. E.g. another problem is what happens if multiple things are maybe null and we want to work on them at the same time if all of them are there or else fail fast. Or what if we have many methods that maybe return null and we want to call them, one after each other and feeding each result into the next function - and fail fast if one of them return null. How do union types help you there? They don't. You have to implement this yourself. And as you do, you are reinventing what `Option` already gives you.
Excellent explanation, thanks!
I can imagine writing something like: type MyException = A | C | D def f: Either[MyException, String] but of course with more functions and exception union types. 
Much amusement to be had: type WTF = (A Either B) | Option[C] | (D | E) 
&gt; not disjoint Umm... what does that mean, practically?
Will `A Either B | C Either D` be `(A Either B) | (C Either D)` or `A Either (B | C) Either D`?
heh - not at the type level but I guess it could mean all kinds of things from a modelling POV.
A disjoint union is one where a value is unambiguously tagged with either the left type, or the right type. A non-disjoint union can simultaneously be both left an right. In the case where that is true, how do you reason about those cases and enforce the possible overlap? $ dotr Starting dotty REPL... Welcome to Scala.next (pre-alpha, git-hash: 05aeeb5) (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112). Type in expressions to have them evaluated. Type :help for more information. scala&gt; trait Foo { def foo: Int } defined trait Foo scala&gt; trait Bar { def bar: String } defined trait Bar scala&gt; case class FooBar(foo: Int, bar: String) extends Foo with Bar defined class FooBar scala&gt; FooBar(7, "Hello World"): Foo | Bar val res3: Object = FooBar(7,Hello World) scala&gt; res3 match { case f: Foo =&gt; f.foo.toString; case b: Bar =&gt; b.bar } val res6: String = "7" However with Either there is no ambiguity. scala&gt; Right(FooBar(7, "Hello World")): Either[Foo, Bar] val res9: scala.util.Either[Foo, Bar] = Right(FooBar(7,Hello World)) scala&gt; res9 match { case Left(f: Foo) =&gt; f.foo.toString; case Right(b: Bar) =&gt; b.bar } val res10: String = "Hello World"
IMO Option enforces some code bureaucracy, which I think is absent in Swift and Kotlin. Besides, Option does not really solve the null problem. A variable of type `Option[T] may be null, None, Some(null) and Some(t) for some non null t that is a T. So I hope that for Scala 3.0 an approach is taken like in Swift or Kotlin.
Agree with Daxten on `Either`, that was one of my main motivations to get it finally right-biased. An unbiased `Either` would have been awkward as we would have had a tagged and an untagged union type. Not so sure on `Option`. If Scala gets only the features necessary to finally state the obvious "95% of my `T`s are not nullable" then everything will be fine. But if they go through with `T?` as shown on some slides, it will not be the outright death of `Option`, but it will split the community with a lot of beginners picking `T?` over `Option[T]`/`Either[S,T]`/`Try[T]`. My guess it that they will go with `T?` based on the [mistaken assumption](https://www.reddit.com/r/scala/comments/6lfltr/when_dotty_is_out_will_people_still_use_option/djufxrr/) that it will make Java interop less painful.
If I have `T | null`, what happens if `T` itself is `Int | null`? `(Int | null) | null` collapses into `Int | null`, so you can no longer distinguish "which layer" became `null` On the other hand, if I have `Option[T]` and `T` is `Option[Int]`, `Option[Option[Int]]` can be `Some(Some(Int))`, `Some(None)`, or `None`. `Some(None)` and `None` are distinct. This has practical ramifications, e.g. if I have a dictionary with a `.get` method, it could be either of trait Dict[K, V]{ def get(k: K): Option[V] def get(k: K): V | null } But if - I go with `def get(k: K): V | null` - `V` is `Int | null` - I call `.get(k: Int)` and get back `null` I do not know whether or not `k` was not present in the dictionary, or present with a value of `null`! We like to think of `null` as a "special value" or "code smell", but despite it being a code smell I'd like to have my generic code be able to handle other people's smelly values transparently. If someone puts a `null` value into my dictionary, I'd like them to be able to properly check if it's present, like any other value. On the other hand, if - I go with `def get(k: K): Option[V]` - `V` is `Option[Int]` - I call `.get(k: Int)` and get back `None` I know that `k` was not present, and if someone had explicitly placed a value `None` into that dictionary, `get` would have returned `Some(None)` Practically, `T | null` works for simple cases, but it fails for more complex cases where "someone else" might also be using `null`, since there can only be one `null` value and you're not sure "whose" `null` it is. On the other hand, since `Option[T]`s can nest, if both you and "someone else" are using `Option`s, you get back a nested `Option[Option[T]]`, and there's never any ambiguity between `Some(None)` (meaning the inner `Option` is `None`) and just plain `None` (meaning the outer `Option` is `None`)
Yes, because I like words and not symbols.
Do the union types in dotty have a performance overhead at runtime? Are the values boxed, for example? If they have no overhead, I would use them for performance critical code, and accept the slight loss in expressiveness.
Two questions, what makes T? different from Option[T] (i.e.: why are there methods that Option have that T? couldn't have) and why is it mistaken to think it will make java interop less painful?
I am deserializing a JSON object, in which a nested object will contain a List of Strings, Ints, Doubles, or Booleans (all values will be only one of these types). case class Document(id: String, fields: List[DocumentField]) case class DocumentField(name: String, values: List[Any]) I'm trying to figure out the best way to deserialize this tricky list using circe. I am guaranteed that all items in the list will be of the same type. I know I could create some value case class wrappers, but I'm not experienced enough with circe to know how to make the custom decoders or if that is even the correct approach.
Just to put my $0.02 in: [Coq](https://coq.inria.fr/) was the right tool to do that presentation with (taking advantage of [Peacoq](https://github.com/Ptival/PeaCoq)) because it's a type-theory-based proof assistant, so on one hand I could make the chain of inferences explicit using "tactics,", since I was trying to do a quick formal logic intro, but on the other hand I wanted to lay some groundwork for the Curry-Howard Isomorphism and provide a backdrop for [Miles Sabin's wonderful unboxed unions post](https://milessabin.com/blog/2011/06/09/scala-union-types-curry-howard/), so the type-theory foundations mattered. In principle, I could do the same thing entirely in Scala. The problem is that scalac isn't an interactive proof assistant. Even with a tool as good as the [Ammonite REPL](http://ammonite.io/#Ammonite-REPL), doing type-level programming interactively is a pain, and you don't have anything like Coq's tactics to help you. That said, the [necessary ingredients](https://github.com/milessabin/shapeless/blob/shapeless-2.3.2/core/src/main/scala/shapeless/package.scala#L30-L60) from the Curry-Howard perspective are indeed all there, so if someone wanted to flex their Curry-Howard muscles in Scala a bit, I don't want to discourage them (more). :-) But for those who aren't masochists, I recommend [Idris](https://www.idris-lang.org/).
Option is not an abstraction over null. It's a better way of solving the problems people use null for.
If I am using Playframework's `ExecutionContext` instead of Scala's default `ExecutionContext`, whether this `blocking` construct do the optimisation for me? Thanks.
As people have noted, `Option` and `Either` are disjoint unions whereas `|` is not. There are advantages to disjoint unions, so `|` is not a replacement of `Option` and `Either` but a complement to them. When should `|` be used? Probably less than most people think. I see two main use cases: - In performance-critical code. A `|`-union between reference types avoids creating an extra object. - When the underlying property is not parametric anyway, e.g. for modelling nullability. `String | Null` is a precise type for modelling a string-returning function we get from Java. Or, interop with Javascript where union types are ubiquitous. So `Option` is definitely here to stay. I am personally much less fond of `Either`. It uses obscure names and conventions ("right-biased Either", really?) and does not fit well into the hierarchy of sum types. I would much prefer if we could replace it by something like Rust's `Result` type, with `Try[T]` a subtype of `Result[T, Throwable]`. But for the moment that's just a personal opinion, and I have not yet started to think about how to propose to migrate from one to the other. 
&gt; Prediction is very difficult, especially about the future. -- Niels Bohr One of the exiting things about new features is speculating about how they will be used, and then watching how wrong we were and what wonderful un-thought of variations experience will bring us. Will people still use Option and Either? I don't know. There are good reasons for it, companion objects being one of them, the fact that `Option[Option[A]]` is something different than `Option[A]`, but that `A | null | null` is just `A | null` - in other words, it being a very similar but fundamentally different abstraction is another one. But maybe it'll turn out that that in a large amount of cases the differences are uninteresting, and the differences that make them look like a different beast entirely now will turn out not to really make a difference. My money is on them remaining important data types in their own right. Here's to the future proving me wrong, in new, creative, exiting and enlightening ways.
&gt; what makes T? different from Option[T] (i.e.: why are there methods that Option have that T? couldn't have) `T?` (aka `T|Null`) doesn't allow nesting (`T??` != `Option[Option[T]]`), this means that all methods that work with some kind of nesting cannot be implemented on `T?`. &gt; why is it mistaken to think it will make java interop less painful? The issue with Java is not nullable vs. not nullable, but unknown nullability vs. known nullability. This is the hierarchy people believe we have: nullability / \ / \ / \ T T? non-nullable nullable This is the hierarchy we actually have, considering Java interop: nullability / \ / \ / \ known nullability unknown nullability / \ | / \ | / \ | T T? ?? non-nullable nullable all Java types It looks really enticing to dump "unknown nullability" into one of the categories of "known nullability". It has been tried before, and it works terribly. This is the reason why e.g. Kotlin kept having substantial changes to the way they handled it until settling on "well, there is nothing really we can do here" with platform types. Scala will probably pull a NIH and repeat all failed experiments again.
As TypeScript dev at day, and Scala dev at night I can say, we are using https://www.npmjs.com/package/ts-option in TS. Its much more convenient to use `map`, `flatMap`, etc. then other structures. EDIT - And yes, that means (in my opinion) `Option` isn't going anywhere. About `Either`, I missed them in TypeScript but with Scala pattern matching it can end we don't need it anymore. 
&gt; Scala will probably do NIH and repeat all failed experiments again. I don't think so. Scala didn't introduce it for the sole `A | null` purpose.
I don't understand.
*compiler and libraries :)
Why do you think that Scala will try to put `unknown nullability` into one of the other categories? Currently it puts this burden onto the developer who has to access Java APIs with care and use `Option` where needed. I don't see why this would change.
A monadic wrapper wouldn't be very hard to create though. implicit class MonadicNullable[A](a: A | Null) { def map[B](f: A =&gt; B): B | Null = if (a == null) null else f(a.asInstanceOf[A]) def flatMap[B](f: A =&gt; B | Null): B | Null = if (a == null) null else f(a.asInstanceOf[A]) def withFilter(p: A =&gt; Boolean): A | Null = if (a == null) null else if (p(a.asInstanceOf[A])) a else null } Note that this doesn't actually work, or at least it doesn't work without extra ceremonies in scastie: https://scastie.scala-lang.org/W622uoZjRmipI5xIvLqpUw I think the signatures should hold but there's a huge loophole with any type potentially being null without having the signature `SomeType | Null`. For instance, the assignment `val boo: String = null` is valid.
If types were non-nullable by default you'd have to instantiate the `Dict` as `val d = new Dict[Int, String | Null]` in order to have this ambiguity, as it wouldn't be possible to store nulls in a `Dict[Int, String]`. It'd be bad for Java-&gt;Scala interop though, but runtime guards could be added I guess..
That's true, but on the other hand, it doesn't really change anything. If I have `trait Dict[K, V]`, `V = String | Null` is just as valid as `V = String` or `V = Int`, so having it not work still isn't great. In fact, this ambiguity extends past `Null`s: the same applies to `Either[Either[Int, String], String]` versus `(Int | String) | String`. In one case, it's ambiguous "which" `String` it is, while in the other, `Left(Right(s: String))` is clearly distinct from `Right(s: String)`. So even `null`s did not exist at all in the Scala language, you'd still bump into the same ambiguity with perfectly normal code and types
Currently `T` stands for all three things. (This is not related to wrapping things in `Option`.) If you now restrict `T` to mean "definitely not nullable", you need to have another way to represent the other options. And if you say `T` is "definitely not nullable, except when you get it from Java" then most of the benefits of `T` just went right out of the window. So it's kind of inevitable to have more than `T` and `T?`, but at the same time everyone tries to get away without introducing yet another concept. 
&gt; If you now restrict T (...) But how do you come to think that anyone would do that? Is there a proposal for Dotty or something like that?
What happens if I call it with `_.flatMap[Int | Null](...)`? I guess that's not possible?
I actually agree with your opinion regarding `Either`. The fact that its right biased doesn't make any sense (like why right, why can't it be left?). Its just something that was copied over from Haskell. The thing is, `Either` originally was meant to represent a union type which is why it had no bias. If you wanted a right biased either (usually because you are representing a Error | Success scenario) then this is what `Try` is for (which also does have monad operations).
why not def withFilter(p: A =&gt; Boolean): A | Null = a match { case x: A if p(x) =&gt; x case _ =&gt; null } ?
Yes, isn't that the whole point of `T|Null`? Otherwise why even bother?
I think it should be possible, but it doesn't communicate the result the same way that A =&gt; Option[Option[B]] would, obviously. You'd get a null but it wouldn't be a "nested" null.
You are right in one of possible "Error | Success" scenarios. When Error equals to `Throwable`. But when Error is not `Throwable` -`Try` wouldn't work. What if Error is `String` (like "You not allowed to get this information now, try tomorrow")? I am not defending right-biased Either, I'm saying that `Try` can't do `Either`s job.
I wasn't sure it would work. I suspect A is currently implicitly typed `A | Null`, since this assignment is valid: `val s: String = null` It might still work if matching against `x: Null`first, and `x: A` second, I'm not sure. As null is currently a subtype of all reference types it might still match. `case x if x == null` should work, but then it's almost the same as an if-match. I decided to finish my example instead of arguing with myself over these things ;)
Isn't unknown nullability a perfect fit for existentials? Types going to/from Java are `F[T] forSome {type F[X]}` where we have some constraints on `F`.
The more I see this kind of example the more I think non-disjoint unions are just a bad idea. Are the use cases really compelling enough?
I would love to see `Either` replaced with something with `Result`-like names that fit the common use cases better. But I would like to keep the convention of having the "freeest" type parameter on the right - I find that helpful as a reader, even if it's no longer so vital to the compiler post-SI-2712.
In which case I suppose it violates monad laws.
&gt; Yes, isn't that the whole point of T|Null? Otherwise why even bother? But there is also `A | B | C` and so on, not only `T | Null`
"`forSome` is going away" is the last thing I heard about it regarding Dotty. I don't think it can be handled gracefully within the type system, it's a bit like multiversal equality: sounds nice, will fail in practice. Sticking to the "minimal" parts of non-nullability will be hard enough as-is as it fundamentally changes the type hierarchy, and I don't think people have really thought about the consequences yet.
Hmm, I was first thinking that it would be nice if `def Dict#get` would have a return value of type `V | Null`. Not having to wrap values in `Option` internally could improve performance. However, instantiating `Dict[K, V | Null]` would make the return signature of get `V | Null | Null`, which is ambiguous. Even worse, `Dict[K, Option[V]]` would make it `Option[V] | Null`, which looks bonkers. Still, I imagine the ability to use nullable types coupled with "monadic" operations over them could be a nice optimisation and lower GC churn quite a bit.
I hope it follows the nominal precedence rules that applies to operators, or a subset of them.
Sure, but it has already been said that `A | B | C` probably won't see much usage. I'm not surprised considering that Dotty intentionally doesn't support union types in exactly those cases where they would have been most beneficial.
Option do conceptually solves the null problem but the type system doesn't currently enforce that nullable types are annotated as such. This is something I really hope will be introduced with Dotty/Scala3
The same applies to `A`. You'd have to restrict it to non-nullable types. I think a more promising approach is to keep all the convenience of `Option` and have the compiler optimize Options of non-nullable types away where possible. This may be tricky with separate compilation and Java compatibility though.
because right is "right", right? Think of it like "RightAnswer" and "Wrong(Left)Answer" still, I would also like to see a Result type in the std lib (instead of either)
I was aware that `T|Null` wouldn't nest for obvious reasons, your diagram is absolutely right and what I had in mind, what I wasn't aware of is that `T?` is a failed experiment in languages such as Kotlin, since all I've heard about it by its users are praise. It's part of the reasons that get cited always for picking it up, for instance by SquareEnix's statement on picking Kotlin and by Google's statement on picking Kotlin. Here's a bit of my own survivor bias, since I've been doing Scala for about 6 years now, I have had to deal with nested options about twice, and both times it was so awful I refactored the code so that didn't happen anymore. Thanks for the answer.
That's where `Result` mentioned above comes into play.
Maybe, I'm not sure. Tried firing up a dotty repl for experimentation but the typer is still too shaky to handle type disjunctions and crashed or reported odd results. Perhaps it's not important if it do. IIRC Future also violates monad laws but is commonly used in a monadic fashion.
&gt; the syntax at times looked ridiculously verbose I guess you have not seen Binding.scala. [Binding.scala's TodoMVC application](http://todomvc.com/examples/binding-scala/) has the least code size among all the TodoMVC implementations, only one source file, 154 lines of code, comparing to ReactJS's 488 lines of code. 
&gt; what I wasn't aware of is that T? is a failed experiment in languages such as Kotlin, since all I've heard about it by its users are praise. People don't complain about `T?` because it is not used for Java interop. In Kotlin, Java types are translated to "platform types" `T!` which act mostly like `T`, not `T?`.
Maybe I'm simple minded but I don't quite see the issue. Primitive Java types are by definition non-nullable so they are inferred as `T`. Reference types are inherently nullable and would be inferred as `T | Null`. If so desired a simple Scala annotation on the call-site could force the inferred return type to be just `T`, if that is what the Java-method promises. If a null leaks in due to the annotation that's no different than what happens today, but by default all nulls will be expected.
Yes, but Future violates it because its not lazy. Which is less of a real-world-problem (but still bad!) than what happens in the case of your wrapper type.
Been working on a blockchain client to facilitate investment and development in emerging economies -- current iteration runs on an [akka-based framework](https://github.com/ScorexFoundation/Scorex). Just released our [testnet](https://medium.com/topl-blog/one-step-at-a-time-building-the-first-global-investment-protocol-4ad4a7d4c407) for the project, with bootstrap nodes up on AWS. You can take a look at [our codebase](https://github.com/Topl/Project-Bifrost). It's fairly dirty right now, but would love contributors or otherwise!
Wouldn't `Int | Null` would be a nonsensical type argument to `flatMap`? I'm not sure it could even occur in practice if disjunctions are compacted.
Yes, Kotlin tried that and they completely abandoned it, because the ergonomically issues are huge. If you write `T?` you basically have a type with a proof obligation where you have to show that the type is not null before using it. How to discharge that proof? This can either be done - **statically**, with a cast. This is bad because it is highly prone to fail if you ever refactor your code. - **dynamically**, with a null check. Here's the kicker: This will only work for "simple" types. Scala has no generic way to check whether `Foo&lt;Bar?, Baz&lt;String?&gt;?&gt;?` is in fact `Foo&lt;Bar, Baz&lt;String&gt;&gt;`. Assuming that every reference type you get from Java also creates a perverse incentive: Those Java devs who accept and return nullable types all over the place are rewarded with the "right" signatures, while those Java devs who avoid null are punished with the "wrong" signatures. Annotations also don't work, as Kotlin devs had to figure out: - call-site annotations don't scale - declaration-site annotations can only be added by the library authors themselves - external annotations don't scale, as they can't be added for a library alone, they need to be written for every version of that library to be useful Also, static analysis is not very helpful either as - you guessed it - Kotlin devs imagined, because you can only infer nullability for a very small subset of Java. Basically final static fields, some final static methods and final classes. This doesn't even work for final methods, as a final method could call a non-final method somewhere. If you have a look at how Java is still written these days, it's clear that the analysis won't be very useful.
Part of Kotlin's mission statement is to interoperate seamlessly between Java, something that Scala hasn't done for a long time. I believe the stakes are higher on their end. Among other things they use all collections from java.lang.* and extend a lot of other classes with custom methods. Their non-null story is bound to be more complex than ours, right? I'm not sure we're talking the exact same semantics either. What I suggested was that `T | Null`/`T?` would be inferred for all Java-related members carrying a reference type and that this inference could be disabled with an explicit on-site annotation. `Option.apply` could carry the same annotation, which would make this a non-issue for instances where a Java return value is directly wrapped. No code-changes needed. Aside from that, a lot of Scala codebases has little or no Java STDL usage and those who do usually won't expose those members to end-users. I think it's at the very least worth considering.
From a very concrete perspective, yes. But when designing such things, one should take great care. Try to take the perspective of library designers (like cats or scalaz) where you want to work with these functions in a very generic, indirect way. Composing and referring to them in different ways. Without be able to rely on the monadic laws, this can lead to serious problems (as it did with e.g. `ListT` in scalaz).
+1 kamon is a really great library. Have been using it in Play prod for more than a year with an integration with datadog. Works great. 
Have your DocumentField take a type parameter T and assign the list to the same parameter. Define your `Decoder[DocumentField]` like so: `implicit def decoder[T: Decoder]: Decoder[DocumentField[T]] = ...` This will allow you to deserialize any DocumentField for which you provide a Decoder[T] (implicitly)
Didn't get the idea behind the pictures. Also zero-length Dev config is not clear. Upvoted anyway. 
I think he's saying that ideally you should be able to start your app with no extra config, just what's available in reference.conf should be enough to at least start the app
I found this to be a nice introduction to Scala, both concepts as well as community. Very disarming and humble approach. 
Correct. The pictures are mostly for fun, to reinforce the parallel between the "laws of config" and the "laws of thermodynamics" they were inspired from. :)
Regarding null checking: What we will try is model NPEs as effects. That is, you can still select on an expression of type `String | Null`, say, but to stay honest, you'll have to declare that you might raise an NPE. So the whole thing rests on how convenient it will be to express effects. I am hopeful, but we have to try it out and gain experience before we can say for sure. 
I hoped for it to be a convenient way for declaring sumtypes that consist of types I can't control, e.g. thirdparty classes. So basically as an alternative to shapeless `CoProduct`...
Doesn't work for ADTs (`enum`s).
&gt; I believe the stakes are higher on their end. Absolutely! And despite that, and a dozen people, multiple years and various breaking rewrites – they couldn't make it work and abandoned this approach. &gt; I'm not sure we're talking the exact same semantics either. Yes, we are talking about the same semantics. It doesn't work due to all the reasons mentioned. &gt; I think it's at the very least worth considering. Sure, but it's madness to do the same thing and expect different results. If Scala's quality standards are lower than Kotlin's so that it can adopt approaches rejected by them it should work.
I mean something like `type MyAdt = FinalThirdPartyClassA | FinalThirdPartyClassB` and then pattern matching on it won't work? :( 
No, won't work. enum entries don't have a type of their own, just the enum base type. And even if they had their own type, union type inference was disabled almost completely, with only a few exceptions.
Excellent summarization
Awesome, I just wrote my own (in progress) - https://gist.github.com/bcherny/615a2c2564965c1869ad8988bf5a8d50.
In Scala a lot of the objects are reused. if you add an element to an immutable list you do indeed create a new list, but its "built on top of" the old one. So it's not inefficient to just discard the original list. You aren't actually moving a lot of data around in ram That said, sometimes mutable lists make perfect sense. I won't go into why immutability is a good thing, I'm sure there are plenty of blog posts that explain the benefits better than I can You're scenario sounds like it'd probably be a case for a mutable list since it sounds like you are expecting many of the values to change during each update and you're not just adding or removing elements, but I'm not totally sure of your use case 
If you're used to Scala's type system and guarantees, Typescript will be a profound disappointment. It is very tangibly unsound, and its language features are unimpressive. Typescript is primarily designed to improve your existing JS codebase, and that required making painful tradeoffs in language design. If you're not in a situation where you want to gradually add some kind of types to an existing project written in JS, I would steer clear. For any other purpose Typescript is the PHP of compile-to-JS languages.
Is there a specific wart or sorely lacking feature (I know it doesn't have higher-kinded types)
Only if you want reusable constructs that compose. There are people who are happy with the ad hoc solutions listed [here](https://philipnilsson.github.io/Badness10k/posts/2017-05-07-escaping-hell-with-monads.html), and people who think that functional programming is a matter of style, and that Java 8 can do everything Scala can--only those unaware of HKT.
Nice, we were using https://github.com/cwmyers/monet.js when working with TS.
Variance for example – if I recall correctly, all type params are basically assumed to be covariant. To clarify, the compiler does not check that such covariance is sound. It just allows all of it all the time. Many, many obscure false positives. Things that should have never compiled pass without a warning and fail at runtime. Structural typing. Types `A { key: String }` and `B { key: String }` are equivalent as far as typescript is concerned. You need to enable a bunch of compiler flags just for type inference to work reliably. Otherwise you will often end up with `any` type inferred, which is a type that Typescript simply does not check at all. And even with those flags enabled you still get these problems once in a while. If you get JS developers writing Typescript, the code you end up with looks like Javascript with extra line noise, not a properly structured, typeful application, since Typescript goes out of its way to allow Javascript code style to work well. Expect to make regular use of escape hatches / unchecked casting too. Because of all this, in practice Typescript's type annotations are more like recommendations, not something you can trust. From Typescript's official [Non-goals](https://github.com/Microsoft/TypeScript/wiki/TypeScript-Design-Goals) list: &gt; Apply a sound or "provably correct" type system. Instead, strike a balance between correctness and productivity. Some more links: https://news.ycombinator.com/item?id=14473526
Damn dude, you paint a bleak picture! Thanks for the info
I'm not sure, you ought to ask somebody familiar with Play. Worst case, you could [make your own](http://www.cakesolutions.net/teamblogs/demystifying-the-blocking-construct-in-scala-futures).
&gt;but it includes all the config values required to start the app. That's all well and good, until an override file doesn't get placed correctly and you accidentally start writing production data to dev. I disagree that apps should be bootable with no config. They should fail early and hard
Yeah, I was really surprised myself when I tried working with TS. JS devs tend to like it though, but I can't get over how fragile it felt to work with...
Oh oh. Reading Martins answers, it seems you are right regarding the `A | Null` thing....
I agree. While in non-critical cases it can be better if the application just warns at startup that is configuration missing and disables the affected features, the common behaviour should be fail fast.
I'm not a fan of outright dismissing superficially-cryptic code without discussing its purpose, and I don't agree that we shouldn't use certain abstractions solely because they are unfamiliar to experienced programmers. There's definitely a learning curve to these, though. Teams have got to determine how steep theirs should be. Other than that, I thought this was a nice, approachable introduction to the language itself.
That's a good talk. The only caveat is that there is no reason to use Spring, IMO. It devolves into cross-dependent pile of doubtful architect decisions. There is a lot of replacements for everything Spring has to offer. I'd strongly recommend not to use it at all with Scala. It's still preferable choice for a lot of things in Java world. 
Spring is literally the bane of my life. Converted a Java/Spring project into Scala and now trying to rip out Spring and replace it with a more scala friendly web framework. Our main issue is with company JARS that just assume that its running within Spring.
There is no inherent need for state. In the mathematical model there is no state---everything is indexed by time. Copying gives the same semantics. The trick to make it efficient is called "structural sharing", which is what /u/_Count_Mackula discusses. The basic idea is to not copy parts of the data structure that are unchanged. When I've worked on these kinds of systems they typically end up being modelled as a bunch of large matrices. In this case copying is often not efficient. You can then pull out the dreaded m-word (monads) to retain the desirable properties of FP without the efficiency hit of copying large matrices. There are other approaches in the literature that do fancy optimisations to avoid copying when not necessary while still maintaining a nice model but none as production level AFAIK.
Personally I like `Either[L,R]` and for me the naming makes sense. `Left` is for the left type parameter, `Right` is obviously for the right type parameter and right biasing makes sense because `Right` is the "right" result, whereas by convention `Left` is used usually for signaling various errors, failures and exceptional results that needs to short-circuit the processing. I haven't paid attention to the whole thread, but here are the advantages of `Either[L,R]` versus `L | R`: 1. Many times you can't discriminate between `L` and `R` due to type erasure, without boxing in an `Either`-like type 2. `Either[?, R]` is a monad (well the right biased version is), so you get operations meant for composing operations like `map` and `flatMap`, along with fold and other utilities, whose advantages won't go away I'm currently working on a JavaScript codebase and `Either` is so useful that I've ported it, with types provided for Typescript and Flow of course: https://funfix.org ... too bad that I'll probably not be able to port `EitherT` (from Cats), but that remains to be seen. As for Dotty and future plans for `Either`, I really hope that it won't go away, as /u/Odersky is hoping. The `Result` type from Rust is basically the same thing, but it attaches unnecessary semantics to the type constructors. So instead of dealing with a `Left`, you deal with an `Err`, short for error. Well, if you care about language, "error" in this context is confusing, because you can have for example software defects, network failures, user input validation errors and they are not the same thing. Of course, "error" is the most generic term you can use for unexpected problems (throwables on the JVM), but an error that is expected (implied by giving it a concrete type) is probably not an error 😉
I can't/won't watch videos but I'm glad there are people talking about this. Spring with Scala works very nicely, and I think it's important to spread the message that Scala is a great way to do *any* on-the-JVM thing you want.
&gt; If I have an appreciable number of dynamical objects being modeled, it doesn't make sense to make copies of each one every time their state changes (unless I'm deleting the old object or something). Try it before you write it off. If you're hitting performance issues you might need to use particular techniques - you might even need to use some carefully scoped mutability - but don't go looking for trouble.
In my experience there is a good and bad parts of Spring. To many people tried to use spring for everything which result in a messy heap of everything. Spring itself is amalgamation of many paradigms and approaches and some of them have a very bad cohesion(spring-integration and java config). In the past I've had some animosity toward Spring as a whole. Now I'm just okay with it.
Kotlin's purpose was to abolish nulls while Scala's is to introduce union and intersection types. I don't think the scenarios are directly comparable. This is why I feel we're talking about slightly different issues. Union and intersection types are already slated for Dotty, the question that remains to me is whether Null should be a subtype of all AnyRefs or be an isolated singleton type — I'm hoping for the latter.
Kotlin's purpose was to make nulls safer. It doesn't abolish them, it doubles down on the pattern of using nulls to indicate errors. Nonetheless, it's completely irrelevant what Kotlin or Scala are trying to do: A compiler pretending to know more of a Java type than it actually does is a really bad idea, regardless of whether it is done in Scala or Kotlin. It is exactly the same issue, regardless of whether your nullable type is a built-in language feature or a union type. &gt; the question that remains to me is whether Null should be a subtype of all AnyRefs or be an isolated singleton type Judging from slides, making reference types non-nullable is the intended plan.
&gt; Kotlin's purpose was to make nulls safer. It doesn't abolish them, it doubles down on the pattern of using nulls to indicate errors. Poor wording on my behalf. What I was trying to express was that Kotlin is (was?) trying to leave a static guarantee that some T returned from Java-code is actually T and not T?. I can imagine this is hard for a lot of reasons. &gt; A compiler pretending to know more of a Java type than it actually does is a really bad idea, regardless of whether it is done in Scala or Kotlin. Yes! Which is why I believe the responsibility has to lie with the programmer. It's really just an .asInstanceOf or pattern-match away. To use Option as an example: `def apply[A](na: A | Null): Option[A] = na match { case _: Null =&gt; None; case a: A =&gt; Some(a) }` That Scala has an established concept of Option is a big win: Lots of interop-code is already wrapping Java return values in Option. Code at these sites wouldn't have to be touched even if Null was pulled out of the AnyRef hierarchy. Doing `def unsafe[A](na: A | Null): A = na.asInstanceOf[A]`, or using an annotation for achieving the same thing is unsafe, but only for the same reasons that make casts unsafe anywhere. They are a responsibility of the caster. &gt; Judging from slides, making reference types non-nullable is the intended plan. :D
Again, the problem is not handling nulls coming from Java, the problem is knowing which methods might return them, and which don't. Yes, you can wrap things in `Option`, but this is orthogonal and unrelated to the issue at hand: How does the compiler/developer know in the first place whether something needs wrapping or not?
I'm not saying the compiler should know this. A developer may know or be sufficiently certain from experience or documentation that a Java method will not return null, and cast the return value to a non-nullable type. This is obviously no guarantee.
`? | Null` does violate the monad laws, e.g. identity: val f: Null =&gt; Unit = {_ =&gt;} val a: Null = null point[? | Null](a).map(f) // null: Unit | Null point[? | Null](f(a)) // {}: Unit | Null `Future` doesn't inherently violate the monad laws (most idiomatic uses of it do, and it's pretty useless in pure code most (though I'd argue not all) of the time).
That's all a step after the compiler has to decide on an actual type. If it decides to assign `T|Null` to everything coming from Java, it will have all the issues mentioned earlier.
That sounds really interesting! Is there a resource where I can read more about this or look at some code examples?
&gt; The Result type from Rust is basically the same thing, but it attaches unnecessary semantics to the type constructors. Would you like to elaborate? What would be the semantics tax here? Result could be a replacement for both Either and Try, which seems pretty nice.
Great news! Why strawman?
As I said, since the difference is basically one of naming, the problem is saying that the exceptional "left" result type is an "error". Also `Result` can replace `Try` in the same way that `Either` can ... in some cases, it works, since you can view `Try[A]` as an alias of `Either[Throwable,A]`, but they are not equivalent since the behavior is different. These two don't have the same behavior: Try(1).map(_ =&gt; throw new RuntimeException) Right(1).map(_ =&gt; throw new RuntimeException) So the current `Try` is not an `Either`, or like Rust's `Result`.
As in this is intended to be a basis for discussion rather than a real proposal. (I'm not sure how true the label is in this case, but that's what it should mean)
Isnt that a micro-optimisation that's essentially redundant in a language like scala where vars are frowned upon?
I think name relates to this things: * [Straw man proposal](https://en.wikipedia.org/wiki/Straw_man_proposal) definition * [Discussion](https://github.com/lampepfl/dotty/issues/818) on github
I confused it with straw man argument.
I like the "fail fast" approach, I have used it a few times. "No config" is more of a desired outcome than a hard rule. If you're worried about starting a production server in "dev mode" because you misplaced the "production" override, you could create both a "prod" and a "dev" override, and specify in the Readme to use "ENV=dev start.sh" to start the app in development mode. My point is that you shouldn't lose time copying/pasting config values or asking other people for config values. Having to specify the environment explicitly and failing fast if no environment is specified is completely fine. Another situation where failing fast can be reasonable is if your app requires a personal API token and you want to make sure all developers use their own personal tokens when developing. In this case I would fail fast and print a clear message to stdout/stderr explaining how you should generate your personal token and where it should be saved.
Agreed, which is why I would have preferred `Try` to be changed or another alias made for `Either` for these Error | Success scenarios
I think strawman means, "this is what you're getting", and not, "let's explore various alternative collections implementations". Practically it's the best way to go (wrt to maintaining binary compatability), but the end result will likely not satisfy everyone. For example, would be nice to trim down the gigantic collections hierarchy; that would reduce Scala.js generated file size significantly, but to-date it seems like hiding (not removing) CBF has taken up much of the development time. In the end there will be many improvements but non-JVM users will probably still be paying the huge-collections tax.
It used to be "let's explore various alternative collections implementations" when it started, almost 2 years ago.
&gt; I think strawman means, "this is what you're getting", and not, "let's explore various alternative collections implementations". No. That might be the reality of the project, but it's not what the word means.
&gt; I think strawman means, "this is what you're getting", and not, "let's explore various alternative collections implementations". It means pretty much the opposite of that. A straw man is something intended to be burned. These are like saying, "This isn't necessarily what we'll end up with, but we're putting *something* out there so people can criticize it." &gt; In software development, a crude plan or document may serve as the strawman or starting point in the evolution of a project. The strawman is not expected to be the last word; it is refined until a final model or document is obtained that resolves all issues concerning the scope and nature of the project. In this context, a strawman can take the form of an outline, a set of charts, a presentation, or a paper -- [Straw man proposal](https://en.m.wikipedia.org/wiki/Straw_man_proposal)
Not all parts of a program are performance critical. For such parts it is better to focus on robustness.
Obviously strawman does not literally mean, "this is what you're getting"; in the context of how things are unfolding, however, this is indeed what we're getting -- in other words, there never was a real strawman since alternative implementations were either never seriously considered, or nothing surfaced that seemed a viable replacement for existing collections.
It may be kind of niche, but the ability to describe arbitrary subsets of a type seems useful to me.
It's not a big topic here, but that sounds very interesting! I will definitely check this out
&gt; Only if you want reusable constructs that compose. But monads don't compose. As soon as you start mixing in multiple side effects and track them with different monads, you start needing monad transformers. I'm not saying functional isn't the way to go, but it's not as clear cut as that.
It'd be nice if scalac/dotty will eventually be used as the backend for the IDE. I'd imagine it would require massive refactoring and changes + performance optimizations though.
&gt; even though I think this mode of thought is good for us as programmers in the long run. Like all things, you need a healthy balance of both. Little creature comforts attract more devs + keep more people using the language. This is valuable as higher adoption means more support/faster evolution of the tooling around the language.
95% of the time, I don't want to do anything all that crazy with my `Option`s. I want to use it kind of like an anaphoric if, and my goal is to converge the `Some` and `None` branches to some new value. I get the argument that untagged unions are more expressive, but I'd argue that it's better to reserve them for the cases where you actually need to differentiate between `None` and `Some(None)` in a nested union. As someone pointed out, collections are clear use case. It's uncontroversial to say that disjoint unions are clearly advantageous in code that's type-parametric, like library code. But I posit that untagged unions are superior for readability. Imagine being able to use `?` to effectively `map`/`flatMap` a nullable within an expression. I think it would lead to really concise business logic, without a whole bunch of ceremony. You could always lift to a proper `Option` to recover the full expressivity of the monad. ---- Regarding `Either` vs untagged unions, one thing untagged unions buys is a very easy way to capture the concept of an ad hoc union, beyond 2 items. For example, I have services that can fail in a number of ways, but it's very cumbersome to capture, say, 5 failure types in some giant either. So instead, I choose to have some giant open supertype of all my failure types, which is almost as bad as an `Any`. Untagged unions solve this with exactly the right amount of syntax.
The problem with the scalaz / cats people is that they claim their way is the "one true way" of doing FP in Scala (which is obviously untrue, since none of that stuff is in the standard library). They are very vocal, and cause great confusion for newbies. Someone asks a simple question like "how do I do dependency injection in Scala", and like clockwork, most of the answers are split between "you don't need that" and "you should use the Reader Monad". Last time, they started talking about Kleisli Monads; despite using Scala for 5 years, I had no idea what they were talking about (when read about Kleisli, I discovered it was just a wrapper around flatMap). This is part of the reason people think Scala is too complex.
Nothing published yet. So far it's just an idea we want to try.
I'm sorry, I don't understand what exactly the point-function does or where the map method is defined. I guess null is lifted to "MonadicNullable"? How does this differ from doing the same with Option? val f: None.type =&gt; Unit = {_ =&gt; ()} val a: None.type = None a.map(f) // None f(a) // ()
&gt; The problem with the scalaz / cats people is that they claim their way is the "one true way" of doing FP in Scala (which is obviously untrue, since none of that stuff is in the standard library). They are very vocal, and cause great confusion for newbies. This is really a definitional problem. If we define "FP" to mean "programming referentially transparently so as to be able to reason about our code using the substitution model of evaluation and equational reasoning," then yes, you need a library like scalaz or Cats to avoid going crazy. A reasonable analogy here would be to programming in Haskell vs. OCaml: Haskell programming is referentially transparent by default, but you can violate that; OCaml programming is not, but you can do it by using libraries and discipline, just like Scala with scalaz or Cats. &gt; Someone asks a simple question like "how do I do dependency injection in Scala", and like clockwork, most of the answers are split between "you don't need that" and "you should use the Reader Monad". And that's true, if you accept the proposition that you shouldn't give up referential transparency, and consider that usually when people say "dependency injection," they're referring to mechanisms that involve bytecode rewriting, reflection, or other approaches that essentially make what's being injected "ambient," for lack of a better term. Things that are ambient aren't referentially transparent, and most often violate another reasoning tool, "parametricity," that we rely on in "pure FP." &gt; Last time, they started talking about Kleisli Monads; despite using Scala for 5 years, I had no idea what they were talking about (when read about Kleisli, I discovered it was just a wrapper around flatMap). Not quite. `Kleisli[M, A, B]` takes any `A =&gt; M[B]`. _If_ `M` is a `Monad` (has an instance of the typeclass), then so is/does `Kleisli[M, A, B]`. That's also true for other typeclasses, not just `Monad`. That is, you can use the `Kleisli` anywhere and everywhere you could use the `M[B]` without actually having an `A` yet. When you need to inject an `A`, `.run` the `Kleisli` and pass your `A` value, and you'll get an `M[B]`. In Verizon Labs code, this often looks something like this: case class Config(...) type ApplicationK[B] = Kleisli[Task, Config, B] def lookupUser(userId: String): ApplicationK[Option[User]] = kleisli { cfg =&gt; sql"select ... from ... where userId = $userId".query[User].option.transact(cfg.xa) } Here, I'm injecting my application's `Config`, which contains a [Doobie `Transactor`](https://tpolecat.github.io/doobie-scalaz-0.4.0/00-index.html), which mediates connections to the database (among other things). When I call `lookupUser` with a `userId`, I don't have to provide my `Config`; I get a `Kleisli[Task, Config, B]` back and can, again, use all of `Monad`'s, `Monoid`'s, `Applicative`'s, `Functor`'s, `Semigroup`'s... combinators with it. When I'm at the point (probably in `Main`) where I've loaded my `Config` and actually want the `Task[B]`, I just `myKleisli.run(myConfig)`. &gt; This is part of the reason people think Scala is too complex. That's true. But the irony—and I say this after a couple of decades of Java before Scala and a few years of Scala before committing to FP—is that pure FP is _dramatically_ simpler than the alternatives. The challenge is that _learning_ it is a bit like learning civil engineering by starting with Newton's _Principia_: you can do it, in principle, and the former _is_ founded on the latter, but it's too much work for all but the crazy to get there from here. Thankfully, works like [Functional and Reactive Domain Modeling](https://www.manning.com/books/functional-and-reactive-domain-modeling), [Functional Programming in Scala](https://www.manning.com/books/functional-programming-in-scala), and [Type-Driven Development with Idris](https://www.manning.com/books/type-driven-development-with-idris) are beginning to narrow some of the gaps.
IIRC it was meant to be a strawman initially, as in "please submit strawman alternative implementations of collections that fulfil these requirements so we can discuss the designs and tradeoffs". But in the end, nobody submitted any strawman alternative implementations, so the one lonely strawman implementation that the "core" Scala guys had worked on became the defacto This Is What You're Getting implementation.
I'm sure there are some uses, but the bar for introducing a core language feature should be higher than that. 
`point` is part of the definition of a monad - I haven't defined the monad instance for `? | Null` but there's only one definition that looks like it might work (but doesn't). For `Option`, `point` is `Some` and `Some(a).map(f)` is `Some({})` which is the same as `Some(f(a))`, as required by the monad laws. 
My point was simply that pure functions compose, and programming with only pure functions is the only way to build composable programs. I also don't think there's anything wrong with monad transformers.
&gt; You can then pull out the dreaded m-word (monads) to retain the desirable properties of FP without the efficiency hit of copying large matrices http://eed3si9n.com/learning-scalaz/Effect+system.html
For a senior Spring developer, there are good reasons to use Spring, because learning only Scala is much easier than learning SBT, Play, Akka and Scala.
[removed]
Oh I see what you mean now. Thanks for explaining!
What do you say to the critics of /u/simon_o in the threads here? JetBrains tried to do this with Kotlin and it seems that they failed. Do you think there is a difference between their and Scalas approach?
&gt; That's true. But the irony—and I say this after a couple of decades of Java before Scala and a few years of Scala before committing to FP—is that pure FP is dramatically simpler than the alternatives. The thing is, that in the case of pure FP people often forget "use the simplest tool available mantra". Continuing in with the example of Kleisli/Reader monad for dependency injection. Its completely overkill for DI, both as an abstraction and in pratice (DI doesn't need to take into account ordering, all you need to take into account is lazy loading so you determine your dependency graph automatically). I for one am really happy about the implicit function providers that are being added to Dotty, they are by far the most elegant solution to DI. Its actually very simple, and very idiomatic for Scala (using an already existing implicit mechanism) The thing is though, the pure FP community was advocating for using Reader monads all over the place even though they were the wrong abstraction, and it actually took the non pure FP community to demonstrate that "this stuff is just overcomplicated" (I think this is a major reason why Haskell has issues with approachable because of their overemphasis on Monad Transformers which start becoming a massive PITA when you have more then 3 types on your stack, they aren't really composable in regards to complexity). This is what I think is the core problem, its that some people in the community strongly advocate the ideas of being superior (just because it happens to be pure FP) when in reality this isn't. Or in other cases they claim that something is pointless because its not solving a real problem (i.e. usually the DI debate), and then you have a look at pure FP non DI code and its very painful especially when it comes to boilerplate (completely agree with Martin in this regard). Then there are other people going around claiming things that are simply untrue, i.e. a purely functional implementation of the Scala (or Dotty) compiler would be faster and more maintainable. I am sorry, this is simply untrue, with purely FP you completely lose the concept of memory and allocation (this is kind of the point of purely FP btw) so you end up losing performance due to doing things like blowing up your cache lines too quickly or creating too many things/boxing (and in some cases you can't avoid boxing, i.e. with HKT's you either box or you have massive code generation to specialize every type parameter in a call) There are just some things that pure FP isn't good at, and its good to be honest about it. Very obvious ones are - Purely functional cache (with TTL and expiry). Try doing this one in a performant way (i.e. completely minimising allocation) that doesn't do side effects which doesn't feel like shooting yourself in the foot when actually using the cache - Immutable datastructures (which by mathematical definition are always slower than mutable counterparts, even ones that are thread safe). They are often incredibly hard to implement, any person with a decent knowledge in comp sci can implement a self balance RB Tree as a mutable data structure (with enough iterations and tests you eventually get there). Try doing the immutable one (hint: its not too easy), or a collection that is backed by one - Continuing from the previous point, any code that needs to be really performant is not going to be purely functional. You are going to get better performance from mutable data structures (even in concurrent situations with fine grained locks). Yes its a lot harder, but if you need this performance this is where you look - Hard to debug issues in production because the PFP is often very different to the bytecode that the underlying VM runs (especially if aggressive optimizations are applied, which you pretty much have to do with PFP) This isn't to say that PFP is bad, quite the opposite, its really powerful but the tl;dr version is that its not a silver bullet.
Nice! I never tried custom matchers and my code is filled with Await.result. Giving it a try next week
I totally agree, in the OOP world, you're expected to know all sorts of jargon-y stuff, like the GOF patterns. I don't understand why we should stray away from powerful abstractions in the FP world, but have it widely accepted that in order to have great OOP code, you need to understand and be able to apply all of these different abstractions. Like how is an Abstract Factory (https://en.wikipedia.org/wiki/Abstract_factory_pattern) less complicated than something like a Monoid? Jargon can actually be good if it's well defined, because it allows us to talk about concepts in a concise way. 
**Abstract factory pattern** The abstract factory pattern provides a way to encapsulate a group of individual factories that have a common theme without specifying their concrete classes. In normal usage, the client software creates a concrete implementation of the abstract factory and then uses the generic interface of the factory to create the concrete objects that are part of the theme. The client doesn't know (or care) which concrete objects it gets from each of these internal factories, since it uses only the generic interfaces of their products. This pattern separates the details of implementation of a set of objects from their general usage and relies on object composition, as object creation is implemented in methods exposed in the factory interface. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
I am glad you like it.
The problem is that Scala can't be learned alone. One required to understand and feel all the environment and ecosystem to fully understand what Scala is. So, in my opinion, that is quite bad idea to use Spring with Scala. 
Could you elaborate more on that? Usally, you learn to write some code that compiles, then you learn to write cleaner code, then you start learning frameworks and other things. Is there another gradual approach to Scala?
There's probably already a matcher for most of your use cases. Scalatest as http://doc.scalatest.org/3.0.1/#org.scalatest.OptionValues for options built in. It also has `futureValue` for futures. And `inside` for pattern matching 
&gt; here's probably already a match yeah, there are some of them. But the post shows the basics of creating your owns.
I've got used to SBT these days, but I wouldn't discount Maven or Gradle either -- especially if I had to work with a team new to Scala. 
I landed a Scala contract after over 6 years of working with Java (with some Python on the side). I had no previous experience, but showed willingness to learn in the interview alongside experience with Java 8's lambdas. The codebase I worked on was like Java++ / Jala. Basically using Scala as a less verbose Java with focus on immutability -- but this is a good place to start for most people from a Java background. When I moved towards interviewing new candidates we put Java 8 experience as one of the requirements -- as most of our candidates came from a Java background. It's easier to pickup Scala if you have an understanding of the methods in the Streams API: map, reduce, etc.
Nested case classes are totally fine and there is no real problem with serialization. Also see sealed trait. Actually, it is better to always plac them inside enclosing object to specify purposeorvisibility on the fine grain. When one using Scala it is better to concentrate on Scala, not on Java serialization problem. There is huge amount of cases when asInstanceOf is totally fine. No reason to scare newcomers as there is type programming ahead. It's fine to use common IDE formatting config and youtube offers handful of videos in that. Sbt and maven or whatever you use usually offer plugins to reformat the code according to company policies. Also seefindbug and coderestile. 80 symbol code line is attavism introduced by 14' inch monitors. Modern code style suggest to keep inside 120symbols. Much more important to watch out for call/function nesting. Any and AnyRef are the ones of important concepts and before faithfully excluding them from the viewpoint it would be worth to explain the idea behind. Option will not help newcomers to copewith NPE, if they will blatantly follow the "return option ot null" suggestion. Intresting that authors do not bother explaining the optional flow and chaining rendering the hint next to useless. There is a book https://www.scala-lang.org/documentation/books.html which gives real head up to neophitesand worthreading even for mature programmers. Better read it. And Knoldus... well, it'salways Knoldus: replacing the understanding with cargo-cult. 
While I agree with the content of this article I find it's poorly written and borderline spam. I would vote to ban content from blog.knoldus.com. I reported this post as spam.
For `Future`s you should actually use asynchronous test suites, such as [`AsyncFunSuite`](http://doc.scalatest.org/3.0.1/#org.scalatest.AsyncFunSuite).
There is another approach to any programming language, actually. take a deep look into it https://mitpress.mit.edu/sicp/full-text/book/book.html
Thank you. I will read it.
Here's a simple Java example you could translate to Scala: https://stackoverflow.com/a/10788242?stw=2
I'd wager not complex, but simply impossible. What part of the Scala standard library allows you listen to a port?
The section on Idempotent functions has some mistakes (starting with the title - "Idempodent") . In the context of *unary* functions, a function is idempotent if applying the function twice produces the same value as applying it once. I.e.: *f(f(x)) = f(x)* For example, a sorting function (i.e. one which takes a list and returns a sorted permutation of that list) is idempotent because sorting an already sorted list has no effect - it produces the same list.
The opening paragraph has been lifted in its entirety from the statement by Martin Odersky on the Lightbend [Why Scala](https://www.lightbend.com/blog/why-scala) page. It should at least be presented as a quote.
You could use an [HTTP library](https://scala.libhunt.com/categories/663-http).
Depends on your definition of complex. Some find it more complex to fit what they need into a framework than simply typing out the logic themselves. I think you should give it a shot and see what you think ʘ‿ʘ
A webserver is just a program so yeah (you would need to use the Java net classes obviously). Why would you though ?
If the standard library doesn't allow for it, then what are all of the various HTTP/socket/networking libraries built on top of? Unicorns and good wishes? I'm pretty sure that java.nio.channels.SocketChannel fits the bill, along with the myriad other ways of doing I/O that are built into the Java and Scala standard libraries. Never drink and post, my friend.
It's too painful. Java's base networking stack is old as sin and feels like it. You're forced to do really low level bit stuff because no one was sure what a good abstraction for this stuff was back in the 90s. And thanks to the many layers of good server frameworks out there, it's never been pressing to fix the base APIs. Now, if you're wanting to get your hands dirty with protocols and "feel" the way they work, use Netty. I've only played with it in Java, but its async nature should make it play well with Scala. The first thing I noticed was how ugly protocols are at that level and that I shouldn't feel the need to work there unless otherwise necessary.
You probably can, is your goal a high quality website or learning? If you want a HQ website then I'd recommend a framework.
I don't think anyone can recommend doing that, it would be the same as developing your own webserver in Java, there is just no reason to, with the mature market already available. Akka-Http is a webserver built in Scala, and is replacing Netty as the default web backend in Play. If it is because you want a light weight project, take a look at [play-slim](https://github.com/lloydmeta/slim-play), Play is probably the biggest Scala 'framework', but as you will see with play-slim, it is possible to cut away basically everything.
yes basically I want something like node.js + express, which IMO is easy to setup and get running
Look up Peregrine the scala library. That might do what you want.
If you want something easy to set up and well documented, I'd just use normal Play. There is quite a few seed templates out there. [Playframework](https://playframework.com) has hot reload and a lot of nice modules, that will make your life easier. You could also use Akka-Http, it is a bit lower level, but still easy to use.
I'm wondering how they deal with commutativity of dimension factors, since they represent their dimension as an `HList` which is ordered. For instance how they can encode that : m^2 * s m * s * m s * m ^2 m^1 * s * m^3 * s * m^-2 * s^-1 are all the same at the type level
Is this a virus website? i clicked on the link and my virus checker (sophos) called out the link as a high risk web site.
You could, but I wouldn't do it as anything else than an exercise. It will take longer an be slower and more insecure than basically any of the good Java/Scala libraries already available.
Simple terminal-based password manager https://github.com/OleksandrBezhan/password-manager I want to make it free, secure and convenient for geeks.
I suggest using Finatra (as the name suggests it is inspired by Sinatra framework for Ruby which is very similar to Express). There are two good things about it - it's supported by Twitter so it's 100% production-ready and since it uses Finagle library for low-level socket stuff there are lots of clients (MySQL, Redis, Memcahed and obviously HTTP) that seamlessly integrate into it's non-blocking pipeline (the underlying socket library is Netty). It's less "functional" than other libraries but I find that an advantage.
Could be because of the Cloudflare setup I'm using. This is on github if you're interested - https://github.com/iravid/iravid.github.io/blob/develop/posts/using-cats-with-reactive-kafka.org
The JAR thing is the worst. Especially when you have Spring conflicts between JARs, and there's no clean fix. Hierarchies of enterprise JARs relying on Spring is a nightmare after long.
This look great, I haven't known about scorex. Implementing a blockchain in Scala (with Alka) myself is a project I want to start soon.
I build a lifelogging platform for myself, it records location and wifi (soon lastfm) to the akka-http backend using an Android app. There is a web frontend for inspecting the recorded data. https://github.com/thepiwo/open-lifelogging
You can also announce here: https://gitter.im/scala/job-board
Agreed, Finatra is definitely my favorite. I found it to be much easier to use than akka-http, and the lack of reliance on scalaz/cats was very nice to have! It does however lock you into the twitter ecosystem somewhat so beware. Http4s is another great alternative although it relies on scalaz for the functional stuff.
Very good question. Using an `HList` is perfectly fine, but we will need a typelevel function that can reduce such dimensions to some canonical-form to compare them. This must also work for dimensionless factors, e.g. `kg*s === 1000*g*s` and so on.
I was a python developer who took a Perl job at a large company who used both Perl and Java. Due to the Perl job market situation, eventually the company has decided to settle on Scala as a middle ground so I was lucky enough to be trained on the job to use Scala.
Disagree. You absolutely can learn Scala piece by piece and many people find it easier to do so; it's what I did, and I started off using Spring in Scala (indeed I still do on occasion). Indeed if you were going to jump straight in at the deep end I wouldn't bother with Scala at all - if that's your style you'd be better off with Haskell.
You don't have to use a full stack framework like play, but you'd better use an http library! Doing all those calls manually is going to be painful. There's Akka HTTP for example.
You surely [can](https://github.com/tim-zh/scala-tools/blob/master/src/Web.scala) (I wouldn't recommend actually using it though :) )
Micro-services are a way to solve organizational problems, they separate teams that work on different parts of the product better than modules. It's not needed to split the application into services if you are the sole developer, a monolith will work just fine. I've developed complex eCommerce solutions and frankly they wouldn't have been better if they were split into microservices. With some discipline it's possible to avoid the worst mistakes often found in such applications - User, Order and Product classes being 2000 lines of code. If you are using Scala there's no reason to use other languages that could be a reason to split application written in say Ruby or PHP because JVM is fast and can do pretty much everything.
Agreed, then you think you've finally got rid of some dependency, run `mvn dependency:tree` and some bastard 'company-util-support-helper-lib-DEPRECATED.jar' has just pulled an ancient version in.
Thanks for helping ! 
Based on your comments about express it sounds like you actually want to **use** this webserver. Building your own is a bad idea in this case. To echo what others have said about learning, however: Building an HTTP/1.1 compliant web server in Java which accepts multiple connections was one of the most valuable assignments of my CompSci undergrad degree. It touched upon use of thread pools, File IO, string parsing, byte manipulation. I thoroughly recommend it if learning is your purpose.
&gt; use this webserver. which one?
Sorry - I meant _you actually want to use the webserver you create_.
The speaker really does a fantastic job of making the subject accessible!
What about: * static site generator * personal music server (like, a self-hosted google music) * an Client SDK for [Matrix](http://matrix.org/docs/projects/try-matrix-now.html) (see Client SDKs for inspiration) 
How about starting easy, and create some wrappers for some java libs like: https://github.com/chargebee/chargebee-java https://github.com/stripe/stripe-java etc. something scala-idiomatic
This could be fun, and hard! Good challenge idea! 
The matrix sdk seems interesting! I'll give it a look see! Thank you! 
Great talk!
It's one of the easier things to do I think, and scala programmers would probably like it. I mean I bet there are lots of people that'd love to use some of wrappers instead of natively these libs. Just guessing. You can either wrap it (easier thing) with more sensible api, or study internals and see what endpoints they call etc., and make something grounds up that's clear from start (more difficult) -&gt; but now you have to track changes of their internal APIs very carefully, so I'm not sure I'd recommend it.
What's the diagram saying? It's a sequential container that maps objects of one type onto another? 
Can anybody tell me which library DeepLearning.scala uses in the backend to vectorize backpropagation on GPUs?
Yeah, that ReactiveX example tells me absolutely nothing. Let's not do that.
I imagine that a formal graph representation of collections could be a useful thing - but unless somebody were to design it carefully they could all end up being a bit meaningless. I wonder if there is a need to navigate the collections classes - some kind of index which would guide you to the collection type with just the properties you need.
X-Post referenced from [/r/androiddev](http://np.reddit.com/r/androiddev) by /u/gr3gg0r [Why I Don't Regret Moving Our Android App to Scala](http://np.reddit.com/r/androiddev/comments/6lgicg/why_i_dont_regret_moving_our_android_app_to_scala/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Very informative basic overview of futures, although I'm not too knowledgeable about parallelism.
I have created a small microservice to practice some common and useful modern scala tools while putting into practice and refining my scala code. Get in touch with HTTP libs. I am currently using akka-http but you have plenty more to explore: http4s and then frameworks like spray (akka-hhtp kinda spray though), finch, scalatra, play, lift. Get comfortable with asynchronous programming and functional progamming concepts, which most of these libraries will help you. Write a small REST API to do something. Look into JSON marshallers like argonaut or circe. Look into asynchronous or reactive DB drivers either NoSQL(Casbah for mongo, Redis) or SQL (Doobie, Slick) I find doing a small self-contained microservice a cool experience to test some cool tools, most very well designed. Oh, and by the way, get experience with sbt!
Yup that is exactly it, it maps objects of one type onto another. Since the image is the example for the map function on a list I would say the example tells a lot.
Ray tracers are great for this sort of thing. They can be challenging and can exercise a lot of the standard library of whichever language you choose 
I've used Spring extensively since 1.2. Spring was a blessing to the Java space, long tormented by the J2EE plague. Spring 2.5, with annotations, ushered in the golden age of Spring. I used it for everything. I even wired Swing apps together with Spring! I especially leveraged Spring AOP. I wouldn't even start a project without Spring. And, then, Scala surfaced. And, I never returned to Java/Spring. Transitioning to Scala was a lot like returning to the farm to learn the basics. Scala didn't require using Spring, nor much of anything else. I haven't used Spring in 5 years. And I haven't missed it. In fact, I've returned to constructor DI. That said, I do miss Spring AOP. Spring gave us the power to do anything we wanted to do with DI and AOP. And we used that power. Yet did we use it wisely? Like many early Scala adopters, coming from the Java/Spring space, I struggled to adapt my Java/Spring space to the Scala space. And, then, I realized I didn't need my old Java/Spring technologies to build solutions in the Scala space. I suspect many developers before and after me have/will come to the same realization. Some will not.
Cheers to Kelly for saying what most of are thinking!
It's [nd4j / nd4s](http://nd4j.org/scala)
Cool stuff, thanks!
As an alternative to Nailgun you can also use [Drip](https://github.com/ninjudd/drip). Drip always starts a fresh Java VM in the background. This helps a lot with resource leaks and deployment of changed code. If you run less than one new task per second, Drip's performance is comparable to Nailgun's.
If you want to reimplement the jdk in Scala take a look at: https://github.com/scala-native/scala-native/issues?page=1&amp;q=is%3Aopen+is%3Aissue+label%3Acomponent%3Ajavalib
&gt; This isn't to say that PFP is bad, quite the opposite, its really powerful but the tl;dr version is that its not a silver bullet. How is it saying the opposite? Nobody's contesting that it's worthwhile to put an impure implementation behind a referentially transparent interface when we don't have a performant purely functional data structure available. I won't contest that using a debugger isn't as easy as with imperative code. And I agree that we as a community could be more upfront about a common answer to how to do dependency injection effectively being "don't," with a preference for having functions explicitly depend on all their dependencies. But what's the alternative? The way I see it, functional programming is the only logically coherent way to build loosely-coupled software. Anywhere you avoid it (for whatever reason), you give up some level of composability and local reasoning ability.
I was expecting that to be a link to [spray can](https://github.com/spray/spray-can). :)
&gt; Last time, they started talking about Kleisli Monads; despite using Scala for 5 years, I had no idea what they were talking about (when read about Kleisli, I discovered it was just a wrapper around flatMap). This is a good argument for Scalaz/Cats people to suck less at teaching these concepts and not so good of an argument against using these concepts. :-) 
True. But worth noting that Netty (for instance) offers ["platform specific JNI transports"](http://netty.io/wiki/native-transports.html) for good reason.
Why can't Scala infer functions with multiple parameter lists if you try to return ADTs? // This works val someInt: Either[String, Int] = Right(4) someInt.fold( l =&gt; Left("Not a number"), r =&gt; Right(r.toString)) // This doesn't work Some(4).fold(Left("Not a number"))(n =&gt; Right(n.toString)) // This works if I write the type explicitly Some(4).fold[Either[String, String]](Left("Not a number"))(n =&gt; Right(n.toString)) 
$ cat Main.scala said it was java and java code. Just a tiny bug
What is the difference between ng and drip? For the n00b
&gt; Nobody's contesting that it's worthwhile to put an impure implementation behind a referentially transparent interface when we don't have a performant purely functional data structure available. Actually people are contesting this (i.e. previous example, someone was claiming that a Scala Dotty compiler written in pure FP Haskell style would be faster and easier to maintain then the current design which is a functional interface over impure code). People are also coding this way in a number of places (which is fine, as long as you don't really care about performance or you have other requirements) &gt; But what's the alternative? The way I see it, functional programming is the only logically coherent way to build loosely-coupled software. Anywhere you avoid it (for whatever reason), you give up some level of composability and local reasoning ability. Sure, but sometimes you have to do it. If you are requirement is making a piece of software that is highly performant, then yes, you may need to give this up. This really is just a case of "use the right tool/methods for your requirement"
The monaco editor might be a good fit for this. You may not even need to implement the tokenizer. You can try to fork https://github.com/scalameta/metadoc and extend it to support HOCON - adapt scala tokenizer here https://github.com/scalameta/metadoc/blob/master/metadoc-js/src/main/resources/scala.ts - register new language extension point https://github.com/scalameta/metadoc/blob/cd23da40e9d2dca3a250b16e2bae68b0be01ec7a/metadoc-js/src/main/scala/metadoc/MetadocApp.scala#L54 - Implement CompletionItemProvider https://github.com/scalameta/metadoc/blob/cd23da40e9d2dca3a250b16e2bae68b0be01ec7a/metadoc-js/src/main/scala/monaco/Monaco.scala#L2095 Here is an example PR where we implemented the DocumentSymbolProvider service https://github.com/scalameta/metadoc/pull/27/files
I think it's just a typo - it should say Main.java.
It's right there in the post you responded to.. &gt; Drip always starts a fresh Java VM in the background.
The problem with starting from zero is that it seems so overwhelming, but with proper guidance and examples from other gits, I don't think it's imposible! Thank you! 
I'll look into sbt! Thanks for the tip. I've been working a bit with mongodb, so I may look into putting the two together! Maybe an access to a remote server and graph the information for a website or something..... That's the fallback plan 
Yes, i've read it. It is a good strategy to free the memory.
I'm not entirely sure what a Ray tracer is. I'll look it up! Thank you for the hint! 
Interesting project! Very alive as well! Thank you! 
I think this is potentially a cool idea, but it should be tested with neophytes, to make sure it actually adds explanatory value.
https://scalafiddle.io/sf/4beVrVc/1
Bear in mind that if you are using Source.actorRef, you're not going to get backpressure when you integrate with a queue -- you need to use Source.queue and Sink.actorRefWithAck (and appropriately wait for the ack): http://doc.akka.io/docs/akka/2.5/scala/stream/stream-integrations.html#Integrating_with_Actors
feel free to ask on weekly ask thread in here any tiny questions you encounter
The point I was making is that when a Java guy is learning Scala, there will be a long uphill slog, just to learn the syntax, new concepts, and the standard library. Telling him to learn about obscure category theory isn't helpful. The correct answer to "How do I do dependency injection?" is: "You can use the same libraries you used in Java, but here are some alternatives..." My first Scala app had some embarrassing use of JPA/Hibernate, but that is OK, it was a learning experience.
Answering myself, it doesn't seem like sbt-jmh deals with java annotation processors, instead relying on either reflection or bytecode processing.
Here is a generally better approach: Parsing the json "manually" and working with `jsonObject` and paths and so on should only be done when performance is really important. Mostly it is better to parse the json into simple raw `case class`es, e.g.: case class RawCP(Duration: String, CPId: Long, SchedulesMM:List[RawSchedule], Name: String, Stacks: List[RawStack]) case class RawSchedule(StartDate: String, EndDate: String, SId: Long, Destination: RawDestination) case class RawDestination(Code: String, Id: Long, Ch: RawCh) case class RawCh(Id: Long, Name: String) (and so on) Then you can use whatever json library to generate the formats for these case classes. Most json libraries support this in some way. Next, you create your domain classes that contain the types you will later work with. They contain types like `LocalDateTime` instead of just `String`s and so on. Last, you write a function to convert e.g. a `RawCP` into your `DomainCP` class, handling all the errors that can occur like malformed strings and other constraints. This makes it easy to understand your code, is very safe and flexible and helps when errors occur. No need to parse json "by hand" at all! Just convert case class into another case class by the help of the compiler.
Thanks I agree with you 100% . But I had a problem with using case class. In some instances where it is sometimes an Int but other times a list of records. I dont really know how to fix this error. Caused by: net.liftweb.json.MappingException: Do not know how to convert JArray(List(JField(ContentId,JInt(1157247)), JField(ContentId,JInt(1161091)), JField(ContentId,JInt(1161127)))) into int. Do you have any idea how can I fix this. I tried to extract[List[Of the Class ] and also also extract[Array[Class] Both didnt work 
I will! Thank you
In this case try to use a sealed trait or Either. I am not familiar with lift so I'm not sure if and how lift supports that. Maybe you will need to create a custom json reader for your sealed trait, trying option A first and falling back to option B if A fails. 
Surprisingly, all answers here are at least incomplete. What's actually going on is type erasure. `A with T` is a compound type, which exist in Scala but not in JVM bytecode. So `A with T` is represented by `A` in bytecode (and casts to `T` are inserted when necessary). `asInstanceOf[A with T]` is the same as `asInstanceOf[A]` (except for the return type), and `isInstanceOf[A with T]` is the same as `isInstanceOf[A]`. This is exactly the same as better known issues with generics, where `List(1, 2, 3).isInstanceOf[List[String]]` is `true` and `List(1, 2, 3).asInstanceOf[List[String]]` succeeds and will probably give you a `ClassCastException` somewhere down the line when you try to actually get a `String` out of it. The exact rules for type erasure are described at http://www.scala-lang.org/files/archive/spec/2.12/03-types.html#type-erasure.
&gt; isInstanceOf[A with T] is the same as isInstanceOf[A] It is not, see here: https://scalafiddle.io/sf/9JCEeV8/1 `isInstanceOf[A with T]` checks for both `A` and `T`. However, when the they compiler statically knows the runtime-type (or thinks it knows it because of my cast) it will just insert `true` even though at runtime there is only `A` and not something that is both `A` and `T`. So the only place where we lose type-information (= the compiler ignores parts of what we specify) is when doing `asInstanceOf[A with T]` which at runtime ignores the `T`. And it does not even issue a warning, compared to something like `(List(1, 2, 3):Any).isInstanceOf[List[String]]`.
I did not know about either . I would like to use that could you you provide me with a small example on how to use either in this scenario. To handle the two different possibilities. Thanks
Are you using this?: https://github.com/lift/lift/tree/master/framework/lift-base/lift-json If so, please post the case class(es) that you tried to read your example json into, which resulted in the `MappingException` you showed. Also, in the link, please see section `Serializing polymorphic Lists` - I guess this is how your problem will be solved.
If you want to do it with case classes, here's an easy way with [circe](https://circe.github.io/circe/): [You can run it in the browser if you like :)](https://scastie.scala-lang.org/felixbr/Np86mse1TIOfVSdDmWyPLw/0)
I'm an absolute noob to Scala, but is this really that complicated? In Perl it would be my $data = decode_json $json; my @start_dates = map { $_-&gt;{StartDate} } @{$data-&gt;{S}[0]{C}[0]{CP}[0]{SchedulesMM}}; my @end_dates = map { $_-&gt;{EndDate} } @{$data-&gt;{S}[0]{C}[0]{CP}[0]{SchedulesMM}}; my @sids = map { $_-&gt;{SId} } @{$data-&gt;{S}[0]{C}[0]{CP}[0]{SchedulesMM}}; Or in Python data = json.loads(json) start_dates = map((lambda x: x['StartDate']), data['S'][0]['C'][0]['CP'][0]['SchedulesMM']) end_dates = map((lambda x: x['EndDate']), data['S'][0]['C'][0]['CP'][0]['SchedulesMM']) sids = map((lambda x: x['SId']), data['S'][0]['C'][0]['CP'][0]['SchedulesMM']) 
Thanks, I was wondering, Why Monaco? Then realised it's the logic for the VSCode editor.
Alright thanks I will post my case classes tomorrow and take a look at the link u posted. Thanks a lot.
I agree. My impression of lifts json library is, that circe (or argonaut) will fit much better for this task. 
I don't know about "why", but it's known behaviour: type inference runs from left to right. It can occasionally be useful in that it gives the developer more control over inference: you can deliberately use multiple parameter lists where you want the first to control type inference, and single parameter lists where you want it to take into account all the values. (Why `Option#fold` takes multiple parameter lists, I couldn't say)
I was so fed up with the fact that there is no proper library for fixed length format that I have created my own. You can check it out here: https://github.com/atais/Fixed-Length
Some parts of it will. Others (that require reflection) will not. I've discussed doing this several times with Denys Shabalin. I think it's worth it. However, note that we can increase sbt's startup time in the JVM now. Feel free to drop by sbt's Gitter channel and we can discuss how.
Could you elaborate why you think it will be much better. Basically the problem I had was that my json was more simple before and I was able to parse it with lift and case classes then I had what I posted yesterday and now it is even more complex and not always the same. Sometimes I get a list of records for some attributes and other times only one record. 
Huh. This inconsistency between `asInstanceOf` and `isInstanceOf` is quite surprising to me, especially since the spec suggests it shouldn't exist. Thanks!
Thanks for posting your example looks really cool . Honestly a bit overwhelmed by it also I tried using it with scala 2.10 and was getting a lot of errors.
Because circe makes stuff like that really easy. See here for a live editable/runnable example how it works: https://scalafiddle.io/sf/5lqr8a3/0 And I am sure you can even get rid of that `barDecoder.map(bar =&gt; bar:Foo).or( quxDecoder.map(qux =&gt; qux:Foo) )` but I have no time to find out how to do that, right now.
For 2.10 you need additional dependencies (see the circe docs). If you have any specific questions, just go ahead, I'm happy to help. If possible use 2.11 or 2.12, as many libraries work best with those. 
Dotty is apparently the "next generation compiler for Scala": http://dotty.epfl.ch/
Not 100% sure what you're referring to, but if you build your ScalaDoc with 2.12 you will get the new layout. Prior to 2.12 it looked more like JavaDoc.
Can I use scaladoc 2.12 to make the doc of a Scala 2.11 project ?
We have several contributions in this milestone release, encourage the community to get involved ! 
Yes, you can. You have two options: The first one is to download Scala 2.12 on your local machine and then go to the root folder of your project and run: scaladoc with all scala related classes, or you simply write a bash script for it (to get all scala related classes). The second one is to go to build.sbt and change scalaVersion to 2.12 and then go to the root folder of your project and run sbt doc. It will generate the scaladoc in to the target/scala-2.12 folder. 
With your examples I figured out my mistakes I had a few typos and some formatting errors . Everything works now with the lift library thanks alot!!!
With your example as well I figured out my mistake had a few typos. I got it working now with lift but will definitely have a good look at circe. Thanks a lot !!!
I just hope that the situation will improve regarding the type system and its stability. Intersection types seem to promise powerful abstraction and composition patterns, but (from the limited experience I've had with the current release candidates) doing anything a little advanced generally results in the compiler either bailing out with a disappointing type error, or crashing from [assertion failures](https://github.com/lampepfl/dotty/issues/2858) related to weird Scala symbol problems, and sometimes even [stack overflows](https://github.com/lampepfl/dotty/issues/2771). Still, intersection types work better than in Scala 2.x and one can tell that the theoretical developments have helped a great deal by providing insight, but it *feels* like the compiler is still being developed with old concepts intermixed with backend concerns in mind (Java compatibility &amp; co), as opposed to from first principles. I may be wrong though, as I have not looked at the code based too much. The compiler still seems to be [a nice piece of engineering](http://dl.acm.org/citation.cfm?doid=3062341.3062346), but if the type system is overlooked I'm afraid that Dotty will not improve *that* much on Scala's quirky, corner-case-riddled typing discipline. 
Or, much easier: $ sbt &gt; ++2.12.2 &gt; doc No need to manually change `scalaVersion` inside the build.
&gt; doing anything a little advanced generally results in the compiler either bailing out with a disappointing type error, or crashing from assertion failures related to weird Scala symbol problems, and sometimes even stack overflows. I think that's to be expected in a young compiler, especially one where all the cool new features are the least tested, because no codebase uses them yet. Usually the compiler itself is the testing ground for new features, but Dotty currently cross-compiles with Scala 2 so we can't do that. (because of this, we've been considering giving up on Scala 2 cross-compilation of the compiler itself, but haven't made the leap yet). &gt; it feels like the compiler is still being developed with old concepts intermixed with backend concerns in mind (Java compatibility &amp; co), as opposed to from first principles. In the end, you want your code to be run, so inevitably we have to take into account what make or doesn't make sense on the JVM and other platforms :). &gt; if the type system is overlooked I'm afraid that Dotty will not improve that much on Scala's quirky, corner-case-riddled typing discipline. I wouldn't say the type system is overlooked, but making a robust type system is a very big engineering effort and something that fundamentally requires time to mature.
Any good guides to IO in scala? I've been reading *Programming in Scala* to teach myself scala but it doesn't seem to have much on IO. Another question I had was the most efficient way to check for membership. Say I have a word and want to check if it's in a certain dictionary, what should I store the dictionary as? A set? A sorted array?
Won't either require all dependencies to be published for 2.12?
Who decided to make the name of this language Scala? It's also my name lol. Legit the only reason I'm here.
This would be good if the images helped.
Currying? So that you can provide a default value at the location of something returning a none, and later use the value and map over it. The times when getOrElse matter, is when your default is of the type of the mapped value, instead of the type of the original value. 
What does the use case for that look like? I really struggle to imagine where you'd want to pass a partially applied `Option#fold` around - it really seems like a method you'd want to use all at the same time, the same as `Either#fold`.
I really like the idea but what really keeps me from using Binding.scala is the fact that it seriously confuses IntelliJ Idea's highlighter + typechecker and lot of red code appears. I've tried some tricks with fooling Idea using implicit conversion (as described [here]( https://github.com/ThoughtWorksInc/Binding.scala/issues/24) ), but it still doesn't fix all problems. So I ended up using Scala.rx and Scalatags and I'm quite happy. Once Idea will work with Binding.scala seamlessly, I'll consider to migrate.
That's cool and all but the bottom line of *Nailgun* is - &gt; Of course, not all Java apps can be run this way. But simple, text-based programs are perfect for Nailgun - in fact, they end up running nearly as fast as their C counterparts. GUI programs work with Nailgun as well, although it can be confusing if the client and server are not on the same physical machine. And &gt; What is it good for? &gt; &gt; I'm not sure yet. It's certainly excellent for simple command line utilities. It would be nice to have some examples of web apps. I would certainly think that it is a good idea to have a nailgun server running close to the app and in case it dies very quickly restart it. But I can come up with the example that having deployed a new version of your jar you will always have to pay the price for the first run. If your app is stable and long running - you don't get that much benefits, in my opinion.
For collection performance look at http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html
Yes, indeed. Good point.
Does Ensime work better with Binding.scala? It is said that they have a better support for macro-paradise used in Binding.scala https://stackoverflow.com/questions/42913217/how-to-make-scala-presentation-compiler-happy-with-binding-scala/43720839
There is also `monadic-html`, it's essentially an IDE/compiler friendly clone of Binding.scala (no macros).
&gt; I think that's to be expected in a young compiler Unfortunately, the current Scala compiler is quite old, and it still crashes regularly for me. I'm just seeing the same pattern that seems to be repeating: "monkey-patching" until it's good enough, but never really complete. &gt; Usually the compiler itself is the testing ground for new features, but Dotty currently cross-compiles with Scala 2 so we can't do that. (because of this, we've been considering giving up on Scala 2 cross-compilation of the compiler itself, but haven't made the leap yet). Good point! &gt; I wouldn't say the type system is overlooked, but making a robust type system is a very big engineering effort and something that fundamentally requires time to mature. Absolutely, I did not mean to diminish the work that is being done. And it's not like I have any comparable experience building a production compiler! EDIT: "monkey-patching" is probably not the right term. I meant patching bugs as they happen, apparently without a good solution to solve the root problems once and for all. I thought that Dotty was meant to solve the root issues with Scala typing, but it is not very encouraging seeing that the same kind of bugs cripple it. Don't get me wrong, there might be no good solution here – and I'm clearly out of my depth talking about that.
Do you think they don't help? Can you elaborate on why?
Note that you don't need this if you use JavaFX for UI, because it has this functionality built in.
Play 2.6 core is a wrapper over Akka-HTTP.
In https://github.com/ThoughtWorksInc/Binding.scala/wiki/FXML you can see &gt; Unlike FXML dynamically loaded from `FXMLLoader`, Binding.scala compiles FXML into monadic expressions, which is statically type checked, which means you can get rid of implicit conversion between XML text and other types, and use Scala expression instead. and &gt; FXMLs loaded from `FXMLLoader` support very limited data-binding expressions via `${...}` syntax. On the other hand, Binding.scala supports arbitrary Scala code as data-binding expressions. Particularly, FXMLs loaded from `FXMLLoader` does not have repeaters, which is very useful to build a complex UI. In Binding.scala, you don't need repeaters, instead, you use simple `for`/`yield` expression to build a reactive FXML template within dynamic number of children.
&gt; I meant patching bugs as they happen, apparently without a good solution to solve the root problems once and for all. I thought that Dotty was meant to solve the root issues with Scala typing, but it is not very encouraging seeing that the same kind of bugs cripple it. Don't get me wrong, there might be no good solution here – and I'm clearly out of my depth talking about that. Sadly, having principles is not enough to write bug-free code :). Even if we fully formalized everything in the type system, it's still ~25k lines of codes where mistakes could be made. What's more important I think is that when we do find bugs, we should be able to fix them without massively redesigning things, because the basic concepts are sound. By the way, bugs in the typechecker are not a problem unique to Scala: if you get deep enough into any language you can probably find some, just check the bug tracker of your favorite non-Scala compiler!
It would be obvious if you'd ever used Scaladocs.
Hi, post author here. Yes, actually, I'm using Ensime myself, so I have no idea about IntelliJ Idea's issues. Binding.scala is working fine with unstable Ensime (2.0 branch) and as far as I remember it worked just as well with stable Ensime 1.0.1. However, XML literal support is far from perfect, so code completion within XML literals seems to be broken, which is such a minor issue I didn't even notice until I tested it a few minutes ago.
Yes, I even mentioned `monadic-html` at the end of the post. It looks promising, but it doesn't provide statically typed HTML, which is a nice feature of `Binding.scala`. Moreover, I think `Binding.scala` may be easier for people without functional programming experience, but that's just my wild guess.
Use two different Classloaders for the Slick 2 and Slick 3 parts of the application. Maybe use something like OSGi if you prefer to not do this manually.
 http://www.scala-lang.org/node/250 *The name comes from two sources. First, "scala" is the Italian word for stairway, which is appropriate since Scala helps you ascend to a better programming language. The Scala logo is an abstraction of a stairway. Also, Scala stands for scalable language, because Scala's concepts scale well to large programs.*
Yeah OSGi is going to be the only well-specified way to do this. It's absolute hell though, so you're probably better off upgrading the existing app.
You get a `MatchError` if you match against some value but no case matches. Say like i match { case 1 =&gt; "foo" case 2 =&gt; "bar" } If `i` is anything other than 1 or 2, you'll get a `MatchError`. My guess is your code is trying to match some value (which has the value 11) but doesn't have a case to handle `11`.
Yeah, no. Running into classloader issues or trying to nail osgi onto a legacy app (or any app for that matter) is not worth it. If you need to use incompatible libraries, make a separate binary and have some rpcs in between. It's 2017, so microservices ftw 😂
Can you please point me to an example / blog any resource which I could use to read up further?
There's nothing wrong in sharing knowledge, specially if it would help beginners. Thanks for sharing :)
How hard would it be to make Binding.scala work with scalatags (https://github.com/lihaoyi/scalatags) instead of scala xml literals?
Quite easy if you don't need partial update. Very trivial otherwise. See https://stackoverflow.com/questions/42893721/can-scalatags-be-used-together-with-binding-scala/42894490#42894490
Another option would be to shade the slick 3 jar in your new library. The sbt-assembly plugin supports this. https://github.com/sbt/sbt-assembly
&gt; By the way, bugs in the typechecker are not a problem unique to Scala Of course! Even OCaml seems to have [plenty of those](https://caml.inria.fr/mantis/changelog_page.php), although it's a language that was mostly formally specified and mechanically verified. I did not mean to sound like it had to be perfect from the start :\^)
The code (inside Spark) takes an `Any`, and matches against either `String` or `UTF8String`. My first look at Spark code, I don't want to look any further. How horrific. Back to the issue though. I haven't used Spark before, but the issue looks fairly obvious: you have defined a field as a `String` in one place, and as a `Long` in another: The metadata is: StructField("media_season_number", StringType, true) While the corresponding case class contains: meadiaSeasonNumber: Int I suspect that these types should match, otherwise Spark blows up with the above crazy error message.
Thanks for the high level explanation. Really helped I am really new to scala. To be honest I though the transition from java would be easier. It has been quite challenging even though its been a few months..
Spot on I new it was something like this I spent like two hours yesterday could not find it. Problem solved. Thanks a lot !!!
No problem, you'll know next time :-) It does seem that Spark violates the "if it compiles, it works" principle here - that's an issue in particular with this API rather than Scala in general. Unfortunately, implementation details are exposed in runtime errors, rather than you getting a good pointer for what to fix.
[removed]
You okay there man?
This.
Do you use photos of you staring at your text editor?
That would be less desperate.
Hahah. Well, TBH I'm not really interested in discussing whether or not my photo is good or not. If you have any comments on the articles, however, I'm happy to hear them.
Just popping off, man. It's the internet after all.
I've been using akka streams for IO. It makes the code very succinct and you can easily say read lines from a file and process them on multiple threads and combine all the results. 
Thanks for sharing.
IIRC someone wrote a blocking API for Slick 3 (don't recall the github repo). It should be much easier to upgrade with it. Then you can gradually switch to the non-blocking API as desired.
Having developed solutions in both versions of Slick, I would strongly recommend a rewrite in Slick 3 due to the IOMonad design of Slick 3. As is the culture of Scala, there are many ways to build a Slick 3 solution. Here is just one example: https://github.com/objektwerks/slick Cheers!
Thank you so much for your response, it really helped reason and figure out which reference my classes should keep around. although i didn't go with a pass by name solution, what i do now is process all data inside an apply function that returns an instance of the class containing only the important data, and let the irrelevant data be garbage collected.
i'm here to thank you again for, thanks to your advice i was able to reduce memory usage from 2GB to 150MB, and i don't think i can go lower without sacrificing performance and maintainability. the problem was present because i was keeping references to long strings (about 200K strings), refrences to a lot of substrings (about 8M) extracted from those long strings.
I'm glad it was useful! At least you learned a lot about jvm :)
Not exactly a Scala-specific question, but more about program design. Say I'm trying to write a program for chess. Is the functional paradigm worth it when it comes to something that is so easily expressed with a mutable 2D array (the board, that is)? I know that in the ideal world, boards would be immutable and each move should return a brand new board, but this seems wasteful from a performance perspective and unintuitive from a design perspective. What are the upsides to this approach?
Thanks, appreciate your articles as a scala newbie.
Glad I could help! Always nice to hear that bit of feedback :)
I mean, immutability isn't strictly necessary, but if you model the chess board and logic of the program using data as much as possible, and therefore ADTs, then you'll likely have a bunch of immutable functional patterns all over. I know I personally would prefer a design where you have a lot of functions that take a ChessBoard as an argument than, for example, a giant mega ChessBoard class that has a bunch of methods that mutate state internally. Afaik case classes and copying are relatively performant but obviously they won't beat an array implementation. Perhaps you could also look into using some kind of functional data structure that shares as much as possible. Or you could even turn the problem on its head and instead of a board being a mutable array you could have the game represented by a collection of pieces which are represented as a list of previous positions. If you need speed then you'd want an array, but I guess it depends on your goals.
In particular, blocking-slick provides Slick2 compatible API (not complete compatibility, but almost OK) on Slick3, so you can port Slick2 based codebase to Slick3 easier than full-rewriting. https://github.com/takezoe/blocking-slick
https://github.com/eaplatanios/tensorflow_scala
This actually sounds really cool! I've been using Scala at work for a couple of months now, but I have no experience with compilers. However, it's something that I've been meaning to learn about.
Hey, thanks for taking the time to grab all the videos and aggregate them in one place. I find this very useful
You're welcome :)
I spent the whole day today trying to solve this problem. I found this resource as well http://manuzhang.github.io/2016/10/15/shading.html https://github.com/wsargent/shade-with-sbt-assembly/blob/master/build.sbt I think in the example above, the guy is trying to do the same thing with gs-collections. He first does shading on the classes, and then adds the shaded classes a unmanaged dependency on the app/root project. but in the end..... his solution did not work for me...
awesome! 
Yes that's what I was thinking of, thanks
The additional code you see in the Scala examples is specifying the types, attempting deserialization, and working with possible failures (e.g., `Either[Error, Seq[String]]`) without throwing runtime errors. Without types specified you could do something like this with circe, parse("""{"S":[{"C":[{"CP":[{"SchedulesMM": [{"SId": "45623"}]}]}]}]}""") .flatMap(_.hcursor.downField("S").as[Seq[Json]]) .flatMap(_(0).hcursor.downField("C").as[Seq[Json]]) .flatMap(_(0).hcursor.downField("CP").as[Seq[Json]]) .flatMap(_(0).hcursor.downField("SchedulesMM").as[Json]) [Optics](https://circe.github.io/circe/optics.html) could cut down some of the remaining boilerplate. However, I would still specify the types (as case classes instead of a group of lists that can be joined by index) since they help make sense of whatever data you are working with at compile-time, giving you compiler feedback and precluding nonsensical programs early on.
IMO he should write a book on software engineering. Much better perspective from him that lots of "architects" I've met. Not to mention people like Uncle Bob...
I agree, I just hope this blog post scales to book length with more content.
That's like the opposite of express though. OP wants something that limited in scope and size and very straightforward.
&gt; Any good guides to IO in scala? Depends. What specifically are you looking to do?
Match expressions end up being compiled to something like a conditional. So i match { case 1 =&gt; "foo" case 2 =&gt; "bar" } gets translated to bytecode that would look something like this in Java: final String result; if(i == 1) { result = "foo"; } else if(i == 2) { result = "bar"; } else { throw new MatchError(i + " of class " + i.getClass.getName); } In your specific case, it seems like Spark is doing the match under the hood somewhere, and giving you an unhelpful message. Since you declare what your data should look like, Spark should be able to tell you something nicer when the actual data looks different; it's a pity it doesn't. 
Check out engine, link in the sidebar :-)
Dang autocorrect, thanks
If this is supposed to be a competitive chess engine, then performance is your primary concern. If you are trying to build a distributed computing chess engine, then it depends on how much hardware you chuck at it.
Ensime's sublime plugin is in the middle of a major overhaul - check out: https://gitter.im/ensime/ensime-sublime for updates - it should be released in the next couple of days.
Am I missing something or is this just commercial spam?
Also on Sierra and the trackpad doesn't scroll when the cursor is on a code-box.
That's really awesome. I now hope both this and ScalaPB (for GRPC) could be added by Google officially.
Yes, you are correct: the project is commercial, although there may be free offerings in the future.
I'm working on my first pet project for in depth learning Scala which is working with Kafka streams / Spark streaming. I want to know how joins between streams happen if one is lagging, how to deal with duplicated messages from producers or if data is composed from multiple messages (how it all affects data sink) and finally how to monitor stream processing application. I'm just a couple of commits in and a lot of reading.
Awesome, 2.12 based builds, finally. Now, if we're lucky sbt 1.0 support will be added to Play framework next year when 3.0 lands (shakes fist at Lightbend).
I wrote a blog post about implementing Future's that timeout without any external dependencies. [Future with Timeout](http://justinhj.github.io/2017/07/16/future-with-timeout.html)
Check reference.conf and how to use it. i.e. https://github.com/typesafehub/config#note-about-resolving-substitutions-in-referenceconf-and-applicationconf or look how akka / play include their default configuration (answer: reference.conf) Btw this config shuffling has always ended terrible for me.
I find this series of comparisons very good! I am still missing a comparison between between Monix and fs2 though.
As a consumer of a library, I find it easier and cleaner to simply pass arguments directly. In short, I would exclude the `application.conf` from the library. But, what type of configuration data is it?
Best practice is to take arguments. Libraries shouldn't make assumptions about configuration.
 Thought I'd just quickly update this now that headers are supported in the 0.2.0 release
Yep. Listen to your brother.
connection to two sql databases. 
Do you like your experiences with Scala so far?
Very much :) it's just a mountain of things to learn in terms of language and functional programming. It's great that I already know the basics and landed a job that I can use it and learn during the week, but sometimes it's a bit difficult to learn the details. Or maybe it's learning new language + new way of thinking that makes it a bit difficult to wrap my head around it. Or that there are just so many different ways to do things in Scala.
Do whichever you feel is the most elegant/maintainable/easy to reason about. An immutable board and a method that applies a move and returns a new board is pretty idiomatic Scala, but a mutable array is pretty much a 1 to 1 mapping with a real chess board so it would be very natural to model it like this. Here are a few reasons why I wouldn't consider performance an issue here: * In my experience the performance of a fully immutable implementation compared to a mutable one won't differ much. Otherwise Scala wouldn't get much use. * Your chess board has a fixed size. It will never grow beyond 64 squares. Even if you copy the entire board after each change, it's still constant time. * Those operations amount to nothing compared to network latency, human reaction time, disk IO, which your chess game will invariably have. 
Shaking your fist at Lightbend may feel good, but won't actually do anything (unless your goal is feeling good through fist-shaking, in which case go ahead). Here's the issue for SBT 1.0 in Play: * https://github.com/playframework/playframework/issues/7261 In a nutshell, Play depends on Twirl. Twirl depends on Scala.js. Scala.js does not compile on sbt 1.0: * https://github.com/scala-js/scala-js/issues/2390 Even with that in mind, Play's release schedule is still being drawn up and discussed. Right now, sbt 1.0 is not even on the roadmap: * https://docs.google.com/document/d/11sVi1-REAIDFVHvwBrfRt1uXkBzROHQYgmcZNGJtDnA/pub If you want it raised as an issue, bring it up on the play-framework google group as a possible 2.6.x patch release, and push sbt 1.0 in scala.js and twirl. That is the way things happen.
The notes don't say how I can I try this out 🤔.
Take arguments. Every time I insert configuration, logging, or http client into a library, I really wish that I hadn't. It ends with rows of excludes and weird debugging situations.
Take a look at the github issues I've linked above, and the PR attached.
You should use reference.conf. Do not include an application.conf.
But this misses the point of convention over configuration. 
[pureimage](https://github.com/stephenjudkins/pureimage) look like a good fit.
Being able to replay a game is valuable. Going further, it's very natural to want to follow up sidelines - what would happen if this move had been played instead, or that move. In a single-player chess game the user probably wants to be able to go back several moves and resume after playing something different instead of what's now revealed as a blunder. A game engine is naturally recursive - if you want to know the best move to play now, it helps to be able to reuse yourself to ask what's the best move for the opponent if you play move X. I find an immutable gameboard a much more intuitive design, but I guess that comes down to functional programming experience. Actually, I'd say it's not the gameboard that's immutable, rather the game is the sequence of moves and that is immutable (or at least append-only), and the gameboard is more of a "materialized view" of the current state of play, a "cache" which could even be implemented mutably if that's necessary for performance. (But premature optimization is the root of all evil: implement the pure way first, then if nothing else you've got a useful comparison point for testing).
Convention over configuration means having good defaults. But you don't need config files to do that.
You need if you don't want the conventional configuration. 
I am trying to learn playframework faster than it updates. At the moment I am working my self into silhouette. 
You need a way to override, sure. But that can be parameters in code rather than external files, as tpolecat suggests. 
Thanks for this, great article about something that I have wondered about too :-)
So you'll pass a `Logger` argument? How will you configure its log level - via an XML file? Example: Use WARN level for `com.mycompany.foo`.
Couldn't find it :(
Thanks! Also since posting here I realized that the synchronization around Promise fulfillment was not needed so I've simplified the code a bit.
Its a bit of a shame that the akka-stream benchmarks are so low. I think that the performance of akka-stream could be greatly improved if they employed macros which would optimize/verify the graphs at compile time, but I suspect that wouldn't work well with JVM support. `flatMap` is its own seperate problem though, this is a general issue with FP + boxing + cache indirection
If you don't need anything fancy, `java.awt.image.BufferedImage` works great and comes built in https://docs.oracle.com/javase/tutorial/2d/images/loadimage.html You can pull the pixels out one by one or get an array containing all of them
&gt; "In addition I've recently found out Akka-Stream is just inspired by Reactive-Streams and the reason they provide converter/wrapper to a Publisher instead of implementing it at every step is because working Reactive-Streams' deferred nature is too hard." This is precisely backwards -- the Akka team inspired Reactive Streams and wrote the Reactive Manifesto: https://www.lightbend.com/blog/why_do_we_need_a_reactive_manifesto%3F
* https://github.com/playframework/playframework/issues/7261#issuecomment-302503930 * https://github.com/playframework/playframework/issues/7261#issuecomment-302504051 * http://developer.lightbend.com/blog/2017-04-18-sbt-1-0-roadmap-and-beta1/#cross-building-sbt-1-0-plugin-from-sbt-0-13 * https://github.com/playframework/twirl/pull/138
Thanks, I did read all of those, so I think what I'm not understanding is, shouldn't I be updating the launcher.jar or something? just by being on the latest .13 version, I can just do ^^ and it will work provided I have sbt cross compile plugin?.
It's not that shaking the fist at Lightbend feels good, but rather that I, among I'm sure many others, have been frustrated by Lightbend's investment in (or lackthereof) in the Scala ecosystem in recent times. The [reason](https://github.com/playframework/playframework/pull/7366) for my original comment can be summarized by: &gt; This won't make it for 2.6.x and will have changed substantially in the meantime. Given how far behind Play was wrt to the Scala 2.12 release, the above comment seems like an sbt 1.0 show stopper.. It's fine if sbt 1.0 lands in Play during 2.6.x release cycle, but to wait for the next major release of Play is really too much when most Scala libraries and frameworks will have long since taken advantage of sbt 1.0. p.s. already got in touch with Scala.js author (who confirmed sbt 1.0 will be supported before Scala.js 1.0 GA). Also, FWIW, I asked on play framework dev gitter channel re: sbt 1.0 adoption and was ignored.
&gt; How will you configure its log level val logger = Logger( defaultLevel = LogLevel.Warn, packageSpecific = Map( Package("com.mycompany.foo") -&gt; LogLevel.Info, Package("com.yourcompany.bar") -&gt; LogLevel.Error, ) ) 
Which part are you looking to get translated? I've worked on a Java/Scala project that used Guice, so I can probably help you out.
well play on 2.12 took a while since 2.5 had a nasty sbt dependency. and it is also really hard to actually adopt scala 2.12 when you are depending on a lot of libraries. also scala 2.12.0 had some rough edges.. however play 2.6 got released way later, cause of the many features it has and it also removed all deprecated sbt stuff along the way, so sbt 1.0 shouldn't be that much of a problem. also play-json depends on scala-js aswell, but I think besides twirl and play-json all projects have an up to date build definition and can hopefully be migrated to sbt 1.0 without breakage/bin incompat. Basically what helps most, is helping with PRs, as amazedballer already said..
&gt; This won't make it for 2.6.x and will have changed substantially in the meantime. That's a typo -- we couldn't afford to wait for sbt 1.0 to come out before 2.6.0 release. &gt; Given how far behind Play was wrt to the Scala 2.12 release, the above comment seems like an sbt 1.0 show stopper. Here's how it's worked in the past -- everything is on the roadmap, features and the estimated release date which was a year out: * https://www.reddit.com/r/scala/comments/6j4y84/playframework_260_is_out/djcyzbh/ If you want sbt 1.0 support in a point release, I would discuss it on the mailing list. Reddit comments are not a community visible channel. &gt; p.s. already got in touch with Scala.js author (who confirmed sbt 1.0 will be supported before Scala.js 1.0 GA). Also, FWIW, I asked on play framework dev gitter channel re: sbt 1.0 adoption and was ignored. If you mean https://gitter.im/playframework/playframework?at=596dcabf1c8697534a35e28a that was asked on at 1:45 am this morning. I was asleep. 
&gt; That's a typo Good to hear. &gt; If you mean https://gitter.im/playframework/playframework?at=596dcabf1c8697534a35e28a that was asked on at 1:45 am this morning. I was asleep. No, that's not me, I asked on Play gitter dev channel several days ago. Anyway, sounds like sbt 1.0 will land in 2.6.x, thanks for the clarification.
The first part where they are defining the custom attributes. I am not sure how all those attributes migrate to Scala. I guess an example of how to use those attributes in a scala world would be nice.
&gt; Anyway, sounds like sbt 1.0 will land in 2.6.x, thanks for the clarification. No, there is no guarantee (or even an implied expectation) that that will happen without your input. Bring it up on the mailing list as something you would like to see.
Just general basics. I don’t understand when I should be using Java IO libraries against Scala ones. I don’t really understand how to deal with streams rather than strings.
This is how I use scala-guice with named attributes. [code example](https://pastebin.com/SdVgMWTW) If you are stuck, feel free to ask! 
Is there a way to avoid it being "stringly"? AFAIK we can't define constant in object, since it's still runtime value... in Java with public static String MY_CONST it would work... but not in Scala.. how can we achieve at least semi-good level of engineering when having to use @Named?
The javalib is not necessarily related to compiler most of it is pure scala code implementing the jdk. Some parts are inter-opt with C like java.io
The Shakespeare plays scrabble code benchmarks sequential, synchronous Streaming which presumably is not the primary use case Akka-streams was designed for (the origin of the benchmark is from a talk on Java 8 Streams by Jose Pramaud). There is a Twitter thread here :- https://twitter.com/akarnokd/status/813822736707551232 I also put together a blog entry, as my findings running the tests with an identical operator-set with my own library were significantly different to David's. https://medium.com/@johnmcclean/cross-library-stream-benchmarking-playing-scrabble-with-shakespeare-8dd1d1654717
I've used Scaladocs. Scaladocs with images not so much. So I guess it is not obvious to me. Is the scaladocs the problem, or including the images in the scaladocs?
You should (I think) be able to define string constants using a companion object (the equivalent of static class level attributes). http://docs.scala-lang.org/tutorials/tour/singleton-objects.html
no I dont think that works. I remember trying it. Even with stuff like "final val X: String" it's still not considered compile time constants.
Hey ysihaoy, I had problems using alpine-java docker image (see https://github.com/anapsix/docker-alpine-java/issues/28), could you check if you have the same problems? With the non-alpine java image I did not have these problems, basicly my problem is the following: $ cat /etc/passwd root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin bin:x:2:2:bin:/bin:/usr/sbin/nologin ... $ docker run anapsix/alpine-java cat /etc/passwd root:x:0:0:root:/root:/bin/ash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin $ mkdir test $ sudo chown daemon:daemon test $ ls -lk total 4 drwxr-xr-x 2 daemon daemon 4096 Jan 18 15:30 test $ docker run -v ~/test:/data/test anapsix/alpine-java ls -lk /data total 4 drwxr-xr-x 2 bin bin 4096 Jan 18 14:30 test (on host folder is owned by daemon, on docker it's owned by bin) this is not specific to your image, but I hope you are interested in checking this and either confirm you have the same problem or tell me what I'm doing wrong :) // Edit: I just saw that you build your own image, I will check if I have the same problem on your image // Edit: Yes same problem with your image, daemon/bin id's are swapped on alpine and the play app will run as daemon in the container, so it won't have owner rights to daemon owned folders on the host. This topic is a bit outside of my comfort zone, can anyone comment on this?
I didn't use my two images to build my app's release image. I will build a separate release image which basically run a jar from a JRE base image. Not sure this could solve your concern? Ah, sounds like I should post this in /r/docker? 
&gt; Ah, sounds like I should post this in /r/docker? Would be great if you want investigate this :) 
Sure, will let you know. As I haven't got this issue again.
Could you explain who/what is not considering them compile time constants? Is it Guice? The 'proper' way to make is less stringly is probably to use annotations bindings, like @Paypal in the example here: https://github.com/google/guice/wiki/BindingAnnotations
Do you mean using a binding annotation? Because that's different from Named. Or do you mean the part where they bind the properties to names? Sorry, still not 100% sure which part you're talking about. Did zsambek's example below help you out at all?
exactly, just be sure to create the snapshot and clear the triemap in one transaction
You can't do object Foo { final val NAME: String = "something" } class myClass @Inject() (@Named(Foo.NAME) something: Something) It would work if Foo.NAME was public static string
Perfect, thanks!
I'm starting a Scala console tetris. No fancy GUI just chars printed 
Yes His example was what I was looking for.
https://www.reddit.com/r/scala/comments/6mfpvh/collections_redesign_with_images_in_the_scaladocs/dk5k5ie/
did you update your build.properties file to point to build.version=1.0.0-RC2?
Your error is caused by a version mismatch. I don't use scalaxb standalone, but the SBT plugin allows you to set the Dispatch version and it generates the SOAP client code accordingly.
I have this in one of my projects that needed scalaxb: // NOTE: scalaxb 1.5.1 basically hard codes a dependency on dispatch 0.12.0 in its wsdl11.Driver class // attempting to use 0.12.x with x &gt; 0 results in a match error at compile time. // it also generates a deprecation error which would need to be disabled in the scalac options. val dispatchVersion = "0.12.0" val scalaxbSettings = Seq( scalaxbPackageName in (Compile, scalaxb) := "generated", scalaxbDispatchVersion in (Compile, scalaxb) := dispatchVersion ) You may need something similar.
I changed my build.sbt to lazy val myproject = (project in file(".")) .enablePlugins(ScalaxbPlugin) .settings( name := "ScalaXBPoc", version := "1.0", scalaVersion := "2.11.8", libraryDependencies ++= Seq( "org.scala-lang" % "scala-xml" % "2.11.0-M4", "org.scala-lang" % "scala-parser-combinators" % "2.11.0-M4", "net.databinder.dispatch" % "dispatch-core_2.11" % "0.12.0" ), scalaxbDispatchVersion in (Compile, scalaxb) := "0.12.0", scalaxbPackageName in (Compile, scalaxb) := "generated", scalaxbAsync in (Compile, scalaxb) := true, sourceGenerators in Compile &lt;+= scalaxb in Compile ) and plugins.sbt to addSbtPlugin("org.scalaxb" % "sbt-scalaxb" % "1.5.2") resolvers += Resolver.sonatypeRepo("public") But it still gets a bunch of compile errors on the code it generates. My project is checked in here just in case if you want to have a look https://github.com/abhsrivastava/ScalaXBPoc.git I also tried with many permutations and combinations of versions... but the thing just doesn't work!
Looks like things have been improved substantially in more recent builds of Akka Streams. https://twitter.com/viktorklang/status/841214884004155392 https://twitter.com/akarnokd/status/841335248138964995
Try to implement an enterprise logging solution for spark applications and not just log your messages BUT all messages, including those from spark himself. And then, we can resume this conversation!
&gt; One of the main problem we have is that hiring Scala programmers is a pain. Location? &gt; We are taking too long to fill positions. That is not very good for company grow. More employees != growth. Hire junior java coders and teach them. &gt; How viable is using Java for Big data/ML/Streaming applications nowdays? It depends: how many and how good employees you've? It can work but if you'll hire java coders you'll need more of them because they'll perform worse.
&gt; "One does not simply hire a Scala developer". In my city we don't have Scala devs at all, but tons of Java devs. I've worked in two places where they used Scala as main language for huge project and hiring problem there solved quite easily. You just hire Java devs and teach them Scala, the important part is to have a guy who knows Scala and all its libraries very well. 
Train java devs. Make positions attractive for new devs. It's hard to hire good java dev. Might be even harder now, since good java-ers move away to Scala.
Same situation here (or they were really hard to come by and thus very expensive). I was said Scala guy at my old company. It can be a real challenge at first but if the company is willing to invest the time it pays off (2 months roughly to full autonomy on a Scala/Spark project). But I will say the interview screening process surprised us. When we said we were using Scala and would be training them on it we actually had a few people decline the position right then and there saying "it wasn't a good fit for their skill set". Deeply set in their ways maybe?
What city are you based in? I've heard stories of other companies in which someone high up decided to restrict developers to Java, and other companies in the area used it as an opportunity to recruit all the best talent out of that company. Good developers can learn a new programming language fairly quickly. I suggest organisations that want to use Scala don't hire Scala programmers, but instead they select for good programmers who are willing to do functional programming in a strongly typed language. The onboarding process might take slightly longer to ramp up new hires to full productivity, but unless the company has high staff turn-over rates (which should be the first priority to fix ahead of decreasing the ramp up time) productivity is likely to be higher longer term.
My quick 2c: I disagree with the last statement. We started a group of Java developers on the Java flavor of Spark for ML and Data Ingestion jobs and they performed just as well as the Scala devs in terms of productivity. The code looked much worse (opinionated Scala dev) but the team was no less productive. It's all about the developer, not the language they code in necessarily
&gt; It's all about the developer, not the language they code in necessarily Not really, java doesn't enforce good code. Also, I don't believe they were as fast: coding is java is *really* slow.
Okay, I guess we can continue the conversation then. How is your outside requirement Spark's problem? Spark cannot possibly account for every instance of where one of their dependencies clash with whatever you are trying to do.
&gt; we actually had a few people decline the position right then and there saying "it wasn't a good fit for their skill set". Deeply set in their ways maybe? Guess quickly: how many java coder are willing to learn any non-java stuff?
The ones worth employing are more than willing. We hire software developers who may have a long background in Java but are more than willing to put forth the effort for the job they're employed for and it's worked quite well for us. Your overgeneralization of Java developers is quite off from what we've seen down here :) I don't discredit the notion that there are many who are set in there ways. I see them all the time. But that trait is not reserved to Java developers. Nor is writing poorly-written code. I've seen just as many Scala gurus write horrific code as I have seen Ruby, Python, Java, and just about every other language in between. All about the developer still in my mind
&gt; Your overgeneralization of Java developers is quite off from what we've seen down here It's my experience with them. But it's good to know you've different experience.
&gt; Not really, java doesn't enforce good code This isn't exactly true &gt; Also, I don't believe they were as fast: coding is java is really slow. Not if you use an IDE like Intellij or Eclipse, which 99% of Java devs do.
&gt; How viable is using Java for Big data/ML/Streaming applications nowdays? I'd say the gap between "you _can_ do it" and "you _want_ to do it" has, if anything, gotten even wider over the years. [Why Scala is taking over the big data world](https://www.slideshare.net/deanwampler/why-scala-is-taking-over-the-big-data-world), from Dean Wampler, is a few years old at this point, but I don't think anything has materially changed. If anything, I'd say the rise of stream processing platforms like [Flink](http://flink.apache.org/) and [Kafka Streams](http://docs.confluent.io/current/streams/index.html) only underscores the point. Kafka and Spark are written in Scala for good reasons. Flink is written in Java for unknown reasons, but knew it had to provide a first-class Scala API, so it does. As for hiring, I never have known a successful Scala shop that didn't have to make big investments in internal education. I mean literally 0.
&gt; This isn't exactly true Then how does it enforce good code? &gt; Not if you use an IDE like Intellij or Eclipse, which 99% of Java devs do. I used to be a java dev and an IDE for java is the *minimum* not a *plus*.
We are on Buenos Aires, Argentina. I think the same thing. With Scala, productivity starts at first very slow but them it start rising. The speed difference between Scala and Java is very palpable.
Location: Buenos Aires Yeah. I know. Actually, adding more employees makes everything much slower if you are not prepared. IMHO. Yup.
scala based company would be a huge plus point for me if I were looking for a job (senior java dev with 19 years of professional experience)
Yes. It's important to look at what your company is doing in context... are they just looking for average developers? Do they "not know" that it's hard to hire no matter what? Or are they consultancy and they dont care just need to rack up numbers for client?
&gt; Then how does it enforce good code? In perspective, Scala doesn't do a better job in enforcing "good code" compared to Java (if anything, I have seen more worse code made in Scala compared to Java, often because in Scala people tend to over engineer and do to much magic. The worse thing about Java is the boilerplate) &gt; I used to be a java dev and an IDE for java is the minimum not a plus. Thats besides the point I was making. People that code in Java use IDE's which generate a lot of code automatically for them (the IDE's also provide a lot of refactoring capabilities) which nullifies the point you are making. In fact, Java IDE's are **better** at this because of Java's deliberately simpler type system (which makes refactoring/auto completion a lot faster and more reliable compared to Scala)
The funny thing with Spark is that if you look at the code, its pretty much very "Java style" Scala. In fact the only reason I think Spark is using Scala over Java is because of the REPL and the fact that making "API"s is easier in Scala because of its very expressive syntax
exactly! 
&gt; In perspective, Scala doesn't do a better job in enforcing "good code" compared to Java (if anything, I have seen more worse code made in Scala compared to Java, often because in Scala people tend to over engineer and do to much magic. Have you seen a typical EE, spring or swing app? Over engineering and spaghetti are not a "maybe" but inevitable there. &gt; People that code in Java use IDE's which generate a lot of code automatically for them... Maybe generate half of the boilerplate you need to write. But there is still much to type and design - intellij can help you write more in scala than in java. By the time you'll auto-complete the imports I'll write the code. &gt; In fact, Java IDE's are better at this because of Java's deliberately simpler type system... Nope, that typesystem is garbage and a nuisance to work with because they can't enforce anything just get in your way. &gt; which makes refactoring/auto completion a lot faster and more reliable compared to Scala How so? If I refactor or try to complete something in Scala the speed is the same - or faster because there is far less code to process. And we are not even talking about all the generated garbage like POJO and its friends... 
Hello, I'm one of the designers/builders of ScalaQuest. Feel free to ask questions if you have any. We're hoping to see this project to the end - because we believe learning can be most effective when there's some fun mixed in, and this is the approach we're taking. We're doing our best to break down a deep subject into the smallest pieces possible, but not too small, so that anyone genuinely interested has a strong chance of going far with Scala. 
thats great!
Wow, I would be completely excited if presented with that. Even when I am at school (part time grad student) I am always surprised by the students who see working with a new language as a chore.
Follow up question do you think circe would be a lot more performant than lift. I have been running into issues with lift with similiar json files to my question with size greater than 150mb. Would you say circe would scale better?
I agree with this: source - contracting for company, mostly java devs, java services are terrible, scala ones are decent. regrets on their side not doing more scala. Thats coming from java devs.
Unfortunately I don't know of any benchmarks, but circe is quite fast. The reason is that circe does not use reflection to do the serialization, it rather works out much of the stuff at compiletime. I would say: create a pseudo case class and do whatever operations you are about to do with both lift and circe and bench them. I would not be surprised if you get much better performance with circe, but I don't know lift.
I strongly believe that what makes a good developer is not the language they know. Assuming that your company already have experiences Scala developers to teach good practicies and explain novelties, a good java developer will become as efficient in Scala as she is in Java in just a few months.
&gt; Have you seen a typical EE, spring or swing app? Over engineering and spaghetti are not a "maybe" but inevitable there. Yup, and I have seen worse in Scala, whats your point? I have seen stuff so over engineered that other people couldn't even work on the codebase (mainly because people were being very "smart" or "dogmatic" about their code) &gt; Maybe generate half of the boilerplate you need to write. But there is still much to type and design - intellij can help you write more in scala than in java. By the time you'll auto-complete the imports I'll write the code. Yes and in Scala you spend a lot of time waiting for code to compile (or resolve in SBT), and Intellij is a *lot less* performant in Scala than in Java &gt; Nope, that typesystem is garbage and a nuisance to work with because they can't enforce anything just get in your way. Its a weird definition of garbage if IDE's have a much better ability to work with it because the type system isn't so complex that you get polynomial complexity for basic completions on certain types ;) Try working on a Scala codebase that has a non trivial amount of LOC. In some cases I have maxed out memory because Intellij (or ensime) is forced to aggressively cache all the type trees with different permutations in memory in order to get decent performance &gt; How so? If I refactor or try to complete something in Scala the speed is the same - or faster because there is far less code to process. And we are not even talking about all the generated garbage like POJO and its friends... In any non trivial project, the auto completion in Java is light years ahead of Scala. I am reminded of this when I am occasionally forced to write some stuff in Java.
Hi, I'm Alejandro, one of the creators of ScalaQuest. Feel free to ask any questions and I'll do my best to answer.
What an interesting conversation. One side was respectful and the other not so much. I always find it interesting when someone uses individual experiences and extrapolates immutable facts from them. I also enjoy the "I've tried X and it was bad, therefore X is stupid!" as a tool for winning an argument. People often dislike things they don't understand, and, it's easier to dismiss something than it is to start understanding it.
Of course, the backend could not be written in anything else :) We do have a fair bit of Javascript on the frontend though. We used a framework called Phaser, which fits our needs very well.
I never see a problem learning a new language for a job, you can usually get to the level of mediocre programmer in two weeks if you have decent previous experience. In two months nobody will know you were fresh at the start. Because if you're a good dev you will understand the concepts below the language level. HR on the other hand usually only scans for what I put in my CV and compares to their requirements. And I don't feel comfortable lying about languages I don't know.
Nonsense.
Since no one in this thread is answering the real question, well, Java is fine for Big Data/ML/Streaming applications. In fact, more than fine. You should have absolutely no problems with it. On the contrary, your applications will actually compile and run faster.
Yeah, I don't think much of the Spark source code _per se_. It'd be nice to see a better implementation of the concepts that Spark pioneered (single programming model for batch, streaming, and query), but with a codebase that seemed to be of better quality.
Sounds like a recruiting fail. Most places will ask for Java developers and let them know they intend for the new hires to learn Scala.