That is the { question };
That is the { question };
&gt;In the original example second calls third and so it itself can suspend and should be marked as such. I'm not sure if we have a misunderstanding at some point, but I was also relating to the original example. I mean, the second function exists on its own right? And if I understand you correctly, you are saying that as soon as the second function calls the third function anywhere in the code (I suppose) its signature should be changed and marked as suspendable too. Do I understand you correct here?
&gt;In the original example second calls third and so it itself can suspend and should be marked as such. I'm not sure if we have a misunderstanding at some point, but I was also relating to the original example. I mean, the second function exists on its own right? And if I understand you correctly, you are saying that as soon as the second function calls the third function anywhere in the code (I suppose) its signature should be changed and marked as suspendable too. Do I understand you correct here?
&gt;In the original example second calls third and so it itself can suspend and should be marked as such. I'm not sure if we have a misunderstanding at some point, but I was also relating to the original example. I mean, the second function exists on its own right? And if I understand you correctly, you are saying that as soon as the second function calls the third function anywhere in the code (I suppose) its signature should be changed and marked as suspendable too. Do I understand you correct here?
&gt;In the original example second calls third and so it itself can suspend and should be marked as such. I'm not sure if we have a misunderstanding at some point, but I was also relating to the original example. I mean, the second function exists on its own right? And if I understand you correctly, you are saying that as soon as the second function calls the third function anywhere in the code (I suppose) its signature should be changed and marked as suspendable too. Do I understand you correct here?
&gt; typeclasses/tagless final FP variant, so FS2 is a no Can you elaborate ? Have you actually tried fs2 and come to the conclusion that it was too complicated for your usecase, or is it that you had a quick look at the documentation, saw that `fs2.Stream` was a higher-kinded type and elected not to use it ?
The latter. I will try to build a simple project with FS2 just to be sure, but my use case (composing many client websocket streams) it seems to be barely any documentation, too. I did try other parts of Cats ecosystem, though, and I didn’t like it. If I wanted to write things in Haskell, I can write in Haskell, as a consultant I am free to choose anything I like. But backporting idioms from Haskell to Scala looks hackish and filled with bad type of magic to me. Zio looks much better adapted to Scala than Cats/FS2, while retaining purism and laziness.
Im not really sure why you're so against higher kinded types? FS2 is more flexible and composable as a result. You could even use the ZIO effect when creating the streams. Im not saying that yout shouldn't try zio- streams or whatnot, but I'd encourage you to explore libraries that use this style as there are several great ones. &gt;But backporting idioms from Haskell to Scala looks hackish and filled with bad type of magic to me. Zio looks much better adapted to Scala than Cats/FS2, while retaining purism and laziness. If anything I'd argue that ZIO is more pure to Haskell than Cats IO...
Obscure errors, slow compilation time, namespace pollution, problems with code analysis and IDEs... the usual. For some, these tradeoffs are perfectly fine, but all these make me frustrated and less productive. I’d be happy to learn how to use higher kinds productively, but I couldn’t so far.
No Zio Streams experience here, sorry. But did you have a look at Monix?
There are a number of OSS projects written using ZIO Stream, including ZIO Kafka, ZIO SQS, etc. Documentation is definitely lacking, though! ZIO Stream is just a simple interface atop files/sockets/persistent queues/database results. FS2 is a more general-purpose library that can handle dataflow / reactive programming (a non-goal for ZIO Stream), so while they overlap a bit, they're not really competitive. Note that while FS2 _supports_ tagless-final, it doesn't _force_ you to use tagless-final. i.e. you can choose a concrete effect type. Now there are still type inference issues, of course, and documentation issues, so if your use case is not reactive / dataflow, ZIO Stream could be a great fit. In addition to FS2 and ZIO Stream, you might also look at Monix, which has various streaming facilities, at least one of which does not use higher-kinded types.
Yes, you are right: Changing every function to suspend just because it calls one is not good. In the example I suggested it because it is the only (except blocking) way to keep the return type (the result of third). It is not the only way though if you are willing to change the return type. In Future terms (rough equivialent): def first(in: A): Future\[B\] = second(in) def second(in: A): B = ??? // how do you get the result from thrid to return? def third(in: A): Future\[B\] = Future { ??? } // Couple of solutions def second(in: A): Future\[B\] = third(in) // -&gt; add suspend to second def second(in: A): B = Await.result(third(in), 0 nanos) // -&gt; run blocking def second(in: A): Unit = third(in) onComplete { ... } // -&gt; change method completelly I have compiled all the ways to call suspending functions in Kotlin here (except the low-level mechanisms): https://pl.kotl.in/nT1-syV__
&gt; I don’t like the typeclasses/tagless final FP variant, so FS2 is a no This doesn't make much sense, have you actually used it? You can use `fs2` without TF and you don't need to get into typeclasses at all. &gt; HTTP and Websockets? You can work with WebSockets in `fs2` though it's a bit underdocumented (PRs welcome!). Here's a real app showcasing WS: https://gitlab.com/dscala/discocat/blob/master/src/main/scala/org/discordscala/discocat/ws/Socket.scala That said, depending on your use case you could use ZIO Stream, Fs2 or Monix but you'd need to dig into their APIs for most advance use cases.
A lot of this seems anecdotal. Higher kinds don't introduce any of these problems by themselves. In fact, the standard library is full of em. Check out the scala collections as a great example. You aren't going to get away from them if you plan on using scala. A HKT like `F[_]` can be so many things. You could have an option, either, list, monad, effect, etc. Its just an abstraction; nothing more. Some libraries may introduce their own issues of course. Libraries that use shapeless tend to have awful compile times as an example. But those should be independently evaluated. If you're honestly evaluating and discarding libraries based on HKTs themselves... you probably shouldn't be using scala.
I love scalafmt! Still, formatting isn't everything. Much of linting has to do with restricting the use of known warts, or guiding users to best practices. Scalafix or wartremover are both great options here.
There's type classes for providing sort methods to the collections, allowing you to do something like `myList.sorted`. Check out [ordering](https://www.scala-lang.org/api/2.12.3/scala/math/Ordering.html).
I am not against any methods or options. Higher-kinded types are not my primary concern; I have to use them even within akka-streams signatures (e.g. as Source[Something, _] to ignore materialized values). I use Scala since 2010. At first, there was scalaz, which was “here are some Haskell idioms for those who likes them”, it didn’t pretend to be the One True Way and I used some cherrypicked parts of it (Validation). Then there was Shapeless, with more magic, which again I had to use for unions / coproducts until Dotty is there. Then there was the entire Typelevel drama, with One True Scala and wholesale importation of Haskell idioms. I was happily ignoring most of the Typelevel stuff, but now, akka-streams, my bread and butter, started to look dated. I am looking for more lightweight alternatives, which still support supervision and robustness. I will try FS2 of course, but I am almost certain that it will get me the same allergy as most of other typelevel stuff.
Yes, a few years ago. Wasn’t a good replacement for akka-streams, but perhaps things have changed back then. I rely heavily on supervision: it is well documented in akka-streams, but with Monix there were only bits and pieces of information.
I really enjoyed this tutorial. This one of these situations that get beginners stuck for ages :)
Do you have your .vimrc shared somewhere? Or another tutorial to setup vim+metals to be like yours? :D
\&gt; zio-streams is underdocumented I figured it couldn't be worse than cats. I was wrong. [https://scalaz.github.io/scalaz-zio/datatypes/stream.html](https://scalaz.github.io/scalaz-zio/datatypes/stream.html) The state of documentation in Scala's FP ecosystem is pathetic across the board. The implicit-typeclass-based APIs are hard to discover and make sense of unless you already have a self signed PhD in category theory. But why would you, nobody cares about that outside of this FP clique. This is the steep part of Scala's steep learning curve, not some collections or implicits. No one seems to care though.
Try to find a documentation for scalaz itself
If you look a little more closely, you’ll see that all the other data types except Semaphore are documented. That’s because Stream is still pretty recent and under changes (several PR in review right now). Documentation is important for ZIO maintainers and it will happen :) Also there aren’t any implicits and typeclasses used in ZIO streams, so it’s pretty easy to discover.
Thank you! I hope I can do more :)
I don't have a tutorial for that but my vim configuration is published here: [https://github.com/gvolpe/vim-setup](https://github.com/gvolpe/vim-setup)
Looking forward to it! Coincidentally, Stream and Semaphore sections were the only ones I clicked this time.
I think that you’re being too harsh on cats here. Sure, there are areas for improvement in their documentation as there is always a way to improve any documentation. But it is certainly far from being pathetic, at least for core and IO. I agree that FP in Scala is hard for average newcomer and documentation could (and provably must) certainly be better to tackle that, but it has to do with the fact that major part of Scala community and business still prefer mixed OOP/little-FP to pure FP approach by given credit to lightbend stack which they find more enterprise-like or whatever it is called. But thanks to very noticeable growing of interest of pure FP lately, much more effort is put into creating learning resources and improving already existing. And I think that typelevel in particular is doing great job in that sense. ZIO also seems to be concerned with this part. So saying that no one cares is an exaggeration
so you dislike fs2 but prefer over complicated slow solution like akka-streams? I was fine with you disliking typelevel stuff until you mentioned this. But cmon. Compare documentation of akka-streams to fs2, compare surface area of api, compare amount of types... Get of your high horse.
My company is a big bank, one of the ten biggest. This means a lot of red tape and auditing. We use git as repo, using maven and Jenkins for the builds. Sonar to review the code quality and Nexus as package manager. Code is deployed to shakedown and pre-production environment via Jenkins and ansible. We have multiple projects in git hosting the different components, each build is independent. Shared libraries are I their own project and produce a jar file. Communication to the team of changes in the shared libraries is very important but as we generate new versions done modules still use old versions. We are looking to remove maven and switch to sbt. One biggest problems right now is the inability to deploy to prod with the pipeline, some modules are too monolithic and sometimes coordination between the devs when using the git branches. The longest build takes 20 minutes, but there was a time it took 2hr until we optimized the code.
I'm on Linux. The terminal does not recognize sbt though, and simply points to a non existing command (sbt is installed though).
&gt; The longest build takes 20 minutes, but there was a time it took 2hr until we optimized the code. Could you discuss more about how you got an order-of-magnitude build-time improvement? Change in architecture? Change in library choices?
Member of a large overall org over here. We have several teams for each service/library; many team members will end up interacting with or directly contributing to multiple services. My current team, which maintains tooling libraries, is only about 6 people, but I've bounced around a bit internally. Version control is in an enterprise Github environment. Teams are given a github org and can manage the repositories however they see fit. I've seen every setup; mono repos and micro services. Some teams use sub modules, though most code is shared as artifacts. Artifacts are published to a private artifactory instance. Teams have their own artifactory repositories and they can be shared with other teams on request. Efforts on a general community repository is ongoing as well, which makes sharing easier. Teams are given a Jenkins instance which can be used however they see fit. The underlying Jenkins master/workers use shared hardware between teams. CI is typically run against pull requests and merges to protected branches. Protected branch commits tend to invoke new versions, which triggers publishing or deploying to a test environment. I've seen the vast majority of projects using SBT. Mill and maven used here at times. SBT has the upside of a vast ecosystem of tooling, so i tend to go in that direction despite some of its shortcomings. Mill is easier to reason about though, so some users go that direction. I tend to use SBT with a multi project approach. Not really for compile time, more for ease of development/versioning. Compile time is more heavily influenced by library inclusions, such as shapeless. Being smart with decisions there will yield more benefits in build time than fine tuning your builds.
Forgot to mention but if you find any issues getting it working please ping me either on Gitter or on Twitter and I'd be happy to help. I'm currently on vacation so bare in mind that I might not be super responsive. A great place to get help as well is the Scala Metals Gitter channel, a bunch of friendly folks there.
Parameter changes in spark ( parallelism 1 ) and better code.
That was runar bjarnason's interview on the corecursive podcast
Thank you! &amp;#x200B; It was actually [this talk](https://www.youtube.com/watch?v=GqmsQeSzMdw) that I saw, but the name you gave is how I found it.
Um.. you won't get any of that with fs2.
Kind of which there was a layer on top of ZIO which added things that are going to come up every time it is being used e.g. logging.
&gt; to handle Future's callback hell I'm pretty sure coroutines are not meant for that. They are "green threads". That's why the `suspend` keyword is there - it's not a sugar to hide `Future` - it's an entirely separate language feature, and marking function as such is required for compile magic to desugar those functions into proper coroutine signature under the hood. As for callback hell - usage of suspend blocks and `.bind()` function is just slightly clumsier analogue for Scala's for comprehensions Also i don't think coroutines are at all analogous to EitherT, as their errors are just as untyped as normal Future.
&gt; Essentially that would be the same as using Future as return type (or IO, ...) for all your methods in Scala. You're saying it like it's a bad thing. Returning Future/IO or other Functor type allows the caller of the function to decide whenever to unwrap the call and use/return the value inside (having to also deal with consequences of potential failure, etc) or to propagate it further up the call stack. Same thing with `suspend`.
Do you have benchmarks to back up the performant statement?
shops?
What you say makes sense and I am inclined to agree. But, then, why is it not the case that *everyone* uses linters, within the scala community?
Because even if something is bad, telling you that is only going to frustrate you if you don't know why and how to fix it. So it just easier to start without linters just for these people who just want to learn something, and as you project/team/etc matures add more compiler flags (though some of these TBH should just be defaults), wartremover options (with these, people might have different opinions about which should be on/off so it's better if the whole team decides) or formatting (some teams would prefer to format in IDE and check in CI, other would prefer git hooks for that). If you look at libraries, usually more popular libraries use linters. If not in the form of wartremover or similar then as compiler flags - [https://tpolecat.github.io/2017/04/25/scalac-flags.html](https://tpolecat.github.io/2017/04/25/scalac-flags.html) . If you don't see any linters nor any initiative towards having them, often it is either single man gig, where the man has opinions (e.g. not wanting their code to be to FP) or something which is more of a "better Java" than "Haskell fanfic" (though I think even Akka uses Scalafmt with a lot of compiler flags). Besides, there are people coming from Java, which *want to* use Guice Injector, Spring Framework and Hibernate, so it is not like everyone is on the same page regarding what they consider quality code.
&gt;Returning Future/IO or other Functor type allows the caller of the function to decide whenever to unwrap the call and use/return the value inside What you say is a good argument for \*certain\* functions. But it is an argument for wrapping \*all\* functions (or rather, their results) in Future/IO. And you actually name the reason for why it is bad yourself: because then I suddenly have to deal with potential failure when I want to retrieve the value inside \*even though there will never be a failure\*. But from the type signature I can't tell that, I will have to look into the whole implementation - in the worst case into all subsequently called functions.
&gt; But it is an argument for wrapping *all* functions Well, not "all"... Just ones that either receive IO-like as parameter or retrieve it as part of their work. To be specific, ones that do that and don't know how/need to handle unwrapping it &gt; because then I suddenly have to deal with potential failure when I want to retrieve the value inside You will always have to deal with that if you use IO/suspend or anything similar. The difference is whenever you will do it at the last possible moment (like in main() or even by delegating that to web/UI framework you use) or inside some internal function that has no indication it does such effectful/deferred operation, which may result in nasty surprises. To say nothing about the fact that not having *intermediary* function be suspend when it is calling suspend function inside it will result in you having to wrap the call to that function in a suspending function to use it in comprehension... tl;dr: if second() is suspend you can un-supend it with just `runBlocking(second)` and get the same effect as if it was normal function (with potential loose exceptions and such) and be able to use it in comprehensions, while the alternative would be calling `async(second)` to make it suspended again, which in terms translates to `async { runBlocking { third() } }` which is just plain silly and counterproductive as it doesn't really benefit from suspending much due to "runBlocking"
Monad transformers in Scala are [widely benchmarked](https://github.com/fosskers/scalaz-and-cats#results) and add up to 4x performance overhead per layer. So compared to the 2-layer monad transformer, the direct use of ZIO will be up to 8x faster.
&gt;Well, not "all"... Just ones that either receive IO-like as parameter or retrieve it as part of their work. With that I fully agree! What I not like is if there is a method which someone makes suspendable because it then is easier (or otherwise not) usable from some other method. &gt; You will always have to deal with that if you use IO/suspend or anything similar. No not always. My point was that there are sometimes functions that return IO or suspend even if they don't need to. In that case the IO will always be successful. A pretty plain example would be `def add(a, b): IO[Int] = IO.pure(a+b)`.
Yeah, but this one doesn't make sense. Why add an additional layer if you don't use it's functionality? It's a pure function with an unnecessary wrap around it. I think, don't quote me on that, Kotlin compiler even issues warning if you mark function as `suspend` while not calling any suspend functions inside it
&gt; Yeah, but this one doesn't make sense. Why add an additional layer if you don't use it's functionality? Thank you, that's exactly my point! It doesn't make sense. But developers still do it out of convenience. E.g. they call the method within a for comprehension twice and decide to make the method return IO instead lifting the result into IO within the for comprehension (or even use inline variables). And if a language makes it hard to work with a mix of suspendable and non-suspendable functions, the chance of the example I gave is way higher. If the compiler complains about it, that would be great!
&gt; And if a language makes it hard to work with a mix of suspendable and non-suspendable functions That's... Actually one thing i miss from Kotlin now that i switched to Scala. Sure, in Scala you can do `=` inside comprehension to do non-map operations, but it's clumsy. In Kotlin you can write any code inside the suspend block/function, and it will behave just like normal, with only suspend functions being given special treatment.
Have a look at [log4cats](https://christopherdavenport.github.io/log4cats/). More generally, ZIO has the relevant typeclass instances for [cats-effect,](https://typelevel.org/cats-effect/) so you can use it anywhere that's parametric in those typeclasses. In particular, you can use it in [http4s](https://http4s.org/) and [Doobie](https://tpolecat.github.io/doobie/docs/01-Introduction.html), so there's your HTTP and SQL use-cases out of the box.
That's great to hear, but I have a few doubts left. If you used Kotlin a bit, then can you tell me what you do if you have a function to make a web request and then have a list of urls. In Scala you then do `(results:List[IO[Result]]).sequence` or if I want it in parallel I will use `.parSequence`. Which is nice because library writers did that for me. How does the Kotlin way look like?
PwC's Scala team is based in Tampa
[logstage](https://izumi.7mind.io/latest/release/doc/logstage/index.html) has ZIO integration (e.g. print current FiberId together with each log) and makes structural logging and logging to JSON extremely easy!
Synthetic answer from several similar places that I worked: * ~100 person company, ~7-person team * Many repos, one per versioned project. Both within-project libraries (distinct maven modules in the same overall build in the same repo) and standalone shared library repos (which again usually consists of several distinct modules). * Maven for build. It's not very customizable, which ends up being an advantage with a lot of projects. Not being able to "mix in" common config (single parent only) is a slight inconvenience which we ended up working around by having profiles activated by the presence of particular files - e.g. ideally there would be a common parent, then a parent for scala projects inheriting from that, one for projects that use protobufs inheriting from the common parent, and so on. But in that case there would be no way to have a project that had both scala and protobufs. So instead we have a scala profile that's activated by the presence of `src/main/scala` and a protobuf profile that's activated by the presence of `src/main/proto`, both in the common parent file (which ends up being pretty big). * Versioning lives in the root of each repo - child modules are expected to use `${project.parent.version}`. We stick to the defaults for things like folder structure and CI tag format, even when we don't like them. * We had Jenkins for CI, with master builds, an "edge build" (a build that first bumps all dependencies to the latest snapshot and then builds - it's not an urgent problem if this is broken but it's there as an early warning), and branch/PR builds. * We tried to keep modules small, not explicitly for build times but it served that purpose as well. There's a nice organic evolution from separate source-level package -&gt; separate module in the same project -&gt; project in it's own right. The other thing was splitting out shared interfaces into their own "api" modules - mostly for code quality reasons (so that we couldn't accidentally use an impl directly in another module) but it serves to make the build faster and more parallelisable too. My current place is a bit different, and I'll have to be vague. Large codebase, large team, monorepo. Built using gradle, but with a specific team reviewing changes to gradle files and keeping them consistently. A lot of effort in the direction of improving build times, but I wouldn't call the resulting build times reasonable TBH.
It depends. Kotlin doesn't have such powerful functional stuff baked in. `suspend fun doSuspendThing(a: Int): Int` If you want them to be executed sequentially, you just call them as usual in a loop `val list: List&lt;Int&gt; = (1..10).map(doSuspendThing)` It will perform every operation sequentially and you will get a proper list of values. With actual async, you have to just iterate and call `.await()` on each `val listJ: List&lt;Job&lt;Int&gt;&gt; = (1..10).map { x -&gt; async { doSuspendThing(x) } }` `val list: List&lt;Int&gt; = listJ.map(it.await())`
Ah, so `Job` kind of is what `Future` is in Scala? Then it might be possible to abstract over it! Thanks for the clarification.
Yep, I don't remember if it's Job or Deferred, but it's one of those. Normal suspend functions return plain values, but can only be called inside suspending context. async is a coroutine builder that can be called anywhere and returns a wrapper around the value (like Future[]), but the `await()` function on it is a suspend function, so you can only wait for it inside a suspending context. Something like that. It's been over a year for me...
Is there a good way to check your dependencies into source control with SBT, and to use those rather than an ivy cache? I've tried using `retrieveManaged` which does add the dependencies, however doing `sbt package` still downloads all dependencies into the ivy cache. &amp;#x200B; Currently I just have the ivy cache local to the repo and I'm git ignoring the ivydata-\*.properties files, but I was hoping for some mechanism that wouldn't bring in so many extra files as the ivy cache.
Is there any github repo to play with presented app ?
&gt;Performant Functional Programming to the max with ZIO See [https://gist.github.com/cloudmark/483972d5b469f354984dd06cb845f223](https://gist.github.com/cloudmark/483972d5b469f354984dd06cb845f223)
You sir have my thanks:)
Thanks for sharing ! The "natural" progression of modules makes good sense, I've had experience with code being split to a separate project prematurely, resulting in annoying workflows caused by not thought-out interfaces. I've got a couple follow-up questions, if you don't mind : * If you're aware, what was the rationale behind not using SBT in either cases ? * The monorepo approach seem to aim at facilitating consistency within an organisation, at the cost of build times and possibly impact on tooling in general. Do you have an opinion on the trade-off ?
software companies, in some dialects or slang apparently
This is very old and common usage; https://en.wikipedia.org/wiki/Job_shop Entrenched in the batch-processing days, when service providers were called "IBM shops", etc... based on what kind of jobs they could complete for you.
Anybody has a Monad use case with Spark for ETL?
&gt; If you're aware, what was the rationale behind not using SBT in either cases ? In the previous cases, I argued for switching to consistent use of maven (we had been a mix of SBT and maven) after SBT went through yet another incompatible upgrade. Fundamentally SBT never solved any problem that I saw: it has some very limited/specific support for cross-building against multiple scala versions that doesn't actually solve the general problem (if you need to cross-build along any other axis, you have the same problems as always - projects that build against multiple versions of scalaz already demonstrated this). Its entire configuration/model is overcomplicated and underdocumented, and it encourages the use of arbitrary code in the build definition. I've never understood why anyone liked it. In the current case I think Gradle happened as a result of an organisational decisionmaking/evaluation process that isn't really set up very well. I don't think it was a good choice but at this point the organisation is too heavily invested already. &gt; The monorepo approach seem to aim at facilitating consistency within an organisation, at the cost of build times and possibly impact on tooling in general. Do you have an opinion on the trade-off ? I think consistency is overrated and a monorepo is a lot more trouble than it's worth. Ultimately a large organisation is always going to have a certain amount of duplication of effort, and at some point the communications overhead of making sure everyone does things consistently becomes higher than any value you derive from it. If one team is using one library for doing x and another is using a different one, it's really not that big a deal (the famous Amazon "platform memo" is an example of taking this philosophy even further); those parts of the codebase that frequently interact will naturally converge on a common style, but you don't need to force it. I have seen switching to a monorepo improve matters, but IMO it was really acting as a crutch for poor test coverage and a system that couldn't really be tested except as a whole. And the biggest reason for that was dividing the functionality the wrong way: we had too many "horizontal slices" based on what a technical layer did, rather than "vertical slices" based on business/user-level functionality. I would say that it's vital for a project's test suite to live in the same repo as part of that project (which I would've thought was common sense) and to only slice projects at a point where you can be confident that any release (that passes tests) will be "good" without having to build a downstream project first. Rather than having a "libraries team", say, let each user-facing team pull out libraries as and when they emerge from their work. Sometimes that will mean overlapping libraries within the organisation, but ultimately that's a lesser evil.
Apparently, I'm showing my age :)
Interesting discussion in the r/programming cross-post, including comments from the lead of Oracle's Project Loom (fibers on the JVM), an imperative-looking approach to concurrency abstractions which will likely compete with Scala Future, Monix/Cats/Trane Task, and ZIO. &amp;#x200B; [https://www.reddit.com/r/programming/comments/bqf8nq/performant\_functional\_programming\_to\_the\_max\_with/eo413wg/](https://www.reddit.com/r/programming/comments/bqf8nq/performant_functional_programming_to_the_max_with/eo413wg/)
&gt; how big is your org ? how big is your team ? I work at a small company on a smallish to medium-sized but growing team. &gt; how do you leverage version control : one repo hosts everything ? each application have their own repo ? do you have shared libraries in separate repos ? do you use git-submodules ? When I first joined, things were very much polyrepo, but we have been trending away from creating new repos for every new thing to cut down on the overhead. Right now I'd say the rule of thumb is that things that tend to change together by the same people tend to to live in the same repo. We do have some libraries in their own repos, and we don't use git submodules or git subtrees. &gt; in the case of "one repo hosts everything", does the CI build/test everything all the time ? do you have refined triggers that only build what changed ? Build times are trending upwards as we add more code. The plan is to address this when we have time or whenever it becomes a blocker, whichever comes first. We are not full-on monorepo, so our build times are still measured on the order of minutes. &gt; what build tool(s) do you use ? any particular upsides/downsides ? We use SBT and a CI server. I'd say right now its biggest weakness is build pipelines can be defined, but there is no automatic correspondence between pipeline dependencies and the actual application code dependencies. &gt; how do you use the build tool ? do you strive to have small compile-units, or do you just keep one module ? We do full clean builds and test runs on CI every time currently. We have made some use of SBT modules, however, but this currently does not factor in to build times. &gt; any particular decision to ensure that CI build times are kept reasonably low ? We use SBT modules now, and theoretically we could trigger builds only on changed and downstream modules in the future. We have a lot of other higher-priority items to consider now than build times, though.
 object bartender { val str1 = "ers" val str2 = "rap".reverse val str3 = "amet" def request(preference: String) = { s"$preference.Secret word: $str2$str3$str1" } }
[https://underscore.io/training/courses/essential-scala/](https://underscore.io/training/courses/essential-scala/)
Because to execute a suspendable function, you need to provide the context that the runtime uses to suspend that function if required. If you want to run "min(x, y)" then it would be annoying to provide that context when clearly it doesn't need to suspend.
Thank you TJSomething and D\_4rch4ng3l. For the code example, thank you. :) Most of the translation makes sense, except for the lone "s" on 7th line. So, I learned that prepending s to a string literal allows the usage of variables directly in the string. Learned something new. :) If you were to re-write this code, what is a more clever way of obfuscating text in Scala code? *Essential Scala* is a goldmine! Thank you. I am reading it already. :)
Thank you for the code translation. :) Most of the translation makes sense, except for the lone "s" on 7th line. So, I learned that prepending s to a string literal allows the usage of variables directly in the string. Learned something new. :) If you were to re-write this code, what is a more clever way of obfuscating text in Scala code?
*Essential Scala* is a goldmine! Thank you. I am reading it already. :)
This one's kind of obnoxious, but: "aunrmtges".sliding(3, 3) .toList .transpose .flatten .mkString
Thank you for the code. I have some problems compiling it, due to the use of the `retry` feature. I made some modification which improves the situation: import scalaz.zio.clock.Clock and modified the line new console.Console.Live with Clock.Live with Writer[Log] { def writer = wref }) With these modifications, the retry in the `main` function compiles, but not the one in the `getAllUserData`. I am not proficient enough in scala to understand exactly what happens (the `Clock` is not accessible but the `Console` is ?), nor how to correct it. Any clue ?
Thank you for the code. I have some problems compiling it, due to the use of the `retry` feature. I made some modification which improves the situation: import scalaz.zio.clock.Clock and modified the line new console.Console.Live with Clock.Live with Writer[Log] { def writer = wref }) With these modifications, the retry in the `main` function compiles, but not the one in the `getAllUserData`. I am not proficient enough in scala to understand exactly what happens (the `Clock` is not accessible but the `Console` is ?), nor how to correct it. Any clue ?
Thanks!
I'm not sure I follow? `RDD` does form a monad (for at least some practical notions of equivalence) so you can write a monad instance for it and then reuse library functions that rely on having one (e.g. if you have a tree structure you've defined with matryoshka and want to `cataM` down it with operations that get datasets from HDFS) - I've done that in the past.
I've been trying to use `json4s` with custom serializers, and have found the automatic reflection-based formatting error-prone, especially for things like encoding an enum as a `sealed trait` + `case object`s: if I forget to include my custom serializer, things will silently serialize to useless representations like `"{}"`, instead of the custom form or an error. Is there a way to make this less error-prone? (I wrote up [a StackOverflow question](https://stackoverflow.com/q/55719643/1256624) with more details, and now a bounty too!)
If you only have a few data types that your values can take, you could enumerate them in an adt sealed trait Data { def str: String = this match { case Str(s) =&gt; s case _ =&gt; throw new Exception } def bigDecimal: BigDecimal = this match { case BigDecimal(v) =&gt; v case _ =&gt; throw new Exception } ... final case class Str(value: String) extends Data final case class BigDec(value: BigDecimal) extends Data ... and then you could pass around essentially `Map[String, Data]`. There will be a problem if you want deterministic ordering of your columns/fields (HashMaps will jumble your keys up). This will be fixed in 2.13 with the addition of `SeqMap` which preserves key ordering. Otherwise you could use the `mutable.LinkedHashMap` if the ordering is important, and you don't mind using something mutable.
How endtest is spamming up tech subs every day with [multiple accounts](https://www.reddit.com/user/boss_scarbos) and [focused spam posting](https://www.reddit.com/user/dragnea_presedinte) that clearly breaks [reddits self promotion rules](https://www.reddit.com/wiki/selfpromotion.)
Thank you
Hm. Yes. I think this might work.
I've had this issue with json4s for a long time. The library is pretty much built around the assumption that every json value is primitive, sequential, a map, or an object translated directly by the fields of a case class. For ADT objects, the value is the class itself and not its fields and because json4s uses runtime reflection instead of compile time reflection it gets treated like any other object, one which of course has no fields. I'm pretty sure addressing this will require updating json4s itself to add some control knobs like `allowEmptyObjects` or an `EmptyObjectStrategy` which can throw an exception or use a simple class name. Then make `Extraction.internalDecomposeWithBuilder.decomposeObject` do something sensible with these features. And the deserialization component would need some changes to recognize when the value corresponds to an `object`. Maybe json4s could probably provide a helper for creating the custom serializer too, allowing something like `CustomSerializer.adt(A, B)`.
Also, you could add a name of column to `Data` trait, and make it parametric.
I'm assuming what you mean by this is that I could have a function in the sealed trait that would resolve the wrapper to the appropriate value? That way the client of Data would just need to call `resolve` to receive the appropriate value? Something like this? &amp;#x200B; sealed trait Data { def resolve: T = this match { case Str(s) =&gt; s } &amp;#x200B; Not sure how I would write that?
Once I had to extract around 160 columns from a table to a Scala class. For reasons I don't remember now I excluded Hibernate and other ORMs and used some low level stuff, jdbc implementation probably. Constructor parameter was DbRow object then each column was mapped to each field like `val field: String = dbrow.get("column")`. Plain pain as hell but app execution was damn fast and that mattered the most.
&gt; Sonar to review the code quality Do you analyze Scala code with Sonar? I haven't tried in a year or two, and it's one of the few things I *really* miss from my Java days.
Yes, is not as good as java but it makes an acceptable job. For example it detected recently a password harcoded in the code on a test project.
scala.io.Source tends to be bashed for its non-functional, Java-like API, but it's simple and it works, so that's what I use.
Could you please describe your supervision use case? It's definitely different than in Akka Streams but maybe it can be solved nicely and I'd gladly add relevant examples to the documentation
Cool, thanks! I've got some googling to do!
[better-files](https://github.com/pathikrit/better-files) or the Scala/Java standard library
One minor improvement in typing would be to generate the column names into a **Java** enum class - these interoperate perfectly with Scala.
Just remember that sonar adds an additional control but it doesn't replace peer reviews.
ListNode cannot be [+B] since it accepts and returns B's. I actually has to be invariant in B.
Of course! I used Sonar for a few years at a past job, and we always looked at the metrics in addition to doing regular reviews.
Wow, thanks! My application collects market data via numerous (20-30) simultaneous Websocket and HTTP client streams. Errors there happen routinely (disconnections, API errors, format changes etc). When a data source goes down, the system should attempt to reconnect periodically via e.g. backoff supervisor. The same thing is necessary for any other inherently streaming IO applications (TCP sockets, video feeds etc.)
If you are doing any sort of filesystem or subprocess work at all, consider trying https://github.com/lihaoyi/os-lib. It's used in a lot of libraries (Ammonite REPL, Mill build tool, ...) and makes a lot of that work both easy, performant and (relatively) type-safe
`scala.io.Source` is adequate for many needs but it's not a good solution for large files or long-running processes. Its common usage pattern leaks because using it doesn't close. It's really meant for reading source code in the compiler, iirc.
I have always thought that `HList` is the proper solution in such cases, the rest being mere workarounds. On the other hand, shapeless may bring its own can of worms into the project, such as a bunch of spurious errors in IDE. Out of curiosity, what kind of entities are represented with this 400-field-jumbo-monolithic models? If I'm honest this does sound like a poor DB schema design to me, although as often is the case with DB schemas it is either too late to change it or you are in no position to do it.
It can, just needs a type param on prepens.
I wrote “eie”, mostly because I just enjoy writing: import eie.io._ val file : Path = “path/to/file.txt”.asPath.text = “hello!” “Another.file”.asPath.bytes = file.bytes
Author here. This is a first draft of an idea I'm toying with, generating properties (in the sense of property based testing) from decision tables. There's still a fair amount of work to do, at the very least on the documentation, but it might still be of interest to some.
Assuming we're talking about a SQL database with a JDBC driver, consider that [Doobie supports reading and writing either \`HList\`s or records out of the box](https://tpolecat.github.io/doobie/docs/12-Custom-Mappings.html#column-vector-mappings). Honestly, I think Shapeless' records are its most unsung heroes. In my mind, they offer all the benefits of \`case class\`es with none of the drawbacks, and offer a significant array of additional desirable features, such as easy field addition and removal. Of course, this assumes Doobie, but if you're doing JDBC in Scala, you should be using Doobie anyway, I think.
&gt; Also curious if any of you use cats.effects to do so. Yes. Let me recommend [fs2's `io` module](https://github.com/functional-streams-for-scala/fs2/blob/v1.0.4/io/src/main/scala/fs2/io/file/file.scala).
So when should Akka Cluster be used?
This sounds like FUD, honestly. The claim that ScalaZ "didn’t pretend to be the One True Way" and Typelevel did is exactly the opposite of my experience with both organisations' support channels over several years. FS2 has no dependency on Shapeless or vice versa; those are completely orthogonal choices. There are legitimate reasons to dislike FS2[1], but smearing Typelevel is not warranted. If your concern is Shapeless-like compile times, those are specific to Shapeless and not any kind of general issue with Typelevel libraries. [1] FS2 is indeed underdocumented - I actually found the easiest way to learn was starting from the play-iteratees documentation, if you imagine their types as being the fs2 types but specialised to `Future` - and you might like to define some type aliases along those lines, at least to start with.
What do you mean by "inter-service communication" ? &amp;#x200B; I think micro-services should not message each other as it generates cascading failures. What the akka is saying is not to rely on akka message to send messages on "different" types of micro-services. Example: a product service asking a user service for something (at this point we are entering event sourcing with message brokers + read/write stores I think). &amp;#x200B; What if the single service cluster is geographically distributed ? (ie.: a user service running as a cluster, distributed in 2 data centres, one in New-York and one in London).
He means deploying disparate services in a single cluster.
Akka does have some specific consideration of Multi-DC deployments of a service. Check out: [https://akka.io/blog/2018/01/17/multidc](https://akka.io/blog/2018/01/17/multidc) &amp;#x200B; Which may be perfectly fine for a system.
The `toString` provided by `case class` can be overridden if you want. They just provide some sane defaults. I am assuming `case class` because just extending `AnyVal` would not return `Wrapper(value)`. `override def toString: String = value.toString` is just `def` that doesn't add any extra fields so you should be OK.
Chartboost | Software Engineer, Scala (Senior and mid level positions) | San Francisco, CA and Barcelona Spain | Onsite | Full-Time My name is Mark and I'm looking to grow my Ad Serving team here at Chartboost! We are currently looking to hire for our San Francisco and Barcelona offices, and we are open to considering relocation for interested candidates. &amp;#x200B; We are looking for passionate backend engineers who love Scala and typed functional programming (cats, cats-effect, http4s, akka-http, shapeless) to join our team and help us build the best advertising platform for mobile developers. You'll work on a system that processes tens of thousands of requests per second and conducts a real-time auction to find and deliver the most effective ads from the Chartboost Network. &amp;#x200B; The team's working on some exciting initiatives! Please find the job descriptions listed below and apply directly. &amp;#x200B; [San Francisco](https://boards.greenhouse.io/chartboost/jobs/1596880) [Barcelona](https://boards.greenhouse.io/chartboost/jobs/1533886)
Chartboost | Software Engineer, Scala (Senior and mid level positions) | San Francisco, CA and Barcelona Spain | Onsite | Full-Time My name is Mark and I'm looking to grow my Ad Serving team here at Chartboost! We are currently looking to hire for our San Francisco and Barcelona offices, and we are open to considering relocation for interested candidates. We are looking for passionate backend engineers who love Scala and typed functional programming (cats, cats-effect, http4s, akka-http, shapeless) to join our team and help us build the best advertising platform for mobile developers. You'll work on a system that processes tens of thousands of requests per second and conducts a real-time auction to find and deliver the most effective ads from the Chartboost Network. The team's working on some exciting initiatives! Please find the job descriptions listed below and apply directly. [San Francisco](https://boards.greenhouse.io/chartboost/jobs/1596880) [Barcelona](https://boards.greenhouse.io/chartboost/jobs/1533886)
You could look at Monix Observable as having `Stop` supervision strategy but you can easily change to `Restart`: See this gist: https://gist.github.com/Avasil/e5e840f4a09e57962015de19bde802c5 Note that this restart entire stream but it seems like it's not an issue in your use case. Basically there is no built-in notion of supervision but most of it can be achieved composing simple constructs with the benefit of easy customization (e.g. you can handle different errors differently) I will add a section in the documentation covering supervision strategies from Akka Streams over the next couple of days so if you have further questions or suggestions I will be happy to hear it
So to be clear, a single cluster should be for scaling N instances of a single service, period?
I would expect that you could scale multiple service types in a single cluster if they are not communicating with each other across akka.
In this case exactly, N instances of single service type
By "inter-service communication" I meant communication between different types of micro-services (not different instances of the same service). It shouldn't be used this way, however people still do this.
Good initiative.
You can just override the toString, but it will only be called in cases where the class will be allocated anyway, thus rendering the \`AnyVal\` part moot (it acts like a normal class in those cases)
But doesn't that lead to binary compatibility problem between all services? to pass messages each services must be using same JDK for binary serialization of akka messages?
why should a cluster boundary cause that?
I meant 2 clusters (having a service each) sending messages to one another. That would force them to have binary compatibility across the cluster. I guess the only way out of this would be either for clusters not to send messages to each other or to use something like Avro for inter cluster communication.
right but it's the same problem as if they are within 1 cluster, no?
Yes just like the original article mentioned. I was trying to comment on &gt; a single cluster should be for scaling N instances of a single service, period? So by deploying a single service per cluster you are not solving all the problems listed in the original article.
If you use the same version of Protobuf to serialize akka messages, there should not be any binary incompatibility.
As mentioned in the article " This concerns not only Akka versions in all of the services, but also Scala and Java versions if you are still using **java serialization** (you shouldn't) " :)
Yup, in all my years of coding, language built-in serialization has always been a minefield, whether it's Java, Python, C or Php.
You can absolutely call overridden \`toString\` on a value class without boxing, as long as the expression that you call \`toString\` on is statically typed as the value class itself.
Yes, overriding \`toString\` will not cause any more boxing that is already present with value classes.
Here's the most terse way, but for very large collections, this is not very performant: def groupByKey[K, V](xs: List[(K, V)]): Map[K, List[V]] = { xs.groupBy(_._1).transform(_._2.map(_._2)) } Here's a more efficient way of doing it: import scala.collection.mutable def groupByKey[K, V](xs: List[(K, V)]): Map[K, List[V]] = { val agg = mutable.Map.empty[K, mutable.Builder[V, List[V]] for { (k, v) &lt;- xs } { agg.getOrElseUpdate(k, List.newBuilder[V]) += v } val result = Map.newBuilder[K, List[V]] for { (k, vs) &lt;- agg } { result += (k -&gt; vs.result()) } result.result() } It can also be done in a way that supports any collection rather than only `List`, but if you're just trying to get started, I doubt you need that.
Note that in 2.13 this will be available on `Iterable`s in the form of method `groupMap` scala&gt; List(1-&gt;1, 1-&gt;2).groupMap(_._1)(_._2) res0: scala.collection.immutable.Map[Int,List[Int]] = Map(1 -&gt; List(1, 2))
Others have given more idiomatic answers which are probably a better approach in the long run, but since you ask for 1:1: def groupByKey[K, V](xs: List[(K, V)]) = { def lookup(k: K, v: V, xs: List[(K, List[V])]) = xs match { case Nil =&gt; List((k, List(v))) case (kp, vs) :: xs =&gt; if(k == kp) ((k, v :: vs) :: xs) else (kp, vs) :: lookup(k, v, xs) } xs.foldLeft(List.empty[(K, List[V])])((acc, kv) =&gt; lookup(kv._1, kv._2, acc)) } groupByKey(List((1, 1), (1, 1), (1, 2), (2, 3)))
TIL, thanks
Thank you, this looks interesting. Are there any ways you know of to pass a tuple in Scala without *unpacking* it, as you do? F# handles this *natively* (as shown above), and I think Python allows it using `*` notation.
Call the function with ```tupled``` val myMultiArityF: (String, String, Int) =&gt; Person = (first, last, age) =&gt; Person(first,last,age) val myPersonTuple = ("Bob","Smith",27) myMultiArityF.tupled(myPersonTuple)
Not directly. The `*` notation is used for (homogeneous) sequences rather than tuples. If you just want a lambda that takes a tuple you can write a partial function literal (with `case`) and (ab)use the fact that this is treated as a (subtype of a) total function: xs map { case (k, v) =&gt; ... } Or as /u/jackcviers says, you can declare a regular function and then call `.tupled`. But for the case where you want to mix tuples with non-tuple arguments the best you can do is a destructuring assignment: def doSomething(kv: (K, V), i: Int) = { val (k, v) = kv ... } Otherwise it comes down to the `_N` syntax or a `match`/`case` block. In idiomatic Scala one tends not to use tuples that much, since it's so easy to declare a `case class` and most of the functionality for tuples will work just the same (you can still `match`/`case`, you can still do destructuring assignments, you can still handle them generically with shapeless or libraries that are built on it).
Our Scala projects are small (couple of hundreds LoC), so build times aren’t a concern. We use sbt and Gitlab CI to deploy them to Kubernetes. I was thinking about switching to Gradle but it requires migrating from sbt-native-packager and sbt-assembly (required for Spark jobs) so I decided it’s not worth it
That is why in Scala 3 Tuples are implemented as HLists are and will superseed HLists.
Hi, struggling with a small piece of code, compiler fails with \`could not find implicit value for parameter F: F\[doobie.Transactor\[F\]\] t ← Transactor\[F\]\` I realize that this code makes little sense, that summoner should actually return \`Transactor\[F\]\` and flatMap is not needed there. But what I REALLT want to understand, is that why compiler complains? \`\`\` **object** Transactor { **def** apply\[F\[\_\]\](**implicit** F: F\[Transactor\[F\]\]): F.**type** = F } **def** listItems: doobie.Query0\[App\] = *???* **def** listApps\[F\[\_\]\] = **for** { t ← *Transactor*\[F\] x ← *listItems*.to\[List\].transact(t) } **yield** x \`\`\` thanks !
The line `t ← Transactor[F]` calls `Transactor.apply` which requires an implicit parameter but there is not one in scope. Because `listApps` is still generic over `F[_]` you will need to add the implicit requirement to that method as well. def listApps[F[_]](implicit F: F[Transactor[F]]) While I don't know your setup, I can say `F[Transactor[F]]` looks odd to me, and I suspect what you want is actually `Transactor[F]`. object Transactor { def apply[F[_]](implicit T: Transactor[F]): Transactor[F] = T } And this would allow you do add the implicit to `listApps` with def listApps[F[_]: Transactor]
We are using `maxConnectionPerHost` property for HTTP config: https://gatling.io/docs/3.1/http/http_protocol/#max-connection-per-host
This seems to be setting each user's amount of connections rather than the total amount of connections from Gatling at any one time (I may be wrong). I have 1000 users each sending one request. But I want 50 users/connections to be sending at a time.
In the closed model they describe, there's an option constantConcurrentUsers which you could set to 50 for your scenario. `setUp(` `scn.inject(` `constantConcurrentUsers(50)` `)` `)` [https://gatling.io/docs/current/general/simulation\_setup](https://gatling.io/docs/current/general/simulation_setup)
But how do I then limit it to send only 1000 total requests?
Both of those answers to those questions are in Akka in action book.
I usually do 50 users for duration of 10 minutes for example - and then can compare the number of served requests between versions of my server software.
Ah, yeah, I've tried that. The problem with that is that the total number of requests is implicit (a byproduct of doing n requests for t seconds) rather than being a hard limit. It is also quite slow. If I want 50 at any one time but they take 0.1s to complete, Gatling is doing nothing for 90% of the time
Thanks for pointing to that book. It says constructor injection of ActorRefs is common. But recommends system.settings.config over other ways of injecting configs. Does anybody disagree with this approach?
To simulate `ab -c 10` I use `val scenario = scenario("Load Test").forever(exec(http("Test").get("/")))` `setUp(scenario.inject(atOnceUsers(numberUsers))).protocols(httpConf)`
Hey, about the questions you have made: &amp;#x200B; * Use actor ref instead actor selection. It's easy to test and you can inject TestProbes instead real actors. After severeal years doing akka architectures i will recommend you to try to test your actors as a black box test and try to test more than one actor at the same time. Try to test behaviour of the system instead actor implementations. Try to isolate stateful code in actors and statless in services. With this approach you can do stateless services and inject them into the actors. Then you can test stateless services in isolation and the test actors mocking this services. IMHO things like testing actor state is an antipattern that in long term makes your tests very fragile. Testing actor systems is more about testing communication and behaviour than testing implementation so try to avoid the "one test per class" phylosophy. Take a look on these images, this is how i test akka applications: [https://imgur.com/a/rvaJtvF](https://imgur.com/a/rvaJtvF) (red lines is the subject under tests, the black ones is what i mock in every test) &amp;#x200B; * Inject configs bc the same. It's easy to inject custom config for test specific scenarios than rely on system settings.
Hello! I've been on a Akka project for a few months now. Configuration using application.conf is fairly efficient, easy to read, powerful and modulable, at least from someone coming from the Java/Spring world like me. I would recommend using it. Sending ActorRefs through constructor is also a great way of injecting your actors, unless your project is extremely complicated. Do check out what you can do with Guice, or another injection framework, if you're worried about your project's growing pains, but it should be fine otherwise.
No, if you tell it "hold 50 active users at any given moment" it will launch new request as soon as one ends.
I used swagger annotations in my previous project and it was total crap. (In general I consider quite a lot of swagger tooling done my its authors to be of poor quality). Currently I am for using some API describing algebra that you can easily turn into client/server/documentation: see [https://github.com/julienrf/endpoints](https://github.com/julienrf/endpoints) [https://github.com/softwaremill/tapir](https://github.com/softwaremill/tapir) and similar.
2.2.1 Talks about creating other actors like this: def createTicketSeller(name: String) = context.actorOf(TicketSeller.props(name), name) Taking arguments in your props and passing them through to the constructor of your class allows you do specify values that would be your dep injection sort of solution. Page 55 has this about testing with different values: "change internal state when it receives a message, multi" in { import SilentActor._ val silentActor = system.actorOf(Props[SilentActor], "s3") silentActor ! SilentMessage("whisper1") silentActor ! SilentMessage("whisper2") silentActor ! GetState(testActor) expectMsg(Vector("whisper1", "whisper2")) } You can replace Props[SilentActor] with Props(new SilentActor(myTestState)) I believe. Then for the config it talks about how you can use `mySystem.settings.config` on page 154.
&gt; I used swagger annotations in my previous project and it was total crap. I can totally relate :D Currently we are having issues with generic response classes and it is a pain in the ass. I did not know about the projects you are using. I will have a look right away.
Not quite sure what your asking, but [SBT Swagger Code Generator](https://github.com/unicredit/sbt-swagger-codegen) is a nice tool.
I was wondering what your approach for generating API documentation is. Or if you generate this at all. Your suggestion seems to go the other way around. Interesting.
After many attemps, I solved the compilation problem. When using `retry`, the compiler inferred `Any` as the error type. To help the compiler, I give now the type parameters when calling `retry`. For example : products &lt;- getAllUserData("wile@acme.com").retry[Writer[Log] with Console with Clock, Error, Int](Schedule.recurs(10))
Oh right. But how do we tell it to do n requests in total?
I have the same experience. I used http4s-rho and cannot recommend it. I spent way too much time to figure out how to make it generate the documentation as I liked it, until I eventually gave up.
Thank you for taking your time and answering; I finally had the time just now to look through your links. Sadly, this is not really what I am looking for. I know how to use the Play-Framework and how to use di inside that framework. I don't know if that really relates to what I am looking for. I **don't** want to create a Play-Framwork-App, but a different App that uses the Play-Libraries to create a simple way to use a web-socket-actor. The link to the websocket example doesn't even include websocket actors, only actors created from sink and source or through future flow. If my original Post wasn't clear enough please tell me, so I can reword it. I am specifically looking for something like the example in the original post - where you only need to put in an actor to create a websocket (just like in the normal Framework). Thank you answering though! Much appreciated :)
We use Scalatra's swagger support. To put it bluntly, it's really lacking. The use cases it enables are basic at best, and really don't adapt well beyond something super simple.
I'm using endpoints too, though uncovered that you don't quite get for free what I thought. I'd like to look more into it, but the general problem is that if you're using circe, which everyone should be ;-), you then have opaque functions to marshal parts of your object model to Json. And so you would end up having to duplicate that marshalling with some annotations/descriptors. &amp;#x200B; In the end I ended up, for now, just creating an example Json object for some request/response types which have an implicit Encoder/Decoder, and then use 'fold' to derive the docs. &amp;#x200B; That's a bit noddy, but is working for me now and is a quick way to determine the json record structure for open-api. &amp;#x200B; See the discussion on julien's gitter channel for more: [https://gitter.im/julienrf/endpoints?at=5cbec2a9b4700e023dba418e](https://gitter.im/julienrf/endpoints?at=5cbec2a9b4700e023dba418e)
We've only ever prototyped it so far, but I'd use [rho](https://github.com/http4s/rho) for this.
From what I remember the idea is that you follow the convention. This convention dictates how JSONs are derived, because this way you are able to force consistency between interpreters. The downside is that you are giving away the control over the codec derivation, so if the codec is not derived in a way you like you are in trouble. Of course, everything can be solved by writing your own interpreter for JsonSchema, so if you stray from "convention" a lot then you basically end up maintaining your own set of interpreters.
In the akka docs are really good patterns to solve common problems with Actors, take a look: \- [https://doc.akka.io/docs/akka/current/howto.html](https://doc.akka.io/docs/akka/current/howto.html) \- [https://doc.akka.io/docs/akka/2.5.4/scala/howto.html#single-use-actor-trees-with-high-level-error-reporting](https://doc.akka.io/docs/akka/2.5.4/scala/howto.html#single-use-actor-trees-with-high-level-error-reporting)
Amateur question: Can akka be used for concurrency in multiple machine apps?
Same here. We tried tapir as well but it forces us to define both an encoder and a decoder for every data type, which we don’t want to (we usually only need one of them). We gave up until a better solution emerges.
Yes
[Tweet](https://twitter.com/scala_native/status/1131948956638941185)
We use Twilio's [guardrail](https://github.com/twilio/guardrail) to generate server routes from a swagger yaml file. Contract-first is nice because it can be shared between multiple teams working in multiple languages as the authoritative spec.
How do I compose **F : String -&gt; Future\[Option\[Sth1\]\]** with **F: Sth1 -&gt; Future\[Sth2\]**, so that I get **F: String -&gt; Future\[Sth2\]**. Is it possible?
Awesome work !
2.12 support is scheduled for next milestone.
this would probably work for you def recastColumns(df: DataFrame, mapping: Map[String, DataType]): DataFrame = { val recastedColumns = df.columns.map{ columnName =&gt; val column = col(columnName) mapping.get(columnName).map(d =&gt; column.cast(d)).getOrElse(column) } df.select(recastedColumns: _*) }
How does native compare to vanilla Scala?
Great !
Basically, you use vanilla scala and when you identify a hot path you use the native `primitives`, i.e. pointers, structs, etc. which lets you manage memory manually (no GC involved). Also, being native, there's no warmup necessary so you get better startup times similar to Go.
How does Scala Native compare to GraalVM's native-image?
it's called akka cluster
Indeed, I might be able to contribute in the future in both clojure and Scala. For now do you want me to contribute my code instead of the current implementation of heapsort as my code uses immutable operations of arrays and Scala's functional expressive code?
Not much information to go on. What's the error? I don't know what breeze viz is tho 😂
Nice work
&gt; **You read a lot. We like that.** &gt; &gt; You’ve reached the end of your free member preview for this month. Become a member now for $5/month to read this story and get unlimited access to all of the best stories on Medium. Damn, even medium paywalls now? I thought that kinda went against the whole idea of medium as a free blog hosting platform. Seems pretty silly that they'd charge almost as much as Netflix...
have you tried endpoints?
i actually got it working - i wasnt using the right repo
Incognito mode to the rescue :)
I'd love to use that, but some more beginner-friendly documentation would help. Thanks!
I didn’t because there is no http4s support
Sure thing, XavierLightman. I added this suggestion in the backlog. I'll improve the docs when I release the next versions.
I forgot to mention but there's also a polymorphic `sharded` function defined [here](https://github.com/profunktor/tutorials/blob/master/src/main/scala/dev/profunktor/tutorials/t2/Sharding.scala#L12).
Actually, I could not prove any of these claims. I've been benchmarking what is the performance drop related to the use of \`EitherT\` and it is nowhere near the \`4x\`. It's more like 20% when the underlying effect is something like \`Future\` where the number of transformations matters most as each one is submitted to the thread pool. For well-behaving effects like \`IO\` (from cats-effect) it's negligible. Benchmarks you point to do not use the new inliner (\`opt-l-inline\` and friends) which, I believe, is the main culprit. Also I found \`ZIO\` to be the slowest of all the effect wrappers I measured - in the use-cases I benchmarked it's consistently almost twice as slow as cats \`IO\`. I have not yet finished the analysis, but a quick glance on flamegraphs seems to indicate that there is a lot of burn on things like \`notifyObservers\`, \`yieldOpCount\` (which may mean that one pays for the functionality not present in the other solutions, yet it feels too big a price) plus \`nextInstr\` takes a lion's share of the overall execution time suggesting there may be some improvements to the interpreter itself. Last but not least \_itable stubs\_ account for ca. 7% of samples which may suggest tons of megamorphic call sites. I will be publishing my findings as a blog post soon. Maybe you could help me out here to analyse \`ZIO\` case better and/or make some improvements.
`Future`'s performance is dominated by submission to the thread pool; the `EitherT` overhead atop this is negligible, because the base effect type is already so slow that additional allocations and virtual calls don't make a significant difference. A more reasonable benchmark would not use `Future`, but use an effect type with optimized performance, such as Cats IO, ZIO, or Monix Task. While you can play tricks with the inliner, note that most real world projects do _not_ use the inliner (all higher-order methods of `EitherT` have to be inlined); and further, if you interact with these projects through polymorphic `F[_]`, which is strongly encouraged in the Cats Effect ecosystem, these tricks won't apply. Moreover, to be a fair comparison with this case, you need a 2 layer transformer (`EitherT` + `WriterT`), which will force the 2nd layer to be through polymorphic `F[_]`, even if you are directly interacting with `EitherT` at the top level. As for ZIO's performance, it's likely your benchmarks are not realistic. If you even see `notifyObservers` show up on a flamegraph, for example, it's a sign your effects are too short-lived to amortize the overhead of fiber setup/teardown. Not only is this unrealistic, since most effects in functional applications are long-running (hence why next-gen effect times are so much faster than `Future`), but it will favor "inline interpreters" (Cats IO) over standalone ones (Monix/ZIO). `nextInstr` (or its equivalent) indeed is going to be the performance bottleneck in any well-optimized effect system: it's the place where the interpreter calls the user-defined function passed to `flatMap` (i.e. the continuation of the program) with the current value computed by the interpreter. If anything _else_ were the bottleneck, it would be a good sign the interpreter could be further optimized. For good benchmarks, see ZIO's benchmark project, or the benchmarks in Monix, which are also well-constructed. These days, they will show the effect systems as comparable across the board. And if you measure library code built on the effect systems, for example, ZIO's `Semaaphore` to Cats Effect `Semaphore`, you'll find that these two are roughly equal across the board. Although you can contrive examples in which one effect system is faster than another (for example, measure ZIO `guarantee` against Cats IO `guarantee`; or, in the other direction, ZIO is not currently optimized for short-lived effects and that shows), the performance of the core interpreter loops is just about the same, and it cannot really be improved by any significant margin in any of the mainstream effect types. Benchmarking is not trivial. It's easy to come up with unrealistic benchmarks that don't reflect real world usage and suggest any desired conclusion. If you study existing benchmarks in the mainstream projects, you'll see they have been carefully constructed to be as realistic synthetic benchmarks as possible; and they show that, with the above caveats, the days of low-hanging fruit in functional effect systems are long gone.
nice!
Does Kojo use a custom table implementation? If so, what is the benefit over existing data structures like Spark DataFrames?
It uses [tablesaw](https://github.com/jtablesaw/tablesaw). The idea with this is to have a lightweight data-frame playground for kids, without the need to install something heavier like Spark. As this progresses, I might well provide a Spark-like API over the current fluent-Java API.
Yeah it would be very nice to see the benchmarks against native-image since realistically it is much easier to deploy native-image: most scala libraries already work with it. That said, native-image is generally a bit slower than the JIT, so if the benchmarks show a win over jvm or graalVM that would imply it probably beats native-image.
Thanks for your long and insightful answer! I have some comments I'd like to point out &gt; Future's performance is dominated by submission to the thread pool Yes, that's painfully true &gt; A more reasonable benchmark would not use Future, but use an effect type with optimized performance, such as Cats IO, ZIO I am more focused on measuring performance of various error handling techniques, hence `EitherT`. I use various effect wrappers, like IO/ZIO etc as well. I actually picked ZIO because of its unique, bifunctor-based approach to errors. &gt; While you can play tricks with the inliner, note that most real world projects do not use the inliner Even if they don't, they should (except for libraries). Applying the inliner isn't an invasive technique. &gt; Moreover, to be a fair comparison with this case, you need a 2 layer transformer (EitherT + WriterT), Yes, you're right, but I do not find it that compelling to be honest. Here I am taking the stance that these "real world" projects do not use multi-layered transformers that much. It's much more common to have a single `EitherT`\\`OptionT` layer for the sake of easier coding . &gt; As for ZIO's performance (...) Right, I looked at flamegraphs only for short-lived tasks - your observation is spot on. Also thanks for the insights about "inlined" interpreters. But, I obviously measured the impact of long-running tasks as well. The thing is - if tasks are long-running then the impact of effect system (and/or various monad transformers) is washed out as the time it takes to run the task is the dominating factor. Thus, saying that monad transformers are 4x or so slower, or some effect system is 8x faster, does not strike me as a correct statement. But, even then, I saw that `IO` (and even `Future`) was about 35% faster than `ZIO`. The example `ZIO` code I benchmarked is ```scala ZIO .succeed(validateEitherStyle(validInvalidThreshold)(benchmarkState.getSampleInput).map(transform(baseTokens))) .absolve .flatMap(validInput =&gt; block(outsideWorldEitherZio(failureThreshold, baseTokens, timeFactor)(validInput))) .absolve .foldM(err =&gt; block(doZioWithFailure(baseTokens, timeFactor)(err)).andThen(ZIO.fail(err)), output =&gt; block(doZioWithOutput(baseTokens, timeFactor)(output))) ``` All these `baseTokens` and `timeFactor` are for turning the long/short knob. As you can see - it is the common "take the input, transform, call the outside world, do something with results (potentially calling the outside world as well)" pattern. When it is measured against the equivalent `IO-with-either` : ```scala IO .pure(validateEitherStyle(validInvalidThreshold)(benchmarkState.getSampleInput).map(transform(baseTokens))) .flatMap { case Right(validInput) =&gt; shift(outsideWorldEitherIo(failureThreshold, baseTokens, timeFactor)(validInput)) case left =&gt; IO.pure(left.asInstanceOf[Either[ThisIsError, Output]]) } .flatMap(either =&gt; shift { either match { case Right(output) =&gt; doIoWithOutput(baseTokens, timeFactor)(output).map(Right(_)) case l @ Left(err) =&gt; doIoWithFailure(baseTokens, timeFactor)(err).map(_ =&gt; l.asInstanceOf[Either[ThisIsError, Result]]) } }) ``` then even for tasks that are simulated to be 200x longer than ordinary method calls, I can see the effects I mentioned (ZIO is slower by ca. 30%). If you see any improvements and/or explanations, I will be delighted to hear it. &gt; If you study existing benchmarks in the mainstream projects, you'll see they have been carefully constructed to be as realistic synthetic benchmarks as possible I studied them and I do not think so :-) There are various flaws I found in `ZIO` benchmarks like sleeping while awaiting on futures. Also benchmarks that measure eg. deep recursion do not strike me as "realistic" in that they model the, so-called, "real-world" coding patterns.
The most flexible way to do it is to use JavaScript-based tools for building the frontend and leave Play to SBT. In this scenario, Webpack simply emits compiled or minified bundles to the `public` directory and Play uses them as regular frontend assets. Here is an example using Play 2.7 and Webpack 4: https://github.com/denisftw/modern-web-scala
Very interesting analysis! Great read even though I'm not informed enough to understand all of it :)
&gt; Yes, you're right, but I do not find it that compelling to be honest. Here it's me taking the stance that these "real world" projects do not use multi-layered transformers that much. It's much more common to have a single EitherT\OptionT layer for the sake of easier coding . That's incorrect. http4s internally uses `KleisliT` _and_ `OptionT`, and if you want to add typed errors, you are already 3 levels deep—to say nothing of state, writer, or other effects you might need. In this blog post, which was inspired by real production code, 2 levels of monad transformers are used, and if the code were written atop http4s, you'd already be looking at 4 levels of monad transformers. &gt; Even if they don't, they should (except for libraries). Applying the inliner isn't an invasive technique. Again, this is not realistic. 95% of the entire functional ecosystem interacts with polymorphic effect types. So if you're using http4s, Doobie, FS2, Aecor, or any one of numerous other functional Scala libraries, then the code is not interacting with `EitherT` directly (for example), but through type classes; which means the higher-order methods of `EitherT` cannot be inlined. Unless you are writing 100% of the code for your application, and not interacting with any functional Scala libraries, then the "inliner trick" is just that—a trick, which people are neither using in production, nor are they able to use due to dependency on polymorphic libraries. &gt; The thing is - if tasks are long-running then the impact of effect system (and/or various monad transformers) is washed out as the time it takes to run the task is the dominating factor. That's completely incorrect. In functional Scala applications, the majority of effects are long-running, or even infinite, and users are led toward minimal use of `unsafeRun`. The design of ZIO and Monix even biases toward long-running effects, because that's what real world applications use. If anyone is writing code that peppers `unsafeRun` every other statement, it's not truly leveraging the benefits of a functional effect system (because those benefits halt at `unsafeRun` boundaries) and would be better off not using them. Benchmarking long-running effects ensures you are not benchmarking setup/teardown times for the interpreter, which is not realistic thing to benchmark; it ensures you are benchmarking the actual interpreter loop, which will dominate execution of all purely functional programs. Benchmarking "useless" work is also critical, as the benchmarks should not be measuring IO overhead or number crunching overhead or anything other than the raw overhead of the underlying effect type. &gt;The example ZIO code I benchmarked is ... This is the least useful benchmark I have ever seen. Not only is this a short-running effect (which would never exist in a functional Scala application), but you are tangling measurement of the effect system's interpreter loop with outside concerns that are not relevant (indeed, highly toxic) to a synthetic benchmark. &gt; There are various flaws I found in ZIO benchmarks like sleeping while awaiting on futures. Also benchmarks that measure eg. deep recursion do not strike me as "realistic" in that they model the, so-called, "real-world" coding patterns. There is no flaw like "sleeping while awaiting on futures" in the ZIO benchmark. Rather, synchronous execution is used consistently across all the effect types, and it just so happens that with Future, in the default setup, it requires blocking a thread until the result is available—which is precisely how Future is used in the real world when it is necessary to convert a Future into a synchronous result. Deep recursion is exactly the kind of synthetic benchmark that measures overhead of the interpreter loop. I admire your enthusiasm but I recommend you further study ZIO benchmarks, Monix benchmarks, Cats IO benchmarks, Trane benchmarks, etc. (take your pick, it's not tenable to say they are all ZIO biased!). They were all written by people who are well-versed in the challenges of designing realistic synthetic benchmarks, and if you ignore the concerns I have explained in the post, your benchmarks will not be telling you anything useful.
Hacky AF but you can (ab)use implicit ambiguity to prove type inequality at compile type, see the answer here [https://stackoverflow.com/questions/50157349/upper-type-bound-allowing-subtypes-but-not-the-parent-type](https://stackoverflow.com/questions/50157349/upper-type-bound-allowing-subtypes-but-not-the-parent-type)
Inheritance works against your goals in this case: by inheriting `Apple` and `Orange` from `Fruit` you're saying that it's ok to use them together, and the result should seamlessly generalize to `Fruit`. If you don't want to mix your `Apple`s with `Orange`s, you can use implicit witnesses instead: ``` scala&gt; :paste // Entering paste mode (ctrl-D to finish) final case class Apple(family: String) final case class Orange(color: Int) trait Fruit[T] {} object Fruit { implicit val appleFruit: Fruit[Apple] = new Fruit[Apple] {} implicit val orangeFruit: Fruit[Orange] = new Fruit[Orange] {} } case class Basket[T : Fruit](fruits: List[T]) // Exiting paste mode, now interpreting. defined class Apple defined class Orange defined trait Fruit defined object Fruit defined class Basket scala&gt; Basket(List(Apple("golden"), Apple("red"))) res0: Basket[Apple] = Basket(List(Apple(golden), Apple(red))) scala&gt; Basket(List(Orange(0xfa2), Orange(0xfb3))) res1: Basket[Orange] = Basket(List(Orange(4002), Orange(4019))) scala&gt; Basket(List(Apple("golden"), Orange(0xfb3))) &lt;console&gt;:18: error: could not find implicit value for evidence parameter of type Fruit[Product with Serializable] Basket(List(Apple("golden"), Orange(0xfb3))) ^ scala&gt; val oneFruit: Basket[Apple] = Basket(List(Apple("golden"))) oneFruit: Basket[Apple] = Basket(List(Apple(golden))) scala&gt; val twoFruits: Basket[Fruit] = oneFruit.copy(fruits = oneFruit.fruits :+ Orange(0xcaa)) &lt;console&gt;:18: error: trait Fruit takes type parameters val twoFruits: Basket[Fruit] = oneFruit.copy(fruits = oneFruit.fruits :+ Orange(0xcaa)) ^ &lt;console&gt;:18: error: could not find implicit value for evidence parameter of type Fruit[Product with Serializable] val twoFruits: Basket[Fruit] = oneFruit.copy(fruits = oneFruit.fruits :+ Orange(0xcaa)) ```
Thanks! This is actually the 10th (and last) post in a series, so I assumed some knowledge of the reader ;) But hopefully, something can still be understood even without reading all the other parts :)
&gt; http4s internally uses KleisliT and OptionT, and if you want to add typed errors, you are already 3 levels deep &gt; 95% of the entire functional ecosystem interacts with polymorphic effect types Interesting, I'll rethink this aspect. Thanks for pointing out. &gt; This is the least useful benchmark I have ever seen. Not only is this a short-running effect (which would never exist in a functional Scala application), but you are tangling measurement of the effect system's interpreter loop with outside concerns that are not relevant (indeed, highly toxic) to a synthetic benchmark. I think I lost it here. How is this benchmark different than ```scala val program = for { _ &lt;- getAllUserData("wile@acme.com") logForValidSearch &lt;- getLogs[String] _ &lt;- console.putStrLn(logForValidSearch.mkString("\n")) } yield () ``` presented in the blog post you're referring to? What exactly do we mean by "long-running" then. I was showing you that I am simulating long-running tasks by injecting unoptimizable time-loops aka. blackholes, but I am not sure you consider that relevant to the validity of benchmark, right? So what would you measure? I was under impression that this kind of code simulates typical request-response kind of method that may very well exist in a (pure) Scala application. Again, my plan is not to benchmark the effect systems in their own, but rather various aspects of error handling in such apps. So - I think my question boils down to: how do people actually use ZIO and how I can see this 8x speed-up? &gt; which is precisely how Future is used in the real world when it is necessary to convert a Future into a synchronous result. Good point, I observed that sleeping skews results by adding random overhead of going out of sleep and busy-loop yields better results, but it is true that it is not _realisitc_ &gt; Deep recursion is exactly the kind of synthetic benchmark that measures overhead of the interpreter loop. True, but it's not my goal to measure this - again, my goal is to measure whether it's justifiable to be "afraid" of using `EitherT`, or is it better to encode `Either` by hand, or rather to throw or use `ZIO`s error handling etc. Hence I find these benchmarks useful only for effect system developers. But you seem to point out that I'm mistaken in what I'm measuring, which may well be true, but it begs to ask - what would be sufficiently realistic?
&gt; Future &gt; 's performance is dominated by submission to the thread pool; the &gt; EitherT &gt; overhead atop this is negligible, because the base effect type is already so slow that additional allocations and virtual calls don't make a significant difference. Note that this being fixed in Scala 2.13, see https://github.com/scala/scala/pull/7470 and https://github.com/scala/scala/pull/7663
&gt; Unless you are writing 100% of the code for your application, and not interacting with any functional Scala libraries, then the "inliner trick" is just that—a trick, which people are neither using in production, nor are they able to use due to dependency on polymorphic libraries. Doesn't the scala compiler now have an option to inline code on an application level, i.e. https://www.lightbend.com/blog/scala-inliner-optimizer (it has an option to do full application level optimization) Not sure if it helps in this case though. Also GraalVM is meant to significantly improve performance as well (it has a partial escape analysis which allows inlining of cases such as this)
Basicly an automated inventory manager. Because people are too lazy to write everything down they used up. (Not judging, just beging realistic)
How is that "hacky"? You are explicitly stating that T must be one and only one Fruit. If there are two, you have violated that constraint.
In the [original announcement](http://www.scala-native.org/en/latest/blog/interflow.html) from a year ago, there are some benchmarks comparing with Graal Native Image. But it appears that Scala Native has made progress since then (and possibly Graal Native Image as well).
AutoScout24 | (Junior / Senior / Lead) Software Engineer | Munich, Germany | ONSITE | Full Time | Above avg salary We're looking for engineers of all levels with a learning mindset. Challenges are surrounding writing in Microservice Architectures and data intensive applications. martin@lechner.work
Something to keep in mind is that a `sealed trait` hierarchy like this is a standard way to encode a feature Scala's type system lacks, namely "sum types" or "`Coproduct`s." When you use a coproduct, you're literally saying "this _or_ that." `Either` is an encoding of a degenerate `Coproduct` of two types. So you're saying "A `Fruit` is either an `Apple` or an `Orange`," and then trying to keep them separate after the fact. As you're finding, the language will fight you on this. What I suggest instead is not using a `sealed trait` hierarchy, modeling `Apple` and `Orange` as distinct as far as subtyping goes, but then using Shapeless' `Coproduct` to say what you mean: `type Fruit = Apple :+: Orange :+: CNil` Now you can have your `List`s be constrained to one type, as you wish, but you can also have values of the `Coproduct` type (e.g. `Coproduct[Fruit](Apple("Golden Delicious"))` and you can develop `Poly`s that can deal with such values, e.g. by `fold`ing them over a `Coproduct` value. That might sound a little mysterious if you haven't used Shapeless before, so please let me know if you need examples. Good luck!
Does your company offer a work visa? Thanks.
What a joke
Cool stuff. I've put together some things with similar intentions. My aim was to implement some hoare logic too, and some other stuff (too much stuff). This seems like it is/was a fun direction.
Credit Karma | most roles and levels, particularly senior and above engineers | focused on LA and London / Leeds but can look at Charlotte or SF | Onsite, full time Credit Karma is a unicorn Bay Area tech startup expanding to the rest of the world. We have 100M users across the globe and our members love our product. We help people understand their credit and give them feedback on what they can do to make financial progress. The lower levels of our stack are filled with scala for scalability in a data heavy distributed system. If you are interested email me matt (at) creditkarma (dot) com.
We have people from all over the globe. We helped lots of them with visas as well.
This is only really scratching the surface of what is possible with higher-kinded data but interesting nevertheless :)
Because it (ab)uses language features in ways they weren't intended to be used. Also because it can introduce subtle bugs (like Fruit =!= Fruit with Product with Serializable as the comments mention). Hackyness is very subjective but i wouldn't want something like that in my code if i could avoid it.
I don't really get the point of this from the example. Why do we have to wrap everything in `F[_]` in the `Person` `case class`? If I only had a `Person(name: String, age: Int)`, it would still be possible to write the `validate` function using shapeless to generate a function that takes the same arguments as the constructor, but wrapped in `F[_]`, and return an `F[Person]` (or any case class).
You can use [Self-Recursive Types](https://github.com/tperrigo/scala-type-system/wiki/Self-Recursive-Types) (a good explanation can be found [here](https://tpolecat.github.io/2015/04/29/f-bounds.html)). And example becomes: sealed trait Fruit[T &lt;: Fruit[T]] final case class Apple(family: String) extends Fruit[Apple] final case class Orange(color: Int) extends Fruit[Orange] case class Basket[A](fruits: List[Fruit[A]]) Basket[Apple](List(Apple("golden"), Apple("red"))) // Compiles Basket[Orange](List(Orange(0xfa2), Orange(0xfb3))) // Compiles Basket[Apple](List(Apple("golden"), Orange(0xfb3))) // Does NOT compile val oneFruit: Basket[Apple] = Basket(List(Apple("golden"))) val twoFruits: Basket[Apple] = oneFruit.copy(fruits = oneFruit.fruits :+ Orange(0xcaa))
Sure, that would be possible. I work in data engineering and I could see uses for the example I used. For example, there may be useful information that can be gained from partially-complete data (i.e., optional fields). On the other hand, in the same application, maybe other information can only be gained from complete records. This could be achieved by simply filtering for records were all fields are nonEmpty. The disadvantage in doing this though is that the fact that all fields are populated is not represented by the type system.
I had a discussion about it that I would like to share here since I would like to know other opinions, here it goes: I think it's a nice quirky thing you can do but I see it breaking down in non-trivial examples or complex domains. I often deal with case classes that have 20, 40 or up to 200 fields if someone is interested in a couple of data sets joined together. So you could describe your data as follows: case class User[SourceA[_], SourceB[_], SourceC[_]](id: UserId, fieldA1: SourceA[Int], ..., fieldAn: SourceA[String], fieldB1: SourceB[Option[String]], ..., fieldCn: SourceC[Int]) So the first problem is that you can have Optional field in SourceB - and then what is the type in the case class is it Option[String] or SourceB[Option[String]]? Which then can translate to Option[Option[String]]. The second problem is that for me it looks like encoding schema on write. What if I have subsequent functionality that needs 5 fields from SourceA and 2 from SourceB? What if the next feature needs different fields from A, B &amp; C? I don't see this helping me.
This worked perfectly, thank you!!!
I would appreciate any hints. Thanks.
The example and source material it's based on talks about constraining all your fields to a given `F[_]`, which actually enables some useful generic operations when `F` exhibits certain capabilities (eg given an `Applicative[F]` you can sequence a `User[F]` to `F[User[Id]]`) - your example is not really useful in any polymorphic context so I can see why you wouldn't expect it to be helpful. Not sure exactly how you read this but I'd suggest clicking through to the referenced blog post if you haven't yet to try and get a clearer picture of the motivation behind it.
Thank you all for your help, I've learned a lot! So to summarize we've different solutions to this kind of problems so far: * [Implicit witnesses](https://www.reddit.com/r/scala/comments/btli10/adt_and_compiletime_guarantees/eozx3oe/) * [Enforcing type difference](https://www.reddit.com/r/scala/comments/btli10/adt_and_compiletime_guarantees/eozvlt0/) * [Using shapeless](https://www.reddit.com/r/scala/comments/btli10/adt_and_compiletime_guarantees/ep1i7f9/) to emulate proper sum types (coproducts) * [Self-Recursive types](https://www.reddit.com/r/scala/comments/btli10/adt_and_compiletime_guarantees/ep4zqd3/) All in all, I think self-recursive types are a good compromise. But I'm also going to properly investigate the use of shapeless. As a side note, AFAIK Dotty seems to have proper sum types which is good.
A bit strange that this doesn’t mention Lagom, which wraps Akka Cluster and is explicitly designed for this use case.
Hand writing them is fine as long as you remember to update them.
No, I've given up...
sbt-assembly worked for me (and is up-to-date as of 6 months ago). Maybe you can explain what went wrong with the other tools
de goes is the lambdaconf guy right?
Community is right.
This post goes to show what an extraordinary fellow John is. His emotional intelligence is up there with his intellectual intelligence - a rare combination.
I think I solved the problem: [https://github.com/shadaj/create-react-scala-app.g8/issues/8](https://github.com/shadaj/create-react-scala-app.g8/issues/8) But I'm not perfectly sure if this causes a other issue.
This is the right thing to do. ZIO is a great project and deserves to be judged on its own merits.
John is the definition of brolic. ZIO is also brolic. Coincidence? Absolutely not
Yes.
&gt; case class Basket[A](fruits: List[Fruit[A]]) It doesn't compile in REPL: ``` scala&gt; :paste // Entering paste mode (ctrl-D to finish) sealed trait Fruit[T &lt;: Fruit[T]] final case class Apple(family: String) extends Fruit[Apple] final case class Orange(color: Int) extends Fruit[Orange] case class Basket[A](fruits: List[Fruit[A]]) // Exiting paste mode, now interpreting. &lt;pastie&gt;:15: error: type arguments [A] do not conform to trait Fruit's type parameter bounds [T &lt;: Fruit[T]] case class Basket[A](fruits: List[Fruit[A]]) ^ &lt;pastie&gt;:15: error: type arguments [A] do not conform to trait Fruit's type parameter bounds [T &lt;: Fruit[T]] case class Basket[A](fruits: List[Fruit[A]]) ^ &lt;pastie&gt;:15: error: type arguments [A] do not conform to trait Fruit's type parameter bounds [T &lt;: Fruit[T]] case class Basket[A](fruits: List[Fruit[A]]) ^ ```
What does that mean?
There was criticism about this conf, because he invited controversial people.
Correct decision.
Great write. The transparency and optimism are more than welcome.
And in the meantime the new developer coming to scala to do FP has no idea whether to use scalaz or not, or to use cats, or what that implies for his code long term.
I've just done this using sbt-assembly. What you need to have in mind is that you sometimes have to shade libraries (e.g. Shapeless) because Spark itself bundles a different version than the one you are using (directly or transitively).Also, you will probably run into problems with duplicate files with different content.In that case it's some detective work to find out which library is the right one to provide the duplicate classes and remove the other ones (e.g. Glassfish's repackaged libraries) using excludeAll. The result could look something like this: libraryDependencies ++= Seq(...).map(_.excludeAll(ExclusionRule(organization = "org.glassfish.hk2.external"))) assemblyMergeStrategy in assembly := { case PathList(ps @ _*) if ps.last == "git.properties" =&gt; MergeStrategy.first case PathList(ps @ _*) if ps.last == "package-info.class" =&gt; MergeStrategy.first case PathList("org", "apache", "commons", xs @ _*) =&gt; MergeStrategy.first case PathList("javax", "servlet", xs @ _*) =&gt; MergeStrategy.first case PathList(ps @ _*) if ps.last endsWith ".html" =&gt; MergeStrategy.first case "application.conf" =&gt; MergeStrategy.concat case "unwanted.txt" =&gt; MergeStrategy.discard case x =&gt; val oldStrategy = (assemblyMergeStrategy in assembly).value oldStrategy(x) }, assemblyShadeRules in assembly := Seq(ShadeRule.rename("shapeless.**" -&gt; "new_shapeless.@1").inAll)
Thanks for pointing. Just corrected in the response. `case class Basket[A](fruits: List[Fruit[A]])` must be `case class Basket[A &lt;: Fruit[A]](fruits: List[Fruit[A]])` Sorry, didnt test in REPL
*raises hand*
It's not like you have a choice anymore. Any relevant lib you import will bring cats along, so that's the only choice. Zio you may want later or not, but don't worry too much about it.
I had read in a random Spark issue about a week ago that 2.4.0 was released with the incorrect default Scala version. I can't find the issue, but the fix was upgrading to Spark 2.4.1 or targeting Scala 2.11. If I remember correctly, the person having issues was trying to bundle Scala 2.12 deps and it was failing. This sounds like your issue could be similar.
&gt; I think you lost me here. How is this benchmark different than scala val program = for { _ &lt;- getAllUserData("wile@acme.com") logForValidSearch &lt;- getLogs[String] _ &lt;- console.putStrLn(logForValidSearch.mkString("\n")) } yield () presented in the blog post you're referring to? The poster of the blog post did not benchmark that snippet, and if the user had benchmarked that snippet, I'd be eagerly telling them the benchmark is neither useful nor telling them what they think it is. &gt; So - I think my question boils down to: how do people actually use ZIO and how I can see this 8x speed-up? ZIO does not give applications an 8x speedup. Rather, ZIO's baseline performance can easily be 8x (or more) higher than the baseline performance of a different effect monad tricked out with all the same features using monad transformers. The choice for users is simple: 1. Do you want poor ergonomics (bad type inference, etc.), and inescapable overhead with each monad transformer layer? 2. Do you want good ergonomics (perfect type inference, etc.), and no overhead for reader/writer/either/state/etc. effects? That's really what the choice boils down to, not "do you want 8x faster application", because synthetic benchmarks don't measure that. &gt; Hence I find these benchmarks useful only for effect system developers. They are mainly useful for effect system developers, but many architects and leads also care about baseline overhead of the effect system (especially if they're moving from a more procedural code base), so they are useful beyond effect system developers. If you want a realistic synthetic benchmark, add EitherT to http4s' KleisliT (ReaderT) over OptionT. Now you have typed errors on the same stack used by a popular library. Then interact with this stack through polymorphic type classes, and measure that against ZIO. Not doing any work, and most certainly not using http4s, because you don't want to be benchmarking the web library if you're measuring effect type overhead. Then you will see quite significant overhead that can't be eliminated with inliner tricks, and you'll know the precise meaning in which "vertical effects" are necessarily slower than "horizontal effects". If you want a non-synthetic benchmark, then go ahead and measure a full application, including doing IO and so forth. What you're really measuring is not the effect system, at that point, but whole system throughput. That's a useful measure for your application, but the results won't be easily generalizable to other people's applications. They'll let you know whether monad transformers have significant overhead for _that specific application_, but a different application will have different results. They're good to drive your own personal decisions but bad to try to generalize and make recommendations from. As "unrealistic" as synthetic benchmarks are, they only make narrow claims that apply uniformly. That's why we build them, publish them, and optimize toward them.
The reality is that most of the Scala community will tell you to use cats.
I get that you're self-promoting your blog, but can you at least put a summary here?
Yes, it does, and that's what I'm referring to when talking about the "inliner". Thanks for pointing out.
I’m not sure what you mean. When you publish a link on reddit, you can only put the link and the title, I wouldn’t mind it having a description here, since as you can see I don’t have ads or anything on the blog, so I get literally nothing out of people clicking the link. Maybe it was a misjudgment of my part, but I though “effects” was enough of a description, but here’s a bit more detail: I try to explain the basics behind how an effect system works and what it gives you when you use it in your application.
Let's try and do without personal insults and name calling please and thank you.
&gt; The poster of the blog post did not benchmark that snippet Well, ok, but he was explicit in saying that it helps with "undesired runtime overheads" and " slow performance and large heap-churns" etc. But, all right - irrelevant to our discussion &gt; If you want a realistic synthetic benchmark, add EitherT to http4s' KleisliT (ReaderT) over OptionT. Now you have typed errors on the same stack used by a popular library. Then interact with this stack through polymorphic type classes, and measure that against ZIO. &gt; If you want a non-synthetic benchmark, then go ahead and measure a full application Understood, yet I do not find this dichotomy to be 100% exhaustive. In my opinion you can draw proper conclusions from measuring simply how the choice of writing `Input =&gt; F[Result]` affects performance. Essentially, the said function calls the outside world which can be simulated in a variety of ways (for me this is simply reflected by consuming time) and can be written using number of techniques - you can use `EitherT` to glue together these interactions, you can throw exceptions etc - regardless of what is up the stack. Now, I agree - this will *not* help you to determine what is the best performant effect wrapper. But it can help you to answer the question - given a `F[_]` should I be worried about using `EitherT` \ exceptions etc (as opposed to, what `F[_]` should I choose?). But you convinced me that you should not cross-compare results between various effect wrappers, for which I am thankful.
It very well may be that cats is the obvious answer once you know the lay of the land, but that wasn't the situation i was pointing out.
Yep, I think this was my true issue, which I discovered yesterday. After downgrading to Scala 2.11.12 my single jar worked without needing my jars in lib\_managed. So, hopefully, I can get by without needing the uberjar. So far so good.
Why not just `case class Basket[T &lt;: Fruit[T]](fruits: List[T])` ?
It's truly pathetic how rare a combination it is.
Keep it going. I find your articles very useful.
He organized LambdaConf and hired a white supremacist and an outspoken misogynist to speak - [https://medium.com/@soniagupta504/bigotry-and-its-amplifiers-a-response-to-lambdaconf-e30577b396c](https://medium.com/@soniagupta504/bigotry-and-its-amplifiers-a-response-to-lambdaconf-e30577b396c), which also mentions this article: [https://www.inc.com/tess-townsend/indiegogo-campaign-funding-tech-conference-white-nationalist.html](https://www.inc.com/tess-townsend/indiegogo-campaign-funding-tech-conference-white-nationalist.html)
Hello, I have found [http://marco-lopes.com/articles/Learning-Scala/](http://marco-lopes.com/articles/Learning-Scala/) to be a useful starting point for me. There are some good books recommended. Also, you may want to try [https://underscore.io/books/](https://underscore.io/books/) as there are some books that I found useful when trying to grasp some functional programming concepts.
Thanks!
Programming in Scala by M.Odersky would be a good starting point.’
I am learning rn: try this: [https://exercism.io/tracks/scala/installation](https://exercism.io/tracks/scala/installation)
https://github.com/fosskers/scalaz-and-cats
That's a bit of a misrepresentation of the events. John himself is hard-working, intelligent, and up-right. I encourage you (or anyone) to meet him in person rather than absorb vitriol from the internet.
From the distance of someone who just coexists in the community, I have a lot of respect for John. Many people cite his inclusivity and leadership, and I can see it in action in the community. He's a fantastic speaker and programming thinker. The letter shows that he's also very thoughtful about community. My biggest criticism of the letter is that it is very forthright about how divisive Tony Morris has been to the community, but John does *not* own that his stewardship of LambdaConf has also been extremely divisive, in particular, [the fact that LambdaConf hosted Curtis Yarvin and Ed Latimore as keynote speakers](https://medium.com/@soniagupta504/bigotry-and-its-amplifiers-a-response-to-lambdaconf-e30577b396c), in consecutive years. My interpretation of John's philosophy is that he believes that the ideologies of these two men outside of the tech sphere can be compartmentalized when it comes to inviting them to make presentations that, by all accounts, do not reference those ideologies. This has forced many individuals in the community to decide where they draw the line. Can they compartmentalize, like John, and still enjoy LambdaConf? Is John transitively connected to their ideologies by giving them a platform? Are people who associate with John transitively connected to those ideologies? And so on. I have been blocked on Twitter by at least one prominent member of the Scala community, presumably because where I've chosen to draw my personal line so far is with LambdaConf, not John, inasmuch as I interact with him on Twitter. I remain a bit puzzled and saddened by this -- but not as much as I remain puzzled by the choice of LambdaConf to select Yarvin and Latimore as keynote speakers, and their steadfast defense of that choice. Perhaps bygones will eventually be bygones, but I bring this up because I don't think the history and present state of the Scalaz community is the *only* problem, and I think the letter gives that impression.
He might be "hard-working, intelligent, and up-right" but he's still the dude who defended Mencius Moldbug.
https://alvinalexander.com/scala/functional-programming-simplified-book
I'm not sure how one can misrepresent hiring these people to speak and defending their position to be there. Also, my comment doesn't mention anything about John's work ethic or intelligence - just that he approved and supported these people speaking at a conference he organized.
[https://twitter.github.io/scala\_school/](https://twitter.github.io/scala_school/) seems useful. Funny enough, I'm going the other way - from Scala to Go. Any recommendations for where I should start?
Most of the other suggestion are good. Additionally, once you get the basics down, I also tend to recommend [https://danielwestheide.com/scala/neophytes.html](https://danielwestheide.com/scala/neophytes.html) for a few intermediate topics.
These statements are false, and I don't believe you know that, so I'd love to help clear up the record. In 2016, Curtis Yarvin submitted a talk about Urbit (a pure FP OS) to LambdaConf, which was accepted by the double-blind committee (I was not a member), and we the organizers (controversially) chose to let him speak about his project, even while strongly denouncing his politics and prejudices. Curtis was not paid for his talk (which was not a keynote, but held in a small room in one of 5 tracks), he behaved himself professionally at the conference, and you can [find his talk online](https://www.youtube.com/watch?v=bTisf4oxIFo). Ed Latimore, an African-American pro boxer, who overcame abject poverty and alcoholism to become a physicist and self-help author, _was_ invited to keynote LambdaConf on overcoming fear. Like Curtis, Ed was not paid; he volunteered his time to share his story. You can also [watch his talk online](https://www.youtube.com/watch?v=17rgUgdZHgg). To my knowledge, Ed is not a misogynist of any kind, let alone an outspoken one, and if correct, characterizing him that way is not only unfair to him, but trivializes real misogyny. Ed did have some older tweets that are sexist (e.g. stating men can't understand women and that women are more emotional / less rational than men), which we didn't know about at the time, and which seem to not represent his current views. Now, I have no desire to defend others (they have to answer for their own actions), and no desire to justify my past actions. I made a lot of mistakes in handling the LambdaConf controversy, and I regret those mistakes and the pain the controversy caused. That's on me. In the near future, I hope to talk about what I've learned since then and deal with some of the hurt. In the meantime, I warmly encourage you to fact check, and avoid (even unintentionally) cherrypicking sources, which are inevitably biased (on both sides). If you want to criticize me, I encourage you to do so factually, thoughtfully, and ideally constructively. I promise to hear that criticism and try to use it to improve myself, even if we never agree on every detail.
Thank you for the kind words! I don't think there's just one source of discord in the Scala community, and I'm disappointed my post created that impression. I believe there are many (most not even connected to functional Scala), and I own that my flawed handling of the LambdaConf controversy is among them. There's more that I can and will do to heal that rift (see my other post), but not everyone is ready for reconciliation. Every public figure in any programming community will always have detractors (Odersky, myself, countless others), and while the reasons will vary over time and by person, this will always be a source of discord. There's no future in which everyone gets along perfectly with everyone else, but small steps like making sure ZIO is judged on its own merits and for its own culture, will help decrease discord and increase alignment of the community. And least that's my hope.
&gt; e.g. stating men can't understand women and that women are more emotional / less rational than men [It's the differences of which there are none that make the sameness exceptional!](https://www.youtube.com/watch?v=aL0MuT0v6ds)
The author of Ensime has deprecated it for some time now, advertising instead that people use [Metals](https://github.com/scalameta/metals).
can you point out where he defended him ? I have hard time believing John would defend racistic views.
Thanks for the thoughtful response, and clarifying the parts where I was mistaken. I hope that LambdaConf speakers will be more thoroughly vetted in the future so that all attendees feel welcomed and included.
Shocked that no one has yet mentioned "Functional Programming in Scala" (or "The Red Book"). A lot of books out there treat Scala like a better Java, which it is, but the best way to really learn Scala is to treat it like Haskell at first and then slowly re-introduce the OOP as necessary.
Chartboost | Software Engineer, Scala (Senior and mid level positions) | San Francisco, CA and Barcelona, Spain | Onsite | Full-Time My name is Mark and I'm looking to grow my Ad Serving team here at Chartboost! We are currently looking to hire for our San Francisco and Barcelona offices, and we are open to considering relocation for interested candidates. We are looking for passionate backend engineers who love Scala and typed functional programming (cats, cats-effect, http4s, akka-http, shapeless) to join our team and help us build the best advertising platform for mobile developers. You'll work on a system that processes tens of thousands of requests per second and conducts a real-time auction to find and deliver the most effective ads from the Chartboost Network. The team's working on some exciting initiatives! Please find the job descriptions listed below and apply directly. [San Francisco](https://grnh.se/8a5df04e1) [Barcelona](https://boards.greenhouse.io/chartboost/jobs/1533886)
Even with ZIO, you can use it in the cats ecosystem easily. No reason not to use cats in my opinion.
I started largely with FPiS, but found having a secondary reference more focused on Scala itself to be nice. FPiS feels like less of a Scala book to me and more of a FP book that just happens to be in Scala -- a great FP book, mind you, but having something to help you with the Scala language on the side is nice. I used Scala for the Impatient for that purpose, but there are a lot of great books of that sort.
&gt; I'm also interested in how everyone else here develops in Scala. I use the last stable release of the Eclipse plugin. (There are dozens of us!) It's very nice, but currently unmaintained, so I'll switch to something else once my setup breaks for good, which will hopefully not be for a couple of years.
Thats the book I learnt scala with. &amp;#x200B; Then.. I joined a team with play project, all my FP dreams came crashing down. lol
Intellij IDEA is not too bad
Ack I've only had one bad experience with eclipse when I was younger and haven't touched it since, but I'll keep this in mind!
I downloaded it and will try it tomorrow :P Does it have any features to let me remotely work on files?
The author of Ensime DM'd me to let me know he was banned for a seemingly silly reason: https://twitter.com/fpmortals/status/1085109073815785472 He's apparently effectively on strike until the ban is lifted :/
This is a great book and definitely the way to learn Scala and functional programming. You'll get the most out of Scala if you approach problems in a new way and not just port over your Java problem solving skills.
This might be unconventional, but effective python https://effectivepython.com/ is a surprisingly good gateway drug to fp.
Database Library Help!!! &amp;#x200B; I've been working on and off on a project for over 3 years, and picking a DB library tool has caused constant friction and has me constantly questioning whether I should use a different language altogether. &amp;#x200B; I have started off using orientDB (I really wanted graphDB functionality) and essentially had to roll my own DB tool. There are consistently new issues I continue to face that have me questioning whether it will ever be ready for production. In addition, I would prefer to use a tool supported by the larger community. &amp;#x200B; Can someone please recommend a DB API that is production ready &amp; high performant with: \-Built-in Generic CRUD operations via DAO/Repo/etc. or at least very easy to implement without sacrificing utility \-Support for transactions \-Not much SQL string interpolation \-Ease of use querying with multiple JOINs (since data is better suited for graph DB) \-limited boilerplate (especially for schema) &amp;#x200B; I've looked into slick3, quill, doobie, and nothing really jumped out as being the solution. &amp;#x200B; Any help or suggestions greatly appreciated!!! &amp;#x200B; Thanks in advance. I hope I'm not asking for too much.
I always point people at the Scala Koans but they may be a little bit out of date by now. I don't know if they've been updated since the 2.10 days. I used them about a year ago to teach somebody Scala. It went okay.
I'd say don't even go down that rabbit hole. Everyone involved is working toward peace and improving the ecosystem these days. Worrying about drama will just distract you. Long story short: Use [ZIO](https://github.com/scalaz/scalaz-zio) for effects/concurrency, and either Cats or ScalaZ for Haskell-like FP constructs depending on your personal answers to [the questions found here.](https://github.com/fosskers/scalaz-and-cats)
I was gonna recommend that book, but then I saw your comment.
Thanks, good to know it’s being helpful. :)
One of the best books for learning Scala is free - [https://underscore.io/training/courses/essential-scala/](https://underscore.io/training/courses/essential-scala/)
It will become increasingly clear as time goes on that cats is the one to choose, as more libraries drop support for scalaz. Also, since AFAIK scalaz8 never released at the moment they are nearly drop-in replacements for one another - just import nonsense to sort and nearly all the words are the same other than that. It's nowhere near as difficult as changing your HTTP server library for example.
I think if you already know some programming, get familiar with scala's syntax by playing around with it, writing simple programs etc. If you like you can also read a book, but I personally think it's important to get your hands dirty when learning the language. For FP I liked https://www.manning.com/books/functional-programming-in-scala a lot. It provides a good guide into all the important concepts of functional programming. The book is based on excersises that you should do, to understand the theory. By solving the excersises you learn as you go.
This is a great read, thanks for the post
If divisive means doing things that not everyone agrees with, then it's not a point of criticism.
Wait, so men can understand women? That's not what I'm hearing from the pro-abortion crowd
You seem to think I'm arguing for using scalaz. I'm just saying it's confusing for someone just showing up to the party. They are not drop in replacements for each other. The typeclasses for even the most basic types have different method names.
this is another amazing article. So clear, for the first time I am beginning to grok how the IO type works and its motivation. Im so so grateful for easy to digest way to explain this. Cant wait for the next article.
Sorry no I wasn't implying that. I was trying to say that this problem of the ecosystem being confusing is eventually going to go away, as in my opinion we're seeing the start of a slow death of scalaz. You're right some of the method names are different, but the majority of the type signatures should be the same making a transition not entirely beyond the ken of man.
I think they do support it, I'm not sure if they do it in the community version though
I think they do support it, I'm not sure if they do it in the community version though
Trivial use cases are probably easy to convert. Trivial use cases are also probably easy to implement without using either. We can agree that they are largely semantically the same. And i don't find having to change some syntax to be a major hurdle to a switch between the two. I think I'd actually prefer to see a new user reading "functional programming in scala" and implementing things by hand to learn the concepts -- and from there using either library (or switching between them) is probably straight forward.
I haven't done this myself, but SBT supports [unmanaged dependencies](https://www.scala-sbt.org/1.x/docs/Library-Dependencies.html#Unmanaged+dependencies). Put the jar files you want into the `lib` directory. They'll be part of the classpath, can be checked into version control, and should be completely separate from ivy.
It's good effort, but it barely scratch the surface of Scala
Yes indeed. I started yesterday, it is a repo that I will update regularly
May I suggest http://learnyouahaskell.com/chapters. It’s not Scala book, but definitely beneficial to your learning of Scala. Don’t start with the Red book.
Thanks for the recommendation! You can get a quick feel for the language by completing [the tour of go](https://tour.golang.org/welcome/1). If you want a deeper understanding [The Go Programming Language](https://www.goodreads.com/book/show/25080953-the-go-programming-language) is an excellent book that also has great exercises. After that make sure you check out the official [Go blog](https://blog.golang.org/), posts are relatively short and full of useful information. I hope you like Go!
This. Do not start with the red book.
These kinds of references are great. I made something similar for Java/Scala. [https://github.com/shawjef3/JavaScalaCheatSheet](https://github.com/shawjef3/JavaScalaCheatSheet) I see you also didn't make yours in Markdown. I tried but I couldn't get code to render inside tables. &amp;#x200B; Looks like I need to get LetsEncrypt working again though.
Disagreement is unavoidable and even healthy. I mean divisive in the sense of literally fragmenting the community.
Thanks. Glad to know the explanation helped, as this one was a bit tricky to write.
This is like 5 sheets!
Obese sheets 😂
thanks! I appreciate the effort.
A "cheat sheet" never really will though, will it? I mean, conceptually.
sbt-assembly with default rules works just fine. By the way, if you don’t need shading, you can use package and pass the dependencies to spark-submit
Maybe you're right in the sense that cheat sheets shouldn't go into a lot depth and maybe he's right in the sense that there's still "a lot of breadth" to research :) Anyway, it's a great job! I've starred this repo. So, OP, keep up with the great work and share whatever other cheat sheets you create!
Hi lihaoyi, are you based in SF or somewhere else?
Netflix | Data Engineer | San Francisco, USA | Onsite | Full Time | Top of the market &amp;#x200B; Hi, I'm a data engineer in the Streaming Data Science &amp; Engineering at Netflix and we are looking for data engineers [https://jobs.netflix.com/jobs/864557](https://jobs.netflix.com/jobs/864557). Feel free to contact me either for more info, apply or just to have a chat because you are curious. Thanks
Are you kidding me? If you said don't start with the red book you definitely don't want to start with learn you a haskell!
Hello, &amp;#x200B; I have a use case where I need to run a simulation N number of times, I want to run them in parallel. I have tried the following in the code, but it doesn't seem to work &amp;#x200B; object Consumer { def consume(res: Int):Int = { println(s"got $res done") res * 2 } } object Sleeper { def sleepAndWork(time:Int): Int = { println(s"sleeping $time seconds") Thread.sleep(time*1000) time * 2000 } } object FutureTester { def main(args: Array[String]): Unit = { (1 to 50).foreach(x =&gt; { println(x) val p = Promise[Int]() val f = p.future val producer = Future{ val r = Sleeper.sleepAndWork(x) p success r } val consumer = Future { f foreach { r =&gt; Consumer.consume(r) } }}) } } When run the main thread is getting killed before the consumer even starts to process them, I am not sure how I await till all the consumers finish too. Or should I change the paradigm using a blocking queue etc, is there a concise/Scala way to do this? Output: &gt;1 &gt; &gt;sleeping 1 seconds &gt; &gt;2 &gt; &gt;sleeping 2 seconds &gt; &gt;3 &gt; &gt;4 &gt; &gt;5 &gt; &gt;6 &gt; &gt;7 &gt; &gt;sleeping 4 seconds &gt; &gt;sleeping 3 seconds &gt; &gt;8 &gt; &gt;9 &gt; &gt;10 &gt; &gt;11 &gt; &gt;12 &gt; &gt;13 &gt; &gt;14 &gt; &gt;15 &gt; &gt;16 &gt; &gt;17 &gt; &gt;18 &gt; &gt;19 &gt; &gt;20 &gt; &gt;21 &gt; &gt;22 &gt; &gt;23 &gt; &gt;24 &gt; &gt;25 &gt; &gt;26 &gt; &gt;27 &gt; &gt;28 &gt; &gt;29 &gt; &gt;30 &gt; &gt;31 &gt; &gt;32 &gt; &gt;33 &gt; &gt;34 &gt; &gt;35 &gt; &gt;36 &gt; &gt;37 &gt; &gt;38 &gt; &gt;39 &gt; &gt;40 &gt; &gt;41 &gt; &gt;42 &gt; &gt;43 &gt; &gt;44 &gt; &gt;45 &gt; &gt;46 &gt; &gt;47 &gt; &gt;48 &gt; &gt;49 &gt; &gt;50 &gt; &gt; &gt; &gt;Process finished with exit code 0
https://www.reddit.com/wiki/selfpromotion &gt; "It's perfectly fine to be a redditor with a website, it's not okay to be a website with a reddit account." - Confucius ... &gt; Self-promotion is generally frowned upon ... &gt; tl;dr: Don't just spam out your links, ... &gt; * You should submit from a variety of sources (a general rule of thumb is that 10% or less of your posting and conversation should link to your own content), talk to people in the comments (and not just on your own links), and generally be a good member of the community. ... &gt; If you submit mostly your own links and your presence on reddit is mostly for your self-promotion of your brand, page, blog, app, or business, you are more likely to be a spammer than you think! Read the FAQ and make sure that you really understand that.
There's a pair of thoughts in this post that, when you take them together, have some really negative effects: * If you're going to attack someone, you can get away with it if you do it subtly, privately and/or in a socially-sanctioned way, so that people can't call you on it; and * If you're attacked, always turn the other cheek. That seems like a recipe for social structures that blame victims who defend themselves, and entrench the positions of the well-connected or otherwise socially powerful.s like a recipe for social structures that blame victims who defend themselves, and entrench the positions of the well-connected or otherwise socially powerful.
Not only does John not own the LambdaConf situation in his post, but he also deflects criticism of the "scalaz community" (which he limits mainly to Tony Morris) towards Typelevel, which I find rather gross. He essentially argues that, since Tony Morris is no longer a maintainer of Scalaz that it is unfairly criticized since Typelevel's community has also had its fair share of "undesirables", but the truth is that there are several very active members of the scalaz community who continue to not exactly be the most agreeable people to work with. To reference [the original thread](https://contributors.scala-lang.org/t/coc-compatible-community-builds/3097) where people are discussing the community build situation, Daniel Spiewak posted a [very good response](https://contributors.scala-lang.org/t/coc-compatible-community-builds/3097/44) that I personally agree with. People in the "scalaz community" who feel singled-out after these I think need to realize that while it's not any one person's problem, this isn't just "getting rid of people we don't like". It has everything to do with systematic behaviors that are persistently present in interactions with people even to this day (just look at Emily's statements and arguments for a good example of what that looks like). They act like they're being "casted out" of the Scala community because they just "want Scala to be better" (see: the entirety of /r/hascalator) but it's so much more than that. An actively antagonistic attitude is going to just make people not want to work with you, full-stop. As far as I can tell, Typelevel and its CoC-related changes is enabling them to work closer with Lightbend than scalaz ever could because of the focus on community, and I am very happy with how that community is working out. So while I'm very much in favor of John moving ZIO away from scalaz, and I'm glad that ZIO can be judged on its own merits going forward (hopefully), I also think it has yet to be seen that John can honestly admit fault without padding articles with deflections (and advertising himself at every opportunity instead of letting statements be statements, another thing that rubs me the wrong way but I digress). He says a lot about community and inclusiveness but I have yet to see it demonstrated that these are not just empty words. I'd be happy to be proven wrong, though. \--- As an aside, about the community build thing: This entire scenario just rubs me the wrong way because while, yes, every community has bad apples, the group of people who are persistently trying to turn this into an "us-vs-them" fight (see: prominent members of the subreddit I linked above among others) and ranting constantly about the competency of core Scala maintainers and Lightbend employees, well, yeah they're probably going to be excluded from some things and this isn't exactly a huge conspiracy. The LambdaConf post you linked is a prime example of people finding ways to attack others who simply want to feel respected and included in the community instead of ostracized. I don't think it's a coincidence that people who "respect all ideologies" would also be liable to attack a woman who felt so unwelcome. I feel so bad for what Sonia had to go through and it was not just "handled poorly" as he stated in this thread. The entire moral core of the situation is just poorly thought out on his/LambdaConf's end and I would be happy to see actions that indicate to me that he is not that kind of person anymore.
&gt; How can I fix this so that m.resolve(StringContainer("string_key")) returns type of Option[String], m.resolve(IntContainer("int_key")) return type of Option[Int] It seems like `HMap.get` already returns the correct type as you want, so maybe call that directly instead of going through `resolve`? Worst case you can probably make `resolve` infer the necessary info with something like def resolve[C &lt;: Container](c: C)(implicit ev: GenericKey2Value[C, C#T]): C#T = p get c &gt; How can I use some sort of implicit transform such that given any RefinedString I will get back the appropriate container? I'm not sure I see the use for this, especially if `resolve` is changed as above. Is the idea that you could call `resolve("string_key")` instead of `resolve(StringContainer("string_key"))` because `StringContainer` is known to always contain `"string_key"` and `IntContainer` will always have `"int_key"`? If so, then I'd suggest not relying on string literals and use objects to bring those into the type system instead. sealed trait Container { type T = String } object StringKey extends Container { type T = String } object IntKey extends Container { type T = Int } implicit def containerKV[C &lt;: Container]: GenericKey2Value[C, C#T] = null and then you call `resolve(StringKey)` and forget about literals. If you can accept the possibility for a match error than I think an implicit conversion function is enough, e.g implicit def toContainer(s: RefinedString): Container = { case "string_key" =&gt; StringContainer("string_key") case "int_key" =&gt; IntContainer("int_key") } When Scala gets singleton type for literals you may be able to do the `implicit literal =&gt; Container` mapping with something like `implicit def stringKey(s: "string_key"): StringKey = StringKey`.
&gt; In the meantime, I warmly encourage you to fact check, and avoid (even unintentionally) cherrypicking sources, which are inevitably biased (on both sides). While I agree that everyone has a bias, I think it's important to point out that there's a false equivocation here. Sonia's Medium post is a primary source on what happened at LambdaConf 2017. She was *personally* the subject of harassment. She also references (though does not link or screenshot) another primary source, which is Latimore's own blog and Twitter feed. These are not "cherry picked" sources. These are primary evidentiaries and should be respected as authoritative as such. Their *perspective* should be considered, certainly. Even their preexisting biases, which govern how we all perceive and recall events. But they are far greater authorities on this topic than any of us. The inc.com article, on the other hand, qualifies under your characterization.
 [https://www.scalacourses.com/](https://www.scalacourses.com/) (full disclosure - I run that site)
Thanks for addressing this. I'm looking forward to a healthier community going forward, and this gives me hope that we'll get there.
It would be more accurate to say that John claimed the following: - The views were not racist, or at the very least, severely watered down and thus any criticism is cherry-picked - The individual in question (Yarvin) only expressed those views outside the context of LambdaConf, and agreed not to act upon them within the context of LambdaConf, and thus they were irrelevant He put a strong emphasis on the latter point. You can find his contemporaneous posts on the matter [here](http://degoes.net/articles/lambdaconf-inclusion), [here](http://degoes.net/articles/lambdaconf-controversy) (note that there's a *really* good link towards the end to an essay on the subject by Alissa Pajer which is, unfortunately, now dead; it's worth digging up [the archive](http://web.archive.org/web/20170818160736/http://alissapajer.github.io/posts/2016-03-26-lambdaconf.html) though), and [here](http://degoes.net/articles/lambdaconf-conclusion).
Here's one way // start a bunch of parallel futures that will sleep then immediately consume the result // this is usually what you want compared to chaining a bunch of futures, unless you have no choice val resultFutures = (1 to 50) map { i =&gt; Future { Consumer consume Sleeper.sleepAndWork(i) } } // turn a bunch of futures into a future of a bunch of things // completes successfully when all inner futures complete successfully // NOTE: can cause errors to be missed and leave things running a long time even if it fails fast val futureResults = Future sequence resultFutures // wait for everything to finish // blocks the main thread so program will not terminate early // throws if the any of the futures are still not finished by the timeout val results = Await.result(futureResults, 60.seconds)
The online harassment Sonia received at the hands of Breitbart readers is inexcusable, and I wish I had done more to support her through that (the publication politicized the incident for its own aims, without regard to the consequences for anyone). To be clear, Sonia did not leave the conference because she was harassed by Ed, but rather, because she was alerted to old tweets. Sonia actually loved Ed's keynote, but left before she had a chance to interact with him. Unfortunately, Sonia failed to fact check and made many false statements, such as the oft-repeated and debunked claim that Curtis Yarvin was invited, as a keynote speaker, no less—and her primary source for this is the Inc.com article that is a poor example of journalistic integrity. Her emotional experience is valid; but that doesn't mean everything she says must be accepted at face value, only that her _emotional experience_ of these events deserves to be accepted (and of course I accept it).
It might help if you call out what in those libraries seems lacking. I find slick to take care of all of those needs but obviously ymmv. I also like jOOQ for something closer to SQL.
All of this I agree with.
Thanks for this, I’ve been looking for something like this. Very interesting.
\&gt; I don't think it's a coincidence that people who "respect all ideologies" would also be liable to attack a woman who felt so unwelcome. I feel so bad for what Sonia had to go through Unless you somehow agree with her radical political stances, I think Gupta is the worst example to bring up since she's practically a professional agitator. Yarvin stuff is fair game though.
Yes, please!
Thanks it works. For future reference, how does producer consumer pattern play out in Scala? where there are multiple producers and multiple consumers, is using a LinkedBlockingQueue the way to go?
Spark, netty, Jackson, scalaj, snappy, avro, gcs library to leverage their compose command (https://cloud.google.com/storage/docs/composite-objects)
I have gone through slick, doobie and quill in production apps. Right now I default to quill, which is super nice to use if you can live without a way to abstract over db operations (or do it with custom macros) and with "slightly" longer compile times.
Scala tends to favor non-shared immutable state over shared mutable state like that kind of queue. However if you did need it, I'd probably consider a `ConcurrentLinkedQueue` first. In the end the theory for when you use particular queues is the same in Java and Scala and I can't think of any Scala-specific library or idiom I would choose over using the Java library classes for this.
Well duh! Of course cheat-sheets can't lose weight, because they're cheating on their diet all the damn time.
I've read Scala for the Impatient, Programming in Scala (Odersky), and have the Scala cookbook. I also have the Programming Scala (Animal Book) on order which I anticipate will be more useful for those already familiar with Scala. I have found Scala for the impatient incredibly valuable for getting a jumpstart on the language, especially for the exercises. The cookbook is mostly good for a reference, but you can learn from it if you are attentive enough.
So, why did you not just use \`HList\` instead of case classes?
Oh man, that’s my bad. I didn’t realize that what I said could come off as an insult... To my knowledge, calling someone “brolic” is like saying someone is “built” or “diesel”. I was just trying to point out how I think ZIO is an incredible library and Johns quality of character and skill really shows itself in it.
To be fair, I learn more about a candidate by pairing with them on a language of their choice than by asking language specific questions.
I'm not too familiar with Play But getting up and running locally is usually relatively painless with docker. Generally I'll default the environment config to point to something in docker, in this case localstack, and then point it to AWS when deploying your API.
I don't think language specific questions are a good indicator of a future performance level on your company. But being a java + scala developer, I would try to understand how FP he is (does he write java-style scala code with tons of vars and mutability?) Besides that I would try to understand how experienced he is. Maybe doing a system design exercise? Or going through his experience and look for topics he claims to be experienced and ask questions
Ahhh ok I'm sorry I hadn't heard of the term and found some pretty odd definitions on urban dictionary :-), will put your comment back now!
I was doubtful I'd be interested in this but that screenshot convinced me in 2 seconds. Lots of useful info and good presentation!
Thanks for sharing and congrats with the release! How does this compare to `pprint.log(x)`? See http://www.lihaoyi.com/PPrint/. You might be able to implement similar functionality without a custom macro by using `sourcecode.Text[T]`, `sourcecode.Line` and `sourcecode.File` from https://github.com/lihaoyi/sourcecode/#logging BTW, note that the `org.scalamacros:paradise` compiler plugin is not needed for def macros.
I believe that if you don't have a way of accessing a development env in aws directly, then a solution like localstack is unavoidable. I guess you can unit test your code in such a way to react to various sns interaction, but if you want to test more, then you will need something locally. If not locally, then at least in a CI environment
My intuition would be to say, no, endpoints should not be type classes. But do you maybe have a code example?
Mocking framework can mock classes too, but I’m not sure I understand how are you modelling that with typeclasses, do you have a code example?
Thanks for the comments :) I actually used \`pprint\` to format the value, but was not aware of the \`pprint.log\` method. Seems that they are quite similar. Only difference is that \`debug\` supports printing the type of the value as well as printing multiple parameters at the same time.
Printing multiple parameters in the same `debug` method is nice, I have wanted that in `pprint.log` myself a few times. As a workaround I add multiple `pprint.log` lines pprint.log(a) pprint.log(b) You can get the type of a value by adding an `implicit tprint: pprint.TPrint[T]` parameter. See https://gist.github.com/olafurpg/bf61edc60dcff8744fd02234298b8c10 for an example.
In this case, I would pair on scala since it does seem to be a strong requirement for the team. Unless your team has a lot of room/patience to let candidates learn up on a language.
pretty cool! thanks! :)
I'm just getting started with scala and FP - specifically for spark programming. Do you have any handy examples of concise, clean FP in scala that you think I might benefit from seeing?
I assume you might be talking about how `Tagless Final` interfaces are injected via context bounds, which is coïncidentally one of the mechanisms people use in Scala to declare typeclass constraints. Those `tagless final` interfaces are not typeclasses per say, and they are okay to mock (as in, you can provide alternative implementations for specific purposes, including testing)
&gt; He essentially argues that, No, I don't argue that. I'm really disappointed when people try to "read between the lines" for hidden messages. There are no hidden messages. I said what I wanted to say—no more, no less. In looking at the functional Scala community, it's impossible to ignore that some people will not work with either "side" because they have been hurt. It's not necessary to pass judgment or "take sides" to see and acknowledge that hurt. And in the interests of full disclosure, I'm also someone who has hurt others—never intentionally or maliciously, but that has been the effect of some of my actions (no matter how well-intentioned they might have been at the time). &gt; He says a lot about community and inclusiveness but I have yet to see it demonstrated that these are not just empty words If you're looking for more than words, I think you'll be thrilled to learn that ZIO has a wonderful and very diverse community, and without quotas, I do believe that LambdaConf has achieved a much higher percentage of women and minority attendees and speakers than most (maybe all) Scala and functional programming conferences. On a personal basis, I've donated a substantial amount of money to scholarships for the underprivileged and both mentored and trained countless people in my personal time without any compensation. I generally don't talk about these things in public (that's not why I do them), but that doesn't mean I am not working hard for both community and inclusiveness, and there are literally hundreds of people all around the world who will testify to this fact. There are many ways for me to improve, but now that you know more, I think you'll agree that characterizing my work toward community and inclusiveness as "just words" is decidedly inaccurate.
The easiest approach without degrading performance too much is abstract away from SNS if you haven't already and wrap the calls you need with Scala. Create environment prop, system prop, and or config prop that you're running locally. Something like \`SNS\_LOCAL=true\`. In the wrapper you can read that value. Something like private val isProd: Boolean = sys.props.get("SNS_LOCAL").isEmpty From that point you should be able to control if the message is sent or not and in "local mode" return a dummy response with \`PublishResult\` Something like def publish(request: PublishRequest): Either[Throwable, PublishResult] = Try(if (isProd) { sns.publish(request) } else { new PublishResult { setMessageId(s"local-${java.util.UUID.randomUUID.toString}") } }).toEither It's not the most elegant approach but should work without too much effort and down the road you can refactor to use something that is more robust
There is no real solution to this. I've worked in a team where we had many issues getting Localstack installed. We encountered all kinds of different python or pip related errors, we spent hours uninstalling and reinstalling stuff like aws Sam, aws cli, pip, etc and it was a ball ache. But for some of us it was a completely smooth process so milage may vary. Localstack will help you set up SNS locally but your application and the AWS SDK will need to be aware of when it's being run locally so it can point to local instead of the real AWS endpoint. At this point you're already halfway to &gt; if (isLocal) justDontBotherWithTheSnsRequest
Would you consider editing your post here to account for clarifications [offered here](https://www.reddit.com/r/scala/comments/bu7p88/john_a_de_goes_why_im_stepping_back_from_scalaz/epcxpkm?utm_source=share&amp;utm_medium=web2x)?
I'm not sure if they are still available, but Functional Programming Principles in Scala is a good introduction. They used to be found in Coursera but I think I saw the videos on YouTube as well. If you then want to go some steps forward you have "Functional Programming in Scala" (the red book) and "Functional Programming for Mortals" by Sam Halliday. Also, the "FP to Max" talk by John de Goes might be interesting. Again, everything mentioned in this paragraph is a "next step" and maybe not exactly introductory for someone with a Java background (as opposed to a Haskell one). Once you get your head around libraries like Cats or Scalaz and ZIO or Cats Effect or Monix you can jump to Scala Pet Store by Paul James Cleary. I know nothing about Spark to be fair so I can't help here :)
Hi, first of all thanks for responding to my long-winded comment. Did not honestly expect to get one considering the tone of my original comment, and I appreciate being able to discuss this since I feel like it gives both myself and other community members a look into the way the Scala community has been feeling from various angles. Anyway: &gt; No, I don't argue that. I'm really disappointed when people try to "read between the lines" for hidden messages. There are no hidden messages. I said what I wanted to say—no more, no less. This is an interesting argument that I see a lot but I would have to disagree with you there. I believe that there is no text without meta-text, no words without hidden meanings. In our modern society filled with political dog-whistles among other things, especially if you factor in the basic philosophy behind "death of the author", the meaning of any text is something not just intended by the author, but rather, by how it is interpreted at the time of reading by whoever is reading it. If an American political figure talks about "violent urban crime", that obviously means two totally different things based on whether or not you hate black people. Of course this does not erase intent - and I can assure you I don't mean to say you are saying things you are not saying - but given how easy it is to deny more subtle readings of any text, I think you would agree that more concrete responses would benefit everybody. It's very easy to respond to critics by saying "that's not what I meant" in any situation, while leaving it as vague as you did here, and not directly addressing the lines that I was referring to at all. So when you say what you say, in your own words, about communities and the people within them who stir up drama for one reason or another, it comes across to me that the intent is along the lines of saying something like, "Officer, why did you pull me over for speeding when those other cars over there are going fast too? Why single me out?" It seems disingenuous, regardless of intention, because what I was expecting and hoping to read was more along the lines of simply how you want ZIO to not be representative of anybody's past wrongs, not by throwing shade at anyone else in the process, at the very least. This only ever became a discussion about "sides" in my post as a response to when you brought them up in your own post; otherwise I'd have had nothing meaningful to add. &gt; LambdaConf has achieved a much higher percentage of women and minority attendees and speakers than most (maybe all) Scala and functional programming conferences With the above quote and your other claims below it, I don't mean to dismiss them outright when I say, based on what I was also saying above, this can be interpreted several ways as well. When I talk about what happened to Sonia, I'm not talking about just general "being a good person" and "doing nice things" and "welcoming women and minorities" as if there's a certain amount of virtue signaling you have to do to be part of the "good guys club". What I mean is, there is a certain underlying moral component of it all that I'm not seeing. Not that you are not helping people, or that you are not doing anything worthwhile - quite the opposite in fact and I'm glad to hear it. For the record, I agree with the entirety of Sonia's philosophy outlined in her "Bigotry and its Amplifiers" post linked above, and as a result I don't think I've seen anything that addresses her moral concerns. I do remember reading a blog post of yours (which may or may not be the most recent response on this - feel free to correct me) in which you reflected on the situation and said something to the effect of how LambdaConf only draws a line at immediate harm or danger to attendees, but your words at the time only gave me the impression that most of the problem was Sonia's fault for being offended in the first place, and that LambdaConf's main issue was simply not clarifying this position of "inclusivity" more clearly or expressing it in a more constructive way. So it's very easy to express the equivalent of an "I'm not racist, I have a black friend"-style argument like the one you post above about LambdaConf inclusivity, and it's very easy to dismiss this entire CoC-related squabble as something between "different sides" as if there is no moral nuance underneath it all (which IMO seems a bit like a dog-whistle as well based on my interpretation). I still see a lot of thinking in many programming communities along the lines of software being a meritocracy where simply the best ideas shine the brightest regardless of their source, but I've come to believe that the truth is that unless we are all taking a collective stand to not give a platform to those who would divide us for any reason - whether based on perceived technical merits or "political" issues alike - we are never going to achieve a true sense of unity and community. The opposite would be similar to saying "racism/sexism/bigotry would just go away if people stop talking about it and making it an issue", which I think everybody knows just doesn't work. Anyway those are my thoughts, you can feel free to correct me on any of the factual details I might be getting wrong. Again I appreciate your response and I hope I don't come across as actively antagonistic. I just don't see this "rift" as something that can naturally heal unless everybody - where necessary - adjusts their entire outlook on what it means to be a community of software developers by being willing to call these problems out when they happen - like Sonia attempted to do. True inclusivity and community can only happen with outright, concrete rejection of bigotry.
There isn't anything "un-functional" about abstracting functionality with traits rather than type classes. Traits simply say "if you have an instance of this type, it will have this functionality". While typeclasses say "here's a set of functionality which can be applied to any valid type. It's just a matter of using the right one at the right time. In your case, it seems like traits are a better fit because the behavior of your endpoints is going to vary based on implementation of the endpoint, not based on type of the parameters of each endpoint.
I've read John's informative reply to me and the statement you linked, and I've re-read what I wrote. I'm not sure what, if anything, should be edited. If I wrote something inaccurate, I'd be glad to correct it.
In particular, Yarvin wasn't a keynote speaker. The linked Medium article has some factual errors, which are the ones that easily catch in one's ear and spread around. Bad news spreads easier, so we should't be surprised to still hear these echoes, but we should also endeavour not to amplify the message, hence I reached out.
Has it been difficult to manage not being able to abstract over the basic CRUD ops? Even with our current code that would be 100+ classes nearly identical code for findbyid, save, etc.
There are a handful of open source projects out there that provide simplistic implementations of some of the AWS service APIs for use in local development or testing. I have used this one for SQS in the past: [https://github.com/iain/fake\_sqs](https://github.com/iain/fake_sqs) &amp;#x200B; But this one looks like it could be a decent option for SNS: [https://github.com/s12v/sns](https://github.com/s12v/sns) &amp;#x200B; From a software perspective, abstracting away from the thing (like what /u/k3mist suggested) is probably the most ideal. But sometimes it can be handy to just fire up a dummy instance and point your client at it.
I found it (after the migration to quill) that I almost never need full basic crud. Instead I started implementing more logic in repo rather than service with better tailored methods which resulted in more performance and less code. I wanted badly to abstract over crud at the beginning but it turned out to be purely artificial need
I'm still waiting for someone to tell me that any scala http lib (favorably http4s) works with \`native-image\` out of the box.
Cool! I think it says a lot of positive that this is useful and compact. A whole 64 lines macro that is easy to read. Good stuff!
Well, I haven't tried native-image, but http4s tests (under scala 2.12) perfectly cleanly with GraalVM 19.0.
Awesome! I don't work in Scala anymore, but in the few years that I did, the support for editors was pretty buggy. It's heartening to see layers like this trickling in!
Thanks! I just converted from vim to spacemacs couples of months ago so I am trying to do everything in Emacs :P It's still buggy for Scala but it's getting there, Metals is really awesome! Are you using IntelliJ Idea?
I found the best mix for me was to mostly write in Spacemacs (later Doom) while having Idea also open to leverage all the import/go-to-symbol-def niceties It wasn't the best experience, but the best I could make of it
We exist! I've been using Ensime in Spacemacs for a while now. I'm quite excited to give this a spin, I'll be sure to provide some feedback once I've had a chance to try it!
I should switch to spacemacs some day... Looks easier than standard emacs to get a nice config. &amp;#x200B; With a bit of config getting lsp + metals working nicely in standard emacs is doable. My config starts at: &amp;#x200B; * [https://github.com/coreyoconnor/RCs/blob/master/emacs.d/local/configure-modes.el#L87](https://github.com/coreyoconnor/RCs/blob/master/emacs.d/local/configure-modes.el#L87) &amp;#x200B; tip: with lsp working \`xref-find-definitions\` will go to symbol definition.
I think (1) is unfortunate, but it's true; the more people who see this behavior and act on it, the better off we'll all be. As for (2), I really think it only applies to community _leaders_, from whom more is expected. Leaders by definition have more power than followers.
It’s an enterprise focused tool that really is a product that solves a problem s businesses care about, not having highly paid developers sit and twiddle their thumbs waiting for code to compile. It’s existed for many years and is developed by a small team that supports most of Scala, a programming language with one of the fancier type checkers/inference schemes. Ultimately, what developers choose to do with code they write is their right, and it’s fully within reason for a tool like that to be closed and sold to enterprises.
&gt; This is an interesting argument that I see a lot but I would have to disagree with you there. You can always ask me _why_ I did something. In this case, imagine being someone who contributes to Scalaz specifically because you've found the community more welcoming, and two or three people on the Typelevel side have hurt, scared, or angered you so much that you want nothing to do with the organization. Now, I happen to know such people (just like I know people with the opposite experience!). If they read my post and they see me mentioning the fact that Tony's communication has pushed people away from Scalaz, without also acknowledging that they don't feel safe contributing to Cats, then I'd expect they will feel I am being decidedly biased, ignoring their hurt, and scapegoating. I don't want that. I want them to understand I don't see the situation as good-versus-evil and that I understand and accept their experiences, too. &gt; When I talk about what happened to Sonia, I'm not talking about just general "being a good person" and "doing nice things" and "welcoming women and minorities" There is no charitable way to characterize what I do as being "just empty words" or "virtue signaling". I know these characterizations are not true, and I think you just didn't know the full story (I don't make most of it public, honestly). For perspective, however, I would like to ask you: - How much money have you donated to community and inclusion? - How much personal time have you spent on community and inclusion? - How many people have you mentored in the community, and at what personal cost? - How many people have you helped transition from unemployment or toxic jobs to better jobs? I will hazard a guess you have spent your time and money on this cause (beyond posting on Reddit), because it is important to you. It's exactly the same with me. My own work spans more than a decade, has involved substantial costs to myself, and has had a positive impact on hundreds of people. If you want to have a meaningful conversation, we must start at the assumption of mutual good-intent. We're _both_ people who care about inclusion and community. We can build on that foundation. In fact, it's the only one we can build on. &gt; What I mean is, there is a certain underlying moral component of it all that I'm not seeing. I'm seeing very rigid thinking, and this disappoints me, especially because I don't think you have all the facts yet. In particular: - Did you do any fact-checking on Sonia's piece? Do you know which statements are false, and which are true? - Did you talk to me, my wife, Ed Latimore, or any other eyewitness, or is Sonia's account your only understanding of the events? - Were you aware that at no point did Sonia reach out to ask for clarity on Ed's views, and that in one case, she misconstrued a typo of his? - Did you know which segments of the story were left out, such as the moment when my wife, moved with empathy for Sonia, reached out to hug her? - Did you know Sonia refused to meet with me to discuss her concerns, preferring instead to quote-tweet? - Did you know most of the Breitbart harassment occurred when Sonia (shortly after the conference) celebrated the shooting of a Republican politician, leading to a second Breitbart piece with much more fallout? - Did you know about the women and people of color who reached out to _me_ for support after being attacked by Sonia in public? There are entire sides of this story that you don't know about; and some you'll never know about. I accept Sonia's experience, and I hope to help end prejudice in all its forms (and I know she wants that too), but if you're looking for a black-and-white story filled with heroes and villains, you're not going to find it here. It's a complex story filled with real people. It's a story in which Sonia was hurt and harassed online, and yes, I did not support her through that. It's a story in which Sonia definitely (even if unintentionally) played fast and loose with the facts and people's reputations. And it's a story in which a few people have come to very dogmatic conclusions without being fully informed.
&gt; The views were not racist, or at the very least, severely watered down and thus any criticism is cherry-picked I've never claimed this, and it's deeply disappointing to me that you would believe that I've made this claim in the absence of any evidence. It's even more disappointing that you would _propagate_ this claim in a public forum, dismissing the damage that false statements can have. For the record, I said the exact opposite: that Yarvin's views on different races having different "average IQs" or "suitability for slavery" were racist views.
Spacemacs is for amateurs like me:P It can be hard to make customization because of the layers it built on top of vanilla emacs. Nice config! I probably will "steal" some of it and put it into my layer :)
Awesome man! Just ping me if you have any questions! I am no spacemacs expert but I will try my best 😂
Well, as history shows, there is very few companies who succeeded at OSS model. Especially, if you are you are looking at the long term survival of a company. * OSS database vendors discovered that e.g. Amazon uses their stack without paying back, so they notice no profit while the maintenance of codebase is completely on them (which is why MongoDB changed their licence) * companies who succeeded at OSS are either offering paid support + consulting (Red Hat, Lightbend, etc), or closed source (proprietary) extensions to their OSS software (e.g. IntelliJ - all the "best" plugins are closed source and paid) * ask yourself: would _you_ make a donation for some OSS you use, because you find it fun? I sometimes wonder, about it but ultimately I _never_ made a donation for OSS project, while I easily paid hundreds to get closed source software. Because, I could feel _I own_ the licence of something maintained, as opposed to not be sure if authors won't give up tomorrow because they have anything out of it. Besides there is also a psychology: many people feel that if you don't pay for it, it has no value (I don't state that it is true, merely that's how psychology works), so if they offered their product as OSS, people would get it for free (legally, or not) and they would went out of business, with no hope that any investor that would save them would look at them seriously. Basically, OSS as a business model works only as an advertising, there you are making money off something else, so you can give something away for free. If that is your core business product, most of the time OSS is the suicide.
At the risk of undermining your high (it's always cool to figure stuff on your own), the example you give in this reddit post doesn't really take advantage of the "higher-kinded" nature of the data and would work with a parametric type \`WeatherData\[A\]\`, which forms a monoid provided \`A\` forms a monoid. And yes, monoids are indeed extremely useful. &amp;#x200B; You are however converging towards something : \`WeatherDataHK\[F\]\` forms a \`monoid\` provided \`F\` forms a \[MonoidK\]([https://typelevel.org/cats/typeclasses/monoidk.html](https://typelevel.org/cats/typeclasses/monoidk.html)). You can extract a pattern from it, and could write a library that would provide a \`MonoidHK\` typeclass for the particular kind you're currently working with, but considering how few people in the industry are actually able to recognise simple monoids in the wild and use them correctly, I'm not sure a higher-kinded version of them would find their audience. Sadly the concept cannot be generalised as Scala current lacks features that would allow for it, though it's \[being explored\]([https://contributors.scala-lang.org/t/proposal-to-add-kind-polymorphism-to-the-language/2958](https://contributors.scala-lang.org/t/proposal-to-add-kind-polymorphism-to-the-language/2958))
If you like to write your web applications using weird DSL then http4s [https://github.com/http4s/http4s](https://github.com/http4s/http4s) looks like the one.
The first one I heard of was [typedapi](https://github.com/pheymann/typedapi) but there was another one coming out almost at the same time though I can't recall the name. There's also [tapir](https://github.com/softwaremill/tapir) which doesn't operate at the type level as `servant` does but it allows you to generate client, server and docs from the api description and it supports `http4s` and `akka-http`.
`http4s` is awesome (and is my goto http library) but he's asking about a similar lib to `servant` where you can define your API at the type level instead.
Thanks I will check those out!
https://github.com/julienrf/endpoints uses traits instead of phantom types, but IMHO it's a more idiomatic alternative to servant in Scala.
Rewards Network | Scala Software Engineer (all levels) - (SDET positions also available) | Chicago, IL, USA | Onsite | Full Time Rewards Network powers the leading dining rewards programs in North America including partnerships with major frequent flyer programs, several of the nation's largest bank card issuers, and dozens of national corporations. We are looking to grow our Scala developer teams to scale out and tackle creating the rewards platform of the future by rethinking our existing infrastructure using functional programming principles. We are looking for people who: \* Have experience with Scala (2+ years preferred), or are eager to learn \* Have experience with Docker, Kubernetes, and/or DC/OS \* Have experience working with Akka, Play, Spark, or other Scala-focused frameworks \* Have experience with Spark SQL, Cassandra, Elasticsearch, and/or Kafka (strongly preferred) \* Are familiar with and passionate about functional programming and software testing (ScalaTest, Specs2, Gatling, BDD/TDD in general) \* Are familiar with functional reactive programming (Reactive Manifesto signers get bonus points) We are primarily hiring for senior-level positions as software engineers as well as SDETs, but we encourage anyone with Scala/FP experience to apply. You can private message me for more details and contact information, or comment with general questions.
I would actually start with ZIO. I've never tried it but if I was starting from scratch it seems like an ideal framework. Plus it would give you a slow introduction to functional programming. Twitter's Finagle or Tumblr's Colossus are another ones that could be useful.
Thanks for your input. No worries, it's all a learning experience. I will go back to drawing board and see what I can come up with. I was pretty impressed with the aggregations that can be performed as shown in the example. I agree though, the abstractions are a bit off. Thanks again.
I would recommend lichess (repo name Lila on GitHub). It’s a real production service made open source, using Akka and the Play framework.
How does this compare to [better-files](https://github.com/pathikrit/better-files)?
The biggest differences are: - No extension methods, no implicits to import. Everything is a normal instance method, or a static function, so it's easy to see where things come from. - A full API: `os.*` provides everything you need, including all the edge cases and flags that the underlying filesystem API can take. You never need to fall back to `java.{io,nio}.*` types except for interop with existing code that uses them. - Full support for subprocess handling. - Only one way to do things Overall this is designed to be an API that would look just as good in any language: python, javascript, ruby, java, C#, etc. without any idiosyncratic Scala-isms.
Sounds like you want something more practical than theoretical, so I'd recommend getting a Play project up and running. It's fairly well documented, and offers support and integrations from the API to the database. Once you understand more about scala and the wider ecosystem, you can exercise your knowledge by purging Play from your project :)
I came to functional programming via ML for the Working Programmer, which is available as a book or also free online. (Standard ML pushes you towards a very strict functional style, e.g. mutable variables are distinctly cumbersome to use. I found that made it a good environment for learning functional programming, though possibly a bad language for a business project for the same reason)
True but not really avoidable at this stage. If things had gone differently, perhaps scalaz could have become a place where people could safely look for help without being attacked[1]. But at this point the least-bad achievable option is migrating the community to an alternative implementation of the same functionality. As with any fork/replacement scenario, there's a certain amount of disruption while we migrate. [1] Contrary to De Goes' statements about "response to ad hominem", I saw Morris turn personal insults on newcomers who'd done nothing more than ask for help and repeat technical claims that are widely taught/accepted.
ZIO is great and all but I can never understand the rationale of recommending it to someone who is starting with Scala. ZIO is a project so heavy in FP that more than half of experienced Scala developers do not understand how to use ZIO, lets alone understanding how it is written.
Most things in the world are closed source.
Thanks all for the responses. I am tinkering with different options and some of the suggestions outlined here. I will report back .
Mr. Morris no longer works on Scala projects from what i can gather, which complicates your position a bit. We really have two viable projects, one with a reputation that was previously tarnished in the community. But new developers are not going to have your scars. People are going to continue to use scalaz -- even if only because the book or website they are reading to learn used it 5 or 10 years ago. I do agree that it's unavoidable at this point, but stand by my original position that it's unfortunate. If it were up to me, i would continue work on interoperation between the two for the longer term, and be producing more free training materials based on cats.
Great article! I am a bit rusty on my maths. Could someone make the point of e being the most optimal base a bit clearer for me? &gt; So we’re trying to minimise 1 / L*B where L = Length of expansion of N in base B and B = Chosen base, with respect to B. It's probably trivial but I have trouble understanding how it follows that we need to minimize 1 / L*B. &gt; We know L = log_b(N) so we have to find the minimum of 1 / (B * log_b(N)). This is the same as finding the minimum of B / log(B), where log is the natural logarithm. Why is it the same as B / ln(B)? How do we get rid of N?
Founder here. It's a good question, and totally legitimate. * We're a small, bootstrapped company. We didn't take investments so far and don't plan to (more about it below) and as such we rely only on revenues to keep going * We've offered free licenses to all open-source projects who asked, and continue to do so. We don't have an official program yet, but we'll get to it. * It's a niche product in a small market. We don't have a sales team to chase down large enterprises to pay for support * I don't know a good OSS model that would work for us within the constraints above (no investor money, no enterprise sales team). This is probably the biggest reason. We talked to several investors but decided to go on our own. The product is too niche, and I wanted to have complete freedom to do the things we believe matter. I can't think of a good MVP for Hydra: if we don't support the full language (macros, compiler plugins, etc), we turn away our most interesting clients: large codebases that use advanced Scala features. That, and the pressure to go for a larger market ("can you do this for Java?") made us say no to investors, and take the decision to charge from day 1. We try our best to give back to the open-source community. We're running a [promotion throughout June](https://typelevel.org/blog/2019/05/29/support-typelevel-thanks-to-triplequote-hydra.html) to donate to the Typelevel organization, and contribute as much as we can to other projects (including [Ensime for VS Code](https://marketplace.visualstudio.com/items?itemName=dragos.scala-lsp)). Others may disagree, or do it differently if they were in our shoes. I'm very eager to hear about it. We still have a lot to learn, and maybe there's a better way.
[removed]
Not sure that an HList would allow me to map keys to values? Can you describe how I would use an HList in this circumstance?
I'm afraid it might not make sense because it's a glaring, horrendous error on my part. If we're minimising `B` and minimising `log_B(N)` then we're minimising `B * log_B(N)` and NOT the inverse. I will fix it right now :) The rest of the maths is fine though, it was just a munging of different bit of maths in my notes. We know: Log_b(x) = Log_a(x)/Log_a(b) where `b` is the base you start with, and `a` is the base you want. So: log_B(N) = ln(N) / ln(B) But we're keeping `N` fixed. We need to measure the 'efficiency' in terms of the base, not `N`, so `N` is basically a constant. Therefore, B * log_B(N) = B * ln(N) / ln(B) We can ignore the `ln(N)` because it's basically a constant - we're minimising with respect to `B` rather than `N`. The minimum would still be the same value of `B` whether `N` were `1` or `100`.
I couldn't figure out for the life of me how the B ended up in the numerator working from the inverse. Makes perfect sense now, also this clears up why we can get rid of `ln(N)`. Thank you!
Thanks for bringing it to my attention before too many people read the article...
&gt; Mr. Morris no longer works on Scala projects from what i can gather, which complicates your position a bit. &gt; We really have two viable projects, one with a reputation that was previously tarnished in the community. But new developers are not going to have your scars. Morris is indelibly associated with scalaz as the founder (much as e.g. van Rossum is indelibly associated with Python). When he talks, scalaz-aligned people listen. And he does still show up in their channels. And even without Morris, scalaz retains a culture of ruthless criticism of code that edges close to - and, in practice, sometimes crosses - the line into personal attacks. And this seems to extend to the way that prominent scalaz contributors behave when interacting with the rest of the community, in a way that's still very much live: in the community build incident we saw e.g. emilypii making personal attacks (subsequently edited out) while at least claiming to speak for scalaz contributors (and FWIW, in my capacity as a mod here I received similar attacks from prominent scalaz folk during previous incidents). So I don't believe scalaz can ever be made viable. The culture is self-sustaining due to the selection bias that De Goes describes; people who don't want to be associated with a culture of personal attacks avoid scalaz, so the project remains a place where newcomers seeking help will suffer harsh criticism of their code and, less often but still often enough to be a problem, themselves. It's a judgement call whether the resulting "technical excellence" is worth it; my view is that the scalaz culture costs far more in newcomers driven off than it ever gains, and for as long as the scala ecosystem remains closely associated with scalaz the whole language community suffers for it. (IMO even De Goes' philosophy errs too far on the side of correctness over kindness; "you are not your code" is true as far as it goes, but it's normal and healthy for people to feel attachment to things they've created, and our community norms should acknowledge this. Sometimes - by no means always - a technically correct criticism is not worth making if the speaker is unable or unwilling to make it kindly enough) &gt; If it were up to me, i would continue work on interoperation between the two for the longer term I don't see what's to be gained from interoperability. If you commit to close compatibility with scalaz APIs then books etc. will continue to write in terms of scalaz and newcomers will continue to head for scalaz spaces looking for help. You might say there's value in competition between two implementations of the same thing, but the only effective competition is for one to implement things the other lacks (and vice versa). It's worth providing a migration path, but it's also worth making as clean and quick a break as possible, IMO.
Haha I love it. Great post.
This looks great! I like that it's so explicit about current working directory / absolute paths. Is it also explicit about character encoding? And is it correctly understood that it's not possible to leak file handles with this library?
I don't think a harsh voice is ever necessary, in a professional or volunteer context, so i support your motivations. But some developers/techies can be prickly people. And sometimes they are so because they have their own developmental limitations, and some have unrelated emotional issues, and sometimes they are just jerks, and sometimes they cross the line between those things without knowing or intending it. We have a few of them here in my office, and over the years ive gone through the different stages of acceptance -- hating them, avoiding them, managing my expectations for them, and ultimately reaching some understanding of what makes them tick. Prickly behavior and over-sensitivity are two reactions to the same thing -- they are both emotional responses to being challenged. And if you get two people at the opposite poles collaborating, i can see the scenarios you laid out above happening. The goal should not be to eliminate people at one pole in favor of the other -- the goal should be to not allow overly emotional people be in charge. Mr. Morris proved this point clearly. But a community should be able to absorb both kinds of people, and being level-headed and dispassionate is how that is done. So i think there is an strong argument for correctness/measurement as a standard.
That first part was **definitely** what I was looking for. Thank you! I will need to read up on this `#` symbol. I've never seen that used in Scala before, what is it called? Still working through the second part, but hopefully your example will help me out.
If I see it right, your original problem was, that the number of fields are too big for a case class. That's why I wanted to know if you tried to solve it with HLists instead and if so, why you didn't/couldn't choose this solution. &amp;#x200B; Usually, whenever I see the name of a Type (like String or Int) in the name of a type/class, all my red alert lights turn on and I know that this will not be an elegant concise solution.
Gotcha -- updated about the keynote part.
The `C#T` syntax is called a type projection. You'll find most of the reading under that term.
Anyone know the recommended way to do DB transactions in ZIO?
I toyed around with hoare logic as well! It turns out my experiment led nowhere and the papers I read on the matter go the other away - from hoare logic to decision tables, where I start with decision tables. It’s quite fun, and I hope to get a chance to present it in a future conference, it seems decision tables have fallen out of fashion but could really, really help in some scenarios
The point wasn't to understand the code but to write the app using it. If you're writing heavily concurrent code it seems a lot better and simpler than something like Akka.
[removed]
Could you use scastie or scalafiddle, or other existing cloud hosted compilers instead?
Your message makes it sound like you have the choice. If it's the case, I'd encourage you to go with another cloud solution, as 190MB is awfully small. There a lots of solutions with decent free-tiers out there. &amp;#x200B; If you don't have the choice or want to use that particular cloud solution for personal reasons, then you could try \[mill\]([http://www.lihaoyi.com/mill/](http://www.lihaoyi.com/mill/)). The executable is 37MB, but you'll obviously have to add the size of the compiler/standard library, which it downloads dynamically as sbt does. Alternatively, you could call the compiler directly without a build tool, or use \[bloop\]([https://github.com/scalacenter/bloop](https://github.com/scalacenter/bloop)) (which would require a little bit of configuration, usually generated by the build tool)
Here is the repository in case someone wants to take a look. https://github.com/ornicar/lila
&gt; only need to compile to JS and am not planning to use any extra libraries. If this is your requirement, starting from the ScalaFiddle codebase is probably the best. That does exactly what you want, and nothing more, since you don't seem to need all the other build-tool stuff that even Mill would contain
&gt; And is it correctly understood that it's not possible to leak file handles with this library? It's still possible, but probably a lot more difficult than through `java.io`/`java.nio`/`scala.io` usage patterns. In particular, there are still `read.inputStream`/`write.outputStream` functions whose returned streams you can still forget to close, but you use those pretty infrequently now that there are other more convenient such as `.stream`
ZIO itself is probably not the right type to do DB transactions. Take a look at doobie and see how it is done there - it uses its own special ConnectionIO type which can be combined (and allows to form bigger transactions) and executed in the end, transforming it into IO (or ZIO of you like).
&gt;f you're writing heavily concurrent code it seems a lot better and simpler than something like Akka. Simple is always subjective.
Sorry, what I meant was that I was not planning on using any extra Scala libraries, as what I have seen so far is that I can reference Javascript dependencies that I need through the html files. I might need a facade here and there though. I am fine with building locally, but I mainly ask for my partner who prefers real time cloud collaboration over things such as github due to its simplicity. My main issue with Scala fiddle is that I do not know how to configure the html file and do not know if it has real time collaboration like Google docs.
Yah, the sbt gzip itself was 40MB, but the online downloading quickly went up to past the 190MB given. I will still look into mill, especially because lihaoyi replied below.
Maybe, from hearing it may suffer from the same issues, but I am hopeful.
Do any of those have real time collaboration or the ability to configure the html file that uses the compiled js output as a script?
Oh and bloop looks great.
Rho is another candidate that allows you to extract an AST from the route definition [https://github.com/http4s/rho](https://github.com/http4s/rho)
Hopefully the tips here save a little head-scratching for fellow Scala devs. Any questions, clarifications or other feedback is welcome.
Probably just wasn't a priority. &amp;#x200B; It helped that Scala and the Scala ecosystem isn't under a copyleft license.
Probably just wasn't a priority. It also helped that Scala and the Scala ecosystem is not under a copyleft license.
I wouldn't recommend using `SettingKey#apply` in ``` libraryDependencies ++= scalaVersion(version =&gt; protobufs(version)).value ``` but rather ``` libraryDependencies ++= protobufs(scalaVersion.value) ```
Could you maybe expand on that ? What is your partner's background ?
We don't really have too much experience, just learning stuff on the side for fun. He mainly played with Javascript and I with Java then Scala. We were thinking of making a multiplayer game possibly, we recognize the enormity of the task but still want to keep trying and learn from our mistakes. We plan on doing a Scala js front end and a JVM or native backend, early in development we were planning on doing a Scala Js backend for simplicity purposes by using a js library for websockets. All libraries we plan on using are js libraries. Neither of us have really used git that much and the live collaboration was a really simple way to Co develop. But we aren't doing anything too serious so we were using a free cloud solution.
Ah I didn’t know about this, is there any benefit besides being easier to read?
Right. So you should absolutely learn git, even if you're just using it in a very rudimentary fashion and learn only the 3 most basic commands, it will make your life easier and will help you recover when you fuck up (everybody does at some point). Secondly, if you're doing pair programming exclusively, you could use the vscode live sharing plugin, it's pretty amazing. Combined with the metals plugin for vscode (Scala support), you could pair program and both benefit from the tooling installed on your machine to get you started.
Yeah `.value` works the same for both `SettingKey` and `TaskKey`, while the other method requires switching from `apply` to `map` for `TaskKey`.
Just as an aside regarding working on remote files. VS Code has early support for working with remote files over SSH or Docker containers. It's only available in the VS Code Insiders build (daily build) - but will install alongside an existing VS Code instance quite happily. You could then use Metals and edit files via the container - all dependencies would be installed within the container with the source code mapped from the container to your local/host directory. I've tested this with Python but not with Metals as yet. &amp;#x200B; I think IntelliJ supports remote working in the Professional edition - i.e. not the community - at least it does for Python.
Search exercism Scala track it’s free
Iirc exercism offers algorithmic exercises and not tutorials on building real world apps
If none of the sources in the bar on the right is good enough, then you don't find much more. As far as I can tell, there is hardly any demand for such approach, because the usual road is that you pick up the basics with something like [*Functional Programming in Scala*](https://www.manning.com/books/functional-programming-in-scala) aka the Red Book or [*The Neophyte's Guide to Scala*](http://danielwestheide.com/scala/neophytes.html)*,* and then decide on where you want to go: more into Akka ecosystem (actors, reactive streams, etc), big data data (Apache Spark), or more functional programming (Scalaz/Cats). Problem with writing such a tutorial is that new Scala programmers would use slightly different approach than advanced users: for beginners you show things that are easy to explain to give them time to absorb some concepts, which underutilizes the benefits of Scala (this is often a "slightly better Java" or "Kotlin with a different syntax"). On the other hand, using all that Scala gives you might require usage of several language features at once - not a problem if you had a time to adjust, but something you could bounce off if you hadn't.
fp to the max by John De Goes. The pattern scales to large applications very easily.
Also Akka in Action from Manning Press.
&gt; I am fine with building locally, but I mainly ask for my partner who prefers real time cloud collaboration over things such as github due to its simplicity. I think you've hit the jackpot, because: * You can use [@lihaoyi](https://www.reddit.com/user/lihaoyi/)'s [workbench](https://github.com/lihaoyi/workbench) for fast interactive Scala.js development with sbt. * You can use [Live Share](https://visualstudio.microsoft.com/services/live-share/) for real-time collaboration in Visual Studio Code. * You can use [Metals](https://scalameta.org/metals/) for a very nice Scala development experience in Visual Studio Code. This way, you have the full power of your local development box, and can customize your environment in whatever way suits you best.
Is it easy to interchange between this and nio?
This course was not mentioned on this sub afaik, but it is pretty cool. It's very practical and exercise oriented. Daniel is a good teacher. I know it's Udemy and non free, but if you wait long enough they will have one those "just 2 more days at this price" promotions. :) Here's the link: https://www.udemy.com/share/1011jaAkAYeFlWQ3o=.
Thanks.
Typically in scala.js you manipulate html of a page from Scale.
You may want to try buying a course on udemy on incognito mode :) Pretty nice prices may appeaar
Yep!
Functional programming in Scala Alexander...I forgot the actual name. Google might be able to help.
I have this book. I tried using it as a supplement, and it only occasionally helped when challenging the red boook tbh.
You could also simply ask for clarification online. There are communities out there which are pretty open for questions like this one for instance https://discord.gg/8ESjA8d
There is an official companion : https://www.amazon.co.uk/companion-booklet-Functional-Programming-Scala/dp/1508537569 I haven't used that myself so I don't know exactly what it adds :)
[removed]
I think you refer to this: https://alvinalexander.com/scala/functional-programming-simplified-book Imho a great book, that really gives a good introduction to functional programming in general. The only thing I dislike is the fact that you don't get the ebook for free if you have bought the printed version. That's really great about Manning publications.
Senior Dev with 2 Years of experience are purely fictional creatures.
There is a company book for the red book, it's called "A companion book to functional programming in Scala". &amp;#x200B; functional programming in Scala by Alexander dude was a great first read, then Advanced Scala with Cats was a second great read. Never felt the need to read the red book after that (although I read the first few chapters and never quite got why it was so highly recommended to beginners in particular - it's a great CS book but not a friendly introduction to the topic of FP in Scala, imo).
I didn't see the video you're referring to but it sounds like you're looking for OpenAPI scaffolding/codegen.
I'm really hoping to find this specific talk. It was not that much about code generation but about the journey that got them there.
The red book is only useful if you want to understand all the intricacies of the Scala FP implementation, but I agree it's a very heavy read with homework as well. I'd look into underscore.io books, cats docs or maybe FP for mortals.
What is the purpose of the \`classOf\` function? What does it return and when is it usually used?
[removed]
I don’t know about a talk but are you referring to guardrail? https://github.com/twilio/guardrail/blob/master/README.md
https://youtu.be/j6ow-UemzBc is that the talk?
Does the Play Framework edge towards the OOP side of Scala vs the FP? I’m trying to brush up for a Scala team and I’m pretty sure they’re using Play Framework. Want to make sure I practice the right stuff to be the most effective :)
Using `classOf[T]` returns the `Class[T]` instance for the type `T`. It's the same as using `T.class` or `t.getClass()` in Java. I find it's mostly used when interacting with libraries which rely on reflection.
It returns a `java.lang.Class` object representing the runtime class of the passed type, in the same way as the `.class` syntax in Java (e.g. `Foo.class` (java) &lt;=&gt; `classOf[Foo]` (Scala)). It's used either for doing reflection via the `java.lang.Class` API, or more likely for passing to a java reflection-based library that uses that API. E.g. a Java reflection-based library for json serialization might have an API that looks like `deserialize(json: String, into: Class[_])` that parses the JSON and uses it to construct/populate an instance of the supplied class, via reflection. Note that Java reflection is not type-safe; often there's a more idiomatic Scala alternative based around shapeless typeclass derivation that has better checking at compile time. E.g. Circe offers an API that will only let you pass a type that it knows how to deserialize (but including custom types recursively made up of types it can deserialize), whereas with a Java reflection-based API there's nothing to stop you passing a class that it would make no sense to deserialize from JSON, such as `java.net.Socket.class`.
google around swagger (https://swagger.io/). I think there are tools to generate code from their specification
I work there :) We use apibuilder instead of swagger. A fairly controversial choose. The specs are used to generate models, json readers and writers, play body and query parsers, data access objects, and database creation scripts. This allows the crud website to get out of the way, and focus on bigger problems. It isn't without it's problems, but a great way to skip most of the boring stuff.
Very cool. Thanks for details. I guess I would try for some form of shared source to get more eyes on the code, combined with the Enterprise sales staff chasing support contracts
Don't know about that talk specifically, but our company did the same, using [Idealingua](https://izumi.7mind.io/latest/release/doc/idealingua/) language.
Awesome work as usual!
Have you checked out the official chapter notes and answers to the exercises: https://github.com/fpinscala/fpinscala
Li- I know you love python, and I love your work, but I have some serious qualms about this line: &gt; sudo sh -c '(echo "#!/usr/bin/env sh" &amp;&amp; curl -L https://github.com/lihaoyi/Ammonite/releases/download/1.6.7/2.12-1.6.7) &gt; /usr/local/bin/amm &amp;&amp; chmod +x /usr/local/bin/amm' Please don’t use this as an installer. Especially with sudo as a solution to “make permission issues go away”. Thanks for your awesome (hard) work and contributions to the scala ecosystem.
Just realized this was Josh, not Li. Josh: when you’re done karma farming and spreading the good word, please forward my regards to Li. I think the approach might be undermining the credibility of the message.
Just realized this was Josh, not Li. Josh: when you’re done karma farming and spreading the good word, please forward my regards to Li. I think the approach might be undermining the credibility of the message.
Disclaimer: I'm the author to \[tapir\]([https://github.com/softwaremill/tapir](https://github.com/softwaremill/tapir)) Documenting APIs is one of the goals of tapir. With the library, you create a description of an endpoint, represented as a value (a case class). This can be then interpreted either as a server, or as Swagger documentation. Further, the generated documentation is a case class as well (of type \`OpenAPI\`, which is a direct reification of the openapi model), and can be adjusted.
I understand this might be a limitation. I suspect you are only interpreting endpoints as a server (not a client) :). As a work around you can define empty encoders/decoders (this can be done generically if your inputs/outputs e.g. extend a common trait). But I agree that's not ideal.
Exactly, we don’t need to generate clients, and we don’t want to rely on Circe auto derivation. Thanks for the workaround, we could give it a try. Tapir stands out from other options by the quality of its documentation, good job on that!
Thanks, though there's still room for improvement, it could certainly use more examples.
Leonteq | (Junior / Senior) Software Engineer | Saint Peter Port, Guernsey | ONSITE | Full Time We are looking for a good software engineer that ideally knows Scala or that is learning it. You'd be working with our Treasury and OpsControl teams. Maintaining and building tools for them (reconciliation engine/projection tool/reporting). Our tech stack is mostly vanilla Scala microservices using Protobufs to communicate (we just got rid of protoc for ScalaPB - what a relief). Knowledge in Kafka, Kubernetes and Akka is a plus. You have to be eligible to work in Guernsey. tristan.overney&lt;at&gt;leonteq.com
Variance doesn't work well (at all?) with implicits - for that purpose all type classes are invariant. If you look at the TTFI code it result in people having to upcast things manually e.g. sealed trait Error case class SpecificError() extends Error implicit val monadError[Either[Error, ?], Error] = ??? import cats.implicits._ SpecificError().raiseError[Either[Error, ?]] // won't work (SpecificError() : Error).raiseError[Either[Error, ?]] // works In your code you have the same problem. If you had `Bar[+T &lt;: A] extends Foo[T]` and declared `Bar[B]` then `Foo[B]` will find it (because it is a simple inheritance) but implicits won't be able to figure out that it should also be used for `Bar[A]` or `Foo[A]`. From what I remember it was a conscious decision, because variance would run you into a lot of troubles in practice (easily colliding implicits etc).
Thanks for your advice. After doing some more readings, I managed to make the code compile by making Foo contravariant. Seems like it's the only solution to my use case, but I don't really like it as it makes the code more confusing and things like you said.
You are better off modeling your expressions as data that can be easily manipulated. A common way to do it is by defining a [GADT](https://en.wikipedia.org/wiki/Generalized_algebraic_data_type): ```scala sealed trait Expr[A] case class Eq[A](a: A, b: A) extends Expr[A] case class Gt[A](a: A, b: A) extends Expr[A] case class Lt[A](a: A, b: A) extends Expr[A] ``` Then you could have an integer parser: ```scala def parseExpr(input: String, a: Int, b: Int): Option[Expr[Int]] = input.toLowerCase.trim match { case "equal" =&gt; Some(Eq(a, b)) case "greaterthan" =&gt; Some(Gt(a, b)) case "lesserthan" =&gt; Some(Lt(a, a)) case _ =&gt; None } ``` Once you have created a set of expressions as data you can interpret them as you wish. What you want in this case is a function `Expr[A] =&gt; Boolean`. ```scala val pred: Expr[Int] =&gt; Boolean = { case Eq(a, b) =&gt; a == b case Gt(a, b) =&gt; a &gt; b case Lt(a, b) =&gt; a &lt; b } ``` Running example: https://scastie.scala-lang.org/R7j6P6eURyKdZiYQtG1dwA
The title is premature. It's not available at [https://www.scala-lang.org/](https://www.scala-lang.org/) yet.
You can also have the trait itself have a method like `execute:Boolean` that each operation implement.
It's on Maven central, that's good enough for me :)
The solution posted above is what you want. But to answer your question, you can use q"..." It's an other string formatter like when you do s"..." And it allows you to build code structure. https://docs.scala-lang.org/overviews/quasiquotes/intro.html but I do not recommend it for your problem
It sounds to me like you want Ammonite's [`MainRunner`](https://github.com/lihaoyi/Ammonite/blob/1.6.7/amm/src/main/scala/ammonite/Main.scala#L319-L325) and, in particular, [`runCode`](https://github.com/lihaoyi/Ammonite/blob/1.6.7/amm/src/main/scala/ammonite/Main.scala#L360).
Also, grab the [anniversary version of jsoniter-scala](https://github.com/plokhotnyuk/jsoniter-scala/releases/tag/v0.50.0) too - it supports _all_ `Iterable` based types of Scala 2.13 collections.
See you in 4 years Spark
This is a 100% valid design however it is my preference to clearly separate the description from the interpretation. I think having that part on the interpreter itself makes intention clearer. You could have many other interpreters for `Expr[A]`: a pretty printer, one that sorts stuff based on Eq, Gt or Lt, etc.
Hurray
´someFunc´ expects a \`Foo\[T\]\` which at your callsite resolves to first \`Foo\[B\]\` (which is found because you created it before) and then \`Foo\[BA.type\]\` - which is not there. You can check with with \`implicitly\[BA.type\]\` which will also fail. \`Foo\[B\]\` also can't be used because \`Foo\[B\]\` is not a subtype of \`Foo\[BA.type\]\`. if you create a \`Foo\[BA.type\]\` instead of creating a \`Foo\[B\]\` both function calls will compile as expected. Marking your traits type parameter contravariant will do the same, just in the other direction (which is why it makes your code compile). Overall, it is probably a good idea to try to stay away from variance annotations if possible and use derive instances (automatically) if needed.
Yea, after reading thoroughly about contravariance, I finally understand why it works in this case. Anyways, what you mean is that instead of using contravariance, I should use something explicit like Foo[B] extends Foo[BA.type] right? Both are semantically equivalent but explicit subtypying is easier to understand or to reason about than variance annotation, I assume.
Where's the `(1 to n).product` version?
That is the most hilarious Scala related post I've ever read. Kudos to you !
Congrats on the release ! Also having a performance guide is an amazing idea,
I love this so much
2 years Scala experience, not 2 years experience.
Only 4?
Must have increased their cadence.
Publishing the hiring conditions will help you to start with. Either to identify why you can't find one
Thanks, Volegabriel. The hiring conditions are quite flexible - I'm looking for a remote developer who is available about 10-40 hours a week. There isn't a lot of time pressure. This is a not-for-profit education based site to deliver exam questions and allow user collaboration (fee for service reinvested in content/development).
I'm surprised you can't find someone, especially remote. Any chance you can make public what's paid per hour? I am available myself (part-time) but to be honest the tech stack doesn't sound so exciting to me but I might be able to get you in touch with someone that would consider it.
Thanks for the comment. The project is not full time, and my efforts to find someone have been limited to searching upwork for freelancers, of which there are few. I have found that many claim to program in scala, but are not able to perform when given simple tasks, or program in a very java-like way. The project doesn't have a rigid specification, and I wish to develop it iteratively in discussion with developers. I had a look at your github and talks - you seem to be an expert in scala. I wonder, are you interested in any freelance work? The first task I have is straightforward - refactor a data access layer to use the neo4j java driver rather than object graph mapper for my existing backend. I can show you the backend code if you'd like.
&gt;I'm surprised you can't find someone, especially remote. Any chance you can make public what's paid per hour? I am available myself (part-time) but to be honest the tech stack doesn't sound so exciting to me however I might be able to get you in touch with someone that would consider it. Sent you a direct message.
Yeah I gave up with Upwork, it's the worst platform for Scala jobs. \&gt; I wonder, are you interested in any freelance work? I am but not available immediately. I could give it a look if you don't mind showing the code but with no commitments for now :) Anyway I could help you find someone interested.
[https://scastie.scala-lang.org/eGseuOiTQu2ULxVdhoGn7g](https://scastie.scala-lang.org/eGseuOiTQu2ULxVdhoGn7g)
Why is the price a secret?
Yeah, the Scala REPL programmer doesn't need that `Enter your number` stuff. The kudos case with him would be: he realize that adding `.par` before `.product` speeds up calculation in 32x times on his 4-core laptop when n=250000.
I would negotiate it with the freelancer based on their skills and proficiency.
What's the pay range? Is that a secret, too?
Industry standard rates are no problem.
Hey, really cool! Do you have some suggestions of when you should use rtree2d vs postgres for example?
use rtree2d when your data fit in memory and you have only _microseconds_ to run the query (from the list of supported one)
I see that the Scala team started to help with that: https://issues.apache.org/jira/browse/SPARK-25075?focusedCommentId=16841287&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16841287 So, I bet that the full migration will complete much faster this time.
Move “someObj =“ before the match expression.
Hey thanks that worked, but i have updated my requirement above.
Beginner Scala Programmer is the sanest version of all.
Hey, I had previously stayed away from this because I wasn't sure our company would be able to afford it, but will take a look and see if it's helpful for our team. Thanks!
I can only take credit for linking the post, I didn't write it ;-)
Your article accelerates climate change... Opening the page gave me continuous 100% utilization of 1 core, and 14 degrees CPU temp raise. Never experienced this with other articles on medium.com.
What's the migration story like? I would love to try the new collections in a serious project but am wary there's a long chain of library dependencies that will prevent me
Which market? Industry standard in New York is different to Manila
 Both of your distance calculations are each referring to only one of the points, which doesn't make sense. I think you need to double check your formulae.
Rates I have seen advertised for permanent positions in California are reasonable.
I think it may work if you call `digest.update(contentBytes)` and then call `digest.digest()`.
Thank you, i'll give that a try and report back!
Hi, check DM please
Hmm, seems to be failing to as with a linking error as well val contentBytes = content.getBytes() val digest = MessageDigest.getInstance("SHA-1") digest.update(contentBytes) val hashString = new String(digest.digest()) results in this [error] cannot link: @java.security.MessageDigest::update_scala.scalanative.runtime.ByteArray_unit [error] unable to link [error] (Compile / nativeLink) unable to link
Missing #2 doesn't make an implementation incorrect because #2 is optional just like #4: Advantage of having #2: Faster when equaling an instance against itself Disadvantage of having #2: One extra comparison when equaling an instance against other instances For most workloads, the latter case is far more common, and thus people intentionally omit #2 for slightly better performance.
I've seen this error before. It happens when you use a Scala version that doesn't have JDK &gt;9 support. I don't know what you're doing but maybe bloop can be of help here [https://github.com/scalacenter/bloop](https://github.com/scalacenter/bloop) It has JDK &gt;=8 support
My 2 cents: 1. `this eq that` is better in separate branch or even out of the `match` -- you don't need to cast to Person to check referencial equality; 2. hash codes comparison is an overkill -- not so many do memoization of hashCode() even for immutable structures and usually you need to access all of inner fields to get hash code. Correct field comparison in #5 will short circuit on a first `false` anyway.
As an aside, if ever you’re implementing `equals` and `hashCode`, always write tests around them with [EqualsVerifier](https://jqno.nl/equalsverifier/). This tool is revelatory in how many subtle problems in finds.
Is your code an entangled mess ?
Does it have automatic encoder/decoder support like Circe?
I would worry more about the team you’re joining and less about the language they use. If they hired you as a junior dev and know you don’t have experience with Scala, they most likely know it’ll take you some time to get trained &amp; up to speed. If the team was nice, honest, and seemed to promote a good environment to learn in, that’s a great sign. If all they were focused on were deliverables and producing immediately, it might be a very difficult/stressful transition. And just a note, while Scala provides functional features, it also provides OO features as well. There will probably be many familiar aspects alongside the new stuff.
Yes. Reader/Writer similar to play-json.
There is a 50% chance that it is!
It seems to be macro based :/
Keep in mind that Scala is a hybrid language. It's a fusion of OOP and FP. So there's probably going to be some familiar stuff. As stated, I would worry more about the team you're joining as a junior dev.
Right.
I really tried to like Go but I just hated it. GAve it more than a fair shake. Solving problems was infuriating. Those same problems were hard in Haskell and scala but they gave me so much more joy. If you are a junior, what you do now will determine what you end up doing in the future. You can still switch, but regardless.. you’ll spend a significant amount of time doing this. Do you enjoy scala? Get beyond the “initial frustration” stage that most languages have and decide if you like it. That should help you choose the right path.
&gt;Do you enjoy scala? Get beyond the “initial frustration” stage that most languages have and decide if you like it. That should help you choose the right path. That makes sense. &amp;#x200B; &gt; Those same problems were hard in Haskell and scala but they gave me so much more joy. Very curious to see examples of these problems!
The new collection redesign aside, this is a great release with many QoL improvements. (`toIntOption`? Sign me up!) Looking forward to upgrading to 2.13!
Hey OP, I recently did this transition myself. Like another user points out, it's more about the team you're on. Ask about mentorship, learning opportunities. Ask if they do pair programming, how much? Ask about code review process. Ask if others have gone onto the team without knowing Scala and succeeded! (Scala isn't hugely popular. In our Scala devs, maybe half used Java beforehand and the rest Python Ruby etc.). The TL;DR is it's never too late to learn a new language. You got this
Enjoy types. 😀
I'm enjoying types in Go already 😊
Not like this. It gets way better.
can't wait!
Extra parenthesis here &lt;...&amp;&amp; (hashCode == person.hashCode) &amp;&amp; ( (name == person.name) &amp;&amp; (age == person.age))...\&gt; Can just be &lt;...&amp;&amp; (hashCode == person.hashCode) &amp;&amp; (name == person.name) &amp;&amp; (age == person.age)...\&gt;
Go through the Odersky’s course on Coursera. If your team is not heavy on the functional side of Scala, you’ll be good to go. If it is then I suggest going through Real World Haskell book. It has nothing to do with Scala, but it teaches FP concepts in a clear and concise way
Oh sweet summer child...
go/python feel close to Scala syntax, so syntax wise transition will actually be easy (although java knowledge would help as well, so brush up both on Scala and java). Kotlin is another language that is practically a clone of Scala. The major gotcha with Scala vs say Go (or even plain java) is performance - tbh, I find Scala slow and memory hungry and thus it requires more careful development style, esp for anything touching production. The more interesting question is the team and the particular codebases you’d be dealing with. It’s possible that you’d see little of Scala and will be doing more of JavaScript development since that’s usually what happens with developers in junior roles, so I’d suggest to make sure you understand specific role you are considering.
Pattern matching...
This is for Java. Is there a Scala version?
The extra parenthesis are for enhancing readability by creating conceptual separation. There is no performance cost to add them.
The reason `eq` is in the `match` is `eq` is only available on `AnyRef`, not `Any`. So, the 'match' to `Person` is killing two birds with one stone as the cast to `Person` implies the cast to its ancestor, `AnyRef'.
The `hashCode` check must literally be left to the discretion of the implementer who has the relevant context to make the tactical trade-off evaluations to inform the choice.
&gt; Very curious to see examples of these problems! More than likely it's related to the fact that Golang doesn't have generics. I really do not think Generics are an optional feature to a language. Like I used [OCaml in college in my Compilers class](http://www.cs.umd.edu/class/spring2016/cmsc430/) (that's where my FP background comes from), and even OCaml's simple type system (which uses the same on as Haskell) had something like Generics when using Algebraic Data Types called [Variants](https://v1.realworldocaml.org/v1/en/html/variants.html). That being said, Generics are in one sense, a performance issue. Generics do not come from free. There is a performance cost associated with boxing and unboxing. https://hackernoon.com/why-go-doesnt-have-generics-b40ef9e69833
I work as a data engineer and in my 14 years almost never had to do math, the rule is usually a data scientist can do that. Could be a selling point for you. As for the language, each industry / country has different preferences. Learn the concepts and make sure you are good in 2 languages. Python has become a must, and Java is still extremely popular (been using it for the past 6 years). Scala isn't bad but harder to learn but from what I've seen not as popular in certain countries. cpp is awesome but unless you want to program micro controllers or for financial institutes were every second counts, don't bother. So look at job positions in the area you want to work and pick up the 2 most asked languages I would say
If they hired you knowing you have no previous Scala experience, it’s because they’re looking for someone who’s willing to learn rather that people that know a specific language. Try to understand what style of Scala they’re using and focus on that, be it a more OO style with Akka and Spark, for example, or a Functional approach with cat, for example. Whichever style just learn it until you’re comfortable, you’ll have time to experiment with other styles later. Ask your team to point you towards resources they used to learn themselves. Also there’s generally people who are more oriented to helping others learn, identify that person, ask them questions, they’ll be more than happy to help someone learn.
Scala's right, your by-name version is tail-recursive. This is not calls to factotialTailRec that blow the stack but the evaluation of the big thunk "f" at the end. It happens often when programming in CPS. The solution is often to defunctionalize the problematic argument and write a tail recrusive evaluation function (and dealing with the mutually recursive functions when it happens).
This made me laugh way more than it should have.
I have recently worked on a project with the same tech stack - akka-http + Sangria + Neo4j. If interested then you can DM me more details.
Similarly to you I switched to a new Scala dev team while still being a junior dev. (I had a C# background though) I'm surprised others are mentioning how Scala supports both FP and OOP. While that is certainly true, as far as I'm aware not many in the industry use Scala as an OOP langauage. I would suggest you clear any doubt during the interview process. Anyway there are many books out there. I'd recommend in the following order 1. "Programming in Scala" (by Martin Odersky, it's a thick book but good for a starter if you have zero knowledge of the language ) 2. "Scala for the impatient" (good reference book, nice read if you know a bit of Java ) 3. "Akka in action" (many will say akka libraries are not functional in the purest form, but they are widely known in the industry, I think it's worth knowing them especially if the company you are planning to join uses Akka. Also it is a good read for distributed computing and streaming and the reference exercises are quite good) 4. "Functional programming in Scala" (aka the red book, many recommend it. I found chapter 1-6 to be fairly easy, I got stuck on chapter 7, but as a junior dev I think that's acceptable) Lastly learn a bit of SBT. It will take some time but it's going to be worth it. Hope that helped
I honestly don’t think you’re a snob, it sounds more like you lack perspective. It sounds like you’re fairly junior, so most companies are not going to expect you to come in as a guru in any particular language. Hell, even at the senior level I don’t look for language-specific expertise. My advice would be to figure out what you want to do and let the technology be the tools in your toolbox. If you want to write ETL code, scala is a good tool but you are going to have a hard time getting away from at least some python. In fact I’d say there are a fair number of cases where python does a better job there (try parsing arbitrary json payloads in any statically-typed language... you can do it but it’s significantly easier in a dynamically-typed language like python). And you’d be surprised where php can pop up. I am a data engineer at a company that you’ve heard of and we have at least some php backend. I hate php, but my team really appreciates when I can step in and help. We also have a lot of Java. If doing math is important to you, you might be happier focusing on data science. I have a PhD in what basically amounts to applied math. Several of my colleagues from grad school are data scientists. I miss doing math for sure but I realized a long time ago that I’m an engineer at heart and I like building things to last. We have a few data scientists at work that can toe the line to engineering, but they’re ultimately focused on the science.
&gt; First of all, is it the right step for a junior software engineer to switch companies AND langages/paradigms at this early stage? (less than one year of professional experience, mainly in Golang and Python) Some people see less than a year as a bad sign, but I'm not sure how current that kind of thinking is. If you've got a good degree I wouldn't worry too much. If your CV has anything else that might be seen as a red flags then I'd be more cautious. &gt; Second, do you guys have any tips/advice to make this transition much easier and less stressful? Keep an open mind. A lot of things will be different and sometimes you won't immediately understand why, but you need to be willing to trust your new team and go along with how they do things. Eventually you'll want to suggest changes where you old team did something better than your new team, but you need to be in a position where you understand what your new team is doing and why, and have shown them that you can produce some value, before you start asking them to change how they do things.
&gt;If you've got a good degree I wouldn't worry too much Yes I've got a good degree. &amp;#x200B; &gt; you need to be willing to trust your new team and go along with how they do things. Thanks, good to know that.
Helped a lot, thank you!
&gt; I know that Scala is considered more dynamic than Java and C++, but it definitely is a language that requires a good amount of discipline to excel in. I see it the other way around actually. In Scala I can be sloppy (or rather, I can focus on the big picture) because the typechecker has my back. When I worked in Python I had to be 100% on top of everything because my screwups could easily make it all the way to production. &gt; I have beyond beginner python knowledge and would be able to run pandas/numpy scripts within a week or two. Would I be better off investing my time in a language like Scala, C++? I don't mean to bog down the thread with noob questions but I just haven't found that one language that I can sink my teeth into. I have to say use your own judgement here. I'm here because I believe Scala is the way forward (and I can go into specifics about what I think it does better than other languages), but objectively speaking there's plenty of grounds for skepticism about the language as well. One thing I wouldn't do is avoid a language because it seems "too easy". Having a talent for something feels a lot like that thing being easy. In particular, if you find a given language easier than other people do, then that language could very well be worth pursuing. &gt; My aspiration is to either find a data engineering position, where I can keep my hands on infrastructure and development, and where I can do a lot of math, I mean A LOT, but NOT be a data scientist. Or get my Master's in Stats and build Pipelines to do linear regression, Gaussian regression, etc. Obviously, I will always sharpen my blade with python but whenever I go back to it, most of the development seems super lib specific, so it's more of being a numpy/pandas engineer, not a python engineer. Am I being dumb? Honestly it does sound like you need a bit of a reality-check about what business, or even research work, involves. People pay you to solve their problems as cheaply as possible, and most of the time that's going to mean using an existing library or, at most, implementing an existing technique. In fact in a trendy, fast-moving field like data engineering you spend 95% of your time dealing with data format issues, cluster-administration flakiness, and the like (whereas in a more mature field like banging out CRUD webapps you can at least rely on the infrastructure and spend most of your time on the actual business problem, even if those business problems aren't always the most interesting). In terms of the work that needs to be done, there just isn't that much call for "a lot of math" versus cleaning up the data, plumbing together the libraries, and fixing all the little problems. It's interesting and fulfilling in its own way, but if you're looking for a job where you can spend most of your time doing math, data engineer is not that job. Why do you say you don't want to be a data scientist? That role is more mathematics-oriented, though even that is more often a case of reading a paper, implementing it, and applying it to your own dataset, rather than doing novel mathematical work. If you want to actually be doing mathematics all day then realistically the only place that's going to happen is a mathematical research position, or maybe the research arms of a handful of very big companies.
What is wrong with macroses anyway? Sometimes they give you a huge advantage avoiding writing the same code constantly yourself or using reflection. Complexity to write and support them? Agree, but they definitely will be improved with Dotty soon. Compilation time: maybe, but after years of experience of waiting for the compilation result of C++ templates I can't say it is a big deal with Scala.
Use a trampoline instead. You've messed up the tail recursion by building an f with a billion stack frames. Things don't have to be recursive to blow the stack. A really deep call stack, even a non-recursive one, will still eat your stack space. In this example, you have a tail recursive function. Just don't make it lazy and you won't have a giant thunk. If you need lazy eval, or can't make something tailrec easily, or whatever, scala comes with TailCall in the std lib.
I am working on identifying voice in a audio music file. I am lost in how to do so. I just need to be pushed in the right direction. I am also trying to learn the common Algorithms for Interviews as I am going into my Junior year. I am also learning scala for this TA position.
I followed the exact same route, though I was more senior (and had some experience in _the other_ functional side, with Lisp). If your team is supportive, it shouldn't matter. It will also depend on how deep into functional they are, the first days could be crazy with jargon. But everything will eventually click.
Since it's not very obvious from the title, chisel is a framework for hardware design using scala as a host language. It is also the reason I originally chose to learn scala.
&gt;as far as I'm aware not many in the industry use Scala as an OOP langauage. If you're trying to say that most industrial uses of Scala are focused on pure FP, that's not my experience. 10+ years, only one gig was focused on FP.
Regarding #2, my reference and justification is Joshua Block's "Effective Java, 2nd Edition" advice. To your point, I have now added that it is "technically optional, but highly recommended". As soon as you used the phrase "most workloads", it implies that it must be specifically and literally left to the discretion of the implementer who has the relevant context to make the tactical trade-off evaluations to inform the choice.
I just graduated CS, and most of my personal projects were in Go, although in school we mostly focused on Java. I can say that knowing java, getting started in scala was a breeze (and pretty joyful). Have to be familiar with the concept of lambda and higher order functions but i think it's east with a little practice
The improvements are great, thanks! &amp;#x200B; BTW any ideas about this: [https://github.com/tpolecat/tut/issues/252](https://github.com/tpolecat/tut/issues/252) ? There's a binary issue with both \`sbt-1.3.0-RC1\` and \`RC2\`.
I’d either grab a lock in your code, or see if the database has atomic compare and swap. If you have a List of some Monad, you should be able to call sequence on a list of them to run them one after another.
What's on Maven seems to differ from the tar.gz I usually download so I'm confused. Why can't they just put it up on the Scala site? Am I missing something?
Hey, I wouldn't run after yet another technology. 2 years seems like a lot, but I bet you will learn more as time goes by. You will work 40 years, wouldn't it be sad if you peak after 5 years of experience. The code I write today looks nothing to the one I wrote 6 month prior. This really comes from practicing over and over again. That practice can come from internships, proper jobs, and side projects. Try coding various projects with different libraries. You can start with play, continue with akka, move on to htt4s, and finish with fs2. Your projects don't need to be smart, but complete. Think about databases, why pick one over the other, how to create the schema, how to run tests, ... Think about the logging, how to surface errors, how to be paged in the middle of the night,... And think about deployment with continuous delivery, environment variables, monitoring,... Add all those projects on your CV. Write an article or two about your chooses, and what you learned. All off this will greatly increase your chances of getting called back, and I bet you will learn loads too. Good luck on your journey
Have you considered becoming a quant? Very math-y and pays well: the amount of coding and the language involved may well be negotiable, as often you would be prototyping an algorithm for subsequent production implementation by a F-T developer.
Sharding your operations (in this case by id) makes this easy. I made a video tutorial for this case using Fs2: [https://www.youtube.com/watch?v=FWYXqYQWAc0&amp;t=9s](https://www.youtube.com/watch?v=FWYXqYQWAc0&amp;t=9s)
I'm not dead set on math, I would just want to be in a development position with skills that are essentially "irreplacable" if that makes sense. Having in depth math knowledge is definitely unique, and I would just want to keep it sharp. I actually don't mind error plumbing and fixing applications. I actually have a hefty background in Linux so I will definitely keep all of this in mind. Thanks.
Have you tried allure?
What city are you based in? I'm a lead engineer at a company that only uses Scala on the backend and we're always happy to interview engineers that have experience with FP and Scala. Of course, experience with Spark, Hadoop, etc would be a huge plus but it's never a requirement for us.
Based out of Toronto atm - but always open for a new adventure :)
We have office spaces in Montreal, Boston, and NYC. DM me with contact info if you want to talk more.
the best way to overcome this is stop saying Scala every three words
hmmm - not sure why I said Scala so often - guess I was just trying to be specific.... &gt;bastardized java-hellscape what's your go-to FP language? Kotlin? Go? Erlang? Rust?...
My point, is that when going about finding career positions, your language of choice _should_ have _nearly_ no bearing on your, and your employers, decision It's about the ideas and the patterns you've learned. For example, have you ever taken a look at Erlang code? [Here's a random code snippet](https://github.com/esl/erlang-web/blob/2e5c2c9725465fc5b522250c305a9d553b3b8243/bin/e_component.erl#L97). Case statements, pattern matching , no obvious types -- feel like reading Scala? Now I know I made a jump in my original comment, implying your resume/CV mentions Scala and the associated libraries heavily, but your presented description may indicate that's the case. I apologize if you've already navigated the issue. If you haven't, realize that these libraries are just implementations of patterns. Know Akka? You likely understand basic TCP/IP interactions. Know how to keep your Scala code from shooting itself every 20 minutes? You likely have a base understanding of fault tolerance and safe programming practices. Know Scalaz? You're adept at leveraging and managing external dependencies to bolster your libraries (dependency management is no joke, and being in the Java universe, it may be hard to appreciate how well, or not well, it is implemented). It's surprising to me you're having difficulty procuring positions, so it leads me to believe it's a problem with both marketing and interview skill. If the interview backs out cause you don't know something, it's more likely they are afraid you _can't_ learn it, due to your inclination to "shoehorn" yourself into a niche. Just for fun, go and learn a new language. Maybe Erlang. Maybe Javascript. Maybe Golang. Each have different and interesting dependency solutions, ideas on code philosophies and formatting, and are fun to use. Just showing an employer you can go outside of "enterprise Scala solutions TM" will make them understand it's rather a problem of depth, not incompetence
You make a lot of valid, good and interesting points. &gt;It's surprising to me you're having difficulty procuring positions, so it leads me to believe it's a problem with both marketing and interview skill. If the interview backs out cause you don't know something, it's more likely they are afraid you *can't* learn it, due to your inclination to "shoehorn" yourself into a niche. I don't think I outlined my past experience well, but I'm almost certain when someone reads my resume/cv it doesn't convey the idea that I'm solely a Scala developer. I've had positions in Networking, Java, QA Automation &amp; Frontend (React/Redux). When I note that I'm having trouble getting past a couple preliminary interviews with companies, I should clarify that I was commenting specifically on the Scala roles. Usually they like the fact I have some Scala experience and can *think* like a Scala developer - not like a Java dev writing Scala code. However, they typically point out that the position is working with *Big Data* technologies (Spark, Hadoop, etc) and they really are looking for someone with proven work experience in those areas... That being said, there is always room for improvement when it comes to interview skills. I find that I typically trip myself up a lot by second guessing and nerves when it comes down to a technical interview &amp; a code challenge. I'm actively working on that at the moment by doing practice problems via Leetcode &amp; HackerRank. &gt;Just for fun, go and learn a new language. Maybe Erlang. Maybe Javascript. Maybe Golang. Each have different and interesting dependency solutions, ideas on code philosophies and formatting, and are fun to use. You raise a good point here. I am actively trying to learn Python3 on the side via books &amp; practice problems...I do want to dive into different languages to see the different philosophies, formatting and ways of working. Believe me when I say it was pretty interesting when I moved from a Java position &amp; solely imperative style programming to a Scala/FP one... My big worry here is that I'm becoming a *jack of all languages, master of none*. When I bounce around between so many different languages and ways of working, I can easily get tripped up and fall on my face when presented with a coding challenge in an interview setting. However the underlying idea you present (learn the principles and core concepts - languages are just implementations of them) is very true and I do think I need to spend more time on that.
By the sounds of it, there just needs to be a nice combination of technologies used and proven interest. If many Big Data positions are what you'd like to get into, you'll absolutely need to show minimal competence using some of the tools -- or at least be able to navigate an interview where you can explain how you don't necessarily know how to use it, but you know what it is for. Some hiring managers will put more emphasis on coding interviews, some won't. Personally, I don't believe you'd always want the person that can ace every one of those questions. - &gt; My big worry here is that I'm becoming a jack of all languages, master of none. When I bounce around between so many different languages and ways of working, I can easily get tripped up and fall on my face when presented with a coding challenge in an interview setting. On this point, having a few languages under your belt will only help you more. If you know what a loop is, what recursion is, some basic understanding of types and functions, I think you _should_ be able to use and at least half understand any language put in front of you. Specializing in a language can be a career choice, but one I'm not sure you should make this early. You can master any language in a couple of years -- but if you don't understand any _other_ languages, how can you compare them or understand why that language is even good?
True, True and True. I do think I could definitely use more exposure to other programming languages - I'm actively trying to work on that. &gt;if you don't understand any *other* languages, how can you compare them or understand why that language is even good? That's a really good point. I'll be completely honest - when I go to pickup a new language or technology and I'm trying to decide between a couple different ones, I end up just looking at what the talking heads on the internet are saying and trust their opinions... usually I'll supplement that with statistics on usage in the industry, overall popularity and ease of use... might not be the greatest strategy but it hasn't burned me *yet*
lolol, I figured that'd be the way to help you sleep -- cause it's what so many in the industry do. To be real, you can solve most problems with most languages. Yeah some are worse than others at some things, but if you never find what you like, you'll never know! happy hunting!
https://safer.chat is an end-to-end encrypted chat that allows to create rooms with up to 4 participants, the idea is to have what cryptocat used to be, without the need to install browser extensions or applications, in 2019 we have the Web Crypto API which allow us to do all operations. We do not store any tracking information or history, the server logs doesn't include any keys, nor encrypted messages, in fact, all the server's data lives in-memory. In order to start a conversation, you need to choose a room and a password, then, only people knowing these details are able to log into that room, this password isn't transferred in plain text.https://safer.chat is an end-to-end encrypted chat that allows to create rooms with up to 4 participants, the idea is to have what cryptocat used to be, without the need to install browser extensions or applications, in 2019 we have the Web Crypto API which allow us to do all operations. We do not store any tracking information or history, the server logs doesn't include any keys, nor encrypted messages, in fact, all the server's data lives in-memory. This is a side project built by me and a friend just because we wanted to have an simple end-to-end encrypted chat available. The server side is built with Scala and Play Framework, we hope you enjoy it.
You can make a shared map that holds semaphores (e.g. from Cats) for each of your object id, or something like that. You'd want to clean it up eventually too. You can make better solutions with streams, this is just a low level example. I use something vaguely similar to keep track of websocket connections on the server. Pseudocode: class SemaphoreGalore { private var semaphoresByObjectId: Map[String, Semaphore[IO]] = Map.empty private val sharedSemaphore: Semaphore[IO] = ??? def getOrCreateSemaphore(objectId: String): IO[Semaphore] = { for { _ &lt;- sharedSemaphore.acquire semaphore &lt;- ??? // get or create semaphore for a given objectId in semaphoresByObjectId _ &lt;- sharedSemaphore.release } yield semaphore } } // And then for { objectSemaphore &lt;- semaphoreGalore.getOrCreateSemaphore(someObjectId) _ &lt;- objectSemaphore.acquire result &lt;- callDatabaseOrWhatever _ &lt;- objectSemaphore.release } yield result
This comment is so unrelated yet so fitting...
How is that a "3 minute read". Am I toooooooo slow ?
Is it open source? Are you looking for contributors?
More information on the role here: &amp;#x200B; I have a new position with a company called Kassu, who are a brand new consumer credit app that will offer P2P lending between users to free up the whole world of credit opportunities. &amp;#x200B; They're looking for a founder level CTO with a strong history of team management and startup growth to help build their platform from scratch. This position will hold significant equity and once the next investment round has been completed, will have a strong six figure salary alongside the equity. This is a unique opportunity to join the ground-floor of a challenger bank, and change finance for the better. &amp;#x200B; They want the platform build using FP, and are open to a choice of language, however Scala is currently their favourite option. The spec atm is rather sparse as they can't give too much away however if you apply through the website you'll be able to speak with the founder, and find out exactly how they plan to implement everything.
Migration of collections seems to be documented here: [https://docs.scala-lang.org/overviews/core/collections-migration-213.html](https://docs.scala-lang.org/overviews/core/collections-migration-213.html)
The documentation is good enough for you to get a general idea: [https://typelevel.org/cats-effect/tutorial/tutorial.html#introduction](https://typelevel.org/cats-effect/tutorial/tutorial.html#introduction) I was thinking earlier today in how cool would it be to have a Cats/Scalaz MOOC (like those in Coursera/EdX) with an emphasis in "Real Life Programming".
&gt;The documentation is good enough for you to get a general idea: [https://typelevel.org/cats-effect/tutorial/tutorial.html#introduction](https://typelevel.org/cats-effect/tutorial/tutorial.html#introduction) Yes, I followed the official cats effects tutorial, but I didn't find it so easy to follow as "Scala with Cats". The book really &gt;I was thinking earlier today in how cool would it be to have a Cats/Scalaz MOOC (like those in Coursera/EdX) with an emphasis in "Real Life Programming" I would definitely take this course!
I’ve been working on this blog post about Monoids http://justinhj.github.io/2019/06/10/monoids-for-production.html
I blame all that heavy type checking
No I haven't, I will check it out
OMG Java enum support! I definitely like the idea of type-based imports and have wanted polymorphic function types in the past so I'm happy to see that. I think I like the minor syntax changes too - feels a little more consistent, particularly with the wildcards. Not sure how I feel about \`delegate\` as the name for a typeclass instance, though it is probably better than \`implied\`, and I don't have an immediate alternative to suggest. I'm not sure I understand the typeclass derivation piece and am having trouble figuring it out from the github threads/diffs. I feel like it could use a description of the schemes and an example of previous vs new codegen. I'm also a little confused by the wording - usually fewer allocations translates to faster compiles, but this introduces a regression? I think that must mean the generated code causes fewer allocations?
https://www.reddit.com/r/scala/comments/bxuear/scala_2130_has_been_released/
I have a question. APIs in the library like `os.read` does not return an IO. My basic FP knowledge would tell me that anything which does a side effect like reading files should be wrapped in an IO. Why did you choose not to do this? it seems os.read returns a string and not IO[String].
`@alpha` and new enums are going to be great for Java interop! Does someone know if there is some work on supporting the Java 9+ module system?
I see, thanks for your input, you surely have more experience than me. Personally I haven't come across any job ad where they specifically look for "Scala as a better Java" type of dev nor I have met developers who use Scala that way. It'd be nice if others could share their experience
Does anyone know how to test dotty with scala 2.13 library? the example dotty project uses scala 2.12
Can you recall which concepts were hard for you (or which still are) ? We really appreciate any feedback that could help us improve the documentation. I even have an issue just for that https://github.com/typelevel/cats-effect/issues/319 but I guess people are reluctant to criticize. :D Obviously improvements won't come right away but sometimes it's hard to see confusing parts because we are so familiar with them by now. Gitter is a great place to ask questions: https://gitter.im/typelevel/cats-effect
[https://www.manning.com/books/functional-and-reactive-domain-modeling](https://www.manning.com/books/functional-and-reactive-domain-modeling) will teach you to do DDD in this style of code.
Thanks for the references and Gitter! &gt;Can you recall which concepts were hard for you (or which still are) ? We really appreciate any feedback that could help us improve the documentation. My general feeling is that the data types were not properly introduced (as well as in Scala with Cats), but maybe I ought to read the documentation on the data types first... Anyway, if I have more concrete examples, I'll contribute to the issue to which you pointed me!
Not specifically on Cats Effects, but looks really interesting! Thanks for the tip!
you can also practice on [https://www.scala-exercises.org/](https://www.scala-exercises.org/)
&gt; Does someone know if there is some work on supporting the Java 9+ module system? See https://github.com/scala/scala-dev/issues/529
It's not possible currently, we can't really support both at the same time so we'll wait a bit before switching to supporting 2.13.
downvoted for good content -- I guess redditors are cunts regardless of how niche the subreddit is
I agree, this does not seem possible.
Anyone know when it'll be supported by Spire?
&gt;but I've come to believe that the truth is that unless we are all taking a collective stand to not give a platform to those who would divide us for any reason Yet you attack john for calling out this behavior in TypeLevel?
Getting better all the time. Is there any time line for Java 11 support?
Depends on your usecase but For simple stuff or early stage projects you may add a last_update timestamp to your db. In your update query add a condition on this ts so you won't touch data if it's different. Look at the query result, if no record matched the query then assume the record was updated and you have to try again. If course if some IDs are heavily concurrently modified you will have to improve your code and there akka + sharding can help you scale.
Since Scala gives you so much freedom to use whatever style you want, I got curious and decided to do a performance profile on various algorithms designs for generating prime numbers. The imperative and lazy approaches implement a true Sieve of Eratosthenes, while the recursive approaches use a sort of pseudo-sieve where they still iterate through the whole collection. I’m actually kind’a surprised the list tail recursion is so inefficient, and that there seems to be so little difference between the two types of recursion with vectors. Also, the first set of data I generated actually contained an error; I forgot to write the recursive call for the vector normal recursion, and it was consistently extremely close to the imperative method. That means the imperative approach is faster than just making a vector and filtering it once. Here’s the code for my two list-based methods as example of my approach: def pLstRec(max: Int): List[Int] = { def pHRec(lst: List[Int]): List[Int] = lst match{ case p::ps =&gt; p +: pHRec(ps.filter(_%p != 0)) case _ =&gt; List[Int]() } 2 +: pHRec(List.range(3, max, 2)) } def pLstTrec(max: Int): List[Int] = { @tailrec def pHTrec(primes: List[Int], src: List[Int]): List[Int] = src match{ case p::ps =&gt; pHTrec(primes :+ p, ps.filter(_%p != 0)) case _ =&gt; primes } pHTrec(List[Int](2), List.range(3, max, 2)) }
The bloop version they updated to claims to support Scala 2.13 and Java 11
Use [the best algorithms](https://pdfs.semanticscholar.org/ba51/12ec470de71a6f1574c41d483ac9393ad0d3.pdf) and [a good profiler](https://github.com/jvm-profiling-tools/async-profiler) if you need efficiency instead of speculating on "styles".
I'd consider non callback based concurrency. Akka has some pretty cool demos.
You can use versions if your database supports updates against searches: case class DoggyDocument(id :String, version :Int, isGoodBoy :Boolean) // your doggy document has a version field so may return this: Dog("Rover", version=1, goodBoy=true) val myDog = getDog(id="Rover") // apply your change and increment version too! val myBadDog = myDog.copy(goodBoy = false, version = myDog.version + 1) // this changes the database state but ONLY if NO-ONE else has updated the system! If someone else had, version would already be 2 and this would do nothing val updateCount = database.update({ id="Rover", version=1 }, myBadDog); if (updateCount == 0) { // the update failed val changes = diff(getDog(id="Rover"), myBadDog) println(s"Your change failed because someone made this change to rover already: $changes") } else { println("dog is going to hell OK") }
Watch this issue https://github.com/typelevel/spire/issues/805
the .lengthIs functions are interesting. I guess if you say .lengthIs &lt; 6 a linked list can count 6 nodes in and then just return false without having to count the whole data structure before comparing two numbers.
It's possible e.g. that tar.gz might not be ready yet (or some other platform); they won't want to make a release announcement until everything's ready. But a lot of us might only need the compiler and library jars to be available via maven central (I know I tend to use Scala via maven, and never manually install a "system" version of Scala).
I wouldn't push nesting monads to an audience that's new to them. I wouldn't even talk about the concept yet - maybe just give a few different examples of how all sorts of different libraries can use the `for` syntax, and let the people who want to do the generalization realise it for themselves. If you show `for`/`yield` syntax for `Future`s that's an understandable, useful feature using concepts that people are familiar with. And then you can say "and custom libraries can use this syntax too" and give an example of something more specialised but where the `for`/`yield` still makes sense - I quite like treelog for this. If you can find something that people would be tempted to use AOP pointcuts for, I find that's the real killer feature, because you can do something that's as expressive/concise as AOP, but automated refactoring won't mess it up because it's "plain old code" (e.g. using a ~~monad~~ custom type that supports `flatMap` to mark "must happen in database transaction" is a personal favourite). The other killer feature I find is typeclass derivation via shapeless (as done by e.g. circe). Don't get stuck in the internals, just show that you can derive a JSON format for your `sealed trait` / `case class` datastructures that you were showing off with one line - but, then if you put a field that can't be serialized into one of them, like a `Socket`, then you get a *compile time* error.
I'm probably doing something wrong, but the sublime instructions do not work for me. Also: I thought this release made it simpler?! Hopefully the simplification just hasn't made it's way to the docs yet...
thanks!
Do not use the word monad. :|
Please, show them [ScalaJS Ray Tracer](https://github.com/lihaoyi/workbench-example-app/tree/ray-tracer) on [ScalaFiddle](https://scalafiddle.io/sf/4beVrVc/1) too.
Thanks for the resources! I'll definitely give that paper a read. The purpose of this particular project was mostly just curiosity. Prime generation isn’t the goal, it’s an example to try and compare the performance of different styles. What initially motivated me was a [blog post](http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html) that compared the relative performance of all of Scala’s collection types, and found that Vectors were generally an order of magnitude slower than Lists. I find it interesting that the List-based tail-recursive implementation is so slow, since I thought both Lists and tail-recursion were more efficient approaches for this sort of algorithm.
The only thing I found helpful in this companion were the hints, and the answers to the solutions. The entire wiki is tangential info and really doesn't help contribute understanding the core material imo.
&gt;found that Vectors were generally an order of magnitude slower than Lists. Do you mean *quicker*, right?
Scalaz is a library full “monadic” code. Dotty is the name of the “Scala 3” compiler that is experimental. Scala 3 is in development.
I used ensime-vim for 6 months daily and it was very good. I could live without IntelliJ but went back when I changed jobs and inherited a newer code base. I plan on trying Metals and potentially switching back.
No, from the blog post: &gt;Constructing a List item-by-item is 5-15x (!) faster than constructing a Vector item-by-item. &gt;De-constructing a List item-by-item using .tail is 50-60x (!) faster than de-constructing a Vector item-by-item using .tail &gt;If you find yourself creating and de-constructing a collection bit by bit, and iterating over it once in a while, using a List is best. However, if you want to look up things by index, using a Vector is necessary. Using a Vector and using :+ to add items or .tail to remove items won't kill you, but it's an order of magnitude slower than the equivalent operations on a List. That's why I did this, I wanted to test the performance difference myself.
There are operations probably quicker for Lists, but overall Vector is way more performant. [https://docs.scala-lang.org/overviews/collections/performance-characteristics.html](https://docs.scala-lang.org/overviews/collections/performance-characteristics.html) [https://stackoverflow.com/a/6934116](https://stackoverflow.com/a/6934116) [https://danielasfregola.com/2015/06/15/which-immutable-scala-collection/](https://danielasfregola.com/2015/06/15/which-immutable-scala-collection/) [https://www.47deg.com/blog/adventures-with-scala-collections/](https://www.47deg.com/blog/adventures-with-scala-collections/)
Hence my curiosity. Side note, from the StackOverflow link, I didn't know you could pattern match with Vectors like that, I was just carrying over Haskell-like syntax and thought Vectors just didn't support it. I'm really glad I know that now.
People actually choose programming language based on poor articles like this?
Is it OK for citizens of the US who live abroad, or do you need residency within the US?
We need this person to work in the U.S. for time zone purposes :)
I’m seriously sick of people referring to Scala as « better Java ». If you want to write good Scala code, you have to treat it as a language of its own, with its own ecosystem. In my experience, companies and developers that try to do Java-like-Scala projects end up with a lot of technical debt and frustration, while missing on the perks of a functional programming style. With implicits, type classes and libraries like Cats, Scala enables a very different style of coding compared to Java. It enables developers to express programs in ways purely OO languages can’t and allow them to reach a higher level of abstraction thanks to category theory. Comparing Scala and Java, or comparing Scala to Kotlin for that matter, isn’t fair. Having said that, this kind of poor articles extend way beyond the JVM world, and generally speaking, I would not make any technical based on such clickbait content. 🤷‍♂️
So it's not a compliance requirement? If it's East coast then it's manageable, if it's West coast it's a huge PITA but for this salary you'll still find way better people in Russia or Ukraine than in SV.
Hey don’t forget my Polish friends! They’re ranked 3rd recently under Russia!
Yeah, unless it's compliance the company will be much better off hiring people in Eastern Europe. I don't know about Poland, but in Russia there are lots of people who will jump on this opportunity and will be way better than an average SV employee. If they are from St. Petersburg and have Sun/Oracle/DataArt in their resumes just hire them.
How about canada?
Just out of curiosity, how do you judge that Eastern European devs are way better than American devs? My experience so far has been the opposite but it's also such limited experience that I wouldn't draw any real conclusions.
Saved you a click: Scala.
Any chance this will be streamed live?
Well I was going to compare this to JS promises and they did nesting before ES6. This will be "but not just promises can do that!"
Akka is a paradigm shift in thinking. It's the best follow up lecture but not for intro imho.
It's really not that bad once you don't use any theoretical jargon. "It's just a container with a function that changes the inside of the container".
Hi, yes it will. The link will be posted on the event page above.
Hmm maybe you're right, I didn't use it that much. What really helped me to understand the material of the book is the exercises. They are really good and very cleverly composed. I would say that the exercises bring you 50% of understanding. But yes, you have to spend a lot of time, but it's worth it.
On average, American devs are of course better - better education, and frankly the best Eastern European devs have probably moved to US but at this salary level ($160K/year) you can hire someone average in Silicon Valley or an absolute top level developer in Russia.
This looks like a great opportunity. I'm not in the US but good luck.
Ah okay I see you mean now. Thanks for clarifying.
Anyone do this sometimes? // returns None on success or an error message def doSomething() :Option[String] Obviously this is an alternative def doSomething() :Either[String, Unit] but sometimes it's unwieldy to check the left right as opposed to normal flatMap / exists access paths. Is there any more self-descriptive middleground than relying on people to see the comment that says None = success?
Yup it does support both
I would never use a wrong-way-round `Option` like that - IMO it's just too confusing to the reader. I would use the `Either[String, Unit]` approach and pimp on any helper methods that I needed. &gt; I do always strive to return something. If I change an object's state I return it so callers can see the transparency in my code, If it's a future I always return that so people can chain and control execution. I find it's more misleading to put the same state in two places. So if I have a method that mutates an object I try to have it return `Unit`, whereas if the method creates a new state then it returns that as a value - that way it's clear which one a given method is doing. I think of it as edging towards linear types - never copy data, ensure that every value is created once and consumed once.
&gt; I’m seriously sick of people referring to Scala as « better Java ». If you want to write good Scala code, you have to treat it as a language of its own, with its own ecosystem. Strongly disagree. I found that that style was very effective (and really how could it not be; you've got everything that Java has, plus a few very helpful improvements). IME the main thing that limits that approach is the poor support, and sometimes outright hostility, of the Scala community.
I'm a fan of sttp. It provides a clean API, it's easy to make it send requests via different HTTP client backends, and it's easy to make it return different effect types (Future, Monix Task, Cats IO, ZIO, your team's custom Task, etc). &amp;#x200B; I just wish the HTTP model (the types for HttpMethod, StatusCodes, Headers, etc) would be shared across sttp, akka-http, and http4s.
This looks really helpful to understand the requests that sttp makes on your behalf and to reproduce requests while debugging. &amp;#x200B; I wonder if we'll also see the reverse: a command-line tool or scala string interpolator that takes a curl command line and produces sttp calls :)
Thanks for sharing this. I will try to find some spare time to go carefully through it.
Currently, I'm developing a new Scala Play! application, and I'd like to integrate Doobie and Circe into it. It's not so simple to find good examples on the Internet though. I heard that Play! is not the right choice in 2019, but I found that it's the most well documented and it looks pretty solid. Any suggestions or advices about that?
When I first tried http4s, I completely fell in love with the HTTP model they have. Combine that with its streaming nature, I don’t use anything else.
you're definitely right, it's just the problem is that it definitely is a little *too* difficult for me to tackle without supplemental resources. I wish functional programming was more stabilized so that I could simply google everything, but the unfortunate reality is that this isn't the case. :( &amp;#x200B; I know that there's a bunch of helpful people on the gitter, but they all have lives and can't be there for me instantly the way I'd want them to(for obvious reasons). I guess I"ll have to just suffer and persevere.
I'm also a big fan of Sttp, especially with the Cats-Effect backend which is simple and damn fast.
This is someone they want to hire and not necessarily someone they actually need. Apply anyway and be honest about your current skill level. Show that you are willing to learn. In the meantime keep learning on your own. Play with these technologies. Over time increase your online presence. Ask/answer questions on stackoverflow/reddit/gitter, contribute to open source (even if it's just documentation... in fact, especially documentation), create your own projects (again these should be visible online, which these days basically means github). &amp;#x200B; I obviously can't guarantee that anything I say here will land you a job, but this is what I would do. Cheers and happy coding.
Nah. Scala's need for early initializers, the complex rules around trait linearazation, and it's poor support for polymorphic methods tends to make it a poorer substitute for java style oop. Now typeclasses, extension methods, implicits, context bounds, sealed types, and path-dependent types make it a much better oop language than Java. It's not that you shouldn't do oop in scala. It's that it is different from Java, and you will need to learn some new concepts to use it effectively.
Thanks a lot.
I'm on GMT-3 but not on the US (one hour offset compared to US-E), does that not work?
That's not the point.
&gt; Nah. Scala's need for early initializers, the complex rules around trait linearazation, and it's poor support for polymorphic methods tends to make it a poorer substitute for java style oop. Early initializers and trait linearization are only relevant when doing things that were impossible in Java (at least pre-8). What specifically goes wrong with polymorphic methods? I don't recall ever having trouble.
Keep practising and learning. Try building a project for yourself, the kind of thing you’d like to be paid to do. You can also contribute to open source projects, as most have easy tasks you can get started on. Keep applying for junior jobs, you never know when somebody will give you a chance. My first job in programming they weren’t sure I could do it but interviewed me based on my home project. Then they had me work on a project for free; it took a couple of days and then they hired me. I don’t think this is typical hiring practise but it was a small company.
This is BS. Just say you will hire only people in the US and stop the lie. If it was for a timezone purpose anyone based either in Canada or anywhere in Latin America would be compliant.
Java tells me at compile time that what I'm trying to do is stupid and won't work how I expect: [https://repl.it/repls/EagerBonySpheres](https://repl.it/repls/EagerBonySpheres) &amp;#x200B; Scala says nothing and behaves differently: [https://scastie.scala-lang.org/LCgHcoOGSfeIpZ73vkbW9Q](https://scastie.scala-lang.org/LCgHcoOGSfeIpZ73vkbW9Q) &amp;#x200B; This happens when, in libraries, like [Franz](https://github.com/kifi/franz/blob/master/src/main/scala/com/kifi/franz/SimpleSQSClient.scala#L17-L19) authors do something with a val set in the body of a class. Sure, don't do that, but if I'm trying to fix it with an override, maybe tell me it won't work. &amp;#x200B; If I remove the `@Override` in java, it behaves consistently, though it doesn't do what I want -- and there's no real way to fix it easily via [extension](https://repl.it/repls/GrotesqueEverlastingDebuggers). In scala, the only way to get it to behave consistently is to use an [early initializer](https://scastie.scala-lang.org/GOKln0foRBmbNr7zQvSO8Q), but at least I *can* get what I want. My point is that it isn't like java. &amp;#x200B; [As for polymorphic methods](https://scastie.scala-lang.org/QJpR3DPMSA29TliHIYuzRg).
Thanks for your input. In Theory, a permanent resident of the U.S. that is currently living in Canada or Latin America, could be considered for this opportunity.
For some reason, on this version, my completions stopped working on a mixed java/scala project in the *java* documents only. I do have java-lsp installed as well for emacs, so perhaps that is the issue.
&gt; If I remove the @Override in java, it behaves consistently, though it doesn't do what I want -- and there's no real way to fix it easily via extension. &gt; In scala, the only way to get it to behave consistently is to use an early initializer, but at least I can get what I want. My point is that it isn't like java. So the thing that doesn't work in Java doesn't work in Scala either (though the syntax does something rather than erroring), and the problem that's impossible to solve in Java is possible to solve in Scala (though with a Scala-specific technique). &gt; As for polymorphic methods. Again, though, only an issue when you're doing something that's completely impossible in Java. Not seeing the case for Scala being worse than Java for this kind of programming.
Erm... how can you possibly think that you are comparing the performance of different styles when you are using _different algorithms_ that don't even have the same complexity? You're using `+:` in `pLstRec` (which has constant time complexity) and `:+` in `pLstTrec` (which is **_linear time_**).
I hadn't even considered that it has to traverse the list. That's why the List tail recursion is so much slower! Now I have to try prepending then doing a final reverse, I wonder how much it'll come down... I am using different algorithms between the recursive methods and the true sieves, but I'm not really trying to compare across algorithms, I'm just trying to compare the collections with recursion and lazy vs. imperative with the sieve.
I'm not saying it's "worse." I'm saying you have to learn scala specific things that matter. As a java user, I would definitely expect curried methods to work polymorphically. Curried methods behave differently than non-curried methods, and that's weird.
 * Immutable-by-default. This is huge and every time I have to use Java/TS/JS I wish they have this. * Some small snippets to show how succinct Scala can be to do everyday stuff * Various code snippets like validation accumulation (sequence), for-comprehension for Option, Either, Future is a great way to show off the usefulness of for-comprehension. * Typeclasses * If your audience has the appetite for it, show them some of the cool stuff enabled by cats-effect/ZIO Just my 2c
The very first thing I latched onto when I started with Scala was operator overloading. I made heavy use of BigDecimals, so Java was a nightmare. Following from that thought, maybe you could talk about generics? That's one of Scala's strong suits, especially with stuff like Spire.
I've just finished modifying my code and benching it (benchmarking takes a long time...), and [here's both the code and the new results](https://github.com/Dash-Lambda/Speed-Testing-Code). The current version of the code is in general_testing. Vector tail recursion is actually faster than all the other types of recursion now, despite the the extra reversal.
Please clarify the point then.
Validation accumulation is a very valid everyday use I'll definitely add this.
Op overload is cute idea 👍🏼
I used to ask some questions here on Reddit, and the community was super helpful. So to me it's the first place to ask for help.
Find an open source project you like and contribute by implementing some of their road map items... You'll be solving real life problems by doing so and adding a great experience to your resume
I used to think have seen very poorly written articles before I saw this. Now I have come to realise the level of poverty in the world.
I will not refer even Kotlin as a "better Java". And that language can be almost summed up as "Java with a better syntax". Every language deserves respect for the uniqueness it brings. Scala is so much different from Java, that any comparison between them feels just pointless and misdirected.
try running your measurements in graalvm. its jitc is probably java's future.
&gt; I'm not saying it's "worse." You called it "a poorer substitute". &gt; I'm saying you have to learn scala specific things that matter. But only when you want to do Scala specific things! As far as I could tell everything that works in Java, works in Scala - none of your examples contradicts that. And I have personal experience of moving from Java to Scala with zero Scala knowledge; everything continued to work (I'm aware that checked exceptions wouldn't, but my then-org avoided them in Java too). Maybe I was lucky, shrug. &gt; As a java user, I would definitely expect curried methods to work polymorphically. Curried methods behave differently than non-curried methods, and that's weird. As a Java user that's not how I saw it; it's exactly the same behaviour you get when implementing a curried method in Java (by returning `Function` - of course Java doesn't have syntax sugar for doing that). In any case it's grossly misleading to round that off to "poor support for polymorphic methods".
Yes! I really don't appreciate being talked down to, or having conversations about some specific topic derailed by people saying "um, actually you don't get even get how inelegant that is" wanting to rehash an fp vs oop debate.
When I asked the same question, they told me to read this: [https://www.manning.com/books/functional-programming-in-scala](https://www.manning.com/books/functional-programming-in-scala)
I'm reading this now, but this is pure FP style
Both the definition of OO and FP are quite a bit off. The best practice would be not to mix paradigms and keep to the best practices of the chosen paradigm whichever it ends up being.
thanks, Martin Odersky says we can fuse them together as OO and side-effects are two orthogonal issues listen to this (you can jump to min 5:00) [https://www.youtube.com/watch?v=K\_g\_xUtydpg](https://www.youtube.com/watch?v=K_g_xUtydpg)
You certainly can, it doesn’t mean you should or that it is anything other than bad practice.
thanks, it's one thing if some Jo says it but this is Martin Odersky saying this, this blog touches on this style [https://underscore.io/blog/posts/2017/06/02/uniting-church-and-state.html](https://underscore.io/blog/posts/2017/06/02/uniting-church-and-state.html) Not much else is out there about his question.
agree about the consistency point
You can make (almost) all your functions pure by using some IO monad - which is more "FP style" - but still use classes as "modules" to group together functions with common dependencies (basically treating the constructor as a tuple -&gt; named tuple of functions function) - which is more "OOP style". However, I would prefer to define anything with terms like "OOP style" and "FP style" because they are not strict. Best practices are virtually impossible to define without knowing your goals because for e.g. HFT best practices are about speed, monomorphism, cache aligning and avoiding allocation, while pratices designed around correctness would embrace reuasbility with strong guarantees and referential transparency (which often implies laziness and copying/allocation) wherever possible - these two are at odds, so you cannot define one "best practices guideline" which would suit everyone. So, before anything else, define your goal, requirements and limitations. The rest will follow.
thanks, please clarify "basically treating the constructor as a tuple -&gt; named tuple of functions function"
I personally think that OO is well-represented within the FP canon and you can use OO-as-FP-sees-it techniques alongside FP with good results. The real place where this technique has been most developed is not within the Scala canon as much as the OCaml/SML canon. &amp;#x200B; There are new ideas being explored and developed in the Scala community, and these are pretty exciting. That said, the \_core\_ elements are things like encapsulation via existential types, interfaces (see below) and implementations, LSP. &amp;#x200B; In my opinion, the big places that this gets confusing is that FP is very likely to invoke three things which are incompatible with "workaday" OO. 1. **Immutability** which makes notions of "state and behavior" very different, but much simpler. Learning about effect controls via something like Task or IO is critical. 2. **Avoidance of much of inheritance** as inheritance, subtyping, objects, and classes have a complex, not entirely coherent relation that's very, very butchered in "Java-like" OO. 3. **Objects as APIs as opposed to data** which is a weird one, perhaps best seen by the example of building an interface which discusses how multiple types interact. "Standard" OO usually has an interface describe one type in principle, but this conflates *data* and *implementation* and *API* in very bad ways. None of these are, in my opinion again, in conflict with the major ideas of OO, but they may feel unfamiliar if you're trying to achieve Java-like OO.
If we regard an object instance as a named tuple of functions (which is equivalent if the object's state is not mutable), then an object constructor is just a function that takes a tuple and returns a named tuple of functions.
``` class UserServices(database: Database) { def createUser(user: User): IO[User] = ... def fetchUser(id: UUID): IO[User] = ... } new UserServices(database).createUser(user) ``` is virtually equal to ``` case class UserServices( createUser: User =&gt; IO[User], fetchUser: UUID =&gt; IO[User] ) def userServices(database: Database): UserServices = ... userServices(database).createUser(user) ``` If your classes are virtually stateless and you store state in a storage accessed through some IO, you can write a code that is seemingly OOP but still following FP principles - and so classes and objects become just a continent way of modularizing code.
This is a good question. One phrase I've heard from a number of sources is "FP in the small, OO in the large". My interpretation: inside of a subsystem you'll use functional programming, but when you have to present a public API, you'll hide the FP and expose the API with lowest common denominator OO. This approach makes sense when you may have other teams which may be using different FP libraries or different styles, i.e. they're not using Tagless Final / cats / ZIO / scalaz / shapeless in exactly the same way that you are.
There is not one sole definition of "object oriented programming" whereas there is only one definiton for "pure functional programming" - or, if you are nitpicky, at least a much more narrow one. What most Java/C#/... developers mean when they say (classical) OOP is the style of bundling data and functionality. This is also the reason why they discourage the usage of getters and/or setters - because it means there is data without functionality (except getting/setting parts of the data). &amp;#x200B; I have heard a few reasons why they prefer this: 1. It is more intuitive because objects are closer how we experience reality, thus more intuitive 2. It is needed to ensure a valid object state for (2.1) simple changes or (2.2) changes in context of a history of previous changes 3. It is easier to find functionality when some data/type is given &amp;#x200B; For #1 I can actually agree on that for very specific situations. E.g. when I think about akka, it can be pretty intuitive for certain scenarios - say, a game simulation like the Sims or Age of Empires. However, in the vast majority of cases, it becomes much more difficult for a lot of reasons, e.g. classes become bigger and bigger, more and more bloated and are not well extendable. I never used FP for such a problem, but it might actually be working just as good or even better. &amp;#x200B; For both #2.1 and #2.2 I completely disagree. Changes on objects that create an invalid state should be prevented by the typesystem, e.g. by having a check in the constructor like assert(param &gt; 0) or, better, using special constructors that only return Option or Either - or even better, using Scalas powerful typesystem with e.g. the refined library if possible. For #2.2 someone once explained to me that OOP is necessary to prevent e.g. a user from trying to login too often. The user-object would have a login-method which saves some state within the object and complains if there is too many logins. However, this solution stops working as soon as the conditions are not just depending on the state of the object itself but on external state too. E.g.: say there must never be users with the same email. Suddenly the user class needs to know (= use) external information - for example from a repository - which immediately breaks encapsulation and separation of concerns. (an easy indicator is that you can't remove the repository anymore without making code changes to the user class). In the end, the repository will need to contain this behaviour and just work on plain user-data - which is not classical OOP anymore. &amp;#x200B; Finally for #3 they got some point when looking at some other languages - but this is only an argument against certain languages, not for the OO paradigm. Scala solves this problem rather elegant with companion objects and implicit classes. That way, it feels almost like Java and to use standard functionality the developer doesn't have to look anywhere. For additional functionality there are imports necessary that need to be known or discovered. However, extending functionality (like with extension methods) is actually not possible in Java anways. &amp;#x200B; In the end, I have not yet heard any convincing argument for classical OOP. Ff separation of concern is followed strictly, classical OOP will always end up splitting data from behaviour.
I have 3 objects in separate files containing implicit vals and defs. I need all 3 for something to work, so that's 3 imports. Is there any way to have a "common" import for these 3 objects? So I only have to import 1 thing instead of 3? (I created 4th file with references to the other implicits, but that feels kind of hacky) (The reasoning behind all this is that it would be easier to forget to import something if you need all 3 imports to work)
I've only read the first couple chapters of [this](https://www.goodreads.com/en/book/show/23488413-functional-and-reactive-domain-modeling), though it's the best I've found. &amp;#x200B; I still want to make my way back around to it, as I think the concepts are valuable; It's unfortunate I found it near the end of my time in Scala.
Tagless Final style as its usually done in Scala is basically the mix you're looking for – [http4s](https://github.com/http4s/http4s) is a great example of this mixed style, it uses both stateless modules and stateful objects that accept mutable resources in constructor (e.g. `Client[F]`). IMO OO best practices generally concern modularity and do not conflict with FP best practices, the difference is often only in terminology, e.g. FP programmers want to talk about languages/algebras and composition operators, whereas OO programmers will talk about state transitions – but in tagless final style, they both look the same. When you use Scala's OO system to represent your modules instead of a more foreign sum-type encoding (Free, Eff), you can easily combine best practices of two paradigms, e.g. [combine FP Tagless Final with an OO DI Framework](https://izumi.7mind.io/latest/release/doc/distage/index.html).
As a fellow developer who first introduced me to scala used to say, in an application sometimes you have to model streams of data, sometimes you have to model things like objects. With scala you can satisfy well these needs when you have them. This to say I don't think mixing because the language allows it is always good. Think about python for instance: it can be a scripting language and oo language, but please don't mix the two aspects
The common approach seems to be to make the 3 objects into traits and then create an object that extends all of them. Additionally you can create 3 more objects where each extends one of the traits (if you need separate imports).
Did you take a look at http4s? I found the documentation to be pretty good.
I'm a proponent of explicitly importing all implicits - at the very least no wildcard imports at file scope. So I'd argue you would be better off not trying to minimize imports. But to answer your question, you can move the implicits to traits, and then have an object which mixes in all the traits, and import from the single object.
I use OO features like methods all the time. It's just that unlike traditional OO, where internal state is mutated, they typically return new instances. Ad hoc polymorphism is a great tool from the FP world. But rigid hierarchies of nominal types are easy to represent and very handy when you control all the types. I typically organize my applications into services, which are modeled as either traits or classes. Service dependencies are modeled by either abstract members or constructor parameters. To me, the real key to effective Scala use is having a coherent strategy for isolating mutable state and effects in a really clear way. I think it's fair to say the consensus in Scala and beyond is that mutable state in every object instance is a bad pattern. But beyond that, there are many paradigms that mix OO and FP techniques.
Yes, I did, but the docs were not so good IMHO. I tried to make it work, but it was a pain at that time. I also tried Akka HTTP
After publishing, we wait to announce until the core of the library ecosystem has been published for the new version. This usually takes 3 or 4 days. That way by the time we announce, people can actually try the release on real projects. (And, if some horrible regression somehow crept in, we have a chance to roll a replacement release, god forbid.) We also use those extra few days to do things like finish preparing detailed release notes.
Return a new deck instead. ``` def drawCards(deck: Deck, cardsToDraw: Int): Deck ``` &gt; Considering dependencies are passed in OO style The way dependencies are passed is not important. The way data flows is important. Data and effects all flow through function returns = FP, anything flows through a side-channel = not FP.
\+1. Another example: [https://github.com/lightbend/config#immutability](https://github.com/lightbend/config#immutability)
I think the definition is not quite apples to apples with FP vs. OO. OO is about using inheritance and privacy/members to reuse code logic in other places. Similarly FP is about mechanisms to be able to reuse functions in other places. One technique to do so, is pulling out effects, but it's not the only technique, and you don't want to miss the other techniques for re-using functions that have nothing to do with effect handling. The key here is "what is the thing I'm trying to reuse without rewriting". Monad/Applicative/etc help you use simple functions in complex contexts, without having to directly lift them (hopefully). However, a method like "traverse" is not solely about limiting effects, it's about being reusable in many contexts and not rewriting fundamental operations.
Another dimension to consider is the OO practice of subclassing with overriding polymorphic methods. Although the resulting issue of co- and contra-variance is present in Java, it seems rather more important in Scala due to more complex type checker. The canonical example would be the various relationships between an `X[Animal]` and an `X[Dog]`. AIUI Haskell avoids this as there is no subclassing in that language: a type is a type is a type!
Hi all so I'm in the process of porting over a Python script to Spark/Scala. The script includes some data processing with pandas that include groupby's combined with apply functions. Right now I'm struggling with how to tackle this concept in Scala. &amp;#x200B; For example let's say i have an ID column I want to group by. And within the group by I want to run a function that does the following (gets the min of a column, max of a column, date range in months of the min and max, etc.) Are there any resources available that would help ease the learning process in conceptually trying to tackle these sort of problems.
I was going to post something similar. I basically follow the typical MVC/onion/layered architecture for the application in general and use more functional style within the individual source files. If you try to follow functional too much at the large scale I find the coupling gets a little too extreme, much like the problem you run into with overusing inheritance. Small changes become very large changes pretty quickly since a lot of source files end up depending on the parameters/return types of a particular function chain. It's probably not not really a functional vs OO argument though. It's more about making sure your overall architecture is loosely coupled and allows change. I think we've been talking about that kind of problem in OO for a lot longer and the ideas are a little more evolved. The meta architecture of services and data stores and such is more akin to OO anyways, so at the large you have to think about that even if within the service you are a bit of a functional zealot.
"Pure FP" or "referential transparency" is a binary condition, just as an object is either immutable or it is not. However, even within the realm of "impure" functions and mutability, there is a spectrum (a range) of how "pure" or "impure" functions and software is. &amp;#x200B; Generally, when starting out with pursuing "pure FP" what your goals should actually be is move a little in the direction of "purer FP." You could potentially make a couple functions pure, and start gaining some benefits, as you now know that small subset of functions always should produce the same output for a given input. Same with immutability; you might make a couple objects immutable, but not rewrite your entire software in one go. &amp;#x200B; Most people start out with "Scala as better Java", essentially using the patterns they know, and itteratively start applying purer FP as they start learning more FP tools, techniques, and patterns. FP and OOP play together just fine in Scala without really using any special techniques. The major downside of not applying pure-FP consistently, is you might not quite see some of the benefits of pure-FP, and be confused why people actovate for those patterns.
I don't think your argument about validators and a strong type system really has anything to do with functional vs OO. Whether of not the language is strongly or weakly typed and how strong that strong type system really is is a whole different issue. Types don't really exist at a low level so really any validation in a way is packing up some logic and sending it around with your data. That's basically what you defined as being OO. Is a really strong type system fundamentally OO? Fundamentally is it really different than just writing a Java class with validators in the setters and constructor?
Imperative shell, functional core anyone?
My probably wrong interpretation is that functional programming suggests that every interface should look something like A+B+C and what "+" does is encapsulated in what object A and B and C are.
This argument is just one that I hear people using quite often, which is why I mentioned it. For their definition of OOP (which I call "classical OOP") it is an important point. As for what you say about validators in setters - I agree, there is not really a difference!
How long has it been? Http4s is quite a young project, but the documentation has changed a lot within the last year. If it has been more than half a year ago, I'd give it a second try. The Doobie documentation was also really bad at one point but improved fast. If you are comfortable with Doobie, I think you should not have trouble with http4s. &amp;#x200B; Otherwise there is also lihayoi's cask which is close to flask if you are looking for something more lightweight. Play! is not a bad choice if you need a lot of the built-in features, but for me it is too heavy and I don't need most if it, e.g. the integreated template language (twirl, if I remember correctly), the whole MVC approach, the integrations with databases etc. I remember Play! updates to bring kind of trouble.
My code is mostly related to data science so I’m more of a user than a developer. That being said, I tend to use an OO style for my work as I’m generally modeling “lazy tasks” which I tend to code as specializations of traits so I can define all the tasks my job must accomplish (regardless of whether they’re the same). Then, depending on the job I use this collection to sequence my jobs, often by running the work inside futures. So, when I’m working on some code for ETL work, I’d define a trait which defines the minimum information my tasks need and define a abstract method like `def transform(): Unit`. In this way I can use the functional aspects of the collection when I’m ready to launch the jobs by `(Seq[T &lt;: MyTrait](...) ++ ... ++ Seq[U &lt;: MyTrait](...)).map(_,transform())` and letting my thread pool manage when tasks complete so another can launch. I model this trait so I can avoid starting and joining a thread pool for each table I need to build, and so I can centralize certain settings for my jobs and have that propagate to child instances. I personally really like this aspect of Scala. Most of my colleagues work in Python but struggle to see how OO inheritance makes their lives easier. The functional aspects of Scala are more interesting to me as a user, but I rarely develop novel collections in my work.
Thanks!
Thanks for the answer! I'm basically importing type class instances for multiple types, so in this case I think it's fine to wildcard import, instead of always picking the types which use have in your case classes you just want to use. I'm not sure what would be the benefit of not importing everything from the that limited scope. Maybe it makes the compiler a bit faster? And this is not really about minimising imports, it's about not avoiding the situation where somebody else will use only 1 of the imports then wastes time to figure out why it's not working. If I put them together it will always work for the first time. I'm kinda new to writing library like things so I'm still just figuring out how to do things right.
It's no different than OO code. Add breakpoints, examine objects, that's it.
Kotlin is for people who want something better than Java but only wanna go half in out of fear
Java, Scala, OO, functional, it's all the same. https://www.jetbrains.com/help/idea/debugging-code.html
I mostly just use `println` or `pprint.log`. I've used debuggers (e.g. IntelliJ's) before, and they're great, but the convenience of `println` or `pprint.log` is hard to beat The one case I've found myself forced to use a proper debuggers is when I'm figuring out code I can't add `println`s too, such as misbehaving code in third-party libraries
Seconding this, I use print macroes based on lihaoyi's (the poster whos comment I'm replying to) printing utilities. I have a macro `say` which does the same as printl but prepends file and line number so I can turn them off. I know it's not an answer to the question, but print debugging works great in my experience
found PHP-coder
Hope you like it; I’ll be happy to answer questions here
Hey guys, I'm new to programming and trying to get my hands on scala. I can't seem to run my sbt with my build.sbt done and I have no clue what is wrong.
IntelliJ Problem solved
The error seems specific to vscode. Try running it on a normal terminal.
You need to verify that your OS actually knows where to find sbt. Find out how to check what's called the "path" on your operating system and make sure that the sbt installation directory is actually on it; this lets programs call sbt without knowing exactly where you installed it on the disk. Otherwise, you'll need the right folder to the path.
First learn to use console workflow. Code editors have lots of configuration issues. When you get familiar with command line, ENV variables, $PATH you can go to the IDE Most likely sbt path exists in your bash profile which is not executed when you run code in your desktop environment. Try to start code within your project dir where sbt already working.
Oh yea I totally forgotten about it, thanks a lot mate! I’ll try if there’s any error again
I'm surprised you use that method. I've been forced to use that method for certain things, and one problem I found is that I have to re-compile to see the print statements. It makes the development a bit slower. With a debugger, I wouldn't need to re-compile. Also, stepping through code with a debugger is a bit more convenient.
&gt; you use that method In Soviet Russia, that method use **you**! ^(this post was made by a highly intelligent bot using the advanced yakov-smirnoff algorithm... okay, thats not a real algorithm. learn more on my profile.)
What are the main differences or deciding factors between ujson and circe?
Yes
It's nice that unlike sttp, when using the requests library you don't have to define an implicit backend, wrap URLs in an sttp Uri, or handle multiple levels of errors (in sending the request, then reading the body, then parsing it). With a bit of massaging it should be possible to have a requests-like api on top of sttp.
[Treelog](https://github.com/lancewalton/treelog) can help lots. Other than that, using logback and adding debug statements is usually how I do it.
I'm more familiar with Circe so this comparison will not be completely balanced, but I'll try to give an idea anyhow. Circe is used to map types—typically case classes—to and from JSON-structures. It comes with a bunch of extras that allow you to automate these mappings through macros, but also have nifty builtins to define mappings manually, for instance when you want full control of how to marshal the data (like asymmetric mappings). Circe is a well-rounded library that promotes safety and error handling: A runtime failure yields either an instance of a parse-error type or a decode-error instance, rather than throwing exceptions. The effect of this is that you as a user need to handle the possibility of these errors explicitly, which is often seen as a good thing in more "strict" types of programming styles, like functional where you don't want uncontrolled exceptions changing the flow of your program. Circe relies on [jawn](https://github.com/typelevel/jawn) for the actual parsing of JSON. Ujson is akin to jawn: it's responsible for parsing JSON, but also as you see here can be used to work more directly with JSON (jawn can also be used for this but it's more common to use a wrapping library like Circe instead). From what I've seen ujson's interfaces are more straight forward and simpler. It's likely much simpler to get an understanding of for newcomers. uPickle is a macro-layer that uses ujson to generate JSON-picklers for case classes, which makes it kinda like the macro facility of Circe, but again with a smaller API-surface. I've used upickle in the past, before ujson was extracted into its own library, and it's really easy to get started with. If you're mostly concerned with first party JSON data, where validation of user-input JSON isn't a necessity, I'd go for upickle first—or ujson if, like in this blogpost, I'd want to work with a known JSON API without setting up extra case classes just to represent the intermediary data representations.
Main differences with ujson vs circe, * ujson supports json mutation, circe does not. * \[ujson supports decoding to a case class, without the use of an intermediary JSON ADT\]([http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html#zero-overhead-serialization](http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html#zero-overhead-serialization)). * circe is designed with functional programming in mine, so errors are encoded more explicitly, etc. * ujson is very much more of a scripting philosophy, as with most of lihaoyis stuff, it tends to be a lot more pythonic. * circe and others tend to be more supported by other libraries, e.g. http4s has a circe library, where ujson doesn't. &amp;#x200B; For full blown applications my personal preference is to use circe with case class semi automatic derivation. &amp;#x200B; \`\`\`scala import io.circe.\_, io.circe.generic.semiauto.\_ final case class Foo(a: Int, b: String, c: Boolean) &amp;#x200B; object Foo { implicit val fooDecoder: Decoder\[Foo\] = deriveDecoder\[Foo\] implicit val fooEncoder: Encoder\[Foo\] = deriveEncoder\[Foo\] } \`\`\` &amp;#x200B; Hopefully this answers some of your question! &amp;#x200B; I would also recommend reading [http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html#zero-overhead-serialization](http://www.lihaoyi.com/post/uJsonfastflexibleandintuitiveJSONforScala.html#zero-overhead-serialization) as it highlights some of lihaoyis goals when designing libraries.
Recompiling is inconvenient, but so is attaching a debugger, so it’s a wash. At least with recompiling I can take a moment and go on facebook or something I end up running code in a wide range of places on a regular basis: {laptop, local-docker, remote-devbox, remote-devbox-docker, kubernetes} X {SBT, Mill, Bazel} X {Scala, Python}. I have instructions somewhere for how to hook up debuggers for everything permutation, but I can never remember them. `println` (or `print` in python) works every time
The benchmarks on Lihaoyi’s website seem to show upickles performance is better than circe as well
Travis runs outgoing requests through a load balancer, and Sonatype associates files in a staging repository by IP... this fails when Travis requests come from multiple IP addresses. Setting the staging id for all requests should solve the issue. I haven't used this plugin in Scala, but I've dealt with Travis deploys using Gradle and a Kotlin Gradle plugin. One of the things I struggled with was setting the staging profile id correctly. This is apparently a hash that doesn't change. I'm not sure how to find it via Repository Manager, but I queried for mine through my Gradle plugin. See https://github.com/OpenAPITools/openapi-generator/blob/242d2960822a02e4bf1e2f874ff5ba5ec770c4db/modules/openapi-generator-gradle-plugin/build.gradle#L152 The plugin I use is querying the REST API for the `/staging/profiles` endpoint (see https://github.com/Codearte/gradle-nexus-staging-plugin/blob/679e4cb67bb0af7050cc6d1fa55e0e7dbeeafc8c/src/main/groovy/io/codearte/gradle/nexus/logic/StagingProfileFetcher.groovy#L12)
I've had issues before with some already open staging areas, which then fails closing the staging area because it doesn't know which to close. Try closing everything via the web UI, and trying again. PS: I noticed you have the version as `0.1` it's generally best practice do set your version in a [semver](https://semver.org/) format, prepended with a `v`. So that would be `v0.1.0`
Yes, it should be straightforward to wrap other libraries to give the same nice API. Requests-Scala is just a thin/dumb wrapper around `HTTPURLConnection` after all, and even the original Requests (Python) is just a thin/dumb wrapper around `urllib`/`urllib2`!
Material here might help: https://github.com/inner-product/scala-days-2019-cats-workshop/blob/master/notes/
I did an intro to Scala in March. My content was 1. A short syntax intro 2. FP 1. expressions instead of statements 2. immutability 3. first-class functions 4. pure functions 3. algebraic datatypes 1. product types (case classes) 2. sum types (traits + case classes) 3. pattern matching with example ADTs option and either 4. classes 1. convenient constructor syntax 2. object keyword and that filled an entire hour. in the FP section I came to the conclusion that imperative programming is the sequence of statements whereas with FP you write your programs more like equations. "if it compiles it does what you want"
I have been watching you.. and am awe struck at your dedication. You upload a video every week and thrive to contribute to Scala dev community. Keep up the good work!!
Thank you for your kind words!
This list is very useful, thank you!
If I'm not mistaken, pandas is just for working on one machine - it is not like using the python API for spark. So is your plan to use Spark because you expect that you won't be able to keep all data in memory in the soon future or would plain Scala be fine too?
Working on Qt bindings for Scala. Specifically, the modern QtQML side. I'm doing this because I wanted to write a custom plasmoid (widget) for KDE Plasma but then found out that QML has a super weak and awkward type system. Hopefully, the end results is that I get to write my plasmoid and also open up Qt to Scala as a general GUI toolkit like JavaFX.
Inspired by some articles about learning English by studying most popular words to understand 90% of speech I am trying to make CSS Parser to learn what properties are the most used one to be able to determine what to learn first to be most effective. I am wondering what will be the result though...
A proof of concept demonstrating more complex programming principles moved to compile-time.
&gt; For example let's say i have an ID column I want to group by. And within the group by I want to run a function that does the following (gets the min of a column, max of a column, date range in months of the min and max, etc.) What did you try and where did you get stuck? This sounds like very normal Scala and I would expect the obvious way of doing it to work.
That's why I'm asking for help. I'm very new to the language and wanted to see if there was a good resource to better understand these things :)
I really enjoy your posts. They're a bit out of reach for me still (funny though how I used to think generic type notation was arcane and now read it like english.) but I think the examples are well-placed. Like when you switch multiplication backend from BNat to TNat: that you show the previous code again right ahead of the change makes a lot of difference. In some time I hope these type-level tricks come naturally too
Thank you for the kind words :) There's no substitute for just having practise with lots of Aux types really! Eventually you'll be able to reason about them as you would a run-time recursive algorithm and then it just becomes incredibly tedious boilerplate as you try to get your idea compiling.
So do you just want a general introduction to Scala? There's a collection of tutorials/courses/books listed in the sidebar if so. Your question sounded like you were asking about a specific problem because of all the details you gave, but I couldn't understand what the specific problem actually was.
Thanks!
Awesome but I really wish they'd just release these to YouTube. You can't increase the play speed here.
Yeah, the current websites player is better then the player from last year (though the website itself seems to be much slower), but why not just a double upload. People who prefer youtube can use it and people who prefer to see the slides side by side with the video can use klewel. Or if they don't want to support youtube then they could also use vimeo which has play speed options as well.
Yeah, would really be nice to upload to both. Especially since the site is super slow for me (taking 30+ seconds just to load the page right now). Not sure what's going on there. By the way, you can increase the speed through your browser's JS console. This seems to work for me: `document.getElementsByTagName('video')[0].playbackRate = 2.0` The slides seem to sync up fine too.
Please release on normal youtube. These sites like Parleys, SkillsMatter and Klewel are just information silos.
I can't find any staging area under my name in the web UI :/
I don't love this syntax: delegate ListOrd[T] for Ord[List[T]] given (ord: Ord[T]) { ... } It doesn't look like anything else in Scala. Maybe it could look like a def instead? implicit ListOrd[T](ord : Ord[T]) : Ord[List[T]] = ...
Distributed computing/Microservices, modular software construction. That is the main other reason to use it. Thanks to Akka and Kafka you can break your App into tiny modules. You can then structure your app to use Mesos ( or similar software) to schedule resources to run each module independently of others, allowing you super agility in development. I am going to use it to build my app and release it at like 60% to 70% completion. This way I can release it mostly complete, hype it for about 2 to 3 months as the user base grows. I will use it to entice people with mystery. What is this last 30%? Why would I be so secretive about the last 30%? What could it actually be? You see my app is totally different than anything out in all ways. For example it has no admin panel to remove content, AI does that. AI can even decide if your content contains nudity. AI can even get to know you, your personality and whether you are a troll or not. Artificial Intelligence requires Big Data, SPARK has the algorithms to analyze big data. Scala has great support for Parallel computing and concurrency with Futures and AKKA. What I am doing would not work with NODE.JS for example. I want to use every last drop of resource to it's max. Scala with Futures/Akka and Mesos will allow me to do just that. I could list tons of things you could do, but then I would be giving away all of my plans. Right now I feel like a Frontiersman standing on the cusp of land never before traveled. I can't learn the Scala atmosphere and type code fast enough. This first app will take some time, but it will be a world changer in so many ways, I'll probably try to remain anonymous. You see Scala will allow me to create such a clean, small, maintainable code base that I may not even have to hire anyone if I write all of the code. And so far thanks to Scala and Playframework it looks like I'll be able to do just that. After so many years of web development with PHP and PERL I've been blown away by how much I can do with so little code. You could use Java but, you will be writing at least 4 times the code and I have read up to 10 times more. PlayFramework believe it or not, makes it easier to create web apps than PHP and its many Frameworks. With PlayFramework you create an SBT project, run it, open the browser and BAM there it is. No configuring a server etc. Play has a built in server that is easy to configure and works right out of the box.
This site doesn't seem to load for me. Are there any mirrors?
it responds '502 Bad Gateway' to me...
I kinda like the `given` syntax how it's at the end of the expression ``` delegate for Ord[List[T]] given Ord[T] { ... } ``` kinda like the typeclass constraint in Haskell (that is at the beginning) ``` instance Ord t =&gt; Ord [t] where ... ``` although I think the parameters for the `given` clause *shouldn't be allowed to be named* -- they should be viable only via `the` (or whatever it's called now)
``` enum A extends java.lang.Enum[A] { ... } ``` /u/odersky wouldn't it be better, if the enums were compatible with Java by default whenever possible. And there would be a annotation (`@javaEnum`?), that would enforce this by failing compilation if it's not the case? kinda like `@tailrec` ensures a function is tail-recursive (but functions are tail-recursion optimised automatically as much as the compiler can). I think F# does something like this, if I remember correctly.
I was able to get my project ID: ``` curl -X GET \ https://oss.sonatype.org/service/local/staging/profiles \ -H 'Accept: */*' \ -H 'Authorization: Basic &lt;xXxXxXxXx&gt;' \ -H 'Cache-Control: no-cache' \ -H 'Connection: keep-alive' \ -H 'accept-encoding: gzip, deflate' \ -H 'cache-control: no-cache' ``` But I am not sure what to do with it now. What env variable should I put it under for travis ? And for the plugin to use it ...
I added a bounty on StackOverflow if any of you knows the solution: [https://stackoverflow.com/questions/56587653/issue-with-no-staging-repository-found-in-sonatype-publish](https://stackoverflow.com/questions/56587653/issue-with-no-staging-repository-found-in-sonatype-publish)
&gt; It doesn't look like anything else in Scala, which makes it hard to understand. I kinda think the opposite is true. One of the things that make implicits imposing in current Scala is how similar all the constructions are, e.g. conversion methods look similar to methods derving typeclasses, which looks similar to methods using typeclasses used as constraints, which look similar to methods abstracting over context. Yes, the syntax is different, but once learned it only has one meaning and that should help understanding, especially for newcomers. I think the emphasis on semantics (typeclass instancing given an existing related instance) over mechanism (implicit val/def/object) also will prove useful. That said, while I think the motivations are good, I too am not sold on this particular terminology or syntax. I don't have anything objectively better though. Maybe there's good value in maintaining a more familiar structure like what you suggest, though I would avoid the word "implicit". In the end this feels like a bikeshed where somebody just has to pick a color and we'll all get along fine afterwards.
about enums, in Java you can: enum X { A, B, ; /*shared methods here*/ } does the dotty implementation support something like that? I would suggest this syntax: enum Y[T](val item: T) extends java.lang.Enum[Y[T]] { case ZERO extends Y[String]("") case ONE extends Y[Int](1) } with { def get:T = item } or maybe even: enum Z { case Q case R with { /*only on R*/ } case _ with { /*shared*/ } } if we want to make the Java developers cry.
To answer that it's useful to know if you're a framework (one central dependency running and you configure it and insert your custom code where it allows you. typically comes with an opinionated set of transitive dependencies) person or a library (you choose several dependencies that each do one thing and come up with patterns to combine them together yourself) person.
This was considered but intentionally rejected. The problem is that making it compatible with Java changes the public API and the binary interface. You don't want your API or binary interface to change without notice when you change a "detail" that makes your enum suddenly fall in the category of those that can be made compatible. `@ tailrec` is OK because making a method tail-recursive or not has no influence on its public API and binary interface.
I agree that it's not so bad when removing the names.
&gt; You don't want your API or binary interface to change without notice when you change a "detail" that makes your enum suddenly fall in the category of those that can be made compatible. Technically, aren't any change to an enum an API and ABI break anyway given its sealedness ?
scala+play+akka is a well opinionated stack
Not *every* change, no. For example, if the `enum E extends A`, and later it `extends B`, as long as `B` is a subclass of `A` it's a backward binary compatible change (including semantically speaking). But if `A` was `AnyRef` and `B` is something else, then `E` could be a Java enum before but not after.
Ah indeed, I hadn't considered that case. I guess that if Enum was implemented today it would be implemented as a Java interface with default methods instead of as a class which would avoid the issue.
Delete yourself
The whole point of `given` is that implicit parameters shouldn't look the same as normal parameters. Neither at definition site or call site. I like it very much! :)
Could you please pm me the details. Thanks
Sounds great, now if only every request didn't result in a 502/504. Not exactly a great way to promote the conference (or wait, maybe it is, show up at the conference or never see anything).
speaking of \`New typeclass derivation scheme\`, can it be used for deriving not just \`Eq\` or \`Show\`, but also \`map\`s for \`Functor\`s?
do you support visas?
I would add that slick is a reasonable data access library to add to this stack 👍
Akka persistence alternative?
I mean as the ORM, it looks like Persistence is for local actor state.
My most developed current project is a bulk benchmarking utility. Basically it's just a tool for generating benchmark information for an arbitrary number of functions over an arbitrary set of inputs, with error handling. I'm using it to get a feel for best practices, especially with documentation.
Are you doing speech to text? If so, Sphinx4 is worth looking into. It's easy to setup but you don't get the best results.
What are the best libraries/APIs for GPU compute with Scala? I've been doing some looking around, and I'm still a little lost. I see a lot of talk about Apache Spark for ML and distributed computing, but it seems to assume you're using Linux -which I can probably work through eventually, but I still rely on Windows quite a lot. Other than that, it seems like Breeze is CPU-based, and I haven't managed to get ND4S or Compute.scala working. Should I continue trying to work with those, or is there a better option? My ultimate goal is working with ML, but right now the example I'm trying to get GPGPU working with is an SHA-256 cracker.
omg you were right ! I changed my version from `0.1` to `v0.1` and it worked !
Resolved ! &amp;#x200B; Just make sure the version number start with the letter `v`
please PM me too!
You guys relocate? Any more details on the job, incl. salary range?
Hit me up.
Thanks for sharing it. I'm going to add that to my current project. Your code is always inspiring. Keep the good work!
I'm already switching to Doobie. I think that, at least for now, Play is the right choice for the project. I tried HTTP4S more than a year ago.
I was trying to read it but the website returns an error.
It's a very interesting reading. I'm going to add links to your post from my website. I'm still trying to learn the category theory so it's very helpful to have access to contents like that. Could you please elaborate the following statement? &gt; We could also just put a locally scoped implicit monoid for multiplication, but that would break type class coherence. See FP for Mortals for more on Tags and type class coherence.
Sounds like a serious design issue in cats-effect? Any plan to fix it in the future? I do see monix in the test cases, but no comparison in the blog though...
Sounds over qualified for sitting if you ask me.
Yes. What this means is when someone is reading your program it should be straightforward to understand what’s going on. So if you create a Monoid for some type, say Int for example, you should use that throughout your program if possible so the person reading the code doesn’t have to track which monoid is in scope for that type. What scalaz allows you to do is to tag things so they become new types, and then it’s clear that they are using a different monoid.
Thank you. Now it makes sense to me. :)
Yes, this was the result of an intentional design choice in cats-effect but will be changing in cats-effect 3, as it has caused too much confusion and auto-shifting is a much better default. Look for more as cats-effect 3 development starts this summer.
I recently stumbled upon [https://github.com/juju2143/flufflepuff](https://github.com/juju2143/flufflepuff). Would you like to support Brainpuff ?
I have been working hard on learning Scala and going away from Java. I recently finished "Functional programming in scala". What do you suggest I should do next to help me improve my FP skills?
Nice and compact! [https://github.com/Dash-Lambda/Functional-Brainf-ck/blob/master/src/main/scala/interpreters/BFFunctional.scala](https://github.com/Dash-Lambda/Functional-Brainf-ck/blob/master/src/main/scala/interpreters/BFFunctional.scala)
[https://underscore.io/books/](https://underscore.io/books/)
Thinking on whether should I publish my scala css DSL. Main features are ability to generate DSL from CSS file, ability to rename css class names, ability to eliminate unused CSS classes and more. &amp;#x200B; Here is example of bootstrap4 converted to my dsl: &amp;#x200B; [https://gist.github.com/scf37/a8e8cccfcfa82bffd0b34b60bf861a00](https://gist.github.com/scf37/a8e8cccfcfa82bffd0b34b60bf861a00) &amp;#x200B; here is my web site using bootstrap with unused css classes eliminated: https://scf37.me
I think you might have to go for a language agnostic ML book or a Python or R one. The Spark/MlLib documentation is ok if you're already familiar with ML. For a mostly language agnostic book, I would recommend ESL 2nd edition. &amp;#x200B; I tried looking for what you're looking for, and was relatively unsuccessful as you can see.
Seems to be down again...
I hit you up via private messages, please check your inbox :)
Yes, as well as Bifunctor, FunctorK, etc. See Miles' talk: https://portal.klewel.com/watch/webcast/scala-days-2019/talk/20/
Oh cool. [Here's a translator](https://github.com/Dash-Lambda/Functional-Brainf-ck/blob/master/src/main/scala/translators/BrainPuff.scala), and I've rewritten my runner to support both Brainfuck and BrainPuff using it.
I think (hope) that a full trifunctor hierarchy is unnecessary. Why? Because unlike the error type, which must change often, the variance on the environment input type is generally not critical – you can more easily get away with pinning the environment type throughout your application. The environment changes are more important when using arrow combinators, but these can be addressed by implementing just the arrow operations and using bifunctor ops otherwise: ``` trait ArrowBIOMonad[Z[-_, +_, +_]] { def &gt;&gt;&gt;[E, E1 &gt;: E, A, B, C](g: Z[A, E, B], f: Z[B, E1, C]): Z[A, E1, C] ... implicit def monad[R]: BIOMonad[F[R, +?, +?]] } ``` The same applies to bifunctor classes actually, except for `catchAll` changing the error type to `Nothing`, you don't need a separate hierarchy to implement variance, a polymorphic monad instance and a variance annotation on the error type are enough to create variance-preserving syntax: ``` trait MonadE[F[+_, _]] { implicit def monad[E]: Monad[F[E, ?]] } implicit class MonadECovariantFlatMap[F[+_, _], E, A](self: F[E, A])(implicit ev: MonadE[F]) { def flatMap[E1 &gt;: E, B](f: A =&gt; F[E1, B]): F[E1, B] = Monad[F[E1, ?]].flatMap(self)(f) } ```
love this plugin, thank you!
Formatting on reddit is different than GitHub.
The best book that i found, but just looking to Manning/Packt books. Many of the things that i have learned on this were from past versions of Spark or more recent medium posts. [https://github.com/prnicolas/ScalaML\_2nd\_Edition](https://github.com/prnicolas/ScalaML_2nd_Edition)
Only on old reddit. ```code``` blocks work correctly on the redesign.
The class wars have begun!!!!
is it possible to implement dropWhile using unfold?
Like this? def dropWhile[T](lst: List[T])(cond: T =&gt; Boolean): List[T] = LazyList.unfold(lst){ case e +: es if cond(e) =&gt; Some((es, es)) case _ =&gt; None }.last
&gt;T\])(cond: T =&gt; Boolean): List\[T\] = LazyList.unfold(lst){ case e +: es if cond(e) =&gt; Some((es, es)) yes, thank you, can you please explain how it works? i'm a beginner at chapter 3 of manning functional. Thank you
&gt;def dropWhile\[T\](lst: List\[T\])(cond: T =&gt; Boolean): List\[T\] = LazyList.unfold(lst){ case e +: es if cond(e) =&gt; Some((es, es)) case \_ =&gt; None }.last by the way how do i implement dropWhile using foldRight? def dropWhile\[A\](s: Stream\[A\])(p: A=&gt; Boolean):Stream\[A\] = ??
Say I have a function `makeCSV(rows :List[Row])` but I want to want say make this work with a streaming or lazy data source, how would I do this? Just change `List[Row]` to `Seq[Row]` or do I need to rewrite the code like `myCsvHandler.append(nextRow :Row)` and have the caller repeatedly call it?
What that's doing is taking a List and using it to build a LazyList of Lists. Starting with the whole input list, it looks at the head (e). If the head satisfies the condition, it puts the tail (es) into the list of lists. It does this until either it hits the end of the list or something doesn't pass the condition, then the .last selects the end of that list of lists. So if you call dropWhile(List(1, 2, 3, 4))(_ &lt; 3), the process looks like this... {{1, 2, 3, 4}}: (1) {2, 3, 4} {{1, 2, 3, 4}, {2, 3, 4}}: (2) {3, 4} {{1, 2, 3, 4}, {2, 3, 4}, {3, 4}}: (3) {4} =&gt; {3, 4} I used a LazyList because that allows it to throw away any values it's not using anymore. If it were a normal list, it could get quite memory intensive.
thank you very much for the explanation.
def inc(n: int)=n+1 def parseAge(str: String): Option\[Int\]={ val age : Option\[Int\]=Try {str.tolnt} inc(age) // Error!, parsAge function has compiletime error. how do i correct it without changing type signature of inc function?
 `def flatten(ffa: Stream[Stream[A]]): Stream[A] = ffa.flatMap(identity)`
thanks
age.map(inc)
This looks like an assignment so I'll just give hints. Looks like you want to use your inc function on the value in the Option but it's wrapped up. That's imperative thinking. To think functionally, think that you want to change the contents of the option of it exists. But if you're trying to do this using functional programming do not use an if-statement to do this. Look at the option methods and find something that let's you do what you want. In the end you'll still have an Option to return.
The problem is that `age` is an Option[Int] and `inc` expects an `Int`. There are several ways you could go about this, but the most idiomatic is probably: ``` def parseAge(s: String): Option[Int] = Try {s.toInt}.toOption.map(inc) ``` This encapsulates `toInt` (which can throw an exception) inside of a Try, converts it to an Option (which is the effect we want to ultimately return from `parseInt`), and then maps the option via the `inc` method. Map, as you may recall, retains the effect (Option in this case) and its implementation for Option is such that if its input is `None` it simply returns `None` and if it's `Some(value)` it applies the function to the value and returns `Some(newvalue)`.
thank you, yes it is assignment
Your entire reddit history is Scala homework questions. You won't get anywhere in your education by trying to take shortcuts. It ultimately will not benefit you.
`Stream.from(1, 2)`
Rewrote this as: `def loop(curr:Int , inc: Int): Stream[Int] = Stream.cons(curr, loop(curr+inc, inc))` `loop(1,2).take(4).toList` Output: List(1, 3, 5, 7)
how to generate infitite odd numbers?
how to generate infinite odd numbers?
Ask yourself what the output of each of these calls should be, and see if that helps you figure out what to do: * `run(pure(1))("hello") * `run(pure("hello"))("world") * `run(pure(True))("") If that helps, you can do the same for the other functions - make up some input, and try to figure out what the output should be.
is it right if i use unfold ? *tream*.unfold(-1)(i =&gt; *Some*(i+2, i+2))
Okay, two things there: * Try wraps the output in a Try class, which can be either Success(result) or Failure(exception). The main difference between Try and Option wrappers is that with Option None doesn't contain anything, while with Try Failure gives you the exception it caught. * You're trying to pass an Option[Int] (or Try[Int]) to a function that expects an Int, it doesn't know how to unwrap it so you'll want to get the Int out first. Pattern matching is generally used to unwrap a Try, which would look something like this: val tried = Try{&lt;something&gt;} tried match{ case Success(result) =&gt; //Do something with result case Failure(e) =&gt; //Handle the exception }
I don't immediately see how you would given the usual signature similar to `unfold[A, S](init: S)(f: S =&gt; Option[(A, S)]): List[A]`. This signature indicates that `unfold` runs only until the first element it should not keep, i.e. `takeWhile`. Put another way, it combines the "include element" check with the "continue iteration" check, but they would need to be separate. A definition like `unfold[A, S](init: S)(f: S =&gt; Option[(Option[A], S)]): List[A]` would allow it, though it wouldn't be efficient. def unfold[A, S](init: S)(f: S =&gt; Option[(Option[A], S)]): List[A] = { val buf = List.newBuilder[A] var next = f(init) while(next.isDefined) { val (a, s) = next.get a foreach { buf += _ } next = f(s) } buf.result() } def dropWhile[A](l: List[A])(f: A =&gt; Boolean): List[A] = { unfold(l -&gt; false) { case (Nil, _) =&gt; None case (h :: rest, true) =&gt; Some(Some(h) -&gt; (rest -&gt; true)) case (h :: rest, false) =&gt; Some(if (f(h)) { None -&gt; (rest -&gt; false) } else { Some(h) -&gt; (rest -&gt; true) }) } } dropWhile(List(1, 2, 3))(_ &lt;= 2) res2: List[Int] = List(3)
What do you mean by overlay? If you want to add elements to a site within the browser, you probably want to make a browser extension.
Sounds like you need an HTTP library. [STTP](https://github.com/softwaremill/sttp) would probably work well for this purpose, but you may need to do some reverse engineering with a traffic inspector to figure out the correct queries. I'm not exactly sure what you're trying to achieve since the details are vague, but you may want to consider an alternative approach. For example, a userscript injector like GreaseMonkey will work well if you're trying to automate some kind of browser-based workflow. It would be much easier than essentially reimplementing whatever webclient you're using.
Yes. Use selenium for that. It's pretty much what it's for. https://www.makeuseof.com/tag/make-web-crawler-selenium/
What version of Scala does that book refer to and is it in any way out of date?
May be «Essential Scalia» by Noel Welsh and Dave Gurnell. “red book”. “Scala with Cats” Http backend with “cask”, “requests”. Akka.io stack.
Oh, I remember I once had the same issue. It's in the docs, but easy to read over!
Thanks, I'll have a look at the bits you mention. Some sort of 'browser-based workflow' is probably the direction I am thinking of heading towards. The intention is to use the app to login to a website, navigate a couple to pages to gather some data, then depending on the result it would then submit some form actions. &amp;#x200B; Basically, it's automating something I have to do on a monthly basis.
Good start would be helping the Scala community in migration to 2.13. You will learn or/and help to adopt best practices and feel a taste of different approaches in Scala.
Wow, great!
This article resonated with me. I also found John's false dichotomy of OOP versus pure FP inaccurate and misleading. In languages like OCaml (strict and functional, like Scala) people have always been using controlled local imperative state; I'd say it's actually the idiomatic thing to do (for example, just look at a project like [this collection of standard type system implementations](https://github.com/tomprimozic/type-systems)). This style often helps in making algorithms more efficient while keeping them simple. I'm not an OOP programmer, let alone a Java programmer. I want all that FP can give me (expression-based syntax, pattern matching, reliance on higher-order functions, immutable collections, type classes, and yes monads when they make sense), yet I also want to be able to drop down to more efficient imperative patterns when required.
AFAIK John's ZIO use OOP for composition of modules: trait Console { def console: Console.Service } trait Logging { def logging: Logging.Service } trait Persistence { def persistence: Persistence.Service } ... val program: ZIO[Console with Logging with Persistence, ProgramError, Unit] = ...
And some of the ZIO high perf internals are written using mutability and nulls. Some with Monix, Akka etc - it's the interface that needs to be functional and easy to use, implementation can be as fucked up as needed (though writing fucked up implementation shouldn't be your first choice).
Will do!
Honestly, much of the "controversy" doesn't really exist: it's a single-digit number of people pushing the more extreme viewpoints. At most, it's dozens. Dozens! In reality that the vast majority don't give two hoots about any of these flavor-of-the-month techniques: they'll use whatever is convenient, familiar, and best documented, without fear or favor. Even among those pushing forward with state-of-the-art techniques, especially those which are getting the most adoption, most aren't particularly dogmatic about it. It is literally impossible to keep up with the "latest hotness" in a significant codebase, when the latest IO monad or streaming paradigm or functional technique or whatever changes on a literally month-by-month basis. Maybe some day one of these newer techniques will find significant adoption in the community, but for now it's just a few outspoken folks experimenting with ideas.
I think GreaseMonkey would be perfect for this purpose then. I've automated several forms that I use on a daily basis using userscripts, which I've found to be much more portable and convenient compared to selenium. Do beware, userscript-based automation is written in JS, not Scala, so you may be forced out of your comfort zone. I'd be happy to help with any questions you may have along the way.
I agree that sticking *strictly* to FP is simply not really feasible in some cases. That being said... there are a number of problems with this. Some nits: * i'm assuming `in.size` at the top of the function is supposed to be `as.size`. * `.sliding(n, n)` is the same as `.grouped(n)`. If `as` is empty, the returned `Future[B]` never completes. This is pretty easy to fix - it would probably return `Future.failed(new UnsupportedOperationException)` Let's be honest - the entire function is *awkward* and frankly does not inspire any confidence there aren't any race conditions between `completeNext` and the `promises(i) = null` lines. There are simply too many possible asynchronous interactions with all these promises to be certain. Even if it's definitely correct, i would still not accept this in code review because of how awkward it is. Personally, I think this is a far better approach: def reduceFutures[A, B](as: Iterable[A]) (f: A =&gt; Future[B]) (g: (B, B) =&gt; Future[B]) (implicit execCtx: ExecutionContext): Future[B] = { if (as.isEmpty) Future.failed(new UnsupportedOperationException("empty.aggregateFutures")) else { val p = Promise[B]() // eagerly start all futures val futures = as.map { a =&gt; // eagerly fail the promise if any of them fail f(a).andThen { case Failure(t) =&gt; p.tryFailure(t) } } // Reduce the future results val finalFuture = futures.tail.foldLeft(futures.head) { (fa, fb) =&gt; for { a &lt;- fa b &lt;- fb // if any `g` fails, finalFuture will fail immediately c &lt;- g(a, b) } yield c } p.tryCompleteWith(finalFuture) p.future } } 1. It is far easier to understand. Do not underestimate the importance of this. 2. It fixes the bug for empty `as` 3. It is actually (mostly) functional and definitely more idiomatic. There is no mutability. We can be confident there are no race conditions. 4. The optimization of calling `g` on any two values as soon as they complete is not beneficial when you still have to wait on all the other Futures to get the final result *regardless*. The only downside is if e.g. the very last Future completed early, but its application of `g` threw (and would have thrown if it were called with any other `B`), then this will not fail as quickly - even *still*, there would be a better way to handle that: def reduceFutures[A, B](as: Iterable[A]) (f: A =&gt; Future[B]) (g: (B, B) =&gt; Future[B]) (implicit execCtx: ExecutionContext): Future[B] = { if (as.isEmpty) Future.failed(new UnsupportedOperationException("empty.aggregateFutures")) else { val p = Promise[B]() // How many pending Futures we have val pending = new AtomicInteger(as.size) // The previous `B` result, waiting to be merged with another B via `g` val previousResult = new AtomicReference[Option[B]](None) // Called when either a `f` Future or a `g` Future completes def complete(result: Try[B]): Unit = { // `complete` can't be tail-recursive because we call it for `g` @tailrec def tryComplete: Unit = { result match { case Failure(t) =&gt; p.tryFailure(t) case Success(b) =&gt; // Merge this result with the pending one, or have it sit and wait for the next one previousResult.get match { case None =&gt; // If another thread grabbed the None before we could, try again if (!previousResult.compareAndSet(None, Some(b))) tryComplete // Otherwise, if we were the last result, complete the promise else if (0 == pending.decrementAndGet()) p.trySuccess(b) case r @ Some(prev) =&gt; // Handle the same race condition as on the None branch if (!previousResult.compareAndSet(r, None)) tryComplete else { // Don't decrement pending -- the completion of `b` would decrement it, // but that would lose track of the pending `g` future, so we'd have to increment it g(prev, b).onComplete(complete) } } } } tryComplete } as.foreach { a =&gt; f(a).onComplete(complete) } p.future } } Definitely not functional, but also *far* less awkward than the mutable array, while loop, null checks, `var`s, etc.
Dude, we're reading lambda bytecode through java reflection to write (first-of-their-kind btw) monadic stacktraces in ZIO. As long as the interface in provably functional and composable – go nuts on implementation.
I enjoyed the article and agree with the "functional boundary mutable core" premise and how easily Scala supports that. I also feel the need to mention that I loathe the site itself. I should not have to unblock Javascript for 4 third party sites and make 20 extra network requests just to see some static text and pictures.
The 2.12 standard library is available out of the box in any Dotty project, there's nothing to enable. Dotty projects cannot use the 2.13 library yet (we can't support both at the same time) but we're planning to make the switch in the near future.
Thanks. And sorry, I don't know how I missed that.
The idea that g could form a semilattice over B raises the question of what happens when business requirements change so that it's no longer that and now it's an abelian semigroup. I've done what I can to sort of nudge things in a more well-behaved direction but at the end of the day software is written to be used and not marveled at as some sort of mathematical edifice. If the rest of the program depends on g forming a semilattice over B then that limitation needs to be communicated to everyone else in case they think that g is something that can easily be arbitrarily changed. If the rest of the program didn't depend on g forming a semilattice over B, then why go through the effort of documenting that property and ensuring it continues to hold after countless revisions? If you're not doing things like documentation and testing, is this really the most pressing concern?
1. implicit being used for 5000 different things is not perfectly fine 2. what's your suggestion for type lambda syntax? 3. clamping down on infix notation and trying to make coding styles a bit more homogenized is not a bad thing. a huge complaint about scala is that code can look wildly different between two different people and yet still do the same thing.
You're obviously posting this because you case about the quality of Scala 3, which is great. Instead of Reddit, you'd be better to get involved at https://contributors.scala-lang.org/ where a lot of this stuff was proposed - that's your best bet to influence the direction of the language and persuade for/against features that you disagree with. At the end of the day, Martin will still go ahead and do what he thinks is best but 1) that's totally his right IMO as Scala is his language, he's shared it with us for free and really owes us nothing, and 2) he and others in his trusted circle will read and consider your viewpoints and although you might not get agreement, but you will be heard.
1. You should check your assumptions, because YES IT IS FINE. Some of the best libraries out there e.g. Spark, Kafka, Akka are written in Scala, Twitter and Verizon and a bajillion companies out there are using Scala, Scala devs are some of the most well paid, everything IS FINE even with all of the existing `implicit` stuff. And **that could honestly change for the worse depending on what kind of changes are made to the language**, which is what I don't want (or any of us wants). Now, regarding the changes, I don't see why you couldn't just get rid of implicit conversions which no one likes, add some `typeclass` keyword (?) and extension methods for desired use cases, and then keep the `implicit val ...` syntax and corresponding parameter list the way it is. I would assume not that many people make sketchy use of implicit conversions in production and such, so getting rid of that only wouldn't break as much code. 2. I don't necessarily have a suggestion, but the matter of the fact is that an arguably VERY confusing operator is being added to the language which is something that shouldn't happen? It feels like stuff is being added willy-nilly. The ideal option would just be to use `=&gt;` but people said that doesn't work for some reason I'm not familiar with. Actually, I just came up with a suggestion. You know how `A =&gt; B` gets desugared into `Function1[A,B]`? Since type lambdas are used, I would assume, more sparingly, maybe just don't add a sugared option for those and have people write `TypeFunction[A,B]` or `TypeLambda[A,B]`? Sounds like a pretty decent option to be honest. 3. You say that infix notation is "a huge complaint". Yet, a poll in Scala Contributors shows that a majority of people would rather keep it: https://contributors.scala-lang.org/t/the-infix-annotation/3340/62. I agree that consistency in code style can be a good thing, but I don't think infix code looks "wildly different" from non-infix code, and I think the readability gain outweighs the small difference in style.
It seems you are afraid. Or you lack confidence in the concepts you already know. Or you are too attached to their representation in the language syntax. The syntax lacks in importante compared to the concepts. Imagine, in the future, where you no longer program in Scala, but in a new language. You won't need to learn most of the new concepts introduced by that language because Scala was already in the cutting edge so you know them already. You will just need to familiarize your self with the syntax, that's the easy part. Let me give you another example. `Future` has nothing special about it, its just a class with some methods, however its not straight forward to use if you have never used it before. But once you learn the concepts they are trivial. You can easily switch to javascript and use them, however in javascript they are called `Promise`. Do you think that will severely hinder your ability to use them? Or do you think it will slow you for a while until your brain gets costumed with the new name? `delegate`s and `given`s are more beginner friendly. For example: ```scala def foo(i: Int, d: Double)(implicit ec: ExecutionContext, s: String) = ??? ``` For a beginner its not obvious that both `ec` and `s` are **both** implicit. With given its much more clear. You also have `the` instead of `implicitly` which is more more legible. When I first saw `=&gt;&gt;` I wasn't a fan. However, how can you represent a type lambda involving regular functions (eg: `(A, B) =&gt; C`) without introducing a new operator? (If writing `Function3[A, B, C]` is acceptable, then you should be complaining they didn't remove the `=&gt;` operator.)
**Implicits** (i.e. typeclasses) were getting a bad reputation because of **implicits** (i.e. implicit conversions). People kept using **implicits** (i.e. implicit conversions) when they should have used **implicits** (i.e. extensions methods). &amp;#x200B; It's almost undeniable that the aforementioned features need to be distinguished by the language. I'm iffy about the current syntax changes (e.g. "given" and method names being placed *after* arguments don't look particularly great when mixed with type parameters), but I'm fine with the "delegate" keyword.
Implicits have real problems that go further than whether you like them or not. Just accepting the faults because Scala devs are well-paid and companies rely on implicits will result in a language that stops improving. * It is pretty hard to distinguish implicit conversions from type classes with other type class constraints (both implicit defs). In general it is pretty hard to find the use case when encountering implicits because they are all so similar. * Call-side and definition site do not mirror each other. So when looking at use-site there is no way of knowing whether applied arguments are implicit argument passed explicitly, or just normal arguments. * Usage with providing or leaving out explicit parameters for implicit parameters has quite a few gotcha's. Ultimately all proposals try to move from mechanism to intent, while getting ride of inconsistencies and surprising behavior. That is not to say that the proposals are perfect, and nothing can be improved. Which is why they are trying things out, sharing their designs, and gather feedback to see how things work. In a similar fashion using \`=&gt;\` for type lambda's is confusing, so much in fact, that even people working on the Dotty compiler (yes even Odersky himself) have had miscommunications in PRs about it. The current changes in Dotty are proposals, that need to go through the SIP comity before actually being part of Scala 3. The whole point of Dotty is to try out new ideas before settling on them in Scala 3. None of this is final yet.
Write a pretty printer for case classes. Since Scala 2.13 there is a new method on scala.Product called productElementsName. You can use this and the Paige library (https://github.com/typelevel/paiges) to pretty print any case classes.
&gt; that's totally his right IMO as Scala is his language, he's shared it with us for free and really owes us nothing gotta say this is not a good argument...
This post is for-profit shit pile. We have all good books in the sidebar. Thank you for referal links, but not.
You can abstract over it a bit more and do it like here this: [https://scalafiddle.io/sf/MuyzlbN/0](https://scalafiddle.io/sf/MuyzlbN/0) That's a bit nicer compared to Seq as it allows you or your users to put in any type that is foldable - even if it is something that comes from a third party library or Java.
&gt;It is pretty hard to distinguish implicit conversions from type classes with other type class constraints (both implicit defs). In general it is pretty hard to find the use case when encountering implicits because they are all so similar. implicit conversions are considered bad style anyways, except for using it to enhance types with methods - which can be done with implicit classes. So I'd say in my codebase, if you ever see an implicit def, it always create or derives an instance for a typeclass. DSLs might be the exception here. I tend to agree that changing the very general way of defining and using implicits, because we now finally seem to know the one and only one correct way of using them, is probably not a good idea. &amp;#x200B; For the rest, I agree with your points though.
TL;DR; Nope, you are good to go Java is a different best. There is some overlap, but most of the pitfalls are no longer in Scala. There are new ones :) Good luck on your journey
So, one cool thing about Self-Types is that they enable you to declare a type relationship with a wider range of types than trait extension. Specifically, they make type parameters and classes fair game. For example, something like this is possible using Self-Types: ``` trait Foo[A] { this: A =&gt; } ``` But that’s not possible to model using extension (i.e. `Foo[A]` may not extend `A`). While this might seem like a trivial thing, it actually enables some pretty cool patterns, particularly when dealing with mixin traits, because it allows you to “attach” behaviors subject to the type constraints on the generic, see [this small example](https://scastie.scala-lang.org/lSVkVBHSSoSZ8fxVkDLTdg) where I define: - a `Named` trait exposing a `def name(): String` - a `ConsoleLogging[A &lt;: Named]` trait exposing a `def debug(msg: String): Unit` - a `Foo` class that extends both traits Running that example will show the `Foo` class can now use `debug()` and the `name` it defines will be used. This has some pretty cool applications outside of toy example code, of course, like [this blog post](http://www.alessandrolacava.com/blog/scala-self-recursive-types/), which really helped me grok Self-Types and their very cool range of applications when I was first learning. Another thing Self-Types allow, which is not highlighted particularly well in the Tour of Scala unfortunately, is that they can be class types, not just traits. This, in essence allows a trait to define an _extension of_ a class without _extending_ that class, as traits may not extend classes. Hope this helps, I can try and answer any other questions that you may have from the links and examples I shared. Best of luck on your learning.
Thanks.
To piggyback on this comment, it’s also fairly common to find that a popular Java library has a Scala shim available to provide a better development experience from Scala.
any suggestions?
It’s also not correct. Every language change in Dotty has to be approved by the SIP committee. He’s on the committee, as he should be, but he’s not a BDFL.
&gt; I just came up with a suggestion. You know how A =&gt; B gets desugared into Function1[A,B]? Since type lambdas are used, I would assume, more sparingly, maybe just don't add a sugared option for those and have people write TypeFunction[A,B] or TypeLambda[A,B]? Sounds like a pretty decent option to be honest. In a regular function `A =&gt; B` no new name is introduced, we're just referring to existing names, so writing it as `Function1[A, B]` makes sense, but in a type lambda `[A] =&gt;&gt; List[A]` we are in fact binding a new name `A` whose scope is the right-hand side of the type lambda arrow, writing it as `TypeLambda[A, List[A]]` would completely obscure this. The syntax for type lambdas was changed when polymorphic function types were added, you can read through https://github.com/lampepfl/dotty/pull/4672 and https://github.com/lampepfl/dotty/issues/4712 to see the various proposals that were made, and if you think you can think of something with better trade-offs, feel free to open a topic at https://contributors.scala-lang.org/.
I didn’t. Whenever I interact with Java API’s it’s kinda weird, with their builders, factories, and beans. I learned quite a bit about the JVM though, you can’t escape that if you’re in Scala
Do you need to know Java to some extent to decide some runtime errors or understand stack traces? I've been wanting to dive into Scala but wondering about this.
Oh I didn't mean it as an argument in defence of OP's criticisms. I meant in more in respond to OP's suggestion of a poll. Feedback from users is a good thing but it's often not going to be the sole deciding factor. If a bunch of Ruby devs brigaded arguing to drop static typing and won a majority, Martin would (rightfully) reject it because that's not part of his vision for Scala. That's more what I meant. To a lesser degree I've faced this as an open-source author too. scalajs-react for example, there was a time I got hammered with complaints and criticisms asking for all kinds of quick hacks, anger that I wouldn't just have impure side-effecting code in various places, etc etc, but part of my vision for my libraries is that I want them to be very reliable and I'm not willing to sacrifice certain properties. My belief and experience is that they're very beneficial and important, but not everyone agrees. Eventually other libraries were born with different properties and now (I think) all our "competing" libraries have happy userbases because people choose according to their own preferences. If I'd always catered to the biggest or loudest voices, the library would've suffered and there'd probably just be one big base of users all disgruntled for different reasons. Each subset would have their own quality criteria and judge the library lacking. I don't always agree with Martin but just like the library example above, I think that accepting that Scala = Martin's vision and ultimately he makes final calls is necessary for Scala 3 being a consistent and high-quality language. (And for the record, he's not a mad dictator; SIP committees and Scala Centres and all that jazz exist.)
You will need to know about the JVM but can comfortably write without learning a lick of Java! In fact I'd say knowing Java hurt my Scala development at first. Good luck!
You'll need to know how the JVM operates - but you don't need to know any Java for that.
This looks an awful lot like homework. What have you tried to solve the problem?
Most stack traces are the same. They list files, and lines, with a description off the error. If you programmed before, you should be fine. Just take your time, read the error message, and Google it of needs be.
Website is not responding now... Is there a mirror somewhere (YouTube, PeerTube, ...)?
Nah. The first few lines of the error message are usually the most important, that's all you need to know.
You don't have to, but it can be helpful. Then again by that you should also learn C before learning Java.
I agree with what has already been said. I’ve been programming in Scala professionally for a couple of years and never learned Java, besides perhaps the date apis. In fact I believe not knowing Java at all will help you learn a more idiomatic Scala right away instead of holding you back. This way you’ll learn something fresh without always feeling the need to compare it to patterns you’ve been used to. You’ll still need to learn a few things about the JVM though :)
You don't need to know Java to use Scala, but it helps if you're going to be interacting with Java code. For example, `NullPointerExceptions` are rare in Scala with `Option` and `Either` but you can easily get them when interfacing with Java code.
No, my whole experience with Java was making a small game about 10 years ago, and I’ve now been a full time Scala dev for almost 3 years.
what a kind of numbers can be in it? how much lines are expected? how you are going to process them?
Knowing Java before Scala can actually be a detriment. It’s far too easy to fall back on Java coding practices.
what a kind of numbers can be in it? - Only Int how many lines are expected? - If you read the question the first line will contain n, so there would be n lines. how you are going to process them? - I just need to process the input and return either type
how big N can be? will it be less than 2^31? Ti and Li are whole numbers or float point one? in which range will their values be? You are going to return `Long` - how to calculate it value? If you don't know how for now then please provide additional argument, like `, f: (Long, Long) =&gt; Long` which will reduce numbers during parsing to the result value.
N is less than 2^31 Ti and Li are whole numbers and there range is less than or equal to 10^5
According to your library example, a split between Scala 2/3 ecosystems similar to the Python 2/3 split would be ok.
 import com.github.plokhotnyuk.jsoniter_scala.core._ // &lt;- require dependency: libraryDependencies += "com.github.plokhotnyuk.jsoniter-scala" %% "jsoniter-scala-core" % "0.51.1" % Compile import java.io._ object Examples { def main(args: Array[String]): Unit = { val in = new FileInputStream(args(0)) val res = try { process(in) { (acc: Long, ti: Int, li: Int) =&gt; acc + ti * li // &lt;-- here can be any your processing logic } } finally in.close() println(s"result is: $res") } def process(in: InputStream)(f: (Long, Int, Int) =&gt; Long): Long = { implicit val codec: JsonValueCodec[Long] = new JsonValueCodec[Long] { override def decodeValue(r: JsonReader, default: Long): Long = { var res = 0L var n = r.readInt() while (n &gt; 0) { val ti = r.readInt() val li = r.readInt() res = f(res, ti, li) n -= 1 } res } override def encodeValue(x: Long, w: JsonWriter): Unit = ??? override def nullValue: Long = 0L } readFromStream(in) } }
Only one limitation: all numbers should not have leading zeros, while just zeroes (`0` values) are allowed.
&gt;import com.github.plokhotnyuk.jsoniter\_scala.core.\_ // &lt;- require dependency: libraryDependencies += "com.github.plokhotnyuk.jsoniter-scala" %% "jsoniter-scala-core" % "0.51.1" % Compile import java.io.\_ Hey, when I'm running this code it's giving me an error Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
and I am taking input from the console
Ok, no problem! You should replace `new FileInputStream(args(0))` by `System.in`.
thanks!!
You are welcome! Next time, please ask such kind of questions in the `Got a quick question? Ask here.` topic that usually pinned in the top. Or in the Gitter chat: https://gitter.im/scala/scala
If you inspect the `items` variable you'll see that it's a lazy `Stream`. The operations inside the `flatMap` operation won't be executed until they are needed.
Do you allow remote?
If they allowed remote the position would be filled.
haha doesn't hurt to ask.
It's really unfortunate that \`Seq\` is so pervasive, given how dangerous it is.
Why is laziness dangerous
What are the issues with wrapping the functions in an IO yourself? &amp;#x200B; I'm not sure if this runs, but I would imagine wrapping an IO around \`os.read\` works something like this &amp;#x200B; \`\`\` object osF { private def readToF\[F\[\_\], A\](read: () =&gt; String)(implicit F: Sync\[F\]): F\[String\] = { val delayedRead = F.delay { read().leftMap\[Throwable\](FileReaderException) } delayedRead.rethrow } def readF\[F\[\_\], A\](path: os.Path)(implicit F: Sync\[F\]): F\[String\] = { configToF(() =&gt; [os.read](https://os.read)(path)) } } \`\`\`
Shameless plug: I wrote a series of blog posts on this recently, which you can find here https://techblog.livongo.com/using-scala-to-read-really-really-large-files-part-0-introduction TL;DR: there are a bunch of options, and which is "better" is highly context-dependent.
Didn't anyone teach you if you don't work you don't eat?
&gt;implicit conversions are considered bad style anyways, except for using it to enhance types with methods - which can be done with implicit classes. True, they are generally seen as bad style, and in the code bases I have worked on I have not seen them much. However there seem to be some 'decent' use cases. Because of type erasure, something like the \_magnet pattern\_ might be necessary since overloading doesn't work. And I also have seen 'wrapper conversions' occasionally, where a simple type like a String is automatically wrapped to a more complex type like Uri, or Keyword, or anything like that. These conversions seem pretty harmless to me, the problem lies in going from one type to a completely unrelated type. That said, I personally have never designed code that either uses wrapper conversions, or the magnet pattern.
What country is this?
This is in Tulsa, Oklahoma in the United States.
Unfortunately this is not a remote position.
Well, you need to reimplement them, this is one of the reasons why migration to other build tools is slow and sbt is still the most common build tool in the Scala community, the quality and amount of sbt plugins out there is astonishing and it's not easy to give them up.
Not moving to bazel. Honestly, I'm not sure what bazel is genuinely good for. We use it with (a forked version of) [Tweag I/O's Haskell rules](https://github.com/tweag/rules_haskell) on the Haskell side of the shop, and with [Nix](https://nixos.org/nix/). The goal is probably obvious: reproducible builds with caching, which neither [Cabal](https://www.haskell.org/cabal/) nor [Stack](https://docs.haskellstack.org/en/stable/README/) offer out-of-the-box. In practice—which in our case means "deploying Haskell binaries to AWS EMR"—this is simply a bachtrian nightmare, and we have an internal presentation on a related subject that also includes some hints and tips for succeeding with this build-and-deployment disaster _that the tools in question are supposed to be the answer to_. On the Scala side of the house, we've steadfastly refused—and continue to refuse—to get enmired in this muck. The [bazel Scala rules](https://github.com/bazelbuild/rules_scala) are, as a practical matter, the work of a [lone genius](https://github.com/bazelbuild/rules_scala/graphs/contributors) with an itch to scratch (I'm being completely serious—Oscar is a genius and does _amazing_ work) that we're not willing to bet the farm on. There's no bazel sbt plugin solution. There's no bazel sbt plugin solution in prospect. There's not even a clear path to the beginning of a thought process for a bazel sbt plugin solution, and again, it's not because no one's thought about thinking about it. My personal opinion with respect to sbt is that I get what frustrations drove the creation of Mill and Fury. But the reality is that sbt 1.2.x is an extremely good piece of software, the build server is extremely handy, the plugin ecosystem is very robust, and there are literally thousands, if not tens of thousands, of examples of sbt builds to study to gain whatever level of understanding you wish, easily 90% of which will leave you saying "Oh, of course" upon reading them, rather than "Dude, WTAF?" (Truth be told, _I've_ written some of the "Dude, WTAF?" ones.)
The issue here is the interaction of eager side-effects, such as assignment to `var` and `println`, with the lazy evaluation of `Stream`. A systematic solution to this consists of cleanly separating description and execution, including of effects. This is what "pure FP" (e.g. with [fs2](https://fs2.io/)) provides.
You re-implement them. Nothing SBT plugins do is magic. They put things in zips/tarballs, make HTTP requests, or mangle JSON or XML. You run `java -jar`, pass in a `-cp` and a main class. These are all things any developer should be able to do with some googling. Replicating them is made easier by the fact that the code of the SBT plugins is open source. Want to see how it works? Read the code, or even copy-paste it and mangle it to remove all the SBT specific bits. Reading/copy-pasting/mangling code is again something any developer should be able to do. Examples: - IntelliJ integration? the SBT's old `gen-idea` plugin spits out XML files. See what they look like, and spit out the same XML files - Scalatest? `java -cp &lt;classpath&gt; org.scalatest.tools.Runner` - Scala console? `java -cp &lt;classpath&gt; scala.tools.nsc.interpreter.IMain` - Uberjar? Just unzip your jars, put them in a folder, and re-zip them - Publishing to sonatype? Just some well-documented JSON HTTP requests Each of these is on the order of ~1-day of effort. Adding them all together takes a bit of time/manpower, but you should already have some time/manpower budgeted in your organization for tooling work of various sorts anyway. You may even find your own implementations to be superior, or simply better fit to your needs, than those that are available in existing SBT plugins.
Thank you. One more question: how would you avoid such a case? or which part produces \`Stream\`?
Turning my seq into a lazy seq is unexpected. I'm not sure which part becomes lazy since the initial array isn't lazy. I'm guessing it's something around \`groupBy\`. I'd prefer an explicit \`Stream\` if I want things to be lazy.
So, seq isn't your problem. It's sprinkling side effects and mutability in places they shouldn't be.
The `values` call returns a `MapLike.DefaultValuesIterable`, on wich calling `toSeq` will produce a lazy `Stream`.
To expand on what Haoyi says, my personal belief is that many SBT plugins exist only because SBT semantics are too complicated for most people to build an instinct for, and some people who have time to learn SBT semantics take time to wrap perfectly simple java/scala library ... &amp;#x200B; On the other hand, mill's semantics are a lot easier which means that most SBT plugins would not have a reason to exist in mill. You want to upload something to S3 in a mill build ? Just pull the AWS SDK and call it in a task, you don't need to wrap it in a "mill plugin" ... Moreover, mill interops really well with other tools by virtue of using json as an output format. As for Fury, it's hard to build an opinion until I put my hands on the tool, but my understanding is that there wasn't any plugin mechanism. However you can declare the compilation of a module to be dependant on the running of another module. This means that if you need any sort of logic to generate some code or upload jars somewhere, you could do it by writing a good old \`main\` method.
Whoever downvoted this obviously didn't get the joke...
Possibility to do it with comonads (or SK combinators or with Brainf\_ck) can not convince a necessity and utility of comonads (or SK combinators, etc): may be "for Life" are redundant words?
It's not dangerous per se, it's dangerous because it's unexpected. OP's question being a prime example. Nothing in the code snippet above says "I want a lazy collection", and yet you get one. That wouldn't have been such a problem if Seq was called EitherStrictOrLazySeq, but it's not, its short name is designed to be widely used, and it's used prominently in Scala's own type signatures such as VarArg-s. As a result, most of the time when people use Seq what they really expect is some kind of NonLazySeq, but that's not what they're getting. \--- Despite what others here are saying, this doesn't have anything to do with side effects. It's a matter of Scala choosing to make ubiquitous a useless type which doesn't even guarantee strict execution. Typed effects are not the only safety feature despite how it may appear in any Scala discussion. Proper naming and good API design is just as important.
Thanks to u/dwijnand who bring this wonderful project to us.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/akka] [Improve Akka HTTP DSL with the help of Dsl.scala](https://www.reddit.com/r/Akka/comments/c4jdl1/improve_akka_http_dsl_with_the_help_of_dslscala/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Thank you for sharing it with /r/scala! :)
The article literally provides an implementation of Conway's Game of Life using comonads, hence the title.
OK. +1 :)
Does anyone have the reverse. Turn my actors into ordinary code :)
Solve a real problem that you or someone you know has. Some repetitive task that you or they do on a computer, that could do with automating. That's the most effective way to learn any language.
`IndexedSeq` is a good default. `Vector` is often better than `List`, and by using `IndexedSeq` you can be agnostic about which until you need to know.
&gt; Implicits (i.e. typeclasses) were getting a bad reputation because of implicits (i.e. implicit conversions). People kept using implicits (i.e. implicit conversions) when they should have used implicits (i.e. extensions methods). I agree with that, but the current proposal seems to be that we will split what they look like at use site, but still have a single keyword at definition site? Which is really the worst of all worlds. &gt; I'm iffy about the current syntax changes (e.g. "given" and method names being placed after arguments don't look particularly great when mixed with type parameters), but I'm fine with the "delegate" keyword. I find `delegate` horribly misleading because I would expect it to mean OO delegation (i.e. forward this method/interface to this member which implements it).