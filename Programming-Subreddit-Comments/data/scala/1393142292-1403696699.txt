Interesting cheers. I notice though that List/Vector both still seem to be limited in that various operations (e.g. indexOf, length) return Ints. I guess if one sticks to iterators this isn't an issue but it just seems like a built in opportunity to shoot oneself in the foot!
Right, I'm not a fan of generating markup client-side so whether it's Backbone, Angular, or ScalaJS doing it, the approach doesn't appeal. I am, however, a huge fan of manipulating server-side generated markup on the client (transitions/animations) as well as AJAXing all form submissions and pulling down in place content from the server (e.g. a drop down menu, select list, etc.) -- this where ScalaJS is interesting for me, as a type safe JQuery + Coffeescript. Of course server-side markup generation is not ideal in my framework of choice (Play), was hoping your ScalaTags library would serve more as a better Twirl than something akin to Scalate's Jade syntax -- maybe I've been building websites for too long, XHTML is so easy for me to visually parse that everything else looks strange ;-)
You can use [fastutil](http://fastutil.di.unimi.it/docs/overview-summary.html) library, which provides (among other things) arrays indexed by `Long`s.
GWT is a very different beast, being a component-oriented framework that comes with everything and the kitchen-sink. First of all, GWT is flawed because component-oriented frameworks don't really work for Javascript - for in-browser development, the various use-cases and needs that happen are too diverse to describe everything in terms of components and listeners and in general the web is page/url driven and so components are a bad fit - GWT is not the first that tried doing it, being preceded by server-side frameworks, such as Tapestry and ASP.NET, frameworks built for cool demos, having awful results in the wild. Add the fact that GWT is not really a general Java -&gt; Javascript compiler. When I worked with GWT a while back, I often had to drop down to Javascript because GWT's features weren't enough for what I wanted to do. ScalaJS on the other hand does nothing more than compiling Scala to Javascript and that's it. It comes with no framework other than the standard library and doesn't impose styles, methodologies or frameworks on you. You're free to generate HTML server-side and spit it in a &lt;div&gt;. You're free to do JQuery-style DOM manipulation. You're free to wrap Facebook's React or AngularJS in a Scala-friendly API and work with those. I also think that compiling other languages to Javascript is a good idea. I don't like Javascript and the messy environment that it brings. For example, whenever I start working with Javascript, I always go through the pain of setting up the environment. Do I use CommonJS or AMD or Google's Closure for dealing with modules? Do I load the modules asynchronously in the browser, or do I package everything up in a single JS? But again, if I package everything up in a single JS, it means I need a build process - so do I use something like GruntJS or Brunch.io or should I simply setup my own Makefile? Brunch.io looks easy enough, but wait, maybe I want this special use-case and now you have to write a plugin. Oh, but the project is going to need tests, so that would be a separate compilation unit. And oh, I would like to use this or that library, but they are too big - JQuery is at least 30 KB of gzipped and minified Javascript, yet I'm doing only very simple DOM manipulation and this app will load on mobile phones and I don't want to pay that price. Oh, but look, Google's Closure compiler can do tree shaking and remove the code you don't actually use, but wait, JQuery is not written for Google's Closure, so Closure's advanced optimizations will mess it up. And for dependency management, there's NPM, but then you discover that NPM is usually used for Node.js server-side or compile-time dependencies, not for browser-side dependencies and so you have to combine it with Twitter's Bower, because that's what people use. And this goes on and on and on, until I get sick of it before I even start to actually write a single line of code and it's not over, because you then have to battle Javascript's idiosyncrasies, like its crazy ass dynamic scoping or its implicit conversions that always generate headaches or having to pick one of the dozens of frameworks that are doing the same thing, each one of them flawed in one way or another. True story - I once spent 2 days hunting a bug that was happening due to an implicit conversion from String to Number and because the String could've started with a "0", Javascript was converting it as if the number was using base 8 and because the app was based on Backbone with listeners and stuff, the error was manifested in a totally unrelated area, far away from its origins. The idea of using a solid language, coming with an environment with modules, dependency-management, testing and a build process baked in, a language that you can also use on the server side and that doesn't impose a certain framework on you, is very appealing. And ScalaJS is not alone, we already have other alternatives. ClojureScript and Dart are in the same league. However I love Scala more, so I'm happy to see it running in the browser.
Ah, very insightful, and thanks for explaining how ScalaJS is different from GWT. I will take a closer look at it.
You want to load more than 2 billion records to RAM to process them? 
For all practical purposes the standard library collections are limited to `Int.MaxValue` elements because `size` is defined to return an `Int`, and many inherited methods use the `size` method either directly or indirectly, which means your gigantic collection may seem to work but will fail at runtime depending on what you try to do. This applies even to sparse pseudo-collections like `Range`. You can have a `Range[Long]` with more than `Int.MaxValue` elements but you will find that you can't do much with it (including `toString`). This is a bug in the design of the stdlib collections. It will never be fixed. See recent talks by Paul Philips for some related ranting. **EDIT**: I should also recommend the `#scala` IRC channel on FreeNode. It's a very helpful group of people. Newbies always welcome.
&gt; You want to load more than 2 billion records to RAM to process them? Not at the moment, but I can see myself wanting to do so in the near future. Oopps, Edit: No I don't want to load the entire DB. Rather some of the arrays generated from the data are likely exceed this. Edit2: Yes, I can work around this. Should I have to though? I'm investigating/evaluating Scala and one of my check-boxes is having a language that I don't have to fight to do what I need to do. Day two and I'm being told that there are battles ahead....
Li Haoyi, One thing that bothers me is the spray.io-style DSL. I haven't checked-out the internals yet, but why do you need to do "onSubmit &lt;~"? On first read, that looks as if you're passing a callback that should execute onSubmit, however why can't that be a regular function call that takes a callback? That custom operator looks a little scary - granted, I have no idea what it does, maybe it's actually useful, but you can't see that when you first look at that code. Another thing - this is cool for showing off how terse a TodoMVC app can be, however what I'm looking for in alternatives is maintainability, malleability, composability and other traits useful for developing complex software. That source-file is like a single expression. Just looking at it, I don't understand if that expression can be broken into multiple parts that can be combined, reused and so on. For example, can a common template be defined and reused across multiple pages? I don't understand by just looking at it. Of course, for TodoMVC the code looks OK, it's always fun to do LOC counts. I also love how you used your ScalaRx library. Your work is great btw. Cheers.
This happens because the interface for those methods gets inherited from Seq[T] and further, from Iterable[T]. On the other hand doing indexOf, length and so on on a freakishly big list is not really a good idea ;-) Personally if I needed to hold lists of things whose length exceeds the capability of Int, I would design my own data-structure, specialized for the use-case at hand.
&gt; This is a bug in the design of the stdlib collections. It will never be fixed. That is more than a little confusing given many people seem to be pushing Scala as the next "enterprise" level technology. I'm on "day two of learning Scala" and can't help but wondering if it has its own obsolescence built in... 
Manipulating server-side generated HTML is also pretty limiting. For simple use-cases it works great, however for more complex apps it's better if you can separate your server-side web-service from client-side rendering/UI. Working like this makes your web-service instantly available not just to in-browser Javascript, but also to native clients for say Android/iOS. And for high-traffic scenarios, it's awesome to serve your static files by means of a CDN. Of course, if you have a classic web app, then this is overkill, but for GMail-style interfaces it's better being able to do this separation, because in such a case the easiness of server-side rendering doesn't buy you much and ends up staying in your way.
Yes, the Scala collections have a lot of limitations like this one, a fact that was recently covered by Paul Phillips (a Typesafe cofounder who recently quit because he was angry at the way Scala is being run). I strongly recommend watching [his presentation](http://www.youtube.com/watch?v=4jh94gowim0) before you decide if you want to commit to Scala. A few post-Scala languages are beginning to emerge, such as Kotlin, Ceylon, Ermine, Frege and Dotty (although since that latter one is being written by the same people who wrote scalac, I don't have high hopes). Of all these languages, Ceylon is the only one released (a few months ago), all the other ones are in beta/alpha. 
You missed my point. Personally I've never reached the limits of Int, even though I used Scala's collections for in-memory caching in a high-load scenario. So you're either fantasizing about what would happen "if", or you have a concrete use-case - in which case I'd like to know what that use-case is. Speaking from experience here - if you have a collection of more than 2,147,483,647 items, then you'll have much bigger problems. Maybe you haven't done the math, but lets say that you want to hold more than 2,147,483,647 integers (so only 32 bits per stored item) - that alone counts for more than 8 GB of RAM used - though we are talking about collections of things, so those items are probably going to be tuples or objects, so a much better prediction would be 16 or 32 GB or even more - if Strings are involved, we could easily be talking about 128 or 256 GB. And maybe you haven't experienced big heap sizes while working with a garbage-collected language, but let me tell - it sometimes sucks. The JVM has probably the most advanced garbage collectors in mainstream usage. But on big heaps, both the parallel GC (the default) and CMS (the concurrent one) are choking, freezing the process for seconds or tens of seconds, or even minutes on a single pass and this latency grows exponentially with the size of the old generation. The new G1 garbage collector from Java 7 is designed to be more predictable and non-blocking, for big heaps running on multiple cores, however it can still suck and depending on use-case (like say, maybe you've got near real-time requirements, like a high-traffic web-service), then you may need to shell out the cash for Azul's Pauseless GC, which I've heard is great, but isn't cheap ;-) And again, the JVM has the best garbage collectors in mainstream usage. If you think you'll have an easier time with Haskell, or Go, or whatever, you'll have a surprise coming. This is a hard problem to solve. In C++, if you have to do a lot of allocations/dealocations with malloc(), you end up with fragmented memory, with the app ending up occupying much more memory than it needs and choking on simple allocations, because compared with usage of a generational GC, malloc() is very expensive. What people do in C++ to avoid this problem is to allocate memory pools and do their own memory management, which is something C/C++ excels at, but it's not for mere mortals to attempt - memory pools incidentally are also used in Java in libraries or apps that are using a lot of memory. And then you've got performance issues. For really big data-structures, you need data-structures designed to still be efficient even if their size exceeds the available RAM, in which case the kernel will resort to swapping memory pages to disk, so you either need data-structures that minimize this swapping (e.g. the String B-Tree), or maybe probabilistic data-structures (e.g. Bloom Filters). Such data-structures are specialized, which is why you won't usually see them in the standard library and incidentally, that's what databases are designed to do. And do you think that iterating over more than 2 billion elements is going to be fast, just because it's in RAM? Hell no, it's freakishly slow - because RAM is in fact slow and the reason for why you're not seeing it is because the CPU is quite good at caching stuff - but for more than 2 billion items you'll need indexes specialized for your use-case and that don't completely trash cache-locality. If it were this easy to load tens of gigabytes of data in RAM and filter it, then everybody would build their own in-memory database. Either way - you're either worrying about a non-issue, or you're worrying about a concrete use-case which you're planning to solve awfully.
Standard libraries are problematic in every language because they ossify very quickly (almost by definition). So I would just file away this data point and keep on going. Scala provides the mechanisms to write correct, expressive programs and remains the best JVM language I think. But no language or stdlib is perfect and you will need to learn which bits to avoid.
I looked into the collections of the different languages you listed in regards to their handling of `size` and the usage of 32-bit sized integer. For Java, it should be noted that the standard library collections tend to use `int` for `size`, index operations, etc. Kotlin and Frege both have collections that use 32-bit sized integers for size-like functions and index operations. I was a bit surprised to see Frege have that, since it is based on Haskell. Ceylon uses 64-bit sized integers for size-like functions and index operations. However, it did note in its documentation for ceylon.language.Iterable that for `size`: "[...] In the case of an iterable whose elements are not countable, this operation never terminates.". For fun, I also looked into OCaml, which also uses 32-bit sized integers for `length` and similar operations. So, only Ceylon amongst the listed JVM languages handles the sizes of collections better than Scala, and it also has the issue of `size` returning an integer for potentially infinite collections. Paul Phillips is working on a [new collections library for Scala](https://github.com/paulp/psp-view). So far, it handles `size` by having cases such as `Finite` and `Infinite`, which can be used to avoid the issue of trying to get the size of an infinite collection. The project is still very young, however, and I believe Paul do not yet accept pull requests for it.
Oh no, don't worry I get your point (and excellently expanded upon, thank you!). The particular scenario I was thinking of when asking the question involves time data...rather easy to exceed a couple of billion data points obviously. We're rapidly approaching the point where a couple of hundred gig in a desktop/server is economical (heck, it *almost* is so now with non-ecc...but I like to sleep at night ) hence I got to dreaming. TBH, I'm getting old. I'm tired of fighting languages and implementation details. I just want to throw my solution at the hardware and let the language/VM abstract away as many technical considerations as possible (even if there is a degree of performance/financial penalty for such ambrosia). I know there's more than one way to skin a cat, so yes, this is essentially a none issue. Today. But I honestly think that these sort of considerations are not *that* far away from being an issue.
Yes, from what I've seen of PSP so far, it looks like Paul is headed toward a very heavy "traitification" of the collections, which is going to be a breath of fresh air. The current Scala collections have accumulated a lot of extra weight which makes their extensibility or even daily use cumbersome. I really hope he sticks with Scala so we can see how far he can push this, but his latest tweets are increasingly pointing toward him migrating to a different language. 
&gt; On first read, that looks as if you're passing a callback that should execute onSubmit If that is indeed what you thought, then it's perfect! Because that's exactly what it means! There isn't any particular reason for `&lt;~`. I just thought it would be nice to use some "operator" similar to `:=`. Changing it to be a regular function is trivial (just change the implicit pimp in `Framework.scala` to use `def apply` instead of `def &lt;~`) As for all the other things, you're going to have to try it out yourself to see. As a first approximation, the answer to all your questions is "yes". It can be broke up *like any other large complex expression*, using functions or objects or whatever you fancy. This was more a quick demo than a real framework, and doesn't impose any structure other than the normal structure of a Scala application. EDIT: just to emphasize, as a quick demo (10hrs of work?), this hasn't been through the same amount of thoughtful consideration that older frameworks like Backbone or React have. I think it's interesting that despite this, it is far more concise and (arguably) just as reading/elegant. Hopefully in the future when we *have* put in the same thoughtful consideration, it will be better still =)
I was one of the last-month downloaders. Sad to report it is so terribly slow that I resort to editing in Eclipse and using IDEA for debugging only.
Odd, it's very fast on my machine which is about 2 years old.
I can't run Idea at all on my work laptop, which is a brand-new Thinkpad T440.
Zoinks, Eclipse + Scala IDE plugin also can be laggy at times o_O, particularly with dense code blocks where semantic highlighting has to play catch up. Otherwise, if it weren't for shoddy play template layer support I'd be _extremely_ happy with the Eclipse solution. We can all be thankful, Eclipse or IntelliJ user, compared to 3 years ago, the stone ages of the IDE story in Scala :D
If you're willing to live on the cutting edge, there's, the HTML5 Canvas itself e.g. [this](http://lihaoyi.github.io/workbench-example-app/triangle.html) and [this](http://lihaoyi.github.io/workbench-example-app/dodge.html) are 100% pure Scala.
I have had both IntelliJ and Eclipse lag (and not just when using Scala), but I have never, ever, had lag in NetBeans, and the `nbscala` and `nbsbt` plugins are very good.
I had one of those. IDEA was slow for me too. 
Thanks for that tip. I will have to check out Netbeans.
I like netbeans, too. However, you can't use the latest plugin to develop project in Scala 2.9. Intellij have no issue with developing multiple projects with different Scala versions. 
&gt; Thinkpad T440 Wow, that's underspec'd for the price. I have a cheaper lenovo idea pad with an i5 and 8GB of RAM. I have multiple instances of Idea running all the time without issue. 
Mine's the "s" version actually, it has 8GB of ram and I think I have the core i7 as well (I'm not at work to check today), but it's only dual-core. The thought of multiple instances of Idea running scares me... yeah, I got ripped off on specs by the company.
I recently released an early and not very well-documented raster image library: https://github.com/stephenjudkins/pureimage You'll have to roll drawing primitives yourself, but that should be a fun challenge to do in a purely functional way. For example, here's a circle: ``` def circle[A](x0: Int, y0: Int, r: Radius, in: (x, y) =&gt; A, out: (x, y) =&gt; A): ( if (math.sqrt(math.pow(x - x0, 2) + math.pow(y - yo, 2)) &lt; r) { in(x,y) } else { out(x,y) } ``` 
In that case, it's surprising that Idea doesn't run well on your machine. Like I said, my machine can have multiple Idea projects open and even different versions of Idea open at once. I got the community edition of Idea 2013 to try out the SBT integration. It's a vast improvement over the previous version for Scala use in my opinion.
Uh, what? You know Netflix isn't just a website and has an entire platform behind it, right?
I wish Jetbrains would publish a graph for this. About the closest I found was in: http://www.dzone.com/links/downloads_of_scala_plugin_for_intellij_rocket_upw.html
The last time I tried Netbeans' Scala plugin the code completion was very iffy.
I haven't tried Prismic. I just requested an invite to try it out. I'm curious about what they are doing. On your points, they are valid, however consider that network bottleneck is the biggest problem on mobile phones, much bigger than rendering performance for most use-cases. When doing client-side development for mobile phones, the biggest issue I'm seeing is the initial payload - which often gets packaged as a single JS file + a single CSS file + a single HTML file, to incur that cost only once. However, this payload can be served by means of a CDN, which drastically reduces latency, especially on mobile phones. Still, using various Javascript libraries can be a problem. For example JQuery is pretty big to serve on mobile phones if you care about latency (though JQuery may be a bad example here, because it's so popular and so it's usually cached). What I find cool about compilers such as Scala.js and ClojureScript is their integration with Google's Closure compiler, which can do tree shaking. And that's pretty cool - because it means you can import whatever library you want and only incur the cost for the parts you actually use. Unfortunately the libraries would have to be written for Closure's advanced optimizations. An interesting fact is that Closure's advanced optimizations are messing up JQuery because JQuery's developers did a lot of optimizations on their own in order to reduce JQuery's size, optimizations that are at odds with what Closure relies on. On Android/iOS - while the web is the most ubiquitous, some users want native apps, some managers/clients think that native apps are a good idea and so on and so forth. It's very, very nice to have your backend API-driven such that a functional API is always ready for whatever it is that want to do. 
Simply put, you're probably using the wrong data structure if you have that many elements in a list. This limit is the same as java, which is the current de facto standard enterprise technology. In 99% of cases you're going to want a stream or iterator of some sort rather than just shoving billions of elements into an in- memory list. 
I have a Thinkpad E530 running Ubuntu and a MacBook Pro, both with Core i5, 8 GB of RAM and SSD - not really a performance beast by today's standards btw. IntelliJ IDEA runs fine on both and haven't had any issues and on my laptops Eclipse is many times more sluggish. I believe the difference is in having a SSD, because code compilation is very I/O sensitive. If you don't have a SSD, then go out and buy one right now, especially since they aren't expensive. RAM is cheap too - though for my needs I haven't had a need for more than 8 GB, but in case you're having issues, just add more RAM. After all, these are power tools that are doing a lot behind the scenes. And btw, in case you're running IntelliJ IDEA on top of OpenJDK, then you should install the official JDK from Oracle, because OpenJDK has Swing-related performance issues in combination with IntelliJ IDEA. I haven't noticed them, but other people reported problems.
| Should I have to though? You're complaining because Scala doesn't work out-of-the-box for your insanely specialized use case which practically nobody will come across? Loading that many elements in memory *all at once* "to process them" is insane. I can't think of a single scenario where that would warrant such a thing.
Now posted to the programming subreddit: http://www.reddit.com/r/programming/comments/1yvpis/scala_plugin_for_intellij_has_been_downloaded/
Yes you are right, there are data structures and algorithms to abstract away this issue and which don't require huge amounts of memory. And actually for the problem I had in mind, those would be fine. I am just surprised to have come across such obvious limitations to Scala's collections so early in my experimentation.
&gt; I strongly recommend watching his presentation[1] before you decide if you want to commit to Scala. Thank you *very* much for providing that, it was eye-opening.
Just interested, what's the use-case which forces to deal with basically random access patterns?
Enjoy the down-votes. :-)
Interesting, thanks for sharing that. I'd love to see even more examples in the library. . I'm only really familiar with Lenses as simple getter/setters 
Why are agents cool and useful?
Agents are just atomic references, wrapped in a lightweight code allowing asynchronous execution of a queue of updates. The agent state can (should) be immutable and the functions sent to the agent can (should) be pure. The only impurity is the update of the atomic reference, but this is hidden from the user.
The plugin programming team (Alexander Podkhalyuzin, Pavel Fatin, ...) is great. Since they more or less write the presentation compiler from scratch, the plugin will probably always produce some discrepancies against scalac. But they are very fast helping on the forum and also doing an amazing job fixing bugs as soon as possible, if they see that it is urgent.
Thanks, that's a helpful explanation of what an agent is. I was also wondering why it's considered an interesting and useful abstraction?
I suppose you know why atomic variables are useful and why functional programming is appealing. So agents are a nice abstraction to combine both. On a modeling point of view, they allow to clearly separate identity from value which is useful/needed in several approaches like DDD. Here are a few pointers if you are interested: * http://www.infoq.com/presentations/Are-We-There-Yet-Rich-Hickey * http://lambda-the-ultimate.org/node/2978 
This is entirely true, but... What if you *didn't* have to develop/maintain 3 or more client applications? What if your client logic was written once, in *Scala*, and then cross compiled to run on the Server/Dalvik/Javascript/iOS, with thin shims for UI elements? In the end, this has always been about code re-use: having the API is a mechanism to avoid having to rewrite the same code on 4 different platforms. But what if you could achieve the same thing via cross-compilation? Not saying that's where we are now, but that's the dream =)
Well, yeah, that would be pretty nice, one language to rule them all ;-) That direction seems quite interesting, as mobile devices become more powerful the render it all on the client approach makes more and more sense. p.s. you're doing impressive work across the board in Scala these days, so don't take my critiques as dismissing the projects you're working on -- just my take on where I'm at now with my web projects, where given the state of the art, server-side makes the most sense in terms of retaining type safety.
^ Don't feed the troll.
This sounds like an amazing opportunity, I don't assume you have a non-senior position? How do you feel about non-US citizens? Is it a possibility at all?
How non-senior is non-senior? We're looking for competent people, first and foremost, and will consider more junior developers who are smart and driven to learn. I don't know how it works regarding non-citizens, but I'll find out and report back.
Cutting all of those qualifications in half I guess? :) It is one of the few job positions that actually rub me the right way though.
I encourage you to apply. I'll PM my email address and some instructions shortly.
Just curious, where do you think possible data loss would come from? String conversion?(ie loss of character encoding?) Seek rounding cuttoff?
&gt; How do you feel about non-US citizens? I heard back from some colleagues earlier. While Harvard employs non-citizens (I work with several), it does not sponsor visas. 
We have a desktop application in C++/CLI and C# which uses ADO.NET for database queries where the queries are expressed as SQL statements. We're also developing a Play Framework-based application that needs to query the same database in similar ways, so Anorm was our choice as the queries could be used almost verbatim.
I suggest you take a look at Slick: http://slick.typesafe.com/ Also, you might find this comparison of relational DB libraries for Scala interesting: http://manuel.bernhardt.io/2014/02/04/a-quick-tour-of-relational-database-access-with-scala/ 
What's better depends on your experience and use case. Personally I think SQL is any time better than an ORM because we can optimize it and use specific database feature. The problem with using plain SQL is that it's annoying to write SQL inside scala files, so I created a sbt task that compiles a file containing list of SQLs to a scala class. So it's like how the view files are used in play, instead of writing HTML chunks in the controller, we put the HTML in the view files and sbt compiles the view files into scala classes. Then in the controller I can simply import the generated scala class like this val userInfo = sqls.User.getInfo.on('username -&gt; "user123").apply.head
My mistake, thanks for the correction. 
As long as you're happy like this enjoy being downvoted. For everyone else is a useful sign to ignore what you write, so we're happy too. To be honest, I don't care about your needs, even though I'm sorry for you.
Anyone have working steps to get a scala.js project working in Eclipse? running "sbt eclipse" just gave me an unhelpful error when I tried to import the project. I forgot what it said but something like "project definition is invalid." I have to work with JS all day so that autocomplete is making me drool.
Aside from the libraries already mentioned, sqlTyped has a pretty innovative approach: type safe string queries via macros. There's also the precursor to Slick which still completely rocks -- no surprise Zeiger was hired by Typesafe. Plenty of choice, dig around and see what works for you.
MyBatis is always under-esteem when talking about ORM. It does respect SRP, it does separate well the queries part from the code, it does respect SoC, and probably other SOLID principles, it is lightweight. Among other things it allow an expert DBA to optimize without knowing the esoteric language/framework of your choice. These features make me prefer it almost every time because in some point of the application life you will need one of them. The biggest drawback: you have write SQL. https://github.com/mybatis/scala
Sorry, but that's really ugly. So much pains to *not* write SQL?
Scala's either is not right biased, therefore, broken. 
Am I missing something? * Type safety * compile time checking on query grammar * no string concatenation * Active record pattern for CRUD * auto-generation of classes from existing schema, including stored procedures, tables, views, keys and sequences * fluent dsl which follow sql-like BNF grammar * generates SQL strings which can be cached * can still be used to run arbitrary SQL strings * No ORM-impedence I get that beauty is in the eye of the beholder, but please enlighten me as to * why you find jooq not to be a beautiful approach * what you find more beautiful
Awesome presentation, man. Enjoyed the hell out of it. 
&gt; what you find more beautiful I like maintaining my queries and mappings in an external config file like XML ala MyBatis. Call me shitty enterprise programmer but I don't really like embedding queries in code...
&gt; Scala's either is not right biased, therefore, broken. Well, let's unbreak it then! implicit class RightBiasedEither[A,B](e: Either[A,B]) { def map[C](f: B =&gt; C) = e.right map f def flatMap[C](f: B =&gt; Either[A,C]) = e.right flatMap f } Now you can chain Eithers to your heart's content. The minimal solution is sometimes better than the kitchen sink solution.
This thread has been linked to from elsewhere on reddit. - [/r/Bitcoin] [Buttercoin recruiting Scala developers \[x-post: /r/scala\]](http://np.reddit.com/r/Bitcoin/comments/1z93tz/buttercoin_recruiting_scala_developers_xpost/) *^I ^am ^a ^bot. ^Comments? ^Complaints? [^Send ^them ^to ^my ^inbox!](http://www.reddit.com/message/compose/?to=totes_meta_bot)* 
Nice presentation, the code completion in particular looks very appealing, yum ;-)
Hmm. Well in my case, I'm doing enterprise QA where I have to deal with thousands of tables scattered across far too many schemas. For me, coupling the queries with the code isn't an issue since the purpose of the class I'm usually working in is to encapsulate the complexity of a desired persistence behavior, e.g. find or create a widget. The real benefit for me is the auto-generation of the persistence building blocks, including the ability to create POJOS with JPA annotations and JSR 303 validation. Being able to write queries with autocomplete for table fields is a real joy, and compile time checking for field name correctness is sublime. Generated active records for each table with a built in store() method are just gravy. Don't see you'd prefer externing strings into XML as that's pretty much the exact opposite of compile time checking and I'd be losing a lot of other functionality. When combined with Jenkins jobs to create the generated building-blocks projects, refactors for field names and store procedures are automatically refreshed by the generated code. If any of my code is using them, I get compile time errors in the build/commit job showing me exactly where the query needs to be fixed. Then consider the use case of selecting a record based on a set of criteria. It's pretty easy to combine criteria using a builder pattern, whereas with a mapper, wouldn't I need to have a named xml query for each combination of criteria? When the critiera can span multiple tables, this leads to an unecessary explosion in xml size, whereas in jooq, I can just programatically combine the criteria using a fluent api.
What would it take to run the Scala compiler in javascript (given that you added a virtual filesystem and the JDK dependencies)?
I think you were trying to reply to http://www.reddit.com/r/scala/comments/1z9byz/platformexecuting_scala_utilities_for_native/
&gt; Call me shitty enterprise programmer but I don't really like embedding queries in code... Why not? Shouldn't your classes have low coupling and high cohesion? How does pulling queries out into xml files do anything good (except require referencing an extra file?)?
Probably a lot of grunt work to take the Scala compiler and strip out all the JVM dependent things (e.g. the threaded parallelism part) Another approach could be to take the [Doppio](https://github.com/int3/doppio) javascript JVM and use it to run scalac. My understand is that they are already most of the way there, so you would just need to implement the last bits necessary to get it working.
What is the name of that color scheme in IntelliJ? It's quite nice! 
We are, if the person is the right fit. Feel free to send a resume or link to your Github over to jobs@buttercoin.com
[Here is a brief overview](https://docs.google.com/a/buttercoin.com/document/d/1w0JPnGVX5IXzuFWPyJOnLA0jRIbqVKJO4n-CcNBmnkg/edit#heading=h.ovvr8v5z718a), though it may be somewhat dated (for instance, our trade engine is finished being rewritten in Scala, hence our posting in this sub). It's not super in-depth, and unfortunately I'm not a developer so I can't answer any specific questions, but if you're interested in finding out more please email jobs@buttercoin.com, and I can connect you with one of our devs.
Awesome tool. And now I see, between ScalaTags and Scala.js you can generate markup in the same way on both server and client, nice. This is really cool, have you thought about doing Scala full-time? ;-)
It depends what do you need to do. ORM or ORM-like as Slick can be good. I like Anorm approach: "SQL is already the best DSL for accessing relational databases. We donâ€™t need to invent something new. Moreover the SQL syntax and features can differ from one database vendor to another." Abstract over queries is not always required (project not required to run with several DB vendors), and when it's required I find easier and more performant to make app use specific query according env. Moreover Anorm in next Play 2.3 (currently in master) is not only more table (extended testing, some fixed column parsers, ...) but coming with interesting features and performance improvements (SQL string interpolation, dynamic parameter lists, ...).
That looks neat, but [this file](https://github.com/lihaoyi/workbench-example-app/blob/todomvc/src/main/scala/example/Framework.scala) looks kind of scary. Is that going to become part of ScalaTags or or Rx, or something? Being relatively inexperienced in Scala myself, that doesn't look like something I could write on my own if I was just given the libraries as they exist now.
Wonderful. I've played around with it a bit before, macros are definitely very cool, but the current API is a disaster.
May be he meant http://rationalwiki.org/wiki/Pleiadians ;-) are Aliens taking over ?
You're right. Thanks.
Sorry for of topic. "My unit tests" link doesn't work. (please see "My unit tests provide an example of extracting with the withInvokableExe function")
Those are some great release notes.
AnyRefMap sounds like a nice performance boost.
Didn't these both come out of the Erlang agent concurrency model?
What's the deal here? How can you improve performance of a data structure by expecting keys to be `AnyRef` (aka `java.lang.Object`)? :-S
Because you know for sure that you'll never be doing boxing and unboxing; that adds up!
Could you explain more. Given that my key type is `Long`, then for each update and access it will be boxed to `java.lang.Long` in an `AnyRefMap`, too, or not? Where do you gain performance?
In that case you are very lucky because there is also a new `LongMap` implementation which uses unboxed Longs as keys...
from http://letitcrash.com/
Hmm I normally really enjoy her presentations but this one was a mess. Clearly the "disaster" with her slides really tripped her up and she seemed to be extremely nervous or something. I wouldn't bring it up but it made it extremely hard to keep watching and in fact I had to eventually stop. Sad as I am really interested in stuff like this. 
Agreed, was a bit scattered, clearly not bringing her A game like in the Pickles &amp; Spores presentation, which was quite awesome, IMO. Anyway, she must have major chops to be doing a doctorate at EPFL, not everyone can go off track and totally free-style like @paulp without a care in the world ;-) 
Nothing kills a presentation like nerves; nothing induces nerves like infrastructure issues. But you're right; she's brilliant and this is important stuff.
Author here. Let me know if you have any questions/need clarifications/etc. I expect to finish and publish this guide on [docs.scala-lang.org](http://docs.scala-lang.org/) by 2.11 release. At the moment this is a rough feature incomplete draft so it probably contains lots of typos and omissions. p.s. Source here: https://github.com/densh/densh.github.io
Yikes! That part about how identifiers are expanded is a pretty massive caveat. Perhaps there should be more emphasis placed on it?
A version of Play Framework compiled with Akka 2.3 support would be great, so that we can leverage the new features :) Or is there some way I can use Akka 2.3 with Play 2.2.2? This is a great release, I'm looking forwards to using Akka Persistence
So you cannot actually define an AnyRefMap[Long, String], for example?
How do you synthesize a method call? So far your options seem to be: * Build the AST yourself * Tedious * Makes me worried that I'm doing something wrong that will break at any moment * Quasiquotes * Requires Macro Paradise * Sorta kinda sometimes breaks IntelliJ compilation completely * Haven't tried Eclipse ScalaIDE, but I wouldn't be surprised if it was the same there * `reify {}` * Can't parametrize the name of the method
Quasiquotes will be a new experimental feature of Scala 2.11 reflection api. We strongly believe that this is the best api for tree manipulation. Reify turned out to be a failure due to its limited expressiveness and will eventually be phased out once quasiquotes are fully polished (i.e. once we fix referential transparency issues in 2.12). We're aiming at tight integration with IntelliJ as one of Palladium goals. Scala IDE should work with 2.11 quasiquotes reasonably well already. Please submit bugs if it doesn't. 
This is an unfortunate side-effect of a goal to emphasize expressiveness of tree manipulation: unlike reify, quasiquote snippets are not meant to be typechecked on quote by quote basis. On one hand this means that quasiquotes are extremely modular and can be easily refactored into smaller quotes whenever necessary. On the other we had to give up some guarantees. We believe that in future (e.g. 2.12) we'll be able to provide an automatic hygiene and referential transparency for macros and quasiquotes. We haven't done so already because current compiler wasn't designed with this concepts in mind and retrofitting them correctly takes quite a bit of hard work.
Wow, that's a really great article. I was actually just about to look this up, so good timing, too.
Why does there need to be a special `Map` implementation with special operations and whatnot, just to keep up with the performance of the general-purpose `java.util.HashMap`?
AnyRefMap is just as general as java.util.HashMap. You can't put primitives into a java.util.HashMap. [This email thread](https://groups.google.com/forum/#!topic/scala-internals/Fw37NmUUiqs) appears to be an early discussion of the problem and the new functionality being introduced to address it. 
Then why isn't `s.c.mutable.HashMap` deprecated?
You can use it with any types, not just subtypes of AnyRef. That's why it's slower. Also, it's just a tiny bit cleaner to instantiate a Scala HashMap than to instantiate a Java HashMap and call asScala on it. It's a small difference, but it's noticeable, and the performance difference often isn't. 
Hmm. Fair enough. Should the documentation for `s.c.mutable.HashMap` mention the existence of the new, higher-performance alternatives? Also, what about the factory methods on `object s.c.mutable.Map`? Should they be altered to construct an appropriate high-performance map based on the type of its keys?
That's correct. `AnyRefMap` is (effectively) defined as `AnyRefMap[Key &lt;: AnyRef, Value]`.
Thanks for this - there's a serious lack of documentation on Scala's macros and this guide looks great. 
There's definitely some very cool stuff here but has the project improved its organization after it was split out into different sub-projects? I remember trying to play with this a couple months back and had a hell of a time trying to get the different parts (nak, chalk) to integrate. The inconsistent documentation didn't really help either, so I eventually just gave up.
Cool find will certainly add this to my bookmarks of cool libraries. Hopefully at some point they get some more of the SciPy stack like Matplotlib and Sympy.
It is to let the language evolve. From the top of my head, reasons to break binary compatibility in the future: - Vararg tuples and functions - Invokedynamic/method handles - Collection library overhaul - Reflection API overhaul - Macro API overhaul In the end, there is always some reason to break compatibility and preserving means not having a single thing changing *ever*. If you change even a single thing incompatibly BC is already gone, and there is no reason to not just improve and fix all the things.
DARPA is pouring money into SciPy and Numpy to try and do away with the costs that come with matlab. It's hard to justify using anything else, but It's good to see a Scala solution.
Unless you don't want to use python... 
Here is my justification. I'm doing realtime work. For reasons which I've explained elsewhere (http://www.chrisstucchio.com/blog/2013/why_not_python.html ), Python is not suitable for it. Scala is. Also, periodically, I need to solve PDEs and munge statistics. Using Numpy, I dumb big arrays to binary, load them with python, dump the result to binary, load it via scala, and continue. Suffice it to say, it's fugly. I just want to solve a diffusion equation, I don't want to deal with serializing stuff. Breeze is pretty nice for that. 
Linear algebra is substantially slower on the JVM than in Python though (which uses Fortran and C libraries behind the scenes)? I don't see any argument for the JVM for high performance numeric computing. And for small, one-offish sort of linear algebra/math stuff, Python seems substantially better (better libraries for numerical applications + better plotting).
Excellent tutorial.
Or unless you want to use Scala and other Scala / JVM libraries...
Great post!
s/Scala/Ruby
How does performance compare to other JSON libs? P.S. it looks pretty neat.
This is called F-bounded polymorphism and in my experience it's not worth it. If you find yourself going down this road, pause and consider using a typeclass instead. You gain much simpler types, better tools for abstraction, and better separation of concerns. PS - If the typeclass pattern is unfamiliar there's a short tutorial [here](https://github.com/tpolecat/tut/blob/master/out/Typeclass.md). 
Is there a way to provide anonymous implicit objects, as it relates to type classes? Having to name each object can get annoying if you have many of them.
It uses the Scala JSON library underneath. The overhead for dynamic dispatch is minimal. `json.foo` gets translated to `json.selectDynamic("foo")` during compilation and internally the json structure is stored as a `HashMap` - so to answer your question, it is as fast as the Scala JSON parser ...
Nope, you have to name them. This ain't Haskell. 
Cool, thanks. I wasn't sure.
Unreadable font in Chrome on Windows.
Looks pretty normal for me on Chrome + Windows: http://imgur.com/Lx1zs6J
For decent performance it's probably best to explore something like json4s, spray-json, Jerkson, or similar. See also: http://engineering.ooyala.com/blog/comparing-scala-json-libraries The json libs that come as part of Scala aren't intended for heavy usage: http://stackoverflow.com/questions/9406272/why-is-this-scala-code-slow https://groups.google.com/forum/#!msg/scala-user/P7-8PEUUj6A/NonA0_IIovsJ
not related to scala
&gt; def foo: IO[String] = { &gt; writeToFile("foo", "/test") flatmap { _ =&gt; &gt; readFromFile("/test") map {data =&gt; &gt; data + "bar" &gt; } &gt; } &gt; } I believe he's arguing that the IO monad ensures there's nothing between writeToFile and readFromFile, but it seems to me that anything may happen between the two, say, when the kernel preempts our process.
Then you would write a trait, with appropriate data, to enforce that constraint. trait Locker[A,L &lt;: Locker[A]] { self: L =&gt; def apply[A](f =&gt; A): Unlocker[L,A](f) } trait Unlocker[A,L &lt;: Locker[A]] { def apply[A](locker: L[A]): A } To use these traits, a Locker implentation can only produce a Unlocker, and the only way to get A is to use the locker. In the example above a Locker would lock a file with whatever locking mechanisms the operating system will respect. Evaluate f =&gt; A which in this case is foo. Produce a Unlocker with the result from Foo. The Unlocker implementation will unlock the file during apply before returning A. With this the only thing you can do once you perform a lock is to then perform a unlock if you are ever to get your value.
You can settle on some simple convention, e.g. `&lt;typeclass&gt;&lt;type&gt;Instance` or just `&lt;typeclass&gt;&lt;type&gt;`.
The first test is somewhat broken. The foreach isn't really doing anything, the map creates a new list, and the for just does an in place update on the list. The for should be for { i &lt;- data } yield { i + z } I am surprised the scala compiler isn't optimizing out the foreach, since it does nothing. The second test is probably worse than the first. Again, the foreach isn't really doing anything and the foldLeft isn't creating a list. To have actual tests you would need foreach: var vec = Vector.empty[Int] (0 to n).foreach( vec :+ _) fold (0 to n).foldLeft( Vector.empty[Int] ) { case (v, i) =&gt; v :+ i } I'd also use Vector instead of List (as in the examples above), since Vector is now the preferred Scala collection (and is also immutable, which would have prevented his original for from compiling). 
Basically it boils down to there is some parameter required that is passed in "implicitly". It will come from somewhere in the scope, without you specifically passing it to the function. There are rules for where it looks, but it starts out locally then works out to more generally. Basic example is a logger that you want to use in your classes. Rather than just calling some Logger object in all your classes, you take as part of your class an implicit Logger trait. In testing, you can then define a Logger for testing using an implicit definition, in light volume applications you can define a logger that writes to files, and in heavy applications you can define a logger that writes to hdfs. You never have to pass in these logger objects to your class, just define the implicit logger at your main entry point.
The implicit scope is a special context that Scala treats as a bucket of items that can be looked up by type. For example, if an argument to a function is declared implicit, it means you can call the function and omit the argument, and Scala will check the bucket to see if there is a matching implicit value of the right type. scala&gt; def neg(implicit x: Int) = { -x } The function neg takes an Int. scala&gt; neg(42) res0: Int = -42 scala&gt; neg &lt;console&gt;:9: error: could not find implicit value for parameter x: Int neg So far, I can't call neg without an argument. But that argument happens to be marked as implicit, so I can omit it if there's an available Int in the bucket of implicit values: scala&gt; implicit val i = 42 i: Int = 42 scala&gt; neg res1: Int = -42 Implicits aren't terribly useful as shown in this contrived example, but they become super handy when you want to use [ad-hoc polymorphism](https://github.com/earldouglas/scala-scratchpad/tree/master/type-classes#type-classes-in-scala) or create [infix functions](https://github.com/earldouglas/scala-scratchpad/tree/master/infix#infix-functions-with-scala-type-classes).
Thank you much for the answer. I've got some followup questions: I don't really understand what you're saying about searching the scope for the implied definition. Can you say more about that? In my tinkering a few minutes ago, if only one implicit definition was expected but there were 2 defined (even defined at different scopes) I got a compiler error. I'm not sure how stages of scope would factor in given you can only have one definition for it. I like the idea of using it for a logger. Can you give a little more detail on 'take as part of your class an implicit Logger trait'? I'm not quite sure what that entails.
There are a couple of cases: * Implicit conversions can be very helpful in switching a "something" into a type you find useful. * Implicit vals are useful when you want a context object or need to pass around a "helper". I wrote a blog post about this that gives examples: [Problems Scala Fixes](http://tersesystems.com/2012/12/16/problems-scala-fixes/).
I felt the same way when I was new to functional programming. As I learned about Haskell and its approach via type classes, my approach to programming in Scala gradually changed, and now I find it indispensable. It's a pattern that, if you don't know about it, isn't of much importance to getting things done. Once you get familiar with it, and start to apply it to your projects, you'll find it hard to put down.
Ya know, it's funny... I'm actually *not* new to functional programming. However, all of my professional experience is with MUMPS, which wouldn't recognize a datatype if one bit it on the ass. It's not the functional aspect of Scala which I find the most challenging to wrap my head around, it's the type system... in fact, Scala's 'functionality' is why it's quickly winning my heart over Java.
Don't worry, considering that Scala's type system is Turing complete, it's no wonder that it seems complex!
Here is an example. Lets say you want to make a bunch of pre-existing classes "addable" in a generic way. You might define an implicit: trait Addable[T] { def plus(x: T, o: T): T } Objects of type `Addable[T]` are called "typeclasses". Then you define a pimp which takes an implicit typeclass: implicit class AddablePimp[T](val x: T) extends AnyVal { def &lt;+&gt;(other: T)(implicit addable: Addable[T]): T = addable.plus(x,other) } What it means is that for any type T, if the compiler can find an implicit instance of `Addable[T]`, it will allow you to call `t &lt;+&gt; o`. For example: implicit def OptionAddable[X] = new Addable[Option[X]] { def plus(x: Option[X], y: Option[X]) = if (x.isDefined) { x } else { y } } Now you can do this: Some(3) &lt;+&gt; None === Some(3) None &lt;+&gt; Some("foo") === Some("foo") If you want to make Lists or Futures addable, you can just define a `ListAddable[X] = new Addable[List[X]]`. And if you want to write code which depends on addability but not the specific properties of the thing being added (e.g., generic caches), all you need to do is require an implicit Addable. (This is how Scalaz works.)
&gt; pimp Just as an aside, "pimp" is no longer the recommended term, due to its sexist connatations. 
Scope is a CS term, which defines allocations similar to russian dolls. object Outer { implicit val o1 object Inner { implicit val i1 object InnerInner { implicit val ii1 } } } In that example, there are three scopes, with inner being in side outer, inner inner being inside the scope of inner and outer. If your implicit function included the scope of inner inner, you'd have an implicit resolution problem. Here's an example of a class taking an implicit logger. class MyAwesomeRoutingService(val errorResource: URL)(implicit val logger: LoggingTrait) { logger.debug("I've instantiated my service") val routes = ( get("index") -&gt; displayResource("index.html") ) + ( unknown -&gt; { log.warn("unhandled route") } ) } Then when you initialize that class you would just need val service = new MyAwesomeRoutingService("whalefail.html") The logger could come from any scope. You may decide to define it way up at the application entry. object MyRestApp extends App { implicit val logger = HigherPerformanceeLogger() //extends logger trait val server = new HttpServer() //will eventually create MyAwesomeRoutingService } 
because they repeat the Ubuntu mistake, i.e. Bankrupting themselves for the Tablet screen format. ;-)
&gt; "gee, this seems like an excellent time to use ad-hoc polymorphism or an infix function!" I've probably never said those exact words, but I have frequently found myself in a situation where I'd like to be able to (conceptually) add a method to a bunch of third-party classes that I don't control and can't change. Or where I'd like to group a disparate set of classes I do control in a way that regular inheritance would make messy: to tag a bunch of classes as "presentable" in a UI, or "serializable" in some way, for example, without having to shoehorn a common trait into them. Type classes really help in those cases.
wait, Haskell has implicits? Or do you mean 'just' type classes?
This is stupid in so many ways. The author doesn't understand the difference between Scala's for and while, nor does he understand that for is just syntactic sugar for map/flatmap/withFilter. He doesn't understand that his example with for is looping over a Range where his other examples are looping over an Array (or whatever Seq data is). As a result, his conclusion is false. TLDR: Oh god, down arrow, down arrow...
"Enrich" is a terrible term. Who wouldn't want to "enrich" something? The term "pimp" hints this is something weird and possibly dangerous that you should think twice about. I wish it weren't the case but labels affect people's thinking. I'll stick to pimp for now - I'd rather offend the politically correct than express my ideas unclearly. If we adopt a term like "monkeypatch" I'll switch simply to avoid conversations like this. I could argue that "monkeypatch" is a better term since it actually gives a hint at the purpose. But s/pimp/enrich/g hinders communication in my view. 
I plan to move to json4s for Scala 2.11 since they are deprecating the default parser in 2.11. That being said, I think @shoelacestied was asking the performance overhead for dynamic dispatch ONCE a string has been parsed to a json object.
Another take on the same subject: http://blog.astrac.me/coding/2014/02/19/akka-actor-retry/
I meant that Scala implicit parameters can be used to emulate Haskell type classes. I don't actually have a lot of Haskell experience, so maybe it does have implicits, but I'm not aware of that.
One example of the awesomeness case class Apple(id: Int) case class Horse(id: Int) trait Crud[T]{ def db: String } object Crud { implicit val apple = new Crud[Apple]{ def db = "apples" } implicit val horse = new Crud[Horse]{ def db = "horses" } } object Db { val db = new { def exec(sql: String) = ??? def getAll[T](implicit crud: Crud[T]): Seq[T] = { exec("Select * from " + crud.db) Nil // dummy } } // find all Apples db.getAll[Apple] // find all Horses db.getAll[Horse] }
Use [bugmenot.com](http://bugmenot.com/view/skillsmatter.com) if you want to skip registration.
Thank you. John Hughes is always a pleasure to listen to. We've recently started to use Riak at work so this was fun and interesting to watch.
I had no idea that Scala could do something like this. Very cool! Scala continues to surprise me.
I've only tried SCCT, but that's still based on line coverage. It was also annoying since they didn't seem to have a proper release for the current Scala versions, instead forcing you to rely on -SNAPSHOT versions. EDIT: Seems like Scoverage is a fork of SCCT.
Is the documentation updated? The front page ones seem to still be 2.9.x
I've seen this: http://timezra.blogspot.com/2013/10/jacoco-and-scala.html In theory, I haven't actually done what he indicated here, but you should be able to use jacoco with scala successfully.
2.10.4 is strictly a bug-fix release
Everything is written in topic link. 2.10.0 had plenty of new features.
So no updates to the PDF documentation planned, I see.
In case you find it helpful, here's how I set up scoverage in my projects: [github.com/earldouglas/scala-ci#scoverage](https://github.com/earldouglas/scala-ci#scoverage)
Pardon me to think that the http://scala-lang.org/files/archive/nightly/pdfs/ScalaReference.pdf should describe the language the compiler available at http://scala-lang.org/download/ compiles, instead of the previous version. Anyway, from the downvotes I am getting just by asking for accessible up to date documentation, I see I don't want to spend time in such welcoming community.
This is the latest version https://github.com/adriaanm/scala-ref-markdown
Funny like you got the answer as soon as you asked the question in a way people actually understood.
Have you gotten any integration with SonarQube working? Sonar is the one thing I miss from my Java days, but I haven't found any good Scala support for it yet.
The blog answers its own question with a "no".
Hi all. For now I managed to integrate scoverage, scalastyle and sbt with jenkins. All of them have jenkins plugins and scoverage also generates cobertura compatible xml so it can be also used with cobertura plugin. JUnit compatible output of test results can be generated by adding the following to build.sbt testOptions in ThisBuild &lt;+= (target in Test) map { t =&gt; Tests.Argument("-o", "-u", t + "/test-reports") } finally, eclipse project files are generated using sbteclipse. I have not worked on SonarQube integration yet and also on integration of scoverage with Eclipse. 
http://en.wikipedia.org/wiki/Betteridge's_law_of_headlines
No. Because Scala is not a "Java with lambdas". 
[Ahahahahahahaha no.](http://youtu.be/gnvhFhHtvYc)
Have you seen Java 8? Java 8 looks like an awesome reason to switch to Scala.
Why the downvotes ? Bad case of irony intolerance ? The talk is actually good, and promotes functional programming while pointing out some issues that you may run into...
Came here to link this, nice work :)
Would love to hear why you think this.
SBT.
For me, the benefits of scala are, in decreasing order of importance: 1. The collections lib, with methods like map, foreach, filter, grouped, groupBy, toSeq/Set, par, being ubiquitous. Hard to estimate how much of the gain of scala is due solely to this. 50% conservative estimate. 2. concise lambdas 3. type inference 4. immutability being the default, rather than the other way around And then there's implicits, typeclasses, unlimited method naming, weird syntax support for weird things, xml support which are alternatively #5 on that list or #1 on the list of things that make scala worse, depending on what was done with these features.
&gt; but most of the interest in scala I've seen hasn't come from java developers in the first place. That's the opposite of what I've seen, though I work in a mostly-JVM shop. If Scala didn't run on the JVM and interoperate very well with Java me and my colleagues would never have used it.
is amazing.
Hear, hear. That SBT gets presented as *the* way to build Scala hurts Scala adoption.
That's not an answer to the question I asked. 
Gradle 
SBT is problematic because it randomly blows up when compiling, not so much for the package manager functions. 
Awesome overview, thanks!
No prob.
Have you tried the recent sbt project support in latest versions of the Scala plugin in IntelliJ IDEA? 
I just use Ant. Have no issues.
My IntelliJ-using coworkers say the SBT support in Intellij is wonky, but getting better. Who knows what that means, but in any case, based on past experience, I have no desire to use IntelliJ for anything. I was able to get a Build.scala loaded in Eclipse for the first time in years by jumping through some non-obvious hoops on the SBT command line. That's something, but the experience is still bad.
I think the problem is people trying to re-write existing projects to build with sbt. If I'm starting a new scala project, I would choose SBT every single time. But porting over a large project from maven to sbt is not trivial.
Wait, how did you get Zinc working with Maven? As far as I knew, the only way to use it that way is with `scala-maven-plugin`, whose Zinc support is horribly broken (brings in `scala-library` 2.9 and 2.10 at the same time, which obviously doesn't work).
I tried Zinc with Maven about a year ago, and couldn't get it to work. I don't remember if the issues were the ones you decscribed, but things were just borked. Now, to my pleasant surprise, it was as easy as &lt;useZincServer&gt;true&lt;/useZincServer&gt;. 
My opinions: --- ## Maven is bad. ## * The POM is ludicrously over-verbose, not at all extensible, contains way too many different bits of information, and generally sucks. * The API (for e.g. writing plugins) is a horrible undocumented mishmash of a decade's worth of workarounds. There are like three different dependency-injection systems being used at the same time, for fuck's sake. * The command-line syntax is very hard to understand and mostly nonsensical. Passing parameters to plugin actions is done in a *very* bizarre way. * There is no standard way to run a Maven project. * Because Maven's lifecycle phase system is so inflexible, there is no clean way to make the Scala compiler run before the Java compiler runs (which you need in a mixed-language project). ## SBT is terrible. ## The syntax of SBT build files is justâ€¦I mean, mandatory blank lines? Are you *kidding* me? ## Gradle is terrible. ## Can't even set the `artifactId` of my project without putting ridiculous hacky workarounds in like three separate configuration files. ## Ant is terrible. ## Its build files are not even remotely declarative. 'Nuff said. ## Code signing doesn't exist. ## *None* of them bother to actually check signatures on the artifacts they download. Your computer could easily be jacked full of malware with every single build, no matter which build system you use. --- To be fair, they also have upsides: ## Mavenâ€¦ ## * â€¦has good support for multi-module projects. As long as there are no circular dependencies between the modules, Maven will build them in the correct order. * â€¦has a sane basic syntax for its build files, i.e. XML. It's well-defined, consistent, and unsurprising, with no syntactic noise or gotchas like SBT's mandatory blank lines. It's just XML, not a programming language DSL pretending to be a serialization format. The POM has its problems, but the basic syntax isn't one of them. * â€¦has a local repository that build artifacts can be installed into, which your other projects can then depend on. If you work on many loosely-coupled projects at once (like a generic library and an application that uses it), this is a life-saver! * â€¦can generate lots of neat reports. You don't actually need Maven generate reports on your project, of course, but it's sometimes nice to have them all in one place. * â€¦can download plugins automatically, just like it can download regular dependencies. You can even invoke a plugin from the command line and Maven will downloaded it if necessary. ## Gradleâ€¦ ## * â€¦arranges build actions into a DAG. This seems to be a much cleaner and more flexible approach to constructing a build plan than Maven's lifecycle phase system. * â€¦has a more terse configuration syntax than Maven's (provided you don't need to do anything even remotely out of the ordinary). * â€¦lets you split project configuration into multiple files. For instance, I might configure Gradle to look in my company's Nexus repositories in a separate config file, and just copy that one file into all of my projects. In Maven, I have to paste a snippet into the POM instead, which is kind of ugly. ## Antâ€¦ ## â€¦umâ€¦well, it's better than `make`, I guess. ## SBTâ€¦ ## â€¦I got nothing. SBT is just terrible. --- In conclusion, I hate them all, but Maven is the least bad of them. In case you're wondering, Maven has the most bullet points because I have the most experience with it. I tried using Gradle until I ran into one too many stupid problems, then gave up on it. As for SBT, I took one look at its build file syntax and noped right on out of there. If I had endless free time, making a new, sane build system would be one of my projects. This situation fucking sucks.
Were you using it with Scala 2.10?
Not before, I am now. The pom: &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;recompileMode&gt;incremental&lt;/recompileMode&gt; &lt;useZincServer&gt;true&lt;/useZincServer&gt; ... Extra stuff elided, but that's it. I was amazed.
And does Zinc still work?
Yes, that's the whole point. It even gracefully falls back to a normal compile if a Zinc daemon isn't running. Incremental compiles with small changes are on the order of seconds. mvn clean install Now takes about 2.5 minutes, down from 4.5.
I need to be running a daemon to do incremental compiles? &amp;#3232;_&amp;#3232;
It's a great headline, actually - even if you may not agree with it
It's not descriptive at all of what the article will or will not be. No indication of if it's well reasoned, or clickbait.
It's a headline, not an abstract. It's supposed to make you click on it. And given the upvotes, it seems to have worked... So: good headline, no?
No, but Java 8 means being crushed between Java at the bottom and Scala at the top for languages which are mostly Java-with-lambdas like Kotlin, Ceylon, Gosu (does it still exist?) ...
Most people don't click through and simply read the headline and that's the takeaway for them.
My answer: No, because Spark and Kafka, two of the most important technologies going forward, are based on Scala.
That pdf is up to date. You seemed think that version 2.9 of the language specification is analogous to version 2.9 of the scala library, which is is not. If you read the document in whole, you'll see the contents of the document reflect language features found in scala-2.10.
If that is the case, maybe an explanation for outsiders would be nice to have. Since 2.8 I haven't done anything Scala related and was trying to find an updated document with all the improvements of the actual stable release, without having to search multiple pages for it.
No. You should perhaps read the Zinc page I linked, it spells out all this stuff . The daemon keeps a warm compiler around, which provides huge speedups (about 2x) even when doing clean rebuilds where incremental compilation wouldn't help.
Ah, whenever I use SBT. I do all of my configuration with .scala files instead of .sbt files. The blank lines are ridiculous, but at least it isn't just a lot of noise, like you have with xml files.
This is a terrible comparison. Most of what listed as postitiives for Maven applies equally well for SBT and Gradle. You admit to not having used SBT and your criticism of Gradle isn't true. This seems to boil down to 'I like Maven syntax the best'.
My criticism of Gradle absolutely *is* true. I had to fucking Google for answers on the matter, and only after a hell of a lot of looking did I finally learn just how bad the solution is. Basically, I can set the project organization and version, but not the name. For no apparent reason. Unless I go to ridiculous lengths to override it. If I don't, it's taken from the name of the folder the project is checked out in, which is pure madness. Mind you, I am talking about changing the project name itself, not just faking it for POM generation. I don't want any nasty surprises about where else the project name is used, and I don't believe in fake-it-'til-you-make-it software development. The project name should be configured *once* and used everywhere, and if I can't even have that, then your build system fucking sucks. And no, I don't like Maven syntax the best. I like Gradle syntax the best. But the horrid hacks I have to put into it, for what is *basic functionality* in Maven, is not only unacceptable, it's fucking pathetic.
Has anyone been to a LambdaConf before? Recommendations for/against?
Good talk. Maybe a little overboard with the whole "the alternative is impossible" point of view. Clearly the alternative is not impossible or else 99% of the software we use today would not exist. But yes, I agree with the summary that happiness through reduced cognitive load and increased predictability via FP is a very good thing.
My apologies that you had to 'fucking Google' for an answer. However, a brief Google search indicates that [this](http://mrhaki.blogspot.com/2009/11/gradle-goodness-changing-project-name.html) is how you rename a project. That doesn't seem that hard. Am I missing something?
Yeah. I have to put it in a separate file, away from where the group and version are configured. That is bullshit.
The method for renaming artifactIds in Gradle that your link describes is pretty bad. I frequently have several branches of a project checked out, each one produces an artifact with the same artifactId, just different versions. With Gradle, I'd need to litter settings.gradle everywhere, or introduce superfluous directories all over the place to make my checkout dirs have the "right" name: 1.x/myproject/ 2.x/myproject/ 2.2-RC1/myproject etc. 
Wait, that is the real problem. Gradle is so "unacceptable, it's fucking pathetic" because you have to put the name of the project in another file? Wow, just... wow.
&gt; Why would you want to rename the artifacts for this? I wouldn't. I'd want the artifactIds to be the same, with different versions. The kooky thing that Gradle does is take the artifactId from the folder containing the project, unless you jump through some hoops. With Gradle, I couldn't check out version 1.0, 2.0, and 2.2.foo into dirs myproject-1.0/ myproject-2.0/ myproject-2.2.foo/ because doing so would give each checkout its own different implicit artifactId (myproject-1.0, myproject-2.0, myproject-2.2.foo, respectively, when the artifactsIds should actually all be myproject), which I would not want.
Sorry, I misread your original post. I can certainly undestand that being an annoying default. However, isn't that solved by setting the project name in settings.gradle in to your project? Isn't this only a problem if you don't specify the project name? Granted, you would end up with a copy of settings.gradle in all of your checked out projects but that is true of all of the other files in there as well. I still don't understand why that method of renaming artifacts is bad.
&gt; don't understand why that method of renaming artifacts is bad. Because other build tools, even Maven - which is not known for making developers' lives easier - don't require sprinkling around extra settings files to do something simple and common. What's worse, the need to do that is an (accidental?) side-effect of the Gradle dev's choice to make one use case "easier" by guessing artifact ids in a rigid way.** Needing to make a settings.gradle file isn't the hardest thing, but it's one more thing to have to do, and have to remember. It's one more thing that can fail silently (by making the wrong artifact) and have to be discovered and dealt with. Builds are enough of a pain in the ass without that. **: I'm reminded of how Groovy got dynamic dispatch. It didn't get that feature as a result of a deliberate design process, it got it as an accidental side-effect of hacks needed for Java interop.
Fair enough
The language reference is of little use to outsiders or novice users. Even a cursory glance at the the specification would reveal that. Unless you're are looking for a exact definition of the domains of language features(ie, you're writing a grammar or a compiler, dealing with corner case language features) the specification isn't appropriate. There are however plenty of other introductory text on the scala-lang website like the [guides and overviews](http://docs.scala-lang.org/overviews/).
@paulp giving a workshop on tradeoffs of FP language design, interesting. Wonder if he and Scalaz team are going to join forces and kick off their version of Scala.Next, or just continue to bitch about the state of Scala.Now while EPFL, Odersky, and co. crank away on [Dotty](https://github.com/lampepfl/dotty) Regardless, Scala as we know it will have radically improved in 2-4 years if all the talk translates to action.
I would hazard a guess though that in many of the hugely expensive failed custom software projects in history, the use of imperative programming, together with a large cohort of programmers who didn't know how to do imperative programming properly, may have played a role in the project failure.
implicits in Haskell http://www.haskell.org/ghc/docs/latest/html/users_guide/other-type-extensions.html#implicit-parameters 
Now, if Google re-engineered Guava to have Java 8 features, that might be give Scala some pressure.
https://github.com/henrikengstrom/akka-meetup-sthlm is another great place to sharpen your Akka skills.
Duplicate - http://www.reddit.com/r/scala/comments/21tyht/httpwwwdzonecomlinksrthis_week_in_scala/
https://github.com/RadoBuransky/sonar-scoverage-plugin
Holy shit. I'll give this a try today. Thanks!
Any open-access options?
There seems to be no direct link to the [source code](https://github.com/wookietreiber/scala-chart).
Replaying journaled events? That sounds hilariously slow, hilariously inefficient with disk space, and hilariously brittle, all at the same time.
- Snapshotting: replay from last known snapshot (but still keep the option to rebuild from scratch if your business logic changes and you want to retroactively apply it!). Bonus: snapshotting happens async. - Brittle: event versioning is a thing (serializer is pluggable through Akka, so e.g. Protobuf), just like schema-migrations are a thing with your RDBMS - Append-only stores (like LevelDB) are made for this scenario, heavily optimized for writing and reading sequential streams of events. - Disk space is cheap. Why throw away information. But then again, I agree that this article is a bit over the top. There are no silver bullets, but event-sourcing is a damn nice pattern for distributed applications. Would be a shame to dismiss it out-of-hand based on your initial observations.
If you have snapshots, then you must already have a database to store them in. So what have you accomplished?
Meh. HFT is a scam anyway.
I have to wonder if the author even really understands what Akka persistence does. There's no database you can query. You can't arbitrarily retrieve data from an index using a predicate. You can't do anything database-like. All it does is store the messages your actor received so it can replay them if the actor has to restart, to get the actor back to the same state.
No. It will only make it better. Seeing as 1.8 presumably adds VM-level support for method references and lambdas, the Scala compiler will be able to leverage those. 
Not necessarily. When using ES you should keep all your aggregates in memory (and maybe use sharding it does not fit in a single machine). To save snapshots, you could then just serialize all the aggregates directly into the file system. This should be faster than DB insertions.
You forgot backward compatibility with JRE 6 and Android. Also, translating a case class into a mutable bean is misleading.
&gt; I am not even slightly tempted to replace Scala as my main language for my next project with Java 8. You are not the target audience. You already decided to pick Scala as your main language, good for you, but you are not part of the community that decides whether Java 8 will kill Scala or not. That community is the 99% of JVM users who are using Java today and who are potential future Scala users. And I think very few of these will actually switch to Scala and a good proportion of them will migrate to Java 8, so in one sense, I do believe that Java 8 "killed" Scala by shrinking the pool of potential adopters. Having said that, I don't think the concept of "killing" Scala makes much sense since the language itself has less than 2% mind share on the JVM. In that sense, Java 8 is going to kill Scala in much the same way it is going to kill Clojure. 
Case classes can also be mutable, I should note. Edit: In case you're wondering how, just prefix the constructor parameters with `var`, as in: case class Foo(var x: Int, var y: Int) Of course, this is usually not the best idea, but it's there if you need it.
can't even load the page.
&gt; A simple ray tracer, taken from the PyPy benchmarks ... Half the lines of code Nice work, [shows](https://github.com/lihaoyi/workbench-example-app/blob/ray-tracer/src/main/scala/example/ScalaJSExample.scala) what an expressive language we've got.
&gt; has less than 2% mind share on the JVM It seems to have all the mind share that matters if you have a look at the features in Java 8.
I am on the pool of Java developers mention on the OP post. At home I am free to use whatever I feel like it, of course. However, the dark enterprise world equates JVM = Java and CLR = C#/VB, so those are the main languages we get to use. I would rather use Scala/Clojure/F#, but those languages don't fit the enterprise consulting world wish of replaceable developers. So I jump of joy for the opportunity to have Java 8 at my disposal, while a the same time cry that our newest greenfield project requires Java 6! Nice remark about Scala compile times. I think whoever complains about them never had to endure a 3h build in C and C++ projects with a *make all*.
Fully agree, unless there is a killer framework that requires enterprise developers to touch Scala or Clojure, those languages will be only part of startups or cool companies Facebook style. The boring enterprise typical LOB ones won't use other languages on the JVM. It doesn't pay off in terms of outsourcing or getting cheaper developers.
&gt; ]: It should be noted that the compilation times for Scala projects still tend to be much lower than the compilation times for C++ projects, for instance. True, but that's an argument that Java was already using against C++ in 1996. All the old timers such as myself who had to endure a decade of C++ programming saw Java as a breath of fresh air when it started emerging because not only were compilations super fast, Java also allowed you to write programs without fighting the compiler at every step along the way. 
I still think it is worth mentioning. I have seen people ask how compilation times compare between C++ and Scala, and if they are about the same. Scala's compilation times are bad, but they are in general still far better than those for C++. I also didn't use it as an argument for Scala, but described it as one of Scala's main challenges, since it decreases productivity in general significantly. With the work towards simplifying the various aspects of the language (XML literals, Dotty, etc.), I think that compilation times may be improved considerably, though not necessarily as good as one might want.
I hope developers will someday understand that once you embed your model in a database, the database BECOMES your model's representation. In a database, the nature of your model's representation changes, and you have to therefore treat the data differently than you were if it were a program object. You have to go from the getter / setter / object-y representation to a select / update / sql-y representation. That's just the tradeoff you make, and the closer you live to that reality, the simpler your program (and life) will be. Don't try to abstract away this essential reality. Data is data, data is simple, and the more refracted we make our interface to it, the more difficulties will arise.
Indeed, takes awhile ;-) The fully rendered page is fairly hallucinogenic (seriously), so the wait is somewhat worth it.
READ THIS NEXT: Why Java Is Still Relevant
via https://twitter.com/odersky/status/452121724449226752
This is a great read and especially good advice for someone like me who's just starting out in Scala and trying to grasp functional programming.
http://www.jetbrains.com/idea/features/scala.html
Would love to see a longer followup by Heather Miller's to her talk on Scala Type systems. Her talk was way too interesting to fit in 1 hour and she had to move quickly to keep it on time. Very interesting topics.
&gt;Weâ€™re looking for software engineers who understand the functional domain extremely well. Ideally you will be a polyglot software engineer with skills and at least twelve months (commercial) experience in Scala (with with Akka) || Clojure || Haskell || NodeJS. We run all of these in production so itâ€™s important you understand the difference between experimentation and production quality code. You should also be able to at least read one of either Python, Bash or C. We donâ€™t allow Ruby anywhere on our platform so you should be comfortable about that. Weâ€™re looking for people who want to live the functional dream, not people who think itâ€™s a â€˜nice to haveâ€™ on their C.V please. Kind of reads like we have no idea we just use whatever we feel like x day of the week.
That's cringe worthy job posting.
We use what is correct for the task at hand. A polyglot environment is a tough but rewarding environment to work in.
Why does the title show dzone.com and I wind up at Cake Solutions? While these updates certainly aren't useless, I don't find a huge amount of value in them either as I run across most of the material via Google Groups, Twitter, HN, and here on Reddit. For others the updates may be of value...just putting forth my 2centimes ;-)
Take a look at ArrayBuffer and ListBuffer in the Scala standard library. It's probably also worthwhile to look into mutable and immutable collections in Scala more generally. You can start here: http://www.scala-lang.org/docu/files/collections-api/collections_1.html You can also use java.util.ArrayList and java.util.LinkedList in Scala programs, though I'm not sure if that would be good form. 
Those for loops are pretty convenient. Thanks for the information!
Thanks for the information!
Spark is nice, but like the article says, configuration can be a finicky.
I think it has a lot of good parts and I think the AmpLabs (and DataBricks) guys will fix some of the configuration issues. Some of the other tools in the BDAS stack have pretty nice interfaces (e.g. Mesos). Anyway, maturity will help the project, for sure.
Not sure exactly what kind of feedback you're looking for exactly (except, thanks for the concise and educational writeup!). It reminds a lot to C++'s template specialization which is something I had always wondered if it would be possible in Scala; I assumed yes, but never really sat down to think about it seriously. Good to know it is :)
That writeup is a result of me trying to understand how Breeze works. After spending a week studying its code I fell in love with this approach. There are two kinds of feedback I was expecting (and was afraid of): * there is a much better way to do this. Here is the link * it is very dangerous, because 2.11 is going to break your code in some unexpected way And one I was hoping for: * Not only Breeze, hundreds of libraries use it already. Don't be afraid and do the same ;)
This is called the typeclass pattern. It's used extensively in principled libraries like scalaz, spire, shapeless, and argonaut. I wrote a little [introduction](https://github.com/tpolecat/tut/blob/master/out/Typeclass.md) a while back that you may find helpful.
As /u/tpolecat says, it's the second one.
In extreme cases implicit search can slow you down because it's a constraint-solving problem (kind of like running a Prolog program at compile-time) and you can trick the compiler into solving [fairly complex problems](https://gist.github.com/travisbrown/3772462) if you're clever. But in everyday coding I don't find that it's in the list of things I think about. I try to limit imports to stuff I actually use, but beyond that I don't worry about it. 
Do you know if there is a good index of all Scala type system patterns? I have been using [this one](http://ktoso.github.io/scala-types-of-types/), which is excellent, but typeclass pattern is not mentioned there.
You can have priorities when you use type classes. You can specify that one implementation should be used over another one, if they both apply.
I don't know of one; I'll see if I can make one! :)
That's a bit of a grab-bag, technology-wise. Why would you use MongoDB for a finance app? And why are you using both Node.js and the Play Framework? As I understand it, they serve more or less the same purpose.
node has the potential to be a presentation layer superior to one written in Java/Scala: it will boot faster and not create a language gulf between the view &amp; model. Let frontend webdevs make the user interface, let backend webdevs make the data infrastructure.
No, I'm using Intellij IDEA. Its "intellisense" seems to do okay.
type mismatch; found : scala.collection.immutable.Set[Card] required: Seq[Card] var deck : Seq[Card] = for(t &lt;- Type.values; s &lt;- Suit.values) yield new Card(t, s) ^ Any suggestions to what this is saying?
Mongo of all things is insane, given how much trouble other exchanges have had with locking (like being spammed with withdrawals with no lock on the thing that holds balances)...with dbs that actually allow locks/transactions. Really this just sounds like a collection of buzz words from 2012, more than a blueprint for an actual product. 
It would appear mutable.Stack is what I needed. I am able to push/pop which handles my cards drawing. Next step would be to shuffle the stack. Is there anyway to perform this? utils.Random.shuffle(deck) The above code doesn't appear to modify the deck at all This doesn't appear to work for ListBuffer either. It's not randomizing. Is there a way to seed this?
util.Random.shuffle returns the shuffled list. The original list is left intact. Try this example: val deck = 1 to 52 toList val shuffledDeck = util.Random.shuffle(deck) println(deck.take(5)) // List(1, 2, 3, 4, 5) println(shuffledDeck.take(5)) // List(12, 6, 48, 7, 41) 
Buzz words or the developers can't make up their minds.
I am not sure I understand what you are trying to do. [Here](https://gist.github.com/nmarshall23/cfa19fc93d83e6bec73c) is some example code. Have you read [**Scala by Example**](http://www.scala-lang.org/docu/files/ScalaByExample.pdfâ€Ž)? PDF download warning.. It really helped me get a good grasp on Scala.
Remember that Scala likes immutability. So rather than modifying the parameters you pass in you'll usually instead get a new copy returned from the function. 
You might want to checkout Play 2.3 and sbt-web. :-)
It means that your for comprehension is producing a Set[Card], but you've told the compiler it should expect a Seq[Card]. You can either declare your deck to be a Set (which are unordered and probably isn't what you want) or change the for expression to return a Seq. There are a few ways to do that. What exactly are the types of Type.values and Suit.values?
`val (hand, remainder) = util.Random.shuffle(1 to 52).splitAt(5)`
Seems that it has not much of a traction here. Is it that not much people use Hadoop? or that there is a better (read: more popular) solution for dealing with this?
I'd ask on the [Scala IDE Dev List](https://groups.google.com/forum/#!forum/scala-ide-dev) IDE support for Macros will be challenging I suspect. Good luck, type providers are a definite nice-to-have
Hehe, Yes, Two different posts from different times, I stand behind both :) 
A more principled treatment is given [here](http://typelevel.org/blog/2014/02/21/error-handling.html).
The blog is a great start to the implementation of what Eric Meijer discussed yesterday at React, ["What does it mean to be Reactive?"](https://www.youtube.com/watch?v=sTSQlYX5DU0)
Please, don't take it the wrong way, I really mean this in a constructive way. But looking at the questions you posted here, it seems to me that you are trying to write Java in Scala. You are still approaching the problems in a very imperative way. You would benefit a lot to learn the idiomatic Scala way, more declarative and functional.
Java 8+ cannot be as bold as Scala is in implementing new features if its not gona break compatibility and we know it wont. So with Java 8 they have opened a Pandoras box to functional programming, soon tycoons will realize the advantages of functional programming and those Java functional developers will move to Scala, specially those who want to capture market and thus make money using speed and reliability that can be achieved using Scala. So expect new kids on the block to take out the oldies with Scala.
Looks neat. Could it be said that this is similar to JDK8's Option class? As in, a class/type used to encapsulate (object oriented style) many possible outcomes?
Yep, java.util.Optional is Java's answer to to scala.Option.
Yep, Try is great. While returning a Try from a method is IMO kind of silly, wrapping calls in Try{} is a great way to reduce the boilerplate of handling exceptions, since you can use map/foreach/recover/etc or just swallow it directly. The best part about Try though, is that it only catches NonFatal errors. No more worrying about linkage errors causing your program to be half-crippled, or OOM condition getting swallowed (admittedly, that particular one you can work around).
Looks like an OSGi ploblem to me. Look into the generated jars and you'll find that the class is provided twice and loaded by 2 different class loaders. You should provide this class only once, export it and import the package in other bundles where you need it.
If this article demonstrates one thing, it's that the idiomatic way to handle errors in Scala is to wrap results in a monadic construction instead of using the JVM exception mechanism. Pick any one. 
That is going to be really helpful. Excited to see the information grow, and make a few contributions myself.
One pitfall I ran into today is that overloading doesn't work with no-arg functions. If you want to overload a function, the argument-less version has to have parentheses. scala&gt; def f1 = println("hi") f1: Unit scala&gt; def f2 = println("hi") f2: Unit scala&gt; def f2(x : String) = println(x) f2: (x: String)Unit scala&gt; f1 hi scala&gt; f2 &lt;console&gt;:9: error: missing arguments for method f2; follow this method with `_' if you want to treat it as a partially applied funct ion f2 ^ scala&gt; f2("hi") hi scala&gt; f2() &lt;console&gt;:9: error: not enough arguments for method f2: (x: String)Unit. Unspecified value parameter x. f2()
The point about Try only catching nonfatal errors is very important (IMO). It's a reason I tend to avoid \\/.fromTryCatch even though I like the \\/ type better in general. Ideally I'd like to see something like \\/ where I can specify the exception class that should be caught. Maybe something like: def fromTryCatch[E &lt;: Throwable, T](a: =&gt; T): E \/ T = try { right(a) } catch { case e : E =&gt; left(e) }
I created something interesting: def fromTryCatch[E &lt;: Throwable, T](a: =&gt; T)(implicit tag: ClassTag[E]): E \/ T = try { a.right } catch { case e : E =&gt; e.left } import scala.util.control.NonFatal def fromTryCatchNonFatal[T](a: =&gt; T): Throwable \/ T = try { a.right } catch { case NonFatal(e) =&gt; e.left } I'd rename these to something shorter, but anyway here they are in use: scala&gt; fromTryCatchNonFatal(throw new Exception) res4: scalaz.\/[Throwable,Nothing] = -\/(java.lang.Exception) scala&gt; fromTryCatch[RuntimeException, Nothing](throw new Exception()) java.lang.Exception . . . scala&gt; fromTryCatch[Exception, Nothing](throw new Exception()) res6: scalaz.\/[Exception,Nothing] = -\/(java.lang.Exception)
good collection of non-trivial pitfalls and their solutions. Curious about the missing info in the second half.
Thank you, this is what I was missing. 
I agree that I'm thinking about this the wrong way. I'm still new to java let alone scala. When I have more time to spare, I will dig deeper into idiomatic Scala and hit up some good books/tutorials
Thanks for the correction!
Hi, author here. Would love some feedback, do you feel after reading this you appreciate the application of it? How could I improve the post?
Simple, clear, and you explain without bias but with a practical example and references for the reader if they want more information. Job well done :) Simple is the important part, I went to the in-depth link you referenced and I would definitely want someone beginning to read your page first.
I had trouble following this article, and there are a number of rhetorical techniques that can be used to make this post far more readable. For example: The title of your article is "What is a Type Class and why should you care?". However your first sentence reads: &gt; Type systems are great because they protect you from lots of errors at compile time. From there the article is already off topic. It doesn't ever give an actual definition of what a type class is, or the problem it solves. Instead 3 (1-2 sentence)paragraphs are spent differentiating type classes from duck typing(which is also not defined). So far the article does little to persuade and only leaves readers speculating the relationship between type classes and duck typing the author is trying to establish, even for audiences who already know the definition of both terms. Often in articles of persuasion like this, it's common to first establish the problem set, then describe the solution and how the solution addresses the problem set. Restructuring the article in this way will make this article far more readable. There are almost an infinite number of resources for introductory level rhetoric and composition materials you can look up for more information on how to further improve your writing.
According to whom? and by what standard? Honest, not trying to sound like a dick, just curious because I'm a scala noobie stuck with a scalding project.
No, that's a totally fair question, or pair of questions. Let me give you my perception of the big picture, so I guess that answers your first question: according to me. :-) 1. Hadoop itself, while a not-bad implementation of the basic MapReduce framework, has been overapplied in domains it doesn't really deal with well, in particular, machine learning. 2. Hadoop 1 is somewhat difficult to install/manage in a cluster, especially alongside other important frameworks. 3. Hadoop does a lot more I/O than you probably want, and does it pretty inefficiently. 4. Hadoop has evolved to do much more than just MapReduce (see Hive, Pig, etc.) but each technology essentially has its own programming model. Spark attempts to address all of these, and I think succeeds quite well: 1. In addition to MapReduce, Spark supports iterative processes such as machine learning algorithms much better than Hadoop, and in fact Spark includes a machine learning library out-of-the-box. Also, the Apache Mahout machine learning project is being evolved to use Spark rather than Hadoop. 2. Hadoop 2 addresses this with YARN. Spark evolved in parallel with [Mesos](http://mesos.apache.org/). IMHO Mesos is more mature and more flexible than YARN. 3. Even when not running strictly in memory, Spark performs significantly better than Hadoop (i.e. does I/O more efficiently). 4. Spark's fundamental RDD data type is key to everything: MapReduce, streaming, Shark (SQL), GraphX, all of it. So there's a great deal of consistency in the programming model and APIs across all of the technologies. This reduces the learning curve as well as exposing manyâ€”and many more obviousâ€”opportunities for integrating your data analysis processes across multiple sources and approaches. The last point might be considered somewhat more aesthetic than the others, and to some extent it is. But I think if you spend some time going through the various Spark-related materials you'll find that even it is comparatively readily quantifiable. That said, I have nothing against Scalding _per se_, or Scoobi, or any other approach to Hadoop, for that matter. I just happen to believe the Spark ecosystem improves in most ways that are important to me on all of them.
I get from your response that Spark is not just another layer on top of Hadoop but a different implementation. Is that correct... Interesting that you list Machine Learning as one of the places where Hadoop has been overapplied. My project is actually a Machine Learning facility. Is Spark a better stack for that kind of applications? Doing most things in memory sounds like a good plus if you combine it with a clear stateless concurrency model would seems like it is, but I'm just throwing concepts at the wall here... 
I squee'd a bit when I watched that episode.
How can you possibly advocate the use of MyBatis and then say that writing SQL is a drawback!? :-)
Thanks for your candour, will try to take this into account next time. When you say I was off-topic, I guess I was unsuccessfully trying to point out how inflexible strongly typed code can be without the aid of things like type classes. 
SQL is a very ugly language but greate for its domain.
Interesting. Thank you for taking the time and answering. I'll take a look at this with my team. Great pointer!
Since implicits are wrappers, I find your assertion that typeclasses allow you to avoid using wrappers kind of wrong. So, the following code seems to do the same, but no typeclasses: object MyAmazingSerialisationLib { trait Serialisable { def toJson: String } def writeJsonToDisk(item: Serializable) { def writeToDisk(x:String) { println(s"I saved [$x] to disk, honest!") } writeToDisk(item.toJson) } } object NoTypeClasses extends App { import MyAmazingSerialisationLib._ case class Dog(name:String) case class ComputerMachine(id:Int, complexity: Int) implicit class DogSerialiser(dog: Dog) extends Serialisable { def toJson = s"""{"name":"${dog.name}"}""" } writeJsonToDisk(Dog("Spot")) } The advantage being that now MyAmazingSerialisationLib does not *require* having an implicit in scope. It can, after all, just be used normally. One can make one's class extend Serialisable. One can attach a mixin: new Dog("Spot") with Serialisable { toJson = s"""{"name":"${dog.name}"}""" } Or one can have an implicit class. What are the advantages of using the more complex "Typeclass" solution?
The "object oriented" approach starts having issues when you need to *de*-serialize an object. The typeclass solution is dead simple: trait Serializer[T] { def fromJson(json: String): T def toJson(obj: T): String } The core issue with the mixin approach is that to de-serialize you need an instance to already exist before you can call a method. Otherwise you will need reflection or some big map of factories. Also, ideally both parts of the serializer should be in the same place, which is natural with typeclasses but trickier otherwise. Finally, this gives you the ability to add serializers for existing classes like tuples, lists, etc.; trying to do this with mixins would be... difficult at best.
If all I have is a String, and I want to deserialize it, how will this help? How will Scala choose the right implicit to go with my call to fromJson(s)?
~~Video not found.~~ &gt; ~~This video is no longer available because the uploader has closed their YouTube account.~~
Just mildly related, but when I interviewed for Workday they asked me if I had learned Scala by following the (then new) Coursera course. When I told them that no, I had learned Scala on my own and was using it in production systems for more than 2 years, they were disappointed and didn't take me seriously anymore, WTF?
I saw from their Twitter feed that it was accidentally included in a delete of another page. They are working on restoring it and I'll update the link when it is available again. It's a very good introduction to Reactive programming and has zero to do with libraries or frameworks.
The compiler needs to know what type you want. You can tell it, like: Json.parse[Double](str) or you can have it infer it, like: val x: Double = Json.parse(str) def f(x: Double) = ??? f(Json.parse(str)) It's weird at first to think about a method whose behavior changes based on the *expected return type*, but it works fine. Another weird example is Nil.sum. Example: val xs: List[Int] = Nil val ys: List[Double] = Nil xs.sum // 0 ys.sum // 0.0 Here we have the *same singleton instance* (Nil) returning two different values from the same method (sum) based on what type we are treating Nil as. And this is very useful, as the sum method would be a huge pain to use if it blew up for an empty list. ps. Numeric[T] is also a very good example of a type class which you can't replicate with inheritance. By comparison the java.lang.Number abstract class is pretty useless if you (for example) want to make a generic "sum" method... 
Good answer, but I'm still not sold on how that is so much better than val x = json.parseDouble(str) or val x = json.parse(str, new Dog) I mean, sure, I see the elegance of your solution, but I also see the complexity, and I've run into more than my share of methods with incomprehensible generics and horrifying implicit params (that are not being satisfied for whatever reason) to know where that solution often leads. In Java 8, the avoid the canBuildFrom fiasco by simply requiring you give the compiler what needs to know - ie see the collect methods on streams. Sure, verbose, but at least it's easy to understand. I do not like using apis I can't comprehend. When things go wrong, and they do, I'm left to randomly flail at my code until the compiler gives me a magical thumbs up.
How does your second example work? Does Dog implement some method like def populateFromJson(str: String): Unit This would only work if Dog "knows about" your serializable trait, and is willing to extend it. So you couldn't add the capability to classes you don't control. This isn't a minor problem; a lot of what you would *want* to serialize already exists (Maps, Lists, Tuples) and it's not feasible to have to wrap or reimplement them all. (You also have to make all of Dog's fields mutable, even if they shouldn't be.) I agree that the example I gave (parsing a double) isn't very exciting. But implicits can chain, so for example you could have: Json.parse[Map[(Int, Double), List[String]]](str) And the compiler could turn that into: Json.parse(str)(new MapParser(new Tuple2Parser(new IntParser(),... You could of course wire all that up yourself. Just think of this as a type driven auto-wiring service by the compiler :) Finally, I wouldn't say that typeclasses themselves are complex. They are just a different way to get polymorphism, with different tradeoffs. Haskell for example has only typeclasses and no inheritance. Of course, when your language offers *both*, this is bound to add complexity. But it can still be worth the cost.
Cool - One question, instead of starting from scratch why not refactor scalatra to use twitter-server instead? The end user dev API seems fairly similar.
Yep! For example: implicit def listParser(implicit base: Parser[T]): Parser[List[T]] = new ListParser(base) implicit val doubleParser: Parser[Double] = ??? implicitly[Parser[Double]] // listParser(doubleParser) It will wire up any of the implicits in scope to create what you need. That's why I called it auto-wiring, it's really just doing the "grunt-work" that you could do on your own. A good example of this is scalacheck's Arbitrary. You can create a generator of random test values for any combination of supported types. Like: arbitrary[Map[String, List[(Int, String)]]] See [the user guide](https://github.com/rickynils/scalacheck/wiki/User-Guide). Of course for more constrained requirements this breaks down, so they do provide ways to wire custom stuff up manually, i.e. "generator for lists of at most 100 MD5 hashes" or whatever.
I see. I avoid third-party scala libs because of things like that. However, I appreciate you setting me straight on this. I've learned a lot!
Greatest pitfall is SBT, it has infect Scala ecosystem irreversible, SBT single handedly has destroyed Scala.
The video link is working again.
Will this be recorded?
Probably? The recording details have not been worked out yet, but I hope they will.
Slides: http://retronym.github.io/welcome-to-scala-211/#/
Very nice, I love the focus on performance.
Good stuff. I'm looking forward to Scala lambdas and traits making use of Java 8 goodness. Should help reduce code size tremendously. Looking forward to quasiquotes, too. Sucks that they aren't hygienic, though. I do wish they'd just deprecate the parallel collections entirely, and redesign them in a way that doesn't intrude so deeply into the regular collections' design. Setting all that `GenWhatever` in the then-new 2.9 scaladocs was a very unpleasant surprise...
Very cool talk.
Hey folks, I'm the Director of Architecture at Metafor Software (http://www.metaforsoftware.com/) in Vancouver, BC. We just raised some money, and we're looking to fill five open positions. Note that the vast majority of our current codebase is in Scala, but we haven't nailed anything down for our next-generation product. * Front-end Web Developer * QA Test Engineer * Graduate Student Internship Position (in Machine Learning) * Development Team Lead * Java/Scala Developer â€“ Big Data We strongly prefer onsite work, and have not previously paid relocation or sponsored any visas, but for the right candidates, we might reconsider. Thanks for your time! 
Funny to see a fellow Scala meetup regular hiring on reddit. Even funnier to see a startup upstairs from ours first in the list of clients. Cool pruduct. I hope it grows big. P.S. Next meetup is unconference. Could be a nice fit to present your product.
Credit &amp; big thanks to the team behind codebrew.io for the awesome "scala worksheet" like functionality! This project is open source, please submit issues / suggestions via https://github.com/scalatutorials/scalatutorials.github.io/issues The goal is making it as fun and as easy as possible to take the first steps into Scala. Since googling Scala Tutorial brings scalatutorials.com as 4th and 5th results, I really would like to get your constructive feedback on how to make it as friendly and "doing justice with Scala" as much as possible. Also any SEO experts out there who can help me understand why the meta description in google search is a code snippet instead of the descriptive test :) I'm looking for collaborators to help write more tutorials, thanks! p.s. this is 100% non for profit and done at my spare time, no company is behind it, just a guy who likes Scala
&gt; You seem to be indicating that there's no such thing as "idiomatic Scala". This is quite false I think saying "quite" is a bit of an exaggeration. Scala has a lot of constructs that very often allow you to implement a solution in many, many different ways. For example, take this article: "[How to handle errors in Scala](http://typelevel.org/blog/2014/02/21/error-handling.html)". It is wonderfully detailed and does a very good job at covering all the options you have to handle errors, but it's hard not to be completely confused at the end and wondering: "Ok, so how do I handle errors idiomatically in Scala"? We see this very often on the scala user mailing-list as well where simple questions such as "How do I group list members based on a predicate" produces a discussion with 50+ responses, most of them with a slightly different way of solving this problem. Personally, I love this power since it gets my brain wired up, but I understand how it can be intimidating. In Scala, there are often many, many different ways to implement something. 
I realize that functional and OOP don't have to be completely distinct. At there same time there's nothing stopping or dissuading you from leaning on one or the other. For example, in languages like Clojure or Haskell they're clearly steering you toward immutability. It's not that you can't mutate things, it's just difficult so people avoid it. From what you said it sounds like the there's some consistency in the way the libraries are coded. I'm glad to hear it.
I didn't mean to imply that there's no idiomatic Scala. I suppose my question really is, "Given the leeway Scala provides has the community adopted an idiomatic direction?" It sounds like the community has some larger best practices and generally adheres to them. Is that right?
When working alone variety is fine. You're in control of what's going on. My concern is more about shared code. All the examples I've done so far are isolated so everything is great. What happens when I start doing real work and pull in lots of other libs? Does stitching them together sometimes become a clunky task? 
Good points. Scala does offer more choices. And more choices means more awareness of when each choice is appropriate. I can see that being potentially burdensome, especially if you're just starting out. It's a steeper learning curve, that's for sure.
It seems perverse to me to recommend Or, basically a right-biased Either, over scalaz's Validation, also basically a right-biased Either, but one that plays nicely with the other abstractions in scalaz. With Eugene Yokota's wonderful [Learning Scalaz](http://eed3si9n.com/learning-scalaz/), John Kodumal's [Typeclassopedia](http://typeclassopedia.bitbucket.org/), and Paul Chiusano and RÃºnar Bjarnason's equally excellent [Functional Programming in Scala](http://www.manning.com/bjarnason/), it's past time to put the fear of scalaz behind us. Grab it, learn it, use itâ€”your code will be better (in terms of concision, precision, and incision) than it would be otherwise.
nuclearqtip said: &gt; Most places you look tend to favor functional over OO You said: &gt; There's a good general guideline that you should use OO in the large, and functional in the small. These sound at odds to me. Frankly, this is exactly the kind of thing I'm concerned about. I've spent a little time with the Play framework and it strikes me as OO in the large. I don't know much about scalaz but I've heard before that it's heavily functional. I don't mind working in OOP or functional but I'm concerned about how they'll mix at times when stitching together different ideologies. Have you ever had to stop using a library because the approach was the wrong style (i.e. largely functional when you're code was largely OO or visa versa)? 
You put your finger right on the sticking point. You will often hear things like "Yes, Scala is complex but you don't have to use all its features". But you do. You're not working in a vacuum, you will end up using other people's code, or other people will use your code and contribute to it. Or you'll be working at a company using Scala and they will be using features that you had stayed away from so far. Or you will contribute to open source projects that use features you are not familiar with, so you'll have to learn them. 
I think so, yes. I think most people would point you at things like: (1) the Scala language itself, (2) the Akka framework, (3) the Play framework, and a few other "big" projects to get an idea of what constitutes good idiomatic Scala. This is a rather immature language still, and new features are being added and old ones removed at a rather startling pace. It's not too surprising that they haven't been able to slow down and worry about proper styles and best practices just yet. Thankfully the community is keeping pace right along side, keeping notes as they go. I think at this point "idiomatic" is more of a democratic "this worked pretty well" vote than a time tested best-practice. The functional and OO folks are still getting used to the idea of sharing the same roof. It's not too surprising there are so many ideas for good ways to approach things.
Sorry, I'm not buying. Any competent programmer can handle [Applicative Programming, Disjoint Unions, Semigroups and Non-breaking Error Handling](http://applicative-errors-scala.googlecode.com/svn/artifacts/0.6/pdf/index.pdf) and in fact is likely to come away saying something like "Is that all an applicative functor is?" or "is that all a semigroup is?" Tony's Option cheat sheet is _convenient_ to show people who struggle to unlearn the bad habits accruing from decades of exposure to languages lacking sum types and having a distinguished subtype-of-every-type, and I'm grateful for it. But the fear-mongering around scalaz at this point is just that: misplaced, and with more than a whiff of condescension. Sure, there's a learning curve, just as there was with OO. But anyone who's releasing code in Scala today can read the above paper, say "cool," add scalaz to their project, and be accumulating errors across N potentially-failing operations immediately without their brains bleeding out of their ears.
Now I'm the one who's thinking that I might not have been clear :) Clojure, as you know, is geared toward functional and immutable. No matter who writes the Clojure code it will have these properties because that's the direction the language takes you. That's not to say that it'll all look the same but there's a certain foundation that all of the code stands on. Scala obviously gives you more leeway. One person's functional-OO mix might be 80-20 and another person's might be 20-80. Does the community tend to find a reasonable balance that they all work with? Or is the mix varied but the language design accounts for this and it all works together nicely? Or am I completely off base here?
Third time's a charm. :) Scala does not "gives you more leeway". There is a single, novel, distinctive style, deserving of the name functional-OO, that fits Scala code well. You will see **this one style** used consistently throughout the community. You will also see regional variations as well, of course, but no more variation than, say, among "pythonic" style in the python community. 
&gt; I've spent a little time with the Play framework and it strikes me as OO in the large. What that means in practice is don't expose `def wat[T]: Function2[String, Function2[String, StringChecker[T]]` in your API. Return and pass in traits, and keep your method signatures tight and cruft free. If you have a particular behavior you want, assign it to the trait, and don't overcomplicate things. Play is functional where it makes sense. A good part of that is because it is fully asynchronous. What that means in practice is that you return `Future[Foo]` instead of `Foo`, and if you want to work with `Foo` then to get the actual value out you have to call map: ``` futureFoo.map { foo:Foo =&gt; foo.doStuff() } ``` This means that when the Future "arrives", you want to do something with it then, but it doesn't have to be "now". You can do this in Java (and Play has an idiomatic Java API that a very large percentage of developers use) but it's more convenient to do it through a Future.map than through Java's anonymous inner class. Although, Java 8 now has lambdas and single access methods, which makes it nicer to deal with. Likewise, Play has an `Action` class which has a function that you pass things into: object FooController { def index = Action { request:Request =&gt; Ok(views.html.index()) } } but what this means in practice is that instead of using parameters `(request:Request)` you use `request:Request =&gt;` instead. No biggie. Scala has a couple of things that make handling input easier to deal with. For comprehensions are excellent for unwrapping things where any failure looks pretty much the same: for { foo &lt;- request.queryString("foo") bar &lt;- request.queryString("bar") } yield { Ok("found both foo and bar") }.getOrElse { BadRequest("required parameter missing") } Error handling and exceptional case management is one area where functional code can get screwy. `Either` and `fold` aren't nearly as fun as `Option`, doing a `try { } catch {}` block looks a lot like a goto from functional programming land, and Try is something you can use internally, but still requires you to handle each error case individually, usually with `recover`. So, functional programming has the same basic tasks as imperative programming. If you've got five different cases and you need to do something different for each one, functional programming isn't going to do that for you. In that case, it's time to use pattern matching. Pattern matching is like regular expressions for types -- you can dump just about anything into it and it'll work. If you're ever confused about the correct idiom for a functional programming construct, fall back to pattern matching and worry about it later. somethingHappened match { case dogsOut =&gt; reachForGun case itsOnFire =&gt; putItOutThen case screaming if ! mardiGras =&gt; stopScreaming } But in any case, Play tries to stay out of your way as much as possible. It doesn't require you mess around with higher kinded types. There are some bits (notably the JSON parsing) that have to have a deeper understanding of data and provide advanced options, but most people just use `as[String]` rather than going into implicit macro based writes and such. Streaming data in asynchronous and non-blocking way has always been a pain, and Iteratees were very much a first pass at a workable solution. However, they're more gun than required for most cases. Don't feel bad if you don't understand it -- everyone takes at least a week to understand Play Iteratees. Play 2.3 (now in RC1, squee) will make dealing with streaming data from an actor directly to websockets much easier. If you want to see more 2.3 stuff, checkout the highlights from Activator: https://typesafe.com/activator/template/play-2.3-highlights 
Gotcha. Thanks for sticking with me.
As a former Java (and briefly C#) developer who starting working on a Scala team about two years ago, my impression of Scala is that it was designed to be a language that can relatively easily be adopted by developers without functional knowledge/experience, and as you become more familiar with functional concepts you will naturally start to adopt them more. If anything I think you might be able to look at someone's Scala code and tell (roughly) how much experience they have with it and functional programming and general, but I think any Scala code base will become more functional over time. Note that by saying "more functional" I'm not saying "less OO", instead I mean immutable over mutable, use of functional collection methods (map, exists, filter, fold*, etc.) and higher-order functions, use of pattern matching, use of Option type, etc.
This looks really nice ( only did the first couple screens in my laptop), however, it didn't work on my iPad (using chrome ;(
&gt; There's a good general guideline that you should use OO in the large, and functional in the small. I like that a lot!
Scala is indeed very flexible. There is, more or less, a consistent sense of "idiomatic Scala" that's widely used in the community, though of course some people lean more functional or more OO. At my organization, we address the flexibility in a pretty simple way: we talk about it. My team talks about what features we're comfortable with using before introducing a new one into the codebase. Implicits were scary for a bunch of us at first, and for a while we didn't use them in our code. Then as we got more experience, we had another talk and decided judicious use of implicits was ok. Some of us are more adventurous, some are more conservative, but we get together and figure things out. If you have the sort of organization or team where people do their own thing a lot, Scala's flexibility might be more of a problem, but I'd call that a team problem more than a programming language problem.
You might find that explanations of monads tend to be made through biased lenses of the author, based on a mental model that they have constructed after considerable effort. While their model makes clear sense to them, it often seems muddled or contrived to you. I recommend reading (and watching) about many different perspectives on both what monads are and why they are useful. Over time you will develop your own conceptual model that, when you explain it to others, continues the chain of discovery. That said, [here is my attempt](https://github.com/earldouglas/scala-scratchpad/tree/master/category-theory/monads#monads-in-scala).
Only one concern? I have many... I started to learn Scala because a friend recommended it, but it's trying to be too many things for too many people and it ends up being a mess of inconsistency. The upside is that you are free to choose. I chose not to use Scala and to use Clojure or Groovy or Java 8 instead, but I can still co-exist with Scala libraries and frameworks because the JVM is the real star of the show.
Not a Monad master by any means but I'd say if you know Scala you're performing monadic operations all the time -- as I understand it any containable thing (e.g. a collection of 0 or more elements) that implements map and flatMap is de facto a Monad. For example, check out Option's definition of map: def map[B](f: A =&gt; B): Option[B] now do something "monadic" scala&gt; val monad: Option[String] = Some("danom") monad: Option[String] = Some(danom) scala&gt; monad.map(_.reverse) res0: Option[String] = Some(monad) The f: A =&gt; B in map definition is exactly as it says, a function that takes an A and returns a B. In the above case we could be more explicit in order to show that _.reverse is in fact an anonymous function. monad.map(x=&gt; x.reverse) x=&gt; x.reverse does that look familiar? Hmm, could it be that x =&gt; x.reverse equals A =&gt; B? Yes! x = A and the reverse method applied to x = B. Monad in a nutshell.
these courses go into details about monads https://www.coursera.org/course/progfun https://www.coursera.org/course/reactive 
In Scala, monads are basically any parametered type that can be used in a For Comprehension (for { ... } yield { ... }). Though not exactly true (a real monad must verify some associativity rules and stuff), you can say that the types Option, List (and iterables, sets, sequences ...), Future, Try are monadic. You can use them in For Comprehensions in order to express a computation (or a chain of computation) without unboxing the value(s) contained in the monad. The best example is the Try : it can express a computation that could throw an exception. For instance, retrieving data from a server. It's always problematic, as there are tons of things that could go wrong (packet loss, server crashing, no access allowed). So instead of applying a try/catch as you would do in Java, you use a Try to express the computation, but until you unboxed it, you don't know if it has fail or not. HOWEVER, there is no need for you to unbox it right away : you can express operations on the result of the computation without knowing whether it has failed or not : val a : Try[Int] = Try(connection.getData(...)) val b = for (x &lt;- c) yield (x * 2) b is still a Try. But you have multiplied it by 2. It means that if the initial computation has succeeded, the new Try will contain the result you want. If the initial computation has failed, you'll still get an exception when you unbox b. To me, monads are boxes on which you can apply operations without actually knowing the content of the box, and For Comprehensions are very nice syntactic sugar that help you do this. As for good tutorial on Monads, Coursera's courses on Functional Programming and Reactive Programming are the best (though they don't use the word monad in the Functional Programming course). 
Have you read this: http://codahale.com/downloads/email-to-donald.txt ? For context, this explains what the email was about: http://codahale.com/the-rest-of-the-story/
&gt; as I understand it any containable thing (e.g. a collection of 0 or more elements) that implements map and flatMap is de facto a Monad. I think you're basically correct. By the Haskell definition, `map` itself needn't be defined (since you can implement map out of `flatMap` (`&gt;&gt;=` in Haskell). In order to have a monad, you also need a way to convert plain values into monadic values - which Haskell calls `return`. (It's worth noting that Haskell's `&gt;&gt;=` / `return` pair was somewhat arbitrary. I remember reading that `map` / `flatten` would have been just as powerful, just different (and maybe a poorer choice for other reasons). In Haskell, Monad is an explicit type. In Scala, it's more conventional. For example, there's no single function that can be used to convert a plain value into a monadic value, though most monadic types provide an apply method on their companion object. Haskell monads are also expected to obey certain laws, although those laws aren't encoded in the type (which I suspect would be very difficult to do). Scala monads are supposed to follow the same laws, or at least, one that does not would be a bad citizen (though I think Try isn't quite perfectly monadic, but I think it's for a good reason). One thing to mention: monads aren't necessarily containers (i.e. sequences). Something is a monad if you can provide implementations of the monad functions while obeying the monadic laws. That's the only requirement. For example, the Haskell `MonadState` (I believe) doesn't really represent a container. At least, not in the same way as, say, `List` or `Option`. And Haskell's `IO` is even weirder. A Monad isn't defined in terms of what it is, it's defined in terms of what you can do with it. It's easy to start by thinking of them as containers, but I'd encourage you to look for other examples.
&gt; Looking forward to quasiquotes, too. Sucks that they aren't hygienic, though. This is one of our high priority goals for the next major release. Stay tuned.
I've read it and it certainly reinforced some of the concerns I was having. However, that's from Nov 2011 and I wanted to get a newer and broader perspective on the state of things.
I haven't played with Future/Promise in Scala for various reasons, but I've used something similar in a hobby project where I wanted to do IO, but was in the UI thread. Fire off a Promise/Future doing the IO and throw the Promise/Future around in the IO code.
[Play Framework](https://github.com/playframework/playframework) makes heavy use of concurrency. You might want to check out [Akka](https://github.com/akka/akka) as well.
Futures let you do tasks asynchronously, so say you have 10 objects in a list and you want to do some long running mathematical computation on each of the them. Normally you would execute the task for first item then second then third etc, each time waiting for the tasks to complete. But with futures you can asynchronously queue up each of the ten requests to start executing and then only after you have started all ten you can get the result from the first future, second future etc. This way your computations will run in parallel instead of consecutively, with you not needing to wait for one to finish before beginning the next. This is a major concept in development of complex systems. 
A product my company released last month does exactly this to generate a report. It's awesome. You can hop between threads almost effortlessly!
Thanks for the explanation, I did indeed miss the bit about providing a means to construct a monad (e.g. Some(1) or List.empty) &gt; A Monad isn't defined in terms of what it is, it's defined in terms of what you can do with it. Interesting, got a concrete Scala example? Very cursory experience with Haskell to-date (LYAH and a bit of GHCi dabbling). 
Twitter's Finagle framework makes extensive use of futures. They're not scala.concurrent.future, they're their own implementation. But it's basically the same thing. It's also pretty much how jquery does ajax requests, if you're familiar with that at all
One example of a real life usage (that I used at work last week) Let's say I want to query one service then based on that result, query another service. The usual code would have been something like this pseudo code val response1 = get(service1URL) //blocking val response2 - post(service2URL).withParams(response1.param("...")) //code here is not going to run until both requests are complete The thread will be blocking until both requests are done, this usually can lead to suboptimal usage of the systems resources. Now Futures and Promises exists in other languages, but what I really like about Scala is the ability to avoid callback hell in a very nice syntax It's beyond the scope of a comment, but I really like the ability to do async code like this (pseudo API) for { response1 &lt;- getAsFuture(request1URL) response2 &lt;- postAsFuture(request2URL).withParams(response1.param(..)) } yield //do something with response2 //code here can run without waiting for the requests to complete The for is merely "wiring" or mapping the output of the first future request to the input of the second future request. It's like saying, go fetch something in the background, when you are back, go fetch something else, and only bother me when both are done, in the meantime, I'll do something else useful instead of just sitting here and waiting... This code above I think is almost as simple as the synchronous version, and I don't know many languages that let you keep things as simple with futures as with blocking code (note that with scala.async the above code can even be simpler using macros, but this is beyond the scope of a comment) Another way looking at it, is - instead of accepting a callback as a parameter for an async function, you return an object that accepts a callback. (Future) This is of course just my interpretation, anyone feel free to correct me if I missed anything. 
I mean that Monad is a protocol, and that protocol is pretty generic about what implementations can and can't do. Haskell, being a completely pure language, gets some of the weirder implementations (State and IO in particular; there may be others, but I don't Haskell much). Scala, being more of a mixed-paradigm language, has other ways to deal with those cases. Having said that, Scalaz does have an implementation of [State](https://github.com/scalaz/scalaz/blob/scalaz-seven/core/src/main/scala/scalaz/StateT.scala). I would definitely encourage you to learn a bit about the State monad. It's a way to simulate mutable state in a purely functional language. Again, Scala has other ways to address that, but it's worth a look. &gt; e.g. Some(1) or List.empty You actually hit on something interesting here. For `Option`, `Some` is the function that turns a plain value into a monadic value. But for List, it's `List.apply`. `List.empty` is more like `mzero` from [`MonadPlus`](http://www.haskell.org/haskellwiki/MonadPlus). Don't get too worried about `MonadPlus`. I just thought I'd mention it for later.
This is however not the strength of future/promise. The scenario you described should be better done using ordinary multithreading. Future/promise should only be used in IO intensive tasks where suitable. Futures will add too much unnecessary overhead to computation tasks. 
Thanks for this really helpful answer! I was able to build a small program to simulate the behavior. But I don't really understand the "&lt;-"-operator. What exactly does that mean? Is it common in Scala or is it specific to Futures?
Its called for comprehension and is the scala syntax for a "for-loop", like the foreach in Java, but with '&lt;-' instead of ':'.
I released an app with Scaloid: https://play.google.com/store/apps/details?id=com.soundcorset.client.android It works fine with more than 100,000 devices, though I am not sure it is medium or larger. 
Ok so I was trying to simplify the example by saying long running mathematical computation. A more real life example is: I want to get the top 5 repos for an organization by pull count from github. First I hit a web service to get all repos, then I need to hit an endpoint for each of the repos to get their pull data, not to mention deserializing the json data. If I wrap this functionality in a future/callable I can have many calls going at the same time, significantly reducing the time to get my results back to a user.
That's the opposite of what you said! A CPU-bound operation isn't a "simplified example" of an I/O-bound one.
another way of looking at it is that you can see it as a syntactic sugar to map / flatMap basically It allows you to chain a series of flatMap calls in a nicer syntactic sugar
Will something like this ever come to the Washington DC area?
https://news.ycombinator.com/item?id=7621622 http://www.reddit.com/r/programming/comments/23lh1y/scala_2110_is_now_available/
The Scala documentation does a pretty good job explaining it [here]( http://docs.scala-lang.org/overviews/core/futures.html)
https://www.youtube.com/watch?v=ByDPifJMSvQ http://www.reddit.com/r/scala/comments/23bku8/welcome_to_scala_211_with_jason_zaugg/
[This post](http://mergeconflict.com/reading-your-future) made it click for me. YMMV.
`&gt;&gt;=`, `bind`, `flatMap`, `SelectMany`, `Î¼â¸°â†‘`, and `embed` are all various names used in various different contexts for the exact same idea: the reduced version of the Kleisli composition operator. Scala tends to use `flatMap` for this concept, even when that name no longer makes intuitive sense. In particular `for` comprehension notation assumes that the `Î¼â¸°â†‘` operation will be called `flatMap`, regardless of the type(s) involved. EDIT: In case any category theorists want to call me out on it, yes, I know the operator that produces a morphism in the target category from a morphism in the source category is usually left implicit (or else the name of the functor is overloaded), but `â†‘` seemed like a reasonable name for it and I had to distinguish between `Î¼` and `Î¼â¸°â†‘` *somehow*.
The problem is dependencies... Right now I am working on a project with deps from 2.9 and 2.10-- custom compiling the old ones... Soon, new versions of certain deps will move to 2.11, and I will be left with either deprecated code, incompatible dependencies, or a recompilation safari.
&gt; deprecated code What do you mean by "deprecated" here? If you're using it in the usual sense - of a thing that will be removed in a future version - staying with 2.10.x and the deps you're currently using leaves you no worse off, right? I'd like to move my projects to 2.11, but their deps aren't all available for 2.11 yet. That's fine; I'll upgrade when those deps are available for 2.11. In the mean time, 2.10.x is just as fine as it was before the announcement of 2.11.
I had been trying to stay with 2.9.x because I use a Spark cluster which requires 2.9... However, other libraries I use revved up to 2.10, and included major improvements and bugfixes... (like scala itself, for instance, but I also use many other libraries which are basically doing the same thing). In order to stay up to date with my their dependencies, I had to scrap out my whole cluster and get it up to Spark 0.9. I'm not complaining about fixing bugs... The backwards compatibility issue is just a major headache that I seem to have to deal with periodically. If none of my dependencies quit maintaining their 2.9 versions, I would probably not upgrade... even though I am really excited about scala language upgrades, esp. macros. I just don't want to get into a situation where a dep. makes major revisions, and I am stuck 3 versions back. What would you say to someone running scala 2.8 still?
I followed the first part of that post easily enough, but had some trouble with the latter. Not that I don't understand its correctness, but I'm curious as to why you'd want to use a monad for the behavior over, say, a "let" form in a language that actually had one, like Haskell (the canonical monadic language), or just some vals here in Scala. I mean, the behavior we're interested in is the function that takes a string and returns a result set, which the author identified. Is this a case of "more than one way to do it" or is there something subtle that I'm missing? edit: After some further Googling, it seems that 'let' is supposed to be very similar to the Identity monad.
It seems many people are confused by the version scheme of Scala. 2.11 is a major release, not a minor. We have &lt;generation&gt;.&lt;major&gt;.&lt;minor&gt;[.&lt;hotfixes&gt;]. There is a defined deprecation policy which means that if there is a method in major x, it will be deprecated but fully functional in x+1 and removed in x+2. If you look at the API of a few major versions back, you can clearly see that Scala's standard library surely benefits from removing and changing things that were not 100% well designed in the first place or (stronger point IMO) adapted to interface with new technology (e.g. futures, try, new collections etc.) The problem of binary compatibility is also increasingly addressed. A few years back there were no tools such as the Migration-Manager. Some of the problems have to do with the way the JVM works. I believe for example that when you add a method to a trait even with a default implementation, there is currently no way to make classes that used the previous implementation forward compatible. Perhaps this improves with Java 8 trait support, I'm not sure.
First, the loosey-goosey "intuitive" explanation: generally what's on the right side of "`&lt;-`" is a container, so you can pronounce "`&lt;-`" as "in:" "for response1 in getAsFuture(request1URL), response2 in getAsFuture(request2URL)..." With this analogy, the `Future` contains the response, so "for response in getAsFuture(...)" reflects that. The analogy kinda-sorta breaks down, though, because the `Future` _eventually_ contains the response, which is pretty different behavior from the usual containers! The tricky bit is that for-comprehensions in Scala aren't actually for-loops. As another reply notes, it's syntactic sugar for a chain of `.flatMap()`, `.filterWith()`, and `.map()` invocations. Any type that has these methods (in such a way that they have certain algebraic relationships) can be used in a for-comprehension, including your own types. `Future` is such a type. The neat thing here is that those methods on `Future` don't block, and compose with the same methods on other `Future`s etc. So you get this nice non-blocking coordination of concurrent activity in a way that's pretty easy to read, doesn't deadlock or race, and scales well. What's really awesome is that, with [Akka](http://akka.io), you get to take this same programming model for concurrency and use it for building distributed systems, going from your program running on your laptop to running on a cluster with hundreds of nodes without code changes. It's really amazing stuff!
What about API 9? Devices on API 8 are pretty miniscule in number.
API 9 and above is fine.
Wow good catch. I hate stuff like this. I think level 10 and above is probably as low as you want to go these days anyway, and a lot of people seem to be advocating for 4x only anyway.
Are you some kind of wizard?
The coproduct types are still boxed, aren't they?
Wow, I feel retarded, still trying to make since of ScalaZ, what is this magic? Is there a book / talk that ELI5 the background to help me understand this. And sorry for the blasphemy question, is this recommended for day to day coding? or still "academic"? I love Scala, and even finally figured out what are type classes and what CanBuildFrom is for. But this, this, is just, ah... I'm feeling so stupid, and I am an MSc student at GA Tech with perfect scores on the 2 Coursera Scala courses, writing Scala for a living at my day job for a few years with a 10K StackOverflow reputation. Any help?
Yes trait ReadMe[T &lt;: ReadMe[T]] { def readme: T } class SubReadMe extends ReadMe[SubReadMe] { def readme: SubReadMe } http://ktoso.github.io/scala-types-of-types/#self-recursive-type Other ways too, but I'm tired.
[This](http://ropas.snu.ac.kr/~bruno/papers/TypeClasses.pdf) is suposed to be a _background material_. I fear what I'll see. Also, [this blog](http://www.chuusai.com/blog/)
Kids won't care, sure, but those who support legacy hardware will.
Catch up vote. You are not alone. 2.11 just eliminated Scala completely for me. 
Oh good, I'm not the only one.
Miles Sabin (the guy who designed Shapeless) gave a talk on it that you can watch [here](https://www.youtube.com/watch?v=GDbNxL8bqkY), and the original Haskell papers on Scrap Your Boilerplate With Class that introduced the ideas can be found [here](http://research.microsoft.com/en-us/um/people/simonpj/papers/hmap/).
Good to hear.
`Arrays.copyOf`, the culprit for this breakage, has been available in Java's standard library since Java SE 1.6 - that's the version that has been [EOL-ed in Feb 2013](http://www.oracle.com/technetwork/java/eol-135779.html) and thus won't even receive security updates anymore. It's preposterous to ask of libraries to depend on anything less than Java SE 6 going forward. You might as well ask, what Java libraries can be used with Android API 8. Oh look, Jackson, the defacto standard for JSON processing [had problems and will stop supporting](https://github.com/FasterXML/jackson-core/pull/124) Android API 8 after version 2.3. You might also consider that Android Froyo (API 8) is about [1% of the market-share](https://developer.android.com/about/dashboards/index.html) and users that are still on it are probably the conservative long-tail that doesn't upgrade easily, the kind that aren't interested in your stupid app anyway ;-)
Guess what Sherlock. There are tons of devices with Android 2.2 and below still in use. Way more than your 1%. Client won't spend $1k just to upgrade to 4.4 if his device is working properly. The same holds for Windows XP users. &gt; It's preposterous to ask of libraries to depend on anything less than Java SE 6 going forward. If you are using a language feature that is essential to run then im ok with it, but if you are using this feature because it is there just for the sake of using it and this breaks backward compatibility than you are a poor dev and I don't want to use your software anymore. JSON is not an industrial standard pal. &gt; the kind that aren't interested in your stupid app anyway ;-) Oh they are. More than you think. Long term support matters most, not fancy numbers, just look at WhatsApp. These are the clients that pay the most and expect the most. Or maybe you are talking about this plastic fad facebook generation? If so then I am not interested in them. ;-) Scala gets a solid kick in its ass. 
Decent tutorial, but these are actually called [F-bounded types](http://twitter.github.io/scala_school/advanced-types.html#fbounded).
In that case you are fine, it all works by default. This is because Options, Lists, are all covariant. So you can assign an Option[B] to an Option[A] without needing to add wildcards. Covariant return types will handle the rest. Of course this won't work if your container is invariant (mutable collections, java lists, etc...) but that shouldn't work anyway :)
Not my tutorial. It's on github and they want updates/changes so make a pull request.
Do you think that picking good enough system version and sticking to it is wrong? I think this is a proof of professionalism. Or maybe we should jump numbers and make our software backwards incompatible just like Microsoft does? What if I take 4.4 and the next Scala version will be incompatible? You know what will I do? I will say thank you Scala and switch to much more stable language. 
Decent trolling, 4/5.
 &gt;However, if you stick to support older Android devices, here is a workaround: Add -dontwarn scala.collection.mutable.** in your proguard settings. Do not use scala.collection.mutable.LongMap and mutable.AnyRefMap. Make sure these classes not be called in transitive way.
Thanks! Both awesome suggestions. I actually didn't know about the SBT global plugins and I completely agree that would be a much better fit for something like this. Will make sure to make the proper updates!
Actually recent versions of IntelliJ support sbt based builds, so do you really need the plugin at all?
Sbt overloaded operators make so much sense don't they? Sbt source code doesn't even bother to document what the operators do. 
Yeah, their SBT support is getting better but I still run into too many issues that I don't have when using this plugin. The built in support doesn't seem to handle adding dependencies to existing projects very well.
Some operators do have documentation in the source, and overall documentation of methods (which includes methods with operator names) can be found in [the main documentation](http://www.scala-sbt.org/release/docs/Name-Index.html#methods).
Possibly. Compatibility (both at source and binary level) is continuously being improved upon, and some of the plans for the future in regards to compatibility are outlined in [the announcement of Scala 2.11](http://www.scala-lang.org/news/2014/04/21/release-notes-2.11.0.html).
I know alot of people don't like "overloaded operators" but everything in this post is just patently false. The "overloaded operators" in sbt aren't overloaded, nor are they operators. They're just methods with symbolic names, most of them have English language equivalents. If naming those methods with symbols was a bad choice is a difference in opinion and style. They're also documented like every other method is in sbt's Scaladoc, with much more informal documentation found in the release documentation as linked by /u/notenoughstuff.
You need to configure your build (SBT, maven, gradle...) to compile Scala files. You can't compile Scala doors with javac.
Are you trying to include the Scala source file or a library written in Scala? For the former, that means you need to use some Scala-aware build tool (SBT, Maven or Gradle with the appropriate plugins, etc). For the latter, you do it exactly as you'd use a Java class.
Cool site. How did you integrate it with google maps?
If you're using Maven - it's a Java project, so you are using Maven, right? - it's super easy to start using Scala. I've written this quick blog spot, [Using Scala and Java together with Maven](http://joaomc.github.io/scala/maven/java/2014/04/28/scala-and-java.html). EDIT: Ooops, fixed the 404!
Joaomc has the answer if you have a java project and you're trying to compile scala files in. Main trick with using java in scala or scala in java is using src/main/java and src/main/scala folders. SBT handles it automatically, maven has a plugin.
https://developers.google.com/maps/
I would like to hear any feedback from you: good or bad. Also, if you like this demo, please star the project on GitHub: https://github.com/Eliah-Lakhin/papa-carlo
I would recommend adding an interface that your Scala classes implement. Bare Scala classes might look funny. 
One real-life example I'm involved in right now is in using RESTful APIs. The Dispatch library allows you to build a request to a server and then hands you back a future to the response. You can then get one with something else (e.g. making other API calls) while the response is prepared and sent back.
404
Oops, the link is OK now. Thanks!
This template currently specifies the scala version (defined at 2.10.4) to be used. As for compatibility, it really depends on if the plugins and libraries are compatible (sbt-revolver, scalastyle, specs2). I imagine they will be soon (if not already) and I'll update the template to use 2.11 relatively soon.
What ragnarlodbrokk said. I'm triggering AJAX loads of new data based on a couple of events that the Maps API throws, too.
Thanks this was super helpful!
I actually already did this. Good design principles are important.
For the uninitiated: &gt; Monocle is a Scala lens library greatly inspired by Haskell [Lens](https://github.com/ekmett/lens). The Haskell [Lens](https://github.com/ekmett/lens) projects has an [introduction to the purpose and motivation behind lenses](https://github.com/ekmett/lens/wiki/Overview) on their wiki.
thank you for adding some context. You can find some examples in Monocle [landing page](https://github.com/julien-truffaut/Monocle) or in the examples [folder](https://github.com/julien-truffaut/Monocle/tree/master/examples/src/test/scala/monocle) 
sure. shapeless lens are the most boiler plate free lenses that I know but they are limited to zoom to a single object. Therefore, you cannot use shapeless lens to zoom into an Option, List, Map, ... because Lens define a 1 to 1 mapping while in that cases you would need a 1 to 0-1 (Option) or 1 to 0-n (List). In Monocle, we defined Lens but also Traversal (kind of 1 to 0-n Lens), Prism (kind of 1 to 0-1 Lens) and Iso (reversable Lens e.g. List[Char] and String). If you look at Monocle [landing page](https://github.com/julien-truffaut/Monocle), you can see in the Traversal examples how do we use Traversal to reduce health points of all characters in game.
on a separate note, shapeless is only a dependency of monocle-generic sub module. It is an experimental module where I am trying to reduce moncole-core boiler plate using shapeless but it is not required.
I saw this yesterday but didn't look at it closely. Now that I see it, WOW, I'm impressed. This is really cool. I'd been looking around for lexer/parser generators for Scala but didn't find anything useful. This seems to fulfill my needs and have unique features to boot. Nice work!
Great, thanks! I'll tell you how it goes, my project isn't quite at the point where I will need to break out the parser-generator. I'm working on implementing a vector processor using [Chisel](https://chisel.eecs.berkeley.edu/), a Hardware Description Language which is a Scala DSL. I was planning on implementing the compiler for the processor in Scala as well in order to keep down the number of languages I would need to use.
Does 'sbt-extra' still provide anything over the default SBT script? I thought the default script was a fork of that project.
Very nice demo! I tried to lookup the from-Scala-generated-JS code, but I cannot find it in the page source. Where is it? How big is it, and how big was the original Scala code? Did you just compile the Scala code into JS, or were there more actions involved?
Woah. Paul Phillips gone rogue. This is an open rebellion :)) 
[Previous discussion of the talk](http://www.reddit.com/r/scala/comments/1wjoay/scala_collections_why_not/).
Thank you for your reply, Andre. &gt; Where is it? This is it: https://github.com/Eliah-Lakhin/Eliah-Lakhin.github.io/blob/master/projects/papa-carlo/demo/target/scala-2.10/papa-carlo-opt.js &gt; How big is it? That is obfuscated version. It weights about ~300kb, that, imo, is not so big for the compiler. Original JS file consists of about 5000 lines of JavaScript code(compiled from Scala). There are instructions on the project's repository page on how to build it locally. If you have troubles, you may ask me by Skype: "eliah.lakhin", or by email: eliah.lakhin [at] gmail.com. I'll send you the file that I can build locally for you. &gt; how big was the original Scala code? Tens of Scala classes: https://github.com/Eliah-Lakhin/papa-carlo/tree/master/src/main/scala/name.lakhin.eliah.projects/papacarlo. &gt; Did you just compile the Scala code into JS, or were there more actions involved? The compiler is compiled completely from the Scala code. There were no additional actions. However, the User Interface you can see in the Demo is done in vanilla JavaScript: https://github.com/Eliah-Lakhin/Eliah-Lakhin.github.io/tree/master/projects/papa-carlo/demo. But this is not a part of the compiler itself. Just a web application.
Hah, I don't know. Back when I first started using it, the included script was nothing close to it. Perhaps they incorporated Paul's work at some point? I still use sbt-extra, works like a charm. Also this: `curl -s https://raw.githubusercontent.com/paulp/sbt-extras/master/sbt &gt; ~/bin/sbt &amp;&amp; chmod 0755 ~/bin/sbt` Bang, and you're done.
Sorry didnt see that one :/
A compiler constructs something called a [Control Flow Graph](http://en.wikipedia.org/wiki/Control_flow_graph), called a "call tree" in this presentation. Once it has the Control Flow Graph, it... does a lot with it, meaning, in terms of the classic [Visitor pattern](http://en.wikipedia.org/wiki/Visitor_pattern), that it visits lots of nodes in the graph, and possibly visits the same nodes many times. This video shows a pretty interesting visualization of this visitation process for the Control Flow Graph built by scalac, in an effort to help identify opportunities to make compiling Scala faster by reducing the number of times nodes in the Control Flow Graph are visited, the number of nodes visited, or both.
In particular, [this post](http://www.chuusai.com/2011/12/19/shapeless-preview/ ) was helpful to me.
My understanding is that video is video is visualizing the method call tree of the of the compiler itself, not the control flow graph of the target the compiler is currently compiling.
Yes, sorry, that's correct. I confused the targets!
From projects I've seen, 8 is commonly used as the MinVersion or whatever. 8 + the support classes give a large percentage of the entire Android installed base.
502 Bad Gateway
Try https://hackandflash.com/ , the previous link wasnt giving me a 502.. 
A fantastic idea for using Actors. You present your topic very well.
I loved your idea. Keep Going !
that worked, awesome site
+1, only for having cited the actor model :)
Play 2 is fairly mature at this point, so development will have died down. Just doing a straight API, Spray is fairly quick to get up and going with, although the DSL does leave a little to be desired. I think the main thing is that most of the Scala ORMs provide you with the tools to fit them to your use case. The code to do that is so minimal that they choose extensability over automation. Beyond that though, we'd have to know what type of database you're looking to use to make any suggestions beyond Slick.
I think your assertions in the first paragraph are strange, at best. It's clearly being actively maintained https://github.com/playframework/playframework/releases
&gt; something in the form of an ORM that doesn't require me to build my own DOA (resource.all, resource.find(123)). In Slick, I have to build basic finder methods and wire up pagination. I have to say I think you're missing the point of Slick: it's not an ORM. It's an implementation of relational algebra that frames interacting with databases in terms very similar to those in interacting with collections (which is what the "C" stands for). That said, if you have a `TableQuery[Resources]` called `resources`, then `.all()` is just `resources.list()` and `.find(123)` is just `resources.findBy(_.id)(123)`. "Pagination" is done with `.drop()` and `.take()` just like it is with [`Stream`](http://www.scala-lang.org/api/2.11.0/index.html#scala.collection.immutable.Stream), Scala's potentially-unbounded lazy `LinearSeq`(uence). The big challenge with Slick, frankly, is convincing people not to make it complicated. It's not. With respect to the first question, I would strongly consider not using MVC at all, but rather a more modern reactive approach as blogged [here](https://github.com/matthiasn/sse-chat). The combination of AngularJS and Server-Sent Events (implemented in Play! 2 with Iteratees) is very powerful, although it would also be good to see this done in terms of [RxJS](https://github.com/Reactive-Extensions/RxJS). Interestingly, Typescript and RxJS are an alternative interface target for [ReactiveTrader](https://github.com/AdaptiveConsulting/ReactiveTrader), so maybe it's worth checking out, too.
Take a look at finagle for API's
I don't know why you feel like Play 2 community "died down", it seems pretty alive to me. Also most libs don't have to be play specific, which is probably why we don't see that many plugins. What do you think is missing exactly?
I wouldn't really recommend OneJar; [SBT Native Packer](https://github.com/sbt/sbt-native-packager) is much better (I usually use universal:packageBin then run as a service using YAJSW for windows or just a simple nohup on linux). As a fellow infosec engineer myself, I have a few long lived services running on akka and it's been rock solid.
Upvote for finagle, but I think it would be too low level for OP
Interesting, could it be that Meijer is finding a post-Microsoft/C# niche with Scala? Or will he return to his Haskell roots? A recent blog post of his leans heavily on the side of pure FP.
His presentation style is very clear and enjoyable at the same time. Add to it the cute Dutch accent and his love for Scala, and you have the perfect presentation ;) ---- With respect to the content, I like how he builds the talk around the symmetry of iterators/iterables (pardon, enumerators/enumerators) and observers/observables.
I haven't watched the video, but it's interesting that he says this while only a few days ago he wrote [this article](http://queue.acm.org/detail.cfm?id=2611829) whose entire premise was "the idea of 'mostly functional programming' is unfeasible", which is exactly where Scala falls. Perhaps to reconcile this inconsistency, we could say the article reflects his idealism and his statement here reflects his pragmatism?
Right, I pointed that out in the first comment re: his blog post saying mostly FP doesn't cut it. Of course, he's free from Microsoft now, he can say as he pleases -- we'll see if he returns to Haskell, not expecting so, he's been doing more Scala/Typesafe related presentations of late (wonder if there's something in the works behind the scenes).
Given his standing, he's a bit too self effacing, but it does make for a more accessible teaching style than say, Martin Odersky, who is somewhat robotic in comparison ;-) In fact, I'd say Meijer presentation style strikes a nice balance between Simon Peyton Jones' over the top enthusiasm and Martin Odersky's no frills approach.
Yes, I agree. Martin's style is very dry and calm, whereas SPJ is too wacky to be following without headache pills.
Scala has more acceptance in the industry and one of his goals always was to bring FP to the masses. So I would say, that he never left Haskell, just found other means to preach FP to the enterprise.
The most shocking part of this talk was learning that he got fired from MS.
I had no problems with Odersky's style in the first Scala course on Coursera. Although I did take advantage of the fast forward button. If only that worked in real life :)
The real problem is APIs (and language features FFS) that use exceptions for flow control. It should not be the user's responsibility to somehow know about (and then hack around) your misuse of the language.
i think he meant more than haskell, either way a recent few of eriks talks and articles recently have had somewhat of a trolling flare to them. Pretty good stuff.
I totally admire scala users' imagination for abusing scala syntax and turn it into incomprehensible new language. 
No, I was talking about Scala vs Haskell. 
or use `Try` with `recover()`, etc., which encapsulates the handling around `NonFatal`.
Erik taught 1/3 of that course - specifically, the bit about Observers and Observables (i.e. the Twitter port of Rx to Scala). Odersky taught the first bit (monads, promises, and futures), and the last part was taught by Roland Kuhn, one of the Akka guys (possibly THE Akka guy) (and it covered Akka actors, natch).
But do you really need it there? If you are at the very top level of a Thread, then why not just catch all Throwables? 
Python does this too (StopIteration) and it seems just as weird there. 
Can you provide some insight on why you prefer sbt native packer? 
It is true that Scala uses `ControlThrowable` as super-type for control flow based exceptions, so they would not be caught by `case e: Exception`. The only exception, as far as I can see, that `NonFatal` also ignores is `InterruptedException` (which seems reasonable).
I really like this... perfect for when a full issue system is an overkill, or when there's plenty of tiny issues. 
Observe: - Your sequences have heterogeneous element types and known lengths, so you need a Tuple type (or an HList, which you can find in Shapeless) to preserve your type information. - Your `Addable` trait would best be encoded as a [typeclass](http://tpolecat.github.io/2013/10/12/typeclass.html) and in fact such a typeclass exists in Scalaz and is called `Semigroup`, which abstracts out the idea of "things you can combine with an associative operator". - Given semigroups for each element type, you can define a semigroup for a tuple of these types. So with Scalaz you can add tuples of addable things: scala&gt; (1, "foo", List(4, 5, 6)) |+| (3, "bar", List(42, 66)) res0: (Int, String, List[Int]) = (4,foobar,List(4, 5, 6, 42, 66)) The typeclass tutorial above provides enough information to do this entirely on your own, or you can just pull in Scalaz and get it for free. Hope this helps. rob 
Thanks! I was just about to go down the rabbit hole of recursive types-- this sounds much easier. I have never delved too deeply into scalaz-- but this might be my golden opportunity.
Scalaz is great. You can ignore the bits you don't need and slowly pull in the ones you want. And slowly (or not), you'll keep reaching for more... 
But I don't trust the Scala guys to know exactly which Throwables are fatal and which are not. Especially not if I upgrade to Java 8 but keep a non-Java-8-aware Scala version, since there might be new Errors introduced in Java 8 which Scala's NonFatal are not aware of and may incorrectly classify as fatal. So using NonFatal can give a false sense of safety, where this is something that kids should not try at home. Exceptions are the only Throwables you can know to be non-fatal.
Is there a proper use case where a and b are not the same type? It seems that by doing that, you then prevent it from performing most standard matrix operations, and it's no longer a matrix, just a multi-dimensional collection. You also can't really enforce this at compile time, since if I don't define an adder for string and 2d double array, I can't use the matrix library. You can also get into ordering situations like in javascript where you add a string and a number and get a string, but add a number and a string and you might get a number, a string, or a compilation error. 
yes, the use case preceded the coding-- its a parameter matrix for a machine-learning library. A high-level algorithm might contain several lower-level algorithms which may have different needs in terms of their parameter matrices. e.g. a takes only binary values, while b is float... To do learning on the high-level algorithm, you need to be able to do simple matrix math on its parameter-- which is actually a collection of parameters of differing types, but which all implement (+, -, *, /, grad) for an argument of the same type. I should clarify that it is not a complete matrix library, since I don't plan to implement standard matrix multiplication, only Hadamard product.
Digging deeper-- it appears that the Shapeless HList (or more precisely, KList functionality) is exactly what I need to store my matrices. It is, however, a little less than clear how to implement my map function-- I will need to research further, but it appears Shapeless provisions for a polymorphic function-- so I would like to be able to define addFun:(Addable[A], Addable[A])~&gt;Addable[A], and apply it to two zipped HLists of Addable. The trouble is that I cannot pass a type parameter to the function constructor, and it seems difficult to enforce a lower type-bound on the elements of my HList... The documentation for Shapeless shows many instances of polymorphic functions being created for things like List-- but the compiler is OK with instantiation List type with no type parameters, while it won't seem to let me refer to Addable without a parameter: Addable[A]... Here is an example of StackOverflow post discussing something like what I want to do: http://stackoverflow.com/questions/5349482/can-map-be-performed-on-a-scala-hlist but where the example is happy to define: object choose extends (Set ~&gt; Option) { def default[T](s : Set[T]) = s.headOption } I can't analogously do: object choose extends ((Addable, Addable) ~&gt; Addable) { def default[T](s : (Addable[T], Addable[T])) = s._1.add(s._2) }
First thing is to recognize that implicits are another parameter, but allow you to define view bounds or constraints. Here's a non-implicit version of Adder: def canAdd: PartialFunction[BaseAlgorithm, BaseAlgorithm] = { case v: A =&gt; //add this to v case v: B =&gt; //add this to v ] def add(other: BaseAlgorithm): Option[BaseAlgorithm] = if(canAdd.isDefinedAt(other)) Some(canAdd(other)) else None Now we'll use implicit objects trait Adder[I1, I2, O] { def apply(a: I1, b: I2): O } implicit object AddAToBProducingA extends Adder[A, B, A] { def apply(a: A, b: B): A = //do stuff ] class A { def add[I2, O](other: I2)(implicit adder: Adder[this.type, I2, O]):O = adder(this, other) } In this case add for the algorithm will look in the scope for an adder that takes AlgorithmA, I2, and O as types, producing O by adding an AlgorithmA and an I2. Since we have AddAToBProducingA, I2 can be a B, and O can be an A. In both this case and the non-implicit cases, we must have a function defined to do the transformation. Your adder trait is more along the lines of pimp my library where you'd have trait Addable[I, O] { def add(other: I): O } implicit class AddableAAndBToA(v: A) extends Addable[B, A] { def add(other: I)(implicit adder: Adder[A, B, A]): A = adder(this, other) } val a = A() val b = B() a.add(b)
You're actually not using `Addable` correctly here, but your first problem is a kind mismatch. This is how you'd declare a natural transformation from a pair of `List`s to a `List` in Shapeless: object myFunc extends ({type f[x]=(List[x], List[x])})#f ~&gt; List { def default[T](s : (List[T], List[T])) = ??? } However, you're no longer (conceptually) working with `List`s; you're working with `HList`s with an `Addable` constraint on each element. This is how you *should* declare the function you want: object pAdd extends Poly1 { implicit def caseAddable[T](implicit ev: Addable[T]) = at[(T, T)] { s =&gt; ev.add(s._1, s._1) } } And this is how you use it: myHList1.zip(myHList2).map(pAdd) This will take two `HList`s (`myHList1` and `myHList2`), `zip` them together into an `HList` of tuples, and then add the members of each tuple with `pAdd`. All of this is completely type-safe, and should be self-evident once you're used to the syntax.
Thank you! this is the most clearly I have seen this explained-- I see that rather than define a single mapper function, I have to define a mapper function which chooses when it wants to be valid, in this case, only when there is the implicit argument ev:Addable[T]. I have a few quick questions though: I see you are using what I recognize as a type-lambda in your first example (natural transformation)-- I have bumped into these a few times while exploring shapeless, and am not 100% sure I understand how they work... my incomplete understanding thinks they are used somehow for disambiguating type wildcards? In this case, you are defining f as a type which is two lists with wildcard type parameter X-- but passing the _entire_ definition, including the wildcard as type f? I know it is, but how exactly is this different from: def myFunc[T, X&lt;:Addable[T], F&lt;:List[X]]:(F,F)=&gt; F = (a:F,b:F)=&gt;{ ??? } I'm sure it's obvious I am a bit confused... Second question: I see for the most part how `caseAddable` works... but could you expand on how the `at` directive works? is it specific to Shapeless, or a standard Scala feature I have been blissfully unaware of all this time? It appears to select where the implicit def will be valid... in this case, at a `(T,T)`. Is it the case that if I attempt to map pAdd over an HList which does not have uniformly `Addable` elements, I will get a compile-time error, since pAdd is not defined for all elements of the HList? Thanks again for taking the time to help out here-- I feel like I'm getting much closer to being able to wrap my head around this! 
Will watch this for sure tonight. Shapeless seems to be complicated, but I feel getting on top of it will help me solidify a lot of scala type-algebra that I'm still a little fuzzy on.
FWIW, I have just bumped into a use-case for shapeless in my "day coding" life. Implementing a sort of semigroup I guess... I agree, its complicated!! I have so far, cherry-picked the scala type-system features I use, and haven't ventured too far away from simple type parameterization... Shapeless has a bit of a learning curve.
I'm not exactly sure this is what I need-- I never really need to add a B to an A-- I just need to add A to A and return A, and B to B and return B... as well as (A,B) add (A,B) return (A,B). I think the snippet you posted would sort of let me do that, but might destroy the types of the list elements-- maybe reducing all the way to AnyRef. Am I missing something that would preserve element types? What is wrong with my Pimp My Library approach, besides not defining implicits to allow adding A+B?
&gt; I see you are using what I recognize as a type-lambda in your first example (natural transformation)-- I have bumped into these a few times while exploring shapeless, and am not 100% sure I understand how they work... my incomplete understanding thinks they are used somehow for disambiguating type wildcards? In this case, you are defining f as a type which is two lists with wildcard type parameter X-- but passing the *entire* definition, including the wildcard as type f? Your understanding of type lambdas is indeed incomplete, and partially just flat-out *wrong*. :) Perhaps it would be useful to compare them to the value-level lambdas they conceptually resemble: // this is a value-level function of one argument def f(x: Int): Int = x + 1 // this is a value-level function of two arguments def g(x: Int, y: Int): Int = x * y // this is a value-level lambda connecting the two (x: Int) =&gt; g(f(x), f(x)) // this is that same lambda declared as a top-level function def myFunc(x : Int): Int = g(f(x), f(x)) // this is a type-level function of one argument class List[X] { ... } // this is a type-level function of two arguments class Tuple2[X, Y] { ... } // this is a type-level lambda connecting the two ({type f[x]=Tuple2[List[X], List[X]]})#f // this is that same lambda declared as a top-level function type MyFunc[X] = Tuple2[List[X], List[X]] Hopefully that clears things up. Type lambdas have nothing (directly) to do with wildcards (or with existential types in general for that matter). They are the direct type-level analogues of the value-level lambdas that you, as a Scala user, are presumably used to working with all the time by now. :) &gt; I see for the most part how `caseAddable` works... but could you expand on how the `at` directive works? is it specific to Shapeless, or a standard Scala feature I have been blissfully unaware of all this time? It's specific to Shapeless. In fact, if you try to use it anywhere else, you'll be in for a *lot* of boilerplate. Regarding how it works, suffice to say that it declares a local type class instance of sorts. This approximation should work for you until you're ready to dive into the convoluted internals of Shapeless itself. :P One thing to be noted is that you *must* use `at` for declaring all of your type-specific (and constraint-specific) cases in a `Poly0`, `Poly1`, or `Poly2`. The Shapeless Scaladoc will refer to some internal nested traits called `Case0Aux`, `Case1Aux` and `Case2Aux`; treat these as type classes, and follow the implicit arguments, and you should be okay. &gt; Is it the case that if I attempt to map pAdd over an HList which does not have uniformly Addable elements, I will get a compile-time error, since pAdd is not defined for all elements of the HList? Yup, this is exactly correct. In fact, this kind of thing was exactly the use case for Shapeless (and its Haskell predecessor, Scrap Your Boilerplate With Class) in the first place. --- EDIT: Wow! Thanks for the gold. :)
Also Finagle. Wraps Netty, but without a lot of the full stack features.
With Pimp My Library, your implicits become monads rather than traits. implicit adder: Adder[A] becomes implicit adder: () =&gt; A Not sure if scalaz supports monads for implicits. Without monads, you can also define your functions in the style: def [A : Adder](other: A): A where Adder is the trait above, and provides the necessary implicit operations.
As good as the advice to use "pimping" is, you don't seem to have any clue what a monad is (unless I'm severely misunderstanding you). :/
That's ... still not anywhere close to the meaning of the word "monad" ... or of "functor", for that matter. Could you demonstrate what you mean? In particular, could you show how the "pimping" pattern implements the `Monad` type class (or its equivalent in whatever category you're talking about)?
Monad (A) =&gt; B Pimp double with a function class PimpedDouble(v: Double) { def stringify(other: Double): String } val monad: (Double) =&gt; String = 0.0.stringify monad(5.0) &gt;&gt;&gt; a stringified double of 0.0 and 5.0
I am only getting more concerned. :/ Let's see if I can try to follow this: I'll give you the benefit of the doubt that by "`(A) =&gt; B`" you are referring to the Reader monad. I'll also assume you meant to make `class PimpedDouble` an `implicit class`, because calling `stringify` on `0.0` would not make sense otherwise. I now, of course, run into the fact that `stringify` has no definition, but I'm willing to say you just left the definition elided. The big problem here, and the first truly insurmountable one, comes when you call a single value of type `(Double) =&gt; String` a "monad". This is not a monad under *any* possible interpretation of the definition. You might make use of operations provided by the fact that `({type f[x]=Function1[Double, x]})#f` is a monad (just one of many under the Reader name, though), but you *didn't*. You don't make use of any operations that would even require a functor. You don't compose your functions with anything on either side, you don't flatten nested functions, you don't inject something into a function (and no, `stringify` is not a monadic injection, at all), and, worst of all, *you use that function as a function*. A function is not a monad. The Reader monad is *implemented* by a function type (as are any number of other common monads, like State or Continuation), but a function is not a monad any more than `1` is a monoid. `1` is essentially of type `Int`, and `Int` happens to be a monoid (in two different ways, actually), but *`1` is not a monoid, and a function is not a monad*. I don't know how I can make this clearer to you, but trying to use terms that you don't completely understand will only hurt others' chances of understanding said terms themselves.
&gt; I'll give you the benefit of the doubt that by "(A) =&gt; B" you are referring to the Reader monad. I'll also assume you meant to make class PimpedDouble an implicit class, because calling stringify on 0.0 would not make sense otherwise. I now, of course, run into the fact that stringify has no definition, but I'm willing to say you just left the definition elided. A. Internet. I'm not going to put in every piece of code so you can stick it somewhere and compile it. B. Ass. C. Mathematical Monad D. Ass.
This is honestly kind of sad. ._.
You probably would need to familiarize yourself with the particulars of the escape analysis they employ. Unfortunately not all static analyses are made equal, the JVM's particular implementation will have strengths and weaknesses. They talk about the analysis they use here: http://docs.oracle.com/javase/7/docs/technotes/guides/vm/performance-enhancements-7.html and one would probably need to read the paper and check out the implementation in order to really get a feel for what kind of programming patterns lose/gain precision (of the analysis). (Source: I'm PL Researcher who has done a fair amount of static analysis).
Also Finatra, which is like Scalatra, but on top of Finagle.
Ahh yes, thanks!-- that's certainly how it looked. one last thing though... Is there a way to require that my HLists are composed of Addable[_] elements-- so if I want to form a constructor of a class which takes an HList of addable elements which extends Addable, and implements add (the zip, pAdd function)-- how do I constrain the lowest common type of the hlist to be itself Addable? I want the compile to fail at the constructor... not the add function call... BTW, do you develop for Shapeless? you seem to know an awful lot about the subject... Even among scala developers, few seem to be well-versed in advanced type-level programming...
&gt; Is there a way to require that my HLists are composed of Addable[_] elements Yes. I actually just looked in the Shapeless source, and was kind of surprised to see that it wasn't there, so for now, here is the code to do this manually: /** * Type class witnessing that every element of `L` has an instance of the type class `C` (of kind `*`). */ trait TypeClassConstraint[L &lt;: HList, C[_]] object TypeClassConstraint { type ObeysTypeClass[C[_]] = { type Î»[L &lt;: HList] = TypeClassConstraint[L, M] } implicit def hnilTypeClass[C[_]] = new TypeClassConstraint[HNil, C] {} implicit def hlistTypeClass[H, T &lt;: HList, C[_]](implicit bct : TypeClassConstraint[T, C], ev : C[H]) = new TypeClassConstraint[H :: T, C] {} } I'll see about getting that into Shapeless proper, but for now, insert it somewhere in your own code base. To use it, just require an implicit parameter of type `TypeClassConstraint[L, C]`, where `L` is the `HList` you want to constrain and `C` is the type class you want `L` to be constrained by (in your example, `C` would be `Addable`). This will fail at the construction of the `HList` if not all of the prospective elements have instances for the type class `C`, like so: // Note that this requires a particular choice of `C`, like `Addable`. // A fully generic version is probably possible, but I'd have to think // pretty hard about how to attain it. :) def constrainToAddable[L &lt;: HList](lst: L)(implicit ev: TypeClassConstraint[L, Addable]: L = lst To use `constrainToAddable` (and whatever other variants of it you might make), simple wrap the construction of your `HList` in it, like so: constrainToAddable { 1 :: 2 :: "hello" :: HNil } `HList`s whose constructions are wrapped in `constrainToAddable` are guaranteed at compile time to have all `Addable` elements (or else type checking will fail). EDIT: I just realized that by "constructor" you meant the *consuming class*, not the `HList` itself! Sorry! XD Anyway, that use case is also easy with `TypeClassConstraint` in place: class MyExampleClass[L &lt;: HList](myHList: L)(implicit ev: TypeClassConstraint[L, Addable]) { ... } &gt; BTW, do you develop for Shapeless? you seem to know an awful lot about the subject... Even among scala developers, few seem to be well-versed in advanced type-level programming... No, I don't develop for Shapeless. (Although I may start now, having found out that `TypeClassConstraint` doesn't exist in the core codebase!) I'm just experienced with type-level shenanigans from working in advanced Haskell. :)
Just curious-- I am converging on using the following pattern to generalize the addition to all of the functions contained in the MatrixLike[Z] trait (which our toy trait Addable[Z] is a special case of, implementing only addition)-- Is there anything wrong with the following pattern? It seems it avoids needing to make implicits for all of the ops which may be defined for MatrixLike[Z]: trait MatrixLike[Z]{ def +(other:Z):Z def -(other:Z):Z } class DumDub(val value:Double) extends MatrixLike[DumDub]{ def+(other:DumDub) = new DumDub(value + other.value) def-(other:DumDub) = new DumDub(value - other.value) } implicit def morph[Z &lt;: MatrixLike[Z]]: (MatrixLike[Z]) =&gt; Z = (a: MatrixLike[Z]) =&gt;{ a.asInstanceOf[Z] } object pAdd extends Poly1 { implicit def caseAddable[T &lt;: MatrixLike[T]](implicit m: MatrixLike[T] =&gt; T) = at[(T, T)] { s =&gt; s._1 + s._2 } } object pSub extends Poly1 { implicit def caseAddable[T &lt;: MatrixLike[T]](implicit m: MatrixLike[T] =&gt; T) = at[(T, T)] { s =&gt; s._1 - s._2 } } It definitely works in my test case, but am I opening myself up to future confusion by non-type-safe casting-- even though, by definition, the argument `a:MatrixLike[Z]` is always also a `Z` because of how I specified `morph`'s type constraint? Is there a good forum for these sorts of questions-- I have found the web/forums unusually quiet... i.e. there seems to be not a huge amount of activity...
There are a *lot* of thing wrong with this, actually: - Your `MatrixLike` trait is not a type class. When you want "return-type polymorphism" like you do here (as in, you are trying to return `Z` directly), type classes are the *only* safe way to go in Scala. F-bounded polymorphism, like what you're using here, will only ever lead to tears and massive existential types like `MatrixLike[Z] forSome { Z &lt;: MatrixLike[ZZ] forSome { ZZ &gt;: Z &lt;: MatrixLike[Z] } }`. Trust me, you don't want to go down that path. - The argument `a: MatrixLike[Z]` is *not* "always also a Z because of how I specified morph's type constraint". Think about this for a second: even if the implicit conversion that gets selected is always `morph`, `Z` being a subtype of `MatrixLike[Z]` *does not mean* that `Z` is a *supertype* of `MatrixLike[Z]`, which is what would be required for the cast to always be safe. In fact, `Z` is liable to *not* be a supertype of `MatrixLike[Z]`, as is already demonstrated in your `DumDub` example. Casting here is profoundly unsafe, and will lead to subtle and hard-to-track bugs for the rest of this code's lifetime. Instead, what I would do is use the stuff I've given you (especially regarding Shapeless) to build `MatrixLike` as a proper type class, and to constrain `Z` with context bounds where necessary: trait MatrixLike[Z]{ def add(x: Z, y: Z): Z def sub(x: Z, y: Z): Z } implicit object DoubleMatrixLike extends MatrixLike[Double] { def add(x: Double, y: Double): Double = x + y def sub(x: Double, y: Double): Double = x - y } implicit class MatrixLikeSyntax[Z](self: Z)(implicit ev: MatrixLike[Z]) { def +(other: Z): Z = ev.add(self, other) def -(other: Z): Z = ev.sub(self, other) } object pAdd extends Poly1 { implicit def caseMatrixLike[Z : MatrixLike] = at[(Z, Z)] { s =&gt; s._1 + s._2 } } object pSub extends Poly1 { implicit def caseMatrixLike[Z : MatrixLike] = at[(Z, Z)] { s =&gt; s._1 - s._2 } } No implicit conversions (aside from the one for syntax), completely type safe, and fully integrable with Scalaz and Shapeless! --- If you want type classes like `MatrixLike` already built for you, and heavily optimized for use cases like `Double` and various array and vector types, I would highly recommend taking a look at [Spire](https://github.com/non/spire), which is part of the same group of libraries ([*typelevel.scala*](http://typelevel.org)) as Scalaz and Shapeless, along with Argonaut, ScalaCheck, Specs2, and many others.
He can be pretty blunt sometimes. Even turned his wrath on me once in IRC. But he knows what he's talking about, and I'd take his advice any day. 
You could do something like this: trait Buildable[T] { type Result def newBuilder: Result } object Buildable { implicit object ABuildable extends Buildable[A] { type Result = ABuilder override def newBuilder = new ABuilder } implicit object BBuildable extends Buildable[B] { type Result = BBuilder override def newBuilder = new BBuilder } } def builder[T](implicit B: Buildable[T]): B.Result = B.newBuilder And if you define ABuilder and BBuilder like this: class ABuilder { def method1() = println("Call from ABuilder") } class BBuilder { def method2() = println("Call from BBuilder") } Then you will get: scala&gt; builder[A].method1() Call from ABuilder scala&gt; builder[B].method2() Call from BBuilder 
Wow, I've tried playing with type aliases many times and could never quite get it working. I kept thinking it must be the solution - wonderful to see that you've got it working. I'll have to try it later - thanks very much!
Okay, appears to work perfectly. Best of all, it even results in a more general solution. In my original solution Builders had to have no-argument constructors whereas your solution can have entirely customised construction. Thanks!
Looked a bit at Spire, and I can't seem to find what I'm looking for in there... I'm going to digest this a bit, and probably make some big changes to my library... It seems I have been misusing the typeclass pattern-- I think I can see the benefits of the _real_ way to do type classes... but must admit, I think this method of composing classes feels a bit messy to me since we will need to define the syntax class and implicit objects somewhere besides _inside_ the concrete implementations of the classes which are supposed to extend MatrixLike. (probably in the companion objects) Plus, we won't get such nice compiler/IDE hints like: "yo dawg... you said the DumDub class was supposed to be matrix-like... but you totes forgot to implement + and -... want me to go ahead and throw in function declarations for you?" (For anyone else reading this, I found the following link quite helpful for motivating and clarifying the proper use of type-classes: http://danielwestheide.com/blog/2013/02/06/the-neophytes-guide-to-scala-part-12-type-classes.html ) It is fairly clear how the proper pattern will work well with our pAdd/pSub functions...
The easiest is Play. But in general, i think Spray.io is better for APIs design, and in performance. In the future, Play framework will use spray as http server instead of netty, as it is nowadays. 
Depends on what you mean by "best" in this context. Plenty of options Some more website focused frameworks are good enough for APIs - Play, Scalatra, Skinny, Finatra Some more API focused frameworks too - Spray, Socko, Finagle, Dispatch. But as I say it depends what you mean by best - each have their own good and bad points.
If you have any questions about spray, please let me know: I've been using it commercially for &gt; 3 years now.
&gt; Looked a bit at Spire, and I can't seem to find what I'm looking for in there... Did you see `Torsor`? What, if anything, was wrong with just using trait MatrixLike[Z] extends Torsor[Z, Z] with AbGroup[Z] { implicit final val scalar = this } ?
I hadn't seen that-- it's not mentioned in the docs or readme, only as a source file with terse comments. Wikipedia says: http://en.wikipedia.org/wiki/Torsor_%28algebraic_geometry%29 I'm afraid I may be getting in over my head here D-: I guess the object I have been considering does resemble a fiber-bundle if I can convince myself that the matrices at the elements lie in well-defined vector-spaces... Which I guess they should since they define addition, subtraction, scalar multiplication, and, I guess inner product and such... However, it appears using abelian group-based objects would be problematic since I intend to implement subtraction, which is non-commutative? (I also seek to implement map, which is certainly not commutative.) http://imgur.com/ChzUb 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Torsor (algebraic geometry)**](https://en.wikipedia.org/wiki/Torsor%20%28algebraic%20geometry%29): [](#sfw) --- &gt;In algebraic geometry, given a smooth [algebraic group](https://en.wikipedia.org/wiki/Algebraic_group) *G*, a __*G*-torsor__ or a __principal *G*-bundle__ *P* over a scheme *X* is a scheme (or even [algebraic space](https://en.wikipedia.org/wiki/Algebraic_space)) with the action of *G* that is locally trivial in the given [Grothendieck topology](https://en.wikipedia.org/wiki/Grothendieck_topology) in the sense that the base change along "some" covering map is the trivial torsor (*G* acts only on the second factor). Equivalently, a *G*-torsor *P* on *X* is a [principal homogeneous space](https://en.wikipedia.org/wiki/Principal_homogeneous_space) for the [group scheme](https://en.wikipedia.org/wiki/Group_scheme) (i.e., acts simply transitively on .) The definition may be formulated in the sheaf-theoretic language: a sheaf *P* on the category of *X*-schemes with some Grothendieck topology is a __*G*-torsor__ if there is a covering in the topology, called the local trivialization, such that the restriction of *P* to each is a trivial -torsor. A line bundle is nothing but a -bundle, and, like a line bundle, the two points of views of torsors, geometric and sheaf-theoretic, are used interchangeably (by permitting *P* to be a stack like an [algebraic space](https://en.wikipedia.org/wiki/Algebraic_space) if necessary ). &gt; --- ^Interesting: [^Principal ^homogeneous ^space](https://en.wikipedia.org/wiki/Principal_homogeneous_space) ^| [^Heap ^\(mathematics)](https://en.wikipedia.org/wiki/Heap_\(mathematics\)) ^| [^P-adic ^TeichmÃ¼ller ^theory](https://en.wikipedia.org/wiki/P-adic_Teichm%C3%BCller_theory) ^| [^Stack ^\(mathematics)](https://en.wikipedia.org/wiki/Stack_\(mathematics\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chdwolo) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chdwolo)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
A torsor *is* subtraction, in a very useful sense. [Here is a much better explanation of them than the dense mess on Wikipedia.](http://math.ucr.edu/home/baez/torsors.html)
&gt; It's design is restful by default What makes you say that? As far as I can see, there's nothing RESTful about Play itself. It's entirely dependent on how you implement your controllers.
&gt; Play is more for building HTML apps vs. REST apps. These things are not opposites. They're not even mutually exclusive. Even an entirely static HTML site is RESTful.
It is a bit amusing that both Spray and Play are developed by Typesafe these days, as well as both Slick and Akka. &gt; Slick (ORM) Slick is completely unrelated to Play. While both are official Typesafe projects, there isn't even an official compatibility layer yet (though IIRC it's supposed to be bundled with 2.3), the current suggested DB layer for Play is ANORM. &gt; but it also builds directly on Akka Play is also based on Akka, but IIRC Spray is just more eager to expose it to you as a developer. That said, Play allows you to access to their actor system, and it even has an API for Actor-based websockets (the main use case IMO for actors in a web framework).
I'm sorry for my muddled terminology. This is what I mean: HTML, browser-based apps for humans vs. JSON/XML RESTful APIs meant to be consumed by computers (or your JS-frontend). REST means that you use HTTP verbs. A browser natively uses get and post verbs, but requires you to use JS if you'd like to put, patch, or delete. So if you built something with merely static pages, you probably would stop being RESTful.
I thought Play only had Akka *integration*. When I was using Activator, I couldn't see that Play was using Akka. Perhaps I was mistaken. You're perfectly right about Slick. My company is using Play + Slick and I completely forgot that's not the Play way, even if Slick is part of the Typesafe Platform.
Hmm... I see. A group which doesn't have an identity. That is way simpler than what the Wiki said... Still not sure I see exactly how this helps... We would get diff()... but need many other operations as well... I'm probably missing the whole point...
You get `add()` and `diff()`, which is what you wanted; everything else in a torsor can be derived from those two (and the proof that there exists at least one element of the underlying set). EDIT: I shouldn't have been so hasty to recommend them, though, anyway. In practice, I think you'll actually just want a Group. I can't imagine any reasonable implementations of `MatrixLike` that don't have identities! At least in your context, anyway. :)
Ahh yes... I guess I should have specified that addition and subtraction are only examples of the types of functions I will need. There will actually a whole boatload of functions associated with our MatrixLike trait... Addition and subtraction obviously-- addition and subtraction of a scalar... Hadamard product, scalar multiplication map, and most importantly, arbitrary matrix multiplication functions... I wanted to be able to specify arbitrary inner type-- not just Double, but it made everything too hard to code up with type parameters/ I couldn't think of a use case. The matrices will definitely have identities. It was good to learn about Torsor anyway. I'll probably put that concept in the back of my head to ferment for a while-- I never took an algebraic geometry or abstract algebra course, so I'm learning as I go...
you may use [spray](http://spray.io) with [my library](https://github.com/fntzr/spray-routing-ext) which add restful directives.
Thank you 
The main gist of REST is to utilize the stateless and resource based nature of the http protocol to design apis. All web frameworks are not stateless by default (LIFT for example). Play also utilizes http verbs for routing. These seem like straight forward things all web frameworks should do, but this isn't always the case. Some web frameworks layer contrived abstractions over top of the simple nature of http.
Yep, everything is booked. ScalaDays 2013 was great, hopefully this years will be even better.
I hope so, even though I wish it was in the US (I'm from Europe and I already visited Berlin :) ).
Is anything gonna be live streamed? 
Note sure, 800 Euros + 20% VAT is a bit on the pricey side, basically twice every other Scala conference (yes it is *the* Scala conference, but still). Might go for it, enjoyed Scala.IO and ScalaExchange this year, could be an interesting detour before leaving Europe for the summer.
I don't think they are doing live streaming - but they normally put the videos up fairly rapidly.
(Actual talk starts 3 mins into the video; check the link at the bottom for more interesting presentations from that conference) 8 mins build time is pretty severe; I don't think I have seen anything beyond two or three minutes for clean builds. Probably they did quite bad in terms of modularizing their code base. I was also surprised that he says switch from HD to SSD didn't make much difference, I heard several people say that SSD improved speed quite a lot. Lastly, I was surprised that traits are such a compile time blocker.
This board is dedicated to the Scala programming language (www.scala-lang.org) Your question is about the a presentation platform which shares the same name (http://scala.com). The two are not related in any way.
only sick psychos and their sheep use PLAY.
(Actual talk starts 3 mins into the video; check the link at the bottom for more interesting presentations from that conference) 8 mins build time is pretty severe; I don't think I have seen anything beyond two or three minutes for clean builds. Probably they did quite bad in terms of modularizing their code base. I was also surprised that he says switch from HD to SSD didn't make much difference, I heard several people say that SSD improved speed quite a lot. Lastly, I was surprised that traits are such a compile time blocker.
8 minutes is indeed an eternity, but they did have 100k LOC of Scala code pre refactoring. Sub projects help hugely for development, can really cut down on compilation times. For deployment, however, there's no way around compiling *everything* Looks like 3X the build time on their CI server is spent in running tests vs. compiling (which isn't that slow at all for a clean/compile on a code base of this size, 90 seconds). As for HD to SSD, may not bring a huge compilation speed up since scalac is primarily CPU bound (i.e. a decent spinning disk should be able to more or less keep up with the writes). re: traits, I bet they went nuts on the cake pattern, which would explain build time reduction post trait refactoring.
&gt; re: traits, I bet they went nuts on the cake pattern, which would explain build time reduction post trait refactoring. The presenter(don't know about the rest of his team) also seemed to have a naive understanding of how traits compose. Which explains in this trait entanglement slide, trait B ended up materializing because of his use of the override keyword, but he didn't understand why. I would also guess overuse of override was common and led to his recompile the world problem. However, I don't think there's much material out there that explains this though, and people often just add override because the compiler tells them to.
I will look into this for sure-- But I had a long think last night, and came up with an observation and a question: O: What I really want here is a data structure which is essentially a monoid under _several_ operations, not just one. Neat! Q: But how exactly does knowing this help me? I will still need to implement the operations by hand anyway-- Sure I can also make my classes extend Monoid, but why? why not just make typeclasses and implicits like we discussed above which specify and implement everything I need? ----- Also-- I can't seem to figure out what is Spire actually for? what projects are using it? What do we get out of having @sp for objects which appear to implement properties, but no actual computation? My system uses JBLAS in the backend-- would using Spire ever be more efficient? 
Just saying that an object is "a monoid under several operations" doesn't tell you much. Presumably, you want particular *relationships* between those operations, yes? If you tell me in greater detail what the algebraic structure of the construct you're looking for should be, I can try to find some relevant candidates in abstract algebra and/or category theory, and hopefully give you some pointers as to implementing them as Scala type classes. &gt; Also-- I can't seem to figure out what is Spire actually for? what projects are using it? What do we get out of having @sp for objects which appear to implement properties, but no actual computation? My system uses JBLAS in the backend-- would using Spire ever be more efficient? Having never run across JBLAS before, but being somewhat familiar with the underlying ATLAS/BLAS/LAPACK libraries, I'd have to say that the two advantages Spire has over (most) BLAS-based libraries are somewhat greater generality (Spire isn't restricted to just linear algebra, but can do numerical processing of other sorts, too) and being completely JVM-native (BLAS-based libraries have to rely on an underlying platform-specific implementation). As for which one is more efficient, I don't know. My advice would be to benchmark the two for your particular use case.
It's baffling that after *3 years* of working in the language the presenter brings up fairly basic Scala concepts as blocker issues. Certainly excessive use of the cake pattern is begging for eternal compile times, not to mention blindly including every sub project in SBT `aggregate(...)`. Perhaps his team members are all very strong in Java and only took up Scala to avoid &lt; Java 8 boilerplate/warts, a move they seem to be reconsidering (although the presenter does say they're in no rush to make a move). I find Scala tooling these days to be really good (tip: turn off automatic builds in Eclipse and let SBT run the show), and build times have incrementally improved since 2.8, so am over the moon happy compared to the dark ages of Scala ;-)
Yeah, I saw that. Actually, my `TypeClassConstraint` is a modified variant of `BasisConstraint` from that file. Once I get around to it, the new constraint will go in that file as well.
&gt; In conclusion, he said that even though they were able to deliver successfully with Scala, they didnâ€™t see much productivity boost. This is probably a good summary why Scala's adoption has been stagnant, and this matches my experience too. There are a lot of sections of code in my code base that I'm quite happy with and proud of (you know, the kind of code you read again a few weeks later and you think it's a pretty cool and concise way to solve this particular problem) but also plenty of areas that make me think that overall, choosing Scala for these projects was not as much of a slam dunk win as I thought it would be. 
Interesting, what alternative(s) would you now choose that _would_ be a slam dunk vs. Scala? The only real contender I see is Haskell, and there are no shortage of issues there in terms of build times, using the language with large teams, and problematic tooling (the main gripes presenter has with Scala).
That's the sad part, so far, Scala is the least worst of the alternatives today. Because Haskell doesn't run on the JVM, it's not on my short list of contenders, but I'm definitely keeping an eye on Kotlin and Ceylon, with a preference for the latter (because it's officially out and its type system looks more interesting to me than Kotlin's, which is a straight increment over Java -- not a bad thing in itself either). 
&gt; but also plenty of areas that make me think that overall, choosing Scala for these projects was not as much of a slam dunk win as I thought it would be. Do does code in question seem correct and idiomatic to you? Or is there a possibly a better implementation that escapes you? I think this distinction is important as it is important to really know if the language is doing something wrong, or if there is something insufficient about the way developers are learning to use the language.
I was not specifically referring to code (more like build times, IDE support, etc...), although there are certainly sections of the code I wrote that I don't like. Some of it undoubtedly my fault because I don't master all of the language (not sure anyone does) but others are clearly due to syntactic warts and legacy constraints that Scala carries and forces upon you. 
I haven't done Websockets work, but [this](https://github.com/wandoulabs/spray-websocket) looks promising.
Yep, look nice. I'm gonna give it a try. Thanks
Would love to see some accompanying video/audio for this; but otherwise looks great. 
I don't know of too many other technologies where you can throw an entire team in with no prior experience and have everything turn out okay. 
It looks like :-( It is strange that there is no something like this - scalatra is cool framework. 
Play uses Akka with its own system for the application. You're welcome to use that system or stand up your own system for your internal Actors.
I've gone with a similar approach to this-- (but not exactly this... for some reason `new TypeClassConstraint[H :: T, C] {}` just won't compile for me...) I have defined another trait `MatrixBase[T]{ implicit val ml:MatrixLike[T] }` and my container-class constructor as: `class MatrixEnsemble[T &lt;: HList : &lt;&lt;:[MatrixBase[_]]#Î»](val subParams: T)` (using the Shapeless LUB operator https://github.com/milessabin/shapeless/blob/master/core/src/test/scala/shapeless/hlistconstraints.scala#L49 ) so we know that there is always a `MatrixLike` object for all elements of our HList... Okay, neat! now it should be as easy as importing our pAdd function (which works fine _outside_ the class) nope. For some reason, we are losing type information for T by using this constraint? Normal HList operations... including a simple `zip` aren't working in my class, even if they work on the HList supplied to the class constructor outside of the class... bummer. Soooo close.
From author, AMA. 
Looks like [Springer](http://www.reddit.com/r/scala/comments/25ck0m/pune_scala_symposium_scala_practice_3_years_later/) could learn a thing or three from Sumo Logic on how to scale a large Scala application ;-) Those are impressive numbers in the blog post, clearly if you're not using SBT sub projects you're doing it wrong&amp;trade;
The article says they are using Maven, not sbt (and without parallel Maven, their build takes anywhere between 20mn and 90+mn). 
Can you show me what the offending code is (anonymizing it if you have to)? It sounds like your usage patterns are incorrect, but I can't be sure without more information.
The related discussion in /r/programming isn't showing up in "related" due to a slightly different url. http://www.reddit.com/r/programming/comments/25gyke/scala_the_simple_parts/
Is Pants really any faster SBT?
In my own experience, casting your things into small individual modules (either as maven dependencies or sbt sub-projects) is a very good approach. As a side effect, you don't end up with one large messy project and you have rather clear knowledge about which part depends on which. On the down side, during refactoring phases, it can be annoying having to propagate API changes bottom up, as you need to republish the intermediate modules.
Maybe if compiling your build configuration takes a long time. Though I've heard complaints before, I haven't seen a sbt conitfiguration thats complex enough to take more than a minute to build(not that a minute is all that great, but it doesn't have to compiled all that often). If the build takes longer to compile than that, you're probably doing something wrong IMO.
What are his motives behind this?
No idea who Cedric is or what he has done wrong, but this is pretty creepy.
For what it's worth he's going to teach an edx course on functional programming in Haskell.
If it is true the Beust is indeed flaming /r/scala, that's significant if only for the fact that he's not just some random troll. As to why he may be doing so, who knows, but it is a bit annoying having Scala cast in a consistently negative light on a thread that is normally meant to, you know, promote the language. And yes, as was posted here in the comments, everyone has a right to an opinion, but when the expressed opinion is only meant to tear down the language (i.e. never point out its strengths), that's something else entirely as prospective Scala users may read these opinions and think, "sounds horrible, I'll go check out [language that is really being promoted]". It's not like /r/scala is a hotbed of regular activity anyway so articulate trolling can carry disproportionate weight. 2cents
Lift has a very good Scala code, yes. What lihaoyi maybe meant is that Lift sometimes invents its own techniques, more often than frameworks like scalatra/spray. For example, they invented and used their own actors when Scala had only scala.actor (deprecated now). They started using net.liftweb.common.Box before Try/Success/Failure were introduced.
It's not that he disagrees, the issue is that he claims to speak from a position of experience/expertise with Scala which doesn't reflect reality.
Money and entertainment are two good reasons. Sociopaths are 1/20 individuals.
Not only that, I'm pretty disappointed by the lack of concern expressed about _anyone_: 1. Falsifying multiple identities 2. In order to materially retard adoption 3. Of a technology 4. In which the person falsifying identities has a non-neutral stake Item 1 alone is worthy of condemnation. Put it all together and you at best have an industrial _agent provocateur_, and at worst a sociopath.
I'm curious about this too. As a case study, it might be nice to take a non-trivial project like [Akka](https://github.com/akka/akka), redo it in Pants, and compare.
Perhaps, there's little concern because it's just the nature of reddit or for that matter a public forum. It's also, for me, about free speech. Whoever this troll is free to troll (as in free speech) as we're free to ignore him or down vote him or her. In the same vein, you're free to expose said troll.
&gt; What are his motives behind this? Mental illness. Seriously. I suspect he's not an agent provocateur as suggested by someone else here -- rather I'd expect that he holds obsessive tendencies, perhaps complemented by a tinge of paranoia, and for whatever reason that has manifested in his behavior towards the Scala community. He may be a sociopath; he may just as likely think he's doing the right thing, those assholes pushing that awful language deserve it. Who knows. He's crazy, the rest is speculation.
Pants (and its explicit deps) + cached artifacts + refactoring helped vastly. It's hard to totally attribute this to Pants but I don't think we could have convinced sbt to work like this. Compile times on our very large codebase used to take 30 mins - an hour. Now they take tens of minutes or less.
There are two branches in the repo [part1](https://github.com/scalamacros/macrology201/commits/part1) and [part2](https://github.com/scalamacros/macrology201/commits/part2) and both of them contain step-by-step equivalent of this weeks' macro workshop at flatMap'14.
Also consider this; on his twitter account he randomly pings known Scala folks, like Daniel Spiewak, RÃºnar Bjarnason or Erik Meijer, i.e. hijacks threads. Some attention whoring going on there. Or maybe he's a T.M. sort of character - hates the language, but always comes back to its community, perhaps because no one pays interest in him elsewhere. Anyways, the best is just to name the sock puppets and then ignore them.
I'm going too! PM me if you are looking for Scala devs and/or would like to talk about Scala so we can arrange a meeting in Berlin :)
How dare you say this heresy, Programming languages *are* a religion. Read one language flame war on r/programming or hacker news and tell me it's not a religion. Rails vs Node, Java vs well, the world. Scala vs Haskell, Clojure. Go vs Rust. People like to camp and bash the other like college students, because, well, some of them are. It's immature, silly, and wastes a lot of people's time for arguing on the internet, where subjective opinion is stronger than any fact. So what part of it is not like a religion? :)
The majority of posts from users /u/fjord_piner and /u/cynthiaj are links to Cedric Beust's blog at beust.com, which is spamming and against reddiquette if those users indeed are by Beust. /u/cynthiaj's last activity was before and on the same day as the previous post from /u/cbeustwatch. The day after, /u/e_engel was created. While I don't personally think that the criticism from these users are that bad (they tend to vary in quality, but they do at least sometimes have good argumentation and points, and the user(s) behind does not always seem to have malicious intentions but instead earnest criticism), the blog spamming and multiple accounts are annoying. The blog spamming ought to stop; if people find the blog entries interesting, let them post them. As for the critical comments and posts, I don't think they are a problem as long as they hold up to a minimum standard, which regrettably is not always the case. The combination of multiple user accounts, single-minded focus on criticizing Scala, and the frequently poor and dishonest argumentation, is as a whole annoying, distracting and not constructive. If just one account was used, it could be blocked if the criticism continued to be poor and dishonest, and if the criticism was good and honest, there wouldn't be a problem with multiple accounts (and there shouldn't as far as I can tell be any need for multiple accounts either). And if the user did not spend a lot of time criticizing Scala in particular, it would be fine. /r/scala is still a relatively small language community, and while we can deal with regular trolls just fine, a seemingly persistent, dishonest and also well-known troll is annoying, distracting and perplexing. Whatever the cause of this behaviour is, it is in my opinion not a good way of coping with that cause. As a final note, I will be happy to point out and explain examples of poor and dishonest argumentation from the accounts if anyone requests it.
Programming languages are not religions, they are tools and technologies. And while language wars tend to be destructive and heated, I think there is a lot of value in honest and insightful discussion of programming languages. Personally, they have helped me understand which languages are likely to be appropriate for which tasks (such as using C, C++ or Ada for real-time programming), and discussing the merits and flaws of programming langauges (especially if it is done in an honest and constructive way) can help guide language designers to create and improve both general-purpose programming languages as well as more specialized languages.
The presenter's book available here: http://www.atomicscala.com/ebook/
Learning Scala is available on Chimera here http://chimera.labs.oreilly.com/books/1234000001798/index.html 
Did not downvote, but maybe slide soup is the reason? The slides probably don't do the talk justice, took awhile to grok what the talk may be about. Look forward to the video if/when available.... 
Having a PHD is CS and not understanding functional programing can make you anxious. I would say that this kind of behavior is a typical consequence of jealousy and arrogance, very common for French PHD graduates.
Setting up even a basic Android-Scala app was a massive pain, but it was definitely worth it, because Scala. I made a basic template that you can just copy. This project is setup with the appropriate Proguard setting and Maven: https://github.com/AliceCengal/android-scala-template-basic I also made a more advanced framework for working with Android: https://github.com/AliceCengal/android-scala-template-eventhub See here for how to prepare your environment before running the Maven build: https://github.com/AliceCengal/vandyvans-android#build-notes Learning materials are really limited. You just have to use the Java materials, which are abundant, and translate them to Scala, which shouldn't be too hard. I really like Scaloid from the looks of it, but so far I haven't been able to get even the sample app to build. I don't know if it's my setup that's wrong, it just won't build.
What's the gist of it? Scala good? Scala bad? Learn Python?
If you want to use sbt instead of maven. A demo by myself https://github.com/molikto/read. I don't think Scaloid is pretty useful. Actually Android-Plugin is sufficient for me. If you want to use sbt 0.13.0, use my fork of android-plugin
I really want someone to convince me that this is worthwhile. I much prefer Scala over Java, but every time I try to set up a Scala-based Android work flow, it just ends up being a bigger headache than it's worth. Please convince me otherwise! :)
thats exactly why NaCl can superseed JVM but i guess baboons are in control of Google.
I am so sorry. A friend directed me here upon hearing about my difficulty with the display software's script. He must have assumed this board had something to do with the Scala display software. And frankly, I was in such a frenzy to get my question out here (hoping for help!) that I didn't even notice the difference in the logos which would have alerted me to the difference of which you speak. Please carry on like I was never here... ! On my way to delete my user ID now... :P 
Sound good! I know Shapeless can be a bit much for someone who is only just starting to explore the advanced side of Scala's functional heritage, but it's there for when you want to dive in, and it will only ever keep getting better. :)
There is a tutorial to setup Scala IDE for Scala/Android development. The last time I tested it, it worked very well. http://scala-ide.org/docs/tutorials/androiddevelopment/index.html
I saw, "Scalaz 7.1.0 was released", got excited, and then realized it was a lie.
I don't see what significant advantages this have over regular case classes in Scala.
I used it to make my library cross compile with 2.10.4 and 2.11.0, it is very convenient! 
So even 10 years olds can figure out how to use Scala then?
Kojo seems to be a way of answering that question.
&gt; But kids work around this in an ingenious manner â€“ they ignore the text of an error message! Instead, they make use of the hyperlink in the error message to navigate to the line in their program that contains the error. Looking at that line, they are mostly able to determine what the problem is. Best part of the whole piece, if only adults could show that willingness to try and solve problems.
Now with [the video](http://tech.gilt.com/post/86420946764/dr-martin-odersky-at-gilt-5-19-2014-video-photos)
To be honest, I often do that myself. Finding an error in my code is often faster than trying to decipher an error message. Glad to see I'm still young in my head. 
So another Python startup moving to the JVM, it seems.
I expected to read some arguments why they think scala is the right choice..
That is probably coming in the next blog post.
As a codebase grows, all of the things that make Python "faster to develop" (dynamic typing, late binding) become an increasing liability. In my experience it ends up causing a net loss in productivity once your program gets large enough that you need to split your code into multiple files. 
I knew his name was familiar! This is the guy who got comically fucking owned in his blog post by the majority of the scala community because his learning disability made scala too hard. Lol this guy is a joke.
Think of it as building buildings. When you build larger builds you need more structured foundations, 2x4's won't cut it for a skyscraper.
Call it Stockholm syndrome if you want, but I actually consider the Enterprise FizzBuzz perfectly understandable. I think I need help...
I personally see it as an abuse of many of the patterns that make Java useful. Its a good reminder of how almost any pattern can be turned into an anti pattern. I feel that Monoidal FizzBuzz does a good job of abusing functional programming in a similar way.
It could almost certainly be ramped up a notch - no applicative functors? No ascii-art operators? I invite the Scala community to improve on my initial effort...
Same here...
[This](http://barkmadley.com/2009/01/22/fun-with-fizzbuzz-and-haskell-monoids.html) is the best one I've seen, and uses monoids like the OP's!
Oh gosh why did I click on all the folders to actually read that. The commit messages are hilarious, too...
Well this one doesn't model a rule as a monoid, but just flattens all the rules with &gt;&gt;= (bind) of the list monad. (flatMap)
Just like any language, do some projects, practice and be inquisitive. http://www.scala-lang.org/old/node/8610 This is old, but still might be helpful in pointing out what you dont know so you can fill in the gaps. 
Scala is compiled and sbt is slow as hell. What are you getting at? I mean I love Scala BUT this makes no sense.
I would recommend picking up a copy of "Scala In Action" followed up by "Scala in Depth" 
Was just a jab at the compile times. Not my favouritest thing about the language..
Scala for the Impatient. My colleague recommends it.
Now kiss!
I've never done one of these from Coursera. Is this really a 100% free course and if you complete it will you really get a certificate signed by the creator of Scala?
I read 400 pages of Programming In Scala by Martin Odersky and others in a weekend, and I didn't feel the learning curve at all. It's was a great jump start.
Good points. However I would disagree about implicits, because they are pretty standard stuff and also found in other languages.
Scala for the Impatient followed by Scala in Action or even O'Reilly Scala Cookbook. I didn't enjoy Scala in Depth.
That's pretty cool. Call me a nerd, but it might be worth it just for that alone.
I'm an android dev so I don't know too much about the java projects in the company. Thought about the spring petclinic example as something to rewrite live in scala.
You didn't plan to actually use Spring with Scala, did you? 
I tried once rewriting a simple project at my previous job that used spring and hibernate and it worked okay. Is there something about this solution that I should avoid ?
It just isn't the usual scala way of doing things. There are a bunch of other, better ways of acheiving the same goals.
https://www.youtube.com/watch?v=grvvKURwGNg Might be a nice inspiration.
http://www.scala-lang.org/docu/files/ScalaTutorial.pdf and http://www.scala-lang.org/docu/files/ScalaByExample.pdf and http://twitter.github.io/effectivescala/
My experience... I've been playing around with Scala for more than a year... And while I love the language, I feel that many developers wouldn't be able to understand the power of it. I consider myself quite good at self-learning, and I have a strong CS background, yet for Scala the path hasn't been as easy as I originally expected... taking my own experience doing R&amp;D with it, I feel like in order to establish a Scala team in my company there are several things that are going to take some time to deal with: poor IDE support, cryptic error messages and confusing operators which are almost impossible to search for in ( try searching for ":+=" in Google ). Also, concepts like monadic design or complex types aren't something "easy" to grasp for your everyday developer, yet every material you read about them (and this is particularly relevant in the case of monadic design) explains the concept in different, sometimes even opposing, terms. You're always left with the doubt about whether you got the concept right or not.
Existing devs, works best for new systems, mixing with existing Java code can be trickier but still possible.
1. IDE support 2. lack of intercompability even though this is supposed Scala's strong point (Java &lt;-&gt; Scala) 3. loose border between FP and OOP, its easy to mix both up which can result in unwanted results 4. unwanted implicit convertions, some should not happen without the programmer somehow knowing about them. 5. collections are inconsistent 6. hidden performance, though this should not always be the issue when you get clear readability, but the hidden costs behind closures in several for loops is extra high. as last I say, I feel like Scala has a high potential to grow like Java did in the last several years, but at the moment I dont feel like using it for production code until it does what it promises. **â‚¬: How can people even downvote in an opinion based thread?** 
We have a large Java codebase and have pretty successfully switched over to Scala for new development and most maintenance. I think the biggest challenge was a few developers who were simply unwilling to switch to a properly-typed functional style and would have been better off sticking with Java (it's easier to create a mess in Scala and harder to clean it up). So my advice is to pick a relatively isolated bit of functionality outside the critical path and use it to experiment with Scala, and see how it goes. You can expect to rewrite it several times as you realize you are an idiot over and over.
We haven't had performance issues, even with some very heavy use cases. 
My organization has a medium-large Scala webapp that we ported over from Java class-by-class and module-by-module over a period of about a year, maybe a year and a half. What it took for us: a) An enthusiastic tech lead b) Devs who were interested in learning. We started with one module, and went clsas-by-class in there. That kept the risk low. When that went well, we kept going. We didn't just port classes to port them. We waited for times when we'd implement a new feature, or refactor, and do the porting then. Scala's Java interop isn't perfect, but it's very good and worked for us in 99% of cases. I've heard of other teams starting by writing tests in Scala, which I suppose is fine. The key is to get started; once devs get a taste of Scala, it's very hard to go back to writing Java.
&gt; I'm amazed that both you and /u/thatsIch say that the Java-Scala interop is not as good as they say it is. You may have misunderstood me. Here's what I said: &gt; Scala's Java interop isn't perfect, but __it's very good and worked for us in 99% of cases__. Nothing's perfect, but I'll reiterate: Scala's Java interop is very good. It wasn't any kind of roadblock for us. Quite the opposite, really: it was a huge reason we were able to use Scala. I'll also remind you that we ported a large app class-by-class over a period of over a year. If Scala's Java interop wasn't good, we couldn't have kept the app in a hybrid state for so long.
Scala fullfilled all it's promises on my projects. In three years of full-time Scala use at work, I haven't really been harmed by any of your bullet points.
My company tried it (before my time). The conclusion was that the language is nice but the tooling isn't. The build times and inability to search in Eclipse for uses of a method were the main pain points. We still have Scala in production but we're not doing new development in it.
I lost interest in Scala after I saw that committer's rant about Scala. edit: thanks for downvoting me for giving my opinion in a thread asking for opinions! 
Well, If the input is a stream, I think the natural Scala-based option there may be by using actors instead of plain Scala... If the input is not a stream but it's coming from some sort of datastore (that somehow supports multidimensional spaces) you could probably use a map/reduce algorithm on a NoSQL datastore.
Can you elaborate on what you mean by poor IDE support? I use IntelliJ IDEA, and it seems to work well, although the syntax checking can be a little slow to catch up sometimes (thankfully it runs in the background).
I don't use Scala because my boss won't let me. Whether this is because nobody at the company knows it, or because you could count the number of local Scala developers on one hand, I don't know. In any case, what I do is write my code in Scala first and then port it to Java, which somehow still ends up being faster than writing it in Java to begin with. 
How dare you!
On my previous job, I was tech lead, and started using Scala for small tools, then for small projects, then eventually for everything I could, including a big critical backend system. I never looked back to Java again, there is no reason for it. Onboarding new devs on the projects was not very hard, most programmers can appreciate the difference in elegance and power between a Java program and a Scala version, even if they can't immediately grasp all the details. This lasted for about three years, now I've moved to a much bigger company, and unfortunately everything is way too big and complex to introduce another language in the mix, so I'm stuck with Java (again) for now :( 
Write an application and run on server with small memory usage. I wrote an application that run on Raspberry PI with 32 MB memory. A JVM application can run with a small resources was very interesting. 
What about exception handling? Isn't that better than Java's checked exception system.
&gt; then obviously you did no calculation which involves millions of loops in multi dimensional spaces This type of extreme edge case doesn't really sound like a good fit for the JVM in general. We're running some very high throughput systems in Scala in production and it's worked very well.
We've also been using Scala in production for a while now and also haven't been affected by any of the bullet points. 
Messaging and distributed data aggregation systems, no Akka, but we are planning to use Akka for a different system, also with high throughput requirements. 
&gt; Edit: ItÂ´s quite strange to see downvotes in an opinion/experience based thread. Is this the norm here? I suspect the downvotes are for the endless (re)raising of "issues" which those of us who use Scala professionally don't actually encounter. Posters who love to link to Paul Phillips video showcasing obscure corner cases of the collections API add little to the discussion. Most full-time Scala developers who've watched the video respond with "so what?", whereas the detractors who seldom use Scala tend to bookmark it for every such discussion. IDE support is another item which has little bearing on reality. Scala support in IntelliJ is simply superb. The only issue with Scala which is annoying from time to time is the binary compatibility restrictions. You have to make sure every single lib that you use (including all those written by us internally) is compiled with the same major version. Moving to a new Scala version is a bit of a nuisance, but still much better than having to code in Java all day!
Back in 2008, when JavaFX was just around the corner, I was maintaining a Swing CRUD app with some custom components and a Hibernate backend. I threw in some Scala (including a trace amount of scala-swing) and it became less horrid. I still wake up screaming every other night though. Whatever you decide to do, for the love of god, don't use Swing.
I haven't worked with Play, SBT or the worksheet (I tend to use the REPL via cmd line rather than the worksheet) so perhaps my IntelliJ experience is different to that of others. 
You're probably being downvoted for not contributing useful content to the discussion. At least discuss your own well-considered opinion rather than admitting that your opinion is simply an echo of someone else's rant about edge cases in a particular package. For the record I wasn't one of the down-voters.
You could check out the Atomic Scala book. I personally like Scala Koans(google it).
I'd be interested to know what IDE you work on. 
&gt;Have you tried using Scala classes (with traits + bodies) in Java? Example?
You don't need to use Google for unknown operators if you use IDE. Also, there aren't that many as some people claim. Here is my comment containing all the operators from the standard library: http://www.reddit.com/r/programming/comments/se7mw/scala_operator_overload/c4dc9p1
A lot of the downvoted comments here have superficial and strange criticisms and/or complain about downvotes (which is against reddiquette). The entry post to this thread seems like an excuse for railing against Scala and bothering the community, not with an eye for discussing the issues and challenges as well as how to solve them. &gt; Also, you know what technologies are not questioned? The ones that nobody uses. I've had to cope with questioning to the Java platform for a good part of the last 15 years. And it's been good that way: it means people still care for the technology I've chosen for my career. This reinforces my impression that your goal was to begin a thread for the sole purpose of railing against Scala and the community. Constructive, relevant criticism is valuable, but the criticism here is generally poor or strange. Complains like "loose border between FP and OOP, its easy to mix both up which can result in unwanted results" and "lack of intercompability even though this is supposed Scala's strong point (Java &lt;-&gt; Scala)" are weird and are not explained. And then there is [this reply](http://www.reddit.com/r/scala/comments/26pdb3/what_is_keeping_you_or_your_company_from_starting/chtddbj) which is simply strange. Complaining about IDE support without being specific, while not reflecting on the past and the future (which would not be in a detractors favour, since the IDE support has improved and continue to improve a lot for both Eclipse and IntelliJ IDEA), give a distorted perspective on the general state of things. Why didn't you explain further in your earlier reply about IDE support, such that the developers of the IDEs could use it to improve the IDEs, or that people could deal with those issues appropriately? And you highlight in this comment the word "painful", which give me the impression that you are trying to give others a specific impression of how things are. It is border-line manipulative. Why not just tell others your experience in an objective, emotionally neutral way?
&gt; But you can't expect people not to question technology. It is this &gt; questioning what brings technology forward, not the lack of it. I &gt; would understand downvotes for an incorrect response, or a &gt; disrespectful answer. But downvoting people just because they &gt; question something you "like" is just plain stupid. That's not what he said.
&gt; 1234567920 if I recall correctly, because round only exists in float &gt; and double and there is an implicit cast from int to float scala&gt; 1234567890.round &lt;console&gt;:8: warning: method round in class RichInt is deprecated: This is an integer type; there is no reason to round it. Perhaps you meant to call this on a floating-point value? 1234567890.round ^ res0: Int = 1234567890 
&gt; Ever used round(1234567890) well, System.out.println(Math.round(1234567890)) has the same problem in Java, and anyone that has read the JLS knows why it happens. Strange? Yes. Scala's fault? Not even a bit.
&gt; WHAT? Sorry, but that's not true. And even if it were true, I think that for any technology that has any potential to be used somewhat seriously, be it in academic or enterprise environments, the best thing you can do to plan ahead and make sure you pick the right tool is not only show off their positive points, but also remark any negative points and see if the trade-off works with you. "And even if it were true": If it is true, you are trolling, and you should be banned from this subreddit. And given that you say "even if it were true", I get the impression that you are seeking to justify trolling this subreddit, now by seemingly saying that you would be doing it for the sake of others, namely so that they can determine whether Scala would be a good fit or not for themselves or their company, and if so in which ways. If you claim that you are doing this for yourself, it is not consistent with the thread and the comments you have made. Why not ask about potential issues to be aware about with Scala? Why not search about previous discussions, such as on /r/scala, /r/programming and elsewhere? The comments you made also indicates that you have plenty of knowledge about Scala and its potential issues, which would not be consistent with you asking earnestly about potential issues. If you claim that you are doing this for others, you ought to let people figure things out on their own, there are plenty of posts and threads that discuss Scala's strengths and weaknesses as well as what to be aware of. And people can always ask questions themselves if they want to, including more specifically for their case (for instance, if they were curious about using Scala for hard real-time programming, they could be advised against it due to its reliance on garbage collection, which can make running time difficult to predict and thus make it difficult to verify that deadlines are indeed always met). Supposedly doing it for the sake of others seem like a really weak excuse. &gt; Don't get me wrong: I started my post saying that I'm "slowly falling in love with Scala"... Now, for me it wouldn't be serious for someone involved in decision making -as me- to overlook negative points about a technology I'm choosing. The way I have to test the waters is to compare my experience with the community's, and that was the original intention of the post. Then why weren't you more straight-forward about that? Why not tell more about your specific issues? This claim doesn't fit well with neither the thread post or the comments you have made. And somehow you both believe Scala is the future of the JVM platform and also you are in serious doubt about Scala and its potential issues. What? &gt; Trying to silence any negative points with downvotes (instead of constructive comments and creating a useful discussion) or "demanding" that any posts in /r/scala (or whatever technology forum/subreddit) consist just of positive facts about the language sounds like circlejerking to me. There are comments in this thread and other threads that discusses the language and various libraries in a constructive and critical manner without being downvoted. There are also a number of comments here that offers criticism and have not been downvoted. Your claim that "any negative points" are being silenced with downvotes is wrong. &gt; Now, if everyone in the community is so self-entitled not to allow anyone to have doubts and only expects praise and worship for their beloved language, then I might be better out of this "community", because it sounds more like a religion than a group of people using a tool and trying to improve it. And this seems like more railing against the community. Constructive criticism is welcomed, trolling isn't.
&gt; The only Java you need to know is maybe some standard library stuff. Later on, you'll need to know a little of how the JVM works, which can lead to (or require) knowing a little about Java because there's such a close correspondence between Java code and JVM bytecode. But that's not something for the very beginning.
I started learning Scala using [the official book](http://www.scala-lang.org/documentation/books.html) (Programming in Scala) with zero Java background. It was tough at the beginning, especially since I never heard of functional programming before, but the book is pretty well written, and it nicely eases into topics. Just download the book, fire up the REPL, and start playing. Not knowing Java, and not being familiar with its idiosyncrasies may even be beneficial. It may save you from having to "unlearn" things. 
I don't think you know what trolling is, then. And if you check my comment history both on this subreddit and everywhere else, you'd see that i'm not the trolling kind, anyway. Maybe it's just that you are overreacting, but I don't see any way in which any of my comments here might have come as trolling, offensive, disrespectful or dismissive of Scala as a language. There's a big difference between objective evaluation of technology and objective evaluation of a technology in an enterprise context. Java succeeded in the enterprise world not necessarily as a result of its innovations as a language, but thanks to support of many companies that saw in it a big platform with relatively good tooling, a very active community and an ever-growing set of open source libraries (in addition to the already impressive feature set of the standard libraries). I can't go to my company's CTO office and just say that I'm going to pick Scala for some new developments because it's a kick-ass language, without giving him an actual evaluation of alternatives and associated risks. The fact that Twitter and LinkedIn are using Scala is a huge selling point, but my company doesn't have the size or power of them, nor the ability to create a 20-person R&amp;D team to investigate during a couple of years the potential uses of the language and its drawbacks. So that's why (And I can't believe i'm having to explain this for a third time) I wrote my post: I know and love all the goodies and nice things of Scala, and i know many of its drawbacks too, but I want to know how relevant are they in an enterprise context and what has been the community's experience trying to walk the road I want to walk too.
Thanks for the excellent links. JavaFX seems like the place to be for UIs. A web-oriented UI isn't off the table, I'm just pressed for time and the more moving pieces I add to this the more stressful it becomes. We did actually start by creating a simple application that ran a spray service, and created a systray icon. We then tried to load a simple javafx webview pointing to the service (a PrimaryStage shown by a menu item on the systray icon). It didn't appear to make any request. Chrome could display responses from the service, and the webview could load www.google.com just fine.. but this was enough to dispel any confidence in that approach. The application is primarily AKKA based (more or less a state machine that maps a serial protocol to a UDP based protocol). The UI is intended to be a simple window into the current state of the app with some controls to adjust settings.
And read their blog even if you do not use the product. It is awesome! One of the best regularly updated blogs on JVM-related issues. Edit: Link to the blog: http://www.takipiblog.com/
So scala is better at dealing with exceptions. Thanks!
Thanks, I'll check that out.
Alright that's nice. Thanks.
&gt; There's a big difference between objective evaluation of technology and objective evaluation of a technology in an enterprise context. Java succeeded in the enterprise world not necessarily as a result of its innovations as a language, but thanks to support of many companies that saw in it a big platform with relatively good tooling, a very active community and an ever-growing set of open source libraries (in addition to the already impressive feature set of the standard libraries). I am not quite sure what you mean by "enterprise" requirements. There is no generic "enterprise" thing even though the term gets bandied about a lot. Let me turn the question around 1. What are your team's requirement? 2. Are you the person who makes technology decisions or is it your manager? 3. What kind of software are you developing? 4. What are your client's requirements? If you have a direct communication channel open to your CTO then it sounds like you work for a small company. Enterprise typically refers to larger companies. We are willing to help you make your decision, but a brief description of the kind of software you are developing and what kind of customers you have would be helpful. The term "enterprise" is too generic to help create any meaningful discussion. Being more specific will get you more helpful answers. Generic pontifications on enterprise, constructiveness, criticism, community, self-entitledness are not going to help anyone.
One thing knowing some Java might help is _appreciating_ what you're learning. Scala has some features that are particularly beneficial in contrast with what you have to do when writing Java (`case class`es being a prime example of this). Though, to be sure, Scala has a lot of the same benefits when compared to C++, so I'm sure you'll be able to appreciate what's going on. But in my short time with Scala so far, I haven't felt that tutorials and so on relied on my (small) knowledge of Java.
This explain many things. Thank you!
I haven't worked with ScalaFX or JavaFX, yet, mainly because it was not fully open-source and available on all (and older) platforms until recently (?). Contrary to what @eriksensei is saying, I think Scala-Swing is a fine choice. The API is fairly simple, the event handling is a lot nicer than Java-Swing, although not "functional" or "reactive" in the sense of composition. As a plus, you can swap out the look-and-feel, for example if you prefer to have native looking GUI. As far as I know, you can also embed JavaFX components in a Swing GUI.
Thought I'd follow up. I'm finding the book to be very helpful, but I'm having trouble. He keeps saying to run scala programs using for example: "$ scala myprogram.scala" Is that for Linux? I'm running Scala in command prompt through sbt right now.
No, it is just in a new artifact name space: [Maven Central](http://search.maven.org/#search|ga|1|a%3A%22scala-swing_2.11%22)
These are pretty good ways of getting into Scala: * [scala-koans](https://bitbucket.org/dickwall/scala-koans/wiki/Home) - Gives a overview of the language * [daily-scala](http://daily-scala.blogspot.co.uk/) - Beginner to Advanced * [Neophytes Guide to Scala](http://danielwestheide.com/scala/neophytes.html) - Intermediate to Advanced
No, it's not linux-specific, it works on Windows too. Just [download and install scala](http://scala-lang.org/), and afterwards make sure it's in your path so you can access it from anywhere. You won't need SBT until you start working on whole projects with dependency management, publishing and all that. How come you started with SBT? 
I was doing the Coursera course, and it had me install SBT. I've been tinkering with it. I can just use Eclipse I guess, right?
&gt; I have found it to be fairly fast What sort of tasks have you managed to get running? On what size inputs and on how large of a cluster? 
My experience at a couple of Java shops has been this: * At one shop, there was openness to Scala and Play in theory, but it would become a wider effort as much of their process (deployment, monitoring, automated code-quality reports, versioned releases and continuous integration) is built around a specific infrastructure and tool chain. Once this shop upgrades past Java SE 6, maybe they'll make the leap to Scala too. * At another Java shop, technical conservatism ran far deeper, and the advice from architects was, "Good luck," with an implicit argument of, "You're tilting at windmills." The company was quite large, culturally rather conservative, and generally slow to adopt any kind of new technology; there overall approach to people, technology, and process was a slow, cautious wait-and-see. Open-source and especially GNU-style free software was looked upon with some skepticism (again, a culturally conservative business leadership that bled over into IT and looked at things like the GPL as verging on *communism!* in the American epithet sense of the word). There were also pragmatic concerns of who would maintain the Scala code? There was, by and large, incuriosity among my coworkers to learn a new technology unless forced by management. All of these things added up to reasons why I left the company, needless to say. In short, I think the skepticism a Scala advocate will have to overcome follows along these lines: * **Maintainability:** Are there other Scala developers in the area we could hire to maintain the codebase if you left? * **Easiness:** Scala can obviously produce more concise code than Java, but the power opens up a wider field of syntax, libraries, and concepts a developer will have to have mastered to understand a random chunk of Scala code compared to Java's simple verbosity. * **Sunk costs:** The company has sunk money, people, and time into application servers, processes, and tool chains that are familiar and have worked for them. Especially if they would still have a sizeable codebase using, say, Java EE, they cannot abandon existing infrastructure, which means more release processes and tool chains to maintain. I think some of this may be more seen in line-of-business enterprise development where people see the way they've done things for the past ten years as fine and see a lot of risk to change with uncertain benefit.
Allow me to rant a bit... One of the most horrible memories I have was trying to figure out how to make a table behave more or less like Excel. I ended up getting reasonably close I think, but it took me ages to figure out going trough numerous tutorials and very dated source code. In the beginning, I also made the naive mistake of thinking: "Hey, NetBeans' GUI builder, cool! I'll use that." The code it generated was, for no apparent reason, tightly coupled to JPA. This meant you couldn't observe a List; instead, you had to observe a QueryResult, which was typically a List. It was near impossible to customize the generated code. I ended up converting bits of that to GlazedLists. Then there's the layout. DesignGridLayout ended up working nicely, but all the 'recommended' alternatives, oh dear... Oh, and decent looking components. I went with a couple of MacWidgets because my client wanted that sort of thing. They looked cool, and worked pretty well, but interaction with more standard Swing components was a bit hairy, and things ended up being not very modular between standard Swing and MacWidgets. Unfortunately, development on it stopped, or at least stagnated for a couple of years. I'm sure a lot of this could have been prevented if I'd known certain things (e.g. which tools and libs to use/avoid) beforehand, but I felt it was like pulling teeth getting anything working quickly, reliably and nicely. Who knows, it might even have been pleasant...
True, but the important difference for me is whether you can look back thinking it was worth it. Learning to play the piano or how to do advanced maths for example are like that; they expand the mind. Swing, on the other hand, turns it into alpaca snot.
Now that you mention it, one of the other issues was that Sun was pushing their Desktop Application Framework at the time, which was supposed to add a lot of stuff missing from Swing. (The GUI builder made use of it as well.) Subsequently, they abandoned it. So if I needed to write a Swing app again, I'd first I'd have write or find something like that. One of my other beefs is the quality and lack of customisability of Swing's components. So I'm sure I could write something ultra simple fairly quickly, but beyond that, I rather doubt it.
I nominate Â¿=+?=Â¿Â±?
Is that a question?
With how many features? How did the performance compare with a single-machine library like Cuda-Convnet?
To "unlearn" things... Well said. For someone who has a quite long background in Java, I had an issues to remove some of my Java habbits. Like writting unnecessary verbose code in imperative way. 
It sames x)
No GPUs for this dataset, eventually, it will need to scale to many millions of rows which can't be stored in RAM on a single machine... Performance factor vs local native JBLAS implementation is linear with #nodes s.t. 1 &gt; k... appx 0.7n to 0.8n
&gt;No GPUs for this dataset, eventually, it will need to scale to many millions of rows which can't be stored in RAM on a single machine... Isn't that typical for deep learning data sets? Typically I've seen the GPU splitting its memory between the weights and a subset of the training set. The remainder of the data doesn't have to necessarily be in memory (you can overlap loading from disk with GPU computation). 
Strange, it worked perfectly for me. I managed to slice, join, group and count more than 15 million records coming from diferente hdfs files. Also ran it on a ec2 cluster with 12 machines. The difference is that I used python instead of Scala. And you are right, with spark you quickly get used to RDD operations. Well, wish you better luck next time. 
The raw RDDs individually, yes. But after the first iteration, combining them, no. That is why I had to start a cluster using the the very spark scripts. Even with 12 machines, it took hours to finish. 
Unrelated question, do you work at github? I didn't know github slung scala.
Second that, really terrible, at least on Windows + Chrome.
There was a .net port of scala in the early days, so porting is possible in the theoretical sense. I heard that it has been cancelled but I can't find anything now, maybe it is alive. 
It is certainly possible to implement the compiler differently things like jython and python implement almost the same language. Also clojure has been implemented in several different ways. However a big advantage to Scala is it's interoperability with java which would be lost if it were implemented in a different way.
Of course it can, and it should. The most serious effort currently is Scala.js (http://www.scala-js.org/). It is reaching version 0.5 soon and it is already quite amazing. Scala.js shows that there is nothing that prevents Scala to be an excellent language outside of the relatively limited context of the Java VM. As mentioned, there was a .NET effort at some point as well. This was abandoned, probably because of lack of interest. But that could be revived at some point. There was also an effort to compile down Scala to LLVM (http://www.infoq.com/presentations/Scala-LLVM), but there doesn't seem to be much progress on this lately. By the way this is what you would need to "compile down to Objective-C". Swift is natively compiled ahead of time, like Objective-C, C, C++, Rust, or Go. Scala is typically compiled down to bytecode (or JavaScript in the case of Scala.js), and then a virtual machine compiles that down further, even to native code for hot code. But you can certainly compile Scala ahead of time, with possibly a few limitations.
a) Syntactical similarities don't necessarily mean the semantics of two programming languages are even remotely comparable. b) Depending on how Generics are translated, the same issues as with the (now defunct) CLR backend apply in terms of supporting Scala-style types. b) Objective-C is no bytecode, nor is the native code generated by Objective-C compilers bytecode. c) I'm not sure that it would be a win from an overhead perspective. Parts of Objective-C and its runtime are incredibly inefficient. I believe that it be worked aournd mostly by avoiding these parts, but I don't believe that using reference counting is a valid technical approach in 2014. (It also means that some code that works on the JVM would leak memory on the Objective-C runtime.) d) I think more backends for Scala are a worthy goal, but I don't think that targeting a proprietary language on top of a proprietary runtime on top of a proprietary operating system should be the first priority.
I don't work for GitHub. Actually I just noticed that BitBucket has a lot of Scala tools, though. 
Thanks for clarifying. I'm sure I made a lot of errors (I'm new to Scala!). Your comments are illuminating. Can I ask you (I know you aren't Google, so apologies ahead of time), what is the difference between "Java bytecode" and whatever Objective-C becomes after it's compiled?
I've been hearing about Scala-js. Does that mean it's usable on node.js?
Java bytecode is machine independent, which means that before execution it needs to be translated to native instructions. This means that one Java "binary" runs on every platform which is supported by the JVM, regardless of the machine's instruction set, the platform's endianess, architecture width etc. If you compile Objective-C code for target X, it will only run on target X. If you want to support multiple platforms, you will need to compile your code for every platform.
Came across this yesterday, thought I would share for anyone who is interesting in Scala/Play Framework. I'm still new to the Scala/Play world, so hoping to be able to learn a lot!
It's been cancelled, and the corresponding branch was deleted from Github: https://github.com/scala/scala/pull/1718 You can still use IKVM to run your Scala programs, just like programs using almost every Java library out there, but this doesn't count as "support for .NET" in my book.
No, i don't work at github. And i don't think they will go for Scala any time soon. At least, didn't read anything about it yet.
Just to muddy the waters a bit... :-) [Clang](http://clang.llvm.org/) is the [LLVM](http://llvm.org) project's C/C++/Objective-C compiler. Like any such compiler, it generates what you might call "bytecode" (LLVM actually calls it "bitcode") as its "intermediate representation" (IR). There are then several things you can do with the bitcode. One is to run the bitcode with [lli](http://llvm.org/releases/3.4/docs/CommandGuide/lli.html), a bitcode interpreter. Another (far more common) is to generate "machine code," which is the set of instructions natively understood by the processor(s) in your machine. [Here](http://clang.llvm.org/docs/UsersManual.html#cpu-architectures-features-and-limitations) is the documentation on which native targets are currently supported by Clang, meaning you can (at least in principle, depending on OS support, etc.) generate machine code for any of them. So the apparently moribund scala-llvm project mentioned above would, by virtue of using the LLVM infrastructure, be able to target a similar range of processor/OS combinations natively, if it were successful.
OK, the reason I asked is because the namespace was com.github :)
I don't think it's the "main obstacle" or "most of the effort", based on what I see in Scala.js. There is a pretty minimal subset of the Java library which needs to be supported, that is true, but it's doable and in fact has been done.
I think that the most important scope for Scala is the server side, and the most cases it runs in Linux server. I think that in the future, we will have a Scala compiler :) I hope it.
True, but while on Scala.js you can do certain shortcuts, like relying on JavaScript's runtime, when targeting native code generation, there are lot of features that need to be implemented from scratch.
It already exists. Just make use of a JVM with AOT compilation. There are quite a few to choose from.
Just found this at the start of the last talk. They mentioned that all the talks are being recorded and should be up on youtube around tomorrow. I'll come back with a link when I know where those will be if someone wants. Edit: looks like they're all up on the [typesafe youtube channel](https://www.youtube.com/user/typesafehub)
Agreed, there would be more work to do if there is no existing runtime. But likely there would be one. I assume, for example, that you wouldn't have to necessarily re-implement all string operations, or regular expression support, if you were to compile natively for iOS or OS X. To be more specific, Scala.js already implements its subset of the Java library directly in Scala, and that could be reused at least in part with different target environments (https://github.com/scala-js/scala-js/tree/master/javalib/source/src/java).
Why on earth would you want to do that? 
To write code in a decent language while using some of the 76,590 npm modules available (https://www.npmjs.org/), maybe?
Do you plan to support password-protected zip files?
All I am saying is that there are valuable libraries running on node. I also like the idea of having an alternative runtime for Scala. As for performance, let's wait and see. The V8 team is not asleep as far as I know.
Sure, i intend to make it support everything that we can do with the ZipEntry java object. Also, it would be amazing if i have some contributors :) EDIT: When i feel safe about it, i am thinking in deploying it at Maven
why does anythingBut match multiple characters? shouldn't it be multiple(anythingBut()) (in some way)? iow: you need some way to express nested expressions.
Well, two points worth considering: * Clojure is considerably easier to port to different platforms for mainly two reasons: it's a LISP (very little and consistent syntax), and it's a dynamic language. This last point is particularly important since a good and advanced static type system is a considerable engineering effort. * Clojure is also an *hosted* language, this means that it doesn't wrap the underlying platform, but interoperate nicely and natively with it. Scala I suppose could do the same (and I think it already does in Scala.js). This has important benefits: a portable language with native interoperability with the host platform. Obviously Scala in the JVM and in a JS VM serve different purposes, but I think this portability has a lot of value.
Looks good. I can imagine it would prevent many mistyping mistakes that otherwise would need extensive testing to catch. Can you give details on what goes on under the hood.
Here is a quick review about Anorm change with Play 2.3: http://applicius-en.tumblr.com/post/87829484643/anorm-whats-new-play-2-3
That's pretty nice! I would have used `exactly` or `just` instead of `andThen`, though, because _all_ those combinators have an implicit `andThen`. It sort of affects the way I read it in my head.
Sorry, I thought that I was looking at my messages. /facepalm
Trust me, you wont get the Luxury of dependable IDEs like C++ has. So forget it, wait for an decade or so, i gave up, just try that SBT , it will make you mad like HELL, only sick demented sheeple use Scala, specially who use PLAY , bloody blind followers, sick to the core academic kids.
Answer : Proper tools eco system with IDE and GUi tools.
Basically what they want are Blind followers, they immediately downvote whoever question or raises valid points against their fandom gems, mostly due to inability to think and fathom the questions, thus they were born sheep.
Mentions plans to open source some things @ 09:50
As a target for the Scala compiler, an [Erlang VM](http://www.erlang.org/) could be interesting since that offers lightweight processes. But I don't know whether this would be practically feasible. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Erlang (programming language)**](https://en.wikipedia.org/wiki/Erlang%20(programming%20language\)): [](#sfw) --- &gt; &gt;__Erlang__ (/ËˆÉœrlÃ¦Å‹/ *__ER__-lang*) is a general-purpose [concurrent](https://en.wikipedia.org/wiki/Concurrent_computing), [garbage-collected](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science\)) [programming language](https://en.wikipedia.org/wiki/Programming_language) and [runtime](https://en.wikipedia.org/wiki/Run_time_system) system. The sequential subset of Erlang is a [functional language](https://en.wikipedia.org/wiki/Functional_language), with [eager evaluation](https://en.wikipedia.org/wiki/Eager_evaluation), [single assignment](https://en.wikipedia.org/wiki/Single_assignment), and [dynamic typing](https://en.wikipedia.org/wiki/Dynamic_typing). It was designed by [Ericsson](https://en.wikipedia.org/wiki/Ericsson) to support distributed, [fault-tolerant](https://en.wikipedia.org/wiki/Fault-tolerance), [soft-real-time](https://en.wikipedia.org/wiki/Soft_real-time), non-stop applications. It supports [hot swapping](https://en.wikipedia.org/wiki/Hot_swapping), so that code can be changed without stopping a system. &gt;While [threads](https://en.wikipedia.org/wiki/Thread_(computer_science\)) require external library support in most languages, Erlang provides language-level features for creating and managing processes with the aim of simplifying concurrent programming. Though all concurrency is explicit in Erlang, processes communicate using [message passing](https://en.wikipedia.org/wiki/Message_passing) instead of shared variables, which removes the need for explicit [locks](https://en.wikipedia.org/wiki/Lock_(computer_science\)) (a locking scheme is still used internally by the [VM](https://en.wikipedia.org/wiki/Virtual_machine) ). &gt;The first version was developed by Joe Armstrong in 1986. It was originally a proprietary language within Ericsson, but was released as [open source](https://en.wikipedia.org/wiki/Open_source) in 1998. &gt;==== &gt;[**Image from article**](https://i.imgur.com/Yvb2PQJ.png) [^(i)](https://commons.wikimedia.org/wiki/File:Erlang_logo.png) --- ^Interesting: [^Functional ^programming](https://en.wikipedia.org/wiki/Functional_programming) ^| [^Elixir ^\(programming ^language)](https://en.wikipedia.org/wiki/Elixir_\(programming_language\)) ^| [^Actor ^model](https://en.wikipedia.org/wiki/Actor_model) ^| [^Aum ^Programming ^Language](https://en.wikipedia.org/wiki/Aum_Programming_Language) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+chzsagh) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+chzsagh)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Once you target a js vm, you could theoretically end up anywhere - ios, android, windows or yes browser. 
Or better yet (and maybe it's already the case), an *abstraction* that enables this kind of things. Consider also that exists [AutoCloseable](http://docs.oracle.com/javase/7/docs/api/java/lang/AutoCloseable.html).
The cleanest method I have seen is to represent your closable operations as composable effects: class Closer[C &lt;: Closeable, A](val f: C =&gt; A) { def run(c: C): A = { val a = f(c) println("***CLOSING***") c.close() a } def map[B, That](g: A =&gt; B): C =&gt; B = f andThen g def flatMap[B](g: A =&gt; Closer[C, B]): C =&gt; B = { c =&gt; (f andThen g)(c).f(c) } } and evaluate them using the composed `run` method and an actual `Closable` instance: def file = { println("***OPENING***") new RandomAccessFile("jabberwocky.txt", "r") } val cat: RandomAccessFile =&gt; Stream[String] = { x =&gt; println("***READING LINE***") Option(x.readLine()) match { case None =&gt; empty case Some(line) =&gt; cons(line, cat(x)) } } val closer = for { stream &lt;- cat lines = stream.take(4) _ = lines foreach println } yield Unit closer run file Check out [Using java.io.Closeable as a Monad](https://github.com/earldouglas/scala-scratchpad/tree/master/closer#using-javaiocloseable-as-a-monad) for a complete example.
TL;DR? Is it just tuning some VM parameters or anything interesting?
I think that calling getConnection twice would lead to a resource leak. This code could be improved in sense of fewer ways to misuse it. Create class Connection, move resource holders there, make them private and immutable, close logically belong there. Assuming .close() are idempotent that class would be impossible to misuse. Depending of your design intentions it could be useful to implement Observable rather than have as an attribute. case class Connection(private val conn:Connection, private val sess:Session, o:Observable[String]) { def close() {...} } From you example I can't see any reason for getConnection to be a member of some class, so: def getConnection(user:String, pass:String, url:String): Connection = { conn = ... create some connection Connection(conn, sess, observable) } It's not particularly related to Scala or 'being idiomatic' (whatever it means), I'd use the same design in Java or Python.
Very easy to understand source: https://github.com/pathikrit/ScalaVerbalExpressions/blob/master/src/main/scala/com/github/verbalexpressions/VerbalExpression.scala
Good idea, will create those aliases too
Its not just VM parameter tuning, they forked the OpenJDK.
That's true, but in the specific case of node.js I just can't see a valid use case (the only one I can think of is running the unit tests for scala.js!) since the JVM is always going to give better performance, more libraries, and will generally be easier to work with in Scala. Even if there is some node.js library you need that isn't available, you'd probably be better off exposing whatever you need as a service instead of doing the whole thing in scala.js. 
From skimming the talk I mostly saw that this part was mostly concerned with improved profiling, not much which would have an impact on performance.
 final def findAvailableRaces(races: List[Race], abilities: List[Ability]): List[Race] = { require(races != null &amp;&amp; races.length &gt; 0, "Races collection is null or empty") require(abilities != null &amp;&amp; abilities.length &gt; 0, "Abilities collection is null or empty") def isSupported(race: Race, ability: Ability) : Boolean = { ability.Name match { case Strength.Name =&gt; (race.MinStr to race.MaxStr).contains(ability.score) case Dexterity.Name =&gt; (race.MinDex to race.MaxDex).contains(ability.score) case Constitution.Name =&gt; (race.MinCon to race.MaxCon).contains(ability.score) case Intelligence.Name =&gt; (race.MinInt to race.MaxInt).contains(ability.score) case Wisdom.Name =&gt; (race.MinWis to race.MaxWis).contains(ability.score) case Charisma.Name =&gt; (race.MinCha to race.MaxCha).contains(ability.score) case _ =&gt; throw new IllegalArgumentException("Unknown ability : " + ability.Name) } } for { ability &lt;- abilities matchingRace &lt;- races if(isSupported(matchingRace,ability)) } yield matchingRace } You can simplify the expression and get rid of the need for an intermediate collection with a for-comprehension. You can find several tutorials on for-comprehensions with the scala-lang article [here](http://www.scala-lang.org/old/node/111). EDIT:Race should have a collection(or a Map) of Abilities with min an max values so you don't have to pattern match on Ability name, and instead filter on ability properties. Race Map: type RacialAbility = Map[Ability, Range.Inclusive] val abilities = (Strength -&gt; (1,20), Dexterity -&gt; (1,20), ...) Character Map: type Score = Int type CharacterAbility = Map[Ability, Score] type CharacterStat = (Ability, Score) val abilities = (Strength -&gt; 3, Dexterity -&gt; 4, ...) def isSupported(racial: RacialAbility, stat: CharacterStat) : Boolean = { val (ability, score) = stat (for { range &lt;- map.get(ability) } yield range contains score).getOrElse(false) } EDIT2: Your test doesn't pass because your implementation is incorrect. Instead using foreach, you want to determine if for all abilities does your pattern hold true. A working implementation looks like this: final def findAvailableRaces(races: List[Race], abilities: List[Ability]): List[Race] = { require(races != null &amp;&amp; races.length &gt; 0, "Races collection is null or empty") require(abilities != null &amp;&amp; abilities.length &gt; 0, "Abilities collection is null or empty") def isSupported(race: Race, ability: Ability) : Boolean = { ability.Name match { case Strength.Name =&gt; (race.MinStr to race.MaxStr).contains(ability.score) case Dexterity.Name =&gt; (race.MinDex to race.MaxDex).contains(ability.score) case Constitution.Name =&gt; (race.MinCon to race.MaxCon).contains(ability.score) case Intelligence.Name =&gt; (race.MinInt to race.MaxInt).contains(ability.score) case Wisdom.Name =&gt; (race.MinWis to race.MaxWis).contains(ability.score) case Charisma.Name =&gt; (race.MinCha to race.MaxCha).contains(ability.score) case _ =&gt; throw new IllegalArgumentException("Unknown ability : " + ability.Name) } } races.filter(race =&gt; abilities.forall(ability =&gt; isSupported(race,ability))) } Your test also doesn't apply values to all abilities, so the values of Int, Cha, Const, and Wis, are 0. Making your test incorrect.
Yes as far as I understood it, this is just the beginning. 
You should use Seq instead of List. List is a specific I implementation while Seq is the trait. When you use Seq.apply it will use an generalist implementation (good for both insertion and random access) but List is a linked list, pretty bad if you need random access. Also usually in Scala we do immutable programming as much as possible. So no var, no mutable collection. Instead of building a result like you do, the more idiomatic way would be to transform from other collections.
Why not use pattern matching here somehow? val req = createRequest.map { res =&gt; if(res.status.isSuccess) res.body.as[String] else throw new Exception("Error " + res.status.code) } 
The current implmentation used for Seq by default is a List
There's a bit near the end where they mention they have a custom GC "generation"/area that can for example hold objects for 5 minutes before becoming part of New (avoiding repeated useless scans in New in the process), and since they know the items will be dead in 5 minutes they all get collected. The example was for a 5 minute cache. They then go on to use this in benchmarks.
Besides PHP, what else has Facebook eng. done wrong?
SBT = STUPID BULLSHIT TECHNOLOGY
PHP is bad enough.
BULLSHIT Video service.
It seems like they forked the OpenJDK and made it fit their use case. They are concerned with GC pauses and object allocation due to the volume of traffic, so they wanted to get better performance and better insight into GC and object allocation. 
Perfectly obvious in hindsight. Great idea, thanks! EDIT: and implemented in 0.2.1 (available off the sonatype repository). It's now possible to write that bit of code as: val req = createRequest.map { case Status.Success(res) =&gt; res.body.as[String] case res =&gt; throw new Exception("Error " + res.status.code) }
What's the deal with the crazy CPU cycles? Just running the REPL takes like 200 mb of memory and playing with the Scalatron JAR was like 70% CPU usage... Super odd. 
False. The default implementation for Seq is Vector.
It should be asyncronious.
There's nothing to prevent you from making asynchronous by wrapping calls in a Future: val res: Future[Response[ResponseEntity]] = Future {req()} Moreover, if it *were* asynchronous by default, it'd become really hard to control the number of open connections. That's not a factor for everyone and is one of these exotic requirements that I mentioned, but I have to interact with services that will start tar-pitting requests if too many connections are open too fast (and keep-alive is not used). That's precisely the reason I had to move away from Dispatch who, while great, is always asynchronous and ended up opening hundreds of HTTPS (expensive) connections where Fetch uses keep-alive and sent hundreds of requests through a total of 4 connections.
I've refactored code as you suggested, thanks!
Their god-awful APIs. Bad documentation, broken OAuth support, randomly change without warning, no industry standards, just best practices that everyone finds a way around. 
Why all the final modifiers on methods? 
I don't know about "crazy" and "super odd", but it makes sense for the REPL, given that it both creates a new JVM instance (which has some overhead) and runs the Scala compiler. On my system, the REPL takes up memory in the range of 80-140 MB. Conversely, the Clojure REPL only takes up about 70 MB, which makes sense given that Scala's compiler requires a lot more than Clojure's compiler. As for the Scalatron, I don't have any experience with it, though I imagine the CPU usage could be due to having a game incorporated in it. High CPU usage is not uncommon for real-time games, since they tend to continuously run and update various simulations (like a physics simulation) in small increments (such as 30 or 60 times per second).
because I wanted to harden the code, I felt that Scala kinda forces you to explicitly state mutable and immutable objects..
Thanks for the answer. This is pretty neat. 
Yes, you're correct, I wanted to state that changes to the class are locked,it shouldn't be extended or its methods/variables be overriden.
Not according to [Scaladoc](http://www.scala-lang.org/api/current/#scala.collection.Seq$). &gt; This object provides a set of operations to create Seq values. The current default implementation of a Seq is a List. 
Thanks, I will definitely check that out!
Sorry, I corrected the code above because it didn't compile, of course.
boxing/unboxing elision
Haxe.
&gt; The V8 team is not asleep as far as I know It will never be as fast as a statically typed language running on an awesome VM, can it? 
Go to a Scala repl and check right now. It is a List and has been that way since at least 2.8. There used to be a typo in the documentation that said it was a Vector but that got fixed in 2.10: https://github.com/scala/scala/commit/36adada0d53f9cd982a122558dd654aa9e99e1bf
Well, that's really strange. I was absolutely sure it was Vector. happens.
IThis optimizer is at least a year old, and even the linked documentation page lists as not having been updated since February. Did something change/improve? Why is this relevant now?
They're integrating it into 2.11.x, so this version isn't maintained anymore. I hope they can finish the work quickly, it can make a huge difference in performance. The bytecode generator is already part of 2.11, you can use it with an experimental option.
Open sourcing thrift, not releasing any of their internal changes, then re open-sourcing an incompatible version called fbthrift.
&gt;Moreover, if it were asynchronous by default, it'd become really hard to control the number of open connections Just configure the [http client](https://github.com/dispatch/dispatch/blob/master/http/src/main/scala/thread/thread.scala) dispatch uses 
Interesting. Thank you for taking time to answer my questions! 
Oh I did, although it took me an embarrassingly long time to work out it was possible, but while it helped, it didn't fix the problem. Tracking all connections with lsof would show many more open ones than the maximum I had set (or the maximum number of threads I had configured, or any combination of the two I could think of). Mind you, this is absolutely fine if you're writing a crawler, for example. But in my very specific case, I was hitting the exact same resource on the same host hundreds of times per second, and it seems the number of connections looked a bit too much like a DOS to the firewall...
That sounds like the usual shitty work-arounds common in the JVM/Hotspot world. I'm hoping that some day people will sit down and decide to fix at least some issues at their core, instead of trying to paper over the symptoms.
instead of working to port AOT to JDK these guy are still living in 1995.
Scala has a DI feature built in: `implicit` parameters. Unless you need to reconfigure them at run time, you don't need any framework or special technique. There's also self-types and the cake pattern, but I'll be blunt: the cake pattern is fucking hideous.
Scala has perhaps an even more fundamental DI feature built in: functions.
Can you produce a test case to demonstrate this? Why not report this as a bug to Dispatch developers? Why create your own library for what's clearly a simple bug? There's nothing about being asynchronous that makes limiting the number of connections difficult, and it's naive to believe that making something asynchronous is as easy wrapping it in a future.
implicit parameters are functions, this doesn't really add anything.
On the contrary, we can build up a minimal DI framework using only non-implicit functions. First we represent "things that need dependencies" as functions `E =&gt; A`, where `E` is the type of the dependency and `A` is the output given the dependency: def run[E,A]: E =&gt; A Next, we wrap them up in a type that defines both `map` and `flatMap`, so that we can build up more complex functionality while abstracting away the dependency without losing track of it: case class Reader[E,A](run: E =&gt; A) { def map[B](g: A =&gt; B): Reader[E,B] = Reader(e =&gt; g(run(e))) def flatMap[B](g: A =&gt; Reader[E,B]): Reader[E,B] = Reader(e =&gt; g(run(e)).run(e)) } Now we can build combinators that represent computations with dependencies, and combine them in a clean/uniform way to get ever-larger computations with depencies: def get(k: String): Reader[Map[String,Int],Int] = Reader(m =&gt; m(k)) val resultR: Reader[Map[String,Int],Tuple4[Int,Int,Int,Int]] = for { foo &lt;- get("foo") bar = foo * 2 baz &lt;- get("baz") raz = foo + bar } yield (foo,bar,baz,raz) Finally we're left with a program, `resultR`, that needs a dependency, a `Map[String,Int]`, to run. We can provide this dependency at the appropriate time (i.e. when the dependency is known): println(resultR.run(Map("foo" -&gt; 14, "baz" -&gt; 2))) // (14,28,2,42) Implicits can indeed be used to extend this capability, but they're not necessary.
Your example illustrates how to perform DI with functions, which wasn't in doubt, instead of illustrating how applying functions yourself is different from having the compiler choose functions to apply. Going from from a monad to implicits to macros, are all 3 different ways of doing the same thing; decoupling types from their dependents without escaping static checking by the compiler by parameterizing them. Implicit parameters just take this model and reduce verbosity in the application. EDIT: I would argue that reducing verbosity is the primary motivation behind DI frameworks, else everyone would parameterize everything.
I'll accept the criticism for not filing a bug - I needed something to work *now* and writing it myself was the only way to meet my deadline, but you're right that once the rush over, I should have taken the time to investigate further and file a bug. As to why create my own library, well, aside from the fact that I needed it, it's fun. It's a learning experience. It's something I enjoy doing. I'm sorry you find this offensive, and can only be relieved that I didn't write a JSON library with parser combinators, *that* would have landed me in deep trouble. I'm confused by your last comment, however. If wrapping a computation in a future does not make it asynchronous, what exactly does it do? I'm pretty sure it does not make it synchronous.
&gt; I'm confused by your last comment, however. If wrapping a computation in a future does not make it asynchronous, what exactly does it do? I'm pretty sure it does not make it synchronous. There's more to asynchronous HTTP clients than running the computation. There's shared mutable state and lmiited resources. The most important of those limited of resources are the connections, of which this discussion is largely centered around. Another is threads, another could be file handlers, or allocated heap for streamed responses. Simply wrapping a future does not manage these resources. Using your example: val res: Future[Response[ResponseEntity]] = Future {req()} would still block a thread if it had to wait for a connection to free up in a connection pool. EDIT: In general it's easy to make something asynchronous become synchronous. The inverse not so much.
I would have liked to hear from this presentation what problems using macros to do DI solves versus implicit parameters.
Ah, well that example is much nicer. Thanks for renewing my faith in Scala! I got into Scala because MATLAB allows native Java interop - and by extension, Scala, once it's compiled to `.class` files or a `.jar`. I'm not considering switching, just adding another tool to my box. (For the record, w.r.t. manually applying the rules: that Haskell version goes on to point out that `fizzbuzz n = mconcat [fizz n, buzz n]` is equivalent to `fizzbuzz = mconcat [fizz, buzz]`, so no worries there!)
Perhaps elaborate on that a bit. It's easy to describe something as "shitty" without suggesting an alternative optimization.
The comparison is just cosmetic unless you know something about Swift, and I don't, so I didn't get much out of it. The ? caught my eye, though. IIRC there was an old Scala white paper in which types were non-nullable by default, and you appended ? to turn a non-nullable type into a nullable one. When I got around to downloading Scala and trying it myself, I was a little disappointed that they had switched to nullable types and Option (maybe because Java interop made non-nullable types impractical.) I wonder what the difference in practice is. 
Like not doing stupid stuff (throwing away types) causing the boxing in the first place. Until boxing elimination can convert an ArrayBuffer[Int] from being backed by Array[AnyRef] to being backed by Array[Int], boxing elimination works only in the least interesting places.
That would be useful for arrays of AnyVal too :)
Why link to a twitter post that links to the PDF? Why not link to the PDF itself? [It's here, incidentally.](https://raw.githubusercontent.com/densh/talks/master/swift-vs-scala-211-2014-06-03/Swift%20vs%20Scala%202.11.pdf)
Why do you feel that it is hideous?
In the specific case of DI, the cake pattern creates an "is-a" relationship among the slices. For the usual "I need a `FoobarLookupService`, a `WidgetFactory`, etc" DI style, this isn't correct: the dependencies are something you *use,* not something you *are.* The cake pattern also makes API documentation much harder to understand. Take a look at `scala.reflect`: the top-level types are cake slices, not the types you'll be working with (`Type`, `Tree`, `Symbol`, etc) when you use that API. I realize *why* the cake pattern was used for `scala.reflect`. All of the types are path-dependent, so a `scala.reflect.runtime.universe.Type` is not the same thing as a `scala.reflect.macros.Context#universe.Type`, and this distinction is actually enforced, which is cool. But this makes an absolute mess of the documentation, which isn't cool. I'm not sure what design approach would be better, but I am sure that this one is pretty bad.
assigns does work in scala val (a,b) = (1,2)
have you take a look at spray http ?
&gt;I'm not sure I understand the first problem. This seems like an arbitrary distinction. If you have dependencies then you 'are' something that needs those dependencies. Can you clarify this a little more? I mean getting dependencies like this: abstract class MyApp { this: DatabaseConnectivity with WebPageTemplate with AuthenticationService =&gt; â€¦ } I think this way is cleaner: class MyApp( protected val db: DatabaseConnectivity, protected val template: WebPageTemplate, protected val auth: AuthenticationService ) { â€¦ } &gt;Isn't the second problem a general problem with DI, or more generally, a problem with coding to interfaces? Maybe I'm not entirely understanding since I haven't spent much time with 'scala.reflect'. No, it's actually got nothing to do with DI as such. Rather, it's a problem with how cakes sometimes end up looking. I think you'd have to familiarize yourself with `scala.reflect` to really understand what I'm talking about. It's kind of a jungle in there.
couldn't you accomplish your cleaner way using traits, which is out of the box in scala? trait DatabaseService { val db: DatabaseConnectivity } trait AuthService { val auth: AuthenticationService } class MyApp with DatabaseService with AuthService { def doSomething =&gt; { db.withAuth(auth).doSomething() } }
giving credit to the author, the guy tweeted did made those slides
&gt; giving credit to the author I'm not trying to be flippant here, but isn't that why the title page of the PDF listed the author's name in a large font?
never said it was cleaner. IMO, it gives you the same result as declaring abstract valS. However, MyApp now has these 'injected' via the traits vs coded in. If you wanted to write a test and not use a real db, then you can have a MockDBService trait that you could mix in instead of DatabaseService. 
It sort of depends on what you mean with "connection pooler", but I've had great success in using and getting into the details of [Spray](http://spray.io/).
+1 for spray. Using it in production for two years, simply the best. But not an easy one for Scala beginners
But using normal parameterization you could already pass a MockDBService as a constructor argument. The cake pattern just looks like a solution in search of a problem. And the "has-a" vs. "is-a" distinction is important when you're creating your object because the services you're using (e.g. DBService that the App 'has') should probably never be accessible outside the owning object (e.g. the App). That's easier to control when you simply pass a service argument and declare it private.
agreed, you can do DI via constructor, via setters, and via traits. However, all these solutions give you compose-able classes (has-a) vs 'is-a'. I like the fact that scala gives you many paths to the same goal. 
Only at declaration. Try: var (a, b) = (1, 2) (a, b) = (3, 4) (a, b) = (b, a) // pythonic swap
Yes, the `Sized` type in shapeless does this. See https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#collections-with-statically-known-sizes 
Thanks!
What's the use case for this? I wonder if there isn't an easier way
Its for a matrix library... I want compile time checking for the validity of matrix ops.
Poor Academic kids.
You should delete that github link if you're going to troll this hard.
Is this sarcastic ? 
The `#scala` IRC channel on FreeNode is excellent.
Cool -- which of these techniques is your go-to btw? http://freenode.net/irc_servers.shtml Thanks for the tip.
I'm coming into this with more experience with Ruby (and javascript, which is less relevant). It seems that JVM can be learned by way of Scala or Java, correct? So that doesn't seem like too much of a barrier to entry. All that said, it seems like there is a high likelihood that as a Scala programmer, I would be working alongside Java developers. So it would be useful to have a bit of a shared language with them even if it's not my core competency. Thanks for your insights. Based on your comment, it sounds like I'd be better off keeping my eye on the ball towards Scala rather than getting too side-tracked with Java.
IntelliJ is no more a Java IDE than Eclipse. The Eclipse plugin is a plugin as well, even if Typesafe calls it "Scala IDE". Scala support in IntelliJ is very good.
Also often you'll want to use a Java library, so be able to read and understand Java code samples is useful to understand how to use it in Scala.
You should start with a build.sbt file with a scalaVersion key and any dependencies defined, and then import the project into IDEA. It will set up the correct Scala version and pull the dependencies for you.
&gt; I figured out early on that using Homebrew to install Scala didn't give you great IntelliJ support Why would it? Installing the JDK into your path doesn't mean IntelliJ or Eclipse will use it. &gt; hey, look in /usr/local/Cellar/scala for the latest Scala version and use that. Stop defaulting to 2.10.2 IntelliJ doesn't configure the version of Scala you compile against, SBT does. &gt; Shouldn't that be updated when Homebrew updates Scala? I still don't understand why you think Homebrew will solve any of your problems. Installing scala in with Homebrew only puts scala executables in your shell's path. It doesn't configure your project, or IntelliJ. &gt; is, are there any plans to develop a non-ported Eclipse-based Scala IDE? IntelliJ is expensive and it seems like I'll just never be happy with it... are there any open source projects out there that could potentially offer a better alternative to IDEA with more native SBT integration? Has the community ever discussed this? IntelliJ support for SBT and Scala is great, you just seemed to be confused for some reason. What resources have you been reading about IntelliJ and Scala? Links to those resources would be helpful.
Looks pretty good. One suggestion would be to use a partial function in displayResults so that you can destructure the tuple at the start and not have to use _1 and _2. You can do that when you use {} rather than () around your lambda. def displayResults(map: Map[String, FileCollectionDetails], total: FileCollectionDetails) = { map.foreach { case (key, value) =&gt; ??? } } One possible improvement would be to generalize your countLinesInFilesRecursively function by allowing the caller to pass in a function to be called for each file found. Then you could reuse your file structure traversal code to do different things. You could further generalize by allowing the caller to specify a predicate to filter the files. For a script like this I probably wouldn't bother with either of them, but it's something to keep in mind. I think there's already a traversal function built in to the NIO file acess API, that might help. Case classes technically don't have public fields, though Scala makes it look like they do. Scala generates a getter (named myField()) and (for vars) a setter (named myField_=()) and calls those instead. This is true for regular classes as well.
Not really. If you understand Scala, then most Java could be understood as very basic Scala. 
Thanks for the suggestions. Exactly what I was hoping to get from the post. I'll probably try over generalizing the functions to see how that feels. It might be overboard for this instance, but I'm more interested in learning than anything else - the script itself it fairly pointless. And thanks so much on the map tip. I knew I had to be doing something wrong with the hacky kv._1
That makes sense! But I should note that the original version of the method (before the student is told to mess around with it) looks like this: def play(e:Char, x:Int, y:Int):String = { if(x &lt; 0 || x &gt; 2 || y &lt; 0 || y &gt; 2) "invalid move" else cells(x)(y).set(e) } and works just fine...?! Still, I'll try having it return some random string and see how it goes.
Ah I got it--that .set(e) ends up returning a string of course. 
Ugh. It actually works better if I just put in a random string at the end of the "else" block--but then of course when I run it that random string displays after the board displays. iunno. It's late. I should just come back to this.
Looking at it more, the for loop is fouling stuff here. I assume you have it there for debugging, but it's snarling up the return. The cells(x)(y).set(e) returns the String which is then returned by play, but adding the for loop interrupts that. If you want the for loop after the set call, you need to do something like val stuff = cells(x)(y).set(e) for {//blah blah} stuff You may be getting confused here by the lack of an explicit "return" keyword. The value of the last statement is what pops out of the function. The for loop returns an empty Unit value which is what the compiler was complaining about. You need some sort of String at the end.
Yeah, it wants a String, so sticking a random one there gives it what it wants to return. If you want the for loop to remain, you need to save the result of the set call to a val assignment or whatever, then stick that value in after the for loop where you have the random string now (see my example in my other comment above)
Question for IntelliJers using the Scala plugin: how do you guys get multiple sub projects imported into the IDE via your build.sbt/Build.scala? With Scala IDE there's the sbt-eclipse plugin. Assume similar plugin for IntelliJ. FWIW, on Linux IntelliJ was truly hideous (as of 2 years ago, last time I tried installing it), the Swing based UI made it really, really unpleasant on the eyes. Anyone on Linux confirm still the same or does the UI look native now like Eclipse? Eclipse is good enough provided you do an absolute minimal build and hack the UI to strip out all the wasted space (i.e. create a VIM style panel interface with no wasted pixels). Anyway, cool that there are at least 2 options for IDE based Scala development...
Figured out something that works. Made the for-loop part its own method ("display()"), then *outside* the class definition, made a method called "playDisplay" that just calls the play method then the display method. 
You have to define the project with subprojects defined in build.sbt: lazy val project1 = (project in file("modules/project1")) .settings( libraryDependencies ++= Seq( "org.scalatest" % "scalatest_2.11" % "2.2.0" % "test" ) ) lazy val project2 = (project in file("modules/project2")) .settings( // same deal ) lazy val root = (project in file(".")) .aggregate(project1, project2) .dependsOn(project1, project2) and then tell IntelliJ IDEA to import it and it'll sort out the subprojects.
well yeah, but *how* do you guys "tell" IntelliJ about the defined sub projects? With sbt-eclipse it's `&gt; eclipse skipParents=false`
Is this actively maintained? The last commit was six months ago. 
&gt; knowing how the JVM works is helpful once you get past being a beginner with Scala. Can you expand on this? I know many very good java and scala developers that don't know anything really about the JVM or the emitted bytecode. 
There is a lot of good technical advice in this thread that I'm not going to repeat. The key problem here is not the tools, it's you. You need to put in a little more effort to master your tools. Even the best tool requires a skilled operator for best results. Read. Learn. 
Improved? It hasn't changed for some time. Darcula has always been the better looking theme on all platforms.
You don't tell IntelliJ anything. When you import the project from SBT, you'll have IntelliJ submodules where the SBT subprojects are.
Fair enough. 
Sure, I'm not surprised--it's a beginner's textbook, and it's one of the first sections. 
And of course I meant Webapp but my phone autocorrected it took weapon :-(
I can't testify to what this functionality is like on non-Play web apps, but I'm currently doing this with a Scala + Play! web app and it's fantastic. Essentially, things like login and registration are handled by Secure Social. After the user is logged in, the client (which is written in Angular JS) opens a websocket with the server. The opening of the websocket creates an actor specifically for exchanging messages with the client. Everything from that point forward is handled asynchronously, and it's glorious. The good: * Play is already running an Akka actor system for you in the background. * Killing off actors whose associated websockets have closed down is easy. * Everything runs asynchronously. * Broadcasting updates to a client is simple. For example, if you have an online auction site and a user bids on an item, you can not only broadcast the update to the web tab from which the user's request initially originated, but you can also broadcast the update in such a way that anyone viewing the item will receive the update. It's magical and I never want to go back to anything else. The bad: * Because Play is already running an Akka system in the background for you, you're locked in to using whichever version of Akka ships with the version of Play you're using. Realistically though, this isn't likely to be an issue. * Things can be confusing to debug if you're new to Akka. For example, if an actor receives a message type that it doesn't know how to handle, it just ignores it and moves on. This can be changed in the settings, but is rather annoying at first. * Older web browsers don't support web sockets (but that's their problem IMO) [Backchat](https://github.com/backchatio/hookup) and [Socko](http://sockoweb.org/) both look like they are lightweight and might be able to get the job done. However, I think it's important to note that Play: * has a large amount of documentation and tutorials online for when you get stuck. * has all of the other features that you'll likely need from your web app (i.e. connection pooling, a templating system, etc.) * is considered stable.
I'm very new to this. And I'm trying hard, so I apologize for being so critical of the toolset. Scala is a very interesting language and I've committed myself to learning it. It really offers so much more than I'm used to from PHP. But getting started with it has proven to be far more challenging. I'm trying to manage my expectations so I can learn it more effectively. Nonetheless, I appreciate that you were direct enough to tell me that the problem is me. I see that now. And the information in this post is really useful. I'm taking some time to learn SBT better so I can build projects in the correct way. So, thank you!
Thanks for clearing that up. I guess I'm not managing my expectations properly. I'm on mobile so I don't have all the resources in front if me, but here's what I can tell you off the top of my head: I'm taking the Functional Programming Principles in Scala class on Coursera. It's a little frustrating because the assignments don't really get mentioned in the lectures. Coming from an imperative background, the class is just hard. SBT, which seems to be an incredibly important part of using Scala isn't really discussed at all. I want to start writing projects in Scala, so I'm also feeling impatient! Speaking of impatient, I'm also reading Scala for The Impatient by Cay Horstmann. I haven't finished it, I'm about halfway through and so far it hasn't talked much about SBT. I've also been playing with the Typesafe Activator and it does so much of the work for me that I end up with a lot of questions after I do a few of the examples. But I do like this tool a lot. I've also created a few small projects in the Play Framework and it's excellent but the documentation seems to be on a higher level. It also uses some Java libraries (Netty, specifically, and I think anorm wraps jdbc), so when I read stuff about Play, I feel a bit confused because there are some Java things I don't know much about. I've also watched a lot of the YouTube videos by Venkat Subramaniam and even Odersky himself to get a better high-level feel for Scala. Finally, I really love Alvin Alexander's blog. He just talks about Scala so eloquently. Really enjoy reading his stuff.
What's a mutable variable, and what's a better way to do the valid/invalid distinction?
Since the more graceful ways to handle it are too advanced for me at this stage, it seems like it would have been counterproductive for the book to provide code that handled it more gracefully--I would not have been able to understand it. (Indeed, I don't even understand your objection really. I get what it means in a semantic sense but have no idea why there should be anything wrong with what you describe. That's how much of a beginner I am.) But did I misunderstand you? From a teaching point of view, what are you thinking they should have been in the book instead of this code? (Everything but the for-loop stuff is what's in the book.) 
Awww, I came here thinking you were handling realtime aiming/firing of a [USB missle launcher](https://www.thinkgeek.com/product/8a0f/) or something.
Interesting, I wonder why the need for a theme at all? I mean if all IDEs were from the ground up completely stripped down of panels, menus, etc. leaving every pixel of available space for one's code, then the question of which theme is better becomes moot. In my Eclipse setup, while the hidden panels look native when I hotkey show/hide them, it's not at all important relative to where the vast majority of my time is spent in the IDE, coding. Saying that, I have no theme, blinding white default background, but have a screen calibration inversion program (xcalib package) that allows me to toggle screen color. By default I have my monitors set to black so in the case of Eclipse the *entire* IDE has a dark "theme". It's funny the dark themes available for Eclipse, you see people asking how to make everything black, including panels, tabs, etc. ;-)
errr, no, as the name *implies* skipParents=false means *include* the parent build definition *and* corresponding subprojects.
Amazing! Thank you. That's really some excellent advice. I'm eager to experiment with your recommendations. It's a little deflating tuning something in sbt and having the project "break", so I think this will remedy that. Scala does feel like a nice replacement for PHP. One thing I fell in love with quickly with respect to Play and Scala was the built-in http server and not having to use htaccess files. I'm pretty excited to keep learning it. Thanks again! 
In regards to the error handling, one issue is that String can encode an arbitrary number of values, and that it is not clear that it in this case means a result type with two values, namely success and failure. This makes error handling more bug-prone later on and makes maintainability harder. For instance, if someone wants to determine whether the result was a failure, they would need to match against the string "invalid move". If that string is later changed, or there is a spelling error, the failure will not be matched correctly. That can partly be avoided by assigning the string values to constants, but then users might accidentally match against the wrong constant later on. These issues can be avoided by encoding the result using a type that is more constrained and precise than String. For instance, a custom [tagged union](http://en.wikipedia.org/wiki/Tagged_union) can be used to encode precisely two values, namely MoveSucces and MoveFailure: sealed trait MoveResult case object MoveSuccess extends MoveResult case object MoveFailure extends MoveResult The sealing of the trait ensures that it cannot be extended outside the file it was defined in, meaning that the only values that extend it are MoveSuccess and MoveFailure. If the values are then matched against later on, the compiler can statically check that all the cases were matched against, and thus ensure that no cases were forgotten and left out. It also ensures that if cases are added later on, the compiler will give a warning everywhere the new cases are not handled, which can be very nice to have when refactoring the program. An example of matching on the values: val moveResult: MoveResult = MoveSuccess moveResult match { case MoveSuccess =&gt; println("Success") case MoveFailure =&gt; println("Failure") } If one of the cases are commented out in the matching, the compiler will give a warning for an unmatched case. For the previous example, a boolean type could be used instead of string with many of the same advantages as the tagged union. A more illustrative example of the utility of tagged unions would be to have several failure cases, as well as successes and failures with parameters. For instance, it might look like this for a registration form: sealed trait RegistrationResult // Success cases. case class RegistrationSuccess(userName: String, birthDate: Date) extends RegistrationResult // Failure cases. sealed trait RegistrationFailure extends RegistrationResult case class UserTooYoung(birthDate: Date) extends RegistrationFailure case class UserNameAlreadyTaken(triedName: String) extends RegistrationFailure case object CaptchaFailed extends RegistrationFailure If the registration results in a success, the values can be used directly, and if the registration instead results in a failure, the failure can be handled in various ways depending on which kind of failure it was. Either and Scalaz's `\/` are generic tagged unions with extra utility methods that both supports having two types of results, namely success (Right for Either) and failure (Left for Either). Another example is the type [Try](http://www.scala-lang.org/api/current/#scala.util.Try) for dealing with successes with some value ([Success(value)](http://www.scala-lang.org/api/current/#scala.util.Success)) and failures caused by exceptions ([Failure(exception)](http://www.scala-lang.org/api/current/#scala.util.Failure)).
I can learn from what you've typed, I think, but note that in the book we're talking about, traits don't come til later. So you may be arguing traits should have come first. But are traits basic enough to come before for loops? (In case it's not clear, the book is written for people with no programming experience. I have a little experience myself, but only very surface level so I thought I'd start with the basics.)
A mutable variable (denoted with `var`) is a variable that can be reassigned: var x = 3 println(x) // Prints 3. x = 12 println(x) // Prints 12. Mutable variables can be handy, but they can also be difficult to reason about, since their values may vary after their initial assignment during execution. Mutable variables tend to be more of an issue in larger programs, where having many mutable variables can make it harder to reason about the program. Constant variables (denoted with `val`), on the other hand, cannot be reassigned after their initial assignment: val x = 3 println(x) // Prints 3. // Commented out due to compile error: Cannot reassign to constant variable. // x = 12 Since they do not change after assignment, constant variables tend to be easier to reason about than mutable variables.
Okay, I thought you might be referring to the val/var distinction, but I wasn't sure because you said the code uses mutable variables everywhere but I only see one "var". This leads me to wonder two things (I'm thinking as a student here): 1. How can the task accomplished using the "entry" mutable variable be accomplished with an immutable variable instead? 2. Are there other mutable variables in this code than "entry" that I'm failing to identify as such as I read through it?
I suppose I could (would this work?) make every value declaration a val, and whenever I want to change a value, re-declare it. Recursive changes to a variable would have to refer to other variables to store intermittent values, which could get complicated. (This is meant as a humorous idea, not a serious suggestion.)
Have you been able to test any sort of scalability limitations with using WebSockets on Play? I'm considering using this for a server-backed mobile app I'm developing, and I'd like to just get an idea as to how long I can delay thinking about scaling. 
Current versions of the intellij Scala plugin pick up changes to the SBT project. If something is wonky, you can force a re-sync with the SBT stuff and IDEA will rebuild it's project data.
As someone who has been using ScalaJS for a while, this is the release that brings it from "works" to "awesome". - Aggressive but correct dead-code-elimination making for small binaries. This is something that is almost impossible to do in more dynamic compile-to-JS languages, or when using javascript itself - Unboxed primitive types, which together with typed code generation, makes for very efficient javascript which is impossible to get when compiling other source languages like python or ruby - A working development toolchain, including packaging, automatic dependency resolution through sbt/maven, and excellent IDE support. This knocks the socks off most *real* languages out there, never mind the compile-to-JS languages - Typed Javascript interop, together with working IDE support! Javascript interop is already easier than most other compile-to-JS languages, since Scala's syntax is similar to Javascript's and many code snippets can be C&amp;Ped verbatim and work. With typed interop, its *even easier than using raw javascript*, because the IDE will spot typos and mistakes even before compilation, as well as providing dropdown menus of available members together with documentation from MDN right inside the editor.
I'm afraid not, though given the interruptions I get at work that is now a very tempting side project...
Sounds pretty awesome. I want to try this out with a reactive framework like React.js
This is a puzzling explanation of something that is fundamentally very simple. An expression is **referentially transparent** if you can replace it with its value without changing the meaning of your program. Example: you can replace `1+1` with `2` but you cannot replace `println("hi")` with `()`. A function `f` is **pure** if `f(e)` is referentially transparent, given referentially transparent `e`. Functions that do not have this property are **side-effecting**. That's really all there is to it. 
Are "side-effecting" and "not referentially transparent" the same? I'm thinking in particular about nondeterminism: let's say ``f(0)`` sometimes returns 0 and sometimes returns 1. ``f`` is not referentially transparent, obviously, but it also doesn't have side-effects, right?
There was an interesting thought about this in one of Odersky's talk: http://youtu.be/kkTFx3-duc8?t=26m28s Here he says that it is immutable if it gives the same result always despite that we have a mutable map in the class. I never thought about this I just assumed that if I use var or mutable then that's mutable but this sounds somewhat reasonable. 
&gt; knowing how the JVM works is helpful once you get past being a beginner with Scala. &gt; &gt; Can you expand on this? I know many very good java and scala developers that don't know anything really about the JVM or the emitted bytecode. Well, you don't need to know *a lot* about how the JVM works, but things like how Scala (currently) compiles anonymous functions can have big performance implications in certain scenarios. Scala compiles functions that way because that's what the JVM required before Java 8. Concurrency is another example: how changes made to shared data become visible to different threads is defined by the Java runtime environment. Knowing how things get compiled - what becomes a field, what becomes a method, what your @volatile annotation turns in to - helps in this case. 
That's heartening. There will always be that competition for which language is most legible, but it's always going to come down to personal preference really. I'm more interested in Java because it is so widely adopted and supported across almost every realm of the tech industry.
There aren't any real production deployments I'm aware of, but there are a whole bunch of demos... - [Todo MVC Example](http://lihaoyi.github.io/workbench-example-app/todo.html) - [Ray Tracer](http://www.scala-js-fiddle.com/gist/9759723/RayTracer.scala) - [Scala-Js-Fiddle](http://www.scala-js-fiddle.com/gist/9759723/Oscilloscope.scala), the client itself being implemented in ScalaJS - [Roll](http://lihaoyi.github.io/roll/) All these come with source code, it's up to you to look at the examples and decide how mature it is =)
Too bad with the current market I can never become "senior" in Scala.
Having a full time job leaves very little time/energy for side projects.
As others have said, it is important to set up the build.sbt / build.scala file. What I have not seen mentioned is that you can make the entire process much easier by also using the [sbt-idea plugin](https://github.com/mpeltonen/sbt-idea). Install the plugin, then `gen-idea` from the sbt console and just Open Existing Project from within Intellij. edit: fix link
&gt; Scala [...] libraries [...] have better performances than their Java counterparts. Might be a more fitting header
I'm sorry, I don't understand what you're saying. I think you may think I was saying I couldn't import a package on one machine that I had installed on another machine. (Is that what you were thinking I was asking?) But that's not what I meant. What I meant was, I am able to import a package on this machine while in the REPL, but unable to import the package (on this same machine) when I run identical code using the "scala" command in the shell. (Also, what exactly is SBT? I see the term here and there but haven't picked up on exactly what it is yet.) 
Okay, today it's working correctly. This is odd. I did many, many tests last night so I know it wasn't me. It's on a machine hooked up to a network I don't control so maybe somebody was messing with something server related last night and it was screwing up path calls on my machine or something. Who knows. I don't know how that stuff works. Anyway, sorry, false alarm.
Couple of things: * you need to read up on what the CLASSPATH is - you'll never have to ask this type of question again! * I do not mean this to sound condescending, but do you know [stackoverflow](http://stackoverflow.com) ? It seems to me that it'd be a better fit for these questions, if they've not already been asked and answered there.
&gt; *Some* scala libraries have better performance than their java counterparts, *under certain circumstances* It's hilarious that it became "scala is faster than java".
Never heard of it, I'll give it a look. I do know about CLASSPATH, though. (Had to learn something about it just to install scala itself, at least by the directions I was following to install it.) It didn't seem to be the problem last night--since it worked correctly from in the REPL--and since it works (without my having done anything to it) this morning, I think the problem must have been something else. Edited to add: maybe CLASSPATH only applies when using the scala command, while in the REPL it has its own path independent of CLASSPATH? 
Just took a glance at stackoverflow--looks perfect for what I need, a place where it's okay to ask completely elementary questions. ;) So thanks for the pointer.
The author even points this out in his article. I think the title was clickbait, and the title was copied verbatim here.
At least the Java code is easier to read. I still don't understand who thought that Play's current JSON validation API was a good idea.
Better yet, most questions have already been answered, just include stackoverflow when you google.
Ignore him. SBT performs the same functionality an Maven and Gradle, but for Scala. http://www.scala-sbt.org . You'll be using it eventually.
I find the Play code much more readable, and, more importantly, concise. I dislike their use of custom operators, but it's not anything too esoteric.
If you don't like the ~, you can use "and" instead
The Scala code is fairly simple IMHO. I think your problem is more familiarity than complexity (btw Rich Hickey made an excellent talk about this called "Simple made easy"). You can read it line by line (see comment): val carValidation = From[JsValue]{ __ =&gt; // I'm working on Json ((__ \ "manufacturer").read[String] ~ // It contains a String called "manufacturer" (__ \ "seatCount").read(min(2)) ~ // It contains a Int called "seatCount", the minimal valid value is 2 (__ \ "licensePlate").read(minLength(2) |+| maxLength(14)))(Car.apply _) // It contains a String called "licensePlate", the minimal valid length is 2, the maximal valid length is 14 } The API also gives you excellent feedback in case of errors (cause, and location in the Json). The feedback in Java is, by comparaison, poor.
Yea, but I cant quote it like this, since its not written like this ^ ^
I can read the code just fine. The problem is when someone unfamiliar with the Play validation API has to do something. It's pretty much impossible to get anything done without reading through the (horrible) documentation, whereas the JSR-303 API is just about simple as it gets (annotate the field with whatever validation you need and run it through a Validator, or if you're using Jersey, just add the @javax.validation.Valid annotation to your method).
Good god, java code is horrendous. It is by far the least readable non joke language I've been exposed to.
Not according to this: http://cgi.di.uoa.gr/~biboudis/clashofthelambdas.pdf http://biboudis.github.io/clashofthelambdas/ This was submitted today to r/programming. 
what side of the line is perl on when it comes to the joke/non-joke language line? 
I'm not surprised, java's streams are (manually) specialized so they don't box. Scala views are broken, unmaintained and very slow especially for numerics. This article could have been a lot more interesting if it compared lambdas to imperative code because even java streams have a lot of overhead. Btw, it's possible to use java 8 streams in scala 2.11, [SAM support is still experimental though](https://github.com/scala/scala/pull/3037).
Also, try this library: https://github.com/pathikrit/dijon
I want to know that too.
And say goodbye to typesafety.
While I get your point, I'd like to point out that this operator is not validation specific, and is actually applicative composition. You can also use and if you don't like it. The API here is not the Json API, but an evolution designed to feel similar to Play users. It can also validate Form data, or pretty much anything (see: http://jto.github.io/articles/play_new_validation_api/). To be fair, the Json API documentation was completely rewritten, and is a lot better than it used to be.
I'm so used to clickbait I hardly even notice it anymore. By the time I've read the conclusion I've forgotten the title. The author clearly meant to debunk the myth that Scala is "always" slower than Java, which I think is a fair discussion to have given the seemingly common stigma.
I've seen some decent software in perl, but I've also seen the worst code imaginable in perl. I don't code in it enough to say why.
Very cool indeed; the live browser update stuff was nice. Haoyi, if you are reading this, you should share you Scala setup with us. :)
This is pretty great. It could have been even greater if they had a more believable voice actor/narrator, but that probably costs money. Anyway, still fairly great.
The setup is in a slide at the end of the presentation; you should be able to just clone the repo from github, hit `~fastOptJS`, and you're all set with a live-reloading TodoMVC app
Hilarious!
"Flatmap that shit" .... I chortled 
I'm not aware of any JSON-RPC clients written in Scala, but one may be out there. In general, if all you can find are Java libs, you might want to write a little wrapper around the Java lib to make using it more natural. Depending on the lib, that's often pretty straightforward.
Take a look at this https://github.com/playframework/play-slick The play-slick-cake-sample shows how to set it up
I did some testing with Play 2.2 for a sorta-kinda chat app I'm building. If you are looking at 10k-50k simultaneous connections you should be fine with a 512MB VPS server (but that is really the lowest you should go, a 1GB will give you some room to breathe). Just don't go with the cheapest offerings out there, I had some real issues with the server itself when accepting all those connections. I noticed a significant rise in kernel wait times which I eventually traced to the emulated network card.
Hooray! My time to shine! I've been using Slick w/ Play! for a few months now. Check out [this part of a project](https://github.com/freiguy1/playground/tree/master/app/models/balance) on github. It's my playground project where I just make little apps for fun and practice. The db/Tables.scala was generated by the slick code generator. I think it does a pretty good job although sometimes I wish it would capitalize variables and things differently (mimic the db). The Balance.scala is an object that I use in my controllers and such to get data. I just put all my data access methods in here.
The Typesafe Activator should have some sample applications that might be useful https://typesafe.com/activator 
May I ask why a Squeryl user is moving to Slick? Syntax-wise Slick is a big step down, IMO, just look at the groupBy/sortBy hoops one has to jump through, pointless boilerplate all over the place, yick. Now, saying that, Slick is arguably the future of Scala database access given that 2 Typesafe employees are working on the project at or near full-time; the same cannot be said for Squeryl which, while having a stable-ish feature set, is a slow moving train. Not sure performance-wise how the 2 libraries stack up against each other, suspect Slick might be faster in the parameterized query department (since Squeryl has to regenerate the statement on each invocation), but overall probably a wash given that Slick does not yet always generate optimal SQL (see extraneous sub selects issues on their ML). As for the question at hand, sorry, not using either library, on a patched ScalaQuery for Scala 2.11 here ;-) p.s. avoid cake pattern unless you *really* need it, increases compile times and adds complexity where a simple trait/class implementation can cover straightforward scenarios.
Holy shit. My body is ready.
Have you taken a look at finagle? I don't think they have a json protocol packaged but you can define your own servers and protocols, maybe you could combine one of the java json parsers.
I do use Slick, actually, and have code in it (hence, not entirely objective). At the very least I would say the code you offer would benefit from refactoring. The principle benefit I find in ScalaQuery/Slick is composable queries, limiting, to some extent, the issues you point out. However, I did have to ask Stefan about issues around the 22-column limit that led to the "nested tuples" work, and hope someday the "comprehensive comprehensions" proposal is taken up, or, alternatively, another mechanism for better destructuring and type inference is adopted (maybe something emerging from the dotty work, for example).
&gt; I do use Slick, actually, and have code in it Oh, so you do know the territory then! Assume Slick was your first experience of Zeiger's work? (i.e. no ScalaQuery) &gt; At the very least I would say the code you offer would benefit from refactoring Interesting, what would you refactor? In `(p,r,s) &lt;- scoring`, scoring is a query that is itself composed of 2 queries that join player detail and scoring stats. The above query is used as a base for both team and league stats, but could also apply to regional, etc. stats. Trust me, the entire dao layer is composed to the max, query code reuse rules ;-) I'd like to see the same top level query written in Slick just to see how calculating sums and counts looks when combined with groupBy/sortBy, am up for a chuckle ;-) Anyway, back to the point -- namely, BP and hoops -- my main gripe with slick is that in order to retain naming you have to repeat yourself everywhere. For example, it would be nice to use the shorthand aliases ur,u,r from the below query, but we can't, have to groupBy case match, totally non-DRY. for({ ur &lt;- UserRoles; u &lt;- ur.userID; r &lt;- ur.roleID }).groupBy{case(ur,u,r)=&gt; (ur.status, ur.expires.desc, r.role} The alternative is to just live with tuple notation: for({ ur &lt;- UserRoles; u &lt;- ur.userID; r &lt;- ur.roleID }).groupBy(_._1.status, _._1.expires.desc, _._3.role} Which works fine for the simple case, but good luck you if you have to map/flatMap/groupBy/sortBy in complex queries/calculations, you'll quickly have an unreadable mess of _._Xs to sift through, or, if named aliases are important to you, you'll need to case match *for each* operation.
This is awesome. 
Hm, this sounds remarkably like spray.io I wonder why spray didn't meet their needs. 
Thanks! I've been trying to learn Scala and thought this would be a fun project.
Thanks. I'm a beginner when it comes to Scala and functional programming so any comments are welcome. Is there a reason you have lists.reverse? In this case order doesn't matter. I'm also wondering, is your method faster and more memory friendly than my implementation? I'm pretty sure mine is horribly inefficient by creating a new List each iteration.
Also, on actually trying to implement it there needs to be a ".flatten" call. Otherwise you get a List[List[String]]. val tmp_words = (for ( list &lt;- lists ) yield words(list)).flatten Thanks again for showing me this trick.
with lists.reverse you have the same order you'd have with your concatenation tmp_words = words(list) ::: tmp_words without the reversing it would be the same as this tmp_words = tmp_words ::: words(list) &gt; I'm also wondering, is your method faster and more memory friendly than my implementation? probably, since my method doesn't require a mutable list. in functional programming you want to try avoiding mutable sequences (hence you could use val instead of var) ~~edit: in second thought, your method doesn't require a mutable list either, so I'm not sure about performance differences~~ edit 2: wait, it does. your code wouldn't work if tmp_words was a val
It says they didn't use Spray because they need WebSocket support. I think Spray's WebSocket support still isn't officially released for some reason.
You could also do it like val tmp_words = for { list &lt;- lists; word &lt;- words(list) } yield word
I can't see the code you're talking about, aside from the words function, but is this what you're looking for? def tmp_words = lists.flatMap(words(_)) In terms of collections of "many" (List, vector, Map, Arrays...), you sometimes can think of flatMap as map(....) flatten, but where the separate functions require two traversals of all members, flatMap requires only one. A naive but equivalent implementation of flatMap could be written: def flatMap[S, R](fun: S =&gt; List[R]): List[R] = xs.fold(List())(_ ++ fun(_)) // assume xs is available to us.... // I used "S" for starting contained type, and "R" for the result contained type, // just for clarity, since we can't see S or R elsewhere.. To drive the point home: tmp_words.flatten == tmp_words.flatMap( s =&gt; s ) // true Again, I can't see if that'll help, since the code's been updated during this discussion - it's possible that you're just looking for List's .mkString function (..?) but I hope I've repaid you for compiling an amazing list of dirty words. If you need some clarity on the underscore (_) usage, let me know. Another separate scala style note about this function: def insertAt[A](e: A, n: Int, ls: List[A]): List[A] = ls.splitAt(n) match { case (pre, post) =&gt; pre ::: e :: post } it's possible to name parts of a tuple with val declarations, a la SML-ish style: def insertAt[A](e: A, n: Int, ls: List[A]): List[A] = { val(pre, post) = ls.splitAt(n) pre ::: e :: post } which could save you a good deal of typing in the future.
Thanks. That's actually exactly what I was looking for. I should probably revisit my approach to learning Scala. So far I've been thinking in a completely non-functional way. As to the insertAt function. That was just something I copied off the internet, but it's good to know. Thanks!
If you're like me, you probably spent a full minute wondering why there was no content before you noticed the little arrow in the bottom right. Click that to move forward in the slides.
If someone already knows the concept I described in the post, I would love to get some pointers to background information on it.
Was there, truly enjoyed the talk! Thanks for sharing the screencast. Hopefully the videos of all talks will be available soon.
UberFunctor would be a type class, fairly similar to ones in scalaz like Applicative, or Bifunctor.
I guess it's a relatively common issue: there are [two](https://github.com/lihaoyi/SprayWebSockets) [websocket](https://github.com/wandoulabs/spray-websocket) implementations for spray to choose from!
I don't get it =/ could someone do a ELI5?
https://en.wikipedia.org/wiki/Natural_transformation
I think you have to figure out which behaviour you need from these abstractions, and then you may have clearer questions of what abstraction you actually need. Try looking into [this](http://eed3si9n.com/learning-scalaz-day15) especially **&amp;&amp;&amp;**; Also you may note that **(a, b)** is [bi-functor](http://scalaz.github.io/scalaz/scalaz-2.10-7.0.4/doc/index.html#scalaz.Bifunctor) and use its API.
Your explanation sounds more like parametric polymorphism (i.e. generics) and less like a type class (ad-hoc polymorphism).
several valid critical points about the paper's methods are being made here: http://www.reddit.com/r/programming/comments/28mub4/clash_of_the_lambdas_comparing_lambda_performance/ Most importantly, it uses scala's collection with view, which are known to be slow (btw, is there a not-slow alternative to view?). Also, java's stream operations are not generic and can avoid boxing, while scala suffers of boxing being generic.
This is a duplicate posting of: http://www.reddit.com/r/programming/comments/28mub4/clash_of_the_lambdas_comparing_lambda_performance/ Edit: only realised now the posting was in a different subreddit Also some relevant discussion here: https://github.com/biboudis/clashofthelambdas/pull/1#issuecomment-46698813
Of course Scala is 840x slower when they use deprecated features (views over parallel collections), number crunching with generic collection operations, but no primitive type specialization (boxing and unboxing) and inappropriate JVM settings. See &lt;http://www.reddit.com/r/programming/comments/28mub4/clash_of_the_lambdas_comparing_lambda_performance/cidabs4&gt; for details. The solution is easy. You can keep using Scala in the currently wrong, but convenient and powerful way for number crunching with ScalaBlitz (or future Scala versions) that figures it out how to transform your high-level idioms into fast bytecode.
pros: scala-swing is actually quite good. the traits/functional class system is really good for making generic re-usable UI components. cons: tried to do everything immutable initially... terrible idea when you realize you're writing sendUpdate(model.copy(foo=bar)) for the 10th time...
Oof. The codebase I work on has a fair number of Option pattern-matches. Most of those are there because after some review (along with the rest of my team, I was learning Scala while working on this code) we tried refactoring the pattern-matches into map/filter/etc chains, and in the end, the pattern-matches were clearer. Some things were clearer as combinator chains too. Shrug. I guess this is why I'm not a blogger. "Consider each situation individually and use your judgment" doesn't make for a jazzy post title.
I agree, while flat mapping through the Option monad is fun and occasionally helpful it is not necessarily more readable. Especially when working on complex logic I prefer my matches to be spelled out in the plain. I do like exists though that's always better than matching. Another thing, the post recommends using Option(foo) instead of Some(foo) since the former automatically turns nulls into Nones. This is dangerous advice: When I get a null where I don't expect one, I rather suffer from the later null pointer exception instead of it being silently turned into a None. Only use option when you expect to get a null and really want that behavior.
&gt; Another thing, the post recommends using Option(foo) instead of Some(foo) since the former automatically turns nulls into Nones. This is dangerous advice: When I get a null where I don't expect one, I rather suffer from the later null pointer exception instead of it being silently turned into a None. Only use option when you expect to get a null and really want that behavior. Yeah, I thought that part of the article was needlessly dogmatic too. Sometimes I call out to Java libs that return nulls. When one of those uses null to mean "an optional field", wrapping with Option() makes sense. Otherwise it just swallows errors. Actually, there is one benefit to Option(), which the article may or may not mention: it gives a less-specific type, which is often what you want. scala&gt; Option(123) res3: Option[Int] = Some(123) scala&gt; Some(123) res4: Some[Int] = Some(123) 
Have you considered lenses for deep record updates, or do you just mean with regard to efficiency?
What's the merit of getting a less specific type?
Wow, I don't understand any of the formalisms :-( I understand the human readable part like section 6.3. &gt; Therefore there are noteworthy differences between Î»? and Scala implicits. In contrast to Î»?, Scala has subtyping. We do not think that subtyping is essential, and it complicates the formalization. Hmmm. So how much do you think the paper helps to understand the Scala type system?
 class Foo { private var state = Some(123) def clear(): Unit = state = None // nope } Although mostly you'll start with `None` and because of covariance would anyway need to write `Option.empty[A]`. __Edit__: For this reason, Scalaz has `some[A]` and `none[A]` instead whose return type is just `Option[A]`.
Can you put this question on Stackoverflow and tag it with "playframework"? I can take a look at it from there.
yeah I looked at lenses. i didn't want to define a separate lens object for each type of mutation.
I'm not using fold until I get an answer to this [question](http://stackoverflow.com/questions/23724220/in-what-way-is-scalas-option-fold-a-catamorphism). In any case Option[A] is just the [coproduct](http://en.wikipedia.org/wiki/Coproduct) of None and A. So to produce a value of type B from an Option[A] you need a map from None to B and a map from A to B. The former is just a B since None is a singleton. Fold, pattern matching, and getOrElse are all equivalent. There's really no way around it. Personally I think pattern matching is the easiest to read. Also it's completely different from the first example.
I don't think this is appropriate for SO because it's basically "write this for me" (and I'm kind of ashamed of it). I'm probably going to get downvoted and it may get locked, but here: http://stackoverflow.com/questions/24397459/java-json-controller-to-scala If you can figure out how to word it better, feel free to make an edit. 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Coproduct**](https://en.wikipedia.org/wiki/Coproduct): [](#sfw) --- &gt;In [category theory](https://en.wikipedia.org/wiki/Category_theory), the __coproduct__, or __categorical sum__, is a category-theoretic construction which includes as examples the [disjoint union of sets](https://en.wikipedia.org/wiki/Disjoint_union) and [of topological spaces](https://en.wikipedia.org/wiki/Disjoint_union_(topology\)), the [free product of groups](https://en.wikipedia.org/wiki/Free_product), and the [direct sum](https://en.wikipedia.org/wiki/Direct_sum) of [modules](https://en.wikipedia.org/wiki/Module_(mathematics\)) and [vector spaces](https://en.wikipedia.org/wiki/Vector_space). The coproduct of a family of objects is essentially the "least specific" object to which each object in the family admits a [morphism](https://en.wikipedia.org/wiki/Morphism). It is the category-theoretic [dual notion](https://en.wikipedia.org/wiki/Dual_(category_theory\)) to the [categorical product](https://en.wikipedia.org/wiki/Product_(category_theory\)), which means the definition is the same as the product but with all [arrows](https://en.wikipedia.org/wiki/Morphism) reversed. Despite this seemingly innocuous change in the name and notation, coproducts can be and typically are dramatically different from products. &gt;==== &gt;[**Image**](https://i.imgur.com/RX76voc.png) [^(i)](https://commons.wikimedia.org/wiki/File:Coproduct-01.svg) --- ^Interesting: [^Pushout ^\(category ^theory)](https://en.wikipedia.org/wiki/Pushout_\(category_theory\)) ^| [^Disjoint ^union ^\(topology)](https://en.wikipedia.org/wiki/Disjoint_union_\(topology\)) ^| [^Limit ^\(category ^theory)](https://en.wikipedia.org/wiki/Limit_\(category_theory\)) ^| [^Product ^\(category ^theory)](https://en.wikipedia.org/wiki/Product_\(category_theory\)) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cig6kt6) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cig6kt6)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Agreed. The downside of going all the way down the `flatMap` way with Option is that you might have a lot of classes to change from accepting a `Foo` into an `Option[Foo]`, which not always desirable and sometimes even not possible (e.g. calling into Java libraries). It's a compromise. I remember having the same dilemma with C++ `const`. Using it as often as possible is the right thing to do but `const` has this effect of infecting your entire code base to the point where it becomes very impractical. `Option` suffers from the same Midas effect. As always, use your judgment. 
Go ahead! :-)