I think what you're missing is that pattern matching in scala has multiple uses, some of which don't lend themselves to compile time checking. For sealed traits, you can rely on the compiler. For other cases, add a `case _ =&gt;` and handle it properly.
Definitely agree, but I'd probably use an HList as the collection for two unrelated types, since they don't really have a related base type (well, Any I suppose).
So in my original reply, I never made the claim that explicitly calling out something's monad-ness is common. BTW, I like OCaml, and ML variants in general. I totally understand that people don't really do FP in OCaml. They, in general, don't do FP in Scala, either. But I need to clarify some stuff. Just being nitpicky, here. Option is a monad, whether it is common to use in production code or not: &gt; Option isn't a monad # let bind option f = match option with | None -&gt; None | Some x -&gt; f x ;; val bind : 'a option -&gt; ('a -&gt; 'b option) -&gt; 'b option = &lt;fun&gt; from - https://realworldocaml.org/v1/en/html/error-handling.html#bind-and-other-error-handling-idioms https://ocaml.janestreet.com/ocaml-core/109.12.00/doc/core/Option.html That doesn't make this untrue, however: &gt; There is a Maybe monad in Haskell but no common equivalent in OCaml. I totally believe that is true. Very few people that program in hybrid FP OO languages actually do FP, and so they probably don't care that every function is explicitly total. Again, being nitpicky here: &gt; I think you're confusing functional with pure. Nope. I'm not confused. The only way code can be functional is if it is referentially transparent. That's the definition of FP. &gt;&gt; Functional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.[49] From https://en.wikipedia.org/wiki/Functional_programming#Referential_transparency There isn't a *pure* and *impure* functional program. There are programs that are referentially transparent except for in the runtime, programs that are referentially transparent except for a final run instruction on the IO instance placed there by the programmer, and imperative programs which mix functional and imperative flow control. Programs that mix aren't referentially transparent, and thus cannot be FP by definition, no matter how many times you call map/flatMap/fold, nor how many pure functions you compose in the application. One escape hatch ruins the batch, and people rarely use just one. Laziness is necessary to achieve purity when interacting with the real world. That is, we only care about purity when our program is at rest. So we delay execution until we are done declaring what the program is as a whole, then run it. When running, by necessity, it will have to change things in the real world. This is largely done through some kind of Monad in FP languages, which is why I said: &gt; You really cannot program effectively without them in functional languages. Now, any amount of pure functions mixed into an imperative program will make that program easier to reason about in the small, so all that mixed FP is useful in an imperative program, and requires discipline, a good debugger, and tribal knowledge to maintain. Not unmaintainable, not bad, but just not FP. &gt; The pedagogical purely functional data structures (e.g. from Okasaki's seminal monograph) are practically unheard of in production OCaml code. Again, I believe you that most OCaml programs in the wild are imperative, and use mutable data structures. I've not used in "in production". However, the language certainly encourages immutable data structures: &gt; As a functional language, most OCaml data structures are immutable and can't be modified after they are created. For example, OCaml lists don't support an operation to change the value of the head or tail of the list (like RPLACA and RPLACD or SETF in Lisp, or lset in Tcl). However, OCaml gives you the tools to create such mutable data structures yourself by building them out of references. http://www2.lib.uchicago.edu/keith/ocaml-class/data.html#ref &gt; IO isn't monadic in OCaml because OCaml isn't pure. A language need not be pure in order for you to do FP in it. It just makes FP easier. However, having impure IO makes the language an imperative language in which you can program functionally. &gt; Explicit monads are rare. Nah, I'm pretty sure that list is all over the place. I believe you that people don't call them that, and don't use the actual return bind/flatMap join unit abstraction that haskell/scala uses. But there are structures that conform to the laws, because they couldn't really work otherwise, and that's what makes them Monads. The name and code reuse doesn't matter. I think we probably agree that OCaml is capable of being used to write FPs, but, much like Scala, in practice it is just a slightly better structured imperative language. Which, for lots of things is OK. If you can follow all your program's state well enough, more power to you. We get so caught up in arguing over *the* right way to do things that we stop asking if it really matters in context.
&gt; Cats emphasizes on building a warm and welcoming community And yet I was blocked by many at typelevel for a single post on twitter for agreeing that someone's livelihood should not be threatened for saying mean things in private by a managing power of a certain project. Considering I'm one of the few women who seem to be active in the FP subset of Scala, it's been hardly warm and welcoming. 
Hahaaaaaaa quote of the month: Miles: People are already pretty widely using this stuff (Refinement Types, etc) in production Adrian: Yeah well people are using Node.js in production so..
I believe that the percentage of Scala vs Java programmers is unlikely to ever increase appreciably. I expect Scala will remain a niche technology. Full disclosure: I program in Scala every day, I operate ScalaCourses.com, I am the founder of SF Scala, and I ran the world's largest Scala user group until a few years ago.
I you and your team are already using Monads and the flatMap function to good effect, why on earth would you and your team avoid using traverse ever?
Do you think it will be a different language or do you think languages like Scala are just too komplex so that only a certain percentage of developers will ever work of them, while the rest is using like GO?
&gt; Option is a monad, whether it is common to use in production code or not: Using that bind function would be monadic but I've never seen anyone do that in OCaml in real code. &gt; The only way code can be functional is if it is referentially transparent. That is one of two common definitions of functional. The other is first-class lexical closures. &gt; There isn't a pure and impure functional program. Using that definition, yes. Most people regard pure functional as referentially transparent and impure function as the use of first-class lexical closures. &gt; the language certainly encourages immutable data structures: OCaml even provides literals for mutable arrays and goes to some lengths to provide a decent mutable hash table implementation. &gt; A language need not be pure in order for you to do FP in it. It just makes FP easier. However, having impure IO makes the language an imperative language in which you can program functionally. I've never seen anyone do monadic IO in OCaml. &gt; Nah, I'm pretty sure that list is all over the place. I believe you that people don't call them that, and don't use the actual return bind/flatMap join unit abstraction that haskell/scala uses. But there are structures that conform to the laws, because they couldn't really work otherwise, and that's what makes them Monads. The name and code reuse doesn't matter. Taking your example of the option type from the OCaml stdlib, what is the structure that conforms to the monad laws? 
This is great news. Really looking forward to the SBT Server support. Exciting stuff. On a mac can use brew to install an RC Release? 
I may have been imprecise. We use monads in the sense that `Option` is a monad and we use that. We're not using any library that provides a `Monad` typeclass and a `traverse` method. So we're not so much avoiding the method as much as it's not already available to us. On top of that, this particular team is not very functionally minded and it's hard to get them on board with bringing in new frameworks and libraries that the majority don't already understand. And like I hinted at before this team "gets" the most basic transformation functions and using generics but they're not really "there" when it comes to abstraction over kinds. Plus over-abstraction is real and the vast majority of our stuff simply doesn't require it. In the few cases where something like `traverse` is useful for us, it's often easier to write something equivalent directly.
So here is your argument if I am not mistaken. p1: "Many" people blocked you from their personal twitter accounts during a debate outside of the Cats community p2: these people are members of typelevel p3: Cats is a member project of typelevel p4: You are "one of the few women who seem to be active in the FP subset of Scala." Conclusion: Cats has "been hardly warm and welcoming" I think there are two missing premises required to make your argument a valid one. 1. How people manage their personal twitter accounts has anything to do with a FLOSS development community. 2. The people who blocked you are actually managing (or at least influencing) the practice of Cats' community. AFAIK, neither of these two premises are true. 
&gt; At the very least you will combine Scala and Java, so why not just limit yourself to Java * At the very least you will have to write some unsafe code in Rust, so why not limit yourself to C? * At the very least you will have to eat cornflakes once in a while, so why not limit yourself to eating cornflakes?
I decided to limit myself to writing JVM bytecode. It's perfectly interoperable with all JVM languages!
I decided to limit myself to writing JVM bytecode. It's perfectly interoperable with all JVM languages!
Polymorphic functions can be achieved probably with this: sealed trait Type[Base] { type T &lt;: Base } val ho: (t: Type[Entry]) =&gt; List[t.T] =&gt; Option[t.T] = _ =&gt; _.headOption Or even make that type-carrier implicit: val ho: (implicit t: Type[Entry]) =&gt; List[t.T] =&gt; Option[t.T] = _.headOption 
Almost what you asked for: (proxy: {type T &lt;: Entry}) =&gt; (t: proxy.T) =&gt; t.Key 
Interesting. See, I was trying to be somewhat confidential and polite, but if the barrier to getting feedback is "it must be positive or it is questioned", that's not really a good way of fostering honest dialogue. I'll play though: 1. It does. Especially when one was a co-owner of Typelevel, and the other was a maintainer of Cats (for example, just using two of them). Regardless of what you might believe about social media, these things do affect the org's presence in the community as a platform which does reach out and display the character of members of the org. Much in the same way Scalaz was shit on due to the abusive actions (justified or unjustified) of a specific maintainer a while ago in private, or in public IRC, the same applies here. In fact, the TL CoC would not exist without that impetus. 2. See above. This is not aimed at you. But it is something to raise as an issue within the famous Typelevel CoC about publicly representing oneself.
&gt; Using that bind function would be monadic but I've never seen anyone do that in OCaml in real code. LOL. I believe you, but I'm guessing people write repetitive pattern matches instead. It is in the lib though, and the "strictly" part of my comment says that it exists, not that people use it (even if they should so they wouldn't reinvent the wheel). &gt; That is one of two common definitions of functional. The other is first-class lexical closures. I have to disagree with that, and need some sources for you to convince me that closures are sufficient for FP. From the below listing, it doesn't seem to be anecdotally exclusive to FP langages: Language| Closures| Functional| ----------|------------|------------ C | No | No Pascal | No | No C++ | Yes | No Java | Yes | No Modula-3 | Yes | No Python | Yes | No Ruby | Yes | No D (2.0) | Yes | No Ocaml | Yes | Yes Erlang | Yes | Yes Haskell | Yes | Yes I think we can all agree that Python is OO imperative and so is Ruby, and mostly C++, Java. Mutation is the common thread among all of these languages. Also, environmental values captured via closure are usually required to be immutable values, so that alone makes them RT, and if they aren't then you are just programming in an OO environment. &gt; On the other hand, many functional languages, such as ML, bind variables directly to values. In this case, since there is no way to change the value of the variable once it is bound, there is no need to share the state between closuresâ€”they just use the same values. This is often called capturing the variable "by value". Java's local and anonymous classes also fall into this categoryâ€”they require captured local variables to be final, which also means there is no need to share state. https://en.wikipedia.org/wiki/Closure_(computer_programming)#Lexical_environment Since closures can be used in imperative and functional environments, they don't provide exclusive criteria for defining FP. Referential transparency clearly does. &gt; I've never seen anyone do monadic IO in OCaml. &gt; Bind is used in async, as I said, but I've never seen it elsewhere in OCaml. Which statement is true? &gt; Taking your example of the option type from the OCaml stdlib, what is the structure that conforms to the monad laws? Well, Option, obviously. It forms a monad, necessarily, since `bind (bind None f) g` can't really execute `f` and still make any sense, and obviously cannot execute g either, and make any sense. I assume I don't ave to inline the definition and match statements everywhere? let return a = Some(a) let bind option f = match option with | None -&gt; None | Some x -&gt; f x ;; associtivity via simple substitution let m = None let f x = Some(x + 1) let g x = Some(x + 1) bind (bind m f) g = bind m (fun x -&gt; bind(f x g)) bind ( bind None f) g = bind None (fun x -&gt; bind(f x g)) None = None let m = Some(1) bind (bind Some(1) f) g = bind Some(1) (fun x -&gt; bind(f x g)) bind Some(2) g = bind Some(1) (fun x -&gt; bind(f x g)) Some(3) = bind Some(1) (fun x -&gt; bind(f x g)) Some(3) = bind(f 1 g) Some(3) = bind(Some(2) g)) Some(3) = Some(3) ;; right identity let m = None bind None return = None None = None let m = Some(1) bind Some(1) return = Some(1) Some(1) = Some(1) ;;left identity let a = 1 bind (return a) f = f a bind Some(1) f = f 1 Some(2) = Some(2) Option and list are shown to be monads via this exercise here: http://www.cs.cornell.edu/courses/cs3110/2016fa/l/23-monads/rec.html Anyway, I don't see any point in defending myself here. Monads exist in OCaml in the standard library. If nobody uses them they are the proverbial tree falling in the wood.
&gt; I'm guessing people write repetitive pattern matches instead Yes, just like all other languages, e.g. C++. &gt; It is in the lib though, Hang on, you seem to be saying that `Option.bind` is in OCaml's stdlib but AFAIK it isn't. Where is it? &gt; and the "strictly" part of my comment says that it exists, not that people use it (even if they should so they wouldn't reinvent the wheel). I don't see how your arguments apply to OCaml any more than, say, C++. &gt; I think we can all agree that Python is OO imperative and so is Ruby, and mostly C++, Java. Mutation is the common thread among all of these languages. I certainly wouldn't agree with that. They are all multiparadigm. See for example [Functional Programming HOWTO](https://docs.python.org/2/howto/functional.html) from the standard Python docs. I often see Ruby pushed as more functional than OO. &gt; Also, environmental values captured via closure are usually required to be immutable values, so that alone makes them RT, and if they aren't then you are just programming in an OO environment. Closures can capture mutable data structures in almost every language that has closures. &gt; Since closures can be used in imperative and functional environments, they don't provide exclusive criteria for defining FP. That is a circular argument. &gt; &gt; I've never seen anyone do monadic IO in OCaml. Bind is used in async, as I said, but I've never seen it elsewhere in OCaml. &gt; Which statement is true? Async is CPS not IO. I've never seen anyone do monadic IO in OCaml (e.g. `printf`). When async is used to process multiple connection concurrently the only way to share information between those connections is via shared mutable state so it isn't referentially transparent. Furthermore, that only works because the GC uses a global lock (!). &gt; Option and list are shown to be monads via this exercise here That shows you can write in a monadic style in OCaml. You can [in C++ too](http://www.modernescpp.com/index.php/monads-in-c). 
I can only hope that everyone finds this article as amusing and clever as I do. If ever there were a Trojan horse against copy-pasta, this is it.
I believe, it's the same with Kotlin. There are a few programmers who immediately want features that Kotlin gives. So it will take a chunk of Android and Java programmers, but its unlikely that it will be a large chunk particularly with the recent Open JDK community changes.
No love for Assembly?
&gt; So it will take a chunk of Android and Java programmers Certainly, and among the alt-Java languages its trajectory is clearly [trending upward](https://trends.google.com/trends/explore?q=%2Fm%2F0_lcrx4,%2Fm%2F091hdj,%2Fm%2F03yb8hb,%2Fm%2F02js86). Scala almost certainly will never become mainstream, but languages that pick off some of its best features may well see much larger adoption as a result.
Who?
&gt; Certainly, and among the alt-Java languages its trajectory is clearly trending upward. That certainly happened when they announced Golang. Where is it now? These trends are nothing but hype. We have a certain class of people saying AI will take over programming. We all know thats not going to happen unless they break the AI complete barrier. &gt; Scala almost certainly will never become mainstream, but languages that pick off some of its best features may well see much larger adoption as a result. Why does it have to be? I think Scala is best if we use it where it is best. The future is about distributed computing and thats where Scala shines and has won. Akka is probably the best library for writing distributed systems on the JVM and that is because of Scala. 
https://www.quora.com/profile/Xavier-Amatriain
I've used silhouette with play if you're looking for suggestions to evaluate yourself.
In some ways Scala has succeeded despite itself (slow compiler, slow/complex build tool, poor IDE support, split community over FP/OO vs. pure FP, language soundness issues, unresolved bugs galore, etc.), which bodes well for the future. If Dotty lands with Scala macros, TASTY support, and Scala Native + Scala.js continue to evolve, then Scala becomes far more compelling outside of big data and distributed computing. Until then it's a slow slog preparing for the transition from "legacy" Scala to the new and improved Scala (that will be largely like the old Scala syntactically), with Scala 2.13 and 2.14 laying the groundwork. Would love to fast forward to 2020...
Yeah, in a way this makes me sad. The JVM community is now split between languages. I just wish that dotty comes through quickly, else I see a lot of people moving for the wrong reasons.
Don't worry, Dotty is coming sooner than we think, I think ;-) They're on a 6 week release schedule; with v0.5 coming out a few days ago that means a 1.0 could land a little more than 6 months from now. Of course the community has to migrate over to Dotty, which will take some time, but 2019 doesn't seem out of the question for the Dotty era to begin. Scala macros and TASTY are of more concern than Dotty since neither one has any clear path forward at the moment, AFAICT, very much research status. If anything delays Dotty adoption it will be the supporting pieces not yet being ready.
&gt; They're on a 6 week release schedule; with v0.5 coming out a few days ago that means a 1.0 could land a little more than 6 months from now. What kind of math are you doing that could possibly justify that conclusion!? Take for example Scala.js: v0.5 was released more than 3 years ago, and has since been on an 8 week release schedule. Yet v1.0 is not there yet. You're basing this "Dotty 1.0 could land about 6 months from now" speculation on nothing.
&gt; What kind of math are you doing that could possibly justify that conclusion!? The naive uninterrupted 6 week release schedule kind of math ;-) That and one of the core committers did mention that Dotty would be landing sooner than most people think (i.e. well before 2020). Of course you work with them, or next to them, so you'd have a better idea than I or anyone else on a realistic timeframe for Dotty 1.0 As for Scala.js, sure, but you guys have been exceedingly conservative on the road to 1.0 (0.6 has been effectively 1.0, though not tagging it as 1.0 has given you some wiggle room). Dotty, on the other hand, I suspect there's pressure to get it into the hands of users much sooner than later (i.e. there will be no protracted pre-1.0 releases that aren't simply design/research roadblocks).
Note - I have never used shims or any scalaz to cats (or vice-versa) converter library. Is shims or an alternative a feasible approach? For example, as a user of http4s and doobie, could I use those libraries with scalaz via shims? If so, why would I take that approach instead of just migrating to cats? I am asking sincerely to explore.
With that said. Can anybody point me out on what features that Scala has and Kotlin doesn't and vice versa. I know that certain libraries like Kategory have added type classes and commonly used monad architectures into Kotlin. I am just curious.
What's the author's main argument for using Scala? I saw something about "your code will run anywhere and everywhere" but that's true of a lot of things these days. What did I miss? Did he even say the word, "Monad?" 
From `shims`: &gt; Shims aims to provide a convenient, bidirectional, and transparent set of conversions between scalaz and cats, covering typeclasses (e.g. Monad) and data types (e.g. \/). By that I mean, with shims, anything that has a cats.Functor instance also has a scalaz.Functor instance, and vice versa. So yeah, you should be able to use it and have things "just work". My own brief experiments have succeeded, at least.
The argument seems to be that Functional Works gets more consulting business and revenue as people use more Scala/FP language.
do you allow remote engineers?
For my part I'd prefer Scala not get embroiled in the blockchain.
I don't see his answer? Also, doesn't Quora use Scala?
Implicits and higher kinded types are probably the main language features that Kotlin is missing (although there's a quite popular [open PR](https://github.com/Kotlin/KEEP/pull/87) in the works for adding HKTs that looks like it will be accepted at some point). And then there are self-types, F-bounded types, phantom types, full blown pattern matching (not the watered down version present in Kotlin), variance annotations, macros, etc. -- Scala is strictly speaking a more powerful and expressive language. On the other hand, Kotlin has better tooling, faster build times, null safe checks, so called flow typing ( `if(isNum(x)) { x * 2 }`), builder support (like Scala xml literals but without the xml baggage), and just enough features borrowed from Scala (data classes, val, var, type-signature-on-right, companion objects, likely many more...) that it appeals to users coming from lesser languages like Java, Javascript, PHP, etc.
What wonderfully quality code case class ArbitBox(override val proposition: PublicKey25519Proposition, override val nonce: Long, override val value: Long) extends BifrostPublic25519NoncedBox(proposition, nonce, value) { override lazy val typeOfBox: String = "Arbit" } case class PolyBox(override val proposition: PublicKey25519Proposition, override val nonce: Long, override val value: Long) extends BifrostPublic25519NoncedBox(proposition, nonce, value) { override lazy val typeOfBox: String = "Poly" } (it's not)
Looking at those code snippets, I wouldn't let Zihe Huang anywhere *near* my codebase.
What is this, java persons first go at scala in the example code? Terrible code 
Why? 
I find this is a very interesting and convincing article, and am shocked by the negativity of some of the other comments, as well as being disgusted by their implied arrogance. 
 def comment(text: String)(implicit arrogance: String =&gt; String): String = ...
&gt; That certainly happened when they announced Golang. Where is it now? But what are you implying about Golang? It is now one of the most successful languages in use today. Lots of hot new things are being built in it like kubernetes, docker, etc.
Hi all I am using SBT 1.0.4 and want to create a fat jar from my project. Is there anyway to create it? I tried https://github.com/sbt/sbt-assembly but it seems, it does not support SBT 1.0.4 at all. Thanks
You could also make the Def expect explicit parameters, rather than returning a higher order function. 
I don't know a whole lot about blockchains, I don't see how the advantages of Scala specifically that are presented here are at all informative or convincing. On the blockchain side, I still don't understand what problem blockchains are being used to solve, but is specific framework(Scorex) is smaller in size than other existing frameworks. I'm unsure if being written in Scala is the reason for this. As for the code example, it doesn't seem to be enough to make conclusions about Scala as a language at at all. He extended some classes, but I don't know what that does in the end. I'm genuinely curious what you found to be interesting in this article.
It seems to be working for me, but I'm using a pretty small build definition. Have you tried it?
Because it's stupid.
&gt; or do you think languages like Scala are just too komplex so that only a certain percentage of developers will ever work of them I think this reason has a lot of merit to it, more then people are willing to believe. I however also think that this reason in general isn't specific to programming languages. For the most part, having appealing and easily approachable (i.e. you can learn with minimal learning upfront, or what people call intuitive) is always a feature that people look out for.
&gt; else I see a lot of people moving for the wrong reasons. I don't think the reasons are wrong, some reasons are entirely justified. At the end of the day its a basic cost model of how much benefit the language gives you versus how much it costs. Depending on the circumstance this isn't *always* positive for Scala.
Hi, Can someone explain why this code throws a NullPointerException? object Outer { val outerVal = "outerVal" val inners = Inner1.inner1Val ++ Inner2.inner2Val object Inner1 { val inner1Val = Seq(outerVal) } object Inner2 { val inner2Val = Seq(outerVal) } } println(Outer.Inner1.inner1Val) If I read `Outer.inners` before reading `Outer.Inner1.inner1Val`, then I don't get the error. Making the `val`s `lazy` fixes it as well. But I'm confused about what's causing the error to begin with. Thanks!
Now it is working, I did something wrong. Excuse me. By the way, what is a small build definition? Earlier, when I was creating a project the file Â´plugin.sbtÂ´ was created too, since version 1.0.4 Â´plugin.sbtÂ´ disappear. Do I have to create manuelly? Thanks 
Seems to work fine: https://scastie.scala-lang.org/E9Hltgn2Qu2X9ehogTP3Ug I also tried it in my local 2.11 console, same result
&gt; Now it is working, I did something wrong. Excuse me. By the way, what is a small build definition? I created the simplest project I could think of. ./project/build.properties: sbt.version=1.0.4 ./project/plugins.sbt: add sbt-assembly plugin ./Hello.scala: main entry that simply prints hello world I then packaged and ran it with `sbt assembly &amp;&amp; java -jar target/scala-2.12/*.jar` and it worked. I've never had project/plugins.sbt be automatically created for me, but then I do all my project setup through a terminal rather than using something like IntelliJ. Maybe that's the difference to your scenario
That's odd. I tried running it in CoderPad (2.12.2) and my local console (also 2.12.2) and get the error reliably. 
&gt;1. How people manage their personal twitter accounts has anything to do with a FLOSS development community. Actually, this has very much been a core issue brought up by Typelevel. Are you going to now claim TypeLevel's CoC does not extend beyond the gitter/mailing list; into interactions on public twitter/forums?
I am not trying to represent Typelevel. I am merely a member of it. But the Typelevel's CoC is solely about forbidding harassment in the community (https://typelevel.org/conduct). My understanding is that harassments through channels beyond the gitter/mailling list are also not allowed in Typelelvel CoC. I don't think how people manage their personal twitter accounts, i.e. blocking anyone (or even following), is any form of harassment. Do you agree? It's clearly not a violation of the CoC by any stretch. Note that managing personal twitter accounts doesn't include the activity of replying or message other people's twitter accounts. 
The article may be interesting, but the Scala code samples and the reasoning for using it do not make a convincing argument for using it over pretty much any other statically typed programming language; that's likely what people are taking exception to in this thread. Their [github repo](https://github.com/ScorexFoundation/Scorex) on the other hand, does show some of the strengths of the language -- better code samples probably would have brought about a different reaction.
Well, harassment is defined by those who feel offended (according to the TL CoC), so I'm not sure you can just unequivocally say that something is not harassment. Clearly Emily doesn't feel welcome by TypeLevel. Your response is not to try to rectify the situation, but rather to say that a FOSS community that has active, participating members that use an out of band signal to show their dislike for Emily doesn't mean said community is 'unwelcoming'. The mental gymnastics you are going through to justify that behavior is astounding. 
Quote from Typelevel CoC "Harassment includes offensive comments related to gender or anything to do with it (identity, history, expression, non-binary gender, etc.), sexual orientation, disability, physical appearance, body size, race, religion, programming experience or background, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of community activities, spaces, or other events, inappropriate physical contact, and unwelcome sexual attention."
(Speaking for myself) I think blockchain is vastly overhyped, and only solves real problems in maybe a dozen of narrow applications. The other 99% of the applications that blockchain technology is being thrown at will not end up using it once the hype cycle is done. So if our ecosystem gets to avoid over-investment in blockchain, all the more time for us to work on things that will still matter a few years from now. Of course this is an oversimplified, subjective statement. I could have dismissed e.g. social networks in the same way 15 years ago. Doesn't change how I feel about this.
 class Foo(val foo: String) class Bar(foo: String) extends Foo(foo) class Baz(override val foo: String) extends Foo(foo) `Bar` and `Baz` will seemingly behave the same, but is there any difference in what happens under the hood? Is one of those known to be more efficient, or otherwise preferred for some reason other than brevity? Curious about this in general, as well as for Scala.js in particular.
IOHK is an impressive company. You should check them out. Among others they have Phil Wadler as senior research fellow and area leader of programming languages. Scorex looks like a very capable distributed contracts platform, and small size matters here! So it's definitely worth checking out as well. I agree the examples in the blog were minimal but there was a link to the full code. 
What if you put the two Inner objects between outerVal and inners? References work a bit weird in primary constuctors, you can get nullpointers if you do forward references (Which wouldn't compile in a function but it's only a warning in primary consturctors). However this case is a bit different, so I don't know what should happen. 
Thanks a lot.
I am talking in relative terms. Golang was intended to replace C/C++. It is successful in comparison to many other languages but Golang should have been like what Rust is today. 
His answer got around 200 upvotes. For some reason I am not able to link it here. Quora was using Scala some years back. Not sure what the current situation is.
What would have happened if the timelines were reversed? Kotlin was released in place of Scala and vice versa. I don't think the people who are complaining now would exist. They would have gotten familiar with Kotlin syntax and would find Scala as a different language. This was what I felt with Scala, I wish there was a guide which would teach me the good OOP parts alone ignoring the FP ones. But then again, that's how the world is now.
&gt; What if you put the two Inner objects between outerVal and inners? Didn't work =/
Hi all I have a monadic recursion and want to know, if it is thread safe or not: private def pool[A] (consumer: =&gt; Consumer[String, String])(cb: Vector[KkConsumerRecord] =&gt; IO[A]): IO[Unit] = { val records: ConsumerRecords[String, String] = consumer.poll(Long.MaxValue) val converted = records.iterator().asScala.map(rec =&gt; { KkConsumerRecord(rec.key(), rec.value(), rec.offset(), rec.partition(), rec.topic(), rec.timestamp()) }) val vec = converted.foldLeft(Vector.empty[KkConsumerRecord]) { (b, a) =&gt; a +: b } cb(vec).flatMap(_ =&gt; pool(consumer)(cb)) } Could someone please tell me if it is thread safe or not? Thanks
So if your community forbids "harassment" (as defined by their CoC), it must be a welcoming place? You seem to be attacking Emily for saying she doesn't feel welcome in the TypeLevel community. 
You can actually checkout [Waves blockchain](https://github.com/wavesplatform/Waves). They are doing some asset management on top of a in house blockchain, if I understood it correctly. It's written in Scala / Akka and is ranked around [~20th in the cryptos](https://coinmarketcap.com/) (sorted by market cap). So indeed, it is very possible to code a blockchain in Scala and succeed :)
I got this with 2.12.4 on my machine too. The source of the problem seems to be that outerVal isn't initialised, if Outer wasn't previously referenced before. (If I put just Outer a line before that also fixes it). This doesn't seem to explain how lazy fixes it too (because by this logic Outer still shouldn't be intialised by that time), but that's all I could figure out so far. 
This looks cool and Iâ€™ve got an application that I want to analyze for bottlenecks. Can this help? How does one read these graphs?
Someone should use it on SBT
Thanks for the suggestion :) Will do this asap :)
Thanks for sharing
To be honest, I think blockchain is a technology revolution. As any technology revolution, a lot of creativity emerges around it therefore the huge hype. As the technology will mature, most of the blockchains will die and only the mature and useful will survive (it is following this [cycle](https://www.google.de/search?q=technology+revolution+cycle&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=0ahUKEwjnkMDW14HYAhXCJFAKHSyKBIoQ_AUICigB&amp;biw=1920&amp;bih=983#imgrc=M3nJNIVzeZIHrM:)). I really like it in a way that it fosters innovation and has the ability to solve real world problems. Let me share some cool examples, more or less serious: * [buy and own your virtual land](https://decentraland.org/) * [credits for games](https://gamecredits.com/) * [cross border payments](https://coinmarketcap.com/currencies/ripple/) * [cheap cloud storage](https://sia.tech/) * [crowdfunding platform](https://www.emc2.foundation/) and so on... Of course those are just examples ! There are many more ! Also, a common mistake is to assume that there is only one kind of blockchain. There are several [types](https://blockchainhub.net/blockchains-and-distributed-ledger-technologies-in-general/). If you are interested in the technology, and you want to implement your own, the [Bitcoin Wikipedia](https://en.bitcoin.it/wiki/Protocol_documentation) is a comprehensive documentation of the protocol. It is a very interesting piece of software.
I do believe that concurrence drives innovation. So I think it's healthy overall ! And I believe that it might be easier to do: Java to Kotlin then Scala, rather than making the huge leap of Java to Scala. So probably it will bring some more people into the Scala ecosystem, because after all, learning a new language might not be so scary.
Looks very cool, hope mature is this library and which solved does it implement?
Do you guys agree with these critiques of Scala? https://www.reddit.com/r/haskell/comments/1pjjy5/odersky_the_trouble_with_types_strange_loop_2013/cd3bgcu/ Although that comment is 4 years old, it was written by a rock star Haskeller and I thought I'd check and see what you guys think. It kind of sounds like a mic drop to me. It would take someone pretty knowledgeable about these things to begin to know (which isn't me).
Have one test that checks the system type and runs one or the other of the specific code? 
Jesus christ i'm dumb, Thank you so much, that's a decent solution. I'm gonna leave this thread up in case someone knows of a ScalaTest feature that does the same thing.
You can write system dependent tests, or you can use testing filesystems such as https://github.com/sbridges/ephemeralfs that let you mock out the internal behavior.
it's terrible for all those applications. dbs are more performant than the blockchain in all these cases. 
Nah you're not dumb. It's not a good habit. Can you restructure your test or code under test so that the interface is the same on both platforms? Then the test wouldn't need to change
It is solving a very different problem, namely: **how to make untrusted parties agree on some facts that happened**. You cannot compare apple and bananas ðŸ˜‚
Can you explain further, why comparing pathes involves interaction with the filesystem? Perhaps you can show us your test? 
Nothing you described requires platform-specific tests or behavior. Abstracting that away is part of the function of the java.io platform. Iâ€™d write platform-agnostic code and trust the Java implementation to properly handle the platform-specific details.
you don't need that for any of the things you listed though, and the cost for the blockchain to do any of those things is incredibly excessive wrt energy and storage consumption. right now, securing the blockchain for bitcoin requires more power than some small nations consume annually. and bitcoin can barely handle any transaction volume (especially compared to payment processors like visa who get the same work done with a small fraction of bitcoin's resource usage). the benefits do not outweigh the costs for any of the examples you listed once the blockchain can be used to secure transactions without leading to an arms race of electricity consumption, it might be worthwhile. but at the moment, it's a nightmare
&gt; you don't need that for any of the things you listed though, and the cost for the blockchain to do any of those things is incredibly excessive wrt energy and storage consumption. right now, securing the blockchain for bitcoin requires more power than some small nations consume annually. and bitcoin can barely handle any transaction volume (especially compared to payment processors like visa who get the same work done with a small fraction of bitcoin's resource usage). the benefits do not outweigh the costs for any of the examples you listed &gt; Blockchain != Bitcoin. There are alternative technologies which can be computed efficiently, such as Algorand: https://www.youtube.com/watch?v=_nQE_HAGlmM 
To be honest, I really believe that your negativity is driven by your ignorance (and I am not meaning bad here), and not by a genuine opinion on what is blockchain technologically speaking. As Martin mentioned, blockchain != Bitcoin. I think here you are talking about the consensus algorithm of the Bitcoin, which, and you are very right, [consumes a lot of energy](https://digiconomist.net/bitcoin-energy-consumption). It is based on Proof of Work algorithm (PoW - brute force double hashes basically) and is CPU intensive task (therefore energy intensive). However there are [many different kind](https://www.coindesk.com/short-guide-blockchain-consensus-protocols/) of consensus algorithms, which are more or less energy intensive such as: * Proof of Work (Bitcoin for instance) * Proof of Stake * ... and I might forget a lot of them as the list grows everyday (innovation boom !). To anyone reading this thread, I would strongly encourage you to have a look at what is a blockchain technology, and itsunderlying mechanics. It is a passionating topic. Don't overlook it. Also, please, in the future, do not downvote (is it you?) a reply just because you disagree with it. I made a clear, structured and balanced answer about why I think blockchain is a technology revolution in itself, sharing my genuine point of view. Downvoting just does not help, and does not encourage anyone to contribute to this sub.
Hey there, i forgot to include the sources, [here is the function](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/main/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPaths.scala) and [here are the unit tests](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/test/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPathsTest.scala). The reason my tests fail is because the path separator is different in windows '\' than in unix '/', and because the function java.io.File.getCanonicalPath uses the drive's letter as a path root for example C:\\. i can also see the tests failing if i use the C:\ as root and an other user clone the repo in his D:\ dirve. The reason i'm using java.io instead of java.nio is because i need to support java 6, and the java.io.File class is used only for the getCanonicalPath function.
You are absolutely right, so you can see [the function code here](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/main/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPaths.scala) and [the unit tests here](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/test/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPathsTest.scala). My unit tests serve only to test that the function removes the common parents directory, and because the user may pass in some bizarre valid paths for example .././.././..////dir, i wanted to rely on the java.io.File.getCanonicalPath to take care of that. The reason my tests fail is because the path separator is different in windows '\' than in unix '/', and because the function java.io.File.getCanonicalPath uses the drive's letter as a path root for example C:\. i can also see the tests failing if i use the C:\ as root and an other user clone the repo in his D:\ dirve.
Thank you sir, I'm a Scala beginner since June 2017 (after 10 years of Java programming), I was really deceived to read this quora, then I read your message, and I feel heart warm now
Hey there, i forgot to link the [function's code](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/main/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPaths.scala) and [the unit tests](https://github.com/MouslihAbdelhakim/Quick/blob/develop/src/test/scala/com/scalableQuality/quick/mantle/labels/FileLabelsFromPathsTest.scala). i will try to take your advice on making os guards and see how it works out, if it gets a bit messy i'll just skip on unit tests and automate some functional tests. Thank you some much for the help.
i would love to use java.nio but this little project is meant to automate tests for legacy systems, so i need to support java 6. but thank you so much. I really appreciate the advice, i didn't know something like this existed.
I agree with it all. They are all problems. It doesn't stop me getting work done, but I do wish many of those problems didn't exist.
I think what you're looking for is a monad transformer, e.g. `EitherT`: import scala.concurrent.Future import cats.data.EitherT import cats.instances.future._ implicit val ec = scala.concurrent.ExecutionContext.global val one: Future[Either[String, Int]] = Future(Right(1)) val two: Future[Either[String, Int]] = Future(Left("boom")) val six = for { a &lt;- EitherT(one) b &lt;- EitherT(two) c = 3 + b } yield a + b + c six.value You can find this in cats or scalaz.
&gt; a rock star Haskeller Is this 2005 again? So, I don't have any issue with his points - pure FP is not that comfortable in an impure FP language(what a surprise!) but if you care about being *practical* over being *pretentious* let's just think about it what he says in the same thread: &gt; if only the benefits Scala derived were proportional to the price they pay. This just makes me wonder how could it be that over the last 3 decades the "largest" program ever written in haskell is a document converter(pandoc) and the rest are just small libraries and webapps created by vocal "enthusiasts". It has nothing to do with the lack of marketing and PR because right now there is far more universities teaching haskell than unis teaching any of the other FP language together and yet haskell's situation in the industry is obvious. Does pure FP worth it if you've primitive tooling, ugly syntax(think about non-trivial algorithms and not the fake-quick-sort, please) and pretty much no demonstrable benefits? What's there to advocate so harshly? Productivity? Nope, you won't be more productive without a viral ecosystem, decent code completion and refactoring tools. Is it more safety? No, you won't have more safety than contemporary FP languages without using dependent and linear types which are not supported well or present in haskell. Or better performance? Haskellers really like to boast about performance and yet there's no evidence that lazyness helps in prod. What they do instead is benchmark their code against nodejs and python code. It's logical to go again the weakest, right? Or how about writing more readable code? I highly doubt that since haskellers *really* like to abuse operators, import 100 language extensions and do barely useful hacks. Shorter code? They can forget that too since almost all the "terseness" of haskell comes from miranda/ml and the rest is just rom short functions and simple "tricks". If his best arguments are having better type inference and better support for his banana monads then it's less like a "mic drop" and more like a "waiter trying to flex with his new iphone at a party full with millionaires". All I see from him is that he's biased - and he's not alone. Just go and try to talk about haskell and any other prog lang with any "rock star" haskeller. They'll end up denying or trying to belittle the drawbacks of haskell while trying to magnify everything in every other language. Benchmarks are against haskell, programmers are too stupid etc etc. Chances are high that (on reddit) you won't meet with any haskeller which will have any positive opinion on any other programming language. The best to do is to avoid subjective opinions and do your research for yourself.
Exactly the same as your map function but instead of Right(f(r)) you would just put f(r)
I got close to that, but don't think it's exactly right. Both of these give compiler error: def flatMap[S](f: R =&gt; FutureEither[L, S])(implicit executor: ExecutionContext): FutureEither[L, S] = { val result = future.map { case Right(r) =&gt; f(r) case Left(error) =&gt; f(error) } new FutureEither(result) } def flatMap[S](f: R =&gt; FutureEither[L, S])(implicit executor: ExecutionContext): FutureEither[L, S] = { val result = future.map { case Right(r) =&gt; f(r) case Left(error) =&gt; f(error) } new FutureEither(result) }
Leave the left case as Left(error). Only the right case should apply the function.
Exactly this. Asking any question like "Do people use Monads in x" shows a fundamental misunderstanding of what a Monad is. Every programmer uses Monads. Scala just happens to be a language where the idea is representable in a convenient and correct way. 
 [error] found : scala.concurrent.Future[Any] [error] required: scala.concurrent.Future[Either[L,S]] [error] new FutureEither(result) 
It just looks as though it can't imply the generics. I'm on mobile atm but will be in the office shortly and will reply then.
I see the problem now. The function returns a FutureEither so it doesn't match the expected result. The function should return an Either[L,S].
Yeah, the point is to make it work in a for-comprehension.
I also use EitherT. In your case it will have type `EitherT[Future, Error, R]`. You can easily convert to `EitherT[Future, Error, R]` from `Either[Error, R]` like `EitherT.fromEither[Future](either)` and from `Future[Either[Error, R]]` like `EitherT.liftT[Future, Error, R](future)`
Not a mic drop -- it's easy to come up with critiques of any programming language. A real mic drop would be making something that was so obviously amazing and superior that it just left everyone's mouths hanging open and couldn't be criticized.
Think this works: def flatMap[S](f: R =&gt; FutureEither[L, S])(implicit executor: ExecutionContext): FutureEither[L, S] = { val result = future.flatMap { case Right(r) =&gt; f(r).future case Left(error) =&gt; Future.successful(Left(error)) } new FutureEither(result) }
Question not just for you but anyone using the same pattern: why use Future[Either] rather than putting your errors in Future.failed?
It's useful when you want to distinguish [errors from failures](https://www.reactivemanifesto.org/glossary#Failure). For example in a request to an external service, you might have `Future[Either[ClientError, Response]]` where the future would fail with any kind of 5xx http response, a timeout, socket connection failure, etc while the left case would contain any kind of 4xx http response such as trying to create a duplicate resource or requesting a resource that doesn't exist.
Godz, these baiting questions are getting *so* bloody tiresome. 
This might get me banned but... *Do fuck right off.*
Nice.
This looks very similar to what I have done in a project, I use a `FutureOr` monad transformer that uses the `scalactic` library instead of `Either` (see https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/commons/FutureOr.scala). What I did is to define a custom type `FutureApplicationResult[A]` instead of linking to `Future[A Or Every[ApplicationError]]`, I'm using `ApplicationError` in the whole application anyway, you could see usage example here: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/services/UserService.scala#L28 It should be trivial to adapt the logic to work with `Either`, one advantage of scalactic is that it has a non empty list.
Is the problem only the `/` in the tests? Try using pathSeperator in the test and building the path from a sequence of path components
An error is something part of business logic, even. E.g. Invalid log in, email already exists, etc. Failure is for example database connection error. 
This is what I'm using in production. Note that I use a case class (to simplify creation) and extends `AnyVal` for better performance (https://docs.scala-lang.org/overviews/core/value-classes.html#extension-methods) case class FutureE[+L, R](future: Future[Either[L, R]]) extends AnyVal { def flatMap[S, G &gt;: L](f: R =&gt; FutureE[G, S]) (implicit ec: ExecutionContext): FutureE[G, S] = FutureE { future.flatMap { case Left(l) =&gt; Future.successful(Left(l)) case Right(r) =&gt; f(r).future } } def map[S](f: R =&gt; S) (implicit ec: ExecutionContext): FutureE[L, S] = FutureE { future.map(_ map f) } }
In my uses Iâ€™ve handled errors and failures with just the future because that composes better and allows me to focus on what is recoverable - since recoverability can sometimes work on errors and sometimes failures.
this is interesting... it doesn't seem to have the same attention as shims. Given that Daniel had to delve into compiler bugs to get shims working, I wonder what the limitations of Harmony are.
my two cents: I've found the scalaz community to be more warm and welcoming than typelevel. When I [removed ENSIME from Typelevel](https://github.com/typelevel/typelevel.github.com/pull/172), I experienced a large backlash with several influential founders and advocates of Typelevel unfollowing me, blocking me or (worse) tone policing me on gitter / github.
AnyVal won't actually help with performance if you're not very careful what you do with subclasses extending it. Many common operations will still result in heap allocation of the subclass and boxing+unboxing.
I think the idea here is that you're forced to handle/acknowledge errors, if errors and failures are in the same place, it's easy to ignore errors the way you would ignore failures. Then you've got `EitherT` for when you decide you want to compose them nicely.
Here's mine, not directly Scala, but my experience... :D ![LOL](https://pbs.twimg.com/media/DPj9gKJXcAAxPUD.jpg)
I prefer articles with unintentional sarcasm, personally Â¯\\\_(ãƒ„)_/Â¯
&gt; Go is great. Lost me there. I don't see anything great about a language that is practically an enemy of abstraction and proper polymorphism.
One word - generics
Â¯\\\_(ãƒ„)_/Â¯
sbt 1.1 doesn't address any of the performance problems you're facing. They are caused by sbt via IntelliJ and are exactly the same if you use sbt CLI. In addition, you may be disappointed by the scope of the sbt 1.1 LSP, it is just reporting compiler errors, it's not providing classpath search, completion, or anything else you get from ENSIME today.
Just wanted to add that `EitherT` is not limited to `Future[Either[...]]` it can inject `Either`'s behavior into any monad which is quite handy.
Agreed. Go's type system is extremely limited (and not in any way suitable for a task like this), in fact its only redeeming qualities seems to be fast compilation and decent built-in primitives for concurrency. Amazingly that's apparently enough to obtain a substantial developer community.
Thanks, I ended up using cats `EitherT`, implementing my own map and flatMap helped me gained a better understanding of how it works. For completeness sake, this is the class and helper methods I ended up using: import scala.concurrent.Future import scala.concurrent.ExecutionContext.Implicits.global import models.Error.ShortError import cats.data.EitherT import cats.implicits._ object AsyncResult { type AsyncResult[T] = EitherT[Future, ShortError, T] def apply[T](fe: =&gt; Future[Either[ShortError, T]]): AsyncResult[T] = EitherT(fe) def apply[T](either: Either[ShortError, T]): AsyncResult[T] = EitherT.fromEither[Future](either) def success[T](res: =&gt; T): AsyncResult[T] = EitherT.rightT[Future, ShortError](res) def error[T](error: ShortError): AsyncResult[T] = EitherT.leftT[Future, T](error) def expectFalse(cond: =&gt; Boolean, err: =&gt; ShortError): AsyncResult[Boolean] = EitherT.cond[Future](cond, false, err) def expectTrue(cond: =&gt; Boolean, err: =&gt; ShortError): AsyncResult[Boolean] = EitherT.cond[Future](cond, true, err) }
haha, definitely. And no offense taken. The article was meant to be a bastardized mixture of Scala and Go and the resulting code is far from ideal/usable. I had originally written this with the Go community in mind but thought some folks in /r/Scala would enjoy the horror show.
Great post! It's interesting to see the benchmarks for something like this. 
You could try out the scala gitter channel too.
Could you please post where the position is for, or if it is remote?
&gt; Personally speaking, when using all this stuff in my own work projects, I often find it nice to provide two sets of interfaces: &gt; A low-level ecosystem of algebras, raw data types, logic, etc. Usually an entire package. &gt; A high-level DSL that very cleanly exposes common high-level functions and hides all the algebra composition, calls to cata and other morphisms, etc. Usually a single object. This approach allows all devs to get the most common types of work done without any knowledge of recursion schemes or theory. They just see a small, high-level DSL that's easy to skim, understand and use. This is a great approach to building maintainable libraries and applications in - general. 
Hi, short answer is: Yes, mostly. We have a team member who spends a couple of weeks with us at the office every couple of months for example, and we have permanent video conference up, so people working from can shout at everyone at the office.
Hey. I'm located in Canada. Our lead developer is local, but I have no issues with working remotely. Where are you located? Also, do you have any angular experience?
Thanks, I will do that!
Scala and Angular to fix the calendar bug sounds like a pretty weird mix. I would assume you're using Scala.js there. Are you looking for someone who would fix that particular problem you have or is there more to it?
My current developers exact message to me is this: Calendar: For now, API don't allow us to create custom event types. Type always equal "Practice" by default. Backend use scala, but we haven't scala specialists. So we need someone who can change it on the back-end, and after that, we will have the ability to implement the feature on FE. For now, API returns all games, players, events to front-end. After that Front-end filters this data to display that we need. This way very insecure, and anyone who wants to take access to data can do this in very easy way. Also, password returns by API in open view. Anyone can log in as any player or coach and broke or change data. Hopefully that makes more sense.
If it's just this once off fix I'd be happy to take a look in the mean while until your team skills up. Feel free to private message me more details. 
Hey I'm in Canada too. Not looking for a job now though :-), just the moderator here trying to make your post more useful. Please include the city as well.
So, what you need now is: - add support for creation of different event types - return only what's needed - improve security (hide password) I think I could help. What are your time requirements?
Thanks for the heads-up Josh. I'm in Newfoundland, Canada. I will make sure to include that information next time.
Of course time is very tight. We are trying to launch, but are being held up by a few minor issues like this. Hoping to find someone who could essential start immediately. 
I'm no expert in this, but it seems to me like maybe you wan the kill switch to terminate the input to your system, ahead of where you buffer. That way, the completion of the stream would be buffered as well. I think. I've tried to do a similar thing with an Akka HTTP web socket server, to establish tight semantics around termination. But I got to the "good enough" point and stopped trying to get it rock solid.
Hello all, I'm in the process of learning scala, specifically the functional programming examples at scala-examples.org, and I'm mentally hurting a little. Exercise 2.3 has this function: def curry[A, B, C](f: (A, B) =&gt; C): A =&gt; (B =&gt; C) = a =&gt; b =&gt; f(a, b) It took a while to parse out mentally, but as I understand it, this can be read as: a function called "curry", which is aware of generic types A, B and C. The "curry" function accepts a single parameter, a function "f", which accepts two arguments of type A and B, returning a value of type C *The next step took a while, because there appears to be a lot going on behind the scenes. Eventually, I came to the following understanding, which seems to accurately describe what is going on* The "curry" function returns an anonymous function accepting a parameter of type A, which itself returns an anonymous second function returning a parameter of type B. The second anonymous function returns a value of type C And the "curry" function's executable code that implements this return value is: a =&gt; b =&gt; f(a, b) which can be described as: return an anonymous function accepting the single parameter 'a', and returns an anonymous second function accepting the single parameter 'b', which itself executes the input parameter function 'f', with the values a and b as the two parameters. So I wrote this out more explicitly: def curry[A, B, C]( input_param_func: (A, B) =&gt; C): A =&gt; (B =&gt; C) = first_param =&gt; second_param =&gt; input_param_func( first_param, second_param) And that still works when I test it. The next step was to try to reduce the syntactic sugar, so I could formalize those anonymous functions, but I don't think I have enough structural understanding of scala to do it right, at least, when I try to create function definitions for the function accepting the first param and the function accepting the second param returning the input function, I can't seem to get it to compile. def explicitCurry[A, B, C](input_param_func: (A, B) =&gt; C): A =&gt; (B =&gt; C) = { def func_of_first_param( pA ) =&gt; (B =&gt; C) = { def func_of_second_param( pB ) =&gt; C = input_param_func( pA, pB ) return func_of_second_param } return func_of_first_param } so the ask is: Can someone either write out this function in explicit form, or point me to a place where that's already been done? I feel like it would be helpful for me to have a fully-described example, so when I come across unusually terse syntax I have a framework for breaking it into chunks that I can mentally comprehend. 
&gt; Also, password returns by API in open view. Anyone can log in as any player or coach and broke or change data. To me this sounds like something that could take considerable effort to correct. It sounds like there is a fundamental issue with how the API works. Without seeing the code it's just a guess though.
Well, an open password is not a minor issue and hints at other security related problems. Please do not make the same mistake as many other startups by taking security lightly. Do it right, delay the release if needed, or don't start at all. Customers vote with their money, and once the trust has been lost, you will not regain it. Be transparent of you have to delay. 
Regardless of whether scala is going to take off or not, I started learning scala in my free time and really loving it so far. I believe it is making me a better programmer and make me apply the right tool for the right job. That's the most important thing that I care about.
We are extracting out things from our data pipeline platform as open source components, and Schemer is the first piece. Feedback and thoughts will help us know if people are finding it helpful.. Here's my talk about our data pipeline platform - https://www.youtube.com/watch?v=UbzMuEXt59c Part of the demo includes the schema registry features.
Some documentation would be nice...
I wonder if it lends itself well to optimization with the proposed macro system.
Wow, thats awesome! The native cryptography capabilities of Java are such a pain to work with in idiomatic Scala, great to see a carefully designed functional alternative! Now I have to think of a project to use this library (related: https://i.redd.it/79yl0lwl2yzz.jpg).
We appreciate the comments! Actually, there's more in store. On master, we have already merged `tsec-libsodium`, which is the first scala-only bridge to the libsodium crypto library, and (in our opinion) the hands-down best libsodium bridge on the JVM, as the api is functional as well as nice to use (i.e [this](https://github.com/jmcardon/tsec/blob/master/bench/src/main/scala/tsec/CipherBench.scala#L66)). That will merit it's own post though, because: * Libsodium is by far hands down the best security library out there right now. * Other java libsodium bridges are neither functional, nor nice to use * Performance, even after JNI overhead is _at least_ equal or better than the JCA alternative. That is: in our Benchmarks for `HMAC`, we have come up with the `tsec`, `tsec-libsodium` and raw JCA to be of almost equal speed, but in the `AES` benchmarks, libsodium even with the JNI copying, yields a 10x+ speed increase! * Tsec-libsodium enables authenticated encrypted streams, which are _not possible_ in the JCA for streams of arbitrary size, because `AES-GCM` implementation buffers in memory, as well as there _is no good implementation_ for XChacha20-Poly1305 or XSalsa20-Poly1305 on the jvm and there will never be, because those algorithms require operations that are optimized for the CPU that you simply cannot do on the JVM. All in all, tsec-libsodium is excited and we're excited to show the community, but we need a tad bit more work behind it (in particular, a nicer installer since it requires swig compilation and libsodium), but the future looks bright.
Unrelated to the post itself but from part 1 linked at the top: &gt; Note: You could also use an implicit object but it can cause implicit resolution problems later. I can't remember why/when anymore, it's just become habit. Does anyone know what they're talking about? I've not encountered resolution problems with implicit objects compared to implicit vals before.
If I were you. I would make a small sample project with some code similar to your actual project. That way, we can look at the code and then give you some hints on what may be going wrong. otherwise its very hard to say why you are getting low throughput in prod vs your dev.
Great work. Been waiting for fart too long for something like this. Thank you &lt;3. The APIs look very nice.
hehe fart.
Formatted: import scala.io.StdIn._ object Question_match { def main(args: Array[String]): Unit = { print("Enter first integer: ") var value1: Int = readInt() print("Enter second integer: ") var value2: Int = readInt() value1 match { case 1 =&gt; value2 += 1 case 2 =&gt; value2 -= 4 case 3 =&gt; value1 = value2 * 2 case 4 =&gt; value1 -= 3 case 5 =&gt; value2 += 1 case _ =&gt; value1 = value2 + 1 } println("value1 = " + value1 + ", value2 = " + value2) } }
What's your question? I'm kinda confused.
Going through the case and inputting the values 4 and 6 respectively for val 1 and val 2, can you explain to me using maths with the cases, how is it calculating the final values. The final values should be 1 and 6 respectively for val 1 and val 2. 
Step 1 value1 = 4 value2 = null Step 2 value1 = 4 value2 = 6 Step 3 value1 = 4 value2 = 6 Match lhs value1 Step 4 value1 = 4 value2 = 6 Match lhs 4 Step 5 value1 = 4 value2 = 6 Match lhs 4 Match rhs 4 =&gt; value1 -= 3 Step 6 value1 = 4 value2 = 6 Match lhs 4 Match rhs value1 -= 3 Step 7 value1 = 4 value2 = 6 Match value1 -= 3 Step 8 value1 = 4 value2 = 6 Match value1 = value1 - 3 Step 9 value1 = 4 value2 = 6 Match value1 = 4 - 3 Step 9 value1 = 4 value2 = 6 Match value1 = 1 Step 10 value1 = 1 value2 = 6 Step 11 value1 = 1 value2 = 6 println("value1 = " + value1 + ", value2 = " + value2) Step 12 value1 = 1 value2 = 6 println("value1 = " + 1 + ", value2 = " + 6)
Yes! Sorry about that.
Hi My name is Lior. I'm an experienced scala developer working as a freelancer these days. I have a rich background with many technologies/environments. I have experience and ability to to build products from scratch to production level by myself. this is my email: liorchen2@gmail.com . feel free to reach me out any time.
I'm trying to close the rabbitMQ channel before triggering the killingSwitch so the source won't pick messages any more, but the thing is that the source is listening on the channel and if I close it, the stream shutdown will be called by the akka streams, resulting in the some problem that I had before.
You can try debugging it step by step, you can also print the two variables after each assignation to see how they change
The match says if value1 is 4, subtract 3 from value1 and donâ€™t change value2. 4-3 is 1, so thatâ€™s why you get 1 for value1, and why value2 doesnâ€™t change from the 6 you entered.
This looks really good, impressive effort. I'll definitely be checking this out soon and maybe even trying to contribute ! 
You can use `Try` to encapsulate exceptions (but there are alternatives like the new [Scalaz 8 effect library](https://www.youtube.com/watch?v=wi_vLNULh9Y)) and then use `map`, `flatMap` etc. to process the result further. Your `takeMovieWithId`, `viewMovies` etc. methods should really return `Try[ResultType]` instead of using some "magical" value to indicate an error.
There are some misconceptions in the article. One that to me at least pops out: &gt; Conversely, Scala uses the actor model which is more modern and efficient. No it doesn't. Scala has a library that allows you to use the actor model (akka) but you can use it from java too.
&gt; As Java was created in the 1990â€™s it was designed like other conventional object-oriented programming languages. However, it has since been upgraded with functional programming capability. I'd like to know where this myth comes from that Java is functional-programming-ready. When you don't have pattern matching, an expression-oriented syntax and facilities for immutable structures, I hardly see how you can call yourself functional. Lambdas are just syntax sugar for single-abstract method classes. In fact, IntelliJ used to _display_ SAM class instantiations as lambdas before Java actually had lambdas, so at least for IntelliJ users adding lambdas to Java changed practically nothing.
Not really helpful for your problem, but have you looked at https://github.com/sksamuel/avro4s ? 
Yeah I'm aware of Avro4s and have looked at it. However I mostly do schemadriven stuff so I generally don't have a case class representing a given Schema. 
It's not that it's functional-programming-ready, just that Java 8 gives developers the option of incorporating some aspects of functional programming if they want to. 
Thanks that scalaz library has some cool stuff.
Sure, but then people go around saying that Java has "functional programming capability" (as here), assuming that having lambdas is all it takes â€“ this is what I'm talking about. This is by far not the first article to show this specific fallacy.
While Java could possibly get [pattern matching](http://openjdk.java.net/jeps/305)/[switch expressions](https://bugs.openjdk.java.net/browse/JDK-8192963), [local type inference](http://openjdk.java.net/jeps/286), [data classes](http://cr.openjdk.java.net/~briangoetz/amber/datum.html) etc. in the near future, I don't think it will ever features like HKT's and typeclasses/implicit parameters which are what makes Scala suitable for more advanced functional programming.
I once had to create a MITM proxy server for recording and replaying integration tests against a third party service. Basically I would connect to my proxy and click my way through the third party website I'd like to record. The MITM proxy stored the responses and I could run integration tests against them with the proxy in replay mode. As third party sites may use HTTPS I needed to dynamically fabricate a certificate from a self-signed root cert and use this certificate in negotiations with the client. I ended up using bouncycastle and let me tell you, it was not a pleasant experience. Not sure if this library covers this exact scenario but I welcome anything that makes the current situation more manageable.
You can do it, with quite a bit of verbosity. Even the pattern matching can be simulated with a match function and some special data structures; expression if/else with ternary. Check out [Vavr](http://www.vavr.io/vavr-docs/). As for who's spreading the news, Venkat Subramanian is one respected member of the Java community I've [seen](https://youtu.be/15X0qFtBqiQ) give a talk on functional Java 8.
&gt; You can do it, with quite a bit of verbosity. Even the pattern matching can be simulated with a match function and some special data structures Yeah, of course, you can do anything in a Turing-complete language, modulo encoding. But when the encoding is hopelessly verbose, there is no point. Yes, you can simulate ADTs with fold functions, but then non-trivial composite patterns require putting everything in CPS or duplicating lots of code â€“ no thanks.
How's that any different than what you have to do in Free or Io to make monads stack safe? It's not hopelessly verbose. Try to write a short program using Vavr. It isn't that much different than Scala in terms of ergonomics. Not that I would enjoy it, just that it isn't hopelessly impossible. 
https://github.com/scala/bug/issues/8697
Thanks! To save others a click &gt; @retronym said: Implicit objects are the same as implicit vals without explicit type annotations. We don't know there type without typechecking there body. &gt; Implicit search will never force inference of an implicit down lower in the same file as the implicit call site. This avoids spurious cycles in inference.
If you aren't constrained to using the default Java sql library, there are other Scala libraries for database access like Slick, Quill, or Doobie. Most of them expose error handling functionality that you can leverage.
If you want to go the pure fp route, this would be a good start: http://tpolecat.github.io/doobie/
Regarding the writer monad found in free monads texts for logging... If the logging is delayed til the business domain objects are run through the interpreter.. how do you debug the actual building of the tree of building domain objects?
Just write a function to log your program as you traverse it and compose that function with your interpreter. def log[F[_]] : ShowK](logger: Logger): F ~&gt; F = new (F ~&gt; F) { def apply[A](fa: F[A]): F[A] = { logger.info(fa.shows) fa } } usage would look something like: val myProgram: Free[F, Foo] = ... val foo = myProgram foldMap (log andThen myInterpreter)
I have been saying this. Kotlin's goal is very unclear. They say that they want to be pragmatic, but so does every language. No language wants to be non-pragmatic. Java has realized this and is changing. Although a little slower, it will still evolve. Except for Android I don't see Kotlin giving any advantage in the foreseeable future. Glad that you share my view.
Isn't akka actors the default starting from scala 2.11? Source - http://docs.scala-lang.org/overviews/core/actors-migration-guide.html
I don't see anything wrong here. Someone is sharing their opinion, if that is wrong then call it out so that others wont make the same mistake. 
It's the default *actor library* for Scala. But it's not the default *library*. Many of us use Scala without any kind of actors.
Yeah, but what the author could have meant is "Scala has a good actor library built into the language" which is a scalable concurrency model.
This feature is not that important, but i tend to write as much tests as possible, to help me during code refactoring. Thank you for your proposed solution to *System.getProperty("user.home")* , I think it is better than what i done.
Maybe you will like chess server for [lichess.org](https://github.com/ornicar/lila)
But it's not built into the language, it's a library like any other. 
Does your method have implicit parameters? I don't use scalamock but forgetting to pass in implicit explicitly is a common mistake I make as well when using other mocking libraries
&gt; ... What's there to advocate so harshly? ...
It does actually yeah. It has an implicit Execution Context, but I have "import scala.concurrent.ExecutionContext.Implicits._" imported in my class which I thought takes care of that.
I think that the title of the post here, is misleading and written in order to get clicks. 
2nd Ed came out in Jan 2011 when the current version was 2.8 and we're now approaching 2.13, and there have been a ton of changes. So I would encourage you to get the third edition if you can. Also see *Programming Scala, 2nd Ed.* by Dean Wampler (O'Reilly).
I'm gradually switching away from using mocking libraries because of this. The resulting test code isn't really that pretty and is annoying to fix when it breaks
Thank you so much!
I was a book reviewer for the 3rd edition, and I gave a talk covering everything that changed in Scala since the 2nd edition: https://github.com/marconilanna/talks
Go is becoming very successful in a space that was often dominated by java. It's being used to replace back end networked service code that was written by startups in languages like Ruby and PHP once the lower performance of those runtimed becomes a problem. I work as a consultant, and I used to see these programs rewritten in Java almost exclusively by our clients. Now I'm seeing more and more go, and much less java. It's really picking up.
Awesome! I'll take a deep look. Thank you so much!
Mask the global implicits so you're never tempted to use them ever (you shouldn't be anyway). It's always a good idea to pass in an implicit at the method level so that you have finer grained control of it, which includes mocking.
Why even use Maps where each key maps to the same value? 
Thankyou for the tip, I will do that. :)
This is great. I met the author at SBTB and he's looking for feedback and contributions. I think this should be a Typelevel project ;-)
I gave a talk on how to achieve this type of compile time enforced ordering in an API at *Scale by the Bay" this year. https://youtu.be/JPVagd9W4Lo
This looks good, too bad Dobbie doesnt have schema creation from classes. Also no login, registration plug-in from http4s. Project also needs front end. I like how elixir's Phoenix and elm plays together
https://www.scalawilliam.com/essential-sbt/ https://github.com/ScalaWilliam/ActionFPS
I have some experience with akka-streams, though not its XML processing, so while I can't definitively answer all your questions I think I can provide insight on a few. &gt; needs to be fast. very fast. My experience is that everybody says this but it's almost never true. You might _want_ it to be "very fast", but it's unlikely to _need_ to be as fast as you think. Avoid whatever obvious performance pitfalls you know of, maybe put in like an hour's research on pieces which simply feel too inefficient, and forget about the rest at the start. Chances are it'll be fast enough, and getting something working at all is already infinitely faster than if it didn't exist. Optimize after you have measured a program that works and found it lacking. In a sense, if you don't have numbers you don't have a problem. &gt; Do you really need to build everything in your application into a stream? My opinion is that the real value in akka-streams is less about the incremental aspect of streaming and more about the automatic back-pressure. If you're not making use of the back-pressure, then I feel you're not gaining much over writing standard functions and avoiding the library. If you are making use of it, it works best when everything in the system is built to understand back-pressure. So I'd hesitate to say everything must be a stream, but I would say that if it benefits from back-pressure and can be processed in stages, then it might make sense to include that in a stream. &gt; Sinks take a second type parameter which is "emitted" during processing. What is this for? Ah, the "materialized value". It took me a while to form a mental model for this and my last attempt to explain it wasn't super effective but I'll try anyway. There may be some technical inaccuracies here - I'm just trying to convey the intuition for what happens - though I'd appreciate corrections on the technical aspects where they are wrong. The materialized value is a pretty general notion so I will be talking in pretty general terms, followed be a couple more specific examples. With akka-streams you construct a graph which describes your data processing pipeline (the `RunnableGraph`). This graph actually does nothing until you "materialize" it. During materialization, the graph is validated to ensure all sources and sinks have been connected and you haven't, e.g., connected an output to more than one input. If you're using the `ActorMaterializer` (and you probably are) that's also when the various actors that will process the messages get created. As a side effect of all this construction you can get a value which is not part of the data processing pipeline but which may tell you something about the stream or let you interact with it: the materialized value. Many stages in the graph may have their own materialized value created upon construction of the graph. These tell you something about the stage and it is up to you how you handle all of them together to give you information about the graph. When you wire one stage to the next you can choose what do with each stage's materialized value using `Keep.left` or `Keep.right` or `Keep.both` or whatever combination function you choose. So, for instance, if you need to do something with the first stage's materialized value you'll have to pipe that through the whole graph so that it gets created and returned ("materialized") at the end. More concretely, one kind of materialized value could be a `Future[Done]`, which might indicate when your stream has processed all the elements that it's going to and is complete, either successfully or with an exception. This `Future` isn't part of the stream and isn't sent through the stages, but does give you some extra info about what's happened within the graph. Another kind of materialized value is one I used through the `ActorPublisher` trait. This materialized an `ActorRef` to which I could send messages, and the result of that actor's processing could be emitted into the rest of stream. So again, the materialized actor itself doesn't go through the graph, but the actor is sort of a "handle" which lets me interact with the stream. I used this to get some RabbitMQ messages from a third library into the akka-streams pipeline as this was before the Alpakka AMQP connector supported late acknowledgment of messages. In fact I needed multiple such actors for different queues, so I made sure I preserved the materialized value from each stage in order to handle them correctly later, outside the graph.
I've used akka-stream to solve a very similar problem, except that it was mostly CSV, not XML. I've extended the app to do XML as well since then, but my initial use case was CSV. Additionally I needed to be able to swap out several different CSV "schemas". Once parsing the data I needed to transform it into an object in our application domain, and then store it in a database. Statistics about what was produced also need to be aggregated and saved for later analysis or potential reprocessing of the file. So I have 2 (or 3) main functions or flow types: 1. Parse a row of a CSV into a domain type. This needs to be aware of the source file. Something also has to be responsible for opening and closing the stream. 2. Store a domain type in the database. In our case this logic is defined per domain type. Once these concepts are defined and only then, I went about writing the akka-stream code, which simply wires everything together. Each individual function is testable (mostly) without involving akka at all, but when needed, different pieces can implement their flow using the akka-stream API. I'm out of time to write this now but hopefully that helps.
&gt; CSV Me too. You have my sympathies. Are you using Alpakka?
It does indeed. This answers possibly the most important question I've been having in that my expectations of the Akka-streams implementation can and will solve this problem, which I'll be honest is daunting. Using a new tool, lib, util, framework for is there's always the existential dread. Constantly re-asking myself if I'm actually going to solve the problem I've set out to solve. Maybe this is common, maybe it's not but with new things, there's always a bit of a dark cloud hanging over. Thanks, friend! 
Wow, this is a great explanation. I wasn't away of the flexibilities in the ActorMaterializer and I think this might be a concept I can look into get get a better understanding. As far as your description of the mental model for this behavior, to me, this makes perfect sense. During stream processing in Spark apps there's always the data going into or produced from the flow, but there are times when a single stage can produce some data that isn't going into the next stage (failures, metrics, etc). So from what I'm gathering you can use the akka-stream to define not just the flow, but the effects (in a type effects type way) of the flow. Am I on the right path with that? Thanks for taking the time to write up this response. Greatly appreciated.
Oh there's another question I had so sorry for the double response. I could edit the comment but maybe this is better. If I have a graph stage where the required operations are IO bound how can I beef up that stage with more "workers". Since backpressure is the primary concern I would assume that the would be a bottleneck. Is there a way to make sure that I've allocated enough processing power to the slow parts?
I'm pretty convinced schema creation from code is backwards. The schema must be designed in the database's terms and by its rules, in a way that maintains as much consistency as possible because it's the bottom of the stack. Also the data will tend to live much longer than any program that accesses it, so it has to be your *axis mundi*; the structure of your program follows from the schema, not the other way around. Code generation from the schema is possible, but not very useful here because doobie doesn't generate SQL and thus has no need for a programmatic representation of your schema. Code generation for Slick, on the other hand, is quite useful.
&gt; there are times when a single stage can produce some data that isn't going into the next stage (failures, metrics, etc). So from what I'm gathering you can use the akka-stream to define not just the flow, but the effects (in a type effects type way) of the flow Not quite. Or at least I wouldn't think of it in those terms. If something doesn't go to the next stage you'd usually just not send it on. All the side effects that the stage could have like making an http call or logging are not inherently captured in the materialized value and it feels like it'd be a substantial amount of work to make that happen. The materialized value is an actual concrete thing that each source/flow/sink will give you once, and only once, upon materialization. How you've decided to `Keep` those values when wiring your stages together determines the materialized value of your graph. If you materialize the graph a second time you will get a second materialized value of the same type. So it does not describe anything about how each stage _behaves_ for each element it processes. It gives you a single value external to the constructed pipeline which may give you some information about what has flowed through the pipeline, or the status of the graph itself, or let you interact with the graph. For instance `Sink.head` materializes to a `Future` that is the first value received by the sink, if any, and completes the stage. `Sink.ignore` materializes to a `Future` which indicates when the stream has ended. `Source.actorPublisher` materializes to an actor that can emit elements into the stream. `Sink.actorRef` materializes to an actor which gets forwarded all the elements that sink receives. You might be able to encode stage/graph effects via the materialized value, but that's not really it's intended purpose. It's for surfacing extra information from each stage which may be useful outside the graph. And you don't have to surface anything. Indeed, the `NotUsed` value is specifically to indicate that no materialized information is available.
&gt; Is there a way to make sure that I've allocated enough processing power to the slow parts? Check out the docs on [pipelining and parallelism](https://doc.akka.io/docs/akka/current/stream/stream-parallelism.html) and [balancing to a pool of workers](https://doc.akka.io/docs/akka/current/stream/stream-cookbook.html#cookbook-balance). You may want to mark those stages as `async` so that other parts of the stream can continue while they do their processing, and you'll need some kind of fanout and merge strategy for sending elements out to the workers and getting the workers responses back into the stream.
OHHHH, ok. I think I get it now. I'd have to play around with it to see if I actually *understand* but thanks for the clarification. This puts me in a much better mental model than before.
Thanks, I'll check those out. As my other comment mentioned I think I'm on my way to thinking about streams in a more detailed way. Instead of a massive black hole, the different parts are starting to make sense. Cheers.
Any particular reason why akka-streams? FS2 appears at least as appropriate, though I am no expert. What I'd like to see is some kind of guide to picking an FP streaming library, however given the pace of evolution in this space it would be a challenge to maintain fairness over time...
I at this point Iâ€™m only interested in an Akka-streams solution because our cluster already has a strong dependency on Akka. I feel like it is appropriate to see what Iâ€™ll be able to make staying in the same solution family. Once Iâ€™ve gotten a POC and a better understanding of what the pros and cons are I think Iâ€™d be in a better place to compare some of the other libs. Iâ€™m a huge monix fan so itâ€™s not that I donâ€™t care about the others theyâ€™re just not what I want to focus on at the moment. 
Nope! I don't think that existed when I started this project. Reviewing it now, I recognize the Xml.scala code from when I was looking at implementing Xml parsing and arrived at akka-stream-contrib. Maybe it was renamed to alpakka? I needed a CSV parser that was battle tested. I open a commons-csv parser inside of a flatMapConcat, emitting the CSVRecords downstream with a lightweight wrapper. I think the official akka-stream documentation glosses over how easy it is to use existing libraries for IO, and pushes people towards custom graph stages or custom parsers that operate on ByteString. Maybe I'm doing something wrong, but it seems to work perfectly fine! I think if you are receiving the files via akka-http then having akka-stream-aware, first class parsers is much more important to abstract away the source of the bytes. 
Good is relative. Many of us flee as far from Akka or any Lightbend product as far as possible.
And why is that? I see they are being used extensively in the industry. I don't think there will be a market for Scala I'd not for lightbend products. 
Hi all I would be very happy, if someone knows the answer: https://stackoverflow.com/questions/47859294/object-kkapi-is-not-a-member-of-package Thanks
I'm not sure if you are using SBT or not, but if you are it has a specific mechanism for multi-projects. So it has to be set there rather than in the IDE. Here's a very minimal example (I'm sure you can find others, which may be better): https://github.com/benkobalog/SbtMultiProjectExample/blob/master/build.sbt Here's the related SBT documentation: http://www.scala-sbt.org/1.0/docs/Multi-Project.html 
First of all thanks so much for your answer. I am using sbt. I will try it out and let you know. Thanks again.
It's true that Lightbend products are used throughout the industry. I've used everything from Akka to Lagom (including teaching their workshops, for a period of about 6 months). My issue with Lightbend comes from the perspective of a functional programmer. My complaints are twofold: 1. There is a lack of code quality that stems from their implementing most of the core of their frameworks in Java, or java-like patterns that eschew functional programming in favor of an opaque "performance" requirement, which is almost never necessary, and leads to very convoluted design patterns which obfuscate the true working nature of the code. Akka, for instance, is implemented using `PartialFunction[Any, Unit]` as a base abstraction for actors, as opposed to making typed actors (which are only paid lip service - they've been experimental for more than half a decade), and an implementation (e.g. the [ActorSystem](https://github.com/akka/akka/blob/master/akka-actor/src/main/scala/akka/actor/ActorSystem.scala)) that is so laden with side effects that it's impossible to truly reason about the code. Yes it might be fast, yes it interoperates with Java, but at the cost of everything FP (referential transparency, type safety, resusability) - is about. If you've ever attempted to maintain a library build on Lightbend products (Play for instance, is a current headache for my team), then you understand the reusability of these libraries is incredibly limited by the quality of the underlying implementations, and the absolutely *dirt poor* level of documentation accompanying them. When Play and Akka Streams fail to document even basic behavior, it becomes a serious problem. Not to mention, it's not even necessarily faster, in many cases. Couple this with the fact that the Actor model is not even necessary for most types of concurrency, and the fact that people tend to use Actors in the place of more suitable concurrency models (such as Streams, or Tasks), this is a nightmare as it explodes in combinatorial complexity the larger systems get. Also, Lagom is godawful and Typesafe configs aren't typesafe. 2. There are only a few people at Lightbend who push for wider adoption of more up-to-date techniques and libraries. Coupled with the massive community adoption at the onset of Scala's popularity, this only reinforces Lightbend, and therefore java-esque, "functional-in-name-only" libraries as a bedrock and barometer for institutions adoption Scala in the future. &gt;I don't think there will be a market for Scala if not for lightbend products. I agree, to some degree. Initially, Lightend products *were* foundational in allowing Scala to become popular, but from what I've seen of the industry lately, it's not true anymore that Scala is *driven* by Lightbend. In fact, the opposite is true. Lightbend products were adopted extensively, and many have migrated away, which is feeding the current explosion in quality 3rd-party support in Scala, such as Typelevel and the Scalaz-ecosystem, less so focusing on Lightbend's core libs. Lightbend only really encompasses Play, Akka (with streams, http), Typesafe config, SBT (which has recieved so much denigration), Lagom, and the Enterprise suite. These are not even the largest, or most interesting draws to the Scala ecosystem. They may feature Cassandra (a Datastax enterprise), Spark (a Databricks enterprise), Kafka (not a Lightbend lib), and many others on their site (and do so in a very sneaky way), but only a few of the "products" they expose are owned and maintained by Lightbend. No hate for Lightbend over here, btw. I think they did a fantastic job helping Scala mature. But I do also think it's time to move on, now that people are demanding more FP-proper solutions to their problems, and we've figured out how to do it in a performant way.
Short answer: they do not exist in required quality. You can use spark for some things. You can load tensorflow model graph and do predictions (and also training if you are brave enough). And yes there is weka and couple of projects with small traction.
Apache Spark and Elastic Search for big data. If you need high performance computing I like [ND4J](https://nd4j.org/). I am not how much of the scientific ready made libraries that Numpy and Scipy has available are available through ND4J though. For visualization maybe Kibana?
At work we're going with a sort of parallel but different stack -- [Flink](https://flink.apache.org/) which feeds into [Druid](https://flink.apache.org/) and with visualization in [Apache Superset](https://github.com/apache/incubator-superset). We are going with Flink because its support for joining streams within a window is amazing (you cannot join two streams in the new structured streams in Spark, not yet anyways). 
Thank you for managing that subreddit!
Yeah, the lack of a Scikit-Learn or even Statsmodels equivalent for Scala is a real sore point for me. If they also supported Spark it'd be better, but even just something that can run on a million element array would be useful.
Yes thanks - it's a great resource!
I think its good. Scala is multi-paradigm and these libraries only add to the ecosystem. &gt; There is a lack of code quality that stems from their implementing most of the core of their frameworks in Java, or java-like patterns that eschew functional programming in favor of an opaque "performance" requirement I agree. I dont have experience with play, but I can definitely say that Akka has mediocre documentation at best. The Typelevel ecosystem is improving a lot and I have been hearing nothing but good things. But the real problem with frameworks is what companies use them? If the community is big, then the support for it automatically is. A company trying to use a framework will look for - Community (Not just users but big companies using them) - Support (Kind of overlaps with the above point. Paid support is kind of necessary and lightbend does that) Frameworks are more important than languages. Many use Play with Scala just because its better to use Scala rather than Java. Same goes with Java. Nobody likes the language (its improving now) but they use it because of the huge number of frameworks and support. 
Didn't know about Apache Superset, thanks for the link. 
any recommendations? 
you should consider using ammonite for this:) 
BTW is there any public preview of this somewhere?
No problem, I hope it was helpful.
Really appreciate the work you put in managing that sub. Props!
Interesting talk, but the end was a bit worrying. I totally agree with striping obsolete and rarely used language parts, like procedure syntax but things like implicit scope or implicit conversions are quite established and widely used in the community (does not mean one should use implicit conversions everywhere but there are occasions they are quite usefull, for example overcoming limitations of HO-types regarding variance). Can someone explain part about existential types, cause i did not understand it properly.
Superset has been really easy and fun to set up and play around with. Definitely would but hesitate to recommend that.
&gt; `val bars = "â– â–‚ â–ƒ â–„ â–… â–† â–‡ â–ˆ".split(' ')` not needed. in scala, strings are already treated as `Array[Char]`: ```scala scala&gt; val bars = "â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ" bars: String = â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ scala&gt; def sparkLine(x: Int, max: Int) = bars(((bars.length*(x-1))/max) max 0) sparkLine: (x: Int, max: Int)Char scala&gt; for (i &lt;- 1 to 8) print(sparkLine(i, 8)) â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ ```
My favorite talk is from John Degoes. https://www.youtube.com/watch?v=CIPGZzbPpeg 
Thanks everyone for complements. 
How about rendering static sites, XMLs etc? You can play with this all you like: https://github.com/sake92/sbt-hepek I made my website with it: https://github.com/sake92/sake-ba-source It is VERY simple, just calls your `render` method. You can do all sorts of interesting things with it. You can use the **same structures** to build HTML, XML sitemap, PDF document etc. I'm planning to make it abstract, to support *pluggable themes*, like Jekyll and similar. But it is more **intuitive**, you have autocomplete, whole Scala language in your toolbox etc.
Thanks it was helpful thanks so much.
I have following setting: lazy val commonSettings = Seq( organization := "io.khinkali", version := "0.1.0-SNAPSHOT", scalaVersion := "2.12.4", libraryDependencies ++= Seq( "org.slf4j" % "slf4j-simple" % Log4j, "ch.qos.logback" % "logback-core" % Logback, "org.apache.shiro" % "shiro-all" % Shiro, "org.typelevel" %% "cats-core" % Cats, "org.typelevel" %% "cats-effect" % CatsEffect, "org.apache.kafka" % "kafka-streams" % Kafka, "org.apache.kafka" % "kafka-clients" % Kafka, "commons-beanutils" % "commons-beanutils" % Bean, "io.circe" %% "circe-core" % Circe, "io.circe" %% "circe-generic" % Circe, "io.circe" %% "circe-parser" % Circe, "com.github.mpilquist" %% "simulacrum" % Simulacrum, "org.scalactic" %% "scalactic" % Scalactic, "org.scalatest" %% "scalatest" % Scalatest % "test", "org.scalacheck" %% "scalacheck" % ScalaCheck % "test", ), resolvers ++= Seq ( "Sonatype OSS Snapshots" at "https://oss.sonatype.org/content/repositories/snapshots" ), fork in run := true ) Could someone please explain it to me what does `fork in run := true` mean? Thanks
&gt; implicit conversions are quite established and widely used in the community i didn't see the talk yet, but would it break http://docs.scala-lang.org/overviews/macros/implicits.html? that's bad
Too bad it doesn't support Apache Ignite or I'd be all over it.
I had that but I need it to return a String not a Char since I scale it by doing `* 3` later. The other alternative is to do `" â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ".map(_.toString)` but the `split` is shorter and bars are more readable.
I think the problem is working under the assumption you'd want the same kind of library in Scala, why not use one of the algebra libraries to do machine learning?
It runs the app in JVM separate from SBT. The information is readily available here: http://www.scala-sbt.org/1.x/docs/Forking.html
Is there any practical difference between using a companion object's apply method vs using the class' constructor? Not having to use the new keyword doesn't seem like enough of an advantage to warrant a new language feature.
I don't entirely understand the problem thats being solved, but what makes you think casting is a solution for it?
I read the doc but could not figure what does it mean. Thanks
Hi Milyardo, I figured out I used map function Thanks 
If you want to have a second constructor to your class, and you want to add some more logic in the constructor (i.e. not just passing the arguments to an other constructor) you would have to use an apply method (or some other pattern of factory method) because secondary constructors must only contain one simple forward to an other constructor. For example this does not compile: class Foo(i: Int) { def this(str: String) = { val x = 1 + 2 this(x) } }
Oh interesting, thanks for the explanation!
I don't think so. They are talking about removing package objects from implicit resolution scope and prefixed objects (which means fully qualified names, I think). As long as you've imported the types to be searched, it should search their companions. 
Are the `[` present in the data and `category` consists of some comma-separated values? Can you give us an actual line of input, please?
Look up parser combinators. I donâ€™t mean this as a smart-ass answer â€” the examples in the documentation for them are almost exactly this use case. 
Here is an example data set: ``` iPhone\n clicks 123\n likes 55\n follow 23\n Android\n clicks 45\n likes 22\n follow 34\n ``` And I want it to look more like: ``` iPhone clicks 123\n iPhone likes 55\n iPhone follow 23\n Android clicks 45\n Android likes 22\n Android follow 34\n ```
Updated
If you can get your lines as an `Iterator[String]`, say from `scala.io.Source.getLines`, you can used `grouped()` to group your iterator of lines into an iterator of chunks/groups of 4 lines. Then it's just a matter of turning chunks of 4 lines into one line.
Nah, unfortunately each category can have arbitrary length. My hope is to have some kind of map where I'm mapping the key to the first entry, and then creating key value tuples from the first line to next lines till it gets to the next category. Again, immutably.
Look into `fold`ing over the collection of lines, accumulating a `Map[String, Iterable[String]]` with categories as the keys and the collection of lines for a category as the values. Then you could `flatMap()` over the `Map`, like `categoriesToLines.flatMap { case (cat, lines) =&gt; Seq(???) }`. 
I didn't mean to come off as harsh. Was typing from the phone, glad you got the answer you were looking for :)
I've compiled the sources. You can download the pdf from https://coding.plus/think-scala.pdf
Point is to have battletested implementations of ML algorithms. Most of them are writtable from scratch in couple of hours/days, but you can get subtle things wrong (the ones not found next to pseudocode descriptions) and spend weeks debugging.
Are you implying that breeze and aglebird aren't tested or used in production?
Breeze and algebird are layer below ML algorithms. You build svm, logicstic regression, ... on top of matrix/numerical manipulation libraries. I was reffering to this.
Besides hiding constructors (you can make them private, even the main one), I think this philosophically is much more about merging OOP and functional paradigms. Being able to succinctly express something like `val listOfFoos = listOfbars.map(Foo)` is nice. Even more so if `Foo.apply` accepts multiple params and you have a method that expects a function with such signature.
The math looks solid - the implementation needs work. I see no type annotations, POJO style objects, and mutable data structures. If you'd consider reimplementing the bulk of this in the FP paradigm, I'd be happy to help.
 &gt; val str = "-iPhone\n clicks 123\n likes 55\n follow 23\n -Android\n clicks 45\n likes 22\n follow 34\n reshare 2\n" &gt; str.split('\n').map(_.trim).foldLeft(List.empty[(String,List[String])]) { case((x,xs)::tl,s) =&gt; if (s startsWith "-") (s,Nil)::(x,xs)::tl else (x,s::xs)::tl case (Nil,s) =&gt; (s,Nil)::Nil }
Awesome! I think the representation as ```(x,xs)::tl,s)``` is the part I have trouble with. I was able to do stuff like ```case x if x.startsWith("-")``` and so on, and still trying to figure out how to do better pattern matching as such.
Removing package objects in favor of top-level elements sounds like opening a huge can of worms though. Granted, the way package objects were declared never made much sense, but allowing to spread the contents of package objects over multiple places (which seems to be what is proposed with "top-level" things) doesn't feel like an improvement. At least you had a single place to check if you wondered where things came from, and even if you allow arbitrary top-level stuff. Also, the compiler is still on the hook for assembling things into a single scope (plus now also recording from which file they came from) to prevent clashes from defining the same thing in multiple places and not completely break incremental compilation.
In fact, "package object bar" is just sugar for: package bar object `package` { ... } So I'd say it's IntelliJ's fault for not displaying it in the way you want.
Yes, I know. The syntactic indirection added on top of it is just baffling, because it obscures what the compiler actually does, and is not even an improvement over the syntax it would have without introducing the special syntax in the first place. Not sure I'd blame IntelliJ. The source clearly says that the package object _bar_ is in package _foo_, and that's exactly what IntelliJ is showing to the user. Arguing that IntelliJ should undo the syntactic indirection introduced by the language is another point for getting rid of the special syntax.
&gt; if you wondered where things came from Removing package objects from implicit resolution means one less place for you to wonder where things came from when reading code outside of an IDE (articles, github, code review, etc). Why not explicitly import implicits like "import scala.concurrent.ExecutionContext.Implicits.global"?
No, not talking about implicits here.
&gt; implicit conversions Looks like the most harmful implicit conversions will be retained anyway. So a `Long` will still convert to a `Float`, except that it's now called "numeric harmonization" instead of "implicit conversions".
FWIW, I haven't checked in details but I don't think that happens in Dotty anymore, Long is only converted to Double when this can be done without losing precision: scala&gt; List(1.0f, 1L) val res0: List[Double] = List(1.0, 1.0) scala&gt; List(1.0f, (1L &lt;&lt; 62) - 1) val res1: List[AnyVal] = List(1.0, 4611686018427387903) scala&gt; val foo = 1L val foo: Long = 1 scala&gt; List(1.0F, foo) val res2: List[AnyVal] = List(1.0, 1) 
This is documented in http://dotty.epfl.ch/docs/reference/dropped/weak-conformance.html
As other people have said, they donâ€™t really exist. Now that said, the Spark data frame API was heavily influenced by pandas and is fairly rich. Iâ€™ve been using it long enough I find it more natural than pandas. For graphing ad hoc stuff Iâ€™d look at Jupyter notebook on top of spark. This is not a competitor to matplotlib but it communicates the idea. 
/u/osamc said it well, and I'd add the python libraries provide an enormous diversity of ML algorithms. Breeze and Aglebird do not. I saw a Breeze PR for a clustering algorithm I wanted rejected as out of scope.
Still pretty wrong though, especially as it has become impossible to explicitly opt out of conversions altogether.
[removed]
Indeed, literal integers have this double use: expressing a literal `Int`, and expressing an integer in arbitrary rings/fields. Ideally, I'd like to have a specific syntax such as `123I` to express an `Int`, which would not implicitly convert. While `123` would be untyped, materialize by default to `Int` but would implicitly convert to any ring/field if required.
assert and ensuring will throw an exception, this is why it's not used a lot. We use Either, Option or any data structure that can imply success/failure. For example in [FastParse](http://www.lihaoyi.com/fastparse) you have Parsed.Success and Parsed.Failure.
Slides here https://adriaanm.github.io/reveal.js/scala-2.13-beyond.html
I expect Java's value types to push this beyond the breaking point. Either the magic needs to get extended to cover more and more numeric types, or there will be some split between "privileged" numbers to which stuff gets automatically converted and "unprivileged" numbers where such a conversion doesn't happen. I'm really interested to see whether we will get to the point of `val c: Complex = 'X'` before the last few remaining people realize that automatic implicit conversions between numbers without any opt-out is kind of a bad idea.
&gt; why such constructs part of the language rather than being part of a library? they *are* part of the standard library, not the language itself
I know they are part of the predef package. What I meant is some external library like scalaz. Edited the question. 
`ensuring` and `required` can also be used in [stainless](http://stainless.epfl.ch/). Contracts are at the runtime and can fail unexpectedly. Stainless verify your program to make sure it's correct before it's runs.
That sounds interesting. Thanks for sharing. Will take a look.
You shouldn't need to - since all you're doing is instantiating a bunch of plain data objects there's nothing to go wrong. Any logic that would warrant debugging should be moved to interpretation time.
There are some things that have been fixed, some that have more or less comfortable "standard" workarounds, some that remain genuine problems. I can go through point-by-point if you like. Haskell does a lot of things right but I don't think laziness is worth it, and while Scala's IDE situation isn't great it's a lot better than Haskell or Idris. As I always say in these discussions, I don't think Scala (at least, a Scala that's faithful to today's Scala) is a thousand-year language. Maybe not even a 20-year language. But it's the best language going today.
I really wouldn't worry about how many passes it takes. Make it work, make it work right, then make it fast if you need to. Once you talk about doing anything complex I'd say write a real parser using something like https://github.com/scala/scala-parser-combinators#example .
I am working on a tool that lets you compare the information of two flat files (delimited and fixed length). The tools lets you compare files containing different types of rows, for examples headers, records and trailers. It also lets you select which columns you want to use during the comparison, so effectively you can leave out some columns from the comparison. there are other helpful features too. Like matching column values with regex, ignoring case, ignoring leading and trailing white space... This tool really shines during the creation and automation of regression tests for batch processes that output files or write to a database. You can easily check that a code change did not introduce a regression by verifying that the batch process' output have not changed. this is achieved by retrieving the batch's current output, either from files or database tables, and comparing it using this tool with the output from a previous batch execution. I am still in the process of writing the documentation, so if this tool interests you, send me a PM, and i will help you get started. [the code and the binary can be found here](https://github.com/MouslihAbdelhakim/Quick). [some usage examples can be found here](https://github.com/MouslihAbdelhakim/Quick-examples).
Scalaz had Validation, which is fantastic for contracts
usually when that object has more complicated ctor class MyStore(service1: Service1) { ... } object MyStore { def apply(): Future[MyStore] = ServiceDiscovery.locate(...).map(new MyStore(_)) } kind of thing
Encoding your invariants into the type system instead of checking at runtime.
The red book was a great introduction to FP concepts in Scala. From there, I tried to use to Scalaz but found the lack of documentation and tutorials a little frustrating. Tutorials would just show you how to use the Scalaz FP construct but never explain why it's useful. This book tied it all together for me. It answers the what, why and how and is presented in a clear concise manner. Really love this book.
Give smile a try https://github.com/haifengl/smile It's built on top of netlib-java, which is [pretty damn fast](http://fommil.com/scalax14/#/)
I use them! Especially in Java exposed code, where people potentially pass in nulls and such, require is very useful. But exceptions are for the unexpected failures, and otherwise you're better off returning an explicit failure type. More here: https://tersesystems.com/blog/2012/12/27/error-handling-in-scala/ If there's pre/post conditions then I would usually define a state machine and only transition from one state to another when conditions are met, and the same for a saga / process manager. The one thing that error handling doesn't manage well is error accumulation, which you can manage using things like [Or and Every](http://www.scalactic.org/user_guide/OrAndEvery), which I prefer over Scalaz's Validation.
You might the Refined library. You can do stuff like `String Refined Url` or `Char Refined Digit`. https://github.com/fthomas/refined
Have you done anything yourself to check the performance? Have you looked at jvm flag arguments? This is an engineering field. measuring things it useful. Pull out jprofiler or jmh and see where your code is slow. Take specific data sets to Target certain conditions.
How deep do you want to encode your contracts. [Refined](https://github.com/fthomas/refined) let's you constrain your primitive types (https://github.com/fthomas/refined#provided-predicates) at compile time rather than runtime with assert. [Shapeless](https://github.com/milessabin/shapeless/wiki/Feature-overview:-shapeless-2.0.0#collections-with-statically-known-sizes) can encode heterogeneous lists, extensible records, and dependently sized collections (among other things). [Value Types](https://docs.scala-lang.org/overviews/core/value-classes.html) can differentiate between a StreetNumber Int and an Age Int. [Leon](http://leon.epfl.ch/) and its scala subset Pure Scala can statically verify scala code. [`EitherT[IO, Throwable, A]`](https://typelevel.org/cats/datatypes/eithert.html) can encode side effects and error handling as a data type, so that even error handling is encoded as data, so that if you are reading data from disk and deserializing it into your model fails, you can encode the failure as data without having to use try/catch flow control statements. You pay varying degrees of compiler execution time penalty with these things.
I tried increasing spark executor and driver memory but that had no effect as well as different spark parameters. You bring up a very good point with the jvm. Which to be honest I had not though abou. Im not sure though if I have any of the tools you mentioned installed will check.
 val ex = "-iPhone\n clicks 123\n likes 55\n follow 23\n -Android\n clicks 45\n likes 22\n follow 34\n reshare 2\n" ex .split("-") .filterNot(_.isEmpty) .map( _.split("\\n") .map(_.trim) .filterNot(_.isEmpty) ).flatMap{ l =&gt; l.headOption.toList.flatMap( category =&gt;l.drop(1).map(v =&gt; s"$category $v")) }.mkString("\n") Alternatively, as a for-comprehension: (for{ string &lt;- ex .split("-") .filterNot(_.isEmpty) l = string.split("\\n") .map(_.trim) .filterNot(_.isEmpty) category &lt;- l.headOption.toList value &lt;- l.drop(1) } yield s"$category $value").mkString("\n") Either encoding outputs: res0: String = iPhone clicks 123 iPhone likes 55 iPhone follow 23 Android clicks 45 Android likes 22 Android follow 34 Android reshare 2
I use it. It's especially useful for catching nasty things at IO boundaries. Others tend to use Either/Option, or other ADTs or monadic formats, which is also good. But it has the drawback that once you've wrapped your type in a monad, you're forever gonna be working in that monad (with the exception of monad transformers where you'll end up in some other monad, or unsafe operations which throw exceptions anyway. The pattern I use is to always use requires or asserts in type constructors at IO boundaries, and use ADTs everywhere else. Then it becomes really easy to catch and handle invalid input errors, allowing you to use monadic code more easily and without worrying about all of the potential error conditions. 
Checked. Problems: Everything uses dense data here: http://haifengl.github.io/smile/api/scala/smile/classification/index.html (except maxent, which uses binary sparse data). I want to use regular sparse matrices (e.g. for tfidf weighting). And there is some SparseMatrix class, which does not seem to be used for any reasonable algorithm input (like logistic regression). This is really weird. No description what type of regularization is used in algorithms http://haifengl.github.io/smile/classification.html (L1 vs L2). This is huge. For me, this is a big dealbreaker. No notion of something similar to sklearn Pipeline, TruncatedSVD on sparse data, minibatch k-means and sgd for anything (either classification or regression). This is minor. And please do not tell me, that for actual prediction I have to call this predict function. http://haifengl.github.io/smile/api/java/index.html That's good damned awful. But seems like nice library (definitely better than things I saw before), but I am note sure, if I would say it has required quality. 
Your best bet with spark is checking you are not recalculating stuff you don't need to, things you can check are 1) the breakdown of a job in the spark ui (i.e .a single window of your stream) / stages / tasks 2) the generated DAG and ensure you are not recalculating anything on the rdd that you shouldn't 3) add timers across the actions and log the output 4) avoid unnecessary operations on the driver I see you are collecting the rdd at one point, which would force all the data in the executors to be brought to the driver - making all the paralellism pretty pointless if that is the case Also distributing that rdd on each window might be very expensive/slow - do you really need to do that?
I find videos from conferences a very good source of information, for example: [Scala for the Intrigued](https://www.youtube.com/watch?v=grvvKURwGNg) [Scala Tricks](https://www.youtube.com/watch?v=tY11Jtj-SA8) from sidebar [/r/ScalaConferenceVideos](https://www.reddit.com/r/ScalaConferenceVideos/) 
Ah, I've already seen the first one! It was a pretty cool introduction and the spokesman made everything easy to grasp. I went ahead and later re-implemented some of the patterns he showed. I also saw [Scala with Style](https://www.youtube.com/watch?v=kkTFx3-duc8&amp;t=3476s), which was pretty insightful. I'll take a look at the subreddit you mentioned, thanks!
Honestly, I would recommend playing with haskell first. I was in the same position as you and the functional programming aspects didn't really make sense for me. I read a few chapters of learn you a haskell for greater good and the functional pattern really just kind of clicked. It became a lot easier to learn scala imo and I really didn't put much time into haskell. Haskell also forces you to be pure. The only thing I've done in it is write a program that takes a number and finds the prime factorization using only the concepts I learned from 2 or 3 chapters of the book. It will give you a better understanding of what you need to learn to make use of functional programming in Scala. [https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687](this book by is also really good) http://learnyouahaskell.com/chapters
You're bringing each data batch from Kafka into the driver with this collect statement. &gt;val data = rdd1.map(_._2).collect() By doing that you have no parallelization of the string splits, timestamp checks and the creation of your objects. Also you're throwing that into a ListBuffer which could easily blow out your memory allocation for the driver as your data set increases in size. Also I'd take a look at those hive calls. Running something on hive that's not just requesting data from the partitions can be very time consuming since it needs to initialize and run its own job on the cluster. msck repair table can also be pretty expensive in terms of runtime since it needs to scan the entire table's directory on hdfs and also query the hive metastore for all the partitions for that table. Try running those commands outside of this spark job to get an idea of runtime.
Not 100% Scala, but does anyone know why the following HOCON doesn't parse in lightbend config, but parses fine on www.hoconlint.com https://gist.githubusercontent.com/DMCK96/bbadf6e7dbb8f0f1f1f34cfcc583cbfc/raw/d4f0a2608889d620c95099d6251dd98bd691b98e/rewards.conf
[Programming in Scala](https://www.amazon.com/Programming-Scala-Updated-2-12/dp/0981531687/ref=sr_1_1?ie=UTF8&amp;qid=1513816444&amp;sr=8-1&amp;keywords=programming+in+scala) is very good and gave me a real understanding of and appreciation for the decisions made in designing the language. If I were starting over, I would probably do Odersky's first [course](https://www.coursera.org/learn/progfun1) on Coursera, read the book, and then jump into a project.
Sorry. Yes. Play2, looking into the SLF4J as indicated by /u/amazedballer.
Make sure you are constantly writing code when following books/courses. Its extremely important to actively be writing code and modifying it to do random stupid things that amuse you so you can commit the concepts to memory faster.
The great thing about Scala is that you don't need to use a strict OO or a strict FP paradigm. Scala works with you to make things possible through rich types and implicits, and doesn't force a particular style on you. The tour of Scala is a good way to get an overview of what's going on, but it doesn't tell you about the idioms of the language, and why things like type variance matter. But when you want to define your types as strictly as possible, it starts to matter more. As you get into things like type safe builder patterns, you may start using F-bounded polymorphism so you can return the most specific type. When you start using type enrichment and type conversion, things like low priority implicits defined on the singleton objects and the type class implicit pattern start to matter more. Implicits especially tie a lot of magic together. I got many tips from Age's presentation, especially when it comes to using implicits: https://www.slideshare.net/NLJUG/scala-design-patterns-age-mooij Konrad has a great website all about the fun you can get up to with types: https://ktoso.github.io/scala-types-of-types/ And David Coupland's Tour of Scala is also useful: http://naildrivin5.com/scalatour/ In terms of "what can you do with implicits", I wrote an emoji handling library mostly as a joke, but it does show how you can use implicits in your API: https://github.com/lightbend/lightbend-emoji Although it's when you combine implicits with type parameters that things start getting really fun and strange.
The right rail of this subreddit has tons of good stuff.
Thanks you are definitely correct about the collect . I just have a question about how to apply the operations I am applying but to the rdd and not the array created by the calling collect. Would I do something like rdd.map(i=&gt;function(...)) where the function would take in i and apply all the operations from my for loop and return back a case class? Thanks
You probably want to flatmap in this case since you're returning multiple case classes per line coming out of your stream. That would be computed in parallel without needing the collect. You could also combine that code with the first map you're doing to pull the second value out of the tuple so you don't need to make two passes across each data set. I'm not at a computer right now to write the code for you right now so let me know if you need further guidance and I'll take a stab at it.
Actually scratch that. I just took a look at the code again and it is only one case classes output per line so you'd be fine with just a map.
This is one of those jobs that I would ask the client to completely refactor. The code features some very questionable practice including: - Printing out a large subset of the target offset table - Printlns everywhere, instead of using a logging framework - `toString.to(Int|Long)` conversions - Using a `ListBuffer` to concat data that was collected from an rdd - Calling `df.unionAll`, just to repartition, and then partition again in the same statement. - Using `return` to return your streaming context - No type annotations - Minimal use of Dataframes - Absence of Datasets, which make sense in several places - `val topics = Array(topicName).toSet`. No need to say more. If you don't understand the collections API, then it's going to be very difficult to understand a complex framework with an API built to *look* like the collections API. Yes, I'm being very harsh here. You need to learn Scala to a reasonable degree and understand the frameworks you're using before you start using them for production tasks. I have resources for you if you do feel like improving. Right now, I see a Java programmer who was forced to learn Spark and Scala, and was given a week to do it.
I think [this](https://gist.github.com/d1egoaz/2180cbbf7d373a0c5575f9a62466e5e1) is a great resource :) 
I think you chose the right language. I found this to be a nice resource, too.: The Neophyteâ€™s Guide to Scala http://danielwestheide.com/scala/neophytes.html 
[A collection of resources I found useful over the years which I recommend to coworkers](https://gist.github.com/felixbr/8d880365c3cd6673935e) 
[removed]
Yeah, for null handling we can use the Option monad. But I don't quite understand the " you're forever gonna be working in that monad". I maybe not understanding it fully but can't we `getOrElse`? We can transform them out of the container in that way right?
I don't have access to gist where I am, so just guessing: &gt; 1) Does anyone have a good Idea how to encode the Typerestriction on AVRO Union. Avro Unions cannot contain other Unions directly, but can for example contain Records which then again can contain Unions. So union -&gt; union is not allowed but union -&gt; record -&gt; union is ok. The most direct way to model this is with types; you can have two mutually recursive sealed traits: sealed trait RecordMember sealed trait UnionMember case class Record(members: Map[String, RecordMember]) extends UnionMember case class Union(members: Seq[UnionMember]) extends RecordMember ... If you want to do this in a fixed-point way, you have the problem that there are two equivalent ways to define the fixedpoint, and there's a direct conversion between the two (so you can define an `Iso` or something) so they're equivalent but not equal: case class RecordF[A](members: Map[String, A]) case class UnionF[A](members: Seq[A]) type Record1 = Fix[({type L[X] = RecordF[UnionF[X]]})#L] type Union1 = UnionF[Record1] type Union2 = Fix[({type L[X] = UnionF[RecordF[X]]})#L] type Record2 = RecordF[Union2] def record1AsRecord2(r1: Record1): Record2 = r1.cata {rur2: RecordF[UnionF[Record2]] =&gt; rur2.map {ur2 =&gt; fix(ur2 map unfix)}} If you're using boxed fixpoints then `record1AsRecord2` is massively inefficient at runtime, which may or may not be a problem for you. If you're using unboxed fixpoints then in theory it's a no-op, and you could even implement it as an `asInstanceOf` for efficiency. There might be a more elegant answer to working with this kind of fixed point - I'm not that experienced with these things. &gt; 2) would using fixpoint recursion in form of Fix, Free and CoFree make the querying later easier? I'm somewhat on the fence since I have no experience using these yet. I don't know about querying (I implemented TQL on top of fixed points but sadly that implementation was private to my client at the time; I'm not aware of a public equivalent). It gives you access to a bunch of nice traversal functions that you don't have to reimplement yourself. But it's always possible to write your own `cataM` etc. by hand.
Thanks for your feedback will try to fix as much as I can. 
Here's a free Beginners Guide to Scala! http://yoppworks.com/scala-guide/ 
Feel free to ask any questions you may have here. Also let me know if you need help experimenting with the framework, and if you would like to try and use it in your own projects ;\^)
see: https://scalabridge.gitbooks.io/curriculum/content/resources.html
Maybe you could describe more in details what you're really looking for? We can't guess what you're not interested in. There are plenty of amazing open source Scala projects out there. For example look here: https://typelevel.org
Did to checked Quasar? https://github.com/quasar-analytics/quasar Or Kafka? Big part of it od written in Scala.
He did specifically say *applications, not libraries* though. Everything on typelevel.org is a library.
Just clarified what I'm looking for on my edit.
There aren't that many sadly. www.lichess.org www.gitbucket.com http://bdgenomics.org/ 
Thanks for the advice! I've heard good things about that book as well and might end up picking it up since I found a free edition of "Scala for the Impatient" online. I've heard mixed stuff about Martin Odersky's course though, some people suggested to get a foothold on some element of FP before doing it, so I'm unsure about that (and also because I've never done any Coursera courses before). What kind of project would you recommend as a start? People often seem to go for web apps with CRUD interactions, is that the right approach (I suppose it is, since it also introduces me to some frameworks)? 
Thanks for the reply, I'll be sure to check those links soon! You just gave me a load of stuff to Google about, lol.
Thank you very much! Everything in there seems helpful, the only thing I really don't want to try for now is IntelliJ. I'm sticking to Sublime Text and the Scala REPL for now.
http://eed3si9n.com/sbt-server-with-sublime-text3
Spark and scalding are libraries...
Hmm yeah you're right.
I think the Coursera course is fine for someone with no, or minimal, exposure to FP. I did the additional courses, but didn't find them particularly worthwhile. My first project was a rewrite of a simple Node API using [Finch](https://github.com/finagle/finch). After that, I started working through the [Red Book](https://www.amazon.com/Functional-Programming-Scala-Paul-Chiusano/dp/1617290653). By that time, I was comfortable enough to take on building a fairly complex data analysis in Scala for work.
Here's a query for github: https://github.com/search?utf8=%E2%9C%93&amp;q=%22application%22+AND+NOT+%22example%22+AND+NOT+%22sample%22++AND+NOT+%22library%22+AND+NOT+%22reference%22++language%3AScala&amp;type=Repositories&amp;ref=advsearch&amp;l=&amp;l= Shows about 1000 applications.
Also in the genomics space is [cromwell](https://github.com/broadinstitute/cromwell/), a bioinformatics workflow system.
lichess is made in scala ? that's pretty cool.
Thank you for the answer and I will take it under advisement. The issue I have is that every UnionMember can be a RecordMember but not vice versa (Unions themselves being the exception) I have formulated it using both with inheritance and using shapeless coproduct. Somehow I find it both a bit clumsy and it clashes with the rest of the type constructs (sealed trait hierarchies) I used. But I guess those two are the only two somewhat sensible ways to do this.
I think [FiloDB](https://github.com/filodb/FiloDB) is something you might be interested in. Check out [those slides](https://www.slideshare.net/EvanChan2/2017-high-performance-database-with-scala-akka-spark). They contain some arguments for Scala.
Oh also https://www.querki.net/
Sadly, I think most of the applications written in scala are backend applications written by organizations that have no intentions of open sourcing their Code base. 
Another in the genomics space is [fgbio](https://github.com/fulcrumgenomics/fgbio).
I'm curious, you said, "I'm looking for good examples of why Scala was chosen for a particular project.", so why not include libs?
Oh wow, this looks useful! Thanks for the link :) 
Can also be applications as well if you run them standalone.
Many people who chose to write applications in Scala came via Play. http://playframework.com It's a very popular web app framework for JVM.
Here are two apps that I open-sourced while working at RetailMeNot: * [Shield](https://github.com/RetailMeNot/shield) - HTTP edge router with middleware * [Hydra](https://github.com/RetailMeNotSandbox/hydra) - A graph database that provides changefeeds for JsonAPI documents I chose Scala for those projects because of it's great async story. Both of those projects are used in environments with thousands of requests per second, and the async design behind Akka and Play allow for a easy/efficient scaling on individual hardware.
The Guardian has a ton of stuff open-sourced, including their Frontend, and they use Scala: https://github.com/guardian/
What was the problem with Odersky's [recent gist](https://gist.github.com/odersky/f91362f6d9c58cc1db53f3f443311140) on a potential new Scala macro systed that uses `'` and `~` operators for quotes and splices? I find [the example](https://gist.github.com/odersky/f91362f6d9c58cc1db53f3f443311140#example) incredibly elegant, but apparently the proposed system falls short in some ways. This isn't to disparage your work, it's more to see if there's potential for common ground between the 2 approaches. Impressive project, BTW.
There's lots of examples in https://www.lightbend.com/case-studies if you're looking for end user applications, and there are a ton of Play projects and examples out there. The larger open source applications tend to be government agencies, such as [https://github.com/hmrc](https://github.com/hmrc) or [delving](https://github.com/delving/culture-hub).
I think it's because using Scala in projects brings deep understanding about real world use cases. Libraries often don't expose much about why a decision was made in a domain-sensitive context.
Apache Spark is written in Scala, and here's a Quotation answer from the CTO about why they did: https://www.quora.com/Why-is-Apache-Spark-implemented-in-Scala
My rule of the thumb is that constructors are always private and do nothing but initialize fields of an instance to a valid state. `apply` and friends are public do all the work of shuffling around arguments for the final `new` call. Another benefit of this is that these factory methods can return an `Option[T]` or an `Either[E, T]`, while `new` can only return `T`s.
Thank you kindly! Very nice!
Fair enough.
I don't understand if his performance numbers are meaningful. If so, other library authors should feel uncomfortable. 
Http4s can be run as an application but of course itâ€™s not domain specific, but there would have been many decisions about how to build a functional web server. You can ask questions on their gitter. 
are any of these (other) approaches used in the while to any extent? my biggest issue with spark is error handling and decrypting exceptions, but I feel like wrapping it in another layer of abstraction would only make it harder in the long run
Squid is older, and it's actually implemented (not just a proposal). It also moves the state of the art further, doing more than MetaOCaml-type staging (for example, it supports code transformation); indeed Squid was the subject of [three research papers](https://github.com/epfldata/squid/blob/master/README.md#publications). Having first-class language support would be a plus (Squid is currently a simple macro-based library); in particular, it would simplify the handling of cross-quotation bindings/references and the integration as part of the macro system, which are nicely done in Martin's proposal. For all other intents and purposes, AFAICT Squid subsumes that proposal.
Yeah. E.g. We work with backend systems for admission to higher education. And even though the security *shouldn't* suffer from open sourcing, it's still safer to keep closed source. Also, people might get to hung up in the algorithms that we use for admission etc. . It's just better to keep closed.
Yeah, I was more getting at somehow combining Odersky's syntax with your established (i.e. actually implemented) library. First class support in the language would be a real plus indeed. AFAIK, there aren't many contenders that are as far along as Squid is.
that's fair. For sparse data, there are low level components https://github.com/fommil/matrix-toolkits-java but it's a far cry from a scipy / ML suite. It has always amazed me why scala has a reputation for big data, when nobody has filled this gap and the big companies making huge bucks out of scala don't appear to have invested in created anything (even proprietary). This is not a huge job, a 4 person team could basically catch up with and overtake scipy easily, it's all textbook stuff, no innovation needed. I have invested a large amount of time into building free high performance libraries for the JVM (and scala) and received nothing but abuse from users (who never wanted to pay for specialist support or development), and the facepalm that comes from seeing people want to do something from scratch (this is a common problem with the scala ecosystem, cf json, fp libraries, and lsp as examples). The big companies who are making huge profits off the ecosystem didn't even bother responding to my emails when I sent requests for funding for netlib-java. So I gave up. I wouldn't recommend that anybody embark on such a feat as a hobby project. It's fair to say that we have the tooling and ecosystem we deserve. Tech teams who make a living off scala need to start investing in our tools and our libraries. A good start is by subscribing to the scala center, an even better start is to dedicate engineer time to work on really critical gaps (like this).
https://github.com/ensime/ensime-server is an application, not a library.
Note that we have a pretty similar syntax to the proposal: in expressions (not patterns) you can both write `code"..."` and `code{...}`. But it's more clumsy when you want cross-quotation references. For example, this in the syntax of the proposal: '{ (x: Int) =&gt; ~{identity('x)} } Is written: code{ (x: Int) =&gt; ${identity(code{?x:Int})} } `?x` is an explicit free variable that is captured later by the `(x:Int)=&gt;` binding. (Also, note that our approach is actually safer, as it keeps track of free variables in the type system.) I'm working on making this use-case nicer, though. Ideally we could achieve a similar syntax, but it's tricky.
Well there were some attempts for Scala ML libraries from big companies (like https://github.com/ThoughtWorksInc/DeepLearning.scala). The thing is, that JVM is not primary enviroment for data science, which fits on one machine (people will just Python for that). And even if you want to use JVM + some ML without Spark, you usually end up coding only algorithms you need and do not push it as open source (because who needs only implementation of LDA + maxent classifier). BTW, the netlib looks really great. 
Good article! I'm not really into Big Data stuff but it's great seeing people advocating FP and its benefits. 
Why no one mentioned Twitter Finagle? Tons of battle-proven, high-performance, real-life scala code. Not that functional but totally practical example on how to build large applications in Scala. 
Here are two of my non-trivial, open source Scala projects: https://github.com/webjars/webjars https://github.com/Salesforce/salesforce-cla I only occasionally work on these projects and Scala / FP has really helped with the intermittent maintenance because when I come back to the code base after a long break, it is easily understandable. This isn't the case with other languages due to mutability, lack of pure functions, and limited expressiveness.
Disclaimer: my views don't always reflect those of the community as a whole or accepted "best practice". I hate mocking frameworks, they're full of incomprehensible magic and just not useful enough to justify it IMO. But yes, with most mock frameworks you can't "partially" mock an object - what the mock framework is actually doing is constructing a reflective proxy that captures all calls to methods of the object. When you call `manager.processCancellation`, that's just a call to this proxy - there's no actual instance of `CancellationsManager` here, so the real body of `CancellationsManager#processCancellation` can never run. &gt; What's the best way to test this? If you really don't want to change the code you could use PowerMock which allows partial mocks through bytecode manipulation. You could do what a mock would do "by hand", by implementing a subclass of `CancellationsManager` directly, something like: var statusCheckCount: Int = 0 val manager = new CancellationManager { override def checkStatus = { statusCheckCount += 1; true } } manager.processCancellation(request) assert(statusCheckCount == 1) Though ideally I'd suggest separating the logic of what needs to be done from the execution of it. Then you can test the logic by just checking it gives the right outputs for the right inputs, and the code that actually executes the things can be so simple that it's obviously correct and doesn't need unit testing. (If you start getting complex logic in that layer, it's time to carve out another layer of interpretation: rather than executing high-level commands directly, you create a function that interprets higher-level commands into (sequences of) lower-level commands, and you can test that function in an outputs-for-inputs way). E.g. maybe you could create a `PartiallyProcessedSalesOrder` type that contains the results of doing `task1` and `task2` and whatever remaining information is needed from the `SalesOrder`, and then the `PartiallyProcessedSalesOrder` can tell you which tasks need to be run next, and you can test that that's correct. Better still, maybe you can have two different subtypes of `PartiallyProcessedSalesOrder`, and then all you'd have to test is that doing `task1` and `task2` returned the right type for the different possible cases. &gt; Is it failing because I'm calling a method of the mocked object directly? Do I need to move all the task methods to it's own class then mock it, which seems a little strange to me just to be able to write test for it. Some TDD advocates would argue that this indicates that your `CancellationsManager` is mixing too many different levels of abstraction - that the very fact that you're having this difficulty testing indicates that moving the task methods out into lower-level classes would improve your design.
Mock objects in general are more trouble than they're worth, because they test the wrong thing. "Do I call the methods I think are the right methods with the arguments I think are the right arguments that return the values I think are the right values in the order I think is the right order" is not a useful substitute for "does my program behave correctly".
json parser and serializer
Most of the snippets in the post are adapted from code running in production :-) FP gives you literal super powers for reasoning about the execution of your program, doubly so with Spark.
Json parser/serializer -- something other than spray-json. Circe actually maps pretty easily to concepts from Jackson, and can auto-generate from scala case classes without any extra help. Docs for generating your own encoders and decoders by hand or autogenerated are easy to follow. files and streaming processing - fs2-io and cats-effect -- these libs have streaming interfaces similar to Java 8's stream. They are useful when working with any buffered content, including files. IO makes sync and async programming easy and safe. Again, the readme docs are easy to follow to get started and also have depth. http library -- gotta say http4s. Performance scales well with number of processors and the dsl is simpler than akka-https. The docs are also easy to follow and have depth. Mongo -- reactivemongo is your only hope, and it's easy to learn and work with too, with great docs. It is difficult to mock, but running mongo in-memory with testcontainers is easy. testing - utest, but not sure if /u/lihaoyi is ready for that to become the default testing lib. build tool - sbt. 
This was beaten to death already, and I'm not sure whether it fits into the definition of a platform module, but I would really like to see Scala.js have a commitment level similar to other core Scala projects like Dotty or collections. And I don't mean that as many engineers should be working on Scala.js as are working on those other projects. I just want to have complete certainty that Scala.js will not wither away / be slowly abandoned, that it is as core to Scala as everything else, because in my frontend-heavy use case, it is. I understand that Scala Center has long ago accepted the "Ensure continuity of Scala.js" proposal, and that as part of it you actually did spend valuable resources on Scala.js and scala-js-bundler with great results (thank you). But what I'm trying to say is, you'd never have an "Ensure continuity of the collections library" proposal, because for something as core as collections, such continuity is implied. I hate mentioning this because in all likelihood this is just FUD â€“ it's hard for me to imagine something as immensely valuable as Scala.js withering away, but the uncertainty is real when you're considering Scala.js for mission critical applications with expected lifespan of 10+ years (not even "enterprise", just normal business projects). It is entirely possible that Lightbend / ScalaCenter are already fully committed to Scala.js as a platform, but for some reason I just don't quite _feel_ that way. Perhaps marketing / communication is what could be improved, I'm honestly not sure. Sorry, I don't have specific action items on this.
Http4s isn't as battle tested as some other options. We ended up switching to finagle after having issues with http4s, e.g. timeouts being inaccurate.
I agree totally on the JSON point, if I remember correctly [this project](https://github.com/mdedetrich/scalajson) is being considered for inclusion. 
Am I missing something? Where's the list of libraries?
If this spec is for CancellationsManager you don't want to mock that class. Rather you would mock the dependencies of that class, such as a dao or rest client. If you find you need to "partially mock" CancellationsManager I'd agree with the other comment that you should break it up into two classes, then mock the second one with the individual operations.
and Play and, gasp, MongoDb ;-)
I almost didn't watch this because the title (and intro) made it look like it was just going to be a rant about Java object overhead, but it turned out really interesting: it's an introduction to [scala-packed](https://github.com/findify/scala-packed), a library that flattens Scala case classes into bytes (using Shapeless to derive codecs) and packs them in memory-efficient collections. 
Unfortunately I'm not able to find a page anywhere online that details the current accepted / ongoing proposals for inclusions. But you can follow the Platform category on the contributors discourse [here](https://contributors.scala-lang.org/c/scala-platform)
Thank you, I want to write the best code possible. Even if I don't get the opportunity to do it this time, something with similar problem will come in the near future. From what you say, seems like I need a new concept that keeps track of what task needs to be performed, i.e. `PartiallyProcessedSalesOrder`. Would the following be a better approach? case class SalesOrder(id: String) case class CancellationProgress(task1Done: Boolean, task2Done: Boolean, task3Done: Boolean, task4Done: Boolean) class SalesOrderCancellationProcess { def execute(salesOrder: SalesOrder): Unit = { val initialProgress = CancellationProgress(false, false, false, false) val p1 = doTask1(salesOrder, initialProgress) val p2 = doTask2(salesOrder, p1) val p3 = doTask3(salesOrder, p2) val p4 = doTask4(salesOrder, p3) val p5 = doTask5(salesOrder, p4) // if (doTask1() &amp;&amp; doTask2()) { // doTask3() // doTask4() // doTask5() // } } def doTask1(salesOrder: SalesOrder, progress: CancellationProgress): CancellationProgress = ??? def doTask2(salesOrder: SalesOrder, progress: CancellationProgress): CancellationProgress = ??? def doTask3(salesOrder: SalesOrder, progress: CancellationProgress): CancellationProgress = { if (progress.task1Done &amp;&amp; progress.task2Done &amp;&amp; !progress.task3Done) { // perform actual task progress.copy(task3Done = true) } progress } def doTask4(salesOrder: SalesOrder, progress: CancellationProgress): CancellationProgress = ??? def doTask5(salesOrder: SalesOrder, progress: CancellationProgress): CancellationProgress = ??? } Or, but now the logic of what needs to be done has simply shifted to method doTask3. Should CancellationProgress somehow have a "getNextTask" method?
Yeah, I've pretty much avoided the Twitter stack - no reasoning behind it, other than we tried to stay on the borrowed future for as long as possible after switching from a hand built actor architecture a long time ago at a company I used to work at. Of course I've also built things with dispatch back in the day, only to see it blown out of the water by spray. Http4s is the only one not built on apache or netty or jetty, which are all java first projects. If you have an error, it might be worth looking into fixing. 
Call me old-fashioned, but I'd rather have a maintained, well-curated standard library instead.
sttp for http client - after having continuous problems everytime I had to choose a http client this is a blessing. You can choose your response container type, synchronicity, its well documented and has a nice API. Its way above any other http client, mostly because its not one. Its just a wrapper, but very convenient one.
Exceptions are still good when there's genuinely an error in your program; e.g. some input your function can't handle. Most of the time you should restrict the input possibilities with good types, but sometimes it's impractical. An exception is amazing in those cases to potray the meaning: "your program has a (serious) fault! please review your code". And that's also where assertions come into play; to sanity check conditions before the error travels too far downstream.
&gt; Would the following be a better approach? You're still thinking in an OO way; you're copying `CancellationProgress` rather than mutating it in place but you're still using it like a mutable object with state. There's no reason for `p1`, `p2`, `p3` and `p4` to be the same type, and having them be the same type only makes mistakes possible. Better to define different types for all of them. &gt; Or, but now the logic of what needs to be done has simply shifted to method doTask3. Should CancellationProgress somehow have a "getNextTask" method? Yeah, that sounds more promising. Each intermediate state should know what the next step is, and be able to, well, do it.
I'd be interested if the same applies to scala native, or if it's not a problem there.
[removed]
Can they be easily serialized? This sounds like a pretty good format for something akin to Minecraft block / chunk storage. Rather then having a case class instance for each block, it gets packed as ints, via a palette map(per chunk, 4096, 16x16x16) which allows for infinite block variations theoretically.
to whoever wants an introduction on Play + Slick, I wrote an article sometime ago (play 2.4 I think): http://pedrorijo.com/blog/play-slick/ I've been thinking about updating the post and code example to play 2.6, but no time yet :)
Mongo is C++... https://github.com/mongodb/mongo/blob/master/docs/building.md
No, Lichess *uses* Scala, Play, and MongoDb.
I couldn't agree more. 
I have nothing to do with the library so I may be wrong, but from what I've seen the packing process is serialization. Case classes are (recursively) flattened to byte-arrays of their members, and the collections themselves are backed by byte buffers.
lihaoyi/sourcecode
Perhaps search through https://index.scala-lang.org/search?q=-library&amp;page=1&amp;sort=stars
mod, delete these spams/ads?
What would happend to Scala if some day Lightbend won't pass a funding round?
For Scala to die, it would have to lose its business funding/revenue, the academic grants of the EPFL group, the sponsors of the Scala Center, and the support of its business users such as Twitter or Verizon. 
If five companies inn the Des Moines Iowa area are using Scala, I highly doubt we need to worry about Scala dying.
As a new Scala developer, this makes me happy :)
I hope that with this money they will put more effort in Scale Native and provide a viable alternative to dinosaurs like C/C++.
Out of curiosity, which companies are those?
There are quite a few alternatives already, between rust, kotlin native, etc. I doubt Scala native will ever exceed rust in both performance and adoption. The project is cool but core development should probably be focused on the jvm
I wonder what they are raising money to do.
Deere, HyVee, JHA, DuPont/Pioneer, and Principal are places that are/were using Scala, Akka, and Spark or Flink for various products that have job openings available today! Just look on glassdoor. There are more every year. Also, shameless plug, the company I consult with, Source Allies, is always interested in hiring quality consultants, and we have about ten people that know and use Scala daily. 
JVM is so passÃ©...
Only if you don't work for enterprises
remote?
I love Scala, but Scala Native just seems kinda... awkward.
I think Scala actually makes more sense for a mature company since its strength is that it can hook into a large existing Java codebase (not typically used in startups) and a lot of 'big data' tooling is written for Scala drivers. A lot of the performance optimization and decoupling benefits of async programming also don't matter much for a small startup that is just struggling to get out an initial product.
It's about breathing the averages for a startup, add Scala helps to do that, and it was primarilly at those type of companied that it was popular here for a few years. I agree that the maintenance benefits of Scala help on long - lived projects, particularly.
Dm'd you.
If users abandones Scala it will die but there is no sign of that. 
This is from July.
Lol. Yep, they have startups. The one I worked at had craft beer on tap, red bull stocked, Wiis, Playstations, worked with cascading, hbase, Kafka, chef, mesos, attended lambdaconf/strangeloop, did some pure fp, lean, etc. I thoroughly enjoyed it. If you are interested in that scene, here's a link. http://startupdesmoines.com/#des-moines-ambassadors The city has also been growing at a very high rate the past few years. Something like 60%, I think. It's really affordable, and fun to live in. The midwest isn't NYC or the Valley, but it has its own charms and plenty of opportunities.
To be fair, I bet at least five companies in Des Moines or any other mid sized city are probably using Perl, too.
It runs core business critical systems at at least two of the companies listed. I know, I worked for them. They made the choice of Scala prior to me arriving. Teams of over 30 people working on Scala apps. It's not trivial usage. Knowing who HyVee has hired, theirs probably isn't trivial use, either. 
Why is that?
Now I know why I had a sense of de ja vu :-) 
I'd much like to see jawn in the standard library. If you're not familiar with it, it's a JSON parser with good performance and no external dependencies. It also supports asynchronous parsing which is a big win for streaming data
Sure why not! It's pretty close to what we have with `Either` or `Try` today, but a bit more light weight syntax wise. It would perhaps encourage the use of `throw` more. Not sure how I feel about that. Checked exceptions always sounded like a good idea but it got clunky for reasons I'm not going to explore in depth here. If return types were dynamically coloured by throw statements that'd be pretty cool, but I'm not sure how it would actually help in the end. The sole point of exceptions is that they bubble through the stack after all, and so you'd either have to enforce a check at the calling side like with checked exceptions, or let them bubble through implicitly in which case the end result would pretty much be what we're seeing today
I can see how streaming processing would be nice to have. Haven't used jawn, but I may take a look the next time I have a huge json file, thanks. 
Wasnt the name typesafe to lightbend a move away from Scala exclusive work? Scala is seriously lacking in bit hitter support. I think both rust and golang have a lot more power behind them. I would love scala to get more popular in the next 5 years but i cannot see it. My last last startup I chose Scala and hiring was a bear. Current startup i chose Python and it is worlds easier. Will see if I ever make it back to Scala....
I feel like that is less expressive than using a Try, other than being able to explicitly state the exception expected within the Try. I feel like for that reason I would be disinclined to use a union type in this way.
There's ScalaFX and scala swing. Maybe there's some methods in there? Also, remember that you can use any Java library from scala too. Also, since you're new, I highly recommend just getting IntelliJ (with Scala) plugin right away :P It's very nice to see syntax errors right away
What exactly is IntelliJ? Ive seen the name before, but is it like an IDE or something? Cause i just use notepad ++ right now
Agreed, and you lose the monadic properties of the Try, which is a big loss IMO. It makes programs much harder to reason about. 
Notepad++?? Omg, that's not that good. I'm gonna assume you're quite new to game (programming)..? IntelliJ is an IDE for programming. It's like you're Notepad++ but with compilation feedback directly inside the editor. There's probably plenty of good introduction videos online. If you don't want to jump straight to an IDE, i can also recommend VS Code. Very similar to Notepad, but nicer.
There is `java.awt.Robot` in the Java standard library ([Javadocs here](https://docs.oracle.com/javase/7/docs/api/java/awt/Robot.html)), which you can use from Scala, which provides mouse access and other automation functionality.
I think Odersky's talk "Plain functional programming" is all about this 
Or atom or sublime text. 
You want to automate clicking on other programs?
I think thats really the main problem with Scala. There arent enough people experienced with it. But thats a temporary problem that will improve over time. For now its still a great choice for large companies that use Spark and can afford to let people train on the job. I wouldn't want to use it for a startup quite yet though.
I'm not sure I agree... current team, when joined, couple of people, all extremely skilled scala guys... new people keep coming every month (we're expanding), all are ex java/C# kind ofppl (no proffesional scala background) code quality is still very high, some manage to get non trivial shapeless / monix PRs in in their first month. Sometimes PRs get tens comment but generally it's been a no problem to include non-scala devs in scala codebase, and they've been proceeding extremely well... I wonder if the whole "scala is super difficult" is not just excuse for "we had bad luck with hiring (hired weak performers)"
I just released version 1.0 of [Tierney](https://github.com/m50d/tierney) a generic hybrid free monad/applicative library. It lets you compose commands to be interpreted later, with explicitly serial and parallel composition - essentially it's a library for doing [this talk](https://speakerdeck.com/markus1189/free-monads-and-free-applicatives) with less manual work (e.g. no need to define `GitHubApplicative` "by hand").
Its not hired weak performers. I got 0 resumes with saa exp. Or functional.
Sure, our guys claimed the same (we are ex java-ee, I haven't done any scala etc...). But high standard in codebase, some initial thorough PRs and it's all good... at laest for now, I'll keep see later down the road :) (although honestly their PRs are pretty damn good so I don't think there will be any problems). Maybe we were just lucky. I guess it's really important that they are open minded.
Sure, those are just fine too. But as for *recommending* one, I'd say VS Code for basic code editing, and IntelliJ if you want something more.
&gt; Mongo -- reactivemongo is your only hope, and it's easy to learn and work with too, with great docs. It is difficult to mock, but running mongo in-memory with testcontainers is easy. Why not the actual Mongo driver for Mongo? e.g. http://rosslawley.co.uk/mongodb-scala-driver-released/
And if you want functional Finagle there's always Finch
Many times while reading code in scala I find the strange symbols in code. How do I tell Google to search what's meaning of it. Example :&gt; Should I say "colon greater than scala" as Google query ? 
This might not work with everything, but many times these functions have named aliases, so intellij ctrl + b. (I don't know what it is in other IDEs, but the functionalitiy is jump to definition) This even helps if it doesn't have an alias, because you can see the implementation and the documentation if exists. 
Go to [gitter](https://gitter.im/scala/scala) and ask. We're happy to help.
Iâ€™ve heard a few stories of scala developers migrating away from actors. Has anyone removed actors from their production codebase to replace it with something else? If so, why? And what was that something else?
Totally anecdotal, and Iâ€™m super noob, but I hang out in scala irc and Iâ€™ve seen discussion disparaging actors in favor for functional programming using something like fs2 and monix. Iâ€™ve bookmarked them for future study. Just trying to be more involved here, offering what I can and trying to learn more to offer more. 
Merry Christmas! 
This is large and in charge news
http://symbolhound.com/
- The problem with throws clauses is not necessarily writing them down, but the inability to compose and abstract over them. - I think `Int|SomeException` can't work as there is a difference between returning an exception instance and throwing it. Your signatures kooks like the former, but you want it to be the latter. - I don't think the distinction between checked and unchecked exceptions, especially the hierarchy as it exists in Java makes sense to emulate. - I think instead of handling it with a return type and making callers deal with it, it makes more sense to investigate a capability a based approach, in which throwing an exception requires some implicit evidence of the method being capable of throwing. So if you want to make sure no method you are calling throws an exception, don't provide that capability to your caller. - In the end, the practicality of any approach hinges on Java interop. If interop hinges on every existing Java library being rewritten or manually annotated, the pain will never be worth the benefit. See nullability as another example of this not working out.
awesome 
Nice! I wanted to migrate several of my libraries in typelevel ecosystem, but I was held back by the fact that all of them were "RC" or "M3" or "hash-version" and their API was still subject to change (until Cats releases). Now I can expect that stable versions will follow and I'll finally migrate :D
Is there any good documentation on this ?
Continue developing their reactive/data platform products, and the support/consulting services around them. I doubt much if any of the money was earmarked for increasing the company's direct investment in Scala.
https://typelevel.org/cats/typeclasses.html the official cats docs are a great place to start out
This might be very helpful : https://underscore.io/books/scala-with-cats/
Would like to hear more about this too. Currently planning out a rather large app using a lot of parallelism. 
You've bookmarked them for *Future* study? You mean Actor study right? Joking aside, can you share those bookmarks? 
You need to distinguish between throwing an exception and returning it, since exceptions can be used as values. But that's easy enough to fix: make a new type `ThrownException[T &lt;: Throwable]` (that *doesn't* extend `java.lang.Object` and so can't participate in Java generics), and represent that as `def foo(): Int | ThrownException[SomeException]`. It seems like a nicer representation to me (I've played with this idea on paper before). It would require the compiler to insert suitable `catch`es around all invocations of methods that throw checked exceptions, which probably destroys the ability to use the language in low-level mechanical-sympathy style performance-critical code. For the kind of use cases I have that's a more than worthwhile trade, but I'm not sure Dotty would accept it.
Where are the best places to find Scala jobs?
/u/jackcviers is [hiring](https://www.reddit.com/r/scala/comments/7lv67z/lightbend_the_startup_behind_the_scala/drphabw/)
watched the video now. no, he said "no more implicit conversions, use implicit classes". maybe you missed it because it was literally two seconds, no further explanation. which is bad, because while implicit classes are a better solution and cover the most important use cases of implicit conversions (extension methods, union types) it *does not* cover *all* use cases.
&gt; scale it by doing * 3 c + c + c
I was thinking of also having a â€œrethrowâ€ feature, wherein thrown checked exceptions of the specified type(s) are *not* caught, but are propagated through. This would give similar behavior to Java `throws`clauses. Example syntax: def f() = throw [SomeException|SomeOtherException] { // Some code that might throw those checked exceptions } Inside that `throw` block, whenever any expression has a type `T &lt;: U|SomeException|SomeOtherException`, the following happens: 1. The expression's type becomes `T = U`. 2. At run time, whenever the expression evaluates to one of the exception types, the exception is thrown.
Hi all I am trying to set environment variable for subprojects as following: scalacOptions += "-Ypartial-unification" scalacOptions += "-feature" addCompilerPlugin("org.scalamacros" % "paradise" % "2.1.0" cross CrossVersion.full) val Cats = "1.0.0" val Shiro = "1.4.0" val Logback = "1.2.3" val CatsEffect = "0.5" val Kafka = "1.0.0" val Bean = "1.9.3" val Circe = "0.9.0-M3" val Log4j = "1.7.25" val ScalaCheck = "1.13.4" val Scalactic = "3.0.4" val Scalatest = "3.0.4" val JavaJwt = "3.3.0" val Simulacrum = "0.11.0" val Http4s = "0.18.0-M7" lazy val commonSettings = Seq( organization := "io.khinkali", version := "0.1.0-SNAPSHOT", scalaVersion := "2.12.4", libraryDependencies ++= Seq( "org.slf4j" % "slf4j-simple" % Log4j, "ch.qos.logback" % "logback-core" % Logback, "org.apache.shiro" % "shiro-all" % Shiro, "org.typelevel" %% "cats-core" % Cats, "org.typelevel" %% "cats-effect" % CatsEffect, "org.apache.kafka" % "kafka-streams" % Kafka, "org.apache.kafka" % "kafka-clients" % Kafka, "commons-beanutils" % "commons-beanutils" % Bean, "io.circe" %% "circe-core" % Circe, "io.circe" %% "circe-generic" % Circe, "io.circe" %% "circe-parser" % Circe, "io.circe" %% "circe-literal" % Circe, "com.github.mpilquist" %% "simulacrum" % Simulacrum, "org.scalactic" %% "scalactic" % Scalactic, "org.scalatest" %% "scalatest" % Scalatest % "test", "org.scalacheck" %% "scalacheck" % ScalaCheck % "test", ), resolvers ++= Seq( "Sonatype OSS Snapshots" at "https://oss.sonatype.org/content/repositories/snapshots" ), fork in run := true, envVars in Test := Map("KAFKA_SERVER" -&gt; "localhost:9092") ) lazy val root = (project in file(".")) .settings(commonSettings) .settings( name := "bary", organization := "io.khinkali", moduleName := "bary" ). aggregate( kafka_api, auth_stream, rest) lazy val kafka_api = (project in file("kafka-api")). settings(commonSettings). settings( name := "kafka-api", moduleName := "kafka-api" ) lazy val auth_stream = (project in file("auth-stream")). settings(commonSettings). settings( name := "auth-stream", moduleName := "auth-stream", libraryDependencies ++= Seq( "com.auth0" % "java-jwt" % JavaJwt, ) ).dependsOn(kafka_api) lazy val rest = (project in file("rest")). settings(commonSettings). settings( name := "rest", moduleName := "rest", libraryDependencies ++= Seq( "org.http4s" %% "http4s-dsl" % Http4s, "org.http4s" %% "http4s-blaze-server" % Http4s, "org.http4s" %% "http4s-blaze-client" % Http4s, "org.http4s" %% "http4s-circe" % Http4s, ) ).dependsOn(kafka_api, auth_stream) Then start the test: [IJ]sbt:bary&gt; test [info] AuthServiceSpec: [info] io.khinkali.rest.auth.AuthServiceSpec *** ABORTED *** [info] java.util.NoSuchElementException: None.get [info] at scala.None$.get(Option.scala:349) [info] at scala.None$.get(Option.scala:347) [info] at io.khinkali.rest.auth.AuthServiceSpec.&lt;init&gt;(AuthServiceSpec.scala:22) [info] KkConsumerSpec: [info] A Consumer [info] - should get all messages send by producer *** FAILED *** [info] java.lang.Exception: System variable KAFKA_SERVER is not set. [info] at io.khinkali.consumerspec.KkConsumerSpec.$anonfun$new$1(KkConsumerSpec.scala:14) As you can see, the error says, that the System variable KAFKA_SERVER is not set. What am I doing wrong? Thanks
Yes so the â€œbest paying jobs in scalaâ€ are in big data or machine learning in general. Iâ€™m an engineer doing â€œtraditionalâ€ back-end development on scala micro-services and I donâ€™t get a significantly different salary than the java engineers in my company. Itâ€™s really about the value you bring rather than the tools you use.
&gt; As for distinguishing between throwing an exception and returning it as a value, does the distinction really matter? Either way, the caller will receive the exception as a return value. I would bet that somewhere out there in an internal corporate Java codebase is a method like public SomeException something() throws SomeException where the distinction between whether it's thrown or returned is critical to correct functioning of the application. It's not good style, but it's part of the language and if we're to offer 100% Java interop then we should support it. &gt; There is the matter of interop with Java/JavaScript/whatnot, which still do distinguish between throwing and returning. Perhaps there should be a type annotation that tells the compiler that a given exception type should be returned rather than thrown. I'd like the type that represents this to be as ordinary and first-class as possible, hence my `ThrownException` concept. Obviously that has serious performance concerns if you want to be able to e.g. form a `List[ThrownException[...]]`, but there's an excluded middle here: any kind of first-class representation of thrown exceptions has to be usable in this way that is inherently inefficient on the JVM. I'd prefer a consistent language whose implementation is sometimes inefficient to an inconsistent language that's always efficient (particularly now that Kotlin exists to fill the latter niche). With either approach there's the question of what the ABI looks like. I'd like`def foo(): A | ThrownException[B]` to always have the ABI of `public A foo() throws B`, but I'm not sure how efficiently idiomatic use of `A | ThrownException[B]` could be compiled into this form, and it moves Dotty further away from JVM mechanical sympathy style. Your `throw` concept probably retains more of that - IMO it's not worth the level of language compromise that such a construct represents, but that's a subjective judgement.
this article is wrong, because a tuple (String, Int) could be encoded as (MyGoodString, ProductId). So the whole article is likely a poor bias.
value classes are mentioned in the article. or are you talking about something else? I think overall the message is correct, especially if you've worked in code that overused tuples and found that it can take hours to change code that should take minutes. I've been bitten by this in the past as well.
I'd agree that would be bad. I don't see that on the milestones discussions, and the implicit conversions Martin has spoken of are any that are not declared in one of the types being converted. Frankly, I don't think the language would work well without implicit conversions. 1.toJava and "string".toRichString everywhere sounds yucky. 
&gt; "string".toRichString such use case is covered by implicit classes. many implicit conversions in the standard library where already replaced with implicit classes: https://github.com/scala/scala/blob/v2.12.4/src/library/scala/Predef.scala#L304 but for some reason not strings? https://github.com/scala/scala/blob/v2.12.4/src/library/scala/Predef.scala#L370 arrays look tricky: https://github.com/scala/scala/blob/v2.12.4/src/library/scala/Predef.scala#L417 can we define multiple implict classes for arrays with different type parameteres? i'm not sure, never tried it before. will try it after my coffee. these are the implicit conversion use case that implicit classes do not cover: https://github.com/scala/scala/blob/v2.12.4/src/library/scala/Predef.scala#L444 i too do some creative stuff with implicit conversions that could not be done with implicit classes
https://github.com/KyleU/scala-js-typescript
he says it at 37:25, which is funny because implicit classes are implemented with implicit conversion. he is talking non-sense.
Hmm, okay. Note that `ThrownException` could (and probably should) be a value type that only exists at compile time. So, the ABI of `List[ThrownException[T]]` should be identical to `List[T]`. I suggest the name be simply `Throw`, though.
I've used Akka a few years ago, but only on the fringes of the system. The internals were functionally pure. However, I have not used actors since, I have not experienced a problem that needs them. Instead, I am using http4s which is just *Request =&gt; Task[Response]*, or if not http4s, a monadic stack evolving Task. I am interested in the [Cluster Singleton](https://doc.akka.io/docs/akka/current/cluster-singleton.html) as that problem has occurred a few times of course.
Looks like a compiler bug. `type R[V] = Unbox[V]` shouldn't compile, in fact it already doesn't when it's defined by itself, but apparently does when it's part of a refined type like your example. I suggest filing an issue at https://github.com/scala/bug/issues/.
&gt; a tuple (String, Int) could be encoded as (MyGoodString, ProductId) `(String, Int)` &lt; `(MyGoodString, ProductId)` &lt; `MyGoodCaseClass(MyGoodString, ProductId)` while `(MyGoodString, ProductId)` does give you semantic information about `MyGoodString` and `ProductId`, it says nothing about what the pair itself represents. do all pairs of `(MyGoodString, ProductId)` in the world unequivocally and unambiguously always represent the same type of entity?
Yep, the string, java interop, and collection ones are the things I was thinking about. AFAIK, you could certainly make multiple array ops implicits using type evidence: object TypeRestrictions { type IsBool[A] = =:=[A,Boolean] type IsInt[A] = =:=[A,Int] implicit class OpsBool[T: IsBool](x: Array[T]){ def hello = "yes" } implicit class OpsInt[T: IsInt](x: Array[T]){ def hello = "nope" } } import TypeRestrictions._ Array(true, false, true).hello Array(2,4,5).hello Array("test", "one", "two").hello // value hello is not a member of Array[String] 
&gt; Creating a Map. Scala has a very easy syntax for creating Maps from tuples: &gt; scala&gt; Seq(("key1", 1), ("key2" , 10), ("key3" , 100)).toMap &gt; res0: scala.collection.immutable.Map[String,Int] = Map(key1 -&gt; 1, key2 -&gt; 10, key3 -&gt; 100) what? `Seq.apply.toMap`? really? what's wrong with good old `Map.apply`? scala&gt; Map(("key1", 1), ("key2" , 10), ("key3" , 100)) res0: scala.collection.immutable.Map[String,Int] = Map(key1 -&gt; 1, key2 -&gt; 10, key3 -&gt; 100) does the author nor realize that `"key1" -&gt; 1` is just syntax sugar for `("key1", 1)`: scala&gt; "key1" -&gt; 1 res1: (String, Int) = (key1,1)
It can be even simpler and straightforward: implicit class OpsBool(x: Array[Boolean]){ def hello = "yes" } implicit class OpsInt(x: Array[Int]){ def hello = "no" }
We used casbah, the previous standard mongo driver, and we have some collections with a large amount of documents in them. casbah would OOM on those collections, even for small subsets of the documents. ReactiveMongo has had 0 issues streaming documents from those collections. I prefer pull-based streaming solutions because they don't require management strategies when consumers are slower than producers. That really comes in handy since our documents contain compressed data that needs to be joined across document bounds to be decompressed. I'd prefer it if rxmongo returned fs2 streams, but iteratees are fine, and easily convertable. The observable API is powerful enough, but is also pretty much a raw wrapper around the java streaming interface in the official mongo docs. And it is hard to implement push based streaming solutions on top of the raw api. Monix/Akka streams are potential solutions here, but they are still push-based, which I prefer because it is harder to implement a good push-based solution (https://medium.com/@olehdokuka/mastering-own-reactive-streams-implementation-part-1-publisher-e8eaf928a78c).
number crunching, seriously?
Often there's no more meaningful name for "MyGoodCaseClass" (after all naming is one of the difficult problems in CS), so if you insist on naming everything you end up with ridiculous names like MyGoodStringAndProductId which is hardly more informative than (MyGoodString, ProductId) and which resists usage with generic "zip"-like functions, etc. etc. &gt; My problem with tuples is that they are not fully type safe. You must be working with some non-typical definition of type safe. Can you explain?
&gt; naming is one of the difficult problems in CS I agree naming is one of the difficult problems in CS. That does not mean, however, we should take shortcuts and abdicate from good practices. &gt; resists usage with generic "zip"-like functions, etc. etc. `Companion.unapply` converts your case class back to tuple if so you need. &gt; You must be working with some non-typical definition of type safe. Can you explain? Tuples are order dependent, yet there is nothing preventing you for mixing order up. You know tuples are not only pairs, right? It goes beyond `Tuple2`, otherwise they'd be called `Duple`. Tuples do not scale (readability wise) beyond two or three elements. Good luck getting all your members in the right order with `Tuple22`. Sure, they are a great solution to implement something internally like `unapply`, but you should not be modelling your data structures on them.
More: tuples do not nest. I mean, yes they do. But because you can does not mean you should. If you start modelling anything on my code base with nested tuples you are serious candidate to be recommended for dismissal. I can have a case class `Person` with a case class `Address`, etc. If you try that with tuples, things get nasty pretty quickly.
Alternative interpretation: all the entry-level Scala developers are unemployed and don't turn up in the statistics, whereas there is great demand for entry-level PHP/JS/C# developers who can always find a job (Not saying that this is the case, just showing how you can really see what you want to see from this statistic)
&gt; Note that ThrownException could (and probably should) be a value type that only exists at compile time. So, the ABI of List[ThrownException[T]] should be identical to List[T]. It can't be if `Throw` is to behave as a plain old value that respects referential transparency. I'd like identities like `list.reversed.map(f) == list.map(f).reversed` to hold even when `f` throws a checked exception - I think that's worth the performance cost, personally.
Yeah Big Data is what's generating those, and my own, salary. We utilize Scala but oddly enough we have been reverting to Python and R for a lot of stuff we envisioned Scala being best at.
I generally think that tuples should not leak out of a function's scope, but they are very useful for pattern matching logic -- bundling up variables to match on multiple things at once and packing up multiple outputs for assignment. Sometimes, it's okay for them to leak out, though. I ask myself, is it more obvious to have a bundle of data simply represented by its components, or for the bundle itself to have a name? Normally, the latter wins out, but not *always*. 
"If you have a case class, you can add a new field, and the code will compile everywhere; now youâ€™d just need to use the new field everywhere you want the extra info, and the remaining code can be left exactly the same." Playing devil's advocate, one could argue that having the code compile after such a change is not a good thing since code using the changed case class might need to account for it. Bit of a stretch though.
As someone in this space who came from Python and R can you elaborate please?
Thanks alot
Thanks :-)
the worst thing on any code base is actually what you describe. creating a case class, just because something **might** need more fields in the future. what if it doesn't? you basically get nothing out of it. actually worse, having a case class means you need to actually maintain more code. also most often people are lazy when using case class which means that most case classes will just have an id: Int field, which is less typesafe than just using a tuple + value types. using a case class might be ok, but just using a case class for your two value return type is basically a no no!
I'm not /u/OIPROCS but both of those languages are more familiar to data scientists. This means there's significantly lower onboarding cost than Scala (which many folks never end up learning because why would they when Python is sufficient?), and Python has enormous momentum for deep learning.
&gt; Tuples are order dependent, yet there is nothing preventing you for mixing order up. Other than the types, you mean? I'm sure you're arguing about *something*, but it's not type safety.
Hey guys, how about Junior jobs or internships in Scala? Or only who have experience can get a job?
&gt; you'd have to have a way to force the user to supply named parameters. &gt; don't forget that you're using the 1-tuple all over your code right now... so should you name every instance of e.g. String in your code? yes, it's called value classes and it is the premise of the first comment on this thread, if you had read that properly. so, before you come here to disagree with someone, first try to understand the context of what they are saying. named parameters are a non-issue if you use value classes. 
this an example with scalaz, with cats shouldn't be too different import scalaz._, Scalaz._ @{ sealed trait Returned[A] object Returned{ case class Value[A](v:A) extends Returned[A] case class Error1[A]() extends Returned[A] case class Error2[A]() extends Returned[A] } } defined trait Returned defined object Returned @implicit object monad extends Monad[Returned] { def point[A](a: =&gt; A): Returned[A] = Returned.Value(a) def bind[A, B](fa: Returned[A])(f: A =&gt; Returned[B]): Returned[B] = fa match { case Returned.Value(v) =&gt; f(v) case Returned.Error1() =&gt; Returned.Error1() case Returned.Error2() =&gt; Returned.Error2() } } defined object monad @def returnSomething(a:Int):Returned[String] = a match{ case 1 =&gt; Returned.Value("oh yeah") case 2 =&gt; Returned.Error1() case _ =&gt; Returned.Error2() } defined function returnSomething @for { v &lt;- returnSomething(1) } yield v res14: Returned[String] = Value("oh yeah") @for { v &lt;- returnSomething(2) } yield v res15: Returned[String] = Error1() @for { v &lt;- returnSomething(22222) } yield v res16: Returned[String] = Error2()
this an example using scalaz instead of cats, but it shouldn't be too different import scalaz._, Scalaz._ @{ sealed trait Returned[A] object Returned{ case class Value[A](v:A) extends Returned[A] case class Error1[A]() extends Returned[A] case class Error2[A]() extends Returned[A] } } defined trait Returned defined object Returned @implicit object monad extends Monad[Returned] { def point[A](a: =&gt; A): Returned[A] = Returned.Value(a) def bind[A, B](fa: Returned[A])(f: A =&gt; Returned[B]): Returned[B] = fa match { case Returned.Value(v) =&gt; f(v) case Returned.Error1() =&gt; Returned.Error1() case Returned.Error2() =&gt; Returned.Error2() } } defined object monad @def returnSomething(a:Int):Returned[String] = a match{ case 1 =&gt; Returned.Value("oh yeah") case 2 =&gt; Returned.Error1() case _ =&gt; Returned.Error2() } defined function returnSomething @for { v &lt;- returnSomething(1) } yield v res14: Returned[String] = Value("oh yeah") @for { v &lt;- returnSomething(2) } yield v res15: Returned[String] = Error1() @for { v &lt;- returnSomething(22222) } yield v res16: Returned[String] = Error2() 
1. Scala provides some basic building blocks for functional programming but it doesn't come with the functional abstractions required by pure functional programming. 2. In short, to enable pure functional programming which has several benefits: e.g. simplify composition, isolate side effects, reduce boilerplate, managing mutable states without mutable variables, etc. This is rather a big topic. I wrote a blog post https://tech.iheart.com/why-fp-its-the-composition-f585d17b01d3 mainly focused on the composition aspect of it. Rob Norris gave a somewhat overlapping but more broad talk on FP with effects: https://www.youtube.com/watch?v=po3wmq4S15A. If you get interested in this topic, you can find more comprehensive resources on Cats' website here https://typelevel.org/cats/resources_for_learners.html 3. Cats use powerful tool called type class, you can find more about it, again on Cats website. https://typelevel.org/cats/typeclasses.html. For a custom monadic type `CMT[_]`, you would need to implement an implementation of the `Monad` trait `Monad[CMT]` for it and make it implicitly available (e.g. an implicit val in the companion object of `CMT` 4. Using cats in your library does not automatically require your downstream users to use cats as well. However, you can enhance your API by providing Cats type class instances to your users, with which your users will be able to take advantage of all syntax enhancement provided by Cats. E.g. if you provide an implicit instance of `Monad` for your `CMT` type, your user will automatically have methods like `map`, `flatten`, `product` available on any instances of `CMT` if they `import cats.implicits._` For examples of a working project using Cats you can find some here. https://github.com/typelevel/cats#general-purpose-libraries-to-support-pure-functional-programming, there are quite a few of them whose domain is simple such as hammock, scanamo. Also feel free to ask any questions in Cats' gitter channel. https://gitter.im/typelevel/cats. It's a lot more responsive than here. 
This is a pretty good resource for learning Cats: https://underscore.io/books/scala-with-cats/ It has examples of how to use many of the abstractions that Cats provides
first disclaimer: I would never put the code I'll show you in production, I consider passing closures this way an anti-pattern, but I guess it helps to show how you can remove futures from your code, while still using them def shipOrders[F[_]:Applicative:Monad]( ids: List[String], shipmentMethod: Shipment =&gt; Option[Shipment], getShipment:String =&gt; F[Shipment], updateShipment:Shipment =&gt; F[Shipment] ): F[List[Shipment]] = { // not all ids are valid, get the list shipments that exists val futureShipments:F[List[Shipment]] = ids.traverse(id =&gt; getShipment(id)) // not all shipments are processable by shipmentMethod, return list of processed shipments val processedShipments:F[List[Shipment]] = futureShipments.map(list =&gt; list.flatMap(s =&gt; shipmentMethod(s))) // save updated shipments back to database val updatedShipments: F[List[Shipment]] = processedShipments.flatMap(list =&gt; list.traverse(s =&gt; updateShipment(s))) updatedShipments } shipOrders[Option]( ids = List("a"), shipmentMethod = Option(_), getShipment = id =&gt; Option(Shipment(id)), updateShipment = s =&gt; Option(Shipment(s.id + "_updated")) ) import scala.concurrent.{Future, ExecutionContext} // you'll need the Applicative instance for Future, I haven't found it on scalaz o_O implicit def FutureApplicative(implicit executor: ExecutionContext) = new Applicative[Future] { def point[A](a: =&gt; A) = Future(a) def ap[A,B](fa: =&gt; Future[A])(f: =&gt; Future[A =&gt; B]) = (f zip fa) map { case (f1, a1) =&gt; f1(a1) } } import scala.concurrent.ExecutionContext.Implicits.global shipOrders[Future]( ids = List("a"), shipmentMethod = Option(_), getShipment = id =&gt; Future(Shipment(id)), updateShipment = s =&gt; Future(Shipment(s.id + "_updated")) ) 
I have some observations that could help you: - Passing `Option[T]` as a method argument is usually a bad practice, passing just the value usually simplifies the code. - As I understand, `DB` would be a scala object and it is where most side effects are occurring, you could abstract a base trait and create the implementation that actually uses the database, with this approach you could write a custom in-memory implementation that doesn't have side effects, here your method (or the class constructor) would receive the base trait and you can exchange between the real database and the in-memory implementation. - Your method is doing quite a lot of things, you might benefit by breaking it into several specific methods.
Yes, you absolutely can just implement a flatMap method yourself (you'll also need to implement map, and possibly filter). I would recommend doing that first. If you need more details on how for yield syntax translates to calls to map/flatMap, see https://www.artima.com/pins1ed/for-expressions-revisited.html Once you've done that, the underscore.io book "scala with cats" posted by lu4nm3 is a good resource, but you don't need to start there.
To make testing easier you can pass the database as an argument rather than using a global `DB` reference, and abstract over implementations using a trait. This will allow you to pass an in-memory database (for instance) for testing. As for removing side-effects, you will need to use something like cats `IO` rather than `Future`, which can't do what you need.
You don't necessarily need Cats to achieve this particular effect. In order to use your class in a for comprehension it just needs map and flatmap methods. For comprehension is just syntactic sugar. 
You don't need cats for this, just use `Either`: case class ReturnedValue(value: Int) sealed abstract class MyErrorType case class ErrorTypeA() extends MyErrorType case class ErrorTypeB() extends MyErrorType def callMyFunction(x: Int, y: Int, z: Int): Either[MyErrorType, ReturnedValue] = if (x &lt; y) Left(ErrorTypeA()) else if (x &lt; z) Left(ErrorTypeB()) else Right(ReturnedValue(x)) callMyFunction(7, 2, 3).fold({ case ErrorTypeA() =&gt; println("Oops, something went wrong") case ErrorTypeB() =&gt; println("Something else went wrong") }, ret =&gt; println(s"Got back ${ret.value}") ) 
So [/u/kailuowangalready](https://www.reddit.com/user/kailuowang) already gave you some good advice and pointed you in the right direction if you want to use Cats (I recommend it). Anyway, you need to know that you can implement your own Monad without bringing any dependency into scope. For example: object MonadDemo { // Simple definition of the Monad typeclass trait Monad[F[_]] { def pure[A](a: A): F[A] // Normally defined in Applicative def map[A, B](fa: F[A])(f: A =&gt; B): F[B] = // Normally defined in Functor flatMap(fa)(a =&gt; pure(f(a))) def flatMap[A, B](fa: F[A])(f: A =&gt; F[B]): F[B] } // Companion object so you can do Monad[Algebra].flatMap(..) for instance object Monad { def apply[F[_]]: Monad[F] = implicitly[Monad[F]] } // Rename your algebra as you wish sealed trait Algebra[+A] object Algebra { final case class Value[A](a: A) extends Algebra[A] case object ErrorA extends Algebra[Nothing] case object ErrorB extends Algebra[Nothing] // A Monad instance of your Algebra implicit def algebraMonadInstance: Monad[Algebra] = new Monad[Algebra] { override def pure[A](a: A) = Value(a) override def flatMap[A, B](fa: Algebra[A])(f: A =&gt; Algebra[B]): Algebra[B] = fa match { case Value(a) =&gt; f(a) case e @ ErrorA =&gt; e case e @ ErrorB =&gt; e } } // Creating syntax so you can use invoke x.flatMap and use it in for-comprehentions implicit class AlgebraOps[A](fa: Algebra[A])(implicit M: Monad[Algebra]) { def pure(a: A): Algebra[A] = M.pure(a) def map[B](f: A =&gt; B): Algebra[B] = M.map(fa)(f) def flatMap[B](f: A =&gt; Algebra[B]): Algebra[B] = M.flatMap(fa)(f) } } def myFunction(x: Int): Algebra[Int] = { if (x == 0) ErrorA else if (x &lt; 0) ErrorB else Value(x) } // For-comprehention val r1: Algebra[Int] = for { v &lt;- myFunction(5) } yield { // Not recommended to perform side effects in a pure function transformation println(s"Got value: $v") v } // Using flatMap syntax val r2: Algebra[Int] = myFunction(0).flatMap(_ =&gt; ErrorB) // Using the Monad instance implicitly val r3: Algebra[Int] = Monad[Algebra].map(myFunction(4))(_ * 2) } Cheers, Gabriel.
&gt; I consider passing closures this way an anti-pattern, To be sure -- this is because there are better ways to wire in your behavior. Take a look at [FP for Mortals, application design](https://leanpub.com/fpmortals/read#leanpub-auto-application-design). That should help you get started. 
thanks! that one was totally off my radar is there any synergy between the projects or are they totally independent operations? are these on any of the various relevant awesome lists or the scalajs site material anywhere? i might've been just using the wrong keywords (hindsight being 20/20 i clearly was ðŸ˜…) but took a bit of searching and moderately long click trail to get here
Hello, let me give your questions a shot: * 1. Scala is already functional, but a lot of the machinery for doing *pure* functional programming is missing in the standard library. Plenty of people do heavy FP without Scalaz or Cats, but eventually you may find yourself reinventing some of the things those libraries provide. * 2. Scalaz or Cats are libraries, think of them like a swiss army knife. You may only need one or two core data structures, or you may want to use tons of them, it really depends on the problem. My gateway drug into using Scalaz was needing to easily work with Future[Option[String]] types, without having to nest maps. Those are called Monad Transformers, and make life easier. * 3. You don't extend Monad, you provide a type class instance. [TypeClasses](http://michele.sciabarra.com/2015/11/11/scala/Scalaz-and-Typeclasses/) * 4. Eh, sort of. Cats/Scalaz will be on the classpath, so it may cause issues if another library uses a different version of Cats. But that is the case with any scala library. However it's up to you if you want to return a Scalaz type class or just always go to a standard library class. Anyway, I recommend checking out Scalaz, and here's a good book for getting started [Functional Programming for Mortals](https://leanpub.com/fpmortals)
The author of definitelyscala.com only recently opened up his compiler, so it wasn't available before that. His compiler, IIUC, is an improved fork of @sjrd's. 
Your side effects here are basically coming from interacting with the DB, right? /u/tpolecat 's Doobie and Lightbend's Slick both provide database effect types (`ConnectionIO` and `DBIO`) that describe database effects in a referentially transparent way. They will wrap your database calls and execute lazily when you ask for the result, rather than eagerly running Futures like you have now. Good for testing because you can build up pure descriptions of database actions in the code and then run them with transaction rollback during integration testing.
Might be that you're trying to grab a system variable rather than an env? You should be able to pass system variables to forked JVMs using `javaOptions in test := "-Dmy.variable=something"`
Thanks, these are great resources and I think I understand a lot better now. I have a lot of reading to do but I am definitely seeing the benefit of a library if you are using the monad pattern all over the place. We've already run into a bunch of cases where we're using Function[Option[T]] so I can imagine that may help clean some things up.
I don't understand this reasoning. Akka actors solve very specific problems that streams don't, which is managing shared, and potentially distributed, mutable state. There is very little overlap if any. If people are leaving actors for streams it's probably because they realise they've misused them.
Again I state, I donâ€™t know wtf Iâ€™m taking about... in golang thereâ€™s a way of getting services to talk to each other in a failure resistant way using raft and serf. Is there something like this for scala that isnâ€™t Akka cluster? 
Doh, link is dead at the moment. Hope it comes back up, or is there an alternative link?
Lightbend has a product called ConductR that seems to be pretty similar to serf. I haven't used either so I can't say for sure if this is the type of framework you're looking for, and I'm pretty sure its built on top of Akka :)
I'm actually using Slick, for example, DB.getShipment eventually calls this method in the persistence layer: def get(id: String): Future[Option[Shipment]] = { val sql = sql""" SELECT blah FROM shipment WHERE id = ${id} """.as[Shipment](getShipmentResult) db.run(sql).map(_.headOption) } What do you mean by describing it in referentially transparent way, is there a better way to do this than returning a Future[Option[T]]? 
https://github.com/fommil/fpmortals/blob/master/manuscript/book.org#application-design
Thank you
Yes, return a `DBIO[Option[Shipment]]`: def get(id: String): DBIO[Option[Shipment]] = sqlâ€...â€.as[Shipment](getShipmentResult).map(_.headOption) `DBIO` btw is a simplified alias for http://slick.lightbend.com/doc/3.2.0/api/index.html#slick.dbio.DBIOAction The above method is referentially transparent because itâ€™s a pure description of the query, and doesnâ€™t actually run anything. The idea is that you would have these for all operations and compose them as needed to build a pure storage layer. Above this layer you would have an impure layer which just runs all the actions using `db.run`. The pure layer is nice and testable because you can wrap all its action descriptions inside a rollback action and run them in integration tests. Or if youâ€™re feeling adventurous, build a `DBIO` interpreter that can run your actions like `db.run`, but always evaluates to whatever values you feed it. This would be like faking, but in a principled way.
ok awesome, I understand the first paragraph. Can you explain/show how to test the nice and testable layer? How do I rollback the action, does this mean we can call these methods without a database?
The key to rollbacks is returning `DBIO.failed` from inside the action which rolls back: http://slick.lightbend.com/doc/3.2.1/dbio.html#rollbacks It would look something like this (not verified): class Rollback[A] private (a: A) extends Throwable object Rollback { def apply[A](dbio: DBIO[A]): DBIO[A] = dbio .flatMap(a =&gt; DBIO.failed(new Rollback(a))) .transactionally .failed .map(_.a) } The above creates a description 'run the wrapped action, capture the resulting value, rollback the wrapped action, and finally return the captured result value'. This is all without actually running anything. You would use it in integration tests that hit the DB, by wrapping the test action inside a `Rollback(action)` to ensure tests don't make persistent changes to your DB. If you want to run these as unit tests without hitting the DB, that's a bit more ambitious and I haven't actually done that myself. But it should be possible to write an implementation that works like `sql.run(...)` but actually returns whatever value you feed it, without actually running the database action.
OK, I see. This would give me a few things to play with. Having all the "persistence" layer code not actually running is something new to me.
Hey, I know you have done some research already, but I am surprised you picked Scalatra. Have you seen [http4s](http://http4s.org/), and [akka http](https://github.com/akka/akka-http) ? The first could be too hard, but the second should be easy enough for all. If Scalatra is your final choice, just mimic other micro framework logic. Apply your knowledge of [Sinatra](http://sinatrarb.com/), and you should be fine. If you really need example projects, you can broaden your search with [finagle](https://github.com/twitter/finagle), [flask](flask.pocoo.org) (python), or even [http4s](http://http4s.org/). They will all share very similar ideas. Don't worry about dependencies on quill, and co. SBT will manage that for you. Lastly, your frontend should be its own service. Just build good APIs, and let the frontenders do their magic. Sorry for the lack of clear examples, hope it helps anyways. 
I agree, OP should look into http4s and akka-http, maybe also Finch. At work I have to work with a legacy project using both Scalatra and Fintrospect (built on top of Finagle), and they both feel unidiomatic and "Java-like" - not in a good way. They use exceptions pervasively etc. Personally I like http4s, but I think akka-http might be easier for some people to get started with. Their documentation is better, I think. 
guys check this out, he did it, a tree. HIRED!
Score! Just a few days late, though, on the xmas tree.;)
&gt; guys check this out, he did it, a tree. &gt; HIRED! no need to be stupid about it. didn't like it, ignore it.
Scalatra, 5 years ago, was my choice for a ScalaFx app that embedded a Scalatra rest and web app in a single jar. I had used Jetty many times previously, with great success, and it all worked brilliantly. Today, though, it's not clear to me that Scalatra can compete with Play, Akka-Http and others in terms of a reactive concurrency model, implying pool threads, smart dispatchers, async handling and intelligent IO blocking. Hopefully, others can chime in with a few worthy sea stories, not just fanboy 'use this' recommendations. Naturally, it goes without saying, do your own testing and come to your own conclusions. Cheers!
This is what I use to prevent queries from going to the db: &gt; object MockDbBackend extends JdbcBackend &gt; &gt; class MockJdbcDataSource extends JdbcDataSource { &gt; override def createConnection(): Connection = new MockConnection &gt; override def close(): Unit = { } &gt; override val maxConnections: Option[Int] = Some(1) &gt; } &gt; &gt; class MockConnection extends Connection { &gt; private def REQUIRES_DB: Nothing = throw new RuntimeException( &gt; "You ran a DBIOAction that requires a real database. This doesn't work in a unit test" &gt; ) &gt; &gt; override def commit(): Unit = { } &gt; override def getHoldability: Int = REQUIRES_DB &gt; override def setCatalog(catalog: String): Unit = REQUIRES_DB &gt; override def setHoldability(holdability: Int): Unit = REQUIRES_DB &gt; override def prepareStatement(sql: String): PreparedStatement = REQUIRES_DB &gt; override def prepareStatement(sql: String, resultSetType: Int, resultSetConcurrency: Int): PreparedStatement = REQUIRES_DB &gt; override def prepareStatement(sql: String, resultSetType: Int, resultSetConcurrency: Int, resultSetHoldability: Int): PreparedStatement = REQUIRES_DB &gt; override def prepareStatement(sql: String, autoGeneratedKeys: Int): PreparedStatement = REQUIRES_DB &gt; override def prepareStatement(sql: String, columnIndexes: Array[Int]): PreparedStatement = REQUIRES_DB &gt; override def prepareStatement(sql: String, columnNames: Array[String]): PreparedStatement = REQUIRES_DB &gt; override def createClob(): Clob = REQUIRES_DB &gt; override def setSchema(schema: String): Unit = REQUIRES_DB &gt; override def setClientInfo(name: String, value: String): Unit = REQUIRES_DB &gt; override def setClientInfo(properties: Properties): Unit = REQUIRES_DB &gt; override def createSQLXML(): SQLXML = REQUIRES_DB &gt; override def getCatalog: String = REQUIRES_DB &gt; override def createBlob(): Blob = REQUIRES_DB &gt; override def createStatement(): Statement = REQUIRES_DB &gt; override def createStatement(resultSetType: Int, resultSetConcurrency: Int): Statement = REQUIRES_DB &gt; override def createStatement(resultSetType: Int, resultSetConcurrency: Int, resultSetHoldability: Int): Statement = REQUIRES_DB &gt; override def abort(executor: Executor): Unit = REQUIRES_DB &gt; override def setAutoCommit(autoCommit: Boolean): Unit = { } &gt; override def getMetaData: DatabaseMetaData = REQUIRES_DB &gt; override def setReadOnly(readOnly: Boolean): Unit = REQUIRES_DB &gt; override def prepareCall(sql: String): CallableStatement = REQUIRES_DB &gt; override def prepareCall(sql: String, resultSetType: Int, resultSetConcurrency: Int): CallableStatement = REQUIRES_DB &gt; override def prepareCall(sql: String, resultSetType: Int, resultSetConcurrency: Int, resultSetHoldability: Int): CallableStatement = REQUIRES_DB &gt; override def setTransactionIsolation(level: Int): Unit = REQUIRES_DB &gt; override def getWarnings: SQLWarning = REQUIRES_DB &gt; override def releaseSavepoint(savepoint: Savepoint): Unit = REQUIRES_DB &gt; override def nativeSQL(sql: String): String = REQUIRES_DB &gt; override def isReadOnly: Boolean = REQUIRES_DB &gt; override def createArrayOf(typeName: String, elements: Array[AnyRef]): sql.Array = REQUIRES_DB &gt; override def setSavepoint(): Savepoint = REQUIRES_DB &gt; override def setSavepoint(name: String): Savepoint = REQUIRES_DB &gt; override def close(): Unit = { } &gt; override def createNClob(): NClob = REQUIRES_DB &gt; override def rollback(): Unit = REQUIRES_DB &gt; override def rollback(savepoint: Savepoint): Unit = REQUIRES_DB &gt; override def setNetworkTimeout(executor: Executor, milliseconds: Int): Unit = REQUIRES_DB &gt; override def setTypeMap(map: util.Map[String, Class[_]]): Unit = REQUIRES_DB &gt; override def isValid(timeout: Int): Boolean = REQUIRES_DB &gt; override def getAutoCommit: Boolean = REQUIRES_DB &gt; override def clearWarnings(): Unit = REQUIRES_DB &gt; override def getSchema: String = REQUIRES_DB &gt; override def getNetworkTimeout: Int = REQUIRES_DB &gt; override def isClosed: Boolean = REQUIRES_DB &gt; override def getTransactionIsolation: Int = REQUIRES_DB &gt; override def createStruct(typeName: String, attributes: Array[AnyRef]): Struct = REQUIRES_DB &gt; override def getClientInfo(name: String): String = REQUIRES_DB &gt; override def getClientInfo: Properties = REQUIRES_DB &gt; override def getTypeMap: util.Map[String, Class[_]] = REQUIRES_DB &gt; override def unwrap[T](iface: Class[T]): T = REQUIRES_DB &gt; override def isWrapperFor(iface: Class[_]): Boolean = REQUIRES_DB &gt; } &gt; &gt; class MockDatabaseConfig(database: JdbcBackend#DatabaseDef) extends DatabaseConfig[JdbcProfile] { &gt; override def db: JdbcBackend#DatabaseDef = database &gt; &gt; override val profile: JdbcProfile = new JdbcProfile { } &gt; override val driver: JdbcProfile = profile &gt; &gt; override def config: Config = ??? &gt; override def profileName: String = ??? &gt; override def profileIsObject: Boolean = ??? &gt; } Then in your unit tests you can create a db object like this: &gt; val db = new MockDbBackend.DatabaseDef(new MockJdbcDataSource, AsyncExecutor.default()) &gt; val dbConfig = new MockDatabaseConfig(db) This only useable for tests where you are doing some logic based on combining several different `DBIO` actions. It isn't useful for testing methods where you are building `DBIO` actions from queries. 
Using react you can make index.html static and just host it from S3 or some other file storage. IT will be easier to manage and faster to set up. Not to mention cheaper if you start throwing any volume of traffic at it.
Cool. Can you give a more specific link? I also reviewed the 3rd edition, but I merely met Bill in a coffee shop here in Lafayette and went over a new section with him and made some suggestions. :-)
I actually liked it very much, I thought it's nice ... was just making a joke with regards to hiring process in most companies, not to the code itself... please accept my sincerest apology. Goal was 100% to make a pun on hiring process.
You may find this useful: https://github.com/japgolly/scalajs-react
&gt; That it has become impossible to explicitly opt out of these conversions altogether (unlike in Scala) How do you opt out of this in Scala out of curiosity?
 private val baseRoute = getFromFile(buildPath+ "index.html") private val publicRoute = pathPrefix("static")(getFromDirectory(buildStaticPath)) Http().bindAndHandle(publicRoute ~ baseRoute, "0.0.0.0", 9000) /static/* will serve from build/static /* will serve index.html
With an explicit type annotation: `List[Any](...` Compare the different results of: val list = List[Any](1, 2.3, 'a') list.foreach(e =&gt; println(s"$e : ${e.getClass}")) in Scala: 1 : class java.lang.Integer 2.3 : class java.lang.Double a : class java.lang.Character and Dotty: 1.0 : class java.lang.Double 2.3 : class java.lang.Double 97.0 : class java.lang.Double As a workaround, you can add some unrelated type to the list and then immediately remove it again: val list = List("", 1, 2.3, 'a').tail list.foreach(e =&gt; println(s"$e : ${e.getClass}"))
This is exactly what I was looking for. Thank you! 
Why Go though? ðŸ¤”
I'm working on solving the 25 2017 puzzles in advent of code in Scala. I was too busy at work in December to solve them all but I've been doing a few during the xmas break. Trying to stretch my working knowledge of Cats and FS2 a little while solving them. https://github.com/justinhj/scalaadvent2017
Interesting! Thanks!
There's no absolute right or wrong here, which is why I think the title is the by far worst part of this article. The rest if the article is actually pretty good. But, like others have mentioned, well named types is a good alternative to well named (case class) fields, and sometimes even better. The end goal is the same: have some sort of name to make sense of the data at hand.
I think most people think out generics as full types rather than type constructors, so they see (Int, String) and call it a Tuple, and they see (String, Int) and call it a Tuple, when one is a NameRecord and the other is a PhoneBookEntry. A single type alias helps with this. The other problem people have with generics, especially tuples, is that the fields aren't named conveniently. Now, you could fix this with an implicit class that provided accessors by named methods, but most people aren't willing to go that extra mile for named fields. The final piece that is problematic for both tuples and shapeless hlists is that it is perfectly legal to modify the type via copy, like below. println((5, "test").copy (_1="hello")) This is what nakes them more useful than case classes, but also more powerful, and that power isn't always necessary, and is often harmful when I'm trying to constrain types. To fix it, you have to add an explicit type annotation: println ((5, "test").copy[Int, String] (_1="hello")) Which is something you can write a lint rule to ensure, but gets tedious after a while.
You should also post this in /r/webdev 
I'm really not sure how you decided upon Scalatra when Play is the defacto standard for Scala webdev. It stands heads and shoulders above any other scala-based framework and has far more support and documentation. Did you wanna keep it super lightweight or something? Just use Play, trust me.
The idea was to keep it super light. Things I've read about Play seems like it was heavier, something meant more for doing full stack webdev than a quick API type thing. Maybe I should revisit Play?
Can you give me one or two sentences about why you think Play would be a better solution than Scalatra for a simple API?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Binding.scala v11 â€“ a reactive templating language for both web and desktop GUI](https://www.reddit.com/r/programming/comments/7n32su/bindingscala_v11_a_reactive_templating_language/) - [/r/webdev] [Binding.scala v11 â€“ a reactive templating language for both web and desktop GUI](https://www.reddit.com/r/webdev/comments/7nd0j2/bindingscala_v11_a_reactive_templating_language/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
&gt; Play is the defacto standard It is absolutely not, and &gt; It stands heads and shoulders above any other scala-based framework it does not. I don't know where you got this from. 
Nice one but too small. Could be great to see something like this but of production quality. With service layer, logging, transaction management, timeouts, configuration, lifecycle management(graceful shutdown) proper error handling (HTTP 200/201/400/500) etc.etc.etc.
With the statement `sys.env.get("KAFKA_SERVER").get` I will get a system variable rather than an env? Thanks
It's just an example to show the principles and to have some important bricks to start a project. For a more comprehensive example I've put a link at the end of the Readme :) 
I created a Giter8 template some time ago and I keep it updated: https://github.com/gvolpe/typelevel-stack.g8 It's not a full project but has some of the features you just mentioned like error handling. It's actually linked in the http4s website but I guess not everybody navigates the website :/ http://http4s.org/adopters/
Yeah, but if you're sure you want an exception on missing value you should use `sys.env("KAFKA_SERVER")` instead
What do I have to do to get the environment variables? 
Gladly - I'll emphasize the documentation part. On stack overflow there are around ~250 questions tagged with scalatra. There are around 15k play framework related questions. You're pretty much on your own. Another thing is lack of SSL support. Theres nothing on stack overflow to help.
Attempting to build a video stream processor with Akka. I have an api endpoint with a camera feed and need to process each frame through a Graph and send data at different stages to different endpoints. Right now Iâ€™m having trouble with the source stream â€” receiving multipart boundary =â€”donotcross and working on the image bytes. Thoughts? Iâ€™m new to a Akka and not super familiar with Scala. 
Say like this : https://github.com/pauljamescleary/scala-pet-store ? It was posted here.
As you can see, I set `envVars in Test := Map("KAFKA_SERVER" -&gt; "localhost:9092")` on root `build.sbt`. How to get the value of `KAFKA_SERVER`? 
I've never seen actors as useful, at least not useful enough to justify losing type safety. Whatever real use case they may have, they've been vastly overhyped as something to use for everything. Two jobs ago I replaced actors in a production codebase with plain old code (I guess with occasional use of `Future { ... }`). At my current job I replaced actors in a production codebase with mutexes.
I would agree with avoiding Play for APIs (and actually in general, but that's more controversial). It's worth looking at akka-http though, which is the more lightweight layer that Play is now built on top of. http4s via Rho is also worth looking at IMO, though it's a bit more purist and might be a big jump for less experienced Scala folk.
I'd like the set of modules whose releases are tied to specific language versions to be small, and the set of modules that are easily available and "standardised" to be much larger than that. So some kind of core/platform distinction seems necessary.
The problem is that the smaller the set of types is that people depend on, the more needless incompatibility you get between third-party libraries. We are already seeing the effects of this with typeclass hierarchies (cats vs scalaz), HLists (shapeless vs. home-grown versions (Slick, akka-http, etc.)), JSON, various encoding/decoding libraries and so on. The way modules have been done to this day was more of an effort to drop the burden of maintaining existing code on someone else, with varied, often poor results. The big benefit of a maintained, well-curated standard library is that users don't need to adapt to completely different styles of API usage and design. You don't get this cohesion from a "platform" where every module is designed by a different person without any consideration of how their API fits with everything else. Sometimes this means that a library doesn't pull all the tricks it can do, in favor of overall consistency. In the end, this would require people to ask "what's the best way we can provide things to users?" instead of "what's the most convenient thing for me, as a library author?". It is unlikely to ever change for the better as I can't see anyone picking up the responsibility for that, considering how poorly this has worked out for the language itself.
&gt; The problem is that the smaller the set of types is that people depend on, the more needless incompatibility you get between third-party libraries. &gt; We are already seeing the effects of this with typeclass hierarchies (cats vs scalaz), HLists (shapeless vs. home-grown versions (Slick, akka-http, etc.)), JSON, various encoding/decoding libraries and so on. &gt; The big benefit of a maintained, well-curated standard library is that users don't need to adapt to completely different styles of API usage and design. &gt; You don't get this cohesion from a "platform" where every module is designed by a different person without any consideration of how their API fits with everything else. Sometimes this means that a library doesn't pull all the tricks it can do, in favor of overall consistency. Isn't the point of the platform to provide a point where we can standardised on single preferred libraries for these things, and integrate tightly between them? That will take buy-in and leadership, but it doesn't need to mean being released together the language proper. Conversely a large standard library is no guarantee of cohesion, and can make fragmentation worse if the standard library includes an implementation with noticeable deficiencies - look at Python's experience with HTTP, or the various scalaz types that are oriented as direct replacements for things in the scala standard library. &gt; There might be a reasonable distinction between core/platform to be had, but the "core" is in such a poor state that I can't even imagine talking about the "platform". Most of the things that are currently "core" are in dire need of wholesale deprecation or replacement, and many of the things considered for the "platform" should probably be completely rewritten and part of a new "core". I'd see a core/platform distinction as a useful path towards doing that kind of deprecation/replacement. E.g. the collections rewrite was hampered by the need to maintain compatibility with existing code, because at the moment any compatibility-breaking change to collections would block people from upgrading to newer versions of Scala. If collections were pulled out of core - a pretty extreme example, but not completely infeasible - then the more radical proposals that broke more compatibility would have a stronger case.
&gt; Isn't the point of the platform to provide a point where we can standardised on single preferred libraries for these things, and integrate tightly between them? This really hinges on the question whether these libraries can depend on each other or not. If they cannot, you end up with tall, fragile towers of dependencies and none of the interop you want. If they can, you end up with a de-facto standard library, but where every single piece uses different API styles and idioms. &gt; Conversely a large standard library is no guarantee of cohesion [...] Of course, but there is the possibility to make things cohesive that doesn't even exist with the platform approach. &gt; look at Python's experience with HTTP, or the various scalaz types that are oriented as direct replacements for things in the scala standard library This is a cautionary tale, but Scala has more abilities to deal with this, due to static typing and the deprecation infrastructure I wrote. It is still one step forward, two step backwards though: Cats dropped their `Xor` type in favor of the standard library's `Either` after I made it right-biased, but at the same time we got `@printAsInfix` added to the language with zero use-case now that `Xor` is gone. &gt; the collections rewrite was hampered by the need to maintain compatibility with existing code, because at the moment any compatibility-breaking change to collections would block people from upgrading to newer versions of Scala I think most of the compatibility requirements that influenced its design have largely no impact on better compatibility for users: All their collections code will still be broken, and it will be next to impossible for library authors to support both &lt;= 2.12 and 2.13 without forking their codebase ... and the benefits in return will be rather slim. This is nothing that could have worked out differently by making collections a "platform" module. Consider the large hit that Scala took when breaking everyone's code with the collection rewrite in 2.8 (see [https://stackoverflow.com/questions/1722726/is-the-scala-2-8-collections-library-a-case-of-the-longest-suicide-note-in-hist](Is the Scala 2.8 collections library a case of â€œthe longest suicide note in historyâ€?), and the promises that have been made after that. It's impressive that nothing have been learned from that.
&gt; If they can, you end up with a de-facto standard library Sure, but one that can have a better pace of development since its pieces are versioned independently. Major changes would still require coordination, deprecation cycles and so on, but could hopefully still happen faster than new language releases. &gt; Of course, but there is the possibility to make things cohesive This possibility doesn't exist with the platform approach. In principle there's no reason it should be harder to enforce cohesion in a "platform" than in a standard library, though it relies on the platform owners/certifiers enforcing it. &gt; Cats dropped their Xor type in favor of the standard library's Either after I made it right-biased, but at the same time we got @printAsInfix added to the language with zero use-cases now that Xor is gone. Cats' `Is` and Shapeless' `::` and `:+:` come immediately to mind as use cases. &gt; I think most of the compatibility requirements that influenced its design have largely no impact on better compatibility for users: All their collections code will still be broken, and it will be next to impossible for library authors to support both &lt;= 2.12 and 2.13 without forking their codebase There are levels of breakage. Almost all libraries depend on collections, but there's a difference between a library that writes `List(a, b)` and a library that makes use of `CanBuildFrom`, views, or similar. I would expect most libraries to fall into the former class and be able to support 2.12 and 2.13 without forking the codebase. I guess we'll find out.
&gt; Sure, but one that can have a better pace of development since its pieces are versioned independently. Existing evidence shows that this didn't happen In the end, it will just be another version number that people will have to update (or more likely forget to update). This could have been addressed by releasing more minor versions of Scala (because that's exactly where most people would have picked up the minor version increase anyway). In the end, the better pace of development is a problem of people wanting to be in charge of everything but responsible for nothing. There are just not enough people anymore who care about this, otherwise we would have already seen improvements regardless of modularization. Pretty much everything that has been modularized is either dead, unmaintained, deprecated or already on the verge of removal. &gt; it relies on the platform owners/certifiers enforcing it This will be even harder to accomplish than for a standard library. Having everything in separate Git repos makes this extremely hard anyway There is just too much friction when you want to make fixes across repo/module boundaries, even if the changes are not user-facing. Have a look at the modularized partest. I backed out of multiple fixes because of the insanity involved in publishing/bootstrapping things that should have been a simple 5 line change. I still can't believe that someone had the idea of saying "hey, let's modularize half of partest, so whenever you make some kind of change to basic parts of the compiler your compiler testing framework stops working" and nobody (who mattered) figured that it might be a bad idea. &gt; Cats' Is and Shapeless' :: and :+: come immediately to mind as use cases. `::` and `:+:` already [print as infix by default](https://github.com/scala/scala/pull/5589).
Unfortunately rho is not yet available for http4s 0.18. I'm trying to upgrade it but I'm not sure if I'll ever finish this because it is a major PITA to port the codebase from fs2.Task to some generic monad. 
It'd be a shame to lose it. I could try to take a look if there's a branch, though no promises as I've been pretty busy lately.
Li Haoyi is making another cool software :)
This is an A+ article. Great stuff.
Hey now, everyone is always complaining that they don't like SBT, and then when people create new build tools people complain? I really appreciate that Li Haoyi and Chris Vogt and others are willing to put in effort to make programming in Scala easier.
You the man Li Haoyi. Looking forward to seeing this project progress. 
There is actually a PR from a former colleague of mine and he appears to have resolved most of the issues, I just had a look. But the amount of changes is crazy, the generic `F[_]` parameter infects _everything_. 
&gt; everyone is always complaining that they don't like SBT, and then when people create new build tools people complain? disjoint sets, vocal minority. i never complained i didn't like sbt. &gt; to put in effort to make programming in Scala easier most people who complain about sbt, complain about the dsl. this tool build file doesn't look any simpler than the sbt equivalent. i just don't see what problem it solves.
I actually think that there can't be too many stabs at build tools at this point. The worst scenario I can imagine is that no one cares and SBT will stay in use everywhere. In the best case scenario however we'll have several good ideas in a collaborative environment where code and concepts can be shared or adopted. That the solution is explored at all is a big win :)
The DSL *is* sbt. I personally also find the slowness unbearable. In the project I work on, starting sbt takes almost 30 seconds. And unfortunately I usually have to restart sbt dozens of times a day. Yeah yeah, 30s isn't the end of the world, I know, but it kinda breaks the flow I'm in. I need to enter sbt to run the tests I'm working on *now*, not half a minute from now. Also worth reading: http://www.lihaoyi.com/post/SowhatswrongwithSBT.html
sys.env
Why do you have to restart sbt? All of the benfit comes from running in a warm vm.
&gt; The DSL is sbt. i'm not trying to disagree with you, but i recommend you also read chapter 1 of sbt in action. it does a really good job highlighting everything sbt does better than most other build tools. it is more than just a dsl.
&gt; This should hopefully happen by the end of 2017. [/u/lihaoyi](https://www.reddit.com/user/lihaoyi) should probably be 2018.
Oh, yeah, need loads of ram. Laptop has 32GB and I'm thinking about getting 64GB for the next laptop upgrade (multiple sbt sessions, multiple node builds, multiple VirtualBox VMs, multiple browsers, etc., the memory goes quickly).
While I've made the repo public, it is not yet ready for use. - If you are interested in helping write code to build a new Scala build tool, I'd love your help. We have a vision, a plan, and most importantly working code that can already build several (small) open-source repositories. IntelliJ support, watch-&amp;-re-evaluate (for both project files and build files), incremental re-builds (both inter-compilation-unit and intra-compilation-unit using Zinc), a build console &amp; plugin system (using Ammonite), sonatype publishing support, uber-jaring, etc. all work. This isn't a toy experiment that'll disappear tomorrow - If you are looking to *use* a new Scala build too, you probably should hold off until we announce that it's ready for use - If you are happy with SBT (or Bazel, Gradle, etc.) then you probably should just ignore this project
Yo. if this gets it so the rest of my team isn't afraid to touch our build scripts (sbt), I'm down.
I think I'm going to learn a lot from this!
Am I the only one that just uses maven? 
&gt; Why do you have to restart sbt? All of the benfit comes from running in a warm vm. it runs out of ram for me
At this point I am convinced that lihaoyi is actually at least identical triplets who all pretend to be a single lihaoyi.
You're not. I do, it works nicely. Everything else seems like a solution in search of a problem. (Cross-building is awful, but SBT doesn't properly support cross-building either, it just has a built-in hack for the specific case of multiple Scala versions)
Wartremover can make inferred `Any` a warning/error, and using `-Ywarn-value-discard` can avoid the risk of discarding a future. I don't think there's any way to prevent `Future[Future[Int]]` but you'd have to flatten it sooner or later or discard it, so I wouldn't worry so much.
&gt; I don't think Free and Tagless final are as mutually exclusive as implied here. Why not use Tagless final and have your first interpreter evaluate F to be Free[MyCoproduct, ?] turning your tagless program into a tagged program, and do all the advanced optimizations you would with Free from there? What would be the advantage of doing that over writing the program in free coproduct style to start with? It sounds like the worst of both worlds to me - the runtime performance cost of free with the awkward control flow of tagless final.
Eh, it will touch a lot of lines but it's a pretty mechanical change. Not so much different from having a new value parameter that you have to pass down/up through everything.
Mostly, yes. A lot of objects have to become traits, and instead of importing the object you have to mix them in. I also ran into trouble with implicit resolution and had to help the compiler in a few places. 
I get a elon musky kind of vibe. First he publishes this - http://www.lihaoyi.com/post/SowhatswrongwithSBT.html and then now this. I am curious on how the SBT community reacts to it. In the original post, it seemed like martin odersky almost agreed to all of the issues and it needs to be fixed - https://docs.google.com/document/d/1QdtRJGxlKTiXcAxsjXWLtWVzeZyUzGVDh8oOYOiuhwg/edit But this separation is concerning. First it started with IDEs i.e Intellij and it moved to Visual studio code, now SBT. I feel that Scala community is smaller to have too many opinions. Java community is big and sometimes I feel that having too many build tools is confusing in many ways. I just wish that the community comes forward and consolidates this.
I don't really think disagreeing about the need for another build tool is toxic, and that censorship would be worse. I'm confused at why none of their other posts are showing, yet they have substantial post and comment karma however.
I am not talking about just the one comment above. There has been several situations. For context, you can refer the thread here - https://www.reddit.com/r/scala/comments/7cqq38/are_you_using_scala_collection_efficiently/ &gt; I'm confused at why none of their other posts are showing, yet they have substantial post and comment karma however. That is because he puts out rants, and then deletes them later. I don't know for what reason? Disagreeing is fine, but just putting an xkcd comic is meaningless. There is a complete blog post written on which the library author has written. 
I think as long as they are limited to private methods and functions, no harm no foul. When they start leaking into other classes I start to get worried.
Thank you!
You could do that, but it's not quite as easy. First of all you have to define an ADT for every Algebra where you want to have optimizations and `Free` alone isn't really going to cut it for the optimizations here, you'd need something like `Free[FreeApplicative[MyCoproduct, ?], ?]` and that can get a bit unwieldy fairly soon. However I don't mean to imply that `Free` and `Tagless` are mutually exclusive at all. If you're interpreting into an `F` that's not stack safe, I definitely recommend just interpreting to `Free` which can be done very easily with a library like Mainecoon. :) For most problems however, tagless should be more then enough and a lot simpler to understand for most people. 
&gt; "The community" in general is not someone who can come forward and consolidate. It consists of individuals (like you and lihaoyi) who make stuff and have opinions. The community I meant is the SBT community. Sure, I can definitely contribute, but my employer sadly does not pay for open source work and I have very little time off the job (married with kids), so all I can is hope that issues are fixed. I wasn't replying to you specifically, was just saying in general. 
The SBT community is pretty much Dale and Eugene
Here's a [stackoverflow answer](https://stackoverflow.com/questions/7762838/forward-references-why-does-this-code-compile) about this topic.
Why start the article by twisting history in order to favor your narrative ? &gt; Most web servers used to use thread per connection model to handle http requests. In this model, the servers also used to use blocking IO libraries to read requests from connections and write responses This is pure bullshit, just because some badly designed servers do this it doesn't mean most good servers don't use an asyncio paradigm to handle request. Nginx, for example, has always been asynchronous. &gt; With non blocking IO and evented APIs, programming is harder. Again, this sounds like utter garbage. From the end user perspective, relying on an asyncio or blocking-io http framework changes very little. A web-server is by it's nature "event based" anyway since it's purpose is to receive and send events. Whether the underlying mechanism by which this is using a single thread end to end (from reading from the socket to sending back the response) or using one or multiple polling threads and then passing of the read information to a processing thread poll and then again to a sender thread, isn't that relevant for the guy doing the coding behind the server's framework. Also the coding style, even from the first 5 line snippet, seems to be awfully inconsistent, e.g.: val serverSocketChannel = ServerSocketChannel.open val provider = SelectorProvider.provider() But maybe I'm judging the beginning of the article to harshly based only on the beginning of it, then again, time is a limited resource.
If you work on modular code that is in different repos, you end up reloading SBT all of the time. i.e. My project depends on module A which is a seperate project, I edit project A and publish its contents (either locally or as a snapshot). In order for SBT to pick this up I need to reload SBT, there isn't any way around this.
100% agree. SBT is one of the biggest anxieties my reports have about working with Scala.
What kind of "why" do you expect? It doesn't generate a compile-time error because that's not how the language is defined - this isn't a compiler bug or anything if that's what you're asking, the behaviour is exactly what I'd expect. Are you asking why the language was designed that way?
&gt; there isn't any way around this I've gone with `ProjectRef`s in place of publishing artifacts. This is especially useful for related source projects on the filesystem that change relatively frequently. There's no silver bullet, but avoiding sbt reloads is a nice side effect of working off of a single (merged) source tree.
There are downsides and upsides of having a monorepo versus modular design, but sometimes you don't really have a choice. There is always the side effect of, as other people have mentioned, SBT has historically had issues with memory and memory leaks. At my work we are forced to use designated work machines which are macbook pro's 13" that have 16 gigs of RAM and a slowish Intel Dual Core CPU, so yeah...
SBT is comprised of many things 1. A purely functional engine which evaluates the SBT graph. There is an explicit State monad which handles tracking of state, as well as all of the models within the graph being implemented as immutable data structures 2. The DSL which is what evaluates `.sbt` files to this underlying purely functional graph. Unfortunately this DSL makes SBT *feel* like its mutable (when in reality its not) via the usage of macros. So you can say it is more than just a DSL, but a DSL is a significant part of SBT and its not something you can isolate or ignore. People never write build files in SBT without the DSL in `.scala` files (in fact the SBT guys have disallowed defining builds with `.scala` files some time ago)
"is-a" != "has-a"
How would this technique work with branches present in program? store.get("X").flatMap { case None =&gt; store.get("A") *&gt; store.get("A") case Some(x) =&gt; store.get(x) *&gt; store.get(x) } 
I disagree with /u/m50d's assessment. This is the worst bug in Scala and sadly it cannot be fixed. It's a consequence of the way Scala classes are compiled to JVM classes and is complicated by the fact that you can override a `def` with a `val`. In fact until 2.10 (iirc) you wouldn't even get a warning, so it could be worse. If you compile with `-Xcheckinit` the compiler will insert code to throw an exception if a field is references prior to initialization, rather than silently propagating an "empty" value. There is a slight performance cost in doing this but I [recommend](http://tpolecat.github.io/2017/04/25/scalac-flags.html) it nonetheless. It's common to work around initialization order problems by using `lazy`, although this isn't always possible. In this case it is: @ class Foo { println(a) lazy val a = "foo" } defined class Foo @ new Foo foo res3: Foo = ammonite.$sess.cmd2$Foo@1e8fd198 Note that this is only an issue with variables in a class/object body that are compiled to JVM class fields. Forward references are disallowed for locals, as you would expect: @ def foo = { println(a) val a = "foo" } cmd2.sc:2: forward reference extends over definition of value a println(a) ^ Compilation Failed 
&gt; and do all the advanced optimizations you would with Free from there It will probably have to work slightly different than this. The optimization that is done is something that is possible for applicative programs, but not monadic programs in general. The `foldMap` function of `Free` won't let you interpret into `Const` since it is not a monad. Meaning that the program would have to be converted to a `FreeApplicative`, then converted to an optimized version and then converted into a `Free` program, which can be embedded into other `Free` programs.
AFAIK it's impossible to inspect a Monadic program (further then the first level atleast), thus it wouldn't work with your program.
Interesting post! Some thoughts: - This method is set up so that optimization points in the program are explicitly marked. On one hand this means that there is a clear distinction which parts are optimized and which are not. On the other hand, the fact that optimizations are happening is unchangeable. This last point might not seem like a bad point, but sometimes we want to leave it open whether we want the 'optimized' or 'unoptimized' versions. Take for example errors with `Either` ('unoptimized') or `Validation` ('optimized'). - When involving multiple effects, namely multiple algebra traits, you sometimes have to restructure your program to avoid `Monad` constraints popping up on the `F[_]` because you need to thread variables through different effects. For example, logging intermediate results with a logging effect. - This style seems basically equivalent to the Haskell mtl-style where the `Monad` constraints are left out. I posed a question related to this style on /r/haskell some days ago: https://www.reddit.com/r/haskell/comments/7hqst3/is_there_any_reason_why_mtlstyle_is_focused_on/ 
The reason why you aren't able to usefully inspect your program is the use of `flatMap` incurring a `Monad` constraint on `F[_]`. If you want you can still use the optimization technique in both branches on the `store.get(x) *&gt; store.get(x)` parts. Nonetheless, there also exist other types of programs inbetween applicative and monadic programs. Yours can be seen as an arrow program, and could be encoded as a `FreeArrow`. This would allow an inspection such as counting the amount of `get` and `put` operation in the program. Unfortunately, arrows don't play as nicely with this tagless final encoding since arrow programs are based on signatures with two type arguments instead of a single type argument.
Where did you find this text? Was it in a .scala file? Why do you think it might be scala?
I'm not sure I understand how `Free[FreeAp[F,?], ?]` makes any sense. If your program is constrained to using `Applicative`, Then then you just optimize with `FreeApplicative`. If your program is constrained to using `Monad`, then you lift, into `Free`. If have a program that is constrained to `Monad` but are attempting to selectively constrain parts of the program to applicative, how do you know what to tag with `Free` and what to tag with `FreeAp`?
[Here](https://youtu.be/gUPuWHAt6SA?t=2533) is an interesting application of `Free` representation. I doubt it's possible to achieve the same with proposed approach.
I wouldn't inspect `Free` structure *like that* :)
I think this snippet of decompiled code will explain the behaviour (note this is decompiled code, and decompilers don't handle Scala classes 100% correctly): public final class TestClass$ { public static TestClass$ MODULE$; private final String a; static { new TestClass$(); } public String a() { return this.a; } public void main(final String[] args) { } private TestClass$() { (TestClass$.MODULE$ = this).a(); Predef$.MODULE$.println((Object)this.a()); this.a = "b"; } } Scala treats every declaration directly inside a class/object as a member declaration and every piece of code (including initializers) as the code for the default constructor. There's nothing that prevents you from printing a field in the constructor before initializing said field, except for common sense.
It was from a crypto challenge that I'm doing. I wasn't familiar with what is was so I did some google searching. The most common item to come up was Scala so I figured I'd check. Looks like this isn't the case though.
Are the Rank-N issues mentioned eliminated in Dotty?
Hmm interesting - thanks!
Yeah, I first noticed it in a trait and then reproduced it in a class. I'm not sure whether I prefer an exception. I did notice the warnings when I was generating an SCCE - which actually confused me further - the warning means the compiler can detect the problem so could prevent compilation.
Because using a variable before it's been declared is generally prevented in languages.
I don't like this behaviour any more than you do, but I think it's misleading to call it a "bug" - that suggests it was unanticipated, unintentional, and will be fixed.
You can call it a design error if you prefer. It's a bug as far as I'm concerned.
Here is one of reasons I still use SBT: lazy val fine = project .dependsOn( RootProject(file("../dql")), ProjectRef(file("../mongorm"), "macro"), //for multimodule project, root project (mongorm) does not work RootProject(file("../fine-commons")), ProjectRef(file("../finecss"), "css"), RootProject(file("../hottie")), RootProject(file("../config3")) ) .settings( libraryDependencies ++= Seq( "org.scala-lang" % "scala-compiler" % scalaVersion.value,
Unless you are missing spaces at one or more positions and mixed up single and double quotations, no this is not valid Scala. 
As I understand Luka, you will have different parts of you program. Those that can be optimized in terms of parallelism and those that can't. Instead of constraining it via the `Parallel` typeclass you will need to encode it different, because with `Free` it doesn't work (you can have dependent computations with that). So you will either need to encode everything with `FreeAp` or - if parts of your program are dependent - you will have to combine both so that you can optimize the `FreeAp` parts and execute them e.g. in parallel.
Yeah we can't do this at work (or at least we would have to maintain 2 `.sbt` files, because our actual production build does need to use maven dependencies (i.e. standard `libraryDependencies`) to resolve the artifacts, its just for local development where you have this workflow
I'll definitely take a closer look. Right now my plan is to stream and extract frames via jpeg start and end bytes. At first look, those links have snippets I should incorporate. 
This is a great article on how creating fat Jars for any SBT project, not only Spark.
For more information, check out the ScalaFiddle integration site at https://github.com/scalafiddle/scalafiddle-core/tree/master/integrations
Needs a lot of work. A few points: - Applicative's signature Is very screwed up: ```abstract class Applicative[A, R, F[_]](implicit f:Functor[F]) { def &lt;*&gt;(fx:F[A =&gt; R])(a:F[A]):F[R] def pure[A](a:A):F[A] }``` You should not be binding `A` and `R` at the instance level Applicative is only a statement about the quality of the Functor `F`. You should bind `A` and `R` at the method level. - If you're going to use implicits at the instance level, `Monad` needs an implicit `Applicative`, rather than `Functor`. - Without `Functor` constraints, your `Free` is not actually "free". Consider lifting `final case class Weird[A](a: A, f: A =&gt; A)`. You could `LiftF` an instance of `Weird` into a free monad, but `Weird` is not even a functor. - `PairMonoid` has an infinite number of potential solutions, it's probably best to make it `PairAdditive` or `PairMultiplicative` - `Traversable` needs `Foldable`, but i don't see a definition for either, and the signature for `traverse` and `sequence` is incorrect. - I'd stay away from the implicits at the definition level, and just go with very judicious subtyping for performance reasons, but I understand where you're coming from, and it definitely illustrates the typeclass hierarchy. Consider this project I wrote as a good [starting point](https://github.com/emilypi/InterviewQuestions) 
This unfortunately doesn't address the question, which was how to write the interpreter to build a `Free[FreeAp[F,?], ?]`. Interpreters don't know in a program you are so it can't decide how to selectively tag different parts of the program.
It's not really about there being some great advantage over just using Free. Perhaps you've made the decision to use Tagless Final, and you don't need to rewrite your entire program to do some analysis later on.
&gt; The foldMap function of Free won't let you interpret into Const since it is not a monad. I get that you're saying we can't use `analyze` but this since you mentioned const, you can do this example's particular optimization with `Writer`. Meaning that the program would have to be converted to a FreeApplicative, then converted to an optimized version and then converted into a Free program, which can be embedded into other Free programs. I don't know why'd you lift into `Free` after performing your optimizations. You would interpret into your effect to combine with the rest of your program you've already done the analysis you need, there' no reason the keep around tags, or tag other parts of your program.
It's just an example to show the principles and to have some important bricks to start a project. There is a bit of error handling by the way. For a more comprehensive example I've put a link at the end of the Readme :) 
Yes, it was linked in the readme of the project too. Scala Pet Store is great, I just needed a smaller example to introduce the concepts to my team.
Well, you can read more about it here: https://github.com/milessabin/shapeless ... but IMO, it's **usually** not a good idea to use it frivolously. The ideas, concepts, typeclasses, etc. are **GREAT**, it's just that it can cause absolutely insane compilation times (and various weird internal compiler failures). The basic reason for this is that Shapeless "abuses" the compiler in the same way that e.g. C++ template code[1] "abuse" the compiler by recursively expanding to huge amounts of code. In Scala it's done through implicit search, but that's not really much better. [1] Well, it used to be a crazy amount of abuse, but these days C++ has constexpr which helps to avoid the "use-compiler-as-lambda-calculus" issue.
Personally, I would go with "sbt-pack". It doesn't require the weird workarounds associated with packaging everything into a single jar.
I'm a fan of much of your work, but... &gt; If you are happy with SBT (or Bazel, Gradle, etc.) then you probably should just ignore this project ... well, I'm not exactly *happy*, but SBT seems to work about as well as most of the other build tools in the JVM ecosystem (abysmally), so... If you don't mind, I'll keep following this with interest, but keep *using* SBT for now :). EDIT: Oh, and if you don't mind a question... Have to looked at Shake (Haskell), redo (C, I think), tup (C) for ideas? I apologize for asking before even looking at the README... which I shall do right now.
No
I would be curious to know how can you inspect past a lambda, since that is totally opaque
I suppose, I'd ask you to define "great". Slow-as-molasses has been the general experience in my vicinity. And, yes, compilation time **DOES MATTER** to most people's productivity. (Again, the ideas and features are great and could be **RIDICULOUSLY** more efficient if directly supported by the compiler. "Deriving", I'm looking at you!)
It is really good input. Indeed applicatives have been implemented in a weird way - I rushed it most likely - implemented now as you suggested - feels way more natural now. Monad is dependent upon functor indeed - I wanted to preserve learning order functor =&gt; monad =&gt; applicative. I think it is more practical than functor =&gt; applicative =&gt; monad (I see functor and monad being used in production code way more often than applicative style). Renamed that Pair monoid as you suggested - hope it is more clear now. Thank you!
Great! Re: Applicative - the hierarchy of strength is Functor =&gt; Applicative =&gt; Monad, with Functor and Monad being seen more often simply because they've been more popularized. The hierarchy is this way because Monads are stronger than Applicatives are stronger than Functors in terms of their laws, which make them practical from a lawful standpoint. Here's a good [paper](http://www.staff.city.ac.uk/~ross/papers/Applicative.pdf) to read on the subject.
It looks good, although I'm somewhat against the use of macros. You can run into things like https://github.com/sbt/sbt/issues/3057.
I just think it's way better than the alternative. Anyways, you don't usually have to touch Shapeless anyways. It's mostly used for stuff like Json serialisation and DB column mapping which there already are libraries for.
https://i.imgur.com/etjgJ2D.jpg
One thing I didn't see mentioned was [Unison](unisonweb.org)-style programming where programs are type-safe "by construction", because the editor won't let you commit a line with a type error. Also, because it discards text-serialization, a whole bunch of programming language design conundrums become irrelevant. You don't need type annotations at all if the type information is just a property in the data structure you're editing. But that's just the tip of the iceberg. I definitely recommend giving some of the blog posts a read.
I believe intellij has an inspection for this. I think it's called nested monads.
My problem with sbt-pack is that it is not available for Scala 2.11
I guess that depends on the nation.
What can you do if you can't find the package you want to use in the right combination of Scala version and sbt version?
I have yet to find a use-case for it personally. (I've been programming Scala full-time for almost 5 years). I am a big fan of macros for things like e.g. json serialization though. But those things are already handled by my json library; be it play-json or circe.
does java have an equivalent function of scala.Predef.require() ?
Which nation? May you suggest?
I see no hint that this is caused by macros.
One the Units of measure &gt;F# picked it up (as with many language experiments!) and I do not know if it's "done" yet or even if users consider it a universally welcome idea, but it seems to me that it has potential. It's "small" but rather important not to mix units! I'm working in embedded systems controlling physical systems. That would be one of the most important problems in terms of cognitive load of developers. But it's really hard to get right. Especially fixed point representations + units of measure place a high cognitive load on the developer. ADA at least had an attempt to do the fixed point better: https://en.wikibooks.org/wiki/Ada_Programming/Types/delta
I agree with that, but want to add and emphasize that it *should* indeed be used rather more than less. But as you say, it should not be used frivolously, indeed. While it will lead to higher compiletimes, one has to see that it also reduces both the runtime, the mental overhead. There is less mental overhead because you do things directly, you need less code and less tests and/or reduce bugs. All in all, in many cases it will lead to faster developement even though compiletimes are higher.
If you don't use it a lot, it will be hard to see where you can apply it. It's actually similiar to monads: if you don't know what it is, you cannot "see" it in the code and you will either reimplement parts of it again and again or use a different solution (usually more complicated but still working). So, I'd like to offer you this: tell me something about what you are doing (in terms of your business) and let me try to find some place in there where I assume you could/should use shapeless but where I suppose you are not doing it and working around (writing repetitive code, using reflection/macros or something else).
Sadly no, I'm still hoping for getting something in Typelevel Scala though.
Thank you Emily :)
While I want to accept your offer, it's quite hard to sum up everything our code does unfortunately :P But ok: One thing we do is to receive a json request from our frontend, convert it to a case class with correctly named fields (thanks to macro from play-json), inspect &amp; correctness-check it, do some DB, catch constraint violations (if they occur) and return a appropriate error message or a success(!!) json message back to the api-user depending on the outcome. I'm not sure if it's possible to take anything from that..? I think it's easier to do it the other way around: What's the best (real life!) use-cases for shapeless in everyday prorgramming? I've tried to look at the example, e.g. https://github.com/milessabin/shapeless/blob/master/examples/src/main/scala/shapeless/examples/fibonacci.scala , but that doesn't really apply to my everyday programming. Also, besides looking kinda cool, it's not even how I would do fibonacci tbh when I can do it with a 1-liner :p The only cool thing I've seen from shapeless is HLists which is like a "better tuple" it seems. It's a great improvement over ordinary tuples, and should (and will?) be the new default for scala/dotty.
&gt; return a appropriate error message or a success(!!) json message How does the format of this json look like? Is it just something like `{error: "..."}` or is there some more structure to that in the error message? E.g. do you want to return in the error message that a validation for the case class failed, so e.g. `customer.adress.street.number is not a valid number`? That would be something where shapeless can come into play to derive the correct error message(s). I find it hard to describe something like "where can shapeless be used, generally". The other way around is better. But maybe here is a library I wrote that might be interesting for you: https://github.com/valenterry/bamboomigrate
There is an `assert` keyword: https://docs.oracle.com/javase/specs/jls/se8/html/jls-14.html#jls-14.10
An older, but still entirely valid take on an indirectly related topic: [The future of compiler optimization](https://blog.regehr.org/archives/247) from John Regehr. The section on Verification and Optimization joining forces is particularly poignant. With Dotty coming, with its more robust type system and the ability to write your own compiler optimizations, we could make a huge amount of headway here. Some recent JVM-land additions like modules and the Graal AOT compiler, and we have a ton of potential for extremely optimized execution and binary sizes. Apart from that, effect systems are definitely the future and are a big win. But I think the holy grail would be a co-effect system to go with it. Co-effects are the dual of effects: effects are what your program *does to the world*, co-effects are what your program *requires of the world*. In fact, you can't even have an effect without a corresponding co-effect; you can't write to stdout if you don't have a console, you can't use random numbers if you don't have a random number generator, and you can't write to a file without a file system. The JVM made cross platform programming easy for the time because it became a lowest common denominator of co-effects...if you had a JVM you were more or less guaranteed to have access to file system and graphics APIs, etc.. But the world has become a lot more complex since the JVM made it's debut. Instead of relying on a monolithic platform to guarantee your program will not fail at runtime, a co-effect system could enforce it for you in the type system, allowing you to write multi-platform code in the same compilation unit without any ridiculous `#ifdef` hackeries. For an (extremely glossed over) example of what this could look like, see my comment on the [JDK9 module system support issue at scala-dev](https://github.com/scala/scala-dev/issues/139#issuecomment-346422762) 
Scala is supported in CoderPad, which is the interview tool I see used most often. But I find that dynamically typed scripting languages are best suited for the kinds of questions that interviewers tend to ask. 
When I've asked to use a functional language I've been encouraged you to use something concise like OCaml and Haskell (for the same reasons why Python is encouraged). I've only ever been told to write Scala by two small (5-15 person) startups who said they would be fine if I wrote Java too.
You may already be using it without realizing it. If you are using circe's [auto-derivation](https://circe.github.io/circe/codec.html#fully-automatic-derivation), you are already using it. [Here](https://github.com/kailuowang/henkan) [are](https://github.com/julien-truffaut/Monocle) [some](https://github.com/milessabin/kittens) are some other example of libraries that are really useful and only possible to do at compile time with shapeless. I think shapeless is a library that everybody wants but doesn't know it yet. I think part of the reason is that it is difficult to describe what it does (partly because it is kind of low level). Basically anytime you have boilerplate in your code there is a decent chance that you could use shapeless to eliminate it while operating completely at compile time.
We're recruiting mainly Scala devs, so Scala is obviously a must-have for any tool / editor (but we accept pretty much any modern language - it's about ability to solve problems, not about knowing any specific language; of course FP skills are massive plus, but not mandatory). Codility allows you to pick Scala and as much as I don't like these kind of tests, I have to say that their selection of languages is quite decent.
We use CoderPad as well and Scala is definitely an option. At the same time, a lot of nicer tools / the REPL don't work with CoderPad, so often sit with the candidates and let them use whatever tools they are most familiar with and then just paste it into CoderPad after completion for the rest of the team to review in the future. Same thing with Clojure.
It was created as part of an unholy ritual in which type safety and a compiler were sacrificed in order to summon objects and JVM interoperability. 
~~makes sense to me~~
F# has a units of measure system somewhat independent from the type system. https://fsharpforfunandprofit.com/posts/units-of-measure/ I'm not familiar with Scala so it's hard for me to compare, I just skimmed what you linked to. One advantage that seems apparent is that it is an in-built feature and thus can have it's own special and nice looking syntax. As opposed to Squants where you need to import every unit and you don't define it as 1&lt;h&gt; but Hours(1) which makes it look less like the physical formula on paper.
I will interview a candidate in whatever language they are most able to prove their skill in. I expect my developers to be able to learn just about any language at a speed commensurate with their experience. That said, some of the best interview candidates I have seen did their technical portion, a pairing exercise, in Go or Scala. I gave offers to both. This does not necessarily mean that if somebody interviews with me in one of those languages that they will get an offer!
Your question sounds like you want to learn about the history of Scala and the conception of the ideas behind it. But I don't really see how that knowledge is relevant for making a derivative of Scala. If you are actually asking how one develops a compiler for a programming language, making a derivative of Scala might not be the best idea for now.
Being that Scala is in some ways a derivative of Java, I figured that its history and the ability to make a compiler for a custom language would be a good starting point for such a project. Perhaps I'm mistaken?
HackerRank supports Scala, that's what Verizon Labs uses for online screening. I imagine some other companies are using this option. 
You forgot to add the worst compilation errors ever when the code doesn't typecheck.
You might be interested in some accrual derivatives of Scala. Check out Dotty, Isabelle and https://github.com/twitter/reasonable-scala. As for how to make a JVM language, here's a talk - https://youtu.be/14hqB7Q0I58 For early Scala, check out Pizza http://pizzacompiler.sourceforge.net/doc/tutorial.html Hope some of this helps. 
Check out ensime too if you havenâ€™t. 
It is not because companies aren't using scala but companies doesn't have any expertise on the same. This happened in my current organisation too.
&gt; 2003 - A drunken Martin Odersky sees a Reese's Peanut Butter Cup ad featuring somebody's peanut butter getting on somebody else's chocolate and has an idea. He creates Scala, a language that unifies constructs from both object oriented and functional languages. This pisses off both groups and each promptly declares jihad. - http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html
Checkout https://www.artima.com/scalazine/articles/origins_of_scala.html
Same here, we don't really care that much which language is used during the interview, as long as the interviewee can think through the problem and code a solution. I've personally seen PHP, C, C++ and C# during interviews, and we don't use any of those languages where I work.
Can you explain better what you mean by derivative? Do you mean you want to create an entirely new language from scratch whose syntax and semantics are inspired by Scala? Do you mean you want to use Scala but modify it a little (like SBT and Ammonite)? Etc.
Primitive type versions of the same stuff as Java, plus unsigned variants of each (unsigned variants also with object versions). Operator overloading. If I'm feeling particularly fancy, then multiple inheritance (doubt it) and/or subclasses for enums (a bit more likely)
Dotty is an in development language that will become Scala 3. Scala already has some of the most flexible operator overloading of any languafe I know, how are you planning to modify it? Are you planning on targeting the JVM? If so I would be worried if it's even possible to easily support unsigned primitives. Though I don't actually know this, but it's something I would check. 
Someone already explained what Dotty is. The reason I mentioned it is because its codebase is a lot more accessible than that of the current scala compiler. My suspicions are increased that what you're looking to do is already possible in scala. Have you considered starting a discussion about how it might be possible to achieve your objectives in current Scala?
Same for multiple inheritance. Scala already has traits. If you want classes with multiple inheritance -- however that would differ from traits (which are going to get constructor parameters BTW) -- you'll need a different runtime. Maybe what you need is a different runtime. If so have you looked into scala-native and scala-js? 
Didn't Scala already reject those ideas? Or perhaps I'm missing something
If I have to go on what the final code looks like I'd say the poster took something that anybody that knows spark could easily understand and made it infinitely more complicated and hard to maintain, at least in that sample, is not worth it in my humble opinion
An uncomfortable way is sometimes better than no way at all...
Totally agree. Worked with Java from 2004 to 2016 and interview code always in python.
&gt; it is not that hard to collect jars from SBT manually. Is there a good guide for that?
I didn't write any guides :-) Here is snippet I used before moving to sbt pack: val libs: List[File] = fullClasspath.all(ScopeFilter( inAnyProject, inConfigurations(Runtime) )).value .flatten.map(_.data).filter(_.isFile).toList.distinct val modules: Seq[File] = packageBin.all(ScopeFilter( inAnyProject, inConfigurations(Runtime) )).value val jars: List[File] = libs ++ modules 
You're aware of the recent compiler changes to improve performance for this kind of case?
I prefer Shapeless to macros because it lets you write plain old Scala - or, if you like, reuse one macro (the one in shapeless) for all your macro needs. Whereas a de novo macro could be doing anything, which makes it harder to understand the code. The big use for Shapeless for me is typeclass derivation for any kind of "walk the object graph" use case that you might be tempted to use reflection for - json serialisation is a perfectly good example, I also use it for parsing case classes out of config. In both these cases there are libraries that might already be built on shapeless for doing these things.
What ideas?
We have a multiproject build.sbt, in which some projects rely on scala.js, but other than that, there's no jvm forks or use of the sequential options (that I can see). Not sure what else I'd be looking for.
Hmm, I would suspect something about multiproject. Let say you run tests for only one module at a time and measure the time. Then run all tests and measure time. Not taking JVM warmup into account I would expect that running everything at once would take less time, that running each module separate - I would expect initialization to be done only once. On the other hand, I might be wrong here, as each module is its own different environment, so class loader might need to try to load classes anew each time. If that was the case, (and the reason tests are slow) you could make some fake module that would "see" all the tests suites (test compile dependency) and run tests aggregated within it. But let's check if that is the right hypothesis first.
Can we have a monthly "who is hiring" thread?
Hi all I have following function, that does not compile: private def save(pea: KStream[String, String]) : Unit = { pea .groupByKey() .aggregate(() =&gt; """{folder: ""}""", (_: String, _: String, value: String) =&gt; value, EventStoreTopology.Store) } the error message is: [error] [VR](x$1: org.apache.kafka.streams.kstream.Initializer[VR], x$2: org.apache.kafka.streams.kstream.Aggregator[_ &gt;: String, _ &gt;: String, VR], x$3: org.apache.kafka.streams.processor.StateStoreSupplier[org.apache.kafka.streams.state.KeyValueStore[_, _]])org.apache.kafka.streams.kstream.KTable[String,VR] &lt;and&gt; [error] [VR](x$1: org.apache.kafka.streams.kstream.Initializer[VR], x$2: org.apache.kafka.streams.kstream.Aggregator[_ &gt;: String, _ &gt;: String, VR], x$3: org.apache.kafka.common.serialization.Serde[VR])org.apache.kafka.streams.kstream.KTable[String,VR] &lt;and&gt; [error] [VR](x$1: org.apache.kafka.streams.kstream.Initializer[VR], x$2: org.apache.kafka.streams.kstream.Aggregator[_ &gt;: String, _ &gt;: String, VR], x$3: org.apache.kafka.streams.kstream.Materialized[String,VR,org.apache.kafka.streams.state.KeyValueStore[org.apache.kafka.common.utils.Bytes,Array[Byte]]])org.apache.kafka.streams.kstream.KTable[String,VR] [error] cannot be applied to (() =&gt; String, (String, String, String) =&gt; String, io.khinkali.eventstore.EventStoreTopology.Persistent) [error] .aggregate(() =&gt; """{folder: ""}""", [error] ^ [error] one error found [error] (eventstore/compile:compileIncremental) Compilation failed the signature of aggregate is: &lt;VR&gt; KTable&lt;K, VR&gt; aggregate(final Initializer&lt;VR&gt; initializer, final Aggregator&lt;? super K, ? super V, VR&gt; aggregator, final Materialized&lt;K, VR, KeyValueStore&lt;Bytes, byte[]&gt;&gt; materialized); And the `EventStoreTopology.Store` is defined as: object EventStoreTopology { type Persistent = Materialized[String, String, KeyValueStore[Bytes, Array[Byte]]] val StoreName: String = "EventStore" val Store: Persistent = Materialized.as(StoreName) What am I doing wrong? Thanks 
I recall doing that... Don't have that file on hand now, but if I recall correctly, each module separately took a bit over half a minute and 2GB of RAM, or something along those lines. I'll see if I can dig it up tomorrow. Thanks for the tip!
Whatâ€™s wrong with IntelliJ?
No problem, there are more things you could look into: memory usage and JVM settings, if there are some tests dependencies between modules ("tests in module B sees tests in module B"), whether warmup matters or if there is some memory leak (basically run tests several times). Surely there is more, but of the top of my head I can only think of these things.
As the error message says, the arguments you're passing to `aggregate` need to have the right types. Are you trying to use the SAM functionality? I would get this working with old-fashioned anonymous classes first, i.e. code like: .aggregate(new Initializer[String]{ override def ... = ... }, new Aggregator[...] {...}, ...) and then gradually replace the anonymous classes with functions one at a time, that should let you narrow down what exactly is failing to convert.
I change it to: .aggregate(new Initializer[String] { override def apply(): String = """{folder: ""}""" }, new Aggregator[String, String, String] { override def apply(key: String, value: String, aggregate: String): String = { value } }, EventStoreTopology.Store) and it works. Why it does not work in lambda? Thanks
Why would it work in lambda form? Is the SAM functionality working elsewhere in the same project for you? (It's relatively recent and may still require a compiler flag) Are you sure these are valid SAM interfaces? Can the type parameters be inferred when you don't put them explicitly? (I don't know anything about these classes specifically. Honestly you need to learn to debug this kind of problem yourself if you want to be able to find the answers)
It's simply not my favorite editor. It lacks feature I love, but provide too many features I *need* in big scala projects to ignore.
What would be your solution to reusing that code to generate two different output types based on two different input types?
IIUC it's a limitation of the JVM, but doesn't spire have such abstractions?
Using postfixOps and the conversions that Squants provides, you can get that syntax import scala.language.postfixOps import squants.energy.PowerConversions._ import squants.time.TimeConversions._ scala&gt; val load1 = 100 kW load1: squants.energy.Power = 100.0 kW scala&gt; val load2 = 100 megawatts load2: squants.energy.Power = 100.0 MW scala&gt; val time = 3.hours + 45.minutes // Compound expressions may need dots time: squants.time.Time = 3.75 h Yes, it does require some imports, but this is actually *closer* to the paper formula than the F# approach, which requires the braces around the units.
This thing was created automatically when I installed sbt. This is my java version. java version "1.8.0_151" Java(TM) SE Runtime Environment (build 1.8.0_151-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode)
The java9-rt thing is in the bin folder as well. 
I meant java9-rt-export.jar should be in your PATH. In addition, if you're on Windows, check the PATH env variable. On Linux I also used the deb package which sets everything up properly.
In my path? Sorry, I'm kinda confused. It's in the SBT folder, it was created when I installed sbt using the installer.
Macros (and staging) are a solved problem in quite a few languages. In particular, Racket is a very demo of how well and cleanly it can be done. I don't think I've ever seen an implementation of "program-the-compiler-itself-in-the-language-its-compiling" be fast enough. (I'm not saying it couldn't possibly be done, but AFAICT it's a research-level problem.)
Lookup $PATH environment variable on [this page](https://en.wikipedia.org/wiki/Environment_variable). You need to make sure this env variable on your system contains the actual path to your SBT bin folder.
**Environment variable** An environment variable is a dynamic-named value that can affect the way running processes will behave on a computer. They are part of the environment in which a process runs. For example, a running process can query the value of the TEMP environment variable to discover a suitable location to store temporary files, or the HOME or USERPROFILE variable to find the directory structure owned by the user running the process. They were introduced in their modern form in 1979 with Version 7 Unix, so are included in all Unix operating system flavors and variants from that point onward including Linux and macOS. From PC DOS 2.0 in 1982, all succeeding Microsoft operating systems including Microsoft Windows, and OS/2 also have included them as a feature, although with somewhat different syntax, usage and standard variable names. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Just tried it, the entire sbt folder is in system variables.
On second look I might be wrong. I assumed a java9 issue. I think the other poster asking about the path is right.
I'd recommend hooking your favourite JVM profiler up to the sbt process and asking it where the time is being spent.
Thank you!
Have you tried only the bin folder? The entire sbt folder may be whats causing the issues. 
Right so your solution is to do the same thing, but at runtime, without type checking. I actually don't disagree that that is simpler in terms of the specific bit of code. However, the reason to do this is that with a fairly complex codebase, it becomes much simpler to use case classes to manage schemas. And once you're in that world, it's a lot better to not return to using dataframe-typed functions, because dataframes (as an untyped/dynamically typed API) are part of the problem. The criticism that this adds boilerplate in this case is valid - I'm not even sure I like the code, and I wrote it. However, it's better than more dataframe-typed functions, and it's better than runtime reflection, because I hate it when my etl breaks at runtime (which usually means after I've launched it and I'm doing something else). 
Yeah and I also tried only the jar file. Didn't work.
Coursera would by my recommendation. You can do everything but the certification for free and very in depth.
That VS code integration looks amazing :O
Hi all, I'm curious what's your opinion on [type all the things](http://jto.github.io/articles/type-all-the-things/) (the first paragraph) approach? This is something we've been doing in my previous company and we were more than happy. We used to have a separate, lightweight type (value class or tagged type) representing various entities in our system, e.g. user's age had a different type than user's tax identification number (even though the underlying value was still an integer). Even though it required writing more code, we liked the benefits: increased type safety, more precise validations, impossibility to represent a lot of illegal states, method signatures that often tell you all without looking at the name of the method, etc. Refactoring of such code was also a breeze - compiler was informing you of every place in your codebase where something "isn't clicking". In my current company however, we're not using that approach. I've tried to convince my colleagues and the team leader, but the response usually was one of: "too much boilerplate", "too complex", "not beneficial enough", "unnecessary performance penalty", etc. That's why I have a couple of questions to all of you: 1. What's your opinion on *type all the things* approach? 2. Do you use it in your commercial codebases? 3. Are you concerned about the performance penalty of using it? 4. Do you think that even non-domain entities should have their own type? For example, should Kafka's port have a different type than PostgreSQL's port? Thanks
(1,2,3) zip (a,b,c) will return ((1,a),(2,b),(3,c)) Pseudo code since im on mobile. Useful in other lnguages too
Like what? I like R
Right, but casting forward and back to dataset is safe because you already know the types. &gt; For a caller there is no difference in type safety in this case, you are just coming up with the right column names to select I'm not sure what you mean here? The caller has a typed interface rather than an untyped interface. 
How long does `sbt compile` take?
The community edition is open source.
I like R too, but IIRC I had to write a graph implementation and implement breadth first search. I'm sure there's some idiomatic / straightforward way to do this kind of work in R, but I never had to learn it and R isn't what you'd reach for anyway.
There is a new update of SBT (1.1.0.1) which the fixes installer for Windows (I assume you are on Windows). There was an issue with an environment variable in .bat file. If you want to change it yourself, change line 132 in &lt;path_to_sbt&gt;/bin/sbt.bat to: set rtexport=!SBT_HOME!java9-rt-export.jar
So? It's pretty much worthless unless you use one of the languages it supports. It doesn't even support javascript (or typescript). Also, it's completely fragmented. If I want to work on javascript, PHP, and scala (with the play framework which is not supported in CE), what do I do? Do I pay for WebStorm, PHPStorm, and Idea Ultimate Edition? Or do I just need Ultimate Edition? Hell if I know. I also have no idea if the only difference between these IDEs in the plugins or if the editor itself. It just doesn't make sense to me. I use Idea for scala only ATM. I can't wait until I can switch to something else.
You do have a good point about the fragmented products lines. I don't know why they can't combine everything into one IDE.
under two minutes *if cleaned first*, otherwise around twenty seconds.
Can I use the LSP in Emacs? Ensime offen loose response.
First of all, thanks for your answer. What is SAM?
SAM = single abstract method It's an interface defined by a single method with no default implementation. A relatively recently development is to be able to use lambdas anywhere a SAM was expected rather than explicitly creating a full anonymous instance an overriding the method. This generally only works provided all the types can be figured out and may require a compilation flag or not be supported at all depending on how old your version of Scala is.
What's your merge strategy for `reference.conf`/`application.conf`? You're probably discarding configuration file.
1. It's the right thing to do. 2. Yes. 3. The performance penalty is negligible to non-existent in most cases. 4. It does not hurt, so why not. In short, you're right, your colleagues are wrong. There is some boilerplate, but it's not much. Literally one line per case class. 
play framework project works perfectly fine in IDEA CE if you open it as sbt project
Perfect! Worked
Now I'm getting this thing Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 [warn] No sbt.version set in project/build.properties, base directory: C:\WINDOWS\system32 [info] Loading settings from idea.sbt ... [info] Loading global plugins from C:\Users\Sherik\.sbt\1.0\plugins [info] Set current project to system32 (in build file:/C:/Windows/System32/) [info] sbt server started at local:sbt-server-15f4e5ec25bc72c1619b Exception in thread "sbt-socket-server" java.lang.RuntimeException: Could not create directory C:\Windows\System32\.\project\target at scala.sys.package$.error(package.scala:27) at sbt.io.IO$.createDirectory(IO.scala:227) at sbt.io.OpenFile.open(Using.scala:42) at sbt.io.OpenFile.open$(Using.scala:39) at sbt.io.Using$$anon$2.open(Using.scala:75) at sbt.io.Using$$anon$2.open(Using.scala:75) at sbt.io.Using.apply(Using.scala:21) at sbt.io.IO$.writer(IO.scala:767) at sbt.io.IO$.write(IO.scala:761) at sbt.internal.server.Server$$anon$2.sbt$internal$server$Server$$anon$$writePortfile(Server.scala:182) at sbt.internal.server.Server$$anon$2$$anon$1.run(Server.scala:75) [warn] sbt server could not start in 10s
What are the features you miss?
You can use everything in IntelliJ IDEA Ultimate Edition: HTML, JS, CSS (and its variants, LESS, Stylus and others), Java, Scala, Php, Go, Ruby, Python with official support plus a bunch of open source plugins provided by the community such as Erlang, Elixir, Rust and Others. 
thanks a lot.
&gt; If have a program that is constrained to Monad but are attempting to selectively constrain parts of the program to applicative, how do you know what to tag with Free and what to tag with FreeAp? Hopefully you-the-programmer know which stages are meant to be parallelizable and which are meant to execute serially. &gt; And if you could know such a thing, why bother with a different tag? So that it's visible in the code and the compiler can check your assumptions, and you avoid accidentally making things serial that were supposed to be parallel. (I tried to make this more explicit and less intimidating with [tierney](https://github.com/m50d/tierney), which is more-or-less the same construction as `Free[FreeAp[F,?], ?]`, though who knows whether I succeeded)
Now I'm getting this thing Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 [warn] No sbt.version set in project/build.properties, base directory: C:\WINDOWS\system32 [info] Loading settings from idea.sbt ... [info] Loading global plugins from C:\Users\1.0\plugins [info] Set current project to system32 (in build file:/C:/Windows/System32/) [info] sbt server started at local:sbt-server-15f4e5ec25bc72c1619b Exception in thread "sbt-socket-server" java.lang.RuntimeException: Could not create directory C:\Windows\System32.\project\target at scala.sys.package$.error(package.scala:27) at sbt.io.IO$.createDirectory(IO.scala:227) at sbt.io.OpenFile.open(Using.scala:42) at sbt.io.OpenFile.open$(Using.scala:39) at sbt.io.Using$$anon$2.open(Using.scala:75) at sbt.io.Using$$anon$2.open(Using.scala:75) at sbt.io.Using.apply(Using.scala:21) at sbt.io.IO$.writer(IO.scala:767) at sbt.io.IO$.write(IO.scala:761) at sbt.internal.server.Server$$anon$2.sbt$internal$server$Server$$anon$$writePortfile(Server.scala:182) at sbt.internal.server.Server$$anon$2$$anon$1.run(Server.scala:75) [warn] sbt server could not start in 10s 
Try adding this to your build.sbt dependencies: "com.typesafe" % "config" % "1.3.2" Explicitly including this in my dependencies fixed an issue similar to this before so doesn't hurt to try it.
Seems like you are starting SBT in the System32 folder, which you should not do. You should start it in a project folder. If you really want to start in System32. You can always run your process as Administrator :P
Alright! Thanks man, it's running when I changed folders.
http://docs.scala-lang.org/ seems to be a good start, then http://danielwestheide.com/scala/neophytes.html is a excellent web site. This one could be very useful too : https://github.com/alexandru/scala-best-practices/
It's missing support for several things. Line twirl templates. It's just another thing that's confusing. In CE it works for the most part but a few random features only work in USE.
Were all the services running in docker, on single machine, at the same time, while the tests were executed? If this is the case, the results are unreliable.
Yay for Spark! Love that little framework 
Try `(style: EggStyle)` ... the type name is uppercased.
have you tried explicitly depending on Akka
Thank you (:
The only solution i can think of is contacting the developer or the maintainer about the combination that you need and offer to implement it, almost a 100% of the time they'll guide you through the migration.
I started doing something similar to the 'type all the things' approach by using Slick's MappedTo utility to make types for database fields. It has proven to be really useful in refactoring. No more passing around longs and hoping you didn't accidentally switch your user ID and your company ID.
I may be repeating myself, but "tastefully", "clean", "usable" are all absurdly subjective. :) Just for context, I find typeclass-based code generation (via implicit parameters) absolutely horrific and impenetrable to reasoning about the generated code. Why, well because there *is* no generated code to inspect! (Likewise in Template Haskell, FWIW. Servant is... highly non-trivial, let's just say. Template Haskell isn't *much* better, but at least there's a compiler switch to let you show the generated code. I can't recall, does scalac have an option to dump generated code? I *think* I tried that once and got &gt;100k lines output, but I think I'm very likely mistaken.) At least with down-to-earth "generate actual files" you get: a) proper change tracking, and b) penetrability. EDIT: and c) CACHING. This is a big thing for modern build tools like Bazel, Pants, Shake, etc. I think that probably concludes our thread. It's always interesting to hear things from a different perspective :). Maybe interestingly, I used to actually *agree* with you -- solely on the "purity" basis -- but as I've grown more experienced I find that I *really* do want an easy way to debug, i.e. inspect the actual code. Probably because as "the expert" I always get called out on those "it could take anywhere from 30 minutes to 2 years" jobs. Sometimes they're fun, but most of the time they're just well-paying :/.
That's an interesting PoV, but it's not really about "generics" is it? Tuples != Generics. Tuples are (AFAIUI) actually basically at their core syntactically special-cased Rank-N types. Of course, Haskell botched this badly, but whatever.
&gt; (It's relatively recent and may still require a compiler flag) SAM adaptation is on by default in Scala 2.12
Yes! I was actually thinking of adding this.
EDIT: I agree that "company" may be an issue. I run my own company, and frankly if you're not making demands in terms of code standards you're you aiming for mediocrity... which may be lucrative, but I believe excellence may be even more lucrative. We'll see, I guess. I believe in "my" path :). There are *very* few people who actually need to work with code-generating macros, so... what's your problem with that? The objections you raised seem... sort-of-hypothetical? About IDEA: IntelliJ has *its own* code model and who knows how accurate it is? Hint: Usually quite good, but sometimes, it's not even in the ball park, especially with advanced Scala type-level features like `\|/' from scalaz). This is not friction we want to have. Embracing macros *as syntax* could actually make things better becuase it would "just" be a matter of the IDE running the code. We'll see, I guess :). EDIT#n: I'm going to state this: There's **NOTHING** outside of code generation that's actually worthwhile outside of actual Dependent Types (or similar). For the Go-haters: Of course generic is on the spectrum of "dependent" types.
&gt; though stepping through Scala implicits also does work, since at runtime they're no different from normal method calls. It's macros that have me worried. No, it doesn't work. That's what you discovered in that very session! It's an absurd amount of manual effort for little-to-no gain. Macros are just code generation and if you integrate them *cleanly* into the host language you can always *ask* for the explicit code. (Racket and most LISPs/Schemes do this. Even MetaOCaml does this, I think.)
Who still uses or chooses Spring? 
Thanks for the links! Will check these out.
Working cross-build in maven? Excellent, will be switching straight away. Can I use it to publish libraries for scala.js as well?
How would that effect the memory usage?
The vast, vast majority. It's a safe bet that finding a developer that has experience with Spring is about ten times easier than any other jvm framework, and that's what managers care about typically.
Most used is this one but the reviews are pretty bad: https://marketplace.visualstudio.com/items?itemName=itryapitsin.Scala#review-details If you don't want to use that one, there's plugins for Scala syntax, sbt, and scalafmt.
I haven't tried it myself yet but in the release notes (https://developer.lightbend.com/blog/2017-11-30-sbt-1-1-0-RC1-sbt-server/?final?final#vs-code-extension) for sbt 1.1.0, they announced the availability of a new VS plugin that uses the new sbt server functionality: https://marketplace.visualstudio.com/items?itemName=lightbend.vscode-sbt-scala Seems like it's worth a try.
Ok for the sake of argument I don't find this article very convincing. I've never seen a bug where someone accidently passed say a zip code instead of an age. Naming params or case classes seems to be clear enough. Ultimately you still have the construct the classes from raw data. There is nothing in the article stop you from doing User.LastName("867-5309"). If you can validate data before constructing the class or provide type-specific methods then it becomes useful, but there is a lot of stuff that is just string data for reading. There are limits to what you can do to narrow that down.
I think this should be posted as a thread, I would love to see a discussion on this. 
I have been using this and it works pretty well.
Does it allow jumping to method/type definitions that originate from artifacts, by any chance ? 
1. I love it, just wish it were zero-cost 1. As much as possible, but not everywhere, due to older systems and lack of a cohesive set of guidelines for the team/org so even new things may or may not have it depending on who implemented it 1. I am, because I value writing efficient software, but I acknowledge that absolute performance is not the only factor, especially for a business 1. I think a type per logical notion and a type to wrap primitives is most useful with a low cost/benefit ratio. In your example I'd say a `Port` is the same concept regardless of whether the connection is to Kafka or to Postgres. Using something like `Kafka.Port` or `Port[Kafka]` or `KafkaConfig[Port]` is still useful, but I think has quickly diminishing returns in terms of complexity and required infrastructure for the problem it solves.
&gt; I've never seen a bug where someone accidently passed say a zip code instead of an age I've personally dealt with bugs where one `Int` parameter was milliseconds and the next was seconds, or were for two different timeouts (e.g. socket vs connection timeout), and the raw values were swapped at the call site. It happens. The likelihood of the bug slipping by both the author and the reviewers is much less had the call site read `...(SocketTimeout(30.millis), IdleTimeout(30.seconds))` instead of `...(30000, 30000)`. Which parameter is which and did you accidentally set one to 30 seconds instead of millis, or set one 30k seconds? Even if you moved the typed parameters out to named variables, you couldn't accidentally swap them (`...(socketTO, idleTO)` vs `...(idleTO, socketTO)`) the way you could with raw primitives. &gt; Naming params. . .seems to be clear enough That certainly helps, but naming params in Scala is optional while types are required, and some languages don't support named params at all. You can't even do it calling a Java method from Scala. Using types to represent these notions solves a more general problem than just something in Scala. &gt; case classes / If you can validate data before constructing the class or provide type-specific methods then it becomes useful That's exactly the kind of thing the article supports and the point it's trying to make! &gt; here is nothing in the article stop you from doing User.LastName("867-5309") No, but it does help with something like `new User(u.firstName, u.firstName, u.address)`, where those parameters could be swapped or duplicated and not caught by the compiler. &gt; Ultimately you still have the construct the classes from raw data. True! But by lifting a value into a specific type you are asserting something stronger and statically verifiable about the value compared to just using a variable name. Doing so can reduce the occurrences of certain bugs.
I'm not saying tuples == generics, I'm saying that tuples are generic types. As for parametric polymorphism and generic methods, I don't think I understand your distinction -- aren't they the same thing? As far as I understand, generic methods (only methods can be generic in scala, functions cannot be parametric) are the OOP equivalent of parametric functions, right?
Here be dragons... The scala IDE situation is quite possibly the worst out of all the industrial languages in existence. They're all terrible, especially vscode or atom. Just use intellij and hope for the best. 
It does jump to class in the same project, that's it as far as I know.
"we love the jvm and debugging complex jvm issues"
Units of measure make sense. Hours are different from miles, and you should be able to convert hours to seconds but not to miles. Its more the dogmatic "wrap everything" position I'm not convinced of. Eg case class FirstName(value: String) extends AnyVal case class LastName(value: String) extends AnyVal No extra functionality, not much you can do by way of validation. Moreover, is LastName("John") == FirstName("John") ? It's an interesting topic.
For that specific case, you can add functionality as methods of the wrapper class, and I'd argue you _do_ get validation in the form of static guarantees. Just by using the wrapper I know that only the strings I've definitively declared as first names can be passed into a method expecting `FirstName`, which significantly reduces the scope of things that can be names from "any `String` in the system" to "usages of `FirstName`". Yes, it's not a runtime validation of the content of the name, but that's a separate concern. I can see how such contrived examples might not convince you though. All I can say is I've found value in the practice in real projects and encourage you to try it. Maybe you'd find something you didn't expect. Maybe you find that it's not worth while and could then articulate the issues you found to the community.
&gt; Have you ever had to delve into the depths of the JVM implementation or your OS implementation? If you have enough experience in the industry, then probably yes, but it is really rare. FTR I've hit one bug in native JVM code (more in the parts of the JDK that are just written in Java) and a couple in OS libraries, but sure. &gt; I'm of the opinion that a similar thing applies to most code-generation tools. Sure, they may occasionally need to be "upgraded" to avoid new compiler warnings (or whatever), but IME it's very very rare for more than two or three people having to know the internals of a code generator[1]. I just don't think this stuff's quite that obscure. Every big codebase I've seen has needed to do some kind of semi-custom walk of the object graph. I've seen it done in Python by just accessing the object dictionary, in Java with direct reflection, using commons-beanutils, or using some xml-driven library that I can't now find, in Scala using shapeless or using a macro. The only codebase that avoided it was a Python one that kept its data values in nested maps/tuples rather than using classes, and that gives you the equal and opposite problem of validating that the expected structure is there. The JSON serialization case is maybe at a level where you can use an off-the-shelf serializer and not customize it, but for comparisons or parsers or database mapping you either need a custom walk, or so much "configuration" of the library/generator as to be more complex than doing it yourself. I find shapeless as nice as anything else the simple case (when one just extends `LabelledProductTypeClassCompanion`) and would say it scales up more neatly to the complex case (I mean one can just copy `LabelledProductTypeClassCompanion` and start customizing the code) while retaining type safety throughout, but eh, I guess that stuff's subjective.
You owe me a new non-sticky keyboard....
Tons of people, currently stuck in such a role. So dull and having the same problems as every other Spring project: "Something we want/in our environment is different to what Spring dictates" and attempting to fix anything with the humongous stack traces...
Everything from underscore.io
Have been using Scala IDE for about 6 years now. While the writing may be on the wall (i.e. Lightbend dropping support for it), it performs admirably with few spurious errors, something I've heard plagues IntelliJ Scala. The key is to let sbt run the show and not build from within the IDE itself (i.e. just use the IDE as the presentation compiler). Use *sbteclipse* to generate the project files and you're good to go. The future is apparently Dotty + vscode, or at least that's what Dotty is targeting as officially supported IDE/editor out of the gate. Personally going to ride with Scala IDE until Dotty lands.
https://scala.epfl.ch/#education
It's possible to do the coursera modules individually for free. 
&gt;I also looked at Haskell, but I felt more confident with a JVM compiled language that could use all the existing Java libraries. Boy is he going to be stoked to hear about Eta!
what you want to learn? better Java? or Functional programming?
I'm curious about this too. Is there something I'm missing out on. Because, in my experience, I find IntelliJ to be an amazing code editor; e.g. expanding selection and selecting next occurence with multiple cursors is extremely efficient. I've tried to look at some"can your editor do **this?****-videoes, and they all look inferior to intellij. But if there's something I'm missing out on; I wanna know! :o
That's probably my biggest compaint about IntelliJ too. It's confusing with so many almost identical products. They should just fuse them together already and take money for plugin (packs) instead. I'm still a huge IntelliJ fan though. Just wish they'd clean up the product line a bit for their own sake.
lol. I disagree. Scala gives you the tools to program functionally, but without leaving the real world which (sadly?) consists of objects and side-effects. You want to use existing libraries? Well you need to include side-effects then; welcome to the real world. The alternative is to live in an isolated empty trumanshow-universe like Haskell. Yes there's no side-effects, but there isn't much else either. :p
I have created free learn-by-doing course for functional programming on scala. It is test driven - all tests are already written, and you have to implement stubs for functor, monoid, applicative, monad, monad transformer, traversable/foldable, free monad and pass that tests. Check it out at https://github.com/dehun/learn-fp/ It is more into advanced Scala though
&gt; I can see how such contrived examples might not convince you though Not the GP, but I don't think that it is a contrived example, these are exactly the kinds of domain objects many applications have to deal with for example during user registration of a web app. 
That's fair. Perhaps I should have said "small" or "simple" or "without larger context" or something along those lines.
I want to learn functional programming
cool! would be better to do it with EduTools plugin https://plugins.jetbrains.com/plugin/10081-edutools
I'm sorry you feel that way, but you were wrong in nearly every portion of your response, and I encourage you to read more about either language so that you can make informed comments in the future.
Typical of the Scala community to only see the glass half empty. I'd say Scala easily has the best IDEs for an independent language not backed by a huge organization. &gt; quite possibly the worst out of all the industrial languages in existence For one thing, I don't even think this is true. Industrial languages like C++ or COBOL have had it worse for a very long time. IntelliJ is actually great. When I use it I disable error highlighting, because it just doesn't work reliably â€“ I think this feature is overrated anyway (personally I don't like to be bothered by the IDE losing its mind because I haven't finished writing an expression). Most other IntelliJ functionalities (and there are plenty) are fairly reliable. Quoting [a previous comment](https://www.reddit.com/r/scala/comments/76zj1g/in_what_context_can_scala_be_a_reasonable_choice/doii27k/) of mine: &gt; Quoting [a previous comment](https://www.reddit.com/r/scala/comments/62ndwf/what_do_macro_cats_shapeless_etc_users_do_about/dfp3d21/?utm_content=permalink&amp;utm_medium=user&amp;utm_source=reddit&amp;utm_name=frontpage) of mine: &gt; &gt; &gt; I [...] disable error reporting for problematic files only. &gt; &gt; &gt; I still think IntelliJ is a net win, for: instant variable/field/class renaming; finding definitions, usages and overridden versions of any declarations in a keystroke; indexing the whole code base so I can instantly jump to any declared class/object/method; the nice grepping capabilities; showing scala/java-doc; auto-complete; etc. &gt; &gt; &gt; You can even still get correct type information from a shortcut in many cases, and get at least approximate type info most of the time. 
Please resubmit this question with a lot more details. We have no idea what you're talking about. What kind of application are you running? What are you trying to do? What sbt command did you run? What have you tried, in attempts to solve this? What are you even talking about??
Look to the right block on this subreddit.
Exactly! ðŸ˜Š 
What version of SBT are you using?
This example is testing a dual clock FIFO by forking 3 simulation threads : - One to generate the two clocks - One to push data into the FIFO - One to pop data from the FIFO and check the value The `fork` is using coroutine instead of the native JVM threading. The reason is that their are way much faster to switch context.
Sure, there *are* bugs in code generators, JVMs or what have you. What happens is this: We/they fix them *once* and suddenly they're fixed *forever more*. That's the value of ratcheting (or, "abstraction", if you will.) Honestly, I think shapeless may be similar (modulo performance), but is just that it's much less *direct* when compared to macros or code generation, and that really irks me (and requires more IDE support). I don't mean to pick on shapeless in particular -- this is basically about the implicit search/class generation which enables shapeless to work. Incidentally, you can see similar "tricks" employed in Haskell in e.g. Servant, etc. but I believe that *if* we want to move in this direction then we should just go all the way and go to full dependent types. &gt; (I mean one can just copy LabelledProductTypeClassCompanion and start customizing the code) while retaining type safety throughout, but eh, I guess that stuff's subjective. I'm not sure if you're being sarcastic, but that's true, I think. This is subjective.
then I think underscore's "Scala with Cats" is a good start. it's free and up-to-date with Cats which is one of top functional lib in Scala.
The latest one.
Cant you merge them using a recursive function?
It should be. That class is not a joke. 
I know there are a lot of way to do it, but I'm hoping for a one-liner method, something like: Stream.pullFrom(A,B).orderBy(_&lt;_)
I'll post one, but I don't think you read the question, no offense. Anyway, I'm not the one who downvoted you, just fyi.
I have never personally hired someone because they had a certificate. IMHO if it would help you learn spark then it might be useful but the actual certificate itself is not necessarily very useful. In the last two years I've done 57 interviews personally and my team has hired 15 devs who work on a 90% scala code base.
You shouldn't have to compromise quality on some insecure person's say-so. Sounds like you need to discuss this with someone above him. Also, maybe seek advice on the Workplace Stack Exchange.
&gt; "I've seen what happens when teams write too many tests." Get out. An institutional mistrust of automated tests is probably the strongest signal that over time, working with this codebase is going to feel more and more like banging your head against a brick wall. If getting out isn't an option, I'd recommend letting it go. Do your best to make sure the code you're responsible for is as clean and solid as possible, but don't waste emotional energy on battles you can't win, and don't create more work for other people by breaking team conventions. Regular meditation might help too.
What the other people are saying is worth listening to. I'd also like to add, most companies doing Scala don't really do FP. If they say they're doing FP, be very skeptical. Usually they mean they just like using lambdas, or they pulled in some libs that claim to be functional... regardless of how true that is. For example, some companies would say they're functional because they use akka-http. Akka-http barely scratches the surface of functional programming, and isn't easy to write functional abstractions for. Directives are Monads, but not really. Well, technically they should be MonadErrors but they're not, because they have nonsensical constraints. This means if you're really wanting to do functional programming and use use akka-http, you'll just wind up confused. 
&gt; Sounds like you need to discuss this with someone above him. I started that process about 2 months ago and spoke with the VP who oversees about 5 teams including ours, after another incident with this manager. The VP has been very supportive so far. Based on project timelines, the natural time for me to switch teams would be about 2-3 months from now. I could probably switch earlier if I had to, but my current plan would put me on a Scala team where I know they care a lot more about best practices. I intend to actually set that plan in motion during our next 1-on-1 (me + VP) which is coming up shortly. &gt; seek advice on the Workplace Stack Exchange. That sounds like a great idea! I've seen Stack Overflow, the Gaming Stack Exchange, and several others, but for some reason it never occurred to me that a Workplace one might exist.
&gt; In my experience, an institutional mistrust of automated tests is one of the strongest signals that code quality just isn't a priority, and if it's something you care about you're going to have a bad time. Thankfully, it's not the entire origination. The organization generally gives teams a lot of freedom. Some of the teams use this well, enforcing high standards, and experimenting in reasonable ways (which is the story of how Scala got started there). A few teams don't use that well, and turn out like the one I'm on. My current "get out" plan is to switch teams, but I always keep my interview skills and resume polished enough that switching companies at any time isn't a big deal. &gt; At work, do your best to make sure the code you're responsible for is as clean and solid as possible, but don't waste emotional energy on battles you can't win, and don't create more work for other people by breaking team conventions. It's good to hear that. That is mostly what I intended to do, but it's also good to hear someone else say it.
&gt; I'm looking for good intermediate resources for learning Scala best practices including patterns, code-smells, what to do instead of the code-smell. I think the best resource for you would be to buy and read [FP in Scala](https://www.manning.com/books/functional-programming-in-scala). Also this is a good list of anti-patterns: http://www.wartremover.org/doc/warts.html &gt; Do you have any advice, or experience dealing with someone who likes Scala, but is constantly trash-talking best practices? Understand them fully, then argue honestly and logically against their understanding. (When I say *honestly*, I mean that once you fully understand their PoV, you should be willing to accept that maybe they have a valid concern that you're not addressing. Often people just have different values in mind, say he/she's thinking project budget, you're thinking quality. In such a case trying to convince them that A is better than B for code quality is futile, but an argument that quality is important (because â€¦) and can be achieved without blowing the project budget is likely to be much more productive.)
Scala is a pretty crazy giant language. Since it has so many features it's really hard to find many consistent rules for how people should write Scala. Honestly the only way I've ever managed to convince anyone to change their coding style is just by leading by example. Work fast and produce quality code and sooner or later everyone has to kind of ask themselves why you're so much more productive than they are.
Which best practices are you trying to push, and how are you going about it? People don't like to change how they write software for its own sake. Even ignoring colleagues or management, you may not be right about the changes being an improvement, so be explicit about the costs of changes when proposing them, not just the expected benefits. A common technique in proposal docs is to write an FAQ section with questions the readers might ask -- it prepares you for when they do ask, and it shows that you've thought about different sides of the issue. It's also easier to introduce systemic changes as a response to current problems (bugs, operational pain, etc) or as a means of accelerating some other upcoming work.
&gt; Sounds like you need to discuss this with someone above him. Well, um, this is almost certainly going to burn any bridges with that person, and might very well contribute to difficulties with any other person working on the team. In general that's a pretty heavy-handed maneuver.
&gt; Scala is a pretty crazy giant language. Since it has so many features it's really hard to find many consistent rules for how people should write Scala. Yeah, I'm pretty skeptical about "code smell". Scala being a huge language, there is usually more than one way to do it. To pretend that one knows the "right" way, among several alternatives, is quite a leap.
OP, you come off as bright, but a little full of yourself. Maybe you can consider that it's possible you don't know what's best for everybody else. Something to think about. 
That's one of the jobs of managersâ€”to solve problems like this. Going to HR would be heavy-handed.
&gt; That's one of the jobs of managersâ€”to solve problems like this. Maybe so, but it's OP's manager that needs to be convinced about that, and it seems unlikely he/she will view it that way.
&gt; Also, maybe seek advice on the Workplace Stack Exchange. I second this recommendation. From what I've seen, the Workplace SE has good discussions of these problems.
I love this story. Iâ€™ll bet you shipped on time and on budget, perhaps more because of and not in spite of your rebellion. Good work!
Did you have to target scala 2.11 because the continuations plugins is otherwise dead?
There has not been a new episode in more than a year but https://twitter.com/thescalawags The podcasts are still available on most major services.
Think 2 of the members (Dick Wall and Josh Sueret) are no longer working in Scala; that or nobody has time to meetup to record another episode. Would be cool if they started up again, I liked the mix of technical, political, and social worlds all blended together.
&gt; Do you have any advice, or experience dealing with someone who likes Scala, but is constantly trash-talking best practices? If you have some minimum knowledge of C and C++, I can recommend you this talk: https://www.youtube.com/watch?v=D7Sd8A6_fYU It makes no mention of Scala, but it's the psychological tricks that an old contractor/mentor in the embedded space uses to convince people to switch from C to C++ (and from bad practices in C++ to better practices in C++). It also goes into details as to why it's hard to make people change their behaviors and biases (be that coding style, library, language, favorite political party or eating habits) and how to take a more "zen" attitude about the subject as to not go mad whilst trying to do so. Sadly enough, making a team or even a single person change their coding style (even assuming all the changes you introduce are 100% positive), is going to range from hard to impossible depending on the team and person, that's just how human brains work (yours included), sorry :/ Also, remember that you are biased and are presenting the problem assuming you are 100% correct and the manager is 100% incorrect in his beliefs, which is likely not the object truth (if we can even define an objective truth). Generally speaking it serves well to see where your teammates are "right" and admitting defeat and encouraging them in that direction, since they will feel better about giving way to your advice in another direction.
There's a sorting algorithm called the merge sort that involves recursively splitting lists in the middle until you have two elements, putting them back in order, and building the list back up. It's fairly efficient, and is really easy to implement in Scala : https://en.wikipedia.org/wiki/Merge_sort 
Highly recommend Lightbend. It's terrible audio, but very high quality content. 
This is a great podcast with some of its episodes related to Scala: https://www.functionalgeekery.com 
Uh, we can avoid the potential confusions of postfixOps and still get really nice aesthetics with just normal method call syntax. Moreover, as you mention, compound expressions.
This is a really good description of scala, I'm totally stealing it!
I like your well thought out response. Tysvm for posting. Honestly, if I have to go back to Java (from Scala) for some reason, I'd WAY prefer Koitlin over Java 8 (and even 9). I'd still be fairly suspicious of Koitlin's longevity. However, if the number of Android starts is really as high as your indicating, then if I was to need to bounce to Android, I'd seek to start it in Koitlin (assuming I found the Scala pathway too difficult - I'll always be biased choosing Scala over either Java or Koitlin). I'm not opposed to Koitlin, per se. I'm opposed to yet another Groovy. If Koitlin can be a substantially better Groovy, then I'm more open to exploring Koitlin. And, honestly, Koitlin will just be the gateway drug as an imperative Java coder leaves the safety of "home" and begins an expolration of OOP+FP using Koitlin as a stepping stone to Scala. Said a slightly different way... Koitlin is a great marketing tool for Scala as a coder moves away from imperative Java.
Hi all Could someone explain me, for what is `https://github.com/non/kind-projector`good for? Thanks
&gt; most companies doing Scala don't really do FP. I definitely ran into that in Java; I'd start showing people Streams, Lambdas, Optional, etc and I'd start to hear "I know Functional Programming" which would sometimes lead to "I'm writing FP in Java, why would I need Scala?" Cringe aside, their heart was usually in the right place which is why I'd tactfully remind them that this is barely scratching the surface of FP, while encouraging them on their progress so far and continued learning. You make a really good point here (in context of my current team) because while the code is Scala, it's an extremely long ways off from functional programming. The code reminds me of Spring Hibernate web-apps. If this manager's only intent is to use Scala as nicer Java, then pushing for any FP standards might pushing him outside his comfort zone. It's not just FP that he resists, pretty much any change like "don't use Strings for ids" seems to rub him the wrong way. Thinking ahead; if I'm to interview for a Scala team in the future, I will definitely dig into "are you doing Scala or are you doing FP?" Similarly, I would expect those teams to screen candidates the same way, which is why I'm going study this subject more in my free time.
&gt; FP in Scala. I apparently already bought that book, I should probably read it. :) &gt; http://www.wartremover.org/doc/warts.html I'll give that a read. &gt; Understand them fully I'll give this more effort. Asking questions (or Socratic method) is probably a much better way to handle this situation than what I've been doing (avoiding futile arguments). I might even learn something.
There are certainly many ways to go above someone in a "heavy-handed bridge burning" way. There are also ways to be far more tactful when going above someone. For example, I did go above him after an extremely rude comment, but when talking to the VP I mostly emphasized that I wasn't looking to have him punished, and I had empathy for his stress and difficult position, but his behavior needed to stop. I'd probably never go above my manager for something like enforcing some programming standard (like testing, etc), but rather try to promote these standards within the engineering department as a whole. For my current situation, going above him is mostly going to be focused on how I'd be a great match for this other team, and not how much I dislike working with this manager.
It's incredibly important also to prepare for the case where certain co-workers or parts of management do not care the slightest bit about doing a good job or the interests of the company. And there are "professional" programmers who prefer not to learn anything, that others learn and improve, or that the code base is in a good shape. I am not talking about issues such as functional programming, but incredibly simple and fundamental things like doing code review, automated tests, learning and knowing about how to use the existing technologies in the system and how the language(s) you program in works in regards to the features of the language being widely used in the code base, etc. Being able to deal with such cases is very, very important (for instance by looking for another job elsewhere and silently inform top management about things once changing, using objective evidence of doings. I am not talking about doing this for things that would be nice or good to have; I am talking about if even the most basic and important things are neglected on purpose and where this negligence without a doubt is extremely detrimental to the company).
Code-smells aren't "bad" code, but rather a general sign that a problem may exist or there's a better way to solve the problem. &gt; To pretend that one knows the "right" way I will fully admit my Scala and FP knowledge is a long ways off from being able to pretend I know the "right" way. That said, throughout my history with Scala, nearly every book I've read, talk I've attended, and person I highly respect (with far more FP background and skill than me and my teammates combined) in the Scala community has treated returning unit as a code-smell. Their reasoning for it being a code-smell (side-effects, mutable state, etc) also makes sense to me. Many years ago, I wrote a sizable and complex application, and nearly the entire application used global state and had zero automated tests. Arguably, the application worked fairly well and was relatively bug free. That said, I'm fairly sure the only reason it wasn't a disaster is because I was the only dev working on it, and you'd never catch me dead writing an app like that these days. I 100% empathize with flexibility around being free to solve problems in a variety of ways, and that rules, no matter how well intended, rarely get it right 100% of the time. However, at some point it makes sense to me that we should have some standards. * Maybe we don't need 100% code-coverage, but please write at least one test. * If you use var, you should have a good reason, and carefully considered the scope of what can mutate that var, and what threading issues it might cause. * Public functions should have names. Doesn't have to be a long name, but I don't know what `g(x: Int, r: Int)` is supposed to do.
&gt; Naah, the project was a disaster, but at least I knew it wasn't my fault. :) "Tis better to have happy coworkers and a dissaster project, than a successful project among coworkers who hate you" 
&gt; If your plan is to switch teams, that's even more of a reason to play it safe on the team you're on now. You're right. It's better if I can part ways having left a good impression, than potentially developing "enemies" in the organization regardless of whether or not I work directly with them anymore. &gt; The team I was on talked amongst ourselves and decided we would keep writing tests, and just not tell anyone (the teams were siloed so nobody else would be looking at our code too closely for months). Similar story: Several years ago I worked on a project that went into crunch-mode for months on end. Most of my coworkers worked nights and weekends for months on end, while I mostly kept my mouth shut and continued my usual routine of going home on time, enjoying my weekends, and getting proper sleep. The project was "complete" by the 5 month deadline, but then required 10 months of bug fixing and maintenance before it was remotely stable. In the mean-time, several coworkers began to notice I was far more productive and produced far fewer defects, which helped in securing a raise, and some job-references that have helped me in the years since.
Your instincts seem to be correct, and you managers position is typical of someone lacking in confidence, expertise, and emotional intelligence. He'd rather shutdown conversation than learn from someone. He is completely wrong to assert that the kinds of practices you suggest are "anal, strict, and outdated.". I've introduced Scala into a number of places over the last 10 years to so, and find that there are a whole range of reactions from the very enthusiastic to the hostile. I don't waste energy with the hostile any more. I suggest you post here more often with specific issues so that we can help you help your team. Encourage them to come here too.
&gt; Which best practices are you trying to push, and how are you going about it? For the moment, I've been mostly focused on extremely basic ones: * Avoiding returning unit * Being mindful of side effects * Using caution around var * Not using single-letter parameters, or names * Writing at least one test * Using Types (instead of String or Int) as appropriate. &gt; you may not be right about the changes being an improvement I can understand that. I do get the sense that a lot of the push-back I get is because he's used to working a certain way &amp; his responses often focus on conventions. &gt; It's also easier to introduce systemic changes as a response to current problems (bugs, operational pain, etc) or as a means of accelerating some other upcoming work. Good advice, I'll keep that in mind.
I'll listen to that talk shortly, it would probably make a good podcast while I'm working on other things. &gt; you are 100% correct and the manager is 100% incorrect in his beliefs I understand your point, but the "100% correct/incorrect" mindset is a stretch. There are plenty of areas where I understand each side has different pros and cons, and a degree of subjectivity. Even the best rules are typically "Wrong" (or inefficient) in some context. It's rare that I ever block a pull-request, and no matter how strongly I believe in my PR comment, I typically treat each one as trusting my coworkers to consider what I said and then do whatever they believe is best. &gt; object truth (if we can even define an objective truth). You're speaking to a nihilist. :) &gt; Sorry if the advice sounds a bit defeatist Not at all &gt; but you seem to have a human problem more than a code problem 100% Agree.
I don't know of anything like this specifically. I don't think there would be though. Specific Scala libraries that are more used would probably be something like spark or akka or Scala.js. But the thing is that Scalas big strength is the entire java ecosystem bring available. 
Yes, I know the merge sort, which will iterate through each element and sort them provided that the sublists are sorted. My intention is to have an interface that given collections of arbitrary size and arbitrary order, I could extract elements in sorted order from all them. Kpws answer above is closer to the intent.
https://to-ithaca.github.io/libra/
I had to look up what Anaconda was, and your description doesn't seem to match what it's website says. Either way from the Anaconda's website I think the closet analog would be Apache Spark.
I had to look up what Anaconda was, and your description doesn't seem to match what it's website says. Either way from the Anaconda's website I think the closet analog would be Apache Spark.
Anaconda as a distribution is made up of packages, mostly from the scientific python community (IMO) â€” Numpy, scipy, matplotlib, etc. Creating your own Anaconda-like thing in python is pretty easy, write a `requirements.txt`, slap your name on it, and youâ€™re golden (somewhat simplified). In the context of scala, there doesnâ€™t exactly exist a one-to-one mapping of python -&gt; scala packages, so you have to roll your own. Pick some packages you like and build a fatjar â€” push that to your artfactory and you have your own â€œdistributionâ€
Usually you can easily convert from iteration with mutable state to recursion with immutable data... in your case i'd do something like @tailrec def mergeSorted[T](a: List[T], b: List[T], m: List[T] = List.empty)(implicit ord: Ordering[T]): List[T] = (a, b) match { case ((headA :: restA), (headB :: restB)) =&gt; if (ord.gt(headA, headB)) mergeSorted(a, restB, m :+ headB) else mergeSorted(restA, b, m :+ headA) case _ =&gt; m ++ a ++ b } Note that if performance was a concern i'd have the list sorted in reverse orders to begin with then prepend instead of appending to the result. 
It basically lets you write something like `type EitherThrowableOr[R] = ({ type ER = Either[Throwable, R] })#ER` like `type EitherThrowableOr[R] = Either[Throwable, ?]`
There is typeclass Parallel for that kind of things
I tried writing a `Parallel[Future, Future]` but it didn't work. It still executed them at the same time. I think the natural transformations being the identity function messed up the transition, or the `Monad[future]` got used instead of the `Applicative[Future]`, or something.
The way the traverse instance is implemented in `cats` will run the Futures in parallel, because the futures will be instantiated before being flatmapped. If you want a proof that it's the case, run this : ```scala import scala.concurrent.{ Await, Future } import scala.concurrent.duration._ import scala.concurrent.ExecutionContext.Implicits.global import cats.implicits._ object Main extends App { val f: Int =&gt; Future[Unit] = i =&gt; Future { Thread.sleep(1000) println(i) } val before = System.currentTimeMillis() val _ = Await.ready((1 to 10).toList.traverse(f), 5.seconds) val after = System.currentTimeMillis() println(s"Ran for ${(after - before) / 1000.0} seconds") } ``` I got 2.209 seconds. If you want pure functions, and rely on an `IO monad` implementation rather than Future, you'll have to use `Parallel`, which basically compose a monad implementation and an applicative that wasn't derived from that monad, in order to discriminate between when to run things sequentially and when to run things in parallel. 
When using Futures if you sleep without adding the blocking { } block in a Future it the results you will get are kinda random as in theory all of them could end up on the same thread and sleep one after another instead of all at once. For example the snippet by /u/Baccata64 runs in ~2 but if you added blocking {} it should run in ~1s (mostly the same but using Future.sequence instead of cats traverse but the end result should be the same) https://scastie.scala-lang.org/nI4tKWuOQbOXWmZPsf45mQ
Very interesting, thank you
&gt; https://www.youtube.com/watch?v=D7Sd8A6_fYU Good talk overall; I'll definitely apply some of the things he mentioned. "If you're arguing, you're losing" There's a lot of truth to this; it's the main reason I've avoided arguing with him so far. 
What is the point of Kotlin Native? The only reason for Kotlin to exists is having good interop with Java and the JVM. Outside the JVM you have far better options to use.
I don't contribute to Scala Native but I think joining scala-native/scala-native and asking the very same question might do the trick :) From my experience people from open source communities are very friendly and don't mind hand holding new contributors.
Probably worth mentioning that [union](http://dotty.epfl.ch/docs/reference/union-types.html) and [intersection](http://dotty.epfl.ch/docs/reference/intersection-types.html) types are coming in as language features in Dotty
[Kilowatch-collector](https://gist.github.com/justinpitts/162e9ec7847ea09870c0b89e402e553e) is something I whipped up last weekend to make use of [ Itron AMR decoding](https://github.com/bemasher/rtlamr) and grab readings from my power meters. It is a [Ammonite Scala Script](http://ammonite.io/#ScalaScripts) and it uses [Doobie](http://tpolecat.github.io/doobie-scalaz-0.4.2/00-index.html) for the database insert. Thanks to Li and Rob.
Like Scala Native, Kotlin Native is trying to get away from being tied to the JVM. If you can use one language and compile to JVM, JavaScript, Native, WebAssembly, etc., that's a pretty big selling point in favor of that language. Rust, Swift, Go, ..., may be better native options, but if you have multiple platforms you're targeting then Scala/Kotlin Native may be the better choice (once they actually become production ready that is).
It still raises the question of why nobody does this in Scala. Would there be value in collecting a bunch of libraries (and their respective versions) that all work together?
&gt; Like many 'multi-paradigm' languages, Scala is a Rorschach test; the language you see is the one you're looking for. This is probably the core of the issue. We basically have competing philosophies of better Java, versus Functional programming. Technically speaking, "better Java" in Scala is no worse than your average Java application. Things like returning Unit may be poor Functional Programming, but it's no worse than a void return in Java (which most Java devs don't seem to mind). I think I might use this as part of how I approach the VP about switching teams. I don't want to walk in there complaining about my manager, instead I think there are opportunities to focus on how from a team culture perspective, how I'd be a better fit for this other team (which actually is interested in proper FP).
I understand. But while Scala is a marvelous language on its own, Kotlin, while being a fine language, has the as the main appeal being a nicer language than Java for Android application. Maybe I'm being too harsh in my judgement, but I don't see the need for Kotlin native while I'm anxious for a full functional Scala native compiler.
Thanks for the suggestion!
I have a fairly new podcast here: https://corecursive.com/ It is not only about scala, but its been fairly scala focused so far. The first 3 episodes I did as a guest host for software engineering daily, but I'm now doing my own thing. Check it out and let me know what you think. I have at new episode coming out soon about Functional and Reactive Domain Modeling with Debasish Ghosh and my last interview was with Runar Bjarnason about fp.
Scala podcast in Russian: http://scalalaz.ru/
I'm writing a [small programming language](github.com/mpardalos/Raza) as a learning project. I have never used Scala before ( or any functional language) but I'm trying as idiomatic code as I can.
I recently started working on [firebase4s](https://github.com/firebase4s/firebase4s), a Scala Firebase SDK. If you take a look at this [blog post](https://medium.com/@RICEaaron/scala-firebase-da433df93bd2) about using the Java SDK with Scala, you can see that there is a lot of boilerplate required. The biggest pain point is that you can't directly use case classes and need to first convert them to JavaBean classes when setting values (and vice versa when getting). My plan is to make everything more Scala-friendly and to enable to use of case classes via macros. 
I was building some ADT for representing filters for a collection, specifically I wanted to be able to support http query parameters like `key:true,anotherKey:*` and then use the typed filters to query my SQL schema, it has worked surprisingly well until now. I'm trying to get a clean and simple way to reject repeated filters, like `key:true,key=false`. The ADT: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/models/FixedPriceAlertFilter.scala The parser: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/parsers/FixedPriceAlertFilterParser.scala 
I was trying to port [sttp](https://github.com/softwaremill/sttp) to scala-js, but as it turns out there are quite a few things in the JDK that are still unavailable in scala-js (particularly, the `java.net` package, which can be [pretty hairy](https://github.com/softwaremill/sttp/issues/57) to rewrite (without looking at the original source that is)). 
Interesting approach. I see that `fromTypeConstructor` works with type constructors, but will it be able to correctly convert an `Either[String,Int]` into an `Either[String,Int|Boolean]`, or will it hit the limitations of higher-kinded type inference here?
I was thinking that it could infer `F[+T] = Either[String,T]`, but it's true that it would prevent upcasting to some type where both type parameters change.
Iâ€™ve been working on a library to generate scalacheck generators from regular expressions https://github.com/wolfendale/scalacheck-gen-regexp
It behaves the same way, of course, i.e., `foo(new A)` does not compile: https://scalafiddle.io/sf/jdi8bsR/0 The error message is even pretty good: ScalaFiddle.scala:10: error: type mismatch; found : ScalaFiddle.this.A required: js.this.$bar[ScalaFiddle.this.B,ScalaFiddle.this.C] foo(new A) // doesn't compile ^ and will be better in I-don't-know-which future version of scalac, which will pretty-print symbolic types as infix by default, which should therefore show: ScalaFiddle.scala:10: error: type mismatch; found : ScalaFiddle.this.A required: ScalaFiddle.this.B | ScalaFiddle.this.C foo(new A) // doesn't compile ^ 
Neat
&gt; His responses were argumentative, and mostly along the lines of these best practices being (paraphrased) "anal, strict, and outdated." This wasn't just limited to PRs or FP, any time best-practices in general are mentioned he often uses it to take pot-shots including "I've seen what happens when teams write too many tests." This is an office politics question. Managers, CTOs, and team leads always have their limits of experience and knowledge, but they have to maintain their authority or they will lose control of the team, which may cost them their job. I have found that the best way to operate as an individual contributor to push people toward a best practice that you know is to be strategic about it. Try to help your boss succeed instead of trying to run the team yourself. Know the difference between these two things. Don't get on your boss's bad side by coming out swinging and contradicting him constantly. Don't make your boss look weak or feeble in front of the team. Agree with him most of the time, especially on minor stuff, and disagree only when it's a battle worth winning. Start by introducing some of the concepts you are interested in during a 1-on-1 and frame it always as suggestions for how to improve the team, then start implementing it in your own work. When you do offer criticism, practice the "shit sandwich" technique. Say something nice ("Our deploy process is working great"), then something critical ("I think we don't have enough tests"), followed by something nice ("The team works well together"). The sad reality is that you are never going to agree with your boss on everything. But most of the time you can make a case. (That is, of course, assuming your boss is competent. If you're on a hopeless team, it's time to either give up and check out emotionally, or else start looking for a new job.)
I looked at it briefly. From what I can tell it's just a bit of sugar around the Kafka streams client, it doesn't add any real functionality. Kafka Streams' client API is written for Java 8 so unfortunately it doesn't have a native Scala feel out of the box. If you look on Confluent's github account they have some examples in Scala that do some similar sugaring to make writing Kafka Streams apps more palatable. On a side note, I like the Kafka streams lib, but I've found it very hard to work with if you can't run 2.12. The SAM conversions are extremely hard to puzzle out in &lt;2.12.
&gt; are coming in as language features in Dotty The question is, when will Dotty land? Dotty has been in development for several years now. AFAIK there is no ETA beyond maybe running in parallel with mainline Scala starting in 2.14 or 2.15. It's 2018, Scala 2.13 will hopefully be released by end of Q1; then 2.14 a year or so after that. So, maybe April 1, 2019 at the absolute earliest -- that would be a good release date.
&gt; has the as the main appeal being a nicer language than Java for Android applications. It's that way now, but that doesn't mean the ecosystem will branch off into other use cases. For example, Kotlin Native is directly targeting iOS, something that Scala Native has stated is a non-goal (i.e. the community will need to take on this massive endeavor). They have the resources (something like 40 full-time engineers working on Kotlin/JS/Native), while Scala Native has 1 1/2 engineers + whatever community contrib comes along. Scala Native is happening, it's just a question of when it will be production ready -- not soon is my guess.
Here is an interview I did with Runar, co-author of functional programming in scala. I think I failed at pronouncing his name, but succeeded in having a great discussion with him about purely functional programming.
&gt; Do you mean type aliases, http://wiki.c2.com/?StringlyTyped I made sure to bring up a couple examples where there's a clear degree of subjectivity. There is an added cost of using custom types like UserId instead of String or Long. Using a type gains you some readability and safety; turning many potential run-time bugs into compile-time errors. It's not without a cost, given there can be a very small performance penalty to wrapping and unwrapping the type, and it can cause some overhead when dealing with serializers and deserializers.
Thanks for this new podcast series, looks great.
Well, technically the difference is that the two sub-lists aren't being merge sorted. Sorting each sub-list in advance is assumed for a merge sort as well, albeit recursively. The outer-most step isn't using a merge sort to return a sorted list, it is returning the head element of an implicitly ordered list. The difference is that a merge sort would require iterating on all n elements across both lists in order to merge and get the minimum value, whereas the method above could say, take the first x number of values without having to iterate over all n elements. It's a pretty specific difference to say that the "outcome is pretty much the same." It was the whole point of the post.
&gt; Note that if performance was a concern i'd have the list sorted in reverse orders to begin with then prepend instead of appending to the result. Can you please elaborate? I'm definitely trying to optimize for performance. Also, isn't this just a standard mergesort? The difference I'm looking for is something I could call _.take(3) on in (n=3) time rather than nlogn time. Again, kpws above kind of gets it albeit a bit more verbose than what I'm hoping to end up with. Something between what he has and what you have is probably the sweet spot.
The scientific python ecosystem is a lot more cohesive than scalaâ€™s is â€” making it a lot easier to do this for python than it is for scala. The python package ecosystem is far far better than scalaâ€™s. Take a look at the packages within anaconda and try to find non-humongous replacements for them in native scala â€” a requirement when we have native and js targets to think about. Hence â€” vendor your own fatjar.
Great, was just recently looking for some scala podcast
If I wanted to be a scala contractor in London (England)... how much work and demand is there around? If I were to secure a 3 month contact, would I expect to be swamped with offers afterwards or would it be a hard fight to secure another contract with me being unemployed for a time afterwards?
&gt; they have nonsensical constraints. Such as? I mean we use akka-http at work and I have never found such a constraint, and we use akka-http to do some fairly complex stuff. Also the magnet pattern is pretty much typeclasses, and as you said the directives act as a Monadic Either (where `Left` are your rejections and `Right` is your result). I agree that http4s has a nicer API, but we also chose akka-http for other reasons including * Support * Metrics (Kamon + Cinammon if you have lightbend subscription) * API/Binary stability (akka guys take this extremely seriously, the binary compatibility support they have for a library is insane) * Very good debugging tools for akka-stream graphs. Although complex, there are a lot of tools that help debugging issues in production. As mentioned before, we concede that http4s/rho have a really nice API and have a purely functional core, but libraries are much more than that (we also had issues with http4s in the earlier days regarding using Blaze and load which we didn't manage to fix at the time, this was the initial event that initiated our move)
The nonsensical context bound on Directive. The nonsensical Tuple "phantom type". Which is really just a nonsensical typeclass which they claim to be a phantom type. This is the worst offense which breaks other things. The Java and Scala dsls are codependent which is frustrating when you try to build some things. 
I think I am confused by your statement : the implementation of `sorted` does iterate over all elements. The implementation does differ and might be more efficient in most cases, but the iteration occurs nonetheless. anyway, as long as you're happy ^^ 
I think his last name is read as "Byarnsson". "J" is usually read as "y" in nordic languages. You could ask him though, next time ;)
Thanks for sharing! I'll be listening to it shortly :)
What do you mean by non homogeneous?
 The 'Â¬' and 'Â¬Â¬' are just Scala identifiers which ought to represent negation and double negation. In some sense anyway. Then '|âˆ¨|' is an identifier as well, which can be read as `|` or union. '&lt;:&lt;' comes from scala.Predef https://www.scala-lang.org/api/current/scala/Predef$$$less$colon$less.html How the below definitions came to be, you'll need to read this article https://milessabin.com/blog/2011/06/09/scala-union-types-curry-howard/ type Â¬[A] = A =&gt; Nothing type Â¬Â¬[A] = Â¬[Â¬[A]] type âˆ¨[T, U] = Â¬[Â¬[T] with Â¬[U]] type |âˆ¨|[T, U] = {type Î»[X] = Â¬Â¬[X] &lt;:&lt; (T âˆ¨ U)}
Has anyone published this for the jvm?
&gt; Check out AnyVal for one workaround for performance, or tagged types for a pure phantom type way of doing the same thing. Will do! The stuff you can do with Scala's type system is pretty amazing. &gt; so thereâ€™s some more backup! Apologies for drifting back into rant-mode: Most of the stuff I've been advocating so far is very basic has a large amount of backup in the Scala community, or in programming in general. I could still be "wrong" of course; there's plenty I don't know about Scala, and even if I heard it from Scala experts it's always possible I misheard or misunderstood. Normally, I'd love to have these conversation (not arguments) about the pros and cons of various approaches. Being wrong often means there's a better way to do something that I was unaware of, and I'd always prefer to improve. The problem is he's so argumentative and combative, you can't have these conversations. It's also hard to take him seriously, given he resorts to passive-aggressive insults and bashing the opposing perspective. Constantly implying the correctness side of the Scala community is anal, mostly only convinces me he's lazy, and does not inspire me to think he might actually know a better way. Anyway, focusing back on the positive side of things, these types of situations are usually what motivates me to do some additional learning in my free time. I learned "clean code" in a similar way - and yes, there absolutely were a few things I was wrong about before I started reading up on clean code.
No, although I've had a mind to submit it as a SIP for some time now. It could use some love from the language itself to enhance it, especially wrt. pattern matching.
Honestly my opinion on the best way of how to write scala code that isn't a fucking mess is to learn a more strictly functional language and write your scala code like it's in that language. The majority of people who use this approach pick haskell, in my case I chose Erlang and Ocaml + SML. If you write a lot of code using Akka, knowing OTP behaviours and thinking in modules can prevent you from writing an unholy mess of mixins. If you do a lot of meta programming and have a lot of parametric polymorphism going on, well knowing haskell + the typelevel stack is probably the easiest way of going about it. If you want to write code that is very self contained and extensible and pretty functionally pure, while being willing to use OO when it's necessary and don't mind having a bit more boiler plate than doing things the Haskell/Typelevel way Ocaml is the way to go and you can pretty much substitute objects with structs, traits with sigs, classes with functors, case classes with records, and type declarations everywhere inside singletons. Another approach is Twitters effective scala guide, which gives you a good comprehension of most of the major language features, and what should you use and how. 
Just read all of the rest of the comments and you'll hopefully understand. It's not confusion, it's a lack of understanding on your part.
Awesome, thank you! This definitely looks and behaves the way what I was imagining, particularly with a Traversable end result. Is there a reason you went with Traversable over say Iterable or Stream? Since it's a super type of them anyway, I imagine the primary benefit is gaining all of the normal collection methods as you said, but I think the same would be true of Iterable/Stream as well, no? Thank you again, this is awesome.
If you are newbie, just use Scala, forget about FP. Scala is still immature language and there are no best practices everyone agrees on. Good start will be to use modern Java best practices, Scala allows to implement them with little to no boilerplate.
&gt; Note that if performance was a concern i'd have the list sorted in reverse orders to begin with then prepend instead of appending to the result. ```mergeSorted(a, restB, headB +: m)``` would prepend the list in reverse order? (more efficient?)
I would add two things: 1. adding a random but small timing factor between each retry could lead to better performance. 2. it is simple to abstract a function that let you retry any operation based on another function that identifies retryable errors, I've used this approach for SQL operations.
Yep! I agree that the method syntax is better. Having it explicitly attached to the number makes it easier to track. I only went with those examples because they wanted a solution that looked "like the physical formula on paper".
You're very welcome! Feel free to take as much as you need and spread the knowledge :)
Thanks for open sourcein this. It's not too small like beginner examples but also not too large. I noticed that most files are Uppercase but that there are some which are lower case. I'm curious, why is that so, could you tell me the rule/convention for this? The files which are lower case are intented for stuff which get's imported like implicits?
If a file contains only one single top-level-type (plus possible companion object) then it starts with an uppercase, and shares the name of the type. If the file contains multiple types, e.g. sealed trait hierarchy, then it starts with lowercase and has a semantically useful name.
Indeed JC yeah! Visa support globally :)
&gt; Is there a reason you went with Traversable over say Iterable or Stream? Stream is sealed so you can't extend it. And i personally not a fan of Iterable/Iterator due to their inherently mutable nature. &gt; Also, this builds them backwards in the same way as what you describe as well, technically, right? Well for most operation it never builds a List so it's a nonissue. If you specifically call .toList on it it will make a (mutable) ListBuffer instead that can be appended to in constant time.
Monix Tasks are also another great option https://monix.io/docs/2x/eval/task.html#restart-on-error
And if you donâ€™t using a default java scheduled executor to retry the work and resolve the promise later is just as good and requires zero dependencies 
The post explains how to do it with akka, and then there's no reason to roll your own thing.
Out of curiosity is that a pretty standard salary for "senior engineer" in Germany? I haven't been to Berlin in almost a decade but would assume it's as expensive as the sf bay area or Boston, but I'd expect a senior engineer position (esp for something a bit niche like Scala) would pay at least 50% more in either of those places...
It's indeed too low for a senior position in Berlin, probably not by 50% but close. This seems to match rather junior developers with about 1year experience in Scala. 
Yup, I use Monix tasks and it's been amazing so far. The only thing I would recommend in addition to using this is what the first commenter mentioned re: adding a small jitter factor to the backoff time.
Hey! Thanks for the info. I actually haven't heard of this particular supervisor, but a quick google seems to indicate this is more for actor restarts (which makes sense... cause supervisors). Maybe you could point me to a good example? Anyways, I guess I should have just rolled my own with a Java timer, I didn't really mean to distract with Akka. Just wanted a simple solution someone could easily paste into a REPL and not distract from the intent of the post too much. :-)
1. This sounds neat! Do you have any reading material on this? I'm curious how the random factor leads to better performance, I'd love to learn more. 2. excellent idea
For (2), take a look at the [atmos library](https://github.com/zmanio/atmos)
My pleasure :) Yes, there are some conventions out there but it's not very unified to be honest so you gotta choose. However, I'm kinda following this style, as explained by [@reidmar](https://www.reddit.com/user/reidmar).
Fair enough. I'm not too familiar with directly using Akka actors, I think the recommended way is to have the parent spawn a child actor that fails when the request fails and add a exponential backoff supervisor. In akka-streams (and most other streaming libraries) it's fairly easy: def doRequest(): Future[_] = ??? val resultF = RestartSource.onFailuresWithBackoff( minBackoff = 2.seconds, maxBackoff = 1.minute, randomFactor = 0.2 )(() =&gt; Source.fromFutureSource(doRequest())).runWith(Sink.head)
&gt; The nonsensical context bound on Directive. What is nonsensical about it? &gt; The nonsensical Tuple "phantom type" This is for java compat iirc, hasn't really been an issue in practice though. I mean the API could be a nicer, sure, but I haven't experienced anything which is on the realms of "breaking" something. 
I'm not sure why that would be needed for java compat. The java dsl isn't going to be looking at context bounds. The API is impossible to abstract over functionally, if you wish to use the DSL. This is because of the Tuple context bound. That tuple phantom type leads to a whole bunch of other shapeless-style operations just so that the dsl can accept arbitrary tuples. This makes it hard to read the source code and debug. Those operations are not easy to follow, especially for somebody new to scala. They could've saved themselves lots of trouble and just nested tuple2s, then created conveniences around nested tuple2s. As far as I can tell, they're not saving on any boxing by doing what they are doing now.
You may be surprised but Berlin isn't really expensive, certainly much less then bay area.
Lift is more or less dead, though they still have some active users (check their google group, or gitter channel if they have one). All of Play, Scalatra, Http4s, and many other web/rest frameworks in the Scala ecosystem will likely fit the bill for your project requirements.
Was just saying if you donâ€™t or canâ€™t pull in akka there are native alternatives that are simple. 
The immediate problem is that you're trying to use the `+` operator on an arbitrary type and you have not defined what it means. So you end up invoking an implicit conversion called `any2stringAdd` which lets you add strings to things, which not only is unhelpful but it doesn't typecheck. This is widely considered to be a misfeature. The second problem is that your method doesn't work for empty lists. Ideally you would want to return some kind of "zero" in that case. So, if you want a generic "sum" method you have to define what `+` means *and* what zero means for the element type `A`. There is an abstraction that does this, called a monoid. I have an example [here](http://tpolecat.github.io/2013/10/12/typeclass.html) that walks through how you can model this in Scala. Hope this helps. Good luck!
The problem is that you are trying to add two types together (two As) and you haven't defined a way to add them together. This works intuitively for Ints but consider something arbitrary like Orders which could look like case class Order(value: BigDecimal, quantity: Int), what now? What you can do is abstract over this by specifying a function to combine two A's and return an A ``` def fold[A](lst: List[A])(combine: (A, A) =&gt; A): A = lst.tail.foldLeft(lst.head)((acc, i) =&gt; combine(acc, i)) ``` The user needs to provide a way to combine those two types together. Now you use this like ``` fold(List(1, 2, 3, 4))((x, y) =&gt; x + y) ``` This pattern is in fact so common, there's a typeclass called Semigroup which functional programming libraries like Cats and ScalaZ provide which defines how to combine two types together. Here's a Semigroup typeclass implementation for integer addition ``` implicit val intAddSemigroup: Semigroup[Int] = new Semigroup[Int] { override def combine(x: Int, y: Int) = x + y } def fold[A](as: List[A])(implicit sg: Semigroup[A]): A = as.foldLeft(as.head)(sg.combine) fold(List(1, 2, 3)) // 7 // underneath the hood, this becomes fold[Int](List(1, 2, 3))(intAddSemigroup) ``` Typeclasses are a form of ad-hoc polymorphism. Hope that helps
This would really make my day!
Not my experience. 60k would be a high entry salary for a junior. 75k for a senior sounds not bad, but the distinction between junior/regular/senior is too coarse to really say how much a senior should make. 
For a Java Developer, maybe. But for Scala? The demand is rather high these days. So even for a 1 year scala developer (which I would most likely call junior with few exceptions) I find 60k to be the right spot, not too high at all. 
&gt; The API is impossible to abstract over functionally, if you wish to use the DSL. This is because of the Tuple context bound. Can you describe what you mean here? I have been able to abstract over the DSL functionally, I do it all the time, unless we are talking about different things (I agree its slightly more verbose then what it could ideally be, but that is different to saying that its impossible)
You mean 1 year Scala developer and no any other programming experience?
But then (in case of the exponential backoff supervisor) you must use akka persistence to repeat request... 
Yes. Having 5 years in Haskell and 1 year in Scala is certainly not a junior dev. :D
thanks, great help
thanks, great help
Hey - TBH, it's more the parametres of what defines a 'Senior' which can usually bump up the salary bandings in Berlin! However, with almost any job description you see across Berlin...you will struggle finding many Mid-Senior roles above 80k euros, especially with Berlin being amongst the cheaper cities.
Hmm I'd certainly beg to differ on this one. Across Berlin, Hamburg, Munich I have yet to see this...except for maybe the odd occasion when it's a top-tier technical univeristy graduate, who has then worked on some really cutting-edge stuff
I created a SBT plugin for SBT plugin authors to test their SBT plugins using ScalaTest: https://github.com/daniel-shuy/scripted-scalatest-sbt-plugin