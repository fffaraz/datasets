I don't see any contradiction. The core of the language is easy to pick and learn. *If you want* you could go into more in-depth stuff, but all is well integrated and cohesive. By "nerdiness" I don't mean anything negative, just some good brain exercise. Now, too much nerdiness can be counterproductive, but I think that Kotlin has the right sweet spot. Very friendly for newcomers, with powerful (Not as powerful as Scala) features if your heart desires so 
I see, is it was the "You don't need some of Scalas features" thing. If Kotlin has "the right sweet spot" then what features does Scala have that are "too much" to be in this "sweet spot"?
&gt; To me, what makes Scala different is that it's pretty good at not taking shortcuts to provide a feature that scratches a specific itch. Completely agreed, and that's exactly why I think it's not going to look better in small codebases. A shortcut for a specific feature is always going to look better as long as you're only using that specific case, and in a small codebase a language that has lots of ad-hoc features isn't much of a disadvantage. Where Scala's generalization approach shines is when you need to combine several of this kind of features (or maybe even do something custom for which there isn't language-level support), but that's much harder to show in examples.
I meant to reply to the comment that mentioned trolling. Not an attack on you, sorry. But in general, yes, the community is upset enough about a constant trolling effort against this subreddit that included pornographic images, that we are oversensitive. At least I am, and my comments were tame compared to the trolling. This is the 3rd Kotlin post in two weeks. The FUD is enough thatI hear it mentioned at work weekly. Every talk about tooling and sbt and Java compat brings it up. But it has 0 to do with Scala, other than (kotlin's tooling is better than yours) and that's about it. Bringing it up certainly attracts newbie devs to the easier learning curve. This inevitably leads to the Scala is too complicated argument that eventually leads to Scala is actually different, unlike everything else, and different is hard, which is an argument I'm also tired of reading. The language is ok. But it doesn't offer enough new features that if I were a Java shop, I'd switch. It doesn't offer enough abstraction to switch if you are a startup. I've made this argument before though. 
Am I the only one who is beginning to feel like applying the nuclear option of banning any posts or comments about Kotlin in this subreddit could actually be a good idea? This is getting really tired.
* Monadic for-comprehensions through coroutines (Not as lovely as Scala) * FP in a lite way through funKTionale, if you want something like cats, there is Kategory (I don't think that they're as complete but there are options). * JetBrains is addressing serialization in upcoming releases, is planned to work and interoperate in all compilation targets, but nothing right now * Kotlin has ADT with exhaustive checks but pattern matching is nicer in Scala 
I understand your viewpoint, but this is really getting silly, with multiple posts that barely mention Scala at all.
Agreed. Don't know why you're getting voted down.
There is a huge difference between a pet project, and a Google PM getting up on stage at I/O during their keynote and saying "Kotlin is an officially supported language now".
I am not quite sure whats going on. Too much has been said in this thread and I cannot differentiate noise from fact. I think its getting voted because there has been too much of trolling. 
You are probably right. I am mostly getting the tooling is better comments. Although tooling is a concern, but I am not sure how much of it is a "I don't like this language because of tooling" level.
I use Scala IDE at work every day, and I hate it. It frequently forgets about implicit (or other such brain-farts), requiring "Clean" commands to force a recompile. This happens at least once per hour, so I don't call this "rare". Changing even a single file can trigger 1 minute+ locked UI while something recompiles. I admit that I am using a 4 year old laptop... However the UI should never lock. Occasionally it just plain crashes, or hangs.
&gt; Honestly, who cares about Kotlin? It's a vastly different language, and I don't know why people keep posting about it in this sub. Zero sum gaming. For Kotlin to "win" and become the "better Java", they have to take developer mind share from Scala, which last I checked was the top alternative language on the JVM. It is the same reason Groovy and Clojure fans like to bash Scala on /r/programming.
I am an Android developer and a backend developer, I want to share my opinion: 1. Android The war is over. Google official supports Kotlin and Kotlin already has a better Android community and a good toolchain. There are lots of share/discussion about Kotlin in reddit/r/androiddev everyday. 2. iOS IMHO, iOS developers have little interest in Kotlin, just like little Android developers have interest in Swift. I guess iOS will gain some Kotlin developers (if they want to share some code between Android and iOS) but not too much. 3.Desktop app Though I still like desktop apps, but nowadays less people care about this. Qt, Electron (though I think JavaFx is better than this) would gain more developers. 4. Backend Spring Framework is popular in Java, so adopting Kotlin in Spring is win-win for them. But I think Play Framework is much better than Spring Boot. But Kotlin + Vert.x and some other solutions for Kotlin should not be underestimated. 5. System programming / JS A larger language user base will win if they don't want to choose Rust, C++(Typescript, JavaScript) etc. 6. Big data / Functional programming Scala wins. Kotlin is easier to learn and adpot than Scala for Java developers (lots of reasons). Though I like Scala and think Scala is better than Kotlin, Kotlin will more popular than Scala in the future. But there is no reason to prevent Scala becoming better and attracting more people. Kotlin coroutine looks interesting and project Loom is on the road, can we learn form these and make a letter one? (Do we need?)
&gt; is the situation so bad right now that people are turning away from scala because of tooling? I suspect that it is - or at least, that there are people on the margin where the tooling is enough to make the difference. Certainly if scala-ide stops being a supported way of doing development then I'll stop working on my libraries, for example.
Oh, I don't see a future for Koitlin. They picked a terrible niche between Java and Groovy. And both of those are growing and will eventually cover any of the incremental advantages Koitlin might have. My contention is more related to the challenges of contributing to Scala from an individual contributor's perspective; i.e. there is no central cohesive "roadmap" from which to be confident AS AN INDIVIDUAL that my contributions won't end up being wasted by each/all of these different impacts to the Scala I would write today.
If there's anything I've learned over the past 6 years working with Scala IDE it's to *never* build your project in the IDE; i.e. just use it for the presentation compiler and build your project in a separate terminal window/sbt session. &gt; It frequently forgets about implicit [or anything else] Whenever the IDE gets out of sync restart the presentation compiler (I have it bound to ctrl-backspace key combo). &gt; requiring "Clean" commands to force a recompile I can't remember the last time I've had to `clean` in daily dev, that's generally not necessary with recent Scala versions -- usually you're just adding to the pain by trying to resolve with a clean build. p.s. use `sbteclipse`; make sure `.classpath` and `.project` files generate correct paths. Select-all projects + F5 is also helpful when generating projects and/or changing build deps.
1, sure. 2, somewhat ridiculous; swift and html5/css/js are just better options, kotlin adds nothing here. 3, see electorn and friends; kotlin seems like an indirect route to get to the same place. 4, so much competition; js/typescript is ubiquitous; only if you already use kotlin on the backend, and even then it's a stretch. 5, C++ is rolling again, rust, crystal, pony.. lots of really nice projects that do native first and fast; unless you do memory management at the bare metal level, have access to SIMD, and do concurrency cleanly, you will not stand a chance in this segment. 6, Javascript may become typescript, but it's not going anywhere. WASM backends are going to make waves, but simple lang-to-lang transpilers will remain niche. 
Thanks for the suggestions. I didn't know about restarting the presentation compiler, I'll try that. I use "clean" as a way to make the errors go away. I use F5 a lot... We can't use sbt, as the team uses Gradle for everything.
&gt; Monadic for-comprehensions through coroutines (Not as lovely as Scala) What would that be? I know monadic operations and I know coroutines, but how would the latter provide the former?
&gt; more than 20x faster than Scala - without type inference - without implicit parameters - without higher kinded types - without abstract types - without nested classes and traits How could you possibly call this a "subset" of Scala?
It's still technically a "subset". After all, isn't even the empty set a subset of every set?
[removed]
It's a subset. But I'm not sure how much you can really learn from being able to typecheck a subset that is weaker than Java 20√ó faster than a full fledged Scala compiler.
1) Validation of a parallel typechecker architecture: https://github.com/twitter/reasonable-scala/blob/master/docs/compiler.md, 2) Foundation for future performance work, where we will be adding features one by one and measuring their individual impact.
I didn't mean to sound dismissive. I don't doubt we can learn some valuable lessons from this project. I was just wondering whether the 20x number is really meaningful at this moment in time.
Kotlin is much better than Java for android dev because of how clunky/verbose Java is (there is a reason why Google adopted it). &gt; Support Kotlin and make it better, but don't run around with blinders on. This hasn't anything to do with having blinders on, tools are tools and people use what they think is the best tool for the job
Agreed, this is why VS Code is getting a lot of praise (its basically almost completely async) where as Atom is getting a lot of flack. Intellij is always fairly blocking when it comes to UI, its just a lot more obvious when you are running Scala
This assertion is erred: &gt; your experience is to be set back because you see many projects going into opposite directions My experience is that there are several (not many) projects which are fundamentally altering the nature of any Scala code I would write at the present time. So, not only do I now have to accommodate thinking in the 2.10.x, 2.11.x, 2.12.x lines of compilation, I have to try and anticipate the 2.13.x line of thinking, the new (strawman) collections line of thinking, and the Dotty line of thinking. IOW, that's a whole lot of dimensions interacting creating a significant amount of complexity for me to move forward even _analyzing_ an open source contribution. And then on top of all that, I would have to consider the specialized nature of Scala Native and/or Scala.js if my interest was to contribute to either/both of these. And that is assuming someone else has take care of all the cross compiling necessary to create all the Maven deliverables per release. This is not a complaint. This is just describing the complexity facing a person, me, when trying on the idea of contributing. So, the assertion corrected is: &gt; your experience is to be set back because you see high complexity coming from multiple projects which will all impact the specific code you would write today Please consider that I love Love LOVE Scala. I have committed +6 years of my intense focus and a huge sum of money (my life savings) into Scala. So, I am DEEPLY invested in Scala continuing to be successful and even grow. I am only emphasizing this to show my motivation as I attempt to answer your question... &gt; Do you have an idea how we could counteract the sentiment that you are experiencing? And I will answer the question in a separate reply. And just to get the Koitlin contrast out of the way: I have no quarrel with Koitlin. Personally, I think it‚Äôs a fad. It doesn't have the core principled design legs of Scala (where this only gets deeply better with the eventual release of Dotty). And it doesn't have the pragmatic longevity legs of Java, or even Groovy (and both get better with each release reducing Koitlin‚Äôs advantages). Sans a deep philosophic rethink, I cannot see how Koitlin sustains serious growth for more than a couple of years. It will become just another variation of Embarcadero‚Äôs (formerly Borland‚Äôs) Delphi.
I was talking to Dobroff.
&gt; You can use `reduce` in Java 8, so this comparison is unfair now. Sort of. For simple examples like the above, Java's `reduce` is concise by Java standards: numberList.stream().reduce((a, b) -&gt; a + b); but check out the overloads of `Stream.reduce`. They let you do the things the `fold*` methods in Scala do, but defining the lambdas involves lots of verbose generics insanity. (Sadly, I just had to do this.)
Not me. The debate is healthy. And it keeps showing just how far ahead Scala is.
Also it doesn't do much actual checking of types yet, it's more a namer and a type-assigner than a full typechecker for now: https://github.com/twitter/reasonable-scala/issues/19 It will be interesting to see if the Kentucky Mule approach to type completers (https://github.com/twitter/reasonable-scala/blob/master/docs/compiler.md#typechecking-in-kentucky-mule) can scale to full typechecking and if it's actually faster than what scalac/dotty currently do. Looking forward to seeing how this progress!
Umm, what? How is this useful then? 
Thanks for the insightful conversation! This is definitely a right question to ask, and I am glad to elaborate. Indeed, it is hard to predict how exactly that 20x will evolve as we accommodate more and more features from vanilla Scala. That's why in multiple places in the docs and in the slides we say something along the lines of "these results should be viewed as work in progress rather than final numbers". However, I think that 20x is quite useful as an indicator that things are going well. If we stripped the language to a bare minimum and ended up being just 2x faster, that would've meant back to a drawing board. 
https://www.reddit.com/r/scala/comments/7df8r6/prototype_of_the_reasonable_scala_typechecker/dpxfnau/
There is nothing wrong with what's said/written in the project communication but it could very easily be misunderstood and harm the community. The main readme.md states that rsc is 20x faster than scalac but doesn't state clearly what features are not included. It just says a "small subset". To know that we have to click and read the language file. Most people won't click and just keep that scalac is 20 times slower. You should either state limitations clearly in the main readme or stay vague about the speedup.
basically 'without scala'
done, but for the future you'll want to consider having less options for the 0 to 7 questions. too many options make people drop off your survey midway (I almost did...) 
I think it might even technically be a strict subset of java, if it's missing all those features?
Thanks u/habitats, will put into consideration in my future survey :)
Thanks u/viso_laci, will put in consideration in my future survey :)
If you take out the ADT aspect of case classes what remains is basically some built-in syntactic sugar to generate `apply`, `equals`, `hashCode` and `toString`though
&gt; is the situation so bad right now that people are turning away from scala because of tooling? I am not sure about now, but in the past companies like Atlassian have greatly reduced the use of Scala with the primary reason being tooling (back then they were using maven with Scala, and because of Scala's horrendous compile times they ended up restricting Scala to only a few services)
&gt; &gt; &gt; It's a macro library on top of Java with a library and a marketing department. Google chose Kotlin because there is 0 learning curve, the apis don't need to change, and your syntax keeps them from being sued over copyright infringement. This isn't correct, Kotlin is its own language with its own proper compiler, its not a macro over Java (which is something closer to Java Lombock). It wouldn't be possible to target javascript/native otherwise
Yeah but they are super handy so I class them as a genuine feature. Everything just desugars down to bytecode at the end of the day, after all.
Scala does have a high standard for safety, but its also balancing this with Java interopt. `null` in Scala only exists because of Java (I think if you made Scala from scratch today, you wouldn't have `null`, only as an import for environments like Scala.js where it would be represented as a union type) Universal equality again is due to Java (all objects need to define a sane equals method) Implicit conversion rules are so so, I think a lot of the complexity here is due to familiarity moreso than explicit Java support (although this may contribute). For example number widening causes a lot of issues when it comes to implicit conversions, and this is because of the familiarity argument where people want 1 + 3.0 to just compile which opens up a lot of loopholes
Have you compared it to Java? 20x sounds great but if we are still slower than Java we may still need to go back to drawing board.
I was being overtly callous to illustrate trolling behavior with the macro library comment. 
I meant that from a compiler perspective, `case class` support is fairly trivial. It doesn't make the language much more powerful.
Because Twitter.
&gt; Nowadays, not many people in their sane mind would pick Scala as a better Java I've found that Scala makes an awesome "better Java", if used with a bit of discernment...wouldn't guarantee I'm in my sane mind, though. Depends what you mean by "better Java", I guess. 
I tend to agree, but once you start digging into Scala and its ecosystem, you won't feel comfortable until you understand _a lot_ of stuff that are very far from Java. I mean, even the standard library is full of advanced features and types. It's a somewhat similar to how C++ is very hard to use effectively if you don't understand most things that happen behind its high-level features, even though ideally you should not have to.
Looking at changeset I feel very dumb suddenly.
I wish Jetbrains would put more effort into Scala and its tools instead of making something else that's brand new. Kotlin smells of "not invented here". Hopefully they can do both.
I believe you can find that info here: https://github.com/twitter/reasonable-scala/blob/master/docs/performance.md
We aren't: https://github.com/twitter/reasonable-scala/blob/master/docs/performance.md#results.
Is there a downside to it being macro-based? I'm only marginally familiar with Rust, wrote some CLI toys in it to see how it was like compared to C++ and that's all.
It would be great if scalac was 20x faster for the parts of your code that didn‚Äôt use these features. Also: would love a linter for compile time performance. i.e.: ‚Äúadd a type annotation here, it‚Äôs expensive to compute‚Äù
&gt; Also: would love a linter for compile time performance. i.e.: ‚Äúadd a type annotation here, it‚Äôs expensive to compute‚Äù Have a look at https://github.com/scalacenter/scalac-profiling
If your language supports inheritance, lazy vals etc. I think you basically need `null` (or you have to limit the expressiveness of constructors/field initalizers). Rust and most functional languages doesn't have these features and thus they don't need `null` and can avoid all the problems that comes with it. I agree that most of Scala's other problems mostly stem from trying to achieve Java/JVM compatibility.
&gt; If your language supports inheritance, lazy vals etc. I think you basically need null (or you have to limit the expressiveness of constructors/field initalizers). Rust and most functional languages doesn't have these features and thus they don't need null and can avoid all the problems that comes with it. Agreed, but as I said there is a big difference between *having null exceptions* and *letting users define null*. The former is a lot more bearable (since users know it will only happen in with trait/lazy val initialization), the latter is whats really problematic. With the former, you can treat null as an internal type which can't be defined (unless imported), this in turn can simply a lot of the soundness issues in the type system (which are even happening in Dotty for similar reasons)
Thank you very much for such thorough answer!
Yes there is. Macros leave the language as is and work on top of that. And this means many things, e.g.: * It makes it harder to reason about the code you see because you have to think about what the macro does * Can make debugging harder * Makes it harder or even impossible for IDEs to give support * Usually makes refactoring harder or impossible * Is usually less stable then when using jsut language features (in terms of maintainability for the macro itself) * Does not compose as well with other macros as code does with other code * and so on... In essence, avoid macros if you can. If there is some feature that is obviously a good idea to use, then a language should support this. Macros are for when a language is to slow in evolving and you need some functionality and can't wait. So you use macros - and if your functionality is good and widely accepted it should be included in the language.
"underscore" for me ;)
underscore. Although, I don't usually "pronounce" it in my head when I'm thinking - the same way I don't pronounce "equals" when I assign a variable.
zahombie zahombie
Really? You don't say "equals" or "is"?
usually like the thing it is representing, 'user' for `userMaybe.map(_.id)`, 'useless result' for `val _ = f()`.
&gt; 2) Ios. &gt; This is not going to be an easy one. Apple is targeting Swift and it is heavily promoting it. I don't think they will focus on Kotlin. Remember that Kotlin became famous for Android since Google supported it. Without native support from the company, I don't think it will become a viable language. I'm not a fan of Apple's ecosystem, and I don't think they would ever put their resources into it, but honestly I can't see them try to stop it either. They've been pretty supportive of Xamarin's work, and have done quite a bit to make it possible for them to release quickly upon iOS updates.
How about a performance comparison between Rsc on one core and Rsc on multiple cores? 
If I'm talking to someone, I say "it" or "them"... as in "map them toInt." Those are basically the english choices for contextually-specified topics.
Funny, I don't pronounce it at all. For me, `_.toInt` is just "to-int", and `_.map(_+1)` is "map plus one". Kind of similar to how it is in Haskell (`map (+1)`).
"uhh-ah-uhh"
I pronounce it like "-" but a little lower and longer.
I'm using the stack Http4s / Doobie / Circe / Cats / Shapeless (just HList) in production in the latest versions but unfortunately the project is not open source yet :/ Though it's going to be open sourced next year at some point... I'll be sharing it here once it's out :)
`sbt-release-early`, a plugin I wrote to solve this problem, removes the cumbersome process of releasing to either Sonatype or Bintray; and dramatically simplifies your sbt builds: https://github.com/scalacenter/sbt-release-early/. I strongly discourage the use of sbt-release. If you want to know why, read on it in the wiki: https://github.com/scalacenter/sbt-release-early/wiki/Essential:-Installation#advantages-over-sbt-release.
Interesting! I pronounce it like "|" but perpendicular and thinner.
I'm really happy about the progress that Scala Native is doing. It's incredible that one person at a research lab outperforms a whole group of engineers working on Kotlin Native. This is my personal opinion. I doubt that "Kotlin Native is generally eating Scala Native's lunch of late", it seems a claim with no foundations at all. The research that Scala Native is bringing to the table will most likely be copied by the Kotlin team, sooner or later.
&gt; For example, when you see "_.toInt", how do you read it? Not sure why, but, "all-dot-toInt"
I usually use "under," but I might use "blank" when there's an operator involved, like `_ == x` would be "blank equals ex."
Seriously this is some low effort shit
Dear /u/zero_coding: /r/scala is not a place to advertise all your Stack Overflow questions. If people want to answer questions on SO, they will be browsing SO and will discover your questions there. People come to Reddit to see news and other Scala discussions that are not questions. They don't come to Reddit to be redirected to SO.
I don't"pronounce" them in my head either. I see it and understand it in an abstract way and move on. Its like looking at a shape and just knowing it's a square
`it` is what groovy chose as a keyword instead of `_`. I think it's not a bad choice.
Looking forward to seeing it üòÄ
Why are you yelling? More seriously, There are links that show the status of various features on the [dotty home page](https://github.com/dotty-linker/dotty)
I pronounce it with a glottal stop.
This is in the works. Stay tuned for updated!
well that was a shit storm 
I think you're misinterpreting the goals of the project. As our readme has been saying from the day one and still says to this date (https://github.com/twitter/reasonable-scala), our goals are to: * Dramatically improve Scala compilation performance * Study compilation time overhead of various Scala features * Identify a subset of Scala that can be compiled with reasonable speed * Facilitate knowledge transfer to other Scala compilers "Removing many of the features of Scala" was never a purpose of Rsc. Instead, we are planning to quantify to impact of language features and idioms, so that users of the language are empowered to reason about compile times. We believe that this will lead to better understanding of what Scala compilers spend time on, which will help to identify and attack the hot spots in the current compilers (and that's why facilitating knowledge transfer is one of the explicit goals of the project). Finally, I want to comment on backward compatibility. As the documentation says, "we take compatibility extremely seriously", and there is a simple reason for that. Here at Twitter, we have millions of lines of Scala code. With this volume of code, incompatible changes don't stand a chance.
having gone down the rabbit hole with slick I'm anxious about pulling in new dsl. I'm an avid spark user, but dislike the dataframe api. is this considered ready for production? 
example? 
if it's used as a positional parameter: underscore if it's use in a generic: confusion and dread.
This is probably as good a place as anywhere to say, isn't it weird that of all the language features that scala adopted which have since seen wider adoption, nobody else has embraced the amazing underscore lambda syntax (that I know of). 
What's with the all caps
at first there's a high pitch squeal like what you hear when you get tinnitus ring in your ears. Then it slowly gets louder and hollower, until you can almost hear words, you try to concentrate on the words, but you don't recognize the language. In fact trying to understand the language gives you flashes of an alien world filled with old gods, and creatures beyond our understanding.
It's always interesting to hear things like this. I am similar with regards to reading code. It makes reading the code out loud for others a little awkward as I have to actively process what to call the symbols. Likewise, people seem to have a lot of shock that I can't "see" images in my head... no mind's eye type of deal. I can think in abstract terms fine, but visualization just doesn't happen.
'sep', but I don't really have a reason why. It's always just been a placeholder syllable for me.
"mm", "nn", or "uhh"
It depends on what your consider production ready. Quill is mature and used in production by many people. This new integration is pretty young, though
That's character is "zot" 
I just make the schwa sound.
ensime.org
I'm curious about this obstacle you're experiencing. If you're talking about making a library, why not just target 2.11 &amp; 2.12? 2.10 seems way too old, unless you really need to support some legacy stuff, and no one's using 2.13 and Dotty in production yet. Or, if you're talking about working on the core projects, I think the answer is maybe "just pick one and run with it". Although that's just me speaking hypothetically as someone who has contributed no code to any of them! I'm just curious.
Stuff
Wow, sorry :)
https://github.com/ensime/ensime-emacs/issues/669#issuecomment-334428101
see also: https://stackoverflow.com/questions/45963559/how-to-release-a-scala-library-to-maven-central-using-sbt/45963560#45963560
I've always been under the impression that it was "blank". As in, it's a blank spot for you to fill in with context details. IIRC the Programming in Scala book also uses this terminology (either that or FP in Scala, I forget because I read parts of them both this past year).
When, I need to: whatever.
Ah! Do you want to learn to read faster? Start reading text while simultaneously have your mouth say nonsense aloud ("lalalalalalala...." usually works). Your mind will notice within 2-5 minutes that you don't need to (sub-vocalize) in order to read, and you'll start getting faster at reading as text no longer needs to pass through your throat.
People have been trying to make the current Scalac compiler faster (there were even Google SOC projects to try and do this). Most people have failed. There were some performance improvements in Scalac recently (thanks to Jason Zaugg) however they were mainly fixing previous performance regressions (plus there were some extra tricks to make it faster) The point is though, to get any significant performance improvements from the compiler you would need to rewrite it. As has been stated, the current compiler is designed in an old fashion (i.e. its single threaded, has massive global passes of the AST which shreds the cache). Its actually nice to see that there is going to be a detailed overview of the compiler performance cost of features, I think some people would think twice about using specific libraries/features if it increases the compile times of their Scala programs by 20x
&gt; Ironically it's also the reason i'm skeptic of kotlin, for me intellij has a proven track-record of delivering broken things (2 year old reported bugs/issues are left ignored as can't be fixed (the shared source root problem) and false negatives are as bad as ever) Jetbrains isn't to blame for this. A lot of these issues are due to Scala's really complex typesystem and/or SBT
It already is, ScalaIDE is in maintenance mode and the core contributor is working on https://github.com/dragos/dragos-vscode-scala
"whatever" -- because it works for all usages of `_`.
Have you increased the memory that you‚Äôre allowing IntelliJ to use? Once I did this all of my laggy problems went away. 
 Well shit. Guess I'm in the market for a new language then, or maybe giving up open source projects entirely. The VS code experience just isn't good enough for my style of working (and I'm not up for maintaining an eclipse plugin without being paid for it), but I wouldn't be happy without HKT either. 
Same here. More concretely, go to Help -&gt; Custom VM Options, and there you can set the JVM options for the IntelliJ process. I added these options and haven't had performance issues with large projects: -Xms1024m -Xmx2048m -XX:MaxPermSize=512m -XX:ReservedCodeCacheSize=512m 
My impression was that he was naming a successor. Guess that didn't happen? 
Isn‚Äôt the project amber stuff using it similarly for their enhanced switch and pattern matching stuff?
Max perm isn't relevant for jdk 8, and I don't know that it is advantageous to set xmx and xms to different sizes
I use vim and have decided to switch to intelliJ for Scala. My justification is that it has debugging tools. I tried spacemacs with Scala layer, but that's basically vim with ensime. How much better are the breakpoints and debugging in intelliJ?
I've been using IntelliJ for Scala for almost 5 years now, and I'm extremely happy with it. Sure it needs to think once in a while, but so does the Scala compiler. It does a ton of heavy analysis - of course it needs to think. I recommend sticking with IntelliJ. I have never seen anyone do anything in Vim or Emacs that I can't easily do just as fast or faster with a IntelliJ &amp; multiple cursors. On the flipside, IntelliJ can do a lot of tricks that pure text editors can't do - such as refactoring, assigning thing to values with the correct type annotation, look up stuff very easily, click to see implementations, and I can keep going for another 20 minutes :P I honestly think you'll be missing out if you go back to a text editor. I think the reason you see people in presentations using text editors instead of IDEs (that is, IntelliJ; Eclipse is no competition tbh), is because they're usually showing off some usually pretty hardcore category theory / typelevel programming and IntelliJ might not benefit you that much when you're pushing the type system to its limits. You could still use it, but it would probably give you some false negatives ("good code red") which can be annoying. ENSIME might do a better job in those cases. But unless you're doing typelevel programming on a library engineer level, I think IntelliJ's analyzer will have you covered in 99.5% of the time. That's my opinion anyways..
You're right -- not sure why I configured MaxPermSize instead of MaxMetaspaceSize in this laptop..
Doesn't really matter if you do typelevel programming on library or application level. Both doesn't go well with IntelliJ unfortunately...
There are plenty of Scala guru's who use an IDE. I find that many who don't are doing it because they want to fit in with a certain category of developers (the coding hipster demographic popularized by the Ruby community) or they're Typelevel/Scalaz purists who can't use IDE's because they're incapable of processing all derivative type system hacks that those libraries base much of their featureset on.
I use IntelliJ with a Vim plugin, used to use Eclipse with Vim plugin. 
Hmm.. In my experience, _using_ libraries like cats, monix, http4s, fs2, free monads, etc. is no problem with IntelliJ. Although there can be an occasional "good code red" which I usually report and then 6 months later there's a fix for it :P But really, good code red is livable imo because all you have to do is add type annotations to it(!), then the rest of your program won't be affected by that 1 intellij miscomprehension. Red wiggly lines _looks_ a little ugly, but I still take it any day over _not_ having supreme code analysis + assistance on my fingertips with every keystroke. Easiest trade-off in my life. I have yet to try Ensime personally though; it's probably really really good too - but you'll miss some of the refactoring abilities(?).
I agree, but I rather consider using shapeless as typelevel programming. Using cats and such is rarely typelevel programming it's just using FP libraries (with some exceptions when it comes to e.g. Leibniz and stuff like that). I also prefer IntelliJ with a few false errors over something like vi. I tried Sublime + Ensime but even renaming didn't work, so I guess IntelliJ is still the best choice for now.
&gt; Its like looking at a shape and just knowing it's a square without having to think the word square Yeah. I think an even better example is parenthesis, brackets &amp; braces. They're a vital part of the code, I see them, but I don't think "open parenthesis" &amp; "close parenthesis" etc out loud.
I'm a hardcore emacs user and I still use IntelliJ for Scala. I've tried to make emacs work for scala but it just cant hold a candle to intellij, unfortunately. 
Nicer syntax does help a lot though. Of course, I could never go back and live without all the FP features from Scala _now_, but coming from Java, just having a 21st century _syntax_ is a breath of fresh air - Java is very ancient looking these days. We care about syntax; it's as easy as that. If you can get rid of a lot of _syntactic noise_, that's money in the bank already imo. But of course, Kotlin is no Scala.
[Finagle](https://twitter.github.io/scala_school/finagle.html) for example
Ew, Emacs. Doesn't even use standard key bindings.
You know, that is kind of impossible to ask. There is a reason why there is so many libraries around (besides legacy and ego-driven development), and that is the fact that many use cases requires different solutions. Instead try to teach yourself how to do research, ask yourself: * what you want to achieve? API client, what it needs to do? CLI, GUI interaction? One shot to API and then display result and quit or operate continuously? * what components would that require high-level speaking? Something to query URL endpoints, something to parse JSON/XML/YAML? Argument parser or GUI libraray? * once you more or less know how things would be wired together on high level (don't think about classes or some specific architecture, just flow of data and actions from one place to another) you might start googling libraries, * pay attention to things like: is library still maintained, how does it cooperate with other libraries (usually someone would describe it someplace), is there some common standards that are shared across them or is library elastic enough that you could adapt it into wide range of use cases? * don't assume that some library would work out. Try to use it in some small test case and see how you like it - if you don't, next! Some libraries are purely about preferences, e.g. Cats vs Scalaz debate could use maturity as argument, but I wouldn't be so sure about it. If you want to write type-level library Shapeless is kind of obvious choice. But other than that? Many different solutions exists next to each other and usually everything works ok so... IMHO no one here can tell you what is best, only what he like the most.
I self published a book like this for node.js. If one doesn't exist for Scala, that doesn't surprise me because they're a beast to write and update. There are some small projects in good books, like Odersky's book I think does a CSV editor. Finatra's repo has some really good examples in their examples directory. I don't think there are any walkthroughs explaining the examples, though.
Yes, I actually set min and max memory to 4gb - using Ultimate for Java, kotlin, scala, database and web dev, so sometimes I might have several related projects open at the same time. No slowdowns except a couple time with the latest version where I had to clear caches and restart IntelliJ. 
Well you can try Intellij, in my opinion the IDE is better, although it has issues with really advanced type usage. It's been getting over time though.
Well, I would argue about maintainability of such a resource. For book - yes, it becomes obsolete write after publishing due to fast changing technologies and libs versions, but for resources like a Coursera course or a YouTube channel there is no such term as 'out of date' because they can be easily updated. So, I what I'm looking for is such a resource that teaches how to write a project from scratch to ready-to-go and it seems there is none of such in Scala
Thanks for your piece of advice! I guess trying to reason about how the components are wired together would be the best solution for me at the beginning of writing the Scala app. The sad thing is that there are tons tutorials and courses of 'how-to-write-your-app' in other languages but I can not find any in Scala
I've tried it (indeed I use it at my current client), I find its error highlights unusably inaccurate.
Well, I don't think there are many good use cases for traits with logic in, so any example I showed wouldn't make sense to be a trait in the first case (IMO). Why don't you show an example of a trait that you might want to test and I'll try reworking that?
Any book about a library isn't very useful, particularly if you're interested doing what others are doing. You're never going to get a single answer for the "community" recommended library to use. If there's anything that's has universal community adoption, it's [SBT](http://www.scala-sbt.org/), so you might want to start with learning the build tool. There is actually a book on that. The book pre-SBT 1.0, but is probably still useful. There's also the [manual](http://www.scala-sbt.org/1.x/docs/sbt-reference.pdf), which I think it is a good read. If you refined your preferences and use cases down you might arrive at an answer. Are you interested in going Scala only? Mixing Scala and Java libraries? Functional or Imperative? Would you prefer to use the Standard library or replace it?
In my experience, Intellij irreparably breaks some sbt projects by inserting settings into the build that are incompatible with each other. It reports false negatives even for code that doesn't use any fancy typelevel magic and transforms errors where the real compiler clearly states what's wrong into useless messages that have nothing to do with the real error. Don't know what I'm doing wrong compared to all these other people...
I might have expressed myself not clearly. I'm not interesting in particular library(s). I'm interesting in a guide of creating an application whatever libraries it is going to use. A step by step resource (book, tutorial, artikle, YouTube channel, etc) which helps to create a Scala app, whether it's a some API client, a game, a desktop scalafx application or some other app.
Look into the Play framework
Thanks, looks like Play has some app tutorials
I would be very happy if someone could give some advices.
The docs on their website will get you up to speed quickly.
Looking forward to seeing what this yields in the future. Right now, probably not the best idea to throw around any sort of benchmark until you have a relatively equivalent base capability to what we'd want out of a typechecker (e.g.g inference, hkt, implicits), but definitely keep us posted. Never mind the haters - this is a *good* thing. :)
*spicy boi* 
https://www.manning.com/books/functional-and-reactive-domain-modeling https://leanpub.com/fpmortals/read
I've noticed a definite lack in the standard subset of Scala regarding their treatment of IO, and I'm very happy to see J. DeGoes taking the reins (along with Andrescu) to see that we have proper, referentially transparent support for safe IO options, which, to be honest, should have been part of the standard library from the start. We can show try/catch/finally (and by extension Try "monad") semantics to be almost totally incoherent, and this will solve many our problems. The fact that it remains separate from the Scalaz std lib is also a plus, as this allows the user to remain modular with respect to their dependencies - a smart choice from an industry standpoint. It's very well known that many of our standard constructs (e.g. Future, Try) fall woefully short in terms of capability, and I'm very very happy to see a relatively complete featureset coming out of John's Fiber and IO work. I was also happy to see his catering to the not-so-theoretically-uptodate crowd, with constructs like `SafeApp` (a replacement for the App construct). And the benchmarks look pretty sweet, too, and open-sourcing them is only improving their correctness. The open review of potentially spurious benchmark claims is always appreciated.
I am having tough time debugging scala with intellij especially asynchronous calls and debug points inside map and flatmap not reaching as it's run on a different thread..how do you solve this? Do you have a nice tutorial I can follow somehow? Scala noob here, asking for help.
But they use Twitter future. I want to do the scala future. Any other examples?
like other users said, I no think you will find a book like that, on the other hand, if you are open to see my opinionated code for a project that is intended to go live, I've been working in this project: https://github.com/AlexITC/crypto-coin-alerts Basically, it is a project that let you received certain notifications about crypto currencies, at the moment it is almost back-end ready, you could see about authentication, error handling, i18n, testing with and without a database, testing the API, scheduling recurrent tasks, consuming external web services, catching mistakes at compile time with value classes, and some other stuff that might help you. The stack is play framework, anorm, play-ws, docker, postgresql and scalactic. While I still haven't write documentation about the architecture and how the project works, there are some comments in commonly used classes, also, feel free to ask anything about the project.
If this is just a learning exercise then it doesn't really matter, does it? Twitter futures and Scala futures are reaaalllllyyy similar.
Are you banned from Google for any reason?
Unless it's a guide to the same thing you're going to build, how does that level of hand-holding help? Part of being a programmer is looking at your problem and making these decisions yourself based on your needs. You're basically asking for something to do the thinking for you.
I recommend using Intellij or Scala IDE. Below is my version of why I don't recommend Emacs. I was a fan of Emacs + Ensime , however after the founder of the project stepped down and some major refactoring happened many of the features are not working. Also , you can't rely which features will break at any point of time. The community being young and expect to raise PR (as a fix) for any issues you point out in their forums. Since they expect all Emacs + Ensime users to learn elisp. If you are ok with the above pls go ahead and use Emacs.
Are you talking about this? http://openjdk.java.net/jeps/302 Looks like that is just for unused parameters, and not for revering to the parameter in the body of the lambda.
Yeah you‚Äôre right. I was thinking the pattern match , specifically, not the unused lambda parameters. In the pattern match it matches anything but I don‚Äôt think you can use it in the resultant case expression / statement. 
I don't get this reference :(
No, I'm basically asking for a tutorial that explains how to build a complete Scala application. Why, for example, I can see a lot of such tutorials in Android or JS world and can not see any in Scala?
Awesome! I will take a look at your project. Thanks and a kind request to add the documentation so any newbie like myself could pick up the ideas, meanwhile I'll get some knowledge of the technology stack used in your project
You are not supposed to pronounce anonymous variables, the idea is that you focus on the other ones
I'm using Intellij IDEA and `-Xfatal-warnings` (and a lot of other flags making Scala stricter, thanks `sbt-tpolecat`). Occasionally I want to test some thoughts in Scala worksheet, but with that flag often worksheet fails to compile, erroneous code being generated by worksheet transformation, not written by me. Is there a way to disable `-Xfatal-warnings` for worksheets only?
For beginners it makes a complete sense. Most of our Scala devs live in a bubble.
People have really incorrect conceptions about `Future`, something which was explained really well by Victor, its creator, over here https://contributors.scala-lang.org/t/upates-to-scala-concurrent-future-wrt-breaking-changes/1281/20 `Future` is not completely optimized for throughput, its optimized for fairness. This means its less performant than other IO/Task types, but it also means that computations which would greedily steal resources (and cause issues such as latency) are much less prevalent on Future compared to other IO types. Its much closer to Erlang in this regard (optimized for fairness, not throughput) `Future` also gives you fine grained segregation of resources at any point in your computation. This means its easy to specify that "this piece of logic will have one dedicated core and the rest of the logic will use the rest of the cores" simply by specifying different `ExecutionContext`s at different points in the code. Doing this is much harder (or not possible, i.e. on Haskell which is what Scalaz8 IO is based on), or it may require a complete rewrite of your logic. As you can see in that thread there is a lot of low hanging fruit wrt `Future` performance which still hasn't been done yet. All in all, I am grateful that there is competition in this space, but personally I wouldn't use Scalaz8 IO for the sole reason that it promotes defensive programming. That is, if you don't program completely correctly, you can have cases of async computations that don't terminate or have issues in determinism. Treating everything as referentially transparent and pure in Scala works fine for single threaded programming, because even if you miss an exception, or cause a `StackOverflowError` due to some type of recursion, its very easy to track. In the context of async programming its another ballgame entirely. I noticed that Scalaz8 IO added the ability to add an ExceptionHandler, and have the ability to run a piece of code expecting some exceptions, but considering exceptions can be thrown everywhere (even in RT pure code because of the JVM), and that issues of stack overflow with recursion are sometimes very hard to spot in async logic (Andrescu has spent a huge amount of effort to make sure this doesn't happen with Monix) means that ultimately its not useful for me. And before someone replies, yes I already know that treating exceptions as values isn't completely lawful (and it can't be). What matters to me more is practical implications, where if someone does something slightly wrong then you can get instances where your application stalls for ever, or you get completely undeterministic bugs which are a nightmare to debug, especially in production. This is what I mean be defensive programming, and its not a good mindset to program in. If Scala had full proper TCO and didn't have exceptions, it would be an entirely different ballgame.
&gt; I have committed +6 years of my intense focus and a huge sum of money (my life savings) into Scala. So, I am DEEPLY invested in Scala continuing to be successful and even grow. Damn. I'm so sorry. I hope you still have the opportunity to diversify your bets a bit.
"It". I'm trying to train myself to say "fa" or "fga" so that any pairs with low levels of experience will start thinking of the collections library methods as generic operations on generalized types F [_] and F [G [_]]. 
Is there a way in scala to keep the application running, until the user press `ctrl-c` for exit?
On that note, if people are interested in how people write code out loud, check out https://www.youtube.com/watch?v=1yn1P72I7CY#t=0m30s =P
It's essentially just better Java without too much of the fancier features of Scala. Could you just not use those features of the confuse you? Yes, or, apparently, you could make a whole new language without them!
My paranoia stems from having a Big Name in Scala declare that implicit parameters and currying are unreasonable because they aren't in the current subset would result in a battle defending FP that I would have a hard time winning with the current reasonable subset definition. I doubt I am alone in that position. The quotes that make me suspicious of the consequences of the outcome (intent doesn't matter, this is a research project after all): "Non-goals * Full backward compatibility (consider Lightbend Scala instead)" "Goals * Identify a subset of Scala that can be compiled with reasonable speed " "We are aiming to discover actionable insight into Scala compiler architecture and language design" I may be misinterpreting it, but it sounds like you will be making recommendations to speed up the compiler, including language changes. If no way can be found to optimize a feature, I assume the recommendation will be for the language to drop support for it, as the feature will not be deemed part of the reasonably fast subset. Is that the case? Who defines "reasonable"? What's the project's current definition of reasonable compiler performance? You say full backward compatibility is not the goal in the readme, but then say that anything less would be unacceptable. You are providing scalafix migrations, so you could conceptually be saying that scalafix provides the backwards compatibility by transpiling scala to reasonable scala. This could even be automatically performed on millions of lines of Twitter's code. Is that what you mean by backwards compatibility? I mean a backwards compatible api, and I don't think that is what you mean by compatibility. Saying trust me we're Twitter isn't telling me anything. Twitter has been great for the Scala community. The compile times there cost lots of money. rsc makes sense for you. Just remember when you make recommendations, your word is going to be interpreted by anybody at Java shops that don't like Scala as a license to hound the usage of that unsupported feature out out codebases at the added cost of any complexity necessitated by new ways needed to reduce that boilerplate. Not all codebase are so large as Twitter's, and many of us are happy to let the compiler do more work for us. Doing it faster would be nice, but I spend much more time writing code and in meetings than compiling. Personally the only time I bail is when macros start eating my day. Also remember the features you don't support speak loud volumes to people who don't like Scala already. Right now that's pretty much all the FP enabling features, and all the typelevel features. They're pretty much the features differentiating Scala from Java, (without method/constructor overloading, which even Java supports) which are rapidly shrinking already. I don't doubt your intent. I even admire it. Just, please be careful, because people are listening.
I just remembered, isn't it called a wunderbar? As in w-underbar (underscore) but also a play on the German word for wonderful.
I just started a side project in Kotlin and I find some of the ‚Äònice‚Äô syntax features rather questionable... like the implicit new operator, forced properties style instead of get/set methods, arbitrary keywords (like ‚Äòwhen‚Äô instead of ‚Äòswitch‚Äô). 
I think the features that don't perform well are likely to be the ones that support type level and functional programming, like inference, code generation, pattern matching, currying, and implicit resolution scope search. If they are left out, is it still Scala?
Hey I'm not sure exactly what you're trying to do. Are you trying to find the first index where the matched element is found? If so, then val matchedElement = 7 val array: Array[Int] = Array(1, 2, 77, 9, 0, 7, 2) val matchedIndex = array.indexOf(matchedElement) // 4
What you're asking/doing here doesn't make much sense. You already have the x(i) value, why do you need to store it again? Unless you're concerned about the case where the array doesn't hold the value, in which case your call to head will throw an exception. You could look into (given an a:Array[Int] and i:Int) * a.exists(i.equals) will return a boolean, true if there exists an element in a equal to i * a.find(i.equals) will return an Option[Int] -- either Some(x) if there is some x equal to i in a, or None otherwise. * a.filter(i.equals) will return an Array[Int] -- those elements of a that are equal to i. More generally, look at the [documentation for Iterable](http://www.scala-lang.org/api/2.12.3/scala/collection/Iterable.html) - these are the data structure functions you will come to know (and maybe love) if you write Scala regularly. 
Thank you for your detailed comment! It clearly highlights the moral responsibilities of compiler authors and provides a lot of food for thought. Indeed, right now we don't support many language features, because what we have is just a prototype, which we did our best to highlight as much as possible in the documentation. As you said yourself, at this point Rsc not only doesn't support advanced FP features, but it also doesn't support overloading and secondary constructors, which have nothing to do with FP. Therefore, I wouldn't read too much into absense of this or that feature. I think it is also premature to speculate about what language design recommendations (if any) we will be making to the Scalac and Dotty teams. Speaking of compatibility. At this point, Scala doesn't have a complete language specification, so I don't think that it is possible to have a factual discussion about compatibility. However, we don't intend to shy away from this discussion - one of our first-priority tickets is writing the specification for the subset of Scala that we're supporting (https://github.com/twitter/reasonable-scala/issues/12), and we aren't going to add features to Rsc until they are specified. This will ensure an open and transparent process that will answer all questions of compatibility. In conclusion, I would like to once again thank you for your incredibly thoughtful comments. It would be great to revisit many of the points that you made once we have something more concrete in Rsc. I would also like to invite you to follow our development (which will be done in the open from now on, since we went opensource with this announcement) and comment on pull requests and issues if you find time. Thank you!
Let's wait before jumping to conclusions. Right now, we are just building the foundation for performance experiments. Performance is such a tricky topic that I doubt that it is possible to accurately predict which features will slow down compilation the most. Also, please see https://www.reddit.com/r/scala/comments/7df8r6/prototype_of_the_reasonable_scala_typechecker/dq2ocwv/.
Wow you‚Äôre right. I‚Äôm new to Scala and my brain doesn‚Äôt work well on a Sunday night. Thank you for the help!
&gt;Future also gives you fine grained segregation of resources at any point in your computation. This means its easy to specify that "this piece of logic will have one dedicated core and the rest of the logic will use the rest of the cores" simply by specifying different ExecutionContexts at different points in the code. You can do this with cats-effect `IO`, and possibly with this one. &gt;That is, if you don't program completely correctly, you can have cases of async computations that don't terminate or have issues in determinism Funny coming from someone defending future, which is the #1 source for nondeterministic errors. &gt; that issues of stack overflow with recursion are sometimes very hard to spot in async logic (Andrescu has spent a huge amount of effort to make sure this doesn't happen with Monix) means that ultimately its not useful for me. Considering how you defend unmanaged side effects like your life depends on it, It feels like even, an IO monad that was baked into the language with perfect semantics and perfect TCO, I bet you probably wouldn't use it because it's not side effecting :) &gt; What matters to me more is practical implications, where if someone does something slightly wrong then you can get instances where your application stalls for ever, or you get completely undeterministic bugs which are a nightmare to debug, especially in production. This is what causes defensive programming, and its not a good mindset to program in. This is _exactly_ what future causes, holy shit. At least with `IO`, you have to semantically specify what you want, instead of having it done under the rug from you. Really, scala.concurrent.future is optimized for nothing. Twitter future vastly outperforms it and it has a completely different use case. At least with IO, you get what you put into it. 
* Occupy the main thread forever (like by sleeping) * Spawn a non-daemon thread (the jvm exits when all non-daemon thread's die)
I usually don't pronounce it, but when it's used in contexts where you could grammatically refer to it as 'it', that's what I call it.
Would be nice to give read-only for Kotlin posts. There is subreddit dedicated to Kotlin, why bringing it here? Forced language advertisement? Very annoying.
&gt; You can do this with cats-effect IO, and possibly with this one. I asked this many times and I haven't been demonstrated with a satisfactory answer. Even contributors from Scalaz (on the topic of Scalaz8 IO) have told me it isn't possible (or it is but its very ugly) &gt; Funny coming from someone defending future, which is the #1 source for nondeterministic errors. Haven't had a single non deterministic error in `Future`. The behavior of `Future` is very well specified &gt; This is exactly what future causes, holy shit. At least with IO, you have to semantically specify what you want, instead of having it done under the rug from you. No it doesn't. You are confusing referential transparency with non-deterministic behavior, this is not the same thing. `Future` catches any `NonFatal` exceptions that may be thrown **in any computation** and thus prevents issue with Future's never completing or having different results. Monix Task also does the exact same thing. With Scalaz8 IO if an exception is thrown in `map` or `flatMap` (since this is the JVM, exceptions can be thrown even if your claim your own code is pure) then you can get cases where your async computation never finishes because an exception is thrown in another thread which you aren't listening on. This is why Scalaz8 IO is so fast, it completely ignores exceptions (where as other implementations treat exceptions as a value and handle it accordingly) &gt; Really, scala.concurrent.future is optimized for nothing. Twitter future vastly outperforms it and it has a completely different use case. At least with IO, you get what you put into it. You did a good job of ignoring what I said. `Future` is optimized for fairness, twitters `Future` is optimized for throughput. This is what an `ExecutionContext` is (which is backed by a fork-join pool, similar to Haskell spark's are). It partitions the computations on different threads, and does a good job of making sure an expensive CPU blocking operation (that you may do in a `Future` .map) doesn't slow down other computations that are running
Fantastic article, however a minor nitpick. SBT's execution model is actually immutable, but the syntax makes it look like its completely mutable (which in fact even further supports your point, it makes it even more confusing!). The SBT task graph is completely made up of immutable `case class`'s (or classes which are modelled to be immutable). The `:=` syntax actually means "copy this value with a new value and adjust the graph with this new value" (which is how immutability works). The thing is, this is being hidden from the end user, hence why `:=` is a macro. I actually had this really long argument with someone on Gitter about this, they had an incorrect conception that SBT wasn't a purely functional immutable build tool and I actually explained to them, SBT (at least internally) is like this. The `:=` syntax gave a confusing impression that SBT is mutable, but if you look at SBT's source (which is quite hard to do since its so complex) you actually see that its completely immutable, with a lot of magic usage of macros to make it look mutable so its easier for people to work with. 
Also Martin has his own critique which is here (https://docs.google.com/document/d/1QdtRJGxlKTiXcAxsjXWLtWVzeZyUzGVDh8oOYOiuhwg/edit), its also posted as a comment in the article itself
I find it quite irritating that we have to see two-three kotlin posts a week when most people in the Scala community simply cannot give a f*** about it.
I switched from IntelliJ to (space)emacs around four years ago; I've been using Ensime for a while but whenever I had a bigger project (like the ones I usually have at work) it froze after a while and I had to restart it. I didn't try it in 1-2 years so it may be better (I know that 2.0 has finally been released). For smaller projects and experiments it works fine. I think that if you switch or not depends on how you value debugging/code-intelligence tools compared to the kind of tools you can find in an editor like Emacs; personally I find that I happily give up the debugger (hardly ever used in my career anyway), auto-completion and all that stuff for modal editing, vim keybindings, clever text-editing plugins (e.g. ace-jump, vim-surround, rainbow-delimiters, etc.), less memory usage, unified environment for all my coding needs (whatever the language I'm coding into it has support), keyboard based layout management (split screens any way you like, move buffer around, save configurations, etc.), the best version-control integration ever (magit) and a few other things here and there. It is also true that given that the kind of code that I often write involves some typelevel programming I find IntelliJ becomes actually an obstacle some times as I can't trust the errors it provides and the types it infers. If you want to go down that way and your project is too big for Ensime (or you don't want to shell out the time needed to make it work properly for your project) you will need to learn coding without auto-completion, which is hard at the beginning but surprisingly not so much after a while. I can advice to put something like Dash for MacOS on a keyboard shortcut to quickly navigate a unified collection of scaladocs when you need to remember the name of a function or what functions an object contains. Moving around the code base can be replaced with clever usage of functions like "search in project" and minimal regular expressions. Renaming symbols can be still done with regex and text-based interactions, albeit this is less solid than using code intelligence. The things I miss most in this setup in the end are "type at point", i.e. a way to know the type of some identifier in the code, semantic renaming and auto-completion. I look forward to developments in scalameta/language-server paired with some Emacs language-server client to get better tools. Nonetheless, this is a perfectly viable way of coding if you have a but of patience at the beginning with the learning curve and the time needed to customise things in the way you prefer. Despite what most people say, nothing prevents anyone to be as productive in an old-school IDE as in IntelliJ.
&gt; I think the features that don't perform well are likely to be the ones that support type level and functional programming, like inference, code generation, pattern matching, currying, and implicit resolution scope search. If they are left out, is it still Scala? This is an orthogonal question. I am stating that the project that Twitter is starting will be useful because it will show how complex certain features are. It doesn't necessarily means that the complete feature by itself adds a lot to compile time, for example implicits can either add very little to compile time but when abused heavily they can add a lot In any case, this is a good thing. Like with most things, there are tradeoffs. I don't think having Scala turn into something akin to a proof engine such as CoQ, where your "proofs" (i.e. compiles) take enormous amounts of time because you end up proving various things that are trivial. Point is that there are tradeoffs, and people should be aware of these things. That is all 
"mmm", "underscore", "thing" or "blyeh" depending on my mood.
its all fun and games until someone writes an abomination like `_.toBla.map(_.filter(_.scooby &gt; 0) ++ _.reduce(_ + _)) `
Thanks /u/lihaoyi for this very interesting blog post. He called for the creation of a new build tool on twitter : https://twitter.com/li_haoyi/status/931969505923817472 I think he is a very competent man and one of the most important contributor to its community (ammonite is soooo good !), but I don't know if it is a good idea to add a new build tool into an already confusing scala build tool ecosystem. Are gradle and cbt solving sbt problems ?
***IT IS*** a subset of a Scala. I don't understand the negative responses. Do people not understand the meaning of "subset"? This is just the beginning and the docs are very clear about the current status. Look at this code snippet here. How can anyone argue or imply that it's not a subset of Scala? https://github.com/twitter/reasonable-scala/blob/master/docs/language.md 
That example is Java with Scala syntax.
Can't speak for gradle, but CBT is solving the problems. In fact its doing (almost) everything that Martin specified in his own critique, i.e. it uses all of Scala's language constructs (instead of creating its own pseudo DSL) and it uses OO to solve the patched-DAG problem
&gt; The := syntax actually means "copy this value with a new value and adjust the graph with this new value" Isn't that indistinguishable from what assignment in a mutable programming language means? `x = 4` could be said to mean "copy x with this new value and adjust the graph with this new value". These terms can't be defined absolutely, but it sounds like SBT is only purely functional and immutable in the same sense that [C is a purely functional programming language](http://conal.net/blog/posts/the-c-language-is-purely-functional).
&gt; Red wiggly lines looks a little ugly You can hide them. On a per-project or per-file basis; or even for specific sections of a file. Red squiggles are not a fatality :\^)
I just find it dog slow
No its not. All of the underlying data structures in SBT are immutable, you can't mutate their references. The graph that SBT makes is completely immutable. In order to make a change, you have to make a new reference with the change data. In a mutable programming language, x=4 means overwriting the value that is stored in memory, which causes all sought of issues related to mutable programming (race conditions with threads, dangling pointers if you are in C, etc etc) 
Keys may be immutable in the scala runtime (layer 1) but in the "sbt runtime" where keys get evaluated (layer 2) they are globally accessible and mutable by any plugin or build code. You can even override private keys by referencing their names `TaskKey[T]("privateSetting") := ...`. That's at least how I see it.
&gt; but in the "sbt runtime" where keys get evaluated into a task graph (layer 2) they are globally accessible and mutable by any plugin or build code. They are globally accessible yes, but the reason why its globally accessible is precisely because of the issue of modifying the graph in an immutable fashion (and is one of the points of the article wrt global keys). With immutable graphs you need a reference to the node that you want to "modify" (or in immutable speak, copy with altered data). To avoid manually traverse the tree to find the node you want to copy and modify, you can just have a global table with a reference to all the nodes and hence this is the reason why all of the keys in SBT are global (and hence all of the issues that come with having global keys) Here is the actual definition of the macro which gets called when you use `:=` def taskAssignMacroImpl[T: c.WeakTypeTag](c: Context)(v: c.Expr[T]): c.Expr[Setting[Task[T]]] = { val init = taskMacroImpl[T](c)(v) val assign = transformMacroImpl(c)(init.tree)(AssignInitName) c.Expr[Setting[Task[T]]](assign) } The definition of `AssignInitName` is `set`. So basically `:=` implementation is `set` which is what you have to sometimes use when you are in the repl. And here is the definition of `set` def set[A](key: AttributeKey[A], value: A) = copy(attributes = this.attributes.put(key, value)) As you can see, its copying an immutable value, you can see the reference here https://github.com/sbt/sbt/blob/05c2c506b2b218d8ed8befcef843951391c07be6/tasks-standard/src/main/scala/sbt/Action.scala
I'm not entirely convinced of the need to handle incrementality in the build tool itself. Some build steps don't need to be done if nothing has changed, but others do, and figuring out what the "input" is is very much nontrivial (`make` assuming that the input is always files with timestamps causes no end of trouble). In my workflow the only case where I invoke a command-line build is when I want a "full" build e.g. for release, in which case I would if anything prefer a "clean" build; my edit-test loop never leaves my IDE, so all I really need is incremental Scala compilation in the IDE with hot reload of running code; webpack or docker don't come into it, and while I wouldn't say no to incremental compile of protobuf changes (again more of an IDE feature than a build tool feature), it's so marginal that I honestly can't remember whether I currently have that or not. For me most of SBT has always been a solution in search of a problem. Maven works very well: it has excellent IDE integration, it enforces that the build remains simple and consistent, and encourages any custom build steps be defined once in a first-class code module (plugin), reusing much of the same infrastructure (e.g. it is no problem to have a multi-module project where one of the modules is a build plugin used by other modules). It doesn't allow having very complex build flows for a single module, but I find those complex build flows are usually better understood as multiple modules in any case. The one thing it doesn't do is cross-building, but SBT's cross-build support seems to be hackish as well: it works for building against multiple versions of Scala, but projects that need to cross-build in other ways (e.g. built against multiple versions of scalaz, or against scalaz and cats, or multiple versions of fs2/scalaz-stream) tend to use multiple branches, template substitutions, and so on, rather than getting any help from the cross-building support built into SBT. I'd be interested to see a build tool that kept the good things maven offers while also offering good support for cross-building in a general way. But I don't know that there's anything from SBT that helps with either of these aspects.
Might be worth building on either scalaz or matryoshka. While building recursion-schemes for yourself is a fun exercise, starting from the use cases might make it easier to see the practical advantages.
The one issue with maven is it doesn't support cross building, and spark afaik actually needs to manually patch the `pom.xml` file to support cross building Maven gets the DAG right, but you can't patch the graph which is what you need for things like cross-version scala building
Martin‚Äôs (lack of) leadership here is really frustrating to me. I was active in Scala back when he wrote his original critique of sbt and remember thinking ‚Äòthank god, this is finally going to get fixed‚Äô. And then... nothing. I think they assigned a Typesafe engineer to work on it but nothing fundamental changed and the community just limped along dragging this anchor. He had an opportunity to set a clear standard of using maven or encourage a new project, instead he just joined the chorus of complaints without offering any real solution, and the community suffered for it.
&gt; Martin‚Äôs (lack of) leadership here is really frustrating to me. I am not sure if its wise to blame martin here. He specified his critique, but he is not a benevolent dictator. Martin wrote his critique and people either ignored it or they noticed it and they didn't do anything about it, thats not really his fault
Nobody cares what SBT does internally. The problem is that it reimplements a mutable programming language, with all the confusion and hard-to-reason-about behavior that this entails. It's as if you wrapped everything in `IO` in Haskell, which would be a huge code smell. There's little value in using functional programming if you're reimplementing imperative programming on top of it.
sbt being immutable itself is IMO an implementation detail, the public interface presented to users is an execution model where every `TaskKey[_]/SettingKey[_]` can be read and updated by any `Setting[_]` that gets evaluated.
@mbseid @ranci @joshlemer ??? 
What's CBT?
Right, I think we are just being pedantic. My point is just, that in every aspect SBT is actually immutable. The way that the current DSL is defined makes it look like its mutable, but it isn't (I mean, if something is immutable you can't get around it, Scala wont allow you to update the references in a mutable way) I mean this is like people saying that in Haskell, the `do` notation makes Haskell look like its imperative. Yes it looks like its imperative, but in fact the `do` notation is syntactic sugar over monads which just translates to chained `map`/`fmap`
You know, I used to think like you do, with the exact same reasoning. Then I realized that the OP is actually right. You can also take any program written in a highly mutable fashion, then instead declare that its statements are building a `Program` data structure, and the implementation of the `=` method is a function of the form: class StateOfTheWorld { def set[A](varKey: VarRef[A], value: A): StateOfTheWorld = this.copy(varValues = this.varValues.put(varKey, value)) } This proves that the *data structures* involved in your "interpreter" are completely irrelevant from the point of view of the (im)mutability of your program. You can implement an imperative program with a functional interpreter, and you can implement a functional program using an imperative interpreter. These things do not matter. What matters is the level at which you think about *your program*, not about the interpreter that interprets your program! And with sbt's `:=`, the level at which you write *your program* (= your build) is using mutable state. One pretty compelling concrete consequence of that mutability is that the order in which you write `:=`s matter (when they have the same left-hand-side), from the point of view of your program. And that also means that if two independent sbt plugins perform `:=` on the same key in the same scope, the order between the plugins being left unspecified means that the result of your program (= build) is unspecified. This is imperative programming for you.
&gt; In a mutable programming language, x=4 means overwriting the value that is stored in memory That's one possible implementation, but not the only one. For example, Brainfuck is a mutable programming language, but I can write a Brainfuck interpreter using only immutable data structures: @tailrec def run(p: Int, data: Vector[Byte], outputAccumulator: Vector[Byte]): Vector[Byte] = data(p) match { case '&gt;' =&gt; step(cmd.tail, p + 1, data, outputAccumulator) case '+' =&gt; step(p, data.patch(p, Vector(data(p) + 1), 1), outputAccumulator) ... } Conversely some immutable languages will actually overwrite a memory location with a new value, if they can see that the old value wasn't used any more. The difference between a mutable and an immutable language is in the language's *semantics*; `x=4` is a mutation because it changes the evaluation of future expressions that refer to `x`. If a `:=` statement does the same thing in SBT (i.e. some expressions that mean one thing before the `:=` mean something different after the `:=`) then it's a mutable language to the extent that we can make the distinction. &gt; which causes all sought of issues related to mutable programming (race conditions with threads, dangling pointers if you are in C, etc etc) Those are real issues but they're not the heart of the problem with mutable programming; there are mutable programming languages that don't have any of those issues. The fundamental maintainability problem with mutation is that it breaks composition: the "action at a distance" makes it impossible to understand small parts of the program in isolation, because any part of the program could affect any other part by changing a value that's used there.
To be honest, I've been using SBT in a cargo-cult programming way for years, and have never felt like trying to dig deeper. I could probably spend a weekend wrapping my head around how it's supposed to work and learning all its idiosyncrasies, but I just don't want to. From what I've seen, my impression is that it's just an immoral amount of incidental complexity.
&gt; One pretty compelling concrete consequence of that mutability is that the order in which you write :=s matter (when they have the same left-hand-side), from the point of view of your program. And that also means that if two independent sbt plugins perform := on the same key in the same scope, the order between the plugins being left unspecified means that the result of your program (= build) is unspecified. I mean if your complaint is that you can use `:=` everywhere similar fashion that you would use in a mutable way, I agree! I even mentioned this much earlier. My general point though, is that if you didn't have `:=` (and we still kept SBT's immutable internals), then it would be even harder (from a user usability POV if making a build tool easy to use). This would be the exact same as if Haskell didn't have `do` notation (which is just syntactic sugar over chained monadic comprehensions that makes it look like imperative programming). So yes, you could argue that you can encapsulate mutable programming over immutable code or vice versa, my argument is that this isn't the case here. SBT has macros which provide syntactic sugar, but mutation in the sense of the literal meaning of the word (updating mutable references) isn't happening here.
See my argument here https://www.reddit.com/r/scala/comments/7e7i6i/so_whats_wrong_with_sbt/dq35y7r/ You can claim this, but this isn't different to Haskell having `do` notation which is just syntactic sugar over chained `map`/`fmap` calls. Its making something look like its imperative, but in reality its just chained monad calls. The core point I am making here, is that the internal model in SBT is immutable, you can't actually get around this. Scala (without runtime bytecode manipulation via reflection) won't allow you to update the references inside the tree.
&gt; Nobody cares what SBT does internally. The problem is that it implements a mutable programming model, with all the pitfalls, confusion, and hard-to-reason-about behavior that this entails. This is ironic, because the SBT DSL (which is what you are actually complaining about), is abstracting over the very well known issues of working with immutable graphs. Its syntactic sugar, and it exists for the very same reason that Haskell's `do` notation exists. The `:=` is not doing mutable programming, its actually using `.copy` on case classes. It looks immutable because keys are referenced in a global table, so you can just reference a key from everywhere, but there isn't any mutation of var variables anywhere. In fact SBT got to this design because its original creator (Mark) insisted on keeping the internals functionally pure, and this was the only known way to make a DSL which was actually usable for people
[Chris' Build Tool (CBT) for Scala](https://github.com/cvogt/cbt)
kotlin: the language for those who aren't smart enough to learn scala
But it *doesn't matter*. `do` notation in Haskell (and `for` comprehensions in Scala) is slightly different because it does not have the "action at a distance" issue. Of course if you reorder statements within a single `do`/`for`, the effect of your program will vary. But they are tight together in a single place, so you always do reordering in full conscience. This is not the case for `:=` to the same key coming from different sbt plugins. We could rewrite the internals of sbt to use actual mutations inside, *and not a single .sbt build would see a difference*. It wouldn't help nor worsen the ability to write a correct build and reason about it. Let me repeat it once more: whether the interpreter is written in imperative or functional style **is completely irrelevant/orthogonal** to whether your program is written in imperative or functional style, and hence has no consequence on the level at which you have to reason about your program. &gt; My general point though, is that if you didn't have := (and we still kept SBT's immutable internals), then it would be even harder (from a user usability POV if making a build tool easy to use). I am not arguing about whether it is a good or a bad idea to have mutable build definitions. In fact, I am very much in favor of the level of mutability that `:=` offers me (i.e., where ordering is relevant for the same scoped key, but irrelevant and really functional for different scoped keys). This is what gives me the ability to write pretty sane builds, while at the same time the ability to write an sbt plugin such as sbt-scalajs which deeply changes core aspects of the compilation. But pretending that it's functional when it's not is not helping.
https://github.com/cvogt/cbt
Kotlin-related posts had the highest activity in this forum lately.
Or maybe it is a documentation issue? If someone who used Scala for weeks suggests using Kotlin, because he can't come up with a single thing Scala does better than Kotlin (except pattern matching) that at least points into the direction of documentation being an issue.
&gt; One pretty compelling concrete consequence of that mutability is that the order in which you write :=s matter (when they have the same left-hand-side), from the point of view of your program. And that also means that if two independent sbt plugins perform := on the same key in the same scope, the order between the plugins being left unspecified means that the result of your program (= build) is unspecified. Saying this is like saying that overriding a method in a subclass is mutability. I believe it's exactly the same analogy. And it doesn't sound to me like a mutable operation. Mutability has a very concrete meaning: https://en.wikipedia.org/wiki/Immutable_object, and I don't think the sbt model is mutable by the conventional meaning of mutability.
**Immutable object** In object-oriented and functional programming, an immutable object (unchangeable object) is an object whose state cannot be modified after it is created. This is in contrast to a mutable object (changeable object), which can be modified after it is created. In some cases, an object is considered immutable even if some internally used attributes change but the object's state appears to be unchanging from an external point of view. For example, an object that uses memoization to cache the results of expensive computations could still be considered an immutable object. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Meanwhile, what's the official status of scala-async?
&gt; But it doesn't matter. &gt; do notation in Haskell (and for comprehensions in Scala) is slightly different because it does not have the "action at a distance" issue. Of course if you reorder statements within a single do/for, the effect of your program will vary. But they are tight together in a single place, so you always do reordering in full conscience. This is not the case for := to the same key coming from different sbt plugins. This is orthogonal to what I am stating though. I mean I posted here https://www.reddit.com/r/scala/comments/7e7i6i/so_whats_wrong_with_sbt/dq354gq/ exactly what `:=` is doing, its a macro that resolves to internally calling `.copy`. If you had a global key table in Haskell, you would have the exact same features/problems. &gt; But pretending that it's functional when it's not is not helping. I am saying that SBT's core is completely functional/immutable but its "style" is mutable. This is factually correct, and its actually really helpful because there are a lot of myths going around SBT and people are getting the completely incorrect idea of what SBT is and how it works. If we are going to have a rational debate about the real usability issues that SBT has, we need to actually correctly know what is being discussed. I mean this from experiences in the past, where people were going around and saying "If only SBT had an immutable core/design" and then I have to proceed to tell them that "thats how its currently designed..."
&gt; Saying this is like saying that overriding a method in a subclass is mutable. Almost. Except that overriding has one set in stone, specified order: that of linearization order from the Scala specification. `:=` does not have that guarantee: two sbt plugins calling `:=` on the same left-hand-side, unless they are explicitly ordered by a dependency, create "true mutability" in the sense that depending on the order in which the sbt engine decides to compose them (and this is *unspecified*), the resulting (functional) data structure inside sbt's model will be different. Hence your program is different -&gt; order matters. The override thing applies to `:=` on *different* left-hand-sides. And that, as I mentioned above, is indeed truly immutable in sbt, and is what allows to create powerful sbt plugins such as sbt-scalajs that nevertheless compose very well with other powerful sbt plugins.
Documentation is a huge issue. If you look at the Scaladoc and pick a random class, you have a high change of it bring complely devoid of _any_ documentation. The breakages with 1.0 were pretty painful, too. I expected that if I fixed all deprecation warnings in 0.13.16, I would be good to go. I thought this was a reasonable expectation to have. But in addition to the things that were deprecated, there were probably at least as much breakages where things just disappeared, without any kind of deprecation.
&gt; See my argument here https://www.reddit.com/r/scala/comments/7e7i6i/so_whats_wrong_with_sbt/dq35y7r/ You linked back to my post? &gt; You can claim this, but this isn't different to Haskell having do notation which is just syntactic sugar over chained map/fmap calls. Its making something look like its imperative, but in reality its just chained monad calls. Sure, I think most Haskell folk acknowledge that `do` with IO is an imperative DSL embedded in Haskell. Haskell programmers are encouraged to keep as much as possible of their code/logic outside IO - using IO means giving up most of the maintainability/testability/... advantages of functional programming, so it's only used at top level where you need to do imperative operations. &gt; The core point I am making here, is that the internal model in SBT is immutable Agreed, I just don't think it's relevant. What affects maintainability, testability and so forth is whether mutability is part of the language's semantics, not the details of how the runtime happens to be implemented.
&gt; := does not have that guarantee: two sbt plugins calling := on the same left-hand-side, unless they are explicitly ordered by a dependency, create "true mutability" I don't think this is correct and the immutability holds as well for the same left-hand side. You said it yourself: `:=` has that guarantee if they are explicitly ordered by a dependency, and for another external task to see the mutability there has to be a dependency. &gt; in the sense that depending on the order in which the sbt engine decides to compose them (and this is unspecified), the resulting (functional) data structure inside sbt's model will be different. The same would hold for a functional, immutable compiler like ghc.
&gt; exactly what := is doing I know exactly what `:=` is doing. &gt; I am saying that SBT's core is completely functional/immutable but its "style" is mutable. I agree with that statement, but from the discussion this is the first time that I understand that this is also what you are saying. Phew, it does seem that on some profound level we do agree. Nevertheless, I think, as I have explained above, that whether or not sbt's core is completely functional and immutable is totally irrelevant from the point of view of someone writing builds. The sbt *style* is what matters. Just the same way that it doesn't matter whether the core of my Lisp interpreter is mutable or immutable when I am writing a Lisp program. Anyway, this is probably where I will stop contributing to this conversation. Good news for you: you can answer once more and have the final word :p
Yes I think we are in agreement, apologies for any misconfusion! I think what the core of something is may not be relevant for people writing builds, but its very relevant to core design issues which the build tool may have (hence the reason why we are having this discussion!)
&gt; You said it yourself: := has that guarantee if they are explicitly ordered by a dependency, and for another external key to see the mutability there has to be an explicit dependency. Not the same notions of "dependency"! The first occurrence of "dependency" in your sentence refers to *dependencies between plugins* (as in `override def requires: Plugins = plugins.JvmPlugin`). The second occurrence of "dependency" refers to *dependencies between tasks* (as in `foo := bar.value`). It is very much possible to have dependencies between tasks that are provided by plugins that are not dependent on each other. There be dragons. For a very dumb example: if I have two sbt plugins that redefine `compile := theirOwnThing`, those plugins will not compose. And I have *no idea* which one will "win".
The [Lack of Caching](http://www.lihaoyi.com/post/SowhatswrongwithSBT.html#lack-of-caching) section is far and away the biggest pain point with Sbt at the moment. For example, start sbt with `sbt -Dsbt.task.timings=true` and notice how the `update` task is run on every `compile`, likely consuming most of the "build" time in a noop compile. Ideally `compile`, `test`, `run` and friends wouldn't be coupled to `update` such that one's time is completely wasted on *every* run of the task. At best `update` is partially cached; there is no out-of-the-box zero overhead implementation of this task. The same holds for many plugins since caching in Sbt is exceedingly difficult, particularly if your plugin task(s) aren't dependent on the filesystem (where you can benefit from `FileFunction.cached`). I naively tried to implement a cache for Coursier to workaround `update` consuming majority of incremental build time: Def.taskDyn { def result() = resolutionsTask(sbtClassifiers).value if (!coursierSkipInCompile.value) Def.task(result()) else Def.task( // result stored in cache but task executed over and over // why? We're in Sbt's macro layer, `value` always re-evaluted resolutionsCache.getOrElseUpdate( coursierProject.value, result() ) ) } It's truly a maddening experience trying to do anything even remotely outside the realm of simple `*.sbt` file assignments. Add to that the IDE issues mentioned in the article and one is really up against it. Hopefully alternative build tools (like CBT) mature to the point that they become viable replacements for Sbt. 
&gt; You linked back to my post? Whoops, I meant https://www.reddit.com/r/scala/comments/7e7i6i/so_whats_wrong_with_sbt/dq354gq/ &gt; Sure, I think most Haskell folk acknowledge that do with IO is an imperative DSL embedded in Haskell. Haskell programmers are encouraged to keep as much as possible of their code/logic outside IO - using IO means giving up most of the maintainability/testability/... advantages of functional programming, so it's only used at top level where you need to do imperative operations. Yes, but the idea is that in a lot of non trivial cases (i.e. you are writing a webserver), you always end up in the `IO` level because this problem space involves a lot of sequencing, so you can't get around it. If your problem space requires you to be in `IO` all of the time, then I don't think you can claim that people are programming in Haskell incorrectly because they are in `IO` all of the time. This is kind of getting to my core argument here, in my opinion the FP approach that SBT uses is the wrong kind of tool to solve the problem which is whats creating a lot of the usability issues. Either a reactive programming approach (described in Martin's document) or an OOP approach (what CBT does) I think is a much better tool to solve the issues we have with SBT. Working with immutable graphs where each node represents expressions that are to be evaluated (which is how SBT is designed) is a massive PITA. You need to have a global set of keys so you can override these expressions (i.e. you need a reference to `compile` without traversing the tree to find `compile`) and then when you have the concept of scopes and namespacing, you get all of these issues (I think one of the weakness of FP is namespacing, its something that I find very painful in Haskell)
Sure, they create flame and flames are always great if you want to just measure "activity". Moreover this doesn't discount the fact that this post is totally off-topic: Scala has a different story for async and this is not even a comparison of solutions evidencing trade-offs and interesting stuff, it is pure trolling.
Oh, absolutely. But is that underspecification of "which will win" mutability? Only one will win and only one will be visible to the rest of the sbt runtime. There's no modification of the state.
So what is Scala's story with scala-async? Looking at the commis, it seems to be somewhere between "on life support" and "maintenance only". I think the " flames" and "trolling", or more general, the bitterness from some members of the Scala community, seems to a result of how the Kotlin/Scala story worked out, the discussion about it in Kotlin-related posts is just a symptom of that.
I don't care about the fact that Kotlin got async, as far as I am concerned the depth of scala makes it way less an important feature at the language level, so much that I would even question wether it is important at all. You have plenty of libraries to do clever async in composable, safe, controlled, functional way (scalaz-io, cats-effect, monix, akka, vanilla `Future`s, scalaz `Task`, fs2 just to mention a few). As I said, the story for Scala is different and this post doesn't even go anywhere near mentioning that so why is it in the Scala subreddit? There is the Kotlin subreddit for talking about Kotlin solutions. I don't think that people in /r/kotlin would be happy if every two days someone from Scala go there and open a post to flaunt about pattern-matching, HKTs and whatever Kotlin doesn't do that Scala does. 
Here's a sneak peek of /r/Kotlin using the [top posts](https://np.reddit.com/r/Kotlin/top/?sort=top&amp;t=year) of the year! \#1: [Kotlin officially supported in Android!](https://np.reddit.com/r/Kotlin/comments/6bqlqb/kotlin_officially_supported_in_android/) \#2: [Kotlin 1.1 is out](https://blog.jetbrains.com/kotlin/) | [11 comments](https://np.reddit.com/r/Kotlin/comments/5wvoue/kotlin_11_is_out/) \#3: [Kotlin 1.1.3 is out](https://blog.jetbrains.com/kotlin/2017/06/kotlin-1-1-3-is-out/) | [9 comments](https://np.reddit.com/r/Kotlin/comments/6j28vk/kotlin_113_is_out/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
&gt; &gt; The core point I am making here, is that the internal model in SBT is immutable &gt; Agreed, I just don't think it's relevant. What affects maintainability, testability and so forth is whether mutability is part of the language's semantics, not the details of how the runtime happens to be implemented. Yes, it's a distinction without a difference.
&gt; someone from Scala go there and open a post to flaunt about pattern-matching, HKTs and whatever the person opening the post thinks that Scala does better than Kotlin [Looks like they are fine with it](https://www.reddit.com/r/Kotlin/comments/7dbnk4/functional_programming_with_kategory_overview_of/).
Maybe I don't understand what you mean, that looks like a talk about a Kotlin library in the Kotlin reddit. Sure, they are talking about category theory and something inspired to work that is being done also in Scala, but those are abstract concepts and not language ones. And anyway, it's not one post the issue it's the pattern of one-post-every-two-days that is pissing people like me off.
&gt; Personally, what put me off of SBT is its insistence on spreading random files everywhere, often mixing config and cache all over the place: &gt; .sbt/, project/, build.sbt in project root etc. Just the fact that it's creating a `target/` in the directory from which I run `sbt new` is pretty annoying. I don't want SBT to leave its litter all over my home directory.
If you replaced `:=` with `.copy` on the immutable attributes `Map` you wouldn't fix anything with SBT, but you could now call its style "immutable". The mutable style that SBT does with its DSL is a solution to boilerplate (i.e. it removes the `.copy` on sbt key's/settings) but this isn't the problem here. This argument over SBT being "mutable" is just a distraction from its core issues
By the creator of Slick.
My sbt sessions hit OOM exceptions far too often. It's not as bad as lein, though.
I agree, there doesn't seem to be any point to the process given from the article.
It sounds like a mutable reference on **im**mutable `List`: var list = List(1) list = 2 :: list 
&gt; Maybe I don't understand what you mean, that looks like a talk about a Kotlin library in the Kotlin reddit. A Scala person going to Kotlin, proposing to implement higher-kinded types and typeclasses, whose design is directly influenced by Scala.
Sure, cut the quote in the way you like, forget about what I say after the `.`: &gt; Sure, they are talking about category theory and something inspired to work that is being done also in Scala, but those are abstract, mathematical concepts and not language ones. And also: &gt; The Kotlin posts in the Scala reddit are more akin to someone going to Kotlin and creating a mostly scala-biased post with title: "The comparison between cats and Kategory". And: &gt; Anyway, it's not even the single post the issue, it's the pattern of one-post-every-two-days that is pissing people like me off. If you want to answer don't cherry pick at your convenience.
&gt; Yes, but the idea is that in a lot of non trivial cases (i.e. you are writing a webserver), you always end up in the IO level because this problem space involves a lot of sequencing, so you can't get around it. If your problem space requires you to be in IO all of the time, then I don't think you can claim that people are programming in Haskell incorrectly because they are in IO all of the time if thats what the problem asks. I'd absolutely say if you're working in IO all the time then you're doing Haskell wrong. A lot of the work of writing something like a webserver in a language like that is finding good declarative representations for things; look at something like rho's routing representation. Even if what you're doing requires sequencing, you can often sequence in a much more limited subset of IO that's less imperative, e.g. if you work with `State` then you've got a mutable value, but only one, locally scoped, and you can reasonably reason about your code from the functional perspective (as a `S =&gt; (S, A)`) because the effect you're dealing with is so small and isolated. Yes at some point you have to interact with IO, but you shouldn't ever need it in the logic layer; worst case you come up with a representation for those operations you do need to perform, and then write your logic in terms of those, and interpret separately into IO. The full IO representation[1] is just too complex to reason about; unfortunately it sounds like the same is true for this SBT variable graph representation. I mean fundamentally if you were using IO all the time with your logic there just wouldn't be any point in Haskell - if what you're doing is so fundamentally imperative (and I'm not sure business problems with this character really exist, but assuming for the sake of the argument they do), it's better to just use an imperative language. &gt; Working with immutable graphs where each node represents expressions that are to be evaluated (which is how SBT is designed) is a massive PITA. You need to have a global set of keys so you can override these expressions (i.e. you need a reference to compile without traversing the tree to find compile) and then when you have the concept of scopes and namespacing, you get all of these issues (I think one of the weakness of FP is namespacing, its something that I find very painful in Haskell) Agreed, though I'm not convinced that a build tool needs to look like that. I would hope and trust it's possible to build a useful build tool in a functional way; indeed I see no reason something like maven couldn't be implemented functionally. [1] Not really a "representation" in a purist sense since IO is conventionally considered to have no denotational semantics, but in practice most of us view IO actions as having meanings if only informally.
~~Complex Build Tool~~ :)
I think the whole idea of quoting something is that it allows the conscious selection of a subset of the original text to make it more clear to what a person responds to. Nevertheless, I removed the quote if that makes you happier.
I agree to stop jumping to conclusions. It's already known that broad implicit search, unification, and macros 
It doesn't make me happier, what you did is a very simple way to create a straw man: https://en.wikipedia.org/wiki/Straw_man#Structure
Any acknowledgment coming?
False, Stephan Zeiger is the creator of Slick (just look at the [commit history](https://github.com/slick/slick/graphs/contributors)).
 // ~/.sbt/1.0/global.sbt target := { file("/tmp/sbt") / name.vaue }
&gt; if what you're doing is so fundamentally imperative (and I'm not sure business problems with this character really exist, but assuming for the sake of the argument they do), it's better to just use an imperative language. Maybe IO was the wrong example, but the same argument applies with Monads (Haskell's `do` notation is syntactic sugar over monads, its nothing specific to IO), i.e. when you say &gt; Even if what you're doing requires sequencing, you can often sequence in a much more limited subset of IO that's less imperative, e.g. if you work with State then you've got a mutable value, but only one, locally scoped, and you can reasonably reason about your code from the functional perspective (as a S =&gt; (S, A)) because the effect you're dealing with is so small and isolated. Yes at some point you have to interact with IO, but you shouldn't ever need it in the logic layer; worst case you come up with a representation for those operations you do need to perform, and then write your logic in terms of those, and interpret separately into IO. The full IO representation[1] is just too complex to reason about; unfortunately it sounds like the same is true for this SBT variable graph representation. A lot of this type of work does involve monads at some point, and sure you may not be using `do`, but then you are using the basic map/fmap functions (which `do` is syntactic sugar for) &gt; Agreed, though I'm not convinced that a build tool needs to look like that. I would hope and trust it's possible to build a useful build tool in a functional way; indeed I see no reason something like maven couldn't be implemented functionally. I have seen it done for build tools which have a much more trivial problem space compared to Scala. It can work fine for source based distribution, but because we are working with binaries in Scala we need to patch these graphs, and now its a fairly common operation and its gotten even more complicated with scala.js and scala-native. 
Thanks for the follow up! I will follow the project with interest. 
I'd like to know about these usages that are not possible without boxing. Do you have an example or a link to further discussions on this?
yes
The argument over the purity of the implementation is just a distraction from its interface to the user; how it's used. I think what you're arguing here, while perhaps relevant in some sense, is not relevant for this article.
&gt; It sounds like Yes exactly, it sounds like, but its not mutation. You can write in this style in Haskell with using `do` notation however you aren't actually writing any new references and you are not mutating anything Actually historically in SBT, you couldn't do this sequential style of programming, i.e. var list = List(1) list = 2 :: list list = 7 :: list If you wanted a task that ran a sequence of tasks, you would have to chain `dependsOn`. Then at some point in time, `Def.sequential` was introduced and if you look at its implementation you can see its recursive, i.e. def sequential[B](tasks: Seq[Initialize[Task[Unit]]], last: Initialize[Task[B]]): Initialize[Task[B]] = tasks.toList match { case Nil =&gt; Def.task { last.value } case x :: xs =&gt; Def.taskDyn { val _ = x.value sequential(xs, last) } } Just like in immutable programming ;)
I thought like this once, I bought and went through Josh Suereth's [SBT in Action](https://www.manning.com/books/sbt-in-action) and it still didn't clear much up for me. But I only had like 6 months of experience at that time so maybe I'd get more out now.
&gt; The argument over the purity of the implementation is just a distraction from its interface to the user; how it's used. Exactly, and my argument here is that the core problem is not due to SBT being "mutable" (or more correctly "mutable in style"). You can get rid of this "mutable" style completely and you would have the exact same problems
It was in one of the threads discussion implementation details of HKT in Rust, I will have to try and find it.
I always love it when people talk about fix-point combinators! Nice job, and it progressed very smoothly. If you'd like some ideas, I'm also in the process of writing a very long and tedious universe based for generalized recursion schemes with some relatively complete example sets for the basics (cata, ana, hylo, meta, postpro etc) over [here](https://github.com/emilypi/Cata-Mu-Fix/tree/master/src/main/scala/org/emilypi/schemes). I always thought showing people how to find the nth fibonacci number by representing it as a hylo over `TreeF` was mindblowing (see the examples package). Great stuff so far!
I don‚Äôt really buy this. At the time he wrote that critique, in addition to being the founder and figurehead behind the language, he was also chief architect at the company trying to commercialize Scala, and actively making strategic decisions about the direction of the platform. It‚Äôs precisely *because* sbt had the inertia of a network effect that it needed a strong voice to say ‚Äúthis is not the way forward‚Äù. Who else would have been in a position to spur a change in direction at that time? But if we grant that it wasn‚Äôt really his place to try to influence this, we can still lament that the statement he did choose to make was in favor of continued sbt development, and that Typesafe chose to put some of its limited resources to work on it. If they weren‚Äôt going to really fix the problem, at least they could have left it alone to avoid giving the appearance of blessing it.
&gt; I don‚Äôt really buy this. At the time he wrote that critique, in addition to being the founder and figurehead behind the language, he was also chief architect at the company trying to commercialize Scala, and actively making strategic decisions about the direction of the platform. It‚Äôs precisely because sbt had the inertia of a network effect that it needed a strong voice to say ‚Äúthis is not the way forward‚Äù. Who else would have been in a position to spur a change in direction at that time? Honestly, only really Martin can answer this. From memory at 2011 (which is when this was written), I don't think this would have been possible. In any case, my point is that a lot of people in the Scala community have been complaining that Scala is not democratic enough (which indirectly means taking away control over Martin in these matters, if he ever had any) &gt; But if we grant that it wasn‚Äôt really his place to try to influence this, we can still lament that the statement he did choose to make was in favor of continued sbt development You can't say this, Afaik Typesafe has various board members, it doesn't have someone that can just do whatever they wan't un-opposed. Furthemore SBT is an open source project, this is not something that typesafe (now lightbend) can influence (and a lot of current work on SBT is being done on by Scala center, which has a board of ~8 people who vote on any decisions that need to be made) Note, Scala-Center is not typesafe (now lightbend). They are completely separate legal entities, and Scala center is a not for profit organization at EPFL. This may be an issue due to design by committee https://en.wikipedia.org/wiki/Design_by_committee
**Design by committee** Design by committee is a disparaging term for a project that has many designers involved but no unifying plan or vision. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
I‚Äôm really not sure what you‚Äôre talking about. Odersky openly advocated for continued sbt development in the document you linked to, and Typesafe actively paid people to work on it around that time. The fact that they can‚Äôt stop people in the community from working on an open source project is irrelevant to whether they appear to be blessing it as the way forward. 
Thanks, that's a plausible workaround. Still, it feels like something like this should be default behaviour.
Removed :-)
Removed for being off-topic
scala-async stalled a long time ago. Use http://monadless.io instead
Same here, except in some situations I think I say "wildcard" as well
I find just the feeling and syntax that makes it look mutable *is* already a problem. If I assume there is mutable stuff around then I go into oh-shit-better-be-super-careful mode, which is quite unproductive. If something goes wrong I will think "hm, maybe I have overwritten some value?" and will search at the wrong places.
It's unmaintained, of course, which you knew already when you asked. Two major things keep me from recommending Scala at work: There are too many fundamental things that are far worse in scala than in java: * compilation times * IDE support * build tooling with SBT * binary compatibility * collections library (Java's is way easier to use, more performant, and has far less corner cases to get things wrong) * community schism of haskell vs. ML style coding For dead-end/experimental/abandoned projects we have: * delimited continuations and scala async * specialization * compiler plugin architecture (not recommended to be used anymore, has major coupling with compiler internals) * reflection (not thread-safe!) * macros (experimental, many features of current iteration are being removed) * type projections (not sound) At the end of the day, one could ask what Scala brings to the table over java other than superficial syntax improvements? I'm not interested in most code that uses HKTs or rank 2 polymorphism because I think that's a dead-end way of programming, at least as I've seen it now. I'm also not interested in most uses of libraries doing type-level programming like HLists. Scala already compiles slowly enough and spits out complicated type errors in certain cases. That really just leaves us mainly with implicits, traits, and maybe macros if they one day solidify. I think scala's biggest success is acting as an experimental playground for new language features that other languages can copy from. I have hopes that dotty and 3.0 can improve things but in the meantime java and kotlin keep improving as well. And for what it's worth, I don't really think kotlin is worth it over java either for most software teams. I think it mostly just adds some superficial syntax improvements over java as well.
&gt; and Typesafe actively paid people to work on it around that time Including the original author, Mark Harrah, who was employed by Typesafe until at least 2013, iirc.
Update is cached completely, however there was a regression in Zinc with sbt 1.0 with regards to the speed of the cache. Currently Zinc hashes the entire classpath using SHA-1. Using a cryptographically secure hash to check for changes in a file is unnecessarily expensive. Just a last week a change was merged to consider timestamps on files on the classpath before hashing. https://github.com/sbt/zinc/pull/371#issuecomment-343197781 Caching isn't magic, even if you do cache you have to do it correctly in invalidate appropriately. Which SBT does provide the utilities to do, it isn't completely ad hoc. Last but not least, the "nothing is cached" idea is completely wrong, because Settings are cached by definition. Tasks are recomputed, Settings are not. It's the most fundamentally basic part of SBT after keys. It's a pretty fundamental mistake I'm surprised /u/lihaoyi would make.
If I had to say it aloud I'd say "blah" or "whatever", but mostly I wouldn't pronounce it at all unless I was saying it.
Fair enough, although as can be seen you don't even need to be actually be mutable to cause these problems ;) I mean a nicer way to solve this problem would have been to arguable use lens but even that wouldn't fix much. The `set` method (which would remain the same) would just call a lens underneath rather than a nested `.copy`.
Fair enough; I wasn't getting that point. (TBH, I think I'm not alone here.)
My problem with SBT at the moment are hand wavy rules around Ivy configurations and scoping. &gt; There is a lot of angst, complaints and confusion about SBT in the Scala community. Many of the people complaining are coming from a position of ignorance: the facts they provide are wrong, the problems they report have solutions, their understanding of SBT is incomplete and incorrect. &gt; But their confusion and frustration is real. Sums up my sentiment of people complaining about SBT, but unfortunately this doesn't help with what should be done. People keep recommending CBT as if it is significantly different, it's implementation looks a lot like SBT pre-11.x backed by coursier instead of ivy(which is a huge improvement on it's own), but I don't see what is going to prevent it from encountering the same issues of SBT over the long term.
hurray! finally the mods are doing something around here!
There's practical effects of the difference, online build introspection with the inspect command wouldn't be possible if the interpreter was immutable. Infact much of sbt's online tooling for some reason often completely ignored or maligned, even though I think it's the tools greatest asset, no other build tool really comes close in that regard.
I would probably have used a builder. Of course not that mutable setter-builder style but rather a typesafe builder, maybe with shapeless. Not sure how well this works with such a complex beast like SBT but to me it seems like the "natural way" to approach this problem.
&gt; Update is cached completely No, it is not, take a look at the output of `sbt -Dsbt.task.timings=true` noop compile in a warm vm. If it were fully cached there wouldn't be a 100ms+ overhead per project on every `compile`. &gt; Just a last week a change was merged to consider timestamps on files on the classpath before hashing Yes, I know, am using it locally, it's a fabulous developer life changing fix (overhead of the sha-1 hashing approach in multi-project builds is just horrendously bad). &gt; Caching isn't magic, even if you do cache you have to do it correctly and invalidate appropriately. Which SBT does provide the utilities to do, it isn't completely ad hoc. If that were true then how does one cache the result of a `task`? Seems completely ad hoc, the only caching utility I know of that Sbt provides is `FileFunction.cached`, which is useless when what you're trying to cache isn't a file. &gt; Last but not least, the "nothing is cached" idea is completely wrong, because Settings are cached by definition. Tasks are recomputed, Settings are not. Well, tasks are where all the work is done; if we can't (fully) cache them then his statement is true, not false.
Oh I didn't know this, guess I learnt something new
&gt; sure you may not be using do, but then you are using the basic map/fmap functions (which do is syntactic sugar for) We're talking about languages though, so syntax matters. A procedure that accesses and modifies a state variable and returns an `A` is imperative and mutative; a function `S =&gt; (S, A)` is neither, even though these could be two perspectives on the same logic. How people understand, reason, and communicate about that logic - the language they use - is what makes the difference.
Gave CBT a try now for the first time. Wow. It's extremely fast! That's one of my main problems with SBT: it takes almost a minute to launch (in our project at least). It's almost unbearable. That alone gives Scala a bad first impression for newcomers :/
&gt; Well, tasks are where all the work is done; if we can't (fully) cache them then his statement is true, not false. And work isn't done in computing settings? I'm confused why you think Settings for some reason shouldn't be considered. &gt; No, it is not, take a look at the output of sbt -Dsbt.task.timings=true noop compile in a warm vm. If it were fully cached there wouldn't be a 100ms+ overhead per project on every compile. Yeah for me, on a rather large project where incremental compilation takes about 20 seconds to compute, all that time is in `compileIncremental` taking well over 1500ms per project, `update` took 5-7 ms for me on each project. The next largest task after update is `coursierResolutions` at 150ms. So I don't have the same experience as you for some reason. I currently have 91 projects in this build. &gt; Yes, I know, am using it locally, it's a fabulous developer life changing fix (overhead of the sha-1 hashing approach in multi-project builds is just horrendously bad). I'd like try this myself, do you recompile all of sbt, or can you just recompile zinc, and publish that locally? &gt; If that were true then how does one cache the result of a task? Make it a Setting. If you want to implement your own cache logic, the Task's [Stream](https://github.com/sbt/sbt/blob/1.x/tasks-standard/src/main/scala/sbt/std/Streams.scala#L81) is what you'd use.
&gt; update took 5-7 ms for me on each project. The next largest task after update is coursierResolutions at 150ms This is because coursier hijacks the `update` task. `coursierResolutions` + `coursierArtifacts` == cost of `update` in plain sbt setup, so you're definitely incurring this pointless overhead on every `compile`. I've tried to cache these two coursier tasks but Sbt is winning the battle; think one has to fight fire with fire (i.e. use a macro). &gt; I'd like try this myself, do you recompile all of sbt, or can you just recompile zinc, and publish that locally? You have publish both zinc and sbt locally, changing versions accordingly, pretty simple fix. &gt; If you want to implement your own cache logic, the Task's Stream is what you'd use. Oh, I will give that a look, thanks for suggestion.
That Harrah has a doctorate in physics perhaps also explains why Sbt's design incorporates 4 dimensions ;-) FWIW, Coursier's creator also has a doctorate in physics. Something about build tools and physics I guess.
Sure thing, but there are language constructs in both languages (`do` syntax in Haskell, over lets say some sought of State monad and `for` comprehension in Scala) which does hide the fact that `S =&gt; (S, A)` is happening underneath. I mean I am not disagreeing here, I am just saying that if you replaced `build.sbt` syntax with lens/for comprehension/whatever you can think of, I don't think that there will be any fundamental difference in people understanding what is going on. I also think if you made people aware of what they are writing (i.e. using `.copy` on the immutable graph/case classes, and using something like Lens) it also wouldn't really help. Also if you are wondering, SBT does track state in an immutable fashion, see https://github.com/sbt/sbt/blob/8eb58791013b2826ab564d8c9e28275c7e7b2e6a/main-command/src/main/scala/sbt/State.scala#L36
I am not sure how precisely it works wrt the repl, but SBT uses an immutable case class to track state, you can have a look here https://github.com/sbt/sbt/blob/8eb58791013b2826ab564d8c9e28275c7e7b2e6a/main-command/src/main/scala/sbt/State.scala#L36 
&gt; That Harrah has a doctorate in physics perhaps also explains why Sbt's design incorporates 4 dimensions ;-) That one gave me a chuckle
&gt; People keep recommending CBT as if it is significantly different, it's implementation looks a lot like SBT pre-11.x backed by coursier instead of ivy(which is a huge improvement on it's own), but I don't see what is going to prevent it from encountering the same issues as SBT over the long term. Well there are differences (from what I can remember since SBT 0.7.x was some time ago) * Uses nailgun to try and speed up the tool as much as possible * The entire graph is simulated with OOP inheritance. SBT 0.7.x didn't really did this (at least not to the degree that CBT does) * CBT is using pure traits as plugins (I can't remember if plugins existed back then, but if it did it was vastly different to how CBT does things) * CBT uses `override` to fix the "patching" part of patched DAG's, iirc SBT back then didn't have a solution to this issue In some ways its similar, but SBT went down the road of being functional/immutable (and it was already somewhat there with 0.7.x) where as CBT is completely is using OOP concepts such as mixins, class inheritance and overrides to model the build. This doesn't mean that CBT doesn't use immutable data structures (i.e. in CBT dependencies is also defined as an immutable `List`), its just that the entire graph of the build isn't immutable, where as in SBT it is
I'm the exact same with the no visualisation... and again, I just see it as a symbol. I'm not saying these things to my self, I have an abstract idea in my mind of what I functionally need to do, not some kind of internal monologue of every keyword I'm going to type out.
"souligner" avoue :)
&gt; No it doesn't. You are confusing referential transparency with non-deterministic behavior, this is not the same thing. I think you're both using alternative definitions of determinism to justify your arguments. From the perspective of a *functional programmer*, determinism should be function-level determinism, which translates into purity and RT, while you're talking about a more subject level of determinism which you can understand. The fact that the very instantiation of `Future` is side-effectful and therefore non-deterministic by any *rigorous* definition of determinism is, in fact, a fault to most people. That you understand how this non-determinism *might* act, is not actual determinism. &gt; Future catches any NonFatal exceptions that may be thrown in any computation and thus prevents issue with Future's never completing or having different results. Monix Task also does the exact same thing. If Scalaz8's IO actually does have something similar to `type errorHandler[E] = MonadError[IO, E]`, then the point is moot, as `E` [can be made to be `Throwable`](https://gist.github.com/etorreborre/17172007514721addbff), which is roughly equivalent to `Future`'s facilities. However, even if it doesn't, the implementation of something to that effect is always possible, and therefore shouldn't matter. The ability to modularize handling throwables should be the user's choice, not a default just because the underlying memory model sucks. In production, we've had to make extensive use of `EitherT[Future, Err, A]` just to get around this, and it's a relatively common pattern we see among other teams. &gt; With Scalaz8 IO if an exception is thrown in map or flatMap (since this is the JVM, exceptions can be thrown even if your claim your own code is pure) then you can get cases where your async computation never finishes because an exception is thrown in another thread which you aren't listening on. This is why Scalaz8 IO is so fast, it completely ignores exceptions (where as other implementations treat exceptions as a value and handle it accordingly) Above, this should be configurable, though, this seems more an argument for `ExecutionContext` than it is for Future. Perhaps Scalaz8's `IO` might have allow being backed by an `ExecutionContext` in the way `Monix` is. We'll have to see. So far, most of your arguments have been for the use of EC's, not necessarily futures. Would you be amenable to `IO` if it were backed by something like a Monix `Scheduler`? It seems to me you've made an argument for either Monix's `Task`, or Scalaz's `IO`, but not really a necessarily good one for `Future`, considering the featuresets you'd like. Hell, `Task` even has backpressure! (even if it's in need of tweaking) P.S. Do you mind defining "Fairness" for the rest of us? AFAIK that's also a property of what `ExecutionContext` you choose. 
At this point I'm half convinced lightbend pays you an allowance to shill nonstop. 
But you don't write your whole program in the IO monad. If you did, people would be similarly frustrated.
The conceptual model is basically at odds with the surface syntax... and that's a problem, I think. For a much simpler *and* more capable build system (parallelism and generality) see e.g. Shake in Haskell-land. It's very simple at its core -- it's just a library for building code and gives you very easy to understand ways to track dependencies JIT and correctly -- and doesn't try to layer all kinds of weird abstractions on top of that to pretend that it's something it isn't. It's built on the Monad and I think it should be pretty easy to do something equivalent in Scala -- maybe an interesting experiment to do in one's Copious Free Time. (Granted, it's not really usable for compiling Java/Scala code, but I think that has more to do with nobody wanting to install Haskell to compile JVM code where there are already JVM-based build systems... not matter how shit they all are.)
Out of curiosity, how would one discover this organically?
&gt; Sure thing, but there are language constructs in both languages (do syntax in Haskell, over lets say some sought of State monad and for comprehension in Scala) which does hide the fact that S =&gt; (S, A) is happening underneath. True, and there are those who argue against even that much. My view is that they do impose a learning burden, and that's justifiable for a language-wide syntax that sees extensive use, but less so for a syntax that's used in a single system. &gt; I also think if you made people aware of what they are writing (i.e. using .copy on the immutable graph/case classes, and using something like Lens) it also wouldn't really help. Well, if `foo := "bar"` became something like `buildActions.append(SetSetting(foo, "bar"))` then I think people might find it easier to understand what was happening. They would still find it hard to trace the flow of the actual build execution, but the difficulty would be a bit more visible, as it were.
I was just wondering where inspiration for sbt's original symbolics came from
Hello again mdedetrich. As before, you're staying a lot of things that are just aren't true, so I'm going to take the opportunity to clear up some confusion for people who may be reading this. &gt; Future also gives you fine grained segregation of resources at any point in your computation. This is absolutely incorrect. Here's a definition of `shift` in a few lines of code: import scala.concurrent.ExecutionContext def shift[A](io: IO[A], ec: ExecutionContext): IO[A] = IO.flatten(IO.async[IO[A]](k =&gt; ec.execute { new Runnable() { def run: Unit = k(\/-(io)) } })) With this simple definition of `shift`, which runs and works in Scalaz 8 this very moment, you can have any amount of control over where your `IO` computation runs, including shifting between thread pools between each and every operation. Only unlike the design of `Future`, this approach requires the user pay for the cost of thread shifting only when they need it. With respect to giving users control over where asynchronous computations execute, the design `Future` of is not even remotely necessary. It's the "most obvious" design in some ways to solve some problems, but it is one choice among many that lead to poor performance and no actual increase in expressivity. &gt; That is, if you don't program completely correctly, you can have cases of async computations that don't terminate or have issues in determinism. This is another statement that is completely incorrect. By completely incorrect, I mean there is literally no possible way to interpret the statement such that there exists even the tiniest sliver of truth in it. Scalaz 8 IO catches all exceptions, even those that would be **lost completely** using try/catch/finally inside of `Future`. In fact, if the functions you pass to `map`/`flatMap` are defective, partial functions that lie in their type signatures and throw exceptions for some inputs, these exceptions will be caught and dispatched to the fiber's uncaught error handler. One step further, if even the *interpreter* that ships with Scalaz 8 IO has an error in it, then this too, will be caught, and dispatched to the fiber's uncaught error handler. The difference between Scalaz 8 IO and, say, Future or Monix, with respect to partial functions, is that defects in a fiber cannot be caught *inside* the fiber. Rather, defects in the fiber, interpreter or finalizers are always processed and handled by a fiber's uncaught error handler. This not only preserves functor laws, ensuring that Scalaz 8 IO is a well-behaved and lawful abstraction with precise semantics, but is also the *only* thing that makes sense, because allowing a defective fiber to "continue" by catching its own defects would result in undefined semantics, because there is no way to know the state of the fiber if the code is broken. Thus, Scalaz 8 IO achieves the essential property that no errors are ever lost under any reason (they can be logged, etc.), while still preserving the lawful, precise, and sane semantics that functional programming is known for. &gt; [Functional programming] its not a good mindset to program in. You are overstepping here. Rather, you should state it's not a good mindset **for you**. You do not prefer functional programming. That's OK. It's not for everyone. But many people, myself included, have found that programming with total, deterministic functions without side-effects (i.e. *math functions*) is the only practical way to program, because it's the only way to program that lets us reason about our software, and compose larger programs from smaller ones, having strong guarantees that the correctness and comprehensibility of the whole depends only on the correctness and composability of the parts. Functional programming is not for everyone. But it **is** for some people who prefer it, such as me. That is who Scalaz 8 IO is designed for. That doesn't mean you have to be perfect, because if you accidentally create a partial function, you can handle those errors however you like, and fix your code as soon as you notice them. But it does mean the library is designed for functional programmers who want to do functional programming. &gt; This is why Scalaz8 IO is so fast, it completely ignores exceptions (where as other implementations treat exceptions as a value and handle it accordingly) This statement is completely and demonstrably false. As I have stated on numerous occassions, Scalaz 8 IO is fast primarily because it minimizes heap allocations and megamorphism, and secondarily because it applies certain runtime fusion operations enabled by laws. In fact, and as stated previously, **Scalaz 8 IO catches all exceptions**, it just does not allow a fiber to "catch" its own defects (or defects in the interpreter) because this is semantically unfounded and violates functor laws. I understand you don't want to believe me. That's OK, I don't care. But how about this: go talk to Alex Nedelcu, the author of Monix. He too agrees with me that differences in performance are not due to behavior around exceptions. Or how about reading more about how the JVM works, to learn that try/catch has trivial cost in the case that exceptions are not actually thrown? Or how about adding try/catch anywhere you want in Scalaz 8 IO, to see that it does not materially affect performance? Or how about actually reading the source code of Scalaz 8 IO to **see that it already catches all exceptions**? Any of these options would yield less confusion than repeating the same incorrect statements about why Scalaz 8 IO is so fast. &gt; Passing this ExecutionContext everywhere is what causes a lot of the throughput issues. Yet again another incorrect statement. In fact, this statement is so incorrect that no one who actually **contributed** to `Future` will back you up here. The mere passing of this variable has no effect on performance. Absolutely none. It's not even measurable. The abysmal performance of `Future` is in no small part due to `Future`'s insistence on continuously shifting each individual operation to the thread pool rather than batching series of sequential operations, which is the high-performance technique used by Monix, Scalaz 7 Task, and Scalaz 8 IO. As before, your responses in this thread are spreading misinformation that will only confuse people. You are making statements which are clearly and obviously incorrect, which do not even contain the smallest grain of truth. There are many reasons to use `Future` and not Monix `Task` or Scalaz 8 `IO`, but you have described exactly zero of them. Please stop spreading these grossly incorrect statements because it takes real effort to correct misinformation.
&gt; SBT went down the road of being functional/immutable This has always bugged be because it's only immutable at the level of Scala syntax. But the DSL quite clearly describes a mutable paradigm. I've been saying this for a while, but the article makes that point really clearly.
What's the "patched-DAG problem"?
Blood, sweat, and tears. In reality, I probably picked it up on Sbt mailing list a few years ago.
It would have to be "soulign√©" (a noun), not "souligner" (a verb)--though their pronunciation is exactly the same. Regardless, I'm afraid it never crossed my mind to pronounce code in French. I would have to also pronounce a `for` as "pour". ;)
&gt; With this simple definition of shift, which runs and works in Scalaz 8 this very moment, you can have any amount of control over where your IO computation runs, including shifting between thread pools between each and every operation. Only unlike the design of Future, this approach requires the user pay for the cost of thread shifting only when they need it. `ExecutionContext` is not a thread pool. `shift` (or allowing you to run a computation on a separate thread) is not what I am talking about. `ExecutionContext` is a fork-join pool (by default). Its the exact same concept that powers the Haskell run time for IO (see http://www.macs.hw.ac.uk/~dsg/gph/docs/Gentle-GPH/sec-gph.html). Fork-join pool is documented here (https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/ForkJoinPool.html). Let me quote the relevant part for you &gt; A ForkJoinPool differs from other kinds of ExecutorService mainly by virtue of employing work-stealing: all threads in the pool attempt to find and execute tasks submitted to the pool and/or created by other active tasks (eventually blocking waiting for work if none exist). This enables efficient processing when most tasks spawn other subtasks (as do most ForkJoinTasks), as well as when many small tasks are submitted to the pool from external clients. Especially when setting asyncMode to true in constructors, ForkJoinPools may also be appropriate for use with event-style tasks that are never joined. You can also read https://www.javaworld.com/article/2078440/enterprise-java/java-tip-when-to-use-forkjoinpool-vs-executorservice.html and http://www.h-online.com/developer/features/The-fork-join-framework-in-Java-7-1762357.html. tl;dr version is, if you *don't know the type of work you are doing in the future*, then fork-join pools are overall better. Thread pools are better if you know exactly how to partition your problem before putting them on different threads. This is how work stealing is implemented. The difference between Haskell vs Scala, is that Haskell's system that powers IO can only be set at runtime, its global and can't be altered while the program is running. In Scala `Future` its an implicit parameter which means it can be altered at any computation. This is what I mean by sacrificing throughput for fairness. Putting computations on a work stealing fork-join pool (or a green-thread spark pool) is not the same as executing on another thread (either by using a `ThreadPool` or by explicitly stating the thread) &gt; The abysmal performance of Future is in no small part due to Future's insistence on continuously shifting each individual operation to the thread pool rather than batching series of sequential operations, which is the high-performance technique used by Monix, Scalaz 7 Task, and Scalaz 8 IO. This is incorrect * ExecutionContext is not a thread pool, as stated earlier its a ForkJoin pool (by default). This means when a computation is passed via a `Future`, it can run on a different thread, or it may run the same thread. It depends on how the fork-join work stealing algorithm decides where to run the Task. Again this is different to `shift` which forces a computation on a different thread. You can simulate a Thread pool with `ExecutionContext` but this isnt' the default setup and its not how its meant to be used (odds are if you want to use a thread pool, then you already know how to partition your problem, in which case something like Monix `Task` is a better fit) * Future requires the `ExecutionContext` on every operation because it passes these computations on the fork-join pool (via the `ExecutionContext`) which is how the work stealing mechanism works. This is exactly what I meant when I said its a sacrifice of throughput vs fairness. Other implementations of `Future` are just an abstraction over a state machine that polls over async events (in which cases operations like `map` would not ask for an `ExecutionContext`). Scala `Future` is not designed this way, you can ask the creator of `Future` (or you know, just read the link I posted earlier, since he is the one that made the post). * The batching is precisely the throughput vs fairness point I made earlier which you seem to have ignored. Batching improves performance/throughput, however it does so at the cost of fairness (at least if the batching is done automatically). Alexcu actually stated in one of this videos (i.e. saying that `Future` is designed for fairness) &gt; Or how about reading more about how the JVM works, to learn that try/catch has trivial cost in the case that exceptions are not actually thrown? Or how about adding try/catch anywhere you want in Scalaz 8 IO, to see that it does not materially affect performance? Or how about actually reading the source code of Scalaz 8 IO to see that it already catches all exceptions? I already read the PR, it catches the exceptions at a point in time where its too late for the situations which I describe, which is again fine if you think that exceptions should never be thrown (or caught).
&gt; Well, if foo := "bar" became something like buildActions.append(SetSetting(foo, "bar")) then I think people might find it easier to understand what was happening. They would still find it hard to trace the flow of the actual build execution, but the difficulty would be a bit more visible, as it were. Well I disagree here, I don't think it would really solve anything. Personally my biggest gripe about SBT is not that `:=` is abstracting away over the `State` variable, but rather how things like scoping works. BTW you can access the state variable in `build.sbt`, you can just type `state` in sbt and you will find it. You can also compose over state in a RT way.
Right, but on the level that SBT works, you have a graph which defines a build, and build definitions alter the state of this initial graph, hence why `State` is everywhere. You actually do need it everywhere. The different abstractions in SBT specify when you can alter and return a new state and when you can't (for example `Task` allows you to read the current state)
It will not run on Windows: Compiling to C:\bench\development\JohnDeere\cbt\stage2\target\scala-2.11\classes [info] Compile success at Nov 20, 2017 4:41:47 PM [0.773s] Exception in thread "main" java.lang.AssertionError: assertion failed: compatibility classpath different from NailgunLauncher at scala.Predef$.assert(Predef.scala:170) at cbt.Stage1$.buildStage2(Stage1.scala:128) at cbt.Stage1$.run(Stage1.scala:188) at cbt.Stage1.run(Stage1.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.lang.reflect.Method.invoke(Unknown Source) at cbt.NailgunLauncher.main(NailgunLauncher.java:120) 
https://youtu.be/-2aMaAPQ35s?t=20m50s
I give up. If you want to understand, I'll re-engage. Until then, I have work to do.
Well to be fair you only posted the actual PR of Scalaz8 IO a few days ago (when you have been posted benchmarks for it for a few months, which I don't think is professional but I know you don't care about this). I am still going through it, but from what I understand it hasn't alleviated the core issues I have until scalaz8 plans to wrap all of the Scala ecosystem to be pure. In any case, it appears that you have had misconceptions or inaccuracies about how scala `Future` works, or the design goals behind `Future`. I mean the source code for `Future` (unlike your PR) has been available for years and its tradeoffs have been well documented/researched/understood for some time. I mean I can go posting the actual source code of `Future`, but I have feeling you would completely ignore it, so *shrugs*. I don't have an issue with re-engaging, but at least don't go around saying that `Future` executes all of its computations on a thread pool (it doesn't). And heck, if you think that this is an issue, you can implement your own `ExecutionContext` which has different mechanisms for evaluation, which is what Play did. You can even run all of your computations on a tight loop on a single thread if you want, and have this `ExecutionContext` as a global variable so its easier for the JVM jit to inline (at least if you care that much about raw throughput performance).
I've started learning Scala this summer and thought it would be a great language for a new project idea I had. I'm writing a web API using Play to support managing your multimedia backlog. That is, if you have a million books, games, movies and TV shows to get through and you lose track of them all as easily as I do, you might find a tool like this useful. For the past several years I've used similar tools like My Anime List or Backloggery but they're simultaneously not comprehensive enough as well as too niche-specific and time-consuming to be of any real help, especially since Backloggery doesn't have a proper public API and explicitly forbids automating aspects of your account. In addition to the API I am also writing a "reference client" in the form of a Scala.js SPA. Because I'm using Scala on both the front-end and back-end it's really convenient to share things like class definitions and common logic. Right now I've extracted all of my data models out into a shared project and I can format/parse JSON with all of the same Play JSON implicits I already wrote for the server. As part of developing this project I'm also working into extracting any sort of useful utilities, libraries, or information I can and sharing them with the community. I wrote a [Google YOLO](https://github.com/sloshy/scalajs-google-yolo) Scala.js facade the other week and have been using it myself for one-tap sign-ups. It's far from perfect right now and it could use a few extra features and some tests, but right now it does everything I need it to do and I'm surprised at how convenient it was to figure out (publishing with sonatype was a bit confusing at first, though). Right now none of the project proper is open-source but once I get it to a point where I feel comfortable showing it off I'll move it to a public repository and happily share it. --- PS: I'm using scalajs-react with Monix observables instead of something like Redux or Diode, and once I clean things up a bit I'm thinkng of making a blog post about it and how well it all works with minimal boilerplate and setup. I have some Angular experience which makes heavy use of RxJS and I was very happy to see a Scala equivalent here, which is working perfectly for the client UI logic.
So there are few things being intertwined here, referential transparency, fairness vs throughput and catching exceptions as values during the computation. Referential transparency is not something I want to debate because its irrelevant to the point I am making. Yes `Future` is not RT unless you make a `Kleisli` out of it so you force its evaluation to be lazy (in which case you are better of using Monix task) Fariness vs Throughput is the fact of how `Future` is defined (more specifically the fact that the default `ExecutionContext` specified on `Future` uses a work-stealing fork join pool). This fairness only works if every computation is executed on this fork join pool. Languages like Erlang have this as part of the core language in their runtime, and for this same reason Erlang will lose to most other languages when it comes to raw throughput (even sometimes by a large margin). However this is also besides the point, because people don't use Erlang to get maximum possible performance (if you want to do this, then use something like C/C++ and use libenv to abstract over async IO), people use Erlang because if some expensive computation is running, it wont negatively impact the performance of the other tasks being run (this is work stealing). This is **not** the same as `shift` or just *running your computation on a Thread* or an explicit fork. Yes you can use `ExecutionContext` to represent a single thread (such as an UI thread), since `ExecutionContext` is completely arbitrary and can specify any way it wants to run a computation, but this is orthogonal to the point I am making (which is, by default, `Future` uses a work stealing fork-join pool which trades performance for fairness) Then there is the point about catching exceptions as values, which is not entirely lawful but is my personal opinion a better tradeoff mainly due to seeing how many issues the alternative causes. Maybe when Scalaz8 wraps the entire Scala IO ecosystem for it to be pure than I would consider it, at least if I cared about pure performance. &gt; It seems to me you've made an argument for either Monix's Task, or Scalaz's IO, but not really a necessarily good one for Future, considering the featuresets you'd like. Hell, Task even has backpressure! (even if it's in need of tweaking) I am personally making a statement for Scala's `Future` or for Monix's `Task` (if you care about RT). Scalaz8 IO is basically a port of Haskell's IO which in my opinion would be fine if we weren't running on a JVM where exceptions can be thrown everywhere (Scalaz8 IO makes perfect sense in Haskell where it matches the runtime very specifically)
/u/mdedetrich noob question - Wouldn't using scalaz#IO with a ForkJoinPool ExecutionContext give the same benefits as a Future? To someone who hasn't read either source, it seems like I could pass any EC to either Future or IO and get the same work-stealing behaviour. To me, it looks like you are listing the benefits of a particular EC rather than Future.
You would have to be strict and to pass `ExecutionContext` to every strict computation you are running. The scalaz8 IO doesn't work with `ExecutionContext` at all. It has shift style operations (which is also similar to what Cats provides) which lets you run a computation on a new (or specific) thread. The thing is, `Future`'s design is actually centralized over `ExecutionContext`. This is why `Future` has `ExecutionContext` as a parameter for every `.map` and `.flatMap` operation (and others as well). Contrast this to Monix `Task` where you only supply it once (which is when you run your `Task` since `Task` is lazy). wrt strict evaluation, from what I have seen this `ExecutionContext` style passing only works well in practice if its strictly evaluated, else you get really interesting results (I may however be wrong in this respect)
That looks very nice! I'm also using Monix in a client-side project (after being expored to RxJS with Angular over the past year and wanting something comparable) and it's very nice. How has your experience been with ScalaFX? My only client experience is in JavaScript-land and some CLI utilities. Can you write an immutable app with it that updates based on your observables, or are you expected to mutate state? Does it play a factor into the final app size in your experience? Once Scala better supports Java 9 I've been considering writing a desktop app with it since Jigsaw will let me cut the final size down and bundle in a JVM.
Thanks for the idea but I'm not going to be building up a recursion schemes library in the series. If I've communicated that, it's a misunderstanding. The main point of the series is to provide example code/problems/solutions to show what recursion scheme abstraction has to offer, and how it can be applied. How it can be applied is the most interesting part, because beyond the basics it's not always clear what it bridges into our the real world projects. In doing so I've found that it profoundly helps clarify the theory itself, like a feedback loop. As to what library users choose to use it doesn't matter; whether it be Matryoshka or something else, the concepts will hold and the only difference will be the syntax. If got a bit more to say/clarify/explain on this actually, but it'll be more relevant to the followup posts so let's wait until then.
Oh I am so screenshotting your comments so we can all laugh at them on IRC 
The point is *"practically speaking, here's how to get started and here are some Scala-specific technical points of interest/caution along the way"*. As such I hope to provide value to people who both do and don't already know the theory.
Thanks! Yeah hylo is an amazing concept! That and algebra zipping really blew me away when I first started learning it. I had a quick look at your library, very cool! I'm going to have a list of Scala libraries for recursion with some comparison notes and if it's cool, I'll add yours to the list.
 ^^^***whispers*** ^^^hey ^^^dude... ^^^fork-join ^^^pools ^^^are ^^^thread-pools. [^^^check ^^^it ^^^out ^^^man](https://github.com/scala/scala/blob/v2.12.4/src/library/scala/concurrent/ExecutionContext.scala#L113)
Oh my. I see the arguments for and against this but it still makes me uncomfortable. I can't imagine using it in my own projects but I'd be thankful to others using it in theirs if the alternative is they don't use my FP lib of choice and then I need to use Daniel S's shims library and pay a runtime cost.
Yes thanks for posting the source, lets look https://github.com/scala/scala/blob/v2.12.4/src/library/scala/concurrent/ExecutionContext.scala#L113 &gt; The default `ExecutionContext` implementation is backed by a work-stealing thread pool. From the source you posted. Also here is the source of the default `ExecutionContext` that gets used https://github.com/scala/scala/blob/07d61ecf134dbba143531f5a1bd3c1289c437296/src/library/scala/concurrent/impl/ExecutionContextImpl.scala#L91-L123 &gt; hey dude... fork-join pools are thread-pools. Sure, in the same way that running everything one on thread is the same as running everything on separate threads. As I posted in the links earlier, at least in JVM terminology, thread pools are not the same as fork-join pools. fork-join pools use work stealing algorithm to allocate tasks thread pools allocate threads on demand (which is very in-efficient unless you can keep your threads busy 100% of the time) Terminology may be confusing, but this is obvious when I mentioned work stealing or fairness (would be helpful if people around here were intellectually honest rather than trying to win an argument)
&gt; Referential transparency is not something I want to debate because its irrelevant to the point I am making. Yes Future is not RT unless you make a Kleisli out of it so you force its evaluation to be lazy (in which case you are most likely better of using Monix task) Totally and completely untrue. You clearly don't understand referential transparency. Next. &gt;Fairness vs Throughput is the fact of how Future is defined (more specifically the fact that the default ExecutionContext specified on Future uses a work-stealing fork join pool). This fairness only works if every computation is executed on this fork join pool (hence why map and flatMap ask for the ExecutionContext as an implicit parameter). Languages like Erlang have this as part of the core language in their runtime, and for this same reason Erlang will lose to most other languages when it comes to raw throughput (even sometimes by a large margin). However this is also besides the point, because people don't use Erlang to get maximum possible performance (if you want to do this, then use something like C/Rust/C++ and use libenv to abstract over async IO or something along those lines). People use Erlang because if some expensive computation is running, it wont negatively impact the performance of the other tasks being run (this is work stealing). They do it for consistency (or fairness) of latency, not raw throughput performance. Not once did you define fairness. &gt;Future uses a work stealing fork-join pool which trades performance for fairness) You keep repeating it, but it's saying nothing. You said the execution context is completely arbitrary, but Future uses a work-stealing thread pool (i.e. fork-join pool in J8). But i can make the EC arbitrary. So the point is self-defeating. &gt;Then there is the point about catching exceptions as values, which is not entirely lawful but is my personal opinion a better tradeoff mainly due to seeing how many issues the alternative causes You haven't. There hasn't been an IO monad for Scala before with the same breadth of implementation, or the same strategies. ... I'm going to have to tap out. You're too thick to spend time on.
&gt; Not once did you define fairness. I did, its a work-stealing algorithm implemented by a fork-join pool. &gt; You keep repeating it, but it's saying nothing. You said the execution context is completely arbitrary, but Future uses a work-stealing thread pool (i.e. fork-join pool in J8). Right, majority of people use the default EC, or one provided by a framework (in the case of Play) which is a different variation. &gt; But i can make the EC arbitrary. So the point is self-defeating. Indeed, so did the Scalaz8 PR use a custom `ExecutionContext` to make a fair comparison? https://github.com/scalaz/scalaz/pull/1519/files#diff-54e07728744cfbc4772ada487c5bf4acR32 Answer is no. Actually now that I am looking at the benchmarks more closely, I see other problems, i.e. using `Future.apply` instead of `Future.successful` (the latter is what you use to lift pure values into `Future`, its a lot faster then `Future.apply`) &gt; You haven't. There hasn't been an IO monad for Scala before with the same breadth of implementation, or the same strategies. Its a port from Haskell IO, thats what I was referring to &gt;... I'm going to have to tap out. You're too thick to spend time on. Would help if people actually read what was being written
Thanks for the great write-up!
&gt;I did, its a work-stealing algorithm implemented by a fork-join pool. I even posted links about how it works. ... that's not the definition of fairness by anyone's definition but yours... &gt;Right, majority of people use the default EC, or one provided by a framework (in the case of Play) which is a different variation. ...the default execution context isn't considered best practice and hasn't been for years... &gt;ndeed, so did the Scalaz8 PR use a custom ExecutionContext to make a fair comparison? Submit a pull request then, instead of bitching about it not being as thorough as you want it. They're benchmarks. They're open to everyone to modify. Everyone *else* has been actively contributing. &gt;Would help if people actually read what was being written The Brick strikes again
Using `Future.successful` would completely bypass the future evaluation stuff. That would mean all its doing is constructing a Future with a value inside of it, not evaluating anything. The evaluation would be at the site where the future is constructed.
&gt;Btw, the thing I'm currently most interested to explore (but forcing myself to wait) is chrono. It's a hylo of futu and histo. Chrono is a fun beast! Definitely a look at Greg Pfeil's [`matryoshka`](https://github.com/slamdata/matryoshka/) to get a feel for probably the best take on generalized recursion schemes anywhere in scala. &gt;I had a quick look at your library, very cool! I'm going to have a list of Scala libraries for recursion with some comparison notes and if it's cool, I'll add yours to the list. For sure. You're welcome to rip the code out entirely if you like, I honestly don't mind. I make do some idiosyncratic syntax in there, but it's all very usable an geared towards being able to use it in a sandboxed console. &gt; Comonad as an F-algebra?!?! well, yeah! `counit: F[A] =&gt; A` naturally defines "extraction from a context", where its dual "unit: A =&gt; F[A]" naturally defines "lifting into a context". When `F` is a functor, combined with `fmap`, this defines an `F-algebra`. It's a consequence of being a functor, more than being a comonad. I just thought it was an interesting point when you consider `unit` and `counit` that things that have a `unit` (such as applicatives and stronger structures) are things that build up structure (i.e. information and context), while, their costructures naturally want to be extracted and interpreted. I found it interesting that every Comonad (indeed, every Coapplicative) admits an F-Algebra, while every Applicative (resp. Monad) admits an F-Coalgebra. You can see the same sort of high-level association among `Free` and `Cofree`, where `Free` represents the building up of a syntactic structure of a functor for some given values, and `Cofree` represents the interpretation of the nodes and leaves of a similar one. You can see an example [here](http://dlaing.org/cofun/posts/free_and_cofree.html).
my god
Cats and scalaz have diverged to the point that yaxing will be miserable for any nontrivial usage. I remain skeptical of shims for the same reason, but haven't tried the new incarnation. I'm now 100% cats on all my projects so it's not an issue for me anymore.
Always strive to write exemplary code :D
This is the second time I have seen someone bust out Yax this week. I still feel applying scalafix rewrites to a core project and having it generate the yaxed project is a better way but I understand that the current tooling is making this look like an appealing solution.
The problem with SBT is the learning curve is now too steep for the benefit of building a program. There was a time when its feature set was rare: incremental, parallel, and continuous builds, a REPL, a great SBT plugin community, etc. It's too easy to mock SBT now but that development experience really was innovative at the time. But now everybody expects their builds to just work without much effort. Like Scala itself it seems there's a tension between the mirror funhouse of beauty and elegance and actually getting things done. I'm squarely in the latter camp, and my hope is for Scala to use Gradle. Focus on the integration with the tooling and you can also reap the ecosystem benefits. This is the approach Google used with Android, and I think its paid off. 
I just wrote some incomplete documentation that might be useful, don't hesitate to ask any question: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/README.md
I just wrote some incomplete documentation that might be useful, don't hesitate to ask any question: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/README.md 
I've been working building a web application for sending alerts for crypto currencies prices based on exchange markets. The back-end is built using Scala, Play Framework and Scalactic for error handling, it uses a simple monad transformer for composing non-blocking results and docker for testing using a real database. At the moment most of the back-end is completed with some pending details (like adding more tests and improving code). I'm using the code to try experiments with Scala and I've been updating the code frequently, feedback and critics are welcomed: https://github.com/AlexITC/crypto-coin-alerts
I don't think the tooling support is quite as dire as the article makes it out to be (although there is certainly room for improvement). While going to definitions for keys doesn't help you determine what is being put in a key. I find that you can get a lot of mileage out 'finding usages' instead. Admittedly, this isn't ideal because you still have to manually determine the order in which order these interactions occurred. I think a lot of this could probably solved more completely by a slight improvement to SBT's `inspect` command. This command can already inform you of the final value for any key as well as where that final value was determined. This could be modified slightly to so that you have an `inspect history` command that includes the complete history of the key. If it included the history of values and which files key was modified and what its value was at the time (assuming it contributed to the final value), it seems like it would make troubleshooting issues with layer 1 and 2 much easier. I'm not sure a lot can be done about layer 3 since tasks are run in parallel. This seems like sort of a problem for any build tool that lets you declare task dependencies (Gradle, Ant, etc.). The only thing I can think of would be if you could get SBT to print the actual order the tasks are run in (is this already possible?).
I have a list of As, Bs, and Cs. I'm looking for a function that has the following behavior: Input | Output ---|--- List(A, A, B, C) | List(A, A) List(A, A, C, B) | List(A, A, C) List(A, A, C, C) | List(A, A, C) It's pretty much takeWhile: keep taking while A, stop upon B or C. **But**, if we stop on C, include this C in the output. (If we stop on B, do not include it). Right now, my code is something like this: def foo(xs: List[ABC]): List[ABC] = { var seen_c = false xs.takeWhile { abc =&gt; abc == A || (abc == C &amp;&amp; !seen_c &amp;&amp; {seen_c = true; true}) } } which is hacky and uses a mutable boolean flag. Is there a more elegant/functional way to write this function?
Gotcha, thanks for the link! I'm guessing it probably took a little time to find. "Patched DAG" does seem like a hard problem, but maybe that's mistaking one potential solution for the problem itself. It seems maybe a little simpler to say it's a DAG with some unbound parameters. AKA a function from those parameters to a DAG. Then a crossbuild is just `map`ing that function over a collection of parameter tuples.
`Future` is strict, the distinction of evaluation doesn't apply because in both cases (referring to `Future ( 5 )` here) it gets applied immediately. The expression `5` does get evaluated immediately (else its a no-op), with `Future.successful`you are just skipping the `ExecutionContext`, which is why this function exists.
Are there any examples where I can see how functors, monads etc are implemented in real life? I understand intlist and binary tree but I cannot seem to apply them in my day to day life.....would really appreciate some help. 
&gt; ... that's not the definition of fairness by anyone's definition but yours... thread fairness isn't even close to being just "work stealing" Look, I can sing till the cows come home about people using incorrect terminology... In any case, I provided a definition in my posts so it was quite clear &gt; ...the default execution context isn't considered best practice and hasn't been for years...I honestly seriously hope you haven't been running the global default in prod. If you all your `Future` computations are asynchronous and non blocking/not CPU bound (or CPU intensive) its the correct one to use. If you don't program this way then yes agreed, the default `ExecutionContext` is not what you should be using (which is why Play ended up using a different one because they found out that a lot of people ended up blocking in their code when using `Future`) &gt; Submit a pull request then, instead of bitching about it not being as thorough as you want it. &gt; They're benchmarks. They're open to everyone to modify. Everyone else has been actively contributing. Yes, and it would have been nice if the benchmarks/PR was posted before going around on a salesman spree with numbers that appear to be not indicative. Continuing from my point here https://www.reddit.com/r/scala/comments/7dg6fx/scalaz_8_io_pull_request_by_jdegoes_pull_request/dq4qysz/, the benchmarks are meaningless/deceptive/inaccurate because its not benchmarking how `Future` is actually used. People don't use `Future` for pure parallelism. Doing stuff like Future.successful(5) .map(_ * 2) .map(_.toString) .map(s =&gt; s"test:$s") .flatMap( Future("hello") ) Is not what `Future` is meant to be used for. If you are writing this type of code (heavy CPU bound, using cores for parallelism), then yes `Future` is the wrong abstraction. Stuff like `Task` would perform better (and if you want to be more fair in your benchmarks, then you would use a custom EC for this type of code which is what you are benchmarking). These are similar reasons as to why Play had to make a custom EC. The biggest use of `Future` in actual code is long running async computations that come from * Http/network requests * Database queries * Queues (kafka, redis, memcached) * Streams (This also happens to be the biggest source of unknown exceptions btw) These all represent long living async computations (i.e. http requests can take anything from 10 to 150ms to complete), not pure strict values that are evaluated basically immediately on the CPU (and maybe put onto other CPU's for parallelism). The benchmarks do not really show this, as all of the banchmarks are just lifting pure values into different async types and running `map`/`flatMap` or other combinators on them. I am interested to see how all of the `Task` types perform when you have 5000k+ long living asynchronous effects existing at the same time and the effects this has when you compose over them, because this actually reflect what the code I write deals with in reality. The benchmarks do not show this. I mean I can improve the benchmarks, but the damage of posting unrepresentative numbers is why posting any benchmarks results with the or code or benchmarks is unprofessional (echoing what was said here https://www.reddit.com/r/scala/comments/6vb9mr/why_im_excited_about_scalaz_8/dlz0owf/). Now that I see the benchmarks, I see how the numbers are not indicative of how `Future` is used in reality (for most cases at least). Would have been more fair to open a PR along with the benchmarks and wait for community feedback before going around throwing numbers like Microsoft did in the 90's with their shady benchmarking practices
Over the last month me and [Johannes](https://github.com/cornerman) have significantly upgraded the API of [Scala DOM Types](http://github.com/raquo/scala-dom-types) for improved type safety and flexibility. Encoding Javascript's typeless wildlife in Scala in a useful way is very challenging. It's [not perfect yet](https://github.com/raquo/scala-dom-types/issues/13), but I'm just very happy with the progress we've made.
no, I mean there's no motivation for why you would do it at all. If you don't know the theory, all you see in your post is a way of doing a thing, for no apparent purpose.
I actually made an issue for this ages ago https://github.com/sbt/sbt/issues/2123
Well I think this is sought of how it works right now with `crossScalaVersions`, but it opens up an entire can of worms (sbt plugins like Scala.js). Then there is the problem of not being able to specify a subset of the list of specific sub projects (see sbt-doge and the problem its solving)
Could you clarify what you're asking about? Intlist and binary tree are more common examples for fixed-point/recursion-schemes style code. If you're asking about functors and monads, are you asking about the implementation of typeclass instances, or how they're used, or something else?
This is the best I could come up with: def fn(xs: List[String]) = { val (as, rest) = xs.partition(_ == "a") rest.headOption.fold(as)(x =&gt; if(x == "c") as ::: List(x) else as) }
&gt; The main point of the series is to provide example code/problems/solutions to show what recursion scheme abstraction has to offer, and how it can be applied. With respect, this post is the complete opposite of that: it shows how you do the fixed-point construction but not what it would be used for, so for a reader who isn't already familiar with recursion schemes it could come off as unmotivated theorising. &gt; As to what library users choose to use it doesn't matter; whether it be Matryoshka or something else, the concepts will hold and the only difference will be the syntax. Agreed, but an existing library might make it easier for people to run the code right away, and easier to get straight into examples. Fundamentally it seems wasteful to write yet another implementation rather than pointing people at an existing one.
I'm looking for something similar for the Play Framework (2.6). Honestly, it doesn't have to be a long tutorial. Anything up-to-date which shows the right way to build a simple database-driven app would be fantastic. The tutorials/samples linked to in the Play 2.6 documentation are all out of date and it's driving me crazy trying to figure out the right way to do things. It seems that things change so fast that most blog posts and SO answers are also out of date. If anybody can point me to anything like that I would be eternally grateful.
I didn't test this, but couldn't this have worked? def toFutureEither[A, B](x: Either[Future[A],Future[B]]): Future[Either[A, B]] = { x match { case Left(lf) =&gt; lf.map(Left(_)) case Right(rf) =&gt; rf.map(Right(_)) } }
Umm, just use `.bisequence`?
On what?
Hmm, it should be on the `Either`, but looks like `BitraverseOps` doesn't define `bisequence`, only `bitraverse`. So you can do `myEither.bitraverse(identity, identity)` but that's not ideal. I'll see if I can send a PR from my phone.
So far I've really been enjoying ScalaFX. I've done a bit of Electron + scala.js (see http://codeninja.blog/2017/scala-js-skeleton), but using Electron to create a simple desktop app always feels a bit like overkill to me. &gt; Can you write an immutable app with it that updates based on your observables...? So far I can (for the most part). Where it gets tricky is if you want performance. For example, for the first pass of the Twitter client I just use the simple REST API (not Twitter's streaming API), and so every 15 minutes I go out and get the user's home timeline. When I did this, I'd update the Monix `BehaviorSubject` with the updated timeline, which would update the UI. That's simple enough, but *all* the tweets would be replaced with the same tweets again. This would cause the UI to "flicker" or even lose focus, which was quite annoying. To fix that involved managing the timeline state and only adding tweets that were new. But, I was also able to do things easily with Monix that might have been... more convoluted otherwise. For example, various parts of the Tweet object could be Monix `Oberserver`s: the retweet count, like counts, etc. When the timeline was updated, any existing tweets could just have those observers updated and the UI would "just work" showing the updated status. If there's something I find annoying using Monix + ScalaFX, it's the terminology overlap. There's times I have a Monix Observable and for other things I use a FX Observable. Keeping track of which is which can be annoying. I'm close to just hunkering down and putting together a wrapper class that keeps an FX observable in sync with a Monix observable and seeing if anyone else would find it useful as well. &gt; I've been considering writing a desktop app with it since Jigsaw... TIL about Jigsaw. Need to look into that. Currently I just use Launch4j for app distribution.
A great post. Just to offer a slightly contrarian opinion: totally agree on the grid as datamodel is unforgivably broken, but when the author says what is needed is a hierarchy, I would say a Composite. WHy, oh why, are otherwise very experienced and capable developers who use collections all over the place so unlikely to use Composites?
I have a bunch of follow up questions but I wouldn't be doing you justice if I asked them without reading up the source at this point but that might take some time. Thank you for the answer.
If you want to build a database driven app, you may check "Essential Slick" from Underscore. This book is like an easy reading tutorial on almost everything that can be done with Scala+relationDB
&gt; Andrescu I'm pretty sure his name is Alexandru Nedelcu, or "Alexelcu" as a portmanteau :) You're probably confusing the name with [Andrei Alexandrescu](https://en.wikipedia.org/wiki/Andrei_Alexandrescu) of C++/D fame, also a Romanian.
**Andrei Alexandrescu** Andrei Alexandrescu (born 1969) is a Romanian-American C++ and D language programmer and author. He is particularly known for his pioneering work on policy-based design implemented via template metaprogramming. These ideas are articulated in his book Modern C++ Design and were first implemented in his programming library, Loki. He also implemented the "move constructors" concept in his MOJO library. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
 What is a "composite"?
The purpose of these benchmarks is to test the speed of the execution model. Using Future.successful would completely bypass Futures execution model. It'd be as useful as benchmarking the Monad instance on `case class Foo[A](a: A)`. To say otherwise is completely naive. Sure, you can say this benchmark is a bad use-case for Future, and the results are skewed because of it. If that is the case, pull down the branch and write a better benchmark. Saying to replace the existing benchmark with Future.successful is the wrong approach. If your next reply is justifying the use of Future.successful here, then I'm not going to continue this discussion. Task is deferring evaluation to the execution engine. For the benchmark to be proper, Future must also perform its evaluation in its execution engine. If you don't like this benchmark because it is a bad one for future, write a better one. Having a variety of benchmarks is a _good_ thing. For everybody. Be mad about the benchmark source not being published until recently all you want, but at least direct that frustration at making the situation better.
Yes. And look at its friend Visitor. Common in parsers. Frankly a build tool is a parser if you think about it. It's a parser that is Interpreter-ing a DSL and building a Composite, often using a Visitor to manipulate the AST.
@kaeblo solution is by far the best way to achieve this. Try to avoid mutation (eg using var). There's always a way to make it work without it. Just for the record, here's another way to achieve this by pattern matching and recursion (not mutation involved): def fn(list: List[String]): List[String] = list match { case (x :: xs) if x == "A" =&gt; x :: fn(xs) case (x :: _) if x == "C" =&gt; List(x) case _ =&gt; List.empty[String] }
My bad I see andrescu everywhere so I assumed it was Alex Andrescu! I'll change it in my post. Thanks
It's still the wrong guy :D You're talking about [this one](https://github.com/alexandru).
These benchmarks are not indicative of how you'd use any of these things properly. They're testing things like the performance of flatMap in their respective interpreters. Go ahead and pull the branch, then modify all of the benchmarks to use strict constructors. If you change Future.apply to Future.successful, be sure to use IOs respective strict constructors as well.
Apparently I can't read, my bad again.
That's what I came up with too. It seems pretty straightforward compared to the other stuff in the blog post. Can anyone suggest why the linked techniques would be better?
Nah, some of us have been using one tool (emacs/vim) for everything for 10+ years and have an incredible amount of investment in that tool. Using other things requires us to learn new productivity strategies that only work for Scala, rather than for Scala and Python and Js and XML and note-taking and... And we feel at home there. I used to use emacs+ensime because Intellij was buggy (which it still is) but now that it has sbt integration my builds in Intellij and at the command line actually agree, so it's at least usable now. Previously, I couldn't trust the intellij build in all cases. The main complaints I have now: * even when building with a custom sbt build jar, if I do a cli sbt build, Intellij does a full rebuild for any action. They don't seem to share incremental builds. * deletion of necessary imports is also annoying * no easy to use macros * requires the use of a mouse for several things * a project with two different types of sources is difficult to configure correctly * It can show me when an implicit conversion is occurring, but not jump to that conversion * It has terrible lag time even with high memory constraints for very large files of any type, let alone scala * Interacting with the repl via worksheets is slow, and gives different results than the raw repl. * It doesn't understand scopes beyond main and test-- so IntegrationTest is lumped in with Test. But I'm able to work around some of these. Sometimes by leaving the intellij tool for the cli. Ensime+emacs also has a lot of issues, but I can hack my way through that source code because I'm familiar with scala, emacs and some elisp. I don't have that level of familiarity with intellij.
Well, one of the contributors then. I'm not going to argue semantics.
Thanks, that does look worthwhile, but it doesn't cover web applications, unfortunately. I can use Slick to query our database in a basic app, but I can't set up Play 2.6 to use Slick.
My fault too, should've written the correct one last :)
That's correct, they're lagging behind with updating the official documentation. But you can start with their books(that what I did) and fill missing parts by stack overflow searches
Sure, but creator implies quite a bit more than contributor, thus my reply.
&gt; Go ahead and pull the branch, then modify all of the benchmarks to use strict constructors. If you change Future.apply to Future.successful, be sure to use IOs respective strict constructors as well. Sure, but I don't even know what is the point of the benchmarks then? I mean benchmarking against Monix makes sense (as would scalaz 7 `Task`) as they have similar designs (optimized for throughput/lazily evaluated/ RT) but `Future` is so different in its design goals that I don't really see what you guys are trying to achieve here apart from getting a false sense of accomplishment. I mean the benchmark I pointed out is testing for `map` and `flatMap`. Changing it so that all of the lifting of values are strict make sense on one hand because you are testing strict lifting of values everywhere, *except* that for the fact that `Future` is *always* strict (unless you put it in a thunk, which you aren't doing anyways). The difference for `Future` with `Future.apply` and `Future.successful` is not strict vs lazy, its just that one option is downright worse then the other (if we are talking about lifting pure non computationally expensive values into `Future` which is what the benchmark is doing). Also the benchmark is testing `map` and `flatMap`. This is what I meant earlier in that you should never use `Future.apply` for pure values that are evaluated basically instantly on the CPU. The inclusion of `Future`in general such benchmarks in my opinion isn't really making any sense. And if your goal is to test the respective interpreters, then you should be testing different `ExecutionContext`'s since the point behind `Future` is that you can swap your interpreter (and benchmarking this in a fair manner is very hard to do). I can do a PR to change these instances to `Future.successful` (and also change all constructors to be strict if you really wan't, although as mentioned before I don't see what the point of this is wrt what you are testing). Honestly remove `Future` from these benchmarks at all, and do some high level testing (which I mentioned before) which compares all of these `IO`/`Task` types.
EDIT: Also another point, if you really just want to test only `map` and `flatMap`, you should put the construction of the various `IO` Tasks in a jmh `@State` variable, because otherwise your not making a proper isolated benchmark.
Love doobie but nope nope nope.
I have just started learning Kotlin, but from what I've gathered so far Kotlin coroutines provide a nice syntax for one specific monad instance (Future\*), but not for monads in general. \* Scala Futures are not strictly monads, but they allow for monadic syntax.
Coroutines might be seen as alternative to futures, but someone has to show me how this has anything to do with monadic behaviour.
So I've been using SBT for many years, and tend to copy my build config from one project to the next (web apps mainly). I've also worked with multiple teams of 20-25 devs with a couple of dozen sbt projects who have all ended up doing the same. My observation is that its possible to write fairly simple sbt build files if you are publishing a library or simple app. But the moment you need to do something different and perhaps write a new task you are in pain, pouring over documentation and reminding yourself again how things actually work. That last point is a killer. A build tool needs a really simple, obvious model. All projects I've worked on set up the build and tend not to have to touch them very often, months can go by before someone needs to change it (ignoring lib updates). This means you aren't spending your days becoming an SBT expert, indeed you shouldn't need to, and periodically having to relearn how things work with SBT is frustrating.
&gt; Intellij does a full rebuild for any action I've not experienced that for quite a while now, especially with the new sbt-shell.
no no no, emacs's keybindings are the original keybindings, everything else is wrong!
That is Scala 2.11.7. I would like to use a compiler plugin, but scalac does not let you plugin a different parser. We use: * parboiled2 for preprocessing * several macros for specific semantics * a SubScript VM for managing a call graph 
Yeah sort of. I am looking for examples where monad and functors are used in day to day data structures. I can see that option and either are monads as they have a flatmap and we chain them. But where would we implement our own monad in day to day life? Hope it makes sense
Yeah true, without knowing the theory this post would seem pretty pointless. From the next episode onwards, I'll be getting into real, practical things that one can do and pointing readers back to this post as the prerequisite step. This first post is unique in this regard. :)
&gt; With respect, this post is the complete opposite of that: it shows how you do the fixed-point construction but not what it would be used for, so for a reader who isn't already familiar with recursion schemes it could come off as unmotivated theorising. Yeah, you're right. I responded above to this. TLDR this first post is the only episode like that because it serves as a prerequisite for every follow-up post where the why will be apparent. &gt; Fundamentally it seems wasteful to write yet another implementation rather than pointing people at an existing one. Matryoshka is awesome but it shouldn't be taken as gospel. It has a lot going for it but it's also got some very, very serious flaws that prevent me from using it in my own code. But I see your point; I'm planning on having a list of available libraries and their tradeoffs for and against (with Matryoshka featured prominently).
I see, thank you, interesting work.
They may be old, but that doesn't mean they're standard. GUI apps these days mostly use Windows-style key bindings, so that's the standard.
how do this relates to http://scala-lang.org/blog/2017/10/09/scalamacros.html? we had scala.reflect, which we know was "experimental". then came scala.meta, which was supposed to replace reflect for good. then, even before meta was "done", it was already deprecated in favor of a new "scala.macros", which we know little about it other than "for real this time, we meant it". and now we have this new proposal by /u/odersky. i'm confused, would the real future of metaprogramming in scala please stand up?
Windows Intellij 2017.2 newest Scala plugin with custom jar specified and use sbt for build and import specified.
Re `~/.sbt/1.0/global.sbt`, I don't know how that would be found organically, but one of the user-guide sections is labeled [Global Settings](http://www.scala-sbt.org/1.x/docs/Global-Settings.html) and mentions this. Re `target`, you could `inspect` a task that places something in the target directory (e.g. compile or doc) and look though its dependencies, possibly recursively, until you find `target`, although I suppose that could be difficult if you don't know what you're looking for. I think I found it after being pointed to `sbt.Keys` by the "plugin best practices" page, but I suppose that that's not organic either.
As I've already posted here, you might want to take a look to: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server The project is not really a tutorial but a working project that might help you, it is built using play 2.6.6 with anorm, it might not be the right way for you, at the moment it is good enough for me, quite more details on the github documentation. Feel free to ask any question.
Thanks very much! [This](https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/data/anorm/dao/UserPostgresDAO.scala) is exactly what I need. Very kind of you to offer to answer questions. I might get back to you on that, later!
You need to fix that attitude young man
I‚Äôve not tried windows. So you‚Äôre saying it does a complete clean/build every time something changes? Sounds like a bug.
This is way over my head. Someone willing to give an abridged version?
great! I look forward to reading them
Either you pass around the ActorRef around in code or you know its [address](https://doc.akka.io/docs/akka/current/general/addressing.html) (put it in some config or something) and use [actor selection](https://doc.akka.io/docs/akka/current/actors.html#identifying-actors-via-actor-selection) 
For me this bit was important: &gt; The framework expresses at the same time compile-time meta-programming and staging. The phase in which code is run is determined by the difference between the number of splice scopes and quote scopes in which it is embedded. &gt; &gt; * If there are more splices than quotes, the code is run at "compile-time". In the general case, this means running an interpreter that evaluates the code, which is represented as a typed abstract syntax tree. The interpreter can fall back to reflective calls when evaluating an application of a previously compiled method. If the splice excess is more than one, it would mean that the interpreter would itself interprete an interpreter code that possibly interpretes another interpreter and so on. It's hard to see a use case for this, however. &gt; * If the number of splices equals the number of quotes, the code is compiled and run as usual. &gt; * If the number of quotes exceeeds the number of splices, the code is staged. That is, it produces a typed abstract syntax tree or type structure at run-time. A quote excess of more than one corresponds to multi-staged programming. 
I'll try. `eitherOfFutures.bisequence` is better because the above code - the match and the unapplying and the re-boxing, is 100% boilerplate. It's canonical, you came up with it independently of the other guy and I would have written that too. What use is there in having such boilerplate, predictable code in amongst your business logic? It's a general structure of code that cats allows us to hide away inside `bitraverse`, so we should use it. It leaves your code cleaner about _what_ it's doing rather than _how_ it's doing it. Of course, under the hood, deep down, eventually, all bitraverse does is the exact above code just with some extra layers of abstraction to burrow through first.
On the flip side, there's innovation after innovation, and it keeps getting better and better! What a time to be alive (and a programmer)! :D But yeah, I hope we can standardize on one of them very soon so we can start making things &amp; get proper IDE &amp; tooling support etc.!
It turned out that while scala.meta was great for tool developers (i.e., IDEs, auto-formatter, etc.) and for syntactic macro annotations, it was not really the right approach for def macros. Apparently, this was mainly due to the lack of good support for semantic information (tree types, symbols, etc.). One new direction has been to try and abstract the reflection APIs of both Dotty and Scalac, in order to provide a cleaned-up and portable version of the old scala.reflect API, supporting many previous use cases of macros. See [this post](http://www.scala-lang.org/blog/2017/10/09/scalamacros.html) for a deeper explanation of these two points. However, a cleaned-up scala-reflect-like interface also incur problems that are still unresolved (e.g., mixing up typed and untyped trees without getting unstable behavior from the compiler). Moreover, there is also a push to get away from the unrestricted power of macro offered by such interfaces, and provide instead some more type-safe and high-level (though more restricted) tools. Martin's proposal goes in that direction. TL;DR: the way forward is still very uncertain, and people don't even quite agree on what use cases macros should eventually support. IN my opinion, we should go with the cleaned-up scala.reflect (i.e., the scala.macros approach), and build type-safe high-level tools on top of these lower-level building blocks, which is exactly what I have been doing with [Squid](https://github.com/epfldata/squid), a type-safe metaprogrammijng framework (currently based on scala.reflect, but hopefully ported to scala.macros in the future).
As people probably know, there's an effort under way to come up with a replacement for scala.reflect macros. We did a thorough survey of existing meta programming systems, and, no surprise, they vary widely in their capabilities. As one possible design I worked out the system in the Gist. It is quite close to Meta OCaml and proposals to do macros in OCaml as well as to a calculus by Davis and Pfenning inspired by modal logic https://dl.acm.org/citation.cfm?doid=382780.382785. One attractive aspect of the system is that it works equally well for staging and for macros. But as a macro system it is quite restrictive because it imposes a strong typing discipline, only works for full expressions, and in its original version does not allow for code inspection. Lionel's Parreaux's Squid system (paper to appear at upcoming POPL) is more powerful in that it does allow inspection. Both systems are quite far removed from current macros, so porting existing code will be a headache. It's quite possible that we will need something closer to current macros as a possible migration point. 
@habvok2191, not exactly a complete project but here there's a G8 template that wires Http4s, Doobie and Circe including Unit Tests -&gt; https://github.com/gvolpe/typelevel-stack.g8
&gt; how does this relate to http://scala-lang.org/blog/2017/10/09/scalamacros.html? Both are related. &gt; i'm confused, will the real future of metaprogramming in scala please stand up? It's a big challenge to develop a feature rich, robust and portable macro system across three (!) compilers each with different internals. So bear with us if it takes a few iterations and time ;) I am personally quite excited about Squid quasiquotes: https://github.com/epfldata/squid/ (esp. see POPL paper https://infoscience.epfl.ch/record/232427). Like the gist proposal, Squid quasiquotes are also statically guaranteed (at macro definition time) to be well-typed, well-scoped and hygienic. In addition, Squid quasiquotes are reusable (it's possible to write generic utility methods) and support more advanced runtime (at macro expansion time) inspection/transformation of terms/types. 
#3 sounds a bit like what LINQ does for .NET. Is my interpretation correct?
```scala object Macros { def mapImpl[T, U](u: Type[U], arr: Expr[Array[T]], op: Expr[T =&gt; U])(implicit ctx: Context): Expr[Array[U]] = '{ var i = 0 val xs = ~arr var len = xs.length val ys = new Array[~u] while (i &lt; len) { // ys(i) = (~op)(xs(i)) (x =&gt; x + 1)(xs(i)) ys(i) = ~op('(xs(i))) i += 1 } ys } } ``` Wow, this is amazing!
Yes, this is pretty much what LINQ does. If you're interested, we have a very rough (and mostly undocumented) prototype implementation of what LINQ does using the Squid type-safe metaprogramming framework, called [DBStage](https://github.com/epfldata/dbstage).
I really think that Ammonite should be included in the default scala distribution. With F# you get an interpreter that can run scripts. Being able to run scala scripts is great for DevOps . Being able to write scripts makes it much easier to learn Scala. Especially now with IDE support in IntelliJ Idea. 
Agreed, I hope some variation of this proposal makes it into Scala Meta. The functionality that the `~` and `'` operators enable looks especially tasty.
No. If you do a cli test. Then go back to Intellij. Then try to run a test in Intellij, it does a full rebuild. If you regenerate sources at the cli, then go to Intellij, and run a test, it will do a full build rather than incremental. Stuff like that. You wouldn't have to use the cli if Intellij understood test scopes, and its repl gave decent results all the time, but it doesn't. I agree that it's a bug. It also doesn't pick up generated sources from dependency projects in multi-project builds. These things are mostly annoyances. I usually can rely on basic Java-like completion, debugging (with downloaded sources) and refactoring. It can't reactor some things that are expressions, though, so even that isn't reliable. Ensime's problem is that it will usually just time out and a bounce brings it back. Which is another set of problems. Windows doesn't help things, but my work laptop has a 256G hd, and I support a .Net project as well, so working in a vm is impossible due to space. 
&gt; Both are related. I don't understand how, this new macro system isn't mentioned in the roadmap on http://scala-lang.org/blog/2017/10/09/scalamacros.html. Does this mean that after what in the roadmap is called "v2: scala.meta" is depracated in favor of "v3: scala.macros", this v3 will be deprecated for something new like squid? It's more strange to me since v2 is already being used, yet in gitter it's mentioned macro annotations might disappear from the language: &gt; Yuriy Badalyantc @LMnet nov. 07 12:49 &gt; @olafurpg So, macro annotations could be completely dropped in the future? &gt; √ìlafur P√°ll Geirsson @olafurpg nov. 07 13:01 &gt; Strictly speaking macro annotations have never part of mainland Scala, they have always required a compiler plugin. &gt; If there is strong enough justification for macro annotations then they could be included in the new macro effort. I'm not the one who makes that call, however, you'll have to convince the compiler teams At the same time macro annotations are mentioned under "v2: scala.meta" The situation between v2 (scalameta) and v3 (scalamacros) cause further confusion to me because the scalamacros gitter says macro annotations belong to scalameta, while scalameta says &gt; √ìlafur P√°ll Geirsson @olafurpg nov. 05 05:46 &gt; FYI Scalameta macro annotations are no longer under development, see notice at the top of the page and again &gt; √ìlafur P√°ll Geirsson @olafurpg nov. 05 16:26 &gt; @ryan-williams it's not decided yet if macro annotations will be included in the new effort on macros, please consider commenting on the Scala contributors thread &gt; there is currently no ongoing effort to implement "real macro annotations", and that probably won't change unless the scala and dotty teams open up to the idea of &gt;supporting them officially in the compilers &gt; Same applies to whitebox macros. So it seems at least macro annotations have been lost in the void. The roadmap on http://scala-lang.org/blog/2017/10/09/scalamacros.html points to two topics, https://contributors.scala-lang.org/t/annotation-macros/1211 about annotations) and https://contributors.scala-lang.org/t/whitebox-def-macros/1210 (whitebox macros) where features for v3 are discussed, yet what i gather from this topic v3 is depracated in favor of v4 (squid). So my understanding, as someone using macro's today is: a) stay away from v2 (scalameta) and v3 (scalamacros) as these projects will be depracted b) if you need macros now, use v1, but note c) c) it's likely dotty will have more limited macro abilities(ie no annotations in v4) than what's available now either via v1 or the new (it's unclear to m whether they belong to v2 or v3) macro annotations For me one of scala's strength are it's meta programming capabilities, including macro annotations, i'd really hate them to disappear.
Yes, switching between Intellij and cli messes things up. However, if you stay within the new sbt-shell you can run whatever commands you like in the shell and Intellij won't do a rebuild.
This is huge! Currently there's no good way to explore a new library while retaining the full autocomplete / go to definition capability. Ammonite made fetching new libraries too easy and with intellij integration it'll will be perfect!
I apologize for the confusion. &gt; yet what i gather from this topic v3 is depracated in favor of v4 (squid). Note that "v3" from the roadmap is still under construction with no published artifacts so there is nothing to deprecate. Squid and v3 are complementary as both me and /u/LPTK see it, they can live together in harmony. Squid is a higher-level API that already works with scala-reflect (v1) and can be adapted to work with another underlying macro API (v3), once it's ready. The tentative design of v3 is heavily inspired by scala-reflect (v1) but tries to improve on some its known issues. &gt; So it seems at least macro annotations have been lost in the void. You are correct. I encourage you to raise your concerns and share your thoughts in this thread here https://contributors.scala-lang.org/t/annotation-macros/1211/22 That's the only place I can guarantee you will be heard by the ones who make the calls.
Did you consider using distributed data to do this? https://doc.akka.io/docs/akka/2.5.7/scala/distributed-data.html
in a nutshell, is v2 a dead-end then? what do you recommend we use now? v1 is the way to go until v3 is done?
&gt; Both are related. how? are (will be) they the same?
Yes, no one is working on building a macro system with Scalameta (v2). Scalameta's primary focus is now on tooling (refactoring, linting, code search, ...).
The `bisequence` is a better solution. But I think we were talking about OP's original solution. I looked at it again, and it seems like you can just do this with fewer lines of code: eitherOfFutures.fold(f =&gt; f.map(Left(_)), f =&gt; f.map(Right(_))) I'm not sure why he needed the Functor from cats. 
They're related in the sense that the "v3" mentioned in the blog post is an blank page and the gist is a proposal for what v3 should look like. I'm currently writing a blog post on what has happened since September where I expand more details :)
Ah fair enough. I believe in your `fold` example inference will fail and it won't compile, it will expect the second argument to have type `Future[Left[_]]`, but I might be wrong.
They're copying a lot of the features. But I guess they can't have the same dependencies
Aha, that's a fair point. It requires one to know what `bisequence` means, but if you can use `Future.sequence`, it wouldn't be much different. 
yes you need to know the terminology, but it's not much different to having to know what `flatMap` means. There are surprisingly few 'magic methods' introduced in these libraries, the `sequence` and `traverse` families being the most important/widely used.
Can someone explain why there's such a strong push to limit macros to run after typing? I've always viewed macros as functions of type syntax -&gt; syntax. By allowing macros to run before typing, their power and utility can greatly increase, though at the cost of the macros not being able to inspect types. Why not allow both kinds of macros? 
This seems to be jumping the gun a bit, I've been using 2017.3 RC and seen no such improvement yet. OP seems to be making assumptions of the status of ammonite tagged issues that it'll be supported.
Glaring into the crystal ball, can we see v2 coexisting with whatever this (v4? man I'm losing track already) new effort will be folded into, or can v2 be rebased onto v4?
The last entry in the bug says it's nearly fixed. Im assuming the feature is tagged for inclusion and that the release will be done when the features are implemented. But that is a guess.
I've held a tentative position on macros but with syntax like this it looks so much more approachable. Now all I need is a few more hands for crossing fingers hoping they will be eventually be allowed in a single compilation run.
You would rarely need your own *implementation* of a monad typeclass. For most stdlib types the heavy lifting is done already, so you'll rarely find yourself needing to write a `Monad` instance. When dealing with recursion schemes you'll at most need `Traverse`, which you are likely to delegate to something of standard library as well. I did, for one, write a `Monad` instance for `ObservableValue` of scalafx so I could use methods from `cats` and for-comprehensions when generating my property binding. You can see the code [there](https://github.com/oleg-py/fxtools/blob/master/cats/src/fx/tools/cats/package.scala) although it's a little more complicated b/c in scalafx properties there are two type parameters, yet the second one is reconstructible from the first. Also I used Free monad in few toy apps: I defined my own type which was a monad. I didn't have to write typeclass instances for it, however: those were already provided with `cats`. I also found myself some other structures emerging naturally sometimes. For example in my (currently private) project I had a simple data structure: case class Key(str: String) extends AnyVal case class Label(str: String) extends AnyVal case class Keyed[A](key: Key, label: Label, get: A) Which I use in place of tuples for the sake of readability basically all over the place. Sometimes I would like to transform a value (e.g. get a field) without changing the key and the label, and so I wrote a method (making it into Functor): def map[B](f: A =&gt; B): Keyed[B] = copy(get = f(get)) Later, I realized that sometimes I also want to do such transformations based on current key and/or label, while also maintaining it, and so I wrote another method def methodRemovedLater[B](f (Key, Label, A) =&gt; B) = copy(get = f(key, label, get)) And then I found out that, well, there's another way to write this method: def coflatMap[B](f: Keyed[A] =&gt; B) = copy(get = f(this)) So, here I had it: `get` and `coflatMap` make a `Comonad` there. I wrote a typeclass instance on a companion object and got methods for free :)
I'd go for this probably. Also it's possible to write `case "A" :: rest` for more clarity too.
It will properly unify types. It's `Option#fold` that has broken inference (it has two parameter lists)
Not sure exactly what you mean. I don't see scalameta macro annotations (what I call v2) co-existing with whatever will be in the "next macros" (what I call v3). However, scala-reflect macros (what I call v1) will probably live with us for a long time. See http://www.scala-lang.org/blog/2017/10/09/scalamacros.html for more context
I meant the meta-functionality used by tools like scalafmt. My question is basically if there will be several concepts of 'meta' floating around or if there can be one common meta-platform
Ah yes, that's what I was thinking of, thanks
They do a flatMap-like composition behind the scenes. Calling a suspendable function from another suspendable function is the equivalent of flatMap. Applying a normal function to a suspenable function is the equivalent of map.
Scalameta is separated from the overall macro effort as it is now. Our lesson is that tooling and macros are quite different things with different needs.
Download doesn't work...
Hey. Sorry to hear that! DM me some details on Twitter (@davegurnell) or ping me at http://underscore.io/contact and I'll sort it out for you (and hopefully everyone else). Best regards, Dave 
Looks like the issue is with gumroad.com. It was returning 503 service unavailable. After a few retries it let me through and was able to download the book. Looks like a good read. 
Good decision to remove the "Advanced" from the name. Between that and the cover, the book really suits how approachable the contents are now
The table of contents still mentions Xor which to my knowledge has actually been removed from the book and replaced with the right biased Either.
That makes sense. I have been doing scala oop for about 8-9 months now. Not truly fp. Just starting work with scalaz. So all of this is going pretty hard for me. But I am loving the challenge. Your post made good sense to me. I am going to try to use functors in my code whenever I need transformations like that. And good to know that people are not writing monad instances in their day to day coding life :) Thanks for the help! Appreciate it very much. 
It depends on your case, if you have one instance of the application only, you could use dependency injection with a custom annotation for your actor, or create the actor once and look for it by its name. If you have several application instances, see previous answers.
Use actor selection. That way you can even share the actor across instances of you have to scale someday. 
Anyone know of a good comparison of quill to spark?
This is looking great, thanks!
IÔ∏è wrote this doc with a comparison some time ago: https://github.com/getquill/quill/blob/master/SLICK.md
Why 100% cats? Lately it seems scalaz is hot in action (John's IO, fommils activity...)
Great book, I've read the preview versions and I really recommend it. It can teach people a lot, not only about Cats but about functional programming in general (great introduction to type classes for instance).
| This potential complexity might be seen as an argument against using Scala at all: maybe using a more basic language will save us, at least partially, from writing complex code? I don‚Äôt think so; as competent, responsible software engineers, we are more than capable to choose how to best solve a specific problem. I don‚Äôt believe that using crippled tools (i.e. languages with fewer features) and taking away some of the freedom that we have in Scala is the way to go. Here, competent and responsible means that we must resist the temptation to over-engineer (and this temptation is no way unique to the Scala ecosystem), and carefully consider the balance between the complexity of language features used and the problems at hand. Exactly.
sadly it's not possible to buy the book physically online :/ 
Finally a book that explains the FP concepts in approachable way! Great job!
What's the error?
`List[Rep[String]]` is an odd type to have I think. How did you get it?
Agree. It was a transitional tool. Not used anymore.
Cats has an increasingly rich native ecosystem (fs2, circe, doobie, http4s, monocle) with seamless interop and an active, friendly community. Scalaz 8 has some interesting technical developments but it remains a work in progress. Also, I left the scalaz organization due to harassment from one of the maintainers, so I won't work on it anymore.
It is unfortunate that the most well known scala libraries are extremely large and complex. often with poor documentation. Couple that with really complex type signatures at the API surface (eg. scalaz), or pervasive lack of type safety (eg. akka actors), and it makes for a rough experience learning the language. Not that those libraries aren't useful or don't have a place...just that they are not friendly to the newcomer. I'm really grateful for the recent push for simple APIs in scala libraries. It really makes an impact on learning, using, and sharing code. 
Indeed, some things in SBT (like not inheriting settings from different configs) are so counter-intuitive that it is just frustrating. It is very hard to remember them unless you are full-time SBT plugin developer.
The fact that you have a List[Rep[String]] suggestes that a previous part of your code could be done in a way that would let you have something more usable, I don't think you can do what you want to that way. If you showed us some more context, we could help you :)
Wow. Who would harras you. One of nicest people and presenters around
Even cats wont save this garbage. LoL
Removed for trolling
Hi all I will be very happy, if something can help me https://stackoverflow.com/questions/47458725/overloaded-method-value-table-with-alternatives Thanks
Well, maybe almost all of the pitfalls. I _think_ it's not possible to have intermediate values leak when settings get reassigned. Part of the problem of mutable programming is that it can be difficult to reason about names over time and space. SBT seems more like a mutable language where everything is by-reference.
It could use maybe an intro on what's meant by staging and what LMS is. I'm sure I'll figure it out eventually, but that's just some initial feedback.
If you are retrieving the same data, like user info, what's wrong with using the same calls in different routes?
where do you store the actor ref and such?
you need to find the actor in some way, here are some ideas: https://www.reddit.com/r/scala/comments/7ep5pv/how_to_share_an_actor_ref_instance_across_an/
so, you are saying scalameta is now fundamentally just a glorified parser?
As I've started to work with larger data systems, I find myself relying more on Java geospatial libraries than I have in the past instead of Python/R. They aren't always well documented though, so I'm going to start posting intro tutorials as I figure them out. Hope you find them useful!
It was an originally private gist that got leaked. At least this version was not intended for general consumption. If there's enough interest to explore this further I'll write a blog post about it.
If I understand things correctly it's akin to a "diet [Roslyn](https://en.wikipedia.org/wiki/.NET_Compiler_Platform)". Nothing to sneer at if it gets to grow a little.
As I've started to work with larger data systems, I find myself relying more on Java geospatial libraries than I have in the past instead of Python/R. They aren't always well documented though, so I'm going to start posting intro tutorials as I figure them out. Hope you find them useful!
You don't need ensime to use Emacs for Scala development. Just use emacs-sbt-mode and emacs-scala-mode. https://github.com/ensime/emacs-sbt-mode https://github.com/ensime/emacs-scala-mode These two packages are independent of ensime. In my opinion they should be decoupled from ensime.
Thanks for checking it out. Take a look at my edit for how this may occur.
Thanks for checking it out. Take a look at my edit for how this may occur.
That's a nice way to put it :) Roslyn has indeed been an inspiration for scalameta. Over 90% of the time I've spent on "scalameta" is doing tooling like scalafmt, scalafix, cli/sbt integrations, code search. Our semantic story wasn't great for a long time but I think it's improved a lot past few months and we have a clear idea how to move it further 
Interesting case! My first thought is that if `Rep` is an `Applicative` you can get a `Rep[List[_]]`, which would then work in your query. Unfortunately either `Rep` doesn't satisfy the criteria for being an `Applicative` or slick's code is too dense for me to work my way through. So I think the only approach is going to have to be some slick hackery tricks. I'll have a look around but can't offer any help at the moment.
In SQL, you would put the data you have into a temporary table and join on it, calling `lower` on the join. The equivalent in slick is not (yet) possible: https://github.com/slick/slick/issues/799 So, I think you're either going to have to do the filtering in memory after loading everything or do a fold over your string list and have one filter statement per string.
And its written by Oracle. Whats going on?
To reply yet _again_, you might be able to do the following: ``` val strings: List[String] = List("Name1", "Name2") val stringStaticQuery = strings.map { str =&gt; Query(str) }.foldLeft { _ union _ } val finalQuery = Employees.join(stringStaticQuery).on(_.name === _.toLowerCase) ``` The idea being that you create a static query `select "Name1"` for each of your strings, using `Query.apply` which wraps a single value into a `Query` object in slick. You then combine all of these into a single query using `query.union`, which concatenates results sets. This query then acts as a temporary table you can join to, and perform your query-time `toLowerCase` on. Please let me know if it works! I've never used `Query.apply` before, you might have to find some funky imports.
I've also found that Python doesn't scale too easily for me. It's easy to do simple things, but for larger codebases it's just missing structure that makes life harder.
Yes, I have tried `union/unionAll` on individual queries. And it works! But the query slick generates is bloated, especially in comparison to what it would have been when using `inSet`. Benchmarking on small dataset of about 30 rows. `inSet -&gt; Planning time: 0.067 ms, Execution time: 0.101 ms` `unionAll -&gt; Planning time: 0.659 ms Execution time: 0.430 ms`. For large datasets, this difference will grow. It is unfortunate slick doesn't support such an operation. 
Not sure what you want to achieve, but a common way to refer to actors by an identifier (think: primary key) is (Akka Cluster Sharding)[https://doc.akka.io/docs/akka/2.5.7/cluster-sharding.html?language=scala].
Thanks a lot ! Actually i was trying to take out Future and then flatMap it out but that does not happen and hence i tried this way. I thought people may have gone through the same problem and hence written a blog about it. Thanks a lot for this code, i've tested it it works :) I will add this to the blog post itself ! Thanks a ton ! :)
That timing is with the union on the static queries? I find it incredible that slick would take ten times as long just to create a few query objects. Incredible but not unbelievable, sadly.
So using akka not as "message passing", but slightly more involved Request-Response protocol. Is this not antipattern? Is this not what Lightbend calls "distributed monolyth"? How is this better than bunch of composed Future(s) ?
Well, I don't think that a set of distributed actors that speak to each other can be considered a "monolith". It is impossible to use only the "fire and forget" pattern. Not all the system can be designed as streams. Can you provide a link or a resource to this "distributed monolith"?
https://www.infoq.com/news/2016/02/services-distributed-monolith It does not seem to me that we are talking about the same thing. Moreover, a system involving Actors is not necessarily a microsystem.
request-response creates tight coupling. RPC over http if you wish to say so. My understanding was that it's antipattern for akka - in a same manner you should not use "ask pattern". But request-response type of message passing (actor A sends msg to actor B, actor B does work and send it back to A) feels to me like Ask pattern in disguise. But don't worry, I believe Lightbend is inconsistent in their definitions as well. I think though you can read some stuff I mentioned in one of their papers (https://www.lightbend.com/blog/white-paper-understanding-reactive-programming-vs-reactive-systems? not sure). Now, don't get me wrong, I personally disagree with the whole RPC-is-super-bad-we-need-to-do-100%-message-passing-via-kafka-and-actors premise, but I just feel it's insane that people write that much code to get that little benefit.
&gt; It is impossible to use only the "fire and forget" pattern. Not all the system can be designed as streams I'd say we agree here. I'm sorry if I come a bit negative... but try to stop for a while, think what problem you're trying to solve, and reflect the akka code that does it. Does it seems sensible to have to do all that machinery for all of that? If yes, then good for you. It doesnt feel right for me. But maybe problem is in me. Blog is pretty good btw.. But I've seen some incarnation of this too often (and you at least explain it nicely) and it always makes me feel bad in stomach.
Nevermind. I think you are one of the more polite users on reddit :) I don't know if you read also the post that described what I am trying to model. Anyway, between the two set of actors, there is a router. Then I need a place where to collect responses. Anyway, I will try to reason about what you said. Many thanks.
I think I will ask a question on SO on this topic. Thanks a lot, you enlightened me!
The timing is after `query.result.statements.head` which produces the query. Slick is great but sometimes mind-bendingly complicated. Thanks for helping out! :)
yes, I've seen, which is what could as well just be val response = for { f1 &lt;- myFirstThing f2 &lt;- mySecondThing f3 &lt;- myThirdThing } yield f1.username + f2.password + f3.myNonsenseString Sure - you can claim these run one-by-one, but nothing limits you from using applicative instead of Monad, and run them in parallel. Sure, now if one fails, you want to have a retry mechanism perhaps? You can write a function `def retry[A](futureToRetry:Future[A], amountOfTimes: Int, backoffStrategy: Int =&gt; Duration` ... and now you have this retry function you can reuse on arbitrary Future... etc... code thats reasonably easy and composable, compared to reasonably simply code. I'm not saying what is right and wrong, just food-for-thought. Keep on learning :-) We all need to, and to strive for nice and simple programming models.
You have kickass attitude when it comes to learning. Skimmed through bunch of your blog posts.
For something that‚Äôs more idiomatically Scala, check out geotrells.org. 
I've actually had a look at their work - it's really impressive. They have open sourced two different GDAL wrappers (https://github.com/geotrellis/gdal-scala &amp; https://github.com/geotrellis/geotrellis-gdal) but they only seem to have wrapped some of the raster parts, so I didn't end up using them.
Highly recommend this approach since it will automatically scale out and place Actors on other nodes when add more nodes to the Cluster (providing you are planning to scale out).
&gt; I personally disagree with the whole RPC-is-super-bad-we-need-to-do-100%-message-passing-via-kafka-and-actors premise What gave you this impression, i.e. only use Kafka and don't use RPC (or HTTP Request/Response)? 
&gt; Not all the system can be designed as streams. just from looking at the diagram tells me that, this would've been a perfect fit for streams.
I found the motivation of avoiding a race condition around `sender` not a compelling motivation for using this pattern. The standard solution to this is just to close over a variable containing the original sender -- `val originalSender = sender`. This is used along with all the other code in the final implementation, but nothing else is needed--this single line solves that problem.
Great post, very slick! And yet, it's a bit unfortunate that Akka requires so much machinery for what feels like a basic effect, and with so many gotchas. I still love the Akka project, though. I think the abstractions it offers on top of actors are fantastic. I'm hopeful that Akka Typed cleans up the abstractions a bit, but I haven't really used it.
Well, it is not only because of the race condition. It does use the ask pattern; It dispatches responsibilities among actors; It isolates mutable state.
I've been drinking Coca-cola for the last month and I have to say, I still prefer Pepsi so if I had to decide what to choose next I'd pick Pepsi because reasons.
‚ÄúBoth of them are Java-based‚Äù is where you lost me; sorry!
Isn't this nit-picking? That's a bit like criticizing Oracle for saying "Java runs on 3 billion devices" with "well, actually, it's the _Java Runtime Environment_". We all know what was meant.
I was about to ask the same thing - what the essence of LMS or "staging" is - so I'd be very interested to learn more about it. The [Wikipedia article](https://en.wikipedia.org/wiki/Multi-stage_programming) doesn't say much. I would also be interested learn if and how these new quoting systems could interact with ways of serializing Scala "fragments"; anything that has to do with the continuum from interpreter to compiler.
**Multi-stage programming** Multi-stage programming (MSP) is a variety of metaprogramming in which compilation is divided into a series of intermediate phases, allowing typesafe run-time code generation. Statically defined types are used to verify that dynamically constructed types are valid and do not violate the type system. In MSP languages, expressions are qualified by notation that specifies the phase at which they are to be evaluated. By allowing the specialization of a program at run-time, MSP can optimize the performance of programs: it can be considered as a form of partial evaluation that performs computations at compile-time as a trade-off to increase the speed of run-time processing. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
/u/joshlemer can we please, please, please ban kotlin posts from this sub once and for all? kotlin guys, we get it, you love your language, we love ours, too. now, please, do everyone a favor and go promote it in /r/java so you can move people up to kotlin. please stop trying to move us down from scala, it is not going to happen. we all here love scala because of the features it has and kotlin lacks. we know you think they just serve to complicate the language, but we *need* them.
Never did Kotlin but I'm really missing goto/break/continue and Kotlin has them.
For the context, I want an actor that holds user's auth tokens for making api calls to an external service. Other actors would request the token for the user from this auth actor and then use the auth details to make a subsequent request
/s?
Hey scala has breaks!
&gt; /u/joshlemer can we please, please, please ban kotlin posts from this sub once and for all? And create another echo chamber? Personally, I don't think these posts hurt anybody.
I am keeping an eye on the Kotlin posts, and I have removed some posts that were just about Kotlin (off topic), but this specific post is more or less on topic, even if in my personal opinion it isn't really bringing any interesting information to light. Since it is more or less on-topic, it's up to the voting to decide how good it is (and the votes seem to agree with you).
thanks for the link. 
No. You don't have to know about macros to use Scala. The great thing is, you can do so many things without opening the macro can.
&gt; Personally, I don't think these posts hurt anybody. don't help, either
&gt; It can basically do everything scala can do lololololololololololololol
Syntactically Kotlin gives you a lot of the quality-of-life features &amp; syntax that Java should've had from the beginning. Kotlin is the bare minimum of what a JVM language should be. I absolutely see why people coming from Java is very happy with Kotlin - it gives you a lot of benefits without really having to learn anything new. It's just a _better_ syntax. Nice! _But_ if you look deeper into the 2 languages, you'll realize that they're fundamentally different in one very specific area: Functional Programming! Scala isn't just a syntactic rewrite on top of Java; it's a completely different language! Scala is a FP-first language and it shows. There's a very heavy emphasis on immutable data structures (immutable being default with case clases etc.; mutation being extremely rare/non-existent), always returning _values_ instead of modifying, sane equality, pattern matching, algebraic datatypes (ADT), using monads for handling asynchronicity, optionals, failure-handling, effects, etc. If you just want nice syntax for your Java, Kotlin is your best friend. If you want "more", that is Functional Programming, without fanatically ruling out existing libraries and effects, Scala is the best language ever. IMHO.
Not this again.
This? https://www.tutorialspoint.com/scala/scala_break_statement.htm It's not in the language. 
I think that something like [ScalaCheck](https://www.scalacheck.org/) might be better suited here than trying to use macros. 
You mean because it's in the standard library rather than in the language proper? Why would that matter?
It matters because the syntax is much cleaner when the break/goto/continue was designed to be a part of the language from the beginning. I'm not mad or anything, it's just fair criticism. I don't plan on switching from Scala to Kotlin... You guys are just idiots with your downvotes.
Haha, sorry if the downvotes bothers you. A lot of people just downvote when they disagree instead of commenting (lazy! :P). I don't think anyone meant any harm. I'm curious, what situation situations do you need goto/break/continue for? I can't think of a single scenario where I'd want them. Do you have a code example?
Fair enough! Well for me, I like the freedom. But I remember this explanation of Linus Torvalds that I think you'll like : http://koblents.com/Ches/Links/Month-Mar-2013/20-Using-Goto-in-Linux-Kernel-Code/
Interesting read; somebody actually defending `goto`s for change. Torvalds is especially entertaining as always with his dramatic language. I still think that none of that applies when you're programming in a functional language like Scala though; or any higher-level language that doesn't rely on looping. In higher-level languages you can just use .map or .fold or whatever and not have to worry about "machine-level details" (contrary to C which is a *completely* different universe)
I've used https://github.com/alexarchambault/scalacheck-shapeless and can recommend it as long as your case classes or ADTs are small.
IMO if you need any of these in Scala you're doing it wrong...
It's kind of solicitation, isn't it? Etalang is more of a competitor, but we don't discuss them much. Same with ReasonML and OCAML-JAVA. 
Bought it and cruised through to just past the Monad section. So far it's helped a lot of things click for me that never clicked before. Definitely much more approachable than the other cats tutorials available online. Nice work!
I can't check myself right now, but yiu should be able to do something with DBIO.sequence http://slick.lightbend.com/doc/3.1.0/dbio.html
Subscribe to /r/kotlin if you want to learn about it. We're not interested in these penis-length comparison posts. And I mean, really this visage on a shaky YouTube video, really? At least post about comparisons with remotely interesting languages.
Here's a sneak peek of /r/Kotlin using the [top posts](https://np.reddit.com/r/Kotlin/top/?sort=top&amp;t=year) of the year! \#1: [Kotlin officially supported in Android!](https://np.reddit.com/r/Kotlin/comments/6bqlqb/kotlin_officially_supported_in_android/) \#2: [Kotlin 1.1 is out](https://blog.jetbrains.com/kotlin/) | [11 comments](https://np.reddit.com/r/Kotlin/comments/5wvoue/kotlin_11_is_out/) \#3: [Kotlin 1.1.3 is out](https://blog.jetbrains.com/kotlin/2017/06/kotlin-1-1-3-is-out/) | [9 comments](https://np.reddit.com/r/Kotlin/comments/6j28vk/kotlin_113_is_out/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
i wonder how much of the "i hate scala" crowd actually is the "i understand immutable is good but i can't use it all (most?) the time; i like lambdas but i hate functional programming; i miss for loops and modifying variables in place, that's the way i've been trained and it's the only way i can think about" crowd.
&gt; At this point, Rsc only implements a subset of functionality provided by the Scala compiler. Performance numbers may significantly deteriorate as we will be implementing more and more functionality of the Scala compiler. For example, adding support for classpath loading or implicit search is very likely to slow down our compiler by a significant factor. i too created a scala compiler that only implements a subset of functionality provided by the Scala compiler. currently, it doesn't output any bytecode nor does it detect any errors in your sources. but it runs in O(1) and is able to compile trillions of lines in a few nanosseconds. honestly, when you leave all the hard parts out, it's trivial to have a much faster "compiler". it's like comparing fuel consumption of a cessna 172 against an a380.
Here's a link to the previous discussion of this topic: https://www.reddit.com/r/scala/comments/7df8r6/prototype_of_the_reasonable_scala_typechecker/.
Actually, there's plenty of useful information in early benchmarks. Kentucky Mule made quite a splash by showing early benchmarks that were &gt;2000x faster than Scalac (details in the readme at (https://github.com/gkossakowski/kentuckymule). Even though Kentucky Mule did even less that Rsc does currently, it was a massive wake-up call that demonstrated that, to quote the author, "Scala does not come with a language design flaw that prevents a fast compiler to be written for". By publishing our early benchmarks, we're demonstrating that the ideas behind Kentucky Mule seem to scale beyond the original demo. There clearly is a lot of work ahead, but current numbers indicate that we're on track to building a fast, parallel Scala compiler - something that seemed impossible about a year ago.
Thank you. It seems scalacheck-shapeless is probably suitable. I need to read more first. But it seems I can use Arbitrary to generate a case class instance.
Would it be possible to disable features of scalac not implemented in rsc and then compare their respective speed? 
Great work!
/u/xeno_by, please don't understand my comment wrong. i appreciate the work you have done and all the effort you put behind it. but when a compiler is only doing a fraction of the work, it will obviously only take a fraction of the time. i do believe rsc will be significantly faster than scalac. but if you leave the sarcasm of my original comment behind (sorry if it offended you), what i was trying to say is that it is not possible to extrapolate the numbers in any meaningful way. it's an apples to wildebeest comparison.
I love how it totes that every millisecond matters, and then tries to sell me on the library by showing me that jdbc is still way faster Perhaps instead of focusing on how native the language is and how much importance speed is, which is exactly what jdbc offers, the front page should be focusing on on why I should use Relate over bare bones jdbc 
[removed]
Not sure if you've seen this, but [doobie](https://github.com/tpolecat/doobie) comes close to what you want, except you just have to give mappings on your custom types, but then your class will be mapped out for you.
Hey how about you show some respect? If you haven't anything nice or at least constructive to say, don't chime in just to insult library creators.
It seems we will end up with literally everything on a full spectrum from JDBC to ORM: * we have [Slick](http://slick.lightbend.com/) that does functional-relational mapping and is most ORM-like (and have `sql` and `tsql` string interpolators), * we have [Doobie](https://github.com/tpolecat/doobie) that does the same as Relate, but with mapping into target type (e.g. case class) and inside IO monad, * we might use [jOOQ](https://www.jooq.org/doc/2.6/manual/getting-started/jooq-and-scala/) that recreates SQL syntax inside Java/Scala * and now we have also Relate... which could work out as a... common building block for other libraries if they haven't already existed? I mean, for me it doesn't bring anything new to the table, so...
You can even use shapeless to generate these mappings automatically, but it's a bit complicated
From the page: &gt; Pure JDBC is the hard bottom line for performance, but Relate gets pretty close to that. At Lucid, we make queries at scale, and we've been extremely pleased with Relate's performance. &gt; If you'd like to run the benchmarks yourself, you can find them in the Relate repo!
Nice. Even though we already have a dozen great libraries for database access, new ideas &amp; competition is always good. This looks like a pretty KISS library to me. Thanks for sharing!
Indeed, I don't think it matters much whether it's 20x or 25x at this point (given that Rsc is currently just a prototype: https://github.com/twitter/reasonable-scala/blob/master/docs/performance.md#disclaimer). What matters though is that the prototype of Rsc is significantly faster. This shows that our architecture is at least remotely sound, which makes me think that we're well on track towards our goal to speed up Scala compilation.
Yes I saw those, the problem is all it does is convince me more to just stick with jdbc. It‚Äôs a problem of conveyance to exactly _why_ I want to bother with Relate in the first place over jdbc. I‚Äôm sacrificing a little speed for why? I‚Äôm even still manually mapping row results to my data models manually one property at a time. A bit of syntactic sugar does not a compelling reason to indulge make, honestly 
Yeah, the unique value offering is maybe not all that clear. Personally I think it's about abstracting away the annoyance of dealing with `PreparedStatement`s. 90% of the annoyance is that you have to pass in the parameters in the correct order by index. With an interpolated query instead you're dealing with a single string that safely does that for you.
Hey, thanks for posting! Unfortunately this is probably off topic for the scala subreddit, but it would be great if you could resubmit this post if it ever gets translated to Scala. In the meantime, it may be more appropriate for /r/programming, /r/java, or /r/akka
Competition drives innovation! 
Don't forget Quill.
I knew I was missing something...
I am curiously following the development of Laminar. Are there any plans on developing it further in the coming time?
Oops, missed that!
I think it can be useful to the extent that there may be things Scala can learn from Kotlin. But unless the whole sub is getting slammed with Kotlin posts, I'd say just tune out the stuff you don't care about.
This week I'm adding a really cool feature to Scalafix. The ability to drop-in starting from a version control software revision ([#462](https://github.com/scalacenter/scalafix/issues/462)). This is useful when you have a huge codebase and you want to start enforcing a new policy on the new code that will be written. For example, we can run the git diff between a starting hash and the current index. From this, we can find all additions and modifications. We can build up a set of position ranges where Scalafix is enabled. 
I really wish we did that as well, we get these we-love-kotlin posts twice a week on average and they stopped bringing new information to the table after the first week. As someone else said, this is very kotlin-specific as all other jvm languages do not have nearly the same amount of posting in here. To me this is a clear hint of trolling in progress.
Depends on what you mean by "getting slammed with Kotlin posts"; do you think that having twice a week a Koltin-biased post that basically repeats the same things over and over about why Kotlin is the future and Scala sucks is fine for this sub?
I would like to see a comparison to quill, for the following reasons 1. JDBC is not the fastest driver you can get (due to blocking reasons in how JDBC was designed). You often have to put JDBC behind something like HikariCP to get any decent performance. This is one of the reasons why I am not a big fan of Doobie (or Relate), because its a nice wrapper over an really outdated technology (JDBC) 2. Quill generates SQL statements at compile time using macros (and it uses static queries be default, so they basically get converted to raw SQL strings). I have a suspicion that Quill will be, if not as fast, as Relate for this reason. Furthermore and expanding on the above point, if you really want performance Quill has backends for things like PostGresAsync, which avoids the pitfalls of JDBC 
Some prefer languages to be extensible and as much as possible to be done through extensions. In that school of thought, having to add such things to the core language is a language smell. If designers have to bundle such functions as primitives, it means your language lacked the needed building blocks. When somebody who‚Äôs not a designer misses another such feature, they won‚Äôt be able to write it themselves, and will have to wait forever for the language designer. For that school of thought, or just for fun, I recommend Guy Steele‚Äôs ‚ÄúGrowing a Language‚Äù ‚Äî there‚Äôs a video, and a transcript. First minutes are weird, but Guy hasn‚Äôt gone mad or something.
goto for loops is overkill and labeled break (as in either Java/Scala) is fine. There‚Äôs a standard argument in favor of goto for error handling and resource cleanup (which is mentioned in passing in that thread; I‚Äôve seen better goto advocacies by Torvalds). But they‚Äôre just compiling by hand C++ with exceptions and destructors. JVM languages lack destructors; for such scenarios, you need Java‚Äôs ‚Äúrecent‚Äù try-with-resources construct (from Java 6? 7? 8?), either from the language (in Java) or from a library in Scala. (I think you can write it yourself with by-name arguments).
Indeed, doobie uses shapeless to do this. Not that hard really.
This is a very big and very active design space and it's always interesting to see what people come up with.
I needed them when doing highly efficient conversions from string to float/int (if the string is a valid number) as part of my scalajson library. This type of code looks very close to C style code, and continue would have been very helpful. Of course you can work around continue using flags and extra if statements, but it makes the code not very nice) For those wondering, using idiomatic Scala code for this problem is out of the question because idiomatic Scala code does a lot of boxing/less performant. There may be libraries that can do this stuff with zero cost abstraction, but scalajson is designed to be dependency free.
Well yes and no. From a purely technical level wrt language design, then yes, ReasonML/Ocaml/etaLang/Haskell you can consider are better competitors. From an economic perspective, i.e. getting more developers to start using Scala, Kotlin is probably a far bigger competitor. If you are really into purely functional programming, then you are much more likely to be using Haskell, or if you really want the JVM, then EtaLang. These languages are designed for purely functional programming, so Scala isn't really going to grab any people from this demographic unless they are forced to use Scala. OCaml has really weird reasons as to why its not as popular as it should be, on many levels its much better then Scala (insanely fast compiler, opam right now is a fantastic build tool and with Reason/Bucklescript it can also compile to JS with insanely fast compile times and automatic modules (without any manually wrapping). Odds are though, if you are using OCaml/Reason you probably don't need Java, and so Scala isn't really a contender for this reason. Even though Scala is trying to push as a language of its own, its still the case that Scala's largest demographic of new people are going to come from Java developers (or from data science with spark).
Eh honestly, I don't think they should be banned, but at the same time this specific comparison post isn't really that informative. Its a 5 minute selfie rant that doesn't really go that much into technical detail. As /u/joshlemer said, its not grounds for a ban and that the votes already do a good job of stating that "this video is really unobjective and not being helpful"
what's the difference between `anorm`? which looks equally close? Just performance?
doobie has one downside. it actually just breaks any IDE you throw it to. which means half your code gets red.
I don't understand the appeal of that option. It seems that it just makes prototyping unnecessarily harder. Sometimes I get warnings while trying things out, but I always remove them before making the final commit. You could rather enforce that all commits are free of warnings by configuring your CI builds properly, no? (i.e., making CI fail if there are warnings during compilation.)
I'm a little annoyed that there does not seem to be a good way to compute the average of a `List` in a declarative and efficient way in Scala (this is just one instance of a more general problem ‚Äì I'm not looking for an ad-hoc `.avg` method). By declarative, I mean that I don't want to implement it with mutable variables, and by efficient I mean that I don't want to traverse the list twice _or_ use a `fold` keeping the current sum and count, thereby allocating tons of useless tuples. Yes, I could define a recursive function, but that's unfortunately pretty heavyweight in Scala, because it means I have to provide all parameter _and_ return types. Are there libraries (perhaps from the pure FP community) that allow me to compose folds efficiently and solve that problem?
Quill does not work with complex SQL.
&gt; Are there libraries (perhaps from the pure FP community) that allow me to compose folds efficiently and solve that problem? Not a general fold, but there is `mean` extension method in [Kollflitz](https://github.com/Sciss/KollFlitz/blob/8918ba847dc7d5c47280e17178c3b08af8c44427/src/main/scala/de/sciss/kollflitz/Ops.scala#L48) that you could use.
Thats why they have infix syntax so that you can inline your SQL (with compile time interpolated strings), see http://getquill.io/#extending-quill-infix
only works in simple cases.
Anorm probably comes with more suck out of the box, but otherwise the two libraries look similar: low level string-y queries with just enough syntactic sugar to keep one from hurling all over the place (the usual outcome when working with straight JDBC). Performance looks decent, for raw speed it's a step up from JDBC for sure.
Can you provide an example? I mean in the complete worse case scenario, you can basically completely embed a raw SQL string with no interpolation and there shouldn't be any reason why it wouldn't work (it also isn't any worse then what Relate or Doobie does in this regard)
Very often I can get away with `foldMap` from cats / scalaz: val (sum, len) = list.foldMap(x =&gt; (x, 1)) sum / len It *does*, however allocate tons of useless tuples. On the positive side, you can use it with more than tuples or ints (using it with Maps and Vectors is quite handy). I'm always amazed on how it lets me get a job done by just aligning the types inside a lambda return value. There are also libraries doing seemingly exactly what you wanted: https://github.com/amarpotghan/scala-fold https://github.com/atnos-org/origami and there are also [slides about the latter](https://www.slideshare.net/etorreborre/origami-a-monadic-fold-library-for-scala)
I would believe that tailrecursive functions calls would be the scala-way for this? But I bet those are far from highly efficient
&gt; doobie has one downside. it actually just breaks any IDE you throw it to. which means half your code gets red. FP of any kind makes Intellij go red in fury. Also it only breaks it for the string interpolation. I.e case class User(i: Int, s: String) val myuser = User(1, "hi") val q: Query0[User] = sql"select * from users where id = ${myuser.i}".query[User] q.option.transact(xa) In this simple example, once you set the type annotation on `q`, the IDE works fine. It only redlines the interpolated SQL is all, but everything else works fine and dandy. If you so wish, you can also use: `Query[A, B](someQueryString).toQuery0(a: A)`, which will autocomplete all the way. So really, this is not an issue outside of your interpolated strings. 
`@tailrec` Only works for basic loops, it doesn't work with complicate branching (tailrec in Scala just converts to a basic while loop but you can't represent this type of branching with it)
better files was voted yes!
Oh yes, definitely! I am actively working on the next version. If you're curious about the code you can check out the [`next`](https://github.com/raquo/laminar/tree/next) branch. However, the reason it's not in master is that it requires a bunch of yet-unpublished changes in Scala DOM Builder and other dependencies, so it won't compile for you. I am planning to release a new version of Laminar in a couple weeks (optimistically). The reason I need a new version is because I tried to implement a TodoMVC app with Laminar, and it very painfully demonstrated some shortcomings in the API. So I'm fixing / improving those as I implement more of the TodoMVC spec. There's a [status-report issue](https://github.com/raquo/laminar/issues/1) that I will occasionally update.
yeah that's what I've did. I checked it out today. another thing which I wasn't sure how to do was `sql"INSERT ... RETURNING *"` currently doobie tracks effects by calling `.insert` however since I return everything from the Insert, how can I express that?
http://tpolecat.github.io/doobie/docs/07-Updating.html
scalikejdbc has been here for many years so why should I bother with relate?
Very interesting read. I must say, I've never really felt comfortable with quasiquotes. Looking only at the surface level I found Odersky's leaked proposal much more intuitive with its literal quotation and splicing along with the type signatures that came with it. The many varieties string interpolators that come with scala.reflect-macros make me uncertain of when one be used over another. At the same time the meta language feels alien; both with its extended syntax ‚Äî dots for extracting sequences of type/method arguments for instance ‚Äî that only exist in the context of those string interpolators, and also because it feels unnatural to write the definitions inside a string. It may simply be me that is unaccustomed to this form of meta programming, and that a slightly different approach on the same solution would make all the difference, I can't tell.
Great answer, thanks!
Hey! I've got a assignment in my programming studio class, where we're supposed to make filters for images with different methods by altering the pixels in the images. Now, I have absolutely no idea what I'm doing, but I need some help for making a help-method for this, that's later going to be used by for making another filter that blurs and sharpens picture. def multiplyWithFilter( x: Int, y: Int, image: Image, filter: Array[ Array[ Int ] ], divisor: Int ): Color = { ??? } This is what I have to work with. class Color( red: Int, green: Int, blue: Int, alpha: Int = 255 ) This is the class Color. Multiply with filter is supposed to work so that I multiply a filter with another one from the pixels, and then divide it with the divisor. So basically somehow the same index of an array is supposed to be multiplied with another arrays' same index. If anyone understands basically whatever I'm trying to explain, then please do tell how I could code this. Otherwise I'll give you more information on it tomorrow.
What is an `Image` here?
ScalikeJDBC is great. You should use it if it works for you. I think Relate is interesting because it focuses on speed and simplicity, with as few frills as possible.
Personally, yeah, I'm cool with it. I take a peak, decide whether it seems worth my time, and move on with my life.
"hmmm" "meh" "mah" "kuuhaku"
Odersky's proposal is indeed appealing, but it sounds like Squid covers all the bases wrt binary compatability and relatively battle tested implementation (2+ years research vs. a notation that Odersky dreamed up over a weekend?). The `~` and `'` operators in [Odersky's gist](https://gist.github.com/odersky/f91362f6d9c58cc1db53f3f443311140#example) are just so damn tasty, would be great if that functionality somehow made it into the new macro system. 
&gt; to remove duplication of events I came up with &gt; case class StartSomething(...) extends P1Event with P2Event &gt; So it can be used as both P1Event and P2Event. Is this a good idea? It's probably fine if you have a small number of programs you need to deal with and they're all defined in the same project. You'd want the traits `sealed` so the compiler warns you if you fail to match exhaustively, and then define something like object P1 extends EventHandler[P1Event] { override def handleEvent(e: P1Event): Result = e match { case ss: StartSomething =&gt; // implement inline or delegate to handleStart(ss) } } This is certainly simple enough to get you going. You can reach for more complex solutions as they become necessary. &gt; Any other approach to remove this event duplication? Because you have the `EventHandler[E]` trait that you cannot modify, it'll be useful to maintain something with a sealed trait hierarchy per program so that you get warned when you don't implement something. You could say there are two distinct notions here: the event type and the event representation. Right now they're pretty coupled; the representation as `case class StartSomething` is the event type by virtue of extending the trait. However event types are almost _necessarily_ duplicated because they correspond to different programs, while the representation is more _incidentally_ duplicated and can be shared. So maybe removing all the duplication isn't appropriate since it accurately represents the problem space, but you could reduce some of it. One way to recognize this in code could be case class StartSomething(...) // just data, can be shared sealed trait P1Event object P1Event { // logical event type is "Start", which happens to be represented solely by StartSomething case class Start(ss: StartSomething) extends P1Event } sealed trait P2Event object P2Event { // logical event type is "Start", which happens to be represented solely by StartSomething case class Start(ss: StartSomething) extends P2Event } And then you get almost the same program as before object P1 extends EventHandler[P1Event] { override def handleEvent(e: P1Event): Result = e match { case P1Event.Start(ss) =&gt; // implement inline or delegate to handleStart(ss) } } But this makes more explicit that some parts happen to be shared (the `StartSomething` representation) and some parts must be distinct (the event hierarchies) while also allowing both programs event representations to vary independently in the future if requirements change.
Really impressive work. This might be an example of how Scala and Kotlin differ. Kotlin is trying to improve the immediate experience of everyday developers. Scala is a fertile ground for exploration of new ideas in how fundamental features work. There's a lot over overlap in the area of practical programming, and devs and teams can decide what their values are in figuring out which languages to use for which projects.
I should also add, part of my goal here is to figure out how to accomplish these meta-modeling tasks, _while still_ capturing the familiarity of working with structures like `case class`. I think my frustration comes from the ways in which the sugar provided by `case class` runs out of gas. I should also add that I'm aware that optics can be used to represent changes to a structure in an abstract way, but my concern there is that most optics libraries I've seen represent their changes as functions, rather than as data. (Would final vs. initial encoding be the appropriate way to describe this?) I'm interested in structured data representations.
Can you clarify what you're having issues with when it comes to testing? For a more functional approach I would do something like: io.Source.stdin.getLines .map(rep) .foreach(println) Or for { input &lt;- io.Source.stdin.getLines } println(rep(input)) You'd probably want to throw in a `takeWhile([...])` as well, but I'll let you figure that one out for yourself.
Procedure style functions are discouraged. Keep the `: Unit =`
Sounds like what you need is optics. [Monocle](https://github.com/julien-truffaut/Monocle) is among the best ways of doing optics in Scala. And [Goggles](https://www.youtube.com/watch?v=t2WTtIwgdLc) is a thin macro wrapper that offers incredibly concise and intuitive syntax for it.
My bad. I was on mobile so the Unit = started a new line. Thought it referred to assignment within the method, not a type signature.
Well, I thought about what you've said, and maybe I understood. I think the problem here is that if actor A1 of type A sends a message to actor B1 of type B, then only actor A1 can handle a response from B1 and not an actor A2 of the same type. In this way, I limit the scalability of the solution. Am I right?
If you don't want to use optics, then you might define the type `PersonF` as case class PersonF[F[_]]( firstName: F[String], middleInitial: F[Option[String]], lastName: F[String] ) type Person = PersonF[Id] type PersonChange = PersonF[Option] That way you don't have to type the fields twice.
 class Image(array: Array[Array[Color]]) { val data = array val width = data(0).length val height = data.length def getImage: BufferedImage = { val colorArray = data.flatten.map(_.argb) val javaImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_ARGB) javaImage.setRGB(0, 0, width, height, colorArray, 0, width) javaImage } } object Image { def apply(javaImage: BufferedImage) = { val width = javaImage.getWidth val height = javaImage.getHeight val data = javaImage.getRGB(0, 0, width, height, null, 0, width) val arrays = data.map(Color(_)).sliding(width, width).toArray new Image(arrays) } def getBlank(width: Int, height: Int) = { import java.awt.Color._ val bufferedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_ARGB) val g = bufferedImage.getGraphics g.setColor(blue) g.fillRect(10, 10, width-20, height-20) g.setColor(white) g.drawString("No image loaded", 30, height/2) Image(bufferedImage) } } This is the image class and object.
In general it is a good idea to decouple your side-effecting code (in this case "println" and "readLine") from your decision logic. That way you can easily (unit-)test your decision logic without having to mock the world using magic helper libraries (Mockito and the like). In your case "rep" is already "pure" (= has no side-effects), so you can write a test (I highly recommend [Scalatest](http://www.scalatest.org/at_a_glance/WordSpec)) like import org.scalatest._ import step0_repl._ class ReplSpec extends WordSpec { "The REPL" should { "return the input unchanged" in { assert(rep("foo") == "foo") } } }
I agree with most of this, but to my mind the Scalatest DSL is more complex than it's worth. So I'd use traditional JUnit: import step0_repl._ class ReplTest { @Test def returnsInputUnchanged(): Unit = { assert(rep("foo") == "foo") } }
You probably want to use `zip`, which will let you iterate over pairs. Since you've got two arrays of arrays you'd need two nested loops. So something along the lines of: new Image(for { (imageRow, filterRow) &lt;- image.data zip filter } yield { for { (imageValue, filterValue) &lt;- imageRow, filterRow } yield { //however you compute the new value from the image value and the filter value } })
I think making the Shapeless functionality more first-class should be enough. When working directly with shapeless records you can already do all the things you'd do in a dynamic language - just with a somewhat more cumbersome syntax. The real problem is needing all the aux helpers whenever you want to write a function that does this kind of thing, because Scala's type-level functions are almost an accident of the implicit support rather than a first-class construct. I mean I can form a *value* of a `PersonChange`-like type instantly: LabelledGeneric[Person].to(myPerson).map(Lambda(Option(_))) but there's no nice way to *name* the type of the result. I can write the body of the `(Person, Seq[PersonChange]) =&gt; Person` function just as quickly: val lg = LabelledGeneric[Person] changes.foldLeft(original, {(value, change) =&gt; lg.from(lg.to(value).zip(change).map(Lambda[A =&gt; (A, Option[A]) =&gt; A]((v, mc) =&gt; mc.getOrElse(v))))}) and even that's probably more cumbersome than an expert would write it. Partly this stuff isn't documented well enough, because good documentation is hard and there's little funded work on this stuff. But I think an even bigger problem is how hard it is to write the type for this function; the only way to do it is to write out the record type longhand: type PersonChange = Record.`'firstName -&gt; Option[String], ..., ...`.T def applyChanges(original: Person, changes: Seq[PersonChange]): Person = //body above in which case you have to manually keep the fields in sync, or else manually resolve the relevant implicits and then take the type from them: val lg = LabelledGeneric[Person] type GenericPerson = lg.Repr val mv = MapVales[Id ~&gt; Option, GenericPerson] type PersonChange = mv.Out neither of which is ideal. Really I want to be able to write def applyChanges(original: Person, changes: Seq[DeltaFor(Person)]) = ... where `DeltaFor` is some type-level function I've written - I think you can do this kind of thing in Idris.
I'm pretty sure the biggest use of macros in my own code comes from Shapeless' Lazy (via LabelledProductTypeClassCompanion) and whatever it is that LabelledGeneric uses. (Unfortunately I very much doubt my current clients would be willing to submit code for analysis).
Something like this? val image: Image = ??? val filter: Array[Array[Int]] = ??? val result: Array[Array[Int]] = Array.fill[Int](width)(new Array[Int](hight)) for { x &lt;- 0 until image.width y &lt;- 0 until image.height } { result(x)(y) = image.data(x)(y) * filter(x)(y) }
The Coursera courses listed in the sidebar have started locking some of the assignment content if you are not paying the monthly subscription fee or are outside of the session time frame. Does anyone know if there is a place that has a list of links to the assignments, I am pretty sure that they are not hosted on Coursera. Unfortunately I can't look try to figure it out since the course session I was in ended yesterday and all of the assignments are locked. 
https://github.com/davegurnell/bulletin
I think I have found what I was looking for. Here is the link for the Functional Programming Principles in Scala http://alaska.epfl.ch/~dockermoocs/progfun1/ 
[Recursion Schemes](http://akmetiuk.com/posts/2017-03-10-matryoshka-intro.html#structure-preserving-transformations) should help you here. What you want to do is model the person changes as a recursive algebra trait [+A] PersonMeta case class PersonWithChanges[A](name: String, ancestor :A) extends PersonMeta [A] When you change the person, make the previous person the optional ancestor using `Fix` and `embed`. When you wish to evaluate them into concrete person objects, you can use cats to evaluate the meta person structure however you wish. This looks like the Nat example. If you want individual change actions, model the changes recursively as an expression tree. This looks like the expression tree example. There's a lot more that you can do, but hopefully this points you on the direction you are interested in. 
Thanks for the link
That is sweet! If it works recursively on ADTs, my mind will be completely blown.
Am I alone in never using views? Laziness has always been more confusing than it's worth as far as I can see.
Both have their use cases, performance can vary widely in both directions depending on the use case. In a way (assuming immutable collections of course), strict collection operations can be seen as memoization of corresponding lazy operations, so similar cost/benefit analysis has to be done.
There are many cases where lazyness can allow you to write more readable code without sacrificing performances. Typically you can write your code as a series of transformations, but only one pass will be done on your data. Without views, either you walk your data once per transformation (fine on small data, not on bigger one) or you accept to write your code in a way that accomodates performances, ensuring you only walk your data once.
I use them here and there. If you need to do like: something.map(somethingExpensive).filter(expensiveCondition).take(5)
Are there any measurable performance implications from this change? I think over-all, the design makes a lot of sense. As Scala mostly invites its users to avoid performing side-effects during operations such as map and filter some of the common drawbacks associated with lazy collections are known and "mitigated". It is also great to see how simple it is for a concrete collection type to choose whether it should be strict or lazy!
i'm just surprised kotlin 1.2 was released and no one posted about it here yet
This looks cool, and it has a tiny amount of code to read. To apply a list of updates updates.reverse.foldLeft(thing)(_ merge _) very nice! Read before write updates are broken, as you'd have to apply them all to get the current value to read, but for tracking changes or serializing to disk or sending updates over the wire this looks nice. I'm not sure it is all that above optics for non-serialized updates, since optics have map to allow for updating an arbitrary field, and optics compose.
What I am reading from this is the following: * We created an experimental macro API. The API has a lot of power but is also clunky, difficult to use, and hard to maintain. * We want to create a new, simpler API, but it lacks the power and utility of the existing one. * We want to look at current usages of macros to try to justify how much power we can safely remove from our macro system without breaking users of macros too badly. Do we have a sense for what features of macros that typesafe/EPFL are intent on removing and what libraries will be irreparably broken by the new macro systems?
I'm about a third or more of the way through http://shop.oreilly.com/product/0636920046967.do and so far it's been pretty good. 
Found the following books very useful. * [High Performance Spark](http://shop.oreilly.com/product/0636920046967.do) * [Spark: The Definitive Guide](http://shop.oreilly.com/product/0636920034957.do) &gt; Databricks has made available few chapters from Spark: The Definitive Guide on their [website](http://go.databricks.com/definitive-guide-apache-spark).
[Learning Spark](http://shop.oreilly.com/product/0636920028512.do) is still the best guide for, well, learning Spark.
Apparently typescript has support for this: https://www.typescriptlang.org/docs/handbook/advanced-types.html#mapped-types
That's because their big headline is reusing code across JVM and JS ... something that Scala/JVM and Scala.js have been doing on a mundane basis for almost 3 years.
You might like this sequence: * Spark online tutorial for the very basics. * Advanced Analytics with Spark, focusing on examples that interest you * High Performance Spark to go beyond the basics and learn more about production ready spark. Learning Spark is a bit old (published Feb 2015, targeting Spark 1.3). It was a good book, but I'm worried that you might be frustrated when the examples do not work. Spark in Action is from Manning publications which typically puts out high quality books. However, it came out after I was already comfortable with Spark, so I don't know good it is. Spark: The Definitive Guide is not yet out, but it looks like a good spiritual successor to Learning Spark. I don't think it will replace either Advanced Analytics with Spark or High Performance Spark though. Finally, do not bother with any books from Packt unless you're absolutely desperate. Other than a handful of counter-examples, the books are poorly written, edited, and just not good at all.
That's a clever idea -- parameterize every container with an unbound wrapper to be applied to all of its fields. I think this would generalize to ADTs. I've seen this approach for decoupling from any particular async wrapper, but really _any_ sort of "meta" semantics could be applied to data structures. My big question would be, how does it scale out in practice? If I parameterize every case class in my system, I wonder if it ends up just being a mess, or whether it works out pretty smoothly. 
&gt; in C# all operations are basically just extension methods It didn't have to be that way. While I don't know the team's exact intentions, I assume it was to maintain backward compatibility while adding functionality, at a time when C# didn't (yet) have default interface method implementations (though I think it's been proposed). If they had other language mechanisms at their disposal, or if they had been willing to break compatibility, these could have been added as proper virtual (or even non-virtual) methods to the various collection types.
Interestingly, Clojure ended up adding a pattern to their collection library called "transducers" to try to eat away at some of the remaining performance problems from their lazy collections. The TL;DR is that (vec (map f2 (map f1 coll))) is slower than: (vec (map (comp f2 f1) coll)) because, in the first case, Clojure still has to build the `cons` cells of the intermediate sequence. Even though it defers the work, when you force it with the call to `vec`, it has to generate the intermediate `cons` cells only to immediately throw them away. That example doesn't actually use transducers, but illustrates the point. The same code using transducers might be: (into [] (comp (map f1) (map f2)) coll) Ultimately, my point is that laziness isn't automatically a panacea for performance. Laziness introduces its own overhead and the naive stacking of transforms might still not be as efficient as you'd like. 
`dates.map(d =&gt; (d.first, d.last))` assuming you know how to get first and last.
`foreach` doesn't create a new collection; it applies a function for its side effects: `def foreach(f: (A) ‚áí Unit): Unit ` You're looking for `map`, which transforms the collection. `def map[B](f: (A) ‚áí B): Seq[B] ` https://www.scala-lang.org/api/2.12.3/scala/collection/Seq.html Notice the difference in the types; `foreach` transforms a `Seq[A]` into `Unit` (think `void` in Java); `map` transforms a `Seq[A]` into a `Seq[B]` given a function `A =&gt; B` Something like `result = inputDates.map(x =&gt; x.getFirstDayOfMonth -&gt; x.getLastDayOfMonth`
That seems a reasonable question to ask before wholesale jumping into a new design. Current state is 1.5x slowdown. No idea where what final numbers will be, or what numbers need to be reached to be considered a viable replacement. If it‚Äôs easier for the maintainers to work with but resulted in a 1.2x slowdown, would that be a win or a loss? https://github.com/scala/collection-strawman/issues/290
&gt; because, in the first case, Clojure still has to build the cons cells of the intermediate sequence. Even though it defers the work, when you force it with the call to vec, it has to generate the intermediate cons cells only to immediately throw them away. I don't understand that example. This is exactly what would happen with _non-lazy-views_ in Scala, like `List(1, 2, 3).map(f1).map(2).toVector`, whereas the lazy views precisely avoid intermediate representations, so `List(1, 2, 3).view.map(f1).map(2).to(Vector)` doesn't generate throw-away stuff. In other words, the 1.5 slowdown must have other reasons.
1.5x slowdown sounds about like 1x to me. To be able to iterate lazily over a potentially infinite collection (or a collection that grows while it is being consumed) and baking this into the core language is a real win. We don't write Scala for speed. We write Scala for the lack of side effects, ease of reasoning about what the code does, and the ability to define tasks at such a high level that concurrency or parallelization come for free, and that's the only way we're going to be able to sanely solve the next generation of computing problems that can't fit inside a single machine. I mostly write python code and being able to write lazy iterators in 1 line that can be consumed, paused, and resumed is amazing. The Java code to accomplish the same is 30 lines of boilerplate.
&gt; We don't write Scala for speed. Actually a lot of people do, and for core collections this is a problem. Remember that these are core collections, and if these are slow by default then it cascades for every software written This is one of the primary reasons why I moved from Ruby to Scala, the ignorance about performance in that language has caused a lot of issues which its dealing with now
I think the issue is that when you are dealing with lazy data structures and/or lazy collections, it gets very hard to reason about when *things actually get executed*. Its not that laziness slow by default (well completely technically it is unless you have a strictness analyzer that Haskell does which is somewhat hit and miss for the language), its that reasoning about when stuff gets executed and when from a performance pov is really hard In the clojure example, it appears that the issue is that it creates a lot of pointles thunks which then get evaluated immediately (this is the performance slowdown that happens when you have lazy collections). If it was strict there is no thunk, it just gets immediately evaluated. As a note, its perfectly possible to solve the same problems that lazy collections/datastructures solve using strict techniques, its just not very nice from an abstraction pov. However if done so, it does end up being faster then the lazy equivalent.
Does this have impact on the filter / withFilter difference? 
I think the reason is that the split between strict-sequential, strict-parallel, iterators, sequential-views and Streams never really worked out. The amount of code duplication due to not splitting the data source from the computation and computation from the execution meant that pretty much everything outside of a heavily tested/used happy path was severely buggy. This proposal seems to adopt some of the things I kept preaching over the years, but I think retaining the coupling between collection classes and the execution is still wrong. For instance persons.groupBy(_.lastName).apply("Miller").sortBy(_.age).take(3) should allocate exactly one data structure with three elements during its execution, regardless of the input type of `persons`. In the end, "strict" evaluation works for pretty much no use-case outside of in-memory collections, and even there the requirements are kind of steep: At every point in time during operations on a strict collection it is required that at least twice the amount of memory of the data-structure is available, even if the result never requires them to be computed. This doesn't work for files, it doesn't work for infinite collections, it doesn't work for reactive streams, it doesn't work for databases and it doesn't work for distributed collection operations (Spark).
&gt; should allocate exactly one data structure with three elements during its execution how, when you have a `sortBy`?
Because you only request three elements, so keeping space to potentially sort all of them is wrong.
&gt; Because you only request three elements, so keeping space to potentially sort all of them is wrong. oh, i see, the mythical [suficiently smart](http://wiki.c2.com/?SufficientlySmartCompiler) ~~compiler~~ standard library. i'd love to see you implement such a thing, where a method contract depends on the methods that will be called upon it's results.
It's exactly the opposite. The "sufficiently smart X" has failed over and over, in the industry and in Scala (see all the approaches to make collections fast). Scala is at a design that has pretty much abandoned by all other languages except Javascript and PHP. Making collections fast is hard, but doing so while having to fight against collections signatures that demand the exact opposite is even harder.
&gt; We don't write Scala for speed. *I* don't write Scala for speed, but I do expect the core library to be; if it isn't, then that puts the burden back on me if I want my code to have reasonable performance.
&gt; Also the current collections are quite slow already (because of "nice" reasons like co-operative equality) which are considered for removal because there is no way around them [Project Valhalla](http://openjdk.java.net/projects/valhalla/)
talk is cheap, show us the code
If you care about performance, actually the opposite is true. Rust is strict by default and the strictness is central to performance and knowing when allocations happen. Haskell, which is lazy by default, actually has far more problems then strict by default languages in this regard. The GHC compiler has to implement a strictness analyzer to figure out when something can be called in a strict manner (because lazy by default creates a thunk for every single statement) and it turns out that for the same reasons why a sufficiently smart compiler doesn't exist (http://wiki.c2.com/?SufficientlySmartCompiler) is a reason why the Haskell performance is so brittle. If you write your code in a way that the strictness analyzer can pick up, then fantastic! But some small rewrite of your business logic can then completely break the analysis and you suddenly get a massive performance regression. This is also similar to the type of problems you have when working with SQL, depending on how the analyzer works, seemingly subtle changes to your SQL can reduce the performance of query thousand fold (and then you end up having to write less than ideal SQL queries just so it can get analyzed correctly) In Haskells case, they had to make their own data-structures like Map to be forced strict, because its practically impossible for their strictness analysis to pick up cases of putting values into a Map, and with forced lazy evalution if you put a `1 -&gt; "some value"` into a Map, then you are not putting `1` as a key, but instead you are putting a thunk that is `() =&gt; 1`. What you are asking for isn't really possible, there is no sufficiently smart compiler that will work in all cases to provide completely optimal queries.
This isn't what I was talking about. Co-operative equality is what you do have something like `Map(1.0 -&gt; "some value")` then you can do `Map(1)` and you will get "some value" back.
Yep, and project valhalla will make this faster, potentially up to the speed of the additional code not even existing.
&gt; potentially up to the speed of the additional code not even existing. I don't think its possible to make it zero cost, it is possible that it will make it faster. This is kind of detracting from the main point I was making though
What I'm saying has already been implemented over and over. Neither laziness nor strictness analysis come into the picture at all.
I got told that no such proposal would even be considered, so there is that.
how convenient
Yes precisely, core libraries have to have a high bar when it comes to both speed and correctness. This is hard to do, but we should not just completely ignore speed because it makes things easier.
I'm not sure what do you expect from me. The evidence is out there.
If the type is known at the call site which will be way more likely with specialized generics, the JIT compiler can discard all the type tests as dead code after inlining. Anyway, I think if the new collections will be only as fast as the old ones, the redesign has failed.
&gt; If the type is known at the call site -- which will be way more likely with specialized generics -- then the JIT compiler can discard all the type tests as dead code after inlining. Its not just type tests, its also testing that `1.0 == 1`. One is a double and one is an int. Depending on how exactly its defined effects how it will get inlined
Yes, and it would cost just as much doing the equivalent in Java, but all this code leading up to it would be a candidate for elimination: https://github.com/scala/scala/blob/2.13.x/src/library/scala/runtime/BoxesRunTime.java#L119-L199
Yeah you're gonna want to include an `application.conf` in the resources of your application, with proper akka configuration because it looks like ReactiveMongo is trying to load one and didn't provide defaults? That's kinda weird in my opinion but yeah you'll need to include akka { } in your app. What version of reactive mongo are you using?
...I can't believe it's that easy. 
i'm just saying that, in my experience, people who like to complain a lot in high level, abstract terms usually are not very good at delivering. nothing prevents you from working on your idea as an alternative, open-source collection library. if it turns out good, many people will adopt it, like google guava.
&gt; "strict" evaluation works for pretty much no use-case outside of in-memory collections True enough, but in-memory collections are the vast majority if not the entirety of the use case. If there turned out to be common abstractions that were reusable between collections, files, streams, databases and spark, great - but I'd want to see those things showing up in a third-party library and experiment with them there before building them into the standard library. My experience has been that things like streams have been better implemented via various third-party approaches, and there's still a lively debate over which of several such third-party approaches is best (akka, monix, fs2...); implementing one of these designs and making it part of the standard library would be premature and counterproductive. The current Scala collections aren't great for the in-memory case, and my sense is that laziness would make things worse (performance is harder to reason about because it's no longer compositional, and when side effects happen becomes very hard to reason about). I'd rather see a tight focus on building a good in-memory collection library than an effort to generalise beyond that.
:-) Spend some time with the Scaladoc for List and try out the various operations in the REPL. You'll learn a lot of tricks. Martin's [Functional Programming Principles in Scala](https://www.coursera.org/learn/progfun1) is a really good introduction to this stuff.
Can't agree more. The most common case of collections is actually small collections that are in memory. Streams are an exceptional case, and for this reason it should be treated exceptionally because they have completely different requirements and performance characteristics. The predominantly common use case (talking about 80% and greater here) are small in memory collections, often collections that are smaller then 5. If these operations are slow, then it will really show because these are the most common operations. This is even seen with the current collections, i.e. if you look at immutable `Map`, its specialized to size 4. In Clojure its specialized to 10 iirc. This is in general due to immutable collections being really slow due to structural sharing requiring shallow trees (immutable `List` is the exception here). In conclusion we really should not lose sight of what the most common use case of collections are, which are small `List`/`Seq`/`Map`/`Vector` of sizes that are less then 10
&gt; Equal things comparing as equal is a price I'm willing to pay for 1.0 == 1 working correctly, and I'm happy to see that the additional overhead might go away in the future. This is what I disagree with. I think the behavior is actually completely un-intuitive, most people don't know that it exists (I didn't at least up until a couple of years ago) and it puts a performance penalty on everyone for a very marginal use case
Actually, you can avoid it in Scala, no need to write Java. But you need to reimplement the collections and have them use `equals` internally instead of `==`.
Correct, but the context of my statement was reusing the current Scala collections and not re-implementing them
&gt; its perfectly possible to solve the same problems that lazy collections/datastructures solve using strict techniques Do you have an example of that? What makes it not nice?
To clarify, I mean the strict solutions for the typical problems you would solve using lazy collections are not nice. I.e. if you are dealing with infinite sequences, then lazy data structures are much nicer. The strict solution to these problems involve using stuff like `Iterator`'s amongst other things. Its not very nice, but it is possible (and Java in general has been dealing with these kinds of problems for decades without having lazy collections/datastructures)
Ah, I see. For me `Iterator` is "lazy," or at least it is non-strict since it doesn't perform the transformations eagerly. 
You are asking whether some things are equal, not whether they are identical. Therefore, `1.0 == 1` makes perfect sense, because they are equal. If you want to know whether there are things identical, ask whether they are identical. How to do that? The solution has been pretty much out there since 2013, it was just proposed by the wrong guy.
Thank you for the help! I'm having some trouble wrapping my head around the functional style, but I am starting to get it. It's a fundamental difference in how I think about programs.
Thanks! I need to read more documentation on scalatest, but things are starting to look great now.
I hate Java &gt;_&gt; But I recognize the usefulness of the JVM, which is one reason why I find Scala so interesting.
&gt; people who like to complain a lot in high level, abstract terms usually are not very good at delivering My contributions to scala/scala are +17KLOC, -35KLOC, work on documentation, spec, and website is around 30KLOC, my port of the `java.time` package to Scala is 40KLOC, plus large chunks of the port to Scala.js, plus the work on porting it to Scala-Native. Please let me know when I reach an acceptable level of delivering.
I agree with you overall. I think what you describe could probably be achieved by using a maximum heap data structure. I would actually find it quite exciting to try and design a library which works this way. But I also agree with the other commenters that you just stop complaining and acting like a victim, continually spewing negativity because things didn't always go your way (which you seem to have been doing since at least 2015). It gets really tiring, and is actually hurting the community. Honestly, at this point it does not matter if you have good points (which I don't doubt you often do), you just read like a frustrated person with an agenda to destroy the Scala community, and that's really not nice.
&gt; but in-memory collections are the vast majority if not the entirety of the use case Maybe they are the vast majority, because the approach collections took doesn't work for pretty much any other use case? Just one example: People have complained for ages about scala's anemic io package. The issue is you can't implement decent support for operations on IO within the constraints of the collections interface gives you. &gt; If there turned out to be common abstractions that were reusable between collections, files, streams, databases and spark I think that's painfully, clearly visible. They even use the same names with pretty much the same signatures, but all of them had to roll their own stuff, because the interface in the std lib doesn't work. &gt; I'd want to see those things showing up in a third-party library and experiment with them there before building them into the standard library That's exactly what separating the data source from the description of the computation and their execution gives you. &gt; laziness would make things worse Yes, laziness is not what you want.
You need to run the exercises: https://www.scala-exercises.org/ Do std lib and scala tutorial. Read all the posts in the series [The Neophyte's Guide to Scala](http://danielwestheide.com/blog/2012/11/21/the-neophytes-guide-to-scala-part-1-extractors.html). Come back and do the cats tutorial. Now go read the scala stdlib docs for [List](http://www.scala-lang.org/api/2.12.3/scala/collection/immutable/List.html) and [Option](http://www.scala-lang.org/api/2.12.3/scala/Option.html). They'll make a lot more sense, now. Basically, all the collections have four basic methods. Below, any capital letters, like `A`, represent generic types (That is, A can be one of Integer, Character, etc). Any capital letter with brackets, like `F[_]`, represents any collection that can contain something else (like a list), and ` A =&gt; B` represents a function from type `A` to type `B.` Here are your four basic methods: // runs a function, f on a collection // containing all A's, and stores the result in the same collection // type. F[A].map[B](f: A =&gt; B): F[B] // runs a function f from A to a collection type F, containing Bs // then removes the Bs from the F[B] collection returned by f // resulting in an unnested F[B]. // F is always the same collection type you started with, so if it was // list, then f would return a list and flatMap returns a list. F[A].flatMap[B](f: A =&gt; F[B]): F[B] // Filters a collection with a predicate f that takes each element and // returns true if the element should remain in the collection after // filter is applied, and false if it should be removed. F[A].filter(f: A =&gt; Boolean): F[A] // For each in, with a starting empty value -- defaultIfEmpty is // passed to f with the first element, the return of f is passed with // the second element, that return is passed with the third element, // and so on, until the collection is consumed. The final return of f is // the result. If nothing is in the collection, this results in // defaultIfEmpty as a return. F[A].foldLeft[B](defaultIfEmpty: B)(f: (B, A) =&gt; B):B With those four methods you can do 95% of what you need to do. Again, read the above sources, do the tutorials, and this will make a lot more sense, and you'll be able to do things very easily. Nearly all the things in FP are built off of these four basic methods. 
&gt; just stop complaining and acting like a victim, continually spewing negativity because things didn't always go your way I think I have the right to be a bit unhappy when certain people think it's appropriate to harass my place of work, publish private information together with wrong claims about me on the internet. It's not like there isn't a reason why I'm not contributing anymore, just like there is a reason why other contributors left before me and contributors that will leave after me. &gt; is actually hurting the community [...] destroy the Scala community How many people have to leave infuriated until someone considers that the problem might not the individual contributors that left? 
It is used all over the place in the std library, cats, scalaz, etc. It's just as scalable as normal generic paramaters, but rarely when using it inference may break down, so you'll have to do some tricks. This is MUCH EASIER when using cats or scalaz. Look at this contrived gist (fire it up in intellij in a project with cats as a dependency): https://gist.github.com/jackcviers/33522d3b59acf1caecb6925dd15cea44
Oh man! Best day ever. I just started using it in a project and its been going great so far. 
When coming from a language like Java, features like implicit parameters seem weird and complicated at first. Or "magic". They look like minor features that save a little bit of boiler plate. Please try to stay open, and learn more about the FP abstractions and how to use them in Scala. Then you will see the *real* benefits of this features. Maybe you should tell us your opinion again in a year or so. That being said, I think Kotlin has its place too. I just don't think it's the next iteration of the JVM land after learning that Scala is too complex... If you're (or anyone else is) interested: I just wrote a blog post ( https://coding.plus/artikel/thoughts-about-choosing-a-language.html ) about the criteria that I find important to decide what language to choose for a project. I also write about the "complexity" of Scala. 
Because instead of having a Graph class, many would argue you need a Comonad typeclass instead. Or instead the standard library need recursion schemes and Cofree.
you forgot to count the KLOC (lines of comments) here on reddit
The lazy-by-default approach is mostly beneficial when you're implementing lazy collections because you don't have to override pretty much everything or get incorrect semantics. The reverse risk is smaller: If you don't override a lazy implementation for a strict collection type you only suffer a small performance impact but it's still correct. To avoid this overhead we're providing strict implementations of the basic methods (as in https://github.com/scala/collection-strawman/pull/305).
This statement is wrong. Quill works fine with complex SQL and makes it much easier to compose a complex query out of small quotations. Also schemaMeta **does not** make the query dynamic (note that you can't do type widening).
It's in the pom.xml, &lt;!-- https://mvnrepository.com/artifact/org.reactivemongo/reactivemongo --&gt; &lt;dependency&gt; &lt;groupId&gt;org.reactivemongo&lt;/groupId&gt; &lt;artifactId&gt;reactivemongo_2.12&lt;/artifactId&gt; &lt;version&gt;0.12.6&lt;/version&gt; &lt;/dependency&gt;
I don't think Java has much to do with it. I just find "make a class with a no-args constructor where each test is a unit method annotated with `@Test`" a much simpler and clearer way of organising tests than these pseudo-English DSLs.
It would be great if the performance impact was documented a bit more. I've left a comment about that at https://github.com/scala/collection-strawman/issues/290#issuecomment-348036126. Please let me know if you'd prefer a separate issue dedicated to documentation.
If only there was a recursion scheme library for cats.
I would be interested in reading something on representing graphs using Comonad - while I will also google, do you have any specific recommendations on reading (preferably scala, though haskell if necessary)?
I think it's a good question, I will be interested in reading responses. Perhaps a partial answer is that there are so many possible ways a graph (or even a tree) could be represented. The scala collections library is complicated enough - trees would be another order of magnitude and then graphs yet another. When reading scala blogs or discussions on libraries there are often comments on 'the object graph' or 'the function graph'. Why not make such graphs explicit?
Work in progress.
&gt; @js.native &gt; @JSGlobal &gt; class OuterClass(x: Int) extends js.Object Out of curiosity, why not `@js.global`?
Because the set of annotations grew a little bit too organically in the beginning. In fact now they are all of the form `@JSSomething`, except `@js.native`. So the real question would be "why not `@JSNative`?". I don't really have a good answer besides history made it that way, though. We typically rationalize it as `@js.native` goes together with `= js.native`, which is a special method, but there's no strong reason nevertheless.
I guess yes. This code has no other expensive stuff (simple calculations can be highly optimized on modern processors). On the other hand immutable arrays are fast due to small cpu cache miss.
I've made some perf benchmarking (not best due to jvm :( ) but cache-misses should be accurate: ``` Performance counter stats for 'scala /home/slovic/Projekty/study/Manacher-Algorithm-in-Scala/target/scala-2.12/palindromes_2.12-0.1.0-SNAPSHOT.jar quadratic': 1763,671845 task-clock (msec) # 2,029 CPUs utilized 5377473460 cycles # 3,049 GHz 8206343568 instructions # 1,53 insn per cycle 223135924 cache-references # 126,518 M/sec 47412156 cache-misses # 21,248 % of all cache refs 0,869382455 seconds time elapsed ``` ``` Performance counter stats for 'scala /home/slovic/Projekty/study/Manacher-Algorithm-in-Scala/target/scala-2.12/palindromes_2.12-0.1.0-SNAPSHOT.jar functional': 3235,989841 task-clock (msec) # 2,631 CPUs utilized 9775014003 cycles # 3,021 GHz 12839187724 instructions # 1,31 insn per cycle 252587935 cache-references # 78,056 M/sec 58124513 cache-misses # 23,012 % of all cache refs 1,229895126 seconds time elapsed ``` ``` Performance counter stats for 'scala /home/slovic/Projekty/study/Manacher-Algorithm-in-Scala/target/scala-2.12/palindromes_2.12-0.1.0-SNAPSHOT.jar imperative': 1217,352018 task-clock (msec) # 1,805 CPUs utilized 3691657142 cycles # 3,033 GHz 4744865789 instructions # 1,29 insn per cycle 148720517 cache-references # 122,167 M/sec 24369674 cache-misses # 16,386 % of all cache refs 0,674409035 seconds time elapsed ```
ps: https://github.com/scalway/Manacher-Algorithm-in-Scala/blob/master/README.md
hi, I just noticed your fork, thanks for your contribution and help:-) 
[removed]
Directed acyclic graphs have a very immediate, natural representation in terms of the object-&gt;field relationship. One could consider the invisible supertype of all Scala datatypes to be the directed acyclic graph.In that sense Tree already extends directed acyclic graph as it should, and the relationship is what you'd expect. &gt; My point is that graphs are a fundamental logical abstraction that I think scala would benefit from having, while also being in the unique position of being able to possibly implement it directly into the sdk in a manner that is both intuitive and efficient. &gt; I'm aware that there are several Graph libraries for scala, and implementations in something like Spark that give a lot of the functionality for real-world use-cases of graphs, however therein lies actually a small part of the problem is that you could find a hundred different "organically" created bare-bones graph models, including dozens of ones written as case classes for temporary problems, and it would just be so much better to have something better from the get-go, something that officially supported the mathematical modeling principles associated with graphs and scala, rather than the dozens of opinionated ones that are likely heavily influenced by the constrained use case they were built to support. I think we have many graph libraries because it's not at all obvious how best to represent graphs. If there were a clear consensus on an intuitive and efficient representation of graphs I would be all for including it in the standard library, but I don't think there is, in which case picking one and making it the standard is extremely premature. Better to allow the libraries to explore the possibilities, and build graphs into the standard library only as and when the best design becomes clear.
I have been working on stabilizing the API for [Jetprobe](https://www.jetprobe.com), an integration test DSL for big data pipelines. This week the primary focus is on making it easy to extend the framework to support additional connectors. I also wrote an [article](https://medium.com/@shadamez/integration-testing-the-functional-way-64109e44fe50), which describe it's usage. 
I'm not aware of the harassment you are talking about, but nothing justifies your attitude. Two wrongs don't make a right. For what it's worth, you're the one looking like a bad apple here, and it just looks like you're trying to find bad excuses to justify your negative behavior. You have the right to be unhappy, but you don't have the right to try and make everyone else unhappy as a result. Also, technical discussions are not the right place to discuss your alleged harassment. I don't know, call the police or talk to the representatives of the community in private; but more importantly, please leave everyone else alone and quit trying to make every discussion topic about yourself and your bad experiences.
What is best book for learning JAVA concurrency primitives...L things such as: LinkedTransferQueue etc... I'm interested in underlying algotihms besides that etc... it seems quite interesting, and I'm sure you folks have some great recommendations
It was a perfectly technical discussion until you stepped in with ... &gt; just stop complaining and acting like a victim, continually spewing negativity because things didn't always go your way [...] So I'm not really sure what you are trying to do. You started with these allegations out of the blue, and I think I have the right to respond to them. Is this what I have to expect whenever someone disagrees with technical opinions, attack the person voicing them?
My favorite scala project by far. Thanks
I have a puzzle for you guys. Which lines lead to compilation errors?
I could answer but this looks suspiciously like some hiring test and that would be cheating...
Dude literally just try compiling the program and see which lines fail if you want to cheat.
And its about compile errors! Just try compiling it and you'll have your answer!
[removed]
Just copy paste that code into a scala REPL or IDE to try compiling it and you'll have your answer OP :)
Line 3. You need to override perimeter because it's already defined in super class Rectangle. In addition, you don't need to override area in Rectangle, since you only assign a type to area in super class Shape, not a definition.
Im a game programmer, and I _really_ want to give Scala Native a try. Especially when you could run the same scala code on client and server. 
The `provided` scope makes me suspicious. Maybe there is already a `reference.conf` somewhere else on the classpath that your class is being run as a plugin from? Check the `reference.conf` in your output jar, and the one in your akka-actor jar, and check that all the keys from the latter are in there. But also check whether there's a `reference.conf` in this `spigot-1.12.2.jar` - maybe that's conflicting with the one you're building.
How does this compare to the approach in Java, which incidentally is the approach I used in Scala back in the day when I coded in it? Specifically, if I wanted things to be lazy, I would basically write collection.iterator.map(fn).filter(fn2) and the force the iterator into a collection at the end. IIUC this is basically how Java streams work, except they try to be a little smarter and more efficient underneath the hood.
I think theres a lomg way to go. And im not sure if you can reuse same code. Doesnt native require annotatioms and platform specific stuff?
that's not a puzzle, that's lack of understanding something that is well defined and documented.
Spigot has no reference.conf, it's just a normal Java application loading plugins with a jar loader. I'm pretty sure it was mangling the classpath and I just couldn't fix it so I switched to casbah and it ended up working.
I havent looked a _huge_ amount into it yet. I mean, Im used to locking certain code in a platform specific abstraction, then just dealing with a subset for general code. I would _hope_ that large chunks of it could be shared, and only some weirdness can be locked away in a library. And Im sure theres a long way for full release, but I only have time for toys right now anyway. 
I would say line 3 because it needs an override modifier?
Any previews? I work on query languages. And usually I am perfectly happy with top-down, bottom-up, and tree-folding (accumulating a value over the tree structure). It is beyond me why there is no established framework for doing that in scala in 2017 that doesn't involve reflection and is accessible to an average scala programmer.
There's a port of Matryoshka in the works. It will be the same but with less scalaz and more cats.
Actually that was a lie. It will be much better than Matryoshka.
Does it do triggered recompilation?
Looking forward to it.
I'm working a lot with graphs. I was never fully happy with any provided graph-library. And all my attempts in writing a general purpose graph library so far failed (I tried a lot). Every project I was working on had different requirements which where difficult to unify in one concept and therefore got its own graph implementation. Examples are: Different vertex/edge types or even a unified vertex/edge concept (atoms), data storage in vertices/edges, hypergraphs, directed/undirected graphs, performance characteristics of the underlying representation (vertex+edge set, adjacency matrix, adjacency list, ...) which affect modification performance and algorithms, mutable/immutable. If I had more time for that again, I think I would follow a type-class-based approach. I'm always open for fresh ideas.
Really glad that 2.13-M2 builds work now. I've been waiting for that to support 2.13 in a personal project for the past week or two. Also I'm very glad that the SBT team and community are taking all of the constructive criticism and such to heart and people are trying their best to make SBT better. Honestly it's already one of my favorite build systems but it is very complex and was quite a deep dive for me to figure out how to configure beyond the basics. Anything that makes it easier to jump in and more powerful is much appreciated.
Still much slower than 0.13 for me. Here are the times in seconds for two `;clean;test:compile`, cold and hot: SBT 0.13.15 - 174 &amp; 130 SBT 1.0.4 - 211 &amp; 162 
Love these guys, they are always coming up with something :-). I really like the sbt automatic import!
Ammonite scrips enables by `Language &amp; Frameworks &gt; Scala &gt; Worksheet &gt; Treat .sc as Ammonite`
&gt; This was a weird one since we had to confirm that it works by compiling the bridge using Scala 2.13.0-M2 using sbt 1.0.3, but sbt 1.0.3 can‚Äôt compile 2.13.0-M2 yet. isn't that always the case with any new version of scala? how was the bootstrap process before?
Command history should work in the default REPL. You might look at [Ammonite](http://ammonite.io/#Ammonite-REPL) for a richer experience.
If you are talking about something like ~compile, not yet: https://github.com/scalacenter/bloop/issues/7
Thank You. I meant to say history search but I will try Ammonite. 
Cool. I'll try it with watchman as the discussion suggests :)
Are you looking for some specific functionality? CTRL+R works for me, with the gotcha that cancelling the search with CTRL+C exits the repl.
So it was an experiment. If such kind of puzzles is entertaining to the community :) I can post more of them, and I am trying to collect more of them. Maybe if it will be interesting will create new subreddit for it So yes, the right answer was 3, because: Scala requires an override modifier for all members that override a concrete member in a parent class. The modifier is optional if a member implements an abstract member with the same name. The modifier is forbidden if a member does not override or implement some other member in a base class
haha, yeap, you can do that!))
Completely correct! :)
Yeap! good one)
not a hiring interview, but yes something you can get on interview. trying to collect such things)
[removed]
I didn't know to name this kind of "quiz". It is an experiment posting questions on some details of scala language
Great! correct) +1 Complete answer
A big realization for me was that filter can be implemented in terms of flatMap. map can be implemented with flatMap too, assuming a constructor function to put individual items into the collection. flatMap can also be implemented in terms of map, assuming the ability to flatten a nested container to one level. But foldLeft can implement all of the above. 90% of the game is learning to choose the appropriate tool for the task. Once you do, it's all gravy! 
Weird to see such handy new features from a company that's trying to kill our language and replace it with Kotlin! ;) 
Slash syntax seems like such a massive improvement to me. I'm the surface, it seems to go a long way toward the axis system Just Working in an intuitive way.
I have no direct experience, but Java Concurrency in Practice is the recommendation I remember seeing going around.
Try Vincenty.
Nice presentation, thanks for sharing! PS.: All the `yield`s are missing in the `for-comprehension`s shown in the slides.
far from an original idea: http://scalapuzzlers.com
https://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/MIT_GNU_Scheme_Logo.svg/500px-MIT_GNU_Scheme_Logo.svg.png
https://shop.spreadshirt.com/47-Degrees/scala+division+unisex-A107561599 If you can get that as a poster :)
If you're looking to save time in the future, or would like to see other implementations, Spatial4j has Haversine, Law of Cosines and Vincenty methods available and is Apache licensed: https://locationtech.github.io/spatial4j/apidocs/org/locationtech/spatial4j/distance/GeodesicSphereDistCalc.html It's really easy to use from Scala as well.
Thanks! I know this thing but it is pretty frozen, I mean it doesn't grow much :)
[removed]
That's a really cool shirt, I'd totally wear it if I could get it without the text. Not that the text is bad either, I just think some things should be left implicit. Personally I have [this atrocity hanging](https://i.imgur.com/zsqFRYH.jpg) on my wall, but it's not really scala related.
You could print out the [cats infographic](https://github.com/tpolecat/cats-infographic) ‚Ä¶ I have been meaning to do that :-)
The future will always have some nonzero overhead... In a system with a lot of messages flying around, having millions of incomplete futures begins to have impact, even though the marginal overhead for a single future is pretty small. 
Okay, I realize that a future will have nonzero overhead. But, from reading the Play documentation (https://www.playframework.com/documentation/2.6.x/ThreadPools), it seems normal and encouraged to create many futures.
The short answer is design intent. Akka apps are designed to scale to large numbers of vms, actors, messages and throughput. Potentially handling application workloads much larger than a play server, and generating way more messages per unit time. If you were getting enough requests in a play app to spawn enough futures to become a problem, you'd likely be looking for a new backend or workload delegation model. There's really nothing wrong with a few properly used futures in most app designs, but akka allows us a better way for when things get extreme. 
Get this printed on a big poster: "A monad is just a monoid in the category of endofunctors, what's the proble‚Öø?"
Yeah, that's a fair point. But I still don't understand why Akka would put that strong of wording in their docs.
I've switched from Secure Social to Silhouette over a year ago. http://silhouette.rocks/
&gt; Reworked implicit search #3421 &gt; &gt; The treatment of ambiguity errors has changed. If an ambiguity is encountered in some recursive step of an implicit search, the ambiguity is propagated to the caller. Example: Say you have the following definitions: &gt; &gt; class A &gt; class B extends C &gt; class C &gt; implicit def a1: A &gt; implicit def a2: A &gt; implicit def b(implicit a: A): B &gt; implicit def c: C &gt; &gt; and the query `implicitly[C]`. &gt; &gt; This query would now be classified as ambiguous. This makes sense, after all there are two possible solutions, `b(a1)` and `b(a2)`, neither of which is better than the other and both of which are better than the third solution, `c`. I'm pretty new to Scala, so maybe this is a dumb question, but if I ask the compiler for a `C` value, why is a value that gets converted to a `C` considered better than a value that is already a `C`?
We use this well-maintained OAuth2 library: https://github.com/nulab/play2-oauth2-provider
but what is the thing on the bike?
They wanted to be strongly typed but settled for strongly worded.
Different audiences. The Akka actor documentation looks at critical path -- doing exactly one thing, all the time, and performance is critical. The Play documentation assumes that every HTTP request is going to be doing a little bit of everything on every request, and there is interaction with blocking APIs.
Note that this doesn't say anything about _conversions_, the methods are abstract, and only the signature can be used to determine implicits. Because `B` is a sub-type of `C`, when you have both values of `B` and `C` in scope, which is the case here, and you ask for `implicitly[C]`, there are several possibilities. From your question, I gather that you'd think that `c` would be better because it looks like the type is "closer". But that's not actually the case. There is no reason to believe that `b` is a worse candidate than `c`. So one could argue that this ambiguous. AFAIK, however, the implicit search would use the _more specific_ value, and therefore (as written in the post), `b` is better than `c`, _because_ it is a sub-type. This is and was the behaviour. Now what is changed, AFAIK, is that before you would indeed get `c` because `b` is ambiguous; whereas now the ambiguity is "bubbled up" and actually reported by the compiler, instead of being silently overriden by a "worse" candidate `c`.
I don't pretend to know the answer, but possibly because B extends C? Just speculating here; that doesn't really make sense but is the only thing I can think of.
Genuine question: why not just fail to compile when more than one potential ambiguous implicit in scope. What benefit is there in handling this situation over the simpler solution of not?
They still have to sell IDEs ;)
https://i.redditmedia.com/rjkV2W2vtA75E0lK11Dbx1aMBqtIjPcBTPGJRTmfbyM.jpg
Thanks, I think that's a good distinction.
[blub-lang](http://www.onlineprogrammingbooks.com/images/an-introduction-to-programming-in-go.jpg)
that looks awesome!
I like the idea!
Hehehe, good one!
I don't think they are trying to kill scala more like trying to get acquired by google :-) 
The way I understand the documentation is not that it‚Äôs bad to use futures in actors in general, but only in the context of a blocking call if you don‚Äôt use a dedicated dispatcher. If the future is not blocking, or if you use a separate dispatcher for the blocking part, it‚Äôs fine. 
Hi all I have the following function, that do recursion: @tailrec private def pool[F[_]: Monad, A] : Consumer[String, String] =&gt; (Vector[KkConsumerRecord] =&gt; F[A]) =&gt; IO[Unit] = consumer =&gt; cb =&gt; { val records: ConsumerRecords[String, String] = consumer.poll(Long.MaxValue) val converted = records.iterator().asScala.map(rec =&gt; { KkConsumerRecord(rec.key(), rec.value(), rec.offset(), rec.partition(), rec.topic()) }) val vec = converted.foldLeft(Vector.empty[KkConsumerRecord]) { (b, a) =&gt; a +: b } cb(vec) pool(consumer)(cb) } The compiler complains: [error] /home/developer/Desktop/microservices/bary/kafka-api/src/main/scala/io/khinkali/Consumer/KkConsumer.scala:57:10: type mismatch; [error] found : org.apache.kafka.clients.consumer.Consumer[String,String] [error] required: cats.Monad[?] [error] pool(consumer)(cb) [error] ^ [error] two errors found What am I doing wrong? As you can see, `cb` is kind a callback function and I wanted to know, if callbacks are implemented in this way. Thanks 
Thanks, yeah, I re-read it and realized that I misinterpreted what they were saying. That makes sense.
Or how about when you change the SBT build and recompile, and then the intelliJ autoimport starts, then the two sbt instances fight over a lock on .ivy2.. 
People here have raised fine points, but also part of it, actually, is the nondeterministic nature of futures combined with the use of actors. Actors are often used to isolate state, and you may use something like the `sender()` function by mistake in a future, only to have it refer to someone else after the future has returned. Another thing that can happen, is that if you hold some state in the actor and use it for at-most-once processing or something, you may find the state reference inside the actor you've been using not to be what you expected, as it may have been changed by the time the future returned. You _can_ use futures just fine as long as you don't expect them to handle state, though. P.s. Note: I don't advocate the use of akka actors at all.
Supports OAuth, CAS, SAML, OpenID Connect, LDAP, JWT... https://github.com/pac4j/play-pac4j
15k keys isn't a whole lot, shouldn't take that long to resolve, not that I think key resolutions takes particularly long anyways
well it looks like this is fixed in the newest intellij 2017.3 (but now it takes longer...)
&gt; ... thereby allocating tons of useless tuples. This is one of those cases where you have to make a trade-off. It's the nature of the usually JVM GCs that lots of short-lived allocations are sub-optimal. It's possible to have a GC that handles lots of short-lived allocations well (e.g. OCaml with its super-fast mark-and-sweep GC) but OCaml has other trade-offs. So in the JVM I'd say mutable variables are actually idiomatic for performant code, especially even that you can compartmentalise the mutation effect and keep your functions pure.
I wrote my own, inspired by SecureSocial. https://github.com/mslinn/play-authenticated Jorge Aliss was never properly compensated for the groundbreaking work he did on SecureSocial.
/u/Bowlslaw IMHO the JUnit DSL is also a little too complicated, instead I recommend ScalaTest's RefSpec DSL (JVM only, though): import org.scalatest.refspec.RefSpec class ReplSpec extends RefSpec { object `A REPL` { def `returns input unchanged` = { assertResult("foo")(rep("foo")) } } }
It's not a zero sum game and I'm glad Jetbrains doesn't think it is.
I recommend disabling auto import! The only changes that I need IntellIJ to pick up in is when I add a new library. I'll just refresh manually then.
`pool` is a method accepting an implicit instance of `Monad[F]`. Whenever you call it using parens, Scala assumes you're trying to explicitly pass in an implicit parameter. Your options include a separate variable: val poolVar = pool poolVar(consumer)(cb) passing the evidence as expected: pool(implicitly)(consumer)(cb) or using apply method: pool.apply(consumer)(cb) You might be able to also do (pool)(consumer)(cb) but I cannot test it
Thanks a lot.
Hi all How to constraint a higher kinded type on multiple typeclass? Thanks
Right, thanks, those are good points. Yeah, I'm aware of the risks of using futures with mutable state in actors (such as the sender changing).
All I can say is GOD BLESS `sbt 1.1 RC1` LSP + Visual Studio Code (and Eugene, and the many other people who made it possible) Its the first time I thought of a small new Scala project, tried to get started and didn't give up frustrated 40 mins later waiting for SBT/3 million dependency downloads/IntelliJ/looking for the right `build.sbt` file to copy/encountering some version issue etc etc. Well the download times almost made me give up, but thats just my 50KBps DL speed.
&gt;the power of Scala with the ease of Python You mean like www.ammonite.io? Did you hear that the newest intelliJ supports ammonite scripts?
Monads are like a box a chocolates, you never know what you're gonna get.
Monads are pervasive in Scala because 1) they're all over the standard library (`Option`, `Either` (from 2.12 on), `Future`, `Seq`, ...) and 2) the language has `for` expressions which, while not limited to monads, can make working with monads much simpler.
For some reason, I missed the fact that Spotify is using Scala. What else big players am I missing? I know about Twitter, Verizon and recently IBM. 
Yup, big fan of ammonite REPL. However, the ammonite scripts are still a little problematic because as soon as something grows into a larger project, or you want to use Scala.JS, you are back to setting up a new SBT project.
Scala supports higher kinded types and typeclasses, which allows you parameter over monadic effects.
Aside from compiling to Scala.JS, what problems do you tend to run into with larger projects in Ammonite? I've never actually used it for a large project (only really played around with it)
This is not a problem if you stay within Intellij, or the sbt cli. Doing both causes pain. The sbt-shell in intellij (new) means you shouldn't need to use the sbt cli at all.
I'm interested how you got it to work. I tried it, followed the instructions, but nothing worked in VS Code at all. I am certain I am missing something, not criticising anything.
Everyone uses them, not everyone knows that. :)
`Future` is not a monad because `Try` because `Failure`
Monads are widely used except that there are those people every time nitpicking that they don't follow the theoretical definition precisely
yada yada
True! Memoizing does technically break the identity laws in the presence of side effects, which I was not considering when I listed that. AFAIK they _are_ monads discounting effects, no? Put another way, they'd definitely be monads if they didn't memoize, like the `Task` types in various libraries, right?
Heh - I almost listed that as my 3rd point and opted not to because I felt that, while useful, it probably doesn't contribute to how often they're used. That is, I expect people would still use monads all over the place even without that ability, like they currently do in Rust.
[Here](https://github.com/himzo-tahic/practice-functional-todolist/blob/master/src/main/scala/ToDoListGroup.scala#L18) the idiomatic scala way of doing this would be to use an `Option[TodoListGroup]` rather than to return a nullable result. In fact, except for java interoperability, and extremely performance sensitive pieces of code, nobody should / does ever use nulls in Scala. [This line](https://github.com/himzo-tahic/practice-functional-todolist/blob/master/src/main/scala/ToDoListGroup.scala#L9) will throw exception if the list is empty. Instread of useing a reduce, you can use a foldLeft, returning an `Option` like... toDoListGroup.toDoLists .map(list =&gt; s"${ToDoListUtils.stringify(list)}") .foldLeft(Option.empty[String])((opt,b) =&gt; opt.map(a =&gt; s"$a ${Properties.lineSeparator} $b")) Or if it is okay to return an empty string in the case of an empty List, you can just do toDoListGroup.toDoLists .map(list =&gt; s"${ToDoListUtils.stringify(list)}") .mkString(s" ${Properties.lineSeparator} ") The nulls thing also applies here https://github.com/himzo-tahic/practice-functional-todolist/blob/master/src/main/scala/ToDoList.scala#L25 Other than that, looking good! 
Only future memoizes, and not only is it not a monad simply because the memoization breaks identity laws, but because of its strictness.. val f1 = Future(...) val f2 = Future(...) for { _ &lt;- f1 f &lt;- f2 } yield f is different from for { _ &lt;- Future(...) f &lt;- Future(...) } yield f Simply because of its strict evaluation.
Also, if you don't advocate the use of Akka actors, what type of concurrency model do you recommend?
What is your definition of them out of curiosity?
It depends on if that is considered an observable effect really. That doesn't actually "break the laws". However, the fact that flatMap (and map) take an implicit execution context means you can break the laws. 
While I haven't tried it myself, I believe people regularly use monads in OCaml as well.
After 5 years of Scala, if you're not using Monads, you're doing it wrong. Note: Majority of that time I spent not knowing what a monad was. 
I'm not sure I follow this line of thought - how does the implicit EC imply laws do not hold?
Apart from monadic bind in async code, monads are basically unheard of in OCaml.
First of all - congrats on joining the world of Functional programming :) I am planning to make a pull request where I can put up all my comments. I will then link it here so that other people can pitch in.
In general I find actors good for storing state. If you don't have any state to store actors are overkill (no long running computation that shares information with other computations) when normal mapping/flatMapping of Futures (or IO/Task) is enough. It might be that underneath in some library in your stack (akka-http or spray for example) actors are used because there is the state of the http connection itself to handle but that doesn't mean your application code of receiving some api call, getting data from some data store, doing something with it and returning it requires actors.
Calling those monads may be misleading (I think) Firstly the pattern that a monad as we have come to known it in computer science follows is: val to_bind = monad.return( if Any =&gt; blax if None =&gt; Buz) However what you call Monads may well be used for side effects, even when used correctly (e.g. getting the value from a future can be thought of as calling thread.join(), thus freeing up resources, but even simpler than that we can affect the external state from the function we use to return a value from the "monad") So in that way, what we have in Scala aren't really Monads, I'd think, because they can have side effects. Secondly, and much more importantly, there's currently a thread to call pure options monads but the origin of the term is from [category theory](https://en.wikipedia.org/wiki/Monad_(category_theory)) and the term (to my knowledge) was first used by Haskell since they wanted to have a "mapping" between haskell code and category theory, further reading about this mapping when it comes to haskell monads and category theory monads (can be found here)[https://en.wikibooks.org/wiki/Haskell/Category_theory#Monads] So, wouldn't it be better to just not call them monads ?
Now that dependent function types has been added, would it also be possible to add polymorphic function types with type parameters (i.e. `[T &lt;: Entry] (t: T) =&gt; T#Key`)?
As I understand it this is exactly what will happen in Dotty. Whereas Scala 2 fails the implicit resolution for both `b(a1)` and `b(a2)`, it will still pick `c` since it doesn't conflict with any other C (which I agree is weird).
I think it does, even if only indirectly by making it easier to have a large library ecosystem supporting use of monads. Having access to standard methods like `traverse` or `cataM` makes using monads much more practical.
That's a myth. It obeys the laws relative to the usual definition of equality in Scala when used with the usual safe subset of Scala - the same terms under which we would evaluate any other monad's lawfulness.
I'd come at it from the other end: what's the problem you're trying to solve by using akka actors? (And it it worth giving up type safety for?) For mutable state that needs to be accessed concurrently I'll sometimes use an actor-like style, but I'll implement it "by hand" (with a mutex and blocking queue). For a very few problems I'll use something like fs2. But for the vast majority of problems I see people using akka for, the solution is simply to write that code without akka, using plain old functions.
`ExecutionContext` only breaks the laws if the `ExecutionContext` is ill properly defined. Note that wrt `Future`, `ExecutionContext` is basically an abstract interpreter, which in languages like Haskell cannot be defined or can only be configured globally.
They (and the rest of typelevel programming) are so popular that they scare a lot of Java developer away from Scala (to Kotlin). I've personally experienced that way too many times. Typelevel programming is great but it does not do any good if someone is just getting started in Scala... 
See my response here https://www.reddit.com/r/scala/comments/7hbm0t/are_monads_popular_in_scala/dqqtoiq/ `ExecutionContext` will only break the laws in `Future` if 1. Its not properly defined (i.e. its not executing tasks in the `ExecutionContext`) 2. There was a problem with executing the task in the `ExecutionContext` due to resource constraints/unhandled errors (such as `OutOfMemoryError`) #1 basically never happens, at least if you use all of the default `ThreadPool`/`ForkJoinPool` constructors to get an `ExecutionContext`. This is basically a downside of "being able to define your own interpreter", it leaves the possibility of getting the interpreter wrong. #2 Happens, but this behavior is not specific to Scala or `Future`. You can get in Haskell, in Monix Task, in Scalaz Task, in FS2 Task (or other implementations). Basically when you run out of resources (whether it be threads or CPU or memory) then all soughts of weird stuff can happen. 2. 
In OCaml, people rarely use Monads, this is for a number of reasons 1. OCaml doesn't have higher kinded types (well it does, but they are very painful to use, you have to simulate them using OCaml's module system with functors) 2. Unlike haskell, OCaml is strict and also is not forced to be referentially transparent so you don't need Monad's do IO (Scala is actually similar in this regard, you aren't actually forced to use Monad's) 3. If Monad's are required, they are used. i.e. async uses a monadic bind and also the new multicore OCaml will use a monad to combine async effects. Expanding on this point, people in OCaml generally only use Monad's when *they are forced to* rather then it being the first tool to use for a problem. Expanding on point 3, this is quite nice because I think it has inadvertently shown that Monad's are an abstraction that is heavily overused and overkill for a lot of the reasons people use it for. I think `Task`/`IO`/`Future` types make sense for a Monad, because in these cases you do explicitly need to keep track of and maintain ordering, but people using monads for things like Config (i.e. `ReaderT`/`Kleisli`) is using a sledgehammer for a simple job
Apple are using scala also
Typelevel programming: I do not think it means what you think it means.
There's also this as a T-shirt: https://www.zazzle.co.uk/hindley_milner_type_inference_t_shirt-235812502357339841 Wouldn't be hard to make a poster of it.
&gt; s"${ToDoListUtils.stringify(list)}" This is just `ToDoListUtils.stringify(list)`. I might even write `" " + Properties.lineSeparator + " "` as I'm not sure the interpolator clarifies that case.
Should I add `blocking { }` whenever I block in future ? (interacting with blocking java apis etc...)
I wouldn't have all these methods that just call `copy` with some parameters - IMO those are clearer just left inline. A `while` loop with a mutable `var` at top-level is fine, but not terribly functional if that's what you're aiming for.
Not quite: Dotty and scalac will complain only when *looking* for such ambiguous implicit. The following compiles fine: ```scala object O { implicit val a: Int = 1 implicit val b: Int = 2 } ``` The reason for not doing that systematically is that it can be an expensive operation. For instance: ```scala object O { implicit val a: Int = 1 implicit def b(implicit ev: TC): Int = 2 } ``` This is ambiguous only if there is a `TC` in scope. But looking for a `TC` instance could be super expensive...
Yeah, thanks, I agree. We overuse actors in our services, and I'm trying to put together some documents to demonstrate how we could improve concurrency in our apps.
Yeah, the problem we're generally using actors for is to asynchronously complete tasks, which could more simply be achieved by futures. In places, we're using actors appropriately to manage mutable state. But, there are many usages of actors where we're spinning up short-lived actors and creating futures in the short-lived actors, which creates 2x the number of necessary threads.
That's a possibility. But that one is a bit harder to do because it does not map as cleanly into the existing value type structure. 
&gt; val to_bind = monad.return( if Any =&gt; blax if None =&gt; Buz) What on earth are you talking about? A monad is formally a triple of an endofunctor and two natural transformations ( https://en.wikipedia.org/wiki/Monad_(category_theory) ) obeying certain conditions; informally we say "monad" to mean "monad in the category of Scala types" and abuse notation to refer to the endofunctor (i.e. "type constructor" i.e. 1-parameter parameterized type) as "the monad", since the two natural transformations are usually obvious for a given type constructor. We also tend to formulate the laws in terms of `point` and `flatMap` rather than in terms of `mu` and `eta`, but the two definitions are equivalent if we make an intuitively "obvious" parametricity assumption. None of the definitions of monads I'm aware of looks at all like the code you've written. &gt; So in that way, what we have in Scala aren't really Monads, I'd think, because they can have side effects. Scala isn't really a programming language (i.e. a term reduction system) in the formal sense, because "functions" can have side effects. We tend to implicitly talk about the scalazzi safe subset of Scala, in which functions are functions and monads are monads; when one writes in a style that makes use of monads, one tends to avoid using side effects. &gt; Secondly, and much more importantly, there's currently a thread to call pure options monads but the origin of the term is from category theory) and the term (to my knowledge) was first used by Haskell since they wanted to have a "mapping" between haskell code and category theory, further reading about this mapping when it comes to haskell monads and category theory monads (can be found here)[https://en.wikibooks.org/wiki/Haskell/Category_theory#Monads] &gt; So, wouldn't it be better to just not call them monads ? Did you read your own link? They really are monads: they have a direct 1:1 correspondence to category-theoretical monads (in the particular category one usually works in when programming). It's possible to have multiple equivalent definitions of the same thing.
I think people are too eager to look at the general concepts before they understand the specific cases. `Future` is useful, `Either` is useful, Treelog is really cool. There are some useful library functions that you can use on these things. Eventually you realise that they're the same library functions because they are in a deep sense the same kind of thing. But you have to take small steps from here to there. The trouble is that once you understand these things it's very hard to remember how you thought without them, and explain them in a way that will make sense to people who don't already know them.
Yes, those concepts are _very_ useful for using them, but one does not need to understand the inner workings (e.g. understand the Monad) in order to use them.
 Agreed. So what, if anything, would you propose to do or change?
Yes, particularly if you're writing a library - it's fairly harmless and acts as a hint to the scheduler. Though it's use-case-dependent what qualifies as "blocking". E.g. a database query that's a pkey lookup on a database hosted on the same network segment might well be reliable enough that switching threads for it does more harm than good. That said, if you find yourself making extensive use of it then think hard about why you're using futures in the first place. If your code does a lot of blocking within futures and it isn't a problem, why would not using futures at all be a problem?
I.e. When I cant block on current dispatcher because its akka / play / ... or i just want to have resources under control ( i e limited pool of threads for blocking to Twilio) Sorry im on phone. Thanks for answe
We need more articles/blog posts/presentations showing the simple/idiomatic parts of Scala. We should support the notion of "Scala as a better Java" (I don't understand the opposite). We should promote the readability of idiomatic Scala code compared to Java... It's obscenely difficult for me to promote Scala in my country because everyone thinks it's too difficult to learn, even though it isn't. 
Have there been any clarifications on the vision about when dotty will become the next scala? Last I heard it was a very nebulous "sometime in the future", but I can't wait that long! 
&gt; We shouldn't criticize built-in stuff like Future and such, we should not criticize the Play Framework ‚Ä¶ Strongly disagree. Every design should be considered critically. This is how things improve.
My point is that the built-in Scala Future (and for comprehension) is way better than in Java. And much easier to learn/read/use... 
Monad is a useful idea because it defines *very precisely* what it means to compose functions that compute effectful values. This ends up being useful because it lets us recover a lot of the expressiveness we seem to give up when we start doing FP. I gave a [talk](https://www.youtube.com/watch?v=po3wmq4S15A) on this a few weeks ago. You can do monadic computations with concrete data types in any language; but to abstract over them and have any chance of getting everything to line up correctly you need higher-kinded types, and as practical matter you need some kind of machinery for passing arguments implicitly. Very few languages let you do this.
Taking a thread off the pool is an *observable* effect. This breaks the functor laws actually. &gt;https://github.com/scalaz/scalaz/blob/series/7.3.x/core/src/main/scala/scalaz/Functor.scala#L106 or, slightly more readable: fa.map(f).map(g) == fa.map(f andThen g)
Don't listen to mdedetrich here he's wrong on this one. ;-) https://www.reddit.com/r/scala/comments/7hbm0t/are_monads_popular_in_scala/dqrf0uk/ 
If I've read all these correctly, your argument is that `Future` is not a monad because it inherently has observable side effects due to its interactions with the execution context? For instance, calling `map` twice in a row and potentially taking different threads is observably different than composing the functions and calling `map` once, and it is precisely this difference in composition that disqualifies `Future` as a monad?
Exactly. It is only an issue because map itself takes an EC. Scalaz's Task doesn't work like this. 
Hi m50d and thanks for your comment. How could the while loop at top level be replaced to make it more functional? Regards
Hi josh and thanks for your input, it is very much appreciated. Your tips where very helpful, I will try to implement them in the next iteration. Regards
Hi zaxme. That sounds great! I'd be very happy to take a look at said PR when its finished. It's very nice of you to offer this, your effort is greatly appreciated! Have a nice day!
That's fair and is probably true for the wider Scala ecosystem. Although as a data point, I have never once used either of those methods and most of the people I work with would probably push back if I tried. They're not quite "there yet" when it comes to that kind of abstraction. I mean, I get responses like "that's too much syntax" and "I have no idea how this works" when I use almost any kind of typeclass, let alone combining that with HKT and bringing in (more) terms from category theory. That doesn't stop us from using the core monadic operations like `flatMap`.
According to [this](https://adriaanm.github.io/reveal.js/scala-2.12-2017.html#/), dotty will be scala 3 and it will come after scala 2.15.
That make sense; thanks for the insight! I'm not sure it'll stop me from generally thinking of `Future` in a monadic way even if it technically isn't a monad. More often than not I only care that I can sequence them with `flatMap` and transform them with `map`. Usually I don't care if, e.g, it takes a different thread because even if I could observe it, I don't, except in rare cases where I'm low on resources. I guess to me,`Future.map(f).map(g)` behaves _enough_ like `Future.map(f andThen g)` that treating it as a monad is still a useful mental model. Of course knowing it's not completely monadic is also useful for understanding the actual runtime properties of the system.
If anybody wants some help on a project let me know. I work full time in java, and just studied scala at home, but I would like to get more hands on experience with the language + libraries. 
Java has four monads (well, one if you care about them being lawful) in its standard library as of Java 9. Any Java developer scared of them isn't going to be a Java developer for long.
The big thing the fact that you're using a global-ish `var` to keep the current state. The first thing I'd do is replace reassignments to the `var` with local `val`s, so that the only assignment to the `var` is the one coming out of the `match` block. Then you could maybe replace the `while` with an `@tailrec` function with `toDoListGroup` as a parameter, so that it can be immutable. A state monad is an option but honestly I think it's probably overkill here. Looking at the code again I'd definitely factor out the common "find to-do list by name, print an error if not found, otherwise do something" logic. Maybe move all of that logic into the `findToDoListByTitle` function. I might also move that function into being a method on the class - I see `*Utils` classes as a bit of a red flag, it's usually better to put functions on the object they go with. 
You seem to define all of your methods for a type on the companion object, which works fine. However, you can define those functions on the class itself as well to make syntax slightly better. For example findTaskByTitle can go from: object ToDoList { def findTaskByTitle(taskTitle: String, toDoList: ToDoList) : Task = toDoList.tasks.find(task =&gt; task.title == taskTitle).orNull } to: case class ToDoList(...) { def findTaskByTitle(taskTitle: String) : Task = tasks.find(task =&gt; task.title == taskTitle).orNull } Then your calling convention just becomes `toDoList.findTaskByTitle(title)`. I would also hesitate using `orNull` in any of my programs. It reduces your confidence that your program is actually going to complete without throwing a runtime exception.
&gt; the spec says the implicit search would use the more specific value; here the "static overloading resolution" is applied that should not be the case when we have an *exact* match. this is puzzler material.
Those are on types. All types are bounded -- as an example, for a normal class Foo, the type is bounded between Any and Nothing. So T &gt;: U means type T is a supertype of type U" or "type T has type U as lower bound". T &lt;: U means "type T is a subtype of type U" or "type T has type U as upper bound". * http://naildrivin5.com/scalatour/wiki_pages/TypeBounds/ * https://docs.scala-lang.org/tour/upper-type-bounds.html * https://docs.scala-lang.org/tour/lower-type-bounds.html
Huh. It's an inference trick but I confess I have never seen it before. For `open` the compiler must infer a type `T` no more specific than `S` but no more general than `Closed`, which is only possible if `T =:= S =:= Closed`. If the example when you call `close` there is no such type and inference fails. I would write it like this, which I think is much easier to reason about. ``` case class Door[S]() { def open(implicit ev: S =:= Closed) = Door[Open]() def close(implicit ev: S =:= Open) = Door[Closed]() } val myDoor = Door[Closed] myDoor.open.open // fails, can't open twice ``` 
Actually I'd say it's not just an inference trick, but rather a fundamental property of the DOT type system ‚Äì and in fact the very thing that made it so hard to formalize (i.e., the ability to use bounded abstract types to "define your own subtyping system"). The version using implicits is more of a trick IMHO, though it might be easier to understand.
My understanding of `implicit ev` is that ev should exist in the context that you call `open` or `close`. Where is that coming from? What would the signature look like if you were to pass another parameter to open or close, maybe like open(percent: Int) to leave the door ajar.
Thanks. I'll check out those links.
But moving it into the class would make it more OO, since it would be a class method. I'm not trying to implement it in the best scala way, I'm trying to implement it in the most FP way possible. I merely choose Scala as a vehicle for this (for various other reasons). Does your tip still apply? Regards
`=:=` instances are forged out of nothing; see [Predef.scala](https://github.com/scala/scala/blob/v2.12.4/src/library/scala/Predef.scala#L510-L521). Normal parameter lists can precede the implicit one, which must be last: `def open(foo: Int, bar: Blah)(baz: Qux)(implicit ev: S =:= Closed)`.
Very cool. Thanks.
&gt; We need more articles/blog posts/presentations showing the simple/idiomatic parts of Scala. We should support the notion of "Scala as a better Java" (I don't understand the opposite). We should promote the readability of idiomatic Scala code compared to Java. In principle I'm for this, but remember that the community is mostly volunteers, and mostly people who care a lot about code quality. So "dumbing down" code is never going to be appealing to them. I've been doing Scala for 7+ years now, and even at this stage I'd say that some of the code I wrote 2 years ago seems embarrassingly bad (I love that Scala is a language where I'm still learning even after this much time). So even though the code I wrote then was a necessary step on my journey, it's hard to put myself back in that mindset and help others write that style of code, when I now feel like I've moved beyond that style. &gt; We shouldn't criticize built-in stuff like Future and such, we should not criticize the Play Framework and by default point people to less documented frameworks. We should promote Lagom, which is unique in it's space by many criterias and so on.... Up to a point. At the same time, again people care about code quality and want to improve things. And in terms of Play specifically I actually felt it was a step backwards compared to what I had been using in Java (Wicket), so I wouldn't want that to be the face of Scala.
&gt; But moving it into the class would make it more OO, since it would be a class method. To my mind the defining characteristic of OO is having mutable state inside the object. Having functions on a value in Scala is not OO as long as they don't mutate that value, it's just general code organization.
Woah never heard of treelog before! Very cool! https://github.com/lancewalton/treelog
What do you mean by an "exact match"?. In any case, the example is a corner case which in actual code would be very rare (putting several implicits in scope which only differ in degree of inheritance).
That's called covariance and invariants if I'm not mistaking. Basically, the Child &lt;: Parent. 
So in your opinion every pattern matching should generate result wrapped in Try? Please, try and write down such a code. Also, you get a warning for incomplete match. 
I agree it's kind of meh, but the compiler simply can't catch all cases of non exhaustive matches. The only way to avoid this would be to restrict pattern matching to sealed abstract classes / traits.
Could you give an example usage of a `for` comprhension used with something that is not a monad?
There's nothing stopping you wrapping all your pattern matches in Try, but also no real need to inflict it on the rest of us! Scala's design choice was to allow pattern matching to be very general, rather than restricted to sealed types. When types are unsealed the compiler can't do exhaustive checking because it can't be sure that there aren't other matching types available at run time. It's more powerful but less safe. I don't like the use of exceptions much but I wouldn't want to have to use Try when most of the time I'm using a sealed type.
Well, that's what most non-dependently-typed functional programming languages do (OCaml, Haskell, etc.) so I don't see a contradiction.
&gt; Taking a thread off the pool is an observable effect. This breaks the functor laws actually. In what sense?
The performance impact of wrapping every pattern match into a Try and checking it would be massive. Also the codebase would be annoying to use. Then you would probably need some different construct that would not wrap the result into a Try for sealed traits as there the compiler can make sure during compile time that all possible cases were handled.
Nice. Btw you can use native lambdas to construct DSL lambda terms (x: Term) =&gt; something by computing them passing some fresh literal in place of `x` (building this fresh literal might be tricky though).
&gt; Scala isn't really a programming language (i.e. a term reduction system) in the formal sense, because "functions" can have side effects Did you want to say a *functional* programming language?
Anything you define `withFilter` on. It can do anything you like.
Yes, but the type of people that often say stuff like `Future` sucks are doing it for either 1. Ideological reasons (i.e. it doesn't follow their ideology well enough, i.e. its not pure FP) 2. Do it in an intellectually dishonest manner (i.e. they refuse to acknowledge that `Future` is designed to solve certain set of problems and because of this design its not good at other things). i.e. `Future` is slower then other competing tasks not because someone was bored/lazy and wrote the code badly, but because `Future` is completely optimized for fairness (at the cost of throughput). Point 1 is annoying because its obvious and kind of missing the point, `Future` is not designed to be forced RT so of course its not going to follow it. This is like going into Ruby and complain in the community that its not a static language. You may be right, but its not being helpful and the language is designed to be dynamic in the first place. Point 2 is what is really painful to see, and it happens more often then I like. I would honestly like to see actual proper constructive criticism that actual discusses meaningful things (like design goals between the different Async/IO/Task types), and as of late I have seen zero of this, just people throwing around dogma or deliberately being misleading because of other reasons.
Observable effect is something that is subjectively defined. Maybe you should read m50d's response here https://www.reddit.com/r/scala/comments/7hbm0t/are_monads_popular_in_scala/dqr3czd/
Mind you I stated this myself here https://www.reddit.com/r/scala/comments/7hbm0t/are_monads_popular_in_scala/ runT1ME saying I am wrong here completely missed the point of what I am saying. If you specify a custom EC in between computations, then yes this isn't completely lawful but this is the cost of being able to define a custom interpreter (aka `ExecutionContext`) at any stage in the computation.
No... Variance is the ability to establish a relationship between two parameterizations of a type. (And therefore only exist on a type, not a method.) Bounds are a constraint on the type parameter to begin with. 
Note that the fork of the compiler from the Typelevel organisation has better exhaustivity checks : https://github.com/typelevel/scala/blob/typelevel-readme/notes/typelevel-4.md#exhaustivity-of-extractors-guards-and-unsealed-traits-pull5617-sellout
If you have fatal warnings enabled, then match errors shouldnt be possible unless the compiler is failing exaustivity checks. You are correct though. It is not functional to throw exceptions. Also, Try would be a bad substitute here, because the failure type is known. If we were to allow match errors and remain functional, you'd return an Either with MatchError on the Left, and the result on the Right. However, like I said before, MatchError just shouldn't ever happen. In a better language, they would be impossible. 
Some context: I'm working on a feature in scalafix to only apply lint message on the recently modified code. We run git diff and we extract all the line that was modified. This gives us a set of intervals, eg between line 1 and 10, 14 and 20. see https://github.com/MasseGuillaume/scalafix/blob/drop-in-jgit/scalafix-cli/src/main/scala/scalafix/internal/jgit/DiffDisable.scala
can we please put this into the std lib?
I wrote this comparison originally as internal R&amp;D for work. We have a number of several-year-old Scala projects that we'd love to apply more FP too, but needed to make an informed decision about the benefits and trade-offs of Scalaz and Cats. I intend to maintain this as long as necessary, for instance by updating the benchmark results as new releases appear.
Why not sort &amp; merge the intervals?
*spoiler* here is my solution: https://gist.github.com/MasseGuillaume/b245fb9d049e04d115a6a7802fc39b34
I'm not sure what you mean. I use the BitSet intersection: https://github.com/MasseGuillaume/scalafix/blob/drop-in-jgit/scalafix-core/shared/src/main/scala/scalafix/internal/util/IntervalSet.scala#L9
How about this? It still creates extra collections, but the collections are smaller. If this is truly performance critical the e.g. the array could be preallocated, but I didn't have the time right now. I didn't test this real well, but seems to produce identical results to the original based spot checking. def asBitMask(start: Int, end: Int): List[Long] = { import Math._ if (end &lt; 0) Nil else { val hi = if (end &lt; 63) -1L &gt;&gt;&gt; 63 - end else -1L val low = if (start &gt; 63) 0L else -1L &lt;&lt; max(start, 0) (low &amp; hi) :: asBitMask(start - 64, end - 64) } } def fromRange2(xs: (Int, Int)*): BitSet = { val range = BitSet.newBuilder xs.foreach { case (start, end) =&gt; range ++= BitSet.fromBitMaskNoCopy(asBitMask(start, end).toArray) } range.result }
Good writeup. A few more data points: - fs2 depends on cats now. - http4s wasn't mentioned (it depends on cats as well). - a recursion schemes lib for cats is in the works. 
Yeah it's a bummer. If you use [Typelevel Scala 4](https://github.com/typelevel/scala/blob/typelevel-readme/notes/typelevel-4.md) with `-Xstrict-patmat-analysis` and `-Xlint:strict-unsealed-patmat` along with `-Xfatal-warnings` you'll get much better exhaustiveness checking.
Ah, okay; I think I misunderstood what you wrote earlier myself. This clarifies it a bit, thanks.
A monad has a rather precise definition, though it may be formulated in a few equivalent ways. One way is that a monad must have an define two methods which obey three laws. One method (`bind`, `flatMap`, ...) accepts a function returning a value of the _same monad context_ but with a different "inner" type and returns a value with the new type, which is how you can go from `Option[A]` to `Option[B]` via `flatMap`. However Scala's for expressions work on methods named `map`, `flatMap`, `foreach`, and `withFilter` _regardless of whether they're monadic_. So if you created a `flatMap` that goes from `Option[A] =&gt; Future[A]` or `Option[A] =&gt; MyType[B]`, you could use that method in a for expression even though it doesn't conform to the structure required of a monad, let alone its laws. In fact, this is used in the standard library! Simplifying a little, `Iterator.flatMap` is defined as trait Iterator[A] { def flatMap[B](f: A =&gt; GenTraversableOnce[B]): Iterator[B] = ??? } Notice that `GenTraversableOnce` is _not_ an `Iterator` so `Iterator` cannot be considered a monad, but it can be used in for expressions. Somewhat amusingly, there is a `FilterMonadic` trait which defines a non-monad-conforming `flatMap` as trait FilterMonadic[+a, +Repr] { def flatMap[B, That](f: A =&gt; GenTraversableOnce[B])(implicit bf: CanBuildFrom[Repr, B, That]): That } Not only does this accept a function returning a different type, it also returns a completely separate type entirely! Yet this is fully usable within a for comprehension and allows you to combine, e.g. a `List[(A, B)]` with a `Map[A, B]` into a `Vector[(A, B)]`.
The only thing I am missing in cats is a DAG library, same in style to the old scalaz tree. On the other hand, cats has a lot of stuff which doesn't really exist in Scalaz, such as a http library (http4s) and database library (doobie). Hope we can get to a point where we don't have to depend on both libraries in the project.
Updated, thanks for those.
Not explicitely depending on both is a goal for us as well.
Might also be worth to note that Monocle was originally designed to be based off Scalaz 7, but its also being ported to Cats for v1.0.0. Scalaz 8 will have its own optics library.
This is awesome! I think it can help users make an educated decision when choosing between the two. One thing that might be worth mentioning is that Cats emphasizes on building a warm and welcoming community. And a good metric for that would be the number of contributors. Cats, being a younger project, has over 183 contributors now. Another thing that might also be worth mentioning is that Cats will start to guarantee binary compat post 1.0, that will make life a lot easier for projects having diamond dependencies against such foundational libraries. 
I'm going to update how Monocle is shown there once it's out of its RC process. 
Also if you are into the Free/Tagless Final world, freestyle and mainecoon, both depends on Cats, might be worth mentioning. Disclaimer: I developed mainecoon. 
Oh I also forgot, there is also Eff which is part of Cats https://github.com/atnos-org/eff
Reminder that you can use either one with libraries that require the other. You just need one import from https://github.com/shawjef3/Harmony
There is https://github.com/markhibberd/pirate if you want to write CLI parsers using Scalaz.
I've been advertising [shims](https://github.com/djspiewak/shims).
That seems pretty old, I'm not sure if I should advertise it.
Looks likes it has [backends for both](https://github.com/atnos-org/eff/blob/master/build.sbt#L11-L13).
"Scala isn't really a formal language" would have been a less tendentious way of putting it. Of course we all understand the term "programming language", but imperative languages aren't actually languages in the usual sense of the term - or at least, to analyse them purely as languages is to ignore important aspects of them. 
Shrug. We have a forked copy and maintain it internally. We heavily use it, and it's fantastic. It's much closer to optparse applicative than the one you linked (grantee, not use that one)
Would you consider rereleasing your internal fork under a different name?
I'd have to check the licensing of the original one. I got verbal permission to use it internally, but not sure about externally
Wrong. OCAML has [option, bind, lazy eithers for error handling, etc...](https://realworldocaml.org/v1/en/html/error-handling.html) Just because the community believes in not talking about category theory doesn't keep mmonads out of OCaml. You really cannot program effectively without them in functional languages, whether you call them Monads, IO, Lync, Stream. They come up any time you use immutable data structures and need to operate on members of said data structures. They're intrinsic properties of the data structures and their operations. 
Why not use an [Interval Tree](https://en.wikipedia.org/wiki/Interval_tree)?
**Interval tree** In computer science, an interval tree is a tree data structure to hold intervals. Specifically, it allows one to efficiently find all intervals that overlap with any given interval or point. It is often used for windowing queries, for instance, to find all roads on a computerized map inside a rectangular viewport, or to find all visible elements inside a three-dimensional scene. A similar data structure is the segment tree. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
That's what I am trying to say, it's a concept for library programmers, at least until fp is main stream.. 
I've noticed this before: I think the disconnect here is because you insist on defining a relative phenomena in absolute terms. Something is "observable" relative to the program *you* define. Otherwise, the absolute line you've drawn is arbitrary. Does the line you draw condemn depletion of the thread pool but not depletion of memory? A simple call to "java.lang.Runtime.getRuntime.freeMemory" can observe the later. The passage of time and the heating up of my CPU can be similarly observed. In many domains, this doesn't matter. In some domains, it does; hence it is observable.
This is what makes Scala awesome. Both of these encode business logic at compile time what would normally be runtime asserts with an intuitive notation. I now have two concrete examples. Thanks guys!
In the code above, Wolf does not extend Dog. They are unrelated. 
Good idea, an Interval Tree would be better. Bitset intersects is O(n) worst case where Interval Tree is O(n log n ).
Yup, that works for me! Congrats.
First of all: nice summary! :) I have just small request to OP - I needed to look into benchmark source code to understand what those numbers are and if bigger is better or smaller is better. I kind of understood that they represents average time of execution in ns, correct? Could you add some description to the table? :)
Not sure if its for the backend or just effects for the `Task` types, will probably have to ask the author or look at the code
I assume you mean that the problem is that the `playWith` in `Dog` and `Wolf` has different signatures. You can partly solve that with F-bounded polymorphism or by using typeclasses. [This blog post](https://tpolecat.github.io/2015/04/29/f-bounds.html) contains a good discussion of the possible solutions. Also you should really reconsider your inheritance hierarchy, a wolf is really not a dog (but they are both canine) and a pony definitely isn't a dog :) In general avoid inheritance unless you are sure the is-a relationship will always hold, even considering future additions. For polymorphism you can use typeclasses instead or just function parameters.
It's the same situation as e.g. FS2, where the project itself depends on cats, but there's also a separate scalaz module :)
Nothing new under the sun there. I think there's about zero correlation between a language "taking off" and its technical merits.
If the intervals aren't sorted, sort them by the start of the interval. Reduce the intervals by looking at them in sequence. If current interval starts where the previous left off (end of previous + 1 == start of current), extend the previous to reach the end of current; drop current. This has better performance than Bitsets, but you lose the O(1) 
Those type of questions are silly, but good to see that there is a [GitHit 2.0](https://madnight.github.io/githut/), where Scala is in good position at #12 with still upward trend.
It's sad in a way, because I could imagine an actually interesting discussion about if Scala will still "take off" and all the things around it (what does it even mean, what has been achieved, what could have gone (or could still go) better, what should we learn from other languages/communities etc.). But instead, it's just another re-iteration of the same old.. following the same pattern laid out by https://www.linkedin.com/pulse/scala-way-out-owen-rubel/ almost 2 yrs ago: * find some statistic that Scala use is declining * link to Yammer article from 2011 and to isolated statements from LinkedIn and Twitter * claim that Scala is just too complicated for the average programmer * optional: the real solution to everyone's problems is Groovy (or Go or Kotlin, depending on when the post was written). No serious argument, no accounts from personal experience, no statistics seem to be able to debunk those beliefs. If you engage in a discussion around this, I admire you for your stamina. Personally I'm just so tired of this. 
The Twitter quote in the top post is stupid. I spoke to Twitter engineers at ScalaDays 2017 and their usage of Scala is increasing, not lowering.
I feel really sad for the people who spout hot opinions on quora
Maybe you were referring to this: [How I Avoid Two-Phase Commit](http://blog.jonathanoliver.com/how-i-avoid-two-phase-commit/)
Great project https://github.com/fomkin/korolev searching for some people, I think they will be great to you help.
I've open-sourced a web framework we've been using at work (documentation, such as is is is here https://github.com/springernature/samatra-extras/wiki). It looks a lot like Scalatra but with a few small differences: 1. you have to return an expression from the route, no contextType = 'text/plain' in the middle of your definition 2. no funky thread local state 3. matching routes on query params is not supported 4. support for testing controllers, and apps without running a server It's a pretty small routing lib over the servlet api, so gzip, http2, jmx monitoring all come for free with jetty. I don't know if anyone else would find it useful. I've also open-sourced a drop in replacement for scala xml (https://github.com/springernature/vtdxml4s). It's made quite a difference to the performance of some of our apps (particularly in reducing gc).
Option isn't a monad. There is a Maybe monad in Haskell but no common equivalent in OCaml. Bind is used in async, as I said, but I've never seen it elsewhere in OCaml. Laziness is very rare in production OCaml code. I cannot remember using Either sum types in production code but I do recall other people using them to write daemons. However, I'm not sure an Either sum type necessitates monadic code in and of itself. &gt; You really cannot program effectively without them in functional languages, I think you're confusing functional with pure. OCaml makes heavy use of functions, of course, but purity is not so common. In point of fact, the pedagogical purely functional data structures (e.g. from Okasaki's seminal monograph) are practically unheard of in production OCaml code. &gt; whether you call them Monads, IO, Lync, Stream. IO isn't monadic in OCaml because OCaml isn't pure. Explicit monads are rare. Perhaps you mean LINQ which is a .NET thing? Stream in OCaml is inherently completely impure so definitely not monadic. That being said, a lot of people seem to see monads everywhere. 
Tpolecat's blog linked above has the only true answer. Just use a typeclass for the related ops, don't use inheritance. Read the 'How about only a typeclass?' section. 
Duh. Sorry. Changed.
Well, it causes some additional complications when used in a collection as is also mentioned in the blog post.
Do you have more information regarding recusion schemes with cats? Is there any project I missed available on github?
In which case FS2 should be made gray on my diagram as well.
That's right. Check out my joint presentation with Stu Hood that talks exactly about this: https://www.youtube.com/watch?v=4yqDFsdKciA.
Whether it takes off or not, learning Scala has improved the code I write for work in Java and JavaScript. Thank you, Scala!
That quora answer is so full of bullshit. There was a discussion that I initiated in this sub previously and later I started to work with Kotlin. Honestly I don't understand what the fuss is all about. It is just simple syntactic sugar over Java. They even ran a reddit AMA and didn't have a proper answer to "What is going to happen if Java itself gave some of the features you have done?" - https://www.reddit.com/r/Kotlin/comments/7hoytl/kotlin_team_ama_ask_us_anything/dqtd07k/ From that comment &gt; We finally have it. Kotlin is Java 2.0. The future of Java is Kotlin. Seriously WTF? I can't take these trolls anymore, they are all over the god damn place. Reddit, Hackernews, Quora. Seriously, I think the Scala core team should write some rebuttal so that people can link to that. 
One thing that is probably worth mentioning is that Scalaz emphasizes building an inclusive and professional community. This is evident in how the Scalaz community behaves. Scalaz doesn't restrict participation to any particular group. Scalaz welcomes contributions from anyone (even me!), and judges them only on their merits, not on the political alignment or ideological purity of their authors. Scalaz doesn't block anyone on Github, and personally I don't block anyone anywhere. Scalaz contributors don't contact employers to get people they don't like fired. In my opinion, this type of behavior is the essence of true inclusion and professionalism, and it's a welcome change of pace for some people like me who were angrily shunned and told to go away from other projects. Another thing worth mentioning is that Scalaz has a unique focus on a culture of technical excellence. While ad hominem of any kind is unprofessional and not part of the Scalaz ethos, critiques of code, architecture, and abstractions are strongly encouraged, and no effort is made to tone police or to mandate flowery language. While it can take a while to learn that _we are not our code_, ultimately a culture that focuses on technical excellence leads to real innovation and uncompromising, principled designs that will not emerge in an environment obsessed with "getting along" and "protecting" people from disagreement. Anyway, thanks for putting this helpful resource together, and I look forward to the update of your comparison post-Scalaz 8!
Just did something I thought was cool and would like to show off :) (TLDR; at the end ) I'm working on a project where I need code generation of a lot of files. So I was thinking first to just make a dedicated code generation program or use quasiquotes. Tried that for a bit but then it occurred to me that if I have a couple of different generation "templates" which generate a lot of files based on many small configuration fragment, I'd have to program an ugly (relatively speaking) dedicated program for each template. And also, I can't see any advantage to doing any compilation during the code generation phase when instead I can just do the generation, and then let the project compilation catch any problems. So then it occurred to me that what could really be great is a template engine (like the HTML ones) where I can write a template and then just apply it to a lot of configuration files (or a big configuration file with a lot of small configuration fragments). Enter scalate! I've already used it in the past for HTML templates, so I thought I might give it a try here. Long story short, here's some code: Generation task: val res = getClass.getResource("template.ssp").getFile val output = new TemplateEngine().layout(res, Map("A" -&gt; "AV", "B" -&gt; List("BV1", "BV2"))) Files.write(Paths.get(outd, "out.scala"), output.getBytes) template.ssp: &lt;%@ var A : String %&gt; &lt;%@ var B : List[String] %&gt; object ${A} { def x = "${B.mkString("")}" } output (out.ssp): object AV { def x = "BV1BV2" } In the end I'm happy with the result since it's very little work and the (code generation) code looks very elegant IMO (haven't actually implemented the generation I need though, so I'm sure I'll have many complaints later). Next I'll probably use typesafe config for easy parsing of the configuration files, and hopefully that'll be exactly what I need. **TLDR; used a template engine (scalate) to do very readable code generation with very little effort (that isn't related to actual code generation logic).**
Everyone uses the standard library, not just library developers. That's why it's the standard library. ;)
Hey John, thanks. I've been prodded elsewhere about not having mentioned community and governance styles between the two. This is on purpose. I'll echo my opinion here: &gt; In my personal experience, contributors to both libraries have been friendly, intelligent, and open to discussion. I have a deep disinterest in politics. We should choose libraries based on their technical merits and project health, which was my guiding philosophy in writing the comparison. My team agrees with me - in an internal questionaire regarding the two libraries, I have the question: &gt; Should the political past of Scalaz and Cats affect our decision? The aggregate answers to which are a resounding "No." Scala needs help. Both Scalaz and Cats are lighthouses through the fog, they help lift suffering developers up from the pain of ad hoc spaghetti. I'm grateful to both of them for that. It's a bit silly that we have to have two FP libraries (I blame Scala itself for this), but hey, competition is good for the ecosystem. Godspeed, all of you.
What does "take off" mean? And is it a metric worth caring about?
&gt; Scala needs help. Both Scalaz and Cats are lighthouses through the fog, they help lift suffering developers up from the pain of ad hoc spaghetti. I'm grateful to both of them for that. It's a bit silly that we have to have two FP libraries (I blame Scala itself for this), but hey, competition is good for the ecosystem. Godspeed, all of you. Amen to that!
OMG, over and out.
Even worse when one of them is the VP of engineering at Quora
Funny the answer claimed that scala is going down *despite* of Spark, while in the chart he gave, Scala's PR percent raised from 1.4% to 2.0% since Spark is released. That's a 40% growth in 3 years. 
To explain why I posted this here, it was more to make people more qualified than me post a better and sane answer there :)
Yeah, we get it. This sub is so pissed off and got people on the edge with the recent trolls from Kotlin. If they spend atleast half on these trolls, they can design better and incorporate more features.
Very cool :-)