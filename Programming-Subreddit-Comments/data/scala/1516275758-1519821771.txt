What exactly do you mean? 75k+ for Senior dev or 60k for scala junior? 
FYI the build is green now. The PR is about to be merged. https://github.com/http4s/rho/pull/193
Great to hear it, thanks for letting me know
Thanks for the explanation and the link. Makes sense and I like the reasoning, I will probably use this convention in the future as well.
I think some people just like different things. I think intellij is the best ide for Scala but I don't love it as a text editor. I'm simply more efficient with other editors because I'm more used to them. Probably a few hours using intellij exclusively would help, but I like what I got.
60k for a 1year experienced Scala Software Engineer. Obviously, there are occasions depending on the role/person/need etc...but overall, not so much!
I live in Berlin For a **senior** engineer (I really don't know what senior means these days), it is on the lowish end but I wouldn't call it underpaid. Compared to the rest of the salaries you can get in Berlin, its actually quite high. Its actually quite hard to find senior software engineer salaries above 80k in Berlin (in general). Berlin used to be one of the cheapest cities in Europe (for a developed country), and its still right now on the cheap end. 60-75k in Berlin is equal to like 120k+ pounds in London or even 150k+ in San Fran. Rent and food here is really cheap. Also I don't know how accurate Payscale is, but you can look for yourself https://www.payscale.com/research/DE/Location=Berlin-Berlin/Salary
I went to it for my school projects because it runs on the JVM (cross platform including Pi, decently fast), supports a lot of the cool syntax and reduced punctuation of Python, and has the pattern matching I loved from OCAML. ed: Also has some fun concurrency models (akka, futures) which is something of a fail for Python.
The best way to get help is to get on the Lift google group: that’s the project’s preferred way to talk. See: https://liftweb.net/community
/u/irregular_regular have you tried the [readme](https://ssl.icu-project.org/repos/icu/tags/release-60-2/icu4j/readme.html#download), or [this](https://maven-repository.com/artifact/com.ibm.icu/icu4j/60.2)?
It's webscale
1. Because you're tired of Java. 2. Because you like to think functionally. (Immutability, lack of side effects, higher order functions, pattern matching, ..) 3. Because learning something new doesn't hurt 4. Because you might learn something that you can apply even somewhere else. 5. Because this language correlates with high salaries.
Because it mostly works with all that java code the Corp already has. Because it makes you think THEN code. Because it’s the easiest procedural Java to functional path you can take. That’s not five, but I’m on mobile so that’s all you get. 
1. Scala pays well 2. It runs on the JVM but the syntax is more "dense" than Java. Less code to get the same stuff done. 3. You like to take breaks because your code is "compiling" (https://xkcd.com/303/); I don't think there is anything in the tech world today slower than SBT 4. Scala pays well 5. Scala pays well
It's not Kotlin
It's a perfectly fine language to do so in. Lots of people have published image processing libraries for Scala, many of them purely functional. Go for it, and tell us how you do!
Ie C++ :)
We do something similar except we load overrides from s3. The idea being a config is alwaus a Relodable[T] and the current config is a merging of the application conf and remote Hocon. Then you just use hocon fallbacks to merge the configs together in priority order and behind the scenes periodically poll an s3 bucket. Secrets are managed by public private key decryption. Services are loaded with a public key and secrets encrypted with the private. Since remote configs are always a nice to have and should be able to fall back to the default, if you cycle the secret keys you just need to redeploy with the update key you need 
Sure, why not? You might look at [PureImage](https://github.com/stephenjudkins/pureimage) for inspiration.
In the Scala interprester, is there a way to find the package of a standard Scala class starting from the name of the class?
something like jtidy? http://infohound.net/tidy/ it's implemented in java
I don't think there's much value, because Scala libraries are already pretty clear about what their dependencies are and it's relatively easy to avoid depending on incompatible versions. Python mainly does this because a) python package management is pretty weak and b) a lot of scientific python relies on native libraries, which python package management is very bad at handling. All that said, the scala platform might become this.
" I don't think there is anything in the tech world today slower than SBT" F# on visual studio is much worse!
 1. ML features 1. Higher-kinded types 1. Safe generic object graph traversal (shapeless-style typeclass derivation) 1. Strict evaluation 1. A decent library/tool ecosystem Plenty of languages have most of these, but I think Scala's the only one that combines them all.
How to reply the Haskell fanboys when "muh typesystem"?
May be a stupid question, but if you’re using Scala, you’re programming in a functional paradigm, right? Even if you’re using it w/Play framework, that’s all written with a functional approach?
How much will Dotty improve Scala in this direction? 
Not much. I mean it gives us a formal semantics for the type system which is nice but not that important when everything happens at compile time. The main advantage of dotty is just cleaning up a decade's worth of random cruft that the language has accumulated. 
Thank you very much for this information!
Care to elaborate?
This. If there's no code written yet, pick just about anything but lift
Thanks! This was a good read. I'll update my post when I get a few spare moments. :-)
To elaborate further: 1. It's not Ruby 2. It's not Go 3. It's not TypeScript 4. It's not C# 5. It's not Perl
1. Modern best practices with less lines of code: immutable classes, final vals and fields, nice closure syntax, local functions, by-name method parameters (bye-bye builders!), concise testing stubs and more 2. Scala collections. Language advantages makes them much more powerful than Java/Guava counterparts. 3. Scala technology. Java really lags behind in asynchronous programming. Scala has better high-performance async networking libraries. ScalaJS is also very promising for enterprise applications. 4. experimental language: compile-time macros, complex type system, implicits. While it hurt newbies if overused, it can be used to solve complex software problems. 5. FP language. While I'm yet to see production-quality application on pure functions, it certainly has its uses.
What can Scala do that Haskell cannot? 
Not being forced to use S-expressions all the time?
Static types
Interactieve with the jvm. Seemlessly integrated Java libraries. Quite important from a business perspective!
&gt; I have programmed a lot in haskell and never encountered a space leak. Yeah, nor have I. But I have to ask myself what would happen if a project I was using Haskell for developed one - I know they can happen and it seems like it takes a lot of expertise to fix them. &gt; Anyway it has more to do with lazyness than with the typesystem. Completely agreed, but it's part of why I use Scala rather than Haskell. 
Perhaps it's related - https://gist.github.com/viktorklang/9414163.
&gt; Yeah, nor have I. But I have to ask myself what would happen if a project I was using Haskell for developed one - I know they can happen and it seems like it takes a lot of expertise to fix them. You can follow [this method](http://neilmitchell.blogspot.be/2015/09/detecting-space-leaks.html) to find any spaceleaks. Optimizing for space is something you typically do when the program (or library) is ready. It requires awareness of how things work, but that's the same for optimizing in any language, and it's not particularly harder in haskell. In most cases you just use libraries, and they have been optimized for performance.
Removed because this is already linked to in the sidebar and is of cours an ad
Oh seriously? Yet another blog post on a union type that is strictly inferior (in terms of user experience) than the one in Scala.js ([source](https://github.com/scala-js/scala-js/blob/master/library/src/main/scala/scala/scalajs/js/Union.scala) and [tests](https://github.com/scala-js/scala-js/blob/master/test-suite/js/src/test/scala/org/scalajs/testsuite/library/UnionTypeTest.scala))? They mention in the beginning that their original thing used implicits that were constructive, and that is why they had issues with it. The implicits in Scala.js do not have this problem, yet are able to encode commutativity, associativity and more (covariance). Moreover, the union type of Scala.js solves all the restrictions mentioned in the conclusion. And to top that, it can be used straightforwardly in any place where a type is accepted (including in type parameters), without all the typeclass noise.
Perhaps use cats effect and it’s IO monad? ``` for { res &lt;- IO(openResource) F &lt;- IO.fromFuture(asyncTask) _ &lt;- IO(closeResource(res)) } yield f ``` 
I guess you should consider writing a blog post about it, then ;\^)
Constrain `F` to `MonadError` like this: def bracket[F[_], E, R, A](open: F[R], close: A =&gt; F[Nothing])(f: R =&gt; F[A])(implicit F: MonadError[F, E]): F[A] = for { resource &lt;- open attempt &lt;- f(resource).attempt _ &lt;- close(a) a &lt;- attempt.fold(F.raiseError, F.pure) yield a
Maybe some sort of variation over the load pattern? // normal load pattern for comparison def withResource[T](thunk: Resource =&gt; T): T = { val resource = ... try { thunk(resource) } finally { resource.close() } } // use like: withResource { resource =&gt; doStuff } def withResourceFuture[T](thunk: Resource =&gt; Future[T]): Future[T] = { val resource = ... val future = thunk(resource) future.onComplete { _ =&gt; resource.close() } future } Then you could do like something: for { x &lt;- doSomething y &lt;- withResourceFuture(callAsyncMethodWithResource) // handled closing resource for you z &lt;- doSthElse } yield result withResourceFuture {}
I think it sucks lol. Option, try, and either are much better
Seems like overall it's safer than Scala
There are times that I wish that Scala had the nullable `?` operator _strictly_ for when dealing with Java libraries that could return null. I think that it could be a better indicator that "a null can be expected here" than wrapping nullable returns in an Option, which could hide the acknowledgement of the legacy null return value.
The Kotlin null type system is more convenient than `Option`, because it is easier to write `Int?` than `Option[Int]`. And it interoperates well with Java code, so you needn't wrap/unwrap nullable values in `Option` sometimes. The `Option` is just a type, it is easier than adding a new syntax for a language to handle nullable type, and the language's syntax is simpler. It is good for Scala to add a new syntax like Kotlin in the future, which desugaring someType? into the unboxed option or `someType | Null` in Scala.
Dotty has [plans to support strict non-nullability](https://contributors.scala-lang.org/t/non-nullability-by-default/964) which will help reduce the amount of allocations where performance is important. However Option everywhere else as it is more composable with the rest of the std lib (just a collection type) and does not require a bunch of [special operators](https://kotlinlang.org/docs/reference/null-safety.html).
I recall some arguments that it does not compose as nicely. I do not know how well it works with generics, for instance. Nullability is in some ways (as least as far as my very superficial understanding goes) a part of the type system of Kotlin, which increases the type system's complexity, and possibly also how it interacts with other features and parts of the language. And that is some of the philosophy of Scala, namely to try to have few fundamental abstractions that are very flexible and where the interaction between those few fundamental abstractions are well-defined and try to build upon those (the original XML literals are an example of a feature that runs counter to this philosophy). I think one of the important properties of a programming language is that it is true that for all sets of two or more of the features and functionalities in that language, the interaction between those features and functionalities is well-defined. And I believe that that property is the main weakness of macros and similar like bytecode generation, reflection etc. (though my experience with macros are very, very limited), since while you can keep the interaction between a macro and the core features and functionalities of a given language well-defined, two macros that were defined independently of each other do not necessarily have that property. But, getting back on track I am not in any way an expert in Kotlin, so whether nullability works well in the language or not, and how it affects or will affect the language, I do not know.
"don't ever use null" is not a great moto when "you may get null anytime you use a Java API" is true.
Maybe it should be in its own cross-compiled library (the standard library perhaps)?
Half like it, half hate it. I like some of the guarantees. I like the elvis operator and the chained null-call operators (e.g. `foo?.bar()?.baz ?: error("fail")`) I hate that some of the guarantees are strict (i.e. if you check that an instance's property is non-null, it isn't assumed it is non-null after that like it would be with a local variable). I also hate that they inject bytecode to maintain some of the null accuracy at runtime. I hate some of the other oddities (e.g. `foo?()` and `foo?[i]` doesn't work on a nullable function/indexable last I checked, you have to `foo?.invoke()` or `foo?.get(i)`). Finally, I hate that they have to have `map` and `mapNotNull` because they can't use `flatMap` because nullability isn't like an option. Now smart casts? Those are awesome.
I'm a fan of nullables in application APIs and `Option`s in library APIs. The former leads to more obvious code, but the latter leads to more composability. Looking forward to union types in Dotty so that I'll have the choice.
AFAIU Scala.js' union types are not full blow union types (e.g. have to pattern match a union type against `Any`, which, among other things, makes exhaustive matches impossible). Apparently Dotty will (may?) deliver on the union type front; Scala.js would then get for union types for free (and be able to remove existing implementation).
`Try` doesn't include the type of the exception. Lame. `Either`'s left/right thing is just atrocious.
&gt; dotty is just cleaning up a decade's worth of random cruft that the language has accumulated. ... and adding its own decade worth of random cruft, so there's that.
Found something that might be relevant for you : https://www.phdata.io/try-with-resources-in-scala/
Indeed, Dotty already has full-blown union and intersection types in the core of its type system.
I like kotlin. T?.let { it.foo } No box. No allocation of closure. No indy. No cognitive load. 
Sure, but I'm the wrong person to ask. I have no part in the project.
I have more or less vague plans to submit a SIP to include it in the standard library, and enhance it a bit through some compiler support. In particular, I believe we can provide good pattern matching with exhaustiveness checking if we do that.
I'm afraid I have zillions other things above writing blog posts on my to-do list.
You could easily build an `Option` type in Kotlin though, with similarly convenient operators (although no `for` comprehension). The issue in Scala is that you just have to hope that people don't use `null`. Something like this compiles just fine def foo(a: String, b: Option[Int]) = ... foo(null, null) I think Swift has the best of both worlds. It's got the same safety as Kotlin while representing nullable values as an `Optional` with convenient operators like Scala. 
`Some(null)` is not a pitfall, you got that backwards. If `Some(null)` evaluated to `None` we would have a big problem as we would violate the monadic laws. Look at it like this: `Some` is just a container that wraps some value, whatever that value is. If you cannot create `Some(null)` then you have a container that can wrap every value *except* for one special value. Having exceptions makes thinks a lot harder when writing generic library code.
&gt; The Kotlin null type system is more convenient than `Option` Well, in very simple cases, yes. But now you have some list of `Int?` and you'd like to sum those that are not null. In Scala that is `list.flatten.sum`. I'm curious, how does it look like in Kotlin? 
How important is category theory? 
Hi all Could someone please explain to me, for what is the libs https://github.com/non/kind-projector good for? Thanks 
Well, "a couple years" has never been good enough for me.
As a mathematical foundation it's very important if you want to develop a lib like cats. A direct advantage is that everything in Cats is verified by mathematical laws. But to use Cats you don't need to have a deep knowledge of Category Theory. 
They add a `filterNotNullTo` extension method to `Iterable&lt;T?&gt;`, so you can use list.filterNotNull().sum().
Like others have said, you don't need to know category theory at all. But just in case anyone here *wants* to learn category theory, I can recommend Bartosz Milewski's [Category Theory for Programmers](https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/).
They add a `filterNotNullTo` extension method to `Iterable&lt;T?&gt;`, so you can use list.filterNotNull().sum().
I mean it is a source of some confusion (note to self; pun-material), not that its function is incorrect. Compared to nullable types, being able to wrap a null in a type that is explained to prevent null errors is a potential pitfall. I agree that it has its uses, *especially* if types will be non-nullable in a future Scala-version :)
FWIW, you build `flatMap` as an extension function on nullable types in Kotlin: fun &lt;T, R&gt; T?.flatMap(fn: (T) -&gt; R?): R? { return if (this != null) { fn(this) } else { null } }
It *can* be confusing, but "point before line calculation" is also confusing to a lot of people, but that doesn't mean there is an inherently better way. So yeah, nullable types with special operators have a benefit in certain situations, but `Some(null)` is not an argument against `Option`. These are two different things.
Everyone says you don't need to know Category Theory to use Cats (or Haskell, or understand the monad etc.) and to a certain extent that may be true, but man it sure helps! I struggled for years trying to understand these concepts, but my brain seemed to keep fighting back, wanting to know the deeper meaning behind all that funny terminology. After learning a bit of Category Theory everything all fell into place. I highly recommend Bartosz Milewski's blog and YouTube videos. https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/
Imagine you work at a car factory. You assemble complex stuff. For some reason a lot of your coworkerd like to talk about and describe physics of combustion engines and so. Some do it out of love if learning, some do it to look smart, some do it to fit. Now you wonder.. do I need to know laws of thermodynamics, differential calculus etc. to be able to work here and assemble these parts? It seems a lot of other good coworkerd do! No. You dont. Youre just biased because lots of people interested in physics of combustion engines are around you. And they are usually heck of good coworkers. But not because theyre good at physics, but because their natural interest and curiosity. Some of them even build their own engines at home, out of love to learn and experiment! 
He means in its type. You can't declare a `Try[IOException, Person]`
Since I think 2.12, Either is right-biased, so you don't need to use the left/right projections.
Maybe we should encourage always using the Option(...) factory (and Option.empty instead of None). Another advantage is the type is inferred to Option, not Some or None, so you avoid needing covariance or to help type inference (like with foldLeft etc.)
Please don't. These "convenience" implicit conversions don't work except in the simpliest situations, screw up type safety, create a lot of confusion for the code reader and are overall a bad idea. "Let's just create an implicit conversion" is one of the most common traps that not-sufficiently-experienced Scala programmers fall into.
`Option.apply` and `Some.apply` are two different things for two different situations. Sometimes you need the former, sometimes the latter.
But a type `Something|Null` has no members other than the `AnyRef` ones, so the usual problem of adding accidental extension methods is avoided. I would appreciate support for implicit conversions that only apply in certain situations, such as only when the other type is specifically requested by name (so, it applies when calling a function or assigning a variable, but doesn't create extension methods).
Not the same thing. Take a look at the following example: import scala.collection.mutable.ListBuffer object Main { class Logger(id : String) { private val log = new ListBuffer[String] def println(s: String): Unit = log += s def dump(): Unit = log.foreach(entry =&gt; Console.println(s"$id: $entry")) } type Logged[T] = implicit Logger =&gt; T def log: Logged[Logger] = implicitly[Logger] def logContext[T](logger: Logger)(op: Logged[T]) = { implicit val implicitLogger: Logger = logger op(logger) logger.dump() } def fact(x: Int): Logged[Int] = { log.println(s"fact: $x") logContext(new Logger("log2")) { log.println("log in here is different!") WebTools.wget(s"http://catpics.net/pic$x") } if x == 1 then 1 else x * fact(x-1) } def main(args: Array[String]) = { logContext(new Logger("log1")) { val in = 3 val out = fact(in) println(s"fact($in): $out") } } } object WebTools { import Main.{Logged,log} def wget(url: String): Logged[Unit] = { log.println(s"Downloading $url") } } This will give you the following output log2: log in here is different! log2: Downloading http://catpics.net/pic3 log2: log in here is different! log2: Downloading http://catpics.net/pic2 log2: log in here is different! log2: Downloading http://catpics.net/pic1 fact(3): 6 log1: fact: 3 log1: fact: 2 log1: fact: 1 In the body of fact, note the difference between the first and the second use of log!
This is a great analogy. To push it further. Some love the materials and construction of the body. How to make it light but pretty. They know a huge amount about material physics, but almost nothing about engines. Some know parts of each. They wont wow you with thermodynamics knowledge, or deep material design, but they can see the flow of the whole set of systems and can help each group when necessary. Without them sure, you will get a good car, but will you get a great car? Some just know everything. Sometimes their jerks about it, and sometimes theyre super nice. Some just _think_ they know everything. Ugh. Different teams have different mixes of all these people. Making it work is more about knowing who to put where, than who you have. 
If you want your conversion to work only in these certain situations then I think much clearer and readable solution is to create a `.toOtherType` converter extension method. Having to call it explicitly is a minor annoyance and makes life so much easier for the compiler and those who will read the code.
Hey thx for the suggestion, this variation i already knew, i was looking for an approach where i didn't have to know that i'm dealing with a future as the example Milyardo example.
Interesting idea with the MonadError that's what i'm looking for, thank you it helped alot!
Very nice to here that there is a WIP, i'll keep following this pull request for it's ideas :)
I will admit, I didn't actually try it to see if it works, I've never actually used cats, just came up with something for how I'd approach it in Scalaz.
Depends on how you define "knowing' category theory. Having a strong sense of diction is one thing. Actually being able to solve original problems by applying category theory is another. Even among the many Scala developers who are quite familiar with with the former, would be in unfamiliar territory in the later.
&gt;You suggested conversion to Option[Something] which would accidentally extend Something's API with Options API which is totally unintended. The conversion was `Something|Null ⇒ Option[Something]`. This would extend `Something|Null`, which doesn't *have* an API (because the nearest common ancestor of `Something` and `Null` is `AnyRef`). Of course, this assumes we get rid of `Null`'s current special “bottom type” status, where it is considered a subtype of all `T &lt;: AnyRef`. `Null` would have to be redefined as just a singleton reference type, which no non-`null` value is assignable to. In other words, `Null &lt;: Something` would have to no longer be true. I am mostly in favor of this, because it would mean that Scala is always null-safe (without relying on a mere convention), and therefore no longer directly suffers from the [billion dollar mistake](https://en.wikipedia.org/wiki/Null_pointer#History). &gt;If you want your conversion to work only in these certain situations then I think much clearer and readable solution is to create a `.toOtherType` converter extension method. That adds unnecessary syntactic noise. I am not fond of unnecessary syntactic noise. If we're going to go that route, then we should probably instead introduce extension methods for any type `T|Null` that mirror the methods of `Option`, without actually converting to `Option`. Come to think of it, that might be a good idea anyway, for performance: allocating an `Option` and then calling its methods is much costlier at run time than a simple null check. Then we'd just deprecate `Option` entirely. &gt;Having to call it explicitly … makes life so much easier for the compiler and those who will read the code. Is it really that hard to only apply an implicit conversion if the exact result type is required by name, and ignore it otherwise? Are people really going to be confused when passing a `obj: scala.collection.Seq[T]` to a function expecting `java.util.List[_ &lt;: T]` works but calling `obj.add` doesn't? It'd be what `scala.collection.JavaConverters` was supposed to do, only it'd actually be safe, because it would only apply if exactly `java.util.List[_ &lt;: T]` is needed, by name, and not under any other circumstances (like finding extension methods). Seems to me that this would let us have our proverbial cake and eat it too.
About Bartosz Milewski's Category Theory for Programmers on chapter 5 I started to struggle, because it is diffucult to understand. For example, I understand what is a Product but I did not get, why he talked about factorizer. 
&gt; Something like this compiles just fine Use wartremover to disallow that. You still have to worry about `null`s coming from Java, but that problem exists in Kotlin too.
Nullability as a special case in the language model is a terrible idea, because it doesn't compose properly in generic cases. (In fact, this case has convinced me that union types in general are a bad idea). Kotlin will never be able to have an information-carrying result type that can be used the same way as an absence type, which is an incredibly common way for a codebase to evolve (i.e. you start out by using `Option` but then you realise you need to carry a reason/explanation in the "failure" case, so you switch to `Either` - in Scala this is a couple-of-lines change). I prefer words to symbols for type names, so I think `Option` is probably better than `?`. And while I like short syntax, I don't think `?.` is common enough to justify a special case in the language; certainly what I've found when maintaining code that works with futures is that Future-specific operations are more confusing than the general-purpose `for`/`yield` even in cases when the latter is slightly longer. A language has a syntax budget and a semantics budget, and it's worth holding on to both for the most general use cases you can.
It lets you write partially applied types "directly" rather than having to create type aliases for them. https://github.com/m50d/tierney/commit/474868da7e5a2dd4eaaa3557f19f383b7db9a626 is an example of me introducing it to one of my projects - I can remove the `Partial1` and `Partial2` traits (which I defined just to have type members in) and instead write the types I meant directly - e.g. I can write `F[G, ?]` instead of having to write `Partial1[F, G]#O` where I've defined `Partial1` so that that's the right type.
The real weakness of Haskell, that's for some reason is rarely mentioned, is it's (lack of a) module system and unmodularity in both the design of the language: * both global type classes and open type families are unmodular: orphan instances somewhere in a library you use can easily break your app's build and in the implementation * GHC optimizations critically depend on cross-module inlining (a JIT could solve that) and dynamic libraries aren't usable in practice because they rely on exact compiler version. Backpack is neither ready yet nor is a good enough solution to Haskell's module situation – it's fully compile-time, causes insane amount of recompilation and its modules are second-class. And if you really want to hurt – you can just mention the records situation.
Can you explain for dummies? :-)
None of that has really bothered me, shrug. I don't see modularity as so much of a problem in practice, and while records have their issues they don't seem *so* bad.
There are several decisions at different levels of the language that play into this: 1. Language level: Haskell has global constructs (type classes &amp; families). This means that two completely unrelated packages can break each other if they define conflicting instances and are both imported by application. 2. Implementation level: GHC made a design choice to make binary libraries incompatible between any two versions of GHC. 3. Optimization level: GHC optimizations rely a lot on inlining and especially cross-module inlining. As a consequence: 1. At least part of Cabal hell is caused by that no one knows where to put orphan instances so that nothing breaks on updates 2. Runtime behavior of your code may change drastically by just moving it into separate package or module (pathological case: OOM because after you moved parts of your pipeline to separate packages [while forgetting {-# INLINE #-} pragmas] it no longer gets picked up by stream fusion and generates a lot of thunks) 3. Shared Haskell libraries can't be used as a backbone of a dynamic system, e.g. OS or an IDE, unless you're willing to recompile your entire system on updates. I know that Haskell apps are usually distributed as static binaries, but there are often situations when you do need shared libraries (e.g. aforementioned IDE) 4. Haskell code can't be distributed in binary form, preventing some commercial uses.
The Scaladex has a "contributing search" feature [here](https://index.scala-lang.org/search?q=&amp;contributingSearch=true)
Thank you very much for this information! 
You may disagree, but IMHO [modules matter most](http://macqueenfest.cs.uchicago.edu/slides/harper.pdf), and proper support for them is worth sacrificing some coherency checks / whole program analysis capabilities that Haskell tries to go all in on. Records situation is bad enough for people to proclaim that [Haskell has no records](http://www.parsonsmatt.org/overcoming-records/#/) – and I agree with that sentiment, I haven't found more uses for Haskell built-in records, other than bringing some identifiers into scope with RecordWildCards (which Scala does better with `import x._`) Also Haskell has lots of simple ergonomic problems with modules, e.g. they're not hierarchical. While syntactically they look like they are, you can't do `import Lib.*` to bring everything into scope. Just this would drastically cut down on import boilerplate, but Haskell can't really add this syntax as an extension, because of a decision to categorize modules by semantics, not by library: `import Data.*` just doesn't make much sense. Also, the trend to design libraries to be imported qualified doesn't help cut down boilerplate, since imports have to be named manually each time. It only dawned on me how bad the situation is, when I read Nikki and the Robots source code and saw 40+ imports in every file – AFAIK there are no auto-import tools for Haskell, so all of these were typed in manually.
Java didn't evolve as other statically typed languages. Even C++ nowadays has type inference, Java doesn't. Kotlin isn't as marvelous as Scala. It's okay, but Scala is far superior. Groovy is slow as hell. JavaScript is a horrible weakly typed language. Using it to create server side applications is masochism. PHP is a single-purpose language: outputting HTML from a server. For applications like Dokuwiki, Wordpress, etc., it's just fine. For information systems, I don't think it's a good idea choosing PHP.
Could you describe your pipeline some more? It's a bit vague right now. Please tell us where you are using Actors, Streams, etc. Visuals would help :-)
The ability to represent nullable types is orthogonal to `Option`. After using TypeScript for a while, which can do `A | null` (untagged unions), I definitely miss them in Scala as well. Yes, it's a pain that the JVM has `null`, but as long as the JVM has it, then languages that want direct interop, like Scala or Kotlin, must represent it in the type-system. AFAIK untagged unions should be coming in Dotty. Also idiomatic, high-level code should use `Option`, but `Option` implies boxing and sometimes a dev gotta do what a dev gotta do. Just a sample from [monix's codebase](https://github.com/monix/monix) of "null" occurrences, which is an otherwise well grown repository by all metrics that count: ``` monix$ grep -r 'null' . | grep -v target | grep -v ^Binary | wc -l 926 ``` Cheers,
SQS
&gt; Every event must be processed, zero lost events. This is not a situation that Akka alone is going to be good at. You're going to need a queue of some sort. The typical suggestion would be Kafka for durable queuing. You could also use Akka persistence. Both can be wired up to streams. &gt; Our second problem is since the service itself is reading the events, we can't run replicas 'cos each one is subscribing to the same stream, and processes the same events. You should rethink this. To give more color on that: if you only have one Sink for your Source, why are you even bothering with the stream to begin with? If it's for durability, you already know you aren't getting the job done. More details would be good, but at first blush you need to analyze how you can properly parallelize your processing of requests. 
One more is coming as I was thinking about writing about Scala.js' approach. I hope you don't mind. ;)
By all means, go ahead :-)
Take a look at this post: http://danielwestheide.com/blog/2013/01/09/the-neophytes-guide-to-scala-part-8-welcome-to-the-future.html . I highly recommend the whole blog also.
Is it me or does this look a lot like just using a Reader monad.
He addresses this point directly in his talk at some point.
I already replied to this here: https://www.reddit.com/r/scala/comments/7oy5ac/fortnightly_scala_ask_anything_and_discussion/dspgstg/ Also this is the wrong thread
There are a lot of easy issues in [frameless](https://github.com/typelevel/frameless) like [issue 163](https://github.com/typelevel/frameless/issues/163) and [issue 164](https://github.com/typelevel/frameless/issues/164) about implementing missing operators. Those shouldn't be very hard and if you get stuck - there are many people willing to help :) Come join typelevel/frameless gitter channel. You can also pick your favorite libraries and either check issues for "beginner friendly/low-hanging fruit etc." (I did that with monix a while ago and now I'm pretty active contributor, yay! :))or just say hi in respective gitter channel and I'm sure people will try to guide you! 
You didn't read my entire proposal. I said that `T &lt;: T|Null` would need to go away first.
It can't be built on the Node JS HTTP library? 
[Here's a diagram](https://imgur.com/a/qV7lC) I hope the diagram makes it a bit clearer. There isn't much of a pipeline. Basically the Akka/Actor subscribes to the event Source, and passes it to the Event Processor. The event source already provides storage for the events so they can be replay. But ideally we would only receive and process each event once. Our current problem is we can only have one instance running, 'cos multiple instance would all listen and process the same events. The proposal is to decouple the event listener/reader from the actual service. The listener calls and endpoint, and the load balancing passes each event to a replica to process. Failed events must be saved and able to be retry. It just seem to make sense to use Akka to read from the event source. I'm open to suggestions however to accomplish what we need.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/lh5HBxM.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dt2t33a) 
So what language do you prefer for professional coding, Scala or Kotlin, and why?
I should finish watching, thanks.
What kind of side effecty stuff?
Hrmm...would depend on the use case. For big data and lots of case/data classes for composability and what not, still Scala. But for cases where I need thin interaction w/ Java libs (i.e. use a Java framework or create a library to be used by Java devs), Kotlin. Kotlin's IDE and build situation have already surpassed Scala IMO. Scala still wins on composability and macros. I would also choose Kotlin Native over Scala Native purely because the former has better Windows support. So...Scala still for most JVM things that don't interact w/ Java. But that is slowly starting to change for me and I suspect if Kotlin ever gets macros and cleans up its quirks (e.g. no @JvmStatic in interface, no Java-compliant default methods in interface, etc), I might abandon Scala altogether. I originally wrote a [list of annoyances](https://gist.github.com/cretz/2a49514b18914ef09b7c518db6db116c) after writing a few large Kotlin projects, but most are alleviated at this point (granted I have a bunch more sense then that I just file as YouTrack items like [this one](https://youtrack.jetbrains.com/issue/KT-22404) a couple days ago). I could probably write forever on this topic, heh.
His example doesn't use the special `Null` type though.
Thanks! After year of Java programming I was curious about Scala and you are of a big help :) 
Yes it does. If, on [Dotty on Scastie](https://scastie.scala-lang.org/?target=dotty), you run the following: object Example { def main(args: Array[String]): Unit = { val s: String = null } } You'll find that it compiles and runs successfully. `null` is assignable to `String`. That's the special `Null` type in action, and as you can see, it's still there in Dotty (so far). My proposal won't work correctly unless this behavior is removed.
Honestly, the best open source project to contribute to is one that you use every day. Anything you use enough will eventually annoy you in some way that seems like someone should have made that easy fix. That someone is you, and that's the project you contribute to.
According to /u/m50d it breaks parametricity https://www.reddit.com/r/scala/comments/6ee77n/scala_vs_kotlin/diagwtz/
One of the easiest way of contributing is joing one of the [scala sprees](https://github.com/scalacenter/sprees). Sadly they are not so often but its a very nice way to work on opensource when you are a newcomer.
This question sounds like a "I want to get my name on Open Source so I become more attractive to potential employers" question. It concerns me because surely you want to make *positive* contributions to Open Source for the benefit of *other people*? In which case wouldn't you prefer to become more skilled in the language, or more invested in the particular project, before making a carefully considered and helpful contribution? I am very much an enthusiast of contributions to Open Source - because of the potential benefit to society as a whole - and as a thank you for the effort other people have made that has benefited me, personally, so much over the years. But I'm not going to start recklessly submit patches on a product I don't understand well or in a language that I can only navigate through [programming by coincidence](https://en.wikipedia.org/wiki/Programming_by_permutation). I would suggest a better question for a beginner would be "what is a beginner friendly Open Source project I could learn from?" - and have patience, when it becomes time for you to make contributions you'll know.
**Programming by permutation** Programming by permutation, sometimes called "programming by accident" or "by-try programming", is an approach to software development wherein a programming problem is solved by iteratively making small changes (permutations) and testing each change to see if it behaves as desired. This approach sometimes seems attractive when the programmer does not fully understand the code and believes that one or more small modifications may result in code that is correct. This tactic is not productive when: There is lack of easily executed automated regression tests with significant coverage of the codebase: a series of small modifications can easily introduce new undetected bugs into the code, leading to a "solution" that is even less correct than the starting point Without Test Driven Development it is rarely possible to measure, by empirical testing, whether the solution will work for all or significant part of the relevant cases No Version Control System is used (for example GIT, Mercurial or SVN) or it is not used during iterations to reset the situation when a change has no visible effect many false starts and corrections usually occur before a satisfactory endpoint is reached in the worst case the original state of the code may be irretrievably lost Programming by permutation gives little or no assurance about the quality of the code produced -- it is the polar opposite of Formal verification. Programmers are often compelled to program by permutation when an API is insufficiently documented. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
your bracket def uses a before defining it (third statement in for block) I think
Thank you so much, i'll definitely be looking into these. a side note, i really appreciate the typelevel code of conduct.
Thank you very much, i didn't know that feature existed.
Thank you! i didn't know about that, i'll definitely looking at that, seems like a great opportunity to meet people fro the community too.
&gt; This question sounds like a "I want to get my name on Open Source so I become more attractive to potential employers" question. Is it unethical? 
Hello there, &gt;&gt; It concerns me because surely you want to make positive contributions to Open Source let me ask you, what is an example of a negative contribution to an open source project? &gt;&gt; But I'm not going to start recklessly submit patches on a product I don't understand. I trust the project's maintainers to not accepte bad code, and i think one of the purposes of beginner friendly issues is to offer feedback on the fixe's implementation. But hey, i salute you for your very pure and selfless intentions.
We're open to ideas. If there're other ways to have replicas of our service, we're interested.
I'm curious why you need to add in the HTTP layer between the database replicas. Can you not just go Kafka -&gt; transform event -&gt; persist into replicated database and use at least once delivery (commit message to Kafka only once it has been persisted in the database). What's the reason for adding the HTTP layer? Seems a bit unnecessary no?
Mmm, there's no database replicas. There is one database, and replicas of the service. The service has business logic to transform events, save to database, call other services, etc.
Have you read up on [Graphs](https://doc.akka.io/docs/akka/2.5.3/scala/stream/stream-graphs.html)? That's how you'd accomplish scalability (via Balance specifically) in Flows. Your "at least once" requirement is also suggests you turn off auto-commit on the kafka consumer. Add a message that allows you to commit the offset once it's processed. It sounds like you may have an implicit assumption that an event is dependent on events that were previously processed. If that's the case, your problem is more complicated. 
interesting, thank you!
The Service, Event Processor, Event transformation all refers to the same thing as far as I understand. It receives an event via http (or web socket, etc) and processes it. Performance isn't much of a problem actually, that's why I didn't mention it in the original post. Reliability is the most important, not losing events during spikes, or when service is down. 
So as long as you pull events from Kafka and commit the offset after persisting data to the database, you won't lose any data. 
Thanks for the write up, but I have no idea why anyone would be recommending Spray (especially when also recommending Akka, which absorbed Spray), when it is literally officially dead and has been for 3 or 4 years.
I would strongly recommend not to support [Scalalab](https://github.com/sterglee/scalalab); the mess of the codebase notwithstanding, the author for several years pisses on open source projects, simply ignoring their license terms. It's not conincidence neither the SourceForge project site nor the GitHub repository contain the word "license". If you look into his "collection" of stuff in lib, you easily find GPL'ed things (JFreeSVG to give an example) that require much more care than just dumping them into a fat jar. Plus multiple libraries whose code is simply pasted into the src directory without mentioning, including an edited version of my own ScalaInterpreterPane code which violates the terms of the LGPL. I find that attitude very disrespectful of open source.
Upvote for the name alone, it's perfect. And I hate you a little bit for coming up with it and not me. ;-) As for the code, I'm on mobile atm, I'll check it out later. 
Oh, I see what you mean. You're right.
Fine for me, although [`.option` and `.opt`](https://github.com/AVSystem/scala-commons/blob/master/commons-core/src/main/scala/com/avsystem/commons/SharedExtensions.scala#L69) that we have in our company's [scala-commons](https://github.com/AVSystem/scala-commons) library are short enough for me.
Here is an interview I did with Debashish. I like way he is trying to codify functional domain modelling and present the functional paradigm in a way that makes sense to people who are more experienced with a traditional OO/DDD approach to structuring software.
Because Spray is still a very nice library for this sort of thing? Akka HTTP does not cover all the use cases, e.g. it doesn't offer a way to run it as a servlet, which means I can't migrate my code to it.
It has nothing to do with SBT, and only limited involvement with Scala at all - breakpoints are JVM-level functionality (though IntelliJ does have at least some understanding of Scala to be able to choose which class to breakpoint when breaking on a line that contains multiple things, and to have the "smart" step-into button that skips over a lot).
Seriously, article about libraries without no direct links to libraries?
Doesn't seem to that uncommon though, Scala Native does similar things.
https://github.com/scala-js/scala-js/tree/master/javalib/src/main/scala/java/math vs. https://github.com/scala-js/scala-js/blob/master/LICENSE
I believe that removing the LICENSE file, and claiming that the code Scala.js distributes is BSD and owned by the EPFL is along the lines of what the OP described as "very disrespectful of open source".
Yeah but did you look at the source files? They contain copyright info for Google, as well as info about the Apache license.
All of them. Assuming `foo` is `Any`, `if (foo is Bar) foo.methodOnBar()`. Same applies to `when` clauses. Basically, like nulls, where the compiler can tell via your if statements that something is a certain type, you get to pretend that it is. So if you have `if (foo !is Bar) return` then for the rest of the function, Kotlin allows you to assume `foo` is `Bar`. See http://kotlinlang.org/docs/reference/typecasts.html
Or scala'z Foldable.
It makes sense to me that the debugger would also have to go down to the JVM level to function at all. It must be very involved, however, to do that mapping, right? I'm not very familiar with JVM level stuff but even in a Java debugger, couldn't the mapping between the java code in your source file and the JVM code be tricky? And then we have Scala where I think that a lot of constructs must be mapped to bytecode in strange ways. In any case, I suppose ultimately the functionality is coming from the scala plugin or is just built in.
&gt; I built my own scala class named "myList" which has no mention of scala inherent List class. Thats basically means i cannot use monoids and call foldLeft etc on myList ? Do i need to give defination for foldleft and foldright functions ? Yes. Your class doesn't define `foldLeft`, and doesn't inherit anything called `foldLeft`. Trying to access a field or method that doesn't exist won't (and shouldn't) compile. 
This is hilarious
&gt; Specifically what is the /: operator? It's `foldLeft`. Use of `/:` instead of `foldLeft` is strongly discouraged; the code is either old or the author never got the memo.
Interesting. So scalac itself is putting debug information there, then? Or is scala being compiled to &lt;some other thing&gt; which is then compiled to bytecode?
I wasn't able to find the default option for scalac but intellij is perfectly able to set the option to "vars" (which has all debug info) and then start the debug process. see -g option: https://www.scala-lang.org/old/sites/default/files/linuxsoft_archives/docu/files/tools/scalac.html
I did find jtidy, but could not use it, as it hasn't been updated in years, although tidy itself has been.
Is there any Scala data science library that supports 3D matrices?
Sorry but that's just wrong in Scala. Arrays do not have the method map neither by definition in "the" Array class nor by inheritance. In Scala you have a third option: the pump my library approach where you define methods for a class outside of this class. That's how basically typeclasses are used in Cats and ScalaZ to enrich a class with methods not defined in it. 
&gt; I don't think there is anything in the tech world today slower than SBT Have you heard of babel?
hi, I understood that i didn't define foldleft is it is not available but how do professional scala developers enrich their custom classes do they give custom defination of all usefull methods like map, fold, fold right. Second as scala List and sets are monoids . if i define my class to obey laws of identity and right fold , isn't scala implicitly should understand that my class follows monoids law so boom i got map and fold, and other methods available on its own.
`/:` : https://www.scala-lang.org/api/current/scala/collection/immutable/List.html#/:[B](z:B)(op:(B,A)=%3EB):B
I don't think this is a good example of overly complex implementations. To the contrary, it looks like a non-recursive implementation of its [definition|https://en.wikipedia.org/wiki/Levenshtein_distance#Definition]. It's a bit complicated, though, because the author decided to avoid recursion (as the definition is not tail-recursive) and uses nested `zip`.
has anybody used scala in Online interviews with tech companies like Facebook, google or competitions like HackerRank and Codecheff where we have to solve 2-3 question in 1 hr time? what was your experience with coding interview in Scala? For example some online interviewer asked me to code Fibonacci series sum with memoization , i am able to complete it in 30 minute using java imperative with hash table optimization but , i find it hard to believe it can be done in scala way of functional thinking . Is cracking online interview in scala difficult? or shall we keep other language like python and java for online competitions?
&gt; how do professional scala developers enrich their custom classes do they give custom definition of all useful methods like map, fold, fold right Aha, I see now. It happens a few ways. Pretty often, people just define those methods on their classes and traits. Other times, people add them via an implicit conversion (see [implicit classes](https://docs.scala-lang.org/overviews/core/implicit-classes.html); if make your implicit class extend `AnyVal`, you won't even get an allocation). There are also various typeclass-driven approaches, which is what people probably mean when they recommend Cats and Scalaz in this situation. &gt; if i define my class to obey laws of identity and right fold , isn't scala implicitly should understand that my class follows monoids law No, this doesn't happen. You can derive these methods - or a `Monoid` instance for your type - but that's not something scalac does magically out of the box. If you want something like this, look at Cats or Scalaz.
&gt; Sorry but that's just wrong in Scala. Arrays do not have the method map neither by definition in "the" Array class nor by inheritance. Aha, Arrays are an exception - but aren't they implemented by an implicit conversion under the hood? I'm aware of implicit conversions, but didn't want to muddy the waters because based on the OP's question and phrasing, they seemed like a beginner. There's also a distinction to be made between a class "having a member called foo" and "being implicitly convertible to a type with a member foo" even if those things look the same to callers.
I've used Scala for that kind of task. Doing them in Scala is easy, easier than Java or Python, for exactly the same reasons that normal programming is easier in Scala. I struggle to understand why you'd think it would be harder to do something like that Fibonacci problem in Scala? That's exactly the sort of thing that Scala makes a lot easier than Java.
i am talking about Fibonacci with memoization not just recursive fibonachi like store already computed iterations in hash table and look up from hash tables along with normal flow of code. I am novice with scala but i understood transforming recursive definitions into functional scala is easy but if want to add some optimization in algorithm like look up in hash table for computed results . or some 2d bit map array for dynamic programing like for longest pallendrome involving 2d matrix array. 
Well in the worst case you can easily just do what you'd do in java. But usually there are better ways to achieve what you want. 
Not sure what your total goal is, but you mentioned building your own "myList" and curiosity about giving definitions of `foldLeft` and `foldRight`. If you are interested in learning more about the topic, and if you can get a copy, check out the book [*Functional Programming in Scala*](https://www.manning.com/books/functional-programming-in-scala). It is available in a number of forms. For example I am reading it through my company subscription to [Safari Books Online](https://www.safaribooksonline.com/library/view/functional-programming-in/9781617290657/). By "Chapter 3. Functional data structures" there are already examples showing `foldRight`, plus an exercise (with solutions [online](https://github.com/fpinscala/fpinscala#readme)) where you will write your own `foldLeft` for a custom `sealed trait List[+A]`. n.b. when we actually need to _use_ `foldLeft` and others we just use the wonderful libraries provided by cats/scalaz/etc.
Coming from a heavily influence Java/Python background it just seemed very compact and hard to follow what was going on. But it's good to hear other opinions.
Optics are pretty limited in use in real-life programming, but sometimes they are useful, [for example when dealing with JSON](https://github.com/circe/circe/blob/master/docs/src/main/tut/optics.md)
Most likely you should not use microservices. Most likely, your team will ignore your advice, as 95% of Java devs are stuck on Java, and advise from "the new guy" almost never wins. 
if you are a small team, do not use micro services. if you are a big team, you could use micro services, but you should also allow a bigger set of languages, else you would not have that many benefits which a micro services architecture would provide.
The problem with the monolith app is that each time it builds it takes 20 minutes. If we had it in microservices, each person could work on their own separate pieces and we wouldn't be waiting 20 minutes each time someone built something. So I think the microservice idea has merit for us. We're discussing that or several splitting the app into a couple of macroservices instead
see my reply to /u/TunaBoo, but we're basically stuck waiting for these gigantic builds to happen which take 20-30 minutes and it slows down our development drastically. so something has to change 
We do have full time ops guys to manage the deployment of only our app. There's about 7 ops guys, and 11 devs on our team. Our app isn't as big as Netflixes, but it has a decent amount of components that make it annoying for multiple people to deploy within the same monolith. Compile time is about 2 minutes, but the entire jenkins build and deployment takes about 19 minutes for our dev and test servers, and 6 minutes for staging/master. (I can't tell you why our builds take about that long). Just checked, there's about 200k lines of code which is a pretty good size that can be broken up 
Usually the goal of microservices is to decouple the deployment of the individual components. They don't need to all go at the same time, and can deploy functionality separately from eachother. Contracts become very important. Something like [gRPC](https://grpc.io/) might make it easier to ensure that contracts exist, and are reasonably well structured, as well as providing a high performing over the wire protocol. As for whether or not to use scala, scala makes your build times take longer, but it does a lot more checking for you. It's a bit easier to do concurrency in scala, if you follow the functional transform model. You can do this in java8, but it's not as intuitive (IMHO). Most scala code that compiles the first time, runs the first time, correctly. Most scala code that does the same thing as java code is less verbose, and is easier to read without the unnecessary ceremony. Most scala code is immutable by default, which is a necessary thing when trying to do concurrency. That being said, most of this can be accomplished in Java, it's just a different style.
Ok so most of the time is not in compile. Micro-services should take longer to deploy, as there are more steps. Funny how people think they are the answer to everything. Also, wow to 7 ops to 11 devs... normal ratio is more like 1:20
The point of micro services in our case are to let people deploy different parts of the application without slowing down someone elses part. So while deploying every single micro service might take more time than our monolith, our goal is to have enough microservices such that I can work on one, and another dev can work on another and we can deploy separately without being slowed by each other (and that each deployment is &lt;5minutes per micro service) As to the ops to dev ratio, I might have that skewed. I know we have 11 devs, there might only be 3 or 4 ops persons and I confused the others for ops when they're not 
To pile on with the "you probably don't want microservices" crowd. Unless you're also breaking up your database into smaller independent databases, you're actually building a distributed monolith. You now have the worst of both worlds.
Zero to Hero? Do the Scala specialization on Coursera. To learn basic syntax, check out Scala on TutorialPoint or “Scala for the Impatient” by Cay Horstmann
&gt; Scala specialization (is this the one you are referring to?)[https://www.coursera.org/specializations/scala]
Yes, it’s a solid course. It’s helped me immensely with getting started in a FP team
3 months. I know some people who completed in several weeks. I would recommend Scala for the impatient on your timeline. It’ll take more than 2 weeks to get to hero status with Scala. 
Ur my hero ;)
I used (and still reference) Twitter's Scala School. https://twitter.github.io/scala_school/
I ihink Programming in Scala 3rd edition (by Odersky) is probably the best way to learn the Scala language. 
Coursera is quiet a good start to get in touch with the programming language itsself. maybe you should also take a look at „Functional programming in Scala“ this is a book containing the principles of functional programming.
exactly my thoughts. Scala is not perfect as many try to convince us. Compile times are awful (probably you won't notice if the microservice is actually micro). The fact that isn't opinionated requires a very high level of team discipline. Kotlin on the other hand seems to solve java problems, without many of the scala drawbacks (it doesn't allow you to do so many exoteric things too, but maybe you can live with that)
I enjoyed, https://alvinalexander.com/scala
I think this is one of the best summaries if you want to learn scala: https://gist.github.com/d1egoaz/2180cbbf7d373a0c5575f9a62466e5e1
Of course, the syntax of the library is terrible! Thereby, we're not promoting its usage for everyday programming at the present time. The post simply shows a connection between two different FP concepts and tries to introduce some research we did revolving around it. Once we have well-founded abstractions, we'll try to make Stateless practical for real-world usage, not only by polishing the syntax but also by making the abstractions more approachable. We're far from there yet but we have many ideas to reach that goal. So, we'll try to convince you again in a few months! ;) Thank you very much for your feedback.
I used that too, but last I checked it was fairly outdated
If you are confident in your general programming knowledge, pragmatic scala is a very good book... A bit old being from 2015, but little to none of the things it presents are outdated. It's very good at explaining the syntax and the common coding practices when working with the std. 
&gt;The for loop is not what I thought it was Why not? for( a &lt;- 1 to 10){ println( "Value of a: " + a ) } That's really not that much different than for-loops in other languages. You might have seen a for-comprehension (for with "yield" at the end) and mixed it up. for{ db &lt;- connectDB() data &lt;- sendQuery(selectStuff) convertedData &lt;- data.convert } yield convertedData.head That's not a loop, that's closer to a bunch of try/catch. &gt;there are so many shorthand operations for manipulating collections, etc. Does anyone have a good resource that goes from zero to hero? ;) Functional Programming in Scala helped me. In the first few chapters you will implement all those shorthand functions yourself and thus will have a good understanding of what they each do. 
I used this and stack overflow for Scala has most novice questions asked and answered. Also they answer very fast there. And there used to be a bunch of docs right on the main Scala site. 
Scala's for loop is a syntax trap. That line of code doesn't do the same thing it does in other languages, because it's actually a for-comprehension with some omitted parts. That second example is misleading too. You can think of it as a sort-of-try-catch only because the expressions you match yield only one element each. This is pretty common, but it's important to understand what's really happening.
I wouldn’t think of for yield as a for loop, for yield is syntactic sugar for a flatMap
Very useful - thank you very much for taking the time to write that out.
Yes that's why I said that this is not a loop 
You're missing that it's also syntax sugar for foreach, which is very close to a traditional loop, and returns Unit. It's often implemented as a loop.
That's why i did not say "implicit conversion" but referred to the pump my library approach. The later is indeed implemented via implicit conversion in Scala but that's just an implementation detail. Regarding the distinction ... it depends on your goal. Implicit conversion is a vast subject covering lots of very distinct usages. If your implicit conversion is between two first-class citizen types, like views, then i agree that there is a distinction. For example if you want to implicitly convert integers into booleans (which i know is a really bad idea) like implicit def int2bool(i : Int) : Boolean = i != 0 then of course we can't really say that boolean's methods are int's methods. But in this approach the conversion target (Boolean here) is a type you care about. It's a first-class type in your mind. Whereas in the pimp my library approach you just don't care about the target type of the implicit conversion. The whole point is adding some methods to some types. For example if i want to add a method `double` on integers: implicit final class MyIntOps(val self : Int) extends AnyVal { def double : Int = self + self } 5.double In this example you just don't care about the type `MyIntOps`. All you care about is pretending there is a method `double` on integers. The goal is not convert Integers into something else, and surely not such a weird type like `MyIntOps`. Most of type-classes in Cats and ScalaZ follow this approach is adding methods to existing types like `|+|` for monoids, `traverse` for traversables, etc. It's not conversion, it's much more like aspects. 
This really did not bring anything new to the table
It is not written in scala, but why not just use [grep](https://en.wikipedia.org/wiki/Grep)?
I tried grep per the suggestion of a few threads, but I just couldn't get it to work. I tried: *sudo grep -i "Sarasota Jazz Project" file1.txt &gt;file2.txt* *sed '/Sarasota Jazz Project/' file1.txt &gt; file4.txt* *awk '/Sarasota Jazz Project/' file1.txt &gt; bar.txt* but all these did was either output empty files or just not change the first file. I'm not quite sure what I was doing wrong. 
You could try cat file1.txt | grep -i "sarasota jazz project" to troubleshoot then route to a new file Shouldnt need sudo tho im guessing
Scala compre to Javascript https://www.scala-js.org/doc/sjs-for-js/es6-to-scala-part3.html I choose Scala.
The best thing you can do to understand theoretical concepts is to apply them in practice. I would suggest writing a pet project where you can practice operations on functional data structures and error handling. A JSON parser might work well, for example. 
*&gt; cat file1.txt | grep -i "sarasota jazz project"* Holy crap, brilliant dude, that worked perfectly. Thanks a lot! I feel kind of dumb now...hahaha but Chris's tool is still awesome and is a great look into filestreams, so I encourage everyone to check it out!
One fun practical FP project I've been working on recently is coming up with a scripted alternative for the manual "Setup your workstation" documentation we have at my company. There are several types of tasks in this process, such as: - Clone a repo - Change permissions on a directory - Install apt package - Personalize config file with your personal info I think cloning a repo is the most important one to keep in mind when trying to weigh the significance of value vs a function that produces a value. Picture the difference in bandwidth of: * Downloading a multi-gig repo, then doing something with the results * Defining a function that downloads a repo, then manipulating that function. I started scripting this as a direct translation of bash commands, using Ammonite Ops (http://ammonite.io/#Ammonite-Ops) &amp; Scala Scripts (http://ammonite.io/#ScalaScripts) . I got the first version of it working, and it was...fine. It executed the commands in the order I had specified, and that was it. Every task would be executed each time you ran the script. At that point, my entire script effectively had the signature () =&gt; Unit. I would run it, it would do *everything*, maybe print different pieces of info, and ultimately return nothing. Though I hadn't actually realized it at this point, each individual task was also a ()=&gt; Unit. If a particular task was acting up, I would add a print statement to it, and then re-run the entire expensive script. The first major change I made was giving each task a return type. I went with Try[String], so now each task was a () =&gt; Try[String]. Now that I had a proper return type, I could start chaining the different sections together, so that they would only execute if the previous step had completed, eg: cloneRepoA .flatMap(cloneRepoB) .flatMap(editFiles) .flatMap(cloneRepoC) Now the signature of my script was closer to () =&gt; Try[String] =&gt; Try[String] =&gt; Try[String] =&gt; Try[String] So you could execute it, it would execute a series of functions that returned Try[String]'s, and it would ultimately return a single Try[String] result. This was certainly progress, but I still had major gripes. - If I wanted to add println's (or any behavior) for debugging, I had to manually add it to every step. - Every step, including cloning the expensive repos, happened every time. Once I realized that all my tasks fit the same rough shape of a () =&gt; Try[String], I could collect them in a Stream. Now my script was a () =&gt; Stream[() =&gt; Try[String]] =&gt; Try[String] With this structure, I could now short circuit the operations in a different way, namely using .takeWhile, rather than directly chaining each operation to the next. val tasks = List( cloneRepoA, cloneRepoB, editFiles, cloneRepoC ) tasks .map{ task =&gt; task.apply } // Still not actually executing here! .takeWhile{ result =&gt; result.isSuccess } // *Now* they will be run and checked Now I could do some fun things to them. To start, I gave each of the tasks a number, via .zipWithIndex. So I had a script that was roughly: // Every item in the Stream is a tuple- (index, theFunctionToRun) () =&gt; Stream[(Int, () =&gt; Try[String])] =&gt; Try[String] Now I could add some decent logging to each step, ala: tasks .map{ (index, task) =&gt; { println(s"Beginning step $index.") val result = task.apply() println(s"Finished step $index with result: $result") result } .takeWhile{ result =&gt; result.isSuccess } Producing: Beginning step 1. Finished step 1 with result: Success("Cloned repoA") Beginning step 2. Finished step 2 with result: Success("Changed permissions") Beginning step 3. Finished step 3 with result: Failure("Could not download repoB") More excitingly, now that each task had a number associated with it, I could keep track of which tasks had executed successfully on previous runs. tasks .map{ (index, task) =&gt; { int lastSuccessfulStep = readFromProgressFile() if ( index &lt; lastSuccessfulStep ) Success("This step was completed by an earlier run of the script!") else { val result = task.apply() if ( result.isSuccess ) writeToProgressFile(index) result } } .takeWhile{ result =&gt; result.isSuccess } I'm happy to talk further about this, but I'll cut it off there for the time being!
It might not be new to you. But everyone does not everything.
I didn't know whether to introduce it in the story I gave you, but at that top of my script, I've got typdef Task = () =&gt; Try[String] So I know that a Task is some function that produces a Try[String] when invoked.
For what it's worth, I find monad transformers troublesome. I eventually ended up using the [eff](https://github.com/atnos-org/eff) library for extensible effects, basically sort of mashing monads together. Pretty practical stuff, imo.
Stack that I typically use: * sttp for http calls * circe for json You may also want to try 47deg/fetch library.
We're using OkHttp and json4s to do similar work. 
Http4s treats response bodies as streams, which are nice for big payloads since you can transform them in-place as you consume the stream. It's a great model, and plays well with circe
Had a look on circe, seems the circe-optics fits my need to do transformation without having local model of the JSON I consume, since I mostly just wanted to transform JSON to NDJSON, with slight modification. I don't know how to decide between sttp or OkHttp as suggested by esquilax, or above suggestion using http4s (I thought this was not client library), and I am not sure about the use case for Fetch. Anyway thanks for your suggestion.
Throttling by akka-stream and flow construction seems very interesting, I will check if I can apply this to my case, thanks. 
IMO don't use spray-json. Circe is much better
Http4s has a client: http://http4s.org/v0.18/client/
Have you thought about using Spark for the transformation part of it? The native output format for dataframes written as json is nd-json/json-l. 
sttp has OkHttp backend.
http4s is built with fs2, which also offers throttling and more. fs2 is a functional alternative to akka-stream.
I concur: "Scala for the Impatient" is appropriate for your goal within your time constraints
There are a few outstanding errors that I need to correct. Please open issues if you see anything amiss.
Hey, Do you just want to mock request/response? You can use MockWs (https://github.com/leanovate/play-mockws) instead of playing with scalaMock/mockito and Ws client. MockWs allow to record demanded response for given requests.
Hmm, on one hand I'd like some reliable compile-time DI for Scala (which inspired me for writing my own DI library), on the other hand I find lifecycle a different responsibility that should be handled separately. However, author compares this to Spring, which is quite tangled with Java EE and its lifecycle issues. In my Play/Akka-HTTP applications I dumped resource management on streams and terminated DB and ActorSystem in shutdown hooks, so perhaps a dirrefent use case.
 Interesting. Please share about your compile-time DI library. Our use cases are mostly coming from runtime-DI such as Google Guice, but its syntax doesn't fit well for Scala, so I developed Airframe. 
Wonder why Scala, and functional languages in general, seem to be flattening out or even declining since 2014.
I wouldn't call it production ready, but here you go: https://github.com/scalalandio/pulp I basically thought, that there is already some build-in DI mechanisms for Scala, and those are implicits. However having all constructors take `implicit` parameters would be a bit too much. Polluting scope with implicits for each type - also not fun. So I defined a separate type-class and added macro annotations and a bit of shapeless to create recipes for those classes, that can be generated without any guessing and for the rest I can provide values manually. Very basic stuff, not production ready, but for smaller and simpler apps (like mine) its passable.
Both iOS languages are too. I'd guess there's something deeper going on.
I wonder what drives you to think you need DI in Scala? Maybe for hybrid OOP / FP apps? I'd rather invest more time learning FP and make use of pure functions (which ironically represents what DI is, eg f: A =&gt; B has a dependency A).
It was an example. Checking the type and then casting ous a common thing to do in JVM langs. The latter part is just done for you. 
It's not, look at his explanation of Ruby's decline. The user base still grows, just at the slower rate. Also, he attributes all of the Jupyter Notebook to Python, even though Scala also has a big share there
I've been working for a while on a programming language targeting Commodore 64 (it can work on other 6502-based machines too): https://github.com/KarolS/millfork It's the first compiler I wrote that goes all the way from the source to the final machine code. Therefore it contains some design mistakes. The language was tailored to the features of the 6502 processor, so the final code is almost as efficient as hand-written assembly. I wrote it for two reasons: writing assembly is long and tedious and the result is hard to refactor, and all the higher-level languages generate large and inefficient code. And before you ask: no, I haven't gotten it to work on NES yet.
&gt; Fibonacci series sum with memoization Do it algebraically using the forumla for the n-th Fibonacci number and then you don't need any memoization ;)
Thanks /u/Andalo :) It uses Mockito (no harm in that). It encapsulates WS really nicely.
We added links to the libraries in the article. Please review it.
As a neutral outside observer, I feel like there isn't a super strong "dev marketing" push for Scala. I see lots of posts on HackerNews, Medium, Twitter feeds, etc. on Go, Kotkin, Rust, Python, etc My general takeaway is Scala isn't gaining traction in SV and startup land. 
This is awesome, thanks! Although you should have used Lucidchart to make the diagram since we actually use cats ;)
I don't have experience with Netty directly. Most Java libraries/frameworks are usable from Scala but not necessarily idiomatic. While using Netty for this is not wrong, I have to ask what you expect to gain from doing so, and whether using a native-to-Scala library for the same thing like akka-io might not be better?
First of all, thanks for your answer. I want to create a streaming topogoly process. What is akka-io? Thanks
akka-io is a Scala library for I/O (including networking), part of the akka umbrella.
True but he apparently tried that and it didnt work, plus pupes are cool
Good point. Actually Airframe can use functions to inject dependencies in the form bind{ A =&gt; B }, in which A is injected. Another direction is using reader monad, but if you need to compose a lot of functions in Scala, it has some performance penalty and makes the code a bit difficult to read. In Airframe, a good thing is that you can manage Scala traits as if they were functions in Scala, because all parameters in a trait can be injected by the framework. For example, if a trait has parameters P1, P2, ... bounded by Airframe, the actual behavior is almost the same with calling a factory function: (P1, P2, P3, ...) =&gt; a new trait instance. 
Just the breeze package for linear algebra? 
I've been writing a web UI library inspired by React and Elm for Scala.js called [Domino](https://github.com/snordgren/domino). It's a small and lightweight alternative to scalajs-react. HTML generation is fast and typesafe, and there are no components, favoring an Elm-style architecture. What do you think? 
At least in Scala's case, it fairly closely tracks the success of Kotlin. Scala's collapse since 2016 and Kotlin announcing version 1.0 might only be a coincidence though. Google trends suggests that (after a huge spike around Google's announcement) interest in Kotlin will permanently surpass interest in Scala in a few months. The PYPL index suggests the same, with Scala being on position 15 and falling, and Kotlin being on position 17 and rising.
I believe Doobie tries to do stuff like this for you out of the box
Finagle and TwitterServer, two service frameworks that Twitter uses to power it's services, are backed by Netty. It works.
yes, I took a fast look to their source code, they have the error code mapping to a SqlState (https://github.com/tpolecat/doobie/blob/series/0.5.x/modules/postgres/src/main/scala/doobie/postgres/sqlstate.scala#L146) but I haven't found if they extract the data as I did, anyway, I haven't tried to use doobie until now and they might do better what I did.
Great suggestion. Thank you I didn’t even think of that or the slack channel that they probably have somewhere.
You can write scala code that directly uses netty, but I'd recommend against it unless you have a lot of netty experience. Finagle is a much easier to use abstraction.
We used Netty because at the time, the was no mature http library in Scala. But working with a Java API in Scala is always a frustrating experience, so today I don't see any reason to use Netty over Akka http.
There is also [blaze](https://github.com/http4s/blaze), but i do not know how it compares to akka-http.
Probably Play WS, Akka Http, or Http4s for websocket handling. [ScalaJs React](https://github.com/japgolly/scalajs-react) is probably the most well maintained/supported frontend library. There's also [Binding.scala](https://github.com/ThoughtWorksInc/Binding.scala) for a pure Scala.js approach to data binding. [ScalaTags](https://github.com/lihaoyi/scalatags) is quite popular for markup. Tons of json libraries available as well. You certainly can build SPAs with Scala.js, but not everything is supported (Angular, for example, has no modern option available, just the 1.x series).
I am going to try finagle. It sounds very promising. Thanks a lot.
This is how I learned Scala: http://aperiodic.net/phil/scala/s-99/.
Thanks!
There is https://olivierblanvillain.github.io/monadic-html/examples/ for a pure Scala.js solution
enlighten us bit more. You mean if used proper tail recursive scala code it will be as efficient as recursive with memoization.
I find free coproduct style easier to understand and work with - my domain operations are plain values, and then I run them however I see fit. Whereas tagless final style feels more like passing in a bunch of callbacks (because at implementation level that's what it is) and so it's less clear what's going on. In theory tagless final should perform better, but I'd only adopt it in cases where I knew the performance of free was going to be an issue, which I haven't had happen yet.
I have done a lot with Free but at this point I'm pretty convinced tagless/MTL style is easier. [This talk](https://www.youtube.com/watch?v=1h11efA4k8E) by Luka Jacobowitz is a long worked example and is very good.
I've successfully mocked json responses in a project, this is the general idea of what I did: val ws = mock[WSClient] val responseBody = "{...}" ... "availableBooks" should { "retrieve available books" in { val expectedBooks = "BTC_DASH ETH_DASH USDT_LTC BNB_LTC".split(" ").map(Book.fromString).map(_.get).toList val request = mock[WSRequest] val response = mock[WSResponse] val json = Json.parse(responseBody) when(ws.url(anyString)).thenReturn(request) when(response.status).thenReturn(200) when(response.json).thenReturn(json) when(request.get()).thenReturn(Future.successful(response)) whenReady(service.availableBooks()) { books =&gt; books.size mustEqual expectedBooks.size books.sortBy(_.string) mustEqual expectedBooks.sortBy(_.string) } } } The complete test: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/test/com/alexitc/coinalerts/services/external/BinanceServiceSpec.scala
One 'pitfall' I have encountered with using mtl/free is choosing the right granularity for operations. For example, imagine an application with a database. We could model our operations as very simple `get(table: Table, key: Key): F[S]` and `put(table: Table, key: Key, value: S): F[Unit]`, on which we can build more meaningful operations. This has different implications than if we modeled it with operations with more clearer goals: `getPictures(album: Album): F[List[Picture]]`, `getAlbums(): F[List[Album]]`. There is no right or wrong choice, it depends on what you want to get out of abstracting away your effects. In the first case you can test a lot more logic as unit tests, but you must basically remodel the idea of a database in these tests. The question is if this is needed. In the second case you abstract away a lot more, which means unit testing the logic becomes simpler. But this means that the actual effectful code in the handler will be larger and thus will need to be tested more thoroughly. But this doesn't really have anything to do with mtl vs free in itself, as it applies to both. The actual difference between mtl (n^2 instance problem) and free (only algebraic effects, possible performance loss) will probably not be as relevant when you are only beginning with experimenting with them. I suggest you try them all and use the one which syntax you like the most.
In the graph of the linked article.
One of the nice things about Elm is the "guarantee" of no runtime errors. Is that possible to deliver using some subset of Scala.js? 
Spark and MLLib are written in scala. Tons of nlp stuff is written in java. 
It's possible. I did some Scala for ML at my current company because it was the language I liked the most and we're mostly a jvm shop. So interoperability was easy and Scala made it easier to experiment. As mentioned, Spark ML is good and there are Java ML frameworks. But I did eventually switch our ML projects to put Python for various reasons. I would encourage you to try it out with Scala. It may be sufficient for most needs but you may out grow it. I'm really hating Python but you can't argue with the ML/NLP tooling around it.
I think it means other new JVM languages, https://www.ibm.com/developerworks/java/library/j-jn1/index.html?ca=drs-
One caution is that Spark is really limited to ETL and iterated mapreduce types of operations - the former buzz around it as The Data Science Solution has died off, and it comes with a lot of overhead. MLlib on a Spark cluster is often slower than other tools running on a single node, and it’s now mostly a sales tool for Databricks. It’s better than Hadoop, but worse than other tools. We use Spark in production for a lot of stuff, but ML happens outside Spark. Many ML libraries are using C or C++ for the bulk of the work, and provide wrappers for R and Python. Unfortunately, JVM support is less common, with H2O and xgboost being the big exceptions. Python has a much richer ecosystem than Scala, particularly in NLP. I personally don’t like working in Python, in part because I really like functional programming and most Python stuff just doesn’t work that way. There are exceptions like Keras and Apache Ray that encourage functional style and are also just really cool tools to play with. 
Have you looked at all at [Flink Machine Learning](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/libs/ml/index.html)? We're using flink a lot at work and are considering the ML package for future projects.
How does Flink compare with Akka Stream? Is it possible to use it on a single node (does it make sense)?
Compared to Python it is quite bad. Lucene analysers, which you can use for doing some text preprocessing have terrible API. Spark ML requires Spark runtime, which you usually do not want. Then there is Smile library, which looks very good on first look, but misses things like handling sparse data (you need that when dealing with text) and L1 regularization (again often needed when dealing with text). DL4J is sort of joke compared to any serious deep learning library. On the bright side you can load Tensorflow model and make predictions in Scala (and in many other languages). With some hacking effort, you can even train the model in Scala, but you need to setup its structure in Python. Then there are some less used libraries like MXNet also available from Scala. Your best bet, when you want to use Scala is either of following: - write required stuff from scratch (strongly not recommended), only good thing here is that you do not need to reimplement whole scikit-learn, only the portions you need (like sparse linear models) - use some less known library like MXNet - do predictions (or optionally training) in Scala using Tensorflow
I used to work at a company that did this. Everyone used Spark because that was where the data was, but often our data scientists would implement something in Python or R first, and then we'd port it to Scala as and when we needed tighter integration with the rest of our code and/or to "productionize" it. I got the impression the Python libraries were richer in theory, but I found everything our data scientists were actually using was available in MLLib. I maybe had to implement one or two distributions by hand that were builtins in Python, but we're talking one-liners. Of course that may not be true in every domain.
IMO both final tagless and free monads allow you to do both. You can have a high level instruction set that uses a lower level one. With your example you can easily have trait StorageAlgebra[F[_], S]{ def get(table: Table, key: Key) def set(table: Table, key: Key, value: S) } and trait PictureAlgebra[F[_]] { def getPictures(album: Album): F[List[Picture]] } and then your implementation of `PictureAlgebra` could require a `StorageAlgebra` as a parameter. class FancyPictureAlgebra(dbAlgebra: StorageAlgebra[IO]) extends PictureAlgebra[IO] { /// } The nice thing about this is that if you ever want to change the storage mechanism from `Postgres` to `S3` you have to do only 2 things: - implement a `S3` version of your storage algebra - pass it as a parameter to your fancy picture one
Hey there, I recently wrote an article and took some of the learnings we did at work and documented it here https://medium.com/@calvin.l.fer/deferring-commitments-tagless-final-704d768f15cb Both Free Monads and Tagless Final have their pros and cons. Tagless Final has a lot less boilerplate compared to Free but with frameworks like FreeStyle you have some nice options
Not a Scala library, but you can always use Java's ND4J. Check https://nd4j.org/tensor for tensors. (3D matrix is a tensor). It also has a scale wrapper - https://github.com/deeplearning4j/nd4s. You may also use scala's breeze - some docs on the data structures can be found here https://github.com/scalanlp/breeze/wiki/Data-Structures. Though there is no implementation for Tensor3 yet, you may be able to do what you want with matrices and vectors combos.
Do you mean better in general? Would you use it for doobie if you were to write it today?
If you want deep learning and scientific computing, there are libraries like Scalnet and ND4S, which are linked with Eclipse Deeplearning4j. https://github.com/deeplearning4j/scalnet https://github.com/deeplearning4j/nd4s 
&gt; But I did eventually switch our ML projects to put Python for various reasons. What are these reasons?
I have a lot of experience with ML. I looked at Spark MLlib a couple of years ago and from what I could tell, MLlib was pretty limited in the kinds of models that you could work with, and there were idiosyncrasies in the stuff that was implemented. I think you might end up having to reinvent some wheels and smash some square pegs into round holes if you really want to use MLlib. My advice is to use Python or R. Just my 2 cents, as I say 
Currently the advantages of Tagless is that it's encoding is more ergonomic, and performant. Free on the other hand makes it much easier to inspect and rewrite programs. For most people I think tagless final encoding should be their default, but I have found cases where reasoning about programs others write with Free to be incredibly useful. Particularly when you want to validate, serialize, or optimize those programs given to you.
I see. I suppose one advantage of Free over Tagless is that Free is stacksafe while Tagless isn't. I suppose you could set the type bounds for Tagless's type parameter to something like a Monad but then it feels like you constrain yourself to that particular Monad's implementations. Not sure if this impacts performance though.
This is true, I've personally never came up with that limitation in practice. In typical usage you're using something like `IO` in large algebras and programs and using more pure effect types in testing.
Avoid nulls, exceptions, casts, and take care with matches and generics, because the Scala type system is unsound (for now), and you should be alright. Scala.js also lets you share code between client and server, and lets you use the ecosystems of Scala, Java, and JavaScript. Elm is a great language but I think Scala.js has huge potential.
Yes, I actually have a WIP rewriting the doobie internals in tagless style. It ends up being much nicer when you're composing effects.
Hmm I don't see a link between this and Elm-style architecture. Perhaps a couple examples of dealing with state / dynamic data would help. ~~Having glanced at the source code, it seems like the general idea is similar to virtual DOM, except – if I got it right – without the virtual part? So in your example in README, to update the DOM tree you would call `page()` again (let's assume it will return different data this time), then call `.render(newPage, sameRoot)` with the result. So then, ScalaTags will create a new DOM element in `page()`, and then domino will try to... update the previously rendered element with data from the new element? Unless I'm misunderstanding what's going on, this would have poor performance when updating nodes with many children/descendants (avoiding unnecessarily creating real DOM elements is the reason for "virtual" in virtual DOM).~~ EDIT: Actually, I got it wrong initially. I saw ScalaTags in build.sbt, but it looks like it's only used for testing... Looked some more at the source, and this looks more like virtual DOM, although you're not tracking previous virtual elements for a given node, which could improve performance by reducing the amount of work needed for diffing, I think. --- Bug report: you're not doing the required HTML escaping when building HTML strings, e.g. here: https://github.com/scalacode/domino/blob/master/domino-core/shared/src/main/scala/org/domino/Attribute.scala#L17 this will cause invalid HTML and security issues when rendering user content.
Serialize to comma separated values CSV file. Each line is 1 object's fields values put as string, separated by comma. Excel loads that natively
React uses components that have lifecycles, while Elm uses pure functions that render HTML based on the contents of a model updated using messages in response to events. Domino implements the "render HTML" part, while another library like [Diode](https://github.com/suzaku-io/diode) would implement the state management. I'll write some examples when I've worked through the current batch of issues. There's no tracking of the previous render tree, that's correct. Right now each render traverses the DOM and updates every single node, in the future it will diff against the previous tree. Good catch on the HTML escaping, I'd forgotten about that. Should be in the next release. Thank you for taking the time to look through my code!
Great article. I have a question about the usage in your for comprehension. In your Dynamo example, does it still work like a `Either` inside of `F`? You don't need a monad transformer for that? Or are you only flatmapping on `F`? 
So in the Dynamo example, I actually take the Either and flatten it out using Future's error handling capabilities so any Left(DynamoReadErrors) become Future failures. Alternatively, you could use a EitherT monad transformer if you want to preserve both effects. At work, if there's a deserialization failure, we just treat it as a critical failure which is what the Future failure mode represents. 
[removed]
I think your worksheet means something else then Excel then, can you specify? 
Create a package and you can put your classes in any file inside the package as a top-level definition. 
Ok so I'm currently learning Scala as well and tried to figure this out. (If I understand correctly what you mean. :D) Disclaimer: This is just what I figured out. There is probably a better way. I have a package folder "example" and in it a testobject.scala file and a worksheet. example |_testobject.scala |_worksheet.sc The testobject file looks like this: package example object testobject{ def number():Int = 5 } When I have compiled this then I can import testobject in the worksheet like so: import example.testobject testobject.number()
Sounds like you need to check out Monad transformers. Converting `Try` and `JsResult` to `Either` should be trivial, so you'd probably just need something like `EitherT[IO, Error, A]` or `EitherT[Future, Error, A]`: https://typelevel.org/cats/datatypes/eithert.html
&gt; Do others have this problem? How do you deal with it? The ideal solution, would be to have a programming language with extensible effects as a core language feature. Then every library maker would create their own monadic effects within the same standard framework, and the world would be a better place. Back to harsh reality: You could use one of several available libraries for extensible effects. Each requires some investment. &gt; The problem I see with this is that not all the contexts might be smoothly converted to Either. For example, It might be tricky to convert a result in Future context to an Either context. Also, It just a little bit like the language is forcing me to write extra boilerplate before I can sanely work with the values I need for computations. If you are adventurous enough to use an experimental library, you might consider my pet project: [Skutek](https://github.com/marcinzh/skutek). - Either - You use `.toEff` extension method to convert any `Either` value to Skutek's `Error` effect, which will then gluelessly compose with other Skutek's effects. - Try - Similar method should be provided, but as of now it's not. A handful of lines will fix this. - Future - Same as above, an omission (I'm treating this thread as accidental feature request). Skutek has `Concurrency` effect, a simple wrapper for Scala's `Future`, whose sole purpose is to allow composition with other Skutek's effects. - JsResult - I'm not familiar with Play, but I took a brief look at the source for JsResult. It seems to me that the solution would be to create new Skutek effect, by taking Skutek's native `Validation` as a template. `Validation` uses an `Either` istance as underlying data, and in our new effect it should be replaced with `JsResult`. 
So you have 4 different monad effect types you're working with. You can either eliminate some those effects, or your can stack them. Or you can do both. You've already hinted at there being an isomorphic relation between `Try`, `Either`, and `JsResult` and reducing these down to one type is probably a good idea. It's also possible that some of the method returning these types don't actually care about what kind of Monad they're working with. In these cases you should parameterize over the monad you're working with. Let take for example a function that gets a user, and then finds the first comment made by that user. def getUser(name: String): Future[User] = ... def getComments(user: User): Future[List[Comment]] = ... def getUserFirstComment(name: String): Future[Option[Comment]] = for { user &lt;- getUser(name) comments &lt;- getComments(user) } yield comments.headOption In the above example, `getUserFirstComment` may seem like it need to be a `Future` because `getUser` and `getComments` are Futures but it really doesn't have to be. The first thing we'll do is remove the explicit dependencies on the above methods. def getUserFirstComment(getUser: String =&gt; Future[User], getComments: User =&gt; Future[List[Comment]]) (name: String): Future[Option[Comment]] = for { user &lt;- getUser(name) comments &lt;- getComments(user) } yield comments.headOption Now that we don't explicitly depend on a particular implementation of `getUser` and `getComments` we can parameterize over information we don't need to know. Specifically we don't need to know that either function uses `Future`. To parameterize over `Future` we're going to a higher kinded type `F[_]` and constrain that type to be a `Monad` using a typeclass(you can find a definition in scalaz or cats). def getUserFirstComment[F[_]: Monad](getUser: String =&gt; F[User], getComments: User =&gt; F[List[Comment]]) (name: String): F[Option[Comment]] = for { user &lt;- getuser(name) comments &lt;- getComments(user) } yield comments.headOption This version of `getUserFirstComment` is completely independent of any dependencies or explicit effects but in reality we'd probably stuff this method into a module with `getUser` and `getComments`. When doing so, we can parameterize over effects at the module level. trait UserOperations[F[_]] { def getUser(name: String): F[User] def getComments(user: User): F[List[Comments]] def getUserFirstComment(name: String)(implicit F: Monad[F]) = for { user &lt;- getuser(name) comments &lt;- getComments(user) } yield comments.headOption } Here in module form, we only constrain `F[_]` to be a `Monad` when we need to, specifically when we call `getUserFirstComment`. It is typically best to constrain effects to only what you need, other constraints you might use would include `MonadError`(a monad that has some kind of Error path that can be recovered from, this includes all 4 of the effects you mentioned above), `Traverse`, and `Applicative`.
I often read this point and dont get it to be honest. Doesnt it depend on the natural transformation, so to say, on your target monad? 
Found interesting blog in combining monad: http://immutables.pl/2016/05/14/for-comprehensions-with-combined-monads/ Haven't read it thoroughly, but it seems to be more than your usual 'at most 2 context' post, since I saw transformation helpers to create Result from ofTask, ofOption and ofEither. 
If you only have these four Monads you might need to check out Monad Transformers. Now if your real stack is larger then you might want to check out Eff -&gt; https://github.com/atnos-org/eff
Well at the end you kind of end up with an acyclic graph of components and I normally use constructors and typeclasses to represent dependencies - for example [here](https://github.com/SmartBackpacker/core/blob/master/api/src/main/scala/com/smartbackpackerapp/repository/PostgresVisaRestrictionsIndexRepository.scala) - . What I don't get is why would you need to bring more dependencies to magically inject instances (either at compile or runtime time) when one can compose them purely and nicely by using language features such as implicit resolution, super powerful.
For number eight, you want to yield(assign to new Val) a list of numbers corresponding to an index of the negative elements in the initial array omitting the first negative number. Then using a for loop, you will use the remove method each of the indexed positions created during the yield. If you figure out the code for that, let me know and I’ll spend the time explaining number nine. 
They appear to be a C# dev based off their other followed pages. Learning object oriented Scala still adds a lot of benefits making Scala for the Impatient a viable book for someone transitioning from an object oriented background. Definitely agree that Functional Programming in Scala is a better book but it conceptually takes longer to become proficient as a PFP. 
From a beginner standpoint, I've encountered far more devs struggling to transition out of bad habits than OO developers transitioning into good ones. I recommend the book not to learn FP (though, FP is nearly synonymous with good design patterns and habits, with some caveats), but to learn how to structure a program correctly using what Scala offers. 
`JsResult` accumulates errors, like cats' `Validated`.
Does this work? ``` Mytask := Mytask.dependsOn(package).value ```
I forgot to say that MyTask is an input key task so I already tried this Mytask := Mytask.dependsOn(package).evaluated But it seems to just run the package for the root project and not run a normal "sbt package". Maybe the problem is the scope but I can't figure it out.
[removed]
why did you feel the need to give us both nonsensical and its definition in the title?
They have given up on the Scala dream... Not a lot of big players support Scala, which is part of the reason I have personally moved on. I am fine using a niche LIBRARY, but it is really hard to use a niche FRAMEWORK or especially a niche LANGUAGE.
that seems a little over the top. Lagom is written in Scala, they still employ the Scala compiler team, they are still releasing scala libs (scala kafka stream lib). I don't think there is any need to worry but also Lightbend is not Scala so even if they did, who cares?
I don't at all share your opinion that Scala is a "Niche Language". The demand for Scala developers alone is a testament to that. I do agree that Scala's opportunity to "kill" Java on the JVM is long past; Java holds on as "the least common denominator" on the JVM. Lightbend appears to be taking a very practical approach to its frameworks and offerings recognizing that Java interop is really important and they are leveraging Scala to do that.
&gt; Lagom Never heard of it and had to google it. Apparently a micro web framework? Great, we need more of those. 
its what the op referenced in his post. its based on play, it seems to me to be Lightbend core product. Might help to know what is coming out of a coming before proclaiming anything about them lol.
According to IEEE "The Top Programming Languages 2017" Scala is #15 (https://spectrum.ieee.org/static/interactive-the-top-programming-languages-2017) Another ranking: https://research.hackerrank.com/developer-skills/2018/
So honestly to me, something not in the top 5 or 6 is not "mainstream". You are obviously welcome to draw the line wherever you want :) 
Thanks guys for the input. I need to do a bit of evaluation and will be back here with my findings/decisions, in a week or two.
If Python's performance is OK for your use, then it definitely is a very, very nifty language. I'd say that the biggest win in Python - a win that's perhaps not widely appreciated - is the ease of tweaking the language itself to your needs. It is possible in Java but takes much more work and involves use of introspection and compilation APIs that aren't a core building block of the language. In Python, introspection is at the base of the language, and doesn't require anything fancy. Not only introspection, but also ease of modification of object semantics - via special attributes and decorators. Python decorators are much more powerful than Java annotations.
How about Performance of Scala and its **memory usage**? Is it OK? 
OP, this is an interesting question, but I suspect nobody on r/scala is really qualified to answer it. Maybe you can somehow ask someone at Lightbend directly? Just thinking out loud here.
&gt; If Python's performance is OK for your use, then it definitely is a very, very nifty language at its core - never mind the oodles of libraries. Indeed. I spend something like 4% of revenue on server costs. Some of that is stuff like Postgres which would not change going to Scala. Maybe I would get it down 1%.. but I would easily lose that in cost to hire of 1 developer. &gt; In Python, introspection is at the base of the language, and doesn't require anything fancy. Right, you basically get duck typing everywhere for free. Adding in MyPy feels a little bit like Scala typing, but obviously still some rough edges around library support ;( &gt; I personally lament the relatively tween-age state of Cython's type inference Very true. I THINK PyPy could be 80% as good as the JVM if someone cared. 
So, JavaScript and Ruby are not mainstream. Ok then.
I certainty wouldn't start a company coding my backend in JS, no. If you want to see how that is going, check out the disaster over at walmart.com (walmart labs) Personally I would not pick Ruby over Python, but I would pick it over Scala.
I was going to edit it and point that out, but figured it'd be a waste of time.
&gt; https://spectrum.ieee.org/static/interactive-the-top-programming-languages-2017 php, also not mainstream apparently
It would help if you included which version of sbt you're using and explain how you're verifying package isn't running first.
JS is a mainstream language regardless of how good it is or how much you like it.
I can think of two use cases: - Object life cycle management. When we need to use a shared singleton instance, we need a help of DI since FP does not care about the object life cycle. - Reusability. If you need to re-use a certain set of bindings in multiple locations, defining this set as a Scala trait is useful (e.g., sharing a thread pool between multiple components http://wvlet.org/airframe/docs/use-cases.html#service-mix-in) 
There is nothing wrong in using constructors as long as the number of arguments is small enough, and Airframe supports constructor injection for this use case. If you need to pass 5 or more constructor arguments, remembering constructor argument order will be a bit difficult and also passing and preparing concrete instances for the constructor can be cumbersome. 
Would you mind sharing your experience with Scala in your previous startup and Python in current one? What was the domain? What libraries/frameworks used?
Sure! Scala was Play for web tier, ScalaJS for frontend, Akka for backend processing pool. Python was Django for web tier, Celery for backend processing tier. Domain: B2B SaaS Apps Scala Upsides: * CPU usage of running servers was very good * Type system was great, having type checked map / filter was great. * Many solid Java libraries to use, though many take work to be Scala friendly Scala Downsides: * Memory usage was a bit crazy to start, as a brand new startup I wanted to use little 512mb VMs to POC, and that just wasn't going to happen. So had to get on pretty beefy servers just to make things happen. * No real comparison for Celery. Celery is not perfect, but it works day in and day out. I ended up had to roll something with Akka, which was cool but wasted a lot of times. * HTML forms and model integration are terrible compared to Django. Why do I have to define my "case class" 3 different times, and then list out my exact field that maps to * Didn't much care for having to mess with futures. It is a nice way to add concurrency, but it can be a lot of work for a young company. Servers are cheap enough, I can just throw 1 thread per request and scale for a pretty dang long time (in my b2b space, I could probably make 1 million / year for an average of 10 requests / second, or 1 very simple VM) * First fire took something like 2 months to find. First hire in Python shop took 2 weeks. To me, Python is just a really good get stuff done language. In the B2B space, the performance is generally just fine.
They might be referring to JVM features that come with Java 8.
Unless you want to turn it into a command, which would emulate you typing in "package" into the shell, you'd have to manually aggregate the non roots. Here's what I've done using ScopeFilter. lazy val packageAll = taskKey[Unit]("package all the projects") lazy val myTask = inputKey[Unit]("foo") lazy val root = (project in file(".")) .settings( scalaVersion in ThisBuild := "2.12.4", packageAll := { (packageBin in Compile).all(nonRootsFilter).value () }, myTask := { packageAll.value } ) lazy val sub1 = (project in file("sub1")) lazy val sub2 = (project in file("sub2")) def nonRootsFilter = { import sbt.internal.inc.ReflectUtilities def nonRoots: List[ProjectReference] = allProjects filter { case LocalProject(p) =&gt; p != "root" case _ =&gt; false } def allProjects: List[ProjectReference] = ReflectUtilities.allVals[Project](this).values.toList map { p =&gt; p: ProjectReference } ScopeFilter(inProjects(nonRoots: _*), inAnyConfiguration) } 
Startups don't have time to teach a language. You need to get going fast or you are too late. Python codes have tons of type info. For example typed method signatures via gradual typing, and unit tests that enforce types in CI.
They announced probably 2 years ago that they were moving towards better supporting Java, although that doesn't necessarily imply they're doing any less Scala. Their Java APIs were absolutely atrocious at the time.
I'm the author of the Finch template for Swagger Codegen (the link you posted is code generated by this template). Finch doesn't currently support generating swagger docs (there's an issue requesting this feature: https://github.com/finagle/finch/issues/73). Swagger annotations really only work when applied to methods, so you might be able to tweak the template so you could apply annotations. Just know that anything that's `Option[A]` in Scala has to be explicitly documented since the annotations in Swagger Core are Java based. A Finatra server would be much easier to modify for Swagger support (I do this at work). I've not released a Finatra template yet, but you can modify the Scalatra template pretty easily, then extend Finatra's RouteDSL to register swagger definitions when you register routes. I would think it would be quite a bit of work to make Finch code less finch-like. For Finch, I would suggest maintaining a json or yaml schema document manually and serving the file directly.
I was just wondering that. I Was looking into using slick after postgresql-async was abandoned, and found that slick also seems to be abandoned. Going to lightbend.com I see lots of lagom java stuff, but not a single link to slick. If they quietly abandon products, it makes me really nervous to use their other products.
Lagom targeted companies with complex Java enterprise monoliths who wanted to migrate to microservices. Companies using scala rarely had that problem so Java APIs first made sense to them. 
I think it's more like: They used to be seen as "the Scala company", which hurt adoption of their multi-language products (Akka, Play, Lagom all have Scala and Java APIs) in non-Scala enterprises. People would think: We don't use Scala, so we can't use their products. The point they wanted to make is: Yes, we fund Scala development, use Scala internally, build products for Scala devs, but we also have products for enterprises using Java, so even if you're a big Java shop with no plans to move to Scala, we might have something for you. 
I think so, yes, or probably not even specifically Java 10, more the future development in general - Project Amber, Project Loom, Graal VM etc. 
https://media.giphy.com/media/xT9IgHCTfp8CRshfQk/giphy.gif
we can use the industry and academic definition for what we mean by "functional programming". Or you can use Martin Odersky's definition and start renaming everything. I'd rather get on with doing real work and let other people argue about names.
I have to disagree. For singletons I would just use a Scala `object`. And Execution Contexts are normally resolved implicitly (eg: https://github.com/typelevel/cats-effect/blob/master/core/shared/src/main/scala/cats/effect/IO.scala#L534). If you have a constructor with 5 or more arguments you don't really need to remember the arguments order, just look at the definition, that's why it's nice to have types. But having constructors with many arguments could also be a code smell. Sorry if it sounded harsh, I'm not trying to be mean, just giving my point of view. I just don't think a library is needed for this but I understand that we can disagree here.
I said "seems to be abandoned". The factors that gave me that vibe were very few merges into master in months, pull requests sitting around unmerged, issues stacking up with none closed in months, and people asking in the gittr channel where all the maintainers have gone. Postgres-async is in even worse shape. The fact that it took two months to merge the 2.12 support should have been an indicator[1]. Since then there's largely been no activity at all, including very simple merges such as updating netty[2]. 1| https://github.com/mauricio/postgresql-async/pull/200 2| https://github.com/mauricio/postgresql-async/pull/223
Slick has been nearly completely abandoned as shown by the 74 outstanding PRs. It's not debatable. Lightbend continue to be demonstrably unable to facilitate a contributing community, they just change their priorities and ghost it like a bad tinder hookup. It's a real concern for any business thinking of betting on their technology long term and the root of the problem lies with Odersky's autocratic nature. It isn't going to get fixed soon is it.
would've prefered: - performance - better http client 
One of the big differences between Akka Streams and Flink's model is that, except for [iterations](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/batch/iterations.html), Flink's data pipelines are acyclic (Directed Acyclic Graphs) whereas of course Akka streams are a more general directed graphs that can have feedback loops. The other big difference is that Flink has very very nice tools for really complicated windowing logic, watermarking. Maybe someone has put out something similar that sits on top of Akka Streams but I haven't come accross it. It also has really cool support for managing state with exactly-once mutations as well, but I think you can do that with Akka Streams if I'm not mistaken? If I were you I would definitely have a look through their docs on [streaming](https://ci.apache.org/projects/flink/flink-docs-release-1.4/dev/datastream_api.html), there really is a lot of really amazing stuff there.
Could you make a post about what idea to use and why and When to use ide
What happens when you try. If you have the idea scala plugin it should auto config. Then 'sbt run' from the cli should as the readme says.
Wow so many downvotes, Shame on you reddit Scala. Here is how you do it: 1) Install java 8 2) If you have Intellij already, you need to make sure you have the Scala plugin. (File -&gt; Settings -&gt; Install JetBrains plugin... -&gt; Search for Scala -&gt; Install). 3) Close Intellij 4) edit with a texteditor project/build.properties to sbt.version=1.1.0 (see https://stackoverflow.com/questions/46827010/no-scala-library-jar-in-every-new-intelij-scala-project) 5) delete the folder .idea if it exist (hidden folder) 6) On the menu bar: File -&gt; Open -&gt; select sangria-akka-http-example (It's a sbt project, Intellij knows how to import it). 7) open build.sbt 8) Click import sbt project (popup on the bottom right of the screen) 9) On the menu bar: Run -&gt; Debug... -&gt; Edit Configuration (popup) -&gt; + (top left of the popup) -&gt; sbt Task Name: Debug Server Tasks: run 10) Click Debug on the popup 11) On the console window you will see: Connected to the target VM, address: '127.0.0.1:39953', transport: 'socket' [info] Loading settings from plugins.sbt ... [info] Loading project definition from /home/gui/sangria-akka-http-example/project [info] Loading settings from build.sbt ... [info] Set current project to sangria-akka-http-example (in build file:/home/gui/sangria-akka-http-example/) [info] Compiling 3 Scala sources to /home/gui/sangria-akka-http-example/target/scala-2.12/classes ... [info] Done compiling. [info] Packaging /home/gui/sangria-akka-http-example/target/scala-2.12/sangria-akka-http-example_2.12-0.1.0-SNAPSHOT.jar ... [info] Done packaging. [info] Running Server 12) Set a breakpoint at line 25 in src/main/scala/Server (`val JsObject(fields) = requestJson`) 13) On annother terminal: ``` curl -X POST localhost:8080/graphql \ -H "Content-Type:application/json" \ -d '{"query": "{hero {name, friends {name}}}"}' ``` 14) Bingo you should see Line 25 is hit
Stack overflow trend for Scala is still fine: https://insights.stackoverflow.com/trends?tags=scala%2Cclojure%2Chaskell%2Cerlang So probably it's just not used in open source projects :D
&gt; The root of the problem lies with Odersky's autocratic nature and because of that it seems likely that it isn't going to get fixed. This sounds like hogwash. Slick is a Lightbend project which is completely seperate from EPFL/Scala Center, which is where Martin works. 
I had no problem doing this straight away. I'm using latest IntelliJ IDEA CE 2017.3.4 and Scala plugin: - clone the repository: `git clone https://github.com/sangria-graphql/sangria-akka-http-example.git` - launch IntelliJ - in the start screen choose 'open' - find and select the _directory_ `sangria-akka-http-example` (the icon has a tiny bronze circle, indicating that IntelliJ sees this is an sbt project). - confirm the next dialogue (if you've never used this, check that you have a JDK installed and selected) https://imgur.com/a/77ozu
When I do it it works, even when I import it into a worksheet.
Thanks, I will go through the streaming doc. Most of my cases are acyclic.
It looks like those line numbers don't match up. Make sure your code is compiled before evaluating the worksheet. IIRC there's a toolbar toggle button that controls whether your code will be auto-compiled before evaluating the worksheet.
Has anyone noticed scalapar only posts language trolls? https://www.reddit.com/user/scalapar 
Whats happening with Slick? It seems there's very little progress all around
This may be unconnected but it is wrong to compare a Double to zero using "==" operator
How much of this can be adapted to Scala? Since the compiler does a lot of optimization of its own and we cannot control the compiler behaviour. 
Wrap it in a future and pipeto sender. You'll want to tweak the dispatchers thread pool. There's a chapter on how in the Akka docs. I think I googled Akka blocking io or something
&gt; better http client If by this you mean faster artifact resolution by default, this appears to be [actively](https://github.com/sbt/sbt/issues/2997) [worked on](https://github.com/sbt/librarymanagement/pull/190).
Nice!
3 months? 
3 years? 
I've been working on an article about using Scala to extract data from Salesforce REST API. I am a beginner using the language. Would appreciate feedback from you guys :) “How to integrate Salesforce data into your Data Warehouse using Scala” @WesleyBatistaS https://medium.com/@wesleybatista/how-to-integrate-salesforce-data-into-your-data-warehouse-using-scala-9959026b5f6 
I found a solution using Scalamock only. Please do check: https://stackoverflow.com/a/48495761/69746
You can also be ambitous and create your own event-loop(-ish) file reader using java nio AsyncFileChannel. There is possibility to add callback after reading a batch so you could pipe data to your actor (this way you do not need to provide different dispatcher). But in reality simpler, blocking solution using dedicated threads is enough i think.
I try to always support the two most recent stable platforms. So in transition period to 2.12, I built against 2.10, 2.11 and 2.12, now dropping 2.10 when updating a library. That way most people will be able to use your library, and if there happens to be a major regression in one version, you still have a backup. 2.11 also has the advantage of running under Java 6 which may be an issue for Android or supporting not-so-update-to-date users on macOS.
3 story points.
I understand where you're coming from, but I believe we're operating on different notions of what it means to be a "good" book to "learn" the language. By "good", I mean that it passes on teaching the ins an outs of the Scala language, which are by and large *fundamentally useless in both real world programming and as concepts*, and has you "learn" only the good parts of program design using Scala. Unfortunately, those of us that understand Scala are morally obligated to steer people away from learning the totality of the language, because such a small subset of it is useful, while the rest (e.g. mutable collections, parallel collections, xml tags, vars, null, return, java-esque design patterns, enumerations, etc) are considered harmful... but are still taught by these books. FPiS is the only one that neglects to teach you what you don't want to know, and focuses on teaching you just the good parts.
FPiS is excellent but it's frankly grueling. I recommend people tackle it after knowing syntax and language basics because I've taught people out of it and watched them struggle with the incidental complexity rather than focusing their time on the material. To be clear I do still recommend people use it, just not as the first book but the second. 
Interesting. Thanks , guys! 
Hi! Check Alvin Alexander Scala Cookbook. It has lots of ready examples and might help you with your tasks
Ive published finatra swagger support here https://github.com/paradoxical-io/finatra-extensions with some examples, along with support for newtype case classes in both finatra and swagger 
You should learn the basics of functional before trying to write a Play service
After learning scala well enough to do spark etl + ml jobs, I just downloaded one of the activator templates (i think play + akka api) and got in running. Then I started building out the endpoints I needed to support following the template example and doing a lot of googling. Now maybe 7 months after starting my first play service i've made a few more and generally feel comfortable with understanding how I should structure things, how to have good unit and integration tests, etc.
I'm linking you a project where you can see an application that is in production, it uses Play Framework and it has a bit of documentation that might help you, the code is not pure FP and hopefully it is simpler enough to understand it: https://github.com/AlexITC/crypto-coin-alerts There is a specific package that consumes external web services using Play WS, and you could see a simple way to test them: - services: https://github.com/AlexITC/crypto-coin-alerts/tree/master/alerts-server/app/com/alexitc/coinalerts/services/external - tests: https://github.com/AlexITC/crypto-coin-alerts/tree/master/alerts-server/test/com/alexitc/coinalerts/services/external I would happily answer any question that you have about the code, good luck!
Some executors take advantage of it (including but not limited to the default one), some don't. 
And what do you think is the best way of doing so? Since I'm taking the coursera course and not feeling that I learn that much... any other suggestion would be very welcome 
Awesome :D Thank you! I'll definitely look at this!
thank you! I will :)
Well, if it ain't broke, don't fix it. The time you save not rewriting this component can be spent on other, presumably more interesting, stuff. Just my 2 cents.
Hi Wesley! I'm also a beginner and I'm looking to learn to do something very similar like what you're doing in this article so I'll read it later and give you some feedback on it :)
Yes scalac -optimise flag does a lot of magic for us but there are still a lot of things which needs help from programmer. Size of methods for inlinig, branch prediction ~ class hierarchies etc. JitWatch as a tool how to check if there is some help from programmer needed. 
Sounds like you should just dive into the Red Book IMO.
The playframework home page has ['starter' projects for 'newbies'| https://playframework.com/download#starters] and [many examples|https://playframework.com/download#examples] like web sockets and rest apis.
Not sure if I get your comment right. But I suggest to follow rule: make it work -&gt; make it clean -&gt; make it fast. JitWatch as a tool which can assist you with the last step
You may have better luck getting an answer if you reword your question and/or provide code examples. As-is, I can't make sense of what you are actually asking. What is "java-compatible async feature"?
10,000 hours.
I would disagree about putting the require in IO. Getting null should indicate a logic bug not an effect you want to model with IO. It is not uncommon is scala code to assume non-null and consider nulls to be fatal. I wouldn’t put the null check in the IO if it were me if I expect it to never occur.
 IO { new ZipInputStream(is) } // this needs to be wrapped in an IO? Maybe, the Javadocs for `ZipInputStream` doesn't say it has an effect, wouldn't entirely surprised if it did. You can lift the value into IO with `point` or `pure` if you don't want to capture creating a new instance. _ &lt;- IO { zipOut.close() } This line will not close `zipOut` if an exception occurs on any of the previous lines. Instead of `flatMap` you want to use a combinator that will always run a given `IO`. Particularly, you can refer to the bracket combinator I suggested last time you asked a question about [cats](https://www.reddit.com/r/scala/comments/7rtmgp/cats_handling_resource_closing/dszkgak/). firstEntry &lt;- IO { zis.getNextEntry } `ZipEntry` is also close-able IIRC.
Yeah, I just don't think they were any helpful in getting me started. I mean either they're too simple or too complicated so that I don't understand how they work. But thanks anyhow!
So I looked a bit more at your project. I understand I can run the spec files using test-only but what will your test files generate? It won't save them as files will they? Because I'd like to know how to do that. I think I'll try out the project but I thought, just getting the service going so it consumes/downloads files using the service would be a good start.
&gt; You're not suspending a side effect there, it will strictly throw on evaluation In this case, i was considering the `NULL` to be a `Programming Error` thats why i null checked, but i understood your point. &gt; Depends. Is there a side effect on creation or not? In this case there are no side effects, it's just the allocation of a new instance, so i'll refactor to use `pure` makes sense. But my doubt was mostly in the sense if i need to lift it in an IO context or no, just create out of the IO block and closes over it. &gt; Btw square brackets in a for comprehension on the same line looks weird dude. Really? For me this looks better, but i guess it's more a matter of taste, or are there any convention about it? &gt; Using IO in an akka project is like putting a mercedes hood ornament on an old Renault 4 buggy because it's cool and shiny. Pretty moot IMO. In your opinion, what would you recommend, i want to keep Akka in the project. thanks for all the informations.
&gt; This line will not close zipOut if an exception occurs... Hey /u/Milyardo i started with this code, then i noticed it would happen and used as you pointed in the last comment in the other question, just forgot to update it here. &gt; Maybe, the Javadocs for ZipInputStream doesn't say it has an effect, wouldn't entirely surprised if it did. I checked the code, and it really don't touch the `InputStream`, just setup some internal things like buffers etc.
sbt is short for stupid building tools lol 
I'd start by doing what you were doing in Java - using the same libraries and the like. The result won't be good idiomatic Scala, but it will let you stay productive, and then you can gradually introduce more functional aspects as and when you feel comfortable.
&gt; In this case, i was considering the NULL to be a Programming Error thats why i null checked, but i understood your point. This is overly defensive in Scala. You really do not see null checks in the wild in OSS nor prod done right. &gt; n this case there are no side effects, it's just the allocation of a new instance, so i'll refactor to use pure makes sense. But my doubt was mostly in the sense if i need to lift it in an IO context or no, just create out of the IO block and closes over it. So technically no, you don't need to use `pure` if there's no side effects. Remember you can do assignments in for comprehension as such: for { a &lt;- IO(hi) b = "lol" c &lt;- IO.pure(1) } yield "Ayylmao" So the equals works basically the same considering how for comprehensions desugar. &gt; Really? For me this looks better, but i guess it's more a matter of taste, or are there any convention about it? I can't say for certain there is, I can just tell you I do not see it in the wild often. Kinda like calling `IO.pure { pure block }`. Don't think I've spotted it in the wild at all. &gt; In your opinion, what would you recommend, i want to keep Akka in the project. I'd recommend either monix or fs2 for handling streams. I have a strong dislike for akka and I sincerely believe it's moot to make a side effecting flatmap or map functional. you might as well work directly in `Future` if you're working with akka, honestly.
There is a library for supporting socket.io in the Play framework: https://github.com/playframework/play-socket.io It work pretty well. The interfaces are using Akka Streams, so some knowledge of it is needed.
My point is that you can start by using Spring or whatever you were using in Java. You could even just take one class in one of your existing Java projects and turn it into Scala.
I own both Scala in Depth and Functional Programming in Scala (ie the red book) and have read both. They are very different books and overlap very little with their content. I would still recommend reading both books, to be honest, though Scala in Depth is starting to show it's age. Regardless there is enough good content in that book, and enough existing code following the discussed patterns, that it is worth having read/skimmed the book. I do hope there is a follow-up book once Scala 3 (dotty) hits. But if as you say you want to know the FP part of Scala, then just jump into the red book. It will walk you through everything you need to know. 
If you're using Akka streams, there are a few streams provided to do non blocking IO. Then just wrap the result in a future.
Hmm. I didn't think about that. Got me something to think about. Thanks!
&gt; This is overly defensive in Scala. You really do not see null checks in the wild in OSS nor prod done right. I see, i'll check more OSS to see more patterns and approaches. &gt; Remember you can do assignments in for comprehension... Oh yeah, i really forgot about it, looks much better this way &gt; I'd recommend either monix or fs2 for handling streams. I have a strong dislike for akka and I sincerely believe it's moot to make a side effecting flatmap or map functional. you might as well work directly in Future if you're working with akka, honestly. I see, about the `Future` there are some places where i use `Future`, but other places where it is not directly tied with Akka i was applying other things to better understand it. I'll check monix and fs2, thanks for the recommendation.
Makes sense, the first option seems reasonable, but i got curious about the second one, what would be examples of this abstractions?
yes, you can run a single spec file using the `sbt testOnly`, if you have docker installed, you should be able to run all the tests using `sbt test` command. &gt; but what will your test files generate? It won't save them as files will they? I don't get what you mean, you could generate test reports but I don't think that's what you are talking about, a test file is just a file inside the `test/` directory, you could run them directly using IntelliJ. &gt; I think I'll try out the project but I thought, just getting the service going so it consumes/downloads files using the service would be a good start If you talk about my project, the documentation specifies that you need PostgreSQL to run it, once you run the project, there are some tasks that will be executed in order to call the web services and download data (but I don't think you are talking of this), see: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/tasks/ExchangeCurrencySeederTask.scala
Thank you for the response! I do want to learn FP, but also want to learn best practices and maybe guidelines how to mix OOP and FP so I was hoping that Scala in Depth will give me some answers on that. Maybe I could go first with Scala in Depth. Can you please tell me what chapters do you consider outdated and not worth reading?
I see, Akka does already have a streaming for that, but i think is too much for a simple operation, in my use case the zip things are very simple and small, i thought that would be other kind of abstraction, but i got your point, thank you! :)
If you feel like you aren't ready for the red book yet, I highly recommend reading Functional Programming Simplified by Alvin Alexander. It's a pretty large book but you can go through it very quickly. It's a good step up to the red book.
Yeah I ended up doing this: addCommandAlias("blackDuckScanMaster", "; clean; package; ivyReport; scanMaster")
"adding scaladocs" part really rocks. I believe staring at the sources is the best way to understand sbt and why it does not work as expected. As for suggestion, I would really like to see bugfixing release. Not all combinations of `.value`, `taskDyn` and `:=` are behaving well.
"expert at scale" - sounds just right for linkedin profile.
To a first approximation I would say you should actively avoid mixing FP with OO. Instead learn how to represent the GoF patterns, and other patterns, using FP primitives and patterns, which is maybe what you are asking for? Regardless, I've yet to read any comprehensive break down of best practices for mixing FP and OO. I think there are just too few folks writing that kind of code to have a well developed, and published, body of work on the web. Plus folks who do write in the FP style tend to eschew OO patterns anyway, and instead write using ADTs and GADTs for data, and algebras for computation. So they use "objects", but only because those are fundamental to data abstraction on the JVM. I would still consider most of Scala in Depth to be worth reading. Certainly chapter 9 (actors) could be skipped, but you should eventually aquire a working knowledge of what actors are, and why they've been used. You could also skip chapters 10 (java interop) and 11 (FP). You likely don't care about Java inter-op, and FP is covered in much more depth in the red book. 
I heard/read somewhere that OOP part of Scala was deliberate and not just to more easily interop with Java. Something along the lines that Odersky considered OOP necessary for good software architecture modeling (I hope I didn't butchered the idea). So I was always wondering does anyone really write in OOP+FP style. I'll follow your advice and avoid it then :D I'm interested in part of Java interop when Scala gets translated to equivalent Java code so I'll read that part if it shows up and skip the rest. Thank you again for your thorough explanation!
What I meant was how the rest actually worked yeah. And yeah what I wanna do is call the webservice and download the data. I was hoping to be able to do it through the testfile alone. Or add to it so I can do that :)
This would be amazing but yeah I haven't found anything like this either.
on the external web services, I call the web service manually using something like, cURL, a browser, Postman, etc, copy the response and paste it on the test, then, I mock the response and expect the service to return a proper result, please note that this might not be enough for your use case and you might want to write complex tests, like mocking failure responses, checking status, etc.
Ah, so the test data inside the test, where you already put some cryptocurrencies values..that's the data you got by the manual call? I understand. I'll keep looking at your test. But where is the response from the real service in the test? Like let's say I wanted to try your test but just have it println each line/string that you receive in the response. Like for example some current value of a cryptocurrency currently being tested?
for example, let's take the `BittrexService#availableBooks()` (https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/services/external/BittrexService.scala#L24), it calls this url `GET https://bittrex.com/api/v1.1/public/getmarkets`, as the response is quite big, I won't paste it all in my test (I could create a file instead) but for my scenario it's enough to grab one or two items and that's what I did, I took one item and the response format and pasted it in the test: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/test/com/alexitc/coinalerts/services/external/BittrexServiceSpec.scala#L21 also, as you could see, the `getTickerList` method contains a different response where I used the same logic: https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/test/com/alexitc/coinalerts/services/external/BittrexServiceSpec.scala#L64 
Hi, Adam from deeplearning4j here. Could you clarify what you mean by maturity? Do you mean flexibility in the same way that folks tend to bash keras? If so, maybe maturity isn't the right word there. I agree that flexibility can be a bit hard with the MLP api. We encourage people to use the computation graph api in that case anyways. We support custom updaters and loss functions right in the api if you're interested. Usually these opinions tend to be held by folks who haven't actually used the framework in the last few years. I also just want to say, we will have a pytorch like api coming out for lower level apis here that's about done that will support everything ranging from tesnrflow import to autodiff. That will just be raw nd4j. Either way, concrete feedback is appreciated. "Maturity" is a broad critique that doesn't serve anyone trying to understand what is good/bad about a framework. We are open to feedback if it's civil.
It's simply the type of a function from T to R. It's an alias for Function1[T, R]
T =&gt; R is a Function that takes an argument of type T and returns a result of type R
[removed]
A lot of the time what looks like new syntax is just a clever combination of existing syntax, or else not syntax at all but plain old methods and datatypes with funny names. See whether you can click through in your IDE to where it's implemented. 
In addition to the other commenters, you shouldn't need this any more in Scala 2.12+ Scala 2.12 does the same thing as Java 8, where you can use something with a single method (SAM) in place for any function
Just as a devils advocate... is it possible to hardcode the data into the app? Otherwise it sort-of complicates how you would deploy and maintain the system I’d say you wanted it running on multiple nodes.
which ide to use for a good scala programming
Do you know what a Functor is?
Given a type X, a Monoid is a "box" or container for that type of X. So Option[Int] or List[Int] are monoids for Ints. It's possible to go from one container to another using a functor. So you can turn an A[X] to a B[X] by using a functor. This is typically called map or fmap (?) So val boxedA :Option[Int] = Some(3) //first monoid val boxedB :List[Int] = boxedA.toList // here toList is the functor ? Is that right so far?
Please buy the book and read it. The door will open for you. You going to waste your time with questions, before not reading this book. I was on the same situation like you a year ago. Please buy and read it. 
which book?
Implicit parameter is very powerful solution. =&gt; **Don't use it!** *Why?* [Principle of Least Power](http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html) (a.k.a KISS, Occam's razor, YAGNI, Worse is better, Less is more, ...) is (are) the most important principle. If you have less powerful solution, use that! There are many of them (by Li Haoyi) (from less powerful to most): 1. First, hard-code the dependency 2. Second, pass it as a parameter to the method which needs it 3. Third, inject it into multiple methods by passing it into a containing class 4. Fourth, if your containing class is split into traits in multiple files, use abstract members 5. (Fifth, make the method parameter implicit) and others: * use simple Monads * Publish/Subscribe * Actors * Free monad (slower) * Tagless final (slower) 
It is a terse and powerful solution, but I think it is not simple. With explicit parameter: import scala.collection.mutable.ListBuffer object Logger { class Logger(id : String) { private val log = new ListBuffer[String] def println(s: String): Unit = log += s def dump(): Unit = log.foreach(entry =&gt; Console.println(s"$id: $entry")) } type Logged[T] = Logger =&gt; T def logContext[T](logger: Logger)(op: Logged[T]) = { op(logger) logger.dump() } def fact(x: Int): Logged[Int] = { log =&gt; log.println(s"fact: $x") logContext(new Logger("log2")) { log2 =&gt; log2.println("log in here is different!") WebTools.wget(s"http://catpics.net/pic$x")(log2) } if (x == 1) 1 else x * fact(x-1)(log) } def main(args: Array[String]) = { logContext(new Logger("log1")) { log1 =&gt; val in = 3 val out = fact(in)(log1) println(s"fact($in): $out") } } } object WebTools { import Logger.Logged def wget(url: String): Logged[Unit] = { log =&gt; log.println(s"Downloading $url") } } With simple monad ([independence package source](https://github.com/enpassant/miniatures/tree/master/src/main/scala/independence)): import independence._ import independence.Level._ import independence.ResultOps._ object MonadLogger { def fact(x: Int): Result[Int] = for { logFact &lt;- log(INFO, "log1", GoodResult(s"fact: $x")) logDiff &lt;- log(DEBUG, "log2", GoodResult("log in here is different!")) response &lt;- MonadWebTools.wget(s"http://catpics.net/pic$x") factorial &lt;- if (x == 1) GoodResult(1) else fact(x-1).map(_ * x) } yield factorial def main(args: Array[String]): Unit = { val in = 3 val result = fact(in) result.processInfos(processLog("log2")) result.map(out =&gt; println(s"fact($in): $out")) result.processInfos(processLog("log1")) } def processLog(place: String)(logs: Vector[Log]) = { logs foreach { case log @ Log(level, place_) if place == place_ =&gt; println(f"/LOG/ [$level%5s] |$place%4s| ${log.message()}") case _ =&gt; } Result(()) } } object MonadWebTools { def wget(url: String): Result[String] = { log(INFO, "log2", GoodResult(s"Downloading $url")) } } 
Lol same here. When does it end?
`Applicative` is a typeclass of context-like types `F[_]` that's stronger than `Functor` but weaker than `Monad`. There are a number of ways to define it, but for me the clearest is to say that as well as supporting `map`, it supports the `mapN` family of functions: `map2[A, B, C]: ((A, B) =&gt; C) =&gt; (F[A], [F[B]) =&gt; F[C]` and similarly `map3`, `map4`, .... So you can "merge" two contexts, which you can't do with `Functor`, but you can't "chain" two functions with contexty effects in general the way you can with `Monad`. A good example is `Validation`, which is a success-or-failure type a bit like `Either` or `Try`, except that if a `Validation` is failed then it will include *all* validation errors rather than just the first one. So you can't write code like: for { name &lt;- validateName(submittedName) address &lt;- validateAddress(name, submittedAddress) } yield new User(name, address) which is what you'd write with `Either`, because if the `submittedName` was invalid then you won't even be able to call `validateAddress`, so this code couldn't ever guarantee that it would return all the failures (you might fix the invalid name but still get an error, because once you run it with a valid name it turns out the address was invalid - not what we want). But if `validateAddress` can be done without needing access to `name`, then you can use an applicative operation to "run the validations in parallel" and then merge them: (validateName(submittedName), validateAddress(submittedAddress)).mapN {new User(_, _)} There's no way to do something like that with a `Functor`, but `Applicative` lets you do this kind of merging without permitting you the full power of monadic chaining like you can do with `for`/`yield`.
Thank you Alex! I really appreciate it :)
you won't have to really learn a ton of functional programming to do play... what you really want to do is get comfortable with the scala core language, after that you can reason about the play APIs fairly easily
I was wondering for what `scalacOptions += "-Ypartial-unification"` is good for.
Sorry I linked to the wrong file. The question was, is `Show` is defined as a companion object and as a trait. Coming from Haskell, for me trait looks like a typeclass and it has not @typeclass annotation like Contravariant. What is exactly `Show`? 
Well, the above example works with `-Ypartial-unification` and doesn't work without it: without `-Ypartial-unification`, if you have `def foo[F[_], A](fa: F[A]) = ...` and want to call it with `e: Either[Int, String]`, you would have to explicitly pass the type parameters `foo[Either[Int, ?], String](e)`. With `-Ypartial-unification`, `foo(e)` will work.
Unfortunately I don't have a good answer to you, but as a tip, instead of `a.tree.tpe.typeSymbol.toString.substring(6,symbolString.length)` you can just write `a.tree.tpe.typeSymbol.name.toString`.
Thanks a lot.
Thanks a lot.
Thanks for the advice, I'll check it out. Sorry, I'll use that thread next time.
http://deeplearning4j.org/compgraph ? The java doc: http://deeplearning4j.org/doc/ - and yes we've had it for a long time now: http://deeplearning4j.org/usingrnns Could you enumerate what "worst" means? I'm curious did you even talk to us in gitter at all? 
Hi all I have the following code: object ContraCats { val showString = Show[String] def main(args: Array[String]): Unit = { val m = showString.contramap[Symbol](_.name).show('dave) val a = showString.contramap[Symbol](_.name)('dave) } } As you can see, it is possible to write as currying version and the other as method call. Why it is possible? Here is the source of `Show` trait.
From the readme: &gt; SBT is hard and slow. I have to say it again, SBT IS SLOW. lol, try using Coursier, that speeds things up a bit. Also, give at least 3GB to the VM when working with Scala.js (cuts down on GC events that slow down `fastOptJS`)
What languages do you know already? Something like Atomic Scala is more of a from-scratch approach. (The most important course on my own path to Scala was probably one based on [ML for the Working Programmer](https://www.cl.cam.ac.uk/~lp15/MLbook/pub-details.html), but it's not really fair to recommend learning another language before you can learn Scala).
Mostly JavaScript, with some python as well. I’ll check out Atomic Scala. Yeah, perhaps not going to learn another language. Basically I need to get up to scratch on Scala for my job, so the quicker the better really.
You really have computation graph under deployment section? No wonder I could not find it. 
Ah cool. Unfortunately a lot of Scala courses/tutorials assume knowledge of the Java ecosystem because that's where a lot of Scala programmers started; Atomic Scala is an exception though (in fairness I don't know about this Coursera course). It might also be worth getting a bit familiar with functional style in javascript or python - just at the basic level of using `map`/`reduce`/`filter`/`flatMap` to replace loops, using result types instead of early return, returning new values instead of modifying input in place, that sort of thing. Just because learning a new language and learning a new paradigm are both hard, and you don't want to have to be doing both at the same time.
All sound advice, I’ll take it all onboard. Appreciate it mate.
How were those benchmarks obtained? To me they appear to be a bit jumpy.
Why give up on the exercises and just ask for help on them here?
Don’t recall saying I gave up with them, I just find the teaching material prior to the exercises to be insufficient to complete the questions. Hence, wanting to find some other teaching material, perhaps something that moves a bit slower, before returning to said exercises. 
What specifically do find to be missing?
&gt; Started Atomic Scala since my post, and seems to be much better suited so far. That's weird Atomic Scala isn't really for teaching programming, it just covers Scala language features. It sounds like you were looking for the wrong thing in the coursera course. The cousera course is a course on the basics of functional programming. The learning objectives (ie, the substitution principle, higher order functions, and strictness and laziness) are not language specific and the course just uses enough scala to teach those concepts. If you're looking to learn scala language features you were in the wrong place.
I’m looking to learn how to program in a functional way, and also learn Scala. As someone else mentioned it’s quite a lot to take in, in one course. I also didn’t choose the course (wasn’t clear in my post) I was told to do it for work, wasn’t a lot of choice.
It's in the text. https://gist.github.com/julienrf/f1cb2b062cd9783a35e2f35778959c76
Just thrown in the deep end of the pool then? It's going to be some work, but try to enjoy it, you're going to come out the other end a far strong programmer than you were before.
Little bit, started here a couple months ago, they’ve just had me doing little bits and pieces up till now. But they want me to help out with the stuff they’re doing in Scala, hence the Coursera course. So far it’s been frustrating rather than enjoyable, here’s hoping there’s a light at the end of the tunnel.
Can you check what I just said in this useful Scala Contributors ticket? I think you may find the ScalaBridge resources valuable.
Seconded, the amount of noise seems to be pretty high and I’d call them at best inconclusive.
Scala exercises are always good: https://www.scala-exercises.org. Also all the books accompanying the Underscore courses are now available as free ebooks https://underscore.io/training/.
If def isn't evaluated until usage, is the type known? I assume so, otherwise I'm not sure how an implicit would work.
I really don't think a bunch of unguided exercises are going to help.
I wonder of this enables stream fusion?
Check DR MARK LEWIS playlists : https://www.youtube.com/user/DrMarkCLewis a way more gentle introduction to scala (you can also check his books) also a small book called "Scala for the impatient" may also be a fast introduction to scala if you have prior knowledge of java.
There's https://gitter.im/scala/scala if you need help. (Beginner questions are welcome)
Programming doesn't compile for me using cats 1.0.1: ``` package com.example import cats.Show import cats.implicits._ object HelloWorld extends App { val showString = Show[String] override def main(args: Array[String]): Unit = { val m = showString.contramap[Symbol](_.name).show('dave) val a = showString.contramap[Symbol](_.name)('dave) } } ``` ``` Error:(13, 49) cats.Show[Symbol] does not take parameters val a = showString.contramap[Symbol](_.name)('dave) ```
Link is broken for me
Heyo, I'm integrating with a Java package (OpenSaml) that needs to be initialized before anything can be done with it, eg: InitializationService.initialize It must be called once per process before library functions can be used. What's the best way to deal with this, in a somewhat idiomatic fashion?
Excellent review
Creative Scala https://gumroad.com/l/creative-scala
Oh yes I was aware of both of those previous critiques, I was wondering specifically about reactions to the new collections redesign. 
I don't think there are any formal critiques, considering everyone thought it was still a poc until yesterday, when the news was posted. At least, in the typelevel/scalaz circles, it's been negative.
I agree, and this is why we have structures for this in Scalaz - ACatenable1, NEL, Foldable1, etc. I agree with Spiewak mostly, in that a lawful hierarchy, with some embracing of the most fundamental FP concepts would provide the simplicity (and [performance!!](https://github.com/fosskers/scala-benchmarks)) we need. However, even in light of very apparent and well-supported documentation providing them with a clear way forward (a lawful hierarchy, perhaps type-aligned data structures, invariance, as opposed to supporting covariance/contravariance), we receive a half-assed justification using Vector of all things - a structure known for its poor performance and vintage novelty - as a justification? I'm pissed. &gt;it would be silly to say that Option[T] is just a special case of Collection where { size &lt;= n} where n = 1 That's the Odersky trap - gand wavy answers that make no sense, just to gaslight you and shut down the conversation.
To be fair, he has obviously a lot of pressure on him I'm sure, and loads of people coming from all angles wishing to push the language in this or that direction and he has to say "no" to the great majority of ideas to avoid resulting in a completely incoherent mess, I just thought it was odd since NonEmptyList's are so obviously useful. To the point where cats and scalaz have implementations. Ah well. Your comments about the performance of Vectors prompted me to re-read through lihaoyi's thorough benchmarking of the collections and I was surprised to read that actually Array concatenation is much faster than Vector concatenation despite the fact that Array concatenation requires a complete copy of all members of both input arrays to form the result. It makes me think we should be maybe using immutable Array wrappers instead of Vectors in some circumstances. 
Yeah, I might not have answered all of your questions. I tried first DL4J a year ago, then have couple of minor retries. All of the time the major pain point was documentation (and thus feeling of lack of flexibility) and I think that putting 'Build Complex Network Architectures with Computation Graph' under 'DEPLOYMENT TO PRODUCTION' section sums the quality of documention quite well.
Sad thing is, that I am able to write quite advanced models (like https://arxiv.org/pdf/1711.02085.pdf) in CNTK, Theano, Tensorflow and Chainer without asking anyone and just looking at documentation. I still think that documentation need reordering (look at Keras, that one is quite good). And don't worry. After I get free time, I might try DL4J in some deeper way (for example trying to implement that skim rnn). Will let you know :P
All types are known at compile time; it's a requirement for the compiler to validate the code.
I agree with a lot of what Paul says, does anybody knows why the scala team didn't include any of his collections work into the new collection? 
I'm curious whether these 2 analogies would help your intuition: Analogy with SQL: - using Functor: Query from 1 table - using Applicative: Query from [cross join](https://en.wikipedia.org/wiki/Join_%28SQL%29#Cross_join) of 2 or more tables - using Monad: Nested query from 2 or more tables (the inner query has access to the row returned by the outer) Analogy with AJAX: - using Functor: 1. make single AJAX request; 2. await it's completion; 3. process the result; - using Applicative: 1. make 2 (or more) independent AJAX requests; 2. pretend it was just one big request that returns 2 (or more) results, combined in an array or an object (no direct javascript analogy here, that's why it's good to have `Applicative` in your language to spare you work); 3. await completion of the big request (implemented as awaiting completion of all small requests); 4. process combined result; - using Monad: 1. make 1st AJAX request; 2. await it's completion; 3. use information in the result to make 2nd AJAX request; 4. await it's completion; 5. use information in the result to make 3rd AJAX request; 6. await it's completion; ...and so on 
I was wondering about where his collections redesign experiment led him. Are people using it? Did it solve any of the problems of Scala collections?
&gt; However as far as I have seen there are 0 plans for this kind of feature in Scala or even in Dotty any time in the future. There is actually [some work](https://dl.acm.org/citation.cfm?id=2998398) currently done by a PhD student at EPFL, but whether this will merged into Dotty eventually or not is very uncertain.
that seems very unfortunate, and somewhat short-sighted from LightBend... without clear transition to different developers etc., a lot of effort seems to be wasted / stagnant (opened PRs, issues, etc...)
I believe there is no stream fusion at the moment. However, it should be possible to add to the new views. I think I got that from the Q&amp;A portion of [this](https://www.youtube.com/watch?v=ofbaM7Yz3IM).
&gt; I don't think there are any formal critiques, considering everyone thought it was still a poc until yesterday, when the news was posted. Why do you think everybody thought it was a poc until yesterday? According to [this post](http://scala-lang.org/blog/2017/02/28/collections-rework.html) from a year ago, on the official scala blog, it seems pretty clear that these were going to become the new collections in 2.13. 
Why the comment about builders? Aren't the new collections largely iterator based rather than build base? Compare the default implementation of most methods in the new collections in the [Iterator](https://github.com/scala/collection-strawman/blob/master/collections/src/main/scala/strawman/collection/Iterable.scala) class to the default implementations (I think) in the old collections class [TraversableLike](https://github.com/scala/scala/blob/v2.12.4/src/library/scala/collection/TraversableLike.scala) 
2.13 already had several three milestones builds by now, and none of them include a preview of the new collections. Given that 2.13-final is scheduled for H1 2018 (https://adriaanm.github.io/reveal.js/scala-2.13-beyond.html#/2/10), this makes things very sudden.
It doesn't seem like these critiques are entirely applicable to these collections. It seems like a lot of the criticisms are contained in the above are solved in the new collections (no more `CanBuildFrom`, simpler collection hierarchy, views work, etc). It seems like a lot of the remaining suggestions from Paul Phillips (lazy by default, etc) conflict with the goal of this redesign conflict with the backwards compatibility requirement of this redesign, which I don't think is unreasonable. Can you be more specific about what you think is wrong with the redesign? &gt; Oh yes, and see for yourself what's going on in the strawman collection linked in the Julien's post above. It's the exact same shit again. See? Wonderful. I'm not sure what you are implying is wrong with this. This looks like an override in the concrete class for performance reasons. This doesn't seem unreasonable to me. Am I misunderstanding this?
&gt; What's wrong with def flatMap[B](f: A =&gt; IterableOnce[B]): List[B]? Everything. It's not a flatMap. This is exactly the set of problems Paul Phillips talks about, where signatures do not align, and if this were kept, would introduce unnecessary casting up and down the Iterable hierarchy, when the solution is to simply enforce def flatMap[B](f: A =&gt; List[B]): List[B] I tend to lean towards your direction, but, I wonder, do you see any utility in being able to do the following? List(1,2,3,null,5).flatMap(number =&gt; Option(number)) 
&gt; do you see any utility in being able to do stuff like the following? List(1,2,3,null,5).flatMap(Option(_)) No, because I use the type system to constrain everything. Types are tests for edge cases like the one you gave. It will never occur in bare scala code, because as soon as you have `val _: List[Int] = List(1, 2, 3, null, 5)`, it fails to compile. In situations like when using Spark, it may be necessary to deal with the `null` case. That would be the only time I would use something like that. For the most part, however, structuring your program to make use of well-typed generic workflow mitigates the need to explicitly handle null. Thinks like the Free-inpreter patterns of Tagless Final algebras make strides toward this. &gt;Saying "everything is wrong with it because it's not flatMap" just seems like a semantic argument. You're right, it is. What is a language, but set of semantic choices? Aside from that, it doesn't align with any other resources, or theoretic al foundations that underly the `flatMap` concept (i.e. monadic `&gt;&gt;=`), which makes it a poor design decision. On top of that, it introduces unnecessary and unsafe type coercion. But sure, in the end, they can have it whatever way they want, and call it whatever they please. It's generally best practice to not do that, but it is what it is. &gt;ith that said, I was also hoping for a more significant redesign that utilized typeclasses more and deep inheritance hierarchies less. This was introduced and apparently dismissed outright by Odersky. Ask him why. 
Honestly: Go with either typescript or purescript. Scalajs is a labor of love from the community that you will always have to write your own wrappers to call js from, for anything nontrivial. I can't comment on production readyness, only on how cumbersome it can be to have to work on porting everything to it, when I personally have little to no motivation to write client-side code. If you want pure FP, there's purescript as well. Typescript in particular has massive industry backing. There's loads of applications for scala, I just am not sold on client side being one of them.
I haven't touched it in a while, but I enjoyed Udash: https://udash.io
I dunno, I'm pretty excited for what's coming down the tube in Dotty, and in the collections redesign I'm also excited for all the new data structures that will be available.
I agree. It doesn't require a genius or some crazy research to figure out that the approach will never fix the issues that plagued the existing code. Simply looking at the existing code and asking "how many mistakes are we repeating" tells you all you need to know. A new implementation cannot fix the design issues if the design requires the implementation to make all the same mistakes again.
I'm not. Looking at the stuff proposed, I can only think that people have completely lost the plot. It's just embarrassing. There are a few nice things in it, but most of the stuff is just "What the hell were they thinking? Did no one actually look at the problems we had, and the deprecations and removals that went with it?"
&gt; The lack of CanBuildFrom is an improvement, but a move from "terrible" to "very bad" is still not even close to ideal. They still feature explicit builders in the new collection which they've cleverly hidden. I'm not sure builders are really hidden. They are mentioned explicitly in the [design docs](https://github.com/scala/collection-strawman/blob/master/documentation/DESIGN.md). Builders still exist, because they provide a way to optimize performance of the creation of strict collections. However, they are not a part of the shared abstraction. I believe this is why it is possible to have views that aren't broken (as well as other non-strict collections). Do you have an alternative to builders that would perform as well for strict collections? &gt; The hierarchy is somewhat simplified, and I don't think anyone has particularly cared about views since they were introduced, unless they introduced them unwittingly to massively negative performance impact. People haven't really cared about views because they were broken. I'm surprised you are so dismissive of views because they provided an answer to one of Paul Phillips' complaints. In particular, they provide a way to represent lazy collections with stream fusion (although the stream fusion portion isn't implemented yet). &gt; The current hierarchy is nonsensical, and unprincipled. Map and Set are both Iterables? In what world does this make sense to anyone? I'm don't understand why that doesn't make sense to you. Why wouldn't you be able to iterate over the elements of a Map or Set? &gt; The fact that the hierarchy is still enforced is a very sore point in contrast to libraries like Scalaz or Dogs which implement the functionality in terms of layering lawful (well, as lawful as they can be) typeclasses, which is the more correct way to think about things. However, Martin Odersky has handwaved this away adn dismissed it outright in the discussions. You can see a more in depth critique in the link I gave via Spiewak. I think the reason a type class solution wasn't used is because it would be difficult to provide a type class solution that functioned sufficiently close to the existing implementation. That doesn't seem unreasonable to me. Am I wrong about this? &gt; Is rewriting the collections library every 5 years unreasonable? I think so. Besides the Burning Bridges proposal, I think Haskell's has stayed relatively the same. Java as well. I think rewriting the collections is reasonable if things need to be fixed. What is the alternative? Are you saying we should have stayed with the Scala 2.7 collections? Your examples of alternatives don't seem to be great cases for leaving things alone. The Java collections still can't be used to implement an immutable List without resorting to `UnsupportedOperationException`. Haskell has 3 different kinds of strings because the default performs so slowly, and until recently Monads weren't Functors. &gt; I've linked the benchmarks done by fosskers. I'm sorry. I can't find the link in your posted. Can you post it again? &gt; What's wrong with def flatMap[B](f: A =&gt; IterableOnce[B]): List[B]? Everything. It's not a flatMap. This is exactly the set of problems Paul Phillips talks about, where signatures do not align, and if this were kept, would introduce unnecessary casting up and down the Iterable hierarchy, when the solution is to simply enforce def flatMap[B](f: A =&gt; List[B]): List[B] &gt; &gt; I see work that is trying to be clever, avoiding the lawful approach as hard as it can to implement a hierarchy we don't need to performance deficit, incoherence, and yet another set of problems. I don't want to have to keep dealing with Scala. I'd like to enjoy using it as much as I've enjoyed other languages. Thanks for clarifying what you are referring to. I'm sympathetic to to a more rule based set of collection operations. However, as far as I can tell, that is difficult to get to without breaking a bunch of code for users of the collections. Do you think there is a straightforward way to convert everybody to such a library? 
&gt;Builders still exist, because they provide a way to optimize performance of the creation of strict collections. No, builders exist because they couldn't find a better way to generically define natural transformations between members of the hierarchy. They do little for performance. &gt; I believe this is why it is possible to have views that aren't broken (as well as other non-strict collections). Do you have an alternative to builders that would perform as well for strict collections? Views allow lazy traversal of a data structure, they don't solve the problem whatsoever, when the problems *are fundamentally architectural*. It's great that views exist. If Odersky et al want to include them, sure, they can - they can have all the unused view api's they want. An alternative to all of this is choosing the more performant, architecturally sound, lawfully coherent architecture (i.e. use typeclasses). If you genuinely need performant datastructures, there's no reason you can't still have imperative or lazy optimized data structures (like ListBuffer, Stream). That's a separate issue from the architectural problems inheritance brings. &gt; In particular, they provide a way to represent lazy collections with stream fusion (although the stream fusion portion isn't implemented yet). Which would be more complicated to do within the hierarchy, and relatively easy do with a typeclass-driven api. You can have both, it's not an exclusive design decision. &gt;I'm don't understand why that doesn't make sense to you. Why wouldn't you be able to iterate over the elements of a Map or Set? The problem with iterating over `Map` and `Set` is the same as the fundamental problem `Builders` inject into the library - in order to have `Map` be `Iterable` it must be *generically* iterable, and same with `Set`. This means the same as what it meant in the previous collections library - that `Map` is now treated as a list of tuples (it's not), and `Set` is now treated as a deduped `List` (it's not). This is a fundamental problem, as neither of these datastructures should ever be considered linked - they are two separate constructions of efficiently balanced trees. It's a fundamentally incorrect way of thinking about the subject. &gt;I think the reason a type class solution wasn't used is because it would be difficult to provide a type class solution that functioned sufficiently close to the existing implementation. That doesn't seem unreasonable to me. Am I wrong about this? Spiewak says a typeclass-driven api would be roughly 80% of the way there, breaking 20% of the api. Considering the current Strawman is going to break roughly the same number of things, it's minimal effort in comparison, and the gains are tremendous. But this is also a deeper question of when to break bincompat - they've already done it once before with 2.8. At some point, in order to keep up with 3rd party libraries offering lawful structures like Scalaz and Cats, they're going to have to, or be forever known as the language where everyone uses 3rd party libs and hides `Predef`, as my team is starting to do, and many [already do](https://twitter.com/tpolecat/status/550419995218292736). &gt;I'm sorry. I can't find the link in your posted. Can you post it again? Of course! :) [fosskers](https://github.com/fosskers/scala-benchmarks) &gt;Do you think there is a straightforward way to convert everybody to such a library? No, unfortunately. Just like in 2.8, there is going to need to be some serious warning and discussion before it happens. I consider this a necessary evil in contrast to another 5-6 years of unlawful, poor performing, bug-ridden collections implementations. And then this will happen all over again. 
Does it make money for Lightbend? Clearly not, so they "donated" it to the community. Lightbend is first and foremost a business, and a small-ish one at that, so given limited resources it's understandable that they allocate staff to the projects that keep the lights on (Akka, Play, Scala, etc.). For the community that depend on their projects it's obviously unfortunate when one that you rely on gets canned. My take on Slick is that it's kind of like Emacs, an operating system with a query DSL :) Incredibly powerful, yes, but terribly difficult to work with under the hood. Maybe the community will step up to the plate and maintain it, have my doubts...
The ironic thing is that after releasing 2.8-that-should-have-been-3.0 and promising to not do that again ... Scala ships another collections rewrite in a "minor version".
I think it would make way more sense to compare the performance of specific chains of collection operations against an implementation that uses a while loop as a baseline. Comparing against the existing collections makes things look much better than they are, due to how slow the existing collections are. Being 20% faster (or slower) just doesn't matter much, if the baseline is _magnitudes_ slower than an efficient implementation.
It's easier to hide the discussion behind "minor" politics than it is for a major version.
I wish you were wrong.
I can't wait for the next rewrite in 2.18!
Three time's the charm. Three times in a single major version. Unheard of, but why not be the first! Scala will have that special distinction. *EPFL* will have that special distinction.
There's this one that started recently: https://corecursive.com/ In more general I like Software Engineering Daily, and Software Engineering Radio. 
I think there was and still is a lack of communication as to exactly what is coming and when. The official github repository (https://github.com/scala/collection-strawman) was and still is saying "Experimenting with Scala Collections designs for Scala 2.13" and "Prototype improvements for Scala collections". Even the repository name suggests that the code there is just an experiment. There are also blog posts and talks presented by Lightbend and Scala Center, but to the best of my knowledge they all focus on high-level design and are light on details about what exactly will be shipped and when. Given all that, I can see how people can get an impression that the collection effort is still at the proof of concept stage. For one, I am not sure about the status myself.
The #1 mistake people make with Scala.js is treat it like Typescript – "let's write a bunch of wrappers around JS libs". This does not work because all you do is multiply your suffering caused by the lame aspects of the JS libraries ecosystem (dynamic typing, constant breaking changes, mutability-riddled APIs, etc.) that you are trying to escape (otherwise why even use Scala.js?) Scala.js is not a better Typescript. Typescript only seems painless because it does not give you anything significantly more useful than pure JS + JSDoc comments and an IDE would provide without the hassle of dealing with outdated third party bindings. Once you start working with Typescript you will quickly see its limitations in soundness, features, and expressiveness. Scala.js offers you a better language and standard library, but you will not benefit much from it if all you do is wrap JS libraries. If you want to build a solid Scala.js frontend app you should use Scala.js solutions to frontend problems. At least for big opinionated things like a reactive UI library. Sure, use moment.js for dates if you need, but go with e.g. Outwatch or Binding.scala instead of React. And IMHO this is where the Scala ecosystem currently falls short. I personally don't like any of the reactive UI libraries for Scala.js, so I'm building my own, Laminar.
But this is my problem: I _don't want_ to be a front-end developer. I've never wanted to and actively avoid jobs where this is a responsibility of mine. The client-side web is a set of broken standards. There's hardly any interesting problems to solve (unless you're into graphics I guess). When it comes to clients, the few times I ever have to touch one anymore, I'm mostly worried about security and functionality. &gt; Scala.js is not a better Typescript. Typescript only seems painless because it does not give you anything significantly more useful than pure JS + JSDoc comments and an IDE would provide without the hassle of dealing with outdated third party bindings. Once you start working with Typescript you will quickly see its limitations in soundness, features, and expressiveness. I really don't give a shit about expressing my client-side functionality as a set of tagless algebras which I can apply beautiful pure transformations. I leave that on the server. Btw, I've built one mobile app in ionic-typescript before, so it's pretty patronizing to assume I've never done so. If you really want to go down the path of "soundness, features and expressiveness" you might as well be talking about purescript, which has a company backing it (Slamdata) and is a pure and functional language. &gt; If you want to build a solid Scala.js frontend app you should use Scala.js solutions to frontend problems. At least for big opinionated things like a reactive UI library. Sure, use moment.js for dates if you need, but go with e.g. Outwatch or Binding.scala instead of React. No I _don't_ wanna build a "scalajs frontend app"... I just want to build a frontend app. I want to get the client out of the way so I can focus on the server where all of the possibly interesting work is, provided you're not just doing CRUD. I don't feel like reinventing the wheel where at best I just need to make sure that I send people into the right places, and that I don't have XSS via reflection when popping up silly looking alerts, or leak tokens. And the best part is, I'm not alone here. There's _tons_ of developers like me that do not enjoy client-side work and just want something to get it out of the way. For these kinds of people, the better ecosystem is the one with more implemented features and libraries, and it's a no-brainer which one that is.
That code you quoted is how writing React in Scala.js looks and feels. That Scalajs React library is very powerful and type safe, even allowing you to use native React.js components (IIRC). Achieving such interop with a JS library that was not meant for Scala's type system is incredibly hard, and so the Scala.js interface to React is not very thin, bringing in its own concepts and methods. React.js itself has no Backend / BackendScope / JsCallback (duh) / renderPC / builder / build / etc. I've worked a lot with both Scala and React.js (I mean separately, without Scala.js), and I don't like dealing with React code in Scala.js. The end code is not easy to read, and does not feel like good Scala or good React. I suggest you look into native Scala.js reactive UI libraries instead of React.js wrappers. I'm writing my own – [Laminar](https://github.com/raquo/Laminar) – that you might want to consider if you don't mind using something so early stage. I started my Scala.js explorations a while ago by writing a type safe interface to Cycle.js, another alternative to React, and I definitely understand how excruciatingly painful it is to wrestle Javascript's dynamically typed wildlife into a decent Scala API. Although I made it work I've abandoned that effort because it's just not worth it, IMHO. If you want the core of your application to work in a JS framework like React it is more productive to simply use JS.
Scala.js is an absolute gem, and it's super-easy to share code between server and client. There's been a few posts about using Scala.js on the frontend recently, so it seems I'm not the only one excited about using Scala on both frontend and backend. Unfortunately, scalajs-react isn't very ergonomic, and there aren't any established Scala.js view libraries. I'm writing one - [Domino](https://github.com/scalacode/domino) - for declarative web UI rendering. I'm currently using it to write the frontend for a website, and it is solid like a rock - no bugs, no unexpected issues. Documentation is lacking, but the core idea is simple: generate HTML based on your state, tell Domino to render it to the DOM, and Domino will ensure that the DOM matches the tags you sent it. This is all implemented and working already.
&gt; &gt; Once you start working with Typescript you will quickly see its limitations in soundness, features, and expressiveness. &gt; it's pretty patronizing to assume I've never done so That "you" was not meant as you personally, but as anyone who goes down that path. &gt; I personally have little to no motivation to write client-side code. &gt; I just want to build a frontend app. I want to get the client out of the way so I can focus on the server where all of the possibly interesting work is &gt; focus on the server where all of the possibly interesting work is I don't know what frontend problems you were trying to solve with Scala.js, but since you imply there is not much interesting work on the frontend, it doesn't sound like you have frontend problems that warrant spending the effort on Scala.js. This is completely fine, and I don't know why you're so defensive about not using Scala.js. I never said or implied that everyone should be using it, just pointed out some of its strengths and weaknesses. People like me come to Scala.js because they've been burned by completely unmaintainable 200KLoC JS codebases, and then were burned by uselessness and false sense of security of Typescript / Flow / etc. I know what I'm getting out of Scala.js and I'm pretty happy with the platform for my use case, which involves complex frontend work.
What i have understood from his talks about the collection is, scala's collection are trying to be java collections with a bit of improvements, but he is advocating that the scala community can do better and offer a collection that is sound and correct. I would highly recommend watching one of [his talks about the collections](https://www.youtube.com/watch?v=uiJycy6dFSQ). I think he is no longer working on scala and his collections library is marked as experimental, and i do not know if any shops are using his collections.
I've been playing w/ reactive streams for real-world use-cases. It's led me to go a bit overboard, writing my own io, config, etc utils on top of the usual suspects: https://github.com/aaronp/agora At the moment I'm using akka io to host REST service as a pub/sub lookup so I can distribute local reactivestreams publishers/subscribers. The 'api' part of that is in support of a work-pulling exchange where workers/clients can find each other based on some matching criteria. A lot of that is playground stuff to work to work through some libraries, as well as a means for people to say "why don't you just use XYZ? That does that already but better!" In my mind though a lot of this is relatively light-weight, so potentially easier to bring into things instead of e.g. kafka. Maybe
"Marketing manager." Random phrases with a loosely connected and no real depth, not even a single code comparison 
Im using *Scala.js* as the front end for an MMO server controller. The server is in C# (I know, I wasnt around to help push it over to *Scala*) Right now I can introspect any and all objects arbitrarily on the server. Much of that is thanks to C#s phenominal reflection API. Thanks to Roslyn I build code at runtime from the reflection information, so its very very fast. (sorry, too much C# info) *Scala.js* has been great and fun to work in. Dont even need another templating language, I just use tags. And its so nice to work in the *Scala* language again. 
https://www.functionalgeekery.com occasionally covers scala. Overall a great podcast even when not about scala 
&gt; Why not compare apples with apples and show the difference between, say, the old and new List implementations, assuming no new optimizations have been applied? I’m sorry if this wasn’t clear, but that’s exactly what I’m measuring: the old `Vector` and the new `Vector`. Both are “builder-based”. I’ve also put a “view-based” `Vector` just to illustrate its overhead.
**Reported for referral link spam.** https://www.reddit.com/user/UnkindTabore/submitted/ https://www.reddit.com/user/fegertsa/overview/ https://www.reddit.com/user/trukirukia/submitted/ https://www.reddit.com/user/buyuksd/submitted/
I am saddened by the direction this thread has taken. If people want to kill open source, insulting posts like the ones here are exactly what's needed. I won't comment further. If there are more threads like that I'll unsubscribe and leave this reddit as an echo chamber for yourselves.
Yeah, but as mentioned by others, no one cares too much about `Vector`, which is a slow data structure anyway. The fact that the new version of `Vector` has some additional optimizations not implemented in the baseline make the comparison pretty pointless (I assume that these optimizations could be ported to the old design just as well) – I think most people want to know how the new implementation strategy affects the performance of collections _all other things being equal_.
This is the `split` method on `java.lang.String`, it's not a Scala thing. `\W` matches a non-word character, so you're basically yanking the words out. @ "foo, bar baz!".split("""\W+""") res0: Array[String] = Array("foo", "bar", "baz") 
Its splitting on a regular expression, which is a way of representing specific patterns in a string. Capital w is any non word character (not a-z or 0-9) and the plus means to match continuously. So if you had abc$xyz it would split on the dollar, similarly for abc $$xyz it would split on tje double dollar.
I'm not clear on how people expect fusion to just "happen". It won't happen without some kind of metaprogramming at runtime or at compile time – both requiring language infrastructure that is simply not there ATM. For the record, Haskell can do it because it has a compile-time rewrite rule system and no problems with untracked effects. Shameless plug: I have been developing [Squid](https://github.com/epfldata/squid), a metaprogramming system that [allows to define this kind of optimizing rewrite rules (and more)](https://dl.acm.org/citation.cfm?doid=3136040.3136043), but it doesn't support the full Scala language, and is achieved via macro (no first-class support from the compiler).
Thanks, just one thing still not clear for me: From the github example, when a request is invoked it is used like this: call(wsClient) .andThen { case _ =&gt; wsClient.close() } .andThen { case _ =&gt; system.terminate() } In real usage, obviously I don't want to issue only one request but rather many. What is the correct way to handle closing the client and terminating the actorsystem? 
Why can't stream fusion be a part of a collection library? Am I misunderstanding stream fusion?
Not sure what you mean by "established", but there certainly are scala.js view libraries, e.g. https://github.com/ThoughtWorksInc/Binding.scala
it's funny, I forget to share the link: https://www.meetup.com/Remote-Scala-Meetup/
This is a very valid question. If one picks a random code example like List.range(0, 100).map(_ * 3).filter(_ &lt; 50).map(_.toString).filter(_.endsWith("5")).headOption one gets 4 ListBuffer allocation and 4 List allocation to do things the user never asked for and are not required to arrive at the results. These allocations don't just happen, people wrote code to allocate these unnecessary data structures. The onus should be on the authors of the implementation to explain why they allocate, not on the users why it shouldn't be that way. Requiring things to be grossly inefficient and then trying to undo the damage later has failed as a concept: None of the add-on libraries, macro magic and compiler plugins that people came up with over the last decade to "optimize" the inefficient collection implementation had any kind of success or adoption. 
/u/youngflash aka /u/808hunna/ is using a referral/affiliate link, his entire post history is just spamming his referral link around which is against Humble/FCC rules. Also, he's using his alt /u/blazek_amdrt to try and shame people out of posting Humble charity links so that he gets more referral money. He got his other alt /u/13378/ banned for exactly this nonsense.
/u/blaze3k aka /u/hnic4lyfe aka /u/nigelarchibold aka overly obsessed, online harasser and cyber stalker that's spreading false information. &gt; Reddit Birthday January 13, 2018
&gt; spreading false information So you're saying the posted link is not an affiliate link?
Your object should work okay but you might consider letting the JVM and/or Scala do the work for you because classes are loaded by the JVM at first reference (in a thread-safe way), and Scala has a `lazy val` feature (initialized in a thread-safe or mostly thread-safe way depending on Scala version)^(1). object A { val service = { // init library } } This will initialize the service the first time `A` is referenced from anywhere. If initialization fails with an exception, subsequent references to `A` result in a `java.lang.NoClassDefFoundError`. object A { lazy val service = { // init library } } This will not initialize the service when `A` is referenced, but instead the first time `A.service` is referenced from anywhere. If initialization fails with an exception, subsequent references to `A.service` will reattempt initialization. One way to force things to be correct would be to have the entry point to your library depend on the initialized service. So if you went with the first example you could have case class Library(thirdPartyInitialized: A.type = A) { ... } And then users of your library will either initialize `A` when they want, or it'll be initialized when they try to create your library type. Either way, you are guaranteed that the library is initialized. Pretty much the same thing is possible in the other example if you depend on the return value of `A.service`. 1. I can't recall when Scala addressed the potential deadlock issue when using lazy vals or if that's only in Dotty.
That's terrible documentation, tbh, but I wouldn't put it past the Lightbulb geniuses to come up with something that useless. Usually you won't need to manually close the standalone or actor system (though, it's always good to double check). What you would normally do is DI the client - something like: ``` class Foo @Inject() (wsClient: WSClient) { //example get def bar(url: String): ... = wsClient.url(url) .get() .... } ``` and so on (same with posts, etc), and Play will handle the resource management accordingly. But since you're using the standalone, I would say don't worry about it for now - it's not a big enough deal. 
Thanks for your comments everyone. After looking at all the recommendations, I find Binding.scala to be the most easy one. the tutorials and documentation is pretty easy to follow. I think if Binding.scala doesn't work out for me, then my next step would be to look at typescript. I think trying to wrap Bootstrap and react under scala.js is nothing short of a nightmare (at least for me).
Thanks! I considered lazy vals but I honestly wasn't sure about the exact semantics in this case. I really appreciate the rundown and I'll take that approach into consideration. 
&gt; Am I misunderstanding stream fusion? How would you do stream fusion in Scala? Let's take the simplistic example of a basic `List` data type on which the user applies `map` twice. What do you have the implementation do?
For me, at least initially, documentation was an abomination. The only issue is I can't tell if it's better now compared to 1~2 years ago or my 1~2 years of experience made me understand everything better!
Any idea if those books are worth it (both money and time)? Especially Programming Scala and Functional Thinking in top tier.
I agree with your feelings about scalajs-react. Take a look at https://slinky.shadaj.me/ I've not tried it myself yet, but it looks less complex than scalajs-react.
I remember spending way too much time trying to make things work with type projections, only later fully understanding that objects were really modules and that you were meant to pass actual objects to root path-dependent types in. In that sense, the language doesn't provide much guidance at all – but also it's also my fault, as I never liked reading TFM and always preferred to tinker. Another thing is that for the longest time, the rules for the provenance of implicits have been mysterious and obscure to me, and I did not take the time to properly learn about it until recently; now I feel much more confident when defining type class instances and extension methods.
&gt; Another useful property is that literal values inherit from Singleton: &gt; "hello": Singleton // Works &gt; "hello": Singleton with String // Works Actually, any value is a singleton value, by definition: (x:Any) =&gt; (x:Singleton) : Any =&gt; Singleton Also, I hope that support for unions of literal types will be there eventually, so we can use them as lightweight enums: type MyEnum = "On" | "Off" | "Unknown" def parse(str: String): Option[MyEnum] = str match { s: MyEnum =&gt; Some(s) case _ =&gt; None } // compiles isInstanceOf correctly def use(state: MyEnum) = x match { case "On" =&gt; } // warns about non-exhaustive pattern matching use(state = "On") use(state = "Up") // raises a type error here 
I learn best through pair-programming with more experienced people. I can't really remember having any pains, but I learned Scala very gradually.
Sbt for me. Still a couple things in projects that I plopped in because stack overflow said it would solve my problem. Sure enough it did but still not quite sure why.
Yeah, that's the idiomatic approach to do it. But it's lacking in many ways, which is why there are tons of enums libraries around, and why [they adopted an ad-hoc solution in Dotty](https://github.com/lampepfl/dotty/issues/1970).
Yea especially when you have some implicit that needs another implicit in scope and you just get implicit not found even though it's like the second or third implicit dependency level
&gt; That's terrible documentation, tbh, but I wouldn't put it past the Lightbulb geniuses to come up with something that useless. I thought the same about the use of an Actor system in Play WS. There are lots of old guides for Play WS, and it used to be much easier to use: https://gist.github.com/cdimascio/46b2b7d2986636c1189c
I find Java `enum`s are better than any of the pure-Scala options; you get a `valueOf` method but still have exhaustiveness checking.
I've heard good things about Programming in Scala, and leafed through it a bit. I'd have probably gotten it for that, but was put off as it's only the second edition (2.8), whereas the third edition is out (2.12). Second ed is available [here](http://ccfit.nsu.ru/~den/Scala/programming_in_scala_2nd.pdf)
The Red Book is nice when you get to the point in Scala where you want to understand why for comprehensions are set up to call `flatMap`. That was the point where I started asking questions and one of my coworkers told me to pick up that book. The first few chapters IMO are a bit slow, but once you get into the definition of `map`, `flatMap`, etc., it begins to make a lot of intuitive sense. However I think it's hard to get this intuitive level of understanding without having built a few complex projects that needed to do lots of flatmapping.
Sure, for a simple gist, that works. Play around with it. Update to 2.12, and then maybe actually use the `StandaloneAhcWSClient` with a materializer back. It's up to you
Banning his reddit account will just have him create another. You can report the abuse by visiting here: https://support.humblebundle.com/hc/en-us Filing a ticket takes only a moment and it won't matter how many reddit accounts he makes if he loses his partner status.
Sbt is impenetrable for a beginner Implicits are difficult to get
Trait initialization is a bit of a minefield: https://docs.scala-lang.org/tutorials/FAQ/initialization-order.html Is this fixed in Dotty?
Pain points: 1. SBT. 2. The SBT documentation. 3. Upgrading a building, working, project to a later version of Scala and, therefore, upgrading all of the libraries used by your project. Oh, that line that compiled before - cryptic compiler error now. Good luck with that. Learning - writing code, generally grasping the nettle. Help - Gitter.
I took `Vector` for the following reason: the goal of the article is to compare the performance of the view-based approach and the builder-based approach for providing *default implementation* of operations. However, `List` overrides the default implementations so it’s not a good candidate either. I agree that comparing the performance of the new implementation strategy with the old collections, *all other things being equal* would have been useful too (and, unsurprisingly, in such cases (e.g. `List`) the performance is exactly the same). (But still, I think it’s valuable to show that we can expect the new collections to be faster than the old collections)
Very nice review. Is the presentation (the text file) available? I would like to study it in more detail.
Thanks for your help, much appreciated. But I think I'm still missing the background on the actorsystem. Now my class works as I wanted, except one thing, the app never terminates. What I did is: class Foo @Inject()(wsClient: WSClient) { //example get def bar(url: String): ... = wsClient.url(url) .get() .... } object Foo { lazy implicit val system = ActorSystem() lazy implicit val materializer = ActorMaterializer() lazy implicit val wsClient = StandaloneAHCWSClient() def apply() = new Foo(wsClient) } To try it: object Myapp extends App { def main(args: Array[String]): Unit = { val myfoo = Foo() myfoo.bar("http://google.com") } } The web request is successful, but then the app just "hangs there", and never terminates? What am I missing? I've been reading articles on this topic, and my impression was that the actorsystem needs to be created -in my case- in my main, and then I'm using the client, and then terminate it with system.terminate() 
For lazy collections, you would categorize your methods as lazy (`map`, `filter`, etc) and strict (`size`, `head`, etc). For the lazy methods you wouldn't run the operation at the time of method call. Instead you would have a representation of all of the operations required to generate the current collection from a previous collection (via free monads or something). So, each time a lazy method is called it would produce a new collection with that additional operation added in. These collected operations then all be run through when one of the strict methods are called. I believe this is how views/iterators are currently implemented. I would assume stream fusion could be implemented in the interpreter for your collection of operations. For instance, if it sees that you have chained together 2 map calls, it could combine them in to a single map call. Obviously, this can't work for eager collections, but I don't see why it couldn't be applied to the lazy collections.
I still don't understand F[_] as a return type. I understand higher kinded types just fine but I don't really understand: def f[F[_]](f: F[_]): F[_] = f It seems like it means `Any` in this context.
This is why I created ScalaCourses.com five years ago. I have worked with dozens of languages. Scala is the hardest. Scala is also one of my favorite languages. It took years to get the course content right so students can travel a learning path built right through the tall mountain of capability that is Scala. getscala.com
A:I am foo method and I take an Option and return an Option. B: Options of what? A: i dont care / know. Of something I guess haha
This is gooood.
learnxinyminutes.com/docs/scala
Thanks for this thorough and excellent article, which makes us see both sides of the coin. There is a loud (at times vociferous?) minority who like to push the idea that type classes are the only way forward, but obviously the problem is more nuanced. For one thing, a full type class-based approach is not realistic as a replacement of the current collections, as it would surely break absolutely all the existing Scala code out there – whereas the new design (which is not perfect by any means) only breaks a subset of uses of the most advanced features. I think you explained very well why the new design is still a worthy improvement over the old/current one. Hopefully the performance holds up. I actually like the second `IteratorTypeClass` design you propose. I don't think "but this will leak implementation details" is a valid criticism of it: it is very easy to hide such details (making the `Pointer[T]` type effectively opaque). Anyway, as long as you program against the interface as a user, I don't see why there would be binary compatibility problems. It is clear that the first `IteratorTypeClass` design you propose introduces some overhead (you have to allocate options and tuples), but saying it's not feasible because data structures like `Array[T]` have inefficient `tail` is misleading: in this case, we'd use a saner wrapper over `Array[T]` that simply stores the array along with an index into that array. This is not "relying on `List`'s encoding", but it's true that it may still be slower, and is why I prefer the other approach. 
I agree that trying to return the same collection type, though it sometimes makes things convenient, was not a good idea. But I also think we're beyond the point of changing that (it would break too much code). Can you elaborate on what the problems with `Stream` are? (Beside the fact that it's rather slow.) &gt; if the only thing that some API needs to do is iterate it, then I'd opt for taking an Iterator as an argument `Iterable` is a better choice, because it allows you to perform several unrelated iterations, and because it's not stateful (it's very easy to shoot yourself in the foot with raw `Iterator`s). 
&gt; I also was pretty disappointed to see that Martin has repeatedly shot down the idea of including NonEmpty collections into the new iteration in any form whatsoever, explaining that NonEmpty collections are really just a special case of Collection where { _.length &gt; n } for some n I opened that ticket. He has shot down that idea because he thinks the only way to solve that issue is having type predicates, and he was looking into how having that into Dotty.
&gt; But I also think we're beyond the point of changing that (it would break too much code). I think the code will be broken either way. If you have a look at how long projects like Play or Spark take to adopt recent Scala versions (especially 2.12, where the core focus was on minimizing breaking changes in the language and the library) you can only imagine how long and painful it will be to adapt to the proposed collection library that actually changes things here and there. Unlike in previous Scala versions, where you could just add another version to SBT and cross-compile, it's likely that many projects will need to fork their code and maintain multiple versions of their library going forward. That's exactly why I said–when the design requirements where presented in 2015–that we should not go forward with this. If Scala is to break its collection library, it should at least address the fundamental design issues, which is impossible because but the design requirements do not allow it. I think the advantages and disadvantages are just not in favor of this design, even if a proper fix would break twice as much code. (You only need _one_ place with a breaking change to force people to fork their libraries. Once you crossed that line, the costs of having to fix one place or two places doesn't matter that much.) &gt; Can you elaborate on what the problems with Stream are? (Beside the fact that it's rather slow.) The issue is that the API we have is incredibly specialized on managing a limited set of data that resides in memory and fits into roughly half of the available RAM available to the JVM. That's why you can't use the collections API for IO. That's why Slick has its own – pretty much identical – API to deal with databases that has no type relationship with the API in the standard library. That's why Spark also rolled its own API. That's why Quill has rolled its own API. That's why FS2 has its own API. The computational model that the collections API forces on its implementations is so bad that even the collections API has their own alternative API with views. _Let's completely ignore how utterly bizarre this situation is for a language that lauds itself for its expressive type-system features like higher-kinded types._ The problem is that Stream combines at least three different features that are complex on their own: - Stream operations produce the result on access to the individual element. - Streams buffer their computed elements. - Streams can generate an unlimited amount of elements. Considering the substantial amount of fixes and maintenance over the years and how many bugs Stream caused in users' code, and how broken the code still is – just try mixing Streams and views for instance – the class should have been jettisoned at last when people gave up on ever fixing parallel views. The issue at hand in regard to Seq is the last item in the list. So you have a collection type that is unpredictably broken – and even if it works, many people seem to be unable to use it correctly, with semantics that are completely at odds with the fundamental assumption of the collection API, and shoe-horn it into a collections hierarchy where people expect that a method reliably provides the same semantics across types. Having Stream as a subtype of Seq basically means that we gave most methods on Seq semantics that can only be described as "this method will terminate, or maybe not". I think it is kind of amusing that many Scala devs criticize that unmodifiable views of collections in Java use the same API as mutable collections (with some methods like `remove` just throwing exceptions), but are oblivious to the fact that Stream single-handedly turns most methods on collections into all-memory-consuming, non-terminating, exception-throwing minefields. 
Found this article and I think it finally started to click for me: https://typelevel.org/blog/2015/02/26/rawtypes.html
Anyone in favor of this who can make the case for it? I have hard time seeing the point.
&gt; Creating a wrapper that keeps a start index changes the type. Well yeah, I was implying that users would be using a wrapper over the raw `Array` type, which by itself is very limited and basically unusable. Such a wrapper could be `ArraySlice(arr,i,j)`, which can be given a reasonable instance for your type class, as well as reasonable implementations of `take`, `drop`, `head`/`tail`, `init`/`last`, etc. &gt; This type class version is not useful because it does rely on List's encoding to be efficient and no other encoding is. I'm not sure I understand what you mean by this. If we had proper value types and unboxed options (which are really implementation details), as in Rust for example, I think the instance of this class for `ArraySlice` would be just as efficient as the instance for `List` (or probably more so, in fact, because of the compact data representation implied by the array).
dude, look at your comment history, you have a hard time seeing the point of *any* scala design decision. all you do is criticizing the language any way you can, all the f-ing time.
OP just means that it would be more general to parametrize over `Monoid` because that class makes _less_ assumptions than `Numeric` and would allow, for example, summing (concatenating) a list of strings (even though `String` is not `Numeric`).
I was referring to Haskell's `foldr`. Scala's `foldRight` has been historically unusable. It's good that the implementation is now reversing `List` though.
&gt; That is completely fine IMO True. The problem I see are not the individual features, but combining them all. I believe things would be way easier to handle if these features where cleanly separated. I have seen your work, the things you do are hard and it is truly impressive what you do. Streams has to be at least three times as hard, and I think at this point is obvious that we crossed the line where the complexity is not manageable anymore. &gt; This is indeed a problem, but btw all operations on Stream should terminate immediately, due to laziness. That Stream has methods that aren't lazy, that's the real problem. I think I agree with your first point, but I think the second point is not all there is to it. You can get by with laziness, but at some point, you actually want to do something with the values inside a Stream. And if that something doesn't start with `head` or `take`, you only have delayed the inevitable crash. And that's the issue with `Seq`! You would have to literally introduce `isInstanceOf[Stream[_]]` checks at every single place where you start using values, because if you accept a `Seq` you can never be sure whether you got a `Stream` or something else. &gt; Which is fine btw because I/O requires resource handling and resource handling is a huge can of worms 😉 This is actually another HUGE point in favor of not evaluating things on the spot. Handling resources gets much easier if you do it at the point where you run the computation.
Here's a natural implementation of `foldr` in Scala. Notice that it does not make any assumptions about the structure of the input – and in particular it does not assume that the input can be decomposed as `head` and `tail` efficiently, contrary to what you were suggesting: def foldr[A,B](xs: Iterable[A])(z: B)(f: (A, =&gt; B) =&gt; B): B = { val it = xs.iterator def go: B = { lazy val rest = go if (it.hasNext) f(it.next,rest) else z } go }
There are 3 cases listed in the proposal. Can you explain what's your problem with those?
&gt; For the lazy methods you wouldn't run the operation at the time of method call. Instead you would have a representation of all of the operations [...] This has always been what lazy collections do (and it's the _very reason_ they are called lazy collections). &gt; I believe this is how views/iterators are currently implemented Yep, and that has nothing to do with stream fusion. BTW, this is still orders of magnitude slower than the corresponding imperative loops, because of the indirections. For example see the benchmarks [in this paper](https://doi.acm.org/10.1145/3136040.3136043). &gt; [...] if it sees that you have chained together 2 map calls, it could combine them in to a single map call This might ironically _degrade_ the operation's performance, as you're replacing two virtual calls that the VM has a shot at predicting (method calls on your lazy collection type) with a virtual call on the megamorphic `Function1` class (which you get from composing the two functions at runtime). And again, none of this is somehow made particularly easier by the new design (which was the original question).
Yes. I don't understand the pressing need to address this now, with Java value types around the corner. It seems that this proposed feature will be obsolete in one or two years time. I'm kind of missing the discussion around that in the document. There was a claim that this proposal would be more "zero overhead" than Java value types, but I'm not really seeing that (it is also not mentioned in the text). The person making this claim was also unable to comment on where Java value types incurred that overhead compared to the proposed opaque types, so I can't offer more insights on that. Also, the implicit conversions this proposal adds to some scopes (see Definition) ... does Scala really need more implicit conversions? 
I'd just drop Stream altogether. Iterator generates values just fine, no need for Stream for that, and the lazy evaluation and memoization shouldn't be part of a special data structure, but just a way collection operations can be run (if people consider it useful enough). Because ... if lazy eval and memoization are such useful features, why only provide it for singly-linked lists? Wouldn't these features be equally useful on some array-like data-structure?
&gt; The requirement that every operation collection needs to immediately return a result of the same collection type is a complete game-over for performance and re-usability of the API. I feel like that's what so nice about Clojure-style transducers. You can just build up your computations and then throw them at which ever collection you feel like using. I completely agree, about the problem with Seq and Stream. As someone who writes a function that accepts a Seq, you have to be completely aware, that it might only be traversable once, instead of fully realized in memory. It would certainly increase robustness, if they were not interchangeable.
&gt; I'm not a fan of implicit conversions either but it doesn't seem to be necessary for opaque types to work, it just gives a nicer syntax for operators. Oh, sorry for the misunderstanding, I meant the implicit conversions between the wrapper type and the underlying type in the companion object. The whole rule seems to be quite a special case. &gt; it seems to only cover use-case 1 from the proposal, I think the biggest win is coming from the 3rd use-case, wrapper types around existing ones to increase type safety But you could just do the same with Java value types, couldn't you? From my understanding the JVM won't care if a value type is defined to write methods to do something different (e. g. unsigned arithmetic) than the underlying type, or whether it is defined to wrap an existing type.
Thanks. I didn't know about this corecursive podcast. Looks interesting.
Thanks!
&gt; I meant the implicit conversions between the wrapper type and the underlying type in the companion object. I don't see that conversion anywhere but maybe I'm wrong. The only implicit conversion I see in the referenced section is between the wrapper type and the wrapperOps type and that's not necessary for opaque types to work as I mentioned earlier. &gt; But you could just do the same with Java value types, couldn't you? It wouldn't be the same thing as far as I can see. I'm not a Java expert so take this with a grain of salt and feel free to correct me if I write something stupid but the way I see it is that with Java value types you'd define a completely new type that is(from the POV of the compiler) unrelated to the wrapped one. With opaque types you could preserve the relation and exploit it when writing type class instances for example(kind of like GeneralizedNewTypeDeriving in Haskell).
This is what I had in mind with implicit conversions: &gt; The key peculiarity of opaque types is that they behave like normal type aliases inside their type companion; that is, users can convert from the type alias and its equivalent definition interchangeably without the use of explicit lift and unlift methods. We can say that opaque types are “transparent” inside their type companion. &gt; However, the story changes for users of this API. Outside of their type companions, opaque types are not transparent and, therefore, the Scala compiler fails to compile code that pretends they are. . &gt; I see it is that with Java value types you'd define a completely new type that is(from the POV of the compiler) unrelated to the wrapped one. With opaque types you could preserve the relation and exploit it when writing type class instances for example(kind of like GeneralizedNewTypeDeriving in Haskell). Couldn't the the same be done with value types? I believe there is plenty of code out there that uses annotations to derive various things in Scala already.
&gt;&gt; The key peculiarity of opaque types is that they behave like normal type aliases inside their type companion: The important part is **inside their type companion** so in the example that means only *inside* the Logarithm *object*. &gt; I believe there is plenty of code out there that uses annotations to derive various things in Scala already. I'd prefer to avoid annotations.
&gt; I feel like that's what so nice about Clojure-style transducers. You can just build up your computations and then throw them at which ever collection you feel like using. So far I've always failed to understand what's so special about Clojure's transducers. Isn't what you describe exactly what you can do with the new implementation of views? Or alternatively, with just iterators?
&gt; The important part is inside their type companion so in the example that means only inside the Logarithm object. Yes, that's what I meant. Feels weird to have some magic implicit conversion added to some scope of the opaque type definition. &gt; I'd prefer to avoid annotations. But how would opaque types know which type classes to derive? I'm not trying to argue about annotations being good or bad, but that everything you could potentially do to opaque types could also be done to value types (regardless of whether that is done with annotations or otherwise).
In this case, `Option[_]` is equivalent to `Option[Any]` because `Option` is covariant. But in general it's not the case; for example `Array[_]` is not `Array[Any]` but really `Array[T] forSome {type T}`, so you can't, say, insert `Int` elements into an `Array[_]` because there's no reason to believe that `Int` is a subtype of that `T`.
&gt; Wrong - if you're passing that type-class instance around, that's effectively a vtable so congratulations, you have an OOP encoding with extra garbage. Explicit can be better than implicit. There's value in saying "I am passing around a value of some unknown type and a dictionary of method implementations appropriate to that type" as such, rather than hiding this behind a magical keyword (`extends`) that brings several other bits of baggage depending on what context you're doing it in. You might still prefer the conciseness of the OOP encoding, but there's a real tradeoff here. (I think it might be possible to get the best of both worlds by defining a language in which `extends` was lightweight syntactic sugar for composition, delegation, and typeclass implementation, but Scala isn't that language). &gt; Can you do that with Foldable / Traverse? No, you can't! Why not? &gt; But this will leak implementation details - not bad for our Array instance, but what if we had some sort of self-balancing tree which you then changed to a HAMT. In that case our Pointer would be some sort of node with links to its neighbors, so is it wise exposing it like that? If you don't want to expose it, don't; use an existential to say that the type exists but the user doesn't get to see what it is. &gt; How is Haskell's binary compatibility anyway? Does it even support dynamic linking? I don't know, but the existential-based solution above should work fine in Scala without breaking binary compatibility - if a type is not accessible to client code then it'll necessarily be erased at runtime.
I'd like not to have to choose between the usability of `extends AnyVal` and the performance of tagged types. I don't use the functionality that autoboxing of `AnyVal`s provides me (the ability to violate parametricity, basically) so I'd rather not pay the performance cost of it. I doubt Java value types will turn out to be suitable for performance-sensitive code; even if they are, that doesn't mean a suitable equivalent will be available for every Scala implementation. But really the language-level semantics are more important than the implementation: I want a way to tell the language "I'm not going to violate parametricity with this type, please error out if I do to the best of your ability, and don't compile in a bunch of overhead to help make parametricity violation "work"".
You probably want `Option[Post]`, not `Optional`; `p.copy`not `s.copy`. Otherwise it's readable, it probably gets the job done. No problem.
Oh, yes, some copy paste problems when changing our business context . To me it looks to much like a "hack".. but you think it is ok? Isn't this something a lot of methods would need to do: - get the previous version - update it - update the database - return some success/failure? 
That looks much better! Thanks! why did i not think of that. It mostly felt like a "hack" because i chained the error with the same option value. No repository is a class to hide the database (a data access object). It is very common to use this pattern in java, Is it not used soo much in Scala?
On a side note, your service method looks like a repository one. I don't know your database but in SQL this can probably be handled straight away with a UPDATE ... WHERE ... RETURNING clause or something. Just return a Left if RETURNING returned nothing.
Yes this service operation is just a proxy to the repository, but i have other operations that contain some checks before using the repository 
They are certainly similar to views, in that they don't do anything with the items in a collection right away, until you decide to turn that view back into a concrete collection, and thus applying all operations to it in an inlined fashion and avoiding the overhead of intermediate collections. It's certainly difficult to wrap your mind around transducers at first, maybe because it's challenging to produce proper type signatures for them in a statically typed language. I just think of them as recipes, which only know how to call the next transducer down the line to receive elements, but know nothing about the collection which they will be used on. Simplified (even wrong I'm sure): class Transducer(nestedTransducer: Transducer) { // ? ... probably a collection, but should not be known/not operated on it def apply[A](result: ?, input: A): ? } def filter[A](f: A =&gt; Boolean): Transducer def map[A, B](f: A =&gt; B): Transducer def take(n: Integer): Transducer def compose(xs: Transducer*): Transducer def into(to: Collection, t: Transducer, from: Collection): Collection There is nothing in transducers about collections about the very end.
As someone else said, you could use pattern matching to transform the `Option` into an `Either`, another approach is to use the `Option.fold` method. &gt; updatedPost.foreach(p =&gt; repository.update(postId, p)) This is what I would improve, you are discarding the result from the repository, as someone else said, if you are using SQL, there is a syntax to return the modified value. This could also probably thrown exceptions where you might want to differentiate between errors and failures. Last, this is not safe on concurrent updates because you are reading the whole object, and then, write the whole new object (if there are changes after the read, you'll lost them). If you like, take a look to these examples where I do something similar: [UserPostgresDataHandler#setPreferences](https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/data/anorm/UserPostgresDataHandler.scala#L71) and [UserPostgresDAO#setPreferences](https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/data/anorm/dao/UserPostgresDAO.scala#L126), the main difference could be that I use scalactic `Good or Bad` which is quite similar to `Either`.
You mean something like this? case class Transducer[A,B](apply: Iterable[A] =&gt; Iterable[B]) { def filter(f: A =&gt; Boolean): Transducer[A,B] = Transducer(apply compose (_ filter f)) def compose[C](t: Transducer[B,C]) = Transducer(apply andThen t.apply) def into(from: Iterable[A], to: mutable.Buffer[B]): Iterable[B] = to ++= apply(from) } What's hard to type about it?
What are the differences may I ask?
Yes, I read both, and follow the OpenJDK mailing list discussions. I don't really see how the cross-platform part plays into the motivation. Scala.js doesn't suffer from the restrictions of JVM bytecode in the first place, and Scala-Native kind of ships with "full value types" already (`@struct`). If I'm missing something, please feel free to point it out.
I fondly remember the times when people could discuss technical issues based on their merits without attacks and harassment, that seems to have gone out of fashion sadly. I talk about the good ideas as well as the bad ideas. That there has been a lot of bad ideas to talk about recently ... please don't pin that on me, I'm not the one having them.
Re the value types document: you'll notice that both the current specification and the prototype are not enough to get the performance speedups that John Rose has claimed he can get. The current encoding is insufficient and quite limited, actually. Re Scala.js, how is the JS runtime not suffering from the boxing and unboxing of types? Opaque types should output better optimizable code by JS virtual machines. Re `@struct`, isn't that only Scala Native specific? The point of opaque types is to be platform independent, you get it for free after typer is done with its job. 
I often go back to comments and try to revise things to improve the wording or avoid misunderstandings, but sometimes there is sadly not much opportunity to sugar-coat cases of "this idea is likely to never work given the evidence of people trying it for the last 5 years and failing". In the end, I think we just have different interests in terms of audience. Someone agreeing with me has neither a positive impact on me nor on that person. I'd rather have a single person making me think, reconsider things from a new perspective, or learn something new than 10 people agreeing with me. &gt; Regarding your point about Reddit, I have no idea. I rarely hang out here. No, not really talking about Reddit. I'm talking about the abuse and harassment, the calls for boycotts or blacklisting. Regardless on whether you think these things are justified or not, I think it's not a sign of a healthy community.
One surprise for Java developers moving to Scala is that **for** statements look superficially similar, however generated code can differ massively in run-time performance. A Scala **for** is syntactic sugar for a set of code transformations, and the result is not at all like a simple mutative loop. Efforts have been made to optimize common cases, but beware.
CanBuildFrom was "clever"? Scala cracks me up.
There is nothing special about them. They are a specialisation of what was already developed in lenses. https://www.reddit.com/r/haskell/comments/2cv6l4/clojures_transducers_are_perverse_lenses/
&gt; Also, development on this seems stalled. Seems quite active to me: http://mail.openjdk.java.net/pipermail/valhalla-spec-experts/2018-February/000556.html
&gt; I often go back to comments and try to revise things to improve the wording or avoid misunderstandings It makes me happy you have good intentions after all. &gt; In the end, I think we just have different interests in terms of audience. Someone agreeing with me has neither a positive impact on me nor on that person. If you have a negative or zero incentive on writing your long comments in the discussions, why are you writing them anyway? If I were to write a post pointing out how someone is wrong, I'd like to make it the best comment possible so that I can convince the other person to do it another way, which I personally consider better. If you do enjoy be proven wrong and others finding flaws in your reasoning, I believe it would be good that you optimize for niceness and don't sound as harsh as you typically do, because then someone more knowledgeable may just say "What's the point in spending my time to correct this guy?". &gt; "this idea is likely to never work, given the evidence of people trying it for the last 5 years and failing, and the lack of any new evidence to the contrary". This is a personal opinion. I personally think you should be more lax when making such strong statements. There are many, many different variables at play for any given decision, and there are wise and informed people making the calls at the end. IMO we should all be more lax about what evidence is, and what we think the best approach is. The best approach for who? The best evidence for what? What are the tradeoffs involved? Have all factors been considered? Most of the discussions we talk about, or that have been discussed in this Reddit lately, are not white-or-black problems that have a direct solution, and I've seen you claiming quite harsh things about them, speaking as if you possessed the truth and you were lecturing the rest of the world. You aren't always right, I'm not always right. Let's have a nice, civic and organized discussion with actionable items so that we see if we can agree on "what best is" and how we can get there. Otherwise we're just wasting everyone's time. I can tell you one thing, saying "you guys are doing it wrong and have been doing it wrong for a long time" or "EPFL sucks and ignores the Scala community" (as if you or anyone else would represent all the Community) is not only false, but it's likely not the most assertive way of proving your technical points (if you have them; and, concretely, you often do). &gt; I'm talking about the abuse and harassment, the calls for boycotts or blacklisting that's happening in the community. I have no idea what you're talking about, and I'm in this community for quite a long time. I would suggest that you don't make such bold generalizations about your close environment and do not extend them to the vastness of the Scala community. It may be your reality, it's certainly not mine. This kind of comments is what really makes me perceive Reddit as an unfriendly place to hang out, even if it isn't true. I have no idea what's going on here, but I can tell you the Scala community IS actually a friendly place, and I commit to keep it that way in all the other place I'm around. &gt; On a personal level, I accepted criticism about Scala regardless of where it came from and how it was worded for years. Perhaps you have the time and energy for it... I don't, and I can tell you I spend a lot of time thinking about how to improve things in the Scala language. I have so many things on my plate and so many ideas that the last thing I want to do is to misuse my limited energy and motivation.
&gt; This has always been what lazy collections do (and it's the very reason they are called lazy collections). I know, I just figured I would spell it out to make sure we were on the same page for our assumptions. Especially since you indicated that stream fusion wasn't possible without metaprogramming and I don't see a reason that is a requirement. &gt; Yep, and that has nothing to do with stream fusion. Isn't laziness a requirement for stream fusion? That is the only link I meant to imply. &gt; BTW, this is still orders of magnitude slower than the corresponding imperative loops, because of the indirections. For example see the benchmarks in this paper. I can't read the paper because it is behind a paywall. However, I'm not terribly surprised that lazy collections are generally slower than imperative loops. I figured the point of lazy collections is that it makes it easy to express your computations in a way that avoids doing unnecessary computations. &gt; This might ironically degrade the operation's performance, as you're replacing two virtual calls that the VM has a shot at predicting (method calls on your lazy collection type) with a virtual call on the megamorphic Function1 class (which you get from composing the two functions at runtime). Do you have benchmarks that it would degrade performance? Don't Java 8 streams have stream fusion because it improves performance? &gt; And again, none of this is somehow made particularly easier by the new design (which was the original question). My understanding of the original question was that they were asking if stream fusion would be included as part of the new design. Hence my response. However, I think it would be easier to include in the new design since operations are lazy by default. So views and iterators and streams might not need entirely separate implementations. 
You brought Iterable into it, which got no business being in the definition. It should be agnostic to what ever it might happen to have to work on, be it Iterable, Collection, channels or what not. Something that has no common super type.
I'm not sure you're adding anything useful to the comments you link to. The insults you add, on the other hand, will make sure you'll get dismissed. Worse, next time somebody tries to actually discuss those problems, they'll be more likely to be ignored because you're being abusive, unless they manage to be especially articulate in arguing their thesis. I don't necessarily agree with DJSpiewak's document, but he gives actual arguments one can debate. Many of them aren't so clear cut. Saying that something "doesn't make sense" is a category error, since not everybody needs the same things from collections, and everybody seems to use unstated assumptions or gut feelings about what use cases are more important. And believe it or not, you can't take for granted that something's good just because Haskell does it.
I remember he iterated through multiple designs but I don't think he ever converged to something he was happy with. Which is not to say that his criticism isn't valid — but I'm not sure anybody has a complete solution.
I was really really confused about the multiple ways to declare and write functions, the differences between procedure syntax and the proper one with equals and then the similar types Function when assigning to vals that are actually a completely different thing... grrr. And sbt, that thing is completely impenetrable. Also lack of proper documentation and examples for things not in easy begginer mode and not in the max hard level. There's a huge gap for documentation. Also doesn't help that over the years it seems scala changed a lot and we still find pretty old information totally not relevant to current usage of scala. I started on scala 2.10 and quickly jumped on 2.11 so I have no idea of what it means when people talk about some changes related to that.
&gt; Oh, sorry for the misunderstanding, I meant the implicit conversions between the wrapper type and the underlying type in the companion object. The whole rule seems to be quite a special case. Most of the ideas behind that are pretty old — opaque types behave like abstract types, except in the companion object implementing them. This is a form of information hiding (in OO terms). You can find an encoding which gives similar behavior using abstract types and upcasts by searching for "No boxing with type tags" under https://failex.blogspot.ch/2017/04/the-high-cost-of-anyval-subclasses.html. The reason why opaque types cannot be easily implemented using that encoding is that abstract types usually erase to `Object`, forcing you to actually call the coercion functions, or at least to insert casts in the bytecode. Opaque types instead behave like abstract types for typechecking, but can be replaced by their implementation when compiling the code. I think the difference between opaque types and the encoding is supposed to mostly/only affect performance and not semantics, and that makes it easier to justify the few ad-hoc aspects of the construct. Right now, this proposal seems almost orthogonal to value types, since it just defines type aliases whose implementation is hidden, but does not discuss how to handle tuples (unless I miss something?).
&gt; I find the assumption that one has to point out good things before being allowed to also mention a few bad things rather absurd. A point that stands alone should neither be strengthened nor weakened by having unrelated good/bad criticism around it. your problem is that you are *really* good at finding bad things, much, much better than anyone else. that doesn't suggest "an opinion", that suggests bias! &gt; That there has been a lot of bad ideas to talk about recently ... please don't pin that on me, I'm not the one having them. case in point. oh, the irony...
&gt; your problem is that you are really good at finding bad things, much, much better than anyone else. that doesn't suggest "an opinion", that suggests bias! I think it largely indicates the effort spent on identifying, deprecating and removing these things over the years. So what you are objecting to doesn't seem to be the findings, but that they aren't coming with a PR anymore, because I can't remember you getting angry before that.
I caught on to a lot of the basic language stuff pretty quickly but it took me lots of bikeshedding to develop the principles of organizing a large system I use today (when to use traits or classes, parameters or abstract members, etc etc etc). Stack Overflow was immensely helpful for me in getting context to make better decisions and Gitter more recently has been useful. On the bright side, I find the Scala has sort of a "bumpy valley of success", where the first instinct is pretty decent, and subsequent refactors just make things better and better.
Yes, it's the age-old question of which connection type aliases/new types/... retain with the type they alias/wrap/new type. &gt; I think the difference between opaque types and the encoding is supposed to mostly/only affect performance and not semantics [...] In the end, I think value types could have adopted semantics – similar to what opaque types propose – that were easier to erase, but people decided that living with that overhead was worth the semantics. I wonder what new evidence has appeared to dismiss this approach shortly before Java starts supporting value types itself that eliminate the overhead Scala's value types suffered from. &gt; that makes it easier to justify the few ad-hoc aspects of the construct From my point of view, there are quite a few parts that are ad-hoc. (In addition to adding a new keyword and new syntax which is kind of a red flag in itself.) If you think about it, defining a method on an opaque types basically entails "define a method inside an implicit class inside a type companion alongside the opaque type inside a package object". That's seems incredibly ad-hoc to me. The way some implicit conversion just "exists" in some pre-defined scope feels pretty ad-hoc. The performance claims also seem to be quite ad-hoc ... opaque types won't safe you from boxing in `opaque type Color = (Float, Float, Float)` while that example is probably one of the most basic goals of what native value types will achieve. &gt; Right now, this proposal seems almost orthogonal to value types ... [This video](https://youtu.be/h48kuflGEDw?t=34m16s) indicates that opaque types replace value types: "I'm not going to fight this out, I will just do it" Taking that all together, I think it is hard for me to see how opaque types add enough benefit and provide so many advantages to replace value types. It throws away the semantics people thought were important enough to have to pay a performance penalty for them, without considering that the performance penalty for these semantics will be eliminated in the foreseeable future.
To be sure who I'm talking with, you are https://github.com/soc, right? (I've heard different opinions on this).
&gt; If you have a negative or zero incentive on writing your long comments in the discussions, why are you writing them anyway? I think that's a rather coarse description of what I said. I have no incentive to make people agree with me, but I have high interest in learning something new. If there is one person raising some point I haven't considered, that comment isn't made better or worse by 10 people writing "I agree with him/you" underneath. &gt; The best approach for who? The best evidence for what? What are the tradeoffs involved? Have all factors been considered? I think that's a basic prerequisite to voicing strong opinions, and a requisite I religiously follow. I only voice a strong opinion after I have done my homework and can back it up with facts, evidence and examples. If you go through the various things I said in the past, you will find that this is the case for everything I voiced opinions about. "you guys are doing it wrong and have been doing it wrong for a long time" might not be the most assertive way of proving technical points, but when it is true, it needs to be pointed out. Sugarcoating things were the verdict is overwhelmingly clear would often just misrepresent the weight of evidence that points into a certain direction. &gt; I have no idea what you're talking about, and I'm in this community for quite a long time. Most of these things happened quite publicly and where things happened non-publicly you can often figure out which person to ask. For instance, the latest incident of calling for the blacklisting of people attending a conference is [this](https://twitter.com/justkelly_ok/status/960797547068641280). If people say "I don't want to have a picture of my face appear in a call to not hire people for attending a conference." I tend to believe them. The last time someone in a leadership position tried to get someone else fired from their job by contacting their employer happened not too long ago. It's not hard to find. &gt; I would suggest that you don't make such bold generalizations about your close environment and do not extend them to the vastness of the Scala community. I made a lot of efforts of reaching out and asking people for their opinion about things. I made it a point of regularly going through the commit history of Scala projects and reaching out to people who silently dropped out. I listened to what they had to say. Often they had very benign reasons for not participating anymore, like a change of jobs, family business, or other interests. But there were a lot of people that were deeply unhappy about how they have been treated and whose decision to not participate or contribute anymore was based on that treatment. I'm not saying you haven't done that, but a common theme when I reached out to them out was "You are the only person who has reached out to me and listened to what I had to say. I feel like nobody else cares.". So we may disagree on what the reality is here, but I think actively going out and asking how people feel about things gets one closer to reality than saying "I dismiss what people have to say if I don't like their tone.".
I agree opaque types don't handle tuples — for those you need VM support. Opaque types are like Haskell newtypes without exported constructors. &gt; In the end, I think value types could have adopted semantics – similar to what opaque types propose – that were easier to erase, but people decided that living with that overhead was worth the semantics. &gt; I wonder what new evidence has appeared to dismiss this approach shortly before Java starts supporting value types itself that eliminate the overhead Scala's value types suffered from. When you say "value types" do you refer to Scala value classes? No clue, I was honestly confused back then as well. I've never seen evidence the semantics was needed back then. &gt; It throws away the semantics people thought were important enough to have to pay a performance penalty for them, without considering that the performance penalty for these semantics will be eliminated in the foreseeable future. Both http://mail.openjdk.java.net/pipermail/valhalla-spec-experts/2018-February/000559.html and http://cr.openjdk.java.net/~acorn/LWorldValueTypesFeb13.pdf suggest instances of Java value types won't have object identity. So what semantics would get cheaper then? It's not clear if value types support `getClass` either. What *is* cool about Java value types is that you'd get multiple members, and I agree that's probably worth looking at. &gt; If you think about it, defining a method on an opaque types basically entails "define a method inside an implicit class inside a type companion alongside the opaque type inside a package object". That's seems incredibly ad-hoc to me. That sentence sounds indeed a bit long, though I can see what causes each design decision. Lots of it comes directly from the encoding though — in fact, this is a pretty small variation. The only other design I know for such a feature is the one from ML modules, which seems to also have lots of boilerplate, similarly to the encoding shown. What would you have instead? &gt; The way some implicit conversion just "exists" in some pre-defined scope feels pretty ad-hoc. Yeah, I suspect that part is under-specified, not sure what's plausible in the existing compilers. At first sight, that shouldn't be a real implicit conversion but a type equality. &gt; &gt; Right now, this proposal seems almost orthogonal to value types ... &gt; This video indicates that opaque types replace value types: "I'm not going to fight this out, I will just do it" I meant that Scala opaque types seem orthogonal to Java value types.
*Programming in Scala* is amazing because it gives an overview of the language and the standard library with the motivation behind why the way things are, with concise tables for common library functions used. This is what I'd recommend to folks new to Scala, followed by *Functional Programming in Scala*. I haven't read Wampler's *Programming Scala* but I haven't heard the same about it.
Just that you were soc and simon_o was somebody else who worked on &lt;something I didn't quite get&gt;.
Last thing I heard was that I was on JetBrain's payroll with the objective to destroy Scala or something. People have imagination, it seems.
* I still don't understand sbt * I didn't understand for a long time what's up with how I can use () or {} interchangeably in some places but breaks in other places but it DOESNT BREAK BUT CHANGES SEMANTICS in other places * I still don't understand the type system entirely tbh but my biggest pain was trying to figure out what the gosh darn heck is going on in the popup window in intellij when I look at the signature of something as basic as a Map
What a *darn* shame.. *** ^^Darn ^^Counter: ^^429694
Well... 1. How do you expect people to connect all your identities if you never use the same nick? 2. Some people do have imagination, sure. But you seem to assume that effecting change in a social organization only requires technical skills instead of social ones (including "being nice", yes). Whoever gave you that idea lied to you. That's a fallacy at least as big as the ones you're quick to point out, even though unlearning it is pretty challenging. If you want to know how soul-crushing this style can be, apparently there's a whole company set up that way. It's Amazon. People have written articles about its effects (I've found https://www.nytimes.com/2015/08/16/technology/inside-amazon-wrestling-big-ideas-in-a-bruising-workplace.html?smid=tw-share&amp;_r=0, though I thought I had read a different one). I've lived in similar environments, and I can tell you that doesn't help sanity or results. So, when people push back, they're trying to preserve their sanity. And I 100% agree people should (1) preserve their sanity and (2) try to do the best they can, in this order. Where "the best they can" is probably not a wartless Scala — seeing how long Haskell took, and how long ML researchers are taking to give clean designs to pieces of Scala, that sort of thing might take multiple decades.
There indeed is an officially announced intention to replace collections in 2.13 (including the blogposts that I referred to in my comment above). However, the officially announced time for 2.13-final (H1 2018) is running out, and the official repository for the new collections is still saying "Prototype improvements for Scala collections". Without being familiar with internal plans, it is hardly possible to discern the status, let alone the roadmap of the effort. What I think is missing in is the communication about what exactly will be shipped and when. Concretely: 1. Is there going to be a milestone release of 2.13 that will include the new collections? 2. After a milestone release that includes the new collections, how will the 2.13 roadmap look like (i.e. how much time will industrial Scala users have to test the new collections)? 3. What is the compatibility / migration story for the new collections (i.e. how much effort will industrial Scala users have to manually apply to upgrade)?
My comments are exclusively about the roadmap, not the actual design of the new collections. Since the new collections are planned to be shipped in a production release of the Scala compiler, it would help if release plans were announced well in advance. Some of us have millions of lines of Scala code at work, which means that testing all this code against the new collections is going to take a while. Therefore, we would like to have an advance notice to be able to schedule this work on our roadmaps, find time for testing and have an opportunity to provide meaningful feedback. Concretely, I have the following questions: https://www.reddit.com/r/scala/comments/7wdcms/on_performance_of_the_new_collections/du9z20q/.
At this point I have more questions than answers, but I'll just ask one: &gt; The thing is that they already have helper methods like ValueType.identity in their prototypes and a definition of what should happen when acmp is used in conjunction with value types, which is underspecified and underdocumented, but makes sense in general. Not sure what you're hinting at, but value types with object identity sounds like a contradiction. I'll have to take a look at the rest.
I think people shouldn't need to connect identities, whether someone agrees or disagrees on some technical thing should not depend on who is saying it. I'm not assuming that effecting change in a social org relies only on technical skills. I was an open-source contributor, and I was happy to offer technical help on things in my free time. Some people feel like everything should be turned into some public social happening, and I can understand that this is important for people in cultures and environments where work consumes a large substantial part of their live and who they are. But in my case, I was a volunteer who was offering help on technical things. I had no interest in socializing. If I want to socialize, I just leave my house. It's as easy as that. Despite that, I spent a tremendous amount of time and effort to listen and talk to people individually. (See other comment.) If I look back to the years I contributed, the thing that cost me the most time and burned me out the most was not the technical work, but the social work that I took on – despite having no interest – in the grand scheme of getting things done. I'm fully aware of the advantages and disadvantages of focusing on technical skills, and fully aware that some things might have played out differently if I had spent more time on social interaction.¹ But at many points in time I already spent 80% on social and only 20% on technical stuff. As an outside contributor that gets exactly nothing for contributing, I have no obligation to keep working on things I don't enjoy. (Even if some people seem to believe that.) When I talked to people in private about my departure and the general level of abuse and harassment I often just got reactions like "This is unacceptable. I would have quit the minute they did X, or Y, or Z. I have no idea why you went on contributing after this." ¹ ... and some things would have went exactly the same thing. I spoke to a very well-known community figure a while ago who had pretty much given up on ever seeing improvements to some important parts of Scala. He just couldn't see how any external effort would be able to cut through the layers of EPFL politics and people defending their fiefdoms from outside ideas. (His words, not mine.) Another contributor who quit recently said to me "they don't care, they just don't listen to anything".
I think the problem is that they are using "identity" in a much wider scope than we do. For them identity also encompasses things like synchronization, locking etc. For us (I guess) identity and equality is more about the specific questions of "is this thing identical to that thing?" and "is this thing equal to that thing?". The thing is that they need to deal with identity (in the more specific sense) in some way or another, because otherwise changing e. g. a `HashMap&lt;Double&gt;` to `HashMap&lt;double&gt;` would not only change the behavior, but would also make it impossible to recover the existing behavior (outside of doing `instanceOf double` checks on every generic type. This specific question is also an important thing in Java, but you have to scroll way down before they mention it in the document. You can see this under the section "acmp behavior options" in the document you linked. Their first option amounts to irrecoverably changing the semantics whenever people want to switch their code (or some library they don't even know they are using) from boxed primitives to unboxed primitives. The second option would require rewriting all existing code that uses `==` (as I mentioned earlier). The third option is treating identity as a valid question with a valid answer (the thing that has been proposed for Scala years ago, and makes the most sense to me), that is necessarily different from equality in some cases. But unlike the first two options, the third option has a substantial impact on Java-the-language and a few problems that are hard to solve, e. g. "how to expose identity and equality to users in a way that works for arbitrary `any T`s, considering that `==` means equality for primitives, but identity for references already?". So I can understand why they are trying to get away with the cheaper, more ugly approaches first.
does `Witness` use `Poly` anywhere? There's a similar problem with `Poly`, where associativity seems to be broken and you must use brackets as you have. https://www.reddit.com/r/scala/comments/774byb/composing_shapeless_polymorphic_functions_over/ Other than the problem looking similar, I'm not familiar with `Witness` so can't help much.
&gt; You brought Iterable into it No, not really – it's an implementation detail. Here is the interface of that class: type Transducer[A,B] &lt;: { def filter(f: A =&gt; Boolean): Transducer[A,B] def compose[C](t: Transducer[B,C]): Transducer[A,C] // etc. def into(from: Iterable[A], to: mutable.Buffer[B]): Iterable[B] // can also have other methods acting on other collection-like types } &gt; agnostic to what ever it might happen to have to work on, be it Iterable, Collection, channels or what not It would be easy to implement this, being with ad-hoc overloading (the closest to Clojure's approach AFAIK), type class-based overloading, or yes indeed a common super type (your restriction on this particular implementation detail seems unjustified BTW).
&gt; Regarding Scala.js: The boxing is more or less given anyway and dropping the restrictions on value types would not change much of that. I think no one will make a decision on JavaScript or Scala.js that is focused on the overhead of value types, and opaque type won't save Scala.js from stuff like having to split a Long value into multiple floating points. Puf. Seriously, don't BS me with corner cases or random examples that cannot happen. The case is that Scala.js and JS runtimes would benefit from opaque types, more specialized code at link time means better opportunities for optimization and less conversions to Any, especially for a powerful optimizer like Scalajs has. &gt; Regarding Scala-Native: The point is that the infrastructure for dealing with value types is already there. If Scala drops the restrictions on value types when Java value types arrive, Scala-Native could wire the trees coming from scalac to the existing infrastructure and probably drop @struct altogether. That means runtime-specific implementations of value types. Avoiding that is exactly the point of opaque types. The SN guys don't have the time or budget to implement that. Denys could probably shed some light in here.
&gt; I can't read the paper because it is behind a paywall The first result on Google is [the freely-available PDF](https://infoscience.epfl.ch/record/231076/files/qsr-gpce17-2.03-pn.pdf). &gt; I'm not terribly surprised that lazy collections are generally slower than imperative loops What's compared is not even lazy collections, it's _bare, low-level iterators_! &gt; Don't Java 8 streams have stream fusion because it improves performance? There is some confusion about what "stream fusion" means, but at least this is not the way it's been used in the functional programming community. What Java 8 does is just called wrapping iterators in a nicer interface and using function composition.
&gt; Seriously, don't BS me with corner cases or random examples that cannot happen. Not sure if anything changed recently, but when I checked, there was no other way to operate on Longs than splitting them into multiple JavaScript numbers. So it's not a corner case or a random example, it's literally how all compile-to-JavaScript languages work. &gt; That means runtime-specific implementations of value types. Avoiding that is exactly the point of opaque types. The SN guys don't have the time or budget to implement that. Denys could probably shed some light in here. `@struct` would probably not exist in the way it does if Scala had multi-field value types (modulo mutability, I guess). I think saying that Scala-Native has their own runtime-specific implementation of multi-field value types because Scala's value types don't support that, but supporting potential Scala multi-field value types would require a runtime-specific implementation is kind of a circular reasoning.
&gt; I think that's a rather coarse description of what I said. I have no incentive to make people agree with me, but I have high interest in learning something new. If there is one person raising some point I haven't considered, that comment isn't made better or worse by 10 people writing "I agree with him/you" underneath. Well, my second paragraph still applies. Read it again. &gt; I think that's a basic prerequisite to voicing strong opinions, and a requisite I religiously follow. Most of the times you do it wrong, you are not even able to discern what the audience are and what the real goals are. Be more rigorous. &gt; "you guys are doing it wrong and have been doing it wrong for a long time" might not be the most assertive way of proving technical points, but when it is true, it needs to be pointed out. Sugarcoating things were the verdict is overwhelmingly clear would often just misrepresent the weight of evidence that points into a certain direction. Do you realize I could say the same about what you have been doing in the Scala community as a volunteer? This is not an argument. Have you ever stopped to think why your priorities were so different from other people's: Scala Center's, EPFL's, Lightbend's and even other contributors' priorities? Perhaps they were not as important as you thought? You dismiss epistemic modesty (http://effective-altruism.com/ea/1g7/in_defence_of_epistemic_modesty/) and think that by quoting isolated technical points you're able to prove something. And at the same time you don't realize how many variables you're leaving out. This is intellectually insulting -- do consider the full context, including hidden motivations and unknown evidence, before making your case. &gt; For instance, the latest incident of calling for the blacklisting of people attending a conference is this. If people say "I don't want to have a picture of my face appear in a call to not hire people for attending a conference." I tend to believe them. So you're quoting one example of an incident in one community that has around 500.000 developers. Do the maths, man, and compute the number of evidences that we would need to have your claim be statistically significant. Generalizing like you do is out of context and only proves to point the underlying point behind every of your comments: the Scala community is not a friendly place. Well, sorry to tell you, that's not the case. &gt; I'm not saying you haven't done that, but a common theme when I reached out to them out was "You are the only person who has reached out to me and listened to what I had to say. I feel like nobody else cares.". &gt; But there were a lot of people that were deeply unhappy about how they have been treated and whose decision to not participate or contribute anymore was based on that treatment. Welcome to human life. Any social system in place will have people with these reactions -- it happens at any scale. This is out of scope and does not prove your point. &gt; So we may disagree on what the reality is here, but I think actively going out and asking how people feel about things gets one closer to reality than saying "I dismiss what people have to say if I don't like their tone.". No matter how lax you're to interpret the evidence of "I feel prosecuted by my ideas", you cannot claim this applies to the vastness of the Scala community. You act in an irrational way. If you really wanted people to correct you when you're wrong, you would write your comments with a pinch of doubt and making a better use of the language to show uncertainty -- so that people feel motivated to correct your mistakes. It could be that you have a lack of understanding of how people work in social circles, and you don't realize that... but I doubt it. I personally think that what you really want to say with your technical, out-of-context appreciations is: "Fuck you guys, I'm right, I've always been right -- and I'm still chasing you!". And you want to say that to people (not me) with way more merits and expertise than you do.
&gt; Not sure if anything changed recently, but when I checked, there was no other way to operate on Longs than splitting them into multiple JavaScript numbers. This still applies, and it's a concrete feature of the JS runtime. At the same time, it has nothing to do with opaque types and does not prove your point: a JavaScript VM can more effectively optimize primitive types (no matter how expensive handling them is) than objects. &gt; @struct would probably not exist in the way it does if Scala had multi-field value types (modulo mutability, I guess). I'm not an expert in SN, and you're not either. So don't guess. If you have a serious question, go to Denys and ask it. I'm gonna stop the technical discussion here. It's really not adding anything.
I don't think so. When using `Witness` you trigger a Macro which creates an instance for you and this Macro does not use `Poly` as far as I can see. Furthermore, this problem already appears when you use a single call of `:&gt;:`. 
A colleague mine found the source of this issue: @ desugar { "hello" :&gt;: "world" :&gt;: Do } res23: Desugared = { val x$2 = "hello"; ({ val x$1 = "world"; ammonite.$sess.cmd4.Do.:&gt;:[this.T](shapeless.Witness.mkWitness[x$1.type](x$1.asInstanceOf[x$1.type])) }).:&gt;:[this.T](shapeless.Witness.mkWitness[x$2.type](x$2.asInstanceOf[x$2.type])) } @ desugar { Do.:&gt;:("world").:&gt;:("world") } res24: Desugared = ammonite.$sess.cmd4.Do.:&gt;:[this.T](shapeless.Witness.mkWitness[String("world")]("world")).:&gt;:[this.T](shapeless.Witness.mkWitness[String("world")]("world")) It seems like a side effect of syntactic sugar resolution in Scala.
A colleague mine found the source of this issue: @ desugar { "hello" :&gt;: "world" :&gt;: Do } res23: Desugared = { val x$2 = "hello"; ({ val x$1 = "world"; ammonite.$sess.cmd4.Do.:&gt;:[this.T](shapeless.Witness.mkWitness[x$1.type](x$1.asInstanceOf[x$1.type])) }).:&gt;:[this.T](shapeless.Witness.mkWitness[x$2.type](x$2.asInstanceOf[x$2.type])) } @ desugar { Do.:&gt;:("world").:&gt;:("world") } res24: Desugared = ammonite.$sess.cmd4.Do.:&gt;:[this.T](shapeless.Witness.mkWitness[String("world")]("world")).:&gt;:[this.T](shapeless.Witness.mkWitness[String("world")]("world")) It seems like a side effect of syntactic sugar resolution in Scala.
You're right, it fails with a single `:&gt;:`. Looking at the types, the type of ``` val test1 = "hello" :&gt;: Do ``` is ``` Cons[x$1.type :: shapeless.HNil]( forSome { val x$1: String }) ``` But the type of ``` val test2 = Do.:&gt;:("hello") ``` is ``` Cons[this.T :: shapeless.HNil] ``` It looks like a bug with macro type inference. It's beyond me, I'd suggest raising an issue or asking in the gitter channel.
You are right, I’m updating our repository to answer these questions.
&gt; I think the general approach saying "opaque types are good because they handle backend stuff in the frontend" didn't work out too well for @specialized. `specialized` was a solution to the problem in the wrong place. Opaque types learn a lot from the mistakes of specialized.
Eventually Scala people tend to find themselves preferring a monadic style where you construct an effectful operation that needs to access the database and do the access to the database in one go, rather than interleaving database access with normal code in a "it runs when it runs" way. But I wouldn't recommend trying to leap straight to that style.
&gt; I don't understand how adding another option instead of preparing AnyVal for Java value types looks like a promising idea. I don't see it as an either/or. To the extent that it's possible to improve the efficiency of the JVM implementation of `AnyVal` of course that work should happen. &gt; The limitations value types have in terms of performance (boxing) and ergonomics (no multi-value value types) don't exist because people wanted them or didn't know better. These limitations were accepted at the time because people hedged their bets on Java coming around on value types sooner or later, so that the existing limitations of the implementation could be dropped in the future. &gt; Now that the predictions have come true, it's kind of surprising that people are walking away instead of getting the pay-off. The scenarios where boxing happens have turned out to be much more common than expected at the time, that's what's driving this. In the light of the experiences we've had with `AnyVal`, we can make different choices. &gt; I think that would be deeply baffling. That's one of the most fundamental requirements that Java value types have. If your Complex value type consists of a real and an imag value, you are basically operating on two naked 64 bit values, no klass header, no locking bits, no hash code, no dynamic dispatch. Well, there's an excluded middle: either Java value types will not be as efficient as opaque types, or they will not be suitable for implementing `AnyVal` with its current semantics, because that kind of runtime behaviour offers much weaker guarantees for parametricity-violating code than current Scala `AnyVal`. The current semantics can't be implemented efficiently, so we need weaker semantics, and the only way to get weaker semantics without breaking existing code is to introduce a new type that has the semantics we want. The opaque type proposal seems like a good point on the functionality/efficient implementation curve; using Java value types is not an end in itself if we can achieve the same thing without needing them. &gt; But why would this require a completely new language feature instead of Scala selectively exposing or hiding what Java gives us in terms of improvements to value types? I've always wanted user-defined types to be able to opt out of implementing the `java.lang.Object` interface, and every step in that direction is necessarily a language-level change. I don't know what you're proposing by "selectively exposing or hiding what Java gives us in terms of improvements to value types", but I don't think the Scala does or should regard it as acceptable for an object to appear to implement an interface where making use of that implementation will behave incorrectly at runtime; if we are to have (whether via Java value types or by some other implementation) values that don't implement object identity, monitor waiting etc. at runtime, then those values need to not offer those interfaces at compile time, and that's necessarily going to be a language change.
I don't see a monorepo as a good idea; what I prefer is (with maven) setting versions of library dependencies in an organisational parent pom. That way projects can use "FooCorp Parent 3.4" and get a known-working set of versions for the core libraries we use everywhere, but each project can do the upgrade at its own pace particularly for big changes like Scala version upgrades (and EOLed projects can stay on "FooCorp Parent 3.2" or whatever), and if a given project really needs to override a particular library version (carefully) then it can.
&gt; I don't see it as an either/or. To the extent that it's possible to improve the efficiency of the JVM implementation of AnyVal of course that work should happen. Isn't that kind of pointless when value types get removed and replaced with opaque types? Why work on AnyVal when it's on the chopping block anyway? &gt; The scenarios where boxing happens have turned out to be much more common than expected at the time, that's what's driving this. In the light of the experiences we've had with AnyVal, we can make different choices. But in which way is the choice of forgoing value types in favor of opaque types being advantageous in terms of performance? Java value types can pretty much completely address the performance issues of Scala's value types, that was the premise of the design of Scala's value types when they were created. Opaque types on the other hand will not benefit from Java's support for native types on the other hand. `opaque type Color = (Float, Float, Float)` will incur multiple levels of boxing that will disappear with Java value types. &gt; either Java value types will not be as efficient as opaque types, or they will not be suitable for implementing AnyVal with its current semantics, because that kind of runtime behaviour offers much weaker guarantees for parametricity-violating code than current Scala AnyVal I'm not sure if I fully understand which guarantees you have in mind, and where runtime performance would suffer compared to opaque types. Do you have some examples of the issues you have in mind? &gt; The opaque type proposal seems like a good point on the functionality/efficient implementation curve I wonder whether the same semantics couldn't simply be provided by adding an annotation to value types, instead of inventing a completely new syntax? &gt; using Java value types is not an end in itself if we can achieve the same thing without needing them As mentioned, I have trouble seeing how e. g. the Color type mentioned above can be implemented efficiently without using Java value types. &gt; if we are to have (whether via Java value types or by some other implementation) values that don't implement object identity, monitor waiting etc. at runtime, then those values need to not offer those interfaces at compile time, and that's necessarily going to be a language change. But isn't much of that the case already with AnyVals and universal traits, e. g. not having `eq` or the methods dealing with locks?
&gt; Explicit can be better than implicit. There's value in saying "I am passing around a value of some unknown type and a dictionary of method implementations appropriate to that type" as such, rather than hiding this behind a magical keyword (extends) that brings several other bits of baggage depending on what context you're doing it in. You might still prefer the conciseness of the OOP encoding, but there's a real tradeoff here. (I think it might be possible to get the best of both worlds by defining a language in which extends was lightweight syntactic sugar for composition, delegation, and typeclass implementation, but Scala isn't that language). I don't think this is true, they are both equally explicit. Its just that one is explicit in the definition site (`extends`) and the other is explicit in the call site (`implicit f: F[...]`). In any case if you are passing that typeclass instance around it has terrible performance, this is what was meant by that vtable comment. &gt; If you don't want to expose it, don't; use an existential to say that the type exists but the user doesn't get to see what it is. Actually this isn't really true. Its not about completely hiding the internal type, its about only hiding what is neccesary. In some way this is kind of the fundamental difference between OOP/FP (in terms of Scala). In one case you code against interfaces (OOP) and in other case you are coding against implementations (FP). Sometimes you don't want people to code against an actual implementation provided against a typeclass because its too inflexible. Its not that you want to hide everything (which is what existentials do), its that you want to only expose some sought of interface. The point is that he wants to expose an `Iterable` type interface without the downsides of coding against a `Foldable` or `Traverse` typeclass.
Perhaps I am lacking context, and I do not wish to invest to acquire it, but that sounds pretty insane. We have NEL. In Java. Since forever. https://github.com/functionaljava/functionaljava/blob/master/core/src/main/java/fj/data/NonEmptyList.java
But this is not stack safe, as was already mentioned
&gt; I think people shouldn't need to connect identities, whether someone agrees or disagrees on some technical thing should not depend on who is saying it. Sure, but once I know who I talk to I'll adapt what I say (say, spell out fewer things). &gt; As an outside contributor that gets exactly nothing for contributing, I have no obligation to keep working on things I don't enjoy. (Even if some people seem to believe that.) I think that's clear (to me at least), what's less clear are the implicit social contracts we use when interacting. If you want that people engage with your comments, those people might have demands on the comments. Anyway, I shouldn't have tried to advise on communication on the Internet, this is per se too hard. Just be aware that part of the reaction you might get has to do with your tone. And others might wonder why you look at these things if you have given up on Scala. &gt; On contributors giving up I'm aware of the concerns. The rest of the world isn't paradise (http://danluu.com/wat/), but I'm curious to understand better the other side of the issue. The easy part is that some criticism includes its own set of biases ("Haskell's perfect" is a lively strand). As an outsider just arrived inside EPFL, I hope to gain some insight, and maybe even contribute my 2 cents.
&gt; I don't think this is true, they are both equally explicit. Its just that one is explicit in the definition site (extends) and the other is explicit in the call site (implicit f: F[...]). "Explicit at the definition site" is much less explicit: it means you understand less of a snippet of code in isolation, you need to also read the definitions of the things it's using. &gt; In any case if you are passing that typeclass instance around it has terrible performance, this is what was meant by that vtable comment. It honestly is a crappy solution, its using OOP without its strongest benefits but with terrible performance. I'd rather have a slow solution with reasonable semantics than a fast solution that's impossible to understand and maintain. &gt; In some way this is kind of the fundamental difference between OOP/FP (in terms of Scala). In one case you code against interfaces (OOP) and in other case you are coding against implementations (FP). Technically true but misleading; we don't generally think of writing code that works with `Int`s or `String`s as "coding against implementation rather than interface". FP draws a distinction between values and functions, which lets you write code that explicitly operates on values if you want, which you can't really do in OO because objects fill both roles. You can write FP code that's polymorphic over statefulness if that's what you want, you just have to do so explicitly. &gt; The point is that he wants to expose an Iterable type interface without the downsides of coding against a Foldable or Traverse typeclass (mainly because there are some Iterable style collections which shouldn't, and don't, implement Traversable or Fold) The OO-style `Iterable` interface is effectively: `Foldable`, but with some unknown extra state that gets used/modified at every step of the iteration. If that's the interface you want, it's easy enough to write that interface in FP style, you just have to do so explicitly (which will pay dividends if there are other effects interacting with your code e.g. if you use `IteratorTypeClass` with continuations, everything will Just Work, whereas if you use OO `Iterable` with continuations you will get horrible failures like corrupting the internals of your datastructures). Flipping the perspective a little, a `Foldable` is an `Iterable` that *explicitly doesn't carry any other state along with the iteration* (except for the head/tail themselves). When we put it that way, it's obvious that a cons `List` implements this (perhaps somewhat odd) interface and that other datastructures can only implement it with an efficiency penalty, and maybe it should be a lot less common than it currently is - but it is a genuine, meaningful interface that it's nice to be able to express in the language, and that you simply couldn't express at all from the OO perspective.
&gt; Isn't that kind of pointless when value types get removed and replaced with opaque types? Why work on AnyVal when it's on the chopping block anyway? Presumably *someone* wanted an `AnyVal` that can be used "safely" in non-parametric code (otherwise why was it implemented that way?); for those people opaque types are not a substitute and `AnyVal` needs to remain in place. Either there are enough use cases to make this kind of `AnyVal` worthwhile or there aren't; in the latter case `AnyVal` should be deprecated and gradually removed. (I can see the attraction of silently weakening its semantics to those of opaque types rather than introducing a new feature that's similar to an existing feature and then deprecating the existing feature, but the former approach would be such a gross violation of backwards compatibility that I don't think it's acceptable even if very few users are making use of those semantics). &gt; I'm not sure if I fully understand which guarantees you think of, and where runtime performance would suffer compared to opaque types. Do you have some examples of the issues you have in mind? https://failex.blogspot.co.uk/2017/04/the-high-cost-of-anyval-subclasses.html#markdown-header-what-can-you-do-with-a-box-what-can-you-do-without-a-box - the likes of `getClass`, `isInstanceOf`, casting from `Any`. &gt; I wonder whether the same semantics couldn't simply be provided by adding an annotation to value types, instead of inventing a completely new syntax? I think the difference in semantics is big enough to justify a keyword rather than an annotation, but I'm not immensely bothered either way; if the proposal were adopted with `@opaque` rather than `opaque` I'd be very happy with that. &gt; As mentioned, I have trouble seeing how e. g. the Color type mentioned above can be implemented efficiently without using Java value types. I'm more worrying about single-member value types, I see the multi-member case as almost an orthogonal concern. Maybe the `Tuple*` classes should compile to Java value types in JVM Scala (if `Color` was an opaque type then it would just be a `Tuple3` at runtime), but that's a question worth thinking about regardless of what we do or don't do with user-defined value types, and the existing `AnyVal` implementation doesn't necessarily help with it at all. &gt; On an unrelated note, how would Scala deal with value types coming from Java code, if AnyVals are dropped? Pretending that they are opaque types doesn't really work, because the semantics are different, and the way they need to be treated at the bytecode level would be very different from opaque types as well. I don't know, but again, I think this is a concern that we have anyway and that the existing `AnyVal` doesn't necessarily help with - we can't necessarily model value types coming from Java as `AnyVal`s if they're going to have different behaviour with regard to e.g. `getClass`. &gt; But isn't much of that the case already with AnyVals and universal traits, e. g. not having eq or the methods dealing with locks? Yes, and I think that's a mistake. I'd like to see attempts to call those methods become a compile-time error; unfortunately it's probably already too late to do that with current-scala `AnyVal`.
`()` groups an expression, `{}` groups a sequence of statements. If you want to do multiple side-effecty steps then you have to use `{}`, if you're not doing that then you should use `()` so that it's clear to the reader that you're not doing that.
&gt; the likes of getClass, isInstanceOf, casting from Any I'm think that Java value types will work exactly like you expect them to work, just without the overhead associated with Scala's value types. That's basically their whole point: "codes like a class, works like an int." &gt; [...] the existing AnyVal implementation doesn't necessarily help with it at all I think the existing AnyVal implementation absolutely helps with it, because it was designed in a way were restrictions can be dropped, as more runtime support arrives without having to scrap all the existing code and invent new syntax. &gt; Yes, and I think that's a mistake. I'd like to see attempts to call those methods become a compile-time error; unfortunately it's probably already too late to do that with current-scala AnyVal. Now I'm confused. These things _are_ compile-time errors. They always have been. Stuff like `x eq q` or `x.synchronized(...` do not compile if they are value types.
I got it through the coursera courses. [Functional Programming with Scala](https://www.coursera.org/learn/progfun1) and the follow up something-something(reactive, I couldn't find it on coursera easily). There's now a 5 course thingy: https://www.coursera.org/specializations/scala I learned it quite well from here, and then started using it as simply a better java. I stayed away from implicits, except when experimenting, or when they're used by well designed libraries (like the kinds lightbend publishes). From there I just did lots of experimenting, slowly pulling in more neat functional things.
&gt; I'm think that Java value types will work exactly like you expect them to work, just without the overhead associated with Scala's value types. That's basically their whole point: "codes like a class, works like an int." It can't be both. If I put one into a `List[Any]`, get it out again, and call `getClass`, do I get the user-defined class or `classOf[Int]`? The former can't be done efficiently, the latter isn't what I expect. &gt; One case I presented (Color) is worse with opaque types on all levels compared to value types, but what are the examples that demonstrate the opposite? If your value types box sometimes, the examples are the cases where they box. Maybe all those cases are bad code, but I'd like to be able to define types for which boxing is impossible and cases that would box are compilation errors rather than silently decaying performance. If your value types never box, the examples are existing code that calls `getClass` or pattern-matches on an `Any` that's a user-defined `AnyVal` type at runtime, which your implementation will necessarily break. &gt; I think the existing AnyVal implementation absolutely helps with it, because it was designed in a way were restrictions can be dropped as more runtime support arrives – without having to scrap all the existing code and invent new syntax. Can you get a bit more concrete about this? What restrictions can be removed without breaking existing code? I think many of the cases that currently box will have to continue to box, because changing the implementation to not box will break existing code that expects to be able to access the type at runtime. &gt; Stuff like x eq q or x.synchronized(... did never compile for value types.¹ Point. I think I got confused because they *do* compile on primitives. Guess we're in a better place than I thought.
Thank you!
Scala wise it's fine. You might want to use Future or an IO type like cats-effect IO to wrap up the side effect. The bigger issue is that you have a race condition from doing read-modify-set
&gt; Simply passing around an opaque type of Color you always have to deal with a reference containing three reference-sized fields, each pointing to a boxed Double value of 16 bytes in size. Your Color box of references alone costs you the same as the complete vboxed value type! That's a separate problem that we already have though - a `(Double, Double, Double)` ought to be implemented efficiently. Even if we were to implement user-defined value types as Java value types we would as want to fix that, and if we assume that we're going to fix that then an opaque type becomes just as efficient as a value type. &gt; Now on to interfaces: If you don't explicitly force boxing by assigning value types to Any, you are basically only left with boxing caused by interfaces. I wouldn't explicitly assign to any, but I work with generics that erase to `Any` all the time, won't that cause boxing too? &gt; The JVM is already very good at eliminating this overhead today, with CHA and inlining. I'd rather not rely on the VM, since it's hard to be certain about this kind of optimization. I'd like to be able to verify statically that my type isn't being boxed, which means at a minimum being able to see this at the bytecode level. Opaque types would also be unboxed in the generic case on existing JVMs and on non-JVM Scala. &gt; This is because the exact type can be known at the call-site with Valhalla. The JVM can immediately decide to inline and remove the overhead in such cases. Would it inline through arbitrarily many levels of generic methods? That seems like it would bring its own inefficiencies. &gt; With Valhalla, value types don't need to be boxed anymore in arrays and collections. An array of Color value types will be a single contiguous, reference-free thing in memory. Interesting. How's that going to be implemented for collections? Does that mean that primitives will be unboxed in collections? &gt; Boxing or not boxing does not have an impact on the runtime type with Java value types. Color instances will always be of type Color, reagardless of whether they are boxed or unboxed, are flattened into class instances or placed in arrays. How can that be implemented if they're just 3*8 bytes in memory? There's nowhere for the information about what type it is to go.
Please let me know when you're done. I'll probably have follow-up questions.
Your code works fine for me - how are you trying to run it? If you want to be able to do this in the REPL you have to use `:paste` or put all the lines on one line with `;`s, since otherwise it will try to run each line one at a time.
I see that as all the more reason not to have a monorepo - with a monorepo there is no good way to leave some projects on Scala 2.11 and have others move to Scala 2.12, whereas with per-project repos this is easy.
you live in your own bubble, don't you?
&gt; if we assume that we're going to fix that then an opaque type becomes just as efficient as a value type. But how would the implementation look like? If opaque types just use value types underneath, why not cut out the middle man, eliminate 4 syntactic levels of layering until you are allowed to define a method, and just go straight with value types in the first place? I mean ... they are already there! &gt; I wouldn't explicitly assign to any, but I work with generics that erase to Any all the time, won't that cause boxing too? No, actually generics would be more efficient than passing things through an interface (even with all JVM optimizations applied), because value types ship with specialized generics. &gt; Would it inline through arbitrarily many levels of generic methods? That seems like it would bring its own inefficiencies. I think that would be the decision of the JVM. Considering that the JVM has to emit specialized code for generic methods using value types anyway, I think that overhead is not that large in comparison. &gt; Interesting. How's that going to be implemented for collections? Does that mean that primitives will be unboxed in collections? Yes. &gt; How can that be implemented if they're just 3*8 bytes in memory? There's nowhere for the information about what type it is to go. Because the type is not shipped with the individual instance of a value type. Either you have a method with a specific value type like `def brighten(color: Color) = ...` where it's trivial to figure out which methods to call, or you have something like `def something[T](value: T)` where the actual type of the argument is recorded at the call site, and the JVM emits specialized code for that generic method specialized to that specific value type (and that's basically where you end up with the first case again). This can be done because value types are final.
&gt; My best practice is creating an open-source project in a monorepo, that is a single GitHub project that contains several sub modules. For example, twitter/util is a famous monorepo example in Scala. Wow, I think that's a really _bad_ advice. From a user perspective, you get presented with a monster and don't even know where to start. That's why I will actually never look at a repo such as twitter/util. From a developer perspective, yes, updating multiple interdependent library repos can be painful, but you can then work on each library at its own pace, release minor updates etc. and keep the specific focus of each module in mind. 
&gt; In any case if you are passing that typeclass instance around it has terrible performance I don't follow here. What makes it terrible performance, and terrible relative to what? At first glance it seems like it'd perform pretty much the same as using any typeclass-based approach. Or are you saying any typeclass-based approach has terrible performance?
&gt; If opaque types just use value types underneath, why not cut out the middle man, eliminate 4 syntactic levels of layering until you are allowed to define a method, and just go straight with value types in the first place? I see it as better separation of concerns, more compositional. Opaque types take responsibility for "type that is physically represented by another type at runtime", whether that other type is a primitive, a tuple, or something else. Tuples take responsibility for efficient representation of tuples, which probably should be implemented by value types. Why have two different implementation paths that are doing the same thing? If I'm looking at `Color` and thinking about how it's going to be represented at runtime, I'd rather be able to think "it'll be a `Tuple3[Double, Double, Double]` at runtime and maybe stop there or maybe go deeper, rather than having to jump straight to "the rules for value types with multiple members are that it's this member with this alignment followed by ...". &gt; The JVM will definitely end up creating more code in the JIT compilation cache, but it will be smarter code with less guards and traps, and fewer cases where things need to run through deopt. I'm just not willing to trust that it can be done efficiently. In the style of code I tend to write there will be long, long chains of generic method calls; if I have ten different `Int`-wrapper value types then the JVM either follows them through all the way and generates 10 copies of the whole code path, or gives up somewhere and boxes. Whereas if I had ten different `Int`-wrapper opaque types then the JVM just sees `Int` all the time and compiles one code path. Surely that's got to be more efficient.
&gt; Right. We are using monorepo as a way to force an effort for upgrading all dependent projects to resolve technical debt. if some module cannot be upgraded or found to be EOL, we should drop it from the monorepo. What if some things can be upgraded but others can't? I remember a scala upgrade where spark and play took a long time to upgrade; having separate repos meant projects that weren't using those things didn't have to be held back by waiting for those libraries. &gt; These Scala (and Scala.js) versions are milestone releases, so using them is not mandatory, but we can kickstart the upgrade work and can manage missing dependencies in PRs like above. These are giving us a better idea what libraries can be a blocker for future updates. Is what you're gaining from that really worth all the effort?
The code looks fine, and works ok in my IDE (Eclipse/ScalaIDE). I'm not sure how you're running it, but I wonder if the name `Pair` is clashing with the deprecated `scala.Predef.Pair` and causing problems. (Note that everything in `scala.Predef` is in scope by default.)
Ah. twitter/util might be a bad example, even though its individual modules are not so big. Actually even if we use a monorepo, it doesn't prohibit us from releasing minor updates. Each module in a monorepo can be updated its own pace. Only side effect is versions of the other modules will be incremented even if there is no change. A monorepo approach can be used to make sure even such minor updates will not break the other modules in the repo. A practical guideline whether to use a monorepo or not would be if you have a shared code between projects, extract and put it into this monorepo. 
You can use a typeclass to represent a type-level function. Shapeless uses this all over the place. For example @annotation.implicitNotFound("Don't know how to make a new thing from ${A} and ${B}") trait Make[A, B] { type Out def make: Out } object Make { // quick way to construct an instance def apply[A, B] = new Partial[A, B] class Partial[A, B] { def apply[O](o: O) = new Make[A, B] { type Out = O def make = o } } } We can use this to define a generic `make` method. def make[A, B](implicit ev: Make[A, B]): ev.Out = ev.make And define some instances implicit val m1 = Make[String, Int](true) implicit val m2 = Make[String, Object](1.23f) And try it! scala&gt; make[String,Int] res8: m1.Out = true scala&gt; make[String,Object] res9: m2.Out = 1.23 scala&gt; implicitly[m2.Out =:= Float] // types are equal res10: m2.Out =:= Float = &lt;function1&gt; scala&gt; make[Object,Int] &lt;console&gt;:21: error: Don't know how to make a new thing from Object and Int make[Object,Int] ^ 
Thanks for your reply, I will study this and try to apply it. I'll probably come back with some more questions.
You have NEL in a third party library in Java. Scala also has third party libraries that have it, if you want it.
Yep, I'm pretty sure `Pair` was conflicting with `scala.Predef.Pair`--when I switch it to `MyPair`, it works! Thanks guys!
Thanks so much for this info and link. I am using scalactic now and changed the text error message to proper types.
I have been postponing this for a long time now. Coming from Java most of the examples are difficult. I have been trying this morning to use cats-effect. Does it mean when i use IO i have to chain it all the way to the main method? def registerToto(t: String): UUID = { for { u &lt;- totoRepository.insert(t) // returns generted id as IO[UUID] } yield u // how to return the UUID (guess turtles all the way down?) }
Because of a comment in this list i might want to dive into the IO Monad while i am at it. How does this pattern work with an IO Monad Would it look something like this? trait Repository[T] { def get(id: UUID) : IO[T] // or even IO[Option[T]] def insert(t : T) : IO[T] def update(id: UUID, t: T) : IO[Unit] def delete(id: UUID) : IO[Unit] } How does the service layer (who uses this) can then get the values from the io monad?
I’m done. I haven’t replied to your question 2, though: we hope we won’t need another milestone after M4.
It's just when you're working with `sealed trait`s in the REPL, because otherwise it will try to run the first lines before you've defined the whole thing. In ordinary code that you're compiling you don't need it. 
Nothing, I pretty much picked it up in a week. I had lots of trouble learning Java though.
 def updatePost1(postId: UUID, newViews: Long): Either[String, PostInfo] = { repository.get(postId) match { case Some(post) =&gt; val updatedPost = post.copy(views = post.views + newViews) repository.update(postId, updatedPost) Right(PostInfo(postId, updatedPost.views)) case None =&gt; Left("Unknown postId") } } def updatePost2(postId: UUID, newViews: Long): Either[String, PostInfo] = { val post = repository.get(postId).getOrElse(return Left("unknown postId")) val updatedPost = post.copy(views = post.views + newViews) repository.update(postId, updatedPost) Right(PostInfo(postId, updatedPost.views)) } def updatePost3(postId: UUID, newViews: Long): Either[String, PostInfo] = { repository.get(postId).map { post =&gt; val updatedPost = post.copy(views = post.views + newViews) repository.update(postId, updatedPost) Right(PostInfo(postId, updatedPost.views)) }.getOrElse(Left("Unknown postId")) } 
&gt; I don't follow here. What makes it terrible performance, and terrible relative to what? At first glance it seems like it'd perform pretty much the same as using any typeclass-based approach. Or are you saying any typeclass-based approach has terrible performance? Passing the `(implicit Seq: Foldable[Seq])` around as mentioned in the blog post has terrible performance because the JVM will have a lot of issues inlining the methods inside the `(implicit Seq: Foldable[Seq])` example. 
&gt; "Explicit at the definition site" is much less explicit: it means you understand less of a snippet of code in isolation, you need to also read the definitions of the things it's using. That has to do with power, not explicitness. One is more powerful then the other. &gt; I'd rather have a slow solution with reasonable semantics than a fast solution that's impossible to understand and maintain. (And since as you point out it's equivalent to an OO vtable, there's no reason it couldn't be implemented just as efficiently - but the most important thing is to get the semantics right first, performance can come later) In the context of this blog article (writing a streaming/task library), the slow solution is just as bad as the "incorrect" solution. Having a really slow streaming library is not useful. &gt; Flipping the perspective a little, a Foldable is an Iterable that explicitly doesn't carry any other state along with the iteration (except for the head/tail themselves). When we put it that way, it's obvious that a cons List implements this (perhaps somewhat odd) interface and that other datastructures can only implement it with an efficiency penalty, and maybe it should be a lot less common than it currently is - but it is a genuine, meaningful interface that it's nice to be able to express in the language, and that you simply couldn't express at all from the OO perspective. The issue is that there are datastructures for which `Foldable` makes no sense for (in a lawful sense) but which `Iterable` does. And this does matter, its one of the implications which are stated here &gt; but what if we had some sort of self-balancing tree which you then changed to a HAMT If internally this streaming library was using a `Map` (for which there was some debate about whether a foldable makes sense) or for mutable collections (of what `Foldable is never completely lawful), then you either have to decide between being correct (in the lawful `Foldable` sense) or having really bad performance (and yes, both are as bad as eachother). This is why `Iterable` exists, it acts as an "interface". Also as the author stated, yes its overly flexible however 
&gt; Most of the times you do it wrong, you are not even able to discern what the audience is and what the real goals are. Be more rigorous. I think this forum had some extremely interesting, constructive discussions about technical issues about the last few days. Just because you dislike the opinions voiced in these discussions doesn't mean they went wrong. Maybe they didn't go your way, that's a possibility. As we are currently considering the topic of how to improve the way we communicate, let me just make one suggestion to you in return: Cut out this "toxic" qualifier you are using to label things. The way you are using this word has more correlation with "I dislike someone's technical opinion, and believe he shouldn't be allowed to voice" it, than with "people are not displaying a civilized treatment" if you consider what you consider toxic, and what you didn't consider toxic. You called a civilized, technical discussion toxic, but right on this thread you side with a person that hasn't contributed a single useful thing to the discussion and is responsible for outright shit-storms with people insulting each other on this forum. Where was your moral indignation when people called for blacklisting people for attending a conference? So the next time you consider using "toxic" to flag technical discussions with it, just say that people shouldn't be allowed to have opinions you disagree with. This would substantially reduce the confusion people are having with your word choice. &gt; Have you ever stopped to think why your priorities were so different from other people's: Scala Center's, EPFL's, Lightbend's and even other contributors' priorities? Perhaps they were not as important as you thought? My number one priority has always been the happiness of the users of the language and finding ways to improve their life. The orgs you mention don't have that priority, as many people who have been members of the community for 10 years, have helpfully pointed out to me. I tried to make a positive impact here, and change the way beginners are treated in Scala. This has clearly not worked out, and one of the multiple reasons I quit working on the documentation. &gt; So you're quoting one example of an incident in one community that has around 500.000 developers. Do the maths, man, and compute the number of evidences that we would need to have your claim be statistically significant. I have pointed out multiple issues, which you conveniently forgot to quote in your response. I can provide many more. Pretending that these monthly incidents are all isolated and completely unrelated events that just happen randomly is maybe a position that the NRA takes on school shootings, but not a position that can be remotely supported with facts in the Scala community. I have worked within different communities over the years I can tell you that Scala is the only community I ever worked in where - I had to listen to people who developed outright depressions from the way they were treated. - I had to advise potential contributors to scrub identifying details from their GitHub accounts to protect them from harassment that might be targeted directly at them, or their employers. - I had people come to me to talk about how they had to take precautions against members of the Scala community threatening their livelihood. &gt; Generalizing like you do is out of context and only proves the underlying point behind every of your comments: the Scala community is not a friendly place. This may be what you believe, but given your stance on accepting opinions, I think it's clear to me that those people who are having issues don't come talking to you. &gt; Welcome to human life. Any social system in place will have people with these reactions -- it happens at any scale. This is out of scope and does not prove your point. Way to go to not treat abuse and harassment seriously. Again, do you really think people feel safe discussing their issues with you given the things you say? &gt; No matter how lax you're to interpret the evidence of "I feel prosecuted by my ideas", you cannot claim this applies to the vastness of the Scala community. I have trouble seeing how this is related to anything I have said. &gt; If you really wanted people to correct you when you're wrong, you would write your comments with a pinch of doubt and making a better use of the language to show uncertainty -- so that people feel motivated to correct your mistakes. I think if you run a word frequency count on the things I write, the words "I think" and "I believe" will show quite close to the top of that count.
Actors are super low level and usually cause way more pain than the solve. Make sure you're using case classes for the actor messages. Declare the message type in the same file of the actor that receives them. Don't put logic in the actor, put it in an object that the actor manages. This helps you write tests easier
&gt; waiting another decade for a new tooling ecosystem to be invented is not an option. Of course it's not, but I don't see anything else happening. Today's scala's tooling support is practically the same it was back in 2010, that's how little it has improved in 8 years. If they manage to deliver something in a 1 year time frame, color me impressed, more than that and I'm sure they will lose focus, interest, and it'll become just another botched project.
Yeah, that's what I'm thinking now. Some things are really painful though. For example the implicit `Timeout` for the `Ask` pattern has to be defined in the actor itself. If the timeout is not handled by the module that creates the actor, there is nothing that can be done about it, and this usually leads to much more code than it would otherwise.
It has vastly improved over the years. It was almost unusable at the beginning. Now its usable, but with a lot of minor gripes. There are languages that had no tooling before and yet still had larger adoption. TBH, the LSP project is interesting and should have been started long back. Standardization for tooling was long pending. I have higher hopes right now than before because of LSP. 
You could put it in the actor Props, so that the value is settable on actor creation. Using sane defaults, so that you only override it when you want to, assuming you're wanting to override it for testing.
Yeah, testing was the use case here. Anyway, thanks for letting me know that it can be added to Props. That would be useful.
Ok, i am starting to get this. But in the case of a "request" you would call `unsafeRun` at the beginning of the thread (for example a service request)?
&gt;Akka toolkit on a large project without loosing their mind? No, that's why people avoid it. &gt;For those who use Akka a lot, which is the best way to manage a large Akka project along with the documentation? Large Akka projects are impossible to manage. Small projects, however, can have the Akka portioned quarantined as much as possible. This is the only way to gaurantee maintainability. &gt;Actors are not very transparent when it comes to managing async calls. If there are futures in the actors, you cannot really set the individual timeouts for them Every one of your points are correct, and very large headaches for people using Akka. Message when you find a solution. &gt;I just being too picky? No, this is why `Monix`, and `Scalaz#Task` exist. Actors are incredibly hard to maintain and do not scale well (in terms of codebase complexity). &gt;I was looking for advice on how others are doing it. I stay away from using it, as I rarely have a case warranting bidirectional communication, and I stick with `IO`, the `Async` monad, or `Task`. If i need backpressure, we either defer to `Monix`, whose `Task`s have backpressure support, or don't worry about it. So far, for highly performant async services serving millions concurrently, we really haven't needed Akka. Especially not untyped actors. I would never let untyped actors into my codebase.
1: yep. An actor meet is just an actor reference. In the past I’ve used HOFs to get a semblance of safety that I’m asking for the right actor. 2: ActorTyped should hep here. In absence of that, you can either setup a “WTF?” Message or understand that you’re not supposed to be introducing that dependency. In the past I’ve found that requiring a certain message type be received by an actor is a sign that I need to rethink how my message flow is designed (look at the worker registration pattern in the docs). 3: accomplishing this is no different than how it would be done in any other code where you want to shadow an existing implicit. 
I guess it is rules that prior + model = posterior ?
Hah! Your comment makes me feel good in that I'm not the only one. Unfortunately, however, the solution to my problem = a lot of work!
My backend code is 100% is Scala.... but the boiler plate code in using bootstrap and react totally kills the hope of using ScalaJS for me.
Conspicuously absent from said future: a good build system.
If you are a server then yes, one very reasonable approach is to have each request do an unsafeRun to return the response. If you're just starting out you can make it a bit more seamless by picking a stack that makes it easier, like http4s+Doobie for http and database respectively. If your stack is already set and not written with an effect type in mind there's nothing wrong with using unsafeRun to convert at the boundaries
`monix.Task` or `cats-effect`?
I see why our opinions are so different. My point is that maintaining multiple repositories is hard indeed, especially when these projects have some dependencies. I'm totally fine with managing a project in a single repo, if its slow update pace doesn't affect other projects. 
If you are going to dive into the IO Monad for you Repository I would take a look a Monad Transformers. The chapter in [Scala With Cats](https://underscore.io/books/scala-with-cats/) does a pretty good job of explaining them. But then you could write IO[Option[A]] as OptionT[A] 
Thanks I'll look into it. I really am surprised by all the help here on reddit. Thanks jo_wil and everyone else 
I suggest you to reply to this thread: https://contributors.scala-lang.org/t/scala-collections-redesign-roadmap/921
be careful!
Well I find myself using Blaze when I need a client these days since I tend to use http4s, but yes you're stuck on 2.11 with that old client.
While it's never a good idea to submit anything on the scala reddit for fear of the typical raptor-like treatment. I thought it might be useful to put forth a few typelevel.http4s links for those struggling with the the http4s documentation. https://github.com/objektwerks/typelevel/blob/master/src/main/scala/objektwerks/app/NowHttp4sApp.scala https://github.com/objektwerks/typelevel/tree/master/src/test/scala/objektwerks/http4s I worked through these examples the hard way. I'm not looking for comments. So fuhgeddaboudit!;) See the readme.md. 
I use that as well
&gt; one gets 4 ListBuffer allocation and 4 List allocation to do things the user never asked for and are not required to arrive at the results. Yes, the user did ask for all those things by choosing a strict collection type. Without an effect system to track side-effects, or a pure functional language to disallow them in the first place, you don't know if the operations are actually required, so the user has to make that choice explicit (for example, by using `Iterator` instead of `List`).
I know how collections work. Maybe there is a misunderstanding of the points I made?
It seems the major goal of this initiative is implementing a Language Server Protocol for Scala / Dotty, and getting various Scala tools to use it. I don't really know sbt ties into this, but it does not look like LSP has anything to do with the problems that people have with sbt.
&gt; I've started doing the Coursera course that pretty much everyone recommends (https://www.coursera.org/learn/progfun1/) , as well as I gotten the Scala for the impatient, book + Functional Programming one. Honestly, I wouldn't recommend any of those. The Red Book is probably the most useful of them, but it is not about learning the language and it will definitely not help with Play. You can check out my bundle here: https://leanpub.com/b/complete-scala-bundle The first book of the bundle basically summarizes my first year experience with Play, and the fact that nobody here mentioned it means that I'm doing something incredibly wrong in the marketing department.
There is a lot of progress in the build system space: [mill](https://github.com/lihaoyi/mill), [cbt](https://github.com/cvogt/cbt), [pants](https://github.com/pantsbuild/pants), etc. The idea is to build a protocol similar to LSP but for build tools: BSP. You can read about the draft here: https://github.com/scalacenter/bsp/blob/master/docs/bsp.md
Count me as a person who never found the need to move from repository model to some sort of effect/algebraic approach. I simply don't think it's worth the overhead. Testing is easy. I just have all my services take their repositories as arguments. In tests, I pass in simple stub implementations. No algebraic library needed, no DI system needed. Just basic language features.
Nice, would you be open to discussing snippets in a README format for different projects you have in there?
https://nerdland.net/2010/06/code-mass-noun-versus-count-noun/ https://english.stackexchange.com/questions/20455/is-it-wrong-to-use-the-word-codes-in-a-programming-context
thx for suggestion. fixed
*Scala in Depth* is about Scala. Specifically, "advanced topics". Despite its reputation, it is a perfectly good introduction to the language, moving on to teach more exotic language features and why you'd use them. Chapter 11 "Patterns in functional programming" should be skipped, though. Chapters 5-7 are the most valuable and unique. To quote the very beginning of the red book, "This is not a book about Scala. This book is an introduction to *functional programming*…" This is not a cheeky lie. Only enough Scala features to cover its material are introduced, i.e. enough of a language to teach all the FP abstraction and design they want. This is not nothing (some necessary features don't exist in Java), but is beside the point. So, do you want a book about Scala, or a book about FP?
The final part of the Scala specialization on coursera
Thank you for response. I began reading Scala in Depth since I want to know better both Scala and FP. Currently at Chapter 6. The book has some really nice details and I'm glad I chose it. I'll skip FP chapter and move on to Red Book after reading this one.
The link should be https://eli-jordan.github.io/2018/02/16/life-is-a-comonad/
The solutions to this are agnostic to the effect type. You could use a compare and set loop, but g this specific example you can just `update posts set views = views + :newviews where id = :id returning views` if it's postgres for example, and then it will have the thread safety given by your transaction isolation settings
Can't attest to its accuracy since I'm still learning this stuff but this was really good/helpful to me. 
Awesome. Thank you so much for this answer! Sorry for the late reply :) I'll remember that. 
np, good luck!
Is anybody yet working on a Play build module for this?
Is it weird that I think this looks like a more object-oriented Gulp?
This was a fantastic post. 10/10. I'm glad someone else is using the `counit` terminology to drive home the duality perspective. Juxtaposing `Memo` and `Store` monads and comonads is a very useful introduction to understanding profunctor optics.
&gt; One question, did you find any adjunctions that gives us streams? Streams are the final coalgebras of polynomial functors. You can define them as `Cofree Identity a`, which is particularly useful for visualizing what they look like. The comonad instance follows from `Cofree`, as does the usual Free/Cofree adjunction.
Thanks! I will definitely look into that. This might not be the place, but from the wiki article on coalgebra, they just look like arrows in a natural transformation, am I missing something?
The (co)algebra constructions for some endofunctor forms its own category (the category of F-(co)algebras). Then the initial and terminal objects of those categories have interesting properties in relation to the underlying category and alphabet. Cata and anamorphisms (initial and final coalgebras) give you maps from FA -&gt; A and A -&gt; FA, but they're not quite natural transformations. They're adjunctions.
I haven't yet studied the details of the relationship of Monad/Comonad/Adjunction. However, my high level understanding is that if two functors `F` and `G` are adjoint, with `F -| G` then `F compose G` forms a comonad and `G compose F` forms a monad. There are some general constructions to derive the adjunction given a monad called the Kleisli construction and the Eilenberg-Moore construction. My assumption is that these could be applied to derive an adjunction for the stream comonad.
Looks a lot more sane to me. But I'm only a novice so I wouldn't notice any shortcomings etc.
`F[A] =&gt; A` is the signature of `counit`. The type signature of the Observer pattern is also `F[A] =&gt; A`. Interestingly the observer pattern — streams — is an excellent alternative to dependency injection in most cases. Thanks to comonads DI is essentially anti-pattern in Scala!
Excellent blog, thanks very much for sharing!
Looks interesting, but I can't see how it could succeed without a sbt interop.
Esp. given the number of SBT plugins available...
Perhaps you could give any examples how we can use comonads as DI?
Using this, how to add the Main-Class line in the manifest file?
It transforms to a megamorphic call if you end up supplying different different objects that conform to `(implicit Seq: Foldable[Seq])`, (i.e. if you supply a `List` vs a `Vector`). The JVM is not able to inline megamorphic call sites (http://insightfullogic.com/2014/May/12/fast-and-megamorphic-what-influences-method-invoca/)
&gt; No, code in one style is more explicit ("stated clearly and in detail, leaving no room for confusion or doubt") than code in the other. Actually no, they are both equally explicit. In one you have to specify `extends`, in the other you are passing the typeclass instance around in a parameter. If `extends` happened automatically, then yes you would have a point. They obviously vary in power, but not in explicitness. &gt; There are cases where a slow streaming library is useful. Of course some use cases for Scala are performance-bound, but many are not. (Likewise there are use cases where maintainability and comprehensibility are unimportant - but I suspect those are rarer than the cases where performance is unimportant). If we are looking for streaming in the first place its usually because of 1. Performance 2. Dealing with infinite sequences Usually its a combination of both &gt; Sure, and those datastructures should implement IteratorTypeClass and not implement Foldable. And the issue that such a `IteratorTypeClass` would only exist in Monix (and in no standard library), so the collections wouldn't be able to implement it. If you do put it into a "standard library", then you will have to get all of the other lawful traversable (as well as the non traversable classes) to implement this and then you have basically frozen the implementation, which means the binary interface to your application is a lot more brittle.
Typed actors would be a good idea. But on the latest Akka documentation it is not marked as "stable". I don't want the API to change very now and then for a project such as this, which contains the code that is run in production.
most sbt plugins are just wrappers around existing libraries to translate them to sbt semantics. Those semantics are hard for most people to reason about, passed the "hello-world" project. My guess is that people will be much happier writing mill code than sbt code, cause it's much closer to "standard" Scala/Java code. 
&gt; In one you have to specify extends, in the other you are passing the typeclass instance around in a parameter. If extends happened automatically, then yes you would have a point. They obviously vary in power, but not in explicitness. `extends` is invisible at the point of use, which is literally the defining characteristic of an `implicit` parameter. By your logic `implicit` parameters would qualify as explicit, since you have to specify them in the definition of the method that receives them. &gt; If we are looking for streaming in the first place its usually because of ... Performance True as far as it goes, but there's performance and then there's performance. Streaming libraries exist and are useful in languages like Python, which is slower than explicit-typeclass-passing Scala by an order of magnitude or more. &gt; And the issue that such a IteratorTypeClass would only exist in Monix (and in no standard library), so the collections wouldn't be able to implement it. If you do put it into a "standard library", then you will have to get all of the other lawful traversable (as well as the non traversable classes) to implement this The same argument applies just as well/badly to the OO `Iterable` interface. Actually we're better off in FP-land, since it's possible to provide typeclasses for existing library types, whereas there is no way to retroactively make an OO type implement a new interface. If it's a good, general interface - and it is - then we should adopt it. Migration is a separate but manageable concern. &gt; then you have basically frozen the implementation, which means the binary interface to your application is a lot more brittle. No you haven't, as I've told you twice already - if you choose to expose the `Pointer` internals you can, but if you don't want to expose them (for binary compatibility reasons or otherwise) you can leave them existential and then you're in the same situation as `Iterable` WRT binary compatibility. Please stop spreading FUD.
Citation needed. I've written a lot of sbt plugins and none of them are "wrappers around existing libraries".
My godness, I think I'll never master FP in my remaining lifespan. So much to learn...
Removed for being unrelated to Scala and/or spam.
To be completely honest I think the reason I was having problems doing grep is because of the unicode format. I was copying and pasting into Textedit on Mac, which only lets you save as .rtf. Then I was opening this in word and saving as a .txt. That probably messed up the encoding and that's why grep wasn't working. When I tried /u/George_Kush_Sr 's solution I did it properly by copying into word and saving as .txt from the beginning. But still, his solution works great so, kick ass!
Nobody loves Windows these days... :'(
de mainClass = Some("foo.bar.Qux")
Everything should work except the REPL, which should work if you swap back to JLine
Tl;dr: sampling the clock when you start an async http operation, and then when it yields the response object, can lead to inaccurate timing results if the entire stream has yet to be read
Depends on why you need Scala FP vs Java? If you don't need any other cruft (i.e. you're not looking for domain-driven or reactive whatever), Functional Programming in Scala is basically the book. I suggest you look over the table of contents to figure out if it's relevant to you?
Its probably better to use the monitoring already built in to your reverse proxy in production. Nginx-VTS or HaProxy both can do this more efficiently.
Like I said- I'll be working at a company that is using Scala and that is the motivation for learning it. Good idea: I'll check out the ToC now.
While actors could be used to model many things, quite often I hear that someone use them for modelling transactions in distributed environment or something similar. They do that by making actors something like a subscribers to events, except sent directly from one group of actors to another group of actors, where size of the group makes things parallel. So, they are basically reinventing reactive streams, except with no guarantees about backpressure, reinventing half of Akka Streams internals from scratch and in a fragile way. Ergo most of the time Akka Streams (or any other reactive/functional streams) are a way to go. Not always - there are cases, where actors would be right abstraction for something - but I would say, that in 99 cases out of 100, what you wanted to do with actors, you can achieve with Streams with less code and no headache.
Right, I'm passingly familiar with that topic and understand it possibly performs worse. But megamorphism isn't inherent to that construction. That is, you could run into the same issue with the other approaches, no? That's what I'm missing here. I'm not seeing how it's any worse inherently, with a problem that the others just do not have. So I guess the next specific question would be: what prevents the other generic approaches from also becoming megamorphic calls?
Nice, hope you enjoy it :) 
Microsoft has basically lobbied against open source software since the 90s, up until recently where a few things (i.e typescript) have been open sourced. This is sort of off topic, but I sincerely don't see why any OSS library developer should show microsoft love, at all.
https://underscore.io/books/essential-scala/ is free, very good and is aimed at experienced developers. 
Thanks. I think it should be included in the doc. in a more visible way. Also, you provide an example.zip with a sample project. This line should be in its `build.sc`.
Scala cookbook is something I've found rather useful.
How have you used Gitter effectively? I've felt that I'm not using it to my potential because I join in on groups that are already loaded with conversations and members. There's no starting point for me, and I'd really hate to annoy strangers
I've been doing this in my interviews, and it's been extremely challenging to do these problem exercises in functional ways of thinking. It's taxing, because, for most imperative looping problems, I'll just end up with the similar boiling plate of implementing tail-recursive functions to implement a loop. Then if we take into account, immutability, I am left with making deep copies of an entire data structure with the updated state. These sort of nuances aren't covered in books like cracking the coding interview, HackerRank, or EPI. Even online code editors or mock interview platforms lack decent Scala support. Usually, when I want to look at functional modes of thinking for these problems, I'll dig into Haskell code. Also, Often times, standard interviewers want us to demonstrate the algorithmic complexity involved, so condensing functions into higher-order functions isn't something I jump into unless I explain the Big-O time complexity of that function. Direct mutability usually translates into replacing instances of nodes in data structures, rather than direct reassignment. I'm still yet to understand some of the higher-order functions for breaking out of for loops more easily upon certain conditions. It's really up to us to solve these problems in a functional way. You won't find solutions using purely, functional code for these typical coding interview questions (like those found in books and competitive programming platforms).
Do you know of any resources to learn reactive streams? the AKKA documentation is great, but i feel like to follow it effectively you need to know the problem you are trying to solve with the streams.
Type classes and OOP sub typing are both ad-hoc pomymorphism. It actually opposes to parametric polymorphism. The distinction is whether or not a polymorphic function (value) is agnostic on the type parameter. If the function treats its type parameter as a black box, doing exactly the same regardless of the actual value of the type parameter, then it is parametric. For example `List.length` is parametric, you don't need to know the type of the elements inside the list to compute its size. On the contrary, if the function needs to perform some operations based on what the type is, then it is ad-hoc. For example `List.sum`. You can sum lots of things: integers, floats, double, big decimals, matrices, etc. But summing list of integers is not the exact same thing as summing a list of matrices. The addition operation is different for each type, so this is ad-hoc. If you have not already read the paper "Theorems for free", I recommand doing so. It clarifies this a lot!
&gt; extends is invisible at the point of use, which is literally the defining characteristic of an implicit parameter. By your logic implicit parameters would qualify as explicit, since you have to specify them in the definition of the method that receives them. Except that this again is only half right. You are right that `implicit` parameter is implicit, but you still can't call the function unless there is an `implicit` variable existing in scope. &gt; True as far as it goes, but there's performance and then there's performance. Streaming libraries exist and are useful in languages like Python, which is slower than explicit-typeclass-passing Scala by an order of magnitude or more. Actually if your calls are being marked as megamorphic, you are probably going to be as slow if not slower than python in the areas that matter. You are definitely going to be slower if Python deligates to a C library, which is often the matter. I am not sure why we are even comparing ourselves to python though, one of the reasons for using Scala is because its not slow like a horse. &gt; No you haven't, as I've told you twice already - if you choose to expose the specific Pointer type you can, but if you don't want to expose it (for binary compatibility reasons or otherwise) you can leave it existential and then you're in the same situation as Iterable WRT binary compatibility. Please stop spreading FUD. Right, the point is that here that for design reasons it needs to be exposed for the reasons detailed earlier, which leaves us back to square one. I mean we are going around in circles here.
What `CanBuildFrom` solves is clear. And no, Haskell's standard library certainly doesn't solve what `CanBuildFrom` does. What was "*noted a decade ago*" is less clear, as I don't have time to track random people from the Internet. Also dude, a decade is a period of 10 years, Scala 2.8 was released in 2010, so the math doesn't add up. So if you're going to say anything useful, then do so, instead of making vague references in order to piss people off. 
You've referenced a tiny bit of Haskell library in your article. You're clearly unaware of improvements beyond the basics. We already knew CanBuildFrom was a bad idea before it was written, because we wrote it (actually we wrote something slightly better, but still no good). Show me this problem you think I cannot solve better without CanBuildFrom. There is nothing vague, except this supposed problem, which was explored thoroughly and never found. Show it off you are so sure it exists. 
&gt;&gt; Can you do that with Foldable / Traverse? No, you can't! &gt; &gt; Why not? Because with a `Foldable` / `Traverse` data type you can't afford a `head` / `tail` decomposition, which `Iterator` does, even if mutable. It's all in the code shown actually. Try doing it with a `Foldable` and you'll see what I mean.
Sounds very sensible and quite straightforward from the content of this blog post. Looking forward to seeing it in action within a more realistic project.
Scala's accessibility issues aren't syntax-level. They're build system level. Step one is establish binary compatibility between point releases of libraries. Step two is fix the Scala Maven/Gradle plugins - forcing users to manually download anything other than Java before they start building is silly. Step three is improve cross-compatibility between Java and Scala.
I added a TL;DR;!
Hmm interesting. I haven't really ever thought of accessibility in code. I deal with it often in my day to day job, but the software itself it's never come up. 
Great proposal! Thanks Sam!
Regarding Java-based tools that's mainly matter of using proper file-separator "/" vs "\". And adding a *startup script*. Windows users are not Microsoft... I'll try to help with this as soon as I find time. :)
I'm assuming this is based on the feedback of actual blind people? Years ago there was a very active member of the Scala community that was blind. I don't recall him every complaining about Scala's syntax. (I did find an email from 2010 where he complained that yourkit wasn't accessible, in contrast.)
* Too much Ambiguity and Heavy overloading of concepts/symbols. For example, the "_" looks like it could mean a thousand and one different things depending on the context it is used. "=&gt;" could also be used with functions, but also to express self types or alias for "this". Another example is the overloading of `for` which is used for both sequencing monadic computation and also for looping. You also have () and {} which seems ambiguous and could be needed or optional or mean different things based on the context they are used. * Deceptive in the feeling of familiarity it gives. Especially for someone with previous experience with Java. I feel this similarity is dangerously deceptive because as soon as you go a little deeper with Scala, you encounter syntax/concepts that do not just fit with your previous mental models, and then you get frustrated. I feel the initial idea (i think this is mostly not the case now) of positioning Scala as a better Java contributes to this deception. * It functional aspect is not as explicit as its OOP aspects. For example, I never could deeply appreciate the `for comprehension` in Scala until I understood monads and `do notation` in Haskell. I also never really understood the whole seal trait pattern until I grokked the idea of ADT while learning Haskell. There are lot more instances of this, where particular FP concepts did not click until I learned it from another language that makes those FP concepts bare. I feel Scala makes it hard for someone not familiar to FP to be quickly exposed to the needed concepts. It feels it intentionally obfuscate its FP side and instead of making the concepts bare and obvious in its syntax, it leaves these FP concepts to be encoded using its other language features. This makes learning it harder. * SBT! * Documentations are generally not helpful especially the ones on the Scala website and also libraries maintained by Lightbend -&gt; (I am looking at you Play Framework and SBT). Most of the documentation has a dismissive tone; as if it is written for someone who already knows the topic, instead of it to be written with a beginner in mind. &gt; how do you learn most effectively It's ironic that I have to say first learning Haskell was maybe one of the most effective things I did that helped in making learning Scala less of a pain.
Functional Programming in Scala is pretty much the book to read intermediate level Scala developers. If you feel you have already have Strong functional programming skills in Java, go ahead and dive right into FPiS.
Thanks- have you read Functional and Reactive Domain Modeling? I went ahead and ordered that one because they cover some subject matter I am interested in that I didn't see in FPiS such as the Actor model and Akka
Figured out what is going on var data:String = null Get converted into two steps 1. variable declaration 2. value assignment In this case value assignment happens twice. 1. when Parents constructor calls initData 2. when Weird constructor happens(assigns as null) 
I think you're talking about Rui Batista which has given his input to the proposal, according to the [proposal itself](https://github.com/fommil/advisoryboard/blob/31329473144a760552fce31680334ff05d653964/proposals/verbal-descriptions.md#proposer).
Initially the hardest part about learning Scala was having to re-reverse my thought process, after spending a decade reversing it by having to learn C++/Java. It was frustrating because I knew that I had been conditioned by programmers who acted more knowledgeable than they were. What I wouldn't do to have had just one person around who could have whispered "lisp" in my ear. When I started coding, which was a while ago, it was a lot like it is now, where there are 100:1 people who pretend to know what they are talking about, to the one person who actually knows, and in the case of functional programming, it's more like 10k:1, and then 100k:1 of people who are in opposition to it because it would require them to learn, so that's at least 109k extra pretenders trying to get in your way for no reason other than their own pride. People in general are so unwilling to learn, let alone admit it, and especially not when they are already a "genious." I saw this article a while back, and found it to be insightful: [What are some of the negative aspects of using Scala](https://www.quora.com/What-are-some-of-the-negative-aspects-of-using-Scala) At least we have r/scala now!
Yep! It's called the [Kind Projector](https://github.com/non/kind-projector) and here you would basically use it like: apply[T[C, ?], A, B] 
Not sure why I didn't notice that name before. Anyway it's great that they're being so helpful.
Sarwen, that you very much for your clarification. I was aware also that simple function overloading is also an ad-hoc polymorphism. That is a very common opinion about type classes being "ad-hoc". I think that reason for that could come from a fact that your are not constrained with super type that you accepts as an argument of your function but your are really free of providing additional implementations. That conclusion reduces to your response about that type classes are parametric-aware. I could mismatch some namings, but my core domain of that article was to show how to build type class on your own. 
Without Kind projector you'd use a type lambda like so: ({type L[B] = T[C, B]})#L
I love this project. Also, like always, a very nice blog post. I have a few questions though. 1. Why, unlike other "standard" tools that put the output in `target`, Mill puts them in the `out` directory? Don't you think it would be nicer to use the same standards? 2. Does this have anything to do with [this](http://www.scala-lang.org/blog/2018/02/14/tooling.html) announcement? 3. Is there any documentation for writing plugins/extensions or using the existing ones from SBT/Maven/Gradle etc.? 4. How can others help/contribute to this project? 5. I see that assembly comes default with mill. That's great! However, does it have support for things like shading and dependency overriding and selection in cases of conflicts? I would love to see where this is going.
Just to remove one possible source of confusion, `λ` is just a normal identifier here. I would usually write `({type L[B] = T[C, B]})#L` which is completely equivalent.
I should probably start by saying that I like Scala, but I've only used it for toy examples, come coursera courses, and as a overpowered calculator. I have tried to do some smaller/mid-size projects but never finished. 1. SBT, still don't know if there is any real accessible documentation for it or if ts secret knowledge held by some underground cabal 2. Structuring a project. Which folder / namespace structure to use 3. Typeclasses 
I think it has to do with the *way* React is wrapped for Scala.js. That's why I'm maintaining an alternative React library for Scala, with the goal of making React fit Scala, instead of the other way around: https://github.com/Ahnfelt/react4s
I felt the same about the existing React wrappers for Scala, so I created a simpler alternative: https://github.com/Ahnfelt/react4s Perhaps that'll be more to your taste?
I'm working on some more examples and documentation for [React4s](https://github.com/Ahnfelt/react4s). The library recently reached 0.9, and no incompatible changes are planned until after 1.0. There are no known bugs, I'd just like to gather some more feedback on 0.9 before I bump it to 1.0.
Sooo, where can I buy a physical copy ?
We're not sure but here you can download the ebook: https://underscore.io/books/scala-with-cats/
I am not done reading it but I can say that so far (page 150) "Scala with Cats" contains the best writing on these topics I have ever read. Kudos to the authors for clear and engaging writing and for making just the right amount of assumptions necessary to the reader.
I'm gonna go for the suspiciously simple answer: the first one prints "got x = 1" and returns 2 every time, and second one prints "got x = 1" and returns "returning x = 1" every time. What's the catch?
Bonus: https://doc.akka.io/docs/akka-http/current/routing-dsl/directives/debugging-directives/index.html It uses underneath `mapRouteResult` mentioned in the article. With it you could do sth like: def logOnLevel(level: String, route: Route): Route = { val logLevel = Logging.levelFor(level) getOrElse Logging.InfoLevel def logResponse(loggingAdapter: LoggingAdapter, reqTimestamp: Long)(req: HttpRequest)(res: RouteResult): Unit = { val resTimestamp = currentTimeMillis() val elapsedTime = resTimestamp - reqTimestamp val (result, message) = res match { case Complete(response) =&gt; ("finished", s"${response.status} ${entity2String(response.entity)}") case Rejected(reasons) =&gt; ("rejected", reasons.map(reason =&gt; s"$reason").mkString(", ")) } LogEntry( s"Request $result in ${elapsedTime}ms: ${req.method.value} ${req.uri} =&gt; $message", logLevel ) logTo loggingAdapter } DebuggingDirectives.logRequestResult(LoggingMagnet(logResponse(_, reqTimestamp = currentTimeMillis())))(route) } and then logOnLevel("info", routes) // routes with logging! 
Nice, that looks much cleaner! Are there any notable tradeoffs that you decided on differently than scalajs-react to make that possible? e.g. in terms of JS compatibility, safety, etc?
The version may be dated now, as I think it is still using FS2's Task , but I wrote this simple service with Http4s which explores various parts of http4s (Json, http clients, twirl, testing, etc) which you may find helpful: https://github.com/RawToast/http4s-twirl-example
Not available generally yet
IMO underscores books are some of the best written programming books I have ever read! With the combination of clear but detailed explanations and relevant examples/exercises mixed in to the text they do a great job of teaching Scala, cats, etc. 
GUESSING: (I have an unfair advantage since I have worked a little with parsers and compilers before. On that note, I can strongly recommend at least the first 14 chapters of "Types and Programming Languages"; it is a really great book). val a = 1 implicit class RichInt(val x: Int) { def s: Int = { println(s"got x = $x") 2 } } /* Expression | Print | Returns -----------|------------|--------- -(1.s) | got x = 1 | -2 -(a.s) | got x = 1 | -2 -a.s | got x = 1 | -2 -1.s | got x = -1 | 2 */ . - 1.1: "1" is parsed as literal integer 1. - 1.2: Same as 1.1. - 1.3: In "-a.s" the method call with dot has higher precedence. - 1.4: "-1.s" is parsed as literal integer "-1" and then the method is called on that. . implicit class RichInt2(val x: Int) { def s: String = { println(s"got x = $x") s"returning x = $x" } } /* Expression | Compiles | Print | Returns -----------|----------|------------|--------- -(1.s) | No | ??? | ??? -(a.s) | No | ??? | ??? -a.s | No | ??? | ??? -1.s | Yes | got x = -1 | "returning x = -1" */ . - 2.1: Parsing is same as the former similar expression 1.1, but the `String` type does not have the `-` operator method AFAIK without checking. - 2.2: Same as 2.1. - 2.3: Similarly parsed and precedence as 1.3, but the same issue as in 2.1. - 2.4: Similarly parsed and precedence as 1.4. Does compile. On a side note, parsing "-1" as integer literal 1 and then a method call to negate it can have other negative consequences. I recall Kotlin parsing it in this direction. I personally prefer it either Scala's way of parsing or a similar way; I vaguely recall there are some further details and some extra checks possible. But it is not something I have thought about a lot. AFTER GUESSING: Yes, I got it right :-). EDIT: On a more appropriate note regarding book references, regarding parsing this book is probably much more relevant: "Introduction to Automata Theory, Languages, and Computation", and I have had good experiences with it. EDIT2: Regarding why I prefer Scala's general way of parsing this over how Kotlin would parse it, consider how the string representing the most negative 64-bit integer is parsed using the two different ways (remember to append "L" whether in Java, Scala or Kotlin). For some reason, it still works in Kotlin for 32-bit integers, but it fails in Kotlin for 64-bit integers.
I'm glad you like it! As for tradeoffs, the central assumption in React4s is that the standard Scala operator `==` correctly implements structural equality for your props. With this assumption, it's possible to simplify the API considerably. However, the programmer must then be careful not to pass functions or mutable data structures as props, since `==` doesn't do the right thing for such types. Other than that, it's very type safe, and I'm not aware of any other tradeoffs.
Is it available specifically?
Great work by the Underscore guys. I found this work much more engaging than the "Red" book; the exercises interwoven in the eBook are perfectly integrated with the content and really help in grasping the ideas.
Great post! It feels as if I've skimmed the book by reading through the review and I'm hungry for more. It's almost unbelievable that these books are available for free. much appreciation to underscore.io!
Thanks :) 
What about integration with plain React.js components? Can you use third party React components that were originally written in JS in a React4s app? If so, do you have an example / syntax doc for that?
The negative symbol in front of each statement is the catch.
Not right now. The run for the conference had errors and they're booked with paid work right now. 
Do you know perhaps of an ereader app (android) that renders code listings IN epub well? I always end up using the PDF because of that.
Does enumeratum not count as pure Scala?
He's a well-known troll, best ignored.
You can ask any question like this in the bi-weekly "Ask Anything and Discussion threads" which are usually stickied at the top of this subreddit. [Here](https://www.reddit.com/r/scala/comments/7ym8w1/fortnightly_scala_ask_anything_and_discussion/) is the current one. You can also ask in the official gitter channel (see sidebar), the official Scala User Forum (again see sidebar) and while I haven't used it, [the Code Review Stack Exchange](https://codereview.stackexchange.com/) might be what you're looking for.
"Scala is still a pioneer"?
I had to make a change to this so that it wouldn't fail when wrapping a web socket endpoint: https://medium.com/@AlanJay1/great-writeup-24217cf051ac
Nice read. Thanks for posting it. I really like how relaxed and down-to-earth Martin comes across.
I'd say, let's first figure out whether one should to Mill rather than cbt, or whether yet another design is needed. We can't waste as many resources as the JavaScript ecosystem. Having said that, all that I've heard on Mill is pretty nice; haven't looked at Mill myself but I was impressed by http://www.lihaoyi.com/post/SowhatswrongwithSBT.html.
While this is nice exercise to the reader, for actual coding you might want to install [slickless](https://github.com/underscoreio/slickless) which does something similar with a different syntax: def * = (id :: name :: age :: HNil).mappedWith(Generic[Person]) DB insert is actually the only part when you do not have ID for everything else do, so IMHO it's better to just split the representations and: persons.map(p =&gt; (p.name, p.age)) += insertTupleOrSimilar BTW - it might be considered a bad taste/antipattern but if id is NOT NULL and auto incremental then: persons += person.copy(id = 0) Slick (at least on Postgres) will fill that id with default value. In such case there is even less reason to bother with wrappers. Still on domain level I would prefer to have explicitly case class Person and case class NewPerson - the later would be converted to Person with missing values filled with defaults.
Other way of achieving it, is to use implicit class with new operations you want to support. One of those operations might be sorting by Sorting ADT but also filtering (each filter would be a case class/object). With that you can have API nicely reflecting your domain.
&gt; We should be able to have these mappings gracefully fall back to no sorting at all. That seems rather dangerous. If someone is passing you a sorting object, wouldn't they expect (i.e. depend on) you to sort it? As you add more sorting objects, you introduce more likelihood that you forget to implement one on a table (which already has other sortings) and cause runtime failures or worse in some other code.
This project is really great for getting started as well: https://github.com/SmartBackpacker/core
My opinion on the matter is: Has the business decided how to sort elephants by `UpdatedAt`? - Yes: Implement the sorting - No: Make our best guess (sorting by default) The elephants _are_ being returned sorted, just by the programmatically generated sort. The sorting is repeatable and dependable. `UpdatedAt` sort needn't refer to an `updatedAt` field, it could be anything you like for elephants (though I'm struggling to think of a good one). If, as you say, someone depends on elephants being sorted by their `updatedAt` field and they aren't, then that's a bug due to a failure in the business to communicate their needs properly / a failure in the dev team to test business cases properly. With lack of input from the business we make our best guess on how to implement functionality (default sort), and await feedback (bug report/UAT) to iteratively improve our model (implement UpdatedAt for elephants). As all programming works. Of course, we could safeguard by having a route for elephants which returned available sorts as a first instance. We could also throw an error if they submit a sort which isn't allowed. Using `Union` types (another post in the above blog) I _think_ it is possible to have the entire stack parameterised by a union type of the available sortings on a resource. That would immediately and automatically throw a user out with a 400, and prevent misuse of the code internally. But that was a thought experiment long ago, I've never implemented it.
Yep, that beats my 4 aces. I didn't see any embedded html client; so I guess that's separate. I've come to the same conclusion - html client deployed to S3 and http4s server deployed to EC2. Thanks!
How does this compare to log4s?
Thanks for this - your new tool looks great. I've been circling it as a prospect for the team since it came out. Without having been too far into the documentation or tutorial yet, is there a way of converting existing `.sbt` builds into mill builds, or at least a partial automated translation available? That would be very welcome and would make it a no-brainer for us.
I might use this for a couple of my projects. Unfortunately, some require sbt-native-packager to use jdkPackager, but otherwise it might be a nice replacement for some of my simpler builds.
Look at map, flat map, and for comprehensions. 
You can use andThen or combine to chain functions directly (only difference is order of functions). If you had them all return results in the same "container" (List, Option Future, Task), you could chain them with map or flatMap (or with for comprehension that desugars to them). Simplifiyng, such interface you can call "monadic", and if they have some properties (monad laws) you could call the container type "monad". 
Seems fine to me, some people like to use a pipe operator for things like this. implicit class PipeOps[A](val a: A) extends AnyVal { def |&gt;[B](f: A =&gt; B): B = f(a) } fileDirectory .|&gt;(function1) .|&gt;(function2) .|&gt;(function3) ... .|&gt;(function7) or you could also just compose the functions themselves like function1 .andThen(function2) .andThen(function3) ... .andThen(function7)(fileDirectory)
There’s currently no support for java builds. I believe there’s an open issue for that. 
Thanks for that pointer. I'll have to try it out on some of my pure-scala projects and wait on the hybrid ones.
To use a different Syntax than /u/joshlemer, I'd write it like this: ``` def runApp(fileDirectority: String): List[String] = function1 andThen function2 ... andThen function7 andThen function8 ``` or even: ``` val runApp: String =&gt; List[String] = function1 andThen function2 ... andThen function7 andThen function8 ``` BTW: It is a good idea to use precise types, instead of primitives. For this example it's okay, but in real world, please use something like a `Directory` type and not `String`.
Tip: use `private val a` and avoid polluting every other type under the sun with a dummy `a` method ;\^)
Cool! A Scala build tool without all of sbt's problems, but also without CBT's ridiculous boilerplate.
Seems like a nice tool, but: * what's the advantage over, say CBT? * I'm not a big fan that [the command-line tools](http://www.lihaoyi.com/mill/index.html#command-line-tools)' syntax is different from the shell syntax. I understand it's because the shell is ammonite, but I think it would be worth it to unify the two syntaxes (otherwise it's one more thing to learn). For example, I find it surprising that I can do `mill describe foo.compile` but not `echo "describe foo.compile" | mill`. Like for SBT, people will probably spend most of their time in the shell anyway, so what's the point of the special command-line tool syntax? Some typos: &gt; While of these snippets are trivially understandable Missing "most"? &gt; It's output will be &gt; [...] &gt; learning from it's mistakes `it's` -&gt; `its`
Repost: https://www.reddit.com/r/scala/comments/7xyce9/towards_a_brighter_tooling_future_for_scala/.
&gt; what's the point of the special command-line tool syntax? Automation and CI integration are big ones I think.
For me, especially with work projects, not having complete IntelliJ support is a non-starter. By complete I mean IntelliJ supports Mill not the other way around -- need to be able to attach the debugger, import libraries automatically, etc. Have you thought about adding an IntelliJ plugin?
I literally just Had a curry haha
The replies suggesting implementing the pipeOps and/or using andThen are interesting but I suggest you be careful as those suggestions quickly break appart and are confusing when going over functions with different number of paramenters. I've tried a few times already and it just becomes a huge ugly mess usually. 
Is this a new feature or just hypothetical? I can't find anything on it in the docs. 
I don't understand what you mean by "ecapsulating the first variable" could you give an example?
Wrapping it in a function
I'm not sure about docs, but there is [release notes](https://akka.io/blog/news/2018/02/23/akka-2.5.10-released).
I haven't tried it myself but [this](http://www.lihaoyi.com/mill/page/common-project-layouts.html#sbt-compatible-modules) looks like exactly what you want. edit: The below snippet is the SbtModule in its entirety. Pretty neat that it's so clearly defined IMO trait SbtModule extends ScalaModule { outer =&gt; override def sources = T.sources( millSourcePath / 'src / 'main / 'scala, millSourcePath / 'src / 'main / 'java ) override def resources = T.sources{ millSourcePath / 'src / 'main / 'resources } trait Tests extends super.Tests { override def millSourcePath = outer.millSourcePath override def sources = T.sources( millSourcePath / 'src / 'test / 'scala, millSourcePath / 'src / 'test / 'java ) override def resources = T.sources{ millSourcePath / 'src / 'test / 'resources } } } 
Primitive types are sometimes called weak types, the reason is that while you can hold values like `fileDirectory` and `name`, they are not the same, hence, if you receive a `fileDirectory` as a `String`, you should validate against the expected format for a directory, as a example, you could mess up with the parameter order while having a function receiving two strings and the program will compile, having specific types like value classes will not let your program compile instead. Once I covered 5 minutes on this topic in a tech talk: https://youtu.be/tIwr9AhCQYs?t=55m49s
https://doc.akka.io/docs/akka/current/stream/stream-refs.html
The best suggestion that I can make is that you seem to not be considering possible errors, based on the `runApp` function, it seems that you are probably interacting with a file system where you always need to check for errors, assuming that I'm correct, the result type from `runApp` function doesn't specify that an error could occur. As there are no error results encoded in the result type from your function, you are very probably required to handle exceptions which break pure functions, a way for improving this would be to encode possible errors like using `Either[ApplicationError, List[String]]` in the result type instead (here is the main usage that I have seen on scalaz and cats). You could be able to get quite clean code using the for-comprehension syntax while keeping the errors (if any), quoting a [piece of code](https://github.com/AlexITC/crypto-coin-alerts/blob/master/alerts-server/app/com/alexitc/coinalerts/services/UserService.scala#L84), you could get something like this: def setPreferences(userId: UserId, preferencesModel: SetUserPreferencesModel): FutureApplicationResult[UserPreferences] = { val result = for { validatedPreferences &lt;- userValidator.validateSetUserPreferencesModel(preferencesModel).toFutureOr userPreferences &lt;- userDataHandler.setUserPreferences(userId, validatedPreferences).toFutureOr } yield userPreferences result.toFuture }
In addition, if you just want to chain the functions, you can and should prefer kleisli composition. It is less powerful than using a for-comprehension but does the same thing in a situation, where the each function only relies on the result of the last one - and not on the result of the functions before.
Looks fine for me. Partial results are properly named, the code is easy to refactor or reformat, it's not too tall and not too wide. I also assume here that you're using exceptions for error handling and the partial results can be considered opaque. If not and if you want to make the code more monadic, then you'd put it in a `for`, but you'd already do it if that was the case.
Thanks that was a good talk. I learned a lot. And it answered some questions I had about my choices. Thanks
Is it really worth it? Such errors are amongst the easiest to fix and catch in tests, and adding types for everything only complicates your code.
It’s worth it in many cases. For example, we have primary keys on all db tables that are Longs, and instead of just passing Longs around, we have a type for each table, eg. class UserId(val id : Long) extends AnyVal class TeamId(val id : Long) extends AnyVal So instead of def isUserOnTeam(userId : Long, teamId : Long) Which is really easy to mess up when calling by giving the arguments in the wrong order, we have: def isUserOnTeam(userId : UserId, teamId : TeamId) The number of times the compiler has caught a mixup for us is ... well, it’s many. And the mixup is pretty seriois, because rather than throwing an exception, it would often generate nonsensical data in the database!
I disagree. This pipe operator is extremely useful and often makes code more readable. If you are doing FP I think it's worth using it on any type. Other languages (like groovy) have it built in already. Also, if you use it rather little, you can only import it in files where you actually need it.
It depends. If you create a new and precise type and use it only once, that's maybe not worth it (but maybe it is). But if you use it more often, please, make sure the types are describing what's going on in your program. Let's take a look at OPs example. He creates something like that: `def runApp(fileDirectority: String): List[String]` or written different: `val runApp: String =&gt; List[String]` The only thing that we get to know here is, that the fileDirectory might be.... a directory. And the result? Well it is probably the overall result of the app, but what is inside there? How will these resulting strings usually look like when the app is run? I don't know, you don't either. How does it look if we type it? `val runApp: FileDirectory =&gt; AppComputationResult` This is so much more readable *even* if we don't have the method parameter called "fileDirectory". Which means, even if you refactor the input parameter's name, the code will still be readable. You could call it `chicken` instead of `fileDirectory` and fool some people, but you cannot fool the compiler (because he will just refuse to compile). So I, as a human, can 100% rely on that this function will take in a file directory and produce some result from that. We can make that even more precise by constraining the `FileDirectory` type further to e.g. `TempFilesDirectory` so that someone cannot just put any directory into it. But there is more: you don't need to create new classes to wrap types like Strings. You can also use type tags which will disappear at runtime and still give you compiletime safety. To dig into that, I recommend starting with this: http://www.vlachjosef.com/tagged-types-introduction/
Oh, haha, I missread you. Sorry about that. ;)
Thanks, but how to integrate it in my current `build.sc` file? // build.sc import mill._ import scalalib._ object foo extends ScalaModule { def scalaVersion = "2.12.4" def mainClass = Some("foo.Example") } Sorry, I'm new to Scala...
I have a problem with such approach - that is it is easier to fall under the [inner platform effect](https://en.wikipedia.org/wiki/Inner-platform_effect). It is all smart and cool, but at some point you are inventing a inner system for reducing complexity... that might become more complex than the domain it is supporting. I was moving towards that direction in my own app, but at some point I noticed, that I started wasting time rewriting things so that they be more functional and smart, but I stopped delivering any actual value. I won't say that deep FP is a bad thing - because it's not - but for cases I worked with it's complexity and mental overhead were greater than those of problems they were solving.
No worries. Just replace `ScalaModule` with `SbtModule` and you should be good to go
Here are the threads I was talking about https://github.com/functional-streams-for-scala/fs2/issues/768 https://github.com/circe/circe/pull/459
It kills mars probes.
The support for custom matrix's is perfect and its the exact issue that I experience with my scalajson library (see https://github.com/mdedetrich/scalajson/issues/34 and https://github.com/portable-scala/sbt-crossproject/issues/47) Unfortunately I am forced to use SBT due to this library being part of the Scala Platform, otherwise I would definitely try it out (I think the main thing missing from the list is an equivalent to sbt-jmh or sbt-scalameter and scala-native support)
What about the Scala Platform specifically requires SBT?
Forgot to answer this earlier, but I don't think you have a point. The use of `Iterator` here is completely incidental. I can give you a version of that `foldr` implementation that works on (immutable) arrays by recursing with the current index. Again, no "list shape" to be seen here.
Still no success. The `build.sc` is here: https://pastebin.com/9qvaLBPe . I get the following error when trying to run it with mill: [19:29:15] ~/scala_reloaded/MillExample2 $ mill foo.run [33/33] foo.run Error: Could not find or load main class foo.Example 1 targets failed foo.run ammonite.ops.InteractiveShelloutException ammonite.ops.Shellout$.executeInteractive(Shellout.scala:56) ammonite.ops.Shellout$.$anonfun$$percent$1(Shellout.scala:14) ammonite.ops.Shellout$.$anonfun$$percent$1$adapted(Shellout.scala:14) ammonite.ops.Command.applyDynamic(Shellout.scala:118) mill.modules.Jvm$.interactiveSubprocess(Jvm.scala:37) mill.scalalib.ScalaModule.$anonfun$run$2(ScalaModule.scala:222) scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)
Some observations: - The `older` from `Director` method you are returning the youngest. - The `oldestDirectorAtTheTime` from Film is supposed to receive 2 films while it is receiving a film a 2 directors. - There are several methods that are not required by the exercise, for example, I'm not sure if `DirectorsAge` is doing what it is supposed to do.
Spark, DL4j, openNLP, coreNLP get me by but I see you know that, lol. LibSVM too I guess.
Would you share with us why you move away from Python? Python is strong in data science (numpy, pandas, etc.). I'm just curious.
Do you have an object Example in the package foo? Like package foo object Example{ def main(args: Array[String]) = println("stuff works!") }
We are working on predictive model with small time granularity ( in financial sector). You are right, Python is strong but not fast enough for our application. We are trying to tackle this problem by benchmarking diff solutions. If you have some recommendations, please let us know. C++ is an option, but its the last one for us. Scala looks promising. 
I have that ( here is a screenshot: https://i.imgur.com/VwiGlhO.png ).
I find DL4J to be just as capable as TensorFlow. Just requires more boilerplate for certain applications. 
Thanks, I will solve those two "problems". 
This one better : class Director(val firstName : String,val lastName : String, val yearOfBirth : Int){} object Director{ def older(director1 : Director, director2 : Director) : Director = if (director1.yearOfBirth &lt; director2.yearOfBirth) director2 else director1 } class Film(val name : String, val yearOfRelease : Int, val idbmRating : Double, val director : Director ){} object Film { def highestRating(film1 : Film, film2 : Film):Film = if (film1.idbmRating &lt; film2.idbmRating) film2 else film1 def oldestDirectorAtTheTime(film1 : Film, film2 :Film) : Director = if (film1.yearOfRelease - film1.director.yearOfBirth &gt; film2.yearOfRelease - film2.director.yearOfBirth) film1.director else film2.director }
Check out Julia. It is relatively new but has promise for numerical/scientific computing by specifically putting performance first. It has equivalents to Numpy and Pandas. I'm not sure if there has been much development on ML packages yet, but it certainly has the base infrastructure to support performant ML tasks. It also natively supports C and C++ code packages, and is being supported as one of the main languages for Jupyter Lab which is the next gen Jupyter workflow. [Here is a link.](https://julialang.org) I have worked with scala trying to do data science tasks and unfortunately it is not up to par with python. Python being the de facto choice among most researchers for data science tasks make it far superior to everything else out there currently. Scala might seem like a good alternative larger datasets because of its tight integration with Spark, but I would say anything that can be done on a single core should really be done with python. Scala is really good for advanced type systems, which in my experience has not really been crucial to data science pipelines and in reality has just slowed development. 
What boilerplate are you referring to ?
`package`, `import`, the top-level `class`/`object`, etc. Such structures are irrelevant noise in a build script.
One of the advantages over CBT is that the structure of the build is statically known, which means mill can do a lot of smart caching, whereas the structure of CBT's build can be only inferred through runtime introspection, meaning that past the first structural level you'd have to actually run the tasks to know the build's structure. So if you want something cached you have to do it yourself. CBT implements its own resolver, whereas mill relies on coursier which is widely used and very efficient. Also, mill uses json as it's default serialisation format, which is a major plus for interoperability, whereas CBT's output is whatever the toString of what you return is. Disclaimer : I love CBT, it's awesome, but mill takes inspiration from it and takes it much further.
Then you're probably in for a disappointment. Below is the most minimal build import mill._, scalalib._ object foo extends ScalaModule { def scalaVersion = "2.12.4" } Note that one of the points of SBT alternatives such are Mill or CBT is to provide users with a way to structure their builds using semantics that users are already familiar with. A class is something that most developers with a little bit of OO experience can easily understand. SBT's most minimal build might be a selling point for you, but when you start having multi-modules / scope and you need to structure things a bit better, SBT's model is a major pain in the ass, whereas this OO "boilerplate" makes sense to me (and probably to a lot other devs) 
Is capable of using GPU's ? Whats the speed like for DL4j ?
Oh. Lame. Coming from Maven, I'm very *very* tired of boilerplate-heavy build scripts.
That's interesting. Because we're a jvm stack and I recently switched our ML to mostly python from Java/Spark because we couldn't get the high throughout, low latency performance we needed without going all in with Spark streaming.
&gt; we couldn't get the high throughout, low latency performance we needed without going all in with Spark streaming I didn't get it. You can use Spark with Scala. Why did you need to switch to python for that ? &gt; I recently switched our ML to mostly python from Java/Spark 
Hehe :D true. The poor orbiter...
I believe it has support for Nvidia’s CUDA
Write neural net in Tensorflow in Python. Load it into Scala.
Have you tried Rust?
MXNet supports Scala, and though it might be lacking some in documentation. As a bonus, depending on your low-level needs, you can take advantage of its Gluon interface, launched last year, for getting up and running quickly.
I didn't mean to imply we ported Spark applications to Python. We explored Spark as a high performance way to do low latency classifications on the jvm but Spark is too slow for that. And the Java ML frameworks had their own scaling performance problems. So we still have Scala/Spark for some uses where some latency is acceptable, but for use cases where we're doing millions of classifications per day with low latency requirements, we're using Python.
I have a question in [here](https://www.reddit.com/r/rust/comments/7zohri/is_there_any_stable_rust_machine_learning/). But it looks like the community is very new. I think Scala ML is more advanced, but not as advanced as Rust. Any insights ? Feel free to enlighten us if u have some expereince 
If I have to give subjective points on state of non-DL ML system in (0-10 range, with 10 being the best), it is like: Python - 10, Scala - 2, Rust - 0.1 The gap between Scala and Rust is quite smaller, than between Scala and Python. In both Rust and Scala, you will often time find yourself in writing your own algos, because libraries do not have them. There is for example very nice Smile library in Scala, but completely lacks sparse data support. In DL you can use Tensorflow practically everywhere (just define your network in Python). Or you can use MxNet, which has Scala bindings. (in Rust you have CNTK bindings). 
How does providing this strong type help if both UserId and TeamId are aliasing a long? As in, what compiler error are you expecting and how is it enforced if I want to use id of userId of 123L and teamID of 3L? Does this rely on some specific business logic (e.g. there are only 6 teams)? What if I really want to use userID 3 and teamID 4? I understand it in case like email (that has SOME validation..) or age (that is a positive Int, since scala does not have dependent types).. but for lastName / firstName || userId / teamId, what extra type information am I getting?
Interesting. Now, the questions remains: Does it worth it to switch to Scala or Rust. If I need to switch, what should I think of Rust or Scala ? Knowing that I have the option of binding libraries 
Well, IMO, sbt's “bare” style is the most conceptually elegant. It's just a list of expressions, each of which evaluates to some sort of information about the build. Sbt is moving away from that style, however, because it doesn't play nice with multi-module projects. But Maven and Pants have already shown us how to deal with that: multiple build scripts, one for each module. Why sbt is going with that awful `lazy val` style instead is beyond me. If you don't want to do the list-of-expressions thing, it would also be conceptually elegant for the whole build script to be a single block expression, which ultimately evaluates to a complete description of the build. E.g.: val scalaSettings = … val task1 = T(…) val task2 = T(…) Project(scalaSettings, task1, task2)
Wait, is that possible?
It's useful because you can push all the validations and translations of UserId/TeamId &lt;-&gt; Long to the edges of your program, like at serialization / deserialization, and then never have to worry about mixing the two up again. You can't accidentally pass in a UserId for a TeamId to any method that takes a TeamId. It also arguably makes the code more readable because you can see in the types what the arguments are. It's also way easier to find everywhere that UserId is used in the program than finding all usages of Long and then figuring out which ones represent UserIds. It also provides a nice class where you can start defining logic that pertains to UserIds, rather than having that logic spread around the code, and likely duplicated.
Hm, what accounts for the faster performance of Python in this case? E.g. is it from calling libraries written in C or something? Which version of Python are you using -- 2 or 3? Are you writing your own Python libraries for data crunching or relying on existing libraries? Likewise for Java &amp; Spark. Thank you very much for your time, I appreciate your comments very much.
Yes we support cuda and cudnn. We have our own tensor library underneath the covers. Next release (out in a few weeks) we will be releasing a pytorch like api that is lower level as well. Come see us on the gitter if you have any questions: https://gitter.im/deeplearning4j/deeplearning4j
It looks like DL4J has this capability. https://deeplearning4j.org/tensorflow
How are you piping data to your models in Python? I'm assuming you were using Spark Streaming to read data in the Scala/Spark environment - what is the Python alternative? I'm in the process of designing a platform around a couple of ML models and am researching the best technologies. Any insight you might have would be greatly appreciated!
I've dl4j a lot in the past, but didn't know if this. awesome, thanks! 
Yes I remember reading this. I just wasn't sure if it is good to do that. Or accepted among Scala programming 
To anyone considering to give mill a try, I just tried it and successfully built and released a library ([playsonify](https://github.com/AlexITC/playsonify)). My experience was really good, the learning curve was light and when i wasn't able to find something in the documentation, being able to navigate to the task source code using IntelliJ was quite useful. The biggest advantages that I could list are: - The build definition is more intuitive than sbt, for example, I was able to abstract common pieces for two modules easily because, well, it's scala, I have never built a definition with several modules before as simple as with mill. - Releasing a library to Sonatype is straightforward, when I did it with sbt, I spent most of the day on the task with several frustrating experiences. - As I understand, mill uses coursier which gives a quite better experience while compiling the project. - I got several answers fast on the gitter channel. On the negative side: - You need to run `mill mill.scalalib.GenIdeaModule/idea` each time that the build definition is updated, this is something that I didn't know and the development gets simpler with this. - Personally, I like how the application logs are not colored and the test results are colored when using scalatest with sbt, with mill, you get a single color only. Give it a try!
Definitely from the libraries written in C and/or C++. Python is slower than Scala by a fair bit.
I recently switched from MLLib to h2o(with Sparkling Water). So far it has been an awesome decision - h2o is way more customizable and user friendly. We still transform our data with Spark tho, h2o serves as main ML engine. 
Apache flink isn't a fit?
Haha - how come? Haskell is a much steeper paradigm shift from what I saw. Is it because Scala is so "overloaded" with alternative syntaxes and ways to accomplish the same thing?
I think this has the same problem as the original solution described in the blog - that is, it doesn't take into account the fact that producing/consuming the response entity may take some time. Although, in your snippet there's also a `entity2String` - I suppose that's your custom code - it would be worth checking how it would behave on streamed entities.
Thanks! I've updated the code in the blog: https://blog.softwaremill.com/measuring-response-time-in-akka-http-7b6312ec70cf (and added a pointer back here)
Right, in general that might add some heavy calculations but I think not in my case: private val entity2String: ResponseEntity =&gt; String = { case Default(contentType, length, _) =&gt; s"$contentType of size $length" case withoutKnownSize =&gt; s"${withoutKnownSize.contentType}" } (correct me if I'm wrong, there is a lot I do knot know about Akka and stuff!)
Indeed. It's simply reduces the surface area where bugs of this sort can occur, eg. only when you use an untyped interface like SQL or JSON. Whenever you do `new UserId(...)` or `new TeamId(...)`, you have to be careful. But then, the extra syntax associated with doing this is a pretty good reminder to be careful. If you take your code example, there's a pretty good chance someone reading it can discover that `new UserId(someInput)` is wrong. But it's hard to tell from that example alone if `isUserOnTeam(uid, tid)` has the arguments in the wrong order.
 private val entity2String: ResponseEntity =&gt; String = { case Default(contentType, length, _) =&gt; s"$contentType of size $length" case withoutKnownSize =&gt; s"${withoutKnownSize.contentType}" } As for the problem you described, `DebuggingDirectives.logRequestResult` takes a function with currentTimeMillis in closure, and call that function once request is processed: def logRequestResult(magnet: LoggingMagnet[HttpRequest ⇒ RouteResult ⇒ Unit]): Directive0 = extractRequestContext.flatMap { ctx ⇒ val logResult = magnet.f(ctx.log)(ctx.request) mapRouteResult { result ⇒ logResult(result) result } } It used the same tools you described in article to fix the issue. So if the passed time is taken when request was created, and finish time is calculated within a function triggered by `logRequestResult` when result is already handled, then I guess it should work as expected.
You are a gold mine for the scala community !
Read it, did exercises few month ago. Good stuff, I approve this book !
&gt;Check out Julia Seems like a during suggestion for /u/__julia.
&gt; In larger application I will probably have multiple points where I need to do this and each of them is contender for mistakes of this sort Disagree; the boundary of a large application tends to be small, most of it is internal business logic. The only places you would instantiate a `UserId` and `TeamId` are at the boundaries - probably something like a JSON API that you expose to clients (ideally you could move the type safety even further by using a typesafe RPC system like Thrift). Otherwise you're just passing them through - if you got a `UserId` from somewhere then you know it's a valid `UserId`. (Even if it's e.g. in a database, you defined the mapping for the database column in one place, so in order to read it as a `UserId` then you must have written it as a `UserId`). You wouldn't use a specific literal `UserId` in code, so `new UserId` should be a big red flag and can be something you e.g. automatically highlight in code review.
Just via REST calls now. We weren't using Spark Streaming. Up front training was done offline.
Yea it definitely wasn't faster because of Python, but rather the ecosystem for ML in Python. We were also using Weka whose predict calls are not thread-safe which caused poor resource use on our servers. There are probably much better JVM alternatives, but I didn't see one that'd fit our needs. Python also had the added benefit of being easier to hire data science/ML folks for. Trust me, if I had my choice, I'd be on Scala, but it just didn't make sense for us.
What's the error message?
[react4s.org](http://www.react4s.org) - I'm working on launching a site full of React4s examples. Feedback appreciated!
This was a year ago. We looked at Flink, but I think FlinkML was pretty limiting (as was Spark ML) in terms of algorithms available. Really our problem was less about big/streaming data throughput and more about scalable predictions and appropriate resource usage. 
&gt; Is it because Scala is so "overloaded" with alternative syntaxes and ways to accomplish the same thing? I guess that, plus the fact that since I was more conversant with OOP side of things, it was difficult to understand some of the FP concepts possible in the language. For example, my OOP worldview about extending and implementing interfaces and classes as a mechanism for re-use and adhering to a defined contract was always standing in the way of me fully understanding ADT when they implemented using sealed traits in Scala...This was not the case in Haskell...ADT's are native to Haskell and once you come across them, there is nothing standing in your mental space preventing you to fully understand them. I also found it easier to reason about types and type signature in Haskell compared to Scala, because in Haskell I did not have the idea of seeing classes as a blueprint for creating objects standing in my way. I could see types for what they are, and it was then easy to build my knowledge of what monoids are, to what applicatives are to what funtors are and to finally what monads are, and when I finally saw the do notation it clicked...so when I came back to Scala, the for/yield syntax was no longer confusing and I stopped thinking it had something to do with looping. It was also easy to start thinking in functions and expressions in Haskell. And yes you do not have that much overloading of syntax as you have in Scala...that also helps. So basically Scala not only overloads concepts (aka it fuses OOP/FP), it also overloads its syntax. I am in no way a Haskell developer now, neither have I mastered the language. Yes, I can read simple Haskell code, but will still struggle to write those simple code myself. But learning the language made the FP concepts that needs learning obvious and allowed me to learn them without tripping over myself with my OOP mental models.
Your link has different code from what you posted here (an extra `&lt;: ...` stanza where it's syntactically invalid), but the main issue is the one in your error message. `succEquals` returns evidence that `succ[this.type]` equals `succ[m]`, but `this.type` could be any subtype of `nat`. So `(n#add[a]#add[b])#succEquals[n#add[a#add[b]],n#addAssoc[a, b]]` is a witness that `succ[n#add[a]#add[b].this.type]` is equal to `succ[n#add[a#add[b]]`, but in the general case that isn't the same as the type you need - `succ[n#add[a]#add[b].this.type]` is `succ[_1] forSome { type _1 &lt;: n#add[a]#add[b]}` and that's not necessarily the same type as `succ[n#add[a]#add[b]]`. You could make `succ` covariant but that's a big hammer and might break other properties. Probably the best thing is to avoid using `this.type` anywhere - maybe move `succEquals` out of the `nat` trait and just statically define `type succEqual[m &lt;: nat, n &lt;: nat, eq &lt;: equal[m, n]] = equal[succ[m], succ[n]]`?
Yea, I thought about that. I might have to define, natural induction as an axiom of `nat` and define `associativity` through that. Because, otherwise I need a way to say that nat is either `zero` or `succ` of `nat`, I am not sure how I could go about defining a generalized structural induction.
I wish the type arguments can be curried. Its complete mess when I try to do that. 
You could define an eliminate-like... thing, on `nat`? Something that says if you pass in a `zero =&gt; A` and a `succ(n) =&gt; A` then you can get an `A` back. But I'm not sure how to do all that at type level - you always have to do this awkward thing where you can't ever have an object that represents `f(A, B)` with an output of `C` directly, all you can do is produce a witness `g(A, B, C)` that encodes this fact. Best of luck.
This is as far as I have got. I just pushed that error message one level down. But I think I am doing something fundamentally wrong. https://scalafiddle.io/sf/enaGqD4/0 And thanks for that `kind-projector` it looks lot better. 
&gt; use private val a and avoid polluting every other type under the sun with a dummy a method serious question, i don't understand what you mean. unless you write `something.a` nothing in your code "gets polluted"
The one thing that really screwed me up learning Scala was the syntactical sugar. I don't know why, but when I first looked at it, things like: def helloWorld(f: String =&gt; Int): Int = f("42") made absolutely no sense, because I was looking at the `=&gt;` and trying to figure out what the operator was. I didn't realize that `String =&gt; Int` was really chrome for `Function2[String, Int]`, I actually thought that `=&gt;` was some kind of magic "functionate" that took an `f: String`. I had the same problem with `(String -&gt; Int)` vs `Tuple2[String, Int]` Once I realized that the variable was on the left and the type was on the right and putting parens would always make it clear, I went through a period of labelling everything `f: (String =&gt; Int)` or `f: (String -&gt; Int)` just to be sure I was getting it right. And now I'm okay reading higher kinded types and Shapeless stuff. So it does get better.
Yeah, it would work, but I want to follow the project layout of IntelliJ. I wonder if Mill can work with that project layout.
Yea, I really wanted to define them as functions and have the types define the proofs for it. But the problem is the definition of functions with dependent types (Pi and Sigma) is too cumbersome in scala. Thanks for the help, I understand why it causes problems. I would try to read up some stuff around the univalence axiom.
Ah Scala, the bastion of systematic abuse, perpetually going nowhere, since all the decent people, and skilled programmers left you all behind.
This code illustrates one of the reasons I'm not a fan of the data frame interface. If you used RDDs this would be one map with a function that converts one case class to another. That function would be about the same number of lines but much more legible. 
Nice work! Any chance prop-passing could be made more concise than `div(new DivProps { className = "divClassName" })(...children...)`?
This is how I learned Scala: https://github.com/pathikrit/scalgos
As the other comment said, the DataFrame API is often not the best when it comes to parsing and extraction of columns. In my own workflows I tend to use RDDs for the initial processing and then dump the columnar data in parquet format for later processing using DataFrames. If you do need to extract starting from a DataFrame then I'd recommend defining a plain old (but pure) function that does the extraction to produce a case class of the fields, making that into a UDF, and then using that in a single `withColumn`. That will produce a column with the extracted fields nested underneath. If that's good enough for you then you're done. If you need / want them flattened out then you can use `select` and `drop` to do so: &lt;code&gt; case class ExtractResult(foo: String, bar: Int) def extract(in: String): ExtractResult = ExtractResult(in, in.length) val extractUDF = udf { (in: String) =&gt; // wrap in an Option since Spark passes nulls around Option(in).map(extract) } myDataFrame .withColumn("temp", extractUDF($"some_column")) .select($"*", $"temp.*") .drop($"temp") &lt;/code&gt; (Sorry if that came out funky, trying to type code on mobile). In any case, that `extract` function can live in a library somewhere where it can be tested or pull in other libraries (so long as it stays pure and serializable).
Ok thanks 
http4s is looking decent though, right?
fintrospect is there though. 
You can't have everything. Maven may be somewhat more declarative (the whole boilerplate example is deceitful because Maven having `xml` means it has more redundant boilerplate but I will give that a pass for now). The thing is, Maven is absolute crap when it comes to handling certain things, for example because its in `xml` it has all of the following issus 1. Anything outside of their XML dsl is not possible/they have to re-implement. For example, you can't do proper scala crossbuilds in maven. Spark which uses Maven has to manually patch the `pom.xml` to support multiple Scala versions. This is even worse if you factor in Scala.js. 2. Anything that you have to do which is even (slightly) not supported by maven's XML has to be a Java maven plugin, which can do all noughts of crazy stuff (I have seen even worse stuff there than in SBT) 3. It not being a programming language means its ultra nice when debugging Maven builds (being sarcastic here). You usually have to have some pseudo maven/xml like interpretor just to figure out how stuff is going wrong I would stop holding maven to some holy grail, yes its nice for Java style projects but they also often tend to be much simpler/more trivial than Scala builds.
I figured it out! This is what you want: import ammonite.ops._ import mill._ import mill.scalalib._ object root extends SbtModule{ def scalaVersion = "2.12.4" def millSourcePath = pwd } and then run with `mill root.run`
I'm happy to see these benchmarks posted. I can only hope the folks behind Http4s take these results to heart, having lost to Akka-Http in 5 out of 6 benchmarks.
update too.
I recommend switching to the free version of IntelliJ. I have found that Eclipse + Scala are rather fussy. Others will disagree.
Same for me, big + for intelliJ. Their vim mode (installable add-on by the jetbrains team) is also quite good, if you're that kind of person.
Are you hitting ctrl? I don't see hyperlinks unless I hover while holding ctrl.
&gt; Others will disagree. Like me! The current de facto IDE monoculture is a bad situation. Eclipse and basically anything that's not IntelliJ need more users.
I wasn't getting anything scrolling over functions, not definitions or return types or anything. 
I hope LSP will solve this. I like Vs code so I gave it a try with SBT 1.1, but the support is not quite complete enough for everything I like to do.
Given something like val f: Int =&gt; Int = identity if I hover over `f` in something like f(42) `f` gets hyperlinked for me. Or did you mean something else?
My usual question when I see other people's Spark code: Why aren't you using a dataset when this is exactly the use-case for datasets? Define your domain and make use of Spark the way it was intended - i.e. not as a glorified SQL wrapper.
&gt;This code illustrates one of the reasons I'm not a fan of the data frame interface This is poor advice. You can do the same if you used the Dataframe api as it was intended - in terms of Datasets. Dataframe is just `type Dataframe = Dataset[Row]`. Use the api. RDD's slow your performance and bloat your memory usage drastically in comparison.
It's not a monoculture, nor is it a dyculture. There's Intellij, Eclipse, ensime, vim, and a healthy set of Atom plugins, and even more if you bother to search.
How did you open the project?
Right, I know about all of those. That's why I said "de facto". Intellij's market share is 90-something percent. After a new hire, I now know of one other Eclipse user at my work, a decent-sized Scala shop, for example.
I think the handling of do-while complicated the proposal quite a bit. I would probably just have removed do-while altogether to avoid the complexity in having to specify how do-while interacts with all the other places using it. `then` would have been nice, because it made the `then` branch line up with the `else` branch. But whether a simplification is substantial enough to go forward with it ... that's a hard question to answer, because the standards around that are rather arbitrary in Scala.
I would say type inference, pattern matching , and case classes would be a few easy examples for why you would use Scala over Java.
I just finished v0.1 of my streaming and state propagation library for Scala.js, [Airstream](https://github.com/raquo/Laminar/tree/master/src/main/scala/com/raquo/laminar/experimental/airstream). It's designed for safely representing both state and events in hierarchical UI components. My own [Laminar](https://github.com/raquo/Laminar) library uses it now instead of xstream.js. Couldn't be happier with this transition. Airstream is different from other reactive solutions in these ways: * **Mandatory ownership of leaky resources** – it is impossible to create a subscription without specifying when it shall be destroyed. This helps prevent memory leaks and unexpected behavior. * **No FRP glitches** – neither observables themselves nor their observers will ever see inconsistent state within a transaction, at no runtime cost. * **One integrated system for three core types of observables** – Event streams alone are not a good enough abstraction for anything other than events. * **Small size, simple implementation** – easy to understand, easy to create custom streams. Does not bloat your Scala.js bundle size. More details in docs. I'm going to publish it as a separate package in a couple weeks when I release the next Laminar version.
Have you looked at http://www.geomesa.org/ ? It's built in Scala, but I don't know a lot more about it.
&gt; Well the other side of that is: should people use what they don't prefer just to do some small part in avoiding a monoculture? Of course people should use what they want. But it's important to point out that whenever people say "use what everyone else does!" that that will inevitably lead to a monoculture if everyone does so. &gt;but I always had great trouble getting eclipse to work how I preferred Eclipse/ScalaIDE is a lot better these days. What you described is how I've always felt about IntelliJ. Different strokes and all that.
&gt; I have not looked into the individual TechEmpower tests yet. I can tell you that in the past we've had issues with TechEmpower, with things like other processes holding onto the port Play is trying to use, so that may be the issue with the failed tests. There are also possibly some issues with configuration that has not been properly updated to 2.6. It would be great to have some help from the community on this if someone has the time. &gt; I just ran a quick test on my Macbook Pro and here are the results I'm seeing. I tested with https://github.com/playframework/play-scala-starter-example/, adding a simple GET endpoint and a POST endpoint receiving a small JSON body. &gt; Play 2.5 (GET): wrk -d30s -c1000 -t8 --latency http://localhost:9000/test ~ Running 30s test @ http://localhost:9000/test 8 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 20.84ms 8.47ms 139.83ms 73.75% Req/Sec 4.98k 1.32k 12.26k 82.98% Latency Distribution 50% 23.72ms 75% 25.89ms 90% 28.97ms 99% 31.60ms 1189187 requests in 30.10s, 145.16MB read Socket errors: connect 0, read 1089, write 0, timeout 0 Requests/sec: 39505.79 Transfer/sec: 4.82MB Play 2.6 (GET): wrk -d30s -c1000 -t8 --latency http://localhost:9000/test ~ Running 30s test @ http://localhost:9000/test 8 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 16.93ms 9.76ms 191.80ms 62.94% Req/Sec 5.28k 1.77k 14.76k 83.46% Latency Distribution 50% 17.76ms 75% 24.59ms 90% 28.21ms 99% 31.58ms 1262238 requests in 30.07s, 154.08MB read Socket errors: connect 0, read 1575, write 0, timeout 0 Requests/sec: 41981.22 Transfer/sec: 5.12MB &gt; Play 2.5 (POST): wrk -d30s -c1000 -t8 -s post.lua --latency http://localhost:9000/testPost ~ Running 30s test @ http://localhost:9000/testPost 8 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 40.47ms 18.94ms 529.91ms 78.19% Req/Sec 2.53k 0.88k 4.10k 69.94% Latency Distribution 50% 38.41ms 75% 48.64ms 90% 60.88ms 99% 92.34ms 598509 requests in 30.09s, 73.06MB read Socket errors: connect 0, read 1179, write 0, timeout 0 Requests/sec: 19887.66 Transfer/sec: 2.43MB &gt; Play 2.6 (POST): wrk -d30s -c1000 -t8 -s post.lua --latency http://localhost:9000/testPost ~ Running 30s test @ http://localhost:9000/testPost 8 threads and 1000 connections Thread Stats Avg Stdev Max +/- Stdev Latency 18.50ms 19.85ms 616.06ms 98.96% Req/Sec 5.15k 838.81 8.74k 85.08% Latency Distribution 50% 16.73ms 75% 22.85ms 90% 25.57ms 99% 39.57ms 1231158 requests in 30.08s, 150.29MB read Socket errors: connect 0, read 1283, write 0, timeout 0 Requests/sec: 40934.58 Transfer/sec: 5.00MB &gt; Of course these are not completely scientific tests, but they seem to show we provide about the same or better performance. The only modification I made from https://github.com/playframework/play-scala-starter-example/ is disabling all filters by setting play.http.filters = play.api.http.NoHttpFilters. This is because there are some filters enabled by default in 2.6 such as the CSRFFilter, and I wanted to test with identical configuration. If you actually needed a CSRF filter on 2.5 you'll also need it on 2.6, so there should be no difference, and if you were concerned about performance and did not need certain filters, you would disable them. This is something we should also check in the TechEmpower tests. https://groups.google.com/forum/#!topic/play-framework/9WIyCTdxaiI
&gt; turning columnar store data into an RDD is a waste He indicated quite clearly that he did not have a columnar data store.
[application.conf](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Scala/play2-scala/play2-scala-slick/conf/application.conf) **# Disable default filters ** play.filters.enabled = [ ] 
If you don't know how Scala suits whatever problem you're trying to pitch the solution to when how would anyone else?
Scala is one of the programming languages that support (ML-style) functional programming very well. Java has gotten closer, but is still very far from good support; and some of its support is... not so nice, like all the many different overloads (https://docs.oracle.com/javase/9/docs/api/java/util/function/package-summary.html covers many or all of them in the standard library) for something so simple as `Function0`, `Function1` and `Function2` in Scala. Why is (ML-style) functional programming great? Well, for one thing, it means you either have to worry a lot less or not worry at all about certain very bitter facets of concurrency, especially memory consistency, memory models and happens-before relationship, etc. The reason is that when you modify memory from one or more threads (or, typically similar abstractions like goroutines in Go), and read it from one or more other threads, race conditions are not the only thing that can go very wrong. For more information on what can go wrong on for instance the JVM, I (strongly) recommend the book `Java: Concurrency in Practice`. Try also skimming through some examples on its website: http://jcip.net/ (for instance code example 3.15 is especially "fun"). While on the JVM the behaviour that can happen if memory consistency is not ensured, is not as bad as undefined behaviour in C++ (where absolutely anything can happen), it is still in that direction. The reason for the strange behaviour when memory consistency is not ensured is stuff like JIT-optimization, which relies on memory consistency being ensured when it does its optimizations. You could then use locks and synchronization primitives and abstractions similar to locks, but locks can be very difficult to get right to ensure that you protect everything you need to protect, especially if you use locks not just to prevent race conditions but also to ensure memory consistency. Furthermore, locks can lead to deadlocks, livelocks and starvation. And the maintainability of locks thus tends to be very bad if you use them more than in a very limited way. But if you use functional programming, you either do not read and mutate memory from multiple different threads (or at all), or you do so only to a very, very limited degree, which makes this either a non-issue or much, much easier. And then there are all the other benefits of functional programming in addition, like not having to worry about global mutable state used for computation (1) in regards to maintainability, refactoring, correctness, modularity and dependencies, etc. (1): (I write "global mutable state used for computation" because databases, at least as far as I understand them, are both global and mutable. But they are generally used for persistent state, not computation, which helps considerably, and relational databases tend to provide a number of very nice guarantees).
Hi, author here. This library is my result of trying to implement an API definition on the type level, as it is done by Haskells Servant. I hope you find it useful and maybe consider adding support for other client libraries (besides htt4s which is already included). Cheers
Looks interesting! I wonder whether the Api could be defined without using the first symbol `:=`? It'll look much cleaner and closer to Servant if you could do this: ```scala val MyApi = ("fetch" :&gt; "user" :&gt; Query[String]('sortBy) :&gt; Get[List[User]]) :|: ("create" :&gt; "user" :&gt; ReqBody[User] :&gt; Post[Unit]) ```
I think I had some problems when using `implicit class StringOps[S &lt;: String](path: S)(implicit wit: Witness.Lt[S]) { ... }` at the beginning. That is why I use `:= :&gt;` to create an initial empty api list instance. I can try the implicit extension of String again. Maybe I oversaw something at the beginning.
That's what I guessed. I think it's doable.
edited my response above
I also thought about that, but then you make `:&gt;` larger and you still have an extra symbol. Not sure if this is a better approach.
`for` comprehensions. The equivalent to your example would look like: for { f &lt;- foo() b &lt;- f.bar() } yield b.baz() 
Thank you! I'll look into syntax highlighting soon.
Yeah it doesn't make it shorter, true. You can use `&gt;:` or so instead of `:&gt;:`. Though, the advantage of doing it like that is, most Scala developers are rather familiar with that kind of style because of e.g. shapeless or slick.
I'm not trying to pitch it, I just have to explore what makes it advantageous over other programs such as haskell and Java. 