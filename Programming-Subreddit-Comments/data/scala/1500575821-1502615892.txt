Yes, it's a web application, so you have to be online. No using ScalaQuest on a plane, sorry :|
Is a monad more like a burrito or a railroad?
&gt; In perspective, Scala doesn't do a better job in enforcing "good code" compared to Java (if anything, I have seen more worse code made in Scala compared to Java, often because in Scala people tend to over engineer and do to much magic. The worse thing about Java is the boilerplate) I don't quite agree with that latter point regarding Java typically having less magic, since Java may end up having reflection, bytecode generation, code generation based on XML, annotations and compiler plugins, etc., which can have much more magic than similar solutions for many of the same problems in Scala given that Scala is overall more expressive than Java.
Regarding the usage of IDEs, while being easier to make tools for is as such a good thing, it is also not a bad thing if some of those tools are less required or necessary in the first place. Case classes in Scala is one case where I personally much prefer the case class over the generated Java equivalent, since the case class in Scala tends to be much simpler and easier to read and maintain compared to the generated Java code. Another argument against tools is that tools are not always widely available or available together. For instance, if you have an online source code repository viewer, you will typically not have the same niceties as you may have in an IDE like folding stuff like boilerplate code together. This means that the code and languages which has less dependence on tools will typically be nicer to view.
Definitely a pizza. 
While Kafka is written in Scala, and used to have a first class Scala client API, that is gone now, and official API is Java only.
I find that comparing compile times and auto completion between Scala and Java is comparing oranges and apples. Yes, maybe auto completion is more stable and reliable in Java. But you get your `String` or `List&lt;String&gt;` or `Builder&lt;X&gt;` and that's it. To be *light years ahead* the autocompleted types must also be as usefull as their counterparts in Scala, but they are not. Inferring `List[String]` works for Scala quite as well. Same goes for compiletimes. Yes, Javacode compiles faster, but to get the same amount of confidence that your program works, while it is in Scala enough to compile, you will need to execute or test your program in Java to see if your annotations/xml etc. work. Is that really faster than? I don't think so. Which is not to say that Scalas compiletimes could be improved, but this is another topic.
&gt; Yup, and I have seen worse in Scala, whats your point? How do you know you've seen worse? As others have said, typical java enterprise apps consists of a bunch of compile-time and annotation magic and the fact that with a significantly weaker typesystem you'll be forced to write less safe code which is worse than writing "clever" code - from your comment history you're really against pure FP so, maybe if monads and scalaz were the worst thing you've seen then you haven't seen enterprise spaghetti. &gt; Yes and in Scala you spend a lot of time waiting for code to compile (or resolve in SBT), and Intellij is a lot less performant in Scala than in Java 1-3 seconds is a lot? Due to incremental compilation we only need to compile 1 (or 2 with the test suite) file(s) at a time which is not that much. &gt; Its a weird definition of garbage if IDE's have a much better ability to work with it because the type system isn't so complex that you get polynomial complexity for basic completions on certain types ;) At this point I'm not sure that you've *ever* worked with Scala code - how the hell would you get "polynomial complexity" for method completion?! &gt; Try working on a Scala codebase that has a non trivial amount of LOC. Done it, no worries. Now go and try to work on an app with millions of lines in java ee 7 or spring. I've worked on multiple java projects with millions of lines and it wasn't a piece of cake as you're trying to tell us... &gt; In some cases I have maxed out memory because Intellij (or ensime) is forced to aggressively cache all the type trees with different permutations in memory in order to get decent performance. How many memory did you have? Because that's pretty important in this context... &gt; In any non trivial project, the auto completion in Java is light years ahead of Scala. I don't believe you, if you don't use macros then auto-completion is almost flawless.
Hey guys, I'm looking to programming in Scala in the future but one thing about it bothers me: **automatic semicolon inference**. This prevents me from using the Allman style of brace placement, where each brace is on its own line, because the compiler may incorrectly place a semicolon. I've always felt the Allman style makes code look cleaner, beautiful, and easier to read by enabling clear alignment of code blocks. In my opinion, K&amp;R style braces look ugly and make it harder to identify isolated code blocks easily. This might be a trivial issue for some, but my point is that coding style should NOT be enforced by the compiler. It should be the personal preference of the developer. I'm sure I'm not the only one who feels this way. Is it possible to add foolproof support for the Allman style of indentation and brace placement in Scala 3.0? An example of Allman style: if (x &lt; y) { // some code } An example of K&amp;R style: if (x &lt; y) { // some code }
Why do you think the compiler might place a semicolon somewhere? I assume you mean it places a NOP somewhere, but still, why do you think so?
I don't know of any circumstances offhand where Allman-style would result in a different parse, although a blank line before the opening brace will. I should note for your sake that this style is basically unheard of in Scala and you will never hear the end of it if you write this way.
Most tools should work with an equivalent Java API. I have had some issues with Akka Java and intellij showing errors where it shouldn't but otherwise it's viable you might just miss using Scala. You could always wrap custom scala and call it from Java. 
I've used Allman style curlies in two or three Scala-projects without noticing any issues. Do you have concrete examples to show where it fails?
Everyone is saying hire Java devs, which is fine. But also try to branch out and get some Haskell or Clojure devs. Or any devs that have functional programming experience. If you hire nothing but java devs you can quickly end up with some bad Scala code.
That's true. I think it's very hard to maintain both effective Java and Scala APIs. The decision to let Scala developers wrap the Java API themselves isn't completely nuts, if the alternative is a "standard," but not necessarily fit-for-client-purpose, Scala API. Arguably, the Spark Scala API falls into this latter category.
Apart from what's already aid (the compiler does nothing to cause problems with the style of code shown, assuming no blank lines before the open curly) you should probably take your opinion and put it aside for now in favor of the company/community conventions. If you learn Python, you write code without curlies. If you learn Go, you use tabs instead of spaces. Especially if you are a newbie, it is a waste of your time to try and fight the community conventions. Better to just fall in line, learn everything else there is to learn about the language, and revisit later if this battle is worth fighting.
Maybe I'm begging for downvotes here but the implicit ExecutionContext doesn't feel cumbersome to me. Why not just pass it into a class that holds these methods so you only have to reference it once? The differences here just feels like splitting hairs. Id rather see coders use the native Future types than the Monix ones for the very reason that they are native...
The stuff listed is already pretty straightforward. Will there be levels that cover stuff like delimited continuations, co- and contravariance, shapeless/scalaz etc?
Both those examples are terrible. Get comfortable writing idiomatic scala code first, then complain how it's different from java. 
The goal of the project is to turn into a continuously growing game world and adventure which covers ever more ground and subject matter. Also part of the project structure includes opportunities for users (especially backers) to request features or even design levels.
I'm excited about this, but not sure I like that it is subscription only (other than higher level pledges of course). Still tempted though.
Pretty much no matter where you are in the world except for perhaps Silicon Valley, you have to hire and train. My old team probably increased the number of Scala developers in my city by a third over the last five years by hiring and training. I'll be doing the same for my new team.
If you need bodies, then there are certainly far more Java devs than Scala devs. However `Big data/ML/Streaming` is an area that most average Java devs would be very under-qualified. Most people with either a skillset and/or even an interest are far more likely to have a skillset and/or interest in Scala as well. Conversely, your typical Java-dev is more likely to be used to writing more "enterprise" type software (CRUD, microservices, REST. etc) and less qualified for a `Big data/ML/Streaming` role. In my opinion, the best thing to do would be to hire for both Scala and Java, and have projects or teams that specialize in each. Focus your big-data style concerns in Scala, and more enterprise/web type concerns in Java. Scala can very easily consume Java, so there is little down-side to having both from a Scala perspective. From a Java perspective though, it would likely be painful for your typical Java dev to read or modify Scala code. However, if you structure teams and (micro) services correctly, that shouldn't pose a problem. Lastly, I've done a lot of Java 8 "functional programming" over the last few years, and while it's a step in the right direction, it's FP support is quite minimal when compared to Scala. Lambdas and streams are nice, but it starts to become a void once you try to do anything more advanced. edit: The other downside you may face is many skilled Scala are less likely to want to work in a Java codebase, as it is slow, painful, and tedious to do many things that are "one-liners" in Scala. As a result, your company may lose devs, and find it's more difficult to find `Big data/ML/Streaming` canidates.
Perhaps they're worth it? I'd expect your average Scala dev to be more ambitious, skilled, and productive. Most competent Scala devs were competent Java devs before, but went out of their way to learn Scala (and probably other things as well) versus being satisfied with Java. 
Agreed! We had some phenomenal developers that did really well on that project and they showed that initiative and most of them were well seated in Java (I believe 5 years was roughly the shortest time one of them worked a Java gig)
Indeed, the Scala Specification already has "foolproof support" for the Allman style (although virtually no Scala dev uses it). In [Lexical Syntax, section New Lines](https://www.scala-lang.org/files/archive/spec/2.11/01-lexical-syntax.html#newline-characters), we can read: &gt; The Scala grammar (given in full here) contains productions where optional nl tokens, but not semicolons, are accepted. This has the effect that a newline in one of these positions does not terminate an expression or statement. &gt; These positions can be summarized as follows: &gt; &gt; [...] &gt; &gt; A single new line token is accepted &gt; &gt; * in front of an opening brace ‘{’, if that brace is a legal continuation of the current statement or expression, &gt; * [...] 
o/ Guess that's why I'm subscribed to /r/scala, /r/haskell, /r/kotlin. I'm trying to build somthing using http4s in my free time to get real experience in Scala. I've been an Android dev for over 7 years and want to move to a back end position.
See here: http://www.scala-archive.org/Brace-style-bias-td1988976.html
An example where Allman style has apparently failed: http://www.scala-archive.org/Brace-style-bias-td1988976.html Also, from the style guide: &gt; Technically, Scala’s parser does support GNU-style notation with opening braces on the line following the declaration. However, the parser is not terribly predictable when dealing with this style due to the way in which semi-colon inference is implemented.
It's bad practice to enforce a style on the developer. And if you don't want to have semi-colons, then why not just have the syntax purely indentation-based like Python's?
Or hell, in a pinch get graduates that have studied Computer Science instead of Software Development.
This is a great point. My main languages now are Clojure, Scala, and Haskell. I had been using Scala for about a year and I was very comfortable with it, but after learning Clojure and making it my go-to language the functional part of Scala just started screaming at me, and from there I started to pick up Haskell piece by piece. If you want to make good code and not just code that works you need people who really understand.
This is over 10 years old.
&gt; you can usually get to the level of mediocre programmer in two weeks if you have decent previous experience. In two months nobody will know you were fresh at the start. Because if you're a good dev you will understand the concepts below the language level. I think that depends on the person. I for myself, coming from java, took way longer to really grasp FP and typelevel programming.
If java and its tooling are so superior that they nullify scala's power then what are you doing under r/scala?
&gt; Why not just pass it into a class that holds these methods so you only have to reference it once? Because that means you have to chose it once and forever, beeing unable to use different ones for the different methods. &gt; The differences here just feels like splitting hairs. Take a look and grep a bit for something like `() =&gt; Future[`. I already experienced the need to do this within a code base using `Future`s to make the futures not start when beeing declared, so that I can decide when the sideeffects should be executed. This actually *is* a big deal, especially when we are talking about important side effects where a mistake can end up bad.
With all due respect, I'm not trying to fight the community conventions. I just feel that the Allman style is better for readability. For example, when reading C or C++ code, I can easily identify where a code block starts and where it ends. Whereas in Scala, I have to unnecessarily spend some cognitive resources on figuring out how code blocks are nested inside one another. I just feel that the compiler should be neutral towards brace placement. I think a few changes to the parser can be made to guarantee the correctness of the Allman style in Scala.
I'd be perfectly happy if it wasn't there. I guess I just want to drag young minds into scala, that it's not anything crazy scientific.
&gt; With all due respect, I'm not trying to fight the community conventions. I just feel that the Allman style is better for readability. I wonder, did you actually run into issues with Allman style when trying out some Scala - or is it just that you read it can cause problems and are just being careful and ask in before you jump into code and use this style? If it is the latter, please just start coding with your preferred style. I suppose you won't run into issues at all, or at least very rare. If it's the former, can you provide us specific and running examples? [scalafiddle.io](https://scalafiddle.io/) is very good for that - just give us a minimal example of what went wrong when you used Allman style so that we can better analyze it and help you. I just tried to reproduce the issue in your 10 years old post, but I couldn't.
(I believe what Scala does is not "semicolon inference" in the sense that JavaScript does; rather the grammar treats line endings as valid statement terminators) Nothing is impossible, but I would oppose any effort to do that. Being able to write code without semicolons is really valuable. Languages taking an opinionated position on formatting is a good thing: consistency is more important than one choice or another. (And for what it's worth I think you're wrong on the merits too: vertical space is one of the scarcest resources). We could talk about macros or preprocessors but honestly that would be a bad idea. When in Scala, try doing things the Scala way; if you really care so much about that brace style then I'd urge you to use a language that supports it rather than trying to contort Scala to do this.
If you cannot choose Scala on work then try to look at Kotlin: it is a "better Java" and it is easy to learn for any Java programmer.
And here's a proof it works now: https://scalafiddle.io/sf/3XdQ2i2/1
The post you're citing is (literally) 10 years old. Here is a proof that the problem mentioned there does not exist anymore: https://scalafiddle.io/sf/3XdQ2i2/1
The paper is really short (2 pages), looks like an abstract that was submitted to a conference. I wonder where one could find out more about this, or get actual access to "Scala-of-Coq" to try it out? 
Answering my own question.. according to [Hacker News](https://news.ycombinator.com/item?id=14818290) the source code repo for the tool is https://github.com/JBakouny/Scallina 
If you've used F# you'll probably be able to get most of it. Or if you're willing to take on faith that people find immutability, purity and so on valuable, and go along with "here's what we did to be able to do this thing in an immutable/pure way" rather than getting stuck with "why would you want to do that when it's easy to do that thing in a non-immutable/pure way?"
There was a [discussion to use this new reporter as the default in sbt 1.0](https://contributors.scala-lang.org/t/improving-the-compilation-error-reporting-of-sbt/935), but this unfortunately won't happen. The good news is that nothing keeps anyone from going crazy with this plugin and experimenting to find the best way to display error messages. I strongly believe that being able to quickly parse the overall message is important for productivity. Note that this plugin doesn't care about the "quality" of error messages (for instance, it won't help you figure why an implicit was not found). It will display exactly the same information as the existing reporter. But the information will be easier to read. Once the format of error messages is satisfying, this plugin could serve as inspiration to design better error messages directly in scalac. Suggestions and PRs are more than welcome!
My wife (an experienced Scala dev) went a couple of years ago and she said that a lot of it went over her head, but she did learn something, and she got to mix with people at all kinds of experience levels. It depends on your circumstances of course, but I would try and go anyway. My view is that if you go and come back with having learned at least one thing then it was worth it.
This plugin has improved my experience coding Scala significantly. Thank you!
I get your point RE using a single context but I'm looking at this example and see a trait with a bunch of methods that are likely to be called by the same executor. And then he shows "cleaner" code by importing the global implicit scheduler with Monix? You could do that with an ExecutionContext too. You have to make the Scheduler for the tasks at some point. Fact is that while you've separated the function from the syntax of needing to provide an implicit EC, you will not be able to call it without that context (in the form of the Scheduler). To me, that makes less sense than explicitly showing that this function requires an asynchronous context to run. I'm pretty used to the behavior of Futures and understand what sequential vs parallel execution, or lazy execution, etc... Like using .flatMap to chain futures sequentially vs a for yield. I dont see these as warts or problems. Just different from the way Monix expresses itself. I dunno. I see things like Task.gather and point to Future.sequence. I see things like the Scheduler and point to akka actors. Monix, to me, seems like a library full of slight syntax differences from standard Futures/Actor model. 
Found the wannabe.
 Here's a motivational example. Spot the bug: def monitorError[T](f: =&gt; Future[T]): Future[T] = { f.onComplete { case Success(t) =&gt; () // do nothing case Failure(e) =&gt; reportError(e) } f } monitor(doSomething()) Replace the above implementation with any of the Task implementations out there and you won't have any problems. The biggest problem with Futures are that they are not referentially transparent, and since they are often side-effecting, mishandling them can have dire consequences (like above). I am a big fan of Monix and we use it exclusively at work for our Play application. It's trivial to convert between Task and Future and we do this when 1. converting our Task to Future, which Play's controller accepts 2. Wrapping some library that uses the Future interface. All our interfaces are much cleaner (no EC pollution) and we can refactor our code with confidence. 
ITT: Retarded weirdos imagining themselves superior to those poor "Java programmers". Pathetic, just pathetic, not to mention just sad.
&gt; And if you don't want to have semi-colons, then why not just have the syntax purely indentation-based like Python's? That's under consideration for Scala 3.
It's not so much learning a new language as it is learning its entire ecosystem. Pretty much every programming language comes with its own preferred development environment, build tool, libraries and frameworks, deployment workflow. Obviously going from java to scala is one of the most straightforward transitions you can do in that regard, but you shouldn't assume a professional developer not being willing to move to a new ecosystem is stubborn and/or stuck in his ways and/or lazy. Reaching the same level of efficiency in a new language can take months, if not years in extreme cases. And this gets in the way of solving problem efficiently. If those guys had no interest in scala for their career, it was a perfectly reasonable choice to decline a scala job.
Making sure I understand here. The bug is that the value is returned in the function before the onComplete is completed, yes? Or am I missing something larger? Isn't this precisely what .fallback is made for?
IMO go. Even if you feel like you dont understand, the brain starts rewiring. It's like when you move to country of which's language you don't speak. You still pick it up, and somehow it's then easier to learn. There'll be plenty of people around you thinking and talking Scala dialect. Pick up as much as you can, and don't feel stressed out if you feel overwhelmed. That's your brain tearing old structures down and re-structuring itself. Also with F# and C# experience you're better than a lot of people... source: moved to country that I didn't speak lang well source2: https://www.coursera.org/learn/learning-how-to-learn
You should visit this gitter channel for help: https://gitter.im/ensime/ensime-atom 
&gt; How do you know you've seen worse? As others have said, typical java enterprise apps consists of a bunch of compile-time and annotation magic and the fact that with a significantly weaker typesystem you'll be forced to write less safe code which is worse than writing "clever" code - from your comment history you're really against pure FP so, maybe if monads and scalaz were the worst thing you've seen then you haven't seen enterprise spaghetti. Actually I use scalaz/cats/eff and associated projects (doobie/circe) all of the time at work. &gt; 1-3 seconds is a lot? Due to incremental compilation we only need to compile 1 (or 2 with the test suite) file(s) at a time which is not that much. Its a lot longer for us, always we have modular microservices so incremental compile doesn't always fix us (because we have to publish locally and reload the project) &gt; At this point I'm not sure that you've ever worked with Scala code - how the hell would you get "polynomial complexity" for method completion?! Libraries like scalaz/eff and cats and shapeless really abuse the type system, so autocompletion (both for Intellij and Eclipse) ends up taking ages. It easily takes 5-10 seconds to autocomplete on certain parts of the code, and IDE's like Intellij end up being really laggy/lock up. This is worse then the 150k monolith Java project we have to work which we occasionally have to work with
Actually, I just said the tooling in Java is better, not the language itself. Its not just about tooling, but it doesn't mean we can't call a spade a spade, the tooling for Java is a lot better than Scala, and certain features of the language (such as a deliberately simpler type system) are the reason behind this
&gt; Is that really faster than? I don't think so. Well the thing is, for your typical web app TM which are usually just wrappers over databases, I can actually see how Java can be more appealing (mainly because you business logic isn't that complex) and the complexity in the type system isn't buying you that much Where Scala really starts to shine is very complex backend apps, and areas like big data/streaming/data science Also personally the biggest issue isn't compile times, per say, but even things like tests take a huge amount of time
&gt; Regarding the usage of IDEs, while being easier to make tools for is as such a good thing, it is also not a bad thing if some of those tools are less required or necessary in the first place. Case classes in Scala is one case where I personally much prefer the case class over the generated Java equivalent, since the case class in Scala tends to be much simpler and easier to read and maintain compared to the generated Java code. Honestly case classes is the only exception I can come up with. Other things like refactoring/auto complete/move are done just as often in Scala as in Java &gt; Another argument against tools is that tools are not always widely available or available together. For instance, if you have an online source code repository viewer, you will typically not have the same niceties as you may have in an IDE like folding stuff like boilerplate code together. This means that the code and languages which has less dependence on tools will typically be nicer to view. This is a double edged sword. It can be argued that the ubiquitous of tools is what made Java is so productive, especially in the early days
You've said that the tooling of java make it faster to write java than scala(see: completion/compilation argument). You've also said that scala makes your code less safer in practice(see: "clever" code and similars).
Hello /u/sassy_samurai , Both styles are valid scala synatx, [here is an example writing in the scala REPL](http://i.imgur.com/jy5HULO.png), and [here are the semi colon inference rules](http://i.imgur.com/0xC0lpl.png). 
I don't share your thoughts. Even with a CRUD app (but which application is really only CRUD?) I'd like to use a good library like `doobie`, `slick` or `quill` to work the the database. Now, such libraries would not be possible without the powerful typesystem of Scala. In Java you will most likely fall back to write huge amounts of boilerplate or use annotations/reflection - circumventing the typesystem, which, regarding compiletimes, again leads to: comparing apples with oranges. As for tests: if you take advantage of the typesystem, you will not execute tests in your workflow but just at the very moment you commit your changes into master. Because the types will catch 99% of your error cases anyways, so no need to test after every code change - which is completely different to Java.
Mind you, until the deep linker is released (and even then it will have its own set of problems), then the one advantage of using Java style code is that well, its going to be very fast, especially because it avoids a lot of boxing and unnecessary objects
&gt; I don't share your thoughts. Even with a CRUD app (but which application is really only CRUD?) I'd like to use a good library like doobie, slick or quill to work the the database. Now, such libraries would not be possible without the powerful typesystem of Scala. In Java you will most likely fall back to write huge amounts of boilerplate or use annotations/reflection - circumventing the typesystem, which, regarding compiletimes, again leads to: comparing apples with oranges. Have you seen JOOQ (https://www.jooq.org/) ? We even have scala people using this library as it has much less issues compared to `doobie`/`slick` or `quill` (note that I have used all 3) &gt; As for tests: if you take advantage of the typesystem, you will not execute tests in your workflow but just at the very moment you commit your changes into master. Because the types will catch 99% of your error cases anyways, so no need to test after every code change - which is completely different to Java. Doesn't happen in reality particularly with integration tests for less then trivial services. I often end up running tests on every code change because I need to test that the changes work against some service. (The alternative is to mock the service, however that often ends up being much worse because the mock gets out of sync with the real service and there is usually an impedance mismatch)
&gt; You've said that the tooling of java make it faster to write java than scala(see: completion/compilation argument). You've also said that scala makes your code less safer in practice(see: "clever" code and similars). Language is much more than autocompletion or "non clever code". I am just calling out things as they are, that doesn't mean that Scala doesn't have other advantages (note that 95% of my work is in Scala and so is my open source work)
Java programmers do the same. It's basic human nature to feel superior to those _you_ think are below you. Isn't only sad. It's pretty stupid behaviour.
&gt; Have you seen JOOQ Not in detail yet. However, do you want to tell me that JOOQ gives me typesafety without leaving the language and doing codegeneration, or using reflection and/or adding custom plugins for IDEs? Hard to believe. :) By which I don't want to talk down JOOQ. If it works for you, that's good! But you cannot just apply that to *all* Scala devs (and one might feel like you do that from what you write here). &gt; Doesn't happen in reality particularly with integration tests for less then trivial services. True - but it works for everything else, which in most application is the biggest part of work. But yeah, obviously, when calling new parts of APIs, you will most likely want to run your integration tests or even manually test if it still works. 
&gt; Not in detail yet. However, do you want to tell me that JOOQ gives me typesafety without leaving the language and doing codegeneration, or using reflection and/or adding custom plugins for IDEs? Hard to believe. :) JOOQ reads SQL schema and creates a typesafe API (i.e. it generates Java code) for that specific schema &gt; True - but it works for everything else, which in most application is the biggest part of work. But yeah, obviously, when calling new parts of APIs, you will most likely want to run your integration tests or even manually test if it still works. Sure, but my point is, I don't think you can say that this is universally true. At least in my line of work, I spend a very non negligible amount of time waiting for tests, and the worst overhead is the compile (and load time) of these tests, which is all down to Scala and SBT
Interesting. Looks like a source to source translation. I was expecting a Scala program extraction plugin for Coq. Similar to what is used for Haskell and OCaml. 
&gt; Its a lot longer for us, always we have modular microservices so incremental compile doesn't always fix us (because we have to publish locally and reload the project) Do you constantly make modifications in multiple modules/services? &gt; It easily takes 5-10 seconds to autocomplete on certain parts of the code, and IDE's like Intellij end up being really laggy/lock up. At that pace you'd be better off without an IDE. Btw, what kind of machines do you use there? 
&gt; JOOQ reads SQL schema and creates a typesafe API (i.e. it generates Java code) for that specific schema I guess this is the best choice you have in Java. However I'm not a big fan of this kind of process - it means if I change something in my database, instead of changing the corresponding part in my code, I will need to generate the new schema with some tool and. This is exactly what I meant with "leaving the language". We are not really talking about Java anymore here, so complaints about Java alone cannot apply anymore. &gt; Sure, but my point is, I don't think you can say that this is universally true. Certainly not for every single project. However in my experience you will - in the mean - only need to write like 5% of the tests you need to write in Java. This is not true if we only look at integration tests. Which, however, makes me curious: is it really Scala the language which makes the tests slow? Are crazy macros/types or typelevel programming used for these integrationtests? I suppose it is rather sbt or something else which makes the tests slow. I don't think this should be blamed on Scala the language.
On the topic of replayability; virtually every "real" game played out there has some kind of event-sourcing implementation to allow people to replay it: games like Starcraft, CounterStrike, etc. store replays as a log of input events, and replaying the game just replays the deterministic simulation using those events, resulting in the same thing happening. Empirically, this is the approach the industry has taken. That lets them keep the performance of the everything-mutable approach, while letting you replay the game from the start at varying speeds for people to watch later, and appending to an event log is cheaper than going to immutable data-structures. Downsides compared to immutable history is that event-sourcing it doesn't let you efficiently replay things backwards, and "jump to point in game" may take O(n) computation to replay the log up to that point. But in practice those seem to be acceptable tradeoffs. It may sound hard to take make arbitrary games 100% deterministic, but with a few guidelines (explicit random seed, avoid floating-point numbers, ...) it turns out to be achievable even for huge projects with hundreds of engineer-years of effort.
It's not better for readibility, this is your _opinion_. You're literally going against the grain because you like something. You will not get a single PR approved in a nontrivial community project using this style.
I get that, I truly do, but the poster above described a scenario where training and, presumably, an understanding of the reduced efficiency a change like this can make. Perhaps I am naive since I have not had to work in "big software" as of yet. My experience there are folks in software who are just doing a job.. nothing wrong with that, its a good job. I have a friend who is full on Java and rolls his eyes when I start blathering about my latest language interest... but others are excited about computer science itself and the job is happy residue of that interest. I think for them the change would switch the process from "drag" to "fun". I definitely don't presume they are lazy. I have two kids, coach soccer, and a house to pay for. I get that priorities can vary. My wife would be perfectly happy for me to stay in my current job, 15 minutes from their school. Still, Next year when I finish my thesis and try to break away from the SQL work I have done for the last 20 years I hope I get a job interview like that..
We're open to suggestions - any thoughts about how to bypass the subscription model? We've thought about it quite a bit, and have the current schedule because we thought it was the most sustainable. There is a need for steady revenue in order to continuously produce content. We aim to make a lot of content available at the start, and to progressively release more advanced content. If you knew how much training costs for advanced content - your eyes would pop at how cheap this is going to be :)
One of the goals of ScalaQuest is definitely to make advanced content part of it - in fact the basics only aim to provide a foundation to reach the real meat - the real meat is the stuff that matters in the real world, which we plan to very carefully integrate into gameplay so that people can really learn it. We're motivated to do this precisely because the basics are easily covered everywhere, but when things get complicated, either you have to be smart enough, dedicated enough and have lots of time or opportunity to learn the stuff, or it stays out of reach. A game is different from a traditional course - you can only move forward if you've mastered the challenges being presented (edit: ie, a course can pass you if you get 50% or 60%. Many courses don't grade you or fail you). We're leveraging that dynamic to make sure that players go through the right steps in order to be fully prepared by the time they encounter the advanced stuff. TLDR: we're trying to cure the problem of Scala learners having to go from "I think I get what Options are" to "Now what are Monads?" without really knowing the path to follow in between. 
Yes, you should go. It's a great conference in an amazing location. Talks anywhere can be kind of specialized or esoteric and this is ok, it opens your mind to new ideas. For me the point is to meet people and kind of get connected with things. [Here](https://www.youtube.com/playlist?list=PL9KXdMOfekvYn_x-GEPTp3wlX6iJk_Jmu) are the videos from last year if you want to get a taste.
Will do, thanks!
&gt; I should note for your sake that this style is basically unheard of in Scala This is true, but as a (sadly) former Allman-style user, I wonder if that has more to do with fear of running afoul of semicolon inference than anything. That's certainly why I stopped writing that way when I first started writing Scala a long time ago (2008 maybe)? It sounds like the the situation regarding inference has changed since then, but now my habits have changed. :\
I wonder why they have not used Coq's extraction approach? reply 
Yes, go. There is so much happening in the Scala ecosystem that conferences like this are a great way to have a first contact with new things being made and released. Also meeting people from different backgrounds and that use Scala in different ways is very valuable. 
&gt; I guess this is the best choice you have in Java. However I'm not a big fan of this kind of process - it means if I change something in my database, instead of changing the corresponding part in my code, I will need to generate the new schema with some tool and. This is exactly what I meant with "leaving the language". We are not really talking about Java anymore here, so complaints about Java alone cannot apply anymore. In some ways its a lot better. The issue with libraries like `quill`/`slick` is they try and fit database style relational algebra into a Scala's typesystem with monads (or more accurately map's/flatMaps). The issue is that this only really works for trivial SQL style code. JOOQ is much better in this regard because it actually lets you idiomatically represent much more SQL (it is also a lot more performant in an IDE compared to `slick`, which does a **really** god job in stressing the typesystem) In regards to `doobie`, I don't think its really solving a problem that needs to be solved. Its nice because its better than bare `JDBC`, but libraries like `quill` with inline/infix SQL can do what doobie does but more &gt; Certainly not for every single project. However in my experience you will - in the mean - only need to write like 5% of the tests you need to write in Java. This is not true if we only look at integration tests. Yeah but this is irrelevant if you have to continuously rerun those 5% of tests all the time which takes a very long amount of time &gt; Which, however, makes me curious: is it really Scala the language which makes the tests slow? Are crazy macros/types or typelevel programming used for these integrationtests? I suppose it is rather sbt or something else which makes the tests slow. I don't think this should be blamed on Scala the language. Its a lot of things tbh. SBT is painfully slow because of it using Java 6 synchronhous IO (it doesn't use nio, although this is fixed with SBT 1.x). It also does lot of classloading stuff, which slows things down even more. Scala itself is also to blame, the main popular test libraries (scalatest and specs2) push the type system quite a bit to allow developers to write elegant tests All in all its the end user experience that matters
wtf us that supposed to mean? I am a Scala dev and I didn't have any experience in any of the languages or java for that matter. I came from C/C++. My client though makes a point of hiring people with functional programming exp over java exp and it works out well.
&gt; Do you constantly make modifications in multiple modules/services? Yup? &gt; At that pace you'd be better off without an IDE. Have tried this, overall its not better because I need a lot of support in regards to refactoring (renaming/moving classes and auto complete). Trust me I have tried like 10 different IDE's editors at this point &gt; Btw, what kind of machines do you use there? Its a dual core macbook pro, but we are forced to use these machines at work. I have a quad core macbook pro with maxed out specs at home though, and its not that much better (although still a significant improvement)
&gt; But also try to branch out and get some Haskell or Clojure devs. Or any devs that have functional programming experience. If you hire nothing but java devs you can quickly end up with some bad Scala code. That's what I mean, son. That is just a stupid statement to make.
&gt; The issue with libraries like quill/slick is they try and fit database style relational algebra into a Scala's typesystem with monads (or more accurately map's/flatMaps). The issue is that this only really works for trivial SQL style code. Not sure if I understand, I'm more used to doobie, which also uses the Free monad. Can you maybe give an example? Also, it possible that JOOQ offers a better experience as it extends the Java language with one specific thing: handling of SQL. I do not question that. But this has nothing to do with Java anymore. This is not what the OP was about when he started this discussion. &gt; Yeah but this is irrelevant if you have to continuously rerun those 5% of tests all the time which takes a very long amount of time It is not, unless you work in a project where every code change has to do with change of how you talk to some service. For most projects I worked with, this was not the case. Maybe we work in vastly different businesses? Well, you can always try CBT instead of SBT and utest instead of scalatest or specs2 (which I both don't like). I can agree with you, that both these test "frameworks" make heavy use of implicits and stuff which is not usefull at all. Try out utest. I really liked it and missed almost nothing.
I really cannot understand. Scala is far more shorter and concise than Java, not as Haskell or ML, but it is one of the best languages for JVM with all objects, functions and Java API right at your fingers. Its even easier to learn about the Java API faster using Scala than using Java as it provides a REPL and scripting capabilities and a similar syntax to Java. As someone coming from Haskell and OCaml, I would not feel comfortable writing something in Java.
the problem with this data structure is when you grab that snapshot in thread 2, thread 1 may dump more data before thread 2 then clears whats in the map. So you have to find a way to atomically create the snapshot then clear the map. The TrieMap implementation is lock free so you could wrap it in an atomic reference using Scala STM or a Java AtomicReference but idk it seems a little odd to me to do that. I would, in general, recommend just staying away from using a TrieMap because the semantics of it are pretty muddled. The only in depth explanation comes from this white paper: http://lampwww.epfl.ch/~prokopec/ctries-snapshot.pdf which is still rather convoluted. It seems in your case, however, you do need a consistent iterator to "analyze the data" and the TrieMap has this unlike Java's ConcurrentHashMap so maybe the above would actually work the best for you
The side effect happens twice because there are two occurrences of `f`. Sneaky huh? ;-)
Same here. Wow. I've always wanted this, but never thought it could be done with a plugin. This will make my day easier. Nice.
I'd like to read up about scala frameworks or a blog about scala and the likes but I have no clue what are the popular ones! Can anyone please help? :)
Definitely go. And we we will help you to understand anything you have trouble with, or find someone that can explain things.
My guess is Scala inherits K&amp;R style from Java--Sun used it in all their docs. And no one wants to mess with that convention now, it's just one of the few things almost everyone in the Scala community agrees on. Anyway, the trend now is to use scalafmt to enforce an automated style guide so chances are most projects you contribute to will just reformat your code before accepting it.
I'm synchronizing on the triemap just long enough to grab a snapshot and clear it. Once I release the lock the map will be clear, I'll have a snapshot I can work with, and the first thread can continue dumping data. 
When i work with large XML files i use the JAX parser method, which basicly means that when you are parsing the file, events are triggered. A dom parser parses the entire thing in memory first, and then you do stuff, and that takes up MUCH more memory. This might be the case with JSON to, but im not sure since i havent written a json parser for large json yet. Ive never encountered json so large. You might want to look at a library or a method to do something to the json on parse time, instead of loading the entire thing in memory first.
Of course, it isn't a criticism. As I said, I'm still tempted. I just like paying once for something and leaving it at that. Maybe I'll just have to look at the higher tiers haha
Have you tried increasing the max heap size? By default, its the minimum between 1gb and 25% of available free memory. You can pass -J-Xms1g to make sure there is 1gb available, otherwise you could try even more. I suspect 1gb should be plenty to parse a 100mb file. I wouldn't think your program would make more than a few copies of the data.
Ah, cool. Good example. I have not encountered this much myself I guess... 
Care to explain how that's a bad statement or why that makes me a wannabe?
Breaks the monad laws
Let me strongly suggest [Jawn](https://github.com/non/jawn/tree/v0.11.0), which supports streaming JSON parsing using any of a variety of ASTs. Since you are familiar with lift-json, you may wish to use [json4s](http://json4s.org/)' "native" module, which is literally just lift-json repackaged.
&gt; Replace the above implementation with any of the Task implementations out there and you won't have any problems. You won't have any logging either. I consider that a problem with this code. And if you *do* manage to produce a similar example that produces the logging and still has the double side effects issue, then with Monix you would also have the double side effects.
Yes you're right the real issue is with the double `f` use and call-by-name parameter. Even if `f` is just `=&gt; Int` you'll still be evaluating the expression twice. As for double side effect, as long as the construction of the Task isn't side-effecting you won't have 2 side effect - you'll just be constructing a Task needlessly (similar to the case when `f` is `=&gt; Int`). What I failed to express is that at use-site this: ``` monitor(doSomething()) ``` has different semantics from ``` val runningFuture = doSomething() monitor(runningFuture) ``` if `monitor` happens to want to perform some side effect before executing `f`. With Task you don't have this issue because they are referentially transparent.
Yes, I pass a function named log from String to Unit. The log level is the one I decide to pass to it. The real issue is that logging libraries end up fighting each other, as configuration libraries do.
Using /u/valenterry's logging below works well. It forces the user to implement the logging interface from your library. By passing a logging function, you can use whatever library you want to log and configure it in the logging library's own style. I use typesafe LazyLogging, and logback configuration at work in the applications I work on. Passing logging looks like `myFunc (logger.info(_))`. 
QED.
And backed!
Seems like streaming JSON is the best approach, as /u/paultypes suggested here https://www.reddit.com/r/scala/comments/6oqrta/parsing_json_problem_with_files_greater_than_100mb/dkjv3wl/ you can use jawn directly. Another approach (which is what we use at work) is https://github.com/knutwalker/akka-stream-json. It also uses jawn but provides marshallers/unmarshalllers/flow's to akka-stream, and with akka-stream its really easy to combine the flows to read from a file (for example)
I want to learn Scala to play with the Apache Spark framework.
so what kind of style code you want to see in Spark? Haskell style? lol
If you hire Haskell or Clojure dev, you would end up with some bad Scala code as well.
You don't need learn scala to useSpark. Their new java bindings are same good as scala one. 
But I think Java code will be "clumsy" for this kind of task, right?
There's also pyspark. 
Let's assume you hire a Clojure dev or two. They end up writing some code in Clojure because ultimately they are Clojure devs... Who is going to maintain this code if they end up leaving at some point. I'm in favour of diversification and new technologies but from a business perspective having such exotic tech can be a massive risk. To a non-Clojure/Haskell person the code is absolutely unreadable. Imagine some ops guy having to fix a bug in the middle of the night. With Java that's possible, Scala - still pretty readable if you don't go crazy, Clojure/Haskell - not sure about that.
This is a very stupid reason, hire people and teach them scala that's as easy as it gets. Almost everybody knows Java and Javascript and they have some similarities with Scala. Java of course more, but JS has also HOF, lambdas, closures, scopes, etc... I think the majority of programmers would be productive withing a 2 to 3 weeks range.
Yes very much so. And you'll be a lot more hard pressed to find tutorials etc.
Nice! On the Scalaz/Cats/fs2/scalaz-stream side, there's [jawn-fs2](https://github.com/rossabaker/jawn-fs2), too.
Pyspark is always behind the scala api in terms of features. They've recently improved it a lot but new features typically are made available with a lag time in PySpark. 
I've only ever used Spark with Scala but my impression is that most users are on Python, no?
The core is all scala, so features come to scala first. I dunno about actual user volume, but personally I find using scala for spark much more enjoyable than pyspark.
&gt; And you'll be a lot more hard pressed to find tutorials etc. This was my experience when trying to learn Akka with Java. Most of the tutorials were devoted to Scala.
It's webscale!
Wait, did you think I was saying hire Clojure or Haskell devs and *not teach them Scala?* 
The proposal [was not enthusiastically received](https://www.reddit.com/r/scala/comments/6cezdz/new_dotty_proposal_consider_syntax_with/) around here or in the github issue; I'd be surprised if it materializes. I'd hate to no longer be able to copy and paste code when refactoring. It would also come with more mental overhead of figuring out which scope a block of code is in.
The akka streams API is well-documented for streaming IO--for when you can't or don't want to keep a whole view of something in memory. (You might instead be interested in fs2, which offers similar functionality but takes a different approach.) The Scala standard library doesn't need to include a whole lot of IO operations, because the java APIs are sufficient. It helps to be familiar with scala.io.Source for slurping URLs and files when scripting. In general, `java.nio.*` is quite useful for file IO and character set encoding and decoding, as are the input streams and encoders and decoders that are available in the java standard library. But what you will find useful really depends on what you want to do.
I was wondering what this offers over decoding line-delimited JSON dictionaries, then I noticed that the example is parsing a giant non-delimited JSON array without loading all of it in memory. That's pretty cool.
You realize many of us were Java programmers before we started programming in Scala, and that many of us still write Java, right? We're familiar with the capabilities and limitations of both. I can do strictly more in Scala than I can do in Java. The compiler can offer more assistance in writing reasonable programs, it's far easier to write composable code, and less code is required to express the same thing. The shared sentiment is that tooling is important.
Much of the community has [an aversion to frameworks](http://timperrett.com/2016/11/12/frameworks-are-fundimentally-broken/); [they don't compose](http://tomasp.net/blog/2015/library-frameworks/). I'd instead recommend reading about functional programming in Scala if that seems interesting, starting with the book with that title.
Is there an out-of-the-box way to rate limit effects with fs2? As in Guava's [`RateLimiter`](https://google.github.io/guava/releases/22.0/api/docs/index.html?com/google/common/util/concurrent/RateLimiter.html) and akka streams's [`groupedWithin`](http://blog.colinbreck.com/patterns-for-streaming-measurement-data-with-akka-streams/#ratelimitingrequests) combinator or its [throttling flow](https://stackoverflow.com/a/36221097).
The biggest framework is Play Framework. There's also Udash. 
When comparing Future with Monix's Task, the comparison is apples vs oranges. The former is strict, the later is lazy. Fact of the matter is you can't use Future if you want to describe pure (FP) code because Future will not suspend side effects. This is also why Future requires an ExecutionContext on each operation, because it sends side effecting runnables in that EC on each map or flatMap. Whereas Task does not require that EC before runAsync happens, which can happen only once per program actually, at the "end of the world" so to speak. Either way, the two concepts are complementary actually. A point I'm making in my ScalaDays presentation if interested: https://www.youtube.com/watch?v=wi97X8_JQUk As far as Future being standard is concerned, I sort of agree, except for the fact that the standard library isn't perfect and requiring only "standard" dependencies in your project will hurt innovation and the ecosystem as a whole. Seriously, we laugh about the Node / JS ecosystem, but those people understand modularity and composability, whereas we don't, the majority clinging on 2 or 3 libraries that have corporate sponsorship. And for a relatively small ecosystem, this isn't healthy at all.
The flip side is that you can write expressive code in Scala that is indecipherable to any other team. Java really doesn't have that problem. Every ecosystem has its pros and cons. It's embarrassing to put down something down just because something new is better is some aspects. That's what I'm referring to - Scala has more expressivity while being slower to compile. Java has better consistency while being more boring and verbose. No need to act like those Rust fanatics trashing C++ at every turn.
In terms of frameworks that provides templating etc, Play Framework is the only one I'm aware of Other popular libraries for serving HTTP: * Akka-http * Http4s (leans heavily towards purely FP) * Finch / Finagle Others that I know of but haven't used myself: * Vert.x * Lift My recommendation would be Akka-http or Play (which as of 2.6 is based off Akka-Http for serving traffic) if you're starting out as they will be the least confusing.
(afaik) it's preffered to use . inbetween calls, and () if it's single line call. You can also split it to multiple lines then: myList.filter(predicate1) .filter(predicate2) .map { x =&gt; .... } .map(mappingFunc2) 
Use dots: val myval = myList .filter1 { predicate1 } .filter2 { predicte2} .map { mappingFunction() } Also consider to use round brackets and save curly brackets for partial functions (with `case`s inside) or multi-line-stuff: val myval = myList .filter1( predicate1 ) .filter2( predicte2 ) .map( mappingFunction ) 
Thanks very much!!
Yup, the point is you stream the JSON as you read it into memory (often with byte buffers), so you don't load the entire JSON into memory at once Really matters when you have to parse giant JSON blobs
[removed]
Excuse me...How does java not have that problem? java totally has the same problem! This seems to be one of lies that is repeated over and over until people start to believe it... Did you never see bloated shitty JavaEE or Spring app that you couldn't for sake of anything decypher? That had issue tracker long than waiting for payday? I have... plenty. Moreso than in Scala.
Actually, no. The java-related part is quite clean ( ofc it is up to what you will end with) contrary to downvoters opinion. 
Might be better to ask here : https://gitter.im/functional-streams-for-scala/fs2 or create issue in fs2 project, they are very responsive
you're welcome, feel free to ask if you meet any problems on weekly ask anything thread.
Thanks will look into this yeah would love to use something similiar to lift would save me a lot of time . as others have suggested other libraries but it takes me quite some time to understand libraries in scala.
Wow this looks really cool will have a look at this too. My files arent too big like 100mb maybe to 300 or 400 mb. But if I have to do any real time ingestions will look into this.
Nonsense. Verbose and bloated, sure. A mishmash of programming paradigms leading to inscrutable code? Nope. Unless you really really tried to write it that way. Martin Odersky himself admitted that Scala does make it trivial to write code in extremely varied styles such that different teams might have difficulties communicating with one another. 
Just to be clear: in this case, [Jawn](https://mvnrepository.com/artifact/org.spire-math/jawn-parser_2.11) would do the [_parsing_](https://github.com/non/jawn#parsing), but [what it parses _into_](https://mvnrepository.com/artifact/org.spire-math/jawn-json4s_2.11) comes from some other library, in this case [json4s-native](https://mvnrepository.com/artifact/org.json4s/json4s-native_2.11): So the native package in this library is in fact verbatim lift-json in a different package name, this means that your import statements will change if you use this library. import org.json4s._ import org.json4s.native.JsonMethods._ After that everything works exactly the same as it would with lift-json. So you get the best of both worlds: streaming parsing from Jawn, and a familiar API for the resulting JSON from json4s-native (i.e. repackaged lift-json).
Yea, that's your opinion. I worked with java codebases that were extremely complicated. I'm not saying you can't do complicated codebase with scala. But it's definitely not problem of only scala. Go through kubernetes sources of GO lang, which is supposed to be super trivial, and you'll see what I'm talking about. Or speak to ex-kubernetes developers.
&gt; Yea, that's your opinion Likewise, my friend. Except that even the Father of Scala agreed that the inclusion of too many programming styles in Scala has led to it being roughly the equivalent of C++ in the JVM world. 
As a side note, you can combine `filter` followed by `map` into a single call to `collect`: val myVal = myList.collect { case x if predicate1 &amp;&amp; predicate 2 =&gt; mappingFunction(x) }
I haven't tried compiling this, but the idea should work: val feeder = Iterator.continually(Map("user" -&gt; (RequestBuilder.createRandomRequest()))) def createRespondent = http("createuser") .post("form/submit") .body(StringBody("${user}").asJSON .check(status.is(200)) scenario("my load test") .feed(feeder) .exec(createRespondent) You basically need to feed your feeder into your scenario then you can reference the feeder map using "${&lt;key&gt;}" There's some documentation at the top here: http://gatling.io/docs/current/session/feeder/
Ah, I hadn't had a chance to look into it in much depth yet. Still though, sounds pretty cool.
and that him being in academia and language designer, I doubt he's seen what wilderness people can do with either Java or Scala in Enterprise :P
It works. thanks. Am I the only one who feels that the DSLs in the Scala world are really exotic and sometimes a person has to be some kind of magician to figure them out. Or maybe I am just plain stupid.
&gt;Am I the only one Probably not
You can't really figure out DSLs - you're supposed to read the docs.
You should not worry so much about formatting. Use [scalafmt](http://scalameta.org/scalafmt/) and let it do the job for you.
IntelliJ is really useful for beginners. It reminds you of things like this.
For some reason I never realized that your first example is valid Scala syntax. I remember trying to do something similar before, but IntelliJ would always complain. It looks like that would look better than my alternative of val myval = myList .filter1({ line1 line2 }) .filter2({ // ... })
Having dabbled in F# a bit I can see where you're coming from. I was thinking of learning a new language in general, assuming you somewhat know about similar concepts. FP is a bit of a jump, but isn't it [much simpler](https://twitter.com/abt_programming/status/536545215310757888)?
Do you have any coupon/discount for trying out your courses?
I've worked with quite a few different DSLs (Slick, Spray, Akka HTTP, Akka Test Kit, Specs2) in Scala and I have to say that the worst by far is Gatling. Gatling is both poorly documented as well as poorly designed. Intellij still can't understand even a mildly complex Gatling script and compared to what Intellij can parse this speaks more to how poorly designed Gatling is rather how complex Scala can get. 
I would expand that and say that it would be awesome to see a JVM to JavaScript roundup. ClojureScript, Scala.js, Kotlin, Grooscript (Groovy), and whatever else folks find interesting. 
I also remember going from a Python and C background to Haskell and being confused for months, it can take a while before it _clicks_, but it's worth it in my opinion (some tips: use `:t` and `:i` in ghci to get type signatures and type/variable information respectively). Scala could indeed be a good language to learn if you're coming from a Python background, though a Java one would be more ideal in my experience. I'm not the biggest fan of Scala's syntax, but tastes vary, so I don't see why you shouldn't at least give it a try.
Ok, this may seem a little roundabout, but, Haskell would force you to think funtionally more than Scala because it's more restricted as a language. Scala will let you program how you've always programmed for the most part, just with different syntax. Haskell will force you to think differently about programming. So if you're going just for "expanding your mind", give Haskell a try first.
Scala's a fine language. It tries to pull in both OO and functional aspects. That leads to a variety of valid approaches to any problem, which can also be a bit of an Achilles heel. (I'm constantly going back and forth over whether I want to use mutable data structures.) In some ways, Scala feels like it's following in C++'s footsteps. It's trying to be multi-paradigm, and as a result the language feels "big". Also, it seems to attract people who love to use its features in surprising, powerful, and sometimes arcane ways. Coming from Python, there are a number of languages that would show you something "different". Something that would be immediately different is that Scala would force you to deal with static types. Due to type inference, Scala actually hides a lot of that... though type inference doesn't always work perfectly, and it can be confusing when it infers the wrong type. Plenty of languages use static typing: Scala, Java, C#, C++, Haskell. Another difference would be Scala's adoption of functional elements. I guess Python has blocks, which let you program with higher-order functions. If you want to go all-in with the functional approach, Haskell would be where it's at. But for a beginner, it might make sense to start with a Lisp, like Scheme or Racket or Clojure. You'll hate the Lisp syntax until it breaks your brain, then you'll love it. Or, you could go with something completely different. I'd bet that most developers have never worked with a logic programming language. And that's not surprising; it's a niche paradigm that isn't well-suited to many kinds of problems. But there's at least some relationship between logic programming and relational databases (both deal with relations, though logic programming languages include a way to synthesize new values for the relation, whereas SQL requires all facts to be known up-front). Don't forget that it's not all about the languages. Things like Software Transactional Memory or Actor-based concurrency change the way that you build software, and have implementations in several languages (though Clojure was more or less designed for STM, and Erlang is one of the big actor-based languages). --- All that was not meant to steer you away from Scala, just to provide you with some other languages to consider. As I said above, Scala is a fine language. It's what I use at home.
Do you have examples of where this has bitten you?
I agree with valenterry user, notice that to get it working on scala REPL, you have to enable paste mode, type :paste, write your code and press ctrl+D.
Definitely! I've spent the last 18 months on a Scala team that make heavy use of cats. I found it really useful to work through "Learn You a Haskell" to understand some of the concepts and then go back and apply them in Scala. 
I've seen a few Python-only software engineer getting familiar with Scala more easily than with Java. While the JVM is a beast, the Scala language has superficially similar language features like tuples and top-level functions starting with `def`, and less amount of type declaration. Especially when compared to Haskell or Rust, I think Scala will provide a good middleground to proceed to the world of functional programming as well as to the world of low-level programming, to someone coming from a Python background. If "thinking of programming in different ways" is your concern, the genetic continuity lies roughly through Python ~ Scala ~ Java ~ C++ ~ Rust, while Haskell is where you can stem from Scala's FP side -- which is totally complementary; there are whole bunch of libraries for FP nerds that are seldom understood or used by people in the industry.
Yes.
Yes and no, I don't think this is a good example for that. val random = Random.nextInt println(random) println(random) This is basicly the same problem you had but imo it's pretty clear that it will print the same value 2 times
A beginner should not be using tools as a crutch to cover their lack of knowledge 
Scala is a great practical language but it lets you program the way you're used to. So if it's about learning it might be better to learn a more strictly ML-ey language first, e.g. Standard ML or F#.
Could add something like TeaVM to that.
Well, you're posting on /r/scala, so you should expect a biased answer. You might want to also ask on /r/Kotlin. First, you have all the typical comparisons between Scala and Kotlin, independently of whether they compile to JS. In addition, here are some things that I think Scala.js did better than Kotlin on JS: * It's the same language as Scala. For example, `Int`s behave the same in Scala/JVM and Scala.js. Not so in Kotlin vs Koltin/JS. * Eco-system, the Scala part: most popular Scala libraries cross-compiled and are published for Scala.js. * Eco-system, the JavaScript part: there are quite a number of facade types for JavaScript libraries published for Scala.js. It's unclear whether such things even exist for Kotlin/JS. * Documentation: honestly, it's hard to find anything useful about Kotlin/JS in terms of documentation. I'm not saying that Scala.js' documentation is stellar, but at least it exists and is quite complete. * DCE+Optimizer: Scala.js has semantics that have been carefully designed so that optimizations are possible and applied: in particular, it will remove the parts of the codebase that are not actually used. * Community: the Scala.js community is amazing: friendly, helping, writing tons of really good libraries, tutorials, etc. Kotlin compiling to JS was a buzz generator when it started, but the JavaScript support has never been on par with the JVM support. And basically the community did not take off, which has left the Kotlin/JS eco-system in a state that is barely usable. With the absolute focus of Kotlin on Android now, this is not going to change any time soon. There are other languages compiling to the JVM and JS that are very good, used, and with a good eco-system and community. First of them being ClojureScript. TBH I don't think Kotlin/JS is part of that set. The only reason you might prefer Kotlin/JS to Scala.js is if you have a pretty strong preference for Kotlin itself versus Scala (which is definitely possible). I told you you'd get a biased answer.
https://www.coursera.org/specializations/scala May be of help. Also, https://booksites.artima.com/programming_in_scala_3ed if she has some spare time. It is fine to skip some topics like new collection design for a while and rush through the core concepts. 
Thanks but I was looking for something more in a sense of summary the most interesting concept with the syntax and practices in 2017. http://danielwestheide.com/scala/neophytes.html is a bit dated... isn't it?
But isn't Scala "a Java one"? 
Oh man apparently I really misphrased my question, because I wanted to know popular scala blogs (where they perhaps post about frameworks, cool tricks, scala updates etc) instead of just using a popular framework.. hope its clear now! 
It is not outdated in my opinion. Daniel propose great insight, but the topics are a bit too far away from starting developer. I'd say it's better to stick with proposed video course course and get back to Daniel when you need more. 
*A key feature of Scala.js is its interoperability with JavaScript code, which far exceeds that of many other languages targeting JavaScript. Except of course for languages that translate almost literally to JavaScript (e.g., TypeScript and CoffeeScript).* *See for yourself: on the right, Scala.js natively talks to the DOM API, and we can hardly notice the border between the two languages.* http://www.scala-js.org/doc/interoperability/
Type classes being a first class feature in Haskell makes things a lot easier. As much as i love Scala, implicits are not the easiest feature of the language.
The Neophyte's Guide to Scala may be a few years old, but that doesn't mean it's outdated. `Either` is now right-biased and the `Future`-API has more features now but that's about it.
Elm may be a good place to start. First, the language is small. You can learn the syntax and most of the features in just a few-hours-long tutorial. Elm is very opinionated: it is pure, it tries hard to avoid runtime exceptions and it strongly encourages you to follow a simple yet powerfull architecture. I think the language may be too "simple" for large projects (i miss HKT, rank2types, type classes, etc) but it is a very good place to get introduced to purity, algebraic data types, pattern matching, functional reactive programming, monads, etc. Scala is very nice language, v3 will make it even better but it is a difficult language.
It is not outdated at all and one of my favourite source for learning Scala! It required a little bit of knowledge about Scala in before, but else it is perfectly fine to start with!
Newline after the `filter` should work: scala&gt; :paste // Entering paste mode (ctrl-D to finish) List(1, 2) filter { _ &lt; 4 } filter { _ &lt; 5 } map { _ + 3 } // Exiting paste mode, now interpreting. res1: List[Int] = List(4, 5)
Although I have no experience with ScalaJS/KotlinJs, I did try Purescript and Typescript so I'll chime in anyway: Generally speaking, when transpiling a language to JS, you need to consider the benefits of using a different language(your experience with it, its power/expressiveness, ecosystem, etc..) versus the drawbacks (difficulty in integrating with JS libraries and build tools, performance, contrasting paradigms) Kotlin is a nice language and all, but I don't think it brings enough benefits. It was designed first and foremost to help accommodate Java developers, with class based OOP and nominal typing, whereas JS uses prototype OOP with emphasis on duck typing, and so you may have difficulties integrating JS libraries with Kotlin and vice-versa. Similar arguments can be made for Scala, but Scala is more powerful than Kotlin, with a better typesystem, great support for functional programming and generic programming, plus there's already a fair amount of Scala libraries with ScalaJS support. 
I'll post it as you suggested. Lack of documentation and practical examples may be a prove of what you said. JavaScript target was just a buzz generator
&gt; Am I the only one who feels that the DSLs in the Scala world are really exotic and sometimes a person has to be some kind of magician to figure them out. I don't know much about gatling, but I don't know what about the sample you've given above makes a DSL. It seems to just be chained function application. Perhaps there's some sort of magnet or something being used to make it seem like a DSL? EDIT: Took a quick look at http and scenario on github. No magnet, no implicits conversions. They just seem to be to java style builders with pretty straightforward arguments. Now I don't understand the DSL criticism.
I actually think that /u/sjrd post is quite objective, but I think the biggest pro really is the javascript -&gt; scala.js compatibility (that is, how easy it is for scala.js to interface with vanilla JS code). This is actually a very big factor, because the Javascript ecosystem is huge and there are a lot of things that you simply have to interface with in Javascript (this is a reality). i.e. if you want to use Google Maps api, you are going to use a facade to their provided javascript library. For this reason, the only serious language -&gt; javascript compilers that I can recommend right now are Scala.js and Clojurescript. The other language compilers generally need too much ceremony (i.e. require a lot of manual wrapping) or they simply don't support (while it is recommended to wrap the libraries in Scala.js, its just to provide typesafety, you aren't actually forced to do so) You can also add Typescript to that list, however Typescript was a language specifically designed to make Javascript development easier (i.e. it wasn't designed to be a general purpose language). Typescript is basically the Kotlin of Javascript.
It's my favorite as well, I just wanted to be cautious with proposing it to someone if it's a bit outdated. But sounds like it's still a great option. Thanks!
Thanks again, much appreciated! Such a helping community, really glad to be here.
You could always check out [learnxinyminutes](https://learnxinyminutes.com/docs/scala/), nice summary of syntax and such. 
Young mind here, please enlighten me.
I use http://scala-ide.org. Kinda hard to miss as it's the top hit for scala ide. 
I wasn't asking for a Scala IDE by itself but rather an implementation for the applications mentioned above.
I'm pretty sure that Scala IDE == Eclipse.
Well, my browser crashed when trying to open that, I couldn't check. Thanks, though
X-Post referenced from [/r/programming](http://np.reddit.com/r/programming) by /u/tomer-ben-david [Functional Programming In Scala For the Working Class Java Programmer Part 1](http://np.reddit.com/r/programming/comments/6paeh7/functional_programming_in_scala_for_the_working/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
I am interested in reading books on advanced Scala. Any recommendations? I am particularly interested in learning advanced typing (path-dependent types etc.) as well as macros and when is best to apply them. I am hoping someone would know of a book or in-depth tutorial on these topics. The few Scala books I have read in the past didn't get very far into advanced subjects. Thanks. Also could use some help with the [Sbtix](https://github.com/teozkr/Sbtix) project I was working on at the end of last year. Sbtix allows SBT projects to be build using the [NixOs](https://nixos.org/) build tools which allow for pure (side-effect free) build environments, all files downloaded are known and specified with a url and sha256. We use an SBT plugin to generate the list of files required for the build, so we are generating something similar to the lock files that you would see in other build environments. Our SBT plugin hasn't been able to identify every one of the build dependencies, so we must supply a handful of the dependencies (url and sha) manually. Those manual dependencies are listed [here](https://github.com/teozkr/Sbtix/blob/master/manual-repo.nix) and we'd be grateful if anyone can tell us how to identify these last dependencies automatically from an SBT plugin. These last few are dependencies of SBT, it's compilerInterface/compilerBridge or the SBT launcher. It has been months since I worked on the problem and I had given up on it, but I'd really like to see it solved at some point. I didn't really try asking for help from the community though.
Scala IDE is a set of Eclipse plugins available at [scala-ide.org](http://scala-ide.org/). * If you don't have Eclipse, use the "Download" button to get a copy of Eclipse with the Scala IDE plugins preinstalled. * If you already have Eclipse, use the update sites link to get instructions on adding the plugins to it. I used Scala IDE for the Coursera specialization and it worked well. The worksheet feature was especially helpful in learning Scala. 
I think he means Java background
Couple of points about that. If you're looking for a plugin that helps you work out why an implicit is not found, [scala-clippy](https://github.com/softwaremill/scala-clippy) might be it - specifically, Shane Delmore's [imclipitly](https://github.com/ShaneDelmore/imclipitly) superset. Second: while I'm all for better error reporting, and I think this plugin is a step in the right direction, I believe it's important to bring up the fact that it breaks basically any tool that uses scalac output as an API, sometimes in a way that is really hard to work around. Take, for example, the way that it cleverly shortens non-ambiguous paths for better readability - it makes it a nightmare for tools that extract the path from error messages to link to the right file &amp; line, for example. 
Last time I tried Netbeans / eclipse was bad experience for me, intellj is quite mature. 
I used to use eclipse for Java and switched to intellij for Scala. It's better
I keep hearing this. I use Eclipse. Can you tell me specific differences that make intellij better for you?
This might be controversial but here's my 2c. scala-ide (eclipse) suffers from woeful under investment and is miles behind IntelliJ. As an example it didn't support 2.12 until fully 6 months after 2.12.0. Lightbend have lost interest in it and done the minimum for a long time. In fairness the eclipse platform is horrendously complex to target and the architectural decision to layer everything on top of java tooling made very early on has probably proved expensive. I expect this to be dumped formally when they can get a compiler to emit Microsoft's language-server-protocol (probably whatever comes after 2.13). It's a shame, if they'd put time into the right things (basically m2eclipse like support for sbt) instead of messing around with gimmicks like worksheets and activator it would have been the defacto standard, might have helped make sbt bearable for beginners and possibly concentrated a lot of community effort which is currently spread out over the ecosystem but heyho. Nobody uses netbeans. Story of its life really. You have two real choices for serious use: * intelliJ community, which still improves a bit with every release. * some text editor (sublime, vscode, emacs etc) with either sbt in watch mode or you can spend a while trying to get an ensime plugin to work. I went with IntelliJ because I can't quite bring myself to give up 3d games and put linux on my home pc, and when I last tried (over a year ago) getting ensime to work reliably in a windows environment was beyond my patience. IntelliJ is a decent experience but the integration with SBT still has a few rough edges - it is getting better though. 
Yes I agree with you. You want to do something simple and you know Scala, but somebody decided to make it "easier" so now you have to learn the DSL as well as how the DSL is mapped to the underlying Scala. I think DSL's have their place but they must be exceptionally documented and simple to use in order to be useful to anyone but the author. 
The autocompletion is quite obvious one, intellj developers make it so powerful.
Help me, with specifics. Eclipse has autocomplete - is intellij's better? If so, how?
Yes, the autocompletion is much better comparing to eclipse, there's a good answer on quora related to the question https://www.quora.com/How-do-Eclipse-and-IntelliJ-compare-for-Scala-development/answer/Denis-Kalinin?share=a357ecc7&amp;srid=CttK. Some answers mentioned that scale IDE team is even moving to vsc https://www.reddit.com/r/scala/comments/52w7n8/future_of_scala_ide_would_you_like_to_see_an/
That answer echoes your response with no specifics.
I hate IntelliJ but I had to start using it recently because ScalaIDE doesn't work with Scala 2.12. It just pops up some error every time it tries to compile. IntelliJ is irritating in pretty much every way I can think of but if you change the key mappings to the eclipse settings and install a new theme it's kind of usable. 
Just use the presentation compiler for Eclipse and build via sbt in a separate terminal. Basically SBT is the source of truth. If you generate your projects with `sbt-eclipse` plugin things should just work.
To be clear, it's not that I dislike IntelliJ or think it is a bad product. I've got over a decade of experience with Eclipse and its predecessor WSAD/RAD. I've got a lot of muscle memory and tool knowledge baked in. As far as the key settings go, I use emacs bindings for editor commands. I've been looking for compelling reasons to switch for over 5 years - people have been telling me that IntelliJ is amazing for at least that long. I haven't found a compelling reason to switch.
I like sbt even less than Intellij. I use maven which works fine with Scala and used to work fine with Scala IDE. 
Sounds like it was useful for weeding out bad candidates too.
I think gatling needs better docs, if you dig into the source code for some time you can make more sense of the DSL but without that it's very difficult to reckon with errors. And it is written in idiomatic scala so it's not very accessible to newbies in scala unfortunately.
Anybody else find the odersky coursera a little obscure? 
Make a java annotation and use that instead of string named bindings
The best book on the language itself is Dean Wampler's [*Programming Scala (2nd. Ed)*](http://shop.oreilly.com/product/0636920033073.do) from O'Reilly.
So I did post the question and the answers fell terribly short compared to what Scala advocates said. Check it out here: comments/6p9876/scalajs_vs_kotlin_compiled_to_javascript
Yup, I learned Scala with this book. It's amazing.
Yes. Python was my favorite language with a REPL before I learned Scala. The Java ecosystem has a saner approach to dependency management (in that dependencies are more sensibly packaged and aren't installed as a bunch of files at the system level--obviating even the need for virtual envs), the language itself is more of a library language (in the sense of having few, orthogonal features at the language level, with a lot of general functionality provided by libraries), *and* it has a REPL and an an expressive static type system.
&gt; I went with IntelliJ because I can't quite bring myself to give up 3d games and put linux on my home pc, No Steam on Linux?
Scala IDE (Eclipse with Scala) is inferior to IntelliJ community edition with the Scala plugin. The Scala plugin is maintained by the makers of IntellJ (JetBrains) and they continually do a great job. IntelliJ is also better for Java IMO. http://www.jetbrains.com/idea/
Yeah but I'd lose access to about 3/4 of my library. It's getting better though. 
Interesting... I'm not hunting linux-only games yet ~90% of my library is compatible with steamos. The rest runs well on wine(skyrim, dishonored etc.).
Good to know Wine is worth trying - thanks.
does anyone have experience with https://github.com/ghcjs/ghcjs (haskell -&gt; js) ?
Wine is definitely improving on the dx12 area(witcher3 support especially) and if companies will keep choosing vulkan then we'll have even better support(without denuvo, of course).
&gt; I use maven While it works for Scala IDE itself (Maven based), SBT is the primary build tool in the Scala ecosystem, so YMMV when working outside of the box.
&gt; It's the same language as Scala. For example, Ints behave the same in Scala/JVM and Scala.js. Not so in Kotlin vs Koltin/JS. Kotlin/JS has *mostly* the same semantics as Kotlin/JVM, except for several rare corner cases. For example, with code like this: val x: Any = 23.0 println(x is Int) Except for that Int behaves exactly like in JVM. &gt; Documentation: honestly, it's hard to find anything useful about Kotlin/JS in terms of documentation. I'm not saying that Scala.js' documentation is stellar, but at least it exists and is quite complete. There *is* documentation on official Kotlin site, both on Kotlin/JS stdlib and tooling support, as well as some tutorials. What else is needed? &gt; DCE+Optimizer: Scala.js has semantics that have been carefully designed so that optimizations are possible and applied: in particular, it will remove the parts of the codebase that are not actually used. A DCE tool for Kotlin/JS will be released in few weeks, for now [you can try it in EAP](https://discuss.kotlinlang.org/t/a-new-dead-code-elimination-tool-for-js/3777). &gt; Kotlin compiling to JS was a buzz generator when it started, but the JavaScript support has never been on par with the JVM support. And basically the community did not take off, which has left the Kotlin/JS eco-system in a state that is barely usable. It was true a year ago, but since Kotlin version 1.1, the JS backend is no more "experimental", and currently it supports all features available on JVM BE. As new language features introduced, they appear both in JS and JVM back-ends. &gt; With the absolute focus of Kotlin on Android now, this is not going to change any time soon. Kotlin is not focused on Android development at all. It's true that Kotlin is quite popular among Android developers, but it's not due to technical or marketing focus from JB. And don't underestimate Kotlin popularity among non-Android developers! In fact, Kotlin is almost as popular among server-side Java developers as among Android developers. Android community just generates more buzz around Kotlin.
I think main differences which you can observe are differences in the languages and differences in tooling (compiler, IDE etc). So: If you like Kotlin or already have code in Kotlin try Kotlin/JS and feel free to join our community in the slack (http://kotl.in/slack) If you like Scala or already have code in Scala try Scala.js Kotlin/JS * has good interoperability with JS (https://kotlinlang.org/docs/reference/js-interop.html) * has excellent support in IDE (because we think about tooling when developing any new feature) * very soon will have the excellent incremental compilation (https://medium.com/keepsafe-engineering/kotlin-vs-java-compilation-speed-e6c174b39b5d)
&gt; Except for that Int behaves exactly like in JVM. It seems you're right. Until not long ago, `1000000000 + 2000000000` would give different results on Kotlin JVM/JS. I cannot reproduce it anymore with version 1.1.3 online, though, so apparently this has been fixed. Good. &gt; There is documentation on official Kotlin site, both on Kotlin/JS stdlib and tooling support, as well as some tutorials. What else is needed? I have checked again, and it seems to have improved since the last time I checked (around the 1.1 announcement IIRC). One important thing that is still unclear to me is how one would go about writing a cross-compiling Kotlin JVM/JS library. Without that being trivial to do, the larger Kotlin eco-system will never cross-compile its libraries, which means that Kotlin/JS will stay hardly usable except for small pet projects. That's spoken from experience: no existing Scala library ever cross-compiled to Scala.js before 0.6.0, which is when we introduced a dead simple build tool construct to do so. &gt; A DCE tool for Kotlin/JS will be released in few weeks, for now you can try it in EAP. I'm not convinced by this tool in its current incarnation. The fact that you have to explicitly tell what you want to keep for JS to be able to call is very brittle. It might work for top-level entry points, but I have no idea how one could be expected to list the methods of arbitrary (local) objects passed to JS libraries that those libraries will call. I have the feeling this hasn't been thought through. I guess the JavaScript platform has received quite some love in the past few months. I wonder whether the community will give it a second chance and try to bootstrap the Kotlin/JS eco-system.
&gt; One important thing that is still unclear to me is how one would go about writing a cross-compiling Kotlin JVM/JS library Yes, there's no tutorial for that part. Currently, this case is not supported directly by Kotlin tooling and you should do things manually. It's not hard to compile code for both platforms, it's hard to get good IDE support. The next major release, 1.2, will likely introduce multiplatform project (currently, it's EAPed) and we'll write full docs and tutorials. BTW, there's not much to cross-compile. AFAIK, there aren't much pure Kotlin libraries which can be used both in JS and JVM. Kotlin is focused on interoperation with underlying ecosystem rather than introducing own ecosystem (like Scala does). I don't know about community, but JB ports its own Kotlin libraries when it makes sense, for example, we do so for [kotlinx.html](https://github.com/Kotlin/kotlinx.html). &gt; I'm not convinced by this tool in its current incarnation. The fact that you have to explicitly tell what you want to keep for JS to be able to call is very brittle. You don't have to tell anything explicitly. By default you have a single entry point ("main" function), but sometimes you may want to declare additional entry points that you call directly from JavaScript, say, from `onclick` event. In this case you should tell DCE about it. But if you are satisfied with default entry point, and you subscribe all events via `Element.addEventListener`, you don't need to specify `keep` option.
In addition: &gt; Eco-system, the JavaScript part: ... . Kotlin/JS has good interoperability with JavaScript. You can use ts2kt to convert a TypeScript file with declarations semi-automatically. Also, we working on a repository with declarations for Kotlin, something like DefinitelyTyped for TypeScript. &gt; Documentation: ... Most of the documentation is platform independent. &gt; Community: ... Feel free to join our excellent Kotlin community in slack (http://kotl.in/slack) ;) &gt; With the absolute focus of Kotlin on Android now, this is not going to change any time soon. From FAQ in "Kotlin on Android" blogpost (https://blog.jetbrains.com/kotlin/2017/05/kotlin-on-android-now-official/) &gt;*Is Kotlin going to become primarily focused on Android?* &gt;One of Kotlin’s goals is to be a language that is available on multiple platforms and this will always be the case. We’ll keep supporting and actively developing Kotlin/JVM (server-side, desktop and other types of applications), and Kotlin/JS. We are working on Kotlin/Native for other platforms such as macOS, iOS and IoT/embedded systems.
I recommend you to use Intellij Idea, it'll be better in the long term. To make a smooth transition you can configure the shortcuts of Eclipse in Idea ;) just go to File -&gt; Settings -&gt; Keymap and select Eclipse in the keymaps dropdown.
The best specifics would be actually trying the IDE Intellij has smarter autocompletion, it will place the most common/generic autocompletes to the start of the list. It will also autocomplete type parameters properly Intellij also has inspections in Scala, something which there isn't an equivalent in Eclipse (afaik). The inspections actually help a lot when it comes to learning Scala and avoiding common mistakes (like fruitless type tests, i.e. checking equality of types that are completely different) My personal favourite though is that Intellij has the best Git diff (on merge) GUI I have seen out of any editor or tool. Once you go to Intellij's GIT diff its hard to go back
But they are what makes the language Scala. It's the one feature you must learn.
Functional Programming in Scala (doing the exercises) The Type Astronaut's Guide to Shapeless I'm not aware of anything about macros
Use braces if using a lambda, parens otherwise. Beyond the dot chaining, you shouldn't chain filters -- they should be combined into one filter, so that they don't iterate over the collection more than once. Additionally, it's probably a good idea to just iterate over the collection once. You can do that with foldLeft myCollection.foldLeft (MyCollectionType.empty [MyResultType]){(acc: MyCollectionType [MyResultType], a: MyInputType) =&gt; if (predicate1 (a) &amp;&amp; predicate2 (a)) { acc.append (myMappingFunction(a)) } else { acc } } Edit: Sorry for stepping on your comment /u/valenterry. Meant to reply to op. Edit 2: Forgot about collect, use /u/Technius solution.
Formatting isn't useful knowledge compared to the collections library. Reduce cognitive load by offloading as much as you can into tools.
That's not how working as a programmer in a team works. It's a complete miss-use of the "cognitive load" notion, and in this context where the guy can't unwrap some formatting into working code then it's much better to understand why than to have a tool do it for you and not understand how Scala works.
IntelliJ is unbelievably good for Java. Almost makes me want to use the language. Almost.
Try it. It's hard to remember specific differences and it takes less than 30 minutes to see how you like it.
Obscure how?
If you want to get a good grasp of functional programming, take the free online [Programming Languages](https://www.coursera.org/learn/programming-languages) course by Prof. Dan Grossman. It's taught using Standard ML, a simple but powerful ML language, and Prof. Grossman has a great teaching style with very clear explanations of what's going on. You'll learn not just FP but also how an ML language works and how it relates to different styles of programming, which will all come in very useful if and when you start using Scala.
Haven't had any problems with Maven so far. Just Scala IDE. Which is sad but not unexpected. The developers seem to be focusing their future efforts elsewhere. 
I find it weirdly paced. Rushing over some things while going too slowly over others. Also when going over certain concepts its not clear as to how they fit into the bigger picture(take currying for example). 
&gt; I wasn't asking for a Scala IDE by itself "Scala IDE" is Eclipse with the Scala plugin pre-installed. You can add the Scala plugins in an existing Eclipse install too, but I've found it easiest to download the bundled version.
&gt; I hate IntelliJ but I had to start using it recently because ScalaIDE doesn't work with Scala 2.12. Not quite. I'm editing a medium-large Scala 2.12 project with ScalaIDE right now.
On the one hand I always hear people saying IntelliJ is super amazing for Scala development. On the other, people constantly complain that IntelliJ throws numerous "compile errors" at them that are only errors for the IntelliJ implementation of the type checker but compile just fine with scalac.
Yes, because of their design decisions, intellij is full of false positives and false negatives. It's just a mess. Until they start using scalac, instead of what they do now, they'll always be implementing an IDE for a scala-like language that ISN'T scala.
&gt; Good to know Wine is worth trying - thanks. It's surprisingly good these days. For things that aren't available natively for Linux, I install and launch Steam via PlayOnLinux, and then run games from Steam as usual. You might need to tweak some things afterward, but this approach works pretty often, and is so low-effort it's always worth a try.
Yeah, not much happening in Scala IDE land these days, though 2.12 support did land so Scala Eclipse users should be OK for a little while. When Dotty lands VS Code will likely be the blessed editor (it is now in the early release cycle at any rate). LSP (Language Server Protocol) is apparently the future, which means Scala IDE will need a rewrite to play ball with Dotty and the new presentation compiler. Scala tooling is in a state of flux, nothing works seamlessly; hopefully in future tooling will be vastly improved. Until then, good luck with Maven, IntelliJ and Scala, sticking with Eclipse here, it's fairly solid with SBT backing the show.
What about logging? Do you favor specifying log configuration in code over an XML config file, e.g. `logback.xml`?
&gt;Best practice is to take arguments Exactly! ( Also described in detail here: https://medium.com/@cvogt/scala-as-a-configuration-language-f075b058a660 )
Sure, but.. gosh. Put it this way, I came from a c/c++/java background. It took me 2 weeks to learn python pretty thoroughly. Two years after embarking on scala and I can cope pretty well most the time, but I still have a there-be-dragons attitude to some parts of scala. On the plus side I can now use a single language for both front and back-end development. I wouldn't have gone down this route without scala.js If the main purpose is to think of programming in different ways, it might be better to learn Haskell first. 
No different. If a library wants to log things, it needs to take a logger as an argument.
Looks like a nice and simple scala project. After a quick browse my one suggestion would be to remove the sealed modifier from the Color class that would allow people to define their own Color types 
I've seen (some) people describe scala as very hard, while Java usually doesn't get described as such. Do you agree with that? If so, I'd wonder what the hard parts are. Right now, it doesn't seem as if it would be harder than Java, based on what I've seen.
IntelliJ continually can't get error highlighting right - code that does compile will be highlighted in red, and code that doesn't compile will look fine. So I wouldn't call Scala IDE inferior; it might be missing some more advanced features but at least the basics work there.
As an immediate practical answer, you can use `=` to start another future in parallel: for { a &lt;- futureA fc = futureC(a) b &lt;- futureB(a) c &lt;- fc d &lt;- futureD(b,c) } yield d In terms of a more representational way of doing this kind of thing, there's an active research area of looking at how to combine applicatives and monads while keeping track of the distinctions, see e.g. https://github.com/markus1189/flatmap-oslo-2016 . I'm vaguely working on what I think is the right way to do this: https://github.com/m50d/tierney , but that isn't release-ready yet (I can't quite get the compiler type inference to work right, https://github.com/scala/bug/issues/10310 ). I would handle failure as a separate concern and not try to deal with it at the same time as futures/parallelism. If you need specific handling for different types of failure from B and C, put those failure possibilities into the type that you have inside the future (and if necessary use a monad transformer or similar so that you can compose both at the same time). 
Thanks for the `=` trick :) I've thought about monad transformers, but if I use an EitherT it would still stop the for at the first failure, and I would have no way to know if both futureB and futureC fail, am I right?
Indeed. Maybe you want to use `Validation` which accumulates all failures, but can only be used applicatively, not monadically. Frankly it sounds like your failure cases are complex enough that you need to handle them "by hand" in custom code.
Seems hard to use Validation in an asychronous world, as I need to use some flatMap to handle results (for dependent futures). With flatMap I'm stuck in the monad world... &gt; there's an active research area of looking at how to combine applicatives and monads while keeping track of the distinctions Indeed it seems to be the only solution for this kind of problem, I'll take a look thanks! 
Hi, You can try to use `Applicative/Cartesian` from cats/scalaz for this. It's maybe not a very clean way but should solve your problem. Here's a snippet: https://scastie.scala-lang.org/mielientiev/jUS4rQX6Ruynz8vutvcxwg
Thanks for the example!
I've tried to handle several errors (in B and C) but I can't get both errors in the recover, I can only get the first one : https://scastie.scala-lang.org/uG81zCfIR1aZTupZjibnxQ My result is Future(B failed)
Also, Intellij supports Scala.meta macros. This is a huge win for me. Much easier to develop and to work with macros, since autocomplete will work fine with the generated method.
&gt; constantly going back and forth over whether I want to use mutable data structures. I never felt that. The only time i use mutable structures is when the immutable would be too slow, and that means one thing: bitmaps. The ratio for val and var in my code is maybe 1000/1
It's still far nicer to use than anything else, even with error highlighting turned off and `sbt ~compile` running in another monitor.
Scala as a language is a much simpler language than Java, and also more composable, Which tends to create complex things. It speaks a lot that you can create DSLs in Scala, with somewhat custom syntax, that works in the IDE, because it's still just scala code. Some libraries can make Scala look harder, mostly because of custom DSLs, macros, type system hacking, etc.. But you don't really need to use any of that, and you'll still do fine.
&gt; Seems hard to use Validation in an asychronous world, as I need to use some flatMap to handle results (for dependent futures). With flatMap I'm stuck in the monad world... The logic here is that dependencies are fundamentally incompatible with "handle *all* of the failures" - if an early task fails, you can't tell whether later tasks failed or not because you can't even start them. So you're obliged to choose between fail-fast-with-dependencies-allowed (monad) and collect-all-failures-but-no-dependencies (applicative).
I understand but it's frustrating because in my case B and C are not dependent on each other, so you could in theory run them in parallel and know if they worked or not, before starting (or not starting if there was previous failures) the D computation.
Wow. Talk about creating an issue out of nothing. Besides the fact that those ugly, line munching braces on the next line do work, In Scala you don't need to, and should not use large code blocks. Make it a functions. The only place you need to actually have braces is when declaring a Class or an Object, and there is absolutely no danger there. Having to write semicolons at the end of every line is the real eyesore.
There is no good answer here. If composition of A, B, and C looks like the same kind of composition then it should act the same, or it will be surprising to the reader; if B and C are silently promoted to run in parallel then this leads to different behaviour (e.g. if they both fail then we see two failures rather than one, whereas if A fails we only see the one failure even if B would also have failed). Being able to write something with A, B and C in such a way that you're explicitly doing different kinds of composition at the two stages is what I'm trying to go for, but it's difficult to do that without making the syntax too cumbersome.
I'd suggest tracking depth explicitly - writing a method that can do "search at depth exactly 6" is easy enough, and then breadth-first search is simply searching at depth exactly 0, 1, .... (the one wrinkle is that you need to distinguish between "not found at this depth" and "the tree doesn't go this deep").
You can try out: https://github.com/scala/async it probably yields a way cleaner way to run your futures
&gt; but JB ports its own Kotlin libraries when it makes sense, for example, we do so for kotlinx.html hm... so kotlin and scala-js have something common that I do not like. DSL's for HTML. I dislike them so hard. I mean why do people still try to write templating inside code? why they react style of components? Didn't we learn from PHP, where the template spaghetti got mixed into code so much, that it was hard to understand anything. sorry for the offtopic. 
Scala.js itself does not have any DSL for HTML. Some libraries for Scala.js have, e.g., Scalatags. If you don't like DSLs for HTML, Scala.js does not force you to use one; you can do your templating outside the code or whatever else you want. I'm pretty sure the same applies to Kotlin/JS.
&gt; In this case you should tell DCE about it. My point is that, in Scala.js, you *never* have to tell the DCE about anything. In fact you *can't* tell anything to the DCE, because it's correct and precise by construction, so anything you tell it would make it worse. With a DCE the way Kotlin/JS does it, if you misconfigure your DCE, the DCE will either break your code (if you did not tell enough) or produce a uselessly larger output (if you told too much). Scala.js leaves no room for either mistake.
Use the zip method: for { a &lt;- futureA (b, c) &lt;- futureB(a).zip(futureC(a)) d &lt;- futureD(b, c) } yield d 
Thanks, but I have the same issue as /u/igor-meli solution, I've tried to handle several errors (in B and C) but I can't get both errors in the recover, I can only get the first one : val futureA = Future("a") def futureB(a: String) = Future{ throw new Exception("b failed") ; a + "b"} def futureC(a: String) = Future{ throw new Exception("c failed") ; a + "c"} def futureD(b: String, c: String) = Future(b + c + "d") val futureResult = (for { a &lt;- futureA (b, c) &lt;- futureB(a).zip(futureC(a)) d &lt;- futureD(b, c) } yield d).recover{ case error =&gt; error.getMessage } val result = Await.result(futureResult, Duration.Inf) //result: String = b failed Because : scala&gt; Future(throw new Exception("foo")).zip(Future(throw new Exception("bar"))) //res0: Future[Tuple2[Nothing, Nothing]] = Failure(java.lang.Exception: foo) 
async looks promising, I never figured out if this will be part of an official scala release one day... Is there a stable and production ready version? 
In my case, I needed something that could vend unique IDs. I could have used GUIDs, but I preferred shorter, numeric IDs. As a result, I needed to thread a state object through many of my function calls, and I had to be very careful to never reuse one that had "expired". That drove me to monads and for comprehensions, which can make debugging a bit of a pain. At that point, I wasn't sure if I was fighting the language. I was emulating mutability in a language that directly supported mutability. I'm still not sure which way I'm going to go with that. 
The problem is implicit is not really a feature on its own but a powerful tool (and dangerous) to encode ones. The thing is learning a concept by using an encoding is much harder than doing so from a language having native a support for it. For exemple, native support comes with descriptive error messages, encoding don't.
well you can just use `libraryDependencies += "org.scala-lang.modules" %% "scala-async" % "0.9.6"` it's probably pre release, but as all scala libraries, pretty stable, with a few limitations: https://github.com/scala/async#limitations
I know that. However the Scala.js people actually do prefer these DSLs. And sadly I don't have time to actually write my own. (Actually there is twirl, which sadly doesn't work with Change Detection, which will probably hard to patch into it.)
Well, yeah, for me that is the 1 var for the 1000 val.
Thanks for taking a look. Your suggestion makes sense, I'll think on it.
Your implementation is pretty good. The reason that nodes are getting discovered twice is that they are likely getting enqueued twice. If you have a graph like this: a -&gt; b a -&gt; c b -&gt; d c -&gt; d ... (i.e. a diamond) then you'll explore `a`, which will discover both `b` and `c`. When you explore `b`, you discover `d`. But then the next node to explore is `c`, which also discovers `d`. You end up enqueueing two copies of `d`. You can fix that by checking nodes as you pull them out of the queue, by checking the queue before enqueueing new nodes, or by ensuring that your `discovered` set contains all the nodes that you've already processed *plus* all the nodes that are in the queue (i.e. mark a node as discovered as it enters the queue, not as it leaves the queue - which I'd note you're already doing with `root`). If you want to generate a list in the end, it's common to use an "accumulator" parameter. The general pattern is: @tailrec def iterativeFunction(acc: AccumulatorType, queue: Queue[V], ...): List[V] = { if (queue.isEmpty) { acc } else { ... iterativeFunction(updatedAccumulator, updatedQueue, ...) } } You can think of this as a sort of generalized `foldLeft`, where the collection that we're iterating isn't fixed up front. If you use `List[V]` as your `AccumulatorType`, you can collect all your results into a list. Note that the `@tailrec` annotation doesn't enable tail recursion optimization (that happens automatically), but it *does* prevent you from making recursive calls in non-tail positions. Another possibility is, instead of using iteration to eagerly produce a list, you could instead generate a stream that will lazily visit the nodes in BFS order. Anyway, good luck!
That would potentially work for a tree, but OP said that they're dealing with graphs. Therefore, a node might exist at multiple depths. And for that matter, OP didn't explicitly say that they're dealing with acyclic graphs. A single node might exist at an infinite number of depths. For graphs, OP's implementation is pretty solid. 
:)
This is the article I needed when I started learning Scala. 
Valid point but this happens in Eclipse too. It does not happen in vim /s. JetBrains does fix these issues over time and there is a reporting tool if you right click on the highlighted text in IntelliJ. Another issue with IntelliJ is that Git integration does at least one odd thing. Occasionally it will tell me that I am working on an ignored file when that is not the case. However, when you are in a Git controlled file and modify a line or group of lines, the left gutter (where the line numbers are) will be colored. If you click on the colored section you can browse the Git history for those lines. This is amazingly useful. Still, IMO, IntelliJ is superior to Eclipse. I value a lot of those advanced features.
It doesn't happen in Eclipse, at least not in the "Problems" tab. There's a lot of FUD going around about bugs in Eclipse's error tracking, but somehow no-one ever has an actual working example. 
Thank you very much!!!!!
There cannot be too much introductions to Monads :) Thanks
One of the easier explanation of Monad I ever saw 
How do you find this: https://scastie.scala-lang.org/marcinzh/ZZ2TFg3ES9e0LGVQwV8Guw/25 This is your example adapted to the effect library I'm working on. It's not ready to advertise yet (I'm writing initial documentation for the first release as we speak), but I'm looking out for cases to test it's usablity in the real world. 
Ah, right, that's what I get for reading on my phone... Can you do something like .map(Success).recover(Failure) on futureB and futureC before you zip them together? Then you have a pair of `Try`s to work with in the for comprehension. It's clunky, but I think that's inevitable, unfortunately.
Not bad, but he says "monads don't compose" when his interface for Monad doesn't actually enforce that...
If they're an experienced developer, I'd recommend *Scala for the Impatient*. The title is extremely appropriate...I was able to pick up the very basics of scala within a few chapters, while still being comprehensive enough to explain most of the more abstract and mind-bending features like covariance/contravariance annotation, higher kinded types, type classes, etc. *Programming Scala* is overall a better book, but compared to *Scala for the Impatient* it feels a lot more like a reference than a get-up-to-speed book. 
your problem is that 'discovered' argument should be global to 'bfs_' method, this is because once you discover a node, you need to not discover it again in different calls, then 'discovered' should be a mutable structure, in this case your 'bfs' method will keep referential transparency.
Yup, you're right, but I tried to create this function without mutable structures, in a purely functional manner. I saw some examples in Haskell but I couldn't adapt.
I feel like I'll never be able to learn Monads in my life. Every material about Monads I read, I feel dumber. I'll give this a try. Edit: Thank you all for the links and responses!!!! You guys from this sub rock!
Unfortunately the article uses monads for things where the order of things doesn't matter at all. Which means, people will now use for comprehensions for that stuff and I have to figure out if there is really some dependency (maybe hidden in deeper sideeffects) or someone just didn't know of applicatives. However, mingling applicatives into this article (or going even deeper) will just scare people away even more. And starting with applicatives in the beginning is - for most - harder to grasp and there are less places where one can see directly that they can be used. But at least there should be a hint to applicatives in the article for when order doesn't matter and e.g. one wants to execute things in parallel.
could you link one example?
You typod/mixed up the (applicative) and (monad) - just in case someone gets confused.
https://lettier.github.io/posts/2016-04-29-breadth-first-search-in-haskell.html https://gist.github.com/jagbolanos/856828
How does Scala.js do that? I mean, you write some JS code manually, which calls Scala function. Does Scala.js parse this code and determines which functions get called? Does Scala.js perform DCE over its IR *before* codegen, or does it perform DCE over generated JS? &gt; With a DCE the way Kotlin/JS does it, if you misconfigure your DCE, the DCE will either break your code I repeat, in *most* cases you don't need to configure DCE at all. It's for rare specific cases. It *most* cases DCE works automatically.
I tried this but I seem to still be getting an out of memory error and I increased the memory as well quite a bit in intellij. Note I am using scala 2.10.5. I got this to work but now I get the error that spark cant save to table because of the jackson maven file conflicting with spark 1.6. I am gonna have a look into it.
Do something with a framework with a functional API. I learned a lot by building a rest api with http4s. 
I tried doing this but it did not work for me. I tried increasing the memory but when running the spark job on the vm and also by increasing the driver memory for garbage collection and executor memory for the running of the job each to about 4gb and was still getting memory error exceptions.
In Scala.js, methods and classes cannot be called from JavaScript by default. At the top-level, only classes/objects/methods/variables annotated with `@JSExportTopLevel` can be seen from JavaScript, *irrespective of whether you use DCE or not* (I mean, you cannot *not* use DCE in Scala.js, but that's another story; it's more like irrespective of whether said method is reachable via other means). Also, it's *in the code*, not in the build tool or in the compilation command line. So you can't break it by accident because you don't use the same build tool or because you want to compile from the command line. This is in fact similar to `export` directives in ES2015 modules: other than explicitly exported entities, everything is module-private (or, in our case, private to Scala.js). This means that the dce precisely knows what JavaScript can possibly call, and eliminate everything else. In addition to top-level export, we can also export members of Scala.js classes, again with an annotation `@JSExport`. Without the annotation, whether or not the method is otherwise reachable from Scala.js code, JavaScript code cannot see the method. Finally, in Scala.js you have *Scala classes* and *JavaScript classes*. In Scala classes (extending `AnyRef`), things are not visible by default (they need `@JSExport`). In JavaScript classes (extending `js.Any`), however, all the members are visible from JavaScript (plus, fields are really fields, getters/setters are really getters/setters, overloading and overriding work as expected from JS, etc.). For JavaScript classes, the DCE knows that, if the class itself is reachable, then so must *all* its members. But for Scala classes, it can remove things that are not reachable. I mean, that's a lot of details ([you can find more here](https://www.scala-js.org/doc/interoperability/export-to-javascript.html)), but the core design decision that allows this is the following: something is visible from JavaScript *if and only if it is explicitly exported in the code* (or it is in a JavaScript class), *irrespective* of whether dce is applied or not. And that means that dce can do its job without any configuration, always being correct and precise. Edit: to answer your question: Scala.js performs DCE over our own IR before codegen, yes. Actually after DCE we still have several passes, including an optimization pass with inlining, constant-folding, stack allocation, closure elimination, etc., followed by a *second* DCE (since optimizations tend to make a lot of things unreachable).
I think that's enforced by the types. When doing a flat map from List to Future it won't compile 
It's not, it just says it must take Monad[T], nothing to enforce the type that extends Monad. You need Higher Kinded Types to do that, check out the Scalaz implementation if Monad :-)
Any class `Foo[A]` that has a constructor and supports these two functions (with their obvious semantics) is a monad: flatten[A](foo : Foo[Foo[A]]) : Foo[A] map[A, B](f : A =&gt; B, foo : Foo[A]) : Foo[B] The combination, `flatMap`, is sometimes called `bind` or `&gt;&gt;=` in other languages, and is what is invoked for all but the last `&lt;-` in `for ... yield`. The last `&lt;-` is just `map`. Unfortunately, in Scala, if you try to make `flatten` or `flatMap` a method on `Foo`, you won't be able to give it a straightforward type. 
Oh I thought you were using the scala command to run the program. I'm not sure how to pass jvm arguments into a spark job. Maybe [this](https://stackoverflow.com/questions/28166667/how-to-pass-d-parameter-or-environment-variable-to-spark-job) stack overflow question may help?
Sorry you're right, I just glanced over it and assumed it was the right definition. 
Whoops, fixed
I don't understand, so I must be missing something. How is a typeclass in Scala more dangerous than one in Haskell? 
flatten is not a part of the monad interface. flatMap is. "flatten" is just flatMap(identity) I'm not sure what you mean by you won't be able to give it a straight forward type. 
That depends on which encoding of Monad you pick. The encoding with `pure` (which the post above has forgotten, but you need it) and `flatMap` is the most useful for programming, but you can also choose an encoding that's closer to the category theory definition of Monad, which means that a monad is a Functor (and therefore has `map`), plus `pure[A](a: A): M[A]` and `join[A](m: M[M[A]]): M[A]` (`join` is `flatten`). The two encodings are equivalent, since you can recover each one from the other: `flatMap` is `map` and then `join`, and `join` is `flatMap(identity)`. Also, a type `M[_]` having these functions is not enough for it to form a Monad: it also needs to abide by the Monad laws. Note: I'm disregarding Applicative in this comment
Albeit I see your broader point (and I very much agree with it), the difference between Applicative and Monad is not about order: most Applicatives depend on a certain order too, meaning that if you switch the order of two arguments, the result will differ. The difference is whether or not a computation needs to depend on the result of a previous computation or not.
I thought going through the book advanced scala with cats was helpful http://underscore.io/books/advanced-scala/
It seems perfect, great work! When is the release planned? 
Hm, you are probably right. I have never seen any case yet where the order matters and can't be changed without changing semantics. E.g. when doing validation, if I change the order of validations, the order of the result will change. But I will not lose any information, as I can adapt the rest of my program to handle the different order and exactly the same as before. What would be an applicative where I lose information and cannot always change my program to adapt to a change of ordering?
Wrote an intermediate level post and accompanying GitHub project for accessing Hacker News via its API using Scala, scalar-http and uPickle http://justinhj.github.io/2017/07/26/hacker-news-api-1.html
 m.flatMap(f) = m.map(f).flatten In my experience, it's easier to teach monads using the two simpler functions flatten and map, than the more complex composed function flatMap. You can say that: m.flatten = m.flatMap(identity) m.map(f) = m.flatMap(f andThen pure) But it seems like the wrong way around.
well, monad is "just" an interface in the end. It as simple as that. It enforces you to implement flatMap method. It comes with syntactic sugar in Scala, with this for comprehension but you could rewrite any for comprehension in a sequence of flatMap.
In my last job I got quite deep into Scala for 2 years - learned Akka, Spark, Play, SBT, Cats, Shapeless etc. It was my favourite language (I didnt know the basics of Rust or Haskell at the time, and I enjoyed the multi-paradigm capabilities). Moved to a new job with an eclectic mix of Scala microservices, Java for DataFlow, Python TensorFlow &amp; NodeJs microservices. Our devops is also pushing Go. We've recently moved away from Scala, replacing it with Go - Akka Cluster doesnt play well with Kubernetes + JVM is not container memory aware We phased out Spark - for streaming we use DataFlow and for ML we use TensorFlow The project velocity of Play / Akka-Http cant match Node on V8 hiring pool - more available, cheaper Node devs
You are thinking of a subset of Monads (Applicatives), called _commutative_ Monads (Applicatives). `Reader` is the perfect example. List is a counterexample (not commutative). 
You are right about the applicative, however there are some good reasons to not mention it with beginners: - as you said it would scare them even more - the std library doesn't give any support for the applicative syntax - in most of the cases, you want to shortcircuit and the Monad behaviour is perfectly fine, the only real use case I have seen when you really need Applicative is validation, other than that Monads work just fine, for the Future you simply put them in a val outside the for and they will run in parallel, maybe not the most functional approach but works well. I think to introduce Monads to beginners and help them to get familiar with them, sticking with the std library without mentioning all the detail is the best way. Cheers, Luigi 
This version is working : https://scastie.scala-lang.org/nvHPC0bgSnO9O5WykliOfA Thanks!
You are right, that interface won't enforce the type constrain, probably is not possible to implement it actually, but I just wanted to keep the example as simple as possible, scalaz and cats use Typeclasses to implement the interface and I didn't want to mention them, an higher kinded type should work or maybe using this.type, but I think this was the most clear way to explain it. Regards, Luigi 
Forgive me, I'm no Haskell expert, but I can't say that the error messages I've seen from Haskell are that great. `No implicit Ordering defined for MyType[Int]` is more helpful to me than ``No instance for (Num a0) arising from the literal `1'``. Both of those messages are what I get for using a method that has an `Ord` or `Ordering` context bound for a type that has no `Ord` or `Ordering` instance. The default error message you get in Scala (`could not find implicit value for evidence parameter of type TC[MyType[Int]]`) is still clearer to me.
Thx for clarifying Fabio! What else would I use to indicate that I don't care about ordering? *Just* using a commutative monad/applicative is not enough then.
 class Succ[N &lt;: Nat] extends Nat { type This = Succ[N] type + [X &lt;: Nat] = Succ[N# + [X]] type * [X &lt;: Nat] = (N# + [X])# + [X] } This should be `type * [X &lt;: Nat] = (N# * [X])# + [X]` right?
Just to clarify, by commutative I mean: `(fa |@| fb).map((a,b) =&gt; f(a,b)) == (fb |@| fa).map((b,a) =&gt; f(a,b))` Which is slightly different from the case of executing things in parallel. We don't (yet?) have a typeclass for CommutativeMonad (or Applicative), so I don't know how to answer that. The case for a sequential monad and parallel applicative (e.g. for Task or IO) is much more nuanced, since it can be done in principle, but it would break the law: `(fa |@| fa).map(f) == for { a &lt;- fa; b &lt;- fb) yield f(a,b)` which is a controversial choice: some libs do it (e.g. Fetch, haskell's Haxl and so so on), some don't (cats, scalaz), and hide it behind a newtype or at least an import.
You are disregarding `pure` though, which is required to have an actual Monad, and you'll find out that in order to implement that, one does indeed need typeclasses, since `pure` involves return type polymorphism.
Also implicit def factorial[N &lt;: Nat, X &lt;: Nat](implicit fact: Factorial[N] { type Res = X}) = new Factorial[Succ[N]] { type Res = X# + [Succ[N]] } this should be `type Res = X# * [Succ[N]]` I think.
I didn't disregard pure is defined in the trait as point (as it is in Scalaz) also you don't need typeclasses necessarly, you could define it as: trait Monad[T] { def point[T](t: T): this.type def flatMap[R](f: T =&gt; this.type): this.type def map[R](f: T =&gt; R): this.type = flatMap(t =&gt; point(f(t))) } or even better: trait Monad[M[_], T] { def point[T](t: T): M[T] def flatMap[R](f: T =&gt; M[R]): M[R] def map[R](f: T =&gt; R): M[R] = flatMap(t =&gt; point(f(t))) } and then: class Foo[T] extends Monad[Foo, T] { def point[T](t: T): Foo[T] = ??? def flatMap[R](f: T =&gt; Foo[R]): Foo[R] = ??? } but still, the idea was to keep the example simple. Regards, Luigi
You're absolutely right about both cases. I just fixed it! Thanks for reporting!
I take the point about simplicity :) but the code you posted above just does not work. `pure` allows me to do `3.pure[Foo]`. How can you call `pure` on `Foo`, if you don't have an instance of `Foo` around (because you are trying to create it)? There's also other problems e.g. the handling of variance, the fact that you have to remove `T` from `Monad[M[_], T]` and `Monad[T]` and put it on each method (which in turn gives you other problems), and so on. 
Except for the above books, I would suggest [Learning Functional Programming in Scala](https://alvinalexander.com/scala/learning-functional-programming-in-scala-book) by Alvin Alexande. He is the author of [Scala Cookbook](http://shop.oreilly.com/product/0636920026914.do). His writing style would let you easy to pick up the concept and learn to write small piece of code. It's like a Scala veteran was alongside you.
It's [released](https://github.com/marcinzh/skutek) but undocumented. Full manual will take me half week. I'll try to publish stripped down, cheat sheet-like version later today.
Ah ok I see it now, point won't actually work without typeclasses, maybe could be in the companion object but probably not :) 
Have you tried my newly published library [future.scala](https://github.com/ThoughtWorksInc/future.scala)? In `future.scala`, parallel exceptions can be handled by creating a `Semigroup[Throwable]`. https://scastie.scala-lang.org/Atry/wzJ34wFXS4WrSn4j24rjyw You will see a `MultipleException` at end: ``` Caused by: java.lang.Exception: c failed &amp; java.lang.Exception: b failed ```
I was not aware of it, seems great, thanks!
Yeah, the companion object might seem like a solution, but you still have problems when you try and write a function that's polymorphic over the specific Monad, since with your encoding you'll have to say: "this function works for all types T, where T is a subtype of Monad, and the companion object of T contains pure", which is impossible since in scala there's no way to generically refer to the companion object of a type, unless you introduce a typeclass for that ;) Compare with "this function works for any type T, given implicit evidence that T forms a Monad", which just works. Ultimately, OO does not have return type polymorphism
From a shallow comparison, it looks very similar to [RösHTTP](https://github.com/hmil/RosHTTP), which has been there for a while, and cross-compiles to Scala.js.
So like [play-ws](https://github.com/playframework/play-ws)'s [typed bodies](https://github.com/playframework/play-ws#typed-bodies)? 
There's at least a couple of differences: * sttp supports multiple backends: like RosHTTP, monix-based, but also akka-http based, and ones integrating with scalaz, as well as a synchronous one. So you can pick one fitting your stack * together with multiple backends, various streaming approaches are available (this will be covered in the next post) * has an uri interpolator which enables convenient creation of complex addresses * allows creating partial requests with any component optional and specified in any order, which also includes response parsing * on the downside, no scala.js suppot (at least yet)
Not only. play-ws certainly is one of the inspirations. However, as mentioned in the comment about RosHTTP, there's quite a lot of differences as well - I think most of the above apply to play-ws as well.
It doesn't seem to me that any of those differences are fundamental or are even different designs. Why not contribute these features to RösHTTP, instead of defining an entirely new library?
So more like [Gigahorse](http://eed3si9n.com/gigahorse/) -- WS with different backends?
Ure suggestion ended up helping me out quite a bit a lot of my code had vars and I converted all of those to vals. Not it runs fine .
There's still an overhead in training before they're up to speed.
What's a good alternative to the Eclipse-based Scala IDE?
For a more attention-getting blog post, of course.
Anyone knows a example of Sangria exposing shapeless HLists or HMaps? Or maybe sangria won't fit these cases. Not sure.
Awesome!! Am I right to expect faster compilations with this version?
Yes!! There are multiple reports of real-world projects compiling at least 15% and oftentimes 30%+ faster.
I hate "transpiling" as a term, because we have "compiling" as a perfectly adequate verb to describe translations from one language to another, as that's literally its definition. Basically "transpiling" happened because of CofeeScript, as people needed a way to say "JS with a sprinkle of unimportant syntactical sugar", being the same type system, the same language in all things that matter. Scala.js is not a transpiler. Sometimes this is a pain, because the generated JavaScript isn't very readable, especially since it's also passed through the Google Closure compiler. We do have source maps of course, for debugging. And Scala the language has a different type system, being no longer JavaScript with sugar on top.
I'm seeing a ~14% compile time improvement which is in line with what everyone else has been getting :)
Totally awesome!
Well, I'd say that even in the request-building API there are some important differences, for example in the way you can define partial requests in a type-safe way (and that you cannot send a request without an URI/method defined). But of course, that's only me. Everybody can choose their preferred flavor :) I'm all for contributing to existing projects, but I don't think this makes sense when you want to experiment with a few new design elements, which would rather radically change the library.
Ah, didn't know that one - added to the list of other http libraries in the readme. Yes, the exchangeable backends idea seems similar.
Great work! I just updated [fs2-rabbit](https://github.com/gvolpe/fs2-rabbit) to use this version and I noticed that the compilation time has improved by means of 12.5% compared with Scala v2.11.8!
Since they have deprecated `--opt:l:project`. What is the equivalent when using `-opt:l:inline` and `-opt-inline-from`? The nice thing about `--opt:l:project` is that it just inlined methods for the current project path (hence you used this for libraries, where as with applications you would use `--opt:l:classpath`). It seems with `-opt:l:inline` you have to do this stuff manually?
I think would be fair to mention that all the first part is heavily "inspired" from http://gigiigig.github.io/tlp-step-by-step/introduction.html 
Looks plagiarized IMO
Nice blog series! I think it would have been fair if I've known of it, however, as I linked at the end of my blog post, most of the inspiration for writing it came after I did this course: https://stepik.org/course/ThCS-Introduction-to-programming-with-dependent-types-in-Scala-2294/
Do you have any other conspiracy theory we can prove mathematically?
There isn't a clearly better option. IntelliJ does some things better and other things worse (the majority seem to prefer it overall; I don't). Ensime is another set of tradeoffs.
The trouble is that Scala IDE is currently broken on GNOME (Linux).
I vaguely remember an issue like that to do with library versioning or something. In any case, I'd look into that specific issue (E.g. there might be a patch, or a nightly build that has it fixed) rather than switching to something different, personally.
Ah c'mon some examples are exactly the same :) I'll just past here this : trait Foo { type T def value: T } object FooString extends Foo { type T = String def value: T = "ciao" } object FooInt extends Foo { type T = Int def value: T = 1 } def getValue(f: Foo): f.T = f.value val fs: String = getValue(FooString) val fi: Int = getValue(FooInt) And this: type Aux[A0, B0] = Foo[A0] { type B = B0 } Those are in my articles .. then say what you want :)
 def weight( e: (V,V) ): W = { val l = edges.filter( x =&gt; x._1 == e._1 &amp;&amp; x._2 == e._2 ) l.head._3 } Please use meaningful names. T_T
Not "taking sides" here, but a type class named `Foo` with an abstract type member `T` and a `def value: T`, and a textbook `Aux` type alias, is about the most generic possible example about dependent types in Scala. It's not unthinkable that two (probably more) people come up with it independently.
You are right, one similar example deosn't mean anything, but look here http://gigiigig.github.io/tlp-step-by-step/dependent-types.html , even the paragraph are the same, he took all the same examples from Idris and then start talking about Path Dependent Types in the same order and with the same words: "Scala is not a fully dependently typed language " "Although Scala it is not a fully dependent type language" And so on .. a lot of coincidences :) But yeah I wasted too much time with this already. 
In Scala this is very easy to use different implicits for the same type whereas in Haskell this is much more difficult and discouraged heavyly. Take the type `Int` and the `Monoid` type-class: -- in Haskell class Monoid a where mempty :: a mappend :: a -&gt; a -&gt; a -- in Scala trait Monoid[A] { def mempty : A def mappend(a1 : A, a2 : A) : A } There are several way for `Int` to implement this type-class : implicit mint1 = new Monoid[Int] { val mempty : Int = 0 def mappend(a1 : Int, a2 : Int) : Int = a1 + a2 } implicit mint2 = new Monoid[Int] { val mempty : Int = 1 def mappend(a1 : Int, a2 : Int) : Int = a1 * a2 } both instances are correct. But which one will you take in : def fold[A](l : List[A])(implicit A : Monoid[A]) : A = l match { case a :: q =&gt; A.mappend(a, fold[A](q)) case Nil =&gt; A.mempty } Imagine that in a function you compute `val v1 = fold(List(1,2,3))` and somewhere else (using a different scope), you compute `val v2 = fold(List(1,2,3))`. You would expect `v1 == v2` but nothing garantees that `v1` and `v2` were computed using the same implicit. This is a problem because `mappend(v1, v2)` can be different from `mappend(fold(List(1,2,3)), fold(List(1,2,3))` depending on which implicit were used on each side.
Hahahaha, sorry, thanks!
Hmm, okay, can't disagree there.
But you can't learn it all at one time. Other than a few specific cases, which are probably bugs, Scala isn't whitespace specific (besides newline ~= semicolon). Learning formatting corner cases is time not spent learning more important parts of the language. It's something you can pick up as you go. In the things that make scala hard to learn column, formatting is the last thing on the list. Use java indentation and bracket styles, or one from YOUR favorite style guide. There are a lot of other important things too navigate and learn. I'd also disagree that this isn't how a team works, and that offloading formatting to a tool isn't reducing cognitive load. On a team, you want beginners to be somewhat productive. This usually means limiting what they have to learn to the basic footprint to complete the task they are assigned. Offloading some trivial parts of it to tools is reducing cognitive load and freeing them up to stay on task is good teamwork. On top of the programming language, there is usually a complex domain that devs have to learn as well. Nobody suggests that new programmers understand all the intricacies of accounting as soon as they jump on a project. They offload that to specifications and subject -matter-experts. Expecting to jump into a new, unfamiliar, programming language and being immediately productive is ludicrous. Expecting everyone to memorize the ASI and type inference rules as they learn isn't worth their time. Expecting the team to code to the lowest common denominator, when that denominator is marginally above zero, thus limiting their productivity, is also not good. As a new team member you have the obligation to learn what you are given to learn. As a new scala user, you have the obligation to learn the language features. As a new FP programmer, you have the obligation to learn FP. Anything that you can offload during these tasks of learning will accelerate your overall progress. Anecdotally, I've taught around 35 people to write type-safe, non-blocking, custom high throughout streaming-data processing , production ready scala code in a domain as difficult as accounting over the last two years. Every six months we get new team members. Every six months I have to reteach map/flatmap, and referential transparency (without FP the system we built is intractable, yes there's easier ways of designing it, nope we're not redoing it cause reasons that are not my decisions to make). The business wants scala programmers. I teach them and make them productive. Then they go off to their own teams. Everyone complains about every feature we use. Nobody understands anything about fp or referential transparency or concurrency or domain modeling, or even SOLID or open/closed at first. Then they parrot. Then they understand. Then they are productive. It takes about three weeks of learning with a senior team member. That's pretty good, considering that it took me 3 years to get to be confident in the language. I believe that users should learn all the language. I just think that learning how to write the code is more important than learning how to format it correctly, especially since that is often dictated by style guides in professional settings as well.
In languages that don't handle tail-calls correctly, e.g. anything on the JVM, you get significantly greater pressure on the GC because of trampolining. This is the biggest issue for Scala. A secondary issue for performance is that purely-functional data structures can't be as efficient as their imperative brethren. This tends not to be an issue in practice—scalaz and Haskell both have `STRef` and `IORef` as well as in-place mutable arrays, for example—but you do sometimes need to pay attention to it. Haskell's laziness is "the wrong default," to quote Epic Games' Tim Sweeney. But to a first approximation only Haskell is lazy, so I'm not even sure this tells us anything about FP. Scala is strict; OCaml is strict; Idris is strict...
Yeah, I agree with that. What about just one sentence / additional text note like "if the second result is not dependent on the value of the first result in any case, there is an alternative to monads: [link to applicatives]"? That would make me happy and at least makes users know that there is something else *if* they want to dig in further. :) Btw, keep on the great work. I especially enjoyed the typelevel stuff of yours!
&gt; Am I right to expect faster compilations with this version? Yes, definitely. My work project compiles ~30% faster. (I wonder what's different about it compared to the other posters here who are seeing smaller - but still great! - speedups.)
a `while`-loop calling `java.io.BufferedReader#readline` over and over and printing stuff out would work well for a first pass. If you want something to nicely deal with colored strings, https://github.com/lihaoyi/fansi helps. If you want something to give you fancier autocomplete, multiline editing, etc. you can try one of - https://github.com/jline/jline2 - https://github.com/jline/jline3 Or the Ammonite's own multi-line colored terminal implementation `"com.lihaoyi" %% "ammonite-terminal" % "1.0.0"` - https://github.com/lihaoyi/Ammonite/blob/master/terminal/src/test/scala/ammonite/terminal/TestMain.scala#L23
If your functions return `Option[A]`, then you can use `.orElse` to chain the methods. attempt1().orElse(attempt2).orElse(attempt3) Note that you still have to handle the case that all three functions fail to produce a value.
Is it possible to use Groovy files ?
The bug tracker said 6-9 months for a fix.
Yeah, jline looks like the winner. Thanks. Do you know if there's some more complete documentation somewhere? The gh wiki seems very incomplete. 
Same for `Try[A]`
It's been a couple of years but from what I recall the problem is because for some reason the framework developers decided to include a logback.xml on the framework distribution classpath. I think they should instead have programmatically configured logback as a fallback so that users could override with their own config. It looks like they've tidied this up a bit in later releases but it's still fairly over complex. Two things you might try. 1) Call your file logback-test.xml which has precedence according to the [logback docs](https://logback.qos.ch/manual/configuration.html). 2) Try the logback property for setting the config file rather than the play one: -Dlogback.configurationFile. Not sure if either of these will work sorry but worth a go - I do remember this being a pain.
For all the projects that have been waiting for Scala.js to try out sbt 1.x, you can now jump ahead with [Scala.js 0.6.19](https://www.scala-js.org/news/2017/07/29/announcing-scalajs-0.6.19/).
Scala.js 0.6.19 just released with SBT 1.0.0-RC2 support. Let's get the ball rolling, or at least consider it (I'll post on the ML).
really nice little write up. like your explanation and examples of natural transformations. Will be stealing this ;)
Is it possible to cross deploy a sbt plugin for both 0.13.x and 1.x using sbt-release?
https://github.com/typelevel/cats/issues/1536
There is an issue but no implementation. I doubt my implementation would be accepted by Cats folks. At the very least it lacks integration with Cats PartialOrder type class. 
I see. I was only posting for visibility!
Thanks for pointing out to that issue. I actually left a comment there.
Same for `PartialFunction`, for example: https://github.com/ThoughtWorksInc/sbt-api-mappings/blob/37af76e/src/main/scala/com/thoughtworks/sbtApiMappings/SonatypeApiMappingRule.scala#L20
I'm working on a pet project in which I am connecting to a web socket using [Akka HTTP client-side websocket support](http://doc.akka.io/docs/akka-http/10.0.9/scala/http/client-side/websocket-support.html#client-side-websocket-support). Right now I'm basically trying to do Ping/Pong conversations between the server and my client, but I'm having trouble finding an example or documentation of how to respond to the server based on a received message. Does anyone know of an example or point me in the right direction? Edit: I wasn't able to find a resource, but I pieced together something that worked from several different resources. I assume there's probably some unsafe bits because I'm still learning, but [here's the gist](https://gist.github.com/ryanmiville/615c850c37bffc7f0c711520f8832a9a) of it for anyone that's curious (no pun intended).
This worked perfectly for me. Thanks a lot. &gt; 2) Try the logback property for setting the config file rather than the play one: -Dlogback.configurationFile. &gt; 
Hey Guys, Could you please help me with this thing I got stuck with lately? Really beginner stuff actually. So I have a DB table containing items, with a handful of attributes, all of them having string values. I'm using Slick to access the DB. I want to filter these items based on user selection, where the user can select basically any number of attributes, could be for a given attribute no selected value - get all of them in this case, one selected value, or multiple values selected. If I look at just one attribute, Slick's inSet works perfectly for me. db.run(myItems.filter(_.attributeA inSet List("selected_value1", "selected_value2").result) What is the best solution to use arbitrary number of such filters? I don't want to hardcore any of these attributes, as the possible set of attributes to filter for can change over time. Hope I was able to express here what I'm trying to achieve. 
I don't know what the type of _.attributeA inSet List("selected_value1", "selected_value2") is but but lets call it `MyItems =&gt; R`, then couldn't you just take in a bunch of filtering functions and foldLeft them? def run(filters: List[MyItems =&gt; R]) = db.run(filters.foldLeft(myItems)(_.filter(_)).result) that should be the same as like db.run(myItems.filter(filters(0)).filter(filters(1)).filter(filters(2)).filter(...)).result)
[Here is my answer](https://www.reddit.com/r/compsci/comments/6pnq7e/what_are_potential_disadvantages_of_functional/dkwroc1/) that uses many examples in Scala.
This is a great news! Thanks! Migration will be interesting;)
You might also look at [decline](https://github.com/bkirwi/decline) for parsing the commands users type, and providing built-in help. I use it for [tuco](https://github.com/tpolecat/tuco) and it works well.
TypeScript can be very beneficial when your company has a lot of existing JS developer talent. I would largely recommend that they use TypeScript over something like scala.js in those cases. 
I did some further googleing on this (yesterday when I posted this it was late in the night, looks like I've been too tired), and this pretty much covers my problem: https://gist.github.com/cvogt/9193220 This brings up a new question, hope it makes sense. Here, in the find, all the "filters" are defined in the method's signature: def find(id: Option[Int], createdMin: Option[Date], createdMax: Option[Date], modifiedMin: Option[Date], modifiedMax: Option[Date]) Can something done, for this to be dynamic. What I mean is, instead of this: find(Some(1),Some('2017-07-30'), None, None, None) to have something like find(List(Some(("id",1)), Some("createdMax",'2017-07-30')) Which would yield .filter(id)(v =&gt; d =&gt; d.id === v) .filter(createdMin)(v =&gt; d =&gt; d.created &gt;= v) But if I were to use the same method with 3 elements in that list, it would yield 3 filters. (promise no futher weird questions :) 
Why beanstalk over ecs?
You should add [Newman](https://github.com/megamsys/newman/) and [scalaj-http](https://github.com/scalaj/scalaj-http) in that case
That's really cool. How do you feel about gatling being proprietary?
Gatling is open source project under Apache License 2.0. I know gatling team have commercial product Frontline, but i did not try it.
SBT has [support](http://www.scala-sbt.org/1.x/docs/Command-Line-Applications.html) for being used as a library for creating command line applications.
I think many new developers aren't aware of these differences enough to even understand where Scala is superior (rather than being a more complex language to learn fully). I compared the languages myself and I'm not aware of coming across this level of details or even mentions regarding how they impact a language's development. Kotlin's picking up steam due to Android and its design decisions merit closer scrutiny.
Still waiting for IntelliJ to update their plugins :-(
I'm a junior dev, and I've picked up Scala for my job, as I work as a junior big data developer. I noticed that data engineering is not really my thing, and I was wondering if there's a lot of demand for Scala in career paths other than Data Science. I fell in love with the language (coming from a Java background), and I would love to continue working with this language. What are some other good use cases for using Scala? 
oh right. My misunderstanding. Thats good.
It's a general-purpose language, it's suitable for most development. I mostly work on backend stuff, sometimes services with a REST interface, sometimes not even that.
Thanks for the answer. I just wish demand was higher in my country. But we'll get there :)
scalaj is the first item on that list already. Newman seems dormant?
Don't worry about performance unless you have a concrete performance issue. Throwing an exception in Scala is "slow" in some sense, but it's probably faster than calling a method in Ruby.
Afaik, at least in Scala, even `STRef` and `IORef` are going to be slower then their mutable counterparts. This is even talked about in the thread (where some functional protagonists demonstrate that Haskell code can be very fast when it turned out that the Haskell code was mainly just calling fast C functions via FFI, and even usage of mutable data structures in Haskell is slower, mainly due to boxing/extra function overhead amongst other things) General takeaway is that, functional programming isn't a silver bullet ;)
what are the minimum system requirements ?
If you're eager to try it I got it to work with intellij 2017.2, sbt 1.0.0-RC3, and newest nightly version of the scala plugin (you can choose which plugin update channel you want to use in intellij preferences) After a few hours use i havent seen much breakage at all, importing, building, running and so on works well.
Faster `for` syntax operations for large collections. https://github.com/shawjef3/DepthFirst
Documentation of Kotlin libraries I find to be a huge issue, not just KotlinJS. Eg. Exposed and Ktor's documentations are really poor considering that they were developed by JB. Compared to Cljs, Kotlinjs is way behind. I have built a couple of apps that are in production using Cljs and it was much more straightforward than when I tried doing something with Kotlinjs. The Cljs tooling is also much more superior than Kotlinjs and anything in JS land at the moment. I haven't tried Scalajs yet to have an opinion on how it compares. Ideally I would like a good type system in the front end as well, but the tooling in Cljs and the rich library ecosystem makes it difficult to jump to something else.
I believe logback.xml has been removed from later versions of Play. If you want to see what logback is doing and which file it is using you can set a system property to debug it: -Dlogback.debug=true.
Nice library. I'm looking at these kinds of libraries at the moment. How does it compare to Monix?
If the fields are "static" you can use default arguments + named parameters: `find(id = Some(1), createdMax = Some(someDate)) // other parameters defaults to None` Inside your `filter` function, each condition can be turned into a function `List[Record] =&gt; List[Record]`. e.g. val filterByCreatedmax: List[Record] =&gt; List[Record] = createdMax match { case Some(maxDate) =&gt; (list =&gt; list.filter(...)) case None =&gt; identity _ // no filter condition, don't do anything } and then you can chain all these functions together (filterById andThen filterByCreatedmax)(inputList) (I didn't compile this so there may be mistakes)
I for some reason this weekend built a pretty neat discord music bot in scala. Not sure why or what I'm gonna do with it, but it's neat. 
I did not use Monix before. I don't know if Monix is not stack-safe. However, I didn't find any method like future.scala's [safeOnComplete](https://static.javadoc.io/com.thoughtworks.future/future_2.11/1.0.1/index.html#com.thoughtworks.future$$ThoughtworksFutureOps). I guess a `StackOverflowError` will be thrown in recursive calls of `Task.runAsync` in Monix, which can be avoid by switching to `future.scala`'s `safeOnComplete`. Another difference: future.scala is threading free and Monix depends on `ExecutionContext`
DeepLearning.scala originally use Scalaz's `Task` for asynchronous programming. However, when we were creating a [recurrent neural network](http://deeplearning.thoughtworks.school/demo/CharRNN.html), it always throws a `StackOverflowError`. The problem got resolved after we switched the asynchronous library to this `future.scala`. I recommend you use this library if you need stack-safety.
&gt;I believe logback.xml has been removed from later versions of Play. It has but they still do a whole bunch of unnecessarily complicated stuff with it. A framework should be leaving it well alone beyond maybe shipping examples. &gt;If you want to see what logback is doing and which file it is using you can set a system property to debug it: -Dlogback.debug=true. Forgot that, very handy. 
That's a pretty broad question. http://www.scala-lang.org/ opens with a reasonable introduction to the most immediate advantages of Scala.
It is a broad question, but the webpage doesn't answer what the resulting benefit above node is. The advantages in that regard are purely subjective, hence why I asked the community.
No, I obviously don't expect the official webpage to answer that question. I just wanted to make clear that I didn't write the question to learn about features of Scala, but to learn about the benefits in comparison to Node.js. &gt;I wanted to ask their respective community what they think the biggest benefits above the alternative are.
Typesafety is the biggest for me, but if the frontend is in ember.js I would say it would make sense to also use javascript in the backend
&gt; ...learn about features of Scala, but to learn about the benefits in comparison to Node.js. Scala 1. can enforce more strict code rules 2. is more safe to use(typesafety etc.) 3. can make the code shorter(high-level functional features) 4. easier to read when there are code rules(strong static typing etc.) 5. can has better performance(JVM, static typing, JIT) -&gt; less machines = less heat and less electricity wasted 6. have a lot of useful concurrency models implemented(future/promise, actors, STM etc.) 7. can use more than one cpu core(no global interpreter lock) 8. can be used effectively for a wide variety of programming domains
I found an article here: https://blog.openshift.com/scala-vs-node-js-as-a-restful-backend-server/ I have used Scala professionally for about 3 years now. I absolutely love it. It's short and easy to write. Compared to java, you will write less than half of the number of lines of code to make something work. You also get functional programming features which, in my experience, made our application more robust. We used the Akka framework -- We've never encountered any multi-threading issues, race condition bugs or any other concurrency issue like we did when we used another OOP language before. There are lots of convenient features which made writing functionality easier -- pattern matching, traits, case classes, shorter syntax, syntactic sugar, etc. Most of the commonly used types in java like strings have an "improved version" in scala even though it's the same class. This was done internally via implicit methods. TLDR; It's easier and more convenient to write code in Scala compared to other OOP languages I've tried such as Java, Objective C, C++, etc. Scala solved lots of problems older OOP languages have. Edit: I've never used node.js Bonus Video: https://www.youtube.com/watch?v=jCPP2A9mHtM :) 
http://naildrivin5.com/scalatour/ If you're looking for a book, probably Programming Scala, by Dean Wampler.
well I would still recommend coursera (at least first two courses) as its not that much time and comes from creator himself and teaches a lot of fundamentals
It's not designed for beginners though, someone new to programming would find that close very difficult if not impossible. 
would you suggest scala for a beginner in the first place? i would not
I'd say that choosing scala being new to programming is itself very difficult way to go through. I started when barely knew Java, didn't understand a thing, then came back after a year and everything went smoothly.
&gt; 1 million lines of code (many of whose will be taken care of by Ember) I would worry about this part. Have you considered using Scala.js ?
Oh nice will try this 
I don't want to scare you away from the choice you've already made, but I didn't have a good experience with Ember. Ember seemed to be based on the same attitude that Ruby on Rails has: "all web apps are fundamentally the same, so we'll provide a universal, one-size-fits-all solution" (and this isn't surprising; [Yehuda Katz](https://github.com/wycats) is one of the principal Ember developers). While that attitude is somewhat appropriate when all your interactions are governed by HTTP, I'm not convinced that it applies to rich client development. I last used Ember I guess 6 years ago. I'm sure a lot has changed, but some of the rough spots are still there. I never liked the verbosity of their [computed properties](https://guides.emberjs.com/v2.14.0/object-model/computed-properties/) implementation, but it doesn't look like they've improved that at all. I ended up switching to Knockout, and found that it was a much better fit for me. KO makes few assumptions about what you're trying to build, and so it exposes things at a lower level. That means that there are fewer guardrails, but you're also less likely to get stuck. Of course, I'm giving you my experience, which is old and very particular to me. Ember is certainly used by plenty of people out in the wild. But compared to things like Angular and React, it's the small fish in the pond. Certainly, when we were hiring front-end devs recently, *everybody* claimed to know Angular. I'm a little surprised that you wouldn't pick one of those "safer" choices. --- As for Scala vs. Node, it's worth mentioning that Node isn't really a language. Sure, you run JS against Node, but there are plenty of languages that compile to JS. You could, for example, write your server code in TypeScript. Or in ClojureScript. Or you could write it in Scala and compile it with Scala.js. It's more correct to compare Node.js to the JVM, rather than comparing Node.js to Scala. I've worked on medium-sized JS codebases, and I'm always wishing for a type system. I've been playing around with TypeScript recently. It's basically "JS + types", which is nice because it sticks to JS semantics and (mostly) syntax. If you do decide to go with Node, you might consider something like TypeScript. I haven't done much on the backend with Node. It has traditionally been very callback-oriented, which was a [special kind of hell](http://callbackhell.com/). But it looks like recent versions of node (I think v7.x and newer) ship with a new enough version of V8 (the JS engine) that they support [`async`/`await`](https://medium.com/@rdsubhas/es6-from-callbacks-to-promises-to-generators-87f1c0cd8f2e) out of the box. All of the Node APIs are still callback-based, but there appears to be [a utility to make these old APIs promise-compatible](http://2ality.com/2017/05/util-promisify.html), which is necessary to use them in `async`/`await` chains. Node requires a strict adherence to asynchronous programming since everything is running in one thread. If that one thread ever blocks, then everything will stall. Node essentially implements cooperative multitasking. This has particular significance if your server would be doing a lot of CPU-intensive work. Node is optimized for I/O-intensive work. If you do intend to do something that's CPU-intensive, you pretty much have to slice your algorithm so that you can run it in "spurts". Alternatively, you could spin up a background process, but the overhead of doing that might be prohibitive. The JS ecosystem right now is very much a turbulent place. Libraries get updated daily, and when you factor in both direct and transitive dependencies, you could easily depend on 150+ third-party libraries. I had a small project that depended explicitly on 11 NPM modules, and after it figured out the transitive dependencies, I ended up with 412 dependencies in total. The community is generally perfectly fine with this situation, so I don't see it changing any time soon. I do sort of wonder how many people actually check all the software licenses for all these projects that they pull into their codebase. I might seem biased against Node.js... and I am. But like I said, I don't have much experience with server-side Node.js... or with server-side Scala for that matter. If I had the choice, I know what I'd pick, but that doesn't mean that my choice is right for you.
Yevgeniy Brikman @ Linkedin had a talk about this: * Talk: https://www.youtube.com/watch?v=b6yLwvNSDck * Slides: https://www.slideshare.net/brikis98/nodejs-vs-play-framework
I would start with Head First Java, in that case. Since Scala builds on Java and Java concepts, you'll benefit from understanding how Java's object hierarchy works.
Yeah, the underscore.io books are good. I'd stay the hell away from slick though.
It has the potential to be such a succinct language though, that it **should** be a good beginner language. 
Why not move to Scala on both fronts? Scala.js on front end.
I tried to use different libraries but ended with Slick. I liked this comparison of DB access libs: https://softwaremill.com/comparing-scala-relational-database-access-libraries/#summary Something really important with Slick is it gives you a full async lib for major DB engines, so you can have the same code base running locally with H2, and running on other environments with MySql.
The question is nonsensical, you're asking us compare a programming language to a more or less an application server. Is there a specific framework on the JVM like play2 or http4s you're looking at to make the comparison "apples and oranges" at least, instead of "apples and orange trees".
I did most of the Coursera courses a few years ago, they aren't good for newcomers to Scala, focus more on the Scala origins and algorithms, rather than Scala features. That being said, 3 years ago, they were not many Scala courses/introductions.
Why worry? - First, that 1 million of lines of 30 years legacy code probably will be re-written to 100k well written and refactored code. - Second, you never ever need to have a project with 100k or 1million, it really smells like your application needs to be fragmented into micro-services. - Finally, I see a lot of people saying "Oh use Scala/Scala.js, is bette than Java, C++" , yes indeed. But what about Node.js for backend and Javascript for frontend? it's a great option, and learn javascript as it is, not write it like java. I work in a company with large amount of pure Javascript code, and is fine. Transpilers like typescript or scala.js is always a hassle. Use code tools, CI, pull requests, and there is no advantage of scala.js over javascript code, I'd say the opposite. You only will start having problems with a large code database written in pure Javascript if people doesn't really know Javascript and don't care to learn JS way, if people only know and care about Scala, then I recommend Scala.js, only in that case.
Good for what? I mean, there is not much to see... `while` loops are rarely used in scala(only recommended for performance critical cases), the usage of postfixops (` (25 to 0 by -1) foreach {`, `if (x&gt;0) Thread sleep 60000` and those single `println`s) is considered to be an anti-pattern.
It honestly looks like a pretty straight-forward port of Python into Scala. I agree with idobai about postfix... it is a great way to end up with hard to figure out compiler errors if you miss a parenthesis in the wrong place!
**Betteridge's law of headlines** Betteridge's law of headlines is one name for an adage that states: "Any headline that ends in a question mark can be answered by the word no." It is named after Ian Betteridge, a British technology journalist, although the principle is much older. As with similar "laws" (e.g., Murphy's law), it is intended as a humorous adage rather than the literal truth. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
I agree with you on the fact that you can write javascript with a certain level of quality. However, at a certain order of complexity, you want a type system to help you catch mistakes. You already know the level of complexity for your application and to me, I would say Scala.js or any other programming language on top of javascript is worth it.
What about this head line "should I learn scala?" 
I've been writing JS full-time for the past few years. I just started learning Scala last fall and am in the early stages of my second Scala project, so I'm still pretty green. My first project was a rewrite of a Node API and I'm currently working on a data analysis tool using http4s, elastic4s, cats, and monix. I have no opinion on Ember as I've only worked with Angular and React. If you do opt for Node, you should absolutely use TypeScript. We've been using TS (or Flow) for all of our new projects and I wouldn't want to go back to untyped JS. There are actually certain aspects of TS/Flow that I wish were available in the Scala type system (e.g. union types). I give the nod to TS over Flow because it seems a little more mature and because some of Flow's error messages are rather ambiguous, making it hard to pin down underlying issues. Some people are mentioning Scala.js. I think this may be a good choice for a Scala dev with no JS experience, but as someone with a fair amount of JS experience, it's not something I would even consider.
This would be a perfect place to have an Akka actor, receiving the Commands you've defined and using the Akka scheduler to schedule jobs instead of blocking the thread. See http://doc.akka.io/docs/akka/2.5.3/scala/scheduler.html Your code have lot of things to improve, you should never block the threads, instead, use async operations, actors, Akka streams , or any other async pattern. Update: I'll put the money where my mouth is, and this is the akka implementation: https://github.com/d1egoaz/akka-pomodoro https://github.com/d1egoaz/akka-pomodoro/blob/master/src/main/scala/com/diegoaz/akka/pomodoro/PomodoroActor.scala
I used to think the same about those problems before working with very-large projects using only Javascript/Node.js, there are tools and libraries which do that type system checking job, and with testing, we really don't miss a bit of static type system. [Joi is an example](https://github.com/hapijs/joi) of a library that offers more security about mentioned mistakes rather than static typing. I love coding in Scala, C#.. also, Scala type system is awesome indeed, and of course we can't achieve that without libraries in Javascript, but that's why I told we need to have interest in Javascript, community, libraries.. really learn as it as, them an argument, such type system because is "safer" will be not valid anymore.
I think for what it is, it's pretty good. My only nitpicks would be to not use the postfix ops, and to give the methods empty parentheses. The convention is that you only use no parentheses methods when they have no side effects.
You make a lot of claims, many of which are non-obvious or go against common experience and current thought, without arguing for these claims. What do you do if your application has a large need for CPU-bound tasks? Will Node.js really be a good platform for that? I mean, I guess one might be able to communicate between processes and try to off-load processing as much as possible when needed, but that is not always feasible due to issues such as overhead of such a solution. And micro-services... while micro-services can be very useful, they can also increase certain other costs and increase complexity in other regards. And they are definitely not a silver bullet. And the incredibly quick dismissal of static typing, and also the implication that Scala is only about static typing. I find your comments really, really strange. Do you believe or have confidence in what you write yourself?
Those are not postfixops, it is infix notation. The printlns are neither, just a regular method call (although it should use parentheses here as it is a side effecting method and not an accessor) 
Yes, I do, that's why I'm caring only to reply author questions instead write personal criticism. Now replying author interests: &gt; What do you do if your application has a large need for CPU-bound tasks? Will Node.js really be a good platform for that? Maybe, if don't then you could write these specific microservices in another platform indeed. Remember author said it's a VB6 codebase and most will be rewrite in Ember, so it's unlikely to it happens &gt; And micro-services... while micro-services can be very useful, they can also increase certain other costs and increase complexity in other regards. And they are definitely not a silver bullet. It's a personal opinion. You said yours, mine is: no one 1 million monolith codelines application is sane. &gt; And the incredibly quick dismissal of static typing, and also the implication that Scala is only about static typing. You assumed that. I program in Haskell and Scala.. I'm very familiar with rich typesystem. Still, the argument about this being safer than Javascript typesystem, only by itself, is very weak. You can guess you are more capable than me and say my comments are "really really strange" and doubt about my skills, but you could also suppose I am more capable than you, have more experience with Node.js and Scala than you, and that's why I can have such confidence and maybe sounds strange *for you*. I only suggested to continue with Javascript because he already said he will have a large codebase in Ember, and with tools you achieve Scala.js goodies. Being defensive and attacking me personally only shows how lack of confidence do you have in yourself. 
... your comment here is not very coherent. And you misrepresent what I wrote, and I have difficulty determining whether it is because you didn't understand it, didn't care to understand it, or whether you misrepresent it on purpose, or some mixture of the previous possibilities. And many claims without arguments for them again. This comment is really, really strange as well.
This would be true in many cases but Pomodoro is by its very nature a blocking operation!
A few things: 1 The `object Foo extends App { ... }` style is not recommended any more, use an explicit `main` method instead: object Pomodoro { def main(args: Array[String]): Unit = { ... } } 2 You don't need to manually insert escaped newlines into strings, you can use triple-quoted strings which will preserve newlines: println("""Pomodoro ====== """) 3 This is not Scala-specific, but since you have two similar blocks (work and pause), I would try to extract out the common portion into a method and call that twice. This will also factor out the magic numbers (60000) in both the blocks. 4 You can write `25 to 0 by -1 foreach { x =&gt; ... }`. Each of `to`, `by`, and `foreach` are methods which take a single argument and lightweight syntax method calls like this are left-associative, so it's the same as writing `25.to(0).by(-1).foreach { x =&gt; ... }`. 5 If any method has return type `Unit`, define and call it with parentheses to mark it as side-effecting.
I think in general it's good. If nitpicking I would say: 1. Standard Scala style is that you should include empty parentheses for methods with side effects, and omit them only for accessors. So readLine() instead of readline 2. You may want to look up [String interpolation in Scala](http://docs.scala-lang.org/overviews/core/string-interpolation.html#the-s-string-interpolator) as it's a little more elegant than appending strings in a print statement. 3. Postfix ops (Thread sleep 60000) are generally considered poor style. I think there are exceptions like BigDecimal (postfix ops let you do a x b which is a hell of a lot more readable than Java's a.multiply(b)), but I'd say Thread.sleep(60000) is more understandable. 4. Those methods can be private. Obviously not important in a single-class app but good style to restrict access as much as possible. 5. People might disagree with me here, but foreach is very useful when traversing the elements of a collection, but I think when you're simply using it as a loop counter it's less readable than the standard "for". So (25 to 0 by -1) foreach { x =&gt; print("\r" + x + " minutes of work left ") if (x&gt;0) Thread sleep 60000 } made me have to stop and think for a second, but for(x &lt;- 25 to 0 by -1) { print("\r" + x + " minutes of work left ") if (x&gt;0) Thread sleep 60000 } In fact, you don't need that if condition if you use "until" instead of "to" as this excludes the end of the range. So it can be simplified as for(x &lt;- 25 until 0 by -1) { print(s"\r $x minutes of work left ") Thread.sleep(60000) }
Another valid criticism of Scala.js is that it's slow. On the frontend, speed of iteration is critical. During a typical work session, I'm constantly iterating on my frontend components, reloading my app, and checking the outcome. Scala.js makes this a rather slow process--usually about 5 to 6 seconds for a compile, then about another 5--10s for a Webpack rebundle and reload. Things can be better--in a BuckleScript project my compile time is less than a second, and Rollup (an alternative to Webpack) bundle time is also about a second.
I'm surprised to hear that postfix ops is poor style. I'm pretty used to using it in the realm of durations, e.g. 30 seconds vs. 30.seconds. To me that's more legible but I'd like to hear from you on this.
You can use FSM, so you can be in just a operation(behavior) at a time. http://doc.akka.io/docs/akka/current/scala/fsm.html
butwhy.gif Akka's supposed to simplify multithreaded systems, but using it in a single threaded application that's *supposed* to block is exactly the opposite. Not to mention massive overkill IMHO.
We used beanstalk at work, it really sucked because it takes so long to deploy something and fiddling around with .ebextensions and their NGINX instance is a PITA. Containerizing our applications and moving over to ECS has been a blast, deploy times have significantly lowered. Apparently using Weave with ECS can bring even more value to the table. I don't mean to bash this post because its very well laid out but please have a look at ECS, its much better. 
Hey Justin, http://justinhj.github.io/2017/07/20/hacker-news-api-1.html doesn't seem to work (linked by your article)
I think "benefits of scala" is as stated before a broad term and implicitly includes the ecosystem of both languages. It's not apples and orange trees but fruit basket and another fruit basket.
Did they post a screenshot of a gist or is that something twitter did?
I posted a screenshot 
Yeah we use ECS and I feel much the same way. Was just curious since I haven't looked into beanstalk with docker before and so wasn't sure if there was something I was missing 
I suggest hanging out in typelevel/cats on gitter. It's an easy way to run into new stuff, and friendly people who can help you learn.
Ill check it out thanks for the suggestion
What did you use? A Java library or implement the APIs yourself? Funny enough, this weekend I started making a scala library for discord bots built on top of Akka. My end goal is for the user to be able to represent their bot as an actor, but I still have a lot of work before it's ready for use by anyone but me at this point. 
LOL Just implemented it on Akka. https://github.com/d1egoaz/akka-pomodoro/blob/master/src/main/scala/com/diegoaz/akka/pomodoro/PomodoroActor.scala It was fun :P I just want to say, blocking the threads is never acceptable, what's more, here there are not even IO involved. Even if you are running a process in your machine you shouldn't block the threads, that's why Task Executors were created. People shouldn't write anti-patterns, there are better abstractions for these scnearios. https://github.com/alexandru/scala-best-practices/blob/master/sections/4-concurrency-parallelism.md#45-should-not-block 
Oops it should be http://justinhj.github.io/2017/07/26/hacker-news-api-1.html Will fix it
&gt; 1 million lines of code (many of whose will be taken care of by Ember) If you have a chance to start from scratch, at this scale I would highly recommend using a statically typed language instead of Javascript. Even Typescript will do better with proper discipline, regardless of all its faults. In addition to that, I would highly recommend avoiding what I can only describe as classical Javascript code style characterized by highly dynamic code, liberal usage of huge batteries-included frameworks, etc. I'm not super familiar with Ember, but to me it seems like everything you shouldn't be doing in such a huge project. Pick small framework / libraries with a small conceptual footprint. React would be one of the good choices for the UI layer if you want to stay on the beaten path (of course you do, on this scale).
please use `30.seconds`. While for durations most people are familiar with it, in other situations this will not be the case, so better avoid it. Also, `30.seconds` is as readable as `30 seconds`, especially if there is more whitespace around those, no?
I like it! I love that style guide, we use it internally. The style guide's convention distinguishes between SHOULD and MUST though - the point being that the SHOULD rules have exceptions at times. I think this is one of those times. I can understand what he wrote in 5 seconds. Parsing an Akka based implementation takes longer, not to mention the added dependency and requiring SBT. For bigger projects, absolutely, but you gain nothing from the abstraction and you lose readability. TL;DR most of the time you're right but here KISS principles &gt; avoiding blocking, especially given that the whole point of this code is to do nothing for some time.
 readLine startWork startPause https://github.com/databricks/scala-style-guide#parentheses http://docs.scala-lang.org/style/method-invocation.html#arity-0 (25 to 0 by -1) foreach { x =&gt; http://docs.scala-lang.org/style/method-invocation.html#infix-notation print("\r" + x + " minutes of work left ") http://docs.scala-lang.org/overviews/core/string-interpolation.html if (x&gt;0) Thread sleep 60000 https://github.com/databricks/scala-style-guide#curly
I think people vary on this. Personally I wouldn't see an issue with your example, or for examples involving common mathematical operators (+, - on numeric classes for example). For anything more complex though, postfix ops are more trouble than they're worth and there's something to be said for applying that rule across the board.
I see a lot of Scala jobs at financial companies, the remaining ones are general webdev leaning towards backend and/or bigdata. I think the only areas which Scala is not that suited for is where C or C++ are used heavily (Embedded systems, 3D graphics and some other applications where single machine performance is very important).
The choice itself looks weird. Usually you don't choose between buying a car or a bike, you choose between different car models or bike models. If you're a web studio (fast development, more or less similar small projects), then your options are Node.js, PHP, Python, etc. If you're a long-term enterprise (reliable code, project maintenance, data mining), then it's Scala, Java, C#, etc.
`Thread sleep 60000` is an infix op, perfectly fine. `private[this]` is better to reach for than `private`.
&gt;infix op D'oh! Sleepy when I wrote this. Yes it's infix not postfix &gt;perfectly fine Disagree. Scala docs suggest using infix for symbolically named methods (a + b) but to avoid for almost all alphabetic methods (names mkString ","). I would definitely prefer Thread.sleep(60000) to Thread sleep 60000
If you want to compare them anyway: * Scala requires higher programming skill -&gt; there are less devs available to hire, they are better programmers in average (but you can hire Java devs and easily convert them) * Scala has better type system and ecosystem (easier long-term maintenance, no ridiculous things like left-pad) * Scala has issues with binary compatibility between major versions
As a scala fan, I think haskell is better suited as a first language. It's smaller, simpler, and with less crazy syntax. And now there's actual good learning material for it
I think that in case of regular references like books or articles these 2 are very advanced, and I cannot really think of some other book from the top of my head. There is the "Lambda Conf Ladder": http://lambdaconf.us/downloads/documents/lambdaconf_slfp.pdf which might help you with setting a direction for further learning. Also suggesting to take a look at Bartosz Milewski's YouTube channel: https://www.youtube.com/user/DrBartosz and his 2 series about Category Theory, also his blog is very good resource: https://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/ (start here) Bartosz Milewski's resources are not Scala-specific though. My suggestion would be to start either contributing to some project or do your own project where you can test those skills in practice. Any Typelevel project probably would be a good choice
Hmm... that's interesting, though it does scratch a different itch because it's not multi-paradigm like Scala. That said, what resources do you think quality as "good learning material" for Haskell?
Scala specific isn't a big deal. Those look cool though. Thanks for the links and ideas
Haskell Programming from First Principles is one of the best programming books I've ever seen. Stack simplifies project setup. It's a good combo
Sorry, but aren't the infix and postfix notations 'belong' to the `language.postfixOps` feature?
I haven't seen people use private[this] very often. I'm curious why you prefer it.
Well if you look at it in an emperical way (there have been studies with statistics and whatnot), there isn't any concrete proof that strongly typed languages are better then the dynamic ones when measured against bugs or code quality. I think it really comes down to how people think. I have noticed that certain types of people really gravitate well to strong typing, others not so much.
It restricts accessing the field from other instances, which is usually what you want; I've heard it also leads to more efficient bytecode.
This one is a great book: https://www.manning.com/books/functional-and-reactive-domain-modeling 
One glaring thing missing here is that you should avoid circular dependencies between files. Circular dependencies basically double (or worse) the expected time taken for incremental compilation. - If your code is laid out linearly A -&gt; B -&gt; C -&gt; D -&gt; E, touch a random file and (on average) half the files will be downstream and need to recompile. - If you code is circular A -&gt; B -&gt; C -&gt; D -&gt; E -&gt; A, every file is downstream of every other file and every file needs to recompile every time. Avoiding circular dependencies won't make individual files compile faster, but it'll definitely make the incremental compiler have to work less hard. Now, circular dependencies are fiendishly hard to spot while coding and during code review, since they are a global property whereas during coding/review you look at local snippets of code. But spotting them is easy for compiler plugins like https://github.com/lihaoyi/acyclic. Lastly, banning circular dependencies automatically ends up forcing you to use dependency-injections/parametrization/coding-against-interfaces and many other "good practices", simply by blocking you from doing the bad thing (concretely depending on something that concretely depends on you). So while it forces you to change your code, arguably the changes are for the better
When you write "strongly typed", do you mean statically typed? For if so, I think I have read a variety of studies, some saying that there is no proof of any difference regarding bugs or code quality (using various ways to define/measure/discover bugs as well as code quality), and some saying there was. I think it is one of those things that are really difficult to measure, though I also believe that statically typed languages tends to be more used in those domains where it gives more value. For instance, if you have an application where it is difficult, time-consuming and/or costly to update it with bug fixes later, such as most embedded hardware, static typing is more of an advantage (there are two arguments against static type systems' advantages I see here regarding this argument, namely partly that static typing can make it easier for compilers to optimize, and partly that dynamically typed languages can be statically analyzed as well, for instance using whole-program analysis and the like (I don't know if dynamically typed languages are easier or more difficult to analyze statically, I could imagine they are typically more difficult, though I also think it partially depends on the analysis approaches used as well as on other factors such as the design/semantics of the language as well as the use cases optimized for)). One can also say that while studies and statistics is one approach (and an approach where I think it for this case is really, really difficult to get a meaningful result - statistics is not a silver bullet, and in my personal and not humble opinion considerably overused or misapplied these days, both for popular and in more scientific circles), another is common experience, despite the considerable and obvious failings that common experience has. This I personally believe should at most be used as a smoke test for things, since common experience can be off as well as be manipulated and warped in a variety of ways, even in spite of the various tactics, strategies, heuristics and the like that can be employed to mitigate at least somewhat and for at least some of these issues. Personal experience is also one approach, trying out both (and this approach is one I recommend), though that can likewise be problematic in various ways, and it is also time-consuming and possibly costly in other ways as well (though also with possible benefits for some cases) if one has to try out both dynamically typed languages and statically typed languages a lot. And it also requires one to be at least in some ways honest with one self, to have good judgment in some regards, to be able to evaluate, etc., etc. And there are other factors there as well. A third approach is to try to deduce, ponder, investigate, argue (with one self, not with others, that is a whole different corner), etc., which tends to be prohibitively difficult for this case, due to two reasons AFAICT: First, it is far too complex a topic to easily approach it, and second, it is not as such easy to reliably test one's answer for this case. In regards to gravitating towards static typing or dynamic typing, I am not convinced things are really that way, though you may be right. I have read that there are people that for instance previously swore to Lisp, but later after having worked a lot with an ML-like language, ended up preferring ML-like languages (though I vaguely recall having read about the other way around as well). Factors that come into this are focuses and specializations for one's career, how much time is available for oneself, past experience, the use cases one works with, how much investment is required, how skilled and which skills you excel at, etc. For instance, regarding use cases, if the overall cost of bugs is really low, performance is a non-issue, and the size of the program will always be small and thus not an issue as such regarding maintainability, dynamic typing is likely strongly preferable. Another is if you are creating a programming language to help solve a challenge or problem, and you do not have the time, (domain) knowledge (for instance in case of a new domain) and/or the resources to create a statically typed language that is also a good fit for your challenge or problem, dynamic typing may be the better choice, since it is in some significant regards easier to implement for instance an interpreter for. A third example is for domains where (current and/or mainstream?) static typing is not much of a help. I believe R is popular with non-programmer statisticians, and static typing may not be of much use there unless certain type systems with some good libraries are available there (which requires that it is possible to create a good library for that case in that type systems). For this latter case, I do not know if for instance some of the [Scala libraries mentioned here](https://stackoverflow.com/questions/8760925/is-there-a-good-math-stats-library-for-scala) fits that bill well or not.
I agree that circular dependencies may be incremental compiler killer (may cause multiple cycles of compilation and IIRC 4th cycle is always a full compilation by default in zinc - regardless how much files needs to be recompiled). However if you providing returns types for all non-private members make this situation much better (since APIs are not change that often). However the biggest problem will be still circular dependencies on the type class/trait level (e.g. when self type is used too often).
Interesting, I didn't know about the more efficient byte code. 
On the topic of traits with self-types, I've found that if you ban circular dependencies between files, you basically do not need them. After all, if you don't need circular dependencies, normal inheritance works fine. There's some nuance in that self-types are "private", but I personally have never had a solid use for that 
The [GitHub page for Scalaz](https://github.com/scalaz/scalaz) also contains a number of links to learning resources.
If you have completed both of those, come help out with Open Source! I am one of the many maintainers of Scalaz, we are always looking to improve with Pull Requests to fix bugs, add features or examples/documentation. Other great projects would be FS2, Ammonite, Scodec to look into contributing to. 
How do you test the time saved for this. I've found it hard. If I do a `sbt clean compile` its no difference. But thats obviously testing the wrong thing, not testing incremental compilation. I cant think of a good way that isnt subjective.
Here are a couple additional docs around how we do distributed neural net training: https://deeplearning4j.org/spark https://deeplearning4j.org/distributed
You can watch all the latest conference talks here :) https://www.reddit.com/r/ScalaConferenceVideos/ Lots of great topics Shapeless, Akka Streams, Scalaz, Cats, Monix ... you name it :)
Where did it go! :(
Didn't know that existed. Thanks
Heh interesting idea. Projects kind of feel intimidating. Ill have to look around and see whats there to do. Thanks
One option is to define a helper in a companion object: case class Vector2D(x: Double, y: Double) object Vector2D { def polar(r: Double, angle: Double) = Vector2D(r * math.cos(angle), r * math.sin(angle)) } val v1 = Vector2D(10, 5) val v2 = Vector2D.polar(5, math.Pi) ~~The auxiliary constructor approach can have better type-safety with the added cost of some boilerplate. For example, if you define~~ case class Mag(val value: Double) extends AnyVal case class Angle(val value: Double) extends AnyVal ~~and use the above classes for the other constructor, then you have to explicitly construct a `Mag` and an `Angle`, which will help prevent accidentally passing `r` and `theta` to the x-y constructor.~~ Edit: elaborated on the wrapper classes, fixed some errors Edit 2: See the comment about type erasure by u/Jasper-M
Have you considered a companion object method which has a named factory methods for the parameters you want? Vector.byCartesian(x,y) Vector.byPolar(r, theta)
Spicy
This was the approach I was thinking of (creating new data types for magnitude and angle) using auxiliary constructors. The use of a companion object is more illuminating now. I'm new to the language and the only use for singleton objects I intuitively saw was aggregating a suite of methods for a common set of tasks, and providing an api for importing into other code. Thanks.
The [model zoo](https://deeplearning4j.org/model-zoo) seems pretty cool.
Sounds like best of both worlds, we will definitely consider it.
Great talk, thank you.
Hey my first question today, imagine you have something like this: object UpdateStuff { def update( futreInData: Future[List[InputData]], getInfo: String =&gt; Future[Option[String]], doAWithId: UUID =&gt; Future[Unit], doBWithId: UUID =&gt; Future[Unit], doCWithId: UUID =&gt; Future[Unit]): Future[Unit] = { // chain these functions in any way e.g. for { inData &lt;- futreInData doneA &lt;- Future.traverse(inData)(data =&gt; doAWithId(data.id)) .... } yield () } } How would you go about testing? Would you mock all the functions to see, if they get called the appropriate times? Would you fake implement them? Would you only integration test something like that? Or separate as much of the logic as possible and not test the function calls?
And another question: If I have some repository speaking to an external resource: abstract class UserRepo[M[_]: Monad] { def getUser(id: Id): M[User] def updateUser(user: User): M[User] } And I want to enforce a throttling rule, e.g. only allowing one request per second when I implement the Repo using `Future`, how would I go about that? If I would not implement the Repo for generic Monad, I could see how I could in the logic take care of just executing one request per second, but how can I enforce that in the future implementation for an abstract repo?
I don't understand what you're asking? If it's about the `Future` I'd use `Await` where appropriate in tests. I probably wouldn't test the chaining; I'd try to make the functions correct by construction (in particular if the functions need to be called in a certain order, I'd have types that represented this). There's no value in repeating just that we call these functions in this order. But really I probably wouldn't write a function like this; too much passing functions around makes code difficult to read or debug, particularly when those functions are anonymous.
It seems to be that the repo itself knows more about what kind of throttling is appropriate, so the right place for the throttling is in the implementation class, where `M` is concrete. Your other option would be to define a typeclass (analogous to `MonadReader` etc.) for the effect that you want, and use that as your constraint.
How does one reverse a route? Their generated route: `GET /cat/findCutestCat` `&lt;a href={router.cats.findCutestCat()}&gt;Cutest Cat&lt;/a&gt;`? In Play reverse routing is built-in. I guess if the back end is handing JSON off to the front end then the client can deal with reverse routing, though it likely won't be generated by the back end, which is a problem.
The second approach with the value classes will not work. Both constructors still have the same erasure: `(x: Double, y: Double)Vector2D`.
Wiro is not an MVC framework. It's a tiny library - it just generates the routes from the controllers. At buildo we generate the client code from wiro controllers using metarpheus (https://links.buildo.io/metarpheus-post). It can be used to build javascript apis and models. That's how we solve the problem.
I'm not sure if I like this approach. Part of the nice thing about routes is that you can use them to enforce a contract between the api and the caller. Having "magic" route generation via macros can be hard to prove out via a contract, and often leads to bugs. Its the same reason that I like statically typed swagger over annotation-based generators of the same. Had I broken contract with a generator, it would be hard to catch said break until you went to integrate the two. Using statistically typed swagger routes helped enforce that I was designing all of this correctly.
We use Wiro (and https://github.com/buildo/metarpheus) to generate both server and client code. Our intermediate language (our contract) is Scala. If you generate both client and server code from swagger you are actually using a very similar approach :) (s/swagger/scala/).
Infix should also, by convention, only be used for (purely) functional operations. Thread.sleep is/has side-effects, and shouldn't be written this way.
Actually, we are also using metarpheus to generate swagger from scala traits. We mainly use swagger to document the routes... but it can be useful also for other purposes.
https://failex.blogspot.com/2017/04/the-high-cost-of-anyval-subclasses.html?m=1
since it uses autowire, I bet it's possible to just use an autowire client when using scalajs right?
Yes. We've been considering working on that. However, we don't actively use scalajs (that's why we don't have a client yet). we currently have two clients: - javascript client (using metarpheus, already mentioned) - scala client (using autowire)
btw I talked to lihaoyi and he isn't too interested atm to work on autowire, so I created a fork and accepted some of the pullrequests here: https://github.com/daxten/autowire if you need some changes / want some PR's to be taken care of feel free to create an issue there
In a sibling comment you mention that you don't use Scala.js. What's the point of a typed API when you throw it away on the client? I mean, sure, it sounds like client router is generated, but invoking it is unsafe unless you use Scala.js
We have a scala client for wiro. We use it as an RPC framework. In that case the typed api is useful. Also, we use https://github.com/buildo/metarpheus-io-ts to generate a typescript interface for the routes.
I think this is the right use case. When going from browser -&gt; server I prefer non-magic routes also, but for backend service communications using the language as the contract is fantastic.
There are no postfix operators in this code. What bothers the eye is that it's an infix application with a static object on the left-hand side, which feels quite unidiomatic/unnatural. At least I have not often seen this style being used. On the other hand, IMO infix operators even with non-symbolic names are often fine in other contexts, as in `ls map f`.
I think I would prefer: `(25 to 0 by -1).foreach { x =&gt;` ...actually. First you define a pure value (a `Range`), and then you call your side-effectful `.foreach` on that. I think that makes the most sense. although it may be a littlebit down to personal preference. Either way works.
This will be my goto library for doing http from now on. Thanks for making it!
I like it. I would be interested in knowing: 1) How does it interpret Seq values? 2) If I wanted to implement throttling on a domain-specific level, via some mechanism in Akka-Streams / Akka-HTTP, could that be done through the akka-http backend? Or would that require writing a new backend? 
As others have said, Scala is "general purpose" and can be used for anything from compilers to websites (both client &amp; server) to backend-servers to data-pipelines to desktop-applications and many more things. Perhaps worth mentioning are the things that Scala *isn't* good at: - Command-line tools, due to the 300-500ms JVM/classloading initialization cost which is noticeably annoying - Low-level/OS/embedded code, due to the heavyweight JVM runtime necessary to run Scala - Hard-real-time systems: due to the JVM's garbage collector resulting in pauses, and the JIT resulting in generally-good-but-unpredictable performance - Memory-constrained environments; the JVM is a bit of a memory hog, and it's worse running Scala code than Java, because Scala code tends to box/allocate like crazy - Super-performance-critical software: you can't really control memory layouts in Scala, no flat arrays of structs, no choosing a custom memory allocator to fit your program behavior, no easy use of CPU intrinsics, no "memcpy" operator to quickly serialize/copy large contiguous data structures. Scala can get pretty fast, but to reach the ultimate levels of performance, C/C++/Fortran are still king
1) `Seq`s in the host part are unwrapped to a (sub)domain list. In path, to path components. So e.g. `uri".../a/${List("b", "c")}"` becomes `".../a/b/c"`. In queries, `Seq`s of pairs are unwrapped to key-value pairs, otherwise to no-value parameters. 2) Throttling - for example, limiting the number of requests per host per second?
Nope. Only the postfix syntax comes from there. The infix syntax, also called the lightweight method call syntax, is part of standard Scala. It's actually heavily used in the Scala source code itself, take a look.
This is a great remark. One thing worth mentioning though is that I would expect the overhead of acyclic to overcome the benefit of incremental compilation caused by cycles given that the majority of incremental changes are local. The perfect solution would be to have a chip in the brain that helps you write acyclic code out of the box, without needing to enforce it with a compiler plugin. But perfection doesn't exist.
Sorry... I conflated postfix and infix in a rush to comment. :-) I still hold that infix operators should be avoided in most cases.
Nice try, Alwin. 
Wow, that was a huge mistake...
Nope, it's what lets Scala unify method calls and operators like `+`, `-`, etc. They've kept the syntax as uniform as possible by not distinguishing between operators (symbols) and methods (alphanumeric names). Anyway, it's nice to be able to write stuff like `xs zip ys take 5 map { ... }`.
Thanks for creating the subreddit and uploading all those videos. It's really nice to have all the scala conference videos in one spot :)
neato
Another similar possibility is to have 2 types, Cartesian and Polar. So it would be case class Cartesian(x: Double, y: Double) case class Polar(x: Double, y: Double) Then define your two constructors. That makes a separate object though for the two types. I wish you could make an object with more than 1 variable with value semantics. 
IntelliJ 2017.2. I find it better than previous versions. 
Akka-http is the successor of spray, including the testkit dsl. Is that an option for you?
http4s client or sttp. Both are very easy to use. Here is an example client similar to yours: https://gist.github.com/dbousamra/dd2833fd2428bd1fe70e8b309f648103
As calippolo said, we're not throwing them away on the client. We typically write clients in React.js and depending on the project we have either runtime typechecking (in development) or static typechecking in place. For runtime typechecking we use tcomb (https://github.com/gcanti/tcomb) and tcomb types are auto-generated (using metarpheus, more about it here: https://blog.buildo.io/metarpheus-a-custom-approach-to-api-contracts-f340a6792d43). tcomb types are also used as React prop-types. In projects where we use TypeScript, we use TS models for static typechecking, but we still validate them also at runtime to make sure the API respects the contract (it's a bit overzealous, but hey, it's free :) ) The cool part about all this is that Scala is the source of truth, and everything else is generated from it, all the way down to the HTTP client used from the browser. Also, we can already generate a swagger model and potentially we could also generate a Scala.js client or whatever else we need.
This configuration is worked in my project (play 2.5.10) -Dlogback.configurationFile=logback.development.xml 
Interesting, sounds like a lot of moving parts. Any reason you don't just go all-in on Scala and use Scala.js for the front end?
Found this [series](https://madusudanan.com/tags/#Scala) of blog posts through [scala enthusiasts](https://www.scala-enthusiasts.com/scala-resources/). Looks good, but only beginner level topics covered, might be a good starting point.
well I think what he meant is: if you control the server, but not the client, magic is bad if you control the server and the client, magic is ok if you control the client but not the server, magis is bad. actually no matter where the client lives (scala-js browser or node, scala whatever) does not matter I think, however it's really hard to keep it in sink and **if you don't have a public api I don't think magic is bad, it is just bad if you have a public api.**
I've been using Vim with Ensime-Vim plugin for the last 3 months and the experience, after the learning curve has been excellent. Especially a long-time VIM user.
If you write public api, you have to support different clients. Different clients have different needs. RPC is not the best in this case, GraphQL or REST would be better. It's not a matter or magic or not. Writing things manually is bad. It's error prone and time consuming. 
I don't agree with the error-mapping approach here (amongst other things). A type-class based approach, where you map an error to an `HttpError` means there is a canonical way to go from an error to an http error. This is not the case. Sometimes, `IllegalArgumentException` should produce a `400` once it hits the route, and sometimes it should produce a `500`, in case it comes from some internal bug. That's the value of the route layer, it knows how to translate your domain correctly into stuff your client understands. Otherwise, you just expose your domain to the client directly - and then _they_ have to deal with incorrectly-supplied `400` responses.
Good question. Part of the motives are circumstantial: we already had a skilled team with a solid knowledge of JS and React way before Scala.js was even a thing. Also, Scala.js has a few downsides regarding ease of integration with the existing ecosystem: it's doable, but it require some non-trivial effort and glue code for js-facades. Also the size of the bundle is always going to be significantly greater than a JS-only app and this matters a lot nowadays. We surely felt the need for more type-safety in our front-ends, and we found that TypeScript is a very reasonable compromise. It has a decent type-system, a great tooling support and it's retro-compatible with JS.
We never found this to be a problem is you're in control of the client. We tend to focus on the semantics of errors, instead of the details of their encoding. As long as the semantics are clear and shared between client and server, a 400 or a 500 doesn't make much of a difference. Also you can be as granular as you want with your errors and have different types for different semantics. I would argue that `IllegalArgumentException` is not a good error to map over, and I would handle the two scenarios (user error -&gt; 400, internal bug -&gt; 500) in the business logic, exposing then two different and specific errors. Of course, as @calippolo said in other comments, if you are not controlling the client (e.g. if your product is the API) this may be not be a good approach.
&gt; Also the size of the bundle is always going to be significantly greater than a JS-only app and this matters a lot nowadays Which is ironic given how gigantic a full blown React app tends to be ;-) Another perspective is: scrap everything and just use Scala.js, there are lightweight React-like libraries in the Scala.js ecosystem that deliver the same functionality. Obviously not viable if you're a mixed shop with Scala and JS devs, but given how typed the back end is it seems only natural to go all-in on types on the front end as well.
Why would you assume you control the client? It's an API, the world ends there. Assuming you control the client just makes it impossible to ever refactor. The point of an API is that it's modular and reusable. And if you split `IllegalArgumentException` up into `external cause` and `internal cause`, then you've made your services aware of the outside world, which again I don't think you should do. Services should churn data through business logic and present a result to another layer, which interprets it for the 'outside', for a given supplied context. If all pieces (services (which == route here) and frontend) must understand all the domain all the time then you might as well just have a giant node app serving JS to the front end. Why not go further and expose your DB to the external world via auto-generated routes like 'selectAllCats' and 'selectCatsJoinDogsOnAgeEquality' and be done with the service layer? I think that sounded angry, sorry :)
[sttp](https://github.com/softwaremill/sttp)
Yep, since Akka Http is Spray (2.0) , so that should be very familiar and an easy transition. But if all you need is making http requests, akka http may be a little overkill. [sttp](https://github.com/softwaremill/sttp) looks much nicer for that.
&gt; Which is ironic given how gigantic a full blown React app tends to be ;-) Scala.js + React is not going to be smaller than just React, I would argue. Anyway, yes we're kinda of a mixed shop, and Typescript was a good fit for us. I've nothing against Scala.js (I'm using it quite a bit for Scala tooling) but it simply wasn't appropriate in our scenario :)
&gt; I think that sounded angry, sorry :) Hehe, no worries, it's easy to get passionate about these stuff. :D &gt; Why would you assume you control the client? It's an API, the world ends there. Because we do. That's the main underlying assumption there, we develop product that involve a front-end and a backend, so our world doesn't end there. In our specific case refactoring became much simpler because you don't care about the routers, the front-end models or the front-end HTTP client. The refactoring of a full-stack feature starts on Scala and the rest simply follows. Most of it is autogenerated and TypeScript points out what remains to be fixed in the front-end. It's a very low-overhead process that lets you focus on the feature at hand. I understand the other points you're making and I'm on board with the spirit of it: abstractions are useful and they should stay. That said, the *specific* abstraction of a router wasn't paying its cost to us, so we automated it away. We weren't doing anything clever in the router, just parsing requests and mapping to the correspondent controller method. What we care about is consistency and maintainability, and scrapping routers away helped us :)
Obviously, operators are good for infix usage but it's not true about alphanumeric methods/functions - they'll make the code harder to read and will make the compilation time worse.
&gt; And if you split IllegalArgumentException up into external cause and internal cause, then you've made your services aware of the outside world, which again I don't think you should do. Services should churn data through business logic and present a result to another layer, which interprets it for the 'outside', for a given supplied context. &gt; if you distinguish between 400 and 500 you're already doing that. Giving proper names to things just helps making the code more understandable. Errors are part of the contract in any case.
&gt; Scala.js + React is not going to be smaller than just React, I would argue Sure, I'm saying ditch React entirely, there are plenty of FP oriented React-like libraries in the Scala.js ecosystem. [Here's one](https://github.com/OlivierBlanvillain/monadic-html) for example, there are others, even direct ports of React to Scala.js. I'd even go so far as to say that implementing powerful/complex JS libraries in pure Scala.js will generally lead to smaller binaries (since the overhead of Scala compiler embedded in the client pays for itself as the application grows).
GUI programming in entirely possible without any OOP. I wrote a proof of concept library, that fully works with almost every functionality an OOP GUI provides (minus keyboard inputs, i haven't gotten around to that yet). I found it very challenging to come up with a mindset to allow me to do this. It's obviously a bit slower, since the whole thing is immutable, and if you scroll a bit, you need to rebuild the whole tree, but even without any optimization, it can do it under 15ms on a thinkpad x220, and that includes redoing the whole layout and reinitialize a bunch of components.. It can avoid a lot of bugs and confusions (especially about lifetime of widgets). Although it's not the most convenient to use, and not totally type safe, it has much less pitfalls than your typical OOP Gui library. Being type safe is pretty much impossible i think, because even you type the whole tree, HList style, you can't realistically model and handle all the possible changes. But obviously you can match the runtime types and do it pretty safely. Although, i can't say i'm satisfied with it, because in the end, it has it's own problems. For example: you have to use Ids for each widget, because you obviously can't identify widgets by reference. And coming from that, for the internal working of widgets, you can't really guarantee that the necessary elements are really there.
I like the metrics we get in Beanstalk....all logs in one place... Easier scaling / load balancing...
I like the metrics we get in Beanstalk....all logs in one place... Easier scaling / load balancing...
Make a GUI based only on functional programming is hard because it is necessary lots of state. Every widget is a state and needs to be remembered and known by all event handlers during the application run-time and every widget needs to have internal states, the windows needs to remember its size, the position from the top and the widgets contained inside of it. The widgets also needs to remember the event handlers associated with it. Even if we didn't know about Object Oriented Programming (OOP) or we were allowed only to use functional programming and imperative programming. We could make objects with functions and there are situation where encapsulated state is a necessary evil and there is no other way around. Even the Hakell library GUI libraries are wrappers around OO GUI libraries like GTK. Example: This record of functions Counter can simulate an object and the function makeCounter simulate a constructor. @ case class Counter( get: () =&gt; Int, increment: () =&gt; Unit, decrement: () =&gt; Unit ) @ def makeCounter() = { var c = 0 // Encaspsulated state Counter( get = () =&gt; c, increment = () =&gt; { c = c + 1 }, decrement = () =&gt; { c = c - 1 } ) } @ val c1 = makeCounter() c1: Counter = Counter(&lt;function0&gt;, &lt;function0&gt;, &lt;function0&gt;) @ @ c1.get() res3: Int = 0 @ @ c1.inc increment @ c1.increment() @ c1.increment() @ c1.get() res6: Int = 2 @ @ c1.decrement() ; c1.get() res7_1: Int = 1 @ c1.decrement() ; c1.get() res8_1: Int = 0 @ c1.decrement() ; c1.get() res9_1: Int = -1 @ c1.decrement() ; c1.get() res10_1: Int = -2 @ @ val c2 = makeCounter() c2: Counter = Counter(&lt;function0&gt;, &lt;function0&gt;, &lt;function0&gt;) @ c2.get() res14: Int = 0 @ c2.inc increment @ c2.increment() @ c2.get() res16: Int = 1 @ 
Code readability is really (literally) in the eye of the beholder, you find things easier to read if you're more familiar with them. As for compilation speed, that's quite an assertion but unfortunately it's unsupported by any evidence.
&gt; if you distinguish between 400 and 500 you're already doing that But the route is doing it, not the service. That's the point. It's the service's job to produce data munged by business logic, and someone else's job to interpret that for the outside world. In this case, a route presenting it over http. Could otherwise be a script, or another service. The service should not care who is calling it.
&gt; Code readability is really (literally) in the eye of the beholder, you find things easier to read if you're more familiar with them. Take this code for an example. Ppl find it hard to read even in this sub. &gt; As for compilation speed, that's quite an assertion but unfortunately it's unsupported by any evidence. A local university made a research on what could improve scala's compilation time. One of the negative effects was infix/postfix ops. If you think about it rationally, it's really harder to parse calls like this.
I'm working on a new sound installation which is part of an exhibition opening in 2 1/2 weeks. This uses my computer music system 'SoundProcesses' (written in Scala) on nine interconnected Raspberry Pis, each of which outputs two channels which in turn are distributed through a relay network to 108 speakers. We're currently setting up, and after lots of wiring I have finally started programming the sound. [Here](https://www.researchcatalogue.net/view/361990/368400) is a short blog on that.
Sorry, what code for example? Also, could you provide a pointer to the study so we can see for ourselves?
In practice, if you turn on com.lihaoyi::acyclic and work with it for a while, after beating through and understanding the various error conditions, your brain starts writing acyclic code almost automatically =P I haven't found the perf drop to be significant; it's using the same code as the incremental compiler (at some point in the past...), so unless incremental compiles are terribly slower than batch compiles, acyclic shouldn't be terribly slower than no-acyclic. It's also plausible to just turn acyclic on for compiles in CI rather than running with it on all the time. You don't *need* it every compile, it's just there to catch you when you accidentally make some terrible architectural choice, which should happen once every few days rather than every few minutes. 
Can you elaborate more on the scaling? Logs for me isn't an issue as we write all our logs to loggly from our apps 
I might be missing the point, but can't you map your coproduct on a Poly1 to get pretty much what you want in an elegant fashion?
&gt; Sorry, what code for example? What the OP posted... &gt; Also, could you provide a pointer to the study so we can see for ourselves? It's a private study(+automatic compilation optimizer) paid by a private company. By the way if you don't believe me make a few tests for yourself.
Agree, I can't thing of any simple way for performance testing of incremental compiler that are not subjective (and that is why I didn't mentioned any numbers). All hits in article above are based on my knowledge of zinc internals. The biggest problem with testing is that it requires some manual work for each test case (so you need to test how much is recompiled for given change with and without my suggestions applied in code). It may be doable but I need to have a codebase will mutiple PRs opened at the same time (all this PRs should be representation of usual changes that are incrementally recompiled). I plan to test [hoarder](https://github.com/romanowski/hoarder) (sbt plugin for reusing compilation artifacts) and during that tests I may be able to measure improvements when my suggestions are used. If you (or anyone) is interested I will be more then happy to use some help :) 
Okay can you show me how you would encode in the type, that the functions are called in the correct order? def update( doAWithId: UUID =&gt; Future[Unit], doBWithId: UUID =&gt; Future[Unit]): Future[Unit] = { val u = UUID.randomUUID doAWithId(u).flatMap(_ =&gt; doBWithId(u)) } I mean this case could be realistic, that you expect, that first doA is called and then doB. How can you guarantee now, that doB is not removed and just `doAWithId(u)` run?
hm... ok I got your point. This looked strange to me, since I usually have much more specific errors internally wrt the ones I expose. However, if you really need a communication layer you're right - this approach is no good. I'd argue that in most cases you don't need that (at least I don't :)). And that is true even if you don't control the client. 
No knowledge of typeclasses, generics and programming in general.
Option is of kind * -&gt; * I wouldn't expect that to be called higher kinded
What would you expect to be called higher kind?
You think that spark devs tend to gave less programming knowledge?
Many "spark devs" are just data scientists with limited python / R / spark experience and have no knowledge of patterns, good practices or more generally code architecture. I know this since I used to be one.
something more than first order, e.g. Functor (* -&gt; *) -&gt; *
Most of so-called spark developers I met in fact knew next to nothing. Especially if they happen to be consultants from India. They do know some rudimentary SQL and thats pretty much it. Give them the task to reverse a list - and watch. One of them once told me that he needs to use a library to find a maximum item in an array, no less.
A hkt is any type constructor from what I understand. That means, anything which accepts a type parameter.
Actually, you're supposed to start with the [play-scala-starter-example](https://github.com/playframework/play-scala-starter-example). Have you read the [tutorials](https://www.playframework.com/documentation/2.6.x/Tutorials#third-party-tutorials-and-templates)? There's also a guide to building a [REST API](https://developer.lightbend.com/guides/play-rest-api/part-1/index.html).
What exactly are you having trouble with? Sending a JSON response and hooking up a database is exactly what you'll find in nearly all example projects. [The underscore books](http://underscore.io/books/) are pretty good. Essential Play probably isn't updated yet for 2.6 but the differences are pretty minor. Essentially you'll need to change every `extends Controller` to `extends InjectedController` and add guice in the dependencies. 
Being play 2.6 was released just a few weeks ago, I can imagine we'll being seeing updated books and other materials coming shortly.
I mean spark is primarily ETL tool. You don't need know hardcore programming to use it. Its meant for Data Engineers/Data Scientist.
You will see it in there resume. If they primarily have Data Engineer work in the resume with spark then you can assume there knowledge with scala is in the context of spark. Its not false advertising by the way. Its your fault you can't tell the difference between a Data Engineer and a Software Engineer.
I mean, you can use all of Beanstalk integration with Elastic Load Balancer, set up scale up/scale down conditions, initial number of machines, max number of machines, step, etc.
I think it's the use of Guice and dependency injection via things like @Inject(). That and the schemes in place that I'm supposed to just infer from looking at a bunch of examples. It's like telling someone to learn Cantonese by being dropped in China. I need to understand what's going on, not just copy what I'm seeing.
Ehm it is not ETL at all. It's used for analytics and machine learning. More generally anything that requires repetitive passes over data in order to compute stuff. Anything but ETL.
My apologies its Batch Processing, but that can include ETL/Analytics/ML. Also here is a blog from Databricks im sure you heard of them talking about building data pipelines with Spark - https://databricks.com/blog/2016/12/08/integrating-apache-airflow-databricks-building-etl-pipelines-apache-spark.html 
Indeed. Higher kinded types are to type functions (type constructors) what higher order is for fonctions on values: arguments can be other functions. Option is a type function but its argument is of kind * , which are not type functions. Functor, Applicative, Monads, on the other side, take one argument of kind * -&gt; * , which are type functions.
So how did you go about becoming a bona fide "spark dev" (if that's what you do now)?
Did you read the Guice documentation?
That's covered in https://www.playframework.com/documentation/2.6.x/ScalaDependencyInjection but https://github.com/google/guice/wiki/GettingStarted is a better place to start if you don't know Guice at all. Dependency injection has three main bits. You ask for things with @Inject. You say you can give things with @Provides or by adding a javax.inject.Provider. And finally you tie producers and consumers together in a Module with binds.
This is a updated book: https://leanpub.com/modern-web-development-with-scala
You could try using phantom types. Refer to this [link](https://blog.codecentric.de/en/2016/02/phantom-types-scala/) You could have `doAwithId` return `Future[DoneWithAState]` and have `doBWithId` require both `UUID` and `DoneWithAState` as input. In which case you can only call `B` after `A` produces something.
We use circe in our project and have recently switched from annotation-based generation of codecs with circe-generic to semi-automatic derivation with https://github.com/circe/circe-derivation. The compilation times are much lower now. It is frustrating that many libraries using shapeless are almost unusable even on fast machines because of exploding compilation times.
Cool, interesting pattern I haven't heard about yet, thanks for sharing :)
Did you mean the compile times are higher with semi-automatic derivation?
Compilation times: semi-automatic with circe-derivation &lt; semi-automatic with circe-generic &lt; fully automatic with circe-generic
And if you keep it simple you won't even need providers, just @Inject is enough to start. Guice will figure out the dependencies for you.
Thanks I'll be sure to add that to the benchmark. 
So I came up with [this](https://gist.github.com/yannick-cw/2ad9ede65acf6b56343d1e4430a3b4ff#file-phantom_db-scala), pretty happy with having discovered phantom types, thanks again :)
Okay, your first idea was what I meant with the `Future` example, so you would need to throttle either `Future` via execution context or whatever library creates the futures, e.g. the `akka.http.host-connection-pool` when using akka http underlying. At this point I am actually not sure if that is easy to implement. The second options seems like a good idea, thanks, I will look into building a `ThrottledMonad` :)
So I hacked something together, a bit rough still, but seems to do the job for an example :) If you want to take a look, if that is in the direction you mentioned, I'd be happy [gist](https://gist.github.com/yannick-cw/cba2fe8b1e0c6962855dcb8601ca6d8b)
So what do you think people mean when they say Java or Rust don't support hkt? Clearly List of T is expressible in both languages, but functor, applicative, monad etc are not without faking it.
Your `ThrottledF` is a monad transformer if I'm not mistaken. That's fine if you're happy to work with concrete transformer stacks (and honestly I think final tagless etc. is overengineering in a lot of cases), but given that you were talking about using `M: Monad` I assumed you wanted to use final tagless style, in which case you've implemented the part that's analogous to `ReaderT` but you also want to implement the typeclass that's analogous to `MonadReader`.
Okay not sure if I understand 100%, I think I am using final tagless with `abstract class DbRepo[M[_]: Monad]` here and then implementing it with `new DbRepo[ThrottledF[?, Future]]`. So my `ThrottledF` is Monad wrapped around `StateT[F, ThrottlingSettings, A]`. What for would I need a `MonadReader` now?
I had always assumed they just mean that they don't support higher kinded type parameters. Like def foo[M[_]] That's not something you could do in Java and `M` is an hkt. Option for example would fit that shape. 
It's not that you need a `MonadReader`, I'm saying you want to implement a similar typeclass for your effect, `MonadThrottled` or something. At the moment you're hardcoding `ThrottledF` as the outermost transformer/innermost effect - if you had two different effects written in this style you wouldn't be able to use both of them together. Whereas you can use a typeclass that represents "has throttling somewhere in the stack". All that said, don't go more abstract than you need to. If this covers your use cases then no sense overcomplicating it. (Indeed I'm not sure I'd even bother with the `M[_]: Monad` - are you getting any value out of that?) If you do stick to your current implementation approach, you can probably simplify the code by implementing a typeclass that says `ThrottledF` is a monad transformer, rather than by implementing the wrapping "by hand" as you have with the monad instance at the moment.
What part of the cycle? I have spent a few years writing Scala, and a few years writing Python. I have worked professionally in Scala, but founded a startup in each language (1 in Scala, 1 in Python), that actually operate in similar spaces. My overall thoughts on the development differences. * A tight dev loop in Scala Play is maybe a 15 second build for a 1 line change (this was a few years back, may be different now). That same loop in Django is like 3 seconds. * Compile on my build server is ~0 for Python vs ~a few minutes for Scala. Since this doesn't happen a lot, not a huge difference. * The type system of Scala is like "free tests". I end up having to write more tests in Python, as the type system does less work for me. * It is way easier to read Python code. I understood my Scala code, new guy I hired would not always. Python code is easier for people to get up to speed in my experience. Really at the end of the day, they both work great. To me, it came down to I really enjoy Django + Django ORM + Celery, so Python for current startup was a no-brainer. It has been MUCH easier to hire Python devs than Scala devs in previous startups. But the costs to me of going python are: * I miss the type system, I do lose some "free" tests that I have to end up writing * Production performance is a bit worse. In my case for my app it doesn't really matter (maybe I pay $20 more for server costs with Python over Scala), but for some apps you could be paying thousands of dollars in penalties for using Python.
Another reply has talked about phantom types; I might also use an indexed monad - I can't find the article I wanted to link, but https://stackoverflow.com/questions/40197310/how-to-encode-possible-state-transitions-in-type seems to cover the idea. I find such a style works if there's a state that it makes sense to think of as a value. Rather than thinking about "do B after A", I'd try to think in terms of e.g. "B can only be called if there's a row in the users table for this ID (which A creates)", and then that's the state that I'd use a type to mark.
Do you find there's a difference in how long it actually takes to code in Scala vs Python? It seems that functional programming can often be faster to dev.
Not the author, but I don't think it's been my experience that I write code faster when writing functionally. But, I'd certainly say that I end up with less code and fewer "fun" surprises in production. OTOH, I wouldn't say that this is a defining difference between the languages. I try to write code in languages like Python, Ruby, and JS to be as functional as possible anyway. Conversely, there's nothing stopping you from writing highly imperative Scala code. So I'd characterize the difference between writing Scala and Python as Scala doing a better job of facilitating functional programming and explicit data modeling. I still do use Python though, too. My guideline for whether to use one or the other often comes down to how much data needs to flow through a process. If I'm writing a script I'm going to use occasionally, Python's often a good choice, because it lets me treat the data as ad hoc. I can look at the output and tell if my process worked correctly. If not, I just fix it. If I'm writing a process I want to run unsupervised, then I'm more interested in tightly characterizing the inputs and outputs of each stage of the process, which is where Scala's ADT's and typed interfaces shine.
A few points where Scala is faster (based on my experience obviously): * Tools are better. An IDE will catch a lot of things right after you type them. Without a compiler you have to run the code to learn that you made a syntax error. (Sure some IDEs can provide some help, but it won't be as good) * Functional programming usually means less code and fewer errors. Imperative programmers use a lot of for loops, which are more verbose (and harder to read than map,filter...) and get worse with nesting. Some errors just won't happen when using immutables. Debugging weird errors is sloooow. * Refactoring. IDE and compiler help a lot here again. When you have to change something at many different places it won't compile until you get the types right. * Reading old/unknown code. In typed languages you have to map the world to classes which are easy to inspect. In dynamic languages you can only see hashmap/dictionary elements here and there, good luck trying to figure out what's in a more complicated data sturcture without running the code and printing the whole thing. * Types help with figuring out how to use libraries. Type inference is also a big plus here. * SBT works pretty well for me, I had annoying issues with installing Python libraries. One reason behind this is that Python relies on binaries many many times. Scala almost always uses Java or Scala. So you won't get magic dependencies missing. (I guess this isn't a big issue if you use Python a lot) 
&gt; It is way easier to read Python code. I understood my Scala code, new guy I hired would not always. Python code is easier for people to get up to speed in my experience. This is where I strongly disagree. Without static types most python code is hardly readable and you need to guess types by variable names or worse - debug and neither of these are smart methods. And since python's FP capabilities are almost non-existent, its expressivity is pretty low.
Scala is a rapid language. Let me explain: - **Static typing + REPL**: Scala is interactive as Python, but with static typing what means that the compiler checks consistence and sanity of the code avoiding lots of bugs by catchign them at runtime and making refactoring easier. - **Type inference**: People think that static typing sucks because it seems verbose. But in languages with type inference, the compiler or runtime deduces the types for you avoiding lots of verbosity and type annotations, although Scala requires minimal type annotation. - **Functional + Object Oriented + Imperative** - Scala allows functional programming alongside object oriented programming. Python also supports FP, but it is not very emphasized by the community. FP is also better with static typing and type inference. - **Scripting** - Scala can also work as scripting language that is very useful for task automation, demonstrations and proof-of-concepts programs. - **Powerful concurrency constructs** - Future, Actors ... - **Java Ecosystem** - Java Platform is proven, tested and robust. So Scala as JVM language can take advantage of JVM cross platform infrastructure. You can distribute your app as a single jar-file and if you use dependencies that doesn't require native code it can work seamless on Windows, Mac Osx and Linux. - **Faster way to learn about Java Platform** - It is not necessary to know Java to in order to learn Scala, actually it is faster to learn about Java platform through Scala scripts and REPLs. There are so much goodness that this space is not enough. Python is still better as scripting language for small tasks like data analysis with Pandas, numeric programming with numpy and plotting with matplotlib. # Some examples: Without static typing: This is a valid Python code, but the compiler/runtime can tell you anything before runt-time or at compile-time. It may happen when some code is refactored and you call a method or function that was removed or doesn't exist anymore and you forgot to make the changes. Again Python runtime or compiler cannot help and code may work until it unexpectedly crashes because the function world() doesn't exist anymore. **Python** &gt;&gt;&gt; def sayHello(): print(world() + "hello") ... &gt;&gt;&gt; &gt;&gt;&gt; sayHello() Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 1, in sayHello NameError: name 'world' is not defined &gt;&gt;&gt; It is not allowed to happen, even in the REPL. If the function world is removed in the code, the compiler will throw an error and tell you what is wrong and the necessary changes. **Scala**. scala&gt; def sayHello() = println(world() + "hello") &lt;console&gt;:11: error: not found: value world def sayHello() = println(world() + "hello") ^ Python runtime cannot tell you anything if you do this or if you call a function or method with an invalid parameter before run-time or at compile-time. **Python** &gt;&gt;&gt; def fun1(x, y): return 10 * x + y ... &gt;&gt;&gt; fun1(4, 5) 45 &gt;&gt;&gt; def f2(x): fun1(x, "hello") ... &gt;&gt;&gt; f2(7) Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; File "&lt;stdin&gt;", line 1, in f2 File "&lt;stdin&gt;", line 1, in fun1 TypeError: unsupported operand type(s) for +: 'int' and 'str' &gt;&gt;&gt; **Scala** The compiler doesn't allow invalid code and does quality control for free with minimal type annotations and type inference! In Python, I was always paranoid fearing that I forgot something and I had to test and review the code more. In statically typed languages such as F#, OCaml, Scala or Haskell the compiler saves lots of time and mental sanity when making changes and doesn't allow silly mistakes to happen. scala&gt; def fun1(x: Int, y: Int) = 10 * x + y fun1: (x: Int, y: Int)Int scala&gt; def f2(x: Int) = fun1(x, "hello") &lt;console&gt;:12: error: type mismatch; found : String("hello") required: Int def f2(x: Int) = fun1(x, "hello") 
I worked with Python for a few years and after that with Ruby (Rails) for about 1.5. While ruby is fast in the sense that you type extremely few lines of code, you'll always hit some strange edge-cases that you have to debug (which is slow). This happens a lot less in Scala (mostly because of immutability and case classes). Overall Scala wins for production code because of that. The bigger difference comes from the necessity to write a relatively high number of tests for Ruby/Python if you want your software to be reliable. In Scala you still need some tests to cover the most important requirements, but the type-system will prevent a lot of silly mistakes, so you don't need tests for that. Others have pointed out tooling and IDE support, which is true as well. edit: If you ever need some form of async/parallel behavior, Scala has you covered out of the box, while Ruby/Python struggle immensely (and often even need something like Redis as message broker).
I may have just *cough* skimmed them *cough*. 
If you do this, _do not_ make users of your library create an actor. Give them a concrete interface to implement, and then something that can take that interface and produce the actors necessary. It is hard to pick up libraries that make you implement an interface of Any =&gt; Unit. 
Personally I find Scala much easier. My day job right now is python, and I've been at this place about two years. Before this, I've had about two years working in a pure Scala shop. Prior to that I used Java and python for 2-3 years. Despite using python more often and for a longer period of time, it still takes me longer to get work done. The difference is that with Scala the first pass takes slightly longer, but most of the time it works With python I usually find myself getting something running faster, but getting it reliable, tested, and working properly takes longer. With ongoing maintenance python gets even worse in my experience. Refactoring is a chore Scala tooling is much better, though the python tools are improving. 
Why in the world would you draw the line at ETL? There are plenty of use cases that involve transferring huge amounts of data between two endpoints; the only way to cluster processing like that is through Hadoop or Spark and Spark is way faster. And I would argue that Spark, at it's core is an ETL solution. ETL stands for Extract-Transform-Load. Spark is literally designed to extract data from 1 place (stream, files, db), transform it in some way via map/reduce, load it to another data endpoint. Analytics and Machine Learning both involve their fair share of ETL. Source - am a data architect that has had lots of experience with both spark/non-spark apps.
I prefer Scala over Python and other languages but i think this question has no sense in a scala reddit, the answer is obvious. To me, the scala advantages are a mature ecosystem (expecially build tools, sbt beats venvs), static typing semi-dynamic due to trait, functional programming, pattern matching, syntactic sugar. I consider Python slightly more powerful and simpler than scala due to the dynamic type system and the syntax of the language. With the last versions of the CPython interpeter has been introduced the optional typing annotation that makes python a semi-static typed language. Another thing that i dislike in python is the concurrency, exepcially thread that can't run in multiple processors.
I've written Python for almost twenty years and Scala for two. This seems like a balanced, reasonable assessment to me. Personally, I'm not very happy with Scala. The language has too many weird corners, and I often find it difficult to work out what is going on library code. The scaladoc is often difficult to decipher so that you can see what is going on. &gt; It is way easier to read Python code. I understood my Scala code, new guy I hired would not always. Python code is easier for people to get up to speed in my experience. Exactly this. However, it is true that compilation and a tight type system do catch errors that take longer to detect and fix in Python. It's true what u/kaeblo writes that map+filter is better than for loops, but Python has list comprehensions that are far better than either. Personally, I think a Scala that had most of the weird features removed and a more powerful type system with better type infererence (more like Standard ML or Haskell) would be a far better language.
I'll add on a personal experience of mine here. I worked for a company that populated their CRM via loading monthly files from the clients into a database. To accomplish these loads, we had: * Operational data analysts, who used a proprietary tool written in C# to validate and load this data into a series of database tables (the original developer for this was long gone so the tool was hardly ever updated/maintained). * A huge team of overseas devs managing the ETL of the raw-loads into the CRM database via a web of stored procedures and SSIS packages. Sprinkle some Perl scripts in there too. * Me, who was a technical lead for the above, but focused on R&amp;D for reporting on the app. So I had some app design experience. The company went an entirely new direction after it realized that its vertically-scaling MSFT stack could not scale to meet future demands. They decided to use Spark / Scala + Cassandra / Elasticsearch to service the CRM solutions. The company didn't have a lick of experience in any of these technologies. The hope was, we have a team of data engineers that have comprehensive experience with implementing the core business logic. They should be able to pick these technologies up and re-build it, while advancing their own careers as well. Fast forward 1.5 years. The company is actively trying to migrate to this new solution. I was the only original data-developer who picked up Spark / Scala (after admittedly struggling early on). They hired a couple of other new devs to help me out too. Every other ETL developer was still supporting the old MSFT stack, and while there were countless efforts to bring them into the new world... they just couldn't grasp the software-engineering aspect of the technology stack. Many even actively resisted the new technologies, as they wanted to code in their comfort zone. People who are used to coding in databases via SQL stored procedures or designing ETL scripts in SSIS simply aren't exposed to software engineering concepts. They are used to being able to open up a SQL editor and create a script to support some new thing. Each script is a snowflake, designed by that engineer. A lot of SQL/SSIS developers don't use source control, because they just save their work as a stored procedure / SQL Agent job. There aren't great unit testing interfaces for SQL/SSIS work, nor are there comprehensive testing procedures. "Did the counts change?" is all-too-often used as an indication of success. Many who tried to pick up Spark designed them like they designed SQL scripts. No shared libraries or code. 1 big file. Very little typing; a lot of hard-coded, field-level transformations. Configuration and unit tests were nowhere to be found. The code wasn't designed to use the cluster's resources properly. Lots of bad commented-out code and lack of attention to the source-control repositories. I could go on and on.
They are creating an actor ¯\_(ツ)_/¯ My primary goal was to make the library that I wish existed and hopefully others might find it useful. There is an object that holds all the events that your bot can respond to, so there's no ambiguity in what you can/should do, but it is very much an actor class Bot(token: String) extends DiscordBot(token) { val channel = system.actorOf(ChannelApi.props(token)) override def botBehavior: Receive = { case MessageCreated(id, "ping" :: Nil) =&gt; channel ! Message(id, "pong") sender ! Ack case MessageCreated(id, "greet" :: who) =&gt; val greeting = ("Hello" :: who).mkString(" ") + "!" channel ! Message(id, greeting) sender ! Ack } } object Main { def main(args: Array[String]): Unit = { val token = args(0) ActorSystem().actorOf(Props(classOf[Bot], token)) } } The DiscordBot abstract class handles all the connection work and backpressure. Each API resource will be represented by an actor that handles rate limiting and stuff like that. If someone isn't familiar or interested in Akka, I wouldn't recommend my library, but it's a fairly pervasive tool in the community and I felt a natural fit for representing a chat bot.
I tried to get dl4j to work on my work spark cluster. I suppose individual experiences may vary, but I had nothing but problems with this whole set of libraries. We really wanted it to work for an all JVM deep learning solution, and worked on it for over a week, but getting this thing to run was Hell. We eventually gave up and returned to TensorFlow. This was a couple months ago, so I don't know if it's more functional now. 
Note that the dev loop or compilation time overhead is not significant problem with Scala, because of the type system and IDE's, which help to highlight errors in the code before the program is run. I often have typed Scala code for an hour or so before running it for the first time, and it usually requires only minor adjustments to get it to do the right thing.
...well, "the folks at Play" would like you to start with the play starter example and the [Main Concepts](https://playframework.com/documentation/2.6.x/ScalaHome) page. Copy and paste to begin with, start with a bog standard Play application and make sure you understand the flow from when you hit a browser, to the routing to the controller to the action to the template. Then, and only then, start messing around with Guice and DI. Bind an instance of `java.util.Clock` in a Module in the `app` directory, with a Provider that gives you `Clock.systemUTC()` for example: bind(classOf[java.util.Clock]).toProvider(new ClockProvider()) And then verify that calling @Inject()) (clock: Clock) gives you the instance you expect. 
I feel that list comprehensions are only a replacemnt for a simple for loop with one "action", and they simply become too hard to read when you try to put too much there. That's not the case with maps, filters etc.
Scala helps but if you're saying 'development' rather than 'coding', it doesn't matter. Development encompasses many things outside of the code, such as proper scoping, architecture, quality assurance and user testing. Doing these things properly will eliminate waste so you wouldn't even have to write most code in the first place. In my experience, people from both languages had written droves and droves of code at varying speeds - sometimes Python is faster, sometimes Scala is faster - depends on you do. If you're developing some data science, Pandas and familiarity with DS tooling is a huge advantage. If you're developing a state machine don't even bat an eye about using Scala. So it doesn't really matter. I use both.
https://github.com/lloydmeta/enumeratum
A higher kinded type at least of kind (* -&gt; *) -&gt; *. For example, Functor. This answer explains it well: https://stackoverflow.com/a/6417328/1870803. Types like List and Option are called First Order types.
I believe this is what they mean... In Haskell: - `Map` is a higher-kinded type (its kind is `* -&gt; * -&gt; *`). - `Map Int` is a higher-kinded type (its kind is `* -&gt; *`). - `Map Int Int` is not a higher-kinded type (its kind is `*`); it is a concrete type. In Java: - `Map` isn't a type (well, it sort of is, but it desugars to `Map&lt;?, ?&gt;` or something, let's just ignore this weird case) - `Map&lt;Int&gt;` isn't a type, it's a compile error. - `Map&lt;Int, Int&gt;` is a concrete type. In other words, all types in Java are concrete. Because if something has type parameters and you don't fill all the type parameters, what you end up with isn't a HKT, it's just code that doesn't compile.
The "don't put too much there" rule applies to list comprehensions and map/filter equally, I think. A typical list comprehension in Python might look something like: [str(pos) for (name, pos) in things if pos &gt; 25] My experience with Scala so far is that it tends to be weak on this kind of thing because the destructuring bind and type inference often fails to line up the types correctly so that you can't write this as compactly and easily.
~~Here's a shorter version:~~ (26 until things.length).map(_.toString) ~~I know this probably isn't what you wanted to show and yes Python has a more concise builtin zipWithIndex. But the reason I'm still writing this example is that I find it very rare that I have to use zipWithIndex. So I think it's ok that there isn't a new special syntax in the language and you have to type .zipWithIndex instead.~~ things //.zipWithIndex .filter(_._2 &gt; 25) .map(_._2.toString) // This is quite similar to the Python example things. //.zipWithIndex .collect { case (name, idx) if idx &gt; 25 =&gt; idx.toString } In Scala multi line expressions work better, for this reason I don't think Scala is affected by the "don't put too much there". If you do a lot of things, it'll just become a long vertical list of simple expressions, which are in the order of how things happen. (I assume this isn't a problem if you are used to Python syntax, but Scala one seems to be more logical to me) EDIT: Looks like I forgot how this works in Python, there's no magic builtin indexes, you still need enumerate(). So the whole zipWithIndex thing isn't relevant. 
This assumes the `pos` is necessarily the index in the list, which was not my intention. Nice code, though.
Makes extensive use of `flatMap` but doesn't know the monad laws, or thinks they're not important. But honestly I find Spark is a better route into Scala than most things. If you want general Scala ability, test for that.
It's easier to hire for Python developers who lean heavily on very well-established idioms though. Like, I can work on any Django project if it was written by someone who followed the patterns outlined in 2 Scoops of Django.
&gt; Without static types most python code is hardly readable and you need to guess types by variable names or worse So that is a cool theory, but not really how it plays out. For example, if you make a method designed for object A and object B and add even 1 unit test around new code.. shoving in the wrong random objects will be pretty obvious. I mean, as much as Scala guys talk about strong typing.. MOST scala apps I see still pass around ids (say USER_ID vs GAME_ID as Ints/Longs, vs bothering to type every single bloody int as userId: UserID, gameId: GameID So you really end up in a similar spot to Python. &gt; And since python's FP capabilities are almost non-existent, its expressivity is pretty low. Depends I guess. For me, map and filter were the main FP paradigms I needed to use. Python has list comprehensions, which are much cleaner and just as good as map and filter. Python has zip, python has lazy iteration... you have most of the same tools, at least the tools that I am comfortable using. Immutable is harder to deal with sure. But the way you structure things, you aren't going to go spawn threads willy nilly in python so it is different.
If I am writing a pure math problem, I can perhaps code a tad faster in Scala. If I am using a webapp and want to code a new page, I can code faster in Python Django vs Scala Play (or Scala anything). So some of it is what domain does your problem fit in. If it is a webapp, I think Django can spit out code faster than anything else. But if I were to say need to code some actor stuff, Scala would win.
Except I think he proved the point that Scala is not easier to read ;) 
You do realize that all of your examples collapse into basically the same code with MyPy? http://mypy-lang.org/ You should check it out if you like static typing.
&gt; Tools are better. An IDE will catch a lot of things right after you type them. Without a compiler you have to run the code to learn that you made a syntax error. (Sure some IDEs can provide some help, but it won't be as good) Try MyPy + PyCharm. Basically the same experience as IntelliJ + Scala. &gt; Functional programming usually means less code and fewer errors. Imperative programmers use a lot of for loops, which are more verbose (and harder to read than map,filter...) and get worse with nesting. Some errors just won't happen when using immutables. Debugging weird errors is sloooow. List comprehensions are often easier to read and reason about than complex chains of filters and maps. &gt; Refactoring. IDE and compiler help a lot here again. When you have to change something at many different places it won't compile until you get the types right. Refer to MyPy + PyCharm. &gt; Reading old/unknown code. In typed languages you have to map the world to classes which are easy to inspect. In dynamic languages you can only see hashmap/dictionary elements here and there, good luck trying to figure out what's in a more complicated data sturcture without running the code and printing the whole thing. How much 5 year old Scala code have you read that someone else wrote? Especially someone not very good? Reading 5 year old Scala code written by someone else is usually a disaster. It's pretty easy to get what a dict is. Not so easy to what what some scala types are. &gt; Types help with figuring out how to use libraries. Type inference is also a big plus here. So library situation is interesting. Python has more libraries, but Scala libraries are generally better. In scala you CAN use Java libraries, but then you end up with an entire shim layer and tons of null checks. &gt; SBT works pretty well for me, I had annoying issues with installing Python libraries. One reason behind this is that Python relies on binaries many many times. Scala almost always uses Java or Scala You can basically get pure Python implementations of anything. 
&gt; Take this code for an example. Ppl find it hard to read even in this sub. Some people. I find the infix version easier to read than the brackety version, personally. &gt; One of the negative effects was infix/postfix ops. If you think about it rationally, it's really harder to parse calls like this. How so? Postfix yes, but infix is just a slightly different syntax for the same thing.
I would tend to agree.
&gt; Try MyPy + PyCharm That looks good, if I have to use python for something again I might try it. But I guess it's hardly the norm amongst python users (unfortunately?). &gt; List comprehensions are often easier to read and reason about than complex chains of filters and maps. Can you write some examples? I found that list comprehensions are quite ugly if you put more than 1 function to each end. While map/filter are easier to format nicely. &gt; How much 5 year old Scala code have you read that someone else wrote? I haven't read that old Scala code, since then the language has changed a lot, so I can imagine that it can be quite bad. I've seen some quite recent python code though with hard to read mutable mess. And 150+ characters long hard to read list comprehensions. And reversed if expressions (Does anybody like this?). &gt; It's pretty easy to get what a dict is. Maybe I'm wrong but, as far as I know you can add anything to a dict any time, so you have to check every reference to it in the code or run the code and print it. A class is defined at one place, also it's members are available on ".". Maybe there's a problem here, I haven't seen it so far, example would be welcome. &gt; So library situation is interesting. I was mainly talking about when you want to use something new. For python I always ended up browsing some online documentation. For Scala the default is to just press Ctrl + P and there are the parameters, with types. I'm sure Python experience can be improved, but it's not the default. This might not be very important, but I found it easier in Scala. Note: I didn't downvote any of your comments. I think many of these things are matters of preference. I'm usually interested to learn other opinions, there's always something to learn. If thing A is more readable to you, I can only write why thing B is more readable to me, thing B won't become magically more readable to you, so I don't see the point of downvoting these comments. 
Sorry to hear that. We've got a dev support channel here https://gitter.im/deeplearning4j/deeplearning4j If you can show us the errors that were thrown, we can help you fix it. 
Could you try filing issues on what you had problems with? Feedback on the docs would help a ton. If you don't tell us we largely can't help you. That is what our live chat is for. Beyond that, we also respond to stack overflow. Most people have success when they start with the examples. "Getting this to run" really should just be a spark integration with picking an nd4j backend. It's kinda hard to tell what your problems are..but usually "getting this to run was hell" maps to "we didn't get how the sbt/maven setup worked". If you come in to our live chat we can help with that. I just have to say.."anymore functional" isn't really on the library here. The library does everything from ETL, hyper parameter tuning, neural net building, tensor library control,.. A lot of it comes down to mapping out the modules and cherrypicking what you need. One thing I'll note, is if your'e largely coming from a pyspark background and you use tensorflow, that is going to seem more natural. A "data scientist using spark from python" and "a guy who knows a bit of play" tend to be very different developer profiles. It's hard to know how to help you if we don't know the usage mode as well. We have something for python on the way which might help a bit (https://github.com/deeplearning4j/jumpy) among other things, but sometimes people prefer JVM solutions. When they do, a lot of them use us but then come and get help on the missing bits. A library that does everything and has lots of modules can be difficult to navigate. Anonymous "I had problems but won't tell them about it" doesn't really do much for us though. I'm sure you have users of your software that appreciate bugs. The reverse would be a huge help here coming here from developer to developer. I'll just note, cloudera didn't have problems with it: http://blog.cloudera.com/blog/2017/06/deep-learning-on-apache-spark-and-hadoop-with-deeplearning4j/ They did deep learning on spark, transfer learning and everything. You can get it to work, you can even get something like distributed cudnn on spark going pretty easily if you can get your cluster setup. So again, I'll ask : come tell us what your problems are. It's frustrating to see "random anon dev #5" complain without understanding what the context was or what the problems were. 
&gt; Can you write some examples? I found that list comprehensions are quite ugly if you put more than 1 function to each end. While map/filter are easier to format nicely. Sure, do you have an example Scala and we can convert to Python?
I recently felt that python is great for people to quickly grasp some vague idea of how things work, but gets pretty annoying if you're trying to understand it in detail, probably because of the dynamic nature, you need to run the code to actually refine your vague understanding. Scala on the other is more dense at first, but once you understand it, it's pretty clear what the code is doing, if you don't consider implicit. I'm definitely biased, as I never had the aha moment for dynamic typing
&gt; Some people. I find the infix version easier to read than the brackety version, personally. I can't really believe that. A println standing alone already requires two looks and the infix foreach is a thing every scala coder who use an ide should fear: how would you use method completion there if you use space instead of the dot? &gt; How so? Postfix yes, but infix is just a slightly different syntax for the same thing. Because in languages like scala, where operators aren't predefined and whitespace matters the infix and postfix operators require more steps to analyze.
&gt; For example, if you make a method designed for object A and object B and add even 1 unit test around new code.. shoving in the wrong random objects will be pretty obvious. That is a cool theory but I usually don't want to "wait" till it gets tested I want to see the errors ASAP, especially if I'm learning a new API of a certain module. &gt; I mean, as much as Scala guys talk about strong typing.. MOST scala apps I see still pass around ids (say USER_ID vs GAME_ID as Ints/Longs, vs bothering to type every single bloody int as I don't know what kind of "development" you're doing but it sounds like stupid simple CRUD apps. You shouldn't count that as evidence. &gt; For me, map and filter were the main FP paradigms I needed to use. map and filter are functions and just a really tiny parts of FP. &gt; Python has list comprehensions, which are much cleaner and just as good as map and filter. Definitely not. Also: 1.) there is no way to get code completion for list comprehension 2.) complex, multiline statements are more complex with it 3.) you really shouldn't use linked list as a general purpose sequential data structure. &gt; Python has zip, python has lazy iteration... you have most of the same tools, at least the tools that I am comfortable using. Then you're probably aren't aware of the possibilities in Scala. &gt; Immutable is harder to deal with sure. But the way you structure things, you aren't going to go spawn threads willy nilly in python so it is different. Correction: you can't spawn threads in python, not in a sane way for sure. And saying that "But the way you structure things," you just kinda admit how flawed is python's concurrent and data modelling capabilities.
No, a higher-kinded type is any type whose kind is not `*`, so `List[Int]` is of kind `*`, but `List` is of kind `* -&gt; *`, so it's a higher kinded type, you can read about SystemFw (e.g. in `Types and Programming Languages` by Pierce) to get a clearer picture of this. /u/kod when we say that they lack higher-kinded types it's a shorthand for "they lack higher-kinded parametric polymorphism", i.e. the ability to abstract _over_ higher-kinded types. This means that a type parameter can only be of kind `*`, and therefore things like `Functor` and `Monad`, which abstract over higher-kinded types of kind `* -&gt; *` (e.g. List, IO, Option) are inexpressible (and even more so, things that abstract over other higher-kinds, e.g. Monad Transformers, Free monads and so on) EDIT: also, at least for Java, they don't even have a kind system, so you can't talk about `Option` on its own, it's always fully applied to something. Contrast with Haskell where Option is a normal type, just with a different (higher) kind: `* -&gt; *` 
&gt; A println standing alone already requires two looks Sure, because it's overloaded; I try to avoid using such mechods. &gt; how would you use method completion there if you use space instead of the dot? Um, exactly the same way? Why would it be any harder. In any case, weren't we talking about reading rather than writing? &gt; Because in languages like scala, where operators aren't predefined and whitespace matters the infix and postfix operators require more steps to analyze. Why/how? There's no customizable precedence in Scala, so `foo bar baz quxxl moo` immediately parses the same as `foo.bar(baz).quxxl(moo)`. Why should that be extra steps?
&gt; You do realize that all of your examples collapse into basically the same code with MyPy? No. It seems that MyPy cannot enforce the type constraints and also Scala provides Haskell-like algebraic data types known as case classes, tuples, powerful collections and pattern matching. In addition to that, Scala provides option type that can avoid many hours headaches debugging annoying Null exceptions, known in Java as NullPointerExceptions. Another problem in dynamic languages is that you don't know what a function returns 6 months after you wrote the code, unless you use lots of assert statements, check the type manually or you document it.
Lunaris's analogy between "higher order functions" and "higher kinded types" was very enlightening to me. http://i.imgur.com/ok7IDw2.png
I'm with this guy. I find jumping into unfamiliar Python code to be a pain unless it's extremely well documented. If I need to make a change in a function that takes three objects as arguments, I may not know the type of the objects. Maybe I can guess from the argument names, but that doesn't guarantee that I'll be able to find the type definitions easily. It's also not necessarily clear what the function needs to return. And sure, if everyone on the project is really good about documenting and following a consistent style, that helps quite a bit. But in Scala the documentation is built into the language. If I want to know what the type of an argument is, it's guaranteed to be in the function's signature, because otherwise the code won't compile. To get the type's definition, I can Ctrl-click it and IDEA will take me there. Static typing provides a level of self-documentation that takes significant extra effort to add to dynamically typed code.
&gt; So that is a cool theory, but not really how it plays out. In my experience, that is exactly how it plays out. &gt; For example, if you make a method designed for object A and object B and add even 1 unit test around new code.. shoving in the wrong random objects will be pretty obvious. Sure, but I don't want to have to run code to figure out what types I'm dealing with, especially when dealing with complex systems that take time to set up. That's the kind of thing that should be checked statically.
Assuming I'm understanding the Python code correctly, the Scala version using for comprehensions is almost identical, just rearranged a bit. for ((name, pos) &lt;- things if pos &gt; 25) yield pos.toString Although /u/kaeblo's collect() implementation probably performs better, since it doesn't create an intermediate collection.
&gt; No. It seems that MyPy cannot enforce the type constraints What? You make a oneline entry on your CI server mypy blah Done, static typing enforced. Like I said, you are saying lots of awesome platitudes that scala users love to say. I have been paid lots of money to write both kinds of code. For most applications, I was able to write the Python code faster and cleaner than the Scala code. You are free to disagree and use what you wish. That is why the world has choices.
I disagree. Even wikipedia (https://en.wikipedia.org/wiki/Kind_(type_theory)) states a "higher order type" is (* -&gt; *) -&gt; *. Types of kind * -&gt; * are first order types (List, Option, etc..)
I don't think this is accurate. I think it should be: In Haskell: * `Map` is a higher-kinded type (its kind is `* -&gt; * -&gt; *`). * `Map Int` is not a higher-kinded type (its kind is `* -&gt; *`); it is a first-order type. * `Map Int Int` is not a higher-kinded type (its kind is `*`); it is a concrete type. In Java the equivalents: * `Map&lt;T,S&gt;` is not a higher-kinded type( is kind is `* -&gt; (*, *)`); it is a first-order type since Java doesn't curry types; * `Map&lt;T&gt;` isn't a type, because Java doesn't let you partially apply types. * `Map&lt;Int, Int&gt;` is a concrete type. If Java let you write higher-kinded types it would look something like `List&lt;A&lt;B&gt;&gt;` but Java won't currently let you write that unless `A` is a concrete type. 
Maybe someone knows any practical usage of the Adjunctions^1 in real world software or at least at toy projects? ([1] - which was discussed at Rúnar talk https://youtu.be/BLk4DlNZkL8)
I think you want `map`. val dfCols = columns map { df col _ }
&gt; Um, exactly the same way? Why would it be any harder. In any case, weren't we talking about reading rather than writing? Well, you said that it is not hard to read for you but I don't know how would the completion work. &gt; Why/how? There's no customizable precedence in Scala, so foo bar baz quxxl moo immediately parses the same as foo.bar(baz).quxxl(moo). Why should that be extra steps? Because it's not obvious what'll be next. When the lexical analyser creates the tokens the dot operator clearly tells what's going on but whitespaces have more meanings. 
 [x + 3 if x % 2 == 0 else x - 3 for x in list if x &gt; 3] Not so long ago I've seen something like this. This doesn't seem readable to me. list.filter(_ &gt; 3).map(x =&gt; if (x % 2 == 0) x + 3 else x - 3) Literally translated map/filter version for quick comparison. Anyway this is my last comment on map/filter topic, because it's really not that important.
I love these puzzlers though. How do you get the essence of it, while keeping it pretty in the new language. So yes, this seems like reasonable common Scala list.filter(_ &gt; 3).map(x =&gt; if (x % 2 == 0) x + 3 else x - 3) I honestly do not like this in scala either, I don't think the map reads that well. The filter up front reads great, but the map is ugly. I think what I would do different is to just split it into 2 lines in python... list = [x for x in list if x &gt; 3] list = [x + 3 if x % 2 == 0 else x - 3 for x in list] However I think both are a bit ugly. I think the easiest option to read might be something like for x in list: if x &lt;= 3: continue if x % 2 == 0: yield x + 3 else: yield x - 3 Perhaps more verbose in lines, but not in intent. It also has the benefit of being lazy, which you are not in the Scala example
&gt; you need to run the code to actually refine your vague understanding. No, you go look at the tests. The method name should declare the intent. The tests should prove the intent. No matter what language you are in, if you need to run the code to figure out what it does you failed. And you can fail in any language.
I thinking lumping python and ruby together is not correct. Ruby encourages monkey patching and other craziness WAY more than Python. I.e. your average Ruby library is way more crazy than your average Python library.
So true, my Python startup has had a MUCH easier time with developers than my Scala startup.
`col` is also called from the `apply` method, so you could also do something like `columns.map(df(_))`. Keep in mind that this will throw an AnalysisException if the column doesn't exist for that dataset. 
I know its not always an option but perhaps the typelevel version of scala https://github.com/typelevel/scala/blob/typelevel-readme/notes/2.12.1.md which contains this PR https://github.com/scala/scala/pull/5649 may help? 
My experience was that while Ruby allows more in terms of syntax and in the past may have had problems with too much monkey patching, currently it often has simply better defaults in every way. For example the package management just works (Python was really painful in that regard), libraries have sane default setups and (most importantly) the standard library is consistent and prefers immutability over mutable operations ("destructive" methods end with "!"). Also a lot of special syntax in python like "with", list comprehensions or 3 different string formatters aren't needed in Ruby because of more powerful and general concepts like closures or expressions vs statements. There are other things like community culture, etc. I won't go into. I used to be a python advocate, but apart from certain niches (like scientific computing), I'd choose Ruby over Python every time. Fortunately Scala has even more to offer while sharing quite a few of the good parts of Ruby.
I think one could argue that some architectures are easier to achieve with certain languages, at least in practice (due to direct factors such as language design and functionality, but also due to indirect factors such as momentum in regards to user communities for different domains, libraries as well as frameworks). I think an example of this is a distributed architecture taking advantage of the actor model to handle many aspects, for which languages/platforms like Erlang and Scala are likely to be considerably better choices. Besides, some factors will be much easier to achieve for certain languages. Quality assurance for long-lived projects will likely be much more feasible if the language enables making and keeping the code base maintainable while not limiting the scope considerably. I definitely agree and think you do overall have an excellent point regarding the various factors, though - scope, QA, etc. matter incredibly much.
Well, it says "higher order type _operator_", to be precise, where a type operator is something of kind `* -&gt;*`. Anyway, after reading around, it seems that both interpretations are used: e.g. the paper [Scala for Generic Programming](http://ropas.snu.ac.kr/~bruno/papers/ScalaGeneric.pdf), by Gibbons and Oliveira, refers to every type whose kind is higher than `*` as a "higher kinded type" (e.g. `f` in section 2.3.6). Otoh, [Generics of a Higher kind](http://adriaanm.github.io/files/higher.pdf) by Odersky and Moors calls "higher kinded types" only higher order type operators: I'm not sure I agree cause Haskell '98 had Functor and Monad, but you couldn't write their type down (before ConstraintKinds). A much more useful distinction is whether or not a language is able to abstract over types of kind greater than `*`, which Scala and Haskell can, and Java and Rust cannot EDIT: that wikipedia excerpt is page 442 on Pierce, which however later on uses "higher-kind" to mean types whose kind is higher than `*` and _not_ "higher order type-operators"
TaPL by Pierce, page 442: &gt;" &gt;we use the word type for any type-level expression - i.e. both for ordinary types like Nat -&gt; Nat and (forall)X.X -&gt; X and for type operators like (lambda)X.X &gt; &gt;Type expressions with kinds like ( * =&gt; * ) =&gt; * are called higher order type operators. &gt;" I'm having a lot of trouble reading that as disagreeing with what I said. The word "type" can be used for type operators, and (* =&gt; *) =&gt; * is a higher order type operator, which sure sounds to me like it's a higher order type. Can you point to something specific in TaPL that adopts your point of view?
So, ~~the thing is that TaPL doesn't actually use the term "higher-kinded types" at all~~, and your description of "higher-kinded type" fits the TaPl definition of "higher-order type operators", you're right on that. However, the use of higher-kinded type to mean types whose kind is higher than `*` is also widespread, even in academic papers (I would actually say is _more_ widespread). I posted one example above([Scala for Generic Programming](http://ropas.snu.ac.kr/%7Ebruno/papers/ScalaGeneric.pdf), `f` in section 2.3.6), another is in the paper [Lightweight higher kinded polymorphism](https://www.cl.cam.ac.uk/~jdy22/papers/lightweight-higher-kinded-polymorphism.pdf) (first page): &gt; The kind ascription ∗ → ∗ makes explicit the fact that m is a higher-kinded type variable Yet another is in [Implementing Higher-Kinded Types in Dotty](http://guillaume.martres.me/publications/dotty-hk.pdf), and so on EDIT: Actually, if you do want something from Pierce, page 443: &gt; Note that proper types—i.e., type expressions of kind *—may include type operators of higher kinds as subphrases, as in (λX.X→X) Nat or Pair Nat Bool, Refers to `LX. X -&gt; X` and `Pair` as having higher kinds, and they don't have higher-order. But the use in papers should be enough: the term is used more loosely than your definition. EDIT2: Indeed, TaPL has quite a few, page 467 &gt; The most interesting new feature is the extension of the subtyping relation from kind * to types of higher kinds. Which should make it clear
You're selectively quoting a lot. Lightweight higher kinded polymorphism: &gt; Here is a **function with a higher-kinded type**. The function when conditionally executes an action: &gt; &gt; when b m = if b then m else return () &gt; &gt; In Haskell, when receives the following type: &gt; &gt; when :: ∀ (m :: ∗ → ∗). Monad m ⇒ Bool → m () → m () &gt; &gt; The kind ascription ∗ → ∗ makes explicit the fact that m is a higher-kinded type variable : it abstracts type constructors such as Maybe and [ ], which can be applied to types such as Int and () to build new types. That's saying that **when** is a function with a higher-kinded type, which I think everyone agrees on. Maybe is a possible candidate for the **variable** in that higher-kinded type. If Maybe on its own was a higher-kinded type, why did the author need to discuss "when"? Similarly, you chose a specific section from [Scala for Generic Programmers](http://ropas.snu.ac.kr/~bruno/papers/ScalaGeneric.pdf) but its actual section 4.3 Higher Kinded Types says: &gt; trait Iterable [A, Container [ ] ] { &gt; def map [B ] (f : A ⇒ B) : Container [B ] &gt; def filter (p : A ⇒ Boolean) : Container [A] &gt; } &gt; &gt; Note that Iterable is parametrized by Container [ ], a type that is itself parametrized by another type – in other words, Container is a type constructor. Again, if Container on its own was enough to explain higher-kinded types, why did the author use Iterable[A, Container[_]] as the example? More to the point, the original article on PHP gains nothing in regards to clarity by calling Option a higher-kinded type. Everything in the article regarding Option could be done without taking advantage of the differences between Scala and, say, Rust. 
&gt; The kind ascription ∗ → ∗ makes explicit the fact that *m is a higher-kinded type variable* it literally says that `m` is a higher kinded type variable, and `m` has kind `* -&gt; *`. It needs to discuss `when` because, as I said in my first comment, it's about _abstracting_ over types of higher kinds, i.e having higher-kinded type variables, such as `m`. Same thing for the other example, we are talking about higher-kinded parametric polymorphism, i.e. being polymorphic over higher-kinded types. The section I quoted is this: &gt; data GRose f a = GFork a (f (GRose f a)) &gt; &gt; The type constructor GRose is parametrized by a higher-kinded argument f. `f` is referred to as "higher-kinded", the kind of `f` is `* -&gt; *`. Also, page 328: &gt; Unfortunately, the dictionary cannot be constructed automatically, because the higher-kinded type List cannot be inferred. "higher-kinded type List" And again in Tapl &gt; The most interesting new feature is the extension of the subtyping relation from kind * to types of higher kinds. Crystal clear to me. Also see the example about `Pair` Anyway there's no point debating this for eternity. Agreed on the article
My primary background is as a data scientist, but I'd say I'm more proficient with Spark and Scala than I am with the rest of the toolkit of your average professional developer. At our company (I suppose I should be a bit coy and say that it's a big Silicon Valley company with name recognition and be a bit vague beyond that), two previous groups had tried within our organization to do proof of concept runs with dl4j in the context of Spark. One was never able to get the back end to initialize no matter what they did or who they contacted on forums (and let's be honest, if you need to go to forums and beg for help just to get it to initialize it doesn't bode well for that library), the other got it to initialize but could never get a basic model to run correctly. I also could not get the back end to initialize correctly (trying to use basic CPU only) on the Spark cluster, no matter what I tried. I am not an expert with regards to Spark, but I would say I'm at the upper end of the middle tier. Regardless, the other two teams were more traditional developers, and they hit major issues with dl4j also. After three different attempts, from different teams, using different Spark configurations which don't share a common template, spread out over almost a one year period, I think it's fair that in our internal wiki deep learning tool kit survey article, dl4j is marked as broken. If all of that sounds like I'm hating on the library, I don't mean to. I'm still frustrated from putting so much time into trying to make this work, and the lack of useful documentation for solving the problems I encountered. I really, really, would like a JVM deep learning solution that could be used with Spark, and I'd be running this in production next month if it worked. Trying to switching from long ETL and feature engineering pipelines in Spark to suddenly using Tensorflow in an ad hoc manner and then deploying the model back to production Play infrastructure violates any number of good engineering principles and has created huge amounts of pain.
Something doesn't sound right here..we've had more than enough folks get up and running on spark fine. I still don't think that's a fair assessment and it's certainly not fair that you didn't at least talk to us when we offer that help in the first place. Dl4j powers some of the biggest applications in the world including components of amazon's retail stack as well as a lot of alibaba. I don't want to hear it's "broken". Otherwise how the heck would we be making money or getting users? Cloudera and other vendors wouldn't be advocating the use of it if they couldn't get it working. IBM did just fine with our deep learning on spark too: https://www.ibm.com/developerworks/library/iot-deep-learning-anomaly-detection-3/ Apache tika and opennlp rely on us now as well. Apache flink is choosing us as part of their tensor library. Rapidminer and knime both rely on us for their deep learning internals. If that's "broken" I'll eat my hat. Again, try interacting with us. Everyone who has had success with it usually gets help from us. We offer that free of charge. If there's problems with the docs, give us feedback then! You can't expect us to be psychic. If something is a problem file issues and ask us about it. That's what will have direct impact on the docs. You're ranting to me that you spent a week on it and yet ignored the best resource the dl4j community offers, real time live chat 24/7 with the actual engineers running the project. No one to this day offers that but us. That's not our fault. Since you refuse to come to our live chat, I can only guess at what your problems were but I'll try to name some common mistakes people make. Common mistakes people make: Mixing versions (you should only ever have 1 version) Our spark examples show how to set this up. Mixing the UI (which uses play) with the spark jars (jackson is always a problem here) Mixing spark 1 and 2 versions. If you look at our latest release: &lt;dependency&gt; &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt; &lt;artifactId&gt;dl4j-spark_${SCALA BINARY VERSION}&lt;/artifactId&gt; &lt;version&gt;0.9.0_spark_{SPARK VERSION}&lt;/version&gt; &lt;/dependency&gt; if you follow that formula you should be fine. We support both spark 1 and spark 2. For doing inference..there are a ton of different ways of doing that. We offer parallelinference as one such utility: https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-scaleout/deeplearning4j-scaleout-parallelwrapper/src/test/java/org/deeplearning4j/parallelism/ParallelInferenceTest.java#L112 Beyond that..I'm not sure how the heck you're doing the ETL pipelines, but we offer an opinionated tool for that as well called datavec. If you have more specific questions, please come to the chat. We aren't asking you for much here. Here I'll even drop the link right here for you. https://gitter.im/deeplearning4j/deeplearning4j I just want to say..we aren't of the opinion we're perfect. That's why we want feedback at least. Especially if you don't use dl4j if something is stopping you we are all ears on blockers. 
Yes we use thrift a lot (though we have moved off it to just regular rest web services for numerous reasons). Can you post your sample project on GitHub somewhere? 
I didn't test scrooge 4.8.0 and finagle 6.45.0. But maybe your scrooge version 4.8.0 is incompatible with [finagle 6.45.0](https://github.com/twitter/finagle/blob/develop/project/plugins.sbt#L4). I recommend using scrooge 4.18.0. Also, [the finatra thrift server example](https://github.com/twitter/finatra/tree/develop/examples/thrift-server/project) would help you. :-) 
&gt; The difference is that with Scala the first pass takes slightly longer, but most of the time it works I think an initial pass is still faster for me in Scala, even when scripting--partly because of case classes, partly composability. In Java or Python, people tend to have a bunch of lists joined by index and work with them imperatively instead of just chaining combinators on explicit arbitrary data types as in Scala. "Following the types" and being free to use more complex types to help understand code is much easier.
I used it for a while (and forked the project to generate non-Finagle implementations), but I was using Maven rather than SBT. I'd look into the templates it's using (they're fairly easy to find in the scrooge source) and see whether you can make a change to them that will fix the generated code.
In Java unparameterized `Map` or `List` isn't a type (ignoring that they're used as syntax for something like `Map&lt;?, ?&gt;` or `List&lt;?&gt;`) in any first-class sense: you can use or pass around types like `List&lt;Integer&gt;` or `List&lt;String&gt;`, but you can't do anything with a type like `List`.
&gt; I mean, as much as Scala guys talk about strong typing.. MOST scala apps I see still pass around ids (say USER_ID vs GAME_ID as Ints/Longs, vs bothering to type every single bloody int as Actually in our code base, we do exactly this `userId: UserID, gameId: GameID` Its especially useful when doing structured logging, because you can do `gameId.log` and since you know the type of the log (in this case `GameID`) you automatically know its key
I've just had to debug some python where I passed in a string to a dict via `dict.get`. The dictionary was annotated with `Dict[int, List[str]]` - would mypy have helped there?
Yes
Sure, and you can do this. I have found this becomes a huge PITA and not something I want to code in. I am fine with just an int called user_id. But that is partially why I use Python ;)
&gt; Try MyPy + PyCharm. Basically the same experience as IntelliJ + Scala. Not even close. I've been using mypy on all my recent work and it's *better* than not using it, but it's still not close to scala. The type system is not as powerful, so there's things you can statically express in scala which you can't in python. It still doesn't have support for structural typing merged. Support across the library ecosystem is anemic, meaning that even with full enforcement it's going to be disabled for most code you import. As a library author, the story for distributing type annotations is really weak - if I write them inline, they don't work. I need to PR stub files into typeshed, which is an obnoxious level of overhead. Pycharm is by jetbrains, so works nicely, but the static analysis isn't as powerful as in scala, and it's basically impossible for it ever to be as good, since python makes runtime hackery possible. A lot of IDE smarts like "find usages" fall back to string matching when it can't figure out the type of an expression (which is most of the time). Pycharm is better than using a text editor, but it's not as good as intellij. &gt; List comprehensions are often easier to read and reason about than complex chains of filters and maps. But they're usually harder to read and reason about than for-comprehensions while simultaneously being less powerful. &gt; How much 5 year old Scala code have you read that someone else wrote? Especially someone not very good? Reading 5 year old Scala code written by someone else is usually a disaster. It's pretty easy to get what a dict is. Not so easy to what what some scala types are. The worst scala codebase I've worked on that I didn't write myself I still found better than *all* of the python codebases I haven't written myself that I've worked on. &gt; Python has more libraries Yes, but virtualenv is a clusterfuck. Installing libraries is a giant pain in the ass, especially when they require C-level dependencies. Most libraries still don't have binary wheels for installation, so your deploy process has to account for linked libraries - you can't just install python+virtualenv and expect an average project to work, whereas installing scala+sbt will let you run basically anything in the ecosystem. &gt; In scala you CAN use Java libraries, but then you end up with an entire shim layer and tons of null checks. The only time I've ever felt a real need to use a java library was the AWS SDK, which is pretty high quality. And I'll still take writing shim layers over playing exception whack-a-mole with python libraries.
`typing.NamedTuple` does a slightly reasonable case class impersonation. The proposed dataclasses coming up should be much closer to real case classes. 
I wish I lived in your world where all the python code that exists is well documented, well factored, perfectly tested, and easy to read. Unfortunately that doesn't match my experience working on either private or open source code. At least with scala the language gives you tools to not shoot yourself in the foot.
So I run my own biz, so I get to pick what code makes it vs not. Perhaps that makes the difference. And i disagree that you cannot shoot yourself in the foot with Scala. I have seen some really really bad Scala code. 
I didn't say it was impossible, just that it's easier. Bad scala code is definitely a thing. I'd be a lot happier refactoring bad scala code that works (I assume it works if it got merged in) when refactoring bad python, *any* day
I am not sure I would choose the language for a project based on the amount of effort required to refactor bad code?
... tested mypy, it does find that error. However, it freaks out due to a backoff annotation. Feels like any patched-on feature of any language - not supported by a decent majority of the ecosystem, which makes it way less useful.
That's not a way to choose a project. Having written both I firmly believe it's easier to write maintainable code in scala.
&gt; List comprehensions are often easier to read and reason about than complex chains of filters and maps. I find Python list comprehensions utterly impossible to read and reason about because of the bizarre middle-endian iteration: for i in foo: i is equivalent to [i for i in foo] but for i in foo: for j in bar: i + j is *not* equivalent to [i + j for j in bar for i in foo] &gt; You can basically get pure Python implementations of anything. Not my experience at all. E.g. reading image file formats (jpeg etc.) always relies on external libraries. All the ways of doing GUI are bindings to external libraries (pytk is nominally part of the standard library but still requires you to have tk built for your architecture).
Well, your preferred approach translates pretty directly into Scala: for { x &lt;- list if(x &gt; 3) } yield if(x % 2 == 0) x + 3 else x - 3 (I'd probably write it as 6 lines, but mirroring the Python line breaks for the sake of the example) If you want to be lazy, just make it `list.view` rather than `list` - IMO laziness is the wrong default (I actually think this is a regression in python3 compared to python2).
The "attrs" library gets you closer to case classes, IIRC
It doesn't play nicely with mypy yet, so that's a 100% deal breaker for me. There is work in that direction though. I'm still looking forward to data classes since they'd be in the standard library
Thanks for offer. Here is my code. https://github.com/abhsrivastava/ThriftLearn
Live video and video streaming in general.
Catalog data pipeline for Walmart.ca. Basically, a whole lot of catalog data (products, sellers, etc) lives in US servers and has to be constantly sent to the Canadian Website (written in Play!). So I work on the pipeline that flushes all that data to where it needs to be in the Canadian website, along with a fair amount of business logic transformations that the business wants to add (things like special promotions, filterable attributes of the catalog, etc). So this is mostly a bunch of spark jobs and Akka HTTP services communicating through Kafka, Rest, and GraphQL, and persisting on Cassandra. 
Are you using Sangria? If so, any pain points?
Observatory control system at [gemini.edu](http://www.gemini.edu/).
Umm... that's mocking, just without using a library.
YES!!!!! we don't need libs for doing so. That is what I say to my team all the time. Thanks for reading
Python lists [are actually array buffers](https://stackoverflow.com/a/3917591/1518588) (not linked lists), but the more important point is that Python list comprehension and other syntax sugar only works with that default, hard-wired data type. Scala is a growable (or "scalable") language. It provides tools to abstract over concrete implementations, which Python completely lacks. For example, see how Scala for comprehensions can be used to manipulate `Future`s.
&gt; Python lists are actually array buffers (not linked lists), That's +1 for python, though. &gt; For example, see how Scala for comprehensions can be used to manipulate Futures. You can do that in any language which has abstractions but not as "nicely" as Scala because `for` is implemented with a structural type.
&gt; That's +1 for python, though. I'd rather say it would be a -1 if it was otherwise. I don't know any imperative language where linked lists are the default sequence data structure, but that would be a big mistake indeed. &gt; You can do that in any language which has abstractions And you can encode any abstraction in the lambda calculus, but that's beside the point. `for` is not the only "overloadable" part of the language. I'm thinking notably of custom extractors, by-name parameters, etc.
&gt; a fair amount of business logic transformations that the business wants to add (things like special promotions, filterable attributes of the catalog, etc). Lay's Ketchup Chips
An obscure but powerful machine learning technique - JSM method. https://github.com/DmitryOlshansky/jsm4s
I was at one of those talks and I'm definitely not convinced. Rúnar is, which has brought me back to thinking about it repeatedly, but I still don't get it. edit: makes me think it's more a problem with myself than the concepts, so I'm happy to hear others are confused.
Exactement
Doing micro services for one of Denmark's biggest newspapers: jp.dk
I really hate when someone posts a not-so-popular acronym and doesn't say what it stands for
I don't know if the guy is on reddit, but Ill post an issue to him!
Sry about that. It won't be any less obscure though as JSM stands for John Stuart Mill a XIX century philosopher. While it may hint at the origin of the formal framework of the algorithm, it's practically only a gesture to the great logician of its era.
Building out back office systems and public services using various components of the Akka Toolkit (Cluster Sharding, Persistence, HTTP), Play Framework and a little bit of Cats
My personal opinion is that it's too complex. Sangria has macro-based schema derivation for custom data types, but it's finicky and you have to line up the types exactly right, otherwise you get weird type errors. You have to pass your API data types through two levels of schema descriptions (not counting the original Scala source): JSON encoders/decoders and the Sangria schema. You still have to do a lot of plumbing to set up the Sangria query processing and server. If you're using Scala.js or another typed language on the client side, you also have to handle wrapping GraphQL requests and responses. There's a bunch of other stuff, but overall it's just a _lot_ to handle. Personally I just want a tool to take care of all the schema management for me, maybe something like PostGraphQL.
I'm not sure why but I did this. def negated(inputWords: List[String], includenT: Boolean = true): Boolean = negate.exists(inputWords.contains) || (includenT &amp;&amp; inputWords.exists(_.contains("n't"))) || (inputWords.indexOf("least") match { case -1 | 0 =&gt; false case idx =&gt; inputWords(idx - 1) != "at" }) 
Well, in the Scala world the ideal we work towards is more type safety than just naming conventions 😉 For example, there are a couple of ways to handle safe IDs. One cool way is using phantom types: case class Id[A](unwrap: Long) extends AnyVal object Id { // Typeclass instances like JSON codecs, etc. } case class Game(id: Id[Game], ...) case class User(id: Id[User], ...) So above, game and user IDs are just `Long`s at runtime but are statically incompatible with each other at compile time. Another approach is [tagging](https://github.com/softwaremill/scala-common#tagging) some basic type with another type without boxing it, in other words an alternative value class mechanism to using `AnyVal`: type Id = Long case class Game(id: Id @@ Game, ...) case class User(id: Id @@ User, ...) So above, `Id @@ Game` is also different from `Id @@ User`, but at runtime they're basically the same, `Long`. The problem is people don't come back and pay technical debt like statically differentiating ID types. I've actually only done it in one major case, to fix a UI bug and stop it from being possible any more.
&gt; I'd rather say it would be a -1 if it was otherwise. I don't know any imperative language where linked lists are the default sequence data structure, but that would be a big mistake indeed. I thought it has a linked list because "list comprehension" - these ppl... &gt; for is not the only "overloadable" part of the language Personally, I rarey use `for` because it has very poor performance as I've seen. Every abstraction has a cost but it'd be great if scala too would pursue the "zero-cost abstractions" mantra. One of the reasons I consider scala to be a good functional programming language is that it doesn't give up performance everywhere for theoretical features and we can choose how we want to do our job.
Don't you mean "one of Northern Germany's biggest newspapers"? 😉
Yes and this does give you some advantages. To _ME_ this is not worth it. It is getting in the too much boiler plate mode. The same reason I find it not worth it to write Java that much anymore. Depends what you are looking to do I guess...
Honestly for scala I'd do: ``` list.collect { case x if x &gt; 3 =&gt; if (x % 2 == 0) x + 3 else x - 3 } ```
&gt;&gt; If you want to see what logback is doing and which file it is using you can set a system property to debug it: -Dlogback.debug=true. &gt; Forgot that, very handy. I created an issue to mention this in the docs: https://github.com/playframework/playframework/issues/7708
Writing Mocks manually? No thanks!
Spark &amp; Play (with Akka) here. B2B marketing etc.
There is a lot of data engineer position which don't require data science knowledge at all
And your response re: Scala algebraic data types, use of the `Option` type to mitigate null references, pattern matching, full-featured anonymous functions, higher-kinded types, and strong information hiding &amp; modularisation features?
Well, you can't have it both ways, either you sacrifice type safety or you accept some boilerplate 😉
Working on an asynchronous data backend that uses Play!, Couchbase, Kinesis, Firehose and Redshift for omni channel communication. Sprinkle some akka for some of the kinesis stuff. Its been both rewarding and illuminating.
I switch hats between data pipeline (mostly Spark) and backend services (mostly Finagle). Throw in a couple other languages as the situation calls for some web dev, dev ops, etc
Last time I used it professionally, it was for backends for games via multiple self healing services. (Now Im on C#, which I dont like nearly as much). Each machine ran 8 VMs. Each hardware box had a mix of the various offered services. We could lose multiple hardware machines and still function without players knowing anything had gone down. I used to bring machines up and down while we did demos. Personally continuing my experiments on building robust services quickly to build truly horizontally scalable MMO style games. 
Low(ish) latency trading system at a big bank. There are quite a few banks using the language now. 
what do you mean by Data pipeline? 
When we talk about performance of data structures we often compare to Array which we say has "O(1)" access - but Array is simply impossible to implement in a way that scales to arbitrary sizes. If we said "Vector has O(log n) access" then, while true under the usuall definitions of those terms, in context this would lead to the grossly misleading conclusion that Vector had worse behaviour than Array when scaled to very large sizes. &gt; we need to accept that O(log2(x)) is also "effectively constant", a claim I think many would agree is absurd. I'm going to claim this is not absurd, in the context of ordinary (i.e. non-`BigInteger`-based) datastructures on the JVM. I accept that this implies that e.g. sorting is effectively linear time. `log(x)` is just so small that paying any attention to it obscures more than it clarifies.
Implementing a database query language. About the language: https://neo4j.com/developer/cypher-query-language/ Code: https://github.com/neo4j/neo4j/tree/3.3/community/cypher https://github.com/neo4j/neo4j/tree/3.3/enterprise/cypher
Newbie here. How does this deal with unknown words in the corpus? Doesn't this suffer from the zero frequency problem similar to normal classification algorithms such as naive bayes?
Although I never thought of it on the course of writing this post, "any log operations are effectively constant" is something I can get behind as long as it's applied consistently: e.g. an AVL-tree is just as effectively-constant as a 32-ary-tree-Vector, though with different constant factors. It's true that in the normal course of analysis, people make simplifying assumptions that aren't strictly true: "memory access is O(1)", "integer arithmetic is O(1)", etc.. However, these simplifying assumptions don't have the same "brings down the house" effect on Big-O analysis that "n &lt; K" does, I suspect because they don't go against the core premise of Big-O notation that `n -&gt; infinity`.
&gt; these simplifying assumptions don't have the same "brings down the house" effect on Big-O analysis that "n &lt; K" does, I suspect because they don't go against the core premise of Big-O notation that n -&gt; infinity. I don't really see that distinction. All of these simplifying assumptions can produce absurdities if you push them far enough (e.g. using arithmetic on large numbers as a data structure). All of them can help make the important parts clearer when used responsibly. That's also true of the "n &lt; K" assumption.
Sure thing, but at least personally ever since I started doing this, code has become a lot more "correct" and it actually solves the original problem you were describing. The only annoyance is the `.toString` on a `case class`, but then again, you shouldn't be using `.toString` for anything that isn't printing to the console 
If you are asking this question just to check someone's qualification for a scala job then you are going in the wrong direction. IMHO, the question should be like "what makes a good scala developer" regardless of what library/tools he has worked in, much easier to answer that. Spark has a good API to hide all the abstractions and that is how it should be. You should not measure language competency from that. 
In the end this is just a semantic discussion isn't it? "Effectively constant" is a meaningless term anyway. 
&gt; Personally, I rarey use for because it has very poor performance as I've seen. You know `for` is just syntax sugar for compositions of `map`, `filter`/`withFilter` and `flatMap`, right? It has the same run time performance as these operations.
Seeing sbt-eta and this blog post and is the first time I'm thinking that Eta might be viable. Nice work. I do have to wonder about use cases though. In many organizations it is hard to introduce functional programming concepts and libraries, due to fear of change, fear of not having people that can then understand the codebase, etc. Introducing another programming language can be much more difficult than that and in general I'm against it as well. I don't believe in being a polyglot, because in my experience polyglots end up with a superficial understanding of the used tools and techniques, plus not all people can cope with multiple programming languages, so a code base built like that leads to natural silos, as in people that only work on one piece or another, losing the big picture perspective of the project in the process. So I'm seeing this integration being useful for teams sold on Haskell and that want to migrate away from Scala, but can't do it over night. For me Scala is a language that's flexible and lovable enough and I've enjoyed working with it very much. My background has been with lesser languages and I'm naturally attracted to pragmatic solutions, so my viewpoint might be biased. But I also enjoy playing with Haskell and I might try some simple projects with Eta, as it seems very promising.
https://8thlight.com/blog/uncle-bob/2014/05/14/TheLittleMocker.html
Hi Alex! It's good to hear this project did trigger your curiosity and that you might give it a try in future :-) You know me well enough so I don't have to do my boring "Haskell &gt; Scala" talk, so I will skip that argument entirely and focus on your points (that being said, I think we would both enjoy discussing in details the run-time characteristic and how Eta is handling tail recursion... compare to something like Monix/Scalaz8/Matterhorn for example). Let me ask you this question: - Don't you think that ten years ago, one could have said the exact same thing about using Scala in a hybrid Java/Scala project? Actually, I remember at the time hearing the *exact* same arguments you are using here, take your text and do `s/Scala/Java/` and `s/Haskell|Eta/Scala`, you'll see what I mean! So yeah I agree, hybrid project should not be an end in itself, but it can be a useful step in the path towards using an other technology. Like folks used to have Java projects where the tests were written in Scala, I would personally avoid such half-arsed solution as much as possible... but it seems to be the only viable migration path in some situation. Plus, sbt is arguably a well featured build tool for the JVM, with a significant plugin ecosystem that could matter for some. 
&gt; You know me well enough so I don't have to do my boring "Haskell &gt; Scala" talk, so I will skip that argument entirely and focus on your points (that being said, I think we would both enjoy discussing in details the run-time characteristic and how Eta is handling tail recursion... compare to something like Monix/Scalaz8/Matterhorn for example). How does eta handle tail recursion?
I'm quoting Rahul Muttineni, the original author, who answered this at gitter (https://gitter.im/typelead/eta): &gt; We don’t do anything to handle direct tail calls per se. At compile-time, we optimize tail recursive-calls into loops and we use the fact that lazy evaluation controls stack growth (unless you’re being TOO lazy :grinning:). You can think of lazy evaluation as a hierachy of trampolines, but unlike general-purpose trampolines, each “trampoline” is specialized to a given type. And instead of executing a virtual method call after getting a Closure, it checks the tag of the closure which will help it select a branch to execute next. That should give you the basic intuition, feel free to join the room and discuss with Rahul directly!
Cool, this should be added to a FAQ somewhere Seems like a similar deal to what Scala is doing with the `@tailrec` annotation. The trampolines appear to be more performant (due to being specialized) but it isn't anything that Scala trampolines can't do
BSS (Business Support Solutions. customer care tools, sales channels, billing, rating and very importantly the integration between all these systems) to telecoms at [Qvantel](https://www.qvantel.com/) Currently working on a REST APIs and tooling around them. Mostly Spray/Akka HTTP and code generation from internal specification format (raw text file generation, macros and shapeless depending on the use case). Basically over the years ended up writing our own internal REST framework for applications that talk jsonapi.org format so current work is mainly maintenance work and new features for it.
Yeah good point about the FAQ! I believe it is something Scala trampolines can't do, at least as of today. The reason is that the "trampolines" (it is really thunk though) are specialized in Eta, which -to my knowledge- is impossible to achieve in Scala as a library (that would require significant support in `scalac` or a sophisticated compiler plugin... or even using multi-stage compilation with something like LMS). Basically you don't have to use `Trampoline` explicitly in Eta, it is dealt automatically during compilation! Then about the `@tailrec` annotation, I believe Eta can do much better thanks to lazy initialization: it can potentially deal with more case (like mutually recursive functions I think) than the highly specialized `scalac` approach (where you even have to annotate explicitly such method).
Raw logs -&gt; cleaned up/normalized events -&gt; data stores -&gt; interpretation of events -&gt; data stores -&gt; aggregations along common dimensions -&gt; data stores That's largely batch processing, though with some parts streaming as needed. The "interpretation" step is largely business rules but also includes some statistical analysis. The prepped data is then used as feedback into the systems, for usage based billing, reporting, ad hoc analysis, etc
&gt; The reason is that the "trampolines" (it is really thunk though) are specialized in Eta, which -to my knowledge- is impossible to achieve in Scala as a library (that would require significant support in scalac or a sophisticated compiler plugin... or even using multi-stage compilation with something like LMS). Dotty with the deep linker is actually fixing the specialization problem, and it can help in this scenario afaik &gt; Then about the @tailrec annotation, I believe Eta can do much better thanks to lazy initialization: it can potentially deal with more case (like mutually recursive functions I think) than the highly specialized scalac approach (where you even have to annotate explicitly such method). The problem with tail recursion has always been due to the JVM and not the language Scala, for example scala-native has proper full support for TCO almost since it has been released. Certain types of TCO don't play well with JVM semantics because JVM fundamentally deals with methods which imply having a stack
This is untrue, the problem here is really that Scala want to keep backward compatibility with Java. If it were not mapping 1 to 1 between Scala method and Java method that would be totally possible, and that's why Eta can achieve it on the JVM: Eta took the FFI path, unlike Scala! That really does not relate to JVM vs Native, it is orthogonal.
&gt; This is untrue, the problem here is really that Scala want to keep backward compatibility with Java. This is the exact same problem, you are just rephrasing it differently. The JVM does expect a stack trace with a method, at least if you want to use the many tools that the JVM has (i.e. debugging/profilers). It even breaks certain security assumptions in the JVM (this is why in the JVM you can't just jump to any method) It doesn't even have anything to do with java interopt (per say, you always have to interopt with Java at some level since the JVM is designed for Java), its the fact that a lot of java tooling that is designed to work with the JVM won't really work with eta &gt; If it were not mapping 1 to 1 between Scala method and Java method that would be totally possible, and that's why Eta can achieve it on the JVM: Eta took the FFI path, unlike Scala! Sure thing, but it also breaks a lot of assumptions that are implied on the JVM, as mentioned before &gt; That really does not relate to JVM vs Native, it is orthogonal. Yes, and this is the precise point. The JVM is based on certain assumptions which are implied (which eta is choosing to ignore, this is its own decision), where as llvm has no such restriction. Scala is choosing to provide "idiomatic valid JVM bytecode" where as Eta is not (by the sounds of it)
&gt; The JVM is based on certain assumptions which are implied (which eta is choosing to ignore, this is its own decision) Could you please precise your thinking here? The JVM bytecode produce by Eta is 100% valid, I have the feeling that you are mixing Java the language with the virtual machine here. Do you have reference to the definition of what is "idiomatic valid JVM bytecode"? Is that a common concept used by the JVM designer or people who wrote the specification? 
&gt; The reason is that the "trampolines" (it is really thunk though) are specialized in Eta, which -to my knowledge- is impossible to achieve in Scala as a library Hi Alois I believe specialized trampolines is what I have proposed in a PoC (for IList only but could be generalized) here: https://github.com/scalaz/scalaz/pull/1366 But I agree that it could be much prettier with a compiler plugin. Do you think that approach could be worth experimenting in scalaz8?
Just dropping by with some points: - I debug bugs in the runtime happily using VisualVM and we even have a command in etlas called `etlas run --trace` that traces your entire program using standard JVM logging frameworks + a Java agent. You just need a rudimentary understanding of the internals to be able to map back the errors to the original function definition, and often times, you'll at minimum know which Eta module caused the exception since the naming of methods/classes was done to make it simple to read. (We still have a lot of work to do in that department, but it's just a matter of time.) - As of Eta v0.0.9, the stack trace is the source of truth for what's going on in your Eta program. In the previous versions, we maintained a stack for collecting update frames (for lazy evaluation) and function application frames (for currying) which hid what was actually going on when you finally got a stack trace. In v0.0.9, we rewrote the runtime to remove the separate stack altogether and the stack traces give much more information than before (with a significant performance increase as well!). On another note, the old approach of maintaing a separate stack did not in any way affect the validity of calling into Java, since we successfully wrote Spark jobs in the old version! I feel this is a common misconception that generating custom bytecode with a different evaluation model cannot allow you to interoperate with existing JVM code. 
Listen. Iterating over arrays is effectively constant, because arrays in Java can't exceed Integer.MAX_VALUE elements. 2147483647 is a constant number. Therefore iteration is constant, not O(n)! Everything you've been told about arrays is a lie! On a more serious note, saying vector operations are effectively constant is like saying TreeSet-based operations are effectively constant because the height of the RB tree is bounded to a constant value based on Integer.MAX_VALUE. While this may be true, it's still a lie in terms of Big-O analysis.
Note inferred type and real type not always match. What type `results` has? In particular what type `results.get` has?
`results` is a `Map[FieldName, ?]` (`?` = `Any`?) `targetField` is an `org.dmg.pmml.FieldName` (I managed to make it work via cast, but I still want to find out what the `?0` token is) **EDIT**: regarding the type of `results.get`, I'm not sure. `results` is set like this: val results = modelEvaluator.evaluate(arguments) `modelEvaluator` is an instance of `org.jpmml.evaluator.tree.TreeModelEvaluator` (implementation [here](https://github.com/jpmml/jpmml-evaluator/blob/bb37828bceaf386cc7ecaeddd9d0816d9f0e7f50/pmml-evaluator/src/main/java/org/jpmml/evaluator/tree/TreeModelEvaluator.java)) This classe's `evaluate` returns a `Map[FieldName, ?]`, but I don't know what `?` is in this case
Agreed, good points.
Seconds my experience. I see both terrible Python &amp; Scala code. PRs are often merged recklessly, reviewed with nothing more than a glance, in the name of faster development (oh the irony! bash out more code!).
Question mark in Java is turned into underscore in Scala. So it basically means `Any`. Thus it's impossible to make compiler happy without cast.
Salut Jean-Baptiste! Yeah you are right, what I had in mind was some form of automated optimization depending of the type being used or stuff like that, but my original statement is wrong. In general there is no reason one could not specialized a Trampoline/Thunk impl. in Scala! It is just that having a proper stage (which you might get with a compiler plugin, but for sure with LMS) for it open room for better optimization. I think that approach is similar to the one I did experiment for the IO system in scalaz8 (not the matterhorn approach, but really the latest experiment in scalaz8). It might sounds weird that I relate such feature with IO, but the thing is that IO can be seen as a sophisticated Free, and interpreting such structure efficiently rely on similar technic. As I am not contributing actively in Scala at the moment, I suggest that you get in touch with John De Goes. I guess that could apply to his current work on IO/Free. 
Confirm. I work on bigdata platform for Nordea Bank. The whole platform is written in scala.
&gt; You know for is just syntax sugar for compositions of map, filter/withFilter and flatMap, right? Two comments ago I've said that "... because `for` is implemented with a structural type." - yeah, I know well how that or the entire standard library are implemented. &gt; It has the same run time performance as these operations. That's not good generally. It's ok sometimes like when you're combining futures but I prefer simple high order functions and tail recursion.
So in the case of a Visitor(?) class, a cast will always be needed?
The fact that theyre using *O(log2(n))* and *O(log32(n))* shows they dont understand big-O notation at all. 
Having had to argue against someone who is otherwise smart making the first statement you made was a down point of last year. 
Some blog posts are an "Effectively Pedant" waste of time
It looks like it. Any time you have a `Map`(1) with values that could be anything, then the compiler can't know ahead of time what the type of any particular value you pull out of the map is. If what you have is a heterogeneous `Map`, there are a variety of approaches to implementing those. In Java-land, one of the most common is to have a map whose values are some wildcard type, or even just `Object`/`Any` directly, where users of the map are required to cast the values extracted from it. Note that in your error message, `?0` is the name the compiler gave to the wildcard type in your map's type signature. - 1: I'm guessing your example uses a `java.util.Map`, but the situation would be the same for `scala.collection.{im,}mutable.Map` too.
Bioinformatics.
If you're like me, you've never heard of Eta before: http://eta-lang.org/ 
Ahh, fair enough. 
&gt; where you even have to annotate explicitly such method No, `@tailrec` only purpose is to output an error if the compiler was not able to perform tail call optimization, but the compiler will optimize tail recursive methods even if they're not annotated.
GraphQling your back-end , like a database, is more complex than a REST API indeed, but consuming a GraphQL service..such a front-end, is just b-e-a-u-t-i-f-u-l and really worths the hassle. Composing your consuming data structure, with only what you need is much more better than orchestrating a REST service with multiple requests. We only use pure Javascript on front-end , so I believe it's much more easier than with Scala.js
While I am a fan of /u/lihaoyi's work, I don't like this article. The author tries to be pedantic, but unfortunately misses the point. If you really stick to the definition, then Scala's vector is indeed `O(1)`: * For `f(x)` being `O(g(x))`, two values must exist: a real factor M and a constant x0, so that for all x&gt;x0 where both functions are defined, `|f(x)| &lt;= M*|g(x)|` * With x being the length of the `Vector` and f being the runtime of an operation over that `Vector`, f is only defined for values up to Int.MAX_VALUE * If you chose x0 bigger, there remains the empty set for x&gt;x0 and f and g are defined for x * For the values x in the empty set, all properties are true * This includes the property necessary for being in O(1) So from a really pedantic point of view, all operations in `Vector` are in O(1). The documentation is technically correct (and that's the best form of correct). From a more practical point of view, most `Vector`operation run approximately on the same time, for all valid sizes of a vector. This is because log over 32 is a really flat function, even over all 32 bit long numbers. And I guess, that is basically meant with the `O(1)` notation in the documentation.
Yeah, I can imagine ... my ideal for a frontend app would be a websocket connection to a backend and a custom-defined API. That should probably speed up the frontend too.
My thoughts exactly
&gt; "Effectively constant" is a meaningless term anyway. I totally agree! That's why I think we should use less meaningless terms, like `O(log(n))` or `100ns per operation with 1,000,000 elements` or [`~10x slower than arrays from 200 to 1,000,000 elements`](http://www.lihaoyi.com/post/BenchmarkingScalaCollections.html#concatenation-performance). Why should we use meaningless terms in our docs, books, blog posts and stackoverflow answers when there are perfectly good meaningful terms we could pick up instead?
Everything you has said could be taken to be correct, but if we do we can apply the exact same 5 logical steps to say that `O(n)` algorithms are effectively constant time when limited by `Int.MAX_VALUE`. "log over 32 is a really flat function" is rather fuzzy logic. It's just as flat as "0.2 * log over 2". If we admit that `log over 32` is flat enough to be `O(1)`, but `log over 2` isn't, that means that calling an "`O(1)`" `log over 32` function 5 times is no longer `O(1)`. After all, 5 times of `log over 32` is *exactly* equivalent to an `log over 2` function. Or we could accept, as md50 suggests, that `log over 2` is "effectively constant" too. Thus quicksort is `O(n)`, quick-select is `O(1)`, balanced AVL/RedBlack tree operations are `O(1)`. That is also a self-consistent view of the world. (All this is in the blog post)
I agree this blog, and I will point out that "hash table operations aren't constant time" for the same reason, considering when n→∞ the hash value should be a `BigInteger`.
Agree completely. The question in contention is whether the vector operations are in "effectively constant" time. But the article spends most of its time simply pointing out what everyone familiar with the implementation already knows: that the operations are in fact in O(log n). In other words, the argument relies on the lemma that there's no coherent sense that can be given to the word "effectively" here, so that we can essentially ignore it and just talk about whether the operations are constant. But that lemma is only supported briefly, and largely by appeals to burden-of-proof shifting. For example: &gt;Now, we *could* be using some heretofore unheard of analysis technique that's not one of the two described above, but until someone rigorously describes that technique to me, I have to assume we're using one of the well-known approaches mentioned here. - &gt;For this argument to be valid, there needs to be some reason why f(x) &lt; 6.2 is enough to call f "effectively constant", but f(x) &lt; 31 isn't. If you don't buy that a mode of analysis needs to be "rigorously described" to inform development, or you have some inkling of why the factor of 31 might be significant in a way that a factor of 31 is not, you probably won't find this very convincing.
Also array access is not `O(1)` either in theory or in practice. In theory, when i→∞, `i` need more than 32 bits, hence `a[i]` requires more complex instructions on the CPU. In practice, the performance will be hurted due to cache missing or virtual memory swapping when `i` is growing. That is to say, all `O(1)` data structures are lies.
Right, thanks for bringing that up. Will that annotation still be useful with dotty? Or it will be able to perform general tail call optimization? Because even if it's not mandatory to use it, in most production code I've seen it's used all the places as it seems relatively hard (and arguably not so user friendly) to work out if the optimization will take place or not.
`getClass` does not give you the wrong class. The difference you're observing is between compile time and runtime. At compile time, the way you declared `results` makes the compiler think its keys are not of type `NodeScoreDistribution`. At runtime, where it can inspect the actual values, it finds out that this very specific one is of class `NodeScoreDistribution`. If you want to fix the compiler problem, declare `results` to be of type, I would assume, `Map[String, NodeScoreDistribution]`.
Your conclusion about sorting being in O(1) when limiting the length of the input is absolutely correct (think about a big pre computed lookup table). The problem is, that big O allows for a finite number of exceptions (hence the x&gt;x0 part). If you only have a finite number of samples, then they are all exceptions. For the remaining elements (empty set...), all properties are true (because you could never construct a counter example). Big O deals with a lim -&gt; infinity. It is pointless for finite input values. So with this in mind, the Scala documentation for Vector is pointless, while being technical correct. Often when dealing with big O, one needs some assumptions about the underlying machine when generalizing the algorithm for infinite large inputs. For arrays one could assume, that accessing an element at given address (index) takes constant time. This is not very practical: for really huge arrays, a given address might be bigger than a machine word. Or not even fit in the main memory at all. Or have more bits than there are atoms in the universe. But we abstract this away and just assume that for any given input the machine is big enough and fast enough that it can execute the array access in constant time. So to get big O to work for Scala vectors, we have to generalize the implementation, so it works for arbitrary big inputs. My proposal: for every input we assume that the underlying tree structure is wide enough, so it has a fixed depth of six. So the underlying, generalized machine has some "tree optimization". With this assumption, the original O(1) assertion should still hold, but now in a meaningful way, without the "finite number of exceptions" loophole.
&gt; In practice, the performance will be hurted due to cache missing or virtual memory swapping when i is growing. I don't think this is correct, at least for primitive arrays. An array in Java/Scala (or C/C++/etc) is always in a single memory allocation, so if you need to swap memory to access `a[Int.MaxValue - 1]`, you also need to swap the same memory to access `a[0]`. The _size_ of the array does matter, just not `i`.
&gt; for the same reason, considering when n→∞ the hash value should be a BigInteger. Even before that, as the table gets full enough.
My own quibble would be with &gt; However, cache effects and similar things are irrelevant when analyzing the "asymptotic" Big-O performance of operations on a data structure. In such a scenario, you only want to analyze the behavior of a data structure in an idealized setting. This idealized setting can certainly include cache effects, if that's what you are interested in: see https://en.wikipedia.org/wiki/Cache-oblivious_algorithm for an example.
**Cache-oblivious algorithm** In computing, a cache-oblivious algorithm (or cache-transcendent algorithm) is an algorithm designed to take advantage of a CPU cache without having the size of the cache (or the length of the cache lines, etc.) as an explicit parameter. An optimal cache-oblivious algorithm is a cache-oblivious algorithm that uses the cache optimally (in an asymptotic sense, ignoring constant factors). Thus, a cache-oblivious algorithm is designed to perform well, without modification, on multiple machines with different cache sizes, or for a memory hierarchy with different levels of cache having different sizes. Cache-oblivious algorithms are contrasted with explicit blocking, as in loop nest optimization, which explicitly breaks a problem into blocks that are optimally sized for a given cache. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
`?0` means some unknown type, a `?`. The compiler numbers them so that different ones can be distinguished.
Don't move to scala without someone who is good at scala on the team. The vast majority of the places I've worked have had 'polyglot' devs who decide to use scala for various reasons, some of which must have been good reasons. But without someone who actually knows scala well, the code will be shit. When you eventually hire scala devs who know what they're doing, they won't like the code and the existing devs will be so used to doing it in their own manner they won't want to change. You end up with perpetually inexperienced devs who don't want to learn to use scala to its fullest potential, because anyone who knows the language and wants to use it to its fullest will leave. You will end up with a worse project than if you'd just used java. I've seen it everywhere. Start right, don't just assume because there's a dev that can write passable JS and passable C++ that they're going to pick up scala and write good code.
&gt; I prefer simple high order functions and tail recursion What do you mean by "simple high order functions"? The main cost incurred while calling something like `Seq.map`beside rebuildind a new collection is the cost of indirection because of the closure you pass it. Any higher-order function is going to pay that cost too.
That's a pretty broad question. I've found Scala has reduced the defect rate enormously, I think mainly because so much more can be done in the language proper rather than with annotations/reflection/monkeypatching/metaprogramming that tends to be a source of bugs. I've actually found that effective Scala is so much more reliable that I can drastically reduce the amount of tests I write, which took a lot of getting used to but obviously ends up speeding up development and maintenance a lot. Performance is fine, faster than scripting languages and not noticeably worse than anything else in the kind of use cases I've seen. There's a lot of flexibility in the language, which makes it easy to bikeshed or, conversely, to adopt a single coding standard that then ends up grating on a lot of people. I recommend tolerating a bit of diversity in coding styles, but having code reviews for everything, so that every piece of code is at least read by 2 or 3 people and you ensure that there are multiple people who understand it. There's also a lot to learn - I think people don't realise how valuable experience is in a language like Scala, because in most languages after a couple of years you've learned everything there is in the language proper and all the libraries are very domain specific, so maybe a senior dev knows 3 different ORMs and 5 different web frameworks (say) but that's not actually a whole lot more useful than a junior who just knows the one ORM and one web framework you're using. Whereas in Scala there are these deep libraries that are complex and detailed but also very general, so the learning curve doesn't flatten out until later, and experience becomes more valuable not because a junior Scala dev is less productive than a junior in another language but because a senior Scala dev can do more than a senior in another language. But make sure you get people who are willing to teach and share their knowledge, and also - to be honest - to "dumb it down" in some cases if the rest of the team isn't ready.
From watching actual large and ultimately successful project - not sure if Scala-specific but this stuff helped a lot: * Automated testing / TDD: helps as you grow the team/different skill levels. * Continuous deployment/auto-build - DevOps * Slack or similar - with bots from above functions * Disciplined Agile - proper sprints 
CRUD mixed in with some occasional fun VRP (vehicle routing problem) at a Aussie startup and Scala powers pretty much the whole backend. We're probably one of the very few companies that uses Scala but not Spark (or any Big Data^TM), at least for now.
As others have mentioned, it is crucial to have at least one experienced Scala engineer on your team who knows how to write idiomatic Scala, can review PRs, teach new developers and enforce some consistency across the code base. The issue is that these kind of experienced Scala engineers are in short supply and hard to come by. This is largely because they arrived at their level of knowledge through years of trial and error, as Scala currently provides no scalable way to increase their numbers. There is no single resource I could point out to developers with good conscience and say "if you have read his and understood it, you are capable of designing a Scala stack and leading a team effort to implement it". Documentation on Scala is largely out of date, missing or incorrect. As an example, the glossary hasn't been updated for the last four major releases of Scala. Core concepts like "implicits", "context bounds" or "higher-kinded types" are not even mentioned, and other things are wildly out-dated. No work on that content has been done since it was added 6 years ago. Most other parts of the documentation are in a similar state of disrepair. Scala is certainly fun and very productive if you know what you are doing, but you have to keep in mind that the developer experience has not been an area of focus and likely never will be. I wish you all the best!
The situation in Dotty right now is similar to the one in scalac. For some discussion on the challenges related to general TCO, see https://github.com/lampepfl/dotty/issues/1497
My experience: * definitely fun * improves the development speed if you know what to do * allows people to pile up crap because they don't understand the language, and because the language is very flexible and concise - they can pile up more crap per LOC than in Java * hard to on-board an average java-ejb-spring-xml developer - they have no clue and no desire to learn something new. * hard to find a good dev who knows how to write an idiomatic code * you can't outsource anymore, unless you want to deal with the sickening nonsense from india teams * you go shapeless - noone understands what you do - in 4 months you have no clue about your own code at a glance * you go scalaz or cats - the pretty much same happens in one week (previous item) * HR hates you because there's no a single Scala dev with experience in the wild to catch, and you demand more of them * your boss is fascinated with the code that he doesn't understand but scared to death of the prospects of what will happen if you leave or die
Forgot to add - a bunch of Scala/Spark developers who have no idea of anything but simple SQL queries and RDD operations, also listing things like "wrote UDF for data transformation" as a medal of honor in their resumes. HR hates you even more - they bring up the people who have all the buzzwords in the resume, but you weed them out after 5 minutes of phone screening.
&gt; . You just need a rudimentary understanding of the internals to be able to map back the errors to the original function definition, and often times, you'll at minimum know which Eta module caused the exception since the naming of methods/classes was done to make it simple to read. Which is the fundamental point I am getting it .Obviously its possible to rewrite the JVM bytecode to eliminate tail call recursion, but what you end up debugging is very different compared to your actual source code (which means the profiling tools become a lot less useful)
I started writing Scala when the company I was working at did "the rewrite". Basically we changed our stack from PHP/Zen/JQuery to Scala/Spray/Akka/AngularJS. No one in the team had any real experience with Scala (or functional programming for that matter). I said then and there that it was a bad idea, which it kind of was. The first few months using Scala were an absolute pain and I don't think a single line of code written in that time survived into version 1.0. There's no doubt that a switch like that comes with a very step learning curve. But once we got the hang of it our codebase became both more stable and easier to read. It also helped that we brought in some experts to look at what we've done and give constructive criticism. As for development speed... I always find it hard to quantify that, but I'd say that while it's often quicker to just throw something together in something like PHP, we saved a lot of time by having code that's easy to refactor and reuse. And less time was spent fixing bugs once a feature was finished. And as for fun - years later Scala is still my favourite language I use every day. As far as I'm aware most of the team we had back then is still writing Scala. Finding new devs is kind of a challenge, because there aren't many experienced Scala developers and a lot of them come from the financial sector (read expensive). We ended up hiring Java developers and retraining them. A good programmer will be able to get into the groove fairly quickly if she's surrounded by a whole team of competent developers who're willing to help. All in all I'm not sure jumping of the cliff like that was a good idea for the company, but in the end our software benefited enormously, even if the "big rewrite" took way longer than originally planned.
&gt; What do you mean by "simple high order functions"? That I prefer to use map/flatMap/filter because they don't really hide what they do as `for`. Also, `for` makes people think that it's actually loop when it isn't. When you tell them they'll understand but how many scala user are there who don't know?
This should be up-voted more. This is for any language, but the barrier for learning Scala is pretty high. 
I kind of agree, but on the other hand, it would be better to simply argue that logarithmic algorithms are generally quite good. I always thought people simply leave the base off the logarithm altogether, since it's arbitrary.
It depends on the kind of effort. In Scala if your code is reasonably well-typed, the effort will be spent mostly squashing type errors. Once all the errors are squashed, usually the refactor is done. On the other hand, with Python you're kind of dependent on your test coverage to tell you when a refactor is done. So to me it seems the effort has a 'long tail' of time spent squashing problems possibly some time after the refactor is done. So, considering the level of certainty you have after a refactor, I'd say Scala requires less physical and mental effort.
Thank you. Do you think there's some point of break even where the team gets more productive compared to other stacks due to scalas density/flexibility for example?
Scalaz seem to be the end boss, I'll check that :)
Sure, this may apply to every stack. 
&gt; you go scalaz or cats - the pretty much same happens in one week Not true. foldLeft (Foldable/Traverse), map (Functor), flatmap (FlatMap/Monad), and .filter (MonadFilter/TraverseFilter) are nearly the same in scalaz/cats as in the std library. Traverse has a .sequence that works for any container, rather than just for List[Future] from the std lib. Applicative adds .pure, which is a constructor for a container and ap, which is map where the mapping function is in a container. Semigroup lets you add things together using combine. SemigroupK lets you add containers together. Monoid lets you add things together and start with an initial empty element. MonoidK lets you add containers together and start with empty containers. These things allow you to use smaller interfaces -- if you use .map only, you don't need the whole Traversable interface. If you only map, there's no reason to restrict the accepted argument to any single concrete container type, it will work for all of them. When added as context bounds, they act as documentation of the type of things you can do with a particular argument in a function. If my arg:A is only provided a Functor in the context bounds, I cannot call sequence on it. A only has .map. Because the interfaces are small, it makes it possible to memorize them easily, increasing the self - documenting of functions that are written with the cats typeclasses as implicit args or context bounds. Because they are generic interfaces, it is easy to implement a map method on your own custom datatypes, which can be tricky with the standard library collection types. You can, of course, use this stuff without adding the context bounds or using implicitly by importing import cats._ import cats.data._ import cats.implicits._ but you lose the documentation aspect of the type signatures, and often end up with code that could have applied to any container coupled to a specific container implementation, which usually means you end up repeating yourself. This all should lead to more readable code made up of simpler (more restrictive, smaller interface) compiler-verified pieces. *IF* you know the basic typeclass interfaces, haven't used all symbolic operators, and know how to read context bounds. However, your function signatures will be more complex, as the bounds and generic names provide more semantic information in the type signature. There are other features Free/Inject/NaturalTransformation in the library that allow you to turn any series of operations into a Applicative/Monad, which is handy to convert things from a business domain language level (GetUsersNamed("Bob")) to a low-level language domain implementation (SelectFromWithName("table", "Bob")) the ability to execute it in many different ways (prod and test, synchronous or async, parralel or sequential), and to execute any recursive function in a stack-safe way. Edit - forgot to add my critique: The problem with using cats/scalaz is that quite often senior devs are the only people who know the interfaces. Junior/new scala devs see the more semantic type signatures and get frightened. There's already so much to learn with Scala that having to recognize map and combine as *separate actual things with their own properties* is just one more level in the we're really not in Kansas anymore first 6 week scala freakout. The typeclass names don't provide semantic information to a new to user, and make people think they have to learn Haskell to read to scala code. Changing the names makes it difficult to Google information on what you are looking at. Many times you don't know you already have something general that you just invented in a concrete way because you can't search documentation by type signatures (List [A] =&gt; B = foldMap). Lots of times you do something convoluted with a fold because you don't know the more obscure Optional interface, or how to work something into whileM. Converting from libraries that return specific container types can be a pain, ESPECIALLY if your library returns Future or Task or some other asynchronous thing. It can be difficult to convert a Task[Int] to a generic F[Int] in a straightforward way even if the F it is called with is Task, and returning an F[Task] is leaky and makes me feel dumb, and converting at the callsite as Task [Task[Int]] using ap/map feels like all I have is a Hammer and everything looks like a flatMap (identity) call. Staying in a Stack of Types can get tedious to maintain, and resorting to Freestyle macros feels like I'm using a framework, even though the generated code is boilerplate that I call rather than the other way around.
Absolutely; for my first team the break-even point was about half an hour (we were already familiar with the Java ecosystem and only had to pick up the language, we had the advantage of all starting at the same time, and we were selective about which parts we adopted), at least for production-quality apps or compared to Java[1]. It's more that your productivity keeps ramping up long after that, well past the point where it would have plateaued on other stacks. [1] It took maybe 2 years of Scala before I'd say I could do a quick throwaway prototype as fast as Python (though I wasn't doing Scala full-time at that point).
I think the problem is just a corner case! In reality you could test a *repository* with an *integration test*, as I wanna make sure it works. It is of much more interest to fake a repository within domain or service classes! Those are the places one *uses* a repository, and there you *should* better replace a real implementation with a fake object in order to fullfill the *isolation* aspect of a *unit test*. And as other redditors allready said: The OP does mocking - just by hand and without any framework 😉
Yeah its a meaningless term the way it is presented, and it is kind of confusing to me. This is I think the spirit of the term: If we say `T(n)` is cost to lookup an element where "cost" is defined as pointer accesses, T(n) ranges from `0 to log_2 Int.MAX=32` for a binary tree, but from `0 to log_32 Int.MAX=6` for a 32-ary tree. I think you bring up a good point though that it would be much better to say something like the above than make up a term thats potentially confusing to many people. I think we know theres no simple way to speak of performance trends in more detail than big O (which is very zoomed out) but less detail than hard numbers on a test rig (very zoomed in).. so its OK to abuse big O notation a little bit if it helps get the point across - why not just write `O(log_32(n)) ~ O(1)` for the vector and `O(log_2(n))` for the rest? Yes, technically its meaningless but it DOES convey more information if you understand the context 
&gt; Don't move to scala without someone who is good at scala on the team &gt; But without someone who actually knows scala well, the code will be shit Truth.
fwiw docs.scala-lang.org had an update recently http://docs.scala-lang.org/ Merged 9 days ago https://github.com/scala/docs.scala-lang/pull/806 
I'm aware of that. Could have had that--and much more--2 years ago already.
I probably did it wrong, but I found it fantastic to use Scala in all its multi-paradigm glory. I took a little from column A, and a little from column B. I built a super nice (least I think so : ) Finite State Machine. It relies heavily on traits to add behaviours to the system. Like you can do (forgive any syntax issues, this is from memory, and Ive been doing C# (shudder)^([1])) object WaitForLogin extends State with Retry with Timeout { retryCount = 10 timeoutTime = 10.seconds } This uses the great trait system, as well as inline unnamed class definition. Simply a wonderful language. Its also powerful enough to put string interpolation in a lib and not a compiler feature. I always felt that its a good barometer on how powerful a language is how much you can build in the libraries vs needing to hit the compiler. While I think its totally doable to bring up a newish team into writing a scala stack, I would say next time I do that, Im going to lay out a set of rules of thumb and a better low level, company specific libraries that show how to solve common problems. We ended up solving similar problems in radically different ways cause we didnt have a sort of company style. (I dont mean the trivial stuff like braces, I mean the deeper semantic ways to approach and solve things). --- ^([1]) I mostly like C#. It has some _great_ things like proper reflection that make writing custom tools a breeze and its super cool easy access to compiler services. Oh, and a solid user defined value types. But it doesnt quite have the expressive power of my fav Scala
&gt; I'm interested in topics like: maintain ability, ~bugs per deployment, on boarding of new members, ability to move written software over different teams, development speed, performance of the artifact and of course fun. Do I miss something? Feel free to add :-) I've used Scala for all sorts of things over the last 6-7 years, from distributed-systems to fast-iteration web stuff to CLI tools to highly-parallel job-orchestration engines. My experience has been very positive. Due to Scala's conciseness and decent IDE support, I write code fast up-front; it's easily comparable with a dynamic language in this regard. More importantly, it's also faster to develop in Scala down the line compared to dynamic lanugages, or even Java, due to Scala's type system and standard library facilitating catching errors early. What drew me to Scala initially after many years of working in Java was that Scala basically made a ton of the bullet points from Josh Bloch's *Effective Java* the defaults, or otherwise baked into the language. Scala made all the things I was already doing much easier, which felt great. I've seen Scala used on teams ranging from 2 to 8 devs, with all senior devs to a mix of junior and senior ones. It generally went well, provided 2 things: - There was at least one skilled Scala dev on the team. - The team agreed on a set of Scala features to use, and a general style (note that these things can evolve as the team gets more comfortable and skilled). 
&gt; This is not very practical: for really huge arrays, a given address might be bigger than a machine word. Or not even fit in the main memory at all. Or have more bits than there are atoms in the universe. But we abstract this away and just assume that for any given input the machine is big enough and fast enough that it can execute the array access in constant time. Not all assumptions are equal. With "Int operations are constant time" or "Memory/Array lookups are constant time", those assumptions have holes but you can still do interesting analyses with them, despite a few shortcomings. With "max size of input is N", *every* algorithm in the entire world *instantly* collapses to `O(1)`. This is stated in the post, and so far nobody has given a real good answer why `f(n) &lt; 6` is constant but `f(n) &lt; 31` or `f(n) &lt; 2147483648` are not. &gt; My proposal: for every input we assume that the underlying tree structure is wide enough, so it has a fixed depth of six If follow this idea just one step further, you would find that while `apply` becomes `O(1)`, `append`/`prepend`/`head`/`tail`/`insert` are now `O(n)`. Your theoretical "fixed height Vector" has the exact same time complexities as an Array, which makes sense since an Array is just a fixed-height-vector with height=1. The whole point of Big-O formalism is to protect us against hand-wavy sounds-good-but-trivially-disprovable ideas. The original "effectively constant" thing is one of them, and the incorrectness of this "wide enough tree" thing demonstrates how easy it is for anyone to fall into logical inconsistencies if you ignore the formalism.
I've met a good number of highly skilled Scala devs who often do not use IDEs. IDEs certainly help with many tasks, such as refactoring, code-navigation, and auto-completion ... however all of that can come with a cost of performance and even significant input lag. Simply put, a lot of more complex or large Scala codebases are extremely taxing in IntelliJ or Eclipse, actually slowing down how quickly you can code as you wait for the IDE to catch up.
I use Atom with the Scala and sbt plugins and it works pretty well. 
Yes - grep, good filenames, the repl, incremental comoilation, library documentation jars, and type annotations go a really long way. These things combined with an editor with syntax highlighting kind of make for an ide based on standard system tools. As an example, in emacs with projectile and autocomplete (just completes words from open files) and sbt-mode, you don't even need an ide for completion. Just run ~compile while you edit files and use grep/google scaladocs to find definitions. This works even for very large programs -- grep and Google are *really* fast, as is incremental compilation. Edit2: The debate is, at this point is emacs really just an ide? Edit: Not being able to jump to source before running grep is kind of annoying, but you can write a quick function in most editors to run grep and search the results for the first line that has class, trait, def, val, or var in it that matches the term under your cursor on demand, and opens the file and jumps to the line or runs a Google search for the scaladoc if it doesn't find any results.
For many smaller things, I've found I can write them entirely in [Ammonite](http://ammonite.io/), testing them interactively, and just pasting them into the main codebase once they're done.
Personally, I wouldn't ever want to write Scala (or any statically-typed language) without an IDE. That's a decidedly un-hip viewpoint these days though. (I prefer Eclipse, no less!) Lots of well-known devs write Scala without IDEs, so it's definitely possible. I don't know what percentage that is, but it's low, at at least at my work.
Emacs isn't an IDE?
I don't think that's unhip. You should use the tools that make you productive. Just don't force your choice on everyone else that doesn't use your tools. 
Does tags work for Scala? That will fix the grep issue
It's text editor, not a development environment necessarily. Granted it's also a lisp interpreter and therefore Turing complete, so it can be whatever you want it to be. IDE, OS, a floor cleaner AND a desert topping. 
I'd say perpetually unprepared instead of perpetually inexperienced. Reading books like Programming in Scala and FPiS instead of just diving in helps a lot. Not everybody does this.
&gt; you would find that while `apply` becomes `O(1)`, `append`/`prepend`/`head`/`tail`/`insert` are now `O(n)` This really depends on how you define the "tree primitive" that work on this "magically wide enough" tree. The situation is similar with the "magic array": Just think about an algorithm, that stores in each cell the index of that cell. When index gets more and more bits, the array cells must just "magically" become wider and wider at some point. Otherwise either they could not hold the index, or array access wouldn't be possible in constant time anymore. With the "magic tree" the situation is similar: We have to carefully define the "tree primitives", so when we add more and more nodes to the tree, it magically becomes wider and somehow rebalances. Depending on how we define the costs on these "tree primitives", we can make `append`/`prepend`/`head`/`tail`/`insert` arbitrary expensive with regards to runtime complexity. So you have chosen primitives that put these operations in O(n). That's completely fine and proves your point. I could probably easily define the "tree primitives" in a way so that the operations are in O(1) (I didn't check if this wouldn't lead to contradictions) The reasoning behind primitives making tree widening cheap would be, that the underlying machine can deal efficiently with all trees we are interested in. Just like the array machine is just big enough to work efficiently with all arrays we are interested in. It's just an abstraction for: "When we get to see the input, we will choose a machine with 'enough bits'. When the problem gets harder, we will migrate to a bigger machine. To simplify the analysis, we will not consider the cost of that migration." 
Lots and lots of people use IDEs. It's only a very small minority that don't, they're just the most vocal about it.
&gt; I could probably easily define the "tree primitives" in a way so that the operations are in O(1) (I didn't check if this wouldn't lead to contradictions) Please do share if you do define it! Especially if you can do so without words like "magic" "magically" and "somehow". I believe such an `true O(1) everywhere` definition of an immutable Vector would be worth multiple PhD theses, but perhaps I'm wrong.
Shameless plug: https://github.com/luben/sctags It supports ctags and etags generation
It's totally easy, you could do it yourself. The only trick is to define the underlying model accordingly. If you don't like the term "magic", you could just replace it with "cleverly chosen definitions". The whole big O thing depends on how you model / define the primitives of the underlying machine. For example it's possible to create sorting algorithms that sort in sub-linear time: the underlying model allows parallel executions and counts them as only one step. Such a model is not only possible, but also allows useful predictions in the real life: sorting with (arbitrarily wide) SIMD primitives. Similar things are possible for our Vector problem: when we allow (arbitrarily wide) tree primitives with a corresponding cost model, we can make the algorithm quite cheap with regards to big O. No PhD thesis needed for this... The question remains, if we can make useful predictions with such a model. With my proposal above, I think we can: if we abstract away the width of the tree, we are in a similar situation as if we abstract away the size of an array. In both situations we assign operations a constant cost, where in they should have logarithmic costs. But because our cost function (log) is really flat, this fits our real live costs good enough to be a useful model.
I use VsCode + Ensime server. Serves me good most of the times, but sometimes (once a week I guess) I have to fallback to Idea then there are some complicated issues. What I didn't like about Idea is that typing and typechecking is somehow slow in it. And when you work in it you're like sleeping, everything seems sooo sluggish.
Why do people need IDEs? I find them incredibly annoying. Either you know what you're doing or you don't. Just type out the damned code.
Well, I certainly need some features to be productive as a developer, mainly code completion, navigation, documentation popup, refactoring and sometimes debugging. If you can be as productive without those features I envy you, but I think you are in a small minority of developers.
https://github.com/pjrt/stags
Usually I have an open shell with the incremental compilation running on my project and sublime, and the scaladoc on the side. As soon as I need to integrate with a project/lib that has a bad scaladoc (or is written in java) I'll fire up an ide for the autocompletion.
&gt; I'd say perpetually unprepared instead of perpetually inexperienced Yes, good distinction
If you get good at scala you become much faster at developing than you could ever hope to be at Java. When you dive in libraries like shapeless and scalaz, you start 'meta-programming', and abstracting over code structure itself, not necessarily just abstracting over code. You start programming with self-contained programs as your objects rather than basic objects. Almost by definition, business logic melts away to become a trivial layer on top of your well-defined and abstracted model / instruction domain. If you do it right, you make the interface to your code clear, safe and simple, and you can safely replace yourself with a junior (what happened in my last job). EDIT: To explain further, in my experience in languages like java you are forever at the implementation layer. Business logic is a big as you think. Everything you do you squeeze in to existing models and bolt things on. With scala, you have the power to make such broad abstractions that business logic can be made to be a trivial addition to a well-structured abstract model. With java, projects just get more and more complex and intertwined and you need a senior to maintain it because it's a ballache and a shit-tonne to remember. With scala, the project can get simpler and simpler to maintain and incrementally update with new business logic, and eventually a senior will not be necessary. You've essentially by that point built a new minilanguage on top of scala which incorporates the business domain into itself.
This project has not reached even half of its intended goal as of today with only 8 days to go. What will happen if it does not reach? Will this be dropped? I would totally love it if it becomes a success. Would be glad if either lightbend/other big scala shops can sponsor this. P.S : I have already backed it.
I find I know scala well enough to be significantly faster without an IDE bogging down my RAM. It would be nice to use one to refactor things though, and occasionally to look at source code of libraries easily. But I find that if I need to look at the source code of a library, I can just clone it and then have it as a sublime project too and have a look in there. Go to declaration in sublime is more than enough for me, I don't find I need to go definition in an IDE. I also find that if you need an IDE to debug things it means your program is written badly (which may not be your fault!). I've been lucky enough to not work on a program that's so mutable it requires interactive debugging tools.
I'd rather say that `cats` is easy-level mid-boss while `scalaz` is hardcore-level mid-boss, however `shapeless` is the real end-boss. ;)
Why do you want to make your life harder? You can do with notepad, if you want, but why would you. An IDE can save you a lot of hassle, for example imports are much easier, typing out manually is silly especially, i don't know the full namespace for all the 20 imports i'm going to use. Looking it up manually takes a lot of time.
Yes, it's dead.
Yes it's dead, it has been for a long time. Although i wonder, if it will be picked up again, since Scala Native and Scala.JS paved the way for alternative runtimes for Scala.
It is. I don't think anyone will resurrect the project because the .net ecosystem is pretty closed.
I wouldn't want to ever force an IDE or editor choice on someone -- people are much happier and most productive when they choose a tool that they are comfortable with. That's totally fine. Having said that, I also believe that if you don't choose an IDE you are going to be a little less productive than you could be if you switched, and I think it's fine to advocate for that.
My team (3 people) is entirely working with vim. We tried Intellij for some time, but went back to vim, because of RAM usage, responsiveness, vim keybinding integration, vim plugins. Code navigation works well enough with ctags and fzf. And refactoring is not a problem with multiple cursors and ag. The only real pain is import code completion, but that is not enough to go back to the IDE. We have to check out ensime again very soon.
Second this. To spell it out: 1. [Visual Studio Code](https://code.visualstudio.com/) 2. [Scala syntax support](https://marketplace.visualstudio.com/items?itemName=daltonjorge.scala) 3. [Scala Language Server](https://marketplace.visualstudio.com/items?itemName=dragos.scala-lsp) Sometimes the language server is a bit flaky. I often don't even use it and just rely on the good things about VS Code and syntax support. So I guess it's fair to say that I try to split the difference between a "text editor" explicitly intended for programming and an IDE. What I instantly set IDEs aside because of tend to be the following: 1. Can't even keep up with my typing because of all the "help" they provide. 2. Tell me my code won't compile when the compiler says it will. 3. Visual clutter that I have to jump through hoops to remove (if it's possible at all) that I use &lt; 1% of. 4. Dysfunctional integration with sbt. When I do use what I think of as an "IDE," it's Eclipse and the Scala-IDE, which often fail 1) and 3), but never 2) and 4). But I still vastly, _vastly_ prefer Visual Studio Code on platforms that will run it. (I use [Zed](http://zedapp.org) on my Chromebook, just FYI.)
I don't think you've ever used a modern text editor.
I've had mixed results in the past worth scala and tags. Grep gives me many options, but the right location is always there, and it's usually quick inside of projectile and helm. I will try out the sctags/stags below, though, thanks! 
The way I use it, it definitely is an ide.
It is dead. F# is the closest companion you will find there, and I'm told functional tools for c# is pretty good, though I just stick to IEnumerable when programming on that side of our applications. 
Blog post: http://developer.lightbend.com/blog/2017-08-11-sbt-1-0-0/
Honestly, 1. Ditch node.js. 2. Avoid Akka. 3. Use [gRPC](https://grpc.io/). Now you have a different question, which is "How do I use gRPC in Scala effectively?" If you want to talk about that, I'm happy to. But it gets involved enough that it warrants its own thread.
If it can resolve the imports for me, it's not a text editor anymore. 
Another vote for gRPC here. 
I like the idea of using grpc, but quite honestly if you have the opportunity to move from node.js to scala you should take it. You cannot go wrong choosing between akka-http or play framework. 
&gt; Either you know what you're doing or you don't. Just type out the damned code. I presume you just `cat &gt;file.scala`, then? No? 
Of course. I don't even make typos.
I switch back and forth, but I mostly use vim and a bash shell. I'm not religious about it; I use IntelliJ frequently to navigate and debug in large code bases. There's many good uses for an IDE in a statically typed language and IntelliJ is definitely part of my tool set, but I'll always spend most of my time on the command line. I'm more productive there. I've always been a linux guy more oriented to shell commands than the mouse. 
Yes you can go wrong for sure! 
Looking at the gRPC java introduction I shuddered multiple times (e.g. the codegeneration). On the other hand I consider you a very experienced Scala dev including FP concepts. Therefore I am quite interested why you vote for grpc + Scala and would enjoy if you go into detail a little bit.
I'm interested as well because I was just researching this. I created a new thread to see if we can solicit some other opinions: https://www.reddit.com/r/scala/comments/6t2bao/using_grpc_in_scala/
I had an opportunity to use [ScalaPB](https://scalapb.github.io/grpc.html). It compiles your gRPC definitions into scala code. It worked for me and i don't know any better tool for scala (ofc. you can always compile to java, and implement java interfaces, but from my experience, the tool i recommended is easier to setup and use)
Light the bonfires, and throw 2.10.x on it
Please check out the REST API guide on Play: http://developer.lightbend.com/guides/play-rest-api/part-1/index.html Play 2.6 uses Akka-HTTP under the hood and comes as a set of modules, so it really comes down to how you want to organize your project and how many modules you want to add (i.e. using Play-WS, Play-JSON, etc.)
Woopwoop!
Why gRPC instead of thrift / finagle?
Hyperbole much? https://gum.co/essential-scala combined with common sense should get you off the ground.
While on one hand saying "ditch node.js," on the other hand I'm assuming they need some reasonably good support for JavaScript, or at least not-Scala. Besides, they may not be able to—or just may choose not to—accept that advice. To begin the gRPC journey for node.js, you have only to look [here](https://grpc.io/docs/tutorials/basic/node.html). However good Finagle may be for JVM-based services (which I don't know; I haven't used it), it is JVM-based.
As you might imagine, the question has come up multiple times before, including (especially) while I was at Verizon Labs. The outcome of that was [Remotely](https://github.com/Verizon/remotely). Since then, I joined Banno and we attempted a proof-of-concept around Remotely that demonstrated that it was unsuitable for our purposes, including simply _not working correctly_ in certain obvious scenarios. So, a few thoughts: 1. If there were an obvious scalaz-/Cats-ecosystem-based RPC system, I'd certainly take a hard look at it. 2. The name of the game isn't really the API, but the wire protocol and what it interops with. scodec(-bits) is brilliant work, but it's a _framework_ for implementing protocols, not a protocol itself. 3. gRPC is mature, battle-tested, available for a wide variety of languages and frameworks, and relies on ProtoBuf, which is also mature, battle-tested, and available for a wide variety of languages and frameworks. 4. If you do FP in Scala, you know by now what issues are likely to arise in wrapping spectacularly ugly non-FP or even non-Scala APIs (hi, Rob and Doobie), so this is likely the shortest path when there's a clear winner in a design space. gRPC is the clear winner in the polyglot RPC space. So, keeping in mind that the OP is coming from a node.js background and, even if they do _some_ services in Scala in the future, they don't seem to be saying they're going to rewrite _all_ of their services in Scala, and even if they did, they likely wouldn't be able to wait to deploy them _all at once_ based on some Scala-specific RPC, let alone FP-specific RPC, gRPC seems like the best bet, and sure, the way I'd use it would be to integrate it into scalaz-stream/fs2, but that may or may not be helpful to the OP.
Silver bullets don't pretend to slay _all_ monsters, just the ones they do. `STRef`, `IORef`, and `STArray` overcome the computational-complexity performance issue discussed in Okasaki by the simple expedient of using in-place mutation in the implementation. Is the result going to be as fast as a non-monadic implementation could possibly be? No, but if you're worried about _that_, it's _very_ questionable whether you can afford to be on the JVM, and if it's correctness-without-bare-metal-performance-compromise you want, I highly recommend [ATS](http://ats-lang.org).
What [/u/wookievx](https://www.reddit.com/user/wookievx) [said](https://www.reddit.com/r/scala/comments/6t2bao/using_grpc_in_scala/dlhd22j/?utm_content=permalink&amp;utm_medium=front&amp;utm_source=reddit&amp;utm_name=scala). tl;dr I do pure FP in Scala, so even with the ScalaPB support, I'd do... something more, like at least use [Delorean](https://github.com/Verizon/delorean) to turn those [Future](https://scalapb.github.io/grpc.html)s into [Task](http://timperrett.com/2014/07/20/scalaz-task-the-missing-documentation/)s. But it's more likely, really, that I'd be looking at how to integrate gRPC into [scalaz-stream/fs2](https://gist.github.com/djspiewak/d93a9c4983f63721c41c), especially since [gRPC itself supports streaming](https://grpc.io/docs/guides/concepts.html#server-streaming-rpc). Besides, I want to ultimately integrate with other streaming APIs (Kafka? Flink?) and streaming is at the heart of other major APIs I use ([http4s](http://http4s.org), [Doobie](https://tpolecat.github.io/doobie-scalaz-0.4.2/00-index.html)) so formulating a consistent architecture around streaming is key for me. That said, others may find just relying on the ScalaPB-generated code to be entirely adequate to their needs. I just wouldn't know, so I can't vouch for it.
I use Vim with Ammonite as a repl and I feel pretty productive. 
No, I think it's accurate. The whole series is quite good (though some things like SBT, higher-kinded types and Scala.js could be worth mentioning too). It's telling that third-party sites have vastly better documentation than the official Scala site. It also doesn't help beginners beginners that these resources aren't mentioned anywhere on the website.
I suspect it won't be resurrected. My understanding is that .NET is actually hard to write other languages for because of reified generics. As a result, if you want to target .NET you basically have to use a type system very similar to C#. I believe that is why F# doesn't unify records and classes.
.NET is not the platform. The platform is CLR. Scala could just choose to ignore its generics implementation and just use any or dynamic on the run time level (hint : Javascript has no generics). I think the reason behind it is more political. Never the less, it does not worth to revive it, it simpler and faster to start from the beginning, since Scala has changed so much. 
Pretty sure thrift supports more languages than gRPC, but it at least definitely supports node.js https://thrift.apache.org/tutorial/nodejs Finagle is just one option for thrift.
ENSIME support in neovim is quite good
Yes, you could ignore reified generics when implementing Scala for the CLR (or use the DLR like JS or IronPython). However, wouldn't you run in to issues when you have to call code that makes use of reified generics, or vice versa? Are there any of languages on the CLR that solve this? This seems like it would be more of a problem for a language that has its own static types. For instance, if you called Scala code from C# you probably wouldn't want to get back a `dynamic` result. Why do you think the reason behind the project dying were political?
Thanks for the links. I'm going to see how well these work together. 
I can't believe I got to see this day. Finally after 6 or so years no more rewriting of my build scripts every other month. Yay!
I think sbt's still got at least one compelled rewrite left in it: the move to a unified scoped key syntax for the shell and build.sbt. Of course it will be one of those "the old syntax is deprecated but still works" sort of things..
I also have [a very small and experimental project](https://github.com/eiennohito/grpc-akka-stream-subrepo) that allows you to use gRPC with akka-stream.
For inter-op reified generics might be necessary, but for running Scala on the CLR it is not, also i don't think it would be impossible. When this project died, .NET was still a Windows only target, with no multiplatform possibility in the near future. I think that is a pretty large reason. I don't think many people wanted to run their servers on Windows, if it was not for the full Microsoft stack.
How about .net core? No longer closed afaik, and it's multiplatform
An example project archetype ported from Giter8 to FreeMarker: https://github.com/jeffreyolchovy/sbt-fmpp-resolver/tree/develop/examples/scala-sbt.ftl This has the same remote resolution capabilities as Giter8. For more examples check out the [examples directory](https://github.com/jeffreyolchovy/sbt-fmpp-resolver/blob/develop/examples) or peek at the [scripted tests](https://github.com/jeffreyolchovy/sbt-fmpp-resolver/tree/develop/plugin/src/sbt-test/sbt-fmpp-template). Its main advantage over Giter8 is that FreeMarker is a very powerful templating engine which supports custom macros, UDFs, conditionals, etc. Pretty much everything that you'd expect from a robust, mature templating solution. I personally would advocate to use Giter8 for most project archetype use cases, but to use FreeMarker when you can't (e.g. you need control flow or you need to define some custom formatting function or you need some repeated piece of boilerplate in every file, etc.).
Not speaking for Runar, and fairly new to the topic so take what I say with about a pound of salt and correct me where I'm wrong - In the talk he is demonstrating how the basic types we always discuss as basic functional data types are not actually primitives. They can be defined in terms of Adjunctions. He uses Option and List as examples, showing the right adjunct to be fold and foldMap, respectively; implementing some, none, and list, and demonstrating that any type can be a monoid. Why is this important? Because it allows you to encode programs and datatypes with lower-level and simpler(made of fewer disparate parts) operations (right and left adjunctions). It also explains why certain functions work the way that they do -- fold and foldMap -- and place them on different, solid theoretical ground. It may not be important to build your implementations out of Adjunctions, but demonstrating that they can be gives us the option of removing some functions from our typeclass hierarchies, reducing the footprint of the basic knowledge needed to write done programs in a typesafe and referentially transparent way.
That's true, but then you're stuck finding something comparable to gRPC, but for Thrift. I don't see an argument for making that search.
VIM is a text editor but it defers those other tasks to better tools (ENSIME in the imports case) all you have to do is hook into the right processes. 
Yes, and those people usually use a bunch of tools external to the editor to build a development environment that gives them similar powers found in an IDE. This makes for very quick editing and feedback when writing code, but sometimes (for me at least) I find that working that way can be clunky. It is personal preferences as usual.
&gt; you go shapeless - noone understands what you do - in 4 months you have no clue about your own code at a glance &gt; you go scalaz or cats - the pretty much same happens in one week (previous item) I've worked with Scala in teams of 4 to 20 over the last 8 years and that has never been an issue. &gt; HR hates you because there's no a single Scala dev we solved that issue by hiring good thinkers willing to learn and train them &gt; your boss is fascinated with the code that he doesn't understand but scared to death of the prospects of what will happen if you leave or die make sure nobody is solely responsible for any part of the code, spread knowledge, make sure everyone is replaceable
I guess the terms "ide" and "editor" are a little outdated and imprecise. The substantial difference (imo) is this: IDEs analyzes your code constantly and gives you intelligent code completions, find-usages, refactoring options, type information, etc. It's like running `sbt compile` with every keystroke and have the result directly in your editor. Actually it's way more than that since you can: e.g. select any sub-expression and have it tell you the type. I guess emacs coouuld be considered "IDE" if you run it with ensime though (which gives you some of the IDE-like code analysis).
Not ready for prime time yet but we are building RPC support in Freestyle as part of a microservices framework for pure FP apps which will use Kafka and Cassandras as well beside gRPC. https://github.com/frees-io/freestyle-rpc/pull/32 Still missing docs which we plan to add in this quarter but you can automatically lift and derive gRPC servers and clients from algebra definitions and it already supports unary and streaming methods.
Yeah, with ENSIME it definitely is an IDE, but I think even without it with ~compile or the repl running in an sbt buffer and projectile + autocomplete you are 90% there. Type information is missing, unless you can import and copy-paste into the repl. Refactoring is not there, but I do a lot of that by hand anyway. Style warnings can come from ~compile. 
&gt; I think sbt's still got at least one compelled rewrite left in it: the move to a unified scoped key syntax for the shell and build.sbt. I would like to bring in [sbt-slash](https://github.com/sbt/sbt-slash) in sbt 1.1. 
I use neovim for everything. I'm happy.
My experience is that of a newbie. I tried learning and using GRPC and found that very few people know anything about it. So if you get stuck you won't get much help. For example this question which I posted on stack overflow received no answers https://stackoverflow.com/questions/44913577/writing-grpc-based-streaming-services-in-scala So if you have team members who are already knowledgable in grpc then fine... otherwise you may get stuck for long period of time with no support from standard watering holes like SO.
I used DropWizard Metrics and instrumented my apps to great effect. http://metrics.dropwizard.io/3.2.3/getting-started.html Meter when messages are received and use timers to see how long they take to process. By default you'll also get memory, cpu, and thread statistics too.
If there's no ambiguity in which types can fulfill the constructor signature, and they can (recursively) have all their constructor parameters supplied, it will just do it and instantiate everything needed automatically. You can also bind specific classes or object instances to the required types manually.
Thanks for your reply mate. That's some pretty sophisticated magic right there. &gt; You can also bind specific classes or object instances to the required types manually. Can you please just point me to where to look for how to do this? I've searched and the docs seem cryptic to me. I need to get my new Global class injected somewhere manually (and eagerly) so it gets started before the sistem, just like onStart. How do I do that?
https://github.com/google/guice/wiki/Bindings Add a "module" class definition to the project root if it's not there, add the bindings to the .configure method body in that class https://www.playframework.com/documentation/2.5.x/ScalaDependencyInjection#providing-custom-bindings
I love you man. If I weren't a broke Brazilian I would pay you gold.
BTC is fine
Self-healing services sounds amazingly interesting. Any pointers for where to get started on learning?
I wrote an article about this topic. https://www.scalawilliam.com/1609/feature-switches-agile-scala-jmx/ Use Java Mission Control.
Even ensime's memory utilization (~5gb/project) is too great for my peasant-tier hardware.
Are you solving some particular problem, or you want to make a generic service?
This thought about fresh possibilities of things composition, by the completely different way is very attractive for me, but anyway, one question bothers me: Do we already have enough express power at Scala/Dotty/Haskell to use adjunctions for a wide range of applications (just lack of libs), or now we just able to express a small piece of this world, and it's time for looking forward, for upgrade our langs/compilers be ready for that? 