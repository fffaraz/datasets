I'd store it in a Trie. Should be pretty efficient
He's likely referring to functional tests rather than performance tests. Seeing how an API is used in those is a good way to gain familiarity with a new codebase.
Take a look at bloom filters. Worth the right tuning, you could dedupe with a fairly high confidence level.
Overhead for a string is about 50 bytes, plus 2 bytes per character. So it looks like each string takes up about 90 bytes. A Seq is actually - by default - a List. So you have another - let's say 60 bytes for the CONS cell. So about 150 bytes per item. 2 million times 150 is only 300MB, so if you set -Xmx up to a few gigabytes, you should be able to keep the whole thing in memory. If you manage the values in an array, you can cut out most of the overhead of the CONS cells. If you convert from string to a byte array encoded as UTF-8, you can save another 20 bytes or so per item. If you are on linux, you could find duplicates using sort | uniq -d If the data were sorted, the problem would be easy. The problem you are running into is that Java/Scala don't have a method of doing streaming sorting. You need the data to be in memory to sort. &amp;#x200B;
You can use a [cuckoo filter](https://github.com/newhoggy/pico-cuckoo-filter).
Instead of turning strings into chararrays it might be more useful to extract the information. Looks like two byte for the numbers, one bool (light, dark), and a pointer for the actual colors. There is also a project called scala packed that provides collections without object headers. 
Do you have an ETA for it?
Yay for the functional API ! &amp;#x200B; Now I just wish that printing to stderr/stdout wasn't the default behaviour. And yes, I'm aware of the workaround : [https://github.com/scopt/scopt#advanced-captured-output](https://github.com/scopt/scopt#advanced-captured-output)
What would be a better default behavior?
Uniqueness is impossible to enforce without remembering all encountered items, so you have a few options: 1. Increase memory (RAM/swap) so it can handle your workloads 2. Limit the uniqueness check to e.g. the last 1 million items 3. Improve the efficiency for how you store the items so you can handle the workload with your current memory budget. Other comments have given good pointers for this
Not print at all and return some data structure that carries the information, and let the user decide when to print
Not print at all but return some data structure that contains the informative that is currently printed
I see, you're intercepting the default-generated `apply` with your own, that's a good idea.
Returning a Validation, with an ArgPath for subcommands similar to what Json parsers do.
Not sure about the \`.sequence\` as it overloads the method from \`Traversable\` with different semantics. \`.mapN\` from Applicative would have been a better choice or \`&gt;&gt;&gt;\` from Arrow. Still, it's only in the specific case of argument parsing, so the cognitive load from the overloaded method name is pretty low.
Right. EMR seems overkill for this? 
That makes sense! I just think in this specific case the most common use case will be to print it, which is why it was made the default behavior.
Unfortunately with this approach you can't add \`TraceId\` to logs that are produced by underlying libraries, e.g doobie. Since we use Monix as our IO monad, the approach from [https://olegpy.com/better-logging-monix-1/](https://olegpy.com/better-logging-monix-1/) has been working really well for us.
Can you post the link to "scala packed?" I didn't see it on a Google search, so I am not sure I have the right term. You are right. The data could be made a lot smaller by extracting the meaning and storing as single bytes and numbers. What we really want is C here. Idiomatic Java is convenient, but it is terrible if you want to store large amounts of data in memory. :)
This is a talk about it. https://youtu.be/OtBTBaA5M4g
Excellent explanation! I didn't realize there was a second dereference to a character array as part of a String. 
I just released a new version of my programming language targeting 8-bit computers: https://github.com/KarolS/millfork This release accumulated tons of changes, the most important being support for Intel 8080, Zilog Z80 and Sharp LR35902 microprocessors. The language itself got tons of new features and now it's much, much faster than C, on both 6502 and Z80. At least that's [what my benchmarks tell me](https://github.com/KarolS/millfork-benchmarks). But since this is /r/scala and not /r/retrodev or /r/retrogamedev: * I increased the optimizer performance +100% by using the fact that if a reference to an immutable list didn't change, then that list didn't change either and I can skip expensive flow analysis. * I added tests of code generation for WDC 65816... by finding (and patching) a JavaScript 65816 emulator and running it via Nashorn. For easier coding, I wrapped JS objects into a Dynamic. All the other emulators used for testing are in Java, I just couldn't find one for 65816; 65816 is an awfully convoluted processor and I wouldn't dare implementing an emulator myself. 
Don‚Äôt bother with this. It‚Äôs two years old and affiliate spam.
A little, but it's really easy to set up and you can use a Jupyter notebook to program a Spark job in a few hours.
I've been studying up on Kafka Streams. &amp;#x200B; However personal projects: \* I created a serialization-checker library. It's meant to help you test out your data objects and serialization options from a number of different sources (right now just files, but it's extendable) [https://github.com/monksy/serialization-checker](https://github.com/monksy/serialization-checker) \* Merged some of the kafka resource lists: [https://github.com/monksy/awesome-kafka](https://github.com/monksy/awesome-kafka) (Please add to this if you can) \* I've been trying to get monitoring on Grafana/Remoria for Kafka: [https://github.com/monksy/kafka-monitoring-remoria](https://github.com/monksy/kafka-monitoring-remoria) (Not done)
You might also be interested in https://github.com/botkop/scorch
I would be interested to read more about the design and the reasoning behind it. Both the high level and low level design.
It's quite a terrible list of books even two years ago. Not single book thats actually about computer science. No SCIP, no book on discrete mathematics, the theory of computation. The only book in that list thats isn't philophical or motivational mastuerbation is the Mythical Man Month, and that's about the economics of Software Development, not about actually developing software.
[SWIG](http://www.swig.org/) or [JNA](https://github.com/java-native-access/jna) might be helpful. SWIG is usually a little bit faster at runtime, but JNA is much easier to set up and to maintain.
See my current effort: github.com/ctongfei/nexus. In the torch subdirectory is a JNI binding for Torch.
I will be happy to answer questions, e-mail is on GitHub, send me a message
I wasn‚Äôt commenting on the age of the post for the content, I was commenting on the age because it makes it even more obvious that it‚Äôs affiliate spam.
That may be true: but in that case you should probably make it be an effect compatible type. Not that big of a deal really, though. It's entirely fine to just wrap it in IO. 
 4. return a probabilistic result to a certain degree of accuracy only, not absolute certainty
See Scalaz's [Liskov](https://github.com/scalaz/scalaz/blob/series/7.3.x/core/src/main/scala/scalaz/Liskov.scala) and [Leibniz](https://github.com/scalaz/scalaz/blob/series/7.3.x/core/src/main/scala/scalaz/Leibniz.scala). /u/alexknvl had a fantastic write up of the rules of the game [here](https://gist.github.com/alexknvl/b477ec0287124c92d72f96b860ad4f79). In response to you question, you can provide a witness to your function in the form of an implicit generalized type constraint: def f[x](x: Q[x])(implicit w: B &lt;:&lt; x): Q[x] = identity x In scalaz, replace `&lt;:&lt;` with `&lt;~&lt;` and you actually provide this witness in the form of a typeclass. My advice is to not play around with subtyping in general unless you know the formal rules. Invariance is preferred in most things, and the spurious use of subtyping within the community leads to many problems.
I think you can prove it with implicitly but I'm not sure if there are better solutions. &amp;#x200B; Welcome to Scala 2.11.12 (OpenJDK 64-Bit Server VM, Java 11.0.1). Type in expressions for evaluation. Or try :help. scala&gt; class C defined class C scala&gt; class B extends C defined class B scala&gt; class A extends B defined class A scala&gt; class Q[-T] defined class Q scala&gt; implicitly[B&lt;:&lt;C] res1: &lt;:&lt;[B,C] = &lt;function1&gt; scala&gt; implicitly[A&lt;:&lt;C] res3: &lt;:&lt;[A,C] = &lt;function1&gt; scala&gt; implicitly[C&lt;:&lt;B] &lt;console&gt;:14: error: Cannot prove that C &lt;:&lt; B. implicitly[C&lt;:&lt;B] ^ scala&gt; implicitly[Q[C]&lt;:&lt;Q[B]] res5: &lt;:&lt;[Q[C],Q[B]] = &lt;function1&gt; scala&gt; implicitly[Q[C]&lt;:&lt;Q[A]] res6: &lt;:&lt;[Q[C],Q[A]] = &lt;function1&gt; scala&gt; implicitly[Q[B]&lt;:&lt;Q[A]] res7: &lt;:&lt;[Q[B],Q[A]] = &lt;function1&gt; scala&gt; implicitly[Q[A]&lt;:&lt;Q[B]] &lt;console&gt;:15: error: Cannot prove that Q[A] &lt;:&lt; Q[B]. implicitly[Q[A]&lt;:&lt;Q[B]] ^ scala&gt; implicitly[Q[A]&lt;:&lt;Q[C]] &lt;console&gt;:15: error: Cannot prove that Q[A] &lt;:&lt; Q[C]. implicitly[Q[A]&lt;:&lt;Q[C]] ^ scala&gt; implicitly[Q[B]&lt;:&lt;Q[C]] &lt;console&gt;:15: error: Cannot prove that Q[B] &lt;:&lt; Q[C]. implicitly[Q[B]&lt;:&lt;Q[C]] ^ scala&gt; &amp;#x200B;
What we do instead is to add logs before or/and after invoking code of external libraries so it can be traced. This approach using `TaskLocal` doesn't work for polymorphic code as it depends on specifics from `Monix` and it's not referentially transparent. 
&gt;Invariance is preferred in most things, and the spurious use of subtyping within the community has lead to many problems I keep reading this but I rarely see any specifics nor have I encountered them myself. Maybe somebody linked a github issue once but I can't find that at the moment. Do you have any examples of variance-related woes you can share?
I don't know why you're being downvoted, this is a great answer!
It's not so much that subtyping is the problem specifically, but rather the type inference algorithm used by Scala. Any ADT inevitably runs into inference issues (such as Option, or List) due to subtyping, which is exacerbated by having variance bounds. For instance, the following does not compile: List(1, 2, 3).foldLeft(Nil)((_:Int)+(_:Int)) The only way we've found around it is to limit the use of subtyping except in extreme cases, such a the `Liskov` or `Leibniz` work where we try and provide interop to provide the subtyping evidence ourselves, which works more deterministically than Scala's. If you'd like more variance-specific issues, you can watch Paulp's talk here about issues he faced in the collections before tapping out: - [Scala collections: Why Not?](https://www.youtube.com/watch?v=uiJycy6dFSQ) - [We're doing it all wrong](https://www.youtube.com/watch?v=TS1lpKBMkgg) Much of this is fixed in dotty, but there are still issues out for correcting many subtyping-related issues, which you can find the status of [here](https://contributors.scala-lang.org/t/better-type-inference-for-scala-send-us-your-problematic-cases/2410).
Why have this post been deleted ?
what are formal rules?
Personally, I dislike Scala and all other multi-paradigm languages because people who code in Scala tend to pollute the functional paradigm with OOP concepts when they can't figure out how to do something the right way (functionally speaking), so you end up with a hodgepodge of functional and OOP code, which sucks. I also dislike Scala for being cumbersome and poorly thought out from a syntactical perspective. It's a language that surprises you, in a bad way, and offers too many different ways to do the same thing. If you want to learn a functional programming language, try Elixir. It is clean, easy to learn, and easy to use. It has better tooling than Scala (or any other JVM-based language for that matter) and helps you understand functional programming without screwing everything up with OOP.
I found Scala to be a cumbersome bastardization of functional programming and OOP with no clear path to properly understanding either paradigm.
I can't tell if I'm misunderstanding something here, in your "does not compile example" you specify the accumulator argument of your fold to be an `Int` whilst giving it an initial value of `Nil` which is a `List` and could never be an `Int`. Why would you expect that to compile?
Thank you, Jasper.
I guess she meant something like: List(1, 2, 3).foldLeft(Nil)(_.::(_)) Which still doesn't work in Dotty, for some reason. I think examples like this ought to work, even if that means being smarter around lambdas, which [is totally possible](https://www.cl.cam.ac.uk/~sd601/mlsub/) in principle.
That makes sense. My understanding is in current Scala when arguments are grouped like that, their types are fully resolved before moving on to the next group and so in this case the types in the first group can't be inferred from those in the second group. I hope it does get better at some point in Dotty!
&gt; when arguments are grouped like that, their types are fully resolved before moving on to the next group This already is no longer the case in Dotty.
Great to hear it!
I need to ask a follow up on this: ``` List(1, 2, 3).foldLeft(Nil)(_.::(_)) ``` How would expect this to work? The lambda at the end uses positional arguments, so the first `_` is the first lambda arg and the second `_` the second. The signature for `foldLeft` is: ``` def foldLeft[B](z: B)(op: (B, A) ‚áí B): B ``` So the first argument should be the accumulator. But the list concatenation operator `::` acts as a method on its rhs, not its lhs. So this means your example is attempting to prepend a `List[_]` to an `Int`, which of course could never work. Okay, I think of course you meant to use a `foldRight`: ``` List(1, 2, 3).foldRight(Nil)(_.::(_)) ``` This then flips the arguments to work smoothly with the `::` method. And unfortunately it still doesn't work with Dotty :( But this does: ``` List(1, 2, 3).foldRight(Nil)((_: Int)::(_: List[Int])) ``` Which isn't perfect, but better!
In Scala, `a :: b` means `b.::(a)`. &gt; [...] Which isn't perfect, but better! It's horrible. I'd much rather write: List(1, 2, 3).foldRight(List.empty[Int])(_ :: _)
Yes of course!
Thanks, yeah, i got my example mixed up from some other ones I was trying.
screwed up my example - sorry. It is fixed now
Very cool! Could this be used to print my own domain-specific abstract syntax trees as Scala code, using the `additionalHandlers`? What steps would you take to achieve that? First convert to string chunks (like `"model = Sequential("` in the blog post)? But then I won't benefit from color highlighting...
This guy is a fucking genius.
Ah okay thanks. I'll go through this links later but sounds the situation is "variance may make sense or be desirable here but practical inference problems means invariance is actually a better experience and is therefore preferred".
&gt; Could this be used to print my own domain-specific abstract syntax trees as Scala code, using the additionalHandlers? Now that it‚Äôs been extracted into a standalone code there are many possibilities that seem obvious. For example, since we only ever have one interpreter for `Tree`s, implementing them using open class hierarchies and doing the recursion via virtual dispatch would make adding new kinds of nodes trivial. We could also convert the iteration to a Push model, rather than Pull, by returning a `geny.Generator` instead of a `scala.Iterator`, without changing the core algorithm. &gt; But thn I won‚Äôt benefit from color highlighting Not sure what color has to do with anything here; if you want color, just work with `fansi.Str`s instead of `java.lang.String`s.
Exactly.
Finally something similar to Data::Dumper of perl
&gt; you do it like anything else, e.g. using doobie to log to a database I didn't get this part. What do you mean? I'm very interested in this question. Thanks.
I mean if you're writing a log of events to the database (which IMO is better practice for "important" logs like audit logs) then you just write those things to the database using a database-access library such as doobie. The point is you treat log events like any other value.
Sorry for the clumsy response - typing on phone. You could take a look at the 'TCInstance' evidence for your players HList which will give you the instance of the typeclass for the member, so you could avoid pattern matching. You could avoid subtyping for the players types that way since the only requirement needed would be the presence of the typeclass. Please correct me if I misunderstood the problem.
Do the abilities of Fighters, Champions and Monsters vary? If a fighter always does fighter things, inheritance is vastly simpler. Type classes are good when you know all your things and want to vary behavior. Inheritance is good, when you know your behavior and want to vary the things. Which seems to be the case here. 
Annoyingly it‚Äôs a combination of the two. At least that‚Äôs how I‚Äôm modelling it currently. For context it‚Äôs a D&amp;D simulator. So every type of player extends Creature, this is where things like health are defined, core properties that everything shares. The abilities of Champions, Fighters and Monsters do vary. There‚Äôs a bit of cross over but for the most part they can each do different things. When the type class instance to get the abilities is summoned, it looks through the list of abilities and finds one which is available (that is: hasn‚Äôt been used already or meets other conditions like ‚Äúhealth is below 50%‚Äù). If none are available then a Option#None is returned and a regular attack is made (not an ability) which any Creature can perform using its core stats (like health and weapon).
Interesting. I see you can handle bloodied or other conditions that way. But if usage is per character, how do you get this dragon's breath and not that dragon's? 
My thinking of this is that a Dragon would have a flag on it along the lines of breathUsed: Boolean. The list of available abilities wouldn‚Äôt list this ability unless that flag was set to false. Only a Ability[Dragon] would look for this Boolean value as it is dealing specifically with a Dragon. An ability can and often would operate on Dragon (or other) specific fields like breathUsed as well as more general fields from Creature (like health).
Thanks! I will take a look.
Interesting read as an exercise. Curious about the motivation, tut is a personal go-to and I have no recollection of ever struggling with errors or build times. Are people including doc generation on every build, or generating markdown with substantial amounts of code? I've always been on the order of 20 seconds 1-5x a week max.
Li, isn't this not referentially transparent? Iterator.next() must be stateful, there's no other way to implement it. I know some logging bindings have thread safety issues around similar constructs causing some unintended consequences. Can we avoid them altogether here? case class Nested(prefix: String, children: Iterator[Tree], sep: String, suffix: String) extends Tree Why even involve the iterator reference? Couldn't it just be case class Nested(prefix: String, children: () =&gt; Tree , sep: String, suffix: String) extends Tree and be referentially transparent and maintain the same goals? You'd even simplify the traversal api: myTree.children.children.body Instead of myTree.children.next.children.next.body You wouldn't need mutation at all. I suppose you are assuming that Tree only exists inside prettyPrint, but then you could just use raw iterators instead of a data type. If you used a third data type Stream[A] sealed trait Stream[+A]{ def hasNext: Boolean } case class Element[+A](value:A, tail: () =&gt; Stream[A]) { def hasNext = true } case object End[Nothing] { def hasNext = false } Your interface could be def prettyprint(t: T, maxWidth: Int, indent: Int): Stream[String] And you could traverse it for printing with a simple recursive function: def printStream(stream:Stream[String]): Unit = if(stream.hasNext){ println(stream.value) printStream(stream.tail) else () You could implement map/flatMap/fold to make it easier, too. And you don't run into mutability issues. My two cents. 
If you are happy with tut then there's no need to use mdoc instead! I created mdoc when the compilation time was over 30 seconds to render this particular page here https://scalameta.org/scalafmt/docs/configuration.html. However, my needs were quite specific and not what tut was designed for in the first place. One important mdoc feature that's not mentioned in the blogpost is custom modifiers like here https://scalameta.org/mdoc/docs/modifiers.html#stringmodifier The scalafmt page uses a `mdoc:scalafmt` modifier which formats the code block with before/after examples without going through expensive compilation/evaluation. See [markdown source](https://raw.githubusercontent.com/scalameta/scalafmt/64bedcc8f6eed7b9912589ae6cdace86bef974b5/docs/configuration.md). With a custom modifier, the Scalafmt configuration page takes only a few hundred milliseconds to render.
Also, this page here documents other nice features in mdoc https://scalameta.org/mdoc/docs/why.html
Scala times usually contains the note-worthy posts on the topic of Scala
Nice job with the modifiers, that seems like a killer feature. The scastie bit is really slick too. Am I right to conclude that you're using this to generate the entire docs site for scalafmt? If so I suppose that's exactly where you'd care about 20 seconds per markdown file.
Yes, the entire scalafmt site is generated with mdoc. It's a lot more enjoyable to write documentation when previews render in 0.5-2 seconds compared to 20 seconds!
I have been working on Akka, Scala, IIoT platform, planning to open source in mid of March 2019, this includes gateway, cloud integration.
The Scala website blog is pretty good.
I've found all of these to be good reads, although Underscore and Typelevel aren't updated too frequently. https://underscore.io/blog/ https://typelevel.org/blog/ https://medium.com/disney-streaming (previously Cake Solutions) https://scalatimes.com/ is a decent newsletter, too, that links to a lot of nice blogs in the Scala community (though I find it frequently has some overlap with Haskell Weekly ü§∑)
Yeah that makes perfect sense, thanks for sharing.
With that background you've probably been exposed to most of the concepts commonly present in Scala and only need to learn the syntax and JVM details. For the former, skimming a book like *Programming in Scala* is probably a great start, and it'll be a good reference for more details later. For the latter, I can't think of a good reference off the top of my head, sadly. For data pipelines specifically, you might consider looking at [Apache Spark](https://spark.apache.org/docs/latest/) and resources around that; I'd expect they have their own guidelines for what goes into a large but readable project. Here's a short list of things I think you're likely to encounter coming from your background: * Generics work more like Haskell and less like C++. Specifically, they are not specialized per-type by default (`List&lt;T&gt;` will only generate one `List` class regardless of whether you pass integers or objects) and you cannot assume any information about the type parameter in the implementation. For example, no assuming default constructors (cannot call `new T()`) or call non-`Object` methods on instances of the generic type (no `t.method(param)`). Anything you want to do must be done by constraining the generic parameter or passing another parameter that can do something with the instance and using it kind of like a callback. * Standard collections have lots of methods similar to functors and monads in Haskell, but called by different names, and are more general than the strict definitions. For instance, `fmap` is called `map` and `bind` is called `flatMap`. And `flatMap` can accept things other than the type of collection on which you are calling it (this is what I mean by "more general" than a strict monad), e.g. `List(1, 2, 3) flatMap { i =&gt; if (i &gt; 2) { Some(i.toString) } else { None } }` will work even though `Option` is not a `List`. * Scala has a "for expression" similar to Haskell's do-notation that can make long `flatMap` and `map` chains easier to read * Scala has support for higher-kinded types, roughly the same notion as template-template parameters in C++ and non-nullary type constructors from Haskell. You will mostly encounter these if you use libraries that rely more heavily on the functional programming style. * Scala is strict by default, more like C++, but does have support for lazy evaluation. So creating a `Future` will start executing that future immediately, and calling `map` on a collection will create a new collection instance immediately. Collections are getting lazier in Scala 2.13, but you probably don't need to concern yourself with those changes just yet. * Pretty much everything inside a class is a method call, including `val` members which act a lot like a C# property if you're familiar with those. When just jumping in to Scala I'd say: * Use IntelliJ and the Scala plugin * Default to using case classes and try to treat them as pure data * Prefer immutable code, but it's fine to be mutable within a single function * Do not rely heavily on inheritance * Consider going through some of the exercises/courses linked in the sidebar
Eugene, who's the maintainer of sbt writes here from time to time: http://eed3si9n.com/
Indeed, subscribing to Scala Times is the best way to see most new material without seeing _all_ material
This week I have been working on making Vectors faster by allowing them to share more structure in the new 2.13 collections https://github.com/scala/scala/pull/7588
We post Scala related content at blog.scalents.com
A parser combinator library based on atto for parsing the regional internet registries' statistics exchange files. https://github.com/ip-num-tools/ristex Initial plan was to have it released to Maven before 2018 runs out, but I guess I would now have to settle for releasing early 2019 :)
It's not 'gateway' if you don't have to go past it because other static FP languages are inferior anyway
personally speaking, I can‚Äôt get a clue about learning scala here, and maybe so do starters 
From my point of view, to a starter, we can start from choices of how to use scala. A better java, or a functional jvm dialect. A better java is easy, and a functional thing is difficult. We can know a lot of syntax staff through learning Scala as a better java. Then we can learn some theories about Categories, after that functional programming is easier to be mastered. 
You gotta mention the real killer feature: multi-line expressions without the ugly. trailing. dot() Looking forward to replacing tut with this!
HLists are a way to have type information at compile time that you can use to resolve typeclass instances at compile time. It's worth bearing in mind that anything you write that involves one will compile to a single codepath; if you need to switch between different paths at runtime you will still ultimately need a pattern match or equivalent. I would pass around the creature abilities explicitly, to the point where you're going to use them. That's actually what typeclass resolution would compile down to anyway, but IMO if there's any dynamism then you want these things to be explicit rather than implicit.
&gt; When the type class instance to get the abilities is summoned, it looks through the list of abilities and finds one which is available (that is: hasn‚Äôt been used already or meets other conditions like ‚Äúhealth is below 50%‚Äù). This really doesn't sound like a case for typeclasses. Typeclasses should be used for cases where the types alone fully determine the behaviour. For dynamic (runtime) behaviour you're better off with conventional functions and values.
Guardian is a notably large Scala shop. https://github.com/guardian
[http://www.lihaoyi.com/](http://www.lihaoyi.com/) Haoyi's blog, author of Ammonite, Mill, etc
What real world applications is Scala best suited towards? My current employer used it as a means to parse huge text documents to with similar structural forms to retrieve specific values to then do other forms data matching in Scala with Spark. But I'm interested in learning it (currently a JavaScript developer looking for a new challenge) but would like to know what benefits would I gain in writing a non-frontend service in Scala as opposed to using JavaScript in NodeJS? Also interested in Go, mostly because the tutorial on the Go website is such a simple and pleasing experience to learn. 
Here is a good one: [https://danielwestheide.com/scala/neophytes.html](https://danielwestheide.com/scala/neophytes.html) 
&gt; if you want color, just work with `fansi.Str`s Nice, I just discovered `fansi.Str` by looping into `pprint` and it seems very useful. Thanks!
Good to know. Thanks for explaining the background to this decision.
There's a lot of work that goes into making Scala suited for massively-concurrent applications. For example, there are many 'effect types' (like JavaScript `Promise`) available in Scala, from the built-in `Future` to more advanced ones like Monix `Task` and Scalaz/cats `IO`, which try to simplify concurrent programming while also making it more efficient. There is also Akka, which is an actor system library that is also often used to write high-concurrency applications. It works by setting up schedulers (usually one per CPU core) that execute many small and cheap processes (called _actors_) in an interleaved fashion. The actors communicate by sending each other _messages_ in an ordered manner. This style can be handy e.g. if you have a backend service, you can _spawn_ an actor for each incoming request and handle it in a concurrent fashion, while the other actors in the system keep doing their thing. Go has similarly powerful but different concurrency techniques, but I haven't explored it much because I personally don't agree with the language design decisions.
Comparing to NodeJS: + Static typing leads to safer programs (or less simple tests necessary) which can be more easily understood by colleagues and new hires. You can also use Typescript to gain this advantage + A compilated language catches errors more often and earlier + Futures are a nicer async model than callbacks. JavaScript Promises are modeled after the same principles as Scala Futures, but has not penetrated the NodeJS ecosystem yet. It's default in Scala. Scala also has good language support (for comprehensions) + JVM ecosystem is huge (much larger than NodeJS) and easily accessible + Streaming is a powerful programming model, Scala integrates very well with it and libraries like RxJava, Akka streams, Monix eyc , much better than Java + Immutability by default leads to fewer bugs and easier to reason about code + Typeclasses are awesome +/- Advanced Functional and Typelevel programming is possible but complicated and verbose - Much less known, much more difficult to learn - Compilation is slow - powerful language with much freedom can lead to hard to understand code (similar to C++) - Deployment requires a JVM - Front-end and backed can less easily share a domain model - Bootstrap is slower/takes a bit more time Comparing to Go: + much more powerful + much larger ecosystem + much more consise in usage (e.g. no `value, err = f()` followed by `if err != nill` everywhere) + Far better abstractions, imho a staticly typed language is incomplete without generics. + Typeclasses are awesome - Not suited for native or system programming, at least until Scala Native is mature - much slower in both compilation and run-time - more memory usage - more difficult to learn - powerful language with much freedom can lead to hard to understand code (similar to C++) - deployment is with a JVM instead of a pre-packaged native binary - Scala has less mind-share among software engineering community All in all: pick the one you like. Go is more likely to inmiadiatly have some useful pay-off. You will learn more from Scala which you might not immidiatly use, but which you will be able to use in other languages for the rest of your career.
Hi. Thanks, your explanation has made me realise that a HList isn't the right solution. To address your comment above I think you're right again :D &amp;#x200B;
Many thanks for your reply, wasn't fully aware of some of these differences in how concurrency is handled. What real world examples have benefited greatly from using Scala?
Check out https://www.lightbend.com/customers (Lightbend is a consulting company that supports Scala). One of the well-known cases is the Walmart Canada website, https://www.lightbend.com/case-studies/walmart-boosts-conversions-by-20-with-lightbend-reactive-platform . But the others are also very interesting, they all show incredible performance boosts compared to previously-used technologies. Of course, take all these with a grain of salt (after all Lightbend has a monetary interest), but I think they still show off Scala's real-world benefits at least to an extent.
Many thanks!!!
Just wanted to say that Reddit is my main window into Scala development and I appreciate your posts in this thread every (other) week and the work you're doing on the collections in particular.
Thanks man, appreciate it!
This blog is awesome, this person explains cats and scalaz very well.
I feel like this is a pretty general phenomenon in Scala - the language is great for mid-level, experts or very talented people who somehow figured out all the warts or had a friend teach them. But it is SCARY to beginners. Most projects don't even have a simple Scaladoc (like the Javadoc) hosted anywhere (or any consistent documentation format), sbt is scary and slow, trying to go outside the "prescribed" path in any sufficiently complex library means reading library-level code (because there are no docs - the types are the docs right??) using all sorts of tricks I don't understand yet potentially running into implicits or weird "self types" or complex types and functional land. I know this is probably ranty or dumb, but thats the way I see it.. Scala is almost hostile to beginners. Thats the reason Akka/Play are still #1 - they care about documentation and being user friendly. 
His background is German, so Herr Professor Doktor Odersky would not be at all inappropriate, and gets you the best of all three. But in our first conversation he told me to call him Martin.
ScalaFx - scala bindings to the java fx packages.
I would recommend this books. They will strongly influence functional programming. https://underscore.io/books/scala-with-cats/ https://leanpub.com/fpmortals
New kid on the block: www.scala.land
Don't do desktop application on JVM. Just don't.
What should they use instead? 
See also this thread https://users.scala-lang.org/t/gui-library-sbt-dependency/3787
There is really no future for swing and javafx. Just use whatever is well maintained and works for you. Electron apps are pretty popular right now and works across the platforms.
Go back to the abyss
I wanted to get some answers on performance (which I did not), but discussion moved to be general, for which scalqa is probably not ready yet. But, thanks for the feedback, I got an idea to create "scalqa Stream", where everybody would benefit from faster processing, but would not need to deal with scalqa "ecosystem" API. The Stream will be virtually the only object in API. For example import scalqa.Stream.Enable.\_ List(1,2,3,4).all.filter(\_ % 2 == 0).to\[Vector\] The method 'all' is virtually attached to all collections. It creates a scalqa.Stream of all elements. Once processing is done, the result is converted back to usual collection. Early tests indicate that performance boost could be from 50 to 100% for Objects and 200-800% for primitives if processing is not trivial. Memory requirements also drastically drop. "scalqa Stream" will be ready for review at the end of January &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Author here. I wrote this mostly because of a [talk](https://nrinaudo.github.io/talk-scala-best-practices/#1) I gave at [scala.io](https://scala.io) which some people felt was exciting, but frustrated because the slides didn't necessarily go in-depth enough about the reason for each rule. &amp;#x200B; It's a work in progress and I'd be delighted to get feedback, corrections or suggestions on this!
I use javafx raw, no scalafx. My experience with scalafx is that it breaks down super fast as you mix other libraries for javafx, and when you develop your own custom stuff. So it's just not worth it.
Aren‚Äôt you also the author of kantan.csv? I‚Äôm a big fan!
You might want to link to wartremover rules to encourage checking for these best practices at build time. &amp;#x200B; For example, [https://nrinaudo.github.io/scala-best-practices//partial\_functions/option\_get.html](https://nrinaudo.github.io/scala-best-practices//partial_functions/option_get.html) can be checked by [http://www.wartremover.org/doc/warts.html#optionpartial](http://www.wartremover.org/doc/warts.html#optionpartial)
Yes, I found it far simpler to use javafx raw. 
 Should change the title to something a bit more informative. From the GitHub page Typesafe Interpolation safeStr"" is just like s"" in scala, but it is type safe and *allows only* * **strings**. * **case classes** which will be converted to json-like string by inspecting all fields, be it deeply nested or not, at compile time. * and provides consistent way to **hide secrets**.
Oh hey, sorry I don't think such a list has been compiled yet but you could maybe filter PR's and issues by "release notes" tag
A lot of these can be check with wartremover, and I think most, if not all of them can be with a combination of wartremover, scalac options, scalafix, scapegoat and scala linter. I'm still hesitating to link these because I have the dim hope of implementing them all in scalafix and being able to point to that instead...
I absolutely am, happy to hear you find it useful!
Scala is a good general-purpose language, I'd use it for virtually any programming problem. I guess I'd say weaknesses are limited GUI libraries and slow JVM startup time, and a particular strength is data transformation, so Scala is particularly suited to long-running processes (e.g. servers) that have to deal with data (though I'm not really sure what a program that didn't have to deal with data would look like). As a JavaScript developer you presumably need some convincing to use a typed language at all. While everyone talks about achieving lower defect rates, realistically you're probably happy enough with the defect rates you're already achieving in JavaScript? So the way I'd reframe it is: you can [achieve the same reduction in defect rates that you're getting through testing, more cheaply, by using types instead](https://spin.atomicobject.com/2014/12/09/typed-language-tdd-part1/). Another advantage is great IDE support that really leverages the power of the GUI, e.g. having the types of things available on mouseover - unlike in e.g. Java or C#, Scala's types don't have to get in the way of normal code (you can just write `val x = ...` very similar to what you'd write in JavaScript), but the type system is still there to help you when you need it. Then for me the biggest advantage of Scala over other typed languages is that you don't have to choose between boilerplate and magic. It's got a couple of simple but very general features that mean that things that would require either repetitive boilerplate or a special language/framework solution, can instead be done using a couple of standard language features and then plain old library functions and values. https://philipnilsson.github.io/Badness10k/escaping-hell-with-monads/ has some examples. A related but distinct aspect is that you can do "traverse the object graph" style operations in a safe way, using something like shapeless. So for example JSON serialization in most Scala codebases is zero-boilerplate, but checked at compile time: you can serialise a complex nested data structure with one line, but if something in that datastructure can't be serialized (e.g. a local file, or a lock) then you get an error at compile time. It's worth noting that ScalaJS exists, so if you do find yourself making use of Scala code then you still have the ability to run that code on both server and client.
oh sorry, I didn't realise you linked to my tweet and it got automatically cleaned up by my garbage collector. Gotta like them tweets if you want them to live!
&gt; It's got 0 to do with sbt. I don't think this is true. The community has spoken and I think they do want a better sbt from their commercial service providers. When I was involved with Scala, I was most interested in working on editor tooling, and hence I know the most about it and can speak with authority there... but you should not dismiss alternatives to sbt so quickly. Both Fury and Mill are excellent new tools, both developed outside of Scala Center. Admitedly, Fury is using Bloop, developed at Scala Center, (nobody really knows what Bloop is, btw, it seems to be trying to be a new build tool under the radar, NIH zinc as far as I can tell) but it could just as well be using scalac or zinc directly. If I was in charge of the Scala Center and Lightbend investment directions I'd be putting effort into improving sbt performance and improving *existing tools*, not starting again (a junior developer falacy). I actually suggested this to the committee several years ago https://github.com/ensime/ensime.github.io/blob/e922e7cb3c06ee5f245083591315e8f564bfc343/contributing/center.md but what do I know...
&gt; Almost nobody reads the sbt manual. It's like 80 pages. Whilst I will be the first one to shout at somebody who complains without having read the manual or FAQ :-) I feel compelled to point out that the [sbt manual](https://www.scala-sbt.org/1.x/docs/sbt-reference.pdf) is 482 pages. I was planning on writing "SBT for Mortals" after "Functional Programming for Mortals with Scalaz", based on some private training I'd done at work... but I lost all passion and energy for contributing to Scala.
this is incorrect. It would be more accurate to say that MetaLS does not use the interactive compiler, and that in 2.13+ there are no API users of the interactive compiler so it will almost certainly rot.
It seems you want a generic list but then need to cast back to the original type to do some more specific thing. It seems as though perhaps you can put the specific thing inside the creature class so that the outer code stays generic. val myGenericList[Creature] myGenericList.map { _.theThingIsHappening } and have `.theThingIsHappenning` inside the Creature trait, since it's specific to each Creature?
Thanks for the assistance. /u/m50d helped me realise that adding a method to List to Creature was the best solution.
Bundling language features with better tooling makes no sense. Bundling better tooling and faster compiler makes sense only because a faster compiler is a better tool. Frankly, the questions were quite poorly constructed. They should have given more consideration to the information they wanted to gain and given more discrete choices that do not overlap. A better way to do it would have been to drill into each of these things and ask "what language features?", "which tooling features?", "performance of what?", "reliability of what?". Then, afterwards, ask the high level question of which is the most important, once everybody knows what the question is. As it stands, somebody wanting better tooling could have clicked on "more features" because they consider "autocomplete in vscode" to be a feature, not a reliability improvement.
ENSIME crashes because the presentation compiler doesn't support whitebox macros, I don't know where you're going with discussion about caches. This is a bug in scalac that neither Scala Center nor Lightbend want to solve. ENSIME is also EOL in 2.13, so move over to MetaLS.
&gt; I don't think binary size is one of the top-10 issues facing SBT users today. Size, sure, nobody cares. But having a single artefact is a problem faced by large organisations who need to do a security audit and archive of all tools so they can make un-networked reproducible builds. I hope you've considered this in Mill.
&gt; Will reading the manual and API docs make sbt start up faster, resolve dependencies faster, and trigger builds faster? If so, I'm happy to read the manual. Yes, it does. And every project I ever worked on is proof of this. I tried to capture my thoughts in `sbt-sensible` but I've decomissioned all my scala projects now.
Sam, I've used ENSIME daily since before you took it over. It's crashed since before macros were a thing. Ooms used to happen too. It's not been perfect, but I've really liked using it. I'm not suggesting that we stick with ENSIME. I'm trying to figure out a way to get workable completions in metals. It's not impossible without without the presentation compiler. Just really slow. There's got to be a way to fix that. If you need a fully compiled source, which no presentation compiler seems to suggest, then you could always keep the last good classfile(s) around for any modified buffers, and analyze off of that, even by doing reflection. Shove it in the db. Incremental compile in the background as you type. Heck, you could recursively remove lines from the source until it compiles based on errors and then complete off of the last good compile. You know a lot more about this than I do. I'm just a user. 
I'm not saying the community doesn't need or want a better build tool, but that your tweets were about ide tooling. Everyone here was picking out sbt comments and missed on your important point that we don't have a fully viable ide going forward. That's a big problem. Half the people on this thread probably would not do scala without autocomplete.
My tweets were about people asking for better tooling in general, and the powers that be ignoring them in favour of language features. Then I provided a few examples in my specialist area, which happens to be editor tooling. It's clear that people want better build tools, static analysis, debugging, monitoring and runtime analysis too.
The getting started pages (stuff you need to know day to day) is 100 pages, with quite a few double-spaced code examples. It's not super well done, but it's ok. It's not a revelation to read. The rest of the manual is reference. You need to know it if you are doing something specific. And you'll eventually need to read the api docs. I'm sorry scala burnt you out. I wish you wouldn't have left. It could be so much better. 
It crashed for different reasons before I took over. It was also compiling the entire project inside the presentation compiler when I took over. Now we understand why it crashes very well, and there are tests around it. There is even a subsystem to bounce the complier when it gets into a bad state. The indexer sometimes crashes, and that's because OrientDB is terrible. I think this is easily fixed by swapping OrientDB out for flat files. Neither Lightbend nor Scala Center want ENSIME to succeed. Somebody needs to replace me, and I've not seen anybody want to do that. Therefore, everybody who relies on ENSIME needs to move onto MetaLS and pressure Scala Center to continue funding it and to add the features everybody is missing from ENSIME. I criticise them for NIH, instead of building on top of ENSIME... the semanticdb / metacp backend would have been easy to add to ENSIME. And batch compiler live red squigglies could already be provided by sbt (as an alternative to ENSIME's, which are unreliable in the face of macros).
People want a better build tool, but they aren't switching oss projects to one. Everyday new devs just want maven and don't want ANYTHING else. They have enough cognitive load just dealing with learning scala libraries. I want a better monitoring and runtime analysis and metrics library. And a faster compiler. And fewer features.
yup, and I think you're very much en vogue with that. This is pretty much what I tried to capture in https://medium.com/@fommil/scala-3-considered-as-a-new-programming-language-a335ff67e075 but the vast majority of people didn't read the article and instead decided to create a strawman of me that was "storming out of Scala" for some reason. I guess some people have vivid imaginations.
but now Haskell gets to be so much better instead :-D
TAPAD - Data Company | Senior Software Engineer | New York, New York, USA | Onsite (will relocate)| Full Time &amp;#x200B; We're hiring in New York City and Oslo, Norway. If you have experience with Scala, Java, or any functional language, and you have an interest in working with massive amounts of data (30 Petabyte's), email me [amanda.mendez@tapad.com](mailto:amanda.mendez@tapad.com) or click here for more info: [https://grnh.se/04a19f781](https://grnh.se/04a19f781) 
I like the concept (I absolutely think that people need a simple step by step instruction on how to write a type-class), but it is very tied up with cats and typelevel libraries. It would be more clear if it were just the typeclass, by itself, with no dependencies.
Shouldn't it be `typeclass`?
I really like this! A couple of things: Does this tie into Kelsey Hightower's healthz pattern of introspective healthchecks? * https://github.com/kelseyhightower/app-healthz * https://vimeo.com/173610242 Also, one of the things that is awkward about healthchecks is that they also tie up resources (database connections, threads, etc) when checking for health. Are there any facilities (circuit breakers, etc) for limiting the pool for healthchecks so that they don't impact normal functionality?
Open source contribution may not be the best metric here: Rust is a (relatively) new language, with less established libraries (I'm comparing with Scala, were I have more knowledge). So, a lot of libraries being worked on for Rust can just mean Rust is interesting and exciting (which it is) but has less available stuff (which can be also the case)
I think that can't explain everything, because Rust has a much wider reach ‚Äì for instance there are people writing bindings to all of Window's APIs, people developing emulators, garbage collectors, safer replacements for C libraries, drivers, support for embedded devices etc. ‚Äì nothing that is worthwhile doing in Scala.
&gt; Open source contribution may not be the best metric here It is quite surprising that almost no one pointed that out in the original thread. No one even _mentioned_ industry usage or availability of jobs. I think Scala is (currently) much bigger than Rust there. They seem to assume that "successful" = "popular in open source", which in fact has a lot to do with hype, and is only one of many ways to judge success.
If you want to know one most used language - learn javascript :)
The ‚ÄúScala being overtaken by Kotlin‚Äù has always seemed to me some weird misdirection. Haven‚Äôt seen any of the companies with heavy use of Scala moving _away_ from Scala for Kotlin (I‚Äôve seen people doing work in both, though, and Scala moving to parts of Rust or Haskell), and in my field (big data engineering, although I keep up to date with service architectures that may interact with the more ‚Äúdata‚Äù side) Kotlin is so out of field of vision as to be invisible. Part of the Scala appeal (the ‚Äúheavy typed world‚Äù part of it) can‚Äôt be done in Rust, people moving away from Scala move to Haskell instead. 
To be overtaken, you don't need to have users migrating to Kotlin, it's enough that K vacuums up a pretty large chunk of potential new users. Android has been completely lost, so don't expect much new users coming from that direction, it looks pretty bleak on the JVM, and JavaScript and Native will have a hard time as soon as Jetbrains starts properly integrating those parts with their IDEs for WebDev/native.
Yeah, but I don't get to do Haskell at work. ;-p
Why does simon\_o continue spending his time trolling a community he's supposedly left? &amp;#x200B; Why do mods continue allowing it? &amp;#x200B;
Yeah, agree, I don't see anyone wanting to use Scala for android development since Kotlin is getting good support there. I think it's much more viable and a good experience if one currently doing scala for backend and wants to touch android to just actually use Kotlin on it. The time and pain required to make scala work good for android doesn't seem make much sense. Since the languages are so similar any scala dev can actually very quickly pick it up. So based on this point I also assume that new devs starting from Java and touching a more powerful language as Kotlin for doing android won't even think or want to try use scala for it. Accepting that scala on android is a lost bet/opportunity is the best we can do to actually focus correctly on what matters.
Not scala, but I've been doing some of this with TornadoFX, and I'm wondering if I should just be doing raw JavaFX. Just for context, I chose TornadoFX instead of ScalaFX, because it seemed to be actively maintained. It's pretty good, and I've built a few things on javafx and been pleased with it. Even built a raspberry pi desk nameplate for work :) (again not scala, sorry)
What else should Scala focus on then?
That would be my question too.
Would an entity component system (ECS) be more useful for you, given you‚Äôre writinv a game?
&gt; Why do mods continue allowing it? Because users aren't report it.
This is amazing, as someone new to the language. I just wish there was a way I could page through it by hitting the right arrow on my keyboard or something.
There absolutely are. It's been a frequent request, so I think I'll end up [adding that information to all articles](https://github.com/nrinaudo/scala-best-practices/issues/23).
It's also weird seeing all these people saying it's because of the "more familiar" C-like syntax of Rust. As if this (which does _not_ look like a C-like language _at all_): let triples = (1..).flat_map(|z| (1..(z+1)).flat_map(move |x| (x..(z+1)) .filter(move |y| x*x + y*y == z*z) .map(move |y| (x, y, z)) ) ); Was significantly clearer than this: import Iterator.{from, range, single, empty} . val triples = from(1).flatMap { z =&gt; range(1,z).flatMap { x =&gt; range(x,z) .filter { y =&gt; x * x + y * y == z * z } .map { y =&gt; (x, y, z) } } } or this: val triples = for { z &lt;- from(1) x &lt;- range(1,z) y &lt;- range(x,z) if x * x + y * y == z * z } yield (x, y, z) 
The rust code you show is influenced by ruby syntax.
I'll have a look ECS. Thanks. I'm not technically writing a game, I'm implementing most of the rules around combat and letting scenarios play out, differing on random dice roles.
&gt; influenced by ruby syntax Exactly. And Ruby syntax is not C-like at all. Hence the weirdness of the answers in the original thread.
Huh, TIL that abstract classes are preferable to traits! And I always figured the abstract class was just a Java relic, while the trait was in all cases superior. Thanks!
your tuto mixes methods with function values these are not the same things.
Off-topic, I notice multiple of the mods are online. Have you considered making a ZIO flair as requested a week or so ago? I haven't heard back since.
I completely agree with you. I was trying to be more approachable so to not make reader run away with technical details that won't affect their performances as a code writer. Maybe I should add a paragraph at the end ? What do you think ? 
Probably best to follow up through modmail for this kind of question, but I've taken a look.
I am not sure I understand what you want but here are several options: &amp;#x200B; You can override a method: trait DataModel { def isValid: Boolean } case class A extend DataModel { @transient lazy val isValid: Boolean = true } case class B extend DataModel { @transient lazy val isValid: Boolean = false } Seq[DataModel](....).filter(_.isValid) You can use pattern matching trait DataModel { } case class A extend DataModel case class B extend DataModel Seq[DataModel](....).flatMap{ case a: A =&gt; true case b: B =&gt; false case _:DataModel =&gt; ??? } &amp;#x200B; &amp;#x200B; &amp;#x200B;
Perhaps the most practical thing to do is to show how everything gets desugared. Lambda expressions to FunctionXX[] instances, f(args) to f.apply(args), show some methods of the Function traits like compose, tupled, curried, etc. and then explain eta-conversion from methods to functions ?
Your goal isn't clear from your question, only your envisioned solution. As presented it isn't really clear what value you are hoping to get from this. Can you clarify the actual problem you are hoping to solve with this approach? I suspect there is likely a simple, well established pattern that you can leverage, but I'm unable to suggest anything without knowing the ultimate goal.
I was thinking things along those lines but I haven't talked about Classes in my series yet ! So it gets tricky to introduce those topics within an article about function/methods ... There is too much to introduce at once ! haha 
I think what you have is probably your best option but if you *really* want the syntax `F[A](predicate)` where the parameter is automatically passed based on the predicate then I'm pretty sure you'll have to specify the mapping somewhere and then provide that mapping implicitly. This isn't terribly hard but does require a bit of boilerplate. // the classes you want trait DataModel {} case class A(s: String) extends DataModel {} case class B(i: Int) extends DataModel {} // a way to get the parameter you want from the classes you want trait Param[DM, Param] { def param(dm: DM): Param } implicit object AParam extends Param[A, String] { def param(a: A): String = a.s } implicit object BParam extends Param[B, Int] { def param(b: B): Int = b.i } // the filtering class class F[DM, P] private (predicate: P =&gt; Boolean)(implicit dmp: Param[DM, P]) { def filter(dm: DM): Boolean = predicate(dmp param dm) } object F { // an intermediate step to "save" the type of DM and extract P from the predicate given def apply[DM]: DMSyntax[DM] = new DMSyntax class DMSyntax[DM] { def apply[P](predicate: P =&gt; Boolean)(implicit param: Param[DM, P]): F[DM, P] = new F(predicate) } } There might be a better way of doing the syntax without the intermediate class, but this makes it so that `F[A]` calls `F.apply[A]` and the `(predicate)` calls `DMSyntax[A].apply(predicate)`, which now has enough information (both the actual type of `DM` as well as the actual type of `P`) to find the implicit `Param` based on those types. The implementation of the `F` class can then combine all that information with an instance you pass later. val predA = (s: String) =&gt; s == "hello" val predB = (i: Int) =&gt; i == 0 val fa = F[A](predA) fa filter A("hello") // true fa filter A("world") // false val fb = F[B](predB) fb filter B(0) // true fb filter B(1) // false val failAB = F[A](predB) // fails compilation: no Param[A, Int] val failBA = F[B](predA) // fails compilation: no Param[B, String] [Scastie](https://scastie.scala-lang.org/0wnlNUIGQAWg3fTHL77AJg)
Plain old pattern matching seems to be exactly what you want. Any other thing seems to me over-engineered. ```scala sealed trait DataModel case class A(param1: String) extends DataModel case class B(param2: Int) extends DataModel val f: DataModel =&gt; Boolean = { case A(p1) =&gt; true // do something case B(p2) =&gt; false // do something } ``` 
Another way of thinking about it is that an \`abstract class\` is a subset of a trait in terms of power so its using the least powerful abstraction (which also happens to be the reason why it handles binary compatibility better, because of this strictness its harder to modify in ways to break binary compatibility). &amp;#x200B;
FYI: "Paiges, an implementation of Wadler's a prettier printer" [https://github.com/typelevel/paiges](https://github.com/typelevel/paiges)
Give a description at least? Jobs spec?
**Asynchronous deep learning: training a neural net with Akka** [https://github.com/botkop/akkordeon](https://github.com/botkop/akkordeon)
One possibly-best-practice I use is to not mix "levels" of ADTs, which would solve the error-ADT common ancestor problem.
IHS Markit | Scala Developer |London, UK | ONSITE | Full Time, Contract # Department overview: IHSMarkit has been heavily investing in their technology and data platform to develop a number of new revenue generating products, leveraging opensource and big data technologies. These include new data integration, advanced analytics, visualisation, aggregation and smart data initiatives that address new customer needs and are highly visible and strategic within the organisation. These initiatives are using best of breed technologies, such as Hadoop, Spark, HDFS, Kafka, SOLR, Cassandra and AWS along with in-house developed technologies, and the successful candidate will be working in a fast paced, Agile, dynamic team environment, building commercial products which are at the heart of the business. # Role Description: * Design and implement ‚ÄúBig Data‚Äù infrastructure for batch and real-time analytics. * Ensure highly interactive response times. Avoid allowing performance bottlenecks to creep into the system. * Interpret and analyse business use-cases and feature requests into technical designs and development tasks. * Be an active player in system architecture and design discussions. * Take ownership of development tasks, participate in regular design and code review meetings. * Be proud of the high quality of your own work. * Work with a number of teams (in multiple worldwide locations) * Embrace Agile Methodology * Always follow quality standards (unit tests, integration tests and documented code) * Be delivery focused, have a passion for technology and will enjoy offering new ideas and approaches. # Education and expertise: **Essential** * Bachelor‚Äôs degree in Computer Science, Applied Mathematics, Engineering, or a related discipline, or equivalent experience. * Computer Science and Software Engineering : Strong software development experience: Ability to build production software in Scala or Python * Distributed file systems/databases/systems * Be able to demonstrate commercial experience on big data/advanced analytics projects * Linux/Unix. * Knowledge of algorithms, data structures, computational complexity. * Functional programming * Git **Preferred** (one or more of) * Hadoop, HDFS, YARN, Spark, Hive, NoSQL databases, SOLR, Kafka, Druid * AWS: EC2, EMR, S3 * Visualisation: knowledge of visualisation styles and tools, and ability to programmatically create visualisations. Able to create visualisations that minimise time needed to understand complex data sets. HTML5/D3 * Apache Zeppelin * Akka * SQL # Personal competencies: * You can effectively provide technical direction, and estimates to fulfill a set of requirements * You can effectively manage timelines and cross team coordination * You can follow relevant technology trends, actively evaluate new technologies, and use this information to improve the product * You get a lot of satisfaction from on-time delivery * Happy clients are really important to you * You take pride in your work * You love to solve complex problems, whether that's making the user experience as responsive as possible or understanding complex client requirements. # Communication: * You can confidently present your own ideas and solutions, as well as guide technical discussions. * You can effectively communicate with external clients # Teamwork: * Your welcoming attitude encourages people to approach you when they have a problem you can help them solve. # Company overview: [IHS Markit](https://ihsmarkit.com/index.html) is a leading global diversified provider of financial information services. We provide products that enhance transparency, reduce risk and improve operational efficiency. Our customers include banks, hedge funds, asset managers, central banks, regulators, auditors, fund administrators and insurance companies. Founded in 2003, we employ over 18,000 people in 30 countries. IHSMarkit shares are listed on Nasdaq under the symbol ‚ÄúINFO‚Äù **Please DM me if you would like to chat about this position. Please mention this Reddit thread if you apply, thanks**
Judging by OP's comment history I'm guessing it's at Swissborg, e.g. something like this one: https://jobs.lever.co/swissborg/45611f41-fe61-478f-9d29-f276b8ce0cc2
Removing because this isn't the place to solicit private messages for mystery job postings. If you have a position available please post the details here at least. Please post the job with the following template: Company Name | Title(s) of position(s) being hired for | City, [State/Province,] Country | {ONSITE, REMOTE} | {Full Time, Part Time, Contract} | (Optional) $approximate salary description contact information
Not all companies want to disclose their hiring activity...
Seriously? It's so secretive you can't even say the positions being hired for, what city it's in, if it's onsite or remote, approx salary, or a brief description of the role?
I have shared some of the info with those who asked. I am getting paid for finding people and I need to make sure the time I spend searching gets remunerated.
I have shared some of the info with those who asked. I am getting paid for finding people and I need to make sure the time I spend searching gets remunerated.
Despite lots of downvotes, he's right - JRE is not suitable for most B2C applications. Swing is old, JavaFX is no longer developed by Oracle, java apps are memory-hungry and have large startup times. Another large issue is JRE deployment, even with compact profiles. Sooo, unless aming for B2B or generally does not caring about startup time/memory usage/looks, better stick to another technology.
Java is memory hungry He suggests electron Are you guys even listening to yourselves
Actually, I wouldn't consider that to be a common opinion in the Scala community :-) There were always tradeoffs to consider for traits vs abstract classes, but even given all those tradeoffs, the _Programming in Scala_ book always recommended starting with a trait unless you _needed_ an abstract class. And as of Scala 2.12, binary incompatibility and Java interop problems have been almost completely mitigated, making traits much more attractive. Here's a more in-depth excerpt from the book, and an annotation by Eugene Yokota (one of the sbt maintainers): https://stackoverflow.com/a/15330312/20371
I don't know if it makes sense to compare traits and abstract classes in terms of 'power'; you're right that traits are a superset of abstract classes but in my mind that makes them more attractive, in the same way that I would reach for a more general solution to my problem _if it's readily available_ rather than a specific solution. Btw see also https://www.reddit.com/r/scala/comments/ablaat/collection_of_scala_best_practices/ed9garr/
Thank you! I'll give it a read.
Is it efficient enough?
Released! And I wrote about it here: http://www.geekabyte.io/2019/01/ristex-010-released.html
In the scenario you are talking about, you could use "Compare And Swap" aka CAS. We'd use &amp; mutate an AtomicReference, but it always points to an immutable class. (see \`AtomicReference&lt;&gt;.compareAndSet\`) In this pattern the threads will "optimistically" read the reference, see the value (100), and use that to calculate the new value (150), without locking. But just before setting the reference to the new value (150), the CAS operation will check to see if the reference changed. If so the thread will 'retry' starting from the value in that newer reference. &amp;#x200B; For example in your situation We start with the AtomicReference containing a ref 0x0001 pointing to an immutable value "100". Thread 1 reads the AtomicReference. It gets a ref 0x0001 pointing to "100" &amp; stores this ref (thread1.original) Thread 2 reads the AtomicReference. It gets a ref 0x0001 pointing to "100" &amp; stores this ref (thread2.original) Thread 1 calculates "150" Thread 2 calculates "150" Thread 1 does AtomicReference's CAS operation. This reads 0x0001, which is the "original" ref that thread 1 saw, so the CAS operation will succeed, and the AtomicReference will be set to point to the "new" value 150. Let's say that has ref = 0x0002. Thread 2 does CAS operation. This sees a new ref 0x0002, which does NOT match the original ref that it saw. Therefore Thread 2 will "retry" &amp; start the cycle again (gets 0x0002 pointing to 150, stores this, calculates "200", tries to set the atomicref, etc) &amp;#x200B; Of course if you really just wanted a number like "100" you could use AtomicInteger. But this AtomicReference&lt;T&gt; method can use any class T. However things would not work out well if the target of the ref T was being mutated - as you point out ! &amp;#x200B; Also, as you note, the "collisions" are unlikely so this optimistic approach of CAS tends to perform much better than pessimistic approach of locking. &amp;#x200B; AFAIK this is how Akka works. You make an immutable message, and just send the ref over to another class. The message is not actually copied. There is no locking involved. Under the hood, it keeps "queues of refs" that use lock-free CAS operations like this.
I should make the comparison with some frameworks, and include it in the README. Will do so in due time.
ScalaFX isn't dead yet, though. Upstream is moving very slowly, and I think that other than for one minor class or two it's at feature parity with openjfx 11. Not saying that interacting with "non wrapped" controls is always as nice as it could be, in my experience it's mostly transparent as long as you have imported the necessary implicit conversions.
&gt; If you just want to use the most popular thing with the largest plugin ecosystem, wouldn't you be better off using Maven? No, because Maven doesn't have the plugins that matter for Scala and isn't the most popular in the Scala ecosystem.
There's a free book from Underscore that covers this: [_Scala with Cats_](https://underscore.io/books/scala-with-cats/)
&gt; Martin Odersky scala course on coursera - Did not really enjoy it I mean, if you're trying to learn Functional Programming, that course is solid. Why didn't you enjoy it? The only reason I found it boring was because it was stuff I already learned in college learning OCaml.
I've heard good things about this from colleagues - [https://www.amazon.co.uk/gp/product/1979788782/ref=oh\_aui\_detailpage\_o04\_s00?ie=UTF8&amp;psc=1](https://www.amazon.co.uk/gp/product/1979788782/ref=oh_aui_detailpage_o04_s00?ie=UTF8&amp;psc=1)
Good for them if itsn't I believe that all that code and wrappers/proxy literally to turn java properties (with getters and setters) into scala properties is not worth it at all. If you take all of that away, you are left with some DSL for animations and another one for bindings, it's one file each in my "javafx support" libary, less than 200 lines of code. They'd do better to implement a scala compiler plugin for this.
Even though you said you were unable to understand a lot of the code samples in Functional Programming in Scala, i still recommend this book. Keep in mind that it's not a book about Scala, but a book about Functional Programming. Did you actually do the exercises? The code samples would be hard to read and understand without trying them out yourself.
I can confirm that learnyouahaskell was really enjoyable and even though I don't know Haskell I understood almost everything. And of course it really helped me with Scala.
Scala Cookbook is by far best book that I came across for Scala , it does not teach the programming in traditional approach, but gives solutions to problems one encounters during coding. It's available for free . Just search "Scala Cookbook PDF download"
Work with us to understand those books. You'll have a much less stressful time working through FPiS and FPfM than having to go through the outdated and often wrong-headed material in the last 2 books you mentioned. Lean on the community for help!
&gt;the outdated and often wrong-headed material in the last 2 books you mentioned Do you mean Martin Odersky's course on Coursera and the Programming in Scala book? &gt;Lean on the community for help! Posting the code snippets here on reddit to better understand them? Is there a better forum/chatroom for learners where I should be posting questions from FPiS and FPfM?
Is it recommended to go through Functional Programming for Mortals before Functional Programming in Scala?
I had actually posted a question regarding this on this subreddit - [https://www.reddit.com/r/scala/comments/a5ssuk/should\_i\_learn\_haskell\_before\_scala/](https://www.reddit.com/r/scala/comments/a5ssuk/should_i_learn_haskell_before_scala/) and the general consensus was to just dive into Scala with the Functional Programming for Mortals book.
I've never read the former, so I cannot answer that. It was no problem for me starting directly on functional programming in scala, but I already had some experience with scala, as well as one class in functional programming some years back.
&gt;Do you mean Martin Odersky's course on Coursera and the Programming in Scala book? Yes &gt;Is there a better forum/chatroom for learners where I should be posting questions from FPiS and FPfM? Join the [Scalaz](https://gitter.im/scalaz/scalaz) and [Typelevel](https://gitter.im/typelevel/cats) gitters, and the Scalaz irc channel (#scalaz on freenode). You'd probably be better off learning the first 4 chapters of FPiS and moving on to FPfM later. FPfM is the most thorough book, hands down, for FP that has been produced (tho, the underscore book is also good). We tend to be very helpful to beginners learning the subject, so no worries about pestering. I am biased, but I'd recommend the scalaz irc to beginners learning FP in general. 
I do not mean any offense to Martin, but his delivery just seemed very monotonous.
I'm in the process of taking that course right now. While I agree his cadence is very monotonous, I feel like I've learned so much from the course that I can't really fault it because of that.
Sounds like you need a tutor or mentor to get through the books to me. Especially if it's only a few code samples that are preventing you from progressing through the book.
To understand scalaz and cats you need to understand `type classes` what are their pros and cons in comparison to normal `classes` and `inheritance`. And how it influence the code. It is often compared to `traits` but it is quite different. `Type classes` are implemented in scala using `implicits`. So learn them too. `Implicits` might be used for more things than `type classes` (you do not need to learn all of them, just be aware of that). This video is really good intro to them [https://www.youtube.com/watch?v=sVMES4RZF-8](https://www.youtube.com/watch?v=sVMES4RZF-8) And these are some patterns [https://github.com/fokot/scala-adventures/blob/master/docs/type-classes.md](https://github.com/fokot/scala-adventures/blob/master/docs/type-classes.md) and workshop [https://github.com/fokot/scala-adventures/blob/master/docs/type-classes-workshop.md](https://github.com/fokot/scala-adventures/blob/master/docs/type-classes-workshop.md) I wrote for them.
https://alvinalexander.com/photos/scala-cookbook-pdf-download
Assuming you already understand the FP basics of higher order functions, and that everything is immutable then I've found [Scala with Cats](https://underscore.io/books/scala-with-cats/) to be awesome for getting into type classes as well as understanding the intimidating FP terms like semigroup, applicative, and monad. 
A friend of mine recommended "Atomic Scala" by Bruce Eckel as a good introduction to the language. I had done Java for years and found it to have a pretty gentle learning curve. He presents the material in bite-sized pieces with an easy conversational tone, so you may prefer that. He doesn't go deep into the language but I think his book will give you a good foundation for more advanced texts where you'd be better off reading as necessary/ &amp;#x200B; &amp;#x200B;
Yeah I feel you. I personally find scalafx code to be pleasing to read and to write so I think it's a goal worth pursuing. Even if the bindings were to be lagging behind too much with newer functions not being made available on a timely fashion, it's always possible to fall back to using the wrapped javafx methods so I don't see big risks either.
Whatever you do, you need to supplement it with the Odersky book, in bite-sized chunks. The first 24 or 25 chapters are essential to understanding core Scala, and the last 10 or so are optional. Scala supports functional, imperative, and object-oriented programming styles - and you can mix them. Sometimes the best solution in Scala is not the functional solution. Sometimes the functional solution is downright bad. Remember, the JVM is being retrofitted for functional, and sometimes the tradeoff is performance and/or memory overhead. So I guess what I am saying is it is good to learn the whole language. I tried to get some coworkers started in Scala using the Odersky course. They had the same comments. They couldn't get into it. Too theoretical.
I found this very useful, thanks! Just one point - "Use override when implementing abstract members" I often am in two minds about whether to use override or not. I prefer to not use override when it isn't required. My reasoning is that by not using override, I am indicating that I am implementing an abstract member (this is quite clear when writing a type class instance). If I had a typo in an abstract method name, the code would not compile since a method implementation would be missing.
Have you heard of [ensime](https://ensime.github.io/)? Haven't used it yet but it "appeals to hackers, minimalists and connoisseurs". It also supports Vim, among other editors.
If you can live without autocompletion (it might be a feature soon) I'd say [metals](https://scalameta.org/metals/docs/editors/vim.html) is the best tool out there.
Have you looked at VSCode with Metals? Vim bindings are not perfect there either, but I've accepted and gotten used to it. Other than that, you can also use Ensime with Vim with a bit of work.
I've never been able to learn programming from videos or books; what I recommend is writing code that solves real problems you have, above all else. The more esoteric techniques/constructs tend to be things that you only need occasionally, but by the same token it's fine to only learn them at the point where you actually have a use case for them. Certainly if technique B builds on technique A and you've never had a real-world use case for technique A, then I think looking into technique B will be more confusing than helpful.
I had some trouble setting up the metals plugin on vim. 
What do you consider to be "good" usages of a) subtyping b) implicits? We can only come up with "good" usages of implicits with subtyping relative to those. E.g. some people use implicits to "autowire" their dependency injection. In that case this behaviour is exactly what you want: if you have a service that needs an `EmailAlertSender` it should get the `EmailAlertSender`, if there's a service that needs any `AlertSender` then if the `EmailAlertSender` is the only one in scope then it should implicitly get that one, but if there's also an `SnmpAlertSender` in the same "application context" then you want compilation to fail until you specify explicitly which one to pass. But I don't know whether you consider that to be a good use of implicits (or indeed of subtyping).
The red book is about fp using Scala for implementation, rather than great for learning Scala. You definitely need to do the exercises though, or you really won't get much out of it
&gt; What do you consider to be "good" usages It's up to interpretation. If _you_ think your usage is good and deserves to be mentioned, I'm interested. &gt; E.g. some people use implicits to "autowire" their dependency injection. I don't see where the "most specific" rule comes up in your example. It's just a classical ambiguity, because neither `EmailAlertSender` nor `SnmpAlertSender` is more specific than the other. It's about preferring them over a general `AlterSender` that would also be in scope. In your use case, I don't think that would typically happen. 
explanation why Cofunctor is just a Functor: [https://stackoverflow.com/questions/34732571/why-there-is-no-cofunctor-typeclass-in-haskell/34732721](https://stackoverflow.com/questions/34732571/why-there-is-no-cofunctor-typeclass-in-haskell/34732721) The Haskell library with Cofunctor is just a joke. But it is co-obvious :) I am curious why free functor is defined the way it is.
How do I prevent "sbt publishLocal" from attaching "-SNAPSHOT" to the end of the version string? I'm already overriding the version string with GitVersioning plugin. Is there a flag I can use to tell it, it's a release and not a snapshot please?
&gt; I don't see where the "most specific" rule comes up in your example. Yeah, sorry, I misread the detail you were asking about. The one case where I can see the rule being useful is for a diamond scenario - either in a cake-style use case, or a typeclass =&gt; typeclass pattern. For example if we have implicits for deriving composition of applicatives and composition of functors, and we request the functor instance for the composition of two applicatives, then we'd like that to be unambiguous (I think?), and it should probably resolve to the applicative instance in the case where the things we're composing are applicative (because that might be more efficient etc.), but we still want the functor case around for cases where one side isn't an applicative. I don't know if that works in practice though?
Short answer, use IntelliJ IDEA + Vim plugin.
I would say that currently, for big project, intellij is better. You could probably get a good environment with vim/ensime but it would be more complicated to configure. I hope this will change with Scala 3 since it will come with an LSP (language server) that should allow to get easily the IDE features in vim, vscode or any editor compatible with the LSP protocol.
Unfortunately I think your time explaining this is not well spent. The author clearly does not care enough to be correct, nor to ask for help. 
In case someone looks back through this post, I'm no longer with the company.
Scala is a language that absolutely requires tooling support if you want to be productive. The first time you have to figure out where an implicit is implemented, or which trait is controlling a particular implementation of a method, you will understand this. :) Import management is also a pain unless you have a tool that does it for you. Once you get full autocomplete support, you will wonder how you ever lived without it. IntelliJ is the best for Scala that I have used. Unlike for most other languages, Scala tooling support is very complicated to do right, so there are not a lot of good options.
You can always ask for help. Most folks I know (including myself) have had no issues in getting it to work. I suggest asking in the [Gitter channel](https://gitter.im/scalameta/metals).
have you considered pants?
[https://github.com/bazelbuild/rules\_scala](https://github.com/bazelbuild/rules_scala)
## ask: Will Scala 3 releases promises binary compatibility? &amp;#x200B; I am curious whether Scala 3.0 release binaries compatible on its minor version? I mean, Scala 3.0 compiled libraries can be used in Scala 3.1, 3.2 etc projects?
We use bazel with scala, java, go, c++, and a bit for other tasks at Stripe. I would only recommend bazel for larger codebases with more than one team working on the code, and especially if you use more than one language. If this meets your description then I recommend it. The reason why is that you get out of the box reproducible builds which enable caching, which makes CI very fast. Setting up a build cache is not too much work (no special scala settings really, the build cache is a bazel wide set up). If you want to invest more you can deploy build farm and get distributed parallel builds. This is high tech stuff for most people coming from sbt or maven. This can give your developers and the CI massively parallel builds. The main downside is that you should write relatively small builds targets and be careful with dependencies. There is fairly good support for intellij with the intellij bazel plugin, but outside of that it‚Äôs somewhat manual to make build files. I wrote https://github.com/johnynek/bazel-deps to manage depending on maven artifacts from bazel. It supports scala first class. If you hit any problems, file an issue on bazelbuild/rules_scala.
We tried to move to Bazel in my team a while ago, and we finally decided not to: the rules\_scala in bazel are a fork or the java rules, which have since evolved (but those changes haven't been backported to rules\_scala). We hit some problems that were fixed in the java rules but not in the scala ones (can't remember the specifics right now). Also, we use the Play Framework and support for routes compilation, twirl, etc in bazel was poor. &amp;#x200B; Finally, Bazel doesn't have good support in Windows (we even found things that didn't work in MacOS - just Linux) so that was another reason not to adopt it.
I second this... I try to solve something in FP (meaning ref. transparent etc.) taking no shortcuts, and once Im stuck I look into how other "fp open source library" solved it and just copy that approach. That way I learn it as well from high quality material.
I disagree: with your rule, you have to always keep the context in mind. Is this an abstract member? am I overriding a default implementation with something more efficient? I prefer to unclutter my brain and have simple default rules (with no downside) to focus on the important bits. But you're absolutely right though, I think - if you accept the (light) mental burden of always considering whether the member you're defining is an implementation, a redefinition or simply new, then there's no need to add \`override\` for implementations.
I have changed to [spacemacs ](https://github.com/syl20bnr/spacemacs) with [ensime](http://ensime.github.io) a while ago after being a intellij user for a couple of years and I am really happy with the decision. A couple of things to take into consideration before you jump there: -if you are an IntelliJ user there are completely new shortcuts and combinations of commands you are gonna need to get used to in order to take full advantage of the tool. In my case I already was a spacemacs user for Haskell development and the change wasn‚Äôt so radical - Installation and configuration is not one click away. Depends on your environment sometimes it takes some work arounds to get it work. - ensime works perfectly fine with Sbt based projects. I don‚Äôt know how it works with gradle or maven projects. In general once everything is properly configured it is a great experience. You have everything you need: auto completion, navigation, refactoring, run sbt commands inside the console, etc with the addition that it is a lightweight vim/emacs editor. I hope my answer can help you Best 
it is referentially transparent if you wrap it into `Task`, e.g: `_ &lt;- Task.delay(MDC.put("foo", "bar"))`. But yes, I agree that having to rely on `Task` everywhere is a limitation, but for us it's fine. I personally don't see much of a benefit from tagless final in the application code and I decided to trade it for better logging.
Transitive deps are a nightmare in bazel. I don‚Äôt know if it‚Äôs changed but if you want to say use something like spark you have to define it and all of its transitive deps manually in order to build. It‚Äôs good because you know 110% what you‚Äôre getting, but compared to one line in any other build tool it‚Äôs absurdly daunting. 
Some work has apparently continued in this area: https://github.com/bazelbuild/proposals/blob/master/designs/2018-11-07-design-recursive-workspaces.md
 &gt;"Compilers is my thing" 
I‚Äôd have to second this. For Scala, pants is pretty straightforward (it does need a little work to write all the BUILD files), and the IntelliJ plugin is updated fairly often.
It's hard to tell whether you are asking about implicit specificity in general, or just cases in which the specificity only comes down to subtyping - because there are other cases. Apologies if these are things you already know (probably yes), but I guess it's still worth mentioning for the readers: people often confuse priority with specificity. In particular, they think there are way more ways of influencing priority than there actually are. There are only two levels of priority: implicits in the direct scope (those you can access unqualified like imports or definitions in the current file) and implicits in the extended scope (supertypes of companion objects of component types), where the former has higher priority than the latter. Within these two priorities, things are only decided by specificity, as you said: each implicit is scored by a series of rules, and the winner gets selected. The rules used are _almost_ the same as the ones used for overloading. For example, here's the common "low priority trait" trick: ```scala trait T[A] trait LowPrio { // implicits at lower "priority" } object T extends LowPrio { // implicits at higher "priority" } ``` is a misnomer: those two implicits are at the same priority, and the ones in LowPrio are _less specific_ (and in fact in some rare cases that trick might not work if other rules are offsetting it). So, if the question was: use cases of specificity, then each instance of the LowPrio trick is a use case, and there are millions of those, from the instance selection that /u/m50d mentions (they are in cats) to the recursive type level function definitions that are at the heart of shapeless. There are also usages of covariant implicits (without ambiguities) in `Predef` : ```scala sealed abstract class &lt;:&lt;[-From, +To] extends (From =&gt; To) with Serializable private[this] final val singleton_&lt;:&lt; = new &lt;:&lt;[Any,Any] { def apply(x: Any): Any = x } implicit def $conforms[A]: A &lt;:&lt; A = singleton_&lt;:&lt;.asInstanceOf[A &lt;:&lt; A] ``` `&lt;:&lt;` is pretty important in shapeless-style code as well, because sometimes the equivalent `&lt;:` constraint on type parameter doesn't play well with some other implicit you are searching for, whereas an additional `&lt;:&lt;` implicit does. Another interesting case of specificity is shapeless `=:!=` and `=:!=`, to assert that two types are _not_ the same and _not_ subtypes of one another respectively. ```scala trait =:!=[A, B] extends Serializable implicit def neq[A, B] : A =:!= B = new =:!=[A, B] {} implicit def neqAmbig1[A] : A =:!= A = unexpected implicit def neqAmbig2[A] : A =:!= A = unexpected @scala.annotation.implicitNotFound("${A} must not be a subtype of ${B}") trait &lt;:!&lt;[A, B] extends Serializable implicit def nsub[A, B] : A &lt;:!&lt; B = new &lt;:!&lt;[A, B] {} implicit def nsubAmbig1[A, B &gt;: A] : A &lt;:!&lt; B = unexpected implicit def nsubAmbig2[A, B &gt;: A] : A &lt;:!&lt; B = unexpected ``` They both use the same trick, _two_ instances for the opposite of what you're asserting, so that you can encode failure as ambiguity, and a less specific instance to encode success. So it feels like specificity is useful overall, albeit with some weird corner cases. I'm not sure any of these examples answer your specific question btw, but they felt relevant. 
hey thanks for the response! Do you know if stripe is still maintaining the sbt-bazel converter? [https://github.com/stripe/sbt-bazel](https://github.com/stripe/sbt-bazel) looks promising for me
It‚Äôs used for two libraries I think. The was we use it is to have a public OSS project with an sbt build, but consume it internally as bazel. Since bazel supports remote source dependencies, we aren‚Äôt using the sbt built artifact. So it works well enough for us. If you hit issues, file them! Good luck!
I have not yet but I do plan to eval pants as well at some point.
On that note- is there a build program that also make allowances for native tools in a multi-language project? E.g. Sbt for Scala Is this even a valid question?
It's a bloated software but until metals language server is ready its the best coding assistant
It seems limiting compared to rules_scala_annex (e.g., all our Scala is 2.12‚Ä¶ except for the Spark stuff).
Awesome, I just gave it a test on a hello-world scala project and it generated the BUILD file for the binary and library but neglected to include the test. I attempted to add it myself but bazel doesn't seem to like it. \`\`\` load( '@io\_bazel\_rules\_scala//scala:scala.bzl', 'scala\_binary', 'scala\_library', 'scala\_test' ) scala\_library( name = 'hello', deps = \[\], runtime\_deps = \[\], exports = \[\], visibility = \[ '//visibility:public', \], srcs = \[ 'src/main/scala/example/Hello.scala', \] ) scala\_binary( name = 'hello-bin', deps = \[ ':hello', \], main\_class = 'example.Hello' ) scala\_test( name = "hello-test", size = "small", srcs = \['src/test/scala/example/HelloSpec.scala',\], main\_class = "example.HelloSpec", deps = \[":hello"\], unused\_dependency\_checker\_mode = "warn" ) \`\`\` &amp;#x200B; &amp;#x200B; Output of bazel test //... &amp;#x200B; \`\`\` exec ${PAGER:-/usr/bin/less} "$0" || exit 1 Executing tests from //:hello-test \----------------------------------------------------------------------------- Error: Main method not found in class example.HelloSpec, please define the main method as: public static void main(String\[\] args) or a JavaFX application class must extend javafx.application.Application \`\`\` &amp;#x200B; Could be my naive understanding, any pointers? I'll note sbt does test correctly.
It looks like the author has made up it's mind, so maybe just my 2c: Keep the Bazel "code" to a minimum and directory layouts as "standard" as possible (similar to sbt). When/If you realize it doesn't suit your use case (what is your use case/problem?), you can always go back. I would certainly opt for something more mainstream other than what's been proposed, for an enterprise project, not for a hobby type of thing.
Leapfin | Software Engineer | San Francisco, CA, US | Onsite or Remote | Full Time We are an enterprise b2b finance/accounting software startup based in SF SOMA. Because we work with finance, we deal with lots and lots of data. We have found product/market fit and are growing rapidly. We are looking for a technical leader who can help us scale. We are looking for someone able to collaborate well in a team environment and who always puts the customer first. Job Posting: https://leapfin.com/careers About: https://leapfin.com/about Let me know if you have any questions! 
Is there any Cats+FP starting guide for newbie in Scala?
This might be a weird offer, but I'll throw it out there anyway. I'm someone with a background/education in supply chain management, but have 8 years experience programming in an analytical context using R, SQL, and Scala (Scala for 4+ years now), as well as a few other technologies (linear, integer, and constraint programming). I'm currently making a decent living by algotrading in futures and forex markets, with 90% of my code written in Scala, and some offline analytics done in SQL and R. I'll stress that this is production code, earning real life money ([for example, here is my results for today](https://i.imgur.com/vCB5kP9.png)). I feel like I am a good programmer when it comes to the core of programming and problem solving in general, but I'm self conscious about my lack of formal CS training, as well as inexperience with some of the more rigorous parts of software engineering as a trade (eg. architecture, testing, development workflows, collaborative development, release cycles, devops, etc.). And I'd like to find a position that will help me to develop those capabilities. I understand that this I'm not the typical entry level candidate, but where I lack in some areas I definitely can make up in others (analytics, machine learning, business experience, etc.). If there are any teams in the Seattle area (no relocation for personal reasons, unfortunately) that are willing to take on someone like me, I feel like I could quickly become a valued member of the team. My expectations on compensation are modest; I'm already earning a living through trading, and I would fully expect that my salary will reflect the amount of risk you'd be taking on with me. Resume available upon request. PM me here, or email at saosebastiao1982@gmail.com
&gt; "you are gonna edit this, right?" haha apparently not üò± maybe there are some highlights in this (?), but I certainly wouldn't have shared it here myself
Scala 2.12 is supported. We run both 2.11 and 2.12 at Stripe.
Just a note, the rules scala were never a fork of the java rules. If anyone else hits issues please file them. These rules are used in prod by several large companies but we still need people to at least file issues for OSS to work.
This is handled for you by e.g. bazel-deps: https://github.com/johnynek/bazel-deps
It is a valid question: but currently either sbt or bazel owns the build state. If you do this, any time the code changes sbt will have to rebuild everything. You could imagine a world where bazel can somehow share state with another tool. Unfortunately sbt is not reproducible which is why caching is hard with sbt. Using sbt with bazel would defeat the key benefits and make the CI slow or require periodic resets.
Note bazel is more popular with scala than pants: https://typesafe.co1.qualtrics.com/results/public/dHlwZXNhZmUtVVJfNlB4cWNSMXdub0liVExmLTVhZjMwZDc4MjAzMGVkMDAxNDhkOTc4OA==#/pages/Page_e4247943-5c33-43a4-bb3a-0ea8d39babfa I used pants for years. I‚Äôm much happier with it and the only people I know who‚Äôve used both in production (like myself) have preferred bazel.
Thanks! I just didn't know if such a thing even made sense. In my very humble and uninformed opinion- I think that there should be a build tool that delegates to these native tools where possible and that these build tools should be able to talk back and forth with such a tool. This way , bazel, pants etc do not have to reproduce sbt, cargo etc functionality and ill be more comfortable jumping in because I won't have to rewrite my existing sbt build. 
I hear you. On the other hand, maybe we can actually stop reinventing build tools and share build tools across languages. If compilers were properly composable, this was be pretty easy. Unfortunately many compilers hide a lot of state and are not designed for separate compilation.
Sounds like a good dream! Thank you for your time and the discussion. 
Oh wow this is quite the interesting survery, thanks!
But it doesn‚Äôt seem possible to use both Scala 2.11 and Scala 2.12 in the same workspace using scala_rules. Is it possible? How?
This is what happens when I type `sbt version`: [info] Loading settings from plugins.sbt ... [info] Loading global plugins from /home/phill/.sbt/1.0/plugins [info] Loading settings from plugins.sbt ... [info] Loading project definition from /home/phill/git/my-application/project [info] Loading settings from build.sbt ... [info] Set current project to my-application (in build file:/home/phill/git/my-application) [info] 2.6.7_gdff3ea1 [INFO] [01/09/2019 08:54:18.641] [Thread-4] [CoordinatedShutdown(akka://sbt-web)] Starting coordinated shutdown from JVM shutdown hook Is there any reliable way of extracting the actual version to use in a bash variable? All my scripts don't print the last line so I could do: version=`sbt version | tail -n 1 | awk '{ print $2 }'` but now I am writing scripts that depend on knowing exactly which line of output is the line with the version info. Any thoughts? 
This [Cats book](https://underscore.io/books/scala-with-cats/) is beginner friendly but I still strongly suggest you read their other others first which sort of teach both basic [FP and Scala](https://underscore.io/books/scala-with-cats/) as a language at the same time. It's just that some of the real basics of FP such as passing around partial functions, preferring immutability and recursion over mutating state and using ADT all kind of need to sink in first or you'll find yourself overwhelmed. I found the best functional book to be [the red one](https://www.manning.com/books/functional-programming-in-scala).
Since it's compiled in a different file, the executable needs to be able to access the compiled classes at runtime. You need to add the files to the classmate of the Scala executable. `scala -classpath . summer bla bla bla`
Perfect, got it working. thanks alot
It made my day :) &amp;#x200B;
I used emacs for several years while I was programming with c++ and python. When I started to work with java I switched to intellij after few weeks of struggle. I have tried ensime. It hangs more than intellij in my experience. Also I think it does not support mixed projects (scala + java) that well. And sometimes I encounter such projects. Especially ones that are migrating. In the end it's significantly easier to configure intellij and change your work process, than to use emacs/vim. Proper IDE gives nice productivity boost when it comes to java/scala. 
Thanks for the detailed answer. Yes, I was looking specifically for uses of specificity that derive from subtyping and variance (and mores specifically _co_-variance), as indicated by the title of my post and by the example I gave. As far as I understand from [the spec](https://www.scala-lang.org/files/archive/spec/2.13/06-expressions.html#overloading-resolution), the specificity derived from inheritance hierarchies has nothing to do with subtyping _per se_ (the fact that inheritance also introduces a subtyping relationship is irrelevant from the viewpoint of overloading/implicit resolution). 
I did not fully understand your use case (a code example would help), but it's funny that you should mention the diamond pattern, because I think the rules of specificity work _against_ us in that case. Consider: class Functor[F[_]] object Functor { implicit case object FunctorOpt extends Functor[Option] } class Monad[F[_]] extends Functor[F] object Monad { implicit case object MonadOpt extends Monad[Option] } class Traverse[F[_]] extends Functor[F] object Traverse { implicit case object ApplicativeOpt extends Traverse[Option] } The only reason `implicitly[Functor[Option]]` works is that implicit resolution does not search into `Monad` and `Traverse`, which are not part of the implicit scope of `Functor[Option]`. But if we actually import all the instances into the current scope, the above fails because `Traverse[Option]` and `Monad[Option]` have the same specificity. If instead, implicit resolution looked for the _least_ specific instance (with respect to subtyping), it would pick the functor instance, and all would be well. This benefit would translate to cases where the problem usually appears, as in: def foo[F[_]:Monad:Traverse] = implicitly[Fonctor[F]] // ambiguous as we could then just add: def foo[F[_]:Monad:Traverse:Functor] = implicitly[Fonctor[F]] // would no longer be ambiguous 
I used ensime for a few years successfully. I've recently started a new project so went back to IntelliJ to learn it, but it was functional and decent.
So I don't know to what extent this actually works, but what I was thinking of was: trait Functor[F[_]] trait Applicative[F[_]] extends Functor[F] implicit def composeFunctors[F[_]: Functor, G[_]: Functor] = new Functor[F[G[?]]{} implicit def composeApplicatives[F[_]: Applicative, G[_]: Applicative] = new Applicative[F[G[?]]{} implicitly[Functor[Writer[Int, Option[?]]]] // derive the applicative instance without error implicitly[Functor[Writer[Int, Const[Octonion, ?]]]] //derive the functor instance without error &gt; But if we actually import all the instances into the current scope, the above fails because Traverse[Option] and Monad[Option] have the same specificity. Indeed, but there's an argument for that behaviour because there's no reason to assume that the two functor instances are coherent with each other; if they are, we'd expect to have a `Monad[F] with Traverse[F]` instance which would be more specific and all would be well. This puts a burden on implementors of monad-transformer-like things to provide derivation logic for all possible combinations of typeclasses that a user might want, but I can't see any other way to make it possible to supply both consistent and inconsistent instances of `Monad` and `Traverse` for a given type while making a useful distinction between them. (You could make the same argument to say that it should be possible to provide inconsistent implementations of `Applicative` and `Functor` and therefore we shouldn't silently resolve to the `Applicative` implementation, but I find that less defensible because an `Applicative` is-a `Functor`. IMO it's reasonable to expect people implementing `Applicative` to know whether there's a `Functor` implementation for that type already; it's less reasonable to expect them to know whether there's a `Traverse` implementation, or even that `Traverse` exists at all. I'm sure there's a lot of status quo bias in that logic, but it's how I'm thinking of it at the moment.) The other argument I can see for more-specific is where the specific thing can be implemented more efficiently. For example: trait Eq[A] trait Ordered[A] extends Eq[A] trait Iterable[A] trait SortedSet[A: Ordered] extends Iterable[A] trait Find[CC, A] { def contains(cc: CC, a: A): Boolean } implicit def findInIterable[A: Eq] = new Find[Iterable[A], A] { /* linear search */ } implicit def findInSortedSet[A: Ordered] = new Find[SortedSet[A], A] { /* binary search */ } We would want looking for a `Find[SortedSet[Int], Int]` to resolve to the `SortedSet` implementation, even though the `Iterable` implementation is also legitimate.
&gt; We would want looking for a `Find[SortedSet[Int], Int]` to resolve to the `SortedSet` implementation, even though the `Iterable` implementation is also legitimate. Well, if you look for a `Find[SortedSet[Int], Int]` the `Iterable` instance will _not_ be valid, as it will have type `Find[Iterable[A], A]`, which is not compatible. Now think about giving `Find` its legitimate variance for the collection argument, that is: `Find[-CC, A]`. Now, you have `Find[Iterable[Int], Int] &lt;: Find[SortedSet[Int], Int]`, so **in fact, you really want the least subtype-specific version** here (it's `Find[SortedSet[Int], Int]`), which is exactly my point. To restate the problem more generally: consider traditional subtype-sensitive overloading (e.g., picking `def find(xs: SortedSet[Int])` over `def find(xs: Iterable[Int])`). Encoding that with the magnet pattern does not work with the current implicit resolution rules, and hints towards the more useful rules being to always use the least specific implicit.
Your workflow would be easier if you set this up as an SBT project and didn't use the Scala compiler directly.
&gt; So I don't know to what extent this actually works, but what I was thinking of was &gt; implicitly[Functor[Writer[Int, Option[?]]]] // derive the applicative instance without error &gt; implicitly[Functor[Writer[Int, Const[Octonion, ?]]]] //derive the functor instance without error Why do you want the `Applicative` instance to be resolved in the first case? You asked for a `Functor`, so it would be fine to get a functor. When you ask for an `Applicative`, you will of course get an `Applicative`, regardless of the specificity rules in use. &gt; there's no reason to assume that the two functor instances are coherent with each other; if they are, we'd expect to have a `Monad[F] with Traverse[F]` But as you said, this does not scale in practice unless you have a closed-world assumption, which is obviously too limiting. I think just observing the convention is sufficient here, similar to providing instances which respect the laws, which is also unchecked. The nice thing with `[F[_]:Monad:Traverse:Functor]` and the least-specific rule is that it puts the burden on users of likely-non-consistent classes: indeed, at the call site, either: * you have an unambiguous instance of `Functor[F]`, as well as `Monad[F]` and `Traverse[F]` ones which are likely to be consistent with it by convention, as you said (note that this include the case where all instances are the same object, of type `Functor[F] with Monad[F] with Traverse[F]`); * you don't have an unambiguous `Functor[F]` instance, in which case you have no reason to assume that your `Monad[F]` and `Traverse[F]` instances are consistent: here it is better to ask the user _at the call site_ to specify which functor instance should be used, or to implement their own `Functor[F] with Monad[F] with Traverse[F]` where they can make sure things are consistent. 
We use scala to input and model streaming sensor data from our customers around the world. We chose scala because of its excellent concurrency and streaming libraries, like fs2. Scala scales very well vertically (on a single machine) and horizontally (load balanced across a cluster). We have two streaming consumer applications. One was written in scala. The other was written in Java, both are deployed on identical aws instances. Both receive the same data. The java service never uses more than 40% of the cpu, as it streams more or less serially. The scala service uses up to 100% of the cpu and memory for the boxes without swapping or crashing. We measure output in epochs (sensor ticks) per second. To get the same throughput in the java service as the scala service, we need 4x more java servers in the cluster. The kicker is that the scala service also does aggregation and transformations and compression on the sensor data, while the java service merely routes the data to different sinks based on some metadata we wrap around the sensor payload. Typically, we'll use 4 scala servers to 40 java servers. That's a huge cost savings in production. Now, the java service could be rewritten to be more concurrent, but idiomatic java concurrency is much more difficult to understand than scala streams. 
What about the official cats documentation on free monad? https://typelevel.org/cats/datatypes/freemonad.html If it doesn't compile it sounds like a bug :)
I'm not sure if there was a lint with wartremover or wtv was the other one that checked if strings seemed suspicious like that. Check it. I've also been bitten by that quite a few times. But then there's also a bunch of false positives where I really want it like that.
I think -XLint might include something like that. I know I got a warning before, but not sure if it was a compiler plug-in or the compiler itself. 
Take a look here: https://youtu.be/IhVdU4Xiz2U https://youtu.be/ycrpJrcWMp4
Section 4-2 at https://github.com/jcouyang/scala-dojo
Have you looked at the [FAQ](https://typelevel.org/cats/faq.html#example-compile)? Are you using the kind projector or partial unification plugin?
IIRC There's a flag on the ide level that can auto prepend the s for you once you introduce interpolation with a dollar sign. 
which IDE? I don't use IntelliJ. Only VSCode with Metal. 
I wrote a simple a simple Free Monads blog post a while ago that still compiles with the latest cats (just tested it) [https://medium.com/@agaro1121/free-monad-vs-tagless-final-623f92313eac](https://medium.com/@agaro1121/free-monad-vs-tagless-final-623f92313eac) &amp;#x200B; Feel free to have a look there Just make sure to include: scalacOptions ++= Seq( "-Ypartial-unification" )
To add to the video comments: I personally really like Daniel Spiewaks talk on the Free Monad where he rolls his own from scratch. It's much more understandable because it doesn't have any of the optimizations etc. https://www.youtube.com/watch?v=aKUQUIHRGec
Hi. I have made a learn-by-doing course https://github.com/dehun/learn-fp which asks you to implement a free monad. It does not relay on cats though. Also as part of articles series I have covered free monad in http://dehun.space/articles/03_dec_2018-Better%20than%20IO,%20part%203.html http://dehun.space/articles/17_dec_2018-Better%20than%20IO,%20part%204.html Code in that articles uses cats `Free`, however it war written very recently, so it should compile :) Hope you will find it useful. 
I'm certain I've seen warning messages for this before (possible missing interpolater)
I think after scale is Haskell of I've followed this sub right
I would strongly recommend putting the book into practice as far as possible before going on to more books. Of course there are plenty more techniques to learn in Scala, but IME it's much easier to learn and appreciate those techniques when you have a foundation of real-world experience and understand the problems they solve. Your current Scala code may not be the best possible, but it'll let you check your current understanding.
Read the blue book from Sam halliday
+1 on this. In addition I'd recommend contributing to open source projects as well or even roll your own library. You'll learn a lot.
What is meant by colors?
And when should be the moment to adventure to the next level?
I‚Äôd recommend finding small to midsize projects on GitHub, and learn to READ them first. I believe once you can read and analyze a project, you‚Äôll be able to write one yourself.
When you've put the previous level into practice and found it to be useful.
https://leanpub.com/fpmortals From the FAQ: &gt; Q: How does it compare to the red book? &gt; &gt; A: This book is blue.
Found it. Thanks everyone. https://stackoverflow.com/questions/39401213/disable-false-warning-possible-missing-interpolator
Get yourself started on koans. Do them periodically!
I gave a talk on, amongst other things, using cats‚Äô Free implementation to model side effects. You can watch the talk here: https://skillsmatter.com/skillscasts/13003-type-driven-development-in-practice-cats-and-akka-http. You can see the code here: https://github.com/mattroberts297/tdd-talk-code. It compiles and the tests pass. I‚Äôm also writing up the Q&amp;A we had after the talk at our stand (we ran out of time in the room). I‚Äôll post that and the code for it on Medium soon. Hope that helps!
+1 from me too. This might just be me, but I learn much faster when I'm actually using the thing rather than just reading about it. That said, I find that [http://twitter.github.io/effectivescala/](http://twitter.github.io/effectivescala/) is a useful resource when putting Scala into practice. 
Sort of. Reading other people's code definitely helps. Re-writing an app from scratch or porting it from a different language is where you really learn to write software though.
Time to find a project. +1
site is down: \`e.org.organization.isDemo\` Uncaught TypeError: Cannot read property 'isDemo' of undefined Probably because [https://leapfin.com/org-data](https://leapfin.com/org-data) requires auth.
https://underscore.io/books/scala-with-cats/
The [Scala Moocs on Coursera](https://www.coursera.org/specializations/scala) are fun. You can probably skip the first two courses if you've read the book. If you'd like to try the language out in a project, you can do the Capstone project
&gt; What is meant by colors? Back in the olden days of computing, really popular reference books were referred to in shorthand by their cover. Now everyone wants to get into the game.
The wizard book and the dragon book are my personal favorites.
Functional and Reactive Domain Modeling by Ghosh, it's a great next step.
I think in my compilers or complexity course there were books known by colors. But I assumed it was just those books. I am not a fan of usurping their Identity - I guess I'm in the same boat as you. 
Learn a web framework - l would suggest Play and dive into programming. You don‚Äôt need more books for the time being. 
SwissBorg | Software Engineer | Lausanne, CH or Toronto, CA | Onsite | Full Time SwissBorg is a fintech company based in Lausanne, Switzerland and is building the new era of wealth management on the blockchain. Developed by a team of financial experts, we are decentralized to the world with teams in Toronto, Tokyo and London and operate as a meritocracy. We recently completed our successful ICO and are now following our community-driven roadmap, rapidly expanding and growing our workforce. We are offering you the opportunity to join our team as Financial Scala engineer: if you like fast-paced environments, agile thinking and flexible work policy, this is your chance to apply! Job Posting: [Lausanne](https://jobs.lever.co/swissborg/45611f41-fe61-478f-9d29-f276b8ce0cc2), [Toronto](https://jobs.lever.co/swissborg/8093795c-c435-4af4-9176-ddef67db8387) About: https://swissborg.com
Okay, nice advice. Thank you!
As a baseline rule I prefer to avoid working with raw actors, \`Any\`, and implicits with non-semantic types. Pretty much any actor could be passed in there and who knows if that actor even replies back to its sender. I also like to forward information I compute, like the authorization response, and let something downstream decide whether to ignore it. I'm personally not a fan of booleans so rather than a valid flag I'd have a token that represents a known-good authentication and an ADT for the various reasons authorization can fail. So I might start with something like trait Authorization { def authorize(resource: ResourceId, authentication: AuthenticationToken): Future[Either[AuthorizationFailure, AuthorizationToken]] } object AuthorizationDirectives { def authorize(implicit authorization: Authorization): Directive1[AuthorizationToken] = { val resource: ResourceId = ??? // extract from request or add as parameters to this method val authentication: AuthenticationToken = ??? // extract from request or add as parameters to this method onComplete(authorization.authorize(resource, authentication)) { case Success(Right(token))=&gt; provide(token) case Success(Left(err)) =&gt; reject(AuthorizationFailedRejection) case Failure(t) =&gt; complete(StatusCodes.InternalServerError -&gt; "unable to authorize") } } } From there you can build up more robust error handling. The implementation of the Authorization trait could involve actors, or not; it's up to you. And it gives you a nice way to control the authorization during tests by, e.g., passing in ones that always succeed or always fail in certain ways. Also check out the built-in security directives - they might be directly usable or give you some more insight into how you want to design these routes.
thanks very much for the detailed response. 
I would not recommend pants. Go with bazel instead. We use pants, but not happy with it due to slowness, bugs in caching code &amp; overall quality, size of the community and support for other languages. Bazel can build all languages that we use and is much better quality tool + great momentum &amp; community. 
TrueAccord | Backend Engineer | San Jose or San Francisco, CA, US | Full Time The technology stack you will be working with includes Scala, Akka, Play! Spark and Docker. This a great opportunity to join an engineering team of a successful fast-growing start-up that is transforming the industry, while expanding your Scala and Functional Programming. ### Responsibilities: * Build highly scalable distributed services that can process millions of events daily as our company on-boards the nation‚Äôs largest banks. * Design and implement new components and features in our platform. * Write high-quality unit tested code that will run in our continuous delivery pipeline. * Own workstreams end to end ### Requirements: * 2+ years of relevant experience * Proficiency with Scala or Java * Insatiable desire for continuous learning and continuous improvement. * Great interpersonal skill and enjoy working with members of other teams. ### Benefits &amp; Perks: * Work with talented and motivated people in a fast paced environment. * Medical/dental/vision insurance, 401k (with match), flex spending plan, and life insurance * Meaningful equity package * **Unlimited PTO** * Transportation benefits * Team lunches and weekly happy hour &amp;#x200B; Positions available in San Jose and San Francisco. PM me here or email at [mtolosa@trueaccord.com](mailto:mtolosa@trueaccord.com) 
TrueAccord | Backend Engineer Back-End | San Jose or San Francisco, CA, US | Full Time The technology stack you will be working with includes Scala, Akka, Play! Spark and Docker. This a great opportunity to join an engineering team of a successful fast-growing start-up that is transforming the industry, while expanding your Scala and Functional Programming. # Responsibilities: * Build highly scalable distributed services that can process millions of events daily as our company on-boards the nation‚Äôs largest banks. * Design and implement new components and features in our platform. * Write high-quality unit tested code that will run in our continuous delivery pipeline. * Own workstreams end to end # Requirements: * 2+ years of relevant experience * Proficiency with Scala or Java * Insatiable desire for continuous learning and continuous improvement. * Great interpersonal skill and enjoy working with members of other teams. # Benefits &amp; Perks: * Work with talented and motivated people in a fast paced environment. * Medical/dental/vision insurance, 401k (with match), flex spending plan, and life insurance * Meaningful equity package * **Unlimited PTO** * Transportation benefits * Team lunches and weekly happy hour &amp;#x200B; Positions available in San Jose and San Francisco. PM me here or email at [mtolosa@trueaccord.com](mailto:mtolosa@trueaccord.com)
Great post on a tricky subject, nice work!
I think it's called bivariance. Here is a hack how to get a bivariant type in Dotty: type M = { type T[+A]; type Ev &gt;: T[Any] &lt;: T[Nothing] } val M: M = ().asInstanceOf[M] You can upcast it easily: def ucast(m: M.T[Int]): M.T[Any] = m The downcasting requires a little help, though: def dcast(m: M.T[Any]): M.T[Int] = m: M.Ev But the typing relation is there by transitivity: we have `M.T[Any] &lt;: M.Ev &lt;: M.T[Int]`. The fun part is that this actually crashes Dotty with "Recursion limit exceeded."
I like that elm has a built in virtual DOM, I believe similar to if scalatags had one so the rendering is pretty fast. Can more knowledgeable people comment on how they see scala.js vs elm matchup?
There is that and a good many more incredibly useful warnings built directly into the 2.12 compiler series; check out https://tpolecat.github.io/2017/04/25/scalac-flags.html for details.
This week I've been working on a set of cucumber features to automate the preproduction tests and a bit of improvements in the automatic pipeline. 
[https://github.com/danilomo/KleinLisp](https://github.com/danilomo/KleinLisp) [https://github.com/danilomo/KleinLisp-Scala](https://github.com/danilomo/KleinLisp-Scala) &amp;#x200B; I'm trying to write a Scheme interpreter for Java, and I wrote neat Scala bindings. &amp;#x200B; Java 8 has Optional and shit, but the lack of pattern matching and the for/yield prevents me to writing concise code. In Scala I can easily move from the dynamically typed world from Scheme to the statically typed world of Scala.
I wonder if [Dynamic](https://www.scala-lang.org/api/current/scala/Dynamic.html) would be useful for you. I've never used it.
I created a Java/Scala translation cheat sheet. I could use some help with formatting or other feedback. &amp;#x200B; [https://github.com/shawjef3/JavaScalaCheatSheet](https://github.com/shawjef3/JavaScalaCheatSheet)
Hello! I wonder if anybody can suggest a path to learn scala+akka+kafka. Are there any small project ideas, which can be written in that stack locally on my machine (w/o using cluster or highload). I know some scala and some akka, but I simply can not come up with any projects that would use these technologies showing their potential. Thanks!
That's pretty cool! I had no idea that such thing existed in Scala. I wonder if it will be helpful, but it will be nice keep this feature in mind. What I had in mind with this wrapper was being able to read Scheme S-expressions and atoms ( dynamic typed variables ) from a REPL or a script file, as Scala typed variables, without having to check its type and doing a manual conversion (as one would do in Kawa, the state-of-the-art Scheme interpreter for Java). I want to do this in a declarative way: &amp;#x200B; &gt;**val** lisp = **new** Lisp **val** exp: LispObj = lisp.evaluate("(list 1 2.0 \\"something\\")") &gt; &gt;**val** t: Option\[(Int, Double, String)\] = exp **match** { **case** *ListObj*(List(*IntObj*(i), *DoubleObj*(d), *StringObj*(s))) =&gt; *Some*((i,d,s)) **case** \_ =&gt; None } I'm still thinking on how to make this less cumbersome, but, as it is now, I think it is declarative and "typesafe" enough. &amp;#x200B;
That looks quite usable. Now that I take a closer look, you are relying on a library for the runtime. I expect that's where Dynamic would be useful.
Scalaz 7 is still quite good, so there's no real urgency to get 8 out the door. New major releases of Scalaz have always been driven by the need to get a major reorganization out based on a new theory of the best trade-offs to make given Scala's constraints. 8 is such a reorganization with such a theory, but much of that theory is quite new, so it's still likely that someone will think of an important refinement that should really be applied before releasing 8. The downside for you is that there's no "timeline", because milestones aren't really a driving force here, whatever status GH issues may show. The upside for you is that there's no chance of the "overly ambitious in its goals" problem that you fear, because the only real goal is to have a significantly better organization than 7. If we find it and apply it, then that's fine; if it is truly too ambitious, then 7's design is already a local optimum.
In terms of the type-theoretical features of Scalaz, I see what you mean. But I wonder specifically about the Scalaz 8 IO framework. Is it true that Scalaz 7 already has many of these IO capabilities?
You are free to use scalaz ZIO, zio-streams and related projects without scalaz 8. They are completely orthogonal, in fact they have great integrations with cats &amp; cats-effect.
Thanks, I'll definitely give them a try!
Haskell's monad is going on 30 years strong. It's still great! True abstractions exist, and don't decay very quickly, which is why Scalaz hasn't really needed to have so much work done. 7.3 is basically finished - we could probably push that one out and call it good for the next few years. We're still playing with 8 and trying to figure out a best course of action. If we were to pick up the pace, it be a radical departure from the current Scalaz, so it's still being discussed. Activity has also died down due to many of the core 8 maintainership leaving for Haskell. Scala is experiencing a significant brain drain now that the Dotty schism is looming closer, and Haskell positions are now moderately easy to find (with salaries to match!).
Interesting perspective, I had no idea that Dotty was so controversial or that Haskell was drawing away Scala talent. Could this mean a shift away from pure FP Scala in the future?
I have seen very little evidence of what this person is talking about re: Dotty. Migration paths will be put in place, and there is a common subset of Scala 2.x and 3.x which is source-compatible (as a last resort). Based on (effectively) saying that Haskell and Scala are somehow opposed, I think they're might be trying to rile people up. While there's certainly some overlap in the two communities (hi!), I don't think it's necessarily a zero-sum game. That is, you might have a reasonable case for using Scala at at primarily-JVM shop, but Haskell might obviously be a much harder sell based purely on business-level concerns.
&gt; Haskell might obviously be a much harder sell based purely on business-level concerns Or on being an inferior language.
Them be fighting words, haha
&gt; now that the Dotty schism is looming closer To explain the down-votes: I think this sounds like pure fear-mongering. There is no evidence of such a schism coming, and avoiding one has been addressed over and over again since Dotty's inception. I may be misled by the lossy nature of online text-based communication, but you almost read like you would _enjoy_ seeing dismay in the Scala community, and are looking forward to it.
&gt; Scala is experiencing a significant brain drain now that the Dotty schism is looming closer Not the kind of claim I'm happy to see tossed around here casually (and, for what it's worth, not one I think is true). There's a place for constructive criticism, but if you're coming here to be negative about Scala you should bring enough evidence and detail to be actionable.
Scala != Scalaz Many people use Scala for Apache Spark only and they don't care about ridiculous wars between strongly-opinionated FP-evangelists. Many Scala-shops depend mostly on Akka ecosystem, with a moderate usage of basic Cats/Scalaz functionality. Hardcore FP community can still use Scalaz 7 and its which is mature and doesn't require much maintenance. If they prefer, they might go with more-active Cats development. Some people slowly (but still) works on Scalaz 8. Interpreting the fact that \_you and your friends\_ moved to Haskell, as the fact that Scala as a whole dies, is at best misunderstanding, and at worst FUD and megalomania. For \_me and my friends\_ Scala is getting better and better: language improvements on their way, tooling improvements on their way, more libraries, more documentation and learning materials, etc.
Hello! I wonder if anybody can suggest a path to learn scala+akka+kafka. Are there any small project ideas, which can be written in that stack locally on my machine (w/o using cluster or highload). I know some scala and some akka, but I simply can not come up with any projects that would use these technologies showing their potential. Thanks!
Is there anything like the `@tailrec` annotation to enforce "no side effects" in methods?
I am the second top contributor to scalaz 7.3 over the last 12 months https://github.com/scalaz/scalaz/graphs/contributors?from=2018-02-23&amp;to=2019-01-14&amp;type=c Likewise for 7.2. I left Scala for Haskell, primarily because of the Dotty schism. https://medium.com/@fommil/scala-3-considered-as-a-new-programming-language-a335ff67e075 I'm expecting, and waiting for, the downvotes to this purely factual comment.
&gt;I may be misled by the lossy nature of online text-based communication, but you almost read like you would enjoy seeing dismay in the Scala community, and are looking forward to it. I give a significant amount of free time to helping people in detail on this subreddit, and I'll be speaking at 2 Scala conferences this year. You tell me.
Way to miss the point of my comment.
You asked the right question that was thinking of asking from long time.
In case people are reading this: here's the Scala FUD guide: https://kubuszok.com/2018/scala-fud-faq-for-newbies/ It addresses the so-called 'Dotty schism' as well.
Are you looking for an application that uses no networking at all or would you be fine using a minimal docker compose set up to simulate multiple machines communicating with one another?
&gt; helping people in detail on this subreddit That's great, but why taint many of your helpful comments with unnecessary negativity? &gt; I'll be speaking at 3 Scala conferences this year. You tell me. This does not change the impressions you leave with your comments, especially to outsiders. Now, I don't know you and I may very well be mistaken about you. I was just giving you my opinion as to why you were being downvoted.
Not op, but have the same issue and don't mind a bit of infra setup
&gt; I am the second top contributor to scalaz 7.3 over the last 12 months One contributor leaving one project does not constitute a "scala brain drain" (particularly as said project becomes increasingly less important to the scala ecosystem). &gt; this purely factual comment that was requested by a moderator That was not a request to reply with substantiation when called out by a mod. It was a warning to refrain from making unsubstantiated negative comments in the first place. &gt; I suggest that you retract the warning, as I consider this to be a moderation error. I have at most limited interest in moderation advice from those who contribute positively to the community; none from someone who purports to have left it.
I haven't looked lately but is Scala native dead?
John DeGoes also makes public statements that Scala 3 "won't save Scala", "Is an entirely different language", and "will split the community". It is clear that they have an agenda, and as a result, I view everything they say with suspicion.
I love how all these people have abandoned Scala, yet keep coming back here to make comments. As the song says, "How can I miss you, if you won't go away?"
I don't know her either, but have reacted the same way...
Well, I would consider any variant that uses these technologies
It's always fun to ask Hoogle to match the haskell library against a type signature and see what it comes up with, there will often be a similar method in Scala's libraries somewhere. https://www.haskell.org/hoogle/?hoogle=%28a+-%3E+b%29+-%3E+a+-%3E+%28a%2C+b%29 But this specific pattern is too "short" to have its own name, I think. 
The most idiomatic way is to use the `map` function like you did. If it is so common in your code that you want a short-hand, define an extension method
I comment when I have something to say, especially if it is to add facts to a conversation that has digressed into tone policing a contributor. I am unaware of having ever "abandoned" Scala: I was unaware that one must hold down a Scala dayjob as a prerequisite to post in this subreddit.
Actually, since there's already a network up and running, how about a bittorrent client? No docker-compose needed.
 import java.util.concurrent.atomic.AtomicInteger def incrementPairs(ints: Seq[Int]): Seq[(Int, Int)] = { // Don't duplicate work! Memoize the increments. val memos = new java.util.concurrent.ConcurrentHashMap[Int, Int]() // prepare the final results val stagedResults = Array.fill(ints.size)((new AtomicInteger, new AtomicInteger)) // set the results, again using all our cores for (i &lt;- ints.indices.par) { val (result, incrementedResult) = stagedResults(i) val value = ints(i) result.set(value) incrementedResult.set(memos.computeIfAbsent(value, _ + 1)) } // unwrap the results for ((value, incrementedValue) &lt;- stagedResults) yield { (value.get, incrementedValue.get) } }
At the cost of a factor 2, `xs.zip(xs map f)`
It seems to be `listens` on `MonadWriter`
No, alas
Does that mean this is a bug? Or is it a feature?
&gt; why taint many of your helpful comments with unnecessary negativity You know what some people will say. Stuff like "if you take my comments as negative, that's on you" or "some people can't handle the truth". Some people also lack empathy, so...
&gt; I am also unaware that one must hold down a Scala dayjob as a prerequisite to comment in this subreddit. Who made this claim?
Be prepared to rename it. Anything with a sexual double-entendre will attract claims of discrimination and harassment. It can most definitely be career-ending.
Any additional info to look up into to get started with bittorrent client in scala? Are there any newbie projects or helping literature to start with?
I would rename it stat.
Is it complicated to setup a Scala+Java project in a docker container? I will be working on a group project where my team-members will likely be using Java but I would prefer to do some of my bits in Scala. Simple resources for setting up a project like this are appreciated.
Come on, nobody cares unless it gains traction. If it does gain traction, well, there's no such thing as bad PR. Sad to see people blowing on water. 
You could use for comprehension: val x = for { i &lt;- List(1,2,3) } yield (i, i+1)
&gt; to be negative about Scala you should bring enough evidence and detail to be actionable Detailed, constructive criticism of Scala has been explicitly prevented. The "brain drain" (not the words I'd use, but still) is an immediate consequence. Most people who have honestly been down the Scala rabbit hole, have since moved on. Stating this fact is also "FUD", I assume. A biased sampling doesn't change the fact that it is true.
I would start with [the official docs](http://www.bittorrent.org/beps/bep_0000.html). I also found [an existing Scala client](https://github.com/TheDom/scala-torrent) that uses Akka you could use as a reference.
Pure FP, at least its practical advantages, is not possible in Scala.
That's basically it. It doesn't get more idiomatic than that.
List(1, 2, 3).foreach(t =&gt; println(s"og: $t, new: ${t + 1}")) This skips the intermediate map and tuple creations.
You can define a couple simple implicit classes and typeclasses in order to be able to write something like: ``` val x = List(1,2,3).map(_.squared(_.map(_ + 1))) ``` where squared turns any `A` into an `(A, A)` Which might be better depending on your actual use case. Scalaz has this stuff already. It's not in cats. But if this is the only such thing you've identified you need you can just implement it as a package in your project. Happy to help if you're interested. 
&gt; That's great, but why taint many of your helpful comments with unnecessary negativity? I'm honestly surprised that the comment received as much of a reaction as it did, since it was stated very mildly and without negativity in my mind. It simply is what it is. People are leaving. Many Scalaz people have left, and many Cats people are leaving as well (I had a core contributor's resume come across my desk just today!). If giving an honest feedback relevant to the question asked is negative, then so be it. 
I was not coming here to be negative, Icame here to answer a question, which I did. You sufficiently derailed this thread with your tone policing and non-constructive criticism. 
I like this one because it's concise
Well, `pure` is the easy one. That's how you get stuff 'into' the monad. For an `Option`, `pure(3)` gives you `Some(3)`. For an `Either`, `pure(3)` gives you `Right(3)`. For a `List`, `pure(3)` gives you `List(3)`. For these particular monads, you probably would just use their normal, more concrete constructors, but for other monads, like when working with an effect you haven't specified yet, `pure` comes in handy. See the [IO monad](https://typelevel.org/cats-effect/datatypes/io.html) documentation for an example of where `pure` is more frequently used. For `ap`, you are sort of looking at it backwards. It's *because* you can implement `ap` in terms of `flatMap` that makes all monads also applicatives.
Thank you for understanding.
As much as I like scala, I'd like it to continue to be a community that allows people to voice out their opinion/criticisms. This comment could be one user's genuine opinion about scala. I'd like to listen to them and decide for myself whether I want to believe them or not. If someone doesn't agree with their comment, they can engage in a discussion in this public forum. Why assume negative intention and limit people's opinion to a smaller range? 
This was pretty good. I like the examples given and felt like I could follow along easy enough.
Since Akka Stream evaluates lazily, there is no performance tradeoff at all
The only name I've seen given to this operation is fproduct found in cats on Functor. Don't think there are Akka instances of those type classes though. 
This isn't going to be idiomatic either way. Here's a hint to get you headed in a different direction: trait ShoppingCart { ... } object ShoppingCart { def addItemsToBasket(quantity: Int, product: Product): ShoppingCart = ??? ... }
&gt;documentation I understand pure, but I'm trying to understand when you would use \`ap\` in the real world. It seems \`ap\` is accepting a function as a wrapped type, i.e it can deal with applying \`Option\[String=&gt;Int\]\` with \`Option\[String\]\`, but I don't see any methods on the \`Option\` monad that expose that to a user. Are we just saying that \`flatMap\` can be implemented with \`ap\` by unwrapping the function for you, is there any real world cases where we would call just \`ap\` on its own?
No-one made that claim. fommil's post is misleading to the point where I am no longer willing to assume good faith.
Unsubstantiated negative statements are indeed FUD; this kind of post is exactly what I *just* warned about, and this is not the first time you've come to my attention.
Most monads are not containers, and assuming they are is a common source of confusion. If you want to understand monads in general you need to work with at least a couple of non-container examples - `Future` and `Reader` might be good simple ones to start with.
&gt; Is it complicated to setup a Scala+Java project in a docker container? In principle no. You might find less support/integration because the JVM already accomplishes most of what Docker offers, so use of docker is less widespread than in other stacks. &gt; I will be working on a group project where my team-members will likely be using Java but I would prefer to do some of my bits in Scala. At deployment time Scala behaves exactly the same as Java (both will just be jvm bytecode in a `.jar` file).
No. You can use WartRemover to rule out some common sources (e.g. `throw`).
Honestly I don't know if they intend to fix that, in light of [this issue](https://github.com/lampepfl/dotty/issues/4805). But I opened [an issue](https://github.com/lampepfl/dotty/issues/5700) anyway just in case.
&gt; I'm mainly having trouble seeing how the normal use of these monads would require use of the ap function as an initial flatMap on the ff would allow you to achieve the same thing? It's the other way around: applicative is a "weaker" interfaces that lets you do less than monad (and functor is an even weaker interface than that). If a type does form a monad then you can implement `ap` in terms of `flatMap`. But some types are applicative but not monads; the most common example is [`Validated`](https://typelevel.org/cats/datatypes/validated.html), which is like `Either` but with the left hand side being a semigroup (think: a "mergeable" type - e.g. a list) and it accumulates all the errors rather than stopping at the first error. Imagine validating a "create user" action where you check if the username is valid and also check if the email address is valid - if you did that using `Either` then someone who submitted an invalid username *and* an invalid email would only get a message about the invalid username, whereas we want them to see all the validation errors at once. I was about to go into an explanation that would've been much the same as the one on that page; honestly just read that documentation for Validated, it says everything I was going to say. I'd recommend understanding applicative in terms of `map2` rather than `ap` - they're equivalent but `map2` is a lot more scala-idiomatic (`ap` makes sense in a currying-only language like Haskell). Hopefully it's clear that `map2` is a sort of middle ground: it's more powerful/flexible than `map`, but more restricted than `flatMap`, since the `fa` and `fb` you're combining have to be "independent" of each other, whereas in `flatMap` the second argument's effect can depend on the first.
Returning `Unit` is "bad" *because* using `ListBuffer`-like types is "bad" (that is to say, it can lead to confusing code, precisely *because* the value of `basket` changes) - the whole point of not returning `Unit` is to write in a style like the first where `basket` never changes and you always know whether you're using `basket` or `basket2`. Note that you don't have to create `newBasket` just to return it; you can rewrite that whole function as: def addItemsToBasket(basket: Seq[Product], quantity: Int, product: Product): Seq[Product] = basket ++ List.fill(quantity)(product) (we don't need any braces either now that the function is just one expression).
THB, I also do not know you, and reading your Reddit comments and Tweets I feel hostility and disdain for Scala community. If that is not your intention, then, I don't know, start adding emoticons or something, since every other post feel like an attack. (In real world you can use facial expressions and voice to indicate you intentions, but in text you just see words and if like 60% of it is about someone/something being wrong - without anything to soften the blow - then it just looks like an attack).
There are some types that are Applicatives but not Monads. So if you can write your code by only using pure and ap, without flatMap, then you often should, since it will work for more types. There are not, however, any types that are Monads but not Applicatives, since you can implement ap in terms of flatMap.
I don't see any difference between List or Task regarding being a container. It's the value extraction effect that matters. For List there could be no value at all. For Task you'll have to wait and possibly get an error after all. Monad being a container is a good intuition.
&gt; It's the value extraction effect that matters. There is not necessarily any "value extraction effect". There is not necessarily any "value" at all. A monad only has to compose in a particular way; there is not necessarily any way to "get the value out" (e.g. this is the design intention of `IO`). &gt; Monad being a container is a good intuition. Disagree. It causes confusion when working with monads that aren't containers. E.g. if someone tries to "get the values out" of a `State`. If you're writing code that only works with containers, use a container interface. You should use the monad interface when you need to generalise beyond containers.
I can get value out of State by providing a state to run with. Of course you can not extract a value from generic F : Monad, and this abstraction just gives you pure and bind - I agree with that. 
&gt; I can get value out of State by providing a state to run with. But that value wasn't "in there" - if you supply a different State you might get a different value out, so if you think of it as a "collection" it's easy to get very confused (and this is even more true for e.g. a database-access monad).
In short, stack safety.
Since his talk, dotty has made many changes that was suggested in the talk (not saying it was because of him) and he rolled back the relevant parts of his statements as a result. &amp;#x200B; Nothing about him (he made an attempt to encourage and manage hundreds of new scala open source developers) or emilypii has ever struck me as disingenuous but people in the community seem to insist on attacking the messengers instead of the message. &amp;#x200B; So my question is what exactly could you be suspicious of? What do you think this agenda is?
I think the creator of specs has also left the community, there is another prominent ( I think) contributor to cats and Scalaz I have recently seen looking for Haskell jobs. I wont name them so as to not foist unwarranted drama on them. &amp;#x200B; I don't agree 100% with Emilypii statement, I think it would be more accurate to say the Scala FP community specifically and 'significant' seems subjective but for such an innocuous, in context, statement to bring out the mods seems more than a little over the top. &amp;#x200B; * I have not abandoned Scala, I like it, its how I feed my family. The community basically bullying people who critique the language is currently the communities biggest embarrassment imo.
The problem is that function composition isn't stack safe in general. 
I'm tired of seeing unsubstantiated, casual negativity from the same handful of users, and I'm not the only one. It makes an unwelcoming environment for users who come seeking help. Ultimately this subreddit is for Scala and comments should be in the context of that. Criticism that helps make Scala better is ok, explaining downsides of Scala is ok; just trashing the language is not, particularly in a comment that's also promoting another language. If you find something valuable in Scala and want to help others with it you are welcome here, whether or not it's your favourite/main language. But if you dislike Scala to the extent that all you're posting is attacks and calls to adopt some other language, this is not the place for you, just as I wouldn't expect any other language community to tolerate people trashing their language and promoting Scala.
The metaphor of monad as a container has its limits, but it is a good "introductory-level approximation" - once one kicks off, use monads for a while, and internalize it, they will be ready to expand their horizon to monads which doesn't play well with a container metaphor. Trying to give someone a 360 view on things before they get a hold of any intuition more often will end up with them not understanding anything and being overwhelmed rather than making them understand everything altogether. TL;DR - I agree that container is not 100% accurate way explaining things, but I am convinced that newbies can learn faster by starting with some simplified mental models.
What /u/kbielefe said means that you can implement \`ap\` using \`flatMap\` not the other way round. [https://github.com/typelevel/cats/blob/774fb51d1365e1adef7fa71f09b1410941264f60/core/src/main/scala/cats/FlatMap.scala#L86](https://github.com/typelevel/cats/blob/774fb51d1365e1adef7fa71f09b1410941264f60/core/src/main/scala/cats/FlatMap.scala#L86): def ap[A, B](ff: F[A =&gt; B])(fa: F[A]): F[B] = flatMap(ff)(f =&gt; map(fa)(f)) // map is from Functor
The sentence you decided to moderate on was perfectly in context with the helpful answer to the ops original question. You quoted it in isolation for your own reasons and decided it was of the negative bent. I read it as stating a fact, Scalaz8 progress has stalled because the people working on it no longer write a lot of, if any, Scala. They chose to stop contributing because, amongst other things, of their views on the direction the language is going. This is not a negative statement. If, for example, I had of posted the same thing I very much doubt it would have elicited a response from you but these prominent Scala critics seem to get chased around the Scala web and repeatedly bullied out of participation. Everyone needs to grow up. IMHO. It is not a reddit moderators job to keep the entire context of a persons internet history in their head and use it as a filter to read each and every post of theirs yet that is what you, and others, are doing.
&gt; The sentence you decided to moderate on was perfectly in context with the helpful answer to the ops original question. You quoted it in isolation for your own reasons and decided it was of the negative bent. I read it as stating a fact, Scalaz8 progress has stalled because the people working on it no longer write a lot of, if any, Scala. They chose to stop contributing because, amongst other things, of their views on the direction the language is going. This is not a negative statement. Of course I have to exercise my own judgement about what a given piece of writing means; how could it be otherwise?(Though for what it's worth from the OP's reply it's clear that they read that part the same way I do, and got the same message I did). &gt; IMHO. It is not a reddit moderators job to keep the entire context of a persons internet history in their head and use it as a filter to read each and every post of theirs yet that is what you, and others, are doing. I literally don't know what you're talking about or what internet history emilypii has. I'm talking about particular users being consistently negative *on /r/scala*, which is absolutely my job to take into account when moderating here.
Many thanks!
Personally, I don't see where Sam was "just trashing the language", that's just your opinion Michael, but oh well... I'm clearly not surprised of your behavior, you have a long history of animosity towards people contributing on the Scalaz project (which seems sadly only based on that trait). &amp;#x200B; Not sure where that comes from, I guess this have to be an ego thing (I have some data points conforming in that direction, happy to give more details) but it's just plain sad in my opinion, I know that core developers of the dotty compiler are always open to hear criticize and discuss their works, it surprising that you are guarding them, they are grown up people. &amp;#x200B; It seems \`1r13h\` wrote most of the points I have in mind so no need to repeat them, but in all honesty the worst thing you can do towards "helps make Scala better" is the attitude you exactly have here. &amp;#x200B; That badge does not suits you well, Michael. &amp;#x200B;
My experience is that people who learnt containers first were consistently confused, and often never reached a good understanding of monads in general. I agree that it's important to teach simple examples first, but IMO users are better off learning a couple of simple/specific monads that are not very container-like - I mentioned `Future` or `Reader`; `Writer` is another good one, as is ["threat level"](http://blog.sigfpe.com/2007/04/homeland-security-threat-level-monad.html) - and then generalising to monads from there. Trying to understand the general concept when you only really have one example (because all containers behave very similarly in terms of their monadic behaviour) is a mistake in my book.
I'd much rather see a comparison about real-world capabilities and problems. For example with finch using anything other than JSON is a huge pain, but on the flip-side it integrates very well with finagle (an excellent choice for microservice architectures) and brings a lot of other goodies with it. These trade-offs are a lot more important imo, but I guess it's more difficult to write an article about
Well, I think the issue is, that many people don't have one teacher with some vision of how things could be taught. IMHO: * container is a good strategy for a first contact, but might be an obstacle later on, when State, IO, Reader etc comes into play. However, if all of these concepts were introduces by one teacher during one course, he could design the course to be done in a way that prepares students to switch their first mental model to something else, * showing people all different cases work amazingly... but only for some people. The rest simply gives up from being overloaded with information. I saw it several times during conferences: great speakers giving presentations, where over half the audience give up half the way. From my point of view is kind of a technical debt: if the company is just starting, you can do it the dirty way, gather debt, and deal with it once you get traction, or you might to try doing everything right from the start and risking getting out of business due to being slowed down by initial cost. You can show people simplified model and make 90% of them get it, but then deal with the fact that 70% of them will have difficulties to fix it, or teach them the fully detailed model and make 25% of them get it and 75% of them give up permanently. (Numbers pulled out of ass for the sake of illustrating what I mean.) I mostly agree with you, if you want to really make the student understand the concept, you should show him several examples, make him/her play around with them, and only when they get comfortable enough with the concepts to notice some common things on their own, name and formalize that abstraction. (I did exactly that in my own post about monads). But I don't think it works that well if you want to give some brief general idea and don't have that much time. Probably the easiest way would be to just give someone a few practical examples, let them play around with these `flatMap` and once they quite comfortable mention that it has some rules and a name.
Both sides in this argument need to cool down a bit. Fommil has made a lot of positive contributions to the Scala community. So have you. We shouldn‚Äôt be throwing barbs at each other like this, it‚Äôs not productive and it‚Äôs a really bad look. I‚Äôm not placing blame but just saying whenever we‚Äôre replying to something, we can always use that as an opportunity to de-escalate. It doesn‚Äôt have to go to the level of ‚ÄòABC is a troll, XYZ has bad faith‚Äô. We really, badly need to work on this culture of escalation.
As others have said, your code is idiomatic and basically fine. But if you do this sort of thing a lot, you could add an extension method. (You can mess around with implicits and make `mapTo` return the same collection type it operates on.) implicit class IterableOps[A, C[_] &lt;: Tra](t: Iterable[A]) extends AnyVal { def mapTo[B](f: A =&gt; B): Iterable[(A, B)] = t.map(a =&gt; (a, f(a))) } 
I honestly haven't had any issues using twirl, html, css, js etc with http4s. It also supports webjars. Extending the base features didn't seem like a pain to me, just implement an EntityEncoder and you're good to go. It's definitely a good choice imo, especially when you're using libraries from the cats ecosystem. Can't speak for finch, haven't tried it yet.
I'd argue the opposite. Talking about composability and extensibility subsumes the "real-world capabilities and problems" you seem to be referring to. Something that is less composable or less extensible is more likely to force its own potentially arbitrary opinions on how you interact with it, leading to things like "other than JSON is a huge pita" and if you prioritize first principles, you have a consistent compass for choosing tools that don't make you write non-transferable idiosyncratic code in order to utilize them.
The simplest way to put it is that flatMap represents sequential dependency. eg a &lt;- fa b &lt;- fb(a) You can't do step 2 until you've done step 1, because the result of step 1 is the input to step 2 By contrast, ap represents computational independence eg def tupleAB = (a: A) =&gt; (b: B) =&gt; (a, b) ap(ap(pure(tupleAB _))(fa))(fb) Note that because fb here has no dependence on fa, some things actually become more flexible. Order of evaluation becomes unimportant, so if your F is capable of concurrency or parallelism you can leverage it here. Halting is not necessary (as in the 1st example, if fa doesn't complete you cannot compute fb) so you can do things like accumulate failures if your F is capable of representing them. In some sense it parallels products vs sums. Linear dependence in evaluation means you have the sum result of line 1 OR line 2 OR line 3 as the evaluation moves forward (monad) where independence means you have the product result of every line together, fa AND fb (applicative)
I've just added [Context support](http://www.react4s.org/examples/contexts) to React4s. I've been resisting it for a long time because I kept hoping a better way to access multiple contexts from lifecycle methods would materialize in React.js.
It mentions at the bottom of the article a Part 2, but doesn't have a link to it... how do we access the additional parts of the lessons? Do we have to create an account?
Hey there! I haven't left the community :-) (specs2 author here). My day job is indeed fully Haskell now but I keep maintaining my Scala libraries and following the developments of other libraries or of the language (I even have the project to port a Haskell library to Scala). My impression is that some people, which were well known in the Scala community are using a lot less Scala these days (or not at all) and, like me, are using Haskell. But some others have gone to Rust, Go, Swift and many other ecosystems, because,... that's the life of a programmer! That doesn't mean that Scala is a sinking ship. On the contrary, libraries are flourishing, the language evolution is being actively discussed and it seems that the community is growing (if job offers is any indication). Now on a more personal note I have to say that I am truly delighted with Haskell. My style of programming has progressively evolved towards full functional programming and I find Haskell better suited at this than Scala. But, mind you, you can still write bad programs in Haskell with global mutable state if that's your thing! So writing good code is still an everyday battle. I also wish there were more Haskell jobs to allow more people to get a taste of fully embracing functional programming without the baggage of OO but that's a Haskell problem, not a Scala one :-). In my mind Scala fills a spot where people can transition from one programming style to another, at their pace, contrasting different approaches. This gradual transition has some benefits: you can always go back to what you know and still deliver a working system; and some drawbacks, for example the `StateT` transformer looks more complicated than it should: https://www.reddit.com/r/scala/comments/ag8f4j/why_both_scalaz_cats_define_statet_differently. So, at the end of the road, my advice would be, for people really liking functional programming, for aesthetic reasons or because they think they can produce more robust programs, to give a go at Haskell (or Purescript). Once you pass the usual initial hurdles: the build system, setting up a dev environment, knowing your way around libraries, etc... you will realize that many things are actually simpler than with Scala and the whole experience quite enjoyable. 
Here's [the PR for cats](https://github.com/typelevel/cats/pull/302) which goes into a bit of detail.
Another good blog post on this same subject is Stephen Compall's https://failex.blogspot.com/2016/09/the-missing-diamond-of-scala-variance.html
Thank you for correcting me and I apologize for misrepresenting you and dragging you into the thread. Twas not my intention. All the best.
found it [http://www.cs.potsdam.edu/faculty/laddbc/SCG/Review/](http://www.cs.potsdam.edu/faculty/laddbc/SCG/Review/)
I've written a lot of web services and I'd say on average, both in terms of development time and lines of code, routing was less than 5%. I'm glad everyone can write a composable routing dsl in scala, but it really has no bearing no how good an fp interface you have for any real world application.
+1 for finally seeing some actual moderation! This is how we can claim back this forum from the eternal forces of casual toxicity
The first way is better for a whole host of reasons, immutability is key. I would not do the List.fill inside of the function as I like to keep functions as singular purposed as possible. I would also use ShoppingCart to hold state instead of being a utility class. Here is how I would do it: case class ShoppingCart(basket: Seq[Product]){ val addItems = (items: Seq[Product]) =&gt; ShoppingCart(basket ++ items) } val cart = ShoppingCart(List.empty[Product]) val newProducts = List.fill(2)(Product(...)) val updatedCart = ShoppingCart.addItems(newProducts) &amp;#x200B;
I think some frameworks get in the way when you‚Äôre trying to use things like Cats IO, eg Akka HTTP.
Part 2 to the article hasn't been written yet but we can ask the author when he's likely to do so.
No need apologize, your post gave me the opportunity to clarify my position which I should maybe have done before.
amen
``` import cats.implicits._ List(1, 2, 3).fproduct(_ + 1) // List((1,2), (2,3), (3,4)) ```
Speaking personally, as a contributor to Scalaz 8, I switched gears to focus on [ZIO](https://github.com/scalaz/scalaz-zio). ZIO was formerly the Scalaz 8 effect system, but now it's a separate repository, and has no dependencies on Scalaz, Cats, or any other library, so you can use it with Scalaz 7.x, Scalaz 8, or Cats 1.x. ZIO, because it solves problems much closer to the business, has the potential to impact more programmers, which is why I'm spending more of my free time developing the library. Now, I'm still _very much_ excited about Scalaz 8, but there are some unresolved questions that require more research and development (in particular, around encoding type classes and the functor hierarchy). When I return to working on Scalaz 8 (hopefully in a couple months), I'll be obsessed over the following: - No macros or compiler plug-ins - Fully compatibility with Scala 3 - Maximally useful type inference - Cleanup of Haskell-isms that should not exist in Scala (like the dreaded \`ap\`) - Excellent user-experience, which is a combination of docs, type inference, consistent &amp; good naming, and idiomatic Scala - Not sacrificing principles or lawful behavior Some of these issues are somewhat contentious but the good news is that Scalaz 8 will take whatever shape the people who are willing to do the work want. Who knows, perhaps with the [recent collaboration between the Scalaz and Cats communities](https://github.com/typelevel/cats-effect/issues/321), maybe the communities could find some common ground in the successor to the current generation design (Cats and Scalaz 7.x share the same basic design and Cats has derived lineage from Scalaz 7.x).
If xs is an effectful steam then this changes the semantics. This refactor is worse in performance and semantics
&gt;Who knows, perhaps with the &gt; &gt;recent collaboration between the Scalaz and Cats communities &gt; &gt;, maybe the communities could find some common ground in the successor to the current generation design This would be so good for Scala as a whole imo and something I have my fingers crossed for. 
OP here. This is just a post I wrote a while back to introduce people to property-based testing (using Scala). Any feedback is appreciated. 
[removed]
StateT is good, but you might want to check out this: [http://degoes.net/articles/effects-without-transformers](http://degoes.net/articles/effects-without-transformers) The gist is to use a type like [cats-effect's Ref](https://typelevel.org/cats-effect/concurrency/ref.html) to achieve an effect-based management of state encapsulated in your type. That way you can avoid all the copying when you run the effect in your given IO effect (Task/IO). It's both functional and more performant. &amp;#x200B;
I don't see here a need for a State monad, and generally I would say that State monad is a lower level concept, used for particular functions. Whereas you're looking for more high level design patterns (e.g. immutable events log or event sourcing as you said).
Hey John, thanks for the update on what you‚Äôre focusing on. :) Question: what‚Äôs wrong with `ap` and what would you replace it with? (I‚Äôm assuming `ap` is just `&lt;*&gt;` but defined in terms of the Monad instance as in Haskell.) Thank you, and keep up your awesome work!
You might link to [http://javatoscala.com/](http://javatoscala.com/) which tries to do an automatic Java to Scala conversion.
Thanks, that's useful. I'm a fan of the Scala language, but I would add one point to that FAQ. Some of the Scala early adopters used Scala's operator overloading feature extensively. Liftweb, SBT, and several other prominent Scala projects have dozens of custom operators defined. It's a great syntactic convenience for wizards with those respective tools, but a real thorn in the side of novices. (Or at least for this novice.) Arguable overuse of operator overloading doesn't mean Scala is dead, dying, or unworthy of use. Just that it's something to keep in mind when designing APIs. 
&gt;I literally don't know what you're talking about or what internet history emilypii has outside reddit (if any). I'm talking about particular users being consistently negative on /r/scala, which it's absolutely my job to take into account when moderating. For what it's worth, I went through my history, and the last negative (or at least, one in which focused on negative content) post I made was with respect to the Scalaz/Cats timeline back when prominent a prominent ex-Cats maintainer was hurling racism accusations at Scalaz folks. Prior to that, the next negative one was 10 months ago. I'm not really sure what history you're referring to, as I've not participated in too much on this subreddit besides helping people and occasionally being drawn into dramatic discussions (as all people are wont to do). Our only discussion was in the Cats/Scalaz thread. So i'm not sure your judgement is well-placed here. I noticed you mentioned my twitter account. Is that what you're basing most of your judgement upon then? If so, fair. I am more outwardly critical of Scala on there. However, not here, so it should not affect the way I'm treated by you. 
Is it possible to configure IntelliJ IDEA so that it gives warnings/errors for non-exhaustive matches? And is there a way to generate code for the missing cases? I know that I can generate cases when writing `x.match` but when I later add new cases I did not find an easy way to find all `match` expressions I need to adapt.
&gt; For what it's worth, I went through my history, and the last negative (or at least, one which focused on negative content) post As we've seen in this thread, you delete comments with some frequency, so any examination of your current comment history is meaningless. My memory is that I've thought many of your comments were close to the line and few were positive contributions. Maybe I'm wrong, maybe I'm being unfair; if so, that's unfortunate for you. Given that your comments here, in the Scalaz/Cats history discussion, and in your recent messages to mods have been at casual variance with reality, I put little weight on your claims. At best, we have radically different worldviews; frankly, I think it more likely you've given up attempting to be truthful. Either way, I have no interest in pursuing the details of your history further. &gt; I made was with respect to the Scalaz/Cats timeline back when prominent a prominent ex-Cats maintainer was hurling racism accusations at Scalaz folks. That claim is, IMO, misleading to the point of dishonesty (as anyone who reads the actual tweet you refer to will see) - as were many of your statements in that thread. &gt; I noticed you mentioned my twitter account. Is that what you're basing most of your judgement upon then? FWIW I didn't mention your twitter account and indeed know nothing about it. &gt; If so, fair. I am more outwardly critical of Scala on there. However, not here, so it should not affect the way I'm treated by you. As a matter of present fact, it hasn't. Whether it should or not is for the moderation team to decide, not you. &gt; Is that the reason for a warning as opposed to a ban? I'm not going to rules-lawyer about what does or doesn't warrant a warning or ban. You have been warned. If and when a moderator feels that it's warranted, whether because you were being overly negative or for any other reason, you will be banned. In this thread I've tried to explain what I think is ok and what I think is not, because I think that's helpful to other users and good for the subreddit. Do not mistake that for a binding obligation. I believe I'm being as fair and reasonable as a moderator reasonably can, if they are to avoid giving trolls free reign. Reasonable people can disagree and are more than welcome to leave and discuss Scala elsewhere.
&gt; Is it possible to configure IntelliJ IDEA so that it gives warnings/errors for non-exhaustive matches? Enable "Unchecked warnings" on the scala compiler settings page. (Or enable that warning in the project definition that you're generating the IntelliJ config from). &gt; And is there a way to generate code for the missing cases? I'm not aware of one, but I didn't even know it was possible to generate cases in the first place, shrug.
&gt; As we've seen in this thread, you delete comments with some frequency, so any examination of your current comment history is meaningless. &gt; &gt; My memory is that I've thought many of your comments were close to the line and few were positive contributions. Maybe I'm wrong, maybe I'm being unfair; if so, that's unfortunate for you. Given that your comments here, in the Scalaz/Cats history discussion, and in your recent messages to mods have been at casual variance with reality, I put little weight on your claims. At best, we have radically different worldviews; frankly, I think it more likely you've given up attempting to be truthful. Either way, I have no interest in pursuing the details of your history further. 
OK, lets just stop the blog self-promotion now before it becomes a habit. Reddit is not for pushing your blog/youtube channel. But that's the only kind of content you've posted. 
Well, I don't think I'm abusing anything. This is a tech blog with posts about scala that we worked hard to create and publish. You may not like it but that's what it is. I don't think I'm doing anything wrong as the content does nothing that can be considered spam and it's not selling you anything. I could write the same comment you wrote about all the posts that come from functional works, software mill or any other company that usually has its posts here. And I like reading all of those everytime they are published.
&gt; Question: what‚Äôs wrong with ap and what would you replace it with? `ap` is the canonical operation of `Applicative` in Haskell, but it's designed in a way similar to `&gt;&gt;=`: the operation has the shape it does for reasons of _ergonomics_. In Haskell, all functions are curried, which means that `ap` is the most convenient way to operate on values inside functors. For example, if we have a "three parameter" function `f` we want to apply on values inside `fa`, `fb`, `fc`, we use the `ap` operator twice: ```haskell f &lt;$&gt; fa &lt;*&gt; fb &lt;*&gt; fc ``` This makes sense in Haskell, but it's not useful in Scala, because functions are not curried by default, and people seldom curry them (runtime and syntactic overhead). In addition, the signature of `ap` is extremely confusing to beginners: why do you have a function inside a functor? It's confusing because to understand that, one has to explain Haskell. In essence, `ap` conflates an abstraction with a particular _expression_ of that abstraction that happens to be useful in Haskell, but not in Scala. In my opinion, it's a big mistake to copy `Applicative` into Scala. Instead, we should look for something more fundamental. The essence of `Applicative` derives from a lax monoidal functor in category theory, which is more precisely captured not by `ap`, but by `zip`: ```scala def zip[A, B](fa: F[A], fb: F[B]): F[(A, B)] ``` `zip` is more true to the category theory origins of `Applicative`; the laws are far easier to state; and `zip` is much easier to use in Scala compared to `ap`. In addition, various types of functors which are not endofunctors in `SCAL` are still lax monoidal functors, which makes this definition strictly more powerful than `ap` from Haskell. In summary, blindly copying things over from Haskell results in poor usability and painful pedagogy, and in some cases actually decreases the ability to abstract. Cleaning up `ap` and other things like it is one of my personal goals for Scalaz 8.
&gt; Well, I don't think I'm abusing anything. Most spammers don't &gt; I don't think I'm doing anything wrong as the content does nothing that can be considered spam and it's not selling you anything. &gt; I could write the same comment you wrote about all the posts that come from functional works, software mill or any other company that usually has its posts here. And I like reading all of those everytime they are published. yeah, those are a problem too. I left /r/programming because it's a dead self-promotion sub. No actual community. You're turning /r/scala into that. https://www.reddit.com/wiki/selfpromotion &gt; But it's not spam! I worked hard on that, I make no money from it, it's original content! I'm not a spammer! &gt; We're not making a judgement on your quality, just your behavior on reddit. Your stuff's probably amazing and someone would be really interested in it but... &gt; If you submit mostly your own links and your presence on reddit is mostly for your self-promotion of your brand, page, blog, app, or business, you are more likely to be a spammer than you think! Read the FAQ and make sure that you really understand that. 
I like blog post (self)promotion on this reddit. Voting filters the good from the bad, and its not like there are poor other posts dying a quiet death on new on this reddit. YMMV. A sepetate meta discussion could be appropriate, but not in the comments of this post IMO.
That was a great explanation, thank you! As a followup question... Does the name `Applicative` (or even just `Apply` as in PS) really make sense under this formulation? I think it's fair to say that `Applicative` name was coined for this encoding because it captures the essence of the applicative style of programming[0], generalized over some arbitrary functor. Compare for example applicative application with normal function application: ```haskell (&lt;*&gt;) :: (Applicative f) =&gt; f (a -&gt; b) -&gt; f a -&gt; f b ($) :: (a -&gt; b) -&gt; a -&gt; b ``` The intuition here is that `&lt;*&gt;` is *just* `$`, embellished with the effect described by `f`. The `zip`-oriented encoding seems to lose the obvious correspondence to normal function application. Would a more apt name for this alternative encoding be `Monoidal`? It more clearly captures the notion that [what we currently know as] `Applicative` is actually *just* a higher-order monoid. Would this new `Applicative`/`Monoidal` interface supply combinators for working with n-ary functions? Because just pairing up the results after sequencing the effects isn't usually that interesting. I'm thinking of something like: ```haskell map2 :: (Applicative f) =&gt; (a -&gt; b -&gt; c) -&gt; f a -&gt; f b -&gt; f c map3 :: (Applicative f) =&gt; (a -&gt; b -&gt; c -&gt; d) -&gt; f a -&gt; f b -&gt; f c -&gt; f d map4 :: (Applicative f) =&gt; (a -&gt; b -&gt; c -&gt; d -&gt; e) -&gt; f a -&gt; f b -&gt; f c -&gt; f d -&gt; f e -- etc. ``` Thank you again for your time and feedback! :) [0]: [Idioms: Applicative programming with effects](http://strictlypositive.org/Idiom.pdf)
You must be a blast to work with. 
My coworkers want my previous company to hire me back, so yeah. Attention to "what's correct" instead of "whatever goes, who cares" is part of that
The name `Applicative` could be retained for historical reasons (i.e. so when people Google it, they find prior art in other languages), but is, as you note, not a very descriptive name. So I would actually prefer to break with the naming scheme and opt for something better. Personally, I'd prefer `[Lax]Monoidal` or `Productive`. I think you _would_ supply combinators for n-ary functions (`map2` through `mapN`), but I probably would _not_ supply `ap` itself, because there are too few cases in Scala where it's actually useful. The `mapN` functions are going to be the interesting ones.
The problem is that most of the mechanisms that people used to use to find new and interesting blogs (most notably, the RSS ecosystem) died about a decade ago because all the money was in fancy walled gardens, and killing independent publishing was a good business strategy. (I could write a long essay about how much damage Google did to the open web when they killed Reader because it competed with Plus, but my New Years resolution is to spend less time angry about things.) This sucked more writers into tools like Medium that offered built-in promotional/syndication, at the cost of them owning everything you write for eternity.
&gt; In addition, various types of functors which are not endofunctors in SCAL are still lax monoidal functors, which makes this definition strictly more powerful than ap from Haskell. This sounds cool! What are some examples?
Adelbert Chang makes a related point in 'The Functor, Applicative, Monad talk' [https://www.slideshare.net/pjschwarz/applicative-functor-116035644#19](https://www.slideshare.net/pjschwarz/applicative-functor-116035644#19)
@m50d I do not know how much you have contributed to Scalaz. You have written a lot of negative judgements about top contributors to Scalaz, that happen to be very active, helpful and nice people. One of them got banned. I am interested in critique and fears of those who leave Scala for Haskell or Elixir. Opinion of those who knows many languages are more insightful that those who don't. Especially if they happen to be co-authors of things being discussed :) Please help with removing ban hammer from fommil. Please do moderation in moderation :) There is a lot of people with awesome brain power in Scalaz8. Some of them went into the world (Haskell, Agda, Idris) and bring back new ideas :) Thanks to John DeGoes there is a lot more people, that can do useful work on hard but very useful OS libraries - like Scalaz or ZIO. Things go into right direction, even if they don't move as fast, as some might expect. Dream fiercely &amp; ROCK HARD &amp; stay chill all you lovely people :)
&gt;Haskell is more elegant Citation needed &amp;#x200B;
That was a personal opinion, in case it wasn't clear. You know, from the whole "As a haskell developer yadda yadda" precursor ;) I'm now two and a half months into a Scala (and a little haskell) job, and I still feel that way. I'm not going to fight you if you prefer Scala, though - it's a good language, and people like different things. 
Piggybacking off of this, is it possible to have non exhaustive matches be errors? I know I can get all warnings to be errors, but that's a bit overkill for me.
Both ways of thinking are just as fallacious, they are just on opposite extremes of the spectrum. Having a language which has beautiful abstractions but doesn't solve business problems such as scaling is just as bad as having an ugly language which solves business problems very well.
&gt; You have written a lot of negative judgements about top contributors to Scalaz, that happen to be very active, helpful and nice people. I found the ScalaZ community to be a toxic place (especially for newcomers who came seeking help) - indeed the reason I came to /r/scala in the first place was to get away from them (and I don't seem to be alone in that view). My first priority is to ensure that doesn't happen here, even if that means missing out on some valuable insights. If you've found the people I've banned to be helpful and nice elsewhere, great - keep talking to them in those other spaces. I can only judge by what I've seen of them. &gt; I am interested in critique and fears of those who leave Scala for Haskell or Elixir. Opinion of those who knows many languages are more insightful that those who don't. Especially if they happen to be co-authors of things being discussed :) True as far as it goes. But neither knowing another language, nor being a co-author of something, absolves you of the responsibility to contribute productively. Too many of those who talk about leaving Scala write things that seem to be provocative for the sake of it rather than any genuine effort to make things better. &gt; Please help with removing ban hammer from fommil. No.
I don't think you can make individual warnings fatal; the recommendation I've seen is to enable fatal warnings and then use something like https://github.com/ghik/silencer to exclude warnings that you don't want.
Such a shame... I have tried that plugin before, but it's not been reliable enough
Thanks for correcting all the "fallacious thinking" in this thread, if I wasn't so grateful to be enlightened I might bother to mention that nothing you have said refutes (or honestly even addresses) my suggestion that composability and extensibility lay a groundwork to solve "business problems such as scaling" in a way that doesn't eventually have you doing ridiculous crap like extending Exception so you can override getMessage with a string that you'll probably have to parse again up the callstack if you want to make use of the int you interpolated into it to show something meaningful to a stakeholder (that's a business problem right?)
How about https://github.com/softwaremill/neme-plugin ? &gt; Scala compiler plugin for turning non exhaustive match warnings into errors Also *ping* /u/peeeq , though it's for sbt, not strictly IDEA (in case you build with gradle or even directly through IDEA)
As a dev who has worked solely with Scala in day jobs for more than 5 years and has plan to continue doing it, in this very first Reddit message of mine I have to login to say that the decision banning @fommil is a big loss to the community. I personally learned from him how to write much better software in Scala, and believe that many others benefit from his softwares/book/articles/reported issues. His criticisms might not sound comfortable to the ears but I perceive them as truthful. The community needs different perspectives other than yours alone.
Thanks for making a strawman out of my argument, I was talking about business problems such as performance and in such cases, Finch/Finagle was by far the fastest http server out there (http4s only started becoming competitive). No one is going to care about the abstractions of the language if it can't handle those are the kinds of issues you are going to solve, and if we are talking about JVM/Java/Scala then Netty/Ratpack/Finagle were the only real frameworks able to solve this.
Was the strawman from the parts where I literally quoted you, or somewhere else? You're right, business problems "such as performance" and "such as scaling" are super specific, and I totally misrepresented what you were saying in order to make myself seem smarter. I guess the only thing I didn't do was explicitly suggest you were thinking fallaciously. I don't have any issue with looking at practical matters and I don't think you're wrong for caring about them. I do have an issue with coming into a discussion starting off with "Ackshually..." followed hand-wavy-statement-of-opinion-as-fact that doesn't even acknowledge the comment you were responding to. Twice. I think we just agree to disagree, there's a clear failure to communicate here.
&gt; You're right, business problems "such as performance" and "such as scaling" are super specific, and I totally misrepresented what you were saying in order to make myself seem smarter. I guess the only thing I didn't do was explicitly suggest you were thinking fallaciously. Well yes thats what I was talking about, at the end of the day you use languages to solve problems have at work. I also wouldn't say those problems are super specific otherwise you could argue that creating super correct software that takes ~2/3 times longer to develop is also a "super specific problem" But yes, lets agree to disagree
I have been observing that the "toxicity" is usually centered around some individuals - troublemakers - that like to create drama, for whatever the reason is. My guess is down the line they will create drama in whatever followup communities they go into. They should look into JohnDeGoes how he structures arguments and criticizes things. He's not banned, nor anyone has a problem with his points. Also the way they structure their arguments (people leaving scala etc.,) is very flawed ("here, my dataset is these few anecdotical examples") - and they refuse to see or acknowledge otherwise. Quite religious and not rational stance.
&gt;I found the ScalaZ community to be a toxic place (especially for newcomers who came seeking help) I find this to be a bit unfairly generalizing. I understand that there are tense interpersonal relationships in the scala community at large. However this throws many small time contributors and members of the ScalaZ community like me under the bus purely by association. I sincerely hope that people can lead discussion with more care in the future. &gt;indeed the reason I came to /r/scala in the first place was to get away from them (and I don't seem to be alone in that view). That's fair enough but I hope this does not impact any decisions concerning people that belong to those communities. &gt;Too many of those who talk about leaving Scala write things that seem to be provocative for the sake of it rather than any genuine effort to make things better. Personally i find critique for critiques sake quite valuable. I don't think it's always possible to accompany a complaint/critique with a constructive proposal for a solution. Be that because of personal reasons i.e. frustration or lack of knowledge. Anyhow this was just my 2 cents and I understand that the mod team doesn't have to justify anything to me but I wanted to put my opinion out there anyway.
&gt; I find this to be a bit unfairly generalizing. I understand that there are tense interpersonal relationships in the scala community at large. However this throws many small time contributors and members of the ScalaZ community like me under the bus purely by association. I'm not claiming that everyone involved in ScalaZ is individually toxic. But I stand by the statement that the community was a toxic place when I was there. Personally I think there's a certain element of "the standard you walk past is the standard you accept"; I decided that I didn't want to be associated with that project no matter how good it is technically. You can and should make your own choices about what projects to involve yourself in. &gt; That's fair enough but I hope this does not impact any decisions concerning people that belong to those communities. I'll always try to give individuals a fair chance. At the same time I can't promise to ignore previous interactions I've had with people; ultimately I'm only human. &gt; Personally i find critique for critiques sake quite valuable. I don't think it's always possible to accompany a complaint/critique with a constructive proposal for a solution. I'm not saying everything has to propose a solution; a critique that breaks down and clarifies a problem is very much a valuable and constructive contribution. 
I might not agree with all of it but thanks for the explanation nontheless. 
&gt; John DeGoes also makes public statements that Scala 3 "won't save Scala", "Is an entirely different language", and "will split the community". Also, since his talk, he has also expressed optimism as regards how Scala 3 is shaping up. See this tweet: https://twitter.com/jdegoes/status/1074719596957978624
In terms of SQL, what you're looking for is a left outer join of the two tables, and then a [coalesce](https://spark.apache.org/docs/2.3.0/api/sql/#coalesce) on the result. So it would look something like val combinedDf = sourceDf.join(otherDf, sourceDf("id") === otherDf("id"), "left\_outer") combinedDf.select($"id", coalesce(/\* This part I'm not sure about, but just find what the two Birthday columns are called in combinedDF \*/).as("Birthday"))
Ok, thank, I'm making progress. After the join, both of the "Birthday" columns retain their original name, "Birthday." There are two of them, which I didn't think was possible. As a result, the coalesce fails because the reference is ambiguous. If I rename one of them, it works! // Very simplified code val joinCols = Seq("id") val intersectColName = "Birthday" val selectCols = joinCols.map(col) ++ Seq(col(intersectColName).as("Right")) val joined = sourceDf.join(otherDf.select(selectCols: _*), joinCols, "leftouter") joined.select(col(joinCols.head), coalesce(Seq(intersectColName, "Right").map(col): _*).as(intersectColName)) Still feels messier than I think it needs to be, but I'll take it.
Thanks, I will add this to my build. Usually I run sbt in a terminal on my second screen with `~compile` so this will be useful.
On the example as presented, you don't actually need a left outer join. An inner join (the default) is appropriate. Apologies if the next paragraph is stuff you already know, you just give the impression of being unfamiliar with sql concepts. Left outer will mean you will get every row in sourceDf and attached to those, where there is one, you will have the rows from otherDf with a matching ID. Rows from otherDf with an ID that doesn't appear in sourceDf won't be in your joined Df. Rows from sourceDf with an ID that doesn't appear in otherDf will have null values for any columns from otherDf. Your example only has matching Ids. If this is the case, inner is more suitable. If you actually wanted to preserve rows from either DF, regardless of whether or not they had a matching ID in the other DF, you would want a full outer join (can just be specified with "outer").
The State monad is generally for 'threading through' immutable states (in memory) in a monadic way, so it may not be applicable to what you're trying. However, you may get some benefit wiht your API by using an IndexedStateT. https://www.youtube.com/watch?v=JPVagd9W4Lo&amp;t=4s
`{ case (key, value) =&gt; ... }` isn't a partial function since it handles all possible inputs. 
&gt;that because fb here has no dependence on fa, some things actually become more flexible. Order of evaluation becomes unimportant, so if your F i Thanks all for your input, much clearer in my mind now. This last example has helped alot also.
Hey, reapes93, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
In scala, `{ case ... =&gt; ... }` is a way to create a `PartialFuntion`. It's always that whether or not it handles all inputs. This in contrast to `... =&gt; ...` which creates a `FunctionN` (with N bring the number of arguments) So in Scala a partial function usually refers to a `PartialFunction` irrespective of whether it's a mathematical partial function. 
For autocompleteion tabnine.com works really well for me inconjunction with metals.
Aww, this is targeting dotty at the moment. Still, nice to see this coming.
I never had to do null check in Scala actually. Most variables are declared and defined with initial value, and options are used when it can be empty.
The amount of work is definitely appreciated but non-issue when working directly in Scala. The only benefit I see is when using Java libraries in Scala which is more and more rare... Outside of say the AWS SDK. And I think the additional type checking would actually complicate things. Because currently in those cases we wrap the library and handle the possibility of `null` in the wrapper with return as `Option` or `Either[Throwable, T]`. `Either` being the more elegant approach imo for example something such as def fn(x: String): Either[Throwable, BigDecimal] = Try(BigDecimal(x)).toEither which is handled quite nicely scala&gt; fn(null) res2: Either[Throwable,BigDecimal] = Left(java.lang.NullPointerException) and the added benefit of say scala&gt; fn("123.456").map(_.setScale(2, BigDecimal.RoundingMode.HALF_UP)) res3: scala.util.Either[Throwable,scala.math.BigDecimal] = Right(123.46) So I don't really see this as a problem in Scala unless I'm misunderstanding the purpose of this PR
This is to make interoperability with Java better, pretty plain and simple, we already have ways to keep pure Scala code pure, but the challenges when using Java APIs is still quite severe, and this could unburden needless defensive programming, to the same degree as Kotlin e.g. has with its nullability capabilities. A welcome change in my opinion.
For me, the point is that right now I "believe" nothing is null and based on this belief I don't do any null checks. But its still just a belief and if someone will put a null somewhere, then I'm screwed. With this change it won't be a belief, it will be afact. Compiler-enforced fact. And this makes me sleep better at night.
I don't get how `JavaNull` being a special case that allows writing `foo.bar().baz()` is a better approach than having `foo?.bar()?.baz()` that works like `map` and `foo!.bar()!.baz` that throws for the regular `Null`.
I think the motivation for `JavaNull` and its special-case semantics is backwards compatibility: to allow existing code to mostly run, without needing to insert a million `?` or `!` characters throughout existing codebases.
agreed with all of this. null-less scala is the dream, and we should be advancing the language to achieve that dream
Something like this would be very welcome. I've worked on a Kotlin project for a few months and its null safety and smart casts are a joy to work with. Much better than monadic `Option` model. If you've worked with Typescript its even better, union types combined with smart-casts are a very good alternative way to model ADTs, and in my opinion better than Scala's sealed trait/class + subclasses model. I believe better support is planned for Dotty.
I don't think it's worth being part of the actual language for a one-time migration from 2.x to 3.x, but if writing code that compiles under both 2.x and 3.x is the goal, it would make sense to have this behaviour behind a compiler switch. 
The purpose is to make it a compiler check, not something you are "pretty sure" about
would this force type checking for `null` then at compile time on types that could be null? most npe's are encountered at runtime when using spark or java.sql for example. data that has unpredictable values which should be coded defensively for to being with. so does this handle that very problem?
Your question seems to stem from library interop. I would highly suggest reading the "Java interop" and "binary compatability" sections for how this proposal will work with your favorite library
I‚Äôve had unit tests blow up, due to options containing null. Thanks to a coworker who thinks it is a good idea to mock case classes.
I never thought that monadic approach is lucking anything, for me it is very elegant solution for null-ability because it does not introduce any ad-hoc features to the language. Provide some examples to make your point more valid.
&gt; With this change it won't be a belief, it will be afact. Compiler-enforced fact. And this makes me sleep better at night. I don't really like this because TypeScript has something similar right? It's a flag called `--strictNullChecks`. The problem is that initial state (for example a value for a key in an object) is still `undefined` by default. So it just makes it a pain to check for that because now you have to explicitly specify the union.
Yeah, this is exactly what I'm against and what you have to do in TypeScript when `strictNullChecks` is enabled.
&gt; With this change it won't be a belief, it will be afact. Compiler-enforced fact. Not really, though (but I agree with the sentiment). On the JVM, you'll always be able to disguise one thing for another as long as we have unsafe casts, which are almost as easy to write as a `null`: val oops = Some(null).asInstanceOf[Option[Int]] // ^ won't be enforced by the compiler &amp; won't throw at runtime // later: println(oops.getOrElse(0)) // crash; you won't sleep better at night!
I think it's even less worthwhile to complicate the core language by adding some _magic operators_ like `?.` and `!.`, which won't even be used much anyway because in Scala people don't normally use `null`.
&gt; smart-casts are a very good alternative way to model ADTs, and in my opinion better than Scala's sealed trait/class + subclasses model Really? You prefer writing this: interface Failure { tag: "failure"; reason: string; } interface Success&lt;T&gt; { tag: "success"; value: T; } type Failible&lt;T&gt; = Failure | Success&lt;T&gt;; function foo(r: Failible&lt;string&gt;): string { let res switch (r.tag) { case "success": res = f(r.value); break; case "failure": res = g(r.reason); break; } res.length } Than this? sealed trait Failible[A] case class Success[A](value: A) case class Failure(reason: String) def foo(r: Failible[String]) = (r match { case Success(x) =&gt; f(x) case Failure(y) =&gt; g(y) }).length The TypeScript version: - has _even more_ boilerplate than Scala at declaration-site; - has an enormous amount of boilerplate at use-site; - is less type safe (even with all the compiler flags), in particular no exhaustive checking; - does not support expression syntax: you need to `return` from the switches or use mutation; - does not have any utility function generated for you (such as the `copy` method); - require storing a _string_ in each object, and comparing strings at runtime (slower than an instanceOf test). I've used TypeScript for work, and this encoding is like the stone age of ADTs. Even a class-based approach with a `fold` method seems better.
Your Typescript can be shortened significantly: ``` type Failible&lt;T&gt; = { tag: "success", value: T }, { tag: "failure", reason: "string" } function foo(r: Failible&lt;string&gt;): string { if(r.tag == "success") { return f(r.value).length } else { return g(r.reason).length } } ``` This is a very comparible amount of code to Scala. So yes, I prefer the Typescript way, because smart-casting combines really well with ADT's
&gt; some *magic operators* Ok... tons of *magic* there. 
I mean you're right, but unsafe casts throw all type guarantees out of the window. The same argument could be used to say that `val y: List[Int] = x; y.foreach(_ =&gt; {})` is unsafe because x could be `1.asInstanceOf[List[Int]]`.
&gt; Personally, I'd rather just be pessimistic and wrap anything from Java in Option to take care of that. That's a solution, but a lot of boilerplate and prone to forgetting. It would be even greater if the compiler automatically made all `T` values from Java `T?` or `T | null` like Kotlin does.
&gt; elegant solution ... because it does not introduce any ad-hoc features to the language That's certainly a definition of elegant and a valid point, but the disadvantage is that non-optional code and optional code look very different from each other. An alternate design philosophy (C#/Kotlin take this more often) is that a language should make common use-cases as easy and clear as possible. None of the two philosophies are better, but for optionality I prefer the special syntax. Code example with a tuple zip Non-optional: `def zip[A, B](a: A, b: B): (A, B) = (a, b)` "Special-syntax"-style: `def zip[A, B](a: A?, b: B?): (A, B) = if(a == null || b == null) (a,b) else null` Monadic-style*: `def zip[A, B](aOpt: Option[A], bOpt: Option[B]): Option[(A, B)] = aOpt.map(a =&gt; bOpt.map(b =&gt; (a,b)))` This seems like a small thing, but in practice all the `Option`s, `(flat)map`s and "double" names add up to make code more cluttered. It also solves the problem that if you want to change a T to a `Option[T]` you have to rewrite all the code behind that to suddenly work with `Option[T]`, which is a major pain in the but compared to `T?`. The large disadvantage is that it isn't generic. If your type changes to `List[T]`, `Future[T]` or whatever you'll have to rewrite the code more compared to the monadic version. In my experience that's a rare occurrence in practice though. \* A for-yield would make this more clear, but in my experience is harder to intermingle with non-for-yield code
Well I guess that goes back to my main point. It seems like it adds unneeded complexities and verbosity to the type system. The beauty of `Option` and `Either` is that you control the application flow easily and succinctly without having to worry about `null`. I just don't see the benefit. I'm not a master of Scala by any stretch and learn something almost every day it feels like. This just doesn't seem be solving real world problems, especially in big data where you are coding defensively to begin with and you're coding with a FP approach in mind for performance.
For me both `if`-based and `for`-based implementations are concise, and it is just a question of preference. I personally dislike `is` construct because it redefines existing symbol implicitly. If anything i would rather prefer `if let` kind of construct.
In my personal opinion Scala and its eco system are great and has never been better so a great time to jump in. It is still popular without doubt. &amp;#x200B; On the job front overall I do think from what I have observed for it to be on the increase but its very regional. I don't know where you are based to comment. &amp;#x200B; I find the community support and help great and have made a lot of friends in the community. I particularly like the gitter channels which are very active. &amp;#x200B; Are you looking to learn scala for a particular use case or area of interest? 
The popularity is trending, according to pypl
java and scala has high interoperability, they not excludes each other. 
Forgot to mention you will find this talk in particular relevant to your question and worth watching: &amp;#x200B; [https://skillsmatter.com/skillscasts/12804-into-the-new-world](https://skillsmatter.com/skillscasts/12804-into-the-new-world)
Yes. All the big streaming frameworks use it under the hood, and it‚Äôs a great language
Yes, I‚Äôd say it‚Äôs popular and growing. I got my first Scala job just four months ago, having previously mostly done Java. (I‚Äôm in the Bay Area.) It‚Äôs a very satisfying language to use, given the mix of OOP and FP sugar. Probably has a higher learning curve than other languages, but at the same time feels amazing when things ‚Äújust work‚Äù the way you‚Äôd expect them too. Def worth trying out. I recommend IntelliJ + Scala plugin, with sbt, and _Scala for the Impatient_ by Cay S Horstmann to read.
Many major well known tech companies use Scala. However, my perception is that its main niche is data processing pipelines. If this is something you're willing to do, then absolutely certainly learn Scala. I believe, chances finding Scala job that is not data processing are pretty thin. Java and/or Kotlin would be a better bet (if we're looking at JVM-only). And, frankly, I wouldn't tell that Scala is a great language worth learning. Maybe Scala 3 will become such, but Scala 2 has a lot of rough edges still. Just my humble opinion.
I strongly recommend it. Even if you won't go with Scala professionally, learning it will help you become a better programmer. The Nodejs and C# code you writer later will be improved greatly.
This is an interesting point, we usually work on microservices. Is there a case to be had for scala in this regard? i.e other than data processing. I understand Java is widely used in this regard
Of course! I apologize if my comment created impression, that there's no place for Scala there. There are companies that utilize Scala for microservices. Well known example is Twitter. There are other companies for sure. Numerous financial institutions use Scala for microservices. However think like this: among all the tech companies only fraction rely heavily on JVM stack. Only fraction of companies relying on JVM stack use Scala. And I'm personally yet to see a company that uses Scala in a beautiful nearly-purely functional way: usually it's Akka + "better Java" style (Twitter is an exception here). And again: this is my personal very limited experience. However, there are many companies that are not relying on JVM as it's primary technology, but they still use Scala for data processing pipelines (few examples: Spotify, Apple). That's why my conclusion about its niche.
Yes! Also this is a good time to plug my little project https://www.jeffshaw.me/cheat
TodoMVC in 139 lines of Scala with React4s **[Source code](https://github.com/Ahnfelt/react4s-todomvc/blob/master/src/main/scala/com/github/ahnfelt/react4s/todomvc/TodoApp.scala) ‚Ä¢ [Live demo](http://react4s.org/todomvc/)** Written in ideomatic React4s. Includes persistence and simple routing. Surprisingly, the smallest TodoMVC in the officital collection seems to be another Scala library! ([Binding.scala](http://todomvc.com/examples/binding-scala/#/) - 154 lines) **Learn more about React4s** Documentation: http://www.react4s.org GitHub: https://github.com/ahnfelt/react4s 
Whilst scala is popular with data processing for sure , it is popular for microservices also and other use cases. Last company I worked for we had REST microservices using Scala using in paricular akka-http and play frameworks as an example. 
Last I looked, scala jobs pay higher. Yes, it is used for data processing and distributed programming a lot, but it also has an excellent web server (play) and supports a very sleek reactive framework (akka). It's lambda implementation is also way more intuitive than Java's. In other words, it is becoming a very versatile toolset. Also, ask any former java programmer with 6 months scala experience if they would like to go back to java. I haven't yet met one. &amp;#x200B;
It is always good to learn new languages. You can learn a new paradigm as well (functional programming) if you don‚Äôt know it already. 
Things are not that bright for Scala if you count Kotlin in ;) I think the biggest issue with Scala atm, is that Scala is community driven and community is still deciding on direction where Scala is going. There's a lot of hype in community about category theory (just look at r/scala) and everything else is considered almost blasphemy, but in real world Scala is mostly used as DSL for Spark and as better Java for Akka-based services. It may appear, that Scala you learn today will be completely different form Scala in 2 years. I wouldn't bet on how it will evolve.
Fair comment, but note a distinction between the scala proselytizers and scala industry engineers. The latter group are not puritan FP by any means. I see some java conventions and design patterns kept by choice, a reluctance to use implicits and a major focus on code readability, which means avoiding some of the high fructose syntax. The point being, scala is chosen because it works well now, not because it might work in the future. 
Yes, especially Dotty/Scala 3. One of the subjects I want to learn in 2019.
I think all three are a good source. I personally used `Programming Scala` as a first reference, but I heard from most people `Programming in Scala` is an excellent book for beginners. I do recommend `Programming Scala`. My coworker, who had 10 years experience in Java, started with `Scala for the Impatient`, so I guess all three are good.
&gt; I believe, chances finding Scala job that is not data processing are pretty thin comparing to Java and Kotlin (if we're looking at JVM-only). From Stack Overflow jobs: - Kotlin [166 results](https://stackoverflow.com/jobs?q=kotlin) - Scala [287 results](https://stackoverflow.com/jobs?q=scala) While the results may differ on other job sites and it varies by region, calling chances of finding a job doing Scala "thin" compared to Kotlin is an overstatement and I haven't seen it backed up by facts. Kotlin is growing (I'd guess a lot of that is due to Android), but so far Scala is still bigger.
&gt; Scala job that is **not data processing** I believe that the number will be less impressive once you filter out all the jobs that have "spark" and "data" in their description. Not insisting that my statement is correct, I still don't have numbers. Just trying to explain my point. And I also mentioned both Kotlin and **Java**. What I meant is that services development is more about frameworks than languages, and Kotlin applications mostly rely on the same frameworks that are used in Java, hence I wouldn't even separate them, Kotlin is indeed an improved Java, while Scala is usually a whole different beast. 
Wow, this looks really neat. I'm using [scalajs-react](https://github.com/japgolly/scalajs-react) and sometimes it feels quite magical. I might check this out later.
Thanks! React4s doesn't use macros, nor does it use implicits, so you could indeed say it's less magical :)
One thing to keep in mind while doing plain search about Scala is, lots of results include "scalable" which starts with "scala" but has nothing to do with it.
Yes Scala is widely used. Lots of people went to Scala because of Data tooling such as Spark but these days these people adopt Python. During the last few years functional programming in Scala evolved a lot. We bow have mature libraries and coherent ecosystems arround them: cats, shapeless, ScalaZ. Scala/Java compatibility is very much like C++/C one. You can use seamlessly any Java library but they are very different languages. Of course you can program in Scala like you would in Java but this approach is not very productive and people following this path often switch to Go or Kotlin quickly because of Scala's "complexity". My advice is, take the time to really learn the language. Take a good book like "Functionnal Programming in Scala", read it completely and do all of the exercices. Depending on your background, you'll learn a lot of things: real FP, type classes, poweful abstractions, doing concurrency with ease, etc
As someone who has been not up to date. Last time I used Scala for Spark 2 years ago. What recent changes make Scala worth learning (or relearning in my case)?
Do you have a plan as to how you're going to go about it? I've been programming in Scala for a long time, but haven't actually paid much attention to Dotty/Scala3.
Depends on your goals. I've done Scala for about 2.5 years now. I'd recommend learning reasonml, f# or Haskell if you're coming from c# and node.js to explore more functional concepts. Scala's multi-paradigm support can be useful for people who are experienced with it but often confuses newcomers and can be an easy way out of thinking in a certain way. You'll read Scala out in the wild that looks like Java and some that looks like Haskell and it will probably confuse you more than anything. It also takes quite a lot of Scala knowledge to know that you shouldn't use most of the language most of the time (fun talk about this - https://youtu.be/Es_JH3_64ys). There's a bunch of best practice resources out there you should be aware of while you're learning (if you decide to): - http://www.lihaoyi.com/post/StrategicScalaStylePrincipleofLeastPower.html - https://nrinaudo.github.io/scala-best-practices/ However if you're looking to do Spark, then you don't have much choice. 
\&gt; And I'm personally yet to see a company that uses Scala in a beautiful nearly-purely functional way: usually it's Akka + "better Java" style There are a lot of companies that wrap Akka components in pure(r) functional abstractions. And of course some that go all in with fs2, http4s, doobie, ... They do exist. \&gt; Apple Besides data processing, they run a few services written in Scala. For instance for Apple Maps.
Scala has shown a very heavy commitment to backwards compatibility (indeed a lot of the things people criticize exist for backward compatibility reasons). I'd have a lot more confidence that you'll be able to write the same kind of Scala 2 years from now than I would for Kotlin.
Scala has a salary premium, and there is more exclusivity as a developer, compared to Java where you are competing with many more devs with the same skill set. Personally if you come from an .NET background, I‚Äôd work the F# niche also - I find it to be a really elegant language - salary is good I believe, though jobs are even more sparse. The benefit there is that you will know the entire ecosystem (.NET runtime and libraries) so you will have some small competitive advantage.
&gt; unsafe casts throw all type guarantees out of the window Exactly. All I was saying is that there will be no such "compiler-enforced fact" about `null` safety, even after this PR is merged. The type system is still unsound, due notably to unsafe casts, but also to initialization problems ‚Äì which the proposal doesn't solved, and is probably the main source of NPEs in Scala. So I don't really see why you would be sleeping better at night.
Scala is still the best general-purpose language I've found (I wouldn't be using it otherwise). While most of the ML baseline has made its way into a lot of languages now, few languages offer the kind of "typeclass derivation"/"safe object graph traversal"/"compile-time reflection" that you can do with shapeless, and there's at most one other mainstream language with higher-kinded types. I can't see any other language displacing Scala as long as that remains the case. Scala is at an age where it's no longer a hyped new technology; Scala.js aside it's much the same language it was a year or two ago. Maturity has its upsides; the library ecosystem feels substantial, tool support is pretty good (IntelliJ in particular has put a lot of work in lately; personally I think dropping the official eclipse integration was a mistake even so, but most people seem to disagree with me) and jobs are still easy to find. The language is unlikely to expand by a factor of 10 any time soon; equally it's not going to disappear overnight. If you just want the most popular language for jobs then that's probably Java. In practice I found the number of frameworks needed to work around language limitations in Java ended up being more effort to learn - and had a shorter shelf life - than learning Scala. But what lets you do the job is not necessarily what gets you to the interview. (Conversely I've had a couple of jobs where I was hired as a Java programmer but ended up working in Scala).
&gt; Your Typescript can be shortened significantly Not really; you're duplicating the `.length` call here. If a language forces you to repeat yourself to avoid excessive boilerplate, I don't consider that a good thing. &gt; either you were unaware of the above or you are misrepresenting on purpose Did you miss when I wrote "you need to return from the switches or use mutation"? &gt; I wouldn't be surprised if somewhere in the Scala 3 future union types will replace class hierarchies ADTs in practice. For following Dotty development I can tell you that it's almost certainly not going to happen. &gt; Comparison is built-in In what sense? I'm not aware of built-in object comparison in JS (but I have a limited knowledge there). &gt; and copy is a JS stdlib function. What `copy` are you talking about, and does it really do the same as case-class copy in TypeScript? &gt; If Scala would adopt more powerful union type √† la Typescript, it could implement them using `instanceOf` How would that work? &gt; This can be emulated Yes, but that's an ugly solution that just makes the encoding even more unwieldy.
I just meant to emphasize that these operators would have a very different semantics than other Scala operators. You couldn't express that semantics in a method signature, unlike all other method-like Scala operators (`&amp;&amp;`/`||`, `eq`, `match`, etc.), so it would really feel outlandish.
No, not in 2019. It was alright in 2018 and would be great in 2020 again but 2019 is an odd year and not just because it's not divisible by 2. Don't use Scala in 2019. Please use Node in 2019. Also check out the best node packages in 2019. Node will be amazing for 2019. .NET would be a good language too for 2019. 
lol at case class
They're not method-like operators, I don't think you can call them operators, they're `.`-like.
* more libraries for FP programming if you are into it (and better documented too!) * more learning resources * akka ecosystem - Akka Streams, Akka HTTP, typed actors (I guess Akka Streams + Akka Cluster might be a godd replacement for Apache Spark for certain use cases) * better tooling - IntelliJ handles implicits better, metals looks promising, formatters and linters are really helpful, 2 alternatives to sbt (mill and fury) * libraries like endpoints/tapir, which let you define schema of your HTTP service, and then use this schema to generate clients interfaces, server implementations, OpenAPI docs (all in a type-safe and ensured compatibility between clients, servers) * scala.js is quite mature, scala native is useful (though significantly less libraries cross-compile to it) Basically, it is quite a good choice if you want to get better with FP, you consider JVM (not necessarily Java) a solid platform and if you prefer static typing to dynamic typing. Kotlin can be a competition to Scala in this regard, but IMHO it has less rich type system and misses certain features that I got used to.
`.` is most definitely an operator, but it is very special. I don't think adding several very special operators would be a good idea, that's all I was trying to say :^)
Really depends who you ask and cite. I saw some source (I think some mostly London-centered job offers site), that shown 2% drop. Some people also say they all their friends leave for Haskell. On the other hand, I saw on another job posting site I saw 65% increase (though the site is quite Poland-oriented, so it also might not reflect global trends). And people I know mostly stay with Scala, and with every year we get better offers - better salary, better projects, less and less Spring Framework + Scala offers to flush down the toilet... So I agree with you though I don't have a definitive proof.
And I think they'd let us express otherwise unwieldy code, while not being so special as to be surprising or unpredictable.
Dotty, tl;dr: * tries to be compatible with Scala 2 where there is not a strong reason to change, so most of the code will be compatible * macros with be remade, so you need to say goodbye to your macro methods and macro annotations * real union types (\`A | B\`) and intersection types (\`A &amp; B\`) - make order of types in your union irrelevant (\`Either\[A, B\] != Either\[B, A\]\`) which is useful if you want to compose types easily, solve issues with compound types (e.g. overriding order) * tuples will be HLists underneath - a lot of shapeless could be removed and generic programming will still be possible * opaque types (basically newtype) * ideas for better typeclass support (with less implicit ceremonies) * a lot of "small" improvements that will make the language sound And other things I cannot think of now. I cannot wait for Dotty to come out! &amp;#x200B;
I am importing data into a Hadoop cluster from two kinds of sources: * Microsoft Excel files * REST server polling What I think I will use is * Apache POI (maybe with spark-excel [https://github.com/crealytics/spark-excel](https://github.com/crealytics/spark-excel) ) to import from Excel files * Akka and Alpakka to poll REST services and write to a Kafka topic Also, I'd like to use Apache Kudu as database. Do you think that it is sound? Are there better choices?
Is Scala 3 backwards compatible with Scala 2?
Scala 3 isn't released yet so no-one can know exactly what will happen. AIUI the intention is to remove some deprecated features and have a few source incompatibilities that can be fixed in a mostly automated way, similar to the 2.9 -&gt; 2.10 migration. I would definitely expect it to break binary compatibility. The big point of concern for many people is that macros will have incompatible changes; officially macros were always an experimental feature and I don't think I've seen much use of them in normal user code, but much of the ecosystem does depend transitively on a couple of macros (in particular the ones in Shapeless that make `Generic`/`LabelledGeneric` work). I can't imagine that Scala 3 will break these, for several reasons - Lightbend are aware how much of the ecosystem depends on Shapeless and have always shown a strong commitment to backwards compatibility, Miles Sabin (Shapeless author) is working for them now, and one of the key selling points for Scala 3 is better direct support for the kind of generic programming that Shapeless exists to enable. What I *expect* is that migration to Scala 3 will be fine for the overwhelming majority of Scala programmers who don't write custom macros and don't use obscure unmaintained macro-based libraries. They're not going to break something that half the ecosystem depends on. But until we actually have a version of Shapeless running on a version of Scala 3 I can understand people being worried.
How would `Option.apply` behave here? Having just `Option.apply[A](x: A): Option[A]` is a bit awkward, since passing a nullable argument would return `Option[A|Null]` Ideally, you would want to define ```scala Option.apply[A](x: A|Null): Option[A] Option.apply[A](x: A): Some[A] ``` However, since there's no way to say `A is not null`, I think that the type signatures would clash. I believe that you also get a type clash if you define both `Option.apply[A &lt;: AnyVal](x: A): Some[A]` and `Option.apply[A &lt;: AnyRef](x: A): Some[A]`, which seems to be the only way to enforce non-nullability. Is there a any way to solve this problem?
At the outset all the three are good. However it depends on the skill set you already have. Let's say you are a Java programmer with good knowledge on OO programming then I would definitely prefer Scala for the impatient, it takes less time to read this book and also this book has some great excercises that helps in writing Functional way of the code. But if you are a COBOL / some other legacy developer I would prefer Programming in Scala (Odersky) as it explains from scratch.
I'm working on [dashing](https://github.com/BenFradet/dashing) which is a collection of dashboards to monitor an open source organization (stars and pull requests at the moment). Under the hood, it uses: - http4s - circe - github4s - scalajs-react At first, I only wanted to try out scalajs given that I had no frontend experience whatsoever but it quickly became an interesting source of data. 
Yeah, but like LPTK said, there's also initialization problems. I mean you're right in what you said that the `Option` solution is a lot of boilerplate and prone to forgetting, but I think initialization requires just as much remembering. I just don't feel like this PR really fixes anything in that regard.
Thanks for your detailed reply
I want to get started with Scala.js too. What did you try doing first?
Thank you for pointing out all those things. SBT replacement looks like a good idea. I hated it when I used Scala.
I would recommend it. I'm actually using it at work. Our company mostly uses Java, but the nice thing is Java has pretty good interop with Scala, so you can call Scala code from Java. The main thing I like about Scala is it's much much better at functional programming than Java. You can do functional programming in Java, but Java was retrofitted with it, while Scala was built with functional programming at the start. There's also the problem of checked exceptions. Scala, like C#, just doesn't have checked exceptions and honestly it's such a blessing.
&gt;The large disadvantage is that it isn't generic. If your type changes to &gt; &gt;List\[T\] &gt; &gt;, &gt; &gt;Future\[T\] &gt; &gt; or whatever you'll have to rewrite the code more compared to the monadic version. In my experience that's a rare occurrence in practice though. I've almost never replaced an Option with a Future; the genericism is still immensely valuable tho. I've witnessed many students light up when I note to them that the same compositional methods I showed them for "Options" also apply to async programming with "Future", error handling with "Either", etc. They don't have to learn a hodge-podge of things like null-coalescing operators, async/yield, throws/catch, etc. This being an abstraction built within a library, as opposed to hard coded into the compiler, is also another huge win. Because eventually I might decide "Futures" aren't right for me. So I switch to "Tasks", and, again, it's the same standard abstraction.
I'm doing some sort a 'Scala job' since 2011 and there were no single one using 'data processing' one was live casino services with military-grade latency constraints, and another was world-wide shipping company package event tracking with huge datasets there are also Scala jobs in fintech, ecommerce and energy management systems any of these - not 'spark/data processing' (aka 'fancy reporting') - but mission-critical business systems (all above applies to North West Europe only) I welcome Kotlin at Android very much though, it seems to be proper response to Swift, and as an easy step from Java to 'better Java' 
I'm also doing 'Scala job' that is not data processing. And it is not even Akka. How does this contradict the statement, that there are more jobs in Scala that are data processing pipelines?
https://typelevel.org/cats-effect/datatypes/io.html#partraverse parTraverse and parSequence is what you want. It's exactly the same as the `Traversable` typeclass except it does stuff in parallel. Sequence is for flattening a bunch of io actions and running them in parallel. I'll start from the top, with just the concept from traverse and sequence. More generally, we have, as part of the traversable typeclass: `traverse :: Applicative f =&gt; t a -&gt; (a -&gt; f b) -&gt; f (t b)` `sequence :: (Applicative f, Traversable t) =&gt; t (f a) -&gt; f (t a)` Where `sequence = traverse id` (except in the cases where somehow this can be specialized). use traverse when you have some collection/sequence `t :: * -&gt; *` with elements that, for each one, you know which monadic action (in this case `IO`) that you want to execute (`a -&gt; IO b` in your case). If that reads a little funky because haskell syntax, replace `t` with `List` and `f` with say, `IO` to instantiate one way of using traverse for example. I.e: `traverse :: List a -&gt; (a -&gt; IO b) -&gt; IO (List b)` for it to click a bit more. The only difference between that, and the `parTraverse` trick that's used in cats, is that they made that `Applicative f` to use a newtype over, say, `IO`, that _isn't_ a monad. Due to monad laws, `ap` on your applicative instance that's a monad will by requirement be implemented using bind, meaning that traverse becomes sequential. By weakening this requirement, you can by some analogy "fork" computations since they will not require that the result of the previous action is fully evaluated to go on to the next one.
That's sometimes true - depends on your search engine. All of the ads in those particular results from Stack Overflow have Scala the programming language in them somewhere.
Thank you for your time and detailed reply. Indeed my Haskell knowledge is very limited. I think I got the beginning of something. I still need to test it but the new "it compiles" version I got so far looks like this: ``` def getAllConfigurations(applications: List[App])( implicit P: Parallel[Option, F], A: Applicative[F]): F[List[(App, Configs)]] = { val maybeAppsAndConfigs = applications.toNel.flatMap { appsNel =&gt; appsNel.parTraverse { app =&gt; getConfigs(app).map { cfg =&gt; (app, cfg) } } } maybeAppsAndConfigs .map(_.toList.pure[F]) .getOrElse(A.pure(Nil: List[(App, Configs)])) } ```
Spark! Burning 3-6 GBP per hour on Azure to bend data into the right shape then taking it to be part of a ETL aws glue job later on.
I work with scala, nullability is for sure an issue for me.
I think you are just seeing one field in which scala is used. I use scala with extensive use of java libraries, and extensive use of nulls. I'd very much appreciate the help from the compiler about it. (I'd also ask for linear and affine types to help cope with some complexity, but oh well)
[/u/fwlega](https://www.reddit.com/user/fwlega) note that there's no instance of `Parallel` for `Option` as it is not possible to define one. There's one for the relationship between `OptionT` and `Nested` though (as well as `EitherT ~&gt; Nested`). So getting back to your code, that might compile but you won't be able to provide the needed instance. Here are a few valid relationships that have instances of `Parallel`: - `Either ~&gt; Validated` - `List ~&gt; ZipList` - `Vector ~&gt; ZipVector` - `Stream ~&gt; ZipStream` - `IO ~&gt; IO.Par` (`cats-effect`) Here's an example using the `IO` instance of `Parallel` together with [cats-par](https://github.com/ChristopherDavenport/cats-par): ```scala import cats._ import cats.effect._ import cats.implicits._ import cats.temp.par._ import MyConf._ object Stuff extends IOApp { private val apps: List[App] = List(App("app1"), App("app2")) val program = new MyConf[IO].getAllConfigurations(apps) override def run(args: List[String]): IO[ExitCode] = program.flatMap(x =&gt; IO(println(x))).as(ExitCode.Success) } class MyConf[F[_]: Monad: Par] { private def getConfigs(app: App): F[Option[Configs]] = if (app.name == "app1") Configs(Map("my_key" -&gt; "my_value")).some.pure[F] else none[Configs].pure[F] def getAllConfigurations(applications: List[App]): F[List[(App, Option[Configs])]] = applications.parTraverse { app =&gt; getConfigs(app).map { cfg =&gt; (app, cfg) } } } object MyConf { case class App(name: String) case class Configs(kvMap: Map[String, String]) } ```
6 years of using Scala on 4 different projects. Scala is strong in data processing(Spark) and distributed computing(Akka). It's weak in general business applications and microservices. Kotlin is also going to eat a lot of Scala market shared in the next 4 years.
By the way, I just found this randomly today: https://stackoverflow.com/questions/1364361/how-to-write-a-proper-null-safe-coalescing-operator-in-scala Wouldn't it solve your needs? It's not that much syntax overhead: ?:("hel")(_ + "lo ")(_ * 2)(_ + "world")()
Yes, there's a way. Just define Option.apply[A](x: A|Null): Option[A] If you have a nullable value `x`, call `Option(x)`. And if you have a non-nullable value `x`, call `Some(x)`. Done.
All this sounds amazing! Really exited for Dotty!
I have been Scala developer, used Scala primarily for Spark, Kafka and Akka related projects, my new project I have been using Java, purposefully, as we had difficulties to find Scala skillset. Since the new project is a framework, we wish the other companies &amp; developers to use it to extend or accelerate their development effort. You will know how much it is painful to write code in Java, if you are from Scala language. Scala is far better in terms of features. &amp;#x200B; I think, Java shall give tough time for Scala as competition, especially their 6 months short cycle, means, they committed to get something out of door every 6 months, the LTS versions updated every 3 years
The `Parallel` typeclass is an abstraction over different possible "task types" that support running in parallel. You eventually have to pick a concrete "task type" to actually use (analogy: imagine you've decided to use a collection. You still have to pick a specific collection like list/vector/treeset/...). IMO for application code you're better off writing in terms of a specific task type (e.g. `IO`), and it only makes sense to use something like `F[_]: Applicative: Parallel` if you're writing library code that you want people to be able to reuse with different `F`s. (But there is a least-power argument for writing in terms of generic `F`). At the moment the only place where you're using the `F`-effect is via `.pure`, so you're not actually doing any effect management and not actually getting any benefit from all of the `F`-based code. Your current code would run just the same without the `F`s, and likely be simpler. Apologies if this is obvious, but I just want to emphasize that code that uses `pure` will never be parallel; you need to do something to make the calls to the AWS API genuinely async first, and only then can you run them in parallel - this is a common mistake I see when getting started. To get getAllConfigurations running in parallel you'll need to do two things: 1. make `getConfigs` (and possibly `getApps`) actually launch an async task rather than running on the current thread 2. Use either `parTraverse`/`parSequence` or a task-type-specific method to gather parallel results in `getAllConfigurations`. For 1., you would ideally use `Async#async` or a task-specific version like `IO.async` to adapt a callback-based method in the AWS API to return `F[_]: Async` or `IO`. If there is no such API then you can use `Concurrent#start` or `IO.start` to run the request on a new JVM fiber. Make sure you've got `getConfigs` actually working async (i.e. `getConfigs` should return a value without invoking the AWS API - the call to AWS should only happen when you call `unsafeRunSync()` or some such on that returned value) before trying to run multiple of them in parallel. Once you have that working you can use `parTraverse` or `parSequence` to combine the results of multiple `getConfigs` calls in parallel. If you're doing it right you should have no (or very few) `.pure` calls and nothing should happen at the AWS level when you call `getAllConfigurations` - you just get an `F` or `IO` value that represents a computation that you can execute in the future.
can we also have an `isNull` function in the standard library?
Thanks Gabriel. That works perfectly! And thanks for putting the time to write the complete example. It really helps. I sometimes feel like a PIA on the sub :p for asking so many dumb questions, but I always need to try something by myself after I read the docs, than make mistakes, ask questions and go back to the docs. This messy process helps me build a certain kind of intuition that helps me understand the docs more easily.
For these cases I wouldn't mind even using Option, the requirement for nulls and raw datatypes is performance (and hence, not using anything depending on generics), so in that case these kind of things would also be inadmissible. Something along the lines of A|Null would provide help from the compiler at no overhead. 
If these helper methods are made `@inline` and the right compiler options are turned on (or using Dotty's `inline`, which is guaranteed to inline), I would expect them to introduce no overhead.
chat with rooms
Derp, you are right, I was overthinking.
Calling an API and just logging stuff and then I directly got into scalajs-react
I have both Odersky and Wampler books. Don't read the impatient in my opinion. &amp;#x200B; If you are short of time read Wampler as it gets you productive faster. If you have more time read Odersky book as more detailed book it will take you longer to read. If you are really short of time consider this one: [https://underscore.io/books/essential-scala/](https://underscore.io/books/essential-scala/) &amp;#x200B; Your learning goal should be to use the above resources to learn the language but once you have you should plan to read: &amp;#x200B; [https://www.manning.com/books/functional-programming-in-scala](https://www.manning.com/books/functional-programming-in-scala) &amp;#x200B; You will find this a good resource as you learn: &amp;#x200B; [https://www.scala-exercises.org/](https://www.scala-exercises.org/) &amp;#x200B; Also come and chat to us on gitter if we can help you along your learning path: &amp;#x200B; [https://gitter.im/scala/scala](https://gitter.im/scala/scala) &amp;#x200B; &amp;#x200B;
Thank you! This is really helpful
[Mu](http://higherkindness.io/mu-services/) is an open source framework representing a purely-functional microservices-based architecture taking into account communications, serialization, reliability, protocols, and schema evolution compatibility. Check it out, play around, and as always, we're looking for new contributors of all levels! 
What it use for streaming? Http SSE? Thanks!
Streaming info is available here: [http://higherkindness.io/mu/streaming](http://higherkindness.io/mu/streaming). 
long story short: [fs2.stream](https://fs2.io/) and [monix.observable](https://monix.io/docs/2x/reactive/observable.html)
Sorry but I didn‚Äôt phrase my question well enough. I was referring to the network layer and which protocol was used, wether custom tcp/ http1/http2/grpc. 
Try this ;) [https://github.com/JetBrains/intellij-scala-bundle](https://github.com/JetBrains/intellij-scala-bundle)
Could you elaborate on that idea, please?
Mu uses gRPC ([https://grpc.io/about/](https://grpc.io/about/)) framework which provides bidirectional streaming with http/2 based transport.
[removed]
But limited types :(
Glad it helped! Please, keep the questions coming, there's always folks ready to help others and that's what this forum is all about :) I believe many of us go through the same process in order to learn more, keep it up!
Main author here! Curious to know what y'all think.
Woo PagerDuty!
I am always pleasantly surprised when people have positive feelings about a company that wakes you up at 3 AM :-).
Y'all are just the messenger! 
Do you want to summon trolls? Because this is how you summon trolls. 
It will have binary compatibility as well as source compatibility with a flag (iirc)
Extension Methods are a nice syntactic simplification of existing implicit defs. Though the parens before and after the name throw me off a bit. 
Why would you say it's weak in general business application and microservices? 
For general business applications and microservices there are languages that fit this market better, such as Go, Node, etc. Scala doesn't provide advantages here, but has its own drawbacks such as complexity, expensive and rare good developers, slow JVM startup.
How would this compare to Finagle/Finatra? I'm interested in the Kotlin use case, but at quick glance i didn't see anything in the github repo. Is there a design doc for Mu Kotlin?
Yep. I wonder what the syntax considerations where on that point. Kotlin's syntax is also quite nice, I would like it's scala-fied version: ``` // Dotty def (c: Circle) circumference: Double = c.radius * math.Pi * 2 // Kotlin fun Circle.circumference(): Double = this.radius * math.Pi * 2 // Scala-fied def Circle.circumference: Double = this.radius * math.Pi * 2 ``` I also wonder why the extended object can be given a name when `this` seems to fit well.
My guess for not using ‚Äòthis‚Äô is that the definition can be in the context of an object and therefore ‚Äòthis‚Äô should still refer to that object. 
See the discussion in https://github.com/lampepfl/dotty/pull/5114, and note that this might still change if there's consensus for something else.
Why not? It's enforced by the compiler. It's not a voluntary null check that you could forget to do.
Please stop reposting this.
Imo, the syntax for the infix extensions works out a bit better when both parameters are named.
Personally i prefer the proposed Dotty version, the syntax is more intuitive for me. And given Scala more generic mechanism Kotlin-like solution would result in conflict of symbols (\`this\` from the class and \`this\` from the parameter): \`\`\`scala trait SemiGroup\[T\] { def (x: T) combine(y: T): T } implicit object StringSemigroup extends SemiGroup\[String\] { def (x: String) combine (y: String): String = x.concat(y) [//this.concat](//this.concat)(y) would require shadowing this from the enclosing class def unit: String = "" } \`\`\` 
Hi! I was looking for a Functional Programming language to learn, and Scala actually has my attention because of its simmilarities(compatibility?) with Java, which I know how to use. Is scala a good language good to learn FP? Would you say Scala is popular right now? Is there a book/resource I can use to learn Scala with a Java background ? 
Personally, I like what Dotty's done except that I feel like it should maybe support a dot between `(c: Circle)` and `circumference`
1) Higher kinded types. Stronger and more powerful type system is like beauty on a woman- it's always better. Plain and simple. Everybody (who knows something) would agree. 2) This may be an unpopular opinion but, perceived sexiness. And academic-ness. If you're gonna be out all day busy working- beauty will not be much in the dark of the night, except for ego. Similarly, if you're not going to be actually needing and using HKTs, haskell is of no use, except for ego. Everybody you see using haskell silently are doing it for point 1. Everybody who claim they have moved to haskell are point 2. There, I said it. Bring on your pitchforks. 
I haven't touched Kotlin in 2 months, but i think there you could use `ClassName.this` to refer to "this" of specific class in hierarchy
I'd say Mu is just another solution to solve similar problems as you can do with Finagle+Finatra. Regarding Mu Kotlin, there is no design doc, but it follows the same architecture but it's based on Arrow Fx.
Syntax. God, I love the Haskell syntax.
&gt;IDL Type, where the available values are currently Avro , Protobuf , OpenAPI. How accurate is that statement ? Cause searching "openapi" or "swagger" in the github repo does not give any result. 
If you did actually need to do a null check, (maybe because of some java library) it can set off Scala style tools. You can get around that by doing ‚ÄòOption(someRef).isDefined‚Äô So even though we currently don‚Äôt use nulls but still have to check for nulls, we can still avoid ever using the null keyword explicitly.
Syntax for some things, like type classes, is more concise.
I use intellij and Scala, it works well. 
Scala IDE for Eclipse has been declining in quality for a while now. It's really a nuisance to deal with. IntelliJ is not without its warts, but IMO it is a lot easier to deal with than Eclipse for Scala projects.
I was using Eclipse for scala for a bit and it was fine. Then one day, I couldn't open projects any more. Any scala workspace, I would try to open the project and Eclipse would core dump on me. Couldn't even start it up. I tried debugging it several times before I gave up and went to intellij. I made sure to change it to eclipse hotkeys so I wouldn't have to retrain years of muscle memory. And so far, a year in, everything's fine. It was an easy change to do. So really, choose what you like, but my personal experience was hitting a heinous bug forcing me to swtich.
I was using ScalaIDE at a previous job, and it was terrible. Medium sized projects would take ages to build. I'm happier with IntelliJ now.
Everyone who has experience with Eclipse should use IntelliJ. 
IntelliJ is great. IntelliJ is good. All will join the hive mind of IntelliJ ;) Seriously though. IntelliJ is pretty great with scala support.
Intellij all the way. Pros (e.g. you don't need autocomplete, ...) also like vscode with metals.
IntelliJ for sure
IntelliJ hands down
&gt; I've been reading that ScalaIDE for Eclipse is not that great I've used ScalaIDE for years, and IntelliJ off and on during that time. ScalaIDE is pretty good: you get very good completion, ok refactoring, and good debugging. Unlike IntelliJ, it uses the actual Scala compiler, so you get fewer spurious red squiggles. The downside to ScalaIDE these days is that it's pretty much abandoned. Typesafe/Lightbend pulled the plug on development a while ago, and now one guy makes unofficial releases for new Eclipse versions. I'm sticking with one of the last official releases (4.6.1) for as long as I can, since it basically works pretty well. This is basically the same situation as with ENSIME, sadly. The new push in the Scala tooling world seems to be toward IDE-agnostic servers that implement LSP. Hopefully that will pan out in the future, but with those servers having a long way to go to get to feature parity with ScalaIDE and IntelliJ, I'm not holding my breath.
Unfortunately, in 2019, I think IntelliJ is the only game in town. Eclipse lost its way.
We are working on providing the generation of OpenAPI schemes automatically through the [Skeuomorph](https://github.com/higherkindness/skeuomorph). Although we go on a [good path](https://github.com/higherkindness/skeuomorph/tree/master/src/main/scala/higherkindness/skeuomorph/openapi), I think that `IDL Type, where the available values are currently Avro, Protobuf, OpenAPI` is a premature statement. Sorry @Baccata64 for the misunderstood.
I feel the exact same way. I was a Scala dev for the last 4 years. A little Spark but mostly normal web apps and services. You either overpay for the complexity of the work to get good Scala devs, or you get a horrible mess of a code base. I‚Äôve recently started a new job where I do similar work but all of our services are in Go. I certainly miss the power and FP tools Scala gives you, but I can‚Äôt deny that I feel Go suites this type of work much better. 
Yeah, this is Spark's fault. I love Spark, but it pulls in hundreds of dependencies which makes it difficult to play nice with others. It can be a real pain sometimes.
It feels like this dependency conflict problem can be automated. It's almost always the same steps for figuring which two dependencies include different versions of the faulty dependency.. Not sure why such tool doesn't exist. Maybe working with Apache Ivy or whatever it is is that hard.
If you're having classloader issues that has nothing to do with the compiler. That's a runtime concern. I wouldn't be surprised if Spark does a lot of classloading magic, so if it's broken you need to take that up with them.
Does the Spark project use Ivy as a package manager?
Half the time it's a runtime issue and the other half its a compiler issue. Still boggles my mind that this is acceptable. It almost makes me just want to start using Python glue code instead of a proper, production ready, pipeline. 
It's a real mess. I think it comes from the fact that Spark tries to do too many things. 
I think Maven/SBT somehow uses Ivy. I don't know exactly what Ivy does. But I can imagine a tool where: 1. it get the NoClassDef error as an input 2. Get the package and the JAR (probably using info produced by Ivy,) 3. Get other JARs that contain the same package/class name 4. Find our direct dependencies that depend on these JARs 5. Print on screen which direct dependencies cause the conflict.
Why does this never work though? Half the time the stacktrace yells at something unrelated or only partially related. It's like blaming bullet for killing someone when really it was the person shooting the gun. 
&gt; other half its a compiler issue I can tell you with certainty that it's not. 
scala issue\* Wrong word, but half the time it's a Scala version that breaks some other dependency, even if they're still in the same major versions (e.g. 2.11)
Well that might be because you're using maven, sbt will automatically select the right artifact cross compiled against the version of Scala you're using and will produce a list of dependencies evicted on demand.
That will never work there are far to many dynamic classloaders and bytecode instrumenting/generating projects out there ever work well. Anything that uses hibernate for example will break the tool you're suggesting.
Scala for Elclipse is abandonware at this point.
Man, life is just easier if you can start a hdinsight cluster (hdp) or databricks. Spark context just works. Pay for what you use. P.s no I dont work for databricks or hortonworks. 
Just out of curiosity, whats wrong with AWS glue as a proper production ready pipeline?
Really??? Hmm maybe I should experiment with it. Can you share an example of the eviction?
https://www.scala-sbt.org/release/docs/Library-Management.html#Conflict+Management
I think typebend stop funding the scalaide (eclipse) and it is no longer maintained. At least by the main developer. 
Ivy is a caching layer for package managers. Both maven and sbt use ivy. Spark is built with maven. 
`sbt -jvm-debug 9999` and then use intellij to remote attach to the sbt-launch.jar process. https://stackoverflow.com/questions/4150776/debugging-scala-code-with-simple-build-tool-sbt-and-intellij
I've try to do 100 times, but it cannot
I've try to do.... Please! But I just want to debug testFile- ScalaTest. Not debug Code
I believe I am the only remaining human using Eclipse for Scala development. That's since Martin Odersky moved to VS Code.
When I first switched to intellij about 8 years ago I did the same thing - mapped all the keys to eclipses. And I still retain them to this day. 
Please! I don't want to debug code, I just want to debug testClasses - ScalaTest
I've been using IntelliJ since last 2 years. It has good type support, with minor issues.
If you honestly can say ScalaIDE was "pretty good" I shudder to think how bad things would need to be before you can admit something is "pretty shit" really.
Use Intellij or vscode with metals (https://github.com/scalameta/metals) 
Thanks for clarifying. I got excited for a minute as you might have had a openapi =&gt; scala generator (none of the offers are there are truly satisfying). 
Dockerize
In maven you can use `mvn dependency:tree` and look for the "omitted for conflict with" lines. There's no specific support for Scala cross-compilation but you can detect transitive dependencies on multiple versions of Scala just like for any other library. You can also [use the enforcer plugin to check for dependency convergence](https://maven.apache.org/enforcer/enforcer-rules/dependencyConvergence.html).
&gt; It's almost always the same steps for figuring which two dependencies include different versions of the faulty dependency. Yeah, I do feel like a tool should be able to do this. What I tend to do is look for the class name where the error is and the class name that's top of the stack trace, use an `fc:` search on search.maven.org to find what packages they're from, and then something like `mvn dependency:tree -Dincludes=somegroup:someartifact` to see where the dependency on that comes from. So all the steps are automatable, it's just plumbing them together. Of course, the actually hard part is figuring out what set of versions to use that will be compatible with each other rather than just finding the two things that are incompatible.
I still use ScalaIDE because for all its faults (and they are many) it has error highlighting which actually works, and IntelliJ doesn't. I'd really like to be able to use all of IntelliJ's fancy features, but none of them makes up for this most basic feature being broken. So I'm afraid it's a case of picking your poison. Try both and see which you get on with better?
&gt; Is scala a good language to learn FP? Scala has a lot of compromises to its design in the interest of JVM compatibility, so I wouldn't normally recommend it as a first functional language. But if you have the Java background to know about e.g. type erasure then hopefully most of Scala's rough edges will make sense, and being able to use the same libraries/frameworks you're familiar with (at least initially), or even introduce a couple of Scala classes into an existing codebase, is a real advantage. IMO the best way to take advantage of that compatibility is to go slowly; start by writing "Java in Scala" and using the same libraries as you're used to even if they're not recommended for Scala. Then you can gradually work up: first use fewer semicolons, then more immutable values, then replacing loops with higher-order functions, then using case classes... and you can stay productive the whole time. That's what I did. (You can of course try to "dive in at the deep end" and use more Scala-idiomatic libraries etc. from day 1 - but IMO if you're doing that then you might be better off with a more traditional ML-family language that disallows e.g. `extends` inheritance.) &gt; Would you say Scala is popular right now? Yes and no. It's a mature language with a substantial library/tool ecosystem. It's used in some big-name systems. It's not as popular as Java or Python and probably never will be. It's no longer a hyped new language - there's less excitement than some younger languages, but by the same token Scala's current popularity is organic and driven by genuine technical merit. &gt; Is there a book/resource I can use to learn Scala with a Java background ? Most books and tutorials assume some level of Java; I don't have a personal recommendation (I learnt Scala on the job and by trial and error) but any of the ones listed in the sidebar should be fine.
I think Scala is a good language to learn FP, however I'd say any "FP" language is good to learn FP :D If you want to work with it, then Scala is a better choice than other FP languages, because it's the most used one among them. If you just want to learn it for fun then it might not be the best one, because of it's complicated type system (OOP/FP mix) and lot of features. I'd say the first choice to make is whether you want a static or a dynamic language. (Haskell, F#, OCaml, Scala - Static)(Clojure, Elixir - Dynamic) Scala is mainly used in 2 areas web backend and data analysis/science. But you can also use it for frontend (Scala.js) it's not too popular for this use case, but when I tried it, it felt much more pleasant than TypeScript or JavaScript. I'd check a course on Udemy or PluralSight. I'm sure there are Scala for Java devs courses on some of these sites. Also check out the side bar's **Free Books, Tutorials and Guides:** part. What's not mentioned there I found that Scala for the Impatient is pretty good too. I think the best is to collect a few books/articles about a topic (eg: Pattern Matching) and skim through all of them, and read the best one in detail. &amp;#x200B; &amp;#x200B;
I have a ssd and i like Eclipse.
I totally understand your frustration - managing dependencies with any Hadoop/Spark project is tough to get right. First make sure your dependency versions in your project match the ones on the cluster. You can do this by finding the actual jars on the server and looking at the filenames. If you are using a distribution like Hortonworks or Cloudera they have their own special versions so watch out for that. Second, make sure you use the maven shade plugin to package your jar for deployment to the cluster. This will create an Uber jar and allow you to relocate packages that have version conflicts. HTH
Is this implicit val IntSemigroup: Semigroup[Int] = new { def (x: Int) combine (y: Int): Int = x + y } preferred over implicit object IntSemigroup extends Semigroup[Int] { def (x: Int) combine (y: Int): Int = x + y } ? I would fine this second version more intuitive and nicer, `new { }` leaves a bad taste IMO.
I'm not familiar with dynamic class loaders and bytecode instrumenting/generating. Could you elaborate more? How does dynamic class loaders look like? and how does bytecode instrumenting/generating look like? More importantly, why is it impossible to track these kinds of dependencies? I imagine that it loads jar file directly in Java code like `load("something/somelib-1.2.2.jar")`, so it is impossible to track. Is that accurate? I'd like to have a high level view of it. Thank you.
Great time to point out tips on how to ask for help: - what have you tried? - what was your expectation? - what is actually happening? - attach code and logs Not much someone can do with your current question other than suggest "read the docs"
Shrug. I've used IntelliJ off and on during the same time, on a team of IntelliJ and VSCode users. ScalaIDE is on par with IntelliJ on major features: completion, red squiggles (better actually), running tests, and debugging. That's pretty good to me. 
ugh... noticed this only after reading your comment... so ugly :( is it really creating a "structural type" object which then needs reflection to access the `combine` member???
Thanks for the answer! I'll check the siderbar's content right now
No it's not a structural type. In Dotty right now `val x: X = new { ... }` is just a shorthand for `val x: X = new X { ... }`.
Agreed. Spark is both an API and an execution context. Not very good at the latter. Hence products like EMR that are pretty much battery included distributions.
And I appreciate everyone that use open source sw. 
VSCODE + Metals. Use IntelliJ only if you have a 32GB Laptop. 
Why not use Maven from the IDE? For IntelliJ, you point at the project, let it recognize it is a Maven project, and you are off to the races. Everyone should get the same dependencies. I haven't used Eclipse for a while, but it worked very well with Maven back when I used it. mvn dependency:tree is your friend. That will go a long way to showing you where dependencies are conflicting. This usually happens when a library has been moved so that the groupId is no longer the same, and different libraries call out a dependency on both versions. This affected a lot of libraries when codehaus went away, including common things like Jackson. Scala is a deep and complex language. Java is a beast. Spark has all the markings of enterprise software of yesteryear. If you are using Cloudera or Hortonworks, which also pull in an even bigger set of conflicting dependencies, you really need a Java expert to help you sort it all out. I have spent literally months of my time over the last several years dealing with these problems, and they are not your fault - just your responsibility. :)
I work better in CLI. I have my .sh scripts written out and life is good. If you start putting things behind IDE buttons, stuff starts breaking magically. I once had Eclipse override my Java version specified in a pom.xml (and this was even when I ran the project through CLI as well) and so I just avoid the IDE in general. I'll try mvn dependency:tree in the future. I should really get good at maven. I think it's a lot more powerful than just running mvn package. &amp;#x200B;
Welcome to JVM
WTH did i just read. Use IDE to develop, use command line to build and run
That's what I do. Some people use the mvn provided with Intellij or Eclipse to build and manage projects. I have a friend that uses maven only through the buttons. That means going in and creating run tasks that execute mvn with specific flags. Personally, that's very tedious for me. 
I agree, i do not trust an IDE for that, the same for GIT
The IDE will do very very weird shit behind your back. It managed to override pom settings! I don't even know how it did that but it did! IDE is great for the linting, the autocompletion, being able to explore class code, et cetera. It's not great for actually building your projects though.
Nothing fancy really, I just created [scala-md-tag](https://github.com/OneWebPro/scala-md-tag). Simple library to generate markdown in safe way - I needed automatically way of generating markdown tables for my next project - that will be shared here soon.
In my opinion, it's the JVM's fault that there isn't a good working way to do that, not Spark's.
Who could have known that one day Scala may have syntax that actually justifies presence of the `new` keyword in the language (i am not criticizing Scala but initial design of Java that included `new` keyword), `new { ... }` (because you could not do that without special keyword).
&gt;ctually spelled delete
Runtime performance for functional style.
That first one java.lang.NoClassDefFoundError: scala/Function1 may be due to issues an anonamous function. Try swapping it for an explicitly named one and see if it helps
oh, but `new { ... }` *does* create "structural type" objects other times, no? or does Dotty not support them? btw `val x = new X { ... }` works too, right? 
`new { ... }` can be used for structural type indeed, but in Dotty unlike Scala 2 this requires to give the type explicitly, i.e. in `val x = new { val foo: Int = 1 }` , `x` has type `AnyRef`, if you want a structural type you need to write `val x: { val foo: Int } = new { val foo: Int = 1 }`, this was done to avoid accidental usages of structural types. Also note that structural types can now be used without Java reflection: http://dotty.epfl.ch/docs/reference/changed-features/structural-types.html &gt; val x = new X { ... } works too, right ? Yes, but for implicit val and def an explicit type is now required, so you can write `implicit val x: X = new X { ... }` or `implicit val x: X = new { ... }` but not `implicit val x = new X { ... }`. (also note that all of this might still change)
Hopefully, neither and we will have instead: ``` instance of SemiGroup[Int] { def (x: Int) combine (y: Int): Int = x + y } ``` See https://github.com/lampepfl/dotty/pull/5458 
Why can't the same technique be applied to autocomplete?
Is there anyway to integrate running the project with Metals? Right now im running sbt in a separate console in vscode.
You can test and run with the Bloop command-line interface https://scalacenter.github.io/bloop/. The compilation is shared between the Bloop command-line interface and Metals.
Can you elaborate?
So we have to query the presentation compiler for the type of a symbol under point, then construct a list of completions for that type, then filter by the current letters of the type. This is expensive, and like navigation to definition, changes as the program is edited. It's kind of slow currently in ensime, and often times out. I'm suggesting that we can build an index of completions using a similar technique to the jump to source. Even if we have to use the batching compiler, with ScalaMeta and various phase outouts / reflection we should be able to get the the type at point, and scan the sources for methods and functions applicable to the type and provide completion to the user, and keep it utd in the background as the user continues to edit. You would only need to initially index the types referenced in open buffers. That can be done in the background, and a notification can tell the user when completion is available. If we have the presentation compiler we can make the query to it if necessary, but keep the index in memory utd. I'm sure we could keep the memory footprint low and the search fast enough to make it at least as usable as intellij's autocomplete, and we could also implement auto import management with it. With dotty we would get completions from the normal compiler. Note, I have looked at pclod(spelling?) but not scalametals internals, and I read your blog post. I have no idea if bloop has anything that could help us either. Correct my incorrect assumptions, but this is just an idea. 
I'm mainly a Python/C++ programmer. I've been looking around for a language that's "like Python" but with static typing. Right now it's down to Scala or Kotlin. Any tips?
Impressive results, especially with JMH! What is the trade-off? Do they work with Scala 2.12?
With Scala 2.12 the results are even better, but marginally. Except for Array, which is much faster in 2.13 
Any advantage for spark ? 
I do not know much about Spark, so would not speculate 
Ok thanks 
Is this what you want? @ import $ivy.`com.lihaoyi::requests:0.1.3` import $ivy.$ @ requests.get("https://en.wikipedia.org/wiki/2000_Baltimore_Ravens_season") res1: requests.Response = ... @ res1.text.contains("where they defeated") res3: Boolean = true @ val idx = res1.text.indexOf("where they defeated"); res1.text.slice(idx - 100, idx + 100) idx: Int = 11864 res4_1: String = "Tampa, Florida&lt;/a&gt; for &lt;a href=\"/wiki/Super_Bowl_XXXV\" title=\"Super Bowl XXXV\"&gt;Super Bowl XXXV&lt;/a&gt;, where they defeated the &lt;a href=\"/wiki/New_York_Giants\" title=\"New York Giants\"&gt;New York Giants&lt;/a&gt;," 
Almost, take 20 more chars to see if that unicode dash character in the score 34-7 appears as a '?' or the actual dash character.
Check Go lang interfaces. Most probably that's what influenced them
I think a better comparison would be against list.toIterable.filter(_ % 2 == 0).flatMap(_ =&gt; list).map(_ / 2L).sum
Scala has more features. You decide if that's good or bad for you. (Implicit parameters and conversions, macros, higher kinder types, pattern matching is a bit more advanced, type bounds) not sure if there are other features what are in Scala not not in Kotlin. Kotlin has a built-in way of handling nulls Scala uses Option for that. Kotlin has smoother integration with Java. Scala is more popular in data science, Kotlin is more popular on Android. The main focus of both languages is web backend. I think Scala is still more popular there. Not sure about Kotlin, but Scala has good repls, what you probably used in Python. You can also run Scala as a script, but it'll have a slow startup time compared to Python. (scala -nc &lt;source&gt; or with ammonite)
Not so! There are at least a handful of us remaining. While the project has been abandoned, it still runs fine on 2.12, and may even have life for 2.13 -- see the scala-ide gitter group. Hoping to get to 2.14/3.0 before switching to VS Code.
Good point. I have just added a [quick test](http://scalqa.org/doc-stream/Performance/Iterator_Benchmark.html) for the Iterator. Still 200-300% gain for the above. Try running the test code yourself, you will be amazed how easy and informative it is
Good tooling support for native code.
would you mind explaining why using `toIterable` yields faster code ?
[Milyardo](https://www.reddit.com/user/Milyardo) misspoke, he meant `toIterator`. `toIterable` clearly should just return List itself. `toIterator` is often used to do lazy processing by people who would not bother calling `.view`
Are you using IntelliJ? You can right click on the file your test is written in and choose `Debug &lt;your test name&gt;`
I don't know yet what the main bottlenecks for completions are so it's hard to say. The presentation compiler only returns completions for symbols that are already in scope, so we might want to use something similar to the fuzzy symbol search described in this post if we want to support completions with auto-import for symbols that are not in scope (like IntelliJ does).
Just started on a course to learn scala. Which IDE to use ? Both eclipse based scala ide and IntelliJ seem to have some bugs. Worksheets in scala ide dont show compilation errors. IntelliJ seems to be too damn slow. Worksheet evaluations hang.
Would you be so kind and show us the code you wrote, so we can try to spot your error? It looks like an encoding problem (since the text in the article is "34‚Äì7"), but without actually knowing what you are doing we are guessing
Just use IntelliJ, that's what the vast majority of the community is using at this point.
Just use IntelliJ, that's what the vast majority of the community is using at this point.
Still working on [Webdocs.io](https://webdocs.io), a service which turns git repos into easy to edit wikis.
thanks for such a detailed explanation! 
FWIW, the gold standard for benchmarking on the JVM is jmh: http://tutorials.jenkov.com/java-performance/jmh.html I highly recommend using that instead of your own homebrewed framework for benchmarking. There's an sbt plugin to make this easier: https://github.com/ktoso/sbt-jmh
Haven't really investigated but what about a [cuckoo filter](https://www.cs.cmu.edu/~dga/papers/cuckoo-conext2014.pdf) though ?
And here I am as a C++ guy amazed that my CI gets back to me in a half hour :(
Looking good. What about Scala.js compatibility? Is there a reason other than lack of time why it isn't published to public repository?
So few with compilation times over 8min,and here I am after almost 5 years barely seeing any cold project compilation taking less than 10min. I can only conclude that it's all toy projects and that's why the frustrations are not so wildly felt across the community. At least incremental compile times are much much better than some years ago and seems few people get hurt by huge times there. 
It can allow to don't recalculate of a whole index, ex. during refactoring... Also, there are the Morton filter that introduces several key improvements to Cuckoo filters: http://www.vldb.org/pvldb/vol11/p1041-breslow.pdf
Pardon the cliche suggestion, but perhaps you should break down that huge thing into smaller services. Of course I'm assuming it's a web service, but that's what most people write nowadays... The company I work definitely isn't a toy project and yet our main API compiles in 62s cold on my 6 core, 12 thread MacBook Pro.
I've not heard of Kudu; my instinct would be to use something a bit more established (e.g. Cassandra). I take it the Hadoop cluster exists already? Hadoop tends to be quite batch-processing-aligned whereas Kafka is more streaming-oriented, which might be a bit of a mismatch unless you're going to use something like spark-streaming.
[msatala](https://www.reddit.com/user/msatala), thanks, now I know my plans for next few days: * Test on Scala.js (primitive specialization will be gone, but the rest is interesting) * Publish to Maven repository (unless there are better ideas) 
There are various Scala learning resources in the sidebar. I don't use Play myself so can't recommend anything Play-specific, but personally I'd try doing a few exercises in basic Scala (maybe something like Project Euler) before focusing on the framework. (But maybe things work differently for you).
There's probably some bias towards open-source project in survey results. My personal experience is that most OSS libraries/projects are relatively small compared to commercial projects. Another angle is that a CI build includes dependency resolution, Sbt startup, source generators, etc. which can take a considerable amount of time.
Is there any idea on the size of the cide bases that are being compiled herein? I feel like just a straight-up time / codebase isn't a very good metric but perhaps something like a seconds / SLOC is more indicative.
Full disclosure: Triplequote founder here. The Scala compiler is single-threaded, so new hardware won't help that much. Sure, there's some work that happens on the other threads, mainly due to the JVM using them for GC or JIT compilation, but the Scala compiler itself won't do it. In fact, there's practically no difference between my colleagues' MacBookPro 2013 and mine (2016) in Scala compilation time. That's why we built Hydra.
Don't go Play too early. Better use Scalatra 
Worry not, C++20 modules are coming :D Should speed it up a bit.
Here's a thread from a year ago that points to some resources: [https://www.reddit.com/r/scala/comments/6rtwal/where\_to\_find\_a\_comprehensive\_resource\_on\_play\_26x/](https://www.reddit.com/r/scala/comments/6rtwal/where_to_find_a_comprehensive_resource_on_play_26x/) . &amp;#x200B; Are you building REST APIs or a full-blown UI using Play (using templates)?
The pitch is really appealing. I would love it if this project got a fair plausibility assessment from involved parties (Lightbend) as it would really be the benefit of everyone if it succeeded.
Project size wasn't part of the survey, the intention was mostly to understand how big is the "pain" and what are some typical values in Scala compilation times. I agree that compilation speed across projects is an interesting metric. In my talk at ScalaSphere I presented compilation speed of a few different Scala "styles" (it varies a lot). I don't know if the talk was recorded, but scroll down to the bottom of this post and you'll find the slides (the table I'm talking about is towards the end): [https://www.triplequote.com/blog/5-things-you-need-to-know-about-scala-compilation/](https://www.triplequote.com/blog/5-things-you-need-to-know-about-scala-compilation/)
For the starting point, go with the in-house examples: * [Hello World Tutorial](https://www.playframework.com/documentation/2.6.x/HelloWorldTutorial) * [Play Framework Video Tutorials](https://www.youtube.com/playlist?list=PLYPFxrXyK0Bx9SBkNhJr1e2-NlIq4E7ED) For more complex stuff: * [Let's Play Series](http://objects-and-lambdas.com/tag/play-framework.html) 
**Scala.js compatibility** I deployed *Stream* code to Scala.js and to make it compile, I had to comment out few java API calls. The project created a 2.5Mb .js file and I ran simple tests for: \* [List](http://scalqa.org/doc-stream/Performance/List_Benchmark/Scala_js.html) \* [Array](http://scalqa.org/doc-stream/Performance/Array_Benchmark/Scala_js.html) The results are encouraging: it was easy, fun, and optimizations do work. Even though the *Array* slowed down in two cases, I will figure this out Bottom line, I plan to support Scala.js in next release &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
You can always count on the committee messing things up tho; https://www.reddit.com/r/cpp/comments/akihlv/c_modules_might_be_deadonarrival
I learned Scala and the Play framework from Essential Scala and Essential Play from Underscore, both available for free. I can only recommend these sources, as well as their Essential Slick book, if you want to get into database management with Slick. 
Scala is way better at making money than Haskell.
How is this related to https://github.com/twitter/rsc ?
Unrelated. Different people, different approaches.
https://github.com/aishfenton/Argus generates case classes from a schema along with the [circe](https://github.com/circe/circe) encoders/decoders 
I wouldn't say so, in Eugene's own words: &gt; We used the findings of Kentucky Mule to build our typechecker, so Greg's work was a direct inspiration for our project. Source: [https://github.com/twitter/rsc/blob/master/docs/relatedwork.md](https://github.com/twitter/rsc/blob/master/docs/relatedwork.md)
Uh. Must have missed that part. I stand corrected.
What we've been doing with our younger ones is to follow the 2 coursera courses, then we do somekind of mini project/exercise so they can consolidate that knowledge and afterwards following I think the official play frameworks docs. At this point looking at some example play apps around, trying them, modify, etc. After this we throw a little more difficult problem to solve with play. So if you are at the beginning I would strongly recommend to not try and learn scala and play at the same time and go at your own pace, starting by the coursera things. My 2 cents.
On-site or remote?
Salary scale?
Say hi when you are open to contractors.
Throw away the crutches of a multi-paradigm language and learn a purely functional one.
So do you mean I should learn languages like Haskell?
So, don't use var. 
I believe that‚Äôs what he means. 
I see. But it is easier to use var and harder to think about a functional solution. I want to practice on that.
Could you recommend some project I can work on to learn Haskell?
Imperative code was hard when you first learned it too. You've just built over time an arsenal of techniques for how to solve problems imperatively, and you don't have that for FP yet. You see a certain situation and immediately know what to do, with mutable variables. You couldn't do that in your first programs, you learned them from practice. If you try to figure it out with immutability and referential transparency, you will know how to do it next time that situation arises, and then your FP arsenal grows just like your imperative one did. FP is not harder, you just arent used to it and don't know the patterns yet.
It's insightful. Thank you very much.
This blog post may help you: http://www.lihaoyi.com/post/WhatsFunctionalProgrammingAllAbout.html 
You essentially need to code as if var does not exist. I can't recall the last time I used one. When exactly do you think you need to use it? For 'mutating' use copy or if the structure is deep use a lens. Instead of iterating use map, flatmap, etc. I suggest you do try something like the red book.
Practice, practice, practice. What helped me was to think through a new problem domain and to try to code solutions to problems in that domain in a purely functional fashion. When you think you need to use mutability, don't. Force yourself to think of an immutable solution. Get used to \`.map\` and \`.flatMap\`, for-comprehensions, type guards, and to abstract over context; these are your useful tools. I try to think less in terms of programming a computer as I used to, and more in the frame of, "how do I describe this problem in terms of the functional algebras I understand?"
Very helpful. Thanh you very much.
Dick Wall from http://www.escalatesoft.com was very good (this was in San Francisco). Ping me if you have questions.
Seem like it will take some time to master this kind of thinking.
Actually if you break a project down into multiple parallel modules, SBT will compile those modules in parallel. While it may not be as good as the hydra compiler, it sure helps a lot in a few cases
Good point. Maven can do the same, and probably most other build tools by now. It‚Äôs really good when it works. The issue is that you need to break down your project into meaningful and relatively equally-sized modules, while keeping a wide, rather than deep, dependency tree. Dependency resolution goes really high as well, at least in Sbt (which may be incidental rather than fundamental). 
Think of your code achieving the end goal via a series of transforms. You start off with some piece of data, which you then transform into a new piece of data. The old piece of data hasn't been changed (immutable); you're just creating something new. Now you transform this new piece of data again, and so on until you reach the end. Your basic transforming functions are filter, map, and reduce. So, for example, you want to count how many people live in coastal states. List of all states =&gt; filter by states that are on the coast =&gt; List of all states on the coast =&gt; map to the population of each state =&gt; List of population numbers =&gt; reduce to a single number by adding =&gt; Total population 
I suggest, as a start, that you put one for the exercises you did l, and are not pleased with the result, in a gist, share it here, so people can give suggestions, and then you can iterate over it.
\- prefer small functions over large ones \- don't hesitate to pass functions to functions e.g. consider 'def fun(age: String =&gt; Int)' over \`def fun(ages: Map\[String, Int\])\` \- don't hesitate to use type parameters if appropriate e.g. 'def fun\[Person\](age: Person =&gt; Int)' \- prefer immutable composition to mutable state, e.g. \`def Group.addStudent(student: Student): Group)\` instead of \`def Group.addStudent(student: Student): Unit)\` \- know your \`map\`, \`flatMap\`, \`fold\`, \`sequence\`,\`traverse\` on collections and monads. They are very useful. \- \`FP in Scala\` book is already deprecated. For monadic approach look at "Tagless Final" and documentation for [https://typelevel.org/cats/](https://typelevel.org/cats/) &amp;#x200B; And, of course, practice. It is the only way to understand.
Why is ‚ÄòFP in Scala‚Äô book deprecated?
Must be getting bored working at Stripe. He mentions two other full-time engineers, left unnamed, that would work with him on the project. I'll bite, Paul Phillips (everyone knows who he is and his background) and Dmitry Petrashko (major contributor to Dotty whole doing his doctorate at EPFL). Probably the latter more than the former though, PP seems to have fallen off the map AFAICT. Regardless, it would be amazing to see Kentucky Mule research merged into Scala 3.0. Imagine 5X compilation speed up, that would be quite the game changer when combined with the language ship-righting effort that is Dotty. Deep pockets I command the, open wide! 
Thank you for the advice. In my company, there are some people who can easily configure an Hadoop cluster with Kafka and Kudu, so we have less problems in configuring.
No, that‚Äôs just a dumb statement. Tagless Final is just a way of doing things that became popular in recent years, and some would argue that for some use cases free monads still outweigh tagless final. The techniques described in the red book are still valid. This being said, the red book is not what I would recommend to learn Scala, as it‚Äôs a bit dense and covers way more than you need as an intro to Scala or FP.
I just can't help myself, but I burst into laughing at that name. Says a lot about me I guess
You said you've read FP in Scala, I suggest you also do ALL the exercises in that book.
What's wrong with crutches?
Hi, looks amazing, is the final cpu usage about the same using \`all\` ?
They give you an easy way out. Why learn how to write a `tailrec` function if you can just give up and use a `var` instead? Why bother with getting used to `Either[TypeCheckedSealedError, T]` when you can just throw `RuntimeException` and catch it somewhere up the stack?
&gt; Instead of iterating use map, flatmap This is a very strong drug. This is precisely how i got dragged into FP...
Dick Wall is one of the best teachers out there, I took some of its courses and I loved the whole lectures, there were too many details, I've learned a lot, kudus for Escalates. Despite what others say about him or make fun of his name, I have a lot of respect for Dick Wall. I wish you much success for your journey learning Scala.
`sealed trait MyTypedError extends Throwable/Exception` - yay/nay (the "extends" part)? I lean "nay", as throwing exceptions is not FP way, but i tend to see that pattern disturbingly often in various articles and examples, including ones about some deep FP stuff like Free or tagless final. Is this actually common thing to do or just something people drag from the Java background?
I am really impressed by the quality of the work of the author! \&gt; Scala has academic roots and, sadly, academia doesn‚Äôt see compiler performance to be a fruitful research area I personnaly think scala need better performance more than a more sound type system which is the selling point of dotty. A few subset of scala user need perfectly sound type system, but 99% of scala user want better compiler optimization. I fear dotty is another instance of the lack of pragmatism of the research world. &amp;#x200B; I'd like to hear what /u/Odersky would say about this work.
I would happily accept even slower compilation if it meant more maintainable code and lower defect rates. There are parts of dotty that I'm skeptical about (inclusive union types; another "simplified" effect system) but certainly e.g. initialization order bugs are a real issue in today's Scala that has hit me in production. Thankfully it sounds like there's no need to choose between the two.
The question I'd ask is what the caller can usefully do about your error condition. If it's a condition that can reasonably be handled (e.g. bad user input for which you can display an appropriate message) then returning it in a type-safe way (`Either`) makes sense. If it's a system error for which all they can reasonably do is retry or report, then an exception makes sense. I'm not sure I'd even bother with a custom subtype in that case (if the error condition is such that the calling code can handle it the same way it handles any other `RuntimeException`), but sometimes you might be able to include some extra useful diagnostic information.
How many lines of code?
Yeah, i guess i see the point, but then what's the point of using `selaed trait` and not just subclassing Exception directly?
During testing with 1 thread it always works 100% Trying to answer your question, as I understand it, I ran the following code in JVM Monitor `var v = 0L` `def asNative100 = for (i &lt;- 1 to 100) v += list .filter(_ % 2 == 0).flatMap(_ =&gt; list).map(_ / 2L).sum` `def asStream100 = for (i &lt;- 1 to 100) v += list.all.filter(_ % 2 == 0).flatMap(_ =&gt; list).map(_ / 2L).sum` CPU Usage: * `asNative100` = 16615ms * `asStream100` *= 868ms* &amp;#x200B;
&gt;https://www.reddit.com/r/scala/comments/6rtwal/where\_to\_find\_a\_comprehensive\_resource\_on\_play\_26x/ Mostly building rest api's
The Dotty plugin for vscode ([instructions here on how to use it](http://dotty.epfl.ch/docs/usage/ide-support.html)) has a worksheet mode: http://dotty.epfl.ch/docs/usage/worksheet-mode.html I'm not aware of anything Scala 2 specific. Perhaps if you combine https://github.com/almond-sh/almond with one of the extensions to run Jupyter in vscode such as https://github.com/pavanagrawal123/VSNotebooks ?
&gt; I fear dotty is another instance of the lack of pragmatism of the research world. I disagree, but I'm biased :). We spend an inordinate amount of time thinking about how to make Dotty a great production compiler and not just an academic exercise. Part of that is focusing on developer experience such as [producing nicer error messages](https://www.scala-lang.org/blog/2016/10/14/dotty-errors.html) but performance is also something we always try to keep in mind which is why we [run benchmarks on every merged PR](http://dotty-bench.epfl.ch/). Getting significantly better performance than scalac has been challenging so far (especially since scalac itself keeps getting steadily faster !), but I'm hoping parallelization might save us, I implemented a prototype here I intend to eventually finish: https://github.com/lampepfl/dotty/pull/4767 And of course we're keeping an eye on other projects such as kentucky mule.
If you see a person with crutches in the physical world do you think they're just taking an easy way out?
&gt; inclusive union types What is the difference between inclusive union types and regular union types? 
I think I have been too extrem in my previous comment. I never thought dotty wasn't created with production usage in mind. I just suspected that they didn't tryied to address one of the biggest problem of scala : compilation speed, and instead focused on academic issues.
&gt; Dick Wall is one of the best teachers out there What do you think makes him one of the best? What distinguishes and separates him as a great teacher? &gt; too many details Is this a knock?
It suggests pattern-matching on `MyTypedError`. I don't know if I agree with that (IMO if you can handle all the cases in any reasonable sense then it should be an `Either` rather than a thrown exception) but I guess for a library it's a way of giving callers a choice about how they handle it.
"Union types" usually means inclusive union types, I just wanted to draw a very clear distinction between them and sum types (AKA disjoint union types).
The main point of union types in Dotty is not as a programmer convenience (although they're useful when micro-optimizing code to avoid boxes, and potentially when making facades to JS code, hence the existence of https://github.com/scala-js/scala-js/blob/master/library/src/main/scala/scala/scalajs/js/Union.scala). Instead union were added to Dotty to make sure the least-upper-bounds of two types can never be infinite (see section 1 of http://lampwww.epfl.ch/~amin/dot/fool.pdf for an example). Scala 2 has to approximate these infinite lubs which can be slow and can produce some big ugly types.
Dick Wall was on Scala path dev since the earliest years of Scala journey, he's using it professionally and he's well known in the community, Confs, Talks and Open Source projects... &amp;#x200B; I've attended some of his classes, I can tell you, for me who have already 2 years Scala professional dev, I've gained a lot of hidden knowledge and great experience dealing with advance and complicated subjects. &amp;#x200B; The workshops and lectures are well structured and well designed for all levels, easy to follow. With "too many details" I meant there is plenty of explanations that you find subtle or taken for granted. 
Just use Ammonite shell and write the script in VSCode. 
Want to think functionally in Scala? Follow John De Goes on Twitter.
I used to teach this course a while back and the curriculum didn't change much. I can't speak from the perspective of a trainee, but it truly is an advanced course. It assumes you code fluently in Scala and jumps into advanced topics quite rapidly. Almost every time there were a couple of people who didn't have much experience in Scala, but were otherwise senior devs. They had trouble keeping up. To get the most value you/your team should prepare a few discussion points based on your project. The course doesn't touch any library or framework so this helps bring those concepts closer to your world.
thanks! that's actually what we're looking for. We have a good knowledge of Scala, but it would be great to be able to get a (faster) intro to the advanced topics.
thanks for the suggestion! I'll take a look for sure
&gt; I see. But it is easier to use var and harder to think about a functional solution. I want to practice on that. The syntax highlighting in ScalaIDE (I know, very unfashionable) makes `var`s a very ugly color, and `val`s a nice, gentle blue. It helped me a lot.
These confused the hell out of me when I first started writing scala trying to get around array erasure. Can't say I'm gonna miss them 
thanks a lot &amp;#x200B; cant wait for maven release (!) &amp;#x200B; Also something is wrong here [http://scalqa.org/doc-stream/scalqa/index.html](http://scalqa.org/doc-stream/scalqa/index.html) chrome ask me to translate (Portuguese)
Working on Maven... Apparently word "scalqa" sounds Portuguese for Google, it will learn :) Thanks 
&gt; Paul Phillips Does he still program/do Scala/do software?
Not a project, but a lot of good materials: https://stackoverflow.com/questions/1012573/getting-started-with-haskell
&gt; new hardware won't help that much It really depends. I got 2-3x compilation speedup switching from 2012 Macbook Pro to a 2018 Mac Mini. More (32GB) RAM and a faster SSD drive probably contributed to this as much as a faster CPU.
My past employer paid for a different Lightbend Scala / Akka course. Definitely recommend, they covered the promised topics well, and engaged the developers with good discussion and hands on exercises.
I have done some of them. Certainly will do more.
A lot of tutorials to work with. Thank you.
If it‚Äôs Europe: Heiko Seeberger. Although, I‚Äôm not sure whether he‚Äôs doing those trainings anymore. Taken both Scala and Akka training. Twice! Very well prepared guy for long running training sessions that take up to 7 hours per day.
Seems like he may have retired? Github activity is minimal, Twitter account gone, rarely if ever shows up on Reddit, HN, Gitter, etc. Good for him if he's well off enough to get out of the game well before normal retirement age. Then again, he does keep things "lively" in the Scala community, would be nice to see him back working on Scala in anger on a project like Kentucky Mule. 
It's not that we're not trying to improve compilation speed, it's just that it's really hard! As an example, here's a PR doing some non-trivial analysis: https://github.com/lampepfl/dotty/pull/5748, all this work improved some compilation benchmarks by a few percent, if we repeat this many time then we get significant performance improvements, but that requires time and effort (and if we spent all our time working on performance we would never ship a working compiler!).
Had never really counted. Interesting metric. Just used tokei right now, here 3 bigger projects I've worked on last few years. ------------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks ------------------------------------------------------------------------------------- Scala 640 82808 68587 449 13772 Scala 2063 185925 153285 5207 27433 Scala 401 58958 48206 2676 8076 
Given these numbers, compilation times of &gt; 10 minutes are unexpected. Are you perhaps using macros in your project (e.g., through Shapeless) ? To compare, the sources of the dotty compiler run through tokei give me: -------------------------------------------------------------------------------------------------------------------- Language Files Lines Code Comments Blanks -------------------------------------------------------------------------------------------------------------------- Java 3 474 387 13 74 Scala 398 110300 77448 19096 13756 -------------------------------------------------------------------------------------------------------------------- Total 401 110774 77835 19109 13830 -------------------------------------------------------------------------------------------------------------------- And compiling those using Scala 2.12.8 on a cold JVM (that is, I start sbt, then compile the project) takes *47 seconds* on my (admittedly beefy) laptop. If I do the same clean compilation a few time using the same instance of sbt to let the JVM warm up, this goes down to *20 seconds*. The Scala Center has written a tool to profile the compiler, this might help you figure out where all the time is spent (though just using something like VisualVM might already give you a rough idea): https://www.scala-lang.org/blog/2018/06/04/scalac-profiling.html
I believe everyone gets shapeless in their projects wanting or not, since eventually something pulls it. We have a lot of circe so that is for sure a major point. Just timed it on my machine: Command being timed: "sbt compile" User time (seconds): 473.78 System time (seconds): 4.48 Percent of CPU this job got: 213% Elapsed (wall clock) time (h:mm:ss or m:ss): 3:43.95 Definitely not fast. What hurts most is the CI compile times, there it really is always cold boot not matter what one wants. On local machines one has to actively make sure to not exit sbt accidently, but yeah, when it's warm is around or less than one minute. Could eventually look into some of those tools and see if there's any really obvious low hanging fruit to change.
circe-generic can be slow, I'm not an expert on how to deal with that but maybe have a look at https://github.com/circe/circe-derivation
&gt; Scala in anger I'll make you faster and leaner, goddammit!
GBP?
Most cost-effective: [https://www.scalacourses.com](https://www.scalacourses.com) (true, I am biased, since I built that site and recorded most of the material on it). \-- Mike Slinn
The name is fine
"sbt compile" includes more than just compilation time. For instance, tasks such as source formatting, source generation, assets compilation, dependency resolution, and others may be playing a considerable role in the reported total time. We will soon be publishing an article with a few tips that can help optimize your sbt compile/build time, as we have a lot of experience on the topic. &amp;#x200B; My experience is that compilation times in Scala are unpredictable, and a single line change can cause a significant compilation time deterioration. What I'm implying is that LoC is not a good enough metric to correlate with compilation time. You can see an example of this in this article from Zalando: [https://jobs.zalando.com/tech/blog/achieving-3.2x-faster-scala-compile-time](https://jobs.zalando.com/tech/blog/achieving-3.2x-faster-scala-compile-time) (initially, 90 seconds compilation time for 5000 LoC) &amp;#x200B; If it is of any help, we (Triplequote) would love to work with you. It's our mission to make Scala compilation as fast as Java, and our parallel Scala compiler, Hydra, can deliver immediate gains. Switching to Hydra is as simple as adding a sbt plugin to your build. Also, Hydra includes [compile-time monitoring](https://www.triplequote.com/#monitoring), and it will help identifying the compilation bottlenecks that may exist in your project. &amp;#x200B;
&gt;Also, if you're interested in immediate significant (up to 40%!) gains for compilation performance, try running the compiler with Graal: [https://medium.com/graalvm/compiling-scala-faster-with-graalvm-86c5c0857fa3](https://medium.com/graalvm/compiling-scala-faster-with-graalvm-86c5c0857fa3) As far as I remember the Graal Community Edition only gives about 10% speedup. The Graal Enterprise Edition (EE) is the one that can deliver up to 30% speedups, but it is not free for development. &amp;#x200B; I also feel obliged to mention that [Triplequote Hydra](https://triplequote.com/) delivers compilation speedups of 50-80%, and it can work together with Graal (the speedups are cumulative). However, just like Graal EE, Hydra is not free for development. (full disclosure, I'm a Triplequote founder) &amp;#x200B;
It does work in worksheet. Are you sure you're importing your implicits on the call site of the second code block?
I seriously doubt those numbers. You will, of course, get a speed bump due to SSD, and perhaps from a faster memory bus but not so much from the CPU. If the bottleneck was IO you can get the same by using a memory drive. However, the amount of memory you give to Scala is user-defined and I have yet to see a project that needs more than 12GB to compile (the default on a Mac is 4GB). 
I didn't thought you didn't tryied, but when you read the article, you get the feeling you didn't: \&gt; the compilation performance hasn‚Äôt been a priority to the original creators of Scala. Scala has academic roots and, sadly, academia doesn‚Äôt see compiler performance to be a fruitful research area. Anyway, what do you think about his promise of a x10 speedup of the compiler?
Given that they will be available for programmers to use, and inherently *mostly* work as expected but create subtle gotchas when combined with generics, I still find them worrying.
I also prefer the kotlin synthax. 
Are you sure your implicits are in scope? Because I basically pasted your code into a repl and it works fine. Also as an aside, if you want to be more idiomatic `*` shouldn‚Äôt be a def in your case class; Instead what you have is a semi group or a monoid over your case class, if you provide the evidence of this implicitly then any generic caller or api operating on a type `T` that only posits the existence of a monoid for that `T` will automatically work with your case class. The philosophy here is that `*` doesnt belong to any specific `T`, `*` is a generic concept that should work for whatever types `T` that it holds a reasonable definition. If this sounds confusing do some research into type classes, monoids, and context bounds. 
Instead of using `implicit def` you might want `implicit class` if all you want to do is adding some operations to Row and Matrix.
What is the difference between \`implicit def\` and \`implicit class\`?
Probably [Great Britain Pound](https://en.wikipedia.org/wiki/Pound_sterling)
**Pound sterling** The pound sterling (symbol: ¬£; ISO code: GBP), commonly known as the pound and less commonly referred to as sterling, is the official currency of the United Kingdom, Jersey, Guernsey, the Isle of Man, South Georgia and the South Sandwich Islands, the British Antarctic Territory, and Tristan da Cunha. It is subdivided into 100 pence (singular: penny, abbreviated: p). A number of nations that do not use sterling also have currencies called the pound. Sterling is the third most-traded currency in the foreign exchange market, after the United States dollar, and the euro. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
And replace with Nihilistic Types, that would be fun **Nihilistic::Nietzsche.new**
&gt; but it is not free for development. To be precise, ‚ÄãEE is "Free for evaluation and other non-production uses"
What gotchas do you have in mind ? The only way you can distinguish between parts of a union type are through pattern matching, and we've beefed up our pattern matching warnings such that they should catch all unsound matches, so I'm not too worried, but if something fishy compiles without warning please open an issue.
&gt; but when you read the article, you get the feeling you didn't The article talks about academia in general, and I've been trying to tell you about how we do things in practice. &gt; Anyway, what do you think about his promise of a x10 speedup of the compiler? It sounds great ! But as the article states, it's not a sure thing, so we'll have to wait and see :). One reason to be skeptical would be that if you do profiling of scalac or dotty, you'll find that there's not one hot spot but rather that cycles are spent in many different parts of the codebase. As Kentucky Mule starts implementing more and more parts of the language, it's possible that it'll get slower, but maybe its different architecture and focus on performance first will let it avoid that, we'll see !
They are the same. Implicit class is shorthand for \`class\` and \`implicit\` def. Only problem which might occur is that implicit def method will have the same name as the class and you might accidentally override it (it happened to me).
I mean if you write some code like: def doSomething[A](a: A, someCondition: Boolean) = { val x: A | String = if(someCondition) a else "something" x match { case s: String =&gt; false case a: A =&gt; true } } then there are usually two paths through this function and it usually returns `someCondition`, but there's a hidden third path (not so hidden in this minimal example) when `A` is `String` where the control flow will be different. I don't imagine this would ever lead to a pattern matching warning because the code is never "unsound", but it violates parametricity and would be thoroughly confusing to encounter.
I have personally asked the Graal team if compilation is production use, and "it depends". Certainly not CI. Compilation for personal projects is probably ok. The extent to which you want to be dealing with Oracle lawyers may vary :)
That particular example will produce a warning: 5 | case a: A =&gt; true | ^^^^ | the type test for A cannot be checked at runtime 
&gt; The article talks about academia in general, and I've been trying to tell you about how we do things in practice. I've read it and I am sure you do care about performance!
Don't forget `extends AnyVal` to avoid class creation/discarding every time you want to call any added method
That sounds like a warning that would go away with `A: ClassTag` or equivalent rather than one that addresses the fundamental issue? I didn't get any warning at all when I tried it on Scastie: https://scastie.scala-lang.org/c3YiQdJiSZq7n4XMCt52cQ
`extends Throwable` does not mean you have to throw them with `throw` (which is the non FP way, given that `catch` breaks referential transparency). You can still use `MyTypedError` with `raiseError/handleErrorWith` and the rest of the ApplicativeError and MonadError combinators, while still conforming with `Sync` (which extends `MonadError[F, Throwable]` because a Sync is able to suspend arbitrary code that can throw). This is not perfect given some of the nasty things that `Throwable` can do, but I think it's a good compromise: you still get flexible handle error combinators, with very low overhead at the type level. The alternative would be something like `zio` or `EitherT`, which imho don't really pay for their weight (especially EitherT).
&gt; create subtle gotchas when combined with generics I think what you're referring to is the misconception that `A|Null` can replace `Option[A]`, which is not true because it doesn't nest. That's not a gotcha created by union types at all, per se. Just a bad program design idea. The same bad idea the designers of the Java standard library had when saying returning `null` is fine to indicate the absence of an element in a `Map` ‚Äì meaning you can't tell whether there was actually a `null` in the `Map` or not. 
Why is \`other\` of type \`RichMatrix\`? It probably should be \`Matrix\`. This works: \`\`\` type Row = Vector\[Double\] type Matrix = Vector\[Row\] &amp;#x200B; implicit class MatrixOps(m: Matrix) { def \*(other: Matrix): Int = 1 } &amp;#x200B; val a = Vector(Vector(1.0)) val b = Vector(Vector(2.0)) a \* b \`\`\`
`implicit def` is more general. Implicit class communicates more clearly that you want to add thing to something, and add an automatic conversion. Additionally, `implicit class X(val y: Y) extends AnyVal` limits you to only `def`'s and is Scala's way of doing [extension methods](https://en.wikipedia.org/wiki/Extension_method). It has much less overhead [because it gets compiled differently](https://docs.scala-lang.org/overviews/core/value-classes.html#extension-methods).
**Extension method** In object-oriented computer programming, an extension method is a method added to an object after the original object was compiled. The modified object is often a class, a prototype or a type. Extension methods are features of some object-oriented programming languages. There is no syntactic difference between calling an extension method and calling a method declared in the type definition.Not all languages implement extension methods in an equally safe manner, however. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/scala/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; I think what you're referring to is the common misconception that A|Null can replace Option[A], which is not true because it doesn't nest. That's one example of the more general problem, yes. &gt; That's not a gotcha created by union types at all, per se. It's just a bad program design idea. The same bad idea the designers of the Java standard library had when saying returning null is fine to indicate the absence of an element in a Map ‚Äì meaning you can't tell whether there was actually a null in the Map or not. But having `null` - or union types - in the language naturally guides users towards such poor design decisions. It's very easy to create code that mostly works but is subtly non-parametric. (A `Map` that returned `A | KeyNotPresent` would have exactly the same problem) Yes we can tell users not to write that kind of code, but it's a "wobbly step" waiting to trip them up - particularly since the code doesn't obviously break, indeed will likely work fine in all their testing.
[It does work](https://scastie.scala-lang.org/OimEpRCcTi2352HvnQfqPg), your problem must be somewhere else. [Additionally there's another version that does not require enabling feature flags and is more efficient](https://scastie.scala-lang.org/aNNRwYBSQzmQygbFq86W6w): ``` type Row = Vector[Double] type Matrix = Vector[Row] implicit class MatrixOps(val m: Matrix) extends AnyVal { def *(other: Matrix): Int = 1 // just returns 1 every time for now } val a = Vector(Vector(1.0)) val b = Vector(Vector(2.0)) a * b ```
But Sync extends `MonadError[F, Throwable]` to catch unhandled exceptions, abusing it with *expected errors* seems like, well, abuse. And besides, doen't extending `Throwable` adds quite a bit of overhead compared to an empty trait?
Yes, wasn't trying to say this addresses the fundamental issue which you correctly pointed out :) (also note that ClassTag-based pattern matching is fishy for other reasons and we might have to warn about it in some cases: https://gist.github.com/Blaisorblade/a0eebb6a4f35344e48c4c60dc2a14ce6). Scastie uses an outdated Dotty unfortunately: https://github.com/scalacenter/scastie/issues/281
I would almost follow your argument if the standard library exhibited that kind of bad practices. But since every Scala library under the sun already uses `Option` when it ought to, I think your point is moot. People will mostly follow the convention unless they start totally _from scratch_ and somehow don't interact with the standard library (very unlikely).
I mentioned RAM because my previous 16GB (which was the maximum on Macbooks until this year) was barely enough to run intellij, scala, a browser, and all the other crap I use for development, and I was getting a couple GB into swap regularly.
Wow. Thank you for investing your time and explaining it, I appreciate it. :-)
Neither the linked forum post, nor the linked dotty doc page, nor the github thread really explain what's happening. Does any of this concern devs who use stuff like `foo: Foo[_]` or `foo: Foo[_&lt; Bar]`?
Looks like you should be fine &gt; Existential types that can be expressed using only wildcards (but not forSome) are still supported, but are treated as refined types. For instance, the type &gt; &gt; Map[_ &lt;: AnyRef, Int]
Finally released to Maven, see top main message
[The changes in syntax will be smaller than from python 2 to 3, and nearly full backwards compatibility is targeted](https://www.scala-lang.org/blog/2018/04/19/scala-3.html) ETA is not certain and tentatively 2020.
This is a great question, I hope someone can answer it in detail. Here is some info I was able to pull up: [https://www.scala-lang.org/blog/2018/04/19/scala-3.html](https://www.scala-lang.org/blog/2018/04/19/scala-3.html) [https://medium.com/@fommil/scala-3-considered-as-a-new-programming-language-a335ff67e075](https://medium.com/@fommil/scala-3-considered-as-a-new-programming-language-a335ff67e075) [https://www.infoworld.com/article/3269370/java/scala-roadmap-what-features-to-expect-to-scala-3.html](https://www.infoworld.com/article/3269370/java/scala-roadmap-what-features-to-expect-to-scala-3.html) tldr: \&gt; Does anyone have a road-map for this? the link to the scala-lang blog is a very basic road-map. \&gt; Lightbend or EPFL? It looks like Dotty (EPFL) is heading things up. \&gt; How much will change is Scala 3? It seems that most people see Scala 3 as a \*new\* language with compatibility for Scala 2 artifacts. The general consensus is very close to a Python 2.7 vs. Python 3.7 split in the language; all \*fancy new\* stuff will happen on Scala 3, and Scala 2 will remain in use and supported for a long time. As to any major syntactic changes in the language, it seems it will be more a case of 'added features' with totally new syntax and common features with some changes to existing syntax.
External user here. &gt; when is the ETA for Scala 3. Martin Odersky was hoping for a 2020 initial release, based on a recent talk he gave. &gt; This heavily influences the decision on whether or not it should be used in a greenfield project or service. Right now, there is no Scala 3, only Dotty, which is the code name for the in-progress software that will become Scala 3. It's fun and interesting to play with, but nobody will tell you to use it for real projects beyond experimentation. &gt; how much will change in Scala3 But the first thing is to check the [current docs](https://dotty.epfl.ch/docs/reference/overview.html). See the "Overview", "New types", "Enums, "Other new features", "New implicits", "Other changed features", and "Dropped features" entries. Those are already quite enlightening. My understanding is that there might be more or less changes in the end. Changes to the language must go through the SIP committee, and there is not always consensus about everything. Some proposals are still potentially in flux regarding syntax, including, lately, the syntax for extension methods. 
Check out [this blog entry](https://www.scala-lang.org/blog/2018/04/19/scala-3.html) for details on timeline, and [this page](http://dotty.epfl.ch/docs/reference/overview.html) for info on features. There are also some good conference talks on it if you search youtube for ScalaDays. Bottom line, is it is scheduled for early 2020, and the upcoming 2.14 release is intended to make migration easier. As far as changes, it will be comparable in scope to python2-&gt;3, especially if you are already writing idiomatic functional code, but in my opinion more people will *want* to switch to Scala 3 than is the case for python, especially if you do a lot of type level programming. It simplifies many things. There is enough compatibility, however, that you don't need to delay starting any projects. Most of what will be dropped was rarely used anyway and has much better alternatives in Scala 3.
&gt; I went through the Python3 situation and although syntax was definitely broken We aim to do better than that: the Dotty compiler itself, as well as the Scala 2.13 standard library, cross-compile between Scala 2.12 and Dotty. In the cases where we introduce syntax changes, the old syntax will be kept around at least until 3.1. And in the rare cases where we had to introduce semantic changes for the same code, we have a flag (currently `-language:Scala2`) to imitate the Scala 2 semantics. Right now, a Dotty project can also use Scala 2.12 dependencies (cf https://github.com/lampepfl/dotty-example-project#getting-your-project-to-compile-with-dotty), and our (ambitious) goal would be to also have Scala 2.14 capable of using Dotty dependencies and vice-versa.
This is bad advice. If you want to learn scala, learn scala. If you want to learn a more narrowly focused FP language to help you learn scala, SML is a better option than haskell. See e.g "The Little MLer".
&gt; But Sync extends MonadError[F, Throwable] to catch unhandled exceptions, abusing it with expected errors Well, no, there's no distinction between unexpected and expected errors, because the design decision here (unlike `zio` or `EitherT`) is to not represent errors in the type. I believe this decision has the best tradeoff profile as the default choice in practice (which doesn't mean that typed errors have zero value in every case). For example, I like exposing `F[Either[Err, A]]` in a tagless algebra (so you get some "types as documentation" value), but `Err extends Throwable` so I can `.rethrow` and handle them at the value level only at call site, via `handleErrorWith { case e: Err =&gt;` and the other MonadError combinators (`onError` and so forth). I find that worrying mostly about the happy path + retrying and/or some supervision combinators (in fs2) gives me the right mix of flexibility, safety and boilerplate reduction. I would use typed errors (compositionally, with EitherT) in some select cases though. Anyway, the original question was whether this was a conscious decision or a remnant from the java days, and the answer is that is a conscious decision. Not a perfect one (e.g. because Throwable itself is a bit weird), but the one I've found to work better in several codebases: removing EitherT especially results in general improvement to the code, with very few drawbacks, especially because many people have a pretty blurred notion of the value that EitherT gives you (e.g. if all of your App errors have the same type, and you only handle them at the top, typed errors give you nothing but boilerplate). &gt; And besides, doen't extending Throwable adds quite a bit of overhead compared to an empty trait Well, first of all the overhead is entirely irrelevant in many cases. But if you are in one of those cases where it matters, it's enough to extend `Throwable with NoStackTrace`: the JVM is actually extremely fast at dealing with exceptions, and the overhead mostly comes from filling the stack trace. Ofc, you won't have a stack trace when you extend that, but that's the same situation you are in with the bare trait. 
I am just starting to play with it, but it seems to encapsulate everything I want. I'm probably going to rewrite a lot of stuff on it; it's a crystallization of several different ad-hoc techniques I myself have used and want to continue using in a more "standardized" manner. 
\&gt; Who is primarily running this effort? Lightbend or EPFL? At Lightbend, we'll be focused on Scala 2.14 while EPFL is focused on Scala 3, but the dominant theme of 2.14 is alignment with Scala 3 to smooth migration, so it's really a combined effort.
Big language versions is a problem for the language adoption. I expretienced Perl 5 to 6 :)
See https://typelevel.org/blog/2018/05/09/product-with-serializable.html
&gt; Throwable with NoStackTrace Oh, i didn't know about that one
In 3.3 in **Functional Programming for Mortals** the author suggests in the `Drone` and `Machine` example: &gt;We can write the interface for the business logic ```scala trait DynAgents[F[_]] { def initial: F[WorldView] def update(old: WorldView): F[WorldView] def act(world: WorldView): F[WorldView] } ``` &gt;and implement it with a *module*. A module depends only on other modules, algebras and pure functions, and can be abstracted over F. If an implementation of an algebraic interface is tied to a specific type, e.g. IO, it is called an interpreter. ```scala final class DynAgentsModule[F[_]: Monad](D: Drone[F], M: Machines[F]) extends DynAgents[F] { ``` What is meant here by *module*? And what would another (less optimal choice presumably be?). Is it a module because the the Interpreter is `final`, i.e. what makes something in Scala a module?
People comparing this to Python and Perl are forgetting that those are scripting languages. That means that interoperability require syntax to be backwards compatible. Scala is not a scripting language. Interoperability requires binary compatibility, which according to what Martin Odersky said at ScalaX 2018, dotty is with the current Scala 2.x branch. This means you can use and import Scala 2 code in Scala 3, the same way you can do with Java code, and vice-versa.
Great work, Yuan Yifan! For GeoJSON parsing you can try jsoniter-scala. Here is a simplest implementation for it: https://github.com/plokhotnyuk/jsoniter-scala/blob/master/jsoniter-scala-benchmark/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/benchmark/GeoJSON.scala It that was tested with different JSON parsers for Scala: https://github.com/plokhotnyuk/jsoniter-scala/blob/master/jsoniter-scala-benchmark/src/main/scala/com/github/plokhotnyuk/jsoniter_scala/benchmark/GeoJSONBenchmark.scala Result of benchmarks can be found here by scrolling down to GeoJSONBenchmark charts: https://plokhotnyuk.github.io/jsoniter-scala/ If you have a different or more complex model then I can help you to write a custom codecs to be safe and efficient. Also, if you are interested in the most efficient RTree implementation for spherical model of Earth then you can try the following project from Sizmek: https://github.com/sizmek-public/rtree2d
After hundreds of articles over decades of telling people `null` was a mistake and never to use `null`, Scala codebases *mostly* avoid it. It does still cause issues, particularly when people are just starting out with the issue. Now we're going to be adding a new language feature with similar properties. Of course users will want to use union types - they're a new and exciting language feature, and language features are normally meant to be used. "The standard library doesn't do that" will not be a convincing reason not to use them, because the standard library was (mostly) written when the language didn't have union types.
&gt; we're going to be adding a new language feature with similar properties How do they have _similar properties_ than `null`?!
Parametricity violation. Code that works most of the time but quietly breaks if someone else tried to do the same thing, or if you nest it. The thing we've just been discussing.
&gt; Parametricity violation. There is no parametricity violation due to type unions AFAIK. That is, none that is not already possible without type unions. Or do demonstrate one. --- Look, I usually agree with you, and I think we mostly agree about this. But right now I don't think the discussion is constructive, as we seem to differ on opinions. To me union types are a necessary and beneficial addition to Dotty, and I don't think they will lead to the problems you describe. I'm not interested in discussing that subject further. What is not an opinion though is that type unions have **_nothing_** to do with the problem of having `null` in the language, which is that it's an unchecked bottom inhabiting all reference types (the billion-dollar mistake). I'm not going to answer further discussions on this either.
&gt; There is no parametricity violation due to type unions AFAIK. That is, none that is not already possible without type unions. Or do demonstrate one. I showed it in the other branch of this thread: https://www.reddit.com/r/scala/comments/aknkm6/from_kentucky_mule_to_faster_scala_compiler/efcpklk/ &gt; What is not an opinion though is that type unions have nothing to do with the problem of having null in the language, which is that it's an unchecked bottom inhabiting all reference types (the billion-dollar mistake). You gave your own example of a `null`-related problem (representing absent values in `Map`). Union types are susceptible to the exact same problem if used the same way. IME your example is pretty representative; the big problems that `null` introduces all boil down to parametricity violation. *Locally* unsafe constructs, while undesirable, aren't a huge problem in practice; constructs where you can't understand `f(g(x))` by examining `f` and `g` individually are the real killer.
&gt; I showed it in the other branch of this thread Unions are inessential to that example. Here is the exact same thing without a union, in current Scala: def doSomething[A](a: A, someCondition: Boolean) = { val x = if (someCondition) a else "something" x match { case s: String =&gt; false case a: A =&gt; true } } I'm not sure you have a precise definition of "parametricity" in mind, which would explain the vagueness of your claims. If you do, please elaborate it here.
I experienced Java 6 -&gt; 7 -&gt; 8 and Kotlin 0.x -&gt; 1.x and have to say: "entirely depends on language". For JVM it's usually either perfect source compatibility (as long as you didn't abuse some obscure language "features") or something that can be solved through automated refactorings in IDE. And, naturally, perfect binary compatibility. I don't see why it would be any different for Scala.
&gt; Unions are inessential to that example. Here is the exact same thing without a union, in current Scala The version without a union (where `x` is of type `Any`) is known-unsafe a la https://typelevel.org/blog/2014/11/10/why_is_adt_pattern_matching_allowed.html#a-selector-subtlety. It's [a known wart in the language](http://www.lihaoyi.com/post/WartsoftheScalaProgrammingLanguage.html#conflating-total-destructuring-with-partial-pattern-matching) that this kind of unsafe code looks the same as safe code, and I would love to see future versions of Scala make a clearer distinction. If you take the position that we should treat pattern-matching on unions as unsafe (and forbidden by the scalazzi safe subset or similar ideas), then fair enough, but since there's no other way to use a union this means unions are of no use in safe Scala (or at best there would be some complex rules to define a restricted subset that can be used safely). &gt; I'm not sure you have a precise definition of "parametricity" in mind, which would explain the vagueness of your claims. If you do, please elaborate it here. I don't know about "precise"; I'm talking about parametricity in the sense of "Theroems for Free!", as described in e.g. [fake theorems for free](https://failex.blogspot.com/2013/06/fake-theorems-for-free.html).
i'm trying to encode protobuf object (prepared by scalapb) and pass it to another service written in python (I think, it's doesn't matter, but I'm using NATS.io as messaging system). val protoMsg: ProtoMessage = ProtoMessage(foo = "21266894", bar = 155) conn.publish("my.topic", protoMsg.toByteArray) Python protobuf deserializer can't deserialize this message, I think, that's expecting something like a hex string \`\\xc0\\xa9\\xb6\\xe3\`, but it receives something like \`\[B@7d958728\` instead. What i'm doing wrong? How I can get a necessary representation of a bytearray?
In regards to Scala.js: Is the library using some package which makes it a no go for javascript? Speed vs size really depends on the circumstances. For example I'm using Scala.js for graph traversals inside a graph database so I don't care about size but the speed is very important.
Sure. I talk about newcomers, they need to believe the language they learn will go strong. Something that sounds like big changes scare people and they choose something else.
&gt; pattern-matching on unions as unsafe It is not, in principle. It does not violate parametricity in the sense of "free theorems" (at least, assuming Dotty implements all the necessary warnings and safeguards (*)), because you're not inventing information from thin air ‚Äì the info _is there_, in the static type, so you're allowed to use it parametrically. (*) Which it currently does not. For example see [the issue](https://github.com/lampepfl/dotty/issues/5826) I just opened after trying something to see if it did.
thx will start using it soon
&gt; It is not, in principle. It does not violate parametricity in the sense of "free theorems" (at least, assuming Dotty implements all the necessary warnings and safeguards (*)), because you're not inventing information from thin air ‚Äì the info is there, in the static type, so you're allowed to use it parametrically. It does, because normal use of unions lets you "look inside" the type parameter. I mean I assume you'd accept that: def const5[A](a: A, a2: A): A = a match { case s: String =&gt; a2 case _ =&gt; a } is a parametricity violation (and not allowed under the scalazzi safe subset) much like `const3`. Clearly the same is true of def const6[A](a: A, a2: A): A = (a: A | String) match { case s: String =&gt; a2 case _ =&gt; a } since its behaviour is exactly the same. But there's no practical set of local rules you can adopt to rule out writing functions like `const6`, except for ruling out use of unions entirely.
What is E2E in this context? It sounds like you should implement you own test service to functionally replace this external server you're sending things to. So you can test your own logic and control delays and failures that may come externally.
end to end
Nothing special, I use [java.util.stream.Stream](https://java.util.stream.Stream) for parallel processing, which has to be turned off for JS When I compiled to Scala.js as is, I got 2.5mB JS library for the example on top. I turned of specialization, which is probably not required, and got 800K library. Performance increase for List was 35% for 10 elements, 60% for 100, and 2000% for 1000 elements. Regular array had little gains 5/10/20%, but Array\[Int\] would fly 700/1200,1300%, so there are gains to be had Scala collections add about 100-150K in size (for the example on top). Stream now adds another 800K, but I think this can be drastically reduced, there is no reason for it to be this big. Let me test for 1-2 days, I will get back with numbers
If mocking or stubbing the external service out is not an option, then there is nothing to do but wait. A Promise/Future with a long timeout is one option, but if you are able to poll periodically to check if B has happened, ScalaTest's [Eventually](http://doc.scalatest.org/1.8/org/scalatest/concurrent/Eventually.html) may be what you're looking for. Essentially, you provide it with a polling interval and a timeout, and it will execute everything within the Eventually block until either the asserts pass, or the timeout is reached.
The problem with Play Framework at the moment is that the documentation is incomplete and partially outdated. Official samples and examples are misleading and when you try to find solutions on Stackoverflow you will feel like everybody and their dog is sticking to Spring. &amp;#x200B; Unless you really want to use Scala or unless you already have Akka components that you want to use, better stay away from Play until the project is properly maintained (and documented) again.
In my interest is to provide first customers with special support, please, [write to me](http://scalqa.org/doc-stream/About/Datamata_Corporation.html) with any issues, I am standing by to answer
Right, I see your point now. Well I guess upcasting to a type union _is_ making up information from thin air, and leads to non-parametric pattern matching. Nevertheless, as /u/gmartres pointed out type unions are still useful for making the type system complete. They _can_ come up naturally and _be useful_ even if you don't match on them. For example consider the MLsub type system, which makes good use of type unions but does not have pattern matching on them! Here is a specific example in Scala, where a type union can be used to circumvent the need for bounding type variables: def concat[A,B]: (List[A], List[B]) =&gt; List[A | B] = _ ++ _ In this case it's easy to use type variable bounds instead, but in the middle of type inference it's not always possible, in all generality (and that's when you look at MLsub if you want to know why). And _additionally_, again, type unions are useful for low-level code and JS interop (where parametricity is not that important). So that's three very well-motivated use cases for type unions right there.
Based on the error message I suspect the problem is not creating the byte array but publishing it. Looks to me like whatever is publishing is ultimately invoking `toString` on the byte array passed in and then publishing that. I'm not sure how much control you have over the publishing but you might have to create a string directly from the bytes yourself, or create a wrapper class that does the right thing when `toString` is called on that.
&gt;It seems that most people see Scala 3 as a new language I don't agree with this at all. It's the same language. It's just not that different. Scala 2 code will need tweaking, not big changes.
There are plenty of lower level tests that check the functionality, but end to end tests are tests that are automating the actual usage of the end product. They basically automate the process a manual tester would have to go throw. For example with Selenium we start several instances of Chrome, Firefox and IE and test most of our functionality in parallel. These are run once per day, but help us to detect if something broke either because we fucked up or because a browser update fucked up. And in this case we offer a functionality that takes quite some time to process and I don't want to have a Future with a ten minute timeout. That's why I'm wondering if there are any non-blocking solutions. 
Thanks I'm gonna read through the docu and see if it helps me. 
Ah, you're right. I was trying a bunch of things in the REPL, so it must have just been in a bad state. Thanks for commenting.
You're right. I was trying a bunch of stuff in the REPL, so I was probably in a bad state. I opened a new window and it works fine. Thanks for helping üëç
Yeah, I was trying a bunch of stuff in the REPL so was probably in a bad state. I'll try out the implicit class approach. Thanks for your help.
It's still not clear to me what's being tested. If you want to test browser compatibility, then you can test that without having to rely on external services.
I basically want to test that I can rely on the external service. 
I've been working on Laminar for more than a year, but somehow never found the time to properly promote it. As a result, currently few people know of and use Laminar, but I built it to solve _my_ problems, and it does that well. It is also the most interesting project I've ever worked on (especially its reactive layer, [Airstream](https://github.com/raquo/Airstream)), so I'm very happy that I embarked on this journey. Nevertheless, Laminar is intended for anyone who shares my ideas on simplicity and safety (which, spoiler alert, is not "pure FP at all costs"), and is fully and thoroughly documented. The linked github page explains my motivations for creating Laminar. Basically I wanted a small library that exploits the strengths of Scala and Scala.js without getting in the way with its own concepts and limitations, and this is the result. I don't have a lot of ideology attached to it, it's just a better way of doing things in my view.
one day you shall see the true face of the bottomless horror which are vars, and their uncountable thread-unsafe mutable states. until then, your castles are made of sand and your happy days, numbered. 
how about github issues [https://github.com/scalqa/stream/issues/1](https://github.com/scalqa/stream/issues/1) Also there seems to be a performance increase in my project so thx again.
How to implement Graph in a functional way not using vars?
Hi guys, &amp;#x200B; Got a job working in Scala, but have no experience in it. I want to buy a couple of books now, and have done some research, but to me, it seems there is no real consensus on which 1 book is best. I am coming from a Javascript and Ruby background, and don't know a lick of Java, so I'd prefer books that don't rely on past Java experience. &amp;#x200B; Thanks in advance! &amp;#x200B;
What use cases will you use it for ? And what other stuff will you *not* use it for ?
As a backend developer, I received a considerable Lagom codebase. We're a fintech company, and for many use cases Lagom makes sense for us, and I think that is a perfect match for the framework. I do have a couple of complaints: * There is no easy way to visualise the full flow of Messages/Commands/Events coming back and forth. I use IDEA, and the best that I can do is use the "Usages" feature (Alt + F7) and track down where this particular case class is being used. We're trying to document the flows with PlantUML diagrams but isn't easy or scalable to maintain, and as in any other form of documentation it gets outdated and can be misleading. I want to start experimenting with diagrams auto-generation, but it'll take a while. * More on our side than on the framework side, but Lagom is overkill for several use cases. Now we're striping our core Lagom application of everything that can be easily implemented on a normal CRUD app and move it to a microservice (Play / Flask / SpringBoot with Kotlin). * We had a problem with our unit testing, one day stop working for no reason (No version upgrading or anything that we can track on our Git Logs). It took us several weeks to fix it. * There is not a real book or any other proper resource to learn it, Packt Publishing has some basic videos but it doesn't cover more than a "hello world" app
&gt; one day stop working for no reason (No version upgrading or anything that we can track on our Git Logs). It took us several weeks to fix it. Wow, what was the cause?
An initialization problem with Cassandra, part of the problem is that it happens in just some machines (Some old laptops, our jenkins machines) but no others. So, even replicating an environment where the error can be reproduced consistently was a challenge.
That's really cool. I'm currently working on similar library except it is using value cells with automatic subscription like those vue.js is based on. Plus scalatags for HTML generation, it fits nicely so far. It is great to see I'm not the only one using components with direct DOM modification instead of virtual DOM
Thank you so much for this. It is so clean, so simple and so easy to use. I don't care about react. If you like Elm and Scala you are going to love Laminar :)
There are a couple of courses in spanish (that I did with another) that I think are üëå * [https://pro.codely.tv/library/introduccion-a-scala/63278/about/](https://pro.codely.tv/library/introduccion-a-scala/63278/about/) * [https://pro.codely.tv/library/api-http-con-scala-y-akka/66747/about/](https://pro.codely.tv/library/api-http-con-scala-y-akka/66747/about/) Hope it helps!
I was hoping the article or slides would give examples. Play with Gradle had been painful for my colleagues.
Thanks for a very interesting project. I haven't got a chance to use it but the code looks nice and clean
Server that manages chat rooms + client of some sort, several clients can connect to the server and exchange/broadcast messages. Chat rooms, message history (probably clients) are handled using akka, message exchange is done using kafka.
Painful in what way?
Why LinkedIn did not sponsor SBT to provide faster builds instead?
They are a lifestyle\[1\] company, not a software company. &amp;#x200B; \[1\] Don't forget to check my latest morning routine: getting up at 4am after 3 solid hours of powersleep, I've got plenty of time to run a marathon and read Plato before starting to work on my successful business!
I can't speak for LinkedIn but sponsoring SBT development would take more time (and money) than switching to Gradle and there's no guarantee that after 6-12 months of that investment SBT would be as fast as Gradle already was in 2018.
I have been tasked to re-write some Hive jobs in Spark using Dataset API. There are a lot of HIve jobs to rewrite so I am gonna need some automation. I searched for a tool to generate case classes from Hive DDL but couldn't find anything. Found plenty of tools that do the reverse though. So is there a tool to help me with this or am I gonna have to write my own parser or something? Also any recommendations for Hive to Spark migration is welcome.
10 minute build time in a small play application? What?
Haha that was exactly what I came here to comment about. How come it takes more than 5 min. If you don‚Äôt use cats / shapeless / scalaz, automatic encoders / decoders, i don‚Äôt see a reason for your build to take more than 5 minutes. If it really takes 10 min then it might not be a microservice anymore...
&gt; Here is a specific example in Scala, where a type union can be used to circumvent the need for bounding type variables: But you'd have to pattern-match eventually to be able to use the elements of the `List[A | B]`, right? You can pass the union type around but eventually you have to deconstruct it and the only way to do that is a pattern match (or so I understood gmartres to say). &gt; So that's three very well-motivated use cases for type unions right there. I'm sure there are use cases but they must be relatively niche. Whereas parametricity is an important global property for general-purpose code; if we can no longer rely on it in Scala codebases that's going to be a significant drag on development (e.g. I'll need to write more tests to have the same level of confidence in a library I'm using - even if it turns out that library isn't actually using union types).
Yeah, I dunno, the claims in the article are extremely suspect, more so given how little information there actually is.
Because there's no single best book, it depends on your experience / what you already understand. What I found the best are Essential Scala and Scala for the Impatient. I think neither of them rely on Java. If you come from dynamic languages the biggest shift will be using exactly defined data types instead of untyped hashmaps. Try to avoid Map[String, Any] and Any in general. 
Agreed. If the app takes 5 to 10 minutes of solid compiling it‚Äôs far from a micro-service. Most Scala apps I work on build 0-90seconds using multi modules etc, any time longer than that is functional tests executing not a build issue. Now if you are running build agents which do not cache artifacts so downloads the internet each time, low cpu power as you are using small cloud instances as cheap and have multiple jobs per agent you‚Äôll easily get to 5-10 minutes, throw in proxies and getting artifacts alone can be 10 minutes plus. Add functional tests running sequentially, well of course it‚Äôs going to be slow. Not defending SBT but most speed issues I see in build pipelines are pipeline design and not the build tool itself. Standardizing on Gradle as that‚Äôs what you use else where in the org, that‚Äôs a different topic.
&gt; But you'd have to pattern-match eventually to be able to use the elements of the `List[A | B]`, right? No, in this example the union type is _only_ used to express the relationship between the input and output types. This means I can call `concat` on a list of `Student` and a list of `Engineer` and get a list of `Person` back. It replaces the need for type parameter bounds. &gt; they must be relatively niche To you it may appear like that, but I don't think they are. &gt; if we can no longer rely on it in Scala codebases You _never_ had parametricity in Scala. It's _always_ been about conventions (cf. scalazzi). Nothing changes here.
I like the joke but I do not see the context :)
For us too! Could not even get the Twirl templating engine working at all. Until some senior dev decided to spend all night and solve it somehow that is T_T
I have to say it looks really good. Recently I played with binding.scala, but it has this not-so-nice feature of infecting all your codebase with `Binding[_]`. In Laminar values are just values, which has the potential to be much easier to work with at the price of hidden side effects. I can pay that price.
I created a [trial release](https://github.com/scalqa/stream/releases/tag/0.1.1) for Scala.js with a JAR. Size is not optimized at all, but it should work [msatala](https://www.reddit.com/user/msatala), could you try to traverse your graph, to see if it makes sense Next library release will compile to JS, no problem. But, should I reorganize it to produce smaller JS code and support it on Maven? This is still a question &amp;#x200B; &amp;#x200B;
It really looks like the SBT builds in question were probably not correctly configured in some way. SBT builds things incrementally by default as well, but there are several ways you can set things up (especially if autogenerating sources incorrectly) such that incrementality is lost. My guess is that their SBT builds were buggy and they simply fixed those builds when moving to Gradle.
&gt; Scala is a language that absolutely requires tooling support if you want to be productive. I'm not sure I agree with this. I use a simple text editor (Sublime) with no extra tooling and am quite happy and have been for over a decade. Most of my coworkers also do the same.
**Scala Cookbook** doesn't rely on Java knowledge, it just assumes that you're familiar with any primarily OOP language. There are rarely ever a consensus about "the one" book about programming language (unless it's so niche and obscure that there are actually only one good book about it), but **Programming in Scala** might be the closest thing, given it was written by the "father of tha language", Martin Odersky. Both aforementioned books don't assume you know anything about Java, but do sometimes use it for comparrisson to illustarte the point.
THanks for your work. I don't mean to belittle it in any way but when I looked at it a while back- it somewhat looked like scalatags. Was that a wrong assessment? In which case I will check it out again.
One of the best frameworks I‚Äôve ever used and looks like it has just got a bit better! Just a shame it‚Äôs quite a task to upgrade from 2.5.x...
Awesome thanks! I already have Programming in Scala on the way but will check out Scala Cookbook too.
Cheers for that will take a look at those two :)
UI libraries like React, Outwatch, Binding.scala and Laminar offer much more than ScalaTags. While technically all of these let you build and manipulate DOM trees, what sets apart some libraries from others is how scalable their methods are, both in terms of performance and code organization / maintenance. ScalaTags is very low level ‚Äì it does not offer a scalable way to map application state to UI state. If you're building an interactive, frontend-heavy application, you need a library that solves this problem. React does this with virtual DOM and Components. Outwatch uses virtual DOM and Observables. Laminar uses Observables and direct DOM mutations [instead](https://github.com/raquo/Laminar/blob/master/docs/Virtual-DOM.md) of virtual DOM. Laminar could have theoretically _depended_ on ScalaTags, but instead I built [Scala DOM Types](https://github.com/raquo/scala-dom-types) and [Scala DOM Builder](https://github.com/raquo/scala-dom-builder) because I found ScalaTags not quite flexible enough for my liking. Scala DOM Types documentation explains the reasoning in more details.
Articles like that are harmful cause people could blindly follow such advice and conclusions. It's not constructive, no reasons, no cause, no numbers. I just managed to `sbt clean publishLocal` (with downloaded dependencies) whole playframework in 7 minutes on 5+ years old laptop. Seriously guys???
Thank you for your reply! I'd implemented part of GeoJSON parser/generator in my code, but you code is amazing and maybe we can cooperate in this area. And the RTree maybe not available because the earth is a S^2 curve but triditional RTree are based on R^n space. Finally, this list: (https://github.com/mourner/projects) is very valuable. Thank you for your reply again~
I also profiled the build. I don't see sbt classes consuming CPU/IO, only scala compiler.
It comes with great benefits though. The change to no global state is great, and makes testing so much easier. Small price to pay! 
Small clarification: rtree2d from Sizmek supports both _plane_ and _spherical_ geometry. Here is an implementation of distance calculator that is used for knn queries over the Earth: https://github.com/sizmek-public/rtree2d/blob/master/rtree2d-core/src/main/scala/com/sizmek/rtree2d/core/Geometry.scala#L178-L228 Only one restriction that index entry for regions which are crossing the anti-meridian should be splitted at it: https://github.com/sizmek-public/rtree2d/blob/master/rtree2d-core/src/main/scala/com/sizmek/rtree2d/core/Geometry.scala#L168-L173 
Isn't this another implementation of [Extensible Effects](http://okmij.org/ftp/Haskell/extensible/exteff.pdf) similar to [eff](http://atnos-org.github.io/eff/)?
I really didn't understand the appeal of map and flat map until I started using Try and Option all the time. Now I've started to see the point of cats IO...
I believe it is just a design pattern to put all the components together (think of "DI" if you will) and it's a very common approach in tagless final style.
Do you think he thinks that?
A good way to practice is find Scala code you wrote imperatively and refactor it to functional. That will make you think, but also make you think about how the two approaches relate to each other. Do this a number of times. Also, be patient. It takes time to change how you think.
After your question I got myself motivated and put on some work into the [http4s-tracer examples](https://github.com/gvolpe/http4s-tracer/tree/master/examples/src/main/scala/com/github/gvolpe/tracer) to show a similar approach. See the [modules documentation](https://gvolpe.github.io/http4s-tracer/guide.html#modules). In this particular case, I decouple the entire tracing and logging capabilities from the business logic which is what we normally run in production systems at work.