This should now work on go 1.8. playground is still at 1.7 tho
I tried this: https://play.golang.org/p/Ozf9yGmIZD on 1.8 and it did not work. Gives the same error: cannot convert p (type Production) to type []string I tried a few other variations and nothing without a loop (or unsafe) worked.
I appreciate it, thank you ðŸ˜‰
Instead of defining all of this behavior in different places within test functions, it seems that it would be less work in total and be more clear to create mock implementations of the required interfaces. However, I'm open to learning from your experience regarding why this is a better way.
Git for snapshots, that means no pruning of old snapshots, right?
In fact, in the actual app I was testing there is a "mock" package, which holds the Expectations util and the mock implementations in a central, reusable place. The tests themselves just import this mock package and define the expectations. I cut down the amount of code for simplicity's sake, so I guess this probably didn't come across very well or at all. ;) If you only test very simple behaviour, it might be more clear and simple to just create a simple mock implementation. I wrote this because I needed multiple different behaviours for each method and would have had to implement all of those cases. For example, if you want to test a method which does 2 calls to the DataSource, you already have 4 different cases with error handling and all, which you would otherwise have to handle with 4 mock implementations for a conditional-based approach.
&gt;Does Go runtime maintain the iteration orders for maps? &gt; &gt;No. In Go 1, the map iteration orders are randomized. If you require a stable iteration order for a map you must maintain the order by yourself. Not quite. The iteration order is unspecified, not random. In fact, if you are relying on it being random, you may be unpleasantly surprised. See my explanation here: https://github.com/lukechampine/randmap
I see some overlap in functionality with [Sia](https://github.com/NebulousLabs/Sia). Maybe we can integrate somehow? :)
I agree, for most cases this approach is sufficient - I also use it by default. However, in this case after trying my usual approach, it got ugly pretty quickly, as I needed different return values for multiple, subsequent calls of the same method, each with possible error values. I tried the simple approach with parameterizing, but quickly got to its limits. But I agree with you completely that this is an edge-case and that for most cases, a simple mock implementation with an optional success/error flag is enough.
Thanks, I think this is a great suggestion. I personally like reading `"foo != ?"` instead of `db.NotEqual{"foo": 34}`, tho, because the former is shorter and easy to read, but the point about improved type safety is a good one. We'll probably start by adding something to negate conditions, like https://github.com/upper/db/issues/284 and we'll see how well users respond to it before adding others.
I literally went through this when I became in charge of our production services at the startup I work at. Within 2 months we were on a hosted ELK service and my life is so much better. As a one man team, the person that set this up before me had unrealistic expectations about how challenging it would be to maintain this. 
"Since Go 1 the runtime randomizes map iteration order, as programmers relied on the stable iteration order of the previous implementation." Not truly "random" but it is intentionally perturbed. https://blog.golang.org/go-maps-in-action
Ah yes, overlap indeed! I wasn't aware of Sia, very interesting project. Seems better thought out than Storj. Not sure what to integrate to which, though, I would first have to familiarize myself with Sia and its source code. One thing that comes to mind with both projects in their current state: Sia as a scat store (no parity proc if Sia is the only store, would be redundant). I'll give Sia a try soon ðŸ™‚ Thanks for reaching out.
Super cool!
I don't know much about upper.io, but what are your difference compared with `gorm` ? , sorry for the comparation, but I need arguments to change me from `gorm` to `upper` 
Thanks for the interest ðŸ˜‰ If you give it a try, have fun! Would be glad to help.
If you are doing this as a personal project then I'd recommend just using Go as both your API and static pages server. You can use Polymer as a Single-Page App much like AngularJS. If this is a production app, then what you use to serve the static pages would require more discussion. *Quick hint: If you serve any Go apps in production make sure you set your timeouts!* https://blog.cloudflare.com/the-complete-guide-to-golang-net-http-timeouts/
Glad you like it! I updated the github readme with some quick instructions for using your own faces, if you'd like to set up your own bot.
wording tweaked, thx.
Hi, I just saw this and I'm happy to try to answer this question. Disclaimer: I've discovered GopherJS roughly in Jun 2014 and I've been actively using it and contributing to it since. I've heard quite a few people tell me about some projects that use GopherJS, but for some reason many of the bigger ones happen to be either closed source, internal, or not yet finished and publicly released... I'm still waiting/looking for some really incredible use cases that I could refer people to. Something equivalent to how "dl.google.com" was rewritten in Go in its early days. That said, I know of a few... Camlistore, for example, uses GopherJS for a small portion of its frontend UI. See https://github.com/camlistore/camlistore/search?q=gopherjs, https://github.com/camlistore/camlistore/issues/798 and https://camlistore-review.googlesource.com/#/c/9086/ specifically. The vast majority of GopherJS-powered projects I know of and can share are the ones I made myself. Where to begin... If you've ever visited https://gotools.org to look at some Go package's source code on one page, and pressed F to jump to an identifier, or just scrolled up and down, all the interactions and frontend components powering that are written in Go, compiled with GopherJS. If you've seen the Idiomatic Go collection of tidbits my personal site, https://dmitri.shuralyov.com/idiomatic-go, and tried to leave a reaction on any one of them, the entire frontend aspect of that is fully written in Go. I have an unfinished port of a game into Go, compiled with GopherJS here: https://dmitri.shuralyov.com/projects/Hover-Demo/. The same codebase runs on desktop and browsers, by using an isomorphic Go library https://godoc.org/github.com/goxjs/gl. There's another unfinished game in Go that uses networking to achieve multiplayer, which also runs in the browser thanks to GopherJS. Try opening two browser tabs at https://dmitri.shuralyov.com/projects/eX0/eX0-go-client/ (and open up the browser console to see networking messages being sent). It uses websockets under the hood, since those are supported both by desktop OSes and browsers. I've used GopherJS to run ivy, Rob Pike's personal Go project, in the browser. Try it at https://dmitri.shuralyov.com/projects/ivy/. I've made a little tic-tac-toe game in Go that runs in browsers https://dmitri.shuralyov.com/projects/tictactoe/. It's really [simple, nice Go code](https://github.com/shurcooL/tictactoe) underneath. I've rewritten a small motion blur demo from really really ugly JavaScript into absolutely beautiful Go, and it continues to run in the browser at 60+ FPS thanks to GopherJS. Feel free to play with https://dmitri.shuralyov.com/projects/MotionBlurDemo/MotionBlurDemo.html. This little fun mini project, built in a day, runs in the browser... Is written in Go (click on source code link to confirm). https://dmitri.shuralyov.com/projects/hash-emoji/ I started working on this Go AST explorer (not completely finished yet), it imports go/parser package to do its job. No way I could've done that without GopherJS, I wouldn't want to attempt to rewrite all of go/parser in JavaScript... https://dmitri.shuralyov.com/projects/AST-explorer/ Another fun fact, if you look at my resume... It's generated entirely on frontend. view-source:https://dmitri.shuralyov.com/resume is proof. I did that because it was more fun than just a simple static .html page, and because I wanted the tooltips to be computed dynamically instead of manually. So I'm someone who's been actively using GopherJS to achieve pretty much everything on the frontend, instead of JavaScript. It's my go-to tool and I haven't run into anything that I couldn't do with it, or things that I'd prefer to do in JavaScript instead. I'm really not a huge of JavaScript, and being able to make stuff run on frontend with Go has been an absolute blessing. It's actually really fun to work on frontend stuff for me now. I look at GopherJS as an intermediate transition tool. Within a few years, I expect Go will gain the ability to target WebAssembly, which will have matured and they'll get to the milestone of allowing DOM access, etc. Then, all my current frontend-specific Go code will continue to work (perhaps with minor modifications) and I can continue to do my frontend work with Go. Until then, the performance and everything else is good enough, I haven't really hit any walls, at least nothing that can't be resolved just by working on the problem. But I also enjoy the process of paving the path no one's travelled before, much more so than being annoyed with JavaScript and continuing to use it. Hope that helps!
&gt; To give you an example why you should not do it - on x86 systems unsafe cast from []rune to []int is safe while on amd64 it will lead to segfault at best and to memory curruption at worst. Out of curiosity, why is this? Different alignment requirements?
What's not to love about jazz vocal improv? Be dee dee dee do whoop!
&gt; which you would choose if you were choosing one. I think this is the wrong forum for this question. It will be hugely biased towards go.
rune is 32 bits on every platform, while int is platform dependant so it's either 32 or 64. Or maybe even 128 in distant future on exotic platforms. 
&gt; my reply to the "Who's using GopherJS" thread I understand the point of Printf and converting modules from Golang to javascript, a lot of dependencies, a lot of things to be converted but as a frontend developer I would rather have modules like Printf to just compile to "console.log();" what is actually the advantage of having fmt in the browser? Maybe I'm seeing it wrong... It is true that file size can be lowered with minification and gzip but for me, that is the wrong approach, to rely on that. First of all, the browser will still need to parse and run the javascript which can become a real bottleneck down the road with so many lines and then if we give that approach a change will be thinking like javascript fans instead of c fans :P (trying to be funny here). I still want my projects to have as low memory consumption and bandwidth as possible. I'm not one of those that thinks "if I have 32gb of memory why shouldn't I use them?" instead I think "what if I just had 128mb?" Besides all that, frontend projects like babel and webpack bring frontend years of experience of building, tree shaking and optimizing. Go wasn't created with that in mind. That said, why not to use what as evolved for so many years? Besides that, using something like webpack for the build would even help you on the convertion process or convincing peers to use "go-to-js transpiler" because you could build on top of it instead of having all your code in Go. Just an idea... I too love the idea of Go instead of javascript. Been a frontend developer for 10+ and everyday it passes, I hate javascript a little bit more. Even with tests and ES6, it still isn't good enough. Too many ways to go wrong, lack of actual compiling (I don't like TypeScript). I've been working with Go for this past week more actively and it is exactly the opposite, everyday that passes, I love it more.
Came for the porn , was disappointed :D
I changed it back, for it is really randomized.
Sadly the article doesn't really address consistency, it's only showing off that etcd is faster and uses less resources of the *contenders*.
I never had any problems with Etcd but know people who had problems with consul, but it was long time ago. I believe raft implementations are more mature now, to the point that we take consistency for granted. 
Hey @aesthetickz, thanks for the answer! But how would I go about doing what you're suggesting? I'm struggling to understand sorry! :)
Well, it's a benchmark article, right? etcd was also [tested with Jepsen](https://aphyr.com/posts/316-call-me-maybe-etcd-and-consul) for consistency (and there have been a couple of problems which AFAIK have since been fixed) but it's a different problem, so there really isn't a lot to say about it here.
The play link is broken.
All 3 applications are consistent stores. Consul and etcd use raft, zookeeper uses paxos. They are all CP systems.
Is there a feature request on rclone for multiple file uploads? I tried to check but saw none and I'm on my phone, so my google-fu is suffering.
I've done a lot of projects like this. To get started quick you can do this. You can structure the repo differently, this is just an example. `/public` folder in your repo has npm/bower configs whatever you wanna use to get Polymer etc. You'll end up with `/public/bower_components`. Just make a `/public/index.html` to run your polymer app (look at their examples). This is where you do your web dev. Now in the go server, just server the repo `./public` dir as the root path `/`. Then do your API handlers, these will be matched before going to the generic `/` handler. // serves your static polymer app // if not spesific hander is found, always goes here http.Handle("/", http.FileServer(http.Dir("./public"))) // v1 in the path if you one day want to change the API but keep old things working // (mainly relevant if you have an API other than your code will be calling) http.HandleFunc("/api/v1/get/x", handleGetX) http.HandleFunc("/api/v1/set/x", handleSetX) http.ListenAndServe(":8080", nil) This should get you started. At some point you might want to make the index.html a template and run some data against it. Or check for auth in the request and redirect to a login page etc. Then you no longer use `FileServer`, but you can built those things in over time.
Having set up Zookeeper ( for kafka ), Consul ( for vault ) and etcd ( for kubernetes )... I'll take etcd any fucking day of the week.
Hey it's great to see more Polymer people around! I know Captain Codeman who I see on this subreddit is very active in the Polymer slack which is where you should go with Polymer questions. In terms of your question, your Go backend would be serving JSON and on the Polymer side you grab the data using the &lt;iron-ajax&gt; element. Let us know if you have more questions :D
Are you using etcd3?
Depends what you mean by random. Did you read [/u/FogleMonster's post](https://www.reddit.com/r/golang/comments/5urvdv/unofficial_golang_faq/ddx8dal/)? IMO calling it random implies that you can pick a random map element by breaking from a map `for range` loop (which people will think implies uniform distribution); you can't. The order is non-deterministic, it is not random; there is a difference.
Yes. For a single core processor, it handled pretty OK. I was doing like some light web servers, as well as some harder tests using ray tracing. It performs very well, especially compared to interpreted languages like Python which routinely chew up most of the CPU
Wonder if docker's acquisition of infinit has anything to do with this. 
You might want to look at using the http method instead of having the action in the path. The former (using GET, PUT, POST, DELETE etc...) is more REST, the latter is more RPC. It will work but is not what many expect an API to be.
That not quicksort. Quicksort is explicitly an algorithm that mutates arrays. This actually illustrates the difference between go and Haskell pretty well. It may take you 2 lines of haskell to write qs [] = [] qs (x:xs) = (qs (filter (&lt;=x) xs)) ++ (x : qs (filter (&gt;x) xs))) but that does not run in O(nlogn) time, which is fairly important. In go your implementation would probably be 20 times as long and take much longer to write (assuming equal proficiency in both languages), but it would be hard to make a mistake about the running time. Haskell is really good at abstraction, sometimes to its own detriment. Edit: bugfix.
Interesting!
Here is yours with some minor bugs fixed: qs [] = [] qs (x:xs) = (qs (filter (&lt;=x) xs)) ++ (x : (qs (filter (&gt;x) xs))) Mine was arbitrarily restricted to Strings, but yours works equally with Strings. It would be interesting for someone who has both Haskell and Go installed on their computer to try both with big strings to see how slow the Haskell really is. An easy way to generate a big String in Haskell is: bigstring e = show (2^(2^e)) and give it a number such as 15 or 20 depending on how big you want the string to be. Then, instead of displaying the whole result, just display part of it, with take n, or its length or something, to not spend time watching your monitor output a lot of text. E.g. GHCi&gt; length (qs (bigstring 15)) 9865 (1.61 secs, 704,357,672 bytes) GHCi&gt; The above is interpreted Haskell. Compiling the functions changes the 1.61 seconds to 0.61 seconds on my computer. 
If your goal is a distributed kv store then you should stay away from etcd, consul or zookeeper. They weren't meant for that. Counter intuitively if you add more nodes to the cluster it makes the whole thing slower, because speed is not the goal, the goal is consistency and resistance to individual node failure. Why are they good for? The primary purpose is to act as a distributed lock, when you are building a distributed system and require coordination between nodes. Zookeeper is best for that purpose, it is battle tested and you can implement more advanced primitives, [for example](http://curator.apache.org/curator-recipes/). Etcd and consul has some primitives for synchronization, but at best they provide a semaphore. They were created to compete with zookeeper for service discovery. Zookeeper allows to created ephemeral keys which can be used this way, but etcd and consul is missing the point that you don't need a CP system for service discovery, AP is perfectly fine. Going to your original problem, you should ask yourself how much data will you store in your KV. Do you really need a distributed system? (most likely you don't). Don't over engineer and don't think what you will need in the future. RDBMS is much more efficient than other NoSQL databases and if your distributed system will only have few nodes you might be surprised that a single RDBMS node will be faster than them. If you still want a distributed store then it looks like those based on DynamoDB paper (Cassandra, Riak) seems to scale.
&gt; if you hire programmers and train them to use Haskell, they can't easily leave I certainly wouldn't use this criteria to choose. &gt; And it's not really a question of which you should choose, but a question of what the reasons might be for choosing one over the other. Here are some reasons why you might choose one or the other (mostly cultural/subjective): Do you like abstraction and purity? Do you like clever or very dense code? Do you like functional programming? Choose Haskell. Do you value pragmatism over purity? Do you like simple code (even if verbose)? Have you come to dislike clever code? Do you like imperative programming? Choose Go. 
Regarding Kafka, could the issues be due to Kafka being configured to store offsets in zookeeper? My experience with ZK is that is very low maintenance as long as you configure it to prune old logs (if you don't, it will eventually use all disk space, with history of changes)
The point is to compare code, and possibly time the code. It doesn't matter if it's a sort or what. Just anything that can be used as an example, to compare the languages, to understand their advantages and disadvantages.
I'm talking about initial setup. Maintencewise no big deal at all. I haven't had any maintence issues at all on any of them, though consul is a fairly new install (only about there weeks old, and only backing vault). etcd, for me was the easiest to setup, especially in a containerized environment. (I'm running all of these on kubernetes)
It seems useful, but you might want to update the README a little to explain what exactly it does. Presumably it cross compiles for the platforms you specify, but then what does it do with the binaries to attach them to the release? Does it rename them so they don't conflict? Does it tar or zip them into different packages so they can use the same binary names? If so, what else does it package with them? Where does it get the release notes to put with the tag on github? etc..
Is performance a huge concern? If so this microbenchmark would tell you very little and I don't think would be a useful comparison of languages. 
I wrote one that embeds the UI in into the Go binary for easy deployment (makes it easy to rollback front end the depends on backend API changes). [blog](http://blog.hackingthought.com/2015/08/putting-polymer-javascript-in-go-binary.html ). It is very old I use govendor instead of gb these days but everything else I would guess is the same. PM me if you have questions.
The issue is whether performance is a big issue. In other words, is there a big difference in performance, that would be a major concern for most types of software development? Or is it more like a few percent slower in some situations? A performance difference most people could safely ignore? Or a big difference they should take into account?
Huh. TIL. Thanks.
&gt; I call it the POGO stack ... Nice name &gt; Checkout the Google videos about Progressive Web Apps (PWAs) and the PRPL pattern for fast loading, mobile friendly apps. Polymer makes it much easier than some other frameworks do. And more importantly, keep in mind the moto of Polymer : "Use the platform"
Also [sync](http://godoc.org/sync)
Looks like an interesting project but I didn't found a "motivation" paragraph. When writing such tools you should literally convince the other party to use it, or at least - present them the use cases. And no - referencing to other utilities doesn't work because that requires the knowledge of said utilities. So - before you dive in tutorial you should probably show me(the user) the use cases, provide a basic example and optionally write about why did you write new utility. Keep in mind that this do NOT need to be objective or fully rationale - I just want to know the idea behind and what I can do with this tool, which I cannot do currently with existing tools, or can but with much more pain points. Other than that - looks like a fine tool. 
Hi. I'm struggling to understand what Plis actually does. Is it similar to Cobra? I did eventually find an example but it would be really helpful if there was a concrete example in the readme. It looks really interesting and I'll definitely have a play with it. Thanks!
You should post it already - it looks like a fine tool and with recent switch to default GOPATH this would also easy the pain of manually updating Go installation. Also - while I'm on the subject - do not forget to check hash of file you had downloaded - they are there for a reason a it would be a additional plus point for your tool. 
Thanks a lot for your feedback, the main idea behind Plis is to use it to automate stuff, I do some frontend stuff and before `angular-cli` was out I had to manually do a lot of the boilerplate code and code generation this was one motivation for plis. Plis is simply a tool to make a developers life easier, create a simple tool fast to do some tasks you need done.
Hey thanks for your feedback, docs of plis are still a work in progress I am going to add examples and create tools for projects that I use plis for. Plis uses `cobra` but plis is not a library it is supposed to be used as an application, you will write the tools in scripting languages (I chose this solution to make it easier for non go users and because it will be easier to install tools since there is no need for any compilation and the user does not have to install any other application to use plis besides git)
*explodes*
I'm glad you brought this up because it demonstrates that the iteration order is liable to change at the whim of the Go developers. The only thing [guaranteed by the specification](https://golang.org/ref/spec#For_statements) is: &gt;The iteration order over maps is not specified and is not guaranteed to be the same from one iteration to the next. As I explain in my writeup, it's misleading to call it "randomized" because it will in fact always iterate in the same order. The only random part about it is _where_ it starts in that order. For example, if your map contains [1,2,3,4], you might see an iteration sequence of [3,4,1,2] or [2,3,4,1], but never [4,3,2,1] ([Playground](https://play.golang.org/p/FKidwB1xDN)). And that part is shaky too because there are some elements that iteration will _never_ begin on. 
I will change it to markdown format and host it and some other articles on github. Welcome to contribute, :)
I just read your post carefully. Sorry, I really misunderstood it before. Yes, the iteration orders of builtin maps are really not fully randomized. I also think it is acceptable to say they are randomized. Anyway, I will improve the wording again in a day. :)
my new comment: https://www.reddit.com/r/golang/comments/5urvdv/unofficial_golang_faq/ddyvfqo/
Did you consider mqtt ? 
Thanks for the headsup. That's pretty scary stuff. Atomic add is fundamental 
Those were simple examples to drive the point home. I don't actually write code like that. He can choose what the API looks like. Granted probably could have used something more concrete.
This optimization must be pretty tame or we would have seen reports for some of the user space concurrent queues that use atomic increments / cas. Probably requires the enclosing function to never exit. So I'm not sure a scenario where this could break real code, would be interested to know the patterns behind any it does!
Hey, is your code on GitHub? I'm curious what it breaks.
Help wanted :)
Alright so I've been using and running Elasticsearch in production for about three years at scale (near-petabyte amount of data, hundreds of billions of documents, 50-100 node clusters). I can't say I completely agree with your assessment but I don't want to disagree either because Elasticsearch is definitely hard to manage if you do it wrong (ie. try to do things that the service was not really designed for). I realize everyone's needs are different so don't intended to get into an argument but I would like you to explain a few things more if you don't mind. From what you're describing it sounds like many of the issues you're having are because of how your data set is structured which can have a massive impact on how reliable Elasticsearch can appear to be. &gt; wasn't created as a distributed datastore and it was an after design addition, hence why it is sharded rather than distributed with replica. Couple of questions: * What do you mean it's not a distributed data store? * What do you mean "sharded rather than distributed with replica"? Elasticsearch has both shards and replicas and they're designed for different purposes. &gt; This leads to the problems where the load gets higher than the current capacity of the nodes and you need to add a node to the cluster. At which point, you'll need to reindex all your data. * Could you explain "load gets higher than the current capacity of nodes"? Are you talking about system resources such as RAM and CPU or something else? * Why do you need to reindex all of your data just to add a node? In our clusters this would be nearly impossible and we add/remove nodes all the time. Are you needing to reindex because your shards are too big and therefor take a very long time to move? &gt; This comes with a whole host of problems such as keeping the multi-node cluster happy when there is a net split, timeouts and so on. * "...when there is a net split" - What's a 'net split'? Are you talking about a split-brain problem with the masters and if so how many masters do you have? * "timeouts" - What kind of timeouts? Searches, indexing issues, master related ops such as shards moving around etc? At least in our clusters we rarely have issues with relocations timing out because we've kept our shards small, 20-50GB, it used to be much worse and be the cause of a massive number of issues until we got that under control. Some operations (such as excluding nodes from allocation, changing cluster settings, etc) just needed a longer master timeout to be requested. 
Ah, those should be under control flow that isn't a tight loop yea? If it's broken I would file a separate issue with a minimal reproduce.
In the case of the former you should use [time.Tick](https://golang.org/pkg/time/#Tick)
Doesn't tick block until the tick is received? If it takes 31 seconds the next iteration would start immediately. Edit: nvm I looked at the source and you are right NewTicker does drop messages for slow receivers
I looked into using goa based on this comment and it looks like gold - really well done, but it doesn't fit the same type of framework pattern as these other options (go-kit, micro). It appears to be a fantastic API code-generating project, complete with go-swagger and a nice DSL. It's well, well documented and the website is almost professionally done - hats off to the team. What I'm missing about it though is that it seems to miss any integration with any service discovery mechanisms, metrics, etc. I've seen in the others. No plugins for messaging services. What am I missing? Where is the "microservices" part of it? Is it just that I should be writing all this glue myself?
Thanks for the recommendation - this almost makes me want to go to go-kit, but I still have some fundamental problems. Code generation is good, but all the complicated wiring doesn't give me confidence in maintaining the code long term. I get why the go-kit author wants something which isn't opinionated, but golang itself is for a reason.
Ah, didn't know about that tool. That is nice if you want to have one installed at a time and use that. The packages for Linux that you can download are ready-to-use binaries, that just needs to be extracted. It's also very portable, which makes it easy to have several versions of Go ready in different folders, where you basically just need to update the PATH and GOROOT environment variables to change which one to use. The tool here is suppose to make this as easy as possible.
Great explaination, it made pointers more understand-able for me, thank you. Still I'm not sure if everything is clear for me, but just tinkered with some samples I found problematic and I can solve it, so - I suppose I get basics finally. Thank you very, very much
I wanna reply to one specific part of your question that no one seems to have covered. &gt;In that case, do I need to pass argument with pointer every time? Yes you do, but not explicitly. Check out https://play.golang.org/p/lylFAxW6Tb In well written go, most variables in your program fall into 1 of 2 categories * Always used as a value * Always used as a pointer It's relatively rare that you actually need to treat a variable sometimes as a value and other times as a pointer. By declaring your variable the right way up front, you won't have to use &amp;s and *s everywhere it get it to compile. 
For now I'm going to stick with all these &amp; and * to get more used to this mechanics, but in general it seems really useful. No problems if some structs are assigned to one type?
I figured it out. Silly me, I'd say :D I was mislead because * is next to type, and I thought that declaring var Thorn *Place could overwrite Place for whole eternity.
/u/gchain Awesome! Thanks for staring this. You might find some others interested in our slack as well. #apm.
Is it possible to persist it somewhere ?
Uff, I did miss your point, indeed! Very interesting point ðŸ˜‰ I created an issue and replied to your questions and suggestion there: https://gitlab.com/Roman2K/scat/issues/14
Himago is a command-line tool that downloads high-resolution images that were originally taken from the Himawari 8 satellite. I posted this to /r/golang around a month ago and have added a few features since then: - You can now specify an electromagnetic band on the command line. Produces a white foreground image - Make some colourful images by specifying a background colour. See examples on GitHub. - Updated to use github.com/ogier/pflag for command-line flags- simple and effective. - Refactored and added a bunch of unit tests Thanks for the feedback last time. Let me know if there are any features you'd like to see! 
A few suggestions: Add some tests. I don't trust Go code without them. You could probably write some benchmarks as well and use them along with pprof to optimize things once your tests are in place. hyperbitbit.NewHyperBitBit should just be hyperbitbit.New. The current name stutters a lot. Your example binary is just called "demo". Even something like "hbbdemo" would be more descriptive when installed in a user's $GOPATH/bin. Finally, please add proper documentation comments, and not just stubs like "// Add ...". Also add a package comment while you're at it. Even though your package is simple, it's frustrating to view a documentation page with little to no details, like this: https://godoc.org/github.com/seiflotfy/hyperbitbit.
Pointers are values like structs or numbers. In fact, pointers are integers. But they have a special functionality: they are numbers that refer to another value. They are an address, like 0xe4a5 or 0x2af6 and go is aware of that. When you pass any value to a function, that value is copied, NO EXCEPTION. If you pass a struct value to a function and you change the value inside the function, you change a copy of the struct value and not the struct value itself. foo := SomeStruct{x:1} add1toX(foo) // func(f SomeStruct) // foo.x is still 1 In order to change the struct value outside the function you need to pass a pointer to that struct instead of the struct itself. The value of the pointer is the address of the struct in your computer's memory, ( 0x5a9b17 for instance ) That way Go knows that you are referring to the outer struct and not a copy and when you do f.x = f.x + 1 , go will know that f is a pointer that points to SomeStruct foo and has a field x. It is called de-referencing. foo := SomeStruct{x:1} add1toX(&amp;foo) // func(f *SomeStruct) // foo.x is now 2 You can also declare a pointer to struct directly foo := &amp;SomeStruct{x:1} add1toX(foo) // func(f *SomeStruct) // foo.x is now 2 Remember that any time you have an operation involving changing a value inside a function call, you need to pass a pointer to that value and not the value directly, or the value will just be copied and any change on the copy won't affect the original value. So all you need to remember is that : - pointers are integers. - when a value is passed to a function that value IS ALWAYS COPIED. and any modification inside the function will not affect the original value - in order to refer to the same value inside the function and outside the function, a pointer to that value needs to be passed to the function instead of the value itself. So most of the time &amp; means "take the address of that variable" and reference this address instead of that variable directly. But when you write a function in Go, you need to use * which is means : accept a pointer of something as an argument. so &amp; is for getting a pointer of something , * is for accepting a pointer of something. Now imagine you want to replace an integer inside a function. https://play.golang.org/p/PsvJ5QJJ2v a := 1 // a is an integer value func(integerPtr *int){ // integerPtr is a pointer to an integer // replacing 1 with 10 *integerPtr = 10 }(&amp;a) // take and pass the address of a // a is now 10 This is the only time when you need to use * ouside a function signature. it means replace the value of whatever pointer with another value.
I think the &amp; and \* are confusing because whenever people are talking about those *operators*, they use terminology without explaining the terminology and how the positions of the symbols matters. For instance, what does (var p \*int) even mean to someone who doesn't know how memory is allocated in a program? It should really be explained step by step in the simplest way possible, so I'll try with some examples. Lets start with: var p \*int : this means that variable p *is* a pointer *to* a value of the datatype int. a := p : this would initialize a variable called "a" that is a pointer *of* the same value as p *to* a value of datatype int. So pointers are values in this way and they hold the *memory address* of whatever value they're referencing. "a" and "p" would be two variables in two *different* address spaces but would *hold* that same value. Every new variable you create has its own address space. If you want to refer to a variable using only one address space, don't declare any new variables! \*p : this is very different from (var p \*int) because notice the position of the (\*) and *this is simply syntax*. In fact, it's confusing because this is *not* a pointer. It's called *dereferencing* (because pointers *reference*) and it gets that value of datatype int instead of returning the address which would just be plain "p". In this case, the output would be &lt;nil&gt; because we haven't pointed "p" to anything. &amp;p : this tells you the memory address of the variable "p" that was instantiated in this program. \*p = 1 : this would *throw an error* because "p" is not pointing to anything so you can't set the value of a nonexistent variable to a value. So instead declare a variable of int value first and then point p to it. (z := 1) then, (p = &amp;z). Then you can change the value of "z" through "p" like (\*p = 2) then "z" will equal 2. b := \*p : this would be how you *get* (rather than *set*) the value of datatype int that p is pointing to. (\*p) is the value that is at the address value "p". Again, (\*p) is *not* a pointer and neither is "b". They are the values *of* the pointer "p" (which is an instantiated variable), in this case "z". c := &amp;b : this would initialize a variable "c" that *gets* the *memory address* of the value "b" (or the value in \*p but at a different memory address since "b" and \*p are distinct) and this *is* a pointer because the value that "c" holds is a *memory address*. &amp;b is *not* a pointer, the variable "c" is the pointer. Remember to distinguish *operations* from *variables* and *values*, which are all different things. **** **EDIT**: wrong_pointers brought up mistakes in the previous version. The statement below is incorrect. c:= &amp;(b) (==) c:= &amp;(\*p) (==) c:= p *Corrected version*: c := &amp;b (==) c:= &amp;(\*p) is **false** it should be false because b is holding the same value as \*p but in different address c:= &amp;(\*p) (==) c:= p is **true** This should be true and correct because dereferencing and referencing gives you same pointer c := &amp;b (==) c:= p is **false** it should be false because b's pointer is pointing to b's address which is different than z's address **** d := &amp;c : d is a pointer *of* a pointer. It holds the memory location of the variable c which holds the memory location of "b" which is a value of datatype int. e := &amp;(&amp;c) : this would *throw an error* because &amp;c *isn't* a variable with a memory address so you can't get an address for it. Remember that "d" and "&amp;c" are two *different* things. e := &amp;(d) would work just fine because "d" has an address somewhere because it was initialized in the program. f := \*(&amp;c) : "f" would be the value of the address space of the pointer "b". Why is that? (&amp;c) gets you the address for "c" and "c" is a pointer to "b" from (c := &amp;b), and *not* (c:= &amp;(\*p)) since (\*p) is an operation and not a variable. That means if we deference (&amp;c) we get the value in the address space *of* "c" which is the address for "b" (c := &amp;b).
Are you referring to this change: https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw (with the related https://docs.google.com/document/d/1ETuA2IOmnaQ4j81AtTGT40Y4_Jr6_IDASEKg0t0dBR8/edit?usp=sharing )? 
Fortunately the cons of M:N threading is handled for us by the built-in runtime of Go. This let's the experts on the subject really be the only ones mucking about down at that level. And as we've seen Go is a really damn fast language so it seems to be on point with this. Of the three cons to M:N threading, I think the second is the one most relevant to your concern. The following link might help in clarifying how this case is handled. https://www.slideshare.net/mobile/matthewrdale/demystifying-the-go-scheduler 
I'm glad that you got some good help here, but if you do want to, the [Golang Slack](https://blog.gopheracademy.com/gophers-slack-community/) has a newbies channel to help with beginners. I've found excellent help there in the past.
Thanks to all the comments! More are definitely welcome though. I was recently digging into this and found Go to be a language that implements this. Any other languages doing similar things? Also, Go is definitely a very performant language so that definitely isn't a concern here.
Not precisely, but if you assign hello.var1 to another variable, then it will copy, just like it would copy an integer. However, if you write `hello.var1.a += 1` then var1 won't be copied. &gt; so, isn't it better to use pointer of struct as element of other struct? That depends on what you want to achieve. If you need to share a member of your struct, you can use a pointer. But a pointer adds a little bit of extra runtime and memory too, so if you don't need to share (i.e. you have a unique var2 per Hello object) or you are concerned about CPU/memory usage, then embedding the struct directly makes more sense. But even that is not completely accurate (it also depends on the way in which you access the embedded object and how frequently you do it), so try to understand the basic use of both and keep it simple.
supposed i dont need to modify the data inside struct, only accessing the elements. but still, isnt it better to define type Foo struct { Mybar *Bar } instead of type Foo struct { Mybar Bar } from https://play.golang.org/p/owXcQ-i82G, i tested behavior each time accessing Foo.Mybar it gave a new copy where Mybar is struct, not a pointer. sorry for no formatting. i'm using phone, no available computer right now
pointer adds extra runtime? can you explain a bit? is it related to runtime stack?
You're not that close to the end of the line. Even at the top end of the estimates you're looking at 10GB out of 16GB. Also so far for me Gogland has only used 2GB of RAM even with a massive project like kubernetes.
&gt;1.5 MB for a 4 line plugin! &gt;Iâ€™m not sure why this is. This was built on a beta version of Go, &gt;maybe that will change for the final release. That's pretty much the default size of a Go binary when you are building it with FMT or any other import since it builds stuff into that one giant binary. For example a simple hello world program containing just this: package main import "fmt" func main() { fmt.Println("vim-go") } Will have a size of 1.6M. -rwxr-xr-x 1 x staff 1.6M 21 Feb 09:49 hello -rw-r--r-- 1 x staff 67B 21 Feb 09:49 main.go
Erlang (or the Beam VM in general) also utilize the M:N model.
Thanks for that *compendium*, it clarified a lot of pointers' magic for me :) Pity I didn't find this sort of explainations in any of beginner-adressed resources I checked. Don't know, maybe I chose wrong books or so...
No github/gh-ost?
Thanks! By &gt; dereferencing and referencing gives you same pointer you mean "the same adress"?
Thanks for pointing this out! I will take a look at this project and update the post accordingly :)
man this is pretty cool, thank you!
A bit offtopic, but let me ask... Your post (thanks!) makes me interested in differences between function values and receivers. After some reading (for example [this article](https://grisha.org/blog/2016/09/22/golang-receiver-vs-function/), also [go faq](https://golang.org/doc/faq#methods_on_values_or_pointers), also some stackoverflow questions) and I find them pretty similar. Are they truly equivalent? 
for range time.Tick(30*time.Second) { ...... }
Alright thanks, even though needing an external tool makes it a bit harder to grasp it on e.g. github, but that's how the language works I guess :) thanks
It all depends on exactly what it's asked to create. Full blown methods on classes, probably not. The overall goal for this was to not have to re-create changes on the client every time the server API models changed. You change a model, run typewriter, and see what broke on the client side via whatever type-system you're using. It uses text/templates, so it's not unrealistic to send types of any language out, since all it is, is a new template. Any example of what you're striving for?
Just use the standard library. Grab a 3rd party router if you need it.
The flag package gets a lot of hate, but the only real problems I've had with it are that there's no easy way to do aliases (ie '-q' or '--quiet' are the same thing), and that there's no way to tell the difference between "this flag was not passed" and "this flag was passed with the same value as the default value" with out annoying hacks..
&gt; It says about identical types. Two interface type are identical if theyâ€™ve exactly the same method set This is not right. In particular, it is only right for two unnamed interface types. Two named interface types are never identical. The first assignability case doesn't apply here. It is also the 3rd case of assignability apply here. In other words, in this example, I1 and I2 implement each other. 
It's not really needed. The compiler catches it. If you try to pass something that's *supposed* to implement an interface, but doesn't, to something that *expects* that interface, the program will not compile.
M:N decouples concurrency (M) from parallelism (N). Many (or most) large, complex codebases will have to address this issue at some point, often by implementing things like threadpools and event loops. For an example of this, see nginx. While a naive program can simply spawn unlimited new OS threads, a more sophisticated and scalable solution requires controlling the number of "real" threads while efficiently distributing work units across them. This is complex, error-prone work that everyone has to implement themselves, and is not at all related to the actual problem domain the programs are intended to address. With Go, you have a solution to this problem out of the box. Furthermore, it's done in a clever, relatively lightweight way (both computationally and cognitively).
Yes, you're absolutely right. Fixed. Thank you.
GoBuffalo
The compiler knows what you want to implement in the sense that anything you satisfy the requirements for, you already do implement. 
net/http
But let's say I have an interface Runnable with a run method. So every single struct with a run method actually implicitly implements the Runnable interface right? What if someone intended the run method for something else (like a Person walking 5 miles). In the code it would actually think, hey the run method works, you can pass the object here without any problem. Or do I overlook something?
Subcommands are different than flags. The flag package works fine for flags, but it doesn't support subcommands.
Check out new package: https://github.com/clbanning/checkxml
I'm enjoying using [echo](https://echo.labstack.com/) as a micro web framework *a lot*. Also, [gorm](http://jinzhu.me/gorm/) for your orm needs.
If you don't mind, I'm going to open a issue on ponzu to formally introduce the feature into planning and tag you and OP in it. Would be grateful for your feedback given your proximity to all of the parts involved. 
Or sqlx 
I wouldn't really recommend GORM anymore: * Automigrate is so basic that it won't even put constraints on foreign keys (unless that has been fixed since, you just end up with simple integer fields without constraints on them) * Embedding *gorm.Model is weird, it adds columns like deleted_at and encourages soft deletes, leaving the record behind and marking it as deleted instead. * I have read multiple times now on Reddit golang that the code base is pretty bad and in dire need of fixing up, all while better alternatives have popped up. See: https://www.reddit.com/r/golang/comments/5pgma4/my_quest_to_improve_gorm_the_object_relational/ Also sqlx is low-level, not high-level, it encourages you to write SQL by hand which makes it low-level in my books.
For anyone interested, here's an open issue to host a discussion about this feature in Ponzu: https://github.com/ponzu-cms/ponzu/issues/77
Delve doesn't support debugging of tests. I've found this feature in Gogland to be extremely helpful.
No worries, wasn't sure if the package was "done" or not, but wanted to offer some feedback anyway.
Whoa! This was a lot of work! Thanks for sharing. 
I've had success with this library: https://github.com/neelance/graphql-go I quite like the design, they use reflection in a cool way - it results in some puzzling type errors at first but once i got the hang of it (the example they provide is essential) I found I'm quite productive with it.
Receivers let you use interfaces and polymorphism, functions do not. [Interface Example](https://play.golang.org/p/mv0nuiejXR). Multiple types can satisfy an interface by defining methods on a receiver of that type. The interface can be used in place of the type, when you want a `Speaker` either `John` or `Mary` will do, or any other type that has `Speak()` defined. Note however, that `*John` implements `Speaker`, but `John` does not - as a pointer receiver is used. This also lets you define the same method name on different types, or polymorphism. Without receivers, [this](https://play.golang.org/p/AWBO9n3lW8) won't compile because `Speak()` gets redefined; this is what is meant by no generics support in Go. Interfaces are a very powerful part of Go, and used extensively throughout the standard library. As an example, [here](https://golang.org/pkg/io/#Writer) is the `io.Writer` interface - any type that implements a `Write(p []byte) (n int, err error)` method can be used as an `io.Writer`.
There's now a fix: https://go-review.googlesource.com/#/c/37333/
I will do so. Thank you.
I'm an engineer, not an academic, unfortunately. :) Seems reasonable to me but I'd ask someone with more domain knowledge.
What is risky about node replacement and failure? Can you be more specific so we can help fix any issues?
Sounds like good old bash scripting is what you need! Go would be suitable for interacting with kernel apis, not tools that already exist on the system. From my experience getting external cli applications to run the way you want can be fiddly 
I'm not really sure now what I would want out of a Go ORM, I have spent quite a few years with Django, and in the last year I have done a lot of very complex SQLAlchemy querying using the ORM and SQLAlchemy query like language. I am kind of over ORM's I think after that project, SQLAlchemy is a complex beast to master but it taught me a lot more about just writing SQL by hand instead. So I was more thinking of avoiding ORMs all together in Go and just doing raw SQL instead.
[bosh](https://github.com/cloudfoundry/bosh) even has a go component ([bosh-agent](https://github.com/cloudfoundry/bosh-agent)) that does, among many other things, exactly what the OP describes above
I'm a big proponent that it actually creates a much better user experience for most sites. It keeps the client dumb, and browsers are actually quite good at rendering html quickly if you can imagine that :). It makes for much snappier and more predictable navigation. I've found that client-side rendering is best for applications that you want to mimic native applications. If your web app requires a complicated front-end that requires interacting with a single global interface, then SPA is the way to go. Everything else(which is most people) should avoid SPAs imo. They are very hard to get right and your navigation will most likely end up buggy.
I'm pretty happy with my own implementation: https://www.npmjs.com/package/pasar I started it about two years ago and I'm currently using it in a few production apps. Documentation is not currently finished (just a draft to ensure not forgiving anything when I find the time to complete it). I apologize for that. But I tryed to document almost all essential things and to provide clues enougth to discover the rest. Of course. You can contact me for any questions you have if you finally find yourself interested in it. Not all "planned" features are yet implemented, but all esentials and most important ones are. Hope you could find it helpfull. I for sure will continue developing it as soon I have the time to spend in it (or the need for any of the projects in which I use it). 
The Go Programming Language by Donovan and Kernighan is wonderful. 
I love it how this sub gets this same question on a weekly basis.
I personally like httprouter. While it is supposed to be fast, I just really like it because I find it easier to create my rest endpoints when I declare them per method and per path vs. how the default net/http mux works. However, this is personal preference more than anything else.
Glad you enjoyed it. I'll do my best to finish up the generic netlink post either late this week or early next week.
From what I can tell, they are using a federated model: &gt;Universal means that no single entity maintains the data; Upspin is in effect a federation. (Key management works well with a single space of keys, but it is not a requirement that a single server host all the keys; the current single server can and likely will develop mirrors and replicas.) Unrelated: It's very interesting to see that Rob Pike is contributing heavily to this project. That alone makes me take it more seriously.
It provides a mechanism to do setup and teardown while avoiding instances from being shared in the parallel tests.
What topics interest you? Are you willing to go with ebooks or video courses as well? There are books on web dev, writing an interpreter, go in general, concurrency, etc. Are any of those topics more interesting than others? I am writing Web Development with Go (www.usegolang.com). You build a web app from scratch and eventually deploy it. If an interpreter interests you, https://interpreterbook.com might be a good fit. If you want more general books on Go The Go Programming Language and Go in Action are good. (http://www.gopl.io and http://goinactionbook.com) I am probably missing a few...
Can someone contrast this with Camlistore or IPFS for me? How is this new, different, better?
Yes. Adding or removing a return will. Adding, or remove, or renaming the name itself will not.
In addition to what has been said already in the other answers: &gt; Like when I have a huge'ish interface Then you have a Java interface. In Go, interfaces are kept small. Interfaces with only one or two methods are common. Read e.g. the section "Liskov Substitution Principle" in [SOLID Go Design](Liskov Substitution Principle) about the intention behind Go-style interfaces. (*Effective Go* also [promotes small interfaces](https://golang.org/doc/effective_go.html#interfaces) but without further explanation.)
&gt; Go disregards 40+ years of developments in programming language theory From another viewpoint: Go is the product of 40 years of practical experience which says that a simple, less expressive language is more productive and performant -- particularly so when you make room for one of the seminal theoretical works of the last 40 years, which every other language completely missed even though UNIX put it front and center, Communicating Sequential Processes. &gt; going off of your definition of simplicity, haskell is far simpler (in that its more succinctly/tersely expressive) I have never seen an example of succinctness/terseness improving simplicity. In fact, both are generally the product of developers who strive for cleverness, something developers should avoid like the plague. I say this with 20/20 hindsight of the terrible things I've done in the past while worshipping both. :-)
That further reinforces the point that this wheel has been sufficiently invented. :)
If you want to create Android applications then my suggestion is stick to Java/Android. Go is yet to be a good alternative for this kind of platform. Go is mostly useful for backend systems (servers, network related apps) and while there's a lot of work getting Go to be more useful in other areas, these are were it currently shines. 
It's not just back-end web development, it's back-end everything. You could e.g. build the server side (if any) of your app with Go. If I remember correctly you can even compile Go for Android but I think the GUI part of an Android app still has to be written in Java.
This is a part of my book chapter, 12 Factor Apps with Go and Docker, where I give an example of the principles of passing configuration via environment variables. All comments welcome ;)
This looks seriously interesting and reminds me of how email works but with files instead. I hope this project finds good attention!
I don't think there's a Go lib that does this currently. Some migration libs allow you have a separate function or SQL that runs when it's a clean database, but you have to write that yourself. What you can try (at least for PG), is using the dump of the schema (without the data). It would be like Rails' `structure.sql` file.
Wuzz is a client, this is a server
[removed]
This is also where reallocation rules come into play. For example: slice := []uint16{1, 2} for i, v := range slice { slice = append(slice, 65535) if i == 0 { slice[i+1] = 65535 } fmt.Print(v, " ") } fmt.Println() fmt.Println(slice) will produce: 1 2 [1 65535 65535 65535] However: slice := append([]uint16{}, 1, 2) for i, v := range slice { slice = append(slice, 65535) if i == 0 { slice[i+1] = 65535 } fmt.Print(v, " ") } fmt.Println() fmt.Println(slice) will output: 1 65535 [1 65535 65535 65535]
&gt; It seems like the command should have the responsibility of building its own flagset. That makes sense because then you get better integration with the way the flags want to work, because you can save the pointers somewhere useful at a time when the command exists. My setup is a bit funkier in that area. However, I do have a ParseFlags([]string) on mine, too, so that's not connected. I also set up my execution to be `Run(Env) int`, where `Env` is type Env interface { Stdin() io.Reader Stdout() io.Writer Stderr() io.Writer Exit(int) int } so I can test the whole command even if it chooses to use the IO streams or exits. I don't have Exit panic with my test environment, though, right now it's just a wrapper around either calling os.Exit or returning an int and only works from the top-level of the command. I didn't even notice that limitation until I was posting this here and I've written a couple dozen commands, so it can't be that fundamental. (Just as functions really shouldn't panic without reason, code that is not explicitly written with the guarantee that it is the sole and singular "subcommand" that is being executed shouldn't call os.Exit. That's even worse than a bare panic.)
&gt; For example, is the Haskell code below hard to understand and follow? Yes. 
I was under the impression that r.Body = http.MaxBytesReader(w, r.Body, maxSize) already reads the request body up to maxSize. But that's not the case right, as it is a ReadCloser? So it's more like saying, when this Reader is used it is only allowed to read maxSize bytes? And ParseMultipartForm actually uses that Reader which returns the error as it exceeds the defined maximum?
So it looks like I may have solved it. I hope this helps someone in the future. I had a file upload. However, before the file was fully saved and moved, I was adding a new record to the database. If the internet connection was slow, the file upload was timing out so I had a record with no file, and when trying again I got a record with a file (if successfully uploaded). Now why it went to IPv4 on the second try, I have no idea. Which now leads me to another question. Is there a built-in way to allow the timeout for a file upload to be longer than the general read/write timeouts?
Probably because those aren't Go topics, they are software engineering topics.
It went IPv4 the second time because of the timeout. The timeout is probably a network issue, not a "it took too long to do the upload" issue, so I don't think there's a way to control that.
That is the approach I am taking for a project. I have two structs for each entity. Like struct author with SQL billable fields, and then authorjson with string and int fields. I know you could achieve a similar result with pointers and custom marshal but I feel like this solution gives me more control. Not sure if idiotmatic but so far it's worked really well.
Although it was last updated in 2011, the 3 day Go course slides by Rob Pike are excellent. It looks like it's hard to find online right now so I uploaded the slides. Day 1: http://docdro.id/YwRvF9T Day 2: http://docdro.id/7kmLbzF Day 3: http://docdro.id/uMsCx8a
Alright thanks!
Piping into sh --&gt; bad Piping into *sudo* sh --&gt; nuts
Thats the thing. The fact that interfaces are implemented automatically means that when you get a Athlete struct from a third party lib that has run() as method. In your code you have the runner interface. You can now use the Athlete as runner in your code without modifying the third party lib.
I am happy to announce **bombardier v1.0**. This tool started as **wrk**/**weighttp** replacement for **Windows**. At this point I consider it performant, feature complete and polished enough for general audience. Features that I find appealing in comparison to other benchmarking tools: * both **timed** and **limited by number of requests** modes; * nice and informative **progress bar**; * much **lower memory consumption** in general(compared to **hey**, **vegeta**); * pretty **high performance**. On my machine up to **265k RPS**.
I looked into gopherjs quite a while back but didn't end up using it. I'm thrilled you've taken the time to thoroughly explore it and provide real world examples. I'm definitely going to take a look at what you've shared here, and very likely start using it myself. Thanks!
I see where it can be nice and useful. I was just thinking about the (even when unlikely) possibility that someone uses the run method in a different context. So the compiler would think "Hey, Athlete has a run method, so it's fine to pass it to the method". But it actually expects a thread-runnable like struct with a run method which should be executed. So both have a run, both satisfy the interface. But an Athlete with run() is not a Thread-Runnable with run() 
I was inspired by TJ Holowaychuk's Apex install script. Is there a better way to make the installation process more efficient? https://github.com/apex/apex 
&gt;Is there a better way to make the installation process more efficient? Yes, use actual packages... Or just link to the binary so people can put it where it belongs 
Can't say enough about how well paced and structured this book is!
Cool, been looking for a wrk replacement as I need more flexibility over my load testing. Just tried it out and I like it a lot, although I do get different results between this and wrk (about 10k req/s difference). How do you get 265k RPS?
Why is it so expensive? Â£32 on amazon
Anyone a fan of [fasthttp](https://github.com/valyala/fasthttp)? Echo v2 used to support it. Not sure why other web frameworks haven't thought of taking advantage of it yet.
It looks like a fantastic subject to write about! I'm highly tempted to buy it. In regards to Cutnunt's comment on price, I would probably expect this to be an expensive book since it is an advanced topic with a much smaller audience than a tutorial book would have. The author isn't getting rich on this book any time soon. That said, would you consider offering a free ebook to someone buying the paperback? I'm having a hard time springing for both, but at this price I'd be grateful to have an ebook too.
Probably because the book is printed on demand. There's no economy of scale here. Buy a book, they print it for you. 
The GC can be set to only be run manually. A good cross-platform GUI is easy to make with HTML/CSS/JavaScript.
You might be interested in https://github.com/markbates/pop/blob/master/nulls/README.md I ran across this last week. I haven't gotten to play with it yet, but it looks promising.
Hey guys, at work I have to use some CloudFormation and it was getting tedious, so I wrote this. It supports generating a parameters file from an existing template (in json or yaml) and will output json/yaml to either stdout or to a file if you specify it. By default, if you're attempting to add a new parameter to a template, it won't overwrite existing Parameters that it finds unless you specify the overwrite option. Also, if you remove a parameter in an existing template, you can use the -r flag to remove that parameter from the parameters file if it's no longer needed. I hope you all find it useful. Let me know if you have any questions/comments/requests! You can get the binary either by using go get as shown in the README, or you can get it from the releases page: https://github.com/alistanis/cf_parameter_generator/releases/latest
It's a pretty niche market.
Here: []string{"a","b","c","d","aa","ab","ac","ad","ba","bb","bc","bd","ca","cb","cc","cd","da","db","dc","dd","aaa","aab","aac","aad","aba","abb","abc","abd","aca","acb","acc","acd","ada","adb","adc","add","baa","bab","bac","bad","bba","bbb","bbc","bbd","bca","bcb","bcc","bcd","bda","bdb","bdc","bdd","caa","cab","cac","cad","cba","cbb","cbc","cbd","cca","ccb","ccc","ccd","cda","cdb","cdc","cdd","daa","dab","dac","dad","dba","dbb","dbc","dbd","dca","dcb","dcc","dcd","dda","ddb","ddc","ddd","aaaa","aaab","aaac","aaad","aaba","aabb","aabc","aabd","aaca","aacb","aacc","aacd","aada","aadb","aadc","aadd","abaa","abab","abac","abad","abba","abbb","abbc","abbd","abca","abcb","abcc","abcd","abda","abdb","abdc","abdd","acaa","acab","acac","acad","acba","acbb","acbc","acbd","acca","accb","accc","accd","acda","acdb","acdc","acdd","adaa","adab","adac","adad","adba","adbb","adbc","adbd","adca","adcb","adcc","adcd","adda","addb","addc","addd","baaa","baab","baac","baad","baba","babb","babc","babd","baca","bacb","bacc","bacd","bada","badb","badc","badd","bbaa","bbab","bbac","bbad","bbba","bbbb","bbbc","bbbd","bbca","bbcb","bbcc","bbcd","bbda","bbdb","bbdc","bbdd","bcaa","bcab","bcac","bcad","bcba","bcbb","bcbc","bcbd","bcca","bccb","bccc","bccd","bcda","bcdb","bcdc","bcdd","bdaa","bdab","bdac","bdad","bdba","bdbb","bdbc","bdbd","bdca","bdcb","bdcc","bdcd","bdda","bddb","bddc","bddd","caaa","caab","caac","caad","caba","cabb","cabc","cabd","caca","cacb","cacc","cacd","cada","cadb","cadc","cadd","cbaa","cbab","cbac","cbad","cbba","cbbb","cbbc","cbbd","cbca","cbcb","cbcc","cbcd","cbda","cbdb","cbdc","cbdd","ccaa","ccab","ccac","ccad","ccba","ccbb","ccbc","ccbd","ccca","cccb","cccc","cccd","ccda","ccdb","ccdc","ccdd","cdaa","cdab","cdac","cdad","cdba","cdbb","cdbc","cdbd","cdca","cdcb","cdcc","cdcd","cdda","cddb","cddc","cddd","daaa","daab","daac","daad","daba","dabb","dabc","dabd","daca","dacb","dacc","dacd","dada","dadb","dadc","dadd","dbaa","dbab","dbac","dbad","dbba","dbbb","dbbc","dbbd","dbca","dbcb","dbcc","dbcd","dbda","dbdb","dbdc","dbdd","dcaa","dcab","dcac","dcad","dcba","dcbb","dcbc","dcbd","dcca","dccb","dccc","dccd","dcda","dcdb","dcdc","dcdd","ddaa","ddab","ddac","ddad","ddba","ddbb","ddbc","ddbd","ddca","ddcb","ddcc","ddcd","ddda","dddb","dddc","dddd"}
It was a network issue. File upload is via cell phone and my signal was poor at the time. I added a timeout message to the user and duplicate records are handled properly.
I'll leave that as an exercise for the reader. ;)
[removed]
I wrote something similar to this in purpose a while ago, built upon the easyjson lexer https://github.com/WatchBeam/hasownproperty. It doesn't work as well with "native" decoding as your implementation does, though there are a few use cases :)
How many books on such a specific topic do you expect somebody to be able to sell? I'd be amazed if the author will break minimum wage for the time spent on this topic.
You can do the same thing *without* both. eg https://play.golang.org/p/DeFkaOCpEe Or you can replace the ptr with a boolean which I find easier to use. So again I'll ask - what is your reasoning for including both?
Oh that, yeah, it was just so we had a primitive in the struct, to more closely reflect the original code and to avoid the need for checking a pointer before dereferencing.
I think this method has the potential to become messy when done at a large scale. A better solution can be to use [JSON Patch](https://tools.ietf.org/html/rfc6902) for partial updates. I find it easier to map to database operations. 
It looks great! I'll bookmark it for now and order a copy as soon as I have the spare cash! Writing languages is something I've always been fascinated in, but have never found a good book to start with. Best of luck to the author and thanks for this.
Makes more sense. I prefer a bool but to each their own :)
I cannot speak to Java -&gt; Scala or Haskell. I can speak to Ruby -&gt; Go. No contest. Go is far simpler at every level. And good developers have been writing functional code for a long time, regardless...
Oh no! Yours produces different output than mine! One of them must be wrong! https://play.golang.org/p/TsbrWdAPDB 
Can you give an example? I've never used this and won't get a chance to read that doc for a bit. Looks interesting.
I just read the sample. Wow it's great! I'll be buying it next paycheck for sure and I'm super excited to read the next chapter
Sadly, no, that's a bit out of my reach :( Iâ€™m using createspace, an Amazon company, for printing and distribution. I don't have too much control over the whole selling process itself, since Amazon is taking care of that themselves. The ebook on the other hand is sold by myself, through Gumroad.com. That does allow me to offer coupons and discounts, but not in combination with a product in an Amazon store. Sorry!
I have not found one that works constantly. I use a little script I wrote to do it via textual search and replace with `ag` and `sed`. Search for the current import with `ag` and then do `ag-replace old/import new/import`. https://github.com/adamryman/dotfiles/blob/master/scripts/ag-replace.sh I know it is not ideal, though it does the job for me. Pretty sure you can use `gofmt` to do the text replace as well, though I have messed with it.
Could you elaborate more?
The book guides you along writing programming language. It gives very good insights on writing a parser, a lexer, a REPL, an AST (Abstract Syntax Tree), etc. It explains exactly what those are and why they are important. It explains the meaning of the language tokens too. Even though the language you create along with the boom is dynamically typed and interpreted, it offers great insights on how compiled language work as well. Rust has a few more features like the borrow checker that os unique to it, that's why I mentioned it helped me understand it just a little. And I don't want to discredit Java. It was great when it first came out. But learnings how you can implement a nicer language today makes me want to use modern languages like Go. Wrote on my phone. Sorry for typos. I'll edit them later.
I don't buy this. You are distributing the ebook yourself, surely you can check for proof of purchase and offer a copy. You don't have to, but surely it's within your reach. To be clear, I have no expectation that you offer both for a single price, but I'm not sure I like the "sorry, it's out of my control" explanation I've seen you give here as well as on HN.
`gomvpkg` is open source. Why would there be another tool that does the same thing, except with fewer bugs? I've never had issues or panics when using `gomvpkg`, it has been very a solid tool in my experience. But I used it a while ago. Maybe there's a regression, or maybe you're using an old or broken version. Did you try doing `go get -u golang.org/x/tools/cmd/gomvpkg`? What version of Go are you using? Your best bet is to report an issue at https://github.com/golang/go/issues.
Sure, I could do that, but probably only by hand and that's what I mean by "a bit out of my reach". I'm not distributing the ebook myself, that's done by Gumroad. They take care of accepting payments and ultimately delivering the goods (via email or via a link) to the buyer. I also send out free updates through Gumroad, which keeps track of the buyers for me and offers me that functionality. Checking "proof" of purchase means either checking that the buyer bought it on Amazon and offering a coupon for Gumroad. The problem with that is that I don't have access to the buyer data on Amazon/Createspace so I can't really "proof" a purchase other than trusting a forwarded email/screenshot. Or I'll do it the other way around and offer a reduced price on the paperback version if someone bought the ebook version, which doesn't work that well, since Amazon/Createspace give me a minimum price and no possibility to offer coupons. What I'm going to do is check out Amazon's Kindle MatchBook program which allows a buyer of a paperback version to purchase the Kindle version through the Amazon store at a reduced price.
Kindle version is going for $20, yours is going for $29. Why?
The second example is not a good one. It is compiler dependent and may only work for the default gc compiler. Changing &gt; slice := append([]uint16{}, 1, 2) to &gt; slice := make([]uint16, 2, 4) is determined and better. 
Works for me. https://play.golang.org/p/KaxuIPhiEa
you could use go to call python directly, interacting via STDIO see: [http://stackoverflow.com/questions/19397986/calling-python-function-from-go-and-getting-the-function-return-value](http://stackoverflow.com/questions/19397986/calling-python-function-from-go-and-getting-the-function-return-value) alternatively you could look into: https://github.com/sbinet/go-python 
You can use zmq/grpc + protobuf/flatbufs to call python code. I do that, but increasingly I am replacing most of my ML stuff with Go Also, I'm curious as to what is your demand that you can't do with Go
i want to purchase this book but there's no paypall support on the site. im not confident paying with gumroad, their site is not even https. edit: http://help.gumroad.com/safe-buying-on-gumroad why not use https all the way?
Are you spamming this three times a day now?
I'm not talking about writing a webapp though. Your ignorance stems from dividing the tech in to specific roles. It is in fact trivial to create a slick, responsive GUI purely in webapp tech and then wire it up to your existing software. I have several programs related to finding and downloading content from Usenet that work exactly like this. You run the software and control it through your browser. Just because you reach it through a browser and you have the option to interact with the system with a separate device does not mean that the web GUI is any less effective than a native one. The time for fat clients has come and gone.
Gokit.io enterprise grade toolkit 
No, but the goto is something that sticks out like a sore thumb to me in any program and I am curious what some legit cases for using it are.
I might pick up the Kindle version when I get some extra cash.
This here describes how to find the PayPal button in the payment form: http://blog.gumroad.com/post/120038042813/paypal Sometimes it takes a while to load the PayPal thingie, so maybe try refreshing and keeping an eye on the area where it's supposed to show up.
&gt;*Note: due to volume limits with our payments processor, this functionality may disappear at times. You can make it always show up by upgrading your account until we have a permanent fix later this year.*
It's more conventional to use a pointer receiver for mutation, and I'm not sure you're gaining anything by returning a copy. Not a big deal either way though.
&gt; What I'm going to do is check out Amazon's Kindle MatchBook program which allows a buyer of a paperback version to purchase the Kindle version through the Amazon store at a reduced price. I have that set up for my book and it works great. Anecdotally, I sell a lot more Kindle versions than I do ePUB and PDF.
He could just rename the method to `withField`
First: you need to return "t" not "myType". Second: you can mix and match receiver types; some can be values where you don't want to manipulate the struct members, and others can be pointers where you do.
I don't know of any tool. However I would suggest you should update Go, and reinstall gomvpkg and dependent packages if possible. I have seen many times errors are due to incompatible versions of Go and Go based tools.
Yep, I think you are on the right track. I personally disagree (mutation is fine imo) but your reasoning seems pretty solid. Immutable state is a great way to manage concurrent access. To me, it's less intuitive and easier to forget (calling `myType.withField()` without assignment, for example). However, as far as best practice, naming is indeed important. `setField` is classically a mutation-- `withField` is classically a copy.
I'm not sure I follow unfortunately :( Can you elaborate?
I prefer consistence also. But a couple of times it seemed a trivial convenience only because other methods were part of an interface signature. If you don't need that kind of compatibility, then make all your receivers pointers - you shouldn't have to change the source code that uses the package; the compiler handles everything for you.
Completely agree. I think naming is important too and there is a convention for it. Concurrent access was not specified as a requirement up front. If that is needed then returning a copy is one way of managing it. append() is a bit different, in my opinion, because it's a built-in function and not a method. &gt; Not wanting to update unit tests and calling code is a wonderful reason in my opinion. I never want to change things unless I have to, you don't agree? I disagree. By that reasoning, nothing would ever be refactored. 
Use a goroutine when you want a function to run independently and asynchronously to the code that invokes the function. Your example of an endpoint that invoked other HTTP requests is a good example. Keep in mind that the net/http package calls your handlerfunc in a goroutine, so it runs asynchronously to all other requests already. One thing I don't understand is what you mean by "queue the database" And it's critical to understand that when you say "waits for all routines to return their results" that you mean "receives results for all goroutines on channels" :-)
I assume they want to link into something like TensorFlow through python.
Trying to encode behaviour in the data, no wonder you're having issues. 
TensorFlow has a Go lib: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go (haven't tested it yet though)
Hm, so my best bet it to change to arrays, or to copy() the slices and build a new one. I'm gonna redo the code with arrays. Thanks! EDIT: actually I just created a new slice, and used it as return value instead....
Go routines should only be used if you need them. You only know if you need them by benchmarking and finding bottlenecks, blocking, etc after the program is built. I wouldn't recommend trying to plan out how to spawn off Go routines when you first start your project unless you really know what you're doing, or else you risk introducing a lot of unnecessary debugging nightmares.
[go noob here, so take this with a block of salt.] Based on the end of [the blog post about error handling](https://blog.golang.org/error-handling-and-go), where it is demonstrated how to wrap your controllers for consistent error handling, I'm gonna vote for the "middleware" approach. I don't think having your own flavor of controllers (ie, diff sig than "standard") is any sin. It seems to me to be A Right Thing.
That's useful. I believe Dave Cheney has a thingie to just dump your package docs into a file, but often you want your README to be based on your godocs but distinct. This fills a nice niche.
Fairly certain that is the common consensus. https://blog.golang.org/error-handling-and-go
So, 2b, or not 2b. That is the question? ... Ok, really... yes. Include the db connection pool as a pointer in the relevant types. If it's a helper function in custom sub-package which does not have a relevant "contextual" type, then the db would likely be the first arg. My main funcs are often simply setting up needed types and passing dependencies into them, then calling some sort of "run" func(s).
And if you do choose to use a 3rd party router, this is the only one you will ever need. https://github.com/julienschmidt/httprouter
That says a lot about the direction Go should be heading. The default net/http package is pretty powerful. I think 1.9 should improve on it a bit (the obvious reasons are obvious) and settle this discussion once and for all.
I personally think handling user auth on your own is pretty comforting. Using 3rd party libraries/frameworks for user auth always leaves me wondering "I know it looks like it's working...but is it really working as expected?"
Thank you
Why not just use the number of cores? https://golang.org/pkg/runtime/#GOMAXPROCS
The optimal mix is a calculus of: How many can the other end handle? How many can you fit in RAM? How many consume all CPU capacity? How many consume all network bandwidth? The answer is as many as possible until one of those limits is reached.
That number is going to be far less than optimal due to network latency between the requestor and the responder.
That's the link I was talking about. It seems to be acceptable to return something, but not change the actual calling signature.
Hm, I have also seen it in nodejs projects with every route having the var db = require('mydb').db() code. I mean at the end if you need to access the database in the handler, you'll use it. 
It doesn't support http2.
Just because Go is written in Go doesn't mean that the compiler isn't being slowed down by the low-latency low-throughput GC setting (ofc theoretically speaking). This is imho one of the good cases where GC tuning would be nice, when you're building something like a compiler or a command line tool that cares more about throughput and less about individual pauses.
&gt; This is imho one of the good cases where GC tuning would be nice, when you're building something like a compiler or a command line tool that cares more about throughput and less about individual pauses. In my opinion, those specific two cases that you mentioned (which are usually IO bound) do not justify paying the cost and complexity dept of adding GC tuning.
I've done similar work with Go and my experience is that your network is almost ALWAYS the limiting factor *unless* you aren't reaching across the internet. Figure out how much bandwidth you have available to you (realistically) and limit the number of requests based on that number. If you can make separate HEAD requests and do a little bit of simple math you can even push things a little harder and try to make sure that the number of in-flight requests doesn't exceed your available bandwidth. Obviously, this isn't super precise but it has worked out pretty well for me.
I have no problem breaking the HandleFunc signature. That is what the http.Handler interface is for. Create a struct with the stuff you need on it and a ServeHTTP method. Instantiate one with your database connection, the actual handler (which can now have whatever signature you like), and whatever else and pass it to your router for each route. ServeHTTP passes the db connection to the *actual* handlers. 
I think the approach you'll see most often is DI, where you make a closure to hold the DB connection: func MakeDBHandler(fn func(db *DB, w http.ResponseWriter, r *http.Request) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { db := GetDB() fn(db, r, w) } } ... http.HandleFunc("/foo/", MakeDBHandler(myDBHandler)) Or you could make MakeDBHandler take the DB as an argument. You really don't want to put the DB connection on the Context (IMO) because you can't tell by API that it needs to be there. https://dave.cheney.net/2017/01/26/context-is-for-cancelation has good arguments against putting most types of values on the context.
My favourite library for this is [Pester ](https://github.com/sethgrid/pester). There's a few others and it's simple enough that you could use the stdlib (pester has no dependencies). 
I wrote this a couple years back and we use it in some of our software. It doesn't support jitter or alternative backoff algorithms like the the library /u/wilywes posted, but it's a bit more generic and useful in other situations: https://github.com/mediafly/retry Could probably use some polish but it gets the job done. 
Aren't compilers almost completely CPU bound?
Pester looks like what I was looking for. It's fairly transparent to use. Thanks!
Sorry to be off topic, but just FYI I think you are confusing opaque and transparent. 
Hi there, We have built our fare share of complex projects and went with using closures to pass in a database interface after trying almost every other option. If you really want to have code that is easy to understand and to test, you could even write specific interfaces for the handler functions and pass in a database struct that will satisfy this specific interface. Although this might sound like overkill, it will make it very easy for you to write mock database structs to test specific handlers. Please note that we are using echo as web-framework. // Define the interface your handler will require type dbInterface { Get(string) string } // GetSample will return the sample value to the given id func GetSample(db dbInterface) echo.HandlerFunc { return func(ctx echo.Context) error { sampleID := ctx.Param("id") if sampleID == "" { return echo.NewHTTPError(http.StatusBadRequest, "please provide an id") } sampleValue := db.Get(sampleID) return ctx.String(http.StatusOK, sampleValue) } } 
I too, do this. You can see an example here: https://github.com/bunsenapp/bunsen/blob/master/services/user/http/user_handler.go It works extremely well and makes testing super easy. 
New Go developer here. I have been thinking about this matter too as I'm sure many new Go developers do. Just want to say thank you for the clear examples.
I'm a Canadian gopher who travelled for nearly 3 years now around the world. My wife and I are now permanently staying in Morocco if you want to come visit :)
Looks nice but I will probably stick to net/http and github.com/PuerkitoBio/goquery.
https://github.com/dimitark/socialauth Very small. First Go project.
Does anyone know what works best for gorilla/rpc? 
I come from C, where everyone has his style, each character I position it in a way that pleases me... for the rest exists gofmt
For a public package, with an often implemented interface I totally agree. Here, the utility of the package/interface is the ability to mock out the database layer for unit testing. I do this using vektra/mockery and testify/mock. Less concerned about the size since I don't expect anybody else to implement this interface.
There's a screenshot on the readme of the project here: https://github.com/lucapette/tracker Thyme looks interesting, I did not know about it. Thank you for sharing!
Simplicity during testing is one of the huge advantages for sure.
Heya, this is really cool thank you! If anyones interested, I tidied up the code a bunch removing globals and moving a lot of code out of main and cut a reasonable API and example usage in main. In your bot program you can then just call the library function rather than using exec. https://github.com/aultimus/gofaceswap
We built a similar package to load settings too. https://github.com/intelligentpos/setting don't have the defaults though! Looks good!
thanks, we enjoyed making the app
I usually try to keep internal implementations like they were public, it will keep the consistency and one day later I'll be my public. I understand your point, I'm still studying around the mockery is indeed a plus, and still not sure if the struct way is 'good' or pleasant for third parties **UPDATE: Sorry I don't want to be rude or anything just reading around I know the pet project of yours is a WIP and noticed that basically you don't use the mocks from mockery in the tests but rather the testdata.go I also found this old [thread](https://www.reddit.com/r/golang/comments/37dy6f/what_are_the_benefits_of_creating_interfaces_for/) related to this, and you are right indeed interfaces seems a proper way to generate the mocks Although I was checking mockery as I got interested it seems that what it does is basically Make a compilable solution of the interface with some kind of proxy to fulfill the methods, while testing we have to implement the test methods: type Proxy interface { passthrough(s string) string } Mock.On("passthrough", AnythingOfType("string")).Return(func(s string) string { return s }) That's something that a struct do not need at all, it would be something like: type Proxy struct { passthrough func(s string) string } on test var p Proxy{func(s string) { return s }} is there any other advantage of mockery besides making the interfaces implementations compilable? 
Very nice! I like the idea of using Grafana as a front-end for this. I've been thinking of using Prometheus for a similar backend.
I'm more used to BeautifulSoup so I made this package the soup way :)
Very neat! I submitted an issue (and ER really) to make this available as a middleware plugin to the Caddy server for CoreDNS. I'd prefer to run as few as services as possible on my raspi, so having this integrated with my web server/dns server would help a lot.
Thanks for the note. That's a great idea. I'll check into what it might take to integrate with CoreDNS.
Why.
Hey, thanks for posting this. The codebase seems really clean, I've been wanting to do some graphics programming with Go and your code is going to be a great learning tool :) ps: my cat does like veggies, she munches on corn and broccoli.
Nice, thanks for the feedback!
Thanks. I'm not that familiar with pi-hole, but it looks like a great option for those running a raspberry pi. Obviously, what I like about having something like this in Go is its cross-platform support and single-binary setup.
The stdlib has an ssh client in it. I'd think that is the heavy lifting part of building a remote command runner.
I don't want to hijack your post but I made a game https://github.com/ranchblt/geometry-jumper. It uses ebiten and I think is a good example for anyone wanting to try game programming in go.
Thank you. That's a great solution. I am investigating it.
To follow up on this, /u/davidrjenni submitted a CL [1] that fixes this issue and it was just reviewed +2 and merged. So `gomvpkg` no longer has this panic. `go get -u golang.org/x/tools/cmd/gomvpkg` will update to latest version. [1] https://golang.org/cl/37432
Yep, it turned out the obvious change was the correct one. I just wanted to point out that one should consult documentation, and nearby code to make sure the proposed code fix is the right thing to do.
This is being very useful for my projects, now it is very easy to have all your configuration and cli parameters in minutes.
* **URL**: https://github.com/chewxy/gorgonia * **Problem solved**: building deep learning/machine learning programs. It's main attraction is building a graph of expressions (tailored to machine learning of course, given that programming in general is all about building a graph of expressions), and then executing them in a highly optimized fashion. * **What works well**: The basic stuff you'd expect. * **What doesn't work well**: * performance related - cgo calls for BLAS and CUDA can be optimized a lot more. * lack of uniformity in APIs (see for example the `Solver` type - ideally all `Solver`s should take the same options - batchSize, eta, etc, but they're not uniform because each solver was implemented at a different time). * lack of equal support for features: the `*lispMachine` type is lagging behind the `*tapeMachine` type because I originally designed it as an exploratory tool. But I feel that it should support things like CUDA and CBLAS. * lack of ops for image-based machine learning algorithms (things like `im2col` or `convolve`) * there are some highly questionable design choices, in the exported API. * Graphviz file generation is a mess of spaghetti code, and doesn't take options. * high reliance on `sync.Pool` for performance (indicating of course, the underlying algorithm needs work) * Very tightly coupled code - there is some current attempts at separating out the packaging, and I'm personally finding it a challenge * **Users** : You run Sourcegraph... you can tell me how many users use Gorgonia (not many I suspect)
is it possible to use without cgo?
&gt; I read somewhere that golang interfaces should be simple as having one method they may be, they don't have to be. I do it like this: type User struct { Id uuid.UUID `db:"id"` Username string `db:"username"` Email string `db:"email"` // other fields } type Users interface { Create(*User) error Read(*User) error Update(*User, func(*User)) error Delete(*User) error } type DB interface { Users() Users } so for me, a DB is actually an interface, and it contains a selection of different interfaces, each of which contains a group of logically related queries. Works well for my needs. I don't do separate methods for querying on different fields that can uniquely identify a row, the Read function just looks like this: func (u *pgUsers) Read(user *User) error { var ( q string args []interface{} ) switch { case user.Id != uuid.Nil: q = `select * from users where id = $1` args = []interface{}{user.Id} *user = User{Id: user.Id} // zero out all the other fields case user.Username != "": q = `select * from users where username = $1` args = []interface{}{user.Username} *user = User{Username: user.Username} case user.Email != "": q = `select * from users where email = $1` args = []interface{}{user.Email} *user = User{Email: user.Email} default: // return an error here } // u.conn is a sqlx.DB return u.conn.Get(user, q, args...) } and then to use it, you invoke it like this: // conn is a db.DB value, userId is some id from before user := db.User{Id: userId} // could just as easily be user := db.User{Email: email} switch err := conn.Users().Read(&amp;user); err { case db.ErrNotFound: // no such user, do whatever pleases you case nil: // successfully found the user default: // some sort of error, do whatever error handling makes sense } which means the handlers have no idea what type of database they're talking to. You can also do something more like this: type Users interface { ReadUser(...) CreateUser(...) // you get it } type DB interface { Users Books Games } just depends on preference.
You're right. Edited. Thanks dude.
It's clear the author has put a lot of effort into this, and that in itself is worth commendation. As an employee of a major participant in the Java ecosystem (Pivotal, but this opinion is mine and not Pivotal's), I think that hiding the user from interacting directly with go and its ecosystem of tools is generally a disservice to the user; they will not learn some go idioms that are generally helpful to be aware of as you participate in the go community and make use of dependencies written in go. Would you replace maven or gradle with a tool written in go? You certainly could, but it would increase the barrier to entry for any Java developer wanting to modify the tool or contribute to it. In some ways this is observable in people who are unwilling to adopt gradle because it is written in Groovy. There are parallels to this scenario for a tool for go written in Java. I think that's ultimately why you're seeing people reject this idea. Deferring the learning of go idioms is just delaying a potentially inevitable conclusion.
Thanks for the link. I was looking for something like that. I use ansible, but it's not pleasant to program. I whish I had a better control on the output sent by mail. I don't want to see this list of Ok. I want a single line in green if ok, and red if na error occured. Etc. Writing a better Ansible in Go is in my idea list. 
If you're using vim-go, `:GoPlay` will export the current file to play and open in browser.
no
URL: https://github.com/surullabs/lint Lint was built to make it easy to run linters as part of go test. This makes it simple to include linters as part of a Dev workflow without any external scripts or extra steps. It also makes it easy to include linting in a continuous build with no extra effort. It can also be used to programmatically run linters. What works well: Does what is advertised. Linters are automatically downloaded and run conveniently as part of go test. What could be improved: There are some low hanging fruit wrt to parallelizing linter execution. Linters are not vendored. The code to vendor linters exists for a vendored version of gometalinter but not for the others. 
If you want to use standard libraries and expect compatibility with community packages, answer is no.
Check out https://github.com/go-gitea/gitea stack, macaron for routing and middleware, xorm for orm.
I have since wrote another post to address rate-limiting in Go, http://joaodlf.com/go-rate-limiting-done-right.html I think this should address some of what we discussed.
Appending to the list of cons of fasthttp (incompatibility, websocket): http/2. Which is a must if you want low-latency future-proof web app.
lol, what are you girls doing?
You're a god
Your own TCP/IP stack? You are exaggerating. Ask anyone who wrote their own TCP/IP stack if that was worth it. You have pretty fast TCP/IP stack in BSD already, no need to write your own. There was also a lot of improvements in network scheduling in Linux 4.x kernel. If you have performance problems and need user space TCP/IP to bypass kernel then there are already solutions that you can use instead of reinventing the wheel, like DPDK (user space networking stack) where you can use polling instead of interrupts. But please don't propose own TCP/IP stacks as viable solution, this should be last resort. I am 100% sure that OP do not need one, so he should not consider it at all.
Please read [this thread on `golang-nuts`](https://groups.google.com/d/topic/golang-nuts/OaQu4QezAr0/discussion) where the `fasthttp`'s author states their stance on the HTTP RFC.
Your're an absolute diamond. Have an upvote :-)
Thanks, glad to help. Remember that missing fields will be empty string or 0 in case of integers, or whatever the type would be per default if you simply declared a variable. So if this is supposed to be used in production where you necessarily might not have control over the input, use any of the json validation libs out there. One example of such a lib is: https://github.com/michaeljs1990/val
&gt;worth it to switch to fasthttp for production We use very heavy application with huge number of intensive connections. In case of 'net/http' it used a huge number of resources, had big delays/latency and bunch of bugs (such as hung and unclosable connections, beast memory leaks; go 1.3|1.4|1.5). Thought already to change language. But has somehow come across fasthttp (likely has seen post right there). After fast migration I could not believe my eyes: on monitoring of system resources impression that is started nothing, but the application works properly, and does all the work, and it makes it really *fast* :) In this case it became a silver bullet. But when I tried to use fashttp in small projects where productivity is not the main task- it has not been the huge win. Incompatibility with a large number middlewares and incomplete realization of rfc (no HEAD method, for example; oh c'mon!). So need choose for each particular case and requirements. &gt;is there an workaround for using sockets? [leavengood/websocket](https://github.com/leavengood/websocket) - websockets for fasthttp.
&gt; In case of 'net/http' it used a huge number of resources, had big delays/latency and bunch of bugs (such as hung and unclosable connections, beast memory leaks; go 1.3|1.4|1.5). Can you quantify "huge number of resources," especially relative to your expectation? And can you provide links to, say, 3 of these "bunch of bugs"?
I wouldn't go that far. If you are running into performance issues with net/http on a large-scale system that fits the niche, fasthttp is fast enough to be worth a look.
I find it hard to imagine any task that requires that performance from the HTTP routing, that you could be using Go for it, and that couldn't be better served with nginx. Then again I am no veteran, I would love to hear an example.
It allows you to quickly approximate cardinality of a set. Basically the number of unique items in a list or something. If you had to have the exact number and the list was large you would need the same amount of menory. This technique lets you get a really close approxmation with very little overhead. 
It's a small project I started yesterday because a friend asked me for something that would do this and there was nothing else available. But nevertheless I would like to get some opinions from you folks here.
Does this look like a modern take on some of the Plan9 concepts to anyone else?
There are so many use cases that net/http would be a bottleneck and fasthttp would help, ex api which serve a value that is changing once every xx seconds, so basically you are serving value from memory to hundreds of thousands clients/s. Try harder next time.
By being a fast cache. Faster than fasthttp.
That's why I wrote that it changes every couple of seconds and you have a lof of those values and routes, if you profile your code then fetching those values would be still little of CPU overall time. I don't teach anyone premature optimization, always measure before, but there are cases you know where you will have bottlenecks, if you are experienced enough you see use patterns and you will optimize in early stage anyway. If i look at the code I can pretty much see almost every allocation, cache impact, branch miss predictions etc. If you architect software and know how it will be used then you know where your bottlenecks are most of the time. So let's not make any perf advice premature optimization advice also.
Ah, sorry, I thought it would be implicit: A HTTP entity cache, so that the Go server does not need to spin up its comparatively slow HTTP stack (regardless of using fasthttp or not). You know, like Varnish/Squid/others. I would recommend Varnish if you're looking to cache, but that's a personal opinion.
... It still sounds like caching by something much faster than what you can write in Go is the way to go, giving your example. But it seems like we agree on measurements before optimisation.
Sure, but this doesn't change that my example is valid. &gt; It still sounds like caching by something much faster than what you can write in Go is the way to go, giving your example. Someone could write it in C to get it even faster but why? if Go will be enough? This what you wrote is an perfect example of premature optimization.
Indeed it is valid. However, given the example, sacrificing net/http for fasthttp to gain the ability to send a static value extremely fast sounds like you're doing what in your backend application what your network architecture should do for you: cache. I cannot imagine a variation of your example where fasthttp would be a good solution. That is, apart if you're trying to serve way too many users with a single server, which is a terrible thing to do for *many* reasons. My original mention of a cache was to show that there are *other*, potentially more suitable ways out of this performance problem. You seemed to be arguing somewhat blindly pro-fasthttp, which to me seemed like premature optimisation (I have now said "premature optimisation" more today than I usually do in a year).
I agree that there are other potentially more suitable ways to solve that problem. It's always about what works for you/your team, if Go+fasthttp works for your team and this solves your problem then use it. I am just opposing to thesis that fasthttp doesn't have any use case comparing to net/http. Like always it depends on many requirements. 
Thanks for the note, you're probably right. I debated what to suggest for those who didn't want to just download a binary and landed on the more verbose "clone and build" steps thinking they would be more descriptive (and figuring gophers would instinctively `go get`). In hindsight, most people probably just want to "get it and go" so I'll revisit this.
Linking to every GitHub issue tagged with net/http is a cop-out, I hope you will agree. Can you please link to 3-5 of the _specific_ issues you've experienced? Since you and the parent have claimed and validated respectively specific pain from specific bugs in the HTTP package, I would hope direct references shouldn't be difficult to dig up.
So it's https://github.com/bitly/oauth2_proxy ?
The existing backends already support things like that: -github-org="": restrict logins to members of this organisation -github-team="": restrict logins to members of any of these teams, separated by a comma You just had to cp github.go discord.go and tweak as needed.
1) Thanks \^\^ 2) Wasn't aware the http error consts are from 1.8, might change it to use something else, though I am not all too interested in supporting now legacy 1.7 :3 3) Yeah, go.rice is my goto for web projects 4) I will update the readme with more info later on, also thanks for the PR, merged :) 5) Originally this was intended to be just one `main`, not a library, so I am not sure what kind of example to go for yet 6) Yeah, good idea It *is* dockerized, though dockerfile is using scratch and I forgot to attach certpool to the binary, so you have to mount a volume with it for it to work. You can look at travis conf to see how the image is built and uploaded. Not sure what you mean by providing it as a service though.
* **URL:** https://github.com/alistanis/st * **Problem solved:** Reads in a go source file and automatically applies struct tags to structs * **What works well:** The tool behaves as it should and makes it very easy to tag your structs so you don't have to type them all in, especially if you need to support multiple encoding/decoding formats for a struct * **What doesn't work well:** * I'd like a better way to leverage go generate or find ways to specifically include/exclude structs, fields, and perhaps more dynamic tagging options. Currently it doesn't support things like `json:"fieldName,omitempty". A more configurable/easy to use format would be awesome. * It *is* written as a library, but it's really more a standalone tool. I'm not sure if that disqualifies it, but I hope not. Thanks for consideration, and good luck with whatever it is you guys pick out!
You might like https://mholt.github.io/json-to-go/ although it takes the data directly to a struct, not a map.
To be fair, the test might have been to measure the overhead added by echo...
Sorry for commenting on an old thread but how would you compare the availability of materials for someone learning to program between Elixir, Go and Haskell? I hear a lot of questions about what is the best language to learn how to program and I mostly stick to Python because there are many resources available. I'd suggest to revisit Elixir in the future as well, especially as the community seems to be pushing more towards explicitness, so hopefully you will run less into issues about which code/keyword is being imported (and the Haskell experience will certainly help too). 
This is exactly what I've been looking for, but I've one question: does it just cover interpreting the AST directly, or compiling to bytecode and running a VM?
[removed]
Fasthttp is actually super slow when you add real world pauses. Most benchmarks are tuned in a particular way to make the author library look fantastic. We performed many real world tests and saw a performance DECREASE against stdlib
https://www.google.com/search?q=Pagination+in+a+REST+API&amp;oq=Pagination+in+a+REST+API&amp;aqs=chrome..69i57j69i60&amp;sourceid=chrome&amp;ie=UTF-8
Hey! I posted about this here: https://www.reddit.com/r/golang/comments/5v0jw2/go_downloader_in_terminal/ and have now put it on github. Please try it out and let me know what you think. I used https://github.com/jroimartin/gocui initially, but had issues with go routines, so switched over to https://github.com/gizak/termui. As a bonus, I got some nice UI graphs because of it. Also, it's basically just thrown together. Can be cleaned up more and use a struct and not global variables etc. But anyway, it works for now.
Gopher, I want one, where! 
This appears to be fixed in later versions of Go, or at least I can't replicate it on the Playground.
No, I do not agree. However, to entertain your laziness, I do recall opening https://github.com/golang/go/issues/14046, which ended up being a fatal runtime bug. As for workaround examples, HTTP/2 had to be disabled as a few browsers couldn't communicate with the server at all on the initial HTTP/2 release, and we had to set various socket options ourselves to ensure proper behaviour that http.ListenAndServe at least initially did not handle. However, I no longer work at the place, and don't have access to the full list of hacks deployed. With all due respect, I cannot understand why you find it hard to believe that net/http, a very large blob of code, should have bugs like any other project would have.
I still feel this answer doesn't sufficiently answer what HyperLogLog is about. Counting the _distinct_ items in a _set_ requires only the cardinality of the set. If counted as a stream as elements are checked, you only require storing a counter (O(1) memory). HyperLogLog is about counting _distinct_ items a _multiset_. This means if you were to receive a stream of elements in the multiset, you would need to maintain a history of each element seen which is proportional to the number of elements in the set, to get the exact cardinality of the underlying set. HyperLogLog allows you to get a close approximation of a large multiset with loglog space proportional to the cardinality of the multiset. I know almost nothing about the algorithm nor used it, but I think the hardest part is actually understanding the problem it is trying to solve.
Not really, unless you really have to use it for some reason (professional project, host constraints, ...). Go and Python are good alternatives to PHP. The latter has a lot of mature web frameworks (Flask or Django, for instance).
Are there as many jobs for node developers as PHP developers?
&gt; better graphql implementation &gt; no documentation at all &gt; nothing seems to work
Wouldn't PHP be useful for quick prototyping and such?
It just covers interpreting the AST directly.
Obligatory https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/ The only reason PHP knowledge is useful is debugging PHP, typically from problems caused by the language's design. You need to tell us why you are asking; in the comments you ask about jobs. And tell us what you already know so we can see where you'd benefit. $ php -r '$a = ""; $a[] = "y";' $ php -r '$a = "x"; $a[] = "y";' Fatal error: [] operator not supported for strings in /tmp/php-wrap-jpk45H on line 2 $
I've been learning Go for the last 3-5 months but I can see that the market for Go developers isn't too big. Most job sites have something like ~5000 PHP jobs and ~50 Go jobs. So I'm wondering whether learning PHP would be a good idea (mostly thinking about remote back-end jobs). 
It really depends on what you plan on doing. It sounds like you are gearing up to find a job (or a new job). If you work at a consulting firm or an agency, then having a wide array of knowledge can be very useful. In this context you can often switch projects frequently and don't usually get to choose the language. If you work at a company that has a web product or service then you are most likely going to be working in one or two languages. This context often allows you time to learn the syntax of a new language. So spending time getting really good at programming and not just knowing a language will be much more beneficial. 
&gt;Not really if you know how to compose an application functionally. So it's not suited for quick-and-dirty bash-like script. &gt; However, if you're running node 7.6 you can now await async executions, e.g. It should be done the other way around IMHO.
Isn't it a bit awkward to require the user to already have Go installed to install your tool? A binary release would be useful.
There's a lot of PHP jobs because a shit ton of things were already made in PHP. They need people to maintain legacy code. Given PHP was created to be a very simple thing in the beginning but as it matures features were added, concepts borrowed from other languages, the legacy code you will be dealing with is very likely to have lived through all these stages or made by programmers choosing to only use feature from a certain point in its evolution timeline. Working with this kind of massive code base is nightmarish, you most likely won't enjoy the work.
What do you mean??
Jobs requiring Node has been increasing rapidly in recent years. Node has some pretty attractive qualities in my opinion that made it popular. A wonderful package management system built right in, performance is very nice for such simple language(can be very memory hungry though), familiarity and convenience of a single language for the entire stack, etc. But some of the early pioneer in node like TJ has abandoned node in favor of Go though ;) I don't think you need to doubt your choice of learning Go.
Unless you have to work with a legacy codebase i'd say no.
Well, I didn't want to put those binaries into git, as it's not nice to put binaries you can build from the code in the repo. But, sure, I can do it here.
if you're a masochist 
I think /u/Finnegan482 is referring to the [built-in webserver](http://www.php.net/manual/en/features.commandline.webserver.php) the you get by running `php -S localhost:8000`. Don't ever use that in production, though.
Every language has its quirks. The user never deals with the majority of what's in this blog post. PHP can be a great language of written by experienced developers. Unfortunately, most the jobs out there are maintaining legacy code that's nasty.
No. That would be a waist of time. Unless you need to use php for a professional reason or you just wanna learn it. Otherwise definitely not.
That is very misleading and wrong. You are assuming someone is using Apache as part of the server stack which is not necessarily the case. 
What do you want out of a job? If you want to build cool things, PHP isn't what you're after. The companies that are building new cool things aren't doing it with PHP. If you want job security, PHP may be the right choice. There are a lot of legacy systems out there in PHP, often written by people who didn't really know how to program, and thus require a lot of continuing maintenance. So long as a company doesn't replace their PHP systems with something else, the jobs will be around for a while. 
Thank you for linking to one specific bug that you found. Very esoteric! I'm glad it was fixed. Can you provide two or four more links like that one? Of course I don't find it hard to believe that net/http has bugs. I'm sure it does. I'm trying to make sure that strong generalizations like "net/http uses a huge number of resources" or that it has a "bunch of bugs [like] beast memory leaks" are reified with specific examples. Otherwise, they're FUD.
Maybe I missed something, but PHP is an hypertext preprocessor since its inception. If you can run PHP, you can put PHP code into an HTML file (or anything else). For development, you can use the built-in server. If you use, let's say, nginx, you will need PHP-FPM.
I have been working with asp.net mvc, after that I tried Laravel and php 7. Yes, it is worth learning PHP. A script language allows to faster develop a project. After that, if you need it, you can rewrite the project in a faster language. I have benchmarked php and golang, for my project php's speed is enough.
I normally limit my webcrawler to 20-40 connections to prevent blacklisting from the server (I only crawl one Website/URL/domain at a time). I found a site that had no limits and saturated my 100mb/s connection for fun with ~3000 connections but I wouldn't recommend that.
Here to say that things have greatly improved in PHP. With PHP 7 the language itself has come to a more mature state
What's special about 1.9 and net/http?
It's also a good idea to use local storage, if available and when possible, to save some round trips. 
It feels like the first thing would be to do: type ISODate time.Time rather than embedding. I think the biggest advantage is that you could cast between types very easily.
This can be a massively useful.
Then he needs go&amp;python, not go||python.
I was just saying they should focus on improving http in version 1.9.
I see your point, but what I think that makes Golang not suited for high budget triple A title is its garbage collector. It just stops everything for a split moment, which is not something you want while programming a game engine.
Those stop the world pauses are now down to 100 microseconds, pretty positive no player will be able to detect that.
It might be just me since I'm not too familiar with the framework you referenced but your question wasn't very clear to me. What exactly is the issue?
Lets say I have the following type HelloArgs struct { Who string } type HelloReply struct { Message string } type HelloService struct {} func (h *HelloService) Say(r *http.Request, args *HelloArgs, reply *HelloReply) error { reply.Message = "Hello, " + args.Who + "!" return nil } Now I want to achieve something like [this](http://www.alexedwards.net/blog/organising-database-access#using-an-interface) in my Say method. However based on the architecture of the framework I cannot use the env approach like http.HandleFunc("/books", env.booksIndex). So what's the best way to have something like this but without using a global variable like var DBCon *sql.DB
Plenty of games are made with mono/java. Which has a vm and stops.
Î¼s not ms. Sure there's other things it could be doing during that time but there's still the other 99.99% of each second available for those other things. At 60 fps with one pause per frame, GC time will only take up 0.6% of each frame. Most engines lose far more CPU time than that to poor algorithm choice, bugs, memory management, whatever. Also you can work on just not producing garbage and then the GC never even needs to run. 0ns pause times are even better eh? That 0.6% frame rate increase will really get people talking.
100ms == 100000Âµs 100Âµs == 0.1ms 6ms is where one might start to notice latency.
I was a PHP developer for ~8 years, moved to Go, then never looked back. (Been writing Go for 5+ years now.)
PHP doesn't just have a few quirks. It's practically nothing but. It's hard to find an area of the language that isn't warty. Experienced developers writing in a common subset of PHP aren't common. Most places that could mandate this happening could also migrate to better languages.
Quite a few reasons. If you are working with a small and simple web app, I'd say that Go's template engine is more than enough. But if you are working on something more complex than that, I find that templates are harder to work with at scale. The alternative is to keep Go as a pure back-end language to develop the API and use your favorite front-end framework to write the HTML/JS/CSS UI, usually as a single page application. This separation gives you quite a few benefits, one of which is that as soon as you have your API then you can attach different front-end apps to it. Think about creating a console UI, a single page UI, a native mobile UI and heck even a Go web app that uses the template engine all of which will get their data from that single API. Some people also call this dog-fooding because in order to build your view, you pass through your own API, instead of just using it for external apps. Another nice benefit, in case you are working with a front-end team/person is that they can focus on building the UI using their favorite front-end tech, while you can enjoy working with pure Go on the back-end. Sometimes it even helps keeping the codebase as two separate projects. One more benefit, in case you're plan on developing an API for your template powered web app later is that with this method you start with the logic and design of the API from the very start, which will end up creating a much better and battle-tested API and will also reduce the amount of work and duplication. Why have a bunch of handlers that parse and validate forms and also an API that does the same using JSON while you can have just the API? Obviously, as with every decision there are trade-offs. Indeed it adds some overhead and complexity as for example suddenly you need to start worrying about implementing some kind of authentication/token mechanism for your API, use stuff like CORS to allow AJAX calls etc. Furthermore, if you are working alone and front-end is not your strongest point, maybe you will find it harder working with the pure HTML/JS/CSS app while sprinkling some CSS or importing a JS library for a template is usually easier to grok. Last but not least there is always the argument of server-side rendering (template generating views) vs client-side rendering (Go API + JS front-end) and depending on what you are trying to achieve, one method might be more suitable for the task than the other.
Add to that everything made with Unity (C#). So, Kerbal Space Program, Subnautica, Superhot, well, [everything on this list](https://en.wikipedia.org/wiki/List_of_Unity_games).
He is pointing out your 'better' implementation that does not have any documentation, and you can't get it to work (due to possibly no documentation) might not actually be better.
The actual Unity engine is written in C++, mind you
I'm pretty sure Gears of War isn't written in C#. It's an Unreal Engine game which is majority C++.
On a side note, you might wanna checkout gRPC
Yes, as a great exercise of what you should not be doing in a programming language. Either in term of development speed or features, Go is far superior. So yeah, it is just for the education - not really useful 
&gt; It should be done the other way around IMHO. So by this logic, everything should be a go-routine in Go - I strongly disagree. C# rocks a similar model to node (in fact, node async-await was heavily influenced by it), and AFAIK it's highly appreciated. &gt; So it's not suited for quick-and-dirty bash-like script. I don't see why not, if you want bash-like scripts, do things synchronously, then again - if you want to do parallel stuff v7.6 + async-await would be my approach of choice. Or, yield an array of promises wrapped in a co-routine, in which case you could do it "bash-sync-style" down to node 4+, or even 0.11 with `--harmony` flag enabled. I'll agree that node isn't the better choice to do bash-like scripts (I'd personally look at perhaps bash or Go), but that's not of the topic here. It's about jobs hunting for OP. I was disagreeing on your comment on callback hell, which is a widely misunderstood problem IMO. Let's just agree to disagree.
I dont know how I got that into my head. Sry.
&gt; see it as a general purpose language Game design without generics? 'Interface{}' everywhere? Performance issues when using C-libs? Hell no .....
Nice, thanks! edit: hmm. #1 was a bug in httputil.DumpRequestOut, not sure if that qualifies. #2 wasn't a bug, but a change in unspec'd behavior. #3 and #4 were legit (though somewhat esoteric) bugs in 1.4.
You are going to get very biased answers here. You might want to ask on /r/PHP, /r/programming or /r/coding to get answers of a different bias.
Ah. Fixed.
Hey guys, I've had a few requests to make my themes public every time I've done a presentation, so here they are. I hope someone likes them.
I work with Go and PHP, and I find way easier to prototype with go.
Most games these days have a network component - even single player games. It wouldn't surprise me at all if matchmaking or MMO servers were written using Go in the future. Eve online famously used python... and this kind of development probably has a lot more in common with making web apps than physics engines.
&gt; I'm just curious about that feature. Can I handle errors or get return values if I'm doing an async DB read but awaits the result, like in your previous post? Yes, when you await a promise you can throw/try/catch as you'd expect in say PHP // note that currently, most of core libraries aren't promisified yet, there's packages that // wrap all of core to promise equivalents for you though try { await doSomethingAsync() } catch (err) { ... } If you want to do something quick and dirty using node.js you could perhaps use the *Sync core lib functions. They're not great for web (obviously) but works for use cases like shell scripts, example: const { existsSync } = require('fs') if (existsSync(somePath)) { ... } IO in node is async by default, but most (if not all?) have *Sync equivalents in core.
Yup, those optimisations are the funny part. &gt;If someone wants to try ... Already done, see for example: https://github.com/dim13/golyb
This is pretty funny. You don't need to send any message-length type stuff, though. Just use a streaming encryption scheme instead of a message-based one.
&gt; in basically every language I knew I could change the length of an array by just pushing things to it. Not so; in Go. Fixed-sized arrays are not uncommon. The following languages do not have dynamic arrays ([according to Wikipedia](https://en.m.wikipedia.org/wiki/Comparison_of_programming_languages_(array))): C, C++, C#, Java, Haskell, Scala, and more. Edit: The Go equivalent to dynamic arrays are slices. The Go equivalent to pushing a value to the end of an array is the built-in function `append()` (for slices).
I think you should learn both PHP and Python. Don't just stick to a single language. The more languages and paradigm you learn the more useful you'll be. (Try functional languages too.) You don't have to be a pro on every language, but write some project and try to understand the concepts. You should always choose the right tool (language) for a given problem. It's possible to write a webshop in bash script, but not so pratcial. Or it's possible to make some realtime websocket stuff in PHP, but you don't want to do that either. Sometimes it's easy to just wire together some components in PHP or NodeJs.
Perhaps you might reconsider how crucial it is to the consumer to be able to filter your data through your own API rather than on its own, or the maximum time you want a client to be waiting on a response. On one hand, filtering can save you bandwidth and make it easier on the consumer. On the other hand, it can hog resources on your machine. Storing a cached response still takes time to retrieve (albeit far less) and also storage space of course. I guess what I'm suggesting is maybe you are over thinking how critical filtering is? Maybe strike a balance between filtering server side and client side. I have zero clue how large your datasets are, nor how often they change, so my point could very well be moot here. But... Full disclosure, I have rarely ever had to cache API responses as my use case is somewhat limited... Most of what I write relates to the game EVE Online (has a nice API set for free if you ever want some fun), and the load my service handles doesn't come anywhere near that of industrial software. I have about a 8gb data set, with everything lumped together. Maybe 300 requests/second at my absolute peak, if even. So it's child's play to many.
Ah, the good old Nagle algorithm, trying its hardest to maximize efficiency and throughput, at the cost of confused programmers and latency. When you say that you implemented your framing protocol with an append... Do you mean prepend? You'd normally write the length first, then the data, with the lazy/slow implementation being: w.Write(encode(len(data))) w.Write(data) ... l := make([]byte, HEADER_LEN) io.ReadFull(r, l) b := make([]byte, decode(l)) io.ReadFull(r, b) There's absolutely nothing wrong with such an implementation. It only needs to be fancier if you want higher performance (avoiding wasting a syscall on write/read of length alone), or more features (multiple channels for things like out-of-order responses, metadata, OOB control messages, checksums, all that jazz that makes up h2/websocket/other framing protocols). Also note that, depending on your requirements, *UDP* is a packet protocol... As long as you don't need retransmit et.al. :)
Ok, forget about echo. I still recommend to bechmark fasthttp and net/http using loader.io or other simmilar site. The difference should be unimportant.
Thank you for the detailed reply, very appreciated!
&gt; The difference should be unimportant. Which difference?
Naaah, I think it's just a classic case of speaking about something w/o understanding it.
I think it's obvious, difference in performance. What do you need? I don't want you to ask me any questions or write me any comments.
Read my comment again. I don't need YOUR comments. Y O U R 4 letters. Read with attention, please.
Here's a comment telling you that I've read it with attention :)
I've been planning on doing something similar for controlling BBQ temps with a fan. Haven't really looked into the implementation, but is this based on [PID controller](https://en.wikipedia.org/wiki/PID_controller) logic?
Ah yes. Sorry about that I'm so used to using the keyword append in go, but yes, the length is a VLQ (Varint as go calls it) **prepended**. Reason against UDP was that, yes, I needed the error correction so I went with TCP. As for the other fancy stuff, I need to do more research to see where I will be going but it's been a fun weekend!
I'm sorry, you got rejected for GopherCon, but I don't have to do anything with that...
Not going to answer them all, decent answers have been provided already, but: &gt; 7. Do you develop in a IDE? If so in what? Microsoft's Visual Code is pretty nice. Install the golang plugin and you're good to go. It also integrates with the delve debugger, which is nice, and there are tons of other plugins available (like the vim plugin etc).
Others have covered 1 through 6 well but I feel the need to add to 7. Vim and emacs are not ides. I have been very happy with intelij with the go plug-in. And they have a go specific ide in beta called Gogland. 
Look again :) And please don't attack my person if you choose to hide behind a username, it's a bit unfair I know nothing about who's spewing random rage comments at me.
1. Web Services 1.1 Performance, fast programming, good standard library, very easy to learn 2. Yes 3. OK for small to medium backend projects 4. Nothing 5. Nothing, until you use it for what it was suppose to do :) 6. Does this question makes any sense ? 7. You can use any basic syntax-highlighter editor you like
I see, I'll take a look at it later. It is a very interesting project anyway - even without the potential for my bbq idea. Especially the external thermometer stuff is cool, this could could address a problem I'm facing where I live: my thermostat is placed in the warmest room in the house, which results in some rooms being very cold. How are multiple sensors handled? Or is only one taken into account at the moment? Also - is there a reason you support both an I2C thermometer and a JSON api? Wouldn't it be cleaner to always use a JSON api and separate out the temp measuring into a separate service offering that same JSON api? You could still compile this into the app so it starts a service like that on localhost or a unix socket...
I agree. It's a fantastic IDE.
I'm just replying in kind, you've started the whole thing by saying you don't need my comments, well, here they are. It started from a poor misunderstanding and you dragged it into a personal attack, instead of staying on topic about the technology / benchmarks at hand. (like I've done in that blob of text that was my second reply to you). Now you back out saying I'm the bully. So be it, I'm the bully, now what?
I don't have time to help you with your problems. Good luck.
What are you talking about? The [example given does exactly what is claimed on the playground](https://play.golang.org/p/97hRyNke-O). Also there is nothing to "fix" in Go, this behaves exactly as intended and designed.
Multiple sensors are not handled at the moment (it's on the wish list/road map). It is probably next on the docket for refactoring. The I2C thermometer exists because originally I had the sensor directly connected to the pi running the thermostat logic but since then I've pulled it out into it's own project (not in Github atm) so it still exists there as a way to retain the code I have for it. It is probably next on the list for a refactor. I envision it being a good candidate for Go 1.8s new plugin library so if you have a sensor connected you can have a plugin supporting something local and the remote would be a separate plugin. Unix sockets are an interesting idea. 
It's more common to use a static size (uint32 or uint64, for example), for both simplicity, portability (what if you end up wanting to make a client in, say, rust or python? Go's varint is go-specific) and theoretically performance (avoiding a call to Read for every byte of the varint header, which may result in a syscall each). Doing *research* before running blindly ahead with over-engineered implementations in a hobby project?! You're *clearly* doing something wrong here! :)
For anyone else confused about what Hugo is and wondered where the website is, it's here: https://gohugo.io
1. Simple web services. 1.1 It's simple and quick to get things going. Also Tooling! 2. Yep. 3. It needs generics and a good package manager. I wrote about that here: http://nomad.so/2015/03/why-gos-design-is-a-disservice-to-intelligent-programmers/ 4. See 3. 5. See 3. 6. I guess it's good? We've had nothing fail yet. 7. Vim, VSCode.
Check out the blog post for more background: https://blog.sourced.tech/post/proteus/
&gt; assuming two calls to Write() ended up with two packets Although you're unlikely to ever actually see this, depending on either side's implementation and any intermediate proxy you could get *any* number of packets (i.e. any number of partial reads). IMO any code using an `io.Reader` of any kind should have a test with the read side [wrapped](https://golang.org/pkg/testing/iotest#OneByteReader) in various ways, e.g. something like this: // r := actual network conn io.Reader r = iotest.OneByteReader(r) r = iotest.DataErrReader(r) r = iotest.TimeoutReader(r) // now use this `r` with your tests if the code behaves any different (other than performance) then it's wrong. (Except that not all code needs to handle the `TimeoutReader` case.)
You've got causality backwards :\ IDLs should generate programming language code, not the other way around. [More](https://peter.bourgon.org/blog/2015/12/01/code-generation-and-microservices.html).
I'd be concerned about the speed at which you run into and resolve problems over the speed of the library. I can only imagine that `net/http` will be an order of magnitude more supported in terms of library support and will always be very easy to find answers to issues simply b/c it is so widespread. In Python was a very common issue. A developer would want to speed up an I/O bound program and turn to an async framework. Everything would look better on the surface but then you realize the database driver could use some serious work. When google for answers the results are thin and lacking. It only hurts more when that app ends up having CPU constraint as well! I have no experience with fasthttp, but I can only imagine pain you'll eventually deal with simply from the fact that it doesn't use the same interface as net/http. Obviously, YMMV!
Ah, it's because the explanation is incorrect. https://play.golang.org/p/sIGbssZ0mJ This is *not* because the type actually has those methods. It's not a property of the type, it's a property of the function. It's because `fmt.Println` uses reflection and gets a little confused. This is not "exactly as intended and designed." Indeed, it would be a terrible design. *Nowhere* else in Go does a type and its pointer have two versions of the same method. It is also counter to Go's typically explicit nature: If you want to use the embedded method you may simply use `x.Time`. There are some similar gotchas with `json.Marshal`. 
Awesome write-up! I really (really really REALLY) hope Go gets better supporting features for Flow-based programming. Right now there's basically two hacks that make go Flow libs work: a) interfacing on Interface{} and b) pre-compilation step. Both have frustrating drawbacks that could be eliminated by some form improved macros or simple generic support (yes yes, I know this idea triggers some Go developers).
Looks like a simpler yeoman, nice job :)
IDL indeed generates the code, except for the language which is used to generate the IDL
I am currently split between two functions: 1. hit the db by using the `where` clause if the user goes from page 1 to page 2 rather than fetching and retaining stuff 2. cache the stuff and send it to the client You are right, thank you! I need to understand my use case and figure out a way to do this.
Thanks, I appreciate it!
Most Google APIs (I work on Google Cloud) only allow you to fetch the *next* page, and give you a page token. This is often a timestamp, or some other indicator they can use to do a simple "createTime &gt; @when limit @count" style queries.
Certainly it doesn't need to be a timestamp - that's just something that makes some page listing easier, and happens to make sense for something I work on. Listing by lexicographical ID sorting works the same way.
True, I was reading Twitter docs, they are amazing. timestamps works wonders for a social media app's API. I wanted to implement offset, but I read a nice document stating the ill effects of offsets, thus, I moved to IDs. Thanks for your time!
Why not improve `protoc` then to generate idiomatic Go code?
&gt; This does not make sense. It's not that language specific. It makes sense for the language to cover latest confirmed security exploits, but beyond that don't expect the language to cover you from SQL injections and other exploits that completely depend on the programmer. There are language-level security features; e.g., memory safety. So it's perfectly reasonable to ask about the security features of the language.
Then why not write a new protobuf compiler, akin to what Square did with [Wire](https://github.com/square/wire)?
[removed]
Because I am sick and tired of being told over and over again that because Vim and Emacs is a proper syntax highlighter, people should not use a real IDE. Well fuck that shit. When someone asked about an IDE, they already know about Vim and Emacs and have already decided that they want an IDE, not Vim or Emacs. If you would read my top level comment, I was pointing out that the OP asked about IDEs, not Vim and not Emacs. I was answering question number 7. 
I think you're missing the point. The whole point of protobufs (or any IDL) is to define a contract between communicating components. A contract, like an RFC or a spec, is only good if it's the authority from which implementations are derived. If you let the components dictate the contract, it's like letting implementations define specs, the exercise is pointless.
&gt; I am sick and tired of being told over and over again that because Vim and Emacs is a proper syntax highlighter, people should not use a real IDE. Nobody said that. Chill, friend ðŸ•Š
&gt; Hugo doesnâ€™t depend on administrative privileges Don't you need sudo to run somethin on a port &lt; 1024? Like port 80.
Strong +1 on this.
Not sure why you needed to go onto an unnecessary rant that right from your first comment the paragraph spreads like cancer. Just tell us what **you** use and why and move on.
[removed]
I think being a site generator, it doesn't actually serve the resulting web pages. Therefore something else has to take up port 80.
I don't know if I think that the utility it brings is worth the maintenance overhead -- but none the less, good on you for getting into the compiler and doing some hacking - it sounds fun and is a big hurdle for many developers. Please don't let some of the hostility in some of the responses deter you from contributing.
Oh, I always saw Hugo as a site generator with integrated webserver. Then I need to reevaluate that.
Ah. Fantastic. Thank you so much!!! It works finally!!
It's worth if you have a use for it. It's not difficult to learn and apply best practices from your previous experience and has similar C like syntax. If you are a web developer it's worth possibly learning to use frameworks such as Laravel or Lumen to develop and create feature rich websites quickly without having to re-invent the wheel yourself or slapping a bunch of libraries together. Same could be said for any similar like language such as Python or Ruby. If you want to learn it as a language on it's own I would not recommend as just about everyone else that's replied has already said. For web development exclusively I'd say sure.
What's wrong?
&gt; Requirements &gt; &gt; fasthttp You had one job!
ðŸ˜…
Besides, nginx is really easy to use for things this simple.
Or [Caddy](https://caddyserver.com/) for that matter.
Cool, maybe to make it even more useful you could provide functions for linux too using the XDG_* env vars.
&gt; When someone asks about IDE support, almost everyone knows they mean an application that is marketed as an integrated development environment. You know, things like Visual Studio (not VSCode), Eclipse, Intellij, etc. And every goddamn time, there are these goddamn neckbeards shitting on anyone who doesn't edit their fucking code in Vim and Emacs. Woe is us for not knowing how to patch together 50 fucked up config files from random blog posts on being faster because of goddamn ctags and yy and dd. Well, some of us damn normies like modern tools and use a fucking IDE. We like being productive instead of gloating about productivity because you have all the fucking time in the world to finally get ctags working. Until you get a new goddamn computer and ctags breaks again. And fucking again. &gt;OP asked about IDEs, not Vim and not Emacs. Go back to your fucking cave and gloat over how much warmer you are for banging rocks together to make a fire while the rest of use just turn the heat up on our thermostat and let the adults talk. This post is so hilarious that it deserved to become a meme! http://9gag.com/gag/a1boeR6
That's not possible. Go doesn't have inheritance, and polymorphism is limited to interfaces. Go also doesn't have algebraic data types. In cases like this, where you have "pure data" structs with little in the way of actual behaviour associated with them, you can sort of cheat with interfaces. For example: type Record interface{ isRecord() } type Pet struct { Name string Weight float64 } func (Pet) isRecord() {} type Customer struct { Name string } func (Customer) isRecord() {} The `isRecord` is really just a dummy method so you can get _some_ type safety at runtime; only `Pet` and `Customer` will be compatible with a variable or argument of type `Record`. Now you can have: func FetchAllRecords() []Record ...which can return a mix of pets and customers if it wants to. You need a type switch to identify what you get back: for _, record := range FetchAllRecords() { switch r := record.(type) { case Pet: // is a Pet case Customer // is a Customer } } It's not ideal, but it does the job. Another method is to not do this, but instead have things like: func FetchAllPets() []Pet func FetchAllCustomers() []Customer // etc. Of course, it's tedious to write the serialization/deserialization code for every possible table, and a common way to avoid such boilerplate is to generate the code. Instead of reinventing the wheel, you should take a look at existing ORMs. [Gorm](https://github.com/jinzhu/gorm) is supposed to be good. 
The Org support is really basic.. maybe it depends on the theme you use but I built from source and tried it out over the weekend and basically it seems you can make posts from Org but that's about it -- don't expect stuff like code blocks to work, and your timestamps have to be in ISO format or they break, also the documentation is somewhat limited for the new feature and I found myself reading the PR to figure out how to use it in the first place :/ Wound up a little confused as to what this buys you over plain old org HTML export
[removed]
time: commonly used. Surprising uses of channels and goroutines. os/exec: how to run a subprocess.
 $ sudo setcap cap_net_bind_service=+ep &lt;path to binary&gt;
Out of curiosity, and perhaps a growing panic for the live website that I'm currently running on a hugo webserver, why should it only be used locally?
&lt;3 
Package io https://medium.com/go-walkthrough/go-walkthrough-io-package-8ac5e95a9fbd#.smdwi7h7b
[removed]
1. b/c https://github.com/golang/go/wiki/whygo Web applications: internal and public facing service portals, Monitoring solution (custom monitoring soln for internal services) 2. Yes, for me (VB/.net for the company) 3. Positive: single binary (easy to deploy &amp; administer), easy to work with codebases. Negative: lacks mgt buy-in within company 4. 5. Not really 6. It is as secure as you implement it e.g. For web-apps; add timeouts to server, use https, use http security headers, use secure sessions, etc i.e. They are no security threats specific to go. As for lang safety, it will panic on exceptional circumstances e.g accessing a nil pointer 7. Vim (with vim-go)
That's all I could think about while reading the article. Edit to add: I created/gave a presentation on this based on the "pipelines" article. http://talks.godoc.org/github.com/euggo/meetup/talks/feeding_concurrent_processes/feeding_concurrent_processes.slide The related repo: https://github.com/daved/conch0
thx, i will do benchmarks test
It'll function, it's just unnecessary
&gt; What happens when your Go domain object contains a type that isn't proto-representable?. Let me ask in the other way: What if your your domain business requires types/logic that is not representable using Protobufs? Then, proceeding as you purpose is not the solution, because the mapping problems keeps existing. &gt; Or, what happens when you change your Go domain objects slightly? You'll generate entirely new Protobufs files, effectively throwing out your contracts And a new alternative question: What if using protobuf as you propose, and then you change the Protobufs because your business requires it?; then the contract is broken -&gt; then new code is needed to be generated! What I understand that Proteus solves, is "where do you like to define the contract", instead of "what happens if the contract changes". - If the contract changes, you will need new code with both solutions, - If your business requires definitions that are not suitable in Protobufs, then you are fucked up with both alternatives, - The good point of defining the contract in your main language (Go, if using Proteus) -&gt; then take it as an pseudo-IDL -&gt; then take it as a contract -&gt; then auto-generate the sourcecode for other environments, is that in that case you have the service implementation for free (and idiomatic) in the original one, and the implementation of the gRPC endpoints as well.
I believe that there is no such thing as perfect code 1) Very few, if any, people can actually write perfect code because "perfect" is incredibly subjective. One person's elegant solution is anothers mess.Ask yourself how many times you have finished some code and been reasonably pleased with it and then come back after 3 months and cringe a little. I've been writing software for 15 years and I would never be arrogant enough to assume the code i have written is perfect. 2) The best systems I have worked with are the ones that are malleable and their needs change, so trying to write a perfect system is a fool's errand. Writing a system where you can change things easily is more important. Ultimately good software is about delivering value and sometimes you will take shortcuts to accomplsh that, and that's actually totally fine. The best project I worked on had an incredibly tight deadline and we took a lot of shortcuts to deliver and the code was really "bad" in some places. But it was an awesome accomplishment and really changed the company I worked for. That's not to discourage writing good code obviously, but there is a balance. Write great code that delivers value in such a way that others can get in there and change and improve it without them having to hold their breath for too long. It's way better to deliver crappy code that does something than agonise over a design which might be wrong 3 months later anyway.
It depends on what kinds of things you're building and what you mean by "study". If by "study" you mean reading through documentation and using the exported functions, then here's the top list of packages I use: * io, ioutil * os (file handling) * os/exec (calling other programs) * fmt, log (printing stuff out) * time (utilities for dates, times, durations) * net/http (writing web servers) * encoding/json (writing useful stuff out for web servers) * sync (additional ways to program with concurrency - learn channels and goroutines, too!) * testing (writing tests!) * text/template and html/template (templates for web apps) * strings, bytes (utility functions used in a lot of programs) Ben Johnson has some excellent walkthroughs of many of these important packages: https://medium.com/@benbjohnson
On the other hand most of the world is currently adhoc implementing REST APIs in code from which the manually export documentation later on - which is even worse. I would put the approach here on the same level as auto-generating swagger definitions from code - even slightly better since protobuf and grpc have more strict type schemes than plain JSON and REST APIs. My personal opinion on this topic is: It depends. I've worked on creating large distributed systems, where dozens to hundreds of engineers are working on it. For these kind of systems I see design and IDL upfront as a mandatory requirement. Everyone can agree on the IDL first and then various parts of the system can nicely be developed independently. The interface design process will however take some time. For small systems (e.g. where less than 5 developers are working on it - and where these are owning both sides of the communication) I think the code-first approach works also good. The upside is that it allows for faster iterations. However one thing that needs to be assured it that everyone on the team knows about the process and how everything works together. Otherwise some developer might change an apparently internal interface without knowing that it might also have an impact on the external interface, since [serialization] code is generated from it.
It's a well known one but i like to quote it ironically when i've written something a bit shoddy ;)
&gt; What if using protobuf as you propose, and then you change the Protobufs because your business requires it?; then the contract is broken -&gt; then new code is needed to be generated! No. Protobufs (and all IDLs) are explicitly designed to be able to manage change over time. This is one of their principal reasons for existing. They do this by enforcing certain strict semantics in the DSL, and by leveraging those semantics in the language-specific de/serializers. &gt; What I understand that Proteus solves, is "where do you like to define the contract" That question isn't up for debate, it has a single, well-defined answer. Assuming you're defining a contract for consumers in multiple languages, the contract _has to be independent of those languages_. I mean, I'm not making these positions up based on my strongly-held beliefs or something. These aren't my opinions. I'm not throwing FUD around, or trying to rain on someone's parade. I'm describing the fundamental properties of IDLs, ideas that have been pretty well-understood and reifying since the mid-80's. It's important that we don't promote fundamentally unsound solutions based on a faulty understanding of the problem space.
[removed]
This is perhaps the most minimal approach to FBP: Just channels and goroutines. The FBP libraries I visited range from almost as minimal as that to large, elaborate frameworks. Me, I am always leaning towards minimalism where possible. The two blog posts on Gopher Academy that I linked to in the article also promote the pure-Go approach. 
&gt; Actually I think makes a lot of sense in a lot of cases because in the real world it's pretty common to have already an implementation of something that over time you decide that needs to interface with stuff in other programming languages That's why [this comment](https://www.reddit.com/r/golang/comments/5whcka/release_of_proteus_generate_proto_files_from_go/deahnei/) exists. If you really want this mapping, a) generate the .proto file *once* and b) automatically generate the grpc server stub. c) Fix up any issues that crept up inevitably and d) going forward, evolve your code by hand to from the IDL as a source of truth. If this were really the concern, several design decisions would have been made differently (e.g. you wouldn't need to maintain annotations of what types to expose) and a lot of work would've been saved (because as it's a tool for moving things, it doesn't have to be perfect).
Ben Johnson assembled them here: https://medium.com/go-walkthrough
I agree it is naively worded, but the question is still legitimate.
Any particular reason why would you prefer TOML or JSON? If many people want I may think about, but I find YAML more legible for this kind of tool.
I really don't understand what is going here. Protobuf is a detail of implementation, and IMHO should be isolated as much as posible from your business logic. If you build your domain on top of protobuf schema and you let others choose how your models and your getters/setters are defined you are doing something TERRIBLE WRONG. Protobuf allow the intercommunication with other languages, is as a detail of implementation, should be easy to replace. Decoupling the protobuf schema from your code is a nice way to do it. And proteus will help to do that. This approach is done by k8s with his own tool (based on proto2): https://github.com/kubernetes/gengo. 
Personally my opinion is that if you want readability and don't need nesting / hierarchy, TOML is best due to simplicity. If you need some complexity, JSON is best because it's minimal in weight but doesn't have the whitespace trickiness of YAML where one might miss a space and have a crash upon launch without specific error messages. Once you start nesting, I find YAML to rapidly lose it's benefits, so I avoid it.
Someone in that thread said something similar which I think is perhaps even more relevant: &gt; Perfect is the enemy of shipping.
Thank you for the feedback, I'll consider it.
This has been very interesting. I think a lot of the time in the past I've been much more of a '2' kind of guy, but I have known for a long time now that it holds me back. It stops me from finishing work (on personal projects), it slows me down, and it just generally makes things more difficult. Someone that I work with was pair programming with me recently and we were going through some pretty heavy Scala code adding to it. We went so far as calling methods "blaBlaBla" because it didn't matter at the time, what mattered was making something that worked. One thing about taking this kind of approach is that naming things (for example) becomes easier once you know everything about the thing you're trying to name. The pair programming was a bit of an eye opener, and I've been a lot more productive since then, and have just valued the fact that refactoring is easy in languages like Scala and Go.
I've been thinking about doing something similar to this (well, similar to giter8 I guess) for a while now, nice one.
1) Web services. System tools. 1.1) Sometimes I do use other languages for web services. As for system tools, I use Go because it produces native binaries, and is easy to build / install / deploy. They run fast, and don't use much resources. For web services, I use Go because it is easy enough to maintain, it's not as nice maybe to write as some other languages, but it's simple, and productive. 2) Unfortunately not. In fact, I'm in a bit of a weird middle ground at the moment. PHP is probably still my main language, but I'm desperately clawing myself away from it. 3) **Positive**: Simple. Consistent. Easy to learn. Productive. Pragmatic. Amazing standard library. Native binaries. Fast. Low resource usage. **Negative**: Somewhat inflexible sometimes. Boilerplate. Some areas of tooling could be improved (e.g. testing and coverage). 4) Not yet! 5) I guess in some ways generics. I completely understand why they're not part of the language though, and I do fear for what would happen if they did become part of the language to be honest. I also kinda miss explicit interfaces sometimes, duck typing is also great, but it can lead to some annoyingly lame bits of code. 6) It seems good? I'm not really that far into Go to know. I would argue that there's a large portion of that burden that should be placed on developers of applications using Go however. I'd say the language is of a higher quality than something like PHP, and you can write safe code in that too! 7) IntelliJ IDEA, with the intention of moving to Gogland upon it's stable release. The JetBrains toolbox is great!
If you look back on that first response to your original comment, you can see that /u/jiimji was actually implying that a text editor can be an IDE if you **integrate** it with the language you're working with. &gt; emacs and vim are absolutely IDE **in the sense that** they can use language services, like Go code to provide a complete **integration** for Go **development**. I understand your point that an IDE is definitely not what I would call something like vim or emacs, but I think your response was a bit uncalled for. &gt; Honestly, it is comments like your that makes me tell people that the Go community, and devs in general, are toxic. I'm going to be blunt here, the only person who's been toxic in this thread, has been you. Just try to remember that maybe you do see things that sound like this a lot, but you are also on a forum that is home to thousands / millions of people, and that may even be the first time they've ever said something like that.
I'm also still learning Go, but here is what I figure could solve your problem package main import ( "fmt" ) type RecordFetcher interface { FetchAllRecords() string GetRecordType() string } type Record struct { Name string } func (r Record) FetchAllRecords() string { return fmt.Sprintf("SELECT * FROM %s", r.GetRecordType()) } func (r Record) GetRecordType() string { return r.Name } type Pet struct { Record Wieght float64 } type Customer struct { Record Address string } func main() { DogPet := Pet{Record: Record{"Pet"}, Wieght: 55.5} John := Customer{Record: Record{"Customer"}, Address: "55 Infintiy street"} rl := []RecordFetcher{DogPet, John} for _, x := range rl { fmt.Println(x.FetchAllRecords()) } } 
&gt; and it will happen the same way whether you use Protobufs or Proteus No it won't. Protobufs are designed to evolve gracefully, and interop with potentially heterogeneous producers/consumers. Go (or Java, or C#, or...) type definitions aren't. &gt; the way you decide how to implement that SSOT is an implementation detail that should not condition your growth. This is not true.
you maybe misunderstood my point; I was answering the @peterbourgon quote: &gt; What happens when your Go domain object contains a type that isn't proto-representable? If it happens, it is because the domain business requires something that can not be modelated by Protobuf itself. If it happens, the problem exists using Proteus or plain Protobufs, so the @peterbourgon argument is neither also valid. 
&gt; it's because the explanation is incorrect Yes, absolutely. &gt; uses reflection and gets a little confused No confusion. It sees a struct without a `String` method so it prints `{`, then each element (using the element's `String` method as appropriate), and a closing `}`. This can be better seen using ["%+v" as a format string or more than a single field in B](https://play.golang.org/p/gppv8wlIQi). 
Protocol specification and serialization isn't an implementation detail, of secondary importance to your domain models. It's the _most important_ part of your distributed system, of primary importance to your own codeâ€”which is, in fact, an implementation detail of the IDL spec! You have this backwards. So do the Proteus authors. Again, I'm not just blasting my opinion at you, this is settled distributed programming practice, and has been for many decades. I kind of hate to just namedrop on you, but look at CORBA, look at ICE, look at DCOM... this is all established.
I definitely think that for type '2' programmers, like I've often been, one way to help break out of the cycle is to literally just to force yourself out of it for at least a good project or two. Pair programming would definitely at least be a fun way to end that cycle :P
- 1st one: When the protobuf changes, the contract is substituted by a new one, and it must be handled by the guys in charge of its implementation. It does not matter if it the chang is caused by a manual operation, or an automatic one, because the change *is just the same* - 2nd one: new mantra again, http://www.jacobabshire.com/wp-content/uploads/2015/02/faith-header.jpg 
&gt; Install &gt; &gt; go install github.com/crgimenes/lpk Just a heads up, `go install` will fail unless the project is downloaded. So I'd recommend to use `go get` instead. Otherwise, cool idea! can't load package: package github.com/crgimenes/lpk: cannot find package "github.com/crgimenes/lpk" in any of: /usr/local/go/src/github.com/crgimenes/lpk (from $GOROOT) /home/user/src/github.com/crgimenes/lpk (from $GOPATH) 
Everything is an implementation detail depending of the level you are. At the level of a domain logic the transport and the serialization are implementation details in the same way that the persistence is. Speaking at domain level, having an implementation detail interfering in the evolution of your domain is a big mistake. 
Yep, I agree with you on that. When you use a quite flexible language to define your SSOT, you must be quite vigilant on how you define it, because otherwise it can happen that your definition is not exportable into other languages. To ensure we do not define "too complex things" we can: - using Protobuf as a clear rules already defined (and this can be the best in most cases), - settle our own and clear rules and use our main language, and then generate code (as Proteus does), - use only floats and strings to define our models to ensure the complexity keeps being low. IMHO you can choose one of the above, and be consequent with your decition; there will be problems in wich one or another will be the best or the worst alternative. 
I just sent a PR that checks the extension for yaml, json, and toml. You can add others easily. Though I agree with the comment below, I would never use json, it seems pretty trivial to support whatever. I don't know much about toml, but I am assuming you can have the equivalent information there. I also added an example directory and a `--help` flag. Edit: I forgot to say, neat project! I struggled with Make (I never needed to do anything very complicated), so I love to see people trying simpler build tools!
Honestly thank you for explaining this. I think what caused my misunderstanding was that every article I found on modifying the contents of these variables suggested changing them in the registry. Now I realize that isn't because it is the correct source from which to get the values but to store those that will be programmatically loaded into the environment. I don't mind having done the work, since it got me remember some things about Windows, but it seems it is superfluous. c'est la vie
Sure, previous experience with one of the LHC experiments (or any other High-Energy physics experiment) is definitely a plus. Feel free to send us questions (and a CV) directly. (Our contact informations are on the go-hep fads project) Cheers, Sebastien
Spacemacs has a go layer that is really nice, and has support for everything mentioned by /r/yauhen_l. I have mine setup to use gometalinter as the flychecker and goimports as the gofmt-command. Feel free to ask if you want any of the setup for .spacemacs
Hey all - I'm the author of the Go parts here. It was basically a prototype - I wasn't too familiar with Go web dev while starting out, and we had to rush to complete a lot of code. There's a lot that I wish I could clean up, and the `www` site is a complete mess. However, grpc-gateway was amazing for building APIs. I also wish that we had split up the packages into `cmd` and `pkg` folders. If I can answer any questions, please let me know. 
Cool! I've read the article, looks cool and not very hard to do! thanks for sharing
Thank you! Do you have any irc chat room or Gitter or similar? For a start I'm learning about fads and writing some computational geometry codes in Go. ( from my C++ ones ;) ) And the project aims to incorporate a faster jet clustering algorithm in fads using Delaunay Triangulation. ( and the usual unit testing ) 1. What's the complexity of delphes included jet clustering? 1. Are Cambridge Aachen and anti-kt performance benchmarks? 1. ;) , where can I find better Gonum Tutorials? Thank you again for replying. 
HCL from Hashicorp is a nice alternative too. I don't find the "unnested" nested TOML declarations that intuitive.
I'm the author of the package. I'm looking for any constructive feedback on the package, the README, the documentation, the wiki, etc. I've at this point tried to get all of those items reasonably well polished but would love to hear any ideas things I could improve! Thanks!
[loopback.io](https://loopback.io/)
Do you plan to do a more formal explanation of algorithm behind scheduling? Also, following m3wm3wm3wm, thank you for sharing fully fledged real-world Go project. Working for a company which does a lot of Go-based development, this is extremely useful.
Thanks for open sourcing this!
as a contributor to grpc-gateway I'm curious how/if your experience could have been improved. As I recall you contributed some documentation.
https://github.com/grpc-ecosystem/grpc-gateway is used and probably could probably use some input.
A Pull Request has been accepted, and now YAML, TOML and JSON is supported. So one can choose which likes more.
https://github.com/vectaport/flowgraph
Check out [Ponzu](https://ponzu-cms.org) - it has some of what you're looking for from loopback e.g. CLI generators etc and an API. But, it's a bit more tuned for CMS-style systems, although you can mold it to do a lot. Ps - it's my project, so excuse the shameless plug! I think it might be useful for you though. Anyway, if you have any questions about it feel free to ask. 
Thanks! Finishing the last repos for open-source right now.
Yes, net/http is part of the standard library and was mainly written by Go devs. I don't know the exact history, but it's very possible there have been commits from people outside of Google as well. My opinion on benchmarks: unless you're running a very large site with a lot of visitors, or are extremely constrained in terms of resources, I don't think it's going to make a difference. Especially if your site uses a database like sqlite or postgres/mysql/etc. One reason I like Go is that the built in testing framework is pretty easy to use and test very small and different parts of a system. I just wish you could benchmark on the playground. You could build a great site without using other libraries/frameworks. But those other options do offer some helpful things. I started with [goji](https://github.com/zenazn/goji) because it had the most example code that I could copy and paste. Now I use [chi](https://github.com/pressly/chi). The only real reason I use any framework is that I'm too lazy to write a function to do dynamic routes, like "/user/:id". Most frameworks/libraries I've seen have ways to do this.
recommend have a look at [gometalinter](https://github.com/alecthomas/gometalinter), it has supported most linters include `go vet --shadow`
Haha glad you did!
So does net/http if it's just a suffix. If you use a route with a trailing slash in the stock mux, anything starting with that path will go to that handler. So /user/ route with a getUser handler would send a request for /user/bear1738 to that handler, and you can easily grab the relevant part of the path.
&gt; What do echo/fasthttp do that net/http doesn't do? For fasthttp: Please note that fasthttpd does **not** implement full HTTP! It provides a strict subset of HTTP which often is enough for simple things and serving web browsers. And please do not look at such sort of "benchmarks" unless you are running Twitter or some realtime stock trading company.
If you're on a mobile device, don't bother clicking.
This is sort of the basic skeleton of most of my build commands &gt; go install -v -ldflags "-v -linkmode=external -s -X main.Build_Key=Value\"-extldflags=-v -static\"" ./...
Note that you are talking between 100K to 200K requests per second. It's unlikely to make a real-world difference for several reasons: - Your clients won't notice the difference between 0.01ms and 0.005ms. It will literally be lost in the noise of packet jitter. - "But I can save money by running on half the boxes" I heard you say. Nope. In the short-term, you are not going to have 100K requests/second hitting your site, so your point is moot. By the time you DO get 100K rps (i.e. millions of users), it is likely you will be able to afford 2 boxes. - Yes, 100K vs 200K is a 2x speed up. But if you start actually doing any work in your app (auth, etc), then your seedup will be a lot less than 2x. In fact, for many applications (such as resizing a jpeg), the contribution of the framework will be almost too small to measure.
What kinda limitations?
Very interesting read, thanks. I might have missed something obvious, but the graphs, for example "Figure 4: Peace, at last", you said the green line is the pprof total process memory, but what about the blue line showing actually memory leaked?
grpc gateway is the only thing gave us a courage to move all services to grpc. And etcd using it proves it won't be abandoned project soon. It doesn't feel fully mature. An dit doesn't feel plug and don't care kind of gateway. Few times I needed to dive in to the codebase. I am looking to contribute back when we have some experience of using it.
+1 for Chi, using it in several of my own pet projects currently.
We tried to use it for a while, and switched to Zipkin because, well, displaying a trace would error...
Looks like this may do what you're after: https://github.com/naoina/toml with a newer version of TOML supported.
That's interesting. At $DAYJOB I'm planning to utilize Stackdriver for tracing, so it'll be super helpful if you can share your experience.
net/http is perfect for whatever you want to build. It comes with http2 push support and websockets. On top of that you will have compatibility if you follow strictly the http.Handler pattern. But what about performance? Yes it's true, fasthttp and other frameworks/libraries achieve a lot more 200 OK responses, but what does it mean? Are you offering a ping service? In this case you should really consider one of these frameworks. But if for example you offer an api in most cases you will burn cpu and ram to marshal and unmarshal data, process stuff and wait for a database to answer your query. So in a realistic scenario you will not benefit much from using such fast library because your bottleneck will be other parts like the database.
http://hepsoftwarefoundation.org/gsoc/proposal_GoInterpreterWagon.html ?
I haven't read the link yet, but last I checked, there was no garbage collection or threading in WebAssembly, so you'll miss quite a few of the benefits even if it gets ported, and porting will be a significant effort.
&gt; tooling (libraries and executables) to be able to consume WebAssembly bytecode (wasm) and interpret it Tool to view WASM bytecode without browser could be useful, indeed, but in the browser it should have something like sourcemap, right? I mean W3C doesn't expect webdevs to read assembly? &gt; The creation of these wasm bytecode modules by the Go toolchain is out of the scope of this project. Oh, well, that's a bummer.
Last time I checked x86 instruction set didn't had garbage collection either ;) As for the threads, I heard that they are in the Post MVP plans.
DevOps Attack II, "What doesnâ€™t present version control system?": **Sooo** tempted to click SourceSafe...
I have been writing Go professionally for three years. Services I have written have served billions upon billions of requests. I use net/http. 
Good point! I totally forgot to mention that.
Thanks for the clue, but [http://llvm.org/llvm/bindings/go/](http://llvm.org/llvm/bindings/go/) says: &gt; Last modified: 13-Oct-2014 20:06 And [http://llvm.org/llvm/bindings/go/llvm/](http://llvm.org/llvm/bindings/go/llvm/) redirects to [http://llvm.org/](http://llvm.org/). So, this repo is even older or am I looking at the wrong place?
It might not cause an issue as you aren't referring to m outside of main, but it looks like it could be adjusted to not cause a vet error anyway-- call srv.Handler.HandleFunc after creating the server, instead of calling it before? 
Right answer! :-)
That fixed it, thanks. What was I doing all this time???
Rust does not have method overloading either
One of the main selling points is the conciseness of the binary protocol, and many people prefer load times to run times/language. &gt; threads... Post MVP As is a garbage collector, and I think the garbage collector is higher on the list since it will allow data to be shared between WebAssembly and JavaScript. Having Go run single threaded is fine (after all, up through 1.4 single threaded was the default), but having the garbage collector thrown in seems like a lot of work and isn't a long term solution IMO.
? I didn't say *anything* about method overloading. Were you responding to someone else? I was just saying that I think WebAssembly is to immature to make using Go with it nice. It'll get there, but I'm not excited until it gets those features I mentioned, since they're the primary reasons I use Go.
Thanks, I think I'll go for a net/http start and look into the features of the different frameworks a bit more. I may have been focusing too much on speed.
No it's not. However, if you're using git, you can perform a git "insteadOf" to change the url (and port) that git uses to fetch the package.
Right, I was aware of that workaround. Is there some reason ports are not supported in the import URL formation rules? A good many private dev shops (including the one in question here) run their own git, nexus, etc. repos. It seems like having this ability would be really helpful, and not having it seems to be no advantage.
This is what I recommend and what I do, personally. It's tempting to go for the fastest or the most feature rich framework, most of the the time, your http.Server or Client implementation is not going to be a bottleneck. Premature optimization might not be the root of all evil but it can definitely lead to needless complexity and dependencies. Even if your http api is a bottleneck, there's almost always something that can be done in terms of design that will have a bigger impact. You can leave the serving of static files to something like nginx. A change to your api might allow for clients to make a few larger requests and/or cache response data. Templates can be pre-rendered partially or completely. But in any case, before you start thinking about that, profiling is the best first step.
Here's what a Fisher-Yates shuffle looks like with that interface: [package fisheryates](https://github.com/matttproud/fisheryates).
Hmmm ... that's weird, have you tried clicking the mouse? Also the code I'm writing in these videos is different to that repo
Cool, I was going to do the same thing before I saw this, thanks for saving me some time :)
[removed]
Mono repo for everything; or submodules in your own namespace; or private internal git https server.. not sure how the ports thing would help.
Presumably there will be wasm APIs for the browser platform that Go could call into.
Use time.UnixNano() please, as you lose probably a half of "entropy" in case of using time.Unix() for seeding.
I'm new to WebAssembly so keep that in mind. People often don't know because of the poor naming, that WebAsm is not Web-specific: http://webassembly.org/docs/non-web/ My understanding is that any Web/DOM APIs would be provided by the Wasm runtime, and all Go would need to provide is a generic Wasm syscall function. DOM libraries could leverage that wasm syscall package.
What if your definition of perfect code is perfectly malleable? I feel the stupid urge to make sure all my dependencies are mocked, my implementations are generic as possible, my programs are configurable as possible, etc, etc.
&gt; Would google/seo notice the speed difference and rank pages higher because of it? Ha ha, really doubtful -- The speed rank was added because Humans don't like to wait for slow pages. Anything under 50ms is probably ranked the same. (See [actual slide](http://www.thesempost.com/how-page-speed-as-a-ranking-factor-works/) from google presentation) Also, when you are talking about 0.01ms, the jitter from all the routers in between you and the server WILL dominate your measurements anyway. Just like the old joke "I don't have to be faster than the bear, I only need to be faster than you", speed doesn't matter unless it serves a specific business purpose. If you spend time speeding up something users don't care about (or can't even measure), you've wasted your time. Work on something user-visible to get the biggest bang for your buck.
Good idea to post this but bad timing my friend @dryski, a couple days ago most of the websites were offline,including isitdownrightnow.com (yes...), or operating very slow because of 'serverless' (amazon web services this time). AWS/Serverless is Wonderfulâ€”Until They Go Wrong. Some links to proof the concept to not use serverless without second thought: - WashingtonPost: https://www.washingtonpost.com/news/the-switch/wp/2017/02/28/why-a-whole-slew-of-websites-are-suddenly-down-or-working-slowly/?tid=sm_fb&amp;utm_term=.583d2ca0921b - CNN: http://money.cnn.com/2017/02/28/technology/amazon-web-services-outages/index.html - CNET: https://www.cnet.com/news/no-the-internet-is-not-broken-amazon-web-services-is-just-having-issues/?ftag=COS-05-10-aa0a&amp;linkId=34980800 - MIT Technology Review: https://www.technologyreview.com/s/603738/centralized-web-services-are-wonderful-until-they-go-wrong/?_ga=1.82562070.1263144274.1488319022 More: https://golangnews.com/stories/1835-serverless-is-wonderfuluntil-they-go-wrong. P.S: I do not want to offend anyone. We should protect new developers that are so enthusiastic about 'serverless', sometimes we are in position that it seems that we have no other option but please think twice before moving `serverless`.
good advice, thanks
&gt; Now, I haven't done much frontend development recently myself, but wouldn't it be awesome to use same Go in the browser too? What are your thoughts on this topic? I am _extremely_ interested in this. Go compiled for WebAssembly is like GopherJS 2.0 for me, and I use/rely on GopherJS every day. I want to improve efficiency, file size, performance of Go that runs in browsers, and WebAssembly is the way of achieving the next step in doing that.
Can Throttled (https://github.com/throttled/throttled) do that?
A complementary and different approach. I had read a while ago this example (rosetta code ?), and think it was more instructive (my own opinion), and did use the interesting rand.Perm() function from the standard lib. /* The approach taken requires 2n memory and will run in O(n^2) time swapping once per final changed character. The algorithm is concise and conceptually simple avoiding the lists of indices, sorting, cycles, groups, and special cases requiring rotation needed by many of the other solutions. It proceeds through the entire string swapping characters ensuring that neither of the two characters are swapped with another instance of themselves in the original string. use of: rand.Perm() sign: func Perm(n int) []int Perm returns, as a slice of n ints, a pseudo-random permutation of the integers [0,n) from the default Source. */ package main import ( "fmt" "math/rand" "time" ) var ts = []string{"abracadabra", "seesaw", "elk", "grrrrrr", "up", "a"} func init() { rand.Seed(time.Now().UnixNano()) } func main() { for _, s := range ts { // create shuffled byte array of original string t := make([]byte, len(s)) permuted := rand.Perm(len(s)) fmt.Printf("len(s): %d\nrand.Perm is: %v\n", len(s), permuted) for i, p := range permuted { t[i] = s[p] } // algorithm: proceeds through the entire string swapping characters ensuring that for i := range t { for j := range t { // neither of the two characters i and j are swapped with another instance of themselves in the original string if i != j &amp;&amp; t[i] != s[j] &amp;&amp; t[j] != s[i] { t[i], t[j] = t[j], t[i] break } } } // count unchanged and output var count int for i, ic := range t { if ic == s[i] { count++ } } fmt.Printf("%s -&gt; %s (%d)\n----\n", s, string(t), count) } } /* Expected Output: len(s): 11 rand.Perm is: [10 7 2 6 8 1 9 3 5 4 0] abracadabra -&gt; baadarrbaac (0) ---- len(s): 6 rand.Perm is: [4 0 5 1 2 3] seesaw -&gt; wasees (0) ---- len(s): 3 rand.Perm is: [2 0 1] elk -&gt; kel (0) ---- len(s): 7 rand.Perm is: [2 3 6 1 4 0 5] grrrrrr -&gt; rrrgrrr (5) ---- len(s): 2 rand.Perm is: [1 0] up -&gt; pu (0) ---- len(s): 1 rand.Perm is: [0] a -&gt; a (1) ---- */ 
Yes, yes, I know. It was just a silly gist :P
I'm quite new to Go but I've given this a stab. https://gist.github.com/eviltofu/c3441eef2ea58d16725f7ef0b2ba1adf edit: *oops* I made the mistake in assuming that it was a socket listener but I think the same principles can apply?
Go was never single-threaded, and a single-threaded Go implementation can never perform well without asynchronous i/o. You are confused because GOMAXPROCS=1 used to be the default, but that is something else, Go programs still used multiple threads. 
&gt; I've read somewhere that as of now there is no efficient way to implement setjmp/longjmp functionality, therefore wasm is not a viable target of a gc port. Go can use a virtual stack pointer, or threaded code, or implement a continuation-passing-style strategy, that's not a problem. The biggest problem seems to be the lack of threads or async i/o which precludes an efficient implementation. 
&gt; Is there some reason ports are not supported in the import URL formation rules? Yes, there is! Imports are not URLs. Really, they are not. Import statements as far as the _compiler_ is concerned are just strings naming object files to import. Common sense and the tooling like to go tool make them file paths. See https://golang.org/cmd/go/#hdr-Import_path_syntax . Do not mix up the import with what `go get` does: `go get` uses _convention_ that package paths may start with a domain name and `go get` has some built knowhow about well-known code hosting sites and how to git clone or hg clone or bzr clone/branch/whetever a remote repository. Remember: Package paths _are_ _not_ _URLs_. But `go get` is clever and can do more: Read https://golang.org/cmd/go/#hdr-Remote_import_paths for details. You might even recompile the go tool and bake your knowledge about your specific code hosting site into the tool. But please do not think of import paths as URLs.
 package llimit import ( "context" "net" "golang.org/x/time/rate" ) type throttledListener struct { net.Listener *rate.Limiter } func NewThrottledListener(l net.Listener, r *rate.Limiter) net.Listener { return &amp;throttledListener{l, r} } func (l *throttledListener) Accept() (net.Conn, error) { l.Wait(context.Background()) return l.Listener.Accept() } . l, _ := net.Listen("tcp", endpoint) r := rate.NewLimiter(rate.Limit(200), 1) l = llimit.NewThrottledListener(l, r) http.Serve(l, nil)
No, no no. Do not use a cryptographic RNG when a plain old PRNG will work.
More importantly, *never ever* re-seed like this. Seeding is something you do *once*. For a utility routine it should either use the default PRNG and leaving seeding to `main` or it should accept a `*rand.Rand` argument and use that.
FFS, the [Fisherâ€“Yates shuffle](https://en.wikipedia.org/wiki/Fisher-Yates_Shuffle) has been know for almost 80 years.
Or "aws s3 sync"?
That's exactly what I meant. User code was not run in parallel until 1.5, and that happened to allow my company to write racey code without ever realizing it (upgrading to 1.5 was really costly for us since some assumptions didn't hold). Yes, Go has threads running in the background (i/o), but the get effect was that Go was single threaded until 1.5 from the user's point of view.
only thing the world ever wanted
Chuck Norris wears Jack Bauer pajamas.
Yes, I see how this article is supposed to be targeted at novices, but if anything, novices should be told upfront "someone thought hard about this algorithm; it's easy to mess up if you do it yourself; just copy the pseudo-code from Wikipedia and translate it into Go."
DISCLAIMER: I'm the co-author of the project. We do not intend to compare Go with other languages but to appreciate the overhead of our shim compared to officially supported AWS Lambda languages. Go 1.8 plugins represent a huge opportunity to make running Go on AWS Lambda native &amp; effective (waiting for an official support from AWS). 
Sorry. It is corrected.
&gt; What was I doing all this time??? Been feeling like that since 1978. :-)
Good place to start looking is the official website https://golang.org/
That is quite informative, thanks. It appears I was looking at the wrong Go language. I did not realize there were two unrelated programming languages named Go.
Coool :) Do not forget we are here also to help https://gitter.im/eawsy/bavardage. Good luck in your path to master AWS and hope to see you soon ;)
I spent the first half the article internally chanting "Please don't iterate and swap." over and over. Then I hit "the best way I have found is to ..." Sigh. 
It's cool to take the initiative to make a command you think would be useful, but I personally would reconsider the command name. First is due to it not being POSIX compliant since the command name is not alphanum, which seems conflicting as a motive was portability. I admit POSIX spec can show it's age at times so a non alpha-num char here and there I could thumbs up, but this is a particularly BAD character to choose because `..` represents the parent directory. Add this to the fact it's a shell command that is 1 character long (g.. is a Control+A and del away from "..") plus the fact it's used to provide "lists" of things that may be sent to subshells for possible mutation of the file system. rm -rf .. ? Just some food for thought, aesthetics are not a good motivator for naming executable's that live in your PATH.
i know, took me a while ;)
Wanted to link this there, hoping to get this out. Got frustrated over the last couple months trying to maintain [xo](https://github.com/knq/xo)'s code and that I had to deal with 5 different command line tools for interfacing with databases. Now that all the databases run natively on Linux, I'm hoping to further expand xo's support, and do some massive cleanup of the code base. As such, I needed a single, universal tool to just execute simple queries against the database, and so built this. I'm hoping that in time usql will be as feature complete as PostgreSQL's psql, and possibly even surpass it, and add some cool / exciting features. The great thing about this, is that it'll work with every database! Already usql supports the major relational databases, and I'm hoping to add support for all the "NewSQL" databases as well. Some of the NewSQL databases are hypothetically compatible already (Cockroach, MemSQL) as they implement compatible wire protocols to one of the other major databases. EDIT: I've added an [asciinema recording here](https://asciinema.org/a/73gxbg62ny2fx9ppxu0kd8c48).
 $ go get -u github.com/knq/usql package github.com/go-sql-driver/mysql: remote origin not found package github.com/lib/pq: remote origin not found I can `go get` those two packages manually, but...what's going on?
I'd be interested in hearing what you do with GopherJS and what your workflow is like. Do you hang out in #go-nuts?
There aren't. The other one is named "Go!" with an exclamation mark.
Agree, but definitely not cross-platform. It doesn't matter anyway I guess.
For the use cases this covers it seems that GNU Make etc. would be less verbose and just as clear, but I can see its relevance as a learning project. EDIT: On a second look, it actually does seem like this will make your life a bit easier if you have a command generating more than one target. GNU Make will have to be tricked into doing that by specifying multiple pattern targets for a rule.
This is seriously misguided.
I think this isn't the question here. The question is about the best way to do something, not just the way to make it works ...
This guy has so many good posts I am gonna buy his book ... 
Could you comment on why there's a Docker dependency for building? One advantage of using a shim like https://github.com/jasonmoo/lambda_proc is that I can cross-compile my program using normal Go tools.
The algorithm explained after that comment is the same as "The modern algorithm" section in the Fischer Yates Wikipedia page and I state that it was intended for when you don't want to create a new list and want to shuffle in the existing list. Could you share why you were disappointed with my approach described there? Am I missing something?
The last two implementations are the same as "The modern algorithm" section in the Fischer Yates Wikipedia page. I didn't know that was the name of the algorithm when I wrote it, but after reading the Wikipedia page they are the same. Are you stating that this is common knowledge and doesn't deserve a blog post, or were you just frustrated that I didn't reference that Wikipedia page, or something else? I'm happy to update the post to make it better I'm just not clear what would make it better aside from a link to Wikipedia perhaps.
It's a Chinese game, not japanese. 
Because of good advice ?
At the very very least, the call to `r.Seed(time.Now().UnixNano())` shouldn't be called while generating it. It's constantly being seeded with a known number!
Thank you for your in-depth and informative explanation of the matter. I do not have the security background to determine if this poses an exploitable threat in the wild, but I wanted to raise attention to the matter after it was brought to my attention earlier today. Thanks again. I'm saving this comment for future reference.
So far I have seen engines that use x/mobile/exp/audio/al , github.com/hajimehoshi/go-openal/openal, and https://github.com/hajimehoshi/ebiten/tree/master/audio . One audio library that all engines can use would be very nice. The one that has support for openal for desktop, android, ios, gopherjs etc.
There is one part that concerns me; &gt;This works! We use net.Conn.RemoteAddr().String() down at the connection level and http.Request.RemoteAddr up at the request level. This could lead to an unprivileged attacker behind the same NAT as a victim being able to introduce faults into this code logic unless it honors the port from which the request comes and also purges closed connections instantly.
hmm, lgtm then. Thanks!
&gt; waste high quality entropy for defeating bots Entropy does not run out. It's a myth. &gt; Using math.rand for for aes iv generation in ctr mode is OK. This is dangerous advice. I know what you mean, CTR IVs only have to be unique, not unpredictable. But `math/rand`is not seeded by default, so that would explode very easily. And even if seeded with the time, time rolls back. And other modes than CTR are less forgiving and users can't be expected to know. And in general, **it's bad advice to suggest to use anything else than `crypto/rand` for anything crypto or security related**.
I gotta start getting code reviews by Reddit. This is awesome.
Thanks :) I appreciate the response. 
&gt; Entropy does not run out. It's a myth. I'm not sure how wasting high quality entropy implies I think it "runs out", perhaps just looking for things to disagree with. Regardless such a sweeping statement that entropy does not "run out" is incorrect without qualifying your source of entropy. When your source is a entropy pool that derives from many origins of both finite noise as well as other hardware entropy sources than your statement is fair. &gt; This is dangerous advice. I'm not giving advice, I am evaluating this discovery in a security context. I'm sorry if this didn't clearly set my intent, maybe I could have worded it better: *I'm curious if this usage opened any attack vectors I searched the repo real fast and there is three places it's used.* &gt; I know what you mean, CTR IVs only have to be unique, not unpredictable. But math/randis not seeded by default, so that would explode very easily. Wrong, the default rand.Read() method uses a source seeded with the numeric constant 1 if memory serves me right. &gt; And even if seeded with the time, time rolls back. And other modes than CTR are less forgiving and users can't be expected to know. I'm not sure what your point is here, it doesn't make sense to me. You would need to elaborate for me to comment further. &gt; And in general, it's bad advice to suggest to use anything else than crypto/rand for anything crypto or security related. Again this was not advice, it was a quick cursory evaluation of the potential attack vectors of this discovery. Regardless this regurgitates what I finished my post with: *If you want a CPRNG, use one. If you want a PRNG, use one.* lol, you could have elaborated on what I said .. adding value instead of taking an adversarial approach that made it seem like our information was conflicting. Have a good one.
If you only need to shuffle what the first item of the array is, you can use the *gc* implementation of map. The earliest versions of *gc* don't have this feature. The rest of the array follows sequentially from the first item in a cycle. package main //must use the "gc" implementation of Go import "fmt" func main(){ vals := []int{10, 12, 14, 16, 18, 20} valMap:= map[int]interface{}{} for _, v:= range vals { valMap[v]= nil } for v, _:= range valMap { fmt.Print(v, " ") } //will print randomized cycle, e.g. 14 16 18 20 10 12 fmt.Println() } 
start out with something well-known like Postgres or MySQL you will almost certainly never get anywhere close to the performance limits for these tools given a reasonably well-designed schema and reasonable hardware a decently provisioned MySQL server should scale to thousands of txns per second it will be more important for you to be able to find good help online and good libraries
I agree w/ the others w/ Postgresql. Depending on the data you might want to try Postgresql's jsonb. I would also recommend using an interface so that if you decide to switch databases later then the transition will be much easier. See https://appliedgo.net/di/
Pretty cool how often you see Chinese translations of Golang docs. Something about the language is making it really take off there. Also, [Faygo](http://www.faygo.com/) is a crappy pop (read: soda) in the US :)
I guess I'll go with postgres then. Thanks for the responses guys. And thanks for the link. I need to do some reading up :-)
It happened on two separate occasions and is reliably reproducible. I don't think my network has anything to do with it.
I didn't touch the repositories; I just let Go do its thing. After the command of paragraph 1, I ran `git status` to check the repositories and they weren't git repositories. It's as if `go get` just raw-downloaded the files.
In that case, the easiest thing to do would be to just delete those packages from your GOPATH and then the commands should work as expected. Not sure how/why the .git folder isn't there. The go tool always does a normal clone afaik.
Huh, I wonder if that name will affect potential adoption... Apart from not being able to read the docs (nor in any language understand), the code is pretty clean (and in English, thank GopherGod) along with the examples and the functionality for documenting and testing the endpoints are truly neat.
Like I said in the question, I did, and when I did that, it worked. However, if I delete the dependencies and run `go get -u` on the original project, the problem re-appears. Edit: and somehow it doesn't now. What a weird problem.
I'd actually expect performance issues with Postgres, not "it can't do it" or anything, but just because that is enough data to probably need an index. I recommend [this resource about indexing](http://use-the-index-luke.com), as it is designed for use by devs in pretty much exactly this use case. But Postgres is still the right choice here, and it's hard to overstate the utility of learning how to use relational databases for this task.
Add -v and -x flags so you could at least see what's happening. 
It's possible some other tool like godep or glide might be pruning out unnecessary files including the .git folder? It wouldn't be the weirdest feature ever.
[removed]
Give ElasticSearch a shot. Your updates are once per day, so ACID transaction support isn't a necessity. It sounds like you're loading this data only to query it, and your query examples are faceted search terms. If you need to do any sort of merging or upsort to append the daily data files to what's already present, you may want to load the data into a relational database or other intermediate system of record for pre-processing before building your search indexes in elasticsearch, but if your daily files always contain all the data, just load them into elasticsearch and your project is mostly done. It's super easy to cluster (if your data or query volume grows) especially with your once daily offline write and 100% read workload. This is exactly what databases such as this were built to handle.
Great write up and great tool!
I seriously thought ElasticSearch was a service and not a db. I'll do some reading up. Didn't know the term "faceted search terms". Thanks! More reading up :) I do need to merge data (if my understanding is correct); say, add the manufactor to the red cars when querying for them. Yes, the data I get is all of the data, and I shouldn't need to validate it as such. I'll do some reading up on ElasticSearch and see where that takes me. Sleeping upon this question though, made me realise that exactly because the data gets flushed every day, it wouldn't be too difficult to change in the future in case I pick the "wrong" option.
I actually went with MySQL for my previous side project (an RSS reader with a web interface) and I've been generally happy with it (except that one time where I had to learn about utf8mb4). I think I'll go with Postgres now though. It seems like a really good "general purpose" db. Has lots of help and documentation. We use it at work already. And I get to use the excellent sqlx library. :)
&gt; &gt; waste high quality entropy for defeating bots &gt; Entropy does not run out. It's a myth. Although perhaps uncommon now, there was a time that system crypto random devices (e.g. `/dev/random`) were a limited system resource. E.g. before FreeBSD switched to Yarrow (and more recently to Fortuna) their `/dev/random` would block based on how much was read from it; `/dev/urandom` was an unlimited/non-blocking device (which for FreeBSD since Yarrow is just a symlink to `/dev/random`). The Go documentation mentions `/dev/urandom` but the code has both `/dev/random` and `/dev/urandom` so it's unclear if there are platforms on which `crypto/rand` will block. 
[removed]
I've heard stripping is not a good idea on go binaries, but instead you can use buildflags to have the same effect (omit symbol and debug info): go build -ldflags="-s -w"
ps if you want to see cool shit in Go, check out this guy: https://github.com/fogleman he is downright *inspirational*
The Linux kernel requires a root file system to find processes to start (such as init), so we have a root file system. In addition, the Raspberry Pi 3 requires a FAT file system holding firmware files, so we have that as well. Optionally, you can create an ext4 file system for permanent data (generated and/or used by the applications you run on top of gokrazy).
interesting idea
This looks cool! How hard would it be to make this work with other versions of Raspberry Pis?
Thank you!!! That was an amazing feedback. I'll implement those changes as soon as I can, I'm really thankful for you reviewing my code and making those suggestions. I'm looking forward to improve my knowledge in Go and security
 tee := io.TeeReader(c.Conn, c.buf) I'd make it this instead: tee := io.TeeReader(io.LimitReader(c.Conn, 5), c.buf) Since you're reading 5 bytes afterwards either way.
[I think so](https://medium.com/@kelseyhightower/optimizing-docker-images-for-static-binaries-b5696e26eb07#.1799pib7c) Maybe it'd be worth looking into, I may actually do so myself. I like to use docker with my go apps, it's actually become so ingrained in my workflow I almost don't feel right going without it. Either way this is a really neat project, I've saved it so I can play with it when I get my hands on one of those new pi's Cheers
Definitely an interesting idea. How does wifi work with encryption? Or doesn't it? 
I realize that you probably did this mostly as an exercise, but for serious use I think that https://godoc.org/golang.org/x/crypto/nacl/secretbox would be a better choice. I'm not a crypto expert but my understanding is that raw AES can be difficult to use correctly and securely.
[removed]
Can you please explain more? Channels are not used for slice context, they work with atomics. Main allocator uses channel but its fine for that purpose. 
That's true, I should have looked up. :P I see `!bytes.Equal(password1, password2)` though and am immediately triggered.
It would be better if you post the code. But the only thing that I can think of right now is that you've given a different package name in the test file. Like both files are in the same folder, but instead of both having package name `views`, the test file has package name `views_test`, and then from this package you can't access the unexported functions of the `views` package.
Thank you! Any feedback about the code? this is my first golang project
Feel free to give any kind of feedback. This is my first go project and I would love to improve myself. Any feedback is greatly appreciated!
This is awesome. I can't wait to see where this Go's in a year or two.
That should be easy to track. It seems you are not connecting to the right database then... 
 views $ go test 2017/03/05 13:08:18 map[name:[business]] 2017/03/05 13:08:18 business 2017/03/05 13:08:18 searching for product name business 2017/03/05 13:08:18 map[name:[business]] PASS ok github.com/thewhitetulip/views 0.068s views $ go test views_test.go # command-line-arguments ./views_test.go:12: undefined: SearchHandler ./views_test.go:36: undefined: SearchHandler ./views_test.go:47: undefined: SearchHandler ./views_test.go:59: undefined: SearchHandler FAIL command-line-arguments [build failed] ``` Okay, so I figured out the issue, I wasn't doing testing properly, I did `go test views_test.go` instead of `go test`, by doing `go test` it works fine!!
Thank you for your help!!
Why would you *need* ro run those things on a different port? I've seen plenty of internal tooling, but pretty much always on the ports you'd expect it on. Seems easier and more predictable to me to do that, rather than change tools that expect things to live at their standard ports. 
Do you have a design description? How does it compare with sync.Pool?
Thanks for the reply. That would be the solution I thought of initially, but I couldn't figure out how to implement, mostly because of JSON. Most of these modules have the same properties, say ```{ "Handler":"net", "Refresh":"3s", ... "Options":{ What differs from module to module is this part, as I'll have say NetInterface as an options for this module but Format as an option for the date one. }``` Thus, how can I know what to unmarshal it into? I don't think I can unmarshal it into []Module, since Module is an interface. EDIT: By the way, here's [an example config file](https://github.com/guglicap/golem/blob/develop/config.json), since I'm on mobile and the example I typed is probably not well formatted. 
You could implement the `json.Unmarshaler` interface on the `Config` struct and use [`json.RawMessage`](https://golang.org/pkg/encoding/json/#RawMessage) for module-specific options. That might work. Have a look [here](https://play.golang.org/p/ywhTDErAka), where I took some of your code and applied those ideas (including, to some extent, what /u/beknowly suggested). EDIT 1: in the code snippet, creating a `json.Decoder` is not necessary (line 51), as a call to `json.Unmarshal(data, &amp;conf)` would suffice there. EDIT 2: here's, what I hope to be, [an improved version](https://play.golang.org/p/QhbpVOHJf_).
Thanks a lot, that should do. 
Wow. A Go program that relies on on a specific compiler implementation choice or detail for correct behaviour is not a correct Go program (at best it *might* be a "correct" "gc v1.8 program"). E.g. if I find some code that only only produces correct results under a Watcom C compiler from 1988 then it's not a correct C program.
E.g. https://play.golang.org/p/CxoINwV6qu.go For me, calling `rand.Int63()` is ~23 ns whereas calling r.Int63() ~7.3 ns. With [B.RunParallel](https://golang.org/pkg/testing/#B.RunParallel) and 8 go-routines (on a 4-core Xeon with 8 processors) calling `rand.Int63()` goes to ~155 ns whereas `r.Int63()` is ~2 ns. BenchmarkRandG 100000000 23.0 ns/op BenchmarkRandL 200000000 7.31 ns/op BenchmarkRandGParallel-2 30000000 52.1 ns/op BenchmarkRandGParallel-4 20000000 114 ns/op BenchmarkRandGParallel-8 10000000 155 ns/op BenchmarkRandLParallel-2 300000000 4.17 ns/op BenchmarkRandLParallel-4 500000000 2.67 ns/op BenchmarkRandLParallel-8 1000000000 2.02 ns/op 
I find the use of the cli library distracts from what would otherwise be a more concise main.go file. I'm probably wrong. I'm terrible at go.
How does this compare to elastic beanstalk? The problem I have with EB is that I still need a manual push or a Jenkins job to update my app. Does CodeDeploy fix that? If so are automated tests an option?
Interesting, all I am using the uuid4's for is to be able to trace a request thru my stack. So randomness isn't as important as uniqueness. Found it strange that crypt/rand had such a high collision rate for something that was "cryptographically ready"
I see. Still definitely an awesome project though. Will keep my eye on this one.
There is an official AWS CLI for elastic beanstalk. It's called 'eb'. I'm really not sure about best way to deploy apps. I've been wondering for a while and every single approach is annoying in some way. Jenkins is the standard tool but it never seemed like a good tool, more like a huge mess of java plugins and a central point of failure. Something else that can test and auto deploy after a git checking would be useful. I've never really played with code commit, it's on the list of things to check out. 
The bridge is JS. There is no way around DOM access without JS at the moment.
I feel you man. Yes, 'eb' exists. But it's certainly not an easy usage. And you still need a wrapper around that for anything that is more complex than a few cli calls, I think. :)
Why, when I run the overwrite command to write my SD card is it trying to create directories in $GOROOT/pkg instead of $GOPATH/pkg? gokr-packer -overwrite=/dev/mmcblk0 github.com/gokrazy/hello 2017/03/05 21:20:09 packer.go:215: installing [github.com/gokrazy/hello] go install runtime/internal/sys: mkdir /usr/local/go/pkg/linux_arm64: permission denied 2017/03/05 21:20:09 packer.go:218: exit status 1 My user doesn't have permission to write there, therefore I can not run the overwrite command.
This is very nice really! Please contact me if you need any kind of support around piladb, I'll be glad to help! Edit: Please note there's no Windows release yet, although you can build the binary from source and it should work as expected.
**UPDATE**: S3 Deployment support has now been added. It's possible to define a bucket and a zipped version of an app to deploy.
Thanks a lot for this!
From their github repo: Awesome project lists using Gin web framework. https://github.com/drone/drone https://github.com/appleboy/gorush
None. Use net/http
Thanks, but I'm looking for frontend heavier projects (so i can look at their structure and management of a lot of templates, etc)
Hi, Congrats on shipping this. What were your motivations for building this? What specific use will you apply this to?
LOL, Git before Linux? XD
It is a cute joke. You can manipulate commit dates easily with `git commit --date "Tue Jul 18 19:05:45 1972" --author "BK &lt;bwk&gt;"`
&gt; If he did, how could anything he did to it be "last-minute"? Note that two of the commits were done on April 1, so you just fell for an April Fool joke.
A big thanks for taking time to review the code! Please check out the modified [Code](https://github.com/BharatKalluri/MovieScore). I corrected all the problems you mentioned in your comment. I am working on the releases branch, and I should also start writing tests. Any more suggestions? and a small question, I made a LogError function which handles error's. But is there any other more simple and neat way of handling errors?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/git] [Why does the Go project's commit history date back to 1972? â€¢ r\/golang](https://np.reddit.com/r/git/comments/5xqecf/why_does_the_go_projects_commit_history_date_back/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Thanks! Have you played with it in this regard and gotten a feel for how powerful it is compared to Python / Perl? I'm assuming it's going to be a magnitude or two more efficient for things like this?
In most real world scenarios, Go will be much faster for most tasks. It has a significant advantage over Python and Perl because it is compiled, though Python and Perl can eke ahead on certain tasks like JSON (de)serialization due to having highly optimized low-level libraries. I'm confident Go will close that gap before long, though. I think the biggest advantage of Go over Python and Perl are 1) first-class concurrency 2) channels, to easily communicate over and synchronize threads 3) strong typing 4) great tooling out of the box (though debugging still has a ways to go) and 5) the most comprehensive standard library I've seen. Have a look at http://gobyexample.com, It showcases many of the language's better features.
Thanks! 
From a quick glance, I noticed you're not using a space in comments after the slashes. I'd recommend to use one, see [rationale](https://dmitri.shuralyov.com/idiomatic-go#comments-for-humans-always-have-a-single-space-after-the-slashes).
Get loose with it.
Are there relatively up to date benchmarks with Go and other common languages for web services? I've seen a few, but they were old. I'd love to see how Go fares in comparison to Node, or Java, and across tasks (JSON serialization, fetching from data stores, etc) 
I just realised that you expose most of [your code](https://godoc.org/github.com/BharatKalluri/MovieScore/getratings). So, it raises the question. Do you want all of this code to be available to other users? If the answer is no, then move this code into [internal package](https://golang.org/s/go14internal). And if the answer is yes, then logging error and aborting user's program is not a nice way to handle errors. You should return them instead or handle internally in some meaningful way. Suggested readings: [error handling and Go](https://blog.golang.org/error-handling-and-go) and [errors are values](https://blog.golang.org/errors-are-values). Not [movieTrailerUrl](https://github.com/BharatKalluri/MovieScore/blob/master/getratings/movie_trailer.go#L32), but movieTrailer**URL**.
WTF. There are even CLI utilities that do the migration for you, including full commit history.
This is a good idea, it'd be great if you build some app which people would use, in their private capacity, there is no bigger joy than building some tool and sharing it with others :) Self promotion: Anyone who is new to Go and wants to build webapps also can read my book, https://github.com/thewhitetulip/web-dev-golang-anti-textbook. Thank you! 
I've used git-svn before too and managed to keep the commit history tidy
You would do yourself a favor and consider separating the front-end and back-end of your app. Use HTML/CSS/JS for UI/UX front-end and go for API back-end.
I suggest you to check out [gometalinter](https://github.com/alecthomas/gometalinter).
/u/bluexavi should show us the code he used to find the seed.
first sha1 and now this
Read the code of math/rand. It's completely deterministic so finding a seed that gives those results becomes an exercise of math then. That is why you should never rely on math/rand for any crypto/password stuff and you should use crypto/rand instead.
It's pretty handy not just for converting repositories - it can also be used as a Subversion client. (I think even Linus Torvalds mentioned people doing this as early as [that Google talk about Git](https://www.youtube.com/watch?v=4XpnKHJAok8).)
Start with seed zero, get random numbers from it. It should produce (modulo 4) 001021001201003. If it doesn't, then add one to the seed and continue until you get it. Fizzbuzz repeats every 15, so I reset the seed every 15 in this program.
Yeap, or brute force it (what you describe). That should work as well :D
I understand that. Otherwise it would be completely pointless to set the seed in the first place :)
%4? In a code like this? What's wrong with &amp;3?
Awesome! I'd love to see the assignments when ready. Some points: - For the REST calls, you can find here the whole API specification: https://docs.piladb.org/specification.html - When building from source on Windows, try to do it from latest release tag (https://github.com/fern4lvarez/piladb/tree/v0.1.2), otherwise `version` will show `undefined`.
It is the good language for back end development for any type of applications....
You are right, I did mistakes, they are mostly fixed now. Your spying is very welcomed :D 
Yes but fuck nodejs on something like the pi. Fuck it right in the face.
both is magic to me
I recently looked at recreate loopback in go using gin cos I too want to shift from it. I used zoom for storage cos my use case requires minimal disk io, so redis was key. I figured I could generate much of the code from my existing json model files but of course the logic would have to be rewritten. I got as far as writing similar poc things to remote hooks, mixins but started to wonder if it was worth it and if I was trying to make go behave like node js. I never answered that question, and never got beyond poc code for a simple model, nor did I get to a generator. The generation is easy enough though I did it before at work for a loopback SDK so we didn't have to write API models for our golang parts of our app. Gotta love json.
Bonus points if you get the job!
Oooh, neat. Never thought of that.
Any way to make GUI in go for android? 
The top answer to "What changes would improve Go most?" was so surprising ðŸ˜‚
I didn't want to go full obfuscation, but this would be a good addition. I like the modulo 15 == 1, because you might expect to see a modulo 15 but it would be == 0. I think for a proper job interview response, the lucky seed should be passed from the command line.
I agree that "examples" are high on the list of helpful things I would amend that to be "complete examples", meaning, I can run it
Fascinating. Trading +15% binary size for +9% performance. I wonder how the Go team decides what tradeoffs are "worth it". Also I couldn't tell from the slides; I wonder if the mid stack inlining will be a toggle in the 1.9 release or will it be the default. 
/me smooths down his knee long grey beard starts in a raspy voice: In my time... In reality, I was 12 when I learned Z80 assembly in 1987 and you can't unlearn it. Due to the limited machine resources I actually learned to disassembly in my head, C9 was RET the rest I forgot. But just because I do not remember the tables any more it doesn't mean my thinking hasn't been deformed for life :D
I understand your point, it should not complicate GC's work because what buffpool does is just keeping references in order to prevent GC from collecting them for a short period of time. This might not be a good solution, I wanted to have something to work with. If you are referring to recycling ticks , this is a useful pattern to dynamically cope with a sudden burst in demands. If you look at circulation flow, allocation drop significantly at peak as more released slices get reused and there will be fewer references in the pool with timestamps older than recycling rate, a decrease in demand causes their reference removal. Assuming that pool had the only references to those slices, they should get collected by GC. The purpose of having a context is exploiting those peaks and valleys to provide byte slices to less demanding parts. It's a very simplistic approach, but hope you get the idea. Currently it uses a channel to fetch slices and concurrently insert them into its buffer. I will eventually drop all channels and make it fully lock-free. Absolutely, is there any specifics you are looking for? Thanks, please let me know if you have suggestions.
What does this do? I have read up on it but still don't completely understand it
Who cares about binary size seriously? I remember people complaining about it month ago it doesn't make any sense... I mean 15% binary size vs 9% perf I take that anyday.
It can be important for cases like these: https://hackernoon.com/a-story-of-a-fat-go-binary-20edc6549b97#.5811c0rts
&gt; What changes would improve Go most? I answered to this that the best changes is the lack of breaking changes.
I'm sure it's a pretty difficult decision. People always hate to increase binary size, but it's basically "free" to use a little more space compared to performance speeds. There are only a very few places where an increase of a binary would have a tangible effect whereas a 9% performance increase would be tangible in a lot more places.
Equal representation of opinions is directly counter to the purpose of a survey. 
Hi jacksonnic. Thanks for sharing. I have some suggestions and opinions. First, for the video, it is painfully slow to watch. This led me to learn I can add `?speed=3` to any asciiinema.org video (neat). You don't really show much in the video. The tutorial page shows the same info and does so very quickly. At the end of the video, I did not learn anything about your project structure or why I would want to leverage it for a new microservice. Being on /r/golang, I want to see the Go code. After going through your site, I see you have some service discovery going on. And the test at the end of the video shows you have a `/health` endpoint. Is that it? I can't find any information on how the newly set up project is structured. This is the part that interests me the most. Maybe I missed the point, and you are only interested in the Docker aspect of starting up a service? Tests. I see you are leveraging Ruby and Cucumber/Gherkin. We had a team at work use Ruby for integration tests on a Go project. Later, those tests were ripped out and replaced with integration tests written in Go. Personally, I don't like Gherkin. To compare the test in the video to what I may have written in a Go integration test. The output from your video (asciinema is pretty neat, copy-paste): Feature: Health check In order to ensure quality As a user I want to be able to test functionality of my API Scenario: Health check returns ok # features/health.feature:10 Given I send and accept JSON # vendor/gems/cucumber-api-0.4/lib/cucumber-api/steps.rb:11 When I send a GET request to the api endpoint "/v1/health" # features/steps/http.rb:1 Then the response status should be "200" # vendor/gems/cucumber-api-0.4/lib/cucumber-api/steps.rb:130 And the JSON response should have key "status_message" I vastly prefer: // TestHealthCheck verfies the functionality of the API func TestHealthCheck(t *testing.T){ // start the service (on a new port each time so tests can parallelize easily) // handle things like closing the resp.Body. Maybe instead of returning http.Response, you have a sanitized version of that returned. resp, err := TestHelper.StartServer().Get("/v1/health") if err != nil { t.Fatal("unable to init the service: %v", err) } if got, want := resp.StatusCode, http.StatusOK; got != want{ t.Error("got status %d, want %d with GET %s", got, want, healthURL) } // could leverage the health check struct and unmarshal, allowing assertions against values instead of string checking if !strings.Contains(string(body), "status_message") { t.Errorf("did not find %s in:\n%s", "status_message", string(body)) } } In summary, I suggest showcasing the structure of the Go microservice. I'd consider having an advanced tutorial where you network together multiple microservices into a pipeline and show how all the things can map together. This is one thing that we had to build at work where if we had something to leverage, we may have got that up and going and more uniform between teams. Leverage the language of the service for writing the integration tests. I guess the argument against this is that you want to generate projects for different languages and wanted one true way to test them all. I'd counter that your $LANGUAGE developer would prefer to write their integration tests in that language. Thanks again for sharing! 
There are options for compressing the table so it won't be as big. Also, more inline functions means larger binary.
I don't think they mean *equal* when they say in the blog post, e.g., about the under-represented India, China and Japan.
https://godoc.org/golang.org/x/tools/cmd/guru ?
&gt;When asked which operating systems they develop Go on, 63% of respondents say they use Linux, 44% use MacOS, and 19% use Windows, with multiple choices allowed and 49% of respondents developing on multiple systems. The 51% of responses choosing a single system split into 29% on Linux, 17% on MacOS, 5% on Windows, and 0.2% on other systems. Surprised at linux usage. 
The regexp package is more powerful, but slower. For slices use https://golang.org/pkg/bytes/
I'm only looking for something like this: hasSubstring("my own text", "own") bool {} but it has to be the fastest way ;)
But the counterargument is more than 30% likes go for it's simplicity / ease of use. The question is if it's possible at all to have Generics while keeping the same level of simplicity.
Damn, I missed the reset. Neat.
It seems most Go users desperately want more dependancies and more management, at 13% and 9% respectively.
strings.Contains
&gt; Thats a problem, most things I interested already did in python You could choose one of the programs you've written in Python which you think that could benefit from Go's performance and rewrite it in Go. Since you are familiar with the code base you will require the net less as well.
I don't, but I am happy to answer any questions you might have. I wrote a little description in another comment. sync.Pool doesn't provide any mechanism to control its recycling and in the case of byte slices the cost of interface unwrapping is not necessary. 
Yup, I'm with you. At least on these scales. +/-15%, who cares. Now if a change causes a 10/100/1000x size increase I might be concerned...
It asked what you develop Go on, not where you deploy it. So I'd assume it means what you run on your workstation/laptop. That's how I interpreted it when I was taking the survey.
&gt; What does this do? ["An Improved Go Experience For The Atom Editor"](https://github.com/joefitzgerald/go-plus/blob/master/README.md). `go-plus` is an [atom](https://atom.io) package that is roughly equivalent to: * vim-go for vim * gosublime for sublime * vscode-go for vscode Atom is an open source text editor that is a GitHub project. The "plus" in the name is an odd idiom from the atom community; there are lots of examples of it (e.g. vim-mode-plus, autocomplete-plus) - basically community enhancements on top of some other functionality included in the core. In retrospect, I wish I had called it `atom-go` or similar, but I'm not going to change the name after ~ 3 1/2 years.
I concur
Same. Vim with the go plugin is quite good but vscode is so smooth and with every release they add some new great tricks. It's my favorite project code editor now, I still use Vim for single file stuff though. 
Are you reading it accurately? &gt; What changes would improve Go most? &gt; &gt; - 1,215 (34%) No response &gt; &gt; - 572 (16%) generics Also from question above: &gt; What do you like most about Go? &gt; &gt; - 940 (26%) No response &gt; &gt; - 595 (17%) simplicity &gt; &gt; - 543 (15%) easy &gt; &gt; - 523 (15%) concurrency &gt; &gt; - 495 (14%) simple
Or maybe people use the Go blog because the signal-to-noise ratio is so high. :-) 
I personally answered that I wanted a debugger, not because I wanted an integrated one, but because I wanted a better one than delve.
Ok, I get your point. Though I personally don't have this problem. I think VSCode uses Delve for Go debugging under the hood but it works like charm.
&gt; The question is if it's possible at all to have Generics while keeping the same level of simplicity. It sounds like a reasonable question to me. Where is the assumption?
go-implements, which comes with vim-go, supports looking up interfaces implemented by the type under the cursor. https://github.com/fatih/vim-go/blob/master/README.md Not sure if that could be integrated into other editors/IDEs.
I agree for now: but what about when it becomes a compound interest problem? That binary could grow exponentially (+15%, +15%, +15%...). At some point +15% won't be worth it, and the Go team must surely be thinking about the future when they make these decisions. Also I wonder if they added any compile time overhead as well. 
Does anyone know if Go employs "tree shaking" and the-like to reduce binary size, or is that for future releases? Because saying "dependency x is 2mb" is ambiguous: is the 2mb the whole dependency or only the code paths you are using.
vscode with the Go plugin is seriously amazing. I was on vim, tried Gogland which felt somewhat inflexible, went back to vim, tried vscode and stayed. Lint on save, code completion, auto formatting, etc. It's surprisingly nice.
You need quotes around your field names inside the json tag, e.g. \`json:"current_observation"\`
I started deploying to mips32 with 16mb storage. Binary size matters to me. A simple echo http server is 6mb.
^ literally the best.
Wow how did I not know about this, this is amazing. Thanks!
Yeah but there are post compile things you can do to deflate binary size pretty decently.
&gt; My contention is (and has been) that generics in Go is more of a moral issue with the core developers than a practical one. If you go and read the previous generics proposals and discussions you might change your opinion. Regardless of that, this is just your own impression. &gt; The assumption is that generics = complexity. You cannot dismiss the data from this survey. Look at how many people replied that they are confident about their Go proficiency. This is very important in my opinion. When the language is simple then more and more people find it easier to work with, collaboration becomes easier etc. In addition, complexity does not necessarily mean programmer's complexity which it seems you are thinking about here. It also means complexity in the tooling. Go was designed from the start to be easy to write tools for. If there's complexity added there then we will get less good tooling and the existing ones will become harder to maintain. This will affect the whole ecosystem. Think about all those platforms and easy cross compilation support that Go has. Also I'd like to add that once a feature gets in the language then it basically can never get out. Thus I totally understand the Go leadership being so skeptic about generics. If they mindlessly added every feature other languages have then we wouldn't be writing Go anymore. Nevertheless, I am *not* going to claim that generics = complexity. What I am going to claim is that generics = less readability. What do you think about that?
&gt;When asked about the biggest challenges to their own personal use of Go, users mentioned many of the technical changes suggested in the previous question. The most common themes in the non-technical challenges were convincing others to use Go and communicating the value of Go to others, including management. Another common theme was learning Go or helping others learn, including finding documentation like getting-started walkthroughs, tutorials, examples, and best practices People find it difficult to get code walk through. As someone who faced this issue not quite long ago, I can relate to this very much and as someone who wrote a guide after getting tremendous response from /r/golang, I wish to make things a bit less painful for the newcomers, the question is, why is it difficult for newcomers to find walkthroughs? I was under the impression that https://awesome-go.com is the best guide there is for Go, my guide gets steady number of visitors from awesome-go. What can we, as a community improve?
Recently I found myself without my good PC for around a week and I was forced to work in an older PC. Though the older PC is fact old, it (should) be good enough to work with code. Since I had windows installed on the old PC I thought I'd go for vscode for a change. I was impressed at the start but as I started working it became slow, sluggish and unresponsive. I installed gvim and although configuring it was more difficult than installing a plugin on vscode, I was finally able to work as it was blazingly fast and performant. I am not arguing vim vs vscode here, I am just sharing my experience and how vim was a better fit for my particular case, at least for that one week.
Thank you for go-plus. Been using it since day one of my golang adventures and I know they would not have been as successful if go-plus did not exist. Thank you again.
Did someone show this to the [FizzBuzzEnterpriseEdition](https://github.com/EnterpriseQualityCoding/FizzBuzzEnterpriseEdition) folks?
Your argument is unreasonable as there is a quickly approaching cap on performance. 
34% gave no response for "what changes would improve Go most," but only 26% gave no response for "what do you like most about Go."
Exactly. So the tradeoffs will get worse over time as there is marginally less performance to squeeze out ie: +20% size for +1% performance. At what point is the binary too big? 100mb? Perhaps all we need is the ability to toggle on and off features depending on business requirements, thus moving a "slider" between runtime performance and compile time overhead (time and size).
Maybe have a look at github.com/buger/jsonparser if you are only interested in some parts of the json.
&gt; On a side note, I think I'd be much less perturbed if the language designers would just come out and said they don't want generics because icky than pretend there are all these technical reasons or otherwise they would add them. I take issue with the fact that you are implying they are liars. You're also implying that the language designers aren't trying to make the best language and ecosystem they can. Of course they are. If they truly believed generics was trivial to add and/or the value was worth it, right here right now, it would have been implemented. Your logic fails in your assumption about the core team's integrity. Unless of course, you can actually prove the core team lacks integrity, and that they have motive to lie about why they haven't implemented generics.
I trust the team to continue making sane, in fact wise, decisions.
Great question. I haven't heard of anything like that, and I keep an eye out for such things...
Because most scripting languages beyond shells are larger than go binaries.
[removed]
Yes, there is a long-standing bug for that. Too big and growing... https://github.com/golang/go/issues/6853
If some people are concerned about the binary size, I wonder if it would be that difficult to offer a compile option which can disable the optimisation? I get the sense that most people would be happy to get the improvements and it would be more a special case to regain lower binary size. 
Of course we do not expect the blog to be like a r/golang subreddit. But would not it be nice if someone in the team share monthly letter regularly?. We are members of a large family who loves each other. Therefore some of us expect this kind of sharing like a letter from our family.
This().Is().Not().Nice().Go() Beyond the questionable aesthetics you've lost the ability to error on all the intermediate steps. The amount of empty interface{} and reflection use is a dead giveaway you've been too clever. Function chained DSLs are one of the worst bits of baggage developers attempt to shoehorn into Go given it doesn't have exceptions for out of band errors.
Hey. /u/alioygur, please stop writing useless blog articles.
He didn't even get the name of the `testing` package right.
Typical bad press. He just selectively rewrote parts of the original article and added a clickbait title. Well, I am not surprised. It's infoworld.com we are talking about.
I would assume that to be the case with any tech changes or new-ish programming languages. I know plenty of companies that refuse to move to Java 8 from 7, for example.
...yeah, so, the author cites the stats from the survey itself, but provides _absolutely zero_ figures, or even discussion, to back up the assertion that this isn't matched by enterprise/businesses. cool journalism, bro!
Less "refuse" and more "*can't* because we can't afford rewriting hundreds of shitty libs that won't run on newer versions".
A game engine in GO? Please tell us more.
Thanks for sharing, and for commenting. My question as a gopher and prolific open sourcer: &gt; are all known issues ticketed and in milestones? Cheers! And Good work so far! Upstream issues with `vendor/` having broken the 99% Go [use-case] have really harmed my faith in the language long-term, and my team, through the mishandling of `vendor/` and the **crippling** performance regressions in the Go tooling since 1.4 are making us urgently look for an alternative, but we find the landscape wanting. [use-case]: https://github.com/golang/go/issues/19090
Fair response, thanks! I do appreciate how detailed each issue is. It looks like there are some really great ideas stubbed out, but I worry that the timeframe is too aggressive to deliver an end-all dependency management tool that can handle every use case. Enterprise in particular is going to be tough, if only because adoption of an unstable tool is going to be near impossible, leading to a sort of chicken-egg problem. I'm going to talk to some coworkers and see if they would be willing to jump in and contribute. If nothing else, it will help raise awareness-- I'd be willing to bet some don't even realize `dep` exists. And on that point, have you plans on talking at gophercon? Even a blog post on the official blog could be a major boon to awareness for the project. Regardless, I appreciate the effort you are all putting in. I'm going to spend some time using and getting familiar with the tool so I can at least be more informed, and who knows, maybe useful. 
https://blog.gopheracademy.com/advent-2014/string-matching/
&gt; that we not try to dissect the project years later. hmm, is not the best case for learning "learn form the errors of others"? ;) I believe this is exactly the case to dissect ;)
&gt; I just want to fetch this or that version of a lib. Thinking only about your immediate deps works fine... until there's a shared dependency. Maybe that ends up being easy to resolve; maybe it doesn't. We're lending structure to that process (among others). &gt; Didn't try dep either, won't until it is official canonized by the go team Sure, I bet there are plenty of people who feel similarly, and that's fine. We just have to hope that there isn't some bit of insight or experience you have that we end up missing when we need it.
Then go study it yourself. But we don't need to publicly dissect it any more than has already been done. I don't see any reason to believe there's anything special enough about this project to justify further public analysis; it failed for the usual boring reasons things fail, from what I can see. There's no reason to impolitely drag up old topics for that.
&gt; are all known issues ticketed Not all, but we're getting pretty close now, with the roadmap up. dep's a little better than gps on this front. &gt; and in milestones? I've been trying to keep a handle on the chaos by using zenhub, actually - so we're not using milestones as much. But idk how much value it's adding for me; the pipelines aren't great for this purpose, but the epic handling is kinda handy: http://imgur.com/a/vZe61 But that's not really publicly visible, so we probably need to duplicate the epic info into milestones so that everyone can see it. Note the call for help with project management in the roadmap :)
&gt; most popular kickstarter failures was Mighty no 9 which despite reaching 4 whooping million dollars, till this day it still hasn't delivered the ports to the rest of the platforms it has promised, not to mention that the game was bad despite being developed by proven industry professionals. Interesting. But unlike this one the developers of haunts had the honesty of handing the game to the community as open source, so that at least theoretical someone could take the game up and achieve what was promised. Therefore I'm surprised no one did this up to now. But when the game was developed against an immature &lt; 1.0 go version it is understandable.
tbh, i'm not terribly bothered by it. not because the problems there aren't real, but because once we have the dep mgmt tooling in place, i think there's a longer-term path out of needing to use `vendor/` at all. even without that, though, i think this whole effort brings stronger semantics to go project layouts, so doing things like excluding `vendor/` from `go list ./...` might seem less crazy. 
Thanks for taking my concerns seriously and writing a well-considered response. Upon re-reading my comment was a bit whiney. Good point about introducing more strict semantics, I suppose if we nail the dep management problem as a community, *where* they're hosted within the file tree becomes next to immaterial. 
no worries - these are all real concerns! :) &gt; where they're hosted within the file tree becomes next to immaterial. yep! as long as tooling has a clear, reliable way of locating the right code (to be clear - not a trivial task), then we'll at least have another option to complement magical filesystem arrangement games.
The reason I love go is because its simple. The more features added to a language, the more time I spend thinking about which feature I should be using in my current situation. I have found that I am so much more "productive" in go than other languages, purely because I don't procrastinate how to achieve my goal. LINQ is the same. I don't want two ways to do the same thing, because then I end up googling x vs y for two hours, it's a character flaw I know. 
Wow, thank you so much for the answer, it works like a charm &lt;3. You really taught me something new and useful for what I'm looking to do ;) 
Probably a bit of both. The quote seemed directly related to the thing he replied to - generics vs. simplicity.
It's attribution to previous work of authors and an inside joke on roots of Go.
Hmm, todi list is nit bad idea. I guess Ill do smth of this. Thx
It does. And I agree it's mostly good enough, though it definitely could be better.
seems a bit presumptuous given that it is not even released yet please let this be out in the wild for a year or more before even thinking of making this part of the core...Go will earn a bad reputation if we have too many "point releases" that serve only to address issues in `dep` i'm rooting for `dep` and it is badly needed...but i'm not ready to accept it as a "done deal" until it has been beaten to a pulp in real-world use 
https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L130 why return an error type when it is always nil The logic is gnarly around https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L210 consider putting it into a separate function https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L105 I wouldn't use 'new' as a variable name, how about 'wd' https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L69 I wouldn't have the error messages start with capital letters, the user will usually be appending them to their output and it would look odd. 
:) if you've got thoughts on monorepos, please do jump in! either right on the epic (https://github.com/golang/dep/issues/286), or you can post a new issue. there hasn't been much serious public discussion on how to bridge this gap yet (at least, that I've seen), and even though that's slated for the "next phase", we really ought to be at least discussing knotty problems like that ASAP.
it's entirely possible we're being too aggressive with the schedule. we'll have to see. folks are already getting it pretty pulpy :) we do also have a catch-22 here, though - building momentum behind it, having it be impending for merge, etc., encourages people to use it, and use it in anger. if we don't push, there's less incentive to try, the evaluation process takes longer, and the community has to suffer longer. so, best i can say is - my deep and abiding worldview is that software is all terrible, my own very much included. and i don't want that outcome any more than you do. best we can do is keep pushing forward, and making careful, cautious calls at each decision point.
Not much to talk about really. Right now it's great if you want to build a game consisting of rotating cubes. It's a vehicle for me to learn techniques like forward rendering and parallelization of game engine logic. It's not public but maybe it will be one day when I'm happy to expose it. 
Just want to note I like that they published the source too. Software kickstarter projects should be required to fully open source if they fail to deliver in my opinion. It's the only way to prove they made a best effort for delivery and didn't fraud &amp; fluff the backers. 
I wasn't saying it will complicate the GC, I was saying that having concurrent free semantics in a buffer allocator nudges it into a competing space. The thing is a buffer pool doesn't have the complexity to justify it and feel heuristics at free &amp; alloc time can cover any case adequately in language like Go. Point is this feels like a naive malloc arena, which doesn't make sense to me in a language with a runtime and scheduler. Your not able to tell the runtime how important your goroutines are and are left at the mercy of the schedulers ability to provide your G's a P to run for buffers. Doing the "work" of allocation management from the active G lowers contention, lowers scheduler pressure and makes things easier to reason about. Just my two cents, figured since you politely got back to me I would elaborate. I still thing ya got a good thing, just think it needs the head chopped off! Being able to stack allocate a buffer pool would be ideal for your concept of a context imo, leaving the developer to choose which areas of his software should have their own pool rather than a central manager. var bp BufferPool 
You should make use of the request recorder https://godoc.org/net/http/httptest#ResponseRecorder 
How would this compare against something like NSQ?
I don't think that returning error from functional option is a good idea because it would imply that this function has state. Also the pattern is interesting and very elegant in some situations, but don't take it to the maximum. IMHO it should only be used before it becomes overly verbose. 
It surprised me as well. I find Go to be quite lovely to develop on Windows, even as a recovering Linux desktop user. Then again, I use it to target servers just about always running Linux. It does make sense that if you are in an area where you always target Linux to at some point also do development on Linux.
[Working link to the blog post](https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html) On mobile, the Reddit app doesn't let me change the URL and I couldn't remove the last `.` from OP's link I like the pattern, but I'm curious why he decided against just using a copy and returning the new version (with the new value). I understand for memory reasons/places you'd be instantiating often, but I'd mostly use this for setup type work (server, handler, etc). Are there downsides to this that I'm overlooking?
you should just test your requests (for client) and responses (for server), then proceed to integration tests. for example, I'd do func TestUploadRequest(t *testing.T) { req, err := client.UploadRequest(params) if err != nil { t.Error(err) } assert(req) }
Hi! Thanks for taking the time to give it a look. &gt;https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L130 why return an error type when it is always nil As far as I can tell, and there isn't much to go on: unix.InotifyInit() Can return an error, so I'm going by that. There isn't much documentation in the unix package, so unless there's something you know that I don't, it needs to return an error. Otherwise you'd be right, just return the struct. &gt;The logic is gnarly around https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L210 consider putting it into a separate function Yep, definitely. I copied most of this from [here](https://github.com/fsnotify/fsnotify/blob/master/inotify.go#L264) since I didn't feel like taking the time to bash my head against it. I'll be going back through and cleaning that up, as well as some other things. &gt;https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L105 I wouldn't use 'new' as a variable name, how about 'wd' Fair enough. &gt; https://github.com/tywkeene/go-fsevents/blob/master/fsevents.go#L69 I wouldn't have the error messages start with capital letters, the user will usually be appending them to their output and it would look odd. Didn't even think about that, another fair point. Again thanks for the input, hopefully I can get this to a stable point and be able to offer it to the community. Cheers. 
&gt; Dynamic, highly parameterized software is harder to understand than more static software. How about interfaces then ? Actually, interfaces (especially empty ones) and polymorphism make code much more dynamic than code with generics. That code package list type Node struct { item interface{} next *Node } is much more dynamic (and harder to reason about) than this one package list [T] type Node struct { item T next *Node } since the type of items in a node are known at compile-time in the second case (and you can't know for sure all items in a list are of the same type or not). Or, to say it the other way round, what do you think of those builtin generic slices/maps/channels ? Wouldn't it be better if all those types were containers of interface{} ? Wouldn't it be easier to read/write/parse var content slice var indexes map rather than the more complicated var content []string var indexes map[string]int ?
I'm not sure what the benefits of that are? It really depends how you create the copy. It becomes a little bit trickier when the struct has private fields. This is easily done if you create the self-referential functions inside of the package. However, self-referential functions outside of the package would need to either use a helper function, or the constructor would need to generate the copy before sending it off to the self-referential function. But - both options are pretty quick to implement all things considered. Also, you could potentially get into odd territory if the struct has pointer fields. Do you do a shallow copy, deep copy, etc.? 
https://github.com/lib/pq
Finally, a _permissioned_ distributed database for Go! This will really revolutionize the [diamond industry.](https://www.youtube.com/watch?v=lD9KAnkZUjU)
This is usually handled using custom type based on int and const variables. It doesn't solve the problem of arbitrary input because untyped numbers will be implicitly converted to the correct type, but it will allow you to express your intentions more precisely. As for function parameters validation, using Rob's pattern one can create functional option which would return another function that would set object state to invalid and those propagate error. 
IMO it seems kinda gross to initialize an object and then have it return an error on first use, which is why I'm not entirely opposed to a functional option returning an error. Not only does it delay the issue a bit (invalid instantiation) but it also requires the object to store an error and also requires the object's methods to have error returns to return a propagated error.
You could always return pair of option, error from the function that creates the option closure. That way you will also enforce the handling of invalid inputs, but for a price of fluent object creation. I don't think that verifying arguments inside functional option is wise. Mainly because it became non obvious how to handle "partially applied" options if error had happened. You could resort to tricks with defer and essentially rollback entire option apply, but what kind of previous option you should return then? There are also problems when several options has been passed to the object builder and one of them had returned an error - in that case you left with partially initialized object or no object at all. Enforce your contracts at the borders, not inside of your app/lib domain. 
I am working on Google App Engine SDK:s, we (ab)use the build package in order to resolve dependencies from GOPATH, which is not ideal going forward. I am really excited about this, mainly just because it would be great if we can get a single official tool. So, will follow this project.
Correct me if I'm wrong, but isn't this more commonly known as the "functional options pattern"? https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis http://halls-of-valhalla.org/beta/articles/functional-options-pattern-in-go,54/ 
Similar post with some more text https://coreos.com/blog/grpc-protobufs-swagger.html
&gt; Your post acknowledges that leaving out generics was a moral issue not just a technical one. This isn't true. It isn't borne by any evidence. Stop repeating it. &gt; Do the current group of primary Go influencers even agree that generics are desirable in Go!? Does the community itself agree generics are desirable in go? Stop the technical misdirection until we get a clear indication of consensus. Yes! They are desirable, in the abstract. But making generic _properly orthogonal_ with the rest of the features of the language is not obvious; no complete proposal has met the bar. And bringing generics into the language will necessarily require a rewrite of much/most of the standard library, which is an incredible amount of engineering effort that needs to be accommodated.
Pick the one that provides you with the durability guarantees and capabilities you need first. You will generally find many other problems before coming close to the performance issues of a modern pg or even (shudder) mysql deployment. 
Hi, I've actually done a similar thing a while back. You can take a look at the code [here](https://github.com/dplesca/go-omxremote). 
Cool! I'm decently sure we'll cover all of your needs, but if we don't (at least initially), then I think gps might provide you a better set of primitives with which to analyze dependency information. Either way, it'd be helpful if you could say more about the problem you're abusing `go/build` to solve to make sure we've considered your use pattern :)
That's surprising - test-only dependencies should be present, but _only_ for the root project (we don't analyze the imports of your dependencies' tests, because...they're not your tests, why are you running them? :P). If dependences required by your own project's tests are missing, that's a bug; please file an issue! With respect to `vendor/` pruning, we're trying to be _very_ safe, which makes this tricky. See https://github.com/golang/dep/issues/120 and https://github.com/golang/dep/issues/121
I agree but you should really elaborate, not for me but at least for OP. Otherwise it sounds condescending and isn't really helping anyone.
I'm looking it to server datas for an graphql api, nothing more. Basically select, insert and kinda that's all..
I would also want to know if my new state was applied or not. Just silently ignoring it seems problematic at best.
Nice, the embedded assets is something really cool. Thanks for sharing. I myself was wondering if there was a way to get around having to distribute html pages etc or if I could build them all into the binary. Good to know.
So is this going into the stdlib?
Email announcement: https://groups.google.com/forum/#!topic/golang-dev/7cThjbB5MRU
I like how you handle the process and I might steal that part from you, if that's OK. 
Clever code is never clear, but this is clever :)
Does this count as an idiomatic example of a Go web app? I see a vendor folder with a vendor.json file. I wonder which dependency tool was used for that. 
The general pattern I follow and I'm sure would apply here is like bufio.Scanner, have options write to private err field and add a Err() method. If the struct that your mutating is not the place that it's used propagate it where it's accepted. This type API is fair enough in places like config setup I think since its more or less a single operation.
A graphql object can be stored on any Redis based in mem DB, with various persistence policies. Pick one, and you should be good to go. Redis is fast and reliable enough that we use it on productions to server real time queries in a highly distributed system and not once have redis failed in last 4 years. So I am biased to some extent, HTH
I don't mind it for buffo.Scanner since you have the Scan method which returns false if err != nil, but it seems like a poor API design to have a configuration or instantiation error pop up when you try to use the object, instead of when you configure or instantiate the object.
The goal here is "right", not "easy." That said, dep is based on https://github.com/sdboyer/gps, which was in the process of becoming the new engine behind glide. It's also the product of collaboration between, and the shared experience of, the most popular community tools. We weren't starting from scratch.
Good catch, I thought it was licensed under the MIT license. 
"Validating dates is not a job of the database" though i am told mariadb is much much better than the old mysql versions I had used in the past.
Hey I appreciate your suggestion. I felt like your advice was relatively lightweight in comparison to creating a mock service. Also I found this pretty [good article](https://www.philosophicalhacker.com/post/integration-tests-in-go/)regarding integration tests. Figured I'd share. Thanks again!
&gt;my biggest struggle was finding out "What implements this interface?" AND "What interface(s) does this struct implement?" Exactly my only struggle with the language right now. Around the language, however, it's the shackles to `GOPATH` and lack of a dep tool (it's coming though!) that bothers me when starting or forking a project.
Yes, and yes.
Yeah, definitely go for it. I picked the GPL because that's the first thing that came to mind, but I'll switch it to a more permissive license when I get the chance later today so you can use it without issue. EDIT: Had enough time to do it before leaving work so you're good to go now.
[Of course it is!](https://devnull-as-a-service.com/)
I beg to differ about moving parts,- much could go wrong when implementing tls app&lt;&gt;db, choosing cipher set on postgres side or choosing where to store the secret (near the data? in consumer app?). even more could happen if the app is encrypting the data itself. It's not "more sound", it's different a use-case and different security scheme. if you really trust your environment in a way that: * neither front-end nor database will never, under any circumstances, leak secrets for data at rest. * **mutually** authenticated connection to db (both parties are able to verify each other and will actually stop handshaking if certificates don't match) really happens properly, which means maintaining things like CRL yourself. * able to control attack surface on secrets protecting the data-at-rest. * able to filter user input on front-side to ensure that no sql injections could leak any data. ... Then you could use traditional tooling. Moreover, given the availability of long verified tools, you should. It's that many modern applications and infrastructures do not have trust levels like that to all or some of it's components, require more than just data-at-rest encryption, would like to compartment secrets in a least risky way. Other modern infrastructures fail to implement traditional security controls you mention properly due to exactly too many moving parts (e.g. parts which can be broken by the user who deploys controls) and poor security knowledge. And, seeing the number of awful leaks from poorly configured traditional tooling, there is a lot of both reasons. Acra is just making a small step into that direction.
No, assembly is entirely platform and architecture specific.
"Hey execs! lets rewrite our entire legacy app in a new Programming language and platform from the ground up. Sure it will cost millions of dollars, the pool of available talent at all levels of expertise in this language is a drop in the bucket compared to other languages and platforms, and our software currently does it's job more than well enough and is actively maintained and supported. But this new Programming language is NEW! And its a lot faster! Sure, for many applications it won't add much noticable speed to users if any at all, but IT'S SO COOL WITH MUH ROUTINES!" Playing with Go in my spare time a lot lately and using it on a personal project at work where I'm the only one coding the application, I absolutely love Go and think it will get a lot of traction in the coming years. But from an enterprise perspective, replatforming and rearchitecting legacy code when the existing platform already does it's job very well is a very hard sell. Especially considering the time and opportunity cost any replatforming effort takes. BUT this would happen with any relatively new technology. The more people in the workforce start learning Go, and the more use cases for replatforming to use Go happen, adoption of Go will grow exponentially. Until that time, switching business-critical software to use a language and toolset that is relatively new and has a vastly smaller amount of people in the world who know it (let alone are expert level users) compared to existing systems just doesn't make business sense. And it's no fault of Go and doesnt speak to anything about the usefulness or viability of Go if you already have the talent and budget/necessity to start a project from scratch. 
top-like utility for monitoring container metrics. Uses [termui](https://github.com/gizak/termui) primarily; though I did end up writing/re-creating a few of the library widgets to fit my use case. As the project matures, I've also considered making the various text/graph widgets available as part of a generic "top" library. Any feedback or suggestions would be most welcome!
I'm all for offering security to developers who may not have the experience, but the complexity here is raising some red flags for me. For example from [Architecture-and-data-flow](https://github.com/cossacklabs/acra/wiki/Architecture-and-data-flow).. &gt; Acra design values simplicity as much as security. Reading data should not require a lot of modifications, just point the database address to AcraProxy and you're good to go: * Application sends database request to AcraProxy, which pretends to be database listener, using standard PostgreSQL protocol. * AcraProxy sends that request to AcraServer using Secure Session (socket protection protocol). * AcraServer sends a request to the database using regular PostgreSQL protocol, receives an answer. If AcraServer, while parsing the answer, detects the presence of AcraStruct, AcraServer attempts to decrypt it and replace * AcraStruct with plaintext result in the answer, adjusting answer's size. If decryption fails, AcraServer just forwards an answer as it is. * AcraServer returns data to AcraProxy, which in turn returns them to an application as if they'd come from the database itself. Also want to note: &gt; AcraWriter - client-side library, which integrates into app flow either through ORM or directly, and provides means to encrypt sensitive data via generating AcraStructs (Golang, Python, Ruby, PHP, Node.js) Feel free to correct me if I am wrong, my cursory understanding is this library provides two security features. First is a communications encryption over the transport to your database, which I will trust is simply a known and well reviewed implementation of TLS. Second is a way to encrypt data that is being stored, it's not clear to me how this is done yet but it is clear it requires further integration as it requires ArcaWriter client library to make a custom struct. So.. to get something supported naively by every major database provider (TLS).. you "simply" add ArcyProxy, ArcaServer in your stack. The increased surface area and operational burden imposed by the extra complexity (one of securities biggest enemies) makes this a impossible justification over a developer simply using the TLS/SSL support within his native language bindings. This means the only possible value this could provide is the ArcaServer's automatic encryption. But it isn't automatic, it requires something called ArcaWriter (Written in (Golang, Python, Ruby, PHP, Node.js)) which is yet another integration that happens in your applications software. But this integration is a client library, that simply generates a encryption payload to be understood by the ArcaServer. From what I see [ArcaWriter](https://github.com/cossacklabs/acra/blob/master/acrawriter/acrawriter.go) is just a single function that.. after creating around 8+ allocations just from what I see in that scope... will encrypt some bytes. No idea if it's secure, it's pulling in way too many other packages to audit easily. Unless I am missing something here, it seems you could achieve the same exact thing with just a client library and avoid all the surface area here. A lot of the code I did click raises some red flags, for example I clicked the keystore (which I was surprised a [keystore](https://github.com/cossacklabs/acra/tree/master/keystore) was invented rather than using a vetted solution like boltdb. The first thing I searched for a FS write for some basic directory traversal attacks and make sure input was sanitized properly. For a product that is advertising security you can not trust callers to do this for you, that is how accidents happen. The very first thing I notice that does writes was "generateKeyPair". I see: dirpath := filepath.Dir(store.getFilePath(filename)) ... err = ioutil.WriteFile(store.getFilePath(filename), keypair.Private.Value, 0600) err = ioutil.WriteFile(store.getFilePath(fmt.Sprintf("%s.pub", filename)), keypair.Public.Value, 0644) Ugh... fmt.Sprintf? Well, this takes me to: func (store *FilesystemKeyStore) getFilePath(filename string) string { return fmt.Sprintf("%s%s%s", store.directory, string(os.PathSeparator), filename) } func (*FilesystemKeyStore) getProxyKeyFilename(id []byte) string { return string(id) } All of these functions simply pass in whatever "ID" is to a function with no validation, some of which don't even add any sort of suffix. At worst these are directory traversal attacks that if well crafted could lead to privileged escalation through since it seems to deal with PKI, at best the burden of validation is shifted to the callers leaving this a pending "at worst". This may have came across a bit harsh, but it needs to be. Security is absolutely critical and I see no mention of "beta" or "early development" .. or anything insinuating this has not undergone rigorous peer review. I think it should, until it does. But that's just my opinion which doesn't mean much. That all said I wish your project the absolute best, spreading awareness of the importance of security alone is worth cheering on. Good luck!
&gt; Before Bit, we often found ourselves re-writing or duplicating code across repositories over and over again. This wasted time and effort while making our code base harder to maintain. I understand that this may be a problem with JavaScript. In Go, however, there is already a strong focus on writing reusable libraries, so there seems no need for an extra tool here. 
* structure-based * subcommand * simple and lightweight
nothing can prevent *passive* leaks, this is correct. if the attacker compromises the point where the data is used when decrypted, and starts silently accumulating the plaintext, we can't do anything. but there are a few classic ways to prevent *active* leaks: if the attacker compromises the front-end application (further - app) in one way or another (compromising the code or just injecting SQL) to perform an active leak, if we pass all request traffic through one point and analyze it there (the few analysis and reaction patterns which are there now is just the beginning of it), we can alarm whoever is responsible for the system. &gt; if a proxy or middleware that is mutating data goes kerplooey, your entire DB could be trashed this is correct with any kind of encryption, but, unfortunately, there's not a lot you can do: - app-side encryption would increase attack surface significantly - splitting the data-at-rest encryption between application and database again increases the surface and trust complexity but there are things we did to give people better control of the process: proxy is *necessary* only to decrypt the data. AcraWriter can be used to generate records, which can then be inserted into the database in virtually any way, it doesn't have to go through AcraProxy. it's as complicated as any other app-side encryption process, with completely the same risks to it's predictability of result. the only difference is that in Acra's case, app side does not contain sufficient secrets, that can be used on DB dump to decrypt it.
Thank you for taking the time to write such elaborate comment, it's extremely useful and is the reason for releasing early. &gt;Security is absolutely critical and I see no mention of "beta" or "early development" .. or anything insinuating this has not undergone rigorous peer review. The sole goal of going out with so early-stage code was exactly to accumulate useful feedback and create grounds for peer review. Many of your questions are equally the problem of documenting acra properly. As it happens, first versions of the docs are being written by engineers who wrote code, not dedicated, articulate technical writers, and even two people responsible for crypto part are yet far from happy with it. Thank you for pointing unsanitized inputs, this was passed to the engineer responsible for this chunk and will be fixed (even though both components calling it are within scope of our code, yes, even I can invent a few attacks). &gt;Unless I am missing something here, it seems you could achieve the same exact thing with just a client library and avoid all the surface area here. No, because the main idea is to get decryption and decryption keys out of the app itself, into a separate compartmented VM. &gt;A lot of the code I did click raises some red flags, for example I clicked the keystore (which I was surprised a keystore was invented rather than using a vetted solution like boltdb. Most of such infrastructural components are drafted out in a way to be easily replaced in future: some might use persistent KV like boltdb, some - dedicated PKI plug, some - would opt for HSM together with decryption itself (people do use them still). Components like keystore been drafted out to cover high-level API and processes, then, after we're content with security guarantees implementation, will be replaced by several options comfortable for various industries. &gt;This may have came across a bit harsh, but it needs to be. It isn't and is exactly the kind of input we've went out for, thank you.
Stdlib flag already supports subcommands and it is even simpler and more lightweight. So I suppose the only thing the Flag package offers is using reflection to get the flags into a struct which is indeed quite handy.
Sure, we use it for deciding what files to bundle with the deployment in order to support multiple versions of Go. Specifically, this means that if a release tag is used for a future version, we need to include that due to forwards compatibility, because the server decides the Go version. However, once we use vendoring, the files will all be automatically bundled, no matter the version. As a start, we should support the vendor directory from the app root I think.
Awesome post! Really useful information and explained very clear. Thank you very much.
[removed]
Awesome! I assume this is one way of achieving parallel processing for Python. By implementing the concurrency in go. 
I have electronic boards with embedded linux. There is no sftp or rsync daemon. However there is option to use scp for file transmitting, but it's need to be compiled (which must be not too complicated).
You'd be suprised how well developped the standard library is. That being said, https://godoc.org/golang.org/x/net/websocket or https://godoc.org/github.com/gorilla/websocket 
I'd say net/http, but if you're not looking to support http2, you can use fasthttp library. There's a lot of web frameworks that implements the basic http package with some cool features, but I'm not sure if you're gonna need all of those stuffs. If you're gonna make it to be distributed, you'll see that after using a big framework (only id), you'll realize that it comes with a lot of useless stuffs that makes your binary larger. I'd go for net/http + http router, that's all I would use.
Okay, that's fair, and scp is pretty limited in capabilities too. If you want range skipping capabilities, have a look at `dd` and its `seek`/`skip` parameters. 
Not sure if this will help you but I had issues with it on either Go 1.6 or Go 1.7 as well. The process would use 100% CPU and I think memory started to leak like crazy as well.
https://blog.golang.org/concurrency-is-not-parallelism
thanks radovskyb, i see your point for the interface seperation. 
&gt; -http &lt;file&gt; Did that ever work? It has always been `-html`.
Actually, I'm in reverse situation. I'm curious about does "cat" utility able to implicitly change the file content, when streaming it to stdout or when streaming from stdin to file. Thanks for help!
cat by itself doesn't change anything, actually. It only opens files read only. "cat &gt; foo" copies from stdin to stdout and then your *shell* writes that to foo.
+1 to pet projects, keep at it! :)
ZeroMQ is a great library for distributed network programming. Use this - https://github.com/pebbe/zmq4. The latest v4 comes in ECC built in to the protocol.
https://github.com/jmoiron/sqlx
You might find https://github.com/knq/xo useful. It can be tweaked to use pgx pretty simply.
This command used to be my bread and butter before vim-go made it easier: go test -covermode=count -coverprofile=count.out &amp;&amp; go tool cover -html=count.out But I still use it sometimes because [heat maps](https://blog.golang.org/cover#TOC_6.) are cool! Oh wait.
Perhaps because it sounds like marketing ad for clarifai.
Unfortunately no, but it looks like it might make it into version 1.9. https://github.com/golang/go/issues/11058
`:` is not valid inside a cookie name. It is a separator. You can read about it in the RFC: https://tools.ietf.org/html/rfc2616#section-2.2
redigo is good. If you want to use a cluster: https://github.com/go-redis/redis
If you're new to Go, I suggest just using the std lib's net/rpc There's no real reason to use anything more complicated.
Looking into the godoc of this package, it doesn't seem to provide conversion only parsing..
&gt; https://github.com/buger/jsonparser https://github.com/buger/jsonparser/issues Seems like many payloads get read incorrectly, so if correctness is more important than speed it may not be worth it.
[No, not yet](https://github.com/golang/go/issues/11058)
Definitely on the roadmap for the next release. Thanks for the feedback!
It seems a little rude to just post this link as a correction of (I guess what you're thinking is) misuse of terminology.
It's truely amazing! Works like a charm and it had a nice api as well, gonna pin it in the post, ty for sharing it with me!
This is wrong last I checked. Decoder is for streams of JSON objects, but each object is fully loaded into memory (I think it actually calls Unmarshal() under the covers). This is a common misconception in my experience.
Developed by Verizon. Go is indeed very popular in telecom/cable companies.
Out of curiosity, what made you create a whole new open source package instead of being a go contributor and improving the standard library? I'm just always curious when I see packages that popup that have the same basic purpose as something in the core library. 
This is true and I should have said `implements interfaces`, but we are all human and make mistakes :P I edited it anyway because you are right even if we are being slightly pedantic about the point here lol!
This isn't the case at all. The library name is fine and qualified by URL in the import path github.com/cosiner/flag as cosiners flag library and not some other flag library. If for some strange reason you want two flag libraries imported just alias one. Go's URL import paths are great not only because you can see where the library comes from but you can reuse sensible library names. Examples of bad flag library naming: github.com/ProfOak/flag2, github.com/spf13/cobra, github.com/alecthomas/kingpin which you would never know are flag libraries if you see them in the import list of someones code but haven't read the README yet. Well flag2 you would know but the '2' is redundant unless ProfOak also has another flag library. Edit: For a fun bit of history regarding library name land rushes and why they suck: Ruby's money gem for years wasn't the best money gem but it was the gem everyone found and used because it was called money. Ironically probably losing people money in the process :)
Not surprised. CenturyLink Labs has done stuff for quite a while (ref: https://github.com/CenturyLinkLabs?language=go, http://panamax.io), as well as NTT (ref: https://osrg.github.io/gobgp/, https://speakerdeck.com/ishidawataru/gobgp-yet-another-bgp-implementation-in-go). Edit: Couldn't forget this one: https://github.com/tylertreat/comcast :D
https://divan.github.io/posts/avoid_gotchas/ &gt; Assuming youâ€™re on the 32-bit machine (which is probably false nowadays), you can see that int64 takes twice as much memory as int32. Like, on a 64-bit machine int64 doesn't take twice as much memory as int32. /u/divan0 &gt; Iâ€™m genuinely interested in a feedback &gt; https://github.com/divan/divan.github.io - Issues are turned off
Well, it definitely works for me. Here's how go tool cover chooses between browsers: https://github.com/golang/go/blob/678f35b676de075375066ade2935296dfb8050ec/src/cmd/internal/browser/browser.go#L17-L38
I'm so confused. Why not just link to Todd's actual course on udemy? This is just a copy/paste of his course description. Don't get me wrong - Todd does great work and I suspect this course is good (I haven't taken it) but I'm just confused about why this page exists and was linked here.
Yes, but my point is that there's a path to make use of multi core with Python call to a binded go function, but I suppose you can say the same with a binded c library.
Not 'ideal', but minimal enough and does the job https://github.com/mattes/migrate
As a consumer of a library, it's much easier for me to just import the library than switch to a forked version of go. In fact, importing a library off of github is really no harder than importing something from stdlib.
FYI: Doing a "go get -u github.com/bcicen/ctop" I got: github.com/bcicen/ctop/cwidgets/expanded/mem.go:73: mbar.NumFmt undefined (type *termui.MBarChart has no field or method NumFmt) ...commented that line out and it appears to work.
Is this necessary in a language with first class functions, access to mutexes and M:N threading?
Valid point - go with the stdlib as far as you can. Still, I am curious... is your point specifically about *frameworks* (I understand it this way), or rather about *any* 3rd-party lib/framework/whatever that should not rely on any other 3rd-party lib/framework/whatever? 
As far as i known, SFTP has no daemon, it's a protocol running over ssh and enabled by default in openssh.
In an enterprise single-sign on (SSO) service I developed, I identified five different kinds of state. In order of durability: 0. Channels. A few asynchronous goroutines only receive work via channels, such as logging or managing connection pool resources. A small queue (~50) for most channels limits blocking. 1. Context, or request level state. We populate a `context.Context` struct with information about the request (IP address, GUID, etc.) that will be needed for logging and some background information for every request. 2. Maps. I use maps to store configuration values (easy to reload, which is a big win for production), resource pools (like a connection pool for the LDAP server), form tokens and XSS tokens/requests, and even login retry throttling. These are in-memory, each has an eviction implementation, and are guarded by mutexes. 3. Session Cookies, used to save session state between requests. The SSO cookie is only available to the SSO server and is encrypted to keep employees and third parties from crafting their own cookies. The cookie stores the user's primary credential (username), session expiration time, and sign-on type. If the cookie is cleared, the session ends with the next authentication challenge. 4. Database records. These are stored in places like the LDAP server (accessed with a wrapper around [go-ldap/ldap](https://github.com/go-ldap/ldap), the RADIUS server for TOTP challenges, and in a MySQL database. LDAP stores credentials, groups, and provides password authentication. MySQL stores single-use tokens for password resets and some secondary data used for verification that aren't in the LDAP records. My next project, porting Hazelcast to provide shared durable state between service instances, will need to provide state for authenticated clients during a TCP session (is the client logged in? does the client have rights to that partition?) and also will need to durably persist state across containers and partitions.
Not so much. This package is extracted from a larger project where it makes a lot more sense. I was wary of posting it here but maybe someone can use this, at least as an implementation guide.
This looks great to me. I was implementing something similar for my own purposes, but I have an insidious race condition I can't figure out. I'll likely use this instead. I'm wondering why it only has 1 commit from November, though. I recognize that libraries don't need to be actively maintained and can just be "done". I'm just wary about test coverage and validation. OP, did you open source this?
Yes I did. This package has been in use for a while internally as part of a larger project. Four months ago I extracted it out , added a few examples and tests and put it up on github. Until now, this package hasn't thrown any issues internally, and therefore hasn't been updated.
A RESTful design where your Go backend serves JSON up to your Polymer frontend seems like would be a great option for you. On the Polymer side you would use &lt;iron-ajax&gt; to interact with the REST server. Just be aware Polymer 2 is officially releasing in the next few weeks, right now it's in release candidate
Its really a helpful information. thanks..
That is indeed a very good point!
I typically downvote "here use my library" answers to beginner questions. The correct answer is: if you have an io.Reader, use Decoder. Otherwise use Unmarshal.
Here's the tracking bug: https://github.com/golang/go/issues/18892
Definitely give Vue.js a try. It's got a tolerable learning curve, all the (optional) tools you'll need for routing and state management and a chrome extension to help with debugging.
I use EmberJS for my front ends. It has a bit of a learning curve but once you get past it, it is a fantastic framework if used alongside Ember Cli
I'm ashamed. I didn't notice that I was connected on a remote host. Of course it could not work in this condition. After switching to the local host it now works. 
webasm is for the web. As in, client side.
Because 99% of the people in the world will not switch browsers to support some random webapp. The use case here is to write software in Go that runs in all (future) browsers.
I would imagine the consensus being reached (see [here](https://lists.w3.org/Archives/Public/public-webassembly/2017Feb/0002.html)) put the gears in motion, as it looks very likely that it will happen across browsers and Go can't really afford to not have a wasm backend if it turns out to be the winning technology in the future (as seems likely to me).
You have a point. Google did spend all that time on the v8 to make it much quicker. But they also spent all that time on Python to make it quickly too. I just hope it's some how in the works. Because you can't compare JS's maintainability to go. But your point still stands again. :(
&gt; Why should the future be slower and limited? I'm not sure slower is an issue with the use cases for wasm - although, it probably will improve performance in browser apps in general - as the programs that this would target would be web applications (think reddit *frontend*, not the backend). As for limited - I'd argue being able to run your program on pretty much all devices that people commonly use these days is *not* a limitation. The browser is quickly turning into a platform and I would personally put my money behind it being the dominant platform for frontends pretty soon. Also, allowing one to write the frontend with Go allows the developer (you) to use that same language for both the front- and backend and share code between them (at least theoretically) reducing the amount of code you'll be needing to write and reducing the likelyhood of bugs. It's *more* choices for you as the developer, not less. edit; think of it more like the fact that Go can compile your program to either ARM or x86 architecture (eg. Macs and PCs .. plus many more architectures too) and this will be just another architecture you can target, it just happens to live in a browser that's accessible to most of the people in the world.
That's the exactly ebb and flow cycle I'm stuck in for quite a while. I really love Rust's offerings and performance, I love the lifetime and borrow checker features, the compiler messages, cargo, they are all amazing. The one thing I can't stand right now is Rust's syntax.
Please help me if I'm missing something here. If you commit the vendor directory (which I think I would want to, for the stability against dependencies going away), is there any benefit to using this tool? The only benefit I see is that you have a log of which commits the deps are, but you certainly don't need a tool to do that. Of course, there's a huge benefit if you don't commit the vendor directory. I sit in the camp that I would want to though, until I'm convinced otherwise that is.
I don't know why this was down voted but my reasoning is also along this line.. As an aside, I'm coming from PHP and this is how it's done 
Do you have some context for this?
Like /u/zeroZshadow said, is there a context for this? .. There is a revision key in the generated package Json structure (in the lock file).. Shouldn't that be enough for fetching the same exact version on running `dep install` (or the equivalent).. If it is, committing `vendor` is actually overkill and just a bloat on our repositories
Another option I think is writing a function that takes a `substrings []string, s string`. Then call `strings#Count` on each string in the array..
What if the origin repo is deleted? Or the owner force pushes your commits away?
&gt; If you commit the vendor directory (which I think I would want to, for the stability against dependencies going away) Unless your dependencies are so numerous and huge that committing them isn't feasible. Kubernetes had this problem for a long time, for example.
Valid point, fair enough.
But then those mirrors will have different urls which will change the import path.
I looked into the regex implementation to see how it handles strings and bytes, and if it did anything inefficient like casting from one to the other. Hilariously. it simply passes both to every function. The prototype of the function that actually executes the regex is func (re *Regexp) doExecute(r io.RuneReader, b []byte, s string, pos int, ncap int, dstCap []int) []int Generics or overloading, anyone?
OK, like I thought. Go get currently allows you to use your own internal repo, which is what should be done instead of putting dependencies in source control.
Nothing git submodules can't already do
Chances are that if you vendor one you vendor all right? But that's just the list thing, checking out a specific version, updating to a next version and showing diffs all work (probably even in much more detail)
Based on the OP's quote: `Is there a more elegant way than writing another new line with strings.count()?`, I would probably assume that the first behavior would be the correct one in this case. Good point though.
A match is being reported in both cases; the first match of zero-or-more-whitespace-characters occurs at index 0, and is 0 characters long. When there is no match, FindStringIndex() returns nil, not [0, 0].
I will take a look at this project in more detail next week. I currently Ansible to do rather complex cf deployments but have strongly been considering writing my own tool. This may be a better option :)
Well, thank you! I'm definitely going to use it! 
It lets you specify a proxy. The proxy would have to somehow rewrite the requests to the internal copies.
Unclear? "... while array of stages are always sequential" ?
Pretty interesting list. But most of these exceptions are semantic, not syntactic. Syntax is what defines AST structure, semantics is what gives it a meaning. The only syntactic issue from the list is "Precedences Of Unary Operators", which is also incorrect, because Go has no "post unary" operators. This is what spec says: &gt; Unary operators have the highest precedence. As the ++ and -- operators form statements, not expressions, they fall outside the operator hierarchy. As a consequence, statement \*p++ is the same as (\*p)++. But yeah, it's a good set of items for any Go developer to read and be aware of. 
I think a good first approach would be a HTML interpreter in go. Although it isn't much of a step but would be fun. https://golang.org/pkg/html/template/#HTML
I would like to point out that map[string]interface{} is a poor choice for the data type you have and probably it would speed up once more if you'd use a proper type instead.
There's no mention of GraphQL there. Also not a valid excuse to show benchmarks for a thing then say but the other thing that I'm not showing forces me to use poor data structures. And maybe you should re-evaluate the library / implement you are using in this case but I'm not familiar with graphql.
&gt; can't tell you anything about what dependencies you haven't yet vendored. You need a go-specific tool to walk your code for that For me, for several years, that "tool" has been a simple alias to the following: go list -f '{{.ImportPath}} {{join .Imports " "}}' ... | digraph reverse `go list`
What is the advantage of using `build.go`? On a cursory look it's just a wrapper over standard tools that does not bring anything new and simplifies nothing. And yes, what was meant by the phrase about gigawatts? :) Hasn't found anything similar in a changelog.
Awesome! Updating now. My backups are important to me and there are few tools I trust that meet my needs. Thanks for building restic.
What is this? Why would I want to use it?
It doesn't right now but that's something I'll add
Nice. We're mostly a Python shop, currently expanding into microservices and Go. Stuff like this might help me convince my colleagues that using message queues for RPC calls is not the way forward.
I threw together a GRPC server for my Elkm1 alarm system a while ago, it uses streaming for things like zone updates and such. https://github.com/pborges/elkm1grpc I havn't put too much work into it yet but it is functional
 i, err := strconv.Atoi("-42") if err != nil { //handle error here } 