I'm sorry about that. I know this is long-overdue. It's about a year later than I wanted it to be. We will work very hard to make sure that transition from dep or any other tools is as smooth as possible. For example, these work today: cd /tmp # not in GOPATH git clone https://github.com/golang/dep dep1 cd dep1/gps vgo test -short ./... or git clone https://github.com/upspin/upspin upspin1 cd upspin1 vgo test -short ./... These projects do not use go.mod files yet (probably dep never will!). But the existing version control metadata is good enough for vgo to get situated and let you work outside GOPATH. Both of these are dep but vgo can also convert from pretty much every other Go vendoring file format I was able to find.
Hmmm... So either I'm misinterpreting the comment that you made an example for, or you did. &gt; This is not clear. as slice is also a pointer to underlying array. you still can modify each element in the function. Same for return value. * A slice is a pointer to an underlying array. I believe we both agree that this is true. * You can modify the elements [of the slice] in the function. I believe you also think this is true. * "Same for return value." Honestly, I'm not sure about this part, but return values are not what we are discussing. What confuses me about your line of reasoning is that you consider the variables the "original data" of the slice. Regardless of whether you use pointers or not, it's really irrelevant how you fill the slice. The slice still has a copy of the data you provide. Granted, if you use pointers you're not copying the underlying storage that they point to (instead you share it), but that isn't relevant to the three bullet points above. Your example isn't what I would call a good example because a slice, whether it's of pointers or values, can be modified by the function it's passed in, and with pointers the side effects are less predictable* (due to which I would argue value slices should always be preferred unless you know you're dealing with stateful objects that must not be copied). *) Imagine instead of your `for` loop, we use a function as stated in the original question: ``` fooPointers := []*Foo{&amp;foo1, &amp;foo2, &amp;foo3} Shuffle(fooPointers) fmt.Println("pointers:", foo1, foo2, foo3) fmt.Println("actual slice:", *fooPointers[0], *fooPointers[1], *fooPointers[2]) // ... func Shuffle(s []*Foo) { for i, v := range s { if v.Value &gt; 1 { // We don't like this one. s[i] = &amp;Foo{0} } } } ``` Two of the original variables are now irrelevant compared to what's in the slice. Which is why I think the example you made doesn't really show the purpose of pointers, nor does it counter the comment it was a response to. Anyway, I think that's as much effort as I will put into this. :) I do it all in good humor, I promise, and I learn from these discussions so hope you don't take it the wrong way!
Cargo's docs have sadly not gotten as much love as the other docs; I'm hoping to rectify that this year. Will do that for sure! &gt; since it's a divergence from most other popular package managers. I'd like to put a teeny fine point on this: The advice to not use `=` is, in my understanding, the same for all of the managers that use semver constraints. Most do treat "no constraint" as being the same as a "=", but as a consequence, people tend to write `~` or `^` out manually. So while it is literally a divergence, it's making the convention the default, rather than the antipattern the default.
Ah, I see. It seems like a major downside of locking to a git commit is that it would make it harder for anything depending on your code to resolve incompatibilities nicely. If nothing depends on your code, then what about the clone-and-use-replace method at the end of the example post? It's shown there for fixing a bug in a dependency, and it's described as a temporary measure, but it seems like it would maybe work for this.
&gt; I'd like to put a teeny fine point on this: The advice to not use = is, in my understanding, the same for all of the managers that use semver constraints. Most do treat "no constraint" as being the same as a "=", but as a consequence, people tend to write ~ or ^ out manually. So while it is literally a divergence, it's making the convention the default, rather than the antipattern the default. Absolutely, all I am saying is that my experience, based on all the other dependency managers I've used, is that `x.y.z` should resolve exactly to version `x.y.z`and nothing else. Cargo diverges from this for good reason.
I'm really excited about this! One thing I'm wondering though, is where vgo downloads the modules? Inside a folder on the project workspace? 
I think this is pretty cool, but seems like a lot of hoops to jump through, and only works on linux. 
Don't want to file this as an issue, since it may just be a misconfiguration on my part. Getting this error when running vgo per the example you provide: ```vgo: cannot determine module path for legacy source directory {somewhere on my filesystem}/scratch/hello (outside GOROOT, no import comments)``` This is on OS X, with Go installed via Homebrew.
I know I'm the exception, but: I'm really disappointed by this. I was really excited to see at least some voices in the [mainstream programmer community](https://www.youtube.com/watch?v=tISy7EJQPzI) catch up with Go's "live at head" mentality - and now, Go is taking what appears to me to be a major step back by making it intentionally hard to stay up-to-date. When I want stable, maintained software, I install them from my distribution, *that's what it's for*. When I use the go tool, I do it as a Go developer and I *always* want the latest and freshest. If stuff breaks - that's fine. It *needs* fixing then. vgo *inverts* what I'd see as the correct pattern here - it makes developers use stale versions, while distributions will have to figure out how to do the upgrades and resolve the breakages (as they need a consistent view on the world as a whole). Lastly (again, knowing that I'm the exception), I really *like* GOPATH and I don't like seeing it go away :( I really hope there will be some way to keep living at head‚Ä¶
[This Article](https://bytearcher.com/articles/parallel-vs-concurrent/) argues that Node is concurrent, by the definition that "concurrency allows a computation to advance regardless of other operations." This is a similar definition to [wikipedia](https://en.wikipedia.org/wiki/Concurrency_\(computer_science\)): &gt; concurrency refers to the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome. Even [Node.js themselves](https://nodejs.org/en/about/) advertise as being concurrent: &gt; many connections can be handled concurrently &gt; This is in contrast to today's more common concurrency model where OS threads are employed. By that definition, I feel that Node is definitely capable of concurrency. Another example: a = 0 setTimeout(() =&gt; { a += 1; }, Math.random()*500) setTimeout(() =&gt; { a += 1; }, Math.random()*500) setTimeout(() =&gt; { console.log(a); }, 1000); &gt; 2 This is highly contrived, but it accomplishes a point; there are two pieces of logic there (the `a += 1` lines) which mutate some global state. They can be executed in any order, and in that code example are executed in random order. They even _could_ be ran at the same time (in parallel) (if that were possible in JS) (it isn't), though we'd need some kind of global mutex to guarantee it would still work. But, most importantly, each branch of that logic acts independently of the other. Line 3 isn't waiting on Line 2 to finish before it can do its work. Line 4 _should_ wait on Line 2 and Line 3 (it doesn't in this example, but it could if you wrote more code via async event emitters/callbacks instead of using setTimeout). This is literally a fan-out, similar to spinning up two goroutines. Next question: "JS is single-threaded, so if that fan-out didn't involve some sort of non-blocking event dispatch they would run synchronously." True. But now we're entering the definition of parallelism. Concurrency just says that they don't depend on each other, not that they have to run at the same time. L2 and L3 don't share stacks. They don't depend on each other; they just share global state. Next question: "This is only possible because the runtime enables it." True. But, isn't that how all concurrency is enabled? Because the Go runtime supports threading, or the JS runtime supports non-blocking delayed event dispatch, etc? Really, what it comes down to is this: Nodejs definitely isn't parallel. If it also weren't concurrent, then web servers written with it would only be able to support 1 request at a time. This is not true. So, it has to be concurrent. There's no other way to explain how it is capable of handling multiple (up to millions per server) simultaneous requests without it being concurrent. 
They are downloaded under .cache/go-build i believe
I hope vendoring *and committing the vendor directory* won't discouraged. Yes, you can set up a proxy for the packages but (a) nobody does it anyway and (b) it doesn't help when I download a 10 year old open source project, because I can't go back in time and make a proxy ten years ago. The vendor directory guarantees that, at least, the project will always compile.
According to the [vgo examples](https://research.swtch.com/vgo-tour): &gt; The vgo get -u behavior of taking the latest of every dependency is exactly what go get does when packages being downloaded aren't in GOPATH. &gt; The important difference is that vgo does not behave this way by default. From a first look it seems to be that you can still have the old behavior if you so wish.
Start with the idea of one goroutine. One thread of logic. I'll use the words "thread" and "goroutine" interchangeably here even though they kind of mean different things. When you make an HTTP request, it will block that thread. The request must finish before more logic in that thread can proceed. Because of this, from the context of a single thread/goroutine, go is not concurrent. It also isn't parallel; you can't do two things at the same time in a single thread. That's a pretty general limitation not just to go. Now, from the context of multiple threads but 1 physical CPU core. Two threads have some work they need to get done. They both ask nicely to be "scheduled" to do that work. The go runtime says "great, but I only have 1 core here so you'll have to take turns." What does taking turns mean? The most simple case: One will be scheduled first. It does what it set out to do and finishes. Then, the other one will be scheduled. It does what it was set out to do. Program finished. There's a special case though, and its the core of your question: IO. Say, making an HTTP request. As we said before; when a thread does IO, it is blocked from the context of within the thread. But, from the context of the go runtime, its just sitting there waiting for a slow server somewhere to return a request. That's wasted time. So, the Go runtime supports asynchronous IO. When a thread is waiting on IO to return, it **may** be moved to the back of the "wait your turn" queue and another thread will start executing. Then, that first thread has to wait until the second thread is finished before it starts executing. That's the simple explanation. It gets a lot more complex. Additionally, this is all assuming one logical CPU. If you have more than 1 CPU, Go will use them and run both of them at the same time. But, then again, its the same problem just at a bigger scale, right? Lets say you have 2 CPUs but 4 threads. You might run into the same problem. So the scheduling algorithms need to be aware of where threads are scheduled, which ones are waiting for IO, which ones are like "let me do some work dude", etc when making a decision of when and where to run threads. Its pretty complicated.
You probably didn't put in the import comment. package main // import "github.com/you/hello" that's where the initial go.mod gets the initial module line from. Or you can just create a go.mod that says module "github.com/you/hello" 
AIUI, this is intended to be used by the upstream developer, not by the user. As a user, I *always* want to use the latest version of everything and as a developer, I want my users to *always* use the latest version possible (at least if they are installing an unpackaged version).
I'm of the opinion that teaching the kernel how to compile files on the fly is not the same as a scripting language... Not even remotely so. 
Thanks! Did not realize the // import "github.com/you/hello" was not just a comment but something akin to a preprocessor directive.
That workflow was better than I expected. It seems to cover all my initial concerns. Though I still of the opinion that module versions are more the business of the version control tool you use, and not really a language or language tool issue, but I know that I'm in the minority as far as that goes.
I was also disappointing with this strange comment ! Wonder why it could not work without and just take the current dir as a default name, or even no name at all.
Alright, here's a few pointers to get you started. I don't believe giving you a solution straight ahead is a good use of either your or my time, but let's split your problem into several different problems that we can solve one at a time: * Reading from a file This is quite straightforward: check out [`os.Open`](https://golang.org/pkg/os/#Open). That will give you a `File`, which is an `io.Reader`. * Parsing the HTML [`/x/net/html`](https://godoc.org/golang.org/x/net/html) will allow you to parse HTML. This is the hard part, but the examples are quite good! Before you venture too far into this, figure out how you would extract the information you want from the website. Can you rely on the order of the `&lt;th&gt;` elements? Their `class` attributes, maybe? Once you've figured out how to extract the information, you can put it in a slice (that's Go's word for arrays). If you've learned about structs already, consider defining a struct that represents each of these rows, containing the item name and count, and then storing the result of parsing the HTML as a slice of these structs. Traversing the HTML document tree is probably the hardest part of this whole exercise. If you feel overwhelmed, try looking for a problem that doesn't require parsing a complex format. * Printing the output Once you have a slice, you can iterate through it and print each element to the screen as desired. If you haven't used [`fmt.Sprintf`](https://golang.org/pkg/fmt/) already, this is a good time to familiarise yourself with it!
Observation: vendored packages are like dereferenced lock files.
DI is a fundamental Go concept. I'm pretty sure that it isn't just the association with Spring that you were seeing, but the fact that for a while there in /r/golang someone linked to their github project for some heavyweight DI framework based on something they used in another language every couple of months. But those frameworks exist due to the way the languages work, and they shouldn't be blindly ported over. Because Go has implicit interface satisfaction, you don't need a huge framework... you just _do_ it. I also think you see some people conflate the idea fundamental idea of DI with the accidental details of those huge frameworks, to the point that they think if they don't see a huge framework it must not be "dependency injection".
This is fantastic thank you for linking the different parts to use. I just now got to "Arrays and Slices" in the course and structs are later in the course. I'm not going to jump ahead because I'm worried I'll miss something important and then learn it the wrong way. If HTML parsing is really a challenge I was thinking of copying the text of the document and parsing that. The problem is that it's messed up when copied. You'll notice things like: 976-7752 ProSupport: 7x24 HW / SW Tech Support and Assistance, 3 Year 1 - - **That should be:** 976-7752 ProSupport: 7x24 HW / SW Tech Support and Assistance, 3 Year 1 - - Text before Parse: https://pastebin.com/gUqHJEkj Ideal Outcome: https://pastebin.com/UTtypFuB Maybe the text would be easier?
To be fair dep was never meant to be "it", at the very top of the readme it says &gt; dep is a prototype dependency management tool for Go. It requires Go 1.8 or newer to compile. dep is safe for production use. &gt; dep is the official experiment, but not yet the official tool. Check out the Roadmap for more on what this means!
I would imagine checksum locks to apply to builds only. As in, they would take effect in one's module, but discarded when the module is imported by other modules. I believe the proposal already separates the feature set in two, depending on whether you are in charge of building or not.
&gt; Lastly (again, knowing that I'm the exception), I really like GOPATH and I don't like seeing it go away :( I like the idea of GOPATH as a developer on my own project. But I don't like it for the idea of some other user wanting to clone and build my project just for the build artifacts. Vendor directory solved that second problem for main programs, but the advice has been to not do this for libraries. So then we want a good way to specify the dependencies of a library, without vendoring them, and programs consuming those libraries can vendor direct and transient dependencies properly. 
Reproducible builds are important. But all that implies for Go, is that it should provide the same output given the same inputs (which it doesn't, right now, coincidentally). If you want reproducible builds, collect the information about all the versions going into your software and publish them as a lockfile. How you do version resolution and how your development cycle works, is independent of that.
&gt; As I've been working on this I've been using cargo as the gold standard, really. If the end of this all is that people say Go's package management is as nice as cargo, then I will be very very happy. A long way to go yet for sure. That makes me very happy to hear! 
&gt; But I don't like it for the idea of some other user wanting to clone and build my project just for the build artifacts. Then publish binary releases, so they don't have to. Ideally, package them for distributions. I don't understand why people think go-get (or vgo or dep) has to be the tool of choice to distribute their software.
&gt; is that dep doesn't support private repositories. can you explain? I'm using private repositories with dep.
https://github.com/golang/dep/issues/174
https://github.com/golang/dep/issues/174
[direnv](https://direnv.net/) works for me.
I don't fully understand why the new import path is necessary for major versions. It seems redundant given the existence of that information in the version tag. If vgo was never meant to be part of the official toolchain, this would make sense to me since it would need to remain go get-compatible. I get that this is to allow having n copies of the repo with different major versions in the build, but why can't vgo mechanically handle the import resolution by some means? I realize that would make vgo itself more complicated but it seems like it would make using it simpler for everyone, especially authors creating a major version.
Thanks, that's a good thought. When I run youtube-dl directly from command line and capture the output, I see that aria2 is outputting it's progress on the same line (i.e. using carriage returns). However, that doesn't seem to be the case for the output capture by Go. When it finally gets the output, all the progress outputs are on new lines... I've modified my code to split on either '\n' or '\r' (in that order of preference) but that hasn't helped.
Thanks! I'm not sure how I missed CommandContext()! I've incorporated those ideas into my code, but unfortunately still no luck. I found a post on stackoverflow with someone having a similar issue with PHP, but not sure if that bears any relevance to the Go context: https://stackoverflow.com/questions/20723319/reading-stdout-from-aria2c-in-php-script
Thanks, I checked that - aria2 is definitely outputting to stdout.
Darn... I had high hopes for Pixel. RIP
Not sure what you're talking about. Pixel is not at all dead. I just can't afford to work on it as hard as I used to in the past, but most of the features are in place already. Hopefully, more contributors will make the remaining features possible.
/u/rsc Could you clarify on what you mean by "older" in the post? I think this is a source of much confusion, especially on HN. In the post you write: &gt; This proposal takes a different approach, which I call minimal version selection. It defaults to using the oldest allowed version of every package involved in the build. This decision does not change from today to tomorrow, because no older version will be published. I think what you are intending here is that semantic versions will always be increasing (within their respective `MAJOR.MINOR.*` categories), so minimal version selection defaults to the minimal semantic version that meets its requirements. ie a package developer can release versions in the following order: 1. `1.2.0` at T0 2. `1.3.0` at T1 3. `1.2.1` at T2 In this case, `1.2.1` is "newer" than `1.3.0`, but as I understand it minimal version selection would prioritize versions in the order: 1. `1.2.0` 2. `1.2.1` 3. `1.3.0` Choosing the first version in that list that satisfies all of its modules, even though `1.2.1` may not technically be the "oldest" version. Is this inaccurate? 
I actually will continue to work on it from time to time, I've had a pretty large simplification of the main abstraction in mind for quite some time that I want to land, so, no need to mourn ;) Yet
I feel like this is taking away one of the best features of `go get` and replacing it with one of the worst ones from other package managers. I can no longer just build a thing with any confidence that it's not going to try to pull in random crap from the internet or advertise to the world that I'm now doing a build.
I've never felt the need to run different go versions, they have great forward compatability. For managing dependencies I use go dep. For managing random binaries needed by my project I use https://github.com/vektah/gorunpkg It's there anything else a virtualenv would add?
For Chrome you could use one of the many "remote debugging protocol" clients. Mine is http://github.com/raff/godet, but there are others (look for "Selenium and browser control tools." in https://github.com/avelino/awesome-go) 
Isn't that was this bit was about? &gt; When developing using Git, developers will define a new semantic version of a module by adding a tag to the module's Git repository. Although semantic versions are strongly preferred, referring to specific commits will be supported as well. [...] &gt; Authors will be expected to tag releases with semantic versions, and vgo encourages using tagged versions, not arbitrary commits. The rsc.io/quote module, served from github.com/rsc/quote, has tagged versions, including v1.5.2. The golang.org/x/text module, however, does not yet provide tagged versions. **To name untagged commits, the pseudo-version v0.0.0-yyyymmddhhmmss-commit identifies a specific commit made on the given date.** In semantic versioning, this string corresponds to a v0.0.0 prerelease, with prerelease identifier yyyymmddhhmmss-commit. Semantic versioning precedence rules order such prereleases before v0.0.0 or any later version, and they order prereleases by string comparison. Placing the date first in the pseudo-version syntax ensures that string comparison matches date comparison. 
Hi raff99, thanks for answering! Indeed godet seems to be na amazing tool, but when I tried it I kept reciving "connect Get http://localhost:9222/json/list: dial tcp [::1]:9222: connectex: No connection could be made because the target machine actively refused it." and could not find anything online about that error. Do you know how to solve it? 
I don't think this is the same. For one, vgo discourages the use of commits here. But also, I think this is the minimum version, so it's not guaranteed to be the exact tree you end up with.
[removed]
&gt; It was created largely in isolation from the community‚Äôs work on dep, to the point where not only is there no shared code and at best moderate conceptual overlap, but a considerable amount of the insight and experience gleaned from dep as the ‚Äúofficial experiment‚Äù is just discarded. I'm worried that the long term reprecussions of this unilateral decision by the go team hurts future community driven efforts, although I'm happy to see more progress on the 2nd biggest complaint that non-go users have with go (behind generics, but before error handling :) If the core team decides to go in a completely different direction, even your blessed project can suddenly find itself on the outside, with potentially wasted effort.
[removed]
Cool! We're using a very similar open source tool called reviewdog for this though: https://github.com/haya14busa/reviewdog
I wish $ echo &gt;go.mod $ vgo build did the equivalent of `vgo get -u`; require everything to be at least the current version. Think about it, I see on the web that rsc.io/sampler v1.3.1 was released with this great new (backwards compatible) improvement, and I want to write a little test app to try it. It's natural for me to expect that I'd start with v1.3.1 by default.
The OP is confused about how a slice can protect against side effects as it is a fat pointer. I simply showed that the data the slice points to needn't be the source of truth. Basically you seem to think the debate is about whether or not the contents of a slice can be modified, but no one disputes that. However, it doesn't mean that the contents are the source of truth for the application state.
vendoring works well for binaries but is usually a really poor idea for libraries and easily runs into dependency nightmares although I am not sure how vgo would help besides allowing for one version to be good for all children dependencies via MVS. 
&gt; There are situations where the application has cgo dependencies that rely on a build in the local system. But `vgo` is neither conscious of what versions of C-libraries are installed on the system, nor would it be able to enforce its notion of what version you'd want. There *is* already a solution for reconciling required versions of code written in different languages, though; the distributions' package manager. This is an argument *in favor* of properly packaging your software, not against. &gt; The user may not be a Go developer, but they can make sure they have system lib deps X, Y, and Z, and run the build commands to produce an artifact. If you already know and specify what versions of what C libraries will be required *and* trust the user to enforce that, why would the Go code need to be built on the users system? It would seem that in that case you can simply ship a dynamically linked binary (which is what would be produced anyway) which loads its dependencies like any other software. &gt; I'm just saying there are valid cases where it should be easy for a project to be checked out and built, without needed special build tools (gb), or Go-specific dev environments. Can you elaborate? Because that seems highly unlikely. I basically never built any software that did not require me to install at least *some* things specific to the language or even software itself. I agree that this is a problem and it shouldn't be the case, but I really don't see how GOPATH is, in any way, more of a special snowflake here than Python's virtualenv or autoconf or manually having to install a random set of headers of extra C-libraries. And again, a generic, language agnostic solution to this would seem preferable. For example debian at least has tools to build a package reproducibly in a container. --- This gets at the heart of the main problem I'm having with this development of language-specific package managers: They conflate development workflows with distribution workflows. But software distribution (and reproducible builds are part of that) should, as much as possible, have a language-agnostic solution. If nothing else, then because most actual software relies on more than libraries written in a single language (be it documentation-generators, databases, testing tools‚Ä¶). `go get` (and `vgo`) as language-specific tools should be tailored for the use case of Go developers hacking on Go code. And for *that* usecase, living at HEAD is the healthier option for the whole ecosystem. Solve the resolution of compatible versions in the distributions, where it needs to happen anyway, instead of the individual Go packages. And save everyone some duplicated work. --- Anyway, as I said, I'm aware that I'm the exception with these views. I'll still mourn what I perceived as one of Go's main areas of innovation going away, though.
The proposal is very clever - looking at existing solutions to the problem and removing as much complexity as possible. Relying on vX.Y.Z tags is a good idea - it's good practice anyway. Relying on the major version being part of the path is a good idea - solves the multiple-installed-versions naturally. The minimal version selection is certainly the most contentious decision - it will be interesting to see how it turns out, in any case. Looking at the vgo tour, I have one question. Let's say I'm writing a package for a binary, so it will not be depended upon. Let's say I depend on package A which depends on package AB at version 1.0.0. Let's say package A is abandoned (or just finished and not in need of maintenance), but package AB has a bug, fixed in version 1.0.1. The way the tour suggests to solve this is to add a `require { "AB" v1.0.1}` to my module. The problem is that it is a transitive dependency, but it looks just like a direct dependency - there is no distinction at the semantics level. That's not very good -- transitive dependencies should not look like direct dependencies. I suppose the tool can mitigate the situation, on the practical level at least - if the dependency is not imported by my module, than it is not direct; if it is not direct, and there is no transitive dependency which depends on a lower version of it, then it is useless; and if it is useless, then it can be a warning/error. But the semantic distinction is still important.
The whole dep issue aside, this does look very interesting. Some things I really like: - No automatic minor version upgrades - Ability to exclude dependencies - No GOPATH - Easy way to downgrade, replace with forks - Seamless integration into existing go tool chain I've been burned so many times because of the automatic minor version upgrades in other languages that all my projects at work specify exact versions. Making this default IMO is a great engineering practice. 
&gt; transitive dependencies should not look like direct dependencies. I thought that at first, but now I'm not so sure. I think in a large program the distinction gets blurry. In any event, if you want to record why you are adding the requirement, you can put comments in the go.mod file and they will be preserved. 
We haven't used this yet at all. If that doesn't work in practice then it's easy to add power to both replace and exclude, such as allowing &gt;=v1.5.2 instead of v1.5.2. They can be just about arbitrarily complex, because they only affect the current module. 
[removed]
We can add a flag for people who are worried about this. I don't honestly see the practical difference between you running "go build", having it fail due to missing dependencies, and then you run "go get" and "go build" again. How often are you really going to stop and not finish the build just because you don't have a dependency cached?
Yes, sorry, "oldest" means "minimal under semver" not time-ordered. I tried pretty hard to avoid saying that in most places, but that one slipped in. 
[removed]
1. Why not use TOML? 2. The meaning of ‚Äúpackage‚Äù and ‚Äúmodule‚Äù are now backwards from Python, where packages are projects and modules are files. Why not some other word?
Yeah this feels a bit ... off to me. I think the dep authors thought their tool would be *the* tool - and now - "hey guys I did this experiment in my spare time and it's awesome, check it out". I'm sure it *is* awesome, but even if I add 10 points for "working code" and 2 points for "elegance", I'm gonna subtract 5 for "community relations". 
&gt; I believe this is the wrong default, for two important reasons. First, the meaning of ‚Äúnewest allowed version‚Äù can change due to external events, namely new versions being published. Maybe tonight someone will introduce a new version of some dependency, and then tomorrow the same sequence of commands you ran today would produce a different result. Maybe the new version is a security patch, and really almost all consumers should be using the newer version. For an end-user fetching and building an executable (`go get github.com/golang/dep`, etc) the correct approach is probably to pull the latest bug fixes and security updates for the tool and its dependencies. For an engineer building a CI/CD pipeline, yes, reproducibility is important, and so there should be an easy option to enable at runtime. But the default, imo, should be to get the latest versions of things that, based on defined semantic versions, are likely to work. (And I already have a method for doing that - committing a populated `vendor/` directory)
My course didn't have anything about Structs but the info I found makes me think I would do something like this: type Quote struct { item_qt string item_name string }
This is critical. And for open source projects, the proxy solution is even less palatable, since it creates a dependency that can disappear. I've said it before and I'll say it again, but if you need an Internet connection to build your project, you probably have a security nightmare waiting to happen. 
There's no TOML parser in the standard lib would be my guess as to why not TOML. 
This is fairly frustrating to me. I am attempting to introduce the use of Go into my workplace; churn around the package manager (which had seemingly finally started to settle down) will hurt my efforts. Oh well! 
&gt; But those frameworks exist due to the way the languages work, and they shouldn't be blindly ported over. Because Go has implicit interface satisfaction, you don't need a huge framework... In what way does implicit interface satisfaction simplify dependency injection? The main overhead in e.g. Java is having to define the interface in the first place and changing all of the code to use the interface instead of the concrete type, not the `implements` clauses. The comment *would* apply to a dynamic language like Python where you can just duck-type away any interface without declaring it. &gt; you just do it. It's definitely a good idea to avoid using a DI framework for as long as possible. But at some point, your Main() will become a huge unholy unencapsulated mess, and you *will* try to abstract some of it away, thus creating your own DI framework :) [Note: speaking from experience with other languages; I haven't done a project which would benefit from DI in Go yet.]
No worries, and thanks for the response. Unfortunately most critical people on HN seem unwilling to give you the benefit of the doubt or to verify which it actually is üò©
The "automatically pull from the Internet" gives me serious security concerns. The simplest is that the packages are served via HTTP, which makes them easier to modify in flight if you own the right router. Even if you fix that, though, you have the go-bindata situation. That author could trivially swapped in a mostly similar but slightly evil version. When I trust a dependency, I only trust it now. I don't necessarily trust all future copies of that source. Signed releases might help here, but that's kinda the opposite of simple. 
[removed]
It *is* frustrating to see this happen now, not a year ago when it should have, instead of wasting a year with dep... ...but ultimately, as it all settles down and gets rolled into the mainline stable releases, it doesn't matter. It just needs to get sorted out, and thats finally happening. This is what you should be taking away from the announcement: - right now, use dep - try this new thing and give feedback - when it rolls out stable, the transition from dep will be seemless. 
`go get` will download the source into your `$GOPATH/src/` directory, and then used from there for builds. `dep` uses a vendor directory, which again is in your source code and won't be randomly cleared. `vgo` seems to use the go build cache and DOES NOT download the source code into your $GOPATH or a vendor directory. Using the cache isn't necessarily an issue, but it is unclear whether this will be retained permanently on all operating systems, or if it could be cleared by a reset or any other random action.
Me too! Seeing cargo acknowledged makes me tangibly more positive that this proposal is actually an incremental step, not just throwing everything away and doing it from scratch (again). &lt;3 cargo. If I can one day say that about go package management, it'll be a fine day.
Check out the `vgo` source - https is used throughout it. When rsc mentions `HTTP` in the post I think he meant it as the protocol (as opposed to say `git clone ...` which doesn't have to use the http protocol).
Why go.mod introduces new keywords while keeping the syntax from Go? module "github.com/you/hello" require ( "golang.org/x/text" v0.0.0-20180208041248-4e4a3210bb54 "rsc.io/quote" v1.5.2 "rsc.io/sampler" v1.99.99 ) vs package "github.com/you/hello" import ( "golang.org/x/text" v0.0.0-20180208041248-4e4a3210bb54 "rsc.io/quote" v1.5.2 "rsc.io/sampler" v1.99.99 )
[removed]
You need to create smaller abstraction here and write a unit test that fetches a local dummy file or something. You also need to see if it works with the pipe just being set to stdout like I previously suggested. When something doesn‚Äôt work, remove code until it works, then add code until it breaks and you find your problem. 
&gt; There is no technical limitation to doing any of this. It's just that Go intentionally doesn't want to go there. Go already did go there, with its syntactic sugar for transforming value -&gt; pointer and pointer -&gt; value on the receiver of a method invocation. Which is why it is a fascinating question, why isn't this transformation implemented for value -&gt; pointer when the receiver is in an interface? Obviously its not because Go didn't want to go there, because it already went there. So there must be some technical reason. The technical reason is pretty simple really. If you allowed a pointer of a concrete type to point to a value in an interface, that pointer would be invalidated by a subsequent assignment of a differently typed value to the interface variable. There's no way around this.
Sounds like an issue with stdout buffer not being flushed and because youtube-dl is calling the program [through a buffered pipe](https://github.com/rg3/youtube-dl/blob/master/youtube_dl/downloader/external.py#L98) you don't see the output until the buffer is flushed after aria terminates. Turning off buffering using `stdbuf -o0 youtube-dl ...` might work, but I'm not sure that will actually turn off the buffering on the pipe when youtube-dl spawns the aria child process.
Just found out the problem! I didn't initialize a remote debugging port 9222. Gone dig godet them. Thanks!
Two thoughts: 1. Go package management got a STD from Javascript, the one that makes anything new age fast as fuck and get deprecated before it's out of beta. 2. Do not be surprised if Go team comes out of the closet with generics, remember when they said you don't need a package manager?
We're using dep with a mixture of public and private repositories. No vanity servers involved. Have you checked your ssh and git config? Admittedly none of us are working behind a proxy. Is it the proxy that makes the difference for you?
[removed]
1. To start, every require directive would take 3-4 lines instead of 1. Honestly I didn't look much further. I want these files to be human-readable first, machine-readable second; not the other way around. 2. Other languages besides Python have modules too, with varying meanings, enough that there's no one right meaning. Python programmers arriving at Go may be confused for a tiny while but I don't anticipate them having a problem adjusting. We spent a while looking at words and didn't find anything better.
Have we reached that point in the life-cycle of Go where the lovely, clean language starts to get the first real hints of "ugly"? I really hope not.
Consider a package with two .go source files that import the same dependency, but at different versions. What should the compiler do?
How does this relate? My concern was the naming.
Ugliness was not my concern. Usually I tend to refer to Go libraries as package - "Hey John, import this package.". Now the module concept is introduced - is this the same as package? Are they supposed to be used interchangeably etc. 
&gt; Yeah this feels a bit ... off to me. This isn't surprising given the history of Go. I won't say more as a welcome the effort anyway but Go designers always had a top/down relationship with the community, beginning with the language itself as opinionated as it has always been. You can't do that by taking cues from 3rd parties.
[removed]
Another thing the actual `defer` does is let you `recover` from a `panic`, which I would imagine adds some amount of overhead even if not used, just to load and compare to see if it needs to unwind the panic.
That's probably because concurrency is a fundamentally important thing to have in a language and runtime. Its not a golden goose that only specific languages have; its a hard requirement for anything that might be used to write a web server (handling multiple requests), a UI (responding to click/touch events), really every paradigm of programming. Moreover, it is fundamentally enabled by UNIX, not by a language runtime. If your language has threading, you're at some point leaning on the UNIX threading model. If you have non-blocking IO, you're leaning on O_NONBLOCK and the accompanying syscalls. C doesn't even have a "runtime" to help with this, but it _does_ have UNIX, so you can write concurrent (and parallel) programs in C. The only language I can really think of off the top of my head without a clear concurrency model is shell scripting. But I could be wrong on that. The important part is that concurrency in many languages sucks. Everything in C sucks, but try writing a concurrent/parallel program and you'll want to gouge your eyes out. In Node, it "happens magically" but isn't designed to scale well; at some point the event loop gets so clogged up that you need to look into process scaling. In Go, its ridiculously easy and you get tools to encourage it like channels. 
Thanks, I'm becoming more convinced that's what's going on here. I've swapped aria2c for axel and can now capture the output in realtime. That would suggest the issue isn't with youtube-dl, but with aria2c itself.
Bundle?
&gt;The uniform representation of modules as zip archives makes possible a trivial protocol for and implementation of a module-downloading proxy. Companies or individuals can run proxies for any number of reasons, including security and wanting to be able to work from cached copies in case the originals are removed. With proxies available to ensure availability and go.mod to define which code to use, vendor directories are no longer needed. Brings tears to my eyes! Thank you! Thank you Russ
We've had similar issues. Dissenters always dissent and it's ok. Other features of the language more than make up for the struggle in this area. We were using glide before dep. And moving from glide to dep was as simple as `dep init` I believe the transition will be this simple again.
To be clear here, I really love working with Go. It is my daily driver, so I'm not coming at this from an angle of "beating Go" in any way. I'm commenting on what I see. 
Thank you as always for you time in publishing these videos. This is a lot of work and dedication! What is the best practice for project organization for web apps at the top level? Here the repo is: /cmd /src I've seen some go people not like "src". Gogs uses somewhat similar structure: /cmd (for go executables.) /pkg (for non-executable go code) /public (for web files that may be exposed, like html, css, js) /template (for precompiled html code) Are there any projects well organized that you would recommend looking at? 
&gt; With proxies available to ensure availability and go.mod to define which code to use, vendor directories are no longer needed Please, please do not remove vendor (or a very similar way to easily check in dependencies into our applications). I don't want to "set up trivial proxies" to ensure that my builds don't suddenly fail when leftpad.go happens. I absolutely want that code checked in and not require the network to build. My build servers are happier and I'm happier. This is for applications, not libraries. Don't make us set up more infrastructure to keep the same level of safety we currently enjoy. By all means, let the proxy solution exist. But don't remove vendor. And thanks Russ for continually driving the language and ecosystem forward. 
Unfortunately, it's [Dunning‚ÄìKruger effect](https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect) although I agree and upvoted you anyway.
`dep` disregards the `vendor/` directory in libraries. The traditional advice to not vendor libraries may be out of date. Vendoring libraries can help make the library's tests more reproducible, since `go test` will use the packages in `vendor/` when it's running.
I think this is important. It removes an important security safeguard I think we take for granted today. Consider a project that's popular, but doesn't see frequent updates and imagine a dependency it imports finds a security flaw. Today, the dependency's maintainer has the ability to update end users when they do a `go get -u`, even if the original project does nothing. For serious flaws, dependency maintainers can essentially communicate with their indirect users: if they break compatibility deliberately, the importing project will break, and that can be a useful signal to potential users that something is out of date and potentially untrustworthy. The proposal at hand inherently removes any such mechanism. All that said, it wasn't a great communication channel anyway, so I think we have an opportunity to make things much much better. /u/rsc - perhaps we need an extra notation in the `go.mod` file of the dependency to declare versions as unsafe. These versions (and below?) would be simply excluded from the minimum version selection process. I think there's some reasons we might need importers to be able to override a ban on a particular version, like for security research. As an end user, though, I think it's best for me to know I'm building a binary that's potentially unsafe, even when the project I'm building has imported a flawed dependency deliberately, and to make me also request that deliberately. There's probably a compromise in here somewhere between errors/warnings, adding more rules and options, and security as a default, but I really enjoy the security as a default. One last note: I think we should avoid telling maintainers to change tags to solve this: that seems like it forces us to hide useful original version information in our repositories. Sometimes those tags are used beyond a build process, like for bisecting bugs, or in some corporate process, and if they move around, they aren't really meaningful in the way I think people expect.
Yea that feels more like a hack'ish way to have /vendor atm.
the website says "These licenses apply to official Caddy binaries obtained from our website. The source code is Apache-licensed." But are those binaries different from the binaries on github releases? Are the github binaries under the apache 2 license?
Oracle acquired Java from Sun in 2010: https://en.wikipedia.org/wiki/Sun_acquisition_by_Oracle
**Sun acquisition by Oracle** The acquisition of Sun Microsystems by Oracle Corporation was completed on January 27, 2010. Significantly, Oracle, previously only a software vendor, now owned both hardware and software product lines from Sun (e.g. SPARC Enterprise and Java, respectively). A major issue of the purchase was that Sun was a major competitor to Oracle, raising many concerns among antitrust regulators, open source advocates, customers, and employees. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/golang/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Same. I can't even bring vgo to the table right now because it doesn't support /vendor. Feels like Google couldn't care less about small shops. I'm sure they will have their decentralized block-chain powered globally redundant omni-present module proxy server providing their CI builds with petabytes of module zips per second. They don't need no stinking /vendor and at this point I'm in disbelief whether they care about community.
can we *please* have the necessary security discussion arising from the "just use the oldest possible version" concept? this is horrible if a project notices an issue and rightfully patches it in a minor version even if 2.1.2 is fine, vgo will happily pull in 2.1.0
Not sure what this is, from a beginners perspective. Who is the person talking (just an empty webpage at root), what is his relation to dep and if everything would be optimal and he is somehow near the core, why not call dep2 instead of vgo?
&gt; I think the dep authors thought their tool would be the tool I think that calling dep an "official experiment" was probably a poor choice of wording. Some people thought that dep is a place to iterate about the final design and implementation, and them merge it into go tool, while making final CLI changes. Other people (/u/rsc among them [1]) thought it is a prototype to learn ‚Äì and then throw away. The first group is offended and frustrated about vgo, the second is excited ‚Äì yay, another prototype to learn from! Personally, I will continue to use dep, and continue to migrate all my projects which are not using dep yet to it ‚Äì for me, it is super-stable and useful right now. And I will try vgo and send feedback. And will drop dep when go tool will be able to handle dependencies natively, which will not happen soon (for my definition of "soon" :) ). My deepest thanks to everyone in the community working on those hard problems. [1] https://groups.google.com/d/msg/golang-nuts/PaGu2s9knao/Bq4vmFh7AgAJ 'I believe we're still very much in the "build and experiment and learn" phase, not the "polish and ship" phase.' But please read the whole piece.
&gt; i also have process concerns. vgo, as currently conceived, is a near-complete departure from dep. It was created largely in isolation from the community‚Äôs work on dep, to the point where not only is there no shared code and at best moderate conceptual overlap, but a considerable amount of the insight and experience gleaned from dep as the ‚Äúofficial experiment‚Äù is just discarded. I guess the concern is natural, but I don't think it's fair to constrain the language authors to follow dep necessarily. It's still not at 1.0, there's still no plan to get it merged into the main go tool and there are significant hurdles before it would seem ready for that (performance and stability seem often cited as major problems, for example). It might be nice to know what the insight and experience gleaned from dep is. I'm sure many things have been learned but - for example - how has the approach resonated with users? Especially users who were *not* using something like Glide before which had a very similar workflow since dep is much higher friction than go get.
Great stuff /u/rsc. Has there been any thought about building something for the go toolchain that publishes package and automatically assigns semver? Similar to how elm-package proposed semver by looking at the exported types land comparing changes from the previous version to the next https://github.com/elm-lang/elm-package#version-rules Very excited to give vgo a run for its money. Awesome work Go team and looking forward to where this is headed :-D
I really don't find TOML or YAML to be golang-y. json5, json, or rsc's solution is much better. 
Have you tried regex: https://stackoverflow.com/a/1732454 ps: This is a joke. I highly recommend reading the linked stackoverflow answer if you haven't seen this one before.
What hoops are you referring to? It seemed like a pretty straightforward setup.
&gt; Go already did go there No, it didn't. It did something *else* and arrived at a *different* point, otherwise we wouldn't have this conversation. &gt; why isn't this transformation implemented for value -&gt; pointer when the receiver is in an interface? I did gave an answer to this (on the danger of sounding repetitive): &gt; the answer is "because it is really convenient and not that surprising behavior in practice". It was decided that one thing would be too confusing and that the other wouldn't. Go is full of these kinds of compromises: It was decided (at least up until now) that full blown generics are too heavy-weight, but that generic containers are just too darn useful, so slices and maps and arrays where added as specially magic generic containers. It was decided that linear types to prevent dataraces are too cumbersome and too much overhead, but that types in general are just too darn useful, so we accepted that dataraces might violate type safety. And it was decided that a full blown formal type system is too much overhead, but that having to annotate everything with types is too cumbersome so `:=` and type-inference using only local information was born. It was decided that a backtracking parser and grammar for optional semicolons is too complicated to work with, but having to type them is too cumbersome, so lexical semicolon-insertion rules were invented (even though they are more limited in their usefulness). I could go on, but the gist is, that in none of these cases the decision was made out of technical impossibility, but because Go is an engineering language, made by engineers for engineers and thus there is a tradeoff to be made. Go only "went there", if you consider it a black-and-white decision: Either Go has full-blown references or it can't use anything that even resembles one. But that's not how Go is designed, everything is a tradeoff. References are totally possible to implement, but it was decided that a) we don't want them and b) we also don't want to have to type `(&amp;x).Foo()` all over the place. And what's more is, that one of the two only requires local information to disambiguate (the call syntax is used close to the definition of `b`), while the other needs non-local information to disambiguate (to know if `Foo(b)` modifies `b`, if you allow references, you would need to know the signature of `Foo`, which is defined far from `b`). Like, there even are very real and direct differences between the two. &gt; The technical reason is pretty simple really. If you allowed a pointer of a concrete type to point to a value in an interface, that pointer would be invalidated by a subsequent assignment of a differently typed value to the interface variable. *This is not true*. It has been demonstrated to you now in two different ways: Once by *actually doing it in the language right now* (just without a specific syntax for it) and once by sketching an addition to the spec that would allow for it. It would be really helpful if you are at least trying to argue why these things wouldn't work, instead of asserting that what has been demonstrated possible is impossible. Why would the addition to the spec I gave above not completely do what OP wants?
Oh my. I was just telling someone earlier today about Go's package management problems and said that dep (developed with input from the community) is here now and it will make things better. Now it's being replaced by a tool that was developed in Google and then I read: &gt;It was created largely in isolation from the community‚Äôs work on dep, to the point where not only is there no shared code and at best moderate conceptual overlap, but a considerable amount of the insight and experience gleaned from dep as the ‚Äúofficial experiment‚Äù is just discarded. Wonderful.
Cool thang to write scripts for Linux in go
Explicit is better than implicit. Even for security patch. An implicit upgrade can also be a security flaw
I've misread and wondered why are you *nuclear*. Twice ;-)
Thanks I missed that.
&gt; If you allowed a pointer of a concrete type to point to a value in an interface, that pointer would be invalidated by a subsequent assignment of a differently typed value to the interface variable. FWIW, this appears to be misunderstanding how interfaces work (in recent Go versions). Interfaces contain a pointer to a type-struct and a pointer to a value - and *always* a pointer, even if the dynamic value is not pointer-shaped. A subsequent assignment overwrites both, but the original pointer stays valid and continue to point to a value of the correct type. What you are saying is true, if you'd try to take the address of that *pointer*; but no one says that that's what you'd have to do to get what the questioner wants. The questioner was asking a semantic question, which boils down to "why doesn't `Foo(b)` store a reference to `b` in the interface, instead of creating a compile-error"; and that's perfectly and *obviously* possible, given that Go could also treat `Foo(b)` to mean `Foo(&amp;b)` instead, if it recognizes this case.
If we look at this from the perspective of something we would expect end users to do, I definitely agree. However, for something a developer does on their machine for convenience, it seems like a fairly reasonable setup. As far as I can tell, there would be two steps: 1. Install gorun 2. Tell the kernel about Go files using the echo command in the article Not too bad!
This looks like a great proposal and seems to work very cleanly in practice. One impression I get, which would be nice to have greater clarity on, is that: - $GOPATH continues to be used for storing dependencies (in $GOPATH/src/v) - $GOPATH continues to be used for storing build artifacts (including output from `go install`) - $GOPATH is no longer used (necessarily) for project code For example, your code could be in $HOME/go and $GOPATH could point to /tmp and everything would work (if inefficiently).
Not a package but a snippet from my point of view. Passing that code through gofmt would be great.
No, a straightforward setup would be something like `#!/usr/bin/env go run` at the top of your .go file, much like perl, bash, lua, php, ruby, python and countless others. That is to say, the shebang could be supported natively by the toolchain.
[removed]
It's only a single line per dependency in `Cargo.toml`, for example: hello_utils = "0.1.0" If you need to specify more than a version, it looks like this (still one line): hello_utils = { path = "hello_utils", version = "0.1.0" } So not sure where "it would take 3-4 lines" comes from?
The intention is to require major versions to use different import paths (except v0, for obvious reasons). So if v0.x.x - v1.x.x live in "host.com/pkg", then v2.x.x lives in "host.com/pkg/v2". (Search for "/v2" in the article)
Aw shucks... I thought this was going to be about the `mobile` package. (Still cool, though. Nice work!)
I don't know if you have an existing `go.mod` file or something else is going on. Can you share code?
I don't know what will exactly become vendor but for now I see a 'vgo vendor' command that put all the packages in vendor.
[removed]
We love simplicity and we all know that simplicity means often to remove something. Who are not happy when he can remove a big part of his own code ? We should not see the works on dep (and glide, govendor, gb...) as a waste of time, instead it's the most valuable experience that could make vgo born. Without java and c++ Go would never exist. So, I thanks a lot all of the people who worked on dep and others, i really see vgo as a direct evolution of their work.
And besides leftpad.go, there's also the story of code.google.com. Our company still has code from there in our `_vendor/` directory (which itself was created before `vendor/` became canonical, and before `go test` et al. started ignoring `vendor/` recently)!
For the very good reason of *because it [Just Works&amp;trade;](https://en.wiktionary.org/wiki/if_it_ain%27t_broke,_don%27t_fix_it)*.
there is a big difference in this matter for libraries and executables. The executable problem was sufficiently solved with the godep, vendor, ... etc.
I would love to see this. I'm reading through The Go Programming Language and 2 other books with the intent of making my own product. I would love to see what a good app structure is like for a larger Go program!
You can continue using dep\glide\govendor, what's the problem? JS introduces new tool every every or so it seems.
I suggest adding your explanation as an answer on stackoverflow. It would be good to have all points of view in responses - just for the future.
`//usr/bin/env go run "${0}"`
Yes, JS does seem to introduce a new tool every few minutes. That‚Äôs an issue. I have been billing Go as a more stable system. It still is, but developments such as this one make it seem to the uninitiated like it‚Äôs not much different. I do plan to continue pushing dep until the ‚Äútruly official‚Äù way to do it is implemented, but my team won‚Äôt like ‚Äúwe do it this way until the real way is decided upon‚Äù. 
Read the post you're commenting on :P Dep has been an official experiment. It will continue to receive support, but it's likely that the go toolchain will incorporate lessons learned from dep like those outlined in the original post. Dep has a lot of issues IMO, and is not very nice to work with overall. This new solution seems to fix several of those issues, but introduces more questions. Over time I'm looking forward to seeing how the new official tooling develops.
Yeah, that's right! In general, types should aim to represent what things _are_, conceptually. That is, `quantity` should be an `int`, not a string, because numeric operations are what makes sense for quantities: in fact, you want to check if it's more than one, which is a numeric comparison! (Note that this also goes the other way around: for example, telephone numbers and postcodes should be represented as strings, not numbers, because, despite most often being composed entirely of numbers, they're opaque identifiers; "adding two phone numbers together" is not meaningful)
would be perfect
I was looking at it from a team perspective, sharing scripts in a team, or even in an organisation only works if they have a homogeneous environment. If one developer runs OSX instead of Linux then they can't use this solution. Not saying the hoops are insurmountable, but it seems a bit strange to get someone to tweak kernel settings and download wrapper scripts to run a script. Python etc is no free lunch either, but most *nix/BSD distributions will have at least python 2.6, and as long as you don't have any dependencies you can share scripts with a shebang much easier and with less friction. 
I think that deprecating the vendor directory is a good decision, if and only if vgo is able to manage an "offline mirror" directory which would contain a copy of all the .zip archives used by the module. This "offline mirror" directory could be committed along the project source code. This is what yarn does for example: https://yarnpkg.com/blog/2016/11/24/offline-mirror/ 
What about the issue described in the discussed article (go run not returning the exit code of the execed script)?
&gt; In what way does implicit interface satisfaction simplify dependency injection? The main overhead in e.g. Java is having to define the interface in the first place and changing all of the code to use the interface instead of the concrete type, You can decide in a single module's scope to change to an interface and don't have to change any other module, and you don't have to plan out what interfaces some other modules might want in advance. &gt; But at some point, your Main() will become a huge unholy unencapsulated mess, and you will try to abstract some of it away, thus creating your own DI framework Actually, I've just been living with the huge unholy unencapsulated mess, because I have not found very much benefit in trying to abstract it away. I expect my abstractions to simplify my code, not just sweep the mess under the rug. A lot of my Go executables have migrated to a model where I put _all_ the mess into the main function where it's dealing with configuration and plumbing and such, and all my modules take configuration and injected dependencies to function. When trying to "clean up" the mess, I only do things that will actually _clean up_ the mess, not just move it around and hide it. So I use a lot of blocks to isolate the temporary variables and make it clear what survives through, and I may have some functions in the main module to abstract out common code, but I don't try to package up the dirt in cute little capsules and then scatter them hither and yon over the codebase, because in my opinion if anything that makes the code even worse in every way that matters. Especially as the capsules become more and more complicated and require more and more chasing indirection and learning some complicated framework which then brings its own problems. But I acknowledge this is probably a minority view. Possibly an idiosyncratic view.
I've got some internal libraries I use to make this easier on myself; mostly what they do is that instead of having error returns, they go ahead and panic, since in general that's what I want a _shell script_ to do if it encounters something it doesn't understand (I also always `set -e` in bash), and once you do that, Go is just slightly less convenient than Python at shell scripting. Are there any good libraries on github already? (Another trick I've used is that I defined myself a "directory" object, and I never use any change directory commands. I just use different directory objects to run commands in different contexts. Changing a directory is convenient for interactive use, but it's a poor tradeoff for a program, where if you want a particular context, well, just keep it and use it again.)
Just think about what they are. Say that Foo contains two ints, x and y in memory a []Foo will look like [x,y,x,y,x,y,x,y...] The x and y would all be contiguous, very fast to iterate over. The struct is also small so if you took a single Foo out of the slice and put it somewhere else, copying it will be fast. But suppose Foo was a very big struct. Now taking a Foo out of the slice means copying a very big thing, that is not so good. Also iterating over all the Foos to change 1 member of it is no longer so fast, because those members are no longer close to each other in the array, as all of the other members are in there too, and you churn through a ton of memory iterating through Foos when all you want to change is 1 field of Foo. Now with []*Foo the array in memory will look like: [pointer,pointer,pointer,pointer...] each pointer is a single word, pointing to a relatively random chunk of memory in the heap, that contains each Foo. Now to take a Foo out of the slice, you just pass the word (assuming that you want to pass a reference, and not a copy. If you DO want a copy then you need to dereference the Foo, so maybe you should have had a []Foo to begin with!) Now if you are iterating over Foos to change a single member, you can do so while churning through a lot less memory if Foos are large. So it is a question of semantics - do you want to work with copies of Foos, or references to the same actual Foo. Also a question of performance - which layout will perform best? Which depends on the size of the Foo and the hardware. As a default, when you aren't sure, use []*Foo, that is most typical. 
Great post! Kudos for having written this so quickly after Russ' post release.
Sounds interesting, might be a good read. You have my curiousity
Seems fun from a screwing around perspective, and seems like a cool thing go could implement so all the screwing around isn't necessary, but... good lord, this sounds like a terrible idea for a production environment as-is.
I'd love to read. I generally like reading about how people structure their application in "free-form" languages. Coming from Ruby on Rails I'm used to a conventional (and enforced) directory structure and file names so seeing people go out on their own to discover what works for them always makes for a good. But the Rubyist is me is hard to push away and so I searched and found Peter's recommendation and that's what I've been following. Here it is: http://howistart.org/posts/go/1/index.html
I guess it wasn't clear that I'm actively discouraging the approach. I was pointing out that the article isn't a straightforward setup, in comparison with non-go interpreters I listed. If the shebang was supported natively, then it would be on-par in regards to the utility of the standard shebang. Ie, if a suggested/standard shebang was used, it (edit: the intended result, ie pass the source to `go run`) would be clear. In fact, I'm definitely not sold on the whole shebang thing in any form for several reasons: - you have to configure an external thing (binfmt_misc) - you have to install and run a non-stdlib wrapper (gorun) - you have to install the go toolchain on all hosts In most cases, if delivery/updates are required, I tend to package things into docker images, allowing for an understood update procedure/steps. If docker is not an option, I literally build [binaries for the target systems](https://github.com/titpetric/pendulum/releases). In addition to what was said so far, people should also be aware that this shebang gorun thing doesn't follow best practices for laying out your code (ie, the lack of `cmd/` folder), and incurs possibly a significant runtime penalty in terms of the compilation step required to start each run. It's pretty simple to have a project with 10s+ compile step, which a hello world example will never demonstrate. As a fellow sysadmin, I use just about anything under the sun, but I don't feel the need to run go apps as I would do scripting languages, nor configure weird wrappers as the article suggests, to enable that. However, your last statements holds only part of the time, the build binaries may rely on local libraries (CGO), so copying them around with no dependencies might not always be possible. But that's why we have docker, right? ;)
/u/campoy looks disappointed with it haha 
&gt; You can decide in a single module's scope to change to an interface and don't have to change any other module, and you don't have to plan out what interfaces some other modules might want in advance. Oh, so in a situation like module A calls a function on module B; Module B used to take `ConcreteLogger`, and module A passed a `ConcreteLogger`. Now module B sees the light and decides to take `LoggerInterface`. Because of implicit interface satisfaction module A still works without change. That's indeed useful and a good point.
From what I understood, the `vgo vendor` command "exports" dependencies to the /vendor for backwards compatibility and is not a first-class citizen. It's a hack that even alters import paths making builds non-reproducible between different machines as a result.
That is true, but I've just got part way through a migration to dep at work. My co-workers and I were just getting used to the gotchas that Go has, and now we're thinking about the possible future gotchas that this new approach will have too. It's a little frustrating that this hasn't happened sooner, and it feels a little like now the problem was just being avoided for a long time.
We did the migration at work as well. And for several projects at once. Overcoming various issues related to kubernetes for example, https://github.com/golang/dep/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen+kubernetes. However vgo still promises: &gt; ..for a smooth migration from dep and its predecessors. So the migration isn't for nothing, it's still better than using glide as we did before. And hopefully some ultimate solution comes from dep and vgo so we can all finally be happy.
It was just a super detailed explanation but the actual configuration steps were just 2 lines of shell commands.
The slow speed of dep is kind of a pain when you're debugging a new Dockerfile, since you end up running it a bunch while you try to figure out the best order for all your COPY and RUN commands. I think it's made worse in my case by the slow speed of SSH here at work (they must be throttling something; git pull is so slow!). 
Are these changes to procfs persistent across reboots ? Might need to make those steps (to enable go scripting) into an init script.
Agreed. I have faith that this will be "the one", because this does seem much more likely to be officially integrated into the Go toolchain, as it's sort of already there in a way. I like how it feels, and am eager to see how some of the problems that have been identified with it will be addressed.
&gt; vgo has special cases to use those hosting sites' APIs to fetch archives. What happens here if I host my git repositories on a site which isn't "christined" by Google to use some special APIs? I like that those are supported. It allows huge websites like Github to do some special CDN caching every time you publish a new release. That's nice. But what if my module is a git repository normally cloneable at the URL `mywebsite.co/left-pad`? Does it use something like `git archive` to generate these zip files dynamically given a git commitish? I'd hope so, but the article seems to imply that vgo isn't even "aware" of git anymore, it just does zip over HTTP? Also, if my organization wanted to run our own CDN (say, leveraging S3) such that my git repository is hosted on `mywebsite.co/left-pad` but my archives are at `archives.mywebsite.co/left-pad`, does that mean all of the import paths now have to reference `archives.mywebsite.co/left-pad`? Is there even a way to do this, given that defining a mapping between commitish tags and archives seems like a problem that doesn't have an easy solution? Overall, its a great change. I think some of these specifics need to be outlined. 
It totally supports /vendor. See the tour post twords the bottom (https://research.swtch.com/vgo-tour) &gt; Even if you want to use vgo for your project, you probably don't want to require all your users to have vgo. Instead, you can create a vendor directory that allows go command users to produce nearly the same builds (building inside GOPATH, of course): $ vgo vendor $ mkdir -p $GOPATH/src/github.com/you $ cp -a . $GOPATH/src/github.com/you/hello $ go build -o vhello github.com/you/hello $ LANG=es ./vhello Puedo comer vidrio, no me hace da√±o. $ 
All the places i've recently worked at have come to the same conclusion. New up dependencies in main and inject with functions/constructors to where they are needed, keep the necessary complexity in one place. All the complicated DI "frameworks" (which _literally have books written on them_) do what you say, move complexity, not solve it. 
You misunderstood the caveat. It is not rewriting imports any more than the current go tool + /vendor does. Go compile with go1.10 with your existing /vendor directories and you will see the same thing. In fact he uses `go` and not `vgo` to compile with the vendor directory in the tour example. He is just pointing out that because the existing code does the rewrite for you the binaries won't be identical. 
In internal docs maybe, but golang is a lot nicer for search.
Every "Go Sucks" article that gets published has as its first bullet point a lack of package management in Go. It looked as if that was about to change. Now more uncertainty and lack of direction has been tossed in. Bah.
Thanks for clarifying. I think my point is that vgo being part of `go` command has the opportunity to reduce possible ways of building binaries to only one using `/vendor`. As for rewriting import paths due to /vendor, it shouldn't need to. The compiler could store a VendorBasePath during compilation time from which all non-standard packages would be imported from while still keeping `import` parameters intact in code. Also when I issue vgo build the packages are updated in $GOPATH/v and not in vendor. So vendor is not really an option during development. It seems to be aimed towards distribution for offline builds only.
I wouldn't call that "support". That's a mere `go export` if you will so devs can easily distribute code that will build offline.
I just use [the Git configuration workaround](https://github.com/golang/dep/issues/174#issuecomment-334578808) mentioned in the thread.
So this is basically a done deal?
&gt;As for rewriting import paths due to /vendor, it shouldn't need to. This functionality is what the `go` tool does today. It has nothing to do with `vgo` at all. The actual source code is never changing it's just a internal artifact on how vendor directories have always been done. Again vgo changed nothing here. 
Agreed but you'd think it would be "fixed" since Google is finally addressing package management in go.
Good post! There's only one bit I'm not convinced about here: &gt; Builds should be more consistent &gt; &gt; I doubt we can ever make builds 100% consistent, but the approach vgo is taking should avoid any of those painful issues that occur when you go to build and a new version of foo was released last night. That doesn‚Äôt mean you can‚Äôt get the latest version of foo, but you won‚Äôt be forced to use it unless you explicitly state you want it. This is what lock files are for, and given that vgo doesn't use a lockfile, and Git tags are mutable, it means that you're more likely to see an accidental breaking change between builds surely? At least with a lock file you would have to explicitly do an update to update the lock file? E: I'm genuinely trying to find an answer to this question. Not sure if the downvotes are just more trolls coming to this subreddit now.
Yes please!
Not sure if useful for anybody, I though I share it :)
https://golang.org/pkg/builtin/#int &gt; int is a signed integer type that is ***at least 32 bits in size***. It is a distinct type, however, and ***not an alias for, say, int32.***
Maybe some kind of aliasing system would be a good fit for this? It would be a little complicated though maybe. Sort of like what gopkg.in provides, but locally, built into the tooling? So that you could include multiple versions of a dependency if you wanted to, but under different names. A Go module could provide the alias and the `go` tool could then resolve dependencies of dependencies that use other versions of a package (like the AWS and Azure examples given) - so to both of those packages, they would use the canonical import name, but if they were exposed to your code, you could use them as the aliased packages if you needed to? That would have the same effect, but not mean duplicating loads of code, and handling different versions of the same library or project in the same tag / branch / commit of a repository. It's not an easy problem to get around, but I feel like this approach could get really confusing, when really I'd rather just have some stuff break when I upgrade my deps to new major versions and then have to update my code. I understand why the approach outlined is being suggested, because yeah, you're code wouldn't be left accidentally using something that has had it's functionality changed, and that's good and all, but how often does that actually happen? Don't tests help prevent issues with that too?
Just like'd to add that I really, really like this proposal. The fact that breaking backwards compatibility gets a little bit harder is a good thing. I also thoroughly love the simplicity of it. There will be some pain in the beginning as the community adapts, but over time I believe this will be a very good thing for the community.
Did you read the article? It explains that different import paths allows different mayor versions in the same program, and that it makes it a tiny bit harder to break backwards compatibility for package developers, making it more likely that they consider options to breaking bc.
/v2 does not necessarily have to be a directory. For example, it could be resolved to a repository tag like "v2.x.y". Import paths and file systems are two different layers of abstraction.
Why would the proposal lead to duplicated code?
Why would you move all the source over to v2? If you want code from v1, there's no reason v2 can't import v1, or that they both rely on the same internals package.
You don't use new directories, you use new branches. Or if you don't intend to release a new v1 tag, just start tagging v2.0.0. The v2 in the import path is virtual and interpreted from the intent for release. Think how gopkg.in works today.
Creates an svg img from text to show how something looks like on a terminal. Saved you a click. 
Why would they contain two different versions of the same code? If v2 requires some code in v1, it can import and use it. No need to duplicate it.
If you were to change a function slightly, but some parts remained the same, or say, only a return type changed or something, you'll likely need to have 2 very similar functions in one repository. Or more than 2 as you add more versions if they also need to change it. Maybe you can split it up so common logic is shared in some cases. Maybe not. 
No idea w/ downvotes. I'm happy to discuss this further, but I'm a little swamped right now. The TL;DR is that ultimately I think this ends up being a problem with where official versions are stored not with `vgo` itself. eg ruby has rubygems, and many other languages have something similar that doesn't allow you to to push changed code for the same version. Git means users get to host this information on their own, but it also means they can break those rules more easily.
Yeah, I do agree with you that it doesn't necessarily need to be something that `vgo` would handle. I do feel like Go needs some kind of central package repository though. The problem now is import paths I suppose. Would/should they change if a central package repository was introduced? Maybe... maybe not. But if they didn't, it may not be clear that you were using some central repository in the first place. For people that really _need_ it, they can use some kind of a proxy I suppose, which is at least some kind of solution.
No, the required v2/ must actually be in the import path. In GitHub repos, that means a separate directory.
This is not correct. In e.g. GitHub repos, you do actually need a separate v2/ directory. - github.com/user/module ‚Äî package module, v0.x.x‚Äì1.x.x - github.com/user/module/v2 ‚Äî package module, v2.x.x - github.com/user/module/v3 ‚Äî package module, v3.x.x
This is *really* important and /u/kardianos thought was how I thought it worked. Are you sure that's not correct? Would love some direct clarification.
Very well written. Had a chuckle at this: &gt; Ugo makes it home on time. He‚Äôs not terribly happy with his package manager, but he‚Äôs now a big fan of StackOverflow. 
I think the idea is that you would use subdirectories, so that you could include 1 version of a dependencies repository that contains all versions that your code _and_ that other dependencies require. Like in the example, that would enable some library to use a `v2` import path, and specify that new version, and an older lib that used the original, root-of-the-repository-v1 version would continue to work, because that would exist in the v2 repository, in the same place. The different `v2` path would be explicit for your imports. It doesn't look like you'd actually be able to use 2 different versions of the same repo, but aliased (though, as I've commented elsewhere, I think that could be a nicer solution that puts more of this onto the tooling, rather than the developers writing packages, and using packages maybe).
The explicit requirement is that e.g. v2.x.x of a package has a separate import path than v1.x.x. It's still _possible_ to constrain a dependency that doesn't fit this model in your go.mod, but it means you'll have to use the so-called psuedoversion form of the constraint, "v0.0.0-timestamp-revhash", which isn't parsed for semantic version meaning.
This doesn't bother me too much, beyond the thought of "what about every existing go package that didn't follow these rules and already has multiple versions"?
In theory any interpretation of the string is possible. In practice as documented and implemented in `vgo` the extra directory is required.
I would probably prefer to just push to separate repositories (like github.com/foo/mymodule.v2) for each major version rather than adding a v2 inside of one repository. If you base v2 on the v1 repo history it will also be a lot easier to merge between v1 and v2 than if thats beneficial to the project workflow. Some people are already doing this on github and it seems to work well. This could essentially also work used as one branch per version where you have multiple worktree branches checked out at once while developing (not really compatible with how go get works right now). I really don't like the idea of always having to have v1/v2/v3/v4/v5.... checked out at once when working on something. I am too often opening files in the wrong version when working on projects like that, source code searches bring up a lot of deprecated code etc. etc. 
Just had a thought about this semantic import versioning... if you wanted the end of your import paths to still match your package, you'd need your package structure to end up being something like below, right? unity/ v2/ unity/ unity.go unity_test.go go.mod (does this exist here too? not sure) unity.go unity_test.go go.mod Of course, you could not structure it like the above, and use it like: package main import "github.com/you/unity/v2" // package name is actually "unity" still func main() { unity.Foo() } I'm not sure I like this really, this whole approach seems to leave many things up to the interpretation of package authors. I can see us ending up with some really inconsistent looking packages across the board. On top of that, a split between existing libraries that use tags like every other language will then be different to new libraries that use this new approach (or some variation of it). I like the idea of using a minimal possible version. I like the speed and simplicity of `vgo` versus dep. But I would still prefer a lock file too to lock to specific commits (Github tags are mutable). And the vendor folder is still too important to give up for people who don't want to host a proxy.
We *really* need a clarification on where the source for `rsc.io/quote/v2` would actually live. Subdirectory of the same github repo? Different branch of the same github repo? Same `master` branch, found via tags? Manually created vanity import rule directing to a different repo? How does that interact with pre-vgo `go get`? Also, I'm looking for *intent* not *current vgo implementation*, so it probably has to come from /u/rsc himself.
There's nothing in this series of posts that says the v2.x.x of a package must live at a path that ends in /v2. All it says is that the import path must be distinctly different than the import path of v1, and he proposes that a common way of doing that might be to append the import path with a /v2. Also: There's nothing which says your _entire_ package has to live at /v1 or /v2. I could imagine it might be common to have your exported API live at /v1 and /v2, but have both of those packages call into a, say, /internal package to share as much internal logic as possible without breaking the API contracts. Or, the /v2 could rely on /v1. &gt; this is pretty clearly untenable. The funny thing is, the posts have practically no "opinions" in them. Its hard fact. What you're interpreting as untenable is the **reality** of dependency management. Its a fucking hard problem. In some ways, it is intractable.
how would tooling know that these separate repositories are different versions of the same thing and not separate things that happen to have a common prefix and disjoint version sets?
No, but there's a file where you drop the line and your init does binfmt registering automatically.
This is, too, my biggest fear. Coming from the verbose Java world, there's a way to do things and it's pretty clear what it is(for modern web applications, at least). I'd like to do a Go app.. But outside of the microservices I've written, I have no clue how not to make spaghetti
As others have said, int and int32 are not the same, and that's why you have both. Most of the time you want a reasonable int. Other times you want an int which truncates at 32 bits. You mention C++. C and C++ have the same destinction. In C/C++ 'int' is a reasonable integer size on the target CPU, and int32_t is a signed integer with exactly 32 bits.
&gt; All [this post] says is that the import path must be distinctly different than the import path of v1, and he proposes that a common way of doing that might be to append the import path with a /v2. Yes, and `vgo` encodes this convention, and breaks when it's not followed. So, &gt; There's nothing . . . that says the v2.x.x of a package must live at a path that ends in /v2. At the moment, yes, there is. ‚Äì &gt; Also: There's nothing which says your entire package has to live at /v1 or /v2. I could imagine it might be common to have your exported API live at /v1 and /v2, but have both of those packages call into a, say, /internal package to share as much internal logic as possible without breaking the API contracts. Or, the /v2 could rely on /v1. Yes, these also seem like possibilities, at first glance. The latter would lead to some pretty fucked-up dep graphs, though!
Surprised no one mentioned that int usually has 64 bits on 64-bit systems and 32 bits on 32-bit systems. The distinction from both int32 and int64 is clear.
Do you also give your emails such descriptive subject lines? üôÉ
Since v1 and v2 will have breaking changes it progably doesn't matter too much if the system known if it's the "same" module or not. (I hope I understood what you were commenting on)
Yes, I agree. In the golang-nuts thread Russ seems to confirm my interpretation, but also promises a post tomorrow with more detail, so maybe we wait for that.
For example, if you have one repo with multiple /vN directories the tooling can alert you that a new major version has been published but if they're separate repos you'd need to find that out on your own.
Awesome! Just two questions: * Is the xhyve driver required? Docker for Mac comes with HyperKit already. * Why would I want to curl-install minicube (and miss out on updates) when a Homebrew formula [is available](https://github.com/kubernetes/minikube)?
Right. The version v2 at `foo/bar` is current not consumable by `vgo` users, unless they use a pseudoversion constraint and give up the semantic part of semantic versioning.
OK, then I read your comment right, thanks. This does make me uneasy, as it means that I'm suddenly breaking the rules that might become common sooner than later. Surely there must be a plan or workaround for this, otherwise the idea that vgo should offer a smooth transition isn't really true.
Can we have a follow up article that shows how to use a database in combination with kubernetes + go app? I find this the hardest part.
wondering why does vgo try to download every package in my $GOPATH/src when I run `vgo build`? NOTE: I tried `vgo build` inside `$HOME/hello/hello.go` as mentioned in the Tour. and my $GOPATH is $HOME
With vgo import paths are no more equal repository URL but they are a combination of the URL and the semver tag.
You mean that's how it should work? Because, as far as I understand, vgo currently does not allow v2 and up without the version being in the import path.
Different repositories means (at least with current GitHub tooling) different issue trackers. You will get issues filed in the wrong one. And migrating them from one to the other is really painful.
Nope. The key here is the `module "interpret as path"` directive. In go.mod `module "github.com/user/repo/v2"` Then when you pull that import path, it will use that module path as the root. I'm 90% sure of this anyway. 
The way I understand it, when you do `vgo get foo/bar` your go.mod will have `foo/bar v2.0.0` because that's the latest version. If you need to do breaking changes then you need `foo/bar/v3`. But I am not sure. I hope Russ can explain.
yeah, I would probably either disable the issue tracker for all but one of the repos. Kind of like the Go repos does it.
I've tested it. https://github.com/kardianos/vmain https://github.com/kardianos/vtest 
This is an awesome, novel (AFAIK) idea. 
Basic networking in Go [is quite easy](https://appliedgo.net/networking/) and fun. For project ideas, check the available [networking](https://awesome-go.com/#networking) and [messaging](https://awesome-go.com/#messaging) projects, or do a [GitHub search](https://github.com/search?o=desc&amp;q=network+language%3Ago&amp;s=stars&amp;type=Repositories&amp;utf8=%E2%9C%93). Only refrain from building another HTTP router, there are already tons of them ;-)
Oh! Great! How deeply confusing! But at least it's an improvement over the stricter constraint. I'll edit my posts.
What sort of networking stuff are you looking to learn? You could build a toy Redis clone using TCP sockets. Or a chat system using websocket or TCP sockets. You could start with 2 users if that's easiest and then try a &gt;2 user version. You could build a simple [DHT](https://en.wikipedia.org/wiki/Distributed_hash_table) if you're looking to learn about p2p style networking. Any of these the kind of thing you're looking for?
This is fantastic. I enjoy all your videos, but the code review episodes are especially helpful to me.
If that is correct, it is the same to use int and int64 on 64-bit systems? 
No. On 64-bit systems, both types allow same sets of values, but are different in the Go type system and values of one need to be explicitly converted to the other.
Why not use https://golang.org/pkg/net/http/httputil/ over https://medium.com/@mlowicki/http-s-proxy-in-golang-in-less-than-100-lines-of-code-6a51c2f2c38c?
According to [peterbourgen/vmain](https://github.com/peterbourgon/vmain) &amp; [peterbourgen/vtest](https://github.com/peterbourgon/vtest), the library `go.mod` file must have suffix `/v2` in the module and a git tag for `v2.x.x`. No extra `/v2` directoy is needed. And lib users must specify `/v2` in import paths. Regular `go get` fails as it is looking for `/v2` as an actual directory. 
Very clever, indeed. I don't know if it'd be a nice way to make a "real" program, but as @014a said it's very novel. Thank you for contributing this. :-)
Seems like a more complicated way to do `select{}` except without all the annoying runtime integration.
I hate these "screenshots". Why not create some CSS and use actual text so that you can copy and paste from these snippets.
You really don't want to do this. It's basically an [inner platform](https://en.wikipedia.org/wiki/Inner-platform_effect) that reimplements the Go runtime itself inside of itself, only not as well as Go reserves some features for itself in a way that doesn't permit pure Go code to re-implement. I hate to be negative like that, but my motivation to share this with you is that your experience with Go is going to be poor if you don't grapple with the features Go itself implements, such as the stuff in the sync package, the context package, and all the other things that will integrate into the runtime and the ecosystem better, and, well, fundamentally that's going to be an unpleasant message to hear after all that work. On the positive side, you've certainly learned a lot of things which will be useful. I'm pretty sure you've got some race conditions in there, too, but rather than enumerating them you're really better off just writing Go.
My understanding with go is that it was built with web applications in mind, thus no need for a framework. Then it seemed like everyone and their mom made a framework. I'd like to see what you did.
Clarifying further... If [github.com/peterbourgon/vtest](https://github.com/peterbourgon/vtest) has [a `go.mod` specifying](https://github.com/peterbourgon/vtest/blob/master/go.mod) module "github.com/peterbourgon/vtest/v3" as well as [a relevant e.g. `v3.0.3` tag](https://github.com/peterbourgon/vtest/releases/tag/v3.0.3) on the repo, and two sub packages [foo](https://github.com/peterbourgon/vtest/blob/v3.0.3/foo/foo.go) and [bar](https://github.com/peterbourgon/vtest/blob/v3.0.3/bar/bar.go), then in order for package foo to import package bar, it must specify import "github.com/peterbourgon/vtest/v3/bar" That is, it must include the vN fragment in the import path at the same "level" so to speak as the go.mod. This is... not great.
This is very clever approach! 
&gt; Since the package manager prefers the oldest versions of my dependencies, the only way to try out the latest B and C is to tell it to upgrade them. That overwrites my build list to require the latest versions of B and C. That's the same manifest that is used by consumers of my package to pull in A's transitive dependencies. Upgrading is something that needs to be released, and your consumers won't upgrade to the new release unless they intentionally do so. That is, if you control package A and upgrade its dependencies on B and C you would then need to release a new version of A before those new requirements are even available to users, but that alone isnt' enough to push those changes to consumers of A. Users of A would also need to manually upgrade their version of A to your newest version before they would start to see your dependency requirement changes on B and C. As a result, you can't ever force that behavior on consumers of A - they have to actively upgrade A before those changes will be taken into account. Your point about testing is slightly different. You are right that the only way to test newer versions of B and C is to tell your package manage (`vgo` in this case) to upgrade and then run your tests, but I'm honestly not sure how this is any different from other dependency managers. `dep` + a lockfile won't automatically test different versions of your dependencies - you will need to create a script or something to change versions of B and C to the ones you want to test with, run tests, then verify that they pass. In both cases - `vgo` or `dep` - you could then just choose to not commit those changes to either the `go.mod` or the lockfile and simply log the results of the tests. &gt; For example, let's say my app uses A which uses B. My app does not use B directly. I don't even know what B is. (Seriously, take a look at the list of transitive dependencies for one of your apps sometime. There's probably all kinds of stuff in there you've never heard of.) But I am at a good point in my dev cycle where I want to get my app onto the latest and great of all of the packages it uses, so I do upgrade all. That adds an entry for B to my app's build list. What is B? Why is that in there? &gt; &gt; Later, I decide to stop using A and remove it from the build list. B is still in there. Should it be? Where did that come from? How do I know when I can safely remove it? This is a good point, but I also suspect it is one that could be handled with `vgo` pretty easily. Eg if you run a build with `vgo`, and it notices that you have a requirement in your module that isn't actually being imported in any of your packages it could simply give you a warning (or if we wanted to be more aggressive - it could give you an error similar to the unused variable error that the Go compiler gives because this seems like a similar scenario to me).
&gt; As a result, you can't ever force that behavior on consumers of A - they have to actively upgrade A before those changes will be taken into account. Good point. &gt; I'm honestly not sure how this is any different from other dependency managers. dep + a lockfile won't automatically test different versions of your dependencies - you will need to create a script or something to change versions of B and C to the ones you want to test with, run tests, then verify that they pass. With a lockfile, I can upgrade the locked dependencies for A which it uses in its tests without touching the *declared* dependency ranges it supports. When you depend on a package, only its manifest comes into play, not it's lockfile. So the maintainer of the package can freely upgrade or otherwise modify the lockfile without change the contract the package it exposes to consumers. &gt; I'm guessing most people would prefer this to be a warning, but part of me feels like this is similar to the unused variable error that the compiler raises and could be a hard error instead of a warning. That handles cases where a transitive dependency is not needed at all any more. But what about cases like: 1. I depend on A and am happy to use any version later than 1.4. 2. I also depend on B, which also uses A. 3. I want to locally test that my app works with the latest B so I upgrade it. 4. The latest B requires a later A, so it updates the version of A in my app's build list to that newer version. As far as I can tell, the initial supported version I deliberately chose is now lost for good. I guess I just don't understand why you're putting all of this effort into avoiding lockfiles. They're great. Having a separation between human-authored intent and machine-generated output seems like a no-brainer to me.
&gt; Delve is a debugger for the Go programming language.
I appreciate the criticism. In all honesty: I mostly made this to scratch a curiosity itch I got earlier today. Though I do think it has some uses. Considering I made this partly out of a desire to put some ideas into a common location, I'd be curious as to how you would recommend using parts of the Go stdlib to do the same in a way that integrates with the runtime and ecosystem better. Considering I'm using at least some of those things you mention, I'm definitely trying to grapple with those features. (though I'll be the first to admit I still have a lot to learn) I think the main things I'd be looking for some advice with would be: * Easy way to "send a message" to something that won't block the sender. It's preferable the message gets to its recipient at some point. Technically I can do something like "go func() { c &lt;- msg }()", but there is decent overhead with starting goroutines willy nilly, hence why I have a slice in my implementation. Though it's not perfect, since it uses interface{} to "get around" the lack of generics. * An easy way to stop something that receives messages. The usual way I'm aware of for this is to have some kind of message that stops the receiver, or closing a channel. And I know that closing a channel can work well in some situations, but that it has to be done carefully at times to ensure nothing sends on it after the close to prevent a panic. * Technically I also wrap around the ability to receive messages in various ways, including a way to stop after a specifiable period of time, (which is a real use case I've used at a job) but that isn't really that hard to write in a few different ways, just slightly verbose at times. * Also: Is there anything that I should have used when writing this that I didn't? Like I said, I still have a lot to learn, so if I can maybe learn some more of that even if I'm implementing something you think is a poor re-implementation of Go's native capabilities, the I'll at least have learned that skill to apply in the future. I'd also be interested to hear what race conditions exist. I thought I was being pretty careful about locking around some critical sections of the code. I know that I don't lock accesses to the running flag, but figured it was largely something that, as long as it eventually sees that it's been stopped, it was fine. Nothing ever sets it back to running once it's been stopped, so I don't think locking would really do much beyond slow it down. As a side note: when I was looking into ways to implement the code, I found the sync.Cond primitive, and deliberately chose to not use it due to finding comments on the Github from the main developers of Go that it isn't something that should be used, and to use channels instead.
&gt; So the maintainer of the package can freely upgrade or otherwise modify the lockfile without change the contract the package it exposes to consumers. I believe what you are saying is that if I have package A I can update its lockfile to `v1.2.0` and run tests to verify it, but I can keep the manifest to specify maybe `&gt;=v1.1.2`. With `vgo` I suspect this process would instead involve making a change to your `go.mod` to upgrade the version, running new tests, then reverting those changes (with git or another VCS this is trivial). How this is done is different in each case, but both are achievable with roughly the same amount of work. I suppose the only real difference is that with a lockfile you could commit those changes, but I honestly don't know if that would be smart. If you advertise your package works with `&gt;=v1.0.1` it would make sense to me to always have the tests default to running with `v1.0.1`. &gt; The latest B requires a later A, so it updates the version of A in my app's build list to that newer version. I don't think this is true (but need to verify to be sure). As I understand it, if you were to add a newer version of B to your requirements, A's new version requirements would be determined at runtime (and the newer version would be used for this build), but A wouldn't have to be added to your `go.mod` file because that updated requirement is inferred by looking at the `go.mod` for the new B. &gt; As far as I can tell, the initial supported version I deliberately chose is now lost for good. Again, if I wanted to upgrade something to see if it works then roll back, I would imagine myself using a VCS, starting with a clean branch, and then throwing away any changes once I have run my tests. &gt; I guess I just don't understand why you're putting all of this effort into avoiding lockfiles. They're great. Having a separation between human-authored intent and machine-generated output seems like a no-brainer to me. I'm not working on `vgo`, and I don't have any issues with lockfiles personally. They are super useful in traditional dependcy management strategies, as they fix the problem of not knowing specifically what version of a package will be used for a build until you check to see what new versions were released. I do believe that in the `vgo` scenario lockfiles are less useful because the version of each package you are going to use in a build doesn't change when a new version is release for a dependency. ie with traditional dependency management if your manifest says "I need `&gt;=v1.0.0` of X" then you don't know what version of X will be used until you look up what the latest version is. A lockfile pins that down. In `vgo` you DO know what version will be used without looking to see what new versions were released, so you don't need a lockfile to pin it to a specific version for consistency. It will already be consistent across builds. 
Buffered channels can still block if the are full. I'm making a deliberate tradeoff where the slice can become larger if the consumer isn't keeping up with sends, versus blocking the send if the consumer isn't keeping up. Though, realistically, if the slice continues to grow, there could be some gnarly delays if a large slice needs to reallocate and copy the contents. I might introduce a hybrid linked-list where each node contains a slice of some size, so that increasing the capacity will be a fixed cost, but speed isn't hit too hard when accessing the elements because it will only need to pointer chase nodes occasionally versus for every entry. Thanks for getting me to think about that!
Thanks for this. Nice post. 
research.swtch.com/vgo-repro describes a start, if this kind of thing is important to you. (Maybe also worth noting that downloaded code is cached, so it's not like you refetch the same tag over and over.)
Lots of discussion here about code being forced into v2 subdirectories. Sorry that was not clear. The import path needs a v2; the code can be in the root of a v2-tagged repo. A post tomorrow will clarify this. If I had this week to do over again, I would have the followup posts ready sooner. :-) 
Your program imports rsc.io/quote. You get the newest version of that. You get the rsc.io/sampler that rsc.io/quote built and tested against. This is by design. The point is that you don't get the untested, buggy 1.99.99. I don't make much distinction between 1.3.1 and 1.99.99. They're both "something newer than what rsc.io/quote has been tested with".
The Go answer is: rearchitect the code so that sender can block. If you really want a waitable queue of infinite size that doesn't block "senders" then you can do it by having a mutex-protected slice that keeps "sent" values and use sync.Cond. The "sender" would Signal() or Broadcast() after appending values to the queue and "receiver" would Wait() and then process all elements from the slice until empty and then Wait() again. 
Much appreciated work from delve team.
That's a fair enough answer :) Like I said earlier, this has been an experiment to scratch a curiosity itch, but which I think/hope has at least some value. I could be wrong. And the correct thing could be to do exactly what you said is the Go way. But at least I had fun working on it :) As to your point about how to implement the features I mentioned above: you described what my code does, but without using sync.Cond. As I mentioned above, the Go maintainers don't think it should be used for the most part (https://github.com/golang/go/issues/9578#issuecomment-69763827), and in order to implement a "Receive" function that could return after some duration I needed to use a channel with select.
So many typos. That's how üôÉ Hopefully I'll have time to fix them tomorrow.
[removed]
So `vgo` now has a lock file like `dep`, except it stores checksums of archives of instead of commit hash.
[removed]
It also doesn't actually lock anything.
[removed]
&gt;The h1: prefix indicates which hash is being reported. Today, there is only ‚Äúhash 1,‚Äù a SHA-256 hash of a list of files and the SHA-256 hashes of their contents. If we need to update to a new hash later, the prefix will help us tell old from new hashes. This is super smart. "SHA1 is no longer considered secure, and has been considered weak for a long time (about 13 years). Bruce Schneier warned in February 2005 that SHA needed to be replaced. Git development didn't start until April 2005. Before git had started developed, SHA was identified as needing to be deprecated. " http://blog.zamicol.com/2018/02/do-not-use-gits-built-in-gpg-signing.html
I took that up to myself and did a project, implementing an IRC server. I'm just reading up on the RFC: https://tools.ietf.org/html/rfc2812 and then figure out what I could best use to get it working. A couple features are working so far and I spend time after work/weekends to add some new features when I can. It sure is a lot of fun. Good luck.
[removed]
Go has so much goodness its package management crisis is almost excusable.
Great post. That dep did not look like it would finish. 
Yes, it just complains of mismatch.
Almost.
Lock file doesn't protect you from accidental depency files changes - it merely points to the node in vcs tree. You can still change the files inside vendor directory and there is no way to detect this without either committing vendor directory or downloading the whole tree of required modules again (if I understand correctly input-digest from the dep lock file). The whole point of vgo is to be VCS agnostic - modules are somewhat decoupled from their storage in VCS. As long as you serve them using http/https - your end user should not care about if you use Git, Mercurial or something more exotic. Even home grown version control system - which many of the big companies do have. 
A word to the wise - your hacky seo tactics are actually working against you at this point. The value of a reddit backlink takes years to manifest unless it immediately results in a significant traffic spike, and is only as valuable the keywords you rank for. With the most recent release of Google Search updates your "article" is almost certainly going to be flagged and cause negative ranking for the intended site. Combined with the massive amount of competition for freelancers in search I'd say your site will rank somewhere around the thousandth page of search. A quick look at your profile reveals that you contribute nothing to any reddit community other than spam. It's sad because it's ineffective. If it was 1999 you would be rolling in search traffic, but...it's 2018. It's harder to game the system at this point than it is to produce content that people want.
For those who want an HTTPS proxy with some added privacy features, check out https://github.com/caddyserver/forwardproxy (not written by me) - I know people who are successfully streaming YouTube at 720p behind GFW. The more people who use and test it, the better it will be!
&gt; Buffered channels can still block if they are full. True, but in the end, every resource is limited, and I see no value in abstracting away such limits. Every code that utilizes a limited resource (and a messaging facility obviously is one) has to consider the case where the resource gets exhausted. And growing channel buffers as needed comes with a performance penalty. Considering that goroutines aim at making thinks faster, a slow messaging facility would seem counterproductive. 
!RemindMe 3 weeks
I will be messaging you on [**2018-03-15 06:43:50 UTC**](http://www.wolframalpha.com/input/?i=2018-03-15 06:43:50 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/golang/comments/7z5anr/writing_an_article_on_web_application_structure/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/golang/comments/7z5anr/writing_an_article_on_web_application_structure/]%0A%0ARemindMe! 3 weeks) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
So, I really like that it doesn't assume an immutable internet, but I'm not sure what I do if I have a mismatch. I notice that my CI has failed with a mismatch. What do I do now? I can either hit the "override" button, in whatever form that takes, or I can just give up? 
You can begin by verifying if the sources have actually changed or if something suspicious is going on. If the sources have changed, are the changes acceptable to you? You can answer these questions and then decide what to do next.
Regarding the index time in Goland, I found the new feature they added to not index the entire GOPATH to be a good performance boost 
&gt; TLSNextProto: make(map[string]func(authority string, c *tls.Conn) http.RoundTripper), I wonder if it would be worth adding var TLSNoProto = make(map[string]func(string, *tls.Conn) http.RoundTripper) to net/http package.
About kardianos/vmain and vtest: - I agree, it's better than what I thought. It makes possible to use a "normal" git tag to mark a major version, for example v2.0.0, without having to copy the whole project in a "v2" subdirectory. - At first look, the mechanism is a bit confusing though. Maybe it will become clearer over time. - Out of curiosity, how are you aware of this "experiment" at kardianos/vmain and vtest?
Oh, I must have missed it somehow.
I really want to use 'tcat' now.
*beep beep* Hi, I'm JobsHelperBot, your friendly neighborhood jobs helper bot! My job in life is to help you with your job search but I'm just 157.4 days old and I'm still learning, so please tell me if I screw up. *boop* It looks like you're asking about job search advice. But, I'm only ~25% sure of this. Let me know if I'm wrong! Have you checked out Forbes, LiveCareer, TalentWorks? They've got some great resources: * https://www.forbes.com/sites/karstenstrauss/2017/03/07/job-hunting-tips-for-2017/#794febea5c12 * https://www.livecareer.com/quintessential/15-job-hunting-tips * https://talent.works/automate-your-job-search
Wrong!
RemindMe! 3 weeks
[removed]
A new bie with this digital world....May I know some great techniques that could boost my traffic as well as Keywords ranking. I feel that you are quite good at SEO Tactics and much. With some word of advice share some precious knowledge too. May be it could be beneficial to me. An awaited reply....
I used Goland through the free early access (when it was Gogland) and greatly miss it, no way am I spending that amount on home projects though. I use Webstorm at work on Node.js projects, so really got on with the interface. Somehow I've ended up on Goclipse (Eclipse + Go plugin) and I'm just about content, it's not great though. Didn't like VS Code or Atom :(
And yes also tell me. How u contributed to reddit? And how actually u started. I also took a quick view on your reddit profile. It's been three years to build such anatomy Tell me how should I get started
Agreed!! Delve has been fantastic and I look forward to it only getting better. üòé
Even with all the work put in, I could never get an answer on whether all the non-standard library dependencies needed to be replaced. Between Bolt and toml, yaml, etc, there is an awful lot without any apparent concern for how to resolve that issue so that dep could become mergeable into an official tool.
I really didn't think it worked like this, but it actually (sort of) does! It doesn't do the automatic translation to the `v2` path, you have to include that in your `go.mod` file yourself, but it does _use_ it automatically. It's not a sub-directory! It's been a bit of a contentious point in the office where I work, because some people are saying it's a little confusing looking. On top of that, this new approach would actually prevent old packages from working, surely? If the libraries all have to refer to their own imports as `v2` or whatever, but that package doesn't actually exist outside of what `vgo` creates, then that means tools like `dep` won't be able to work with these dependencies anymore? As new libraries are introduced that use this method, people would be _forced_ to switch to `vgo` (or... `go` in the future of course) or they'd have to find a different library.
Dep to the rescue. But agreed, it's one of those things, if say Guido Van Rossum or Matz created Python or Ruby back in 2009 when Go was first released, there's no way in hell they wouldn't have included dependency management right out of the box. 
&gt; you have to include that in your go.mod file yourself What do you mean? I was under the impression that if I do a `vgo build` then the v2 will be added automatically in the `go.mod` file. Is there a manual step I have to do? &gt; people would be forced to switch to vgo Assuming vgo becomes part of the toolchain I don't think it will be too hard to add a special case to `dep` for when it sees `/foo/bar/vX`. I think it would be nice to have a "super safe" official manager and keep `dep` around for people who want a cargo-like experience. Of course it would be nice if the whole ecosystem used the same tool (like `gofmt`) but people are people.
Not currently.. No official word from rsc, but there has been tests for this instance. https://www.reddit.com/r/golang/comments/7z7fdy/researchrsc_semantic_import_versioning_go_amp/dumnnkl/
I took a look at Delve a few years ago and thought it was a neat idea but very fiddly as it was very early and not many environments/IDEs were around back then that could make use of it. I tried it out the other day in VS Code and it blew me away, it feels so natural and easy to use! Congrats to the Delve team on the 1.0 release, looking forward to hearing what comes next!
At my place, go is used a lot for "scripts", as in "administrative tasks our servers run nightly". It's great because it does not break on updates as the executables are all self-contained, and you don't need to ask root "hey, can you install package/module xxxx on the server plz?" Plus, static typing makes updates much less scary.
[removed]
&gt; What do you mean? I was under the impression that if I do a vgo build then the v2 will be added automatically in the go.mod file. Is there a manual step I have to do? Sorry, I'm not being clear here really. I mean, the `go.mod` file will end up with a `"foo.com/bar/v2" v2.x.x` entry in, rather than just a `"foo.com/bar" v2.x.x`. It's an opt-in sort of deal. You could either add that yourself, I guess (?) or more likely you've used `vgo get` somehow to fetch it (whether directly asking for the package, or from `vgo` figuring it out from your existing imports in the code). &gt; Assuming vgo becomes part of the toolchain I don't think it will be too hard to add a special case to dep for when it sees /foo/bar/vX. I think it would be nice to have a "super safe" official manager and keep dep around for people who want a cargo-like experience.
https://blog.golang.org/share-memory-by-communicating
But would they have included a JIT or compiler? 
&gt; Lock file doesn't protect you from accidental depency files changes - it merely points to the node in vcs tree. You can still change the files inside vendor directory and there is no way to detect this without either committing vendor directory or downloading the whole tree of required modules again (if I understand correctly input-digest from the dep lock file). This depends entirely on the implementation. Cargo has a lockfile, and supports vendoring. If you modify the vendored code, it will fail to compile. This works because the lockfile contains the hash of your dependencies, and will verify that it's still correct before building.
I like go.modverify because it ensures that the both ways work. It also ensures that CI caching works too. 
Does it offer anything different than the viper project?
What if I want to make some local patching on my dev machine? 
Probably not and I am not asserting that Python or Ruby are better than Go (fwiw, I like all 3 languages). However, my point is that the Go folks could have taken a look at Ruby, Python, Java, etc and realized early on that dependency management was an important problem to solve. I love Go, but I am not one to ignore warts in any language. I am happy about dep.
They did do that but didn't like the way ruby/python/java did it I guess. I'm not saying they are right or wrong, I don't really have an opinion. I don't tend to use nearly as sprawling a library set as most people so it isn't that big of a deal for me to deal with. I wonder if that is part of their motivation? Seeing how the simplest little react project turns into 200 node_modules is kind of horrifying sometimes. C#'s system has always been nice to use though and doesn't seem to inspire quite as much insanity. 
This sounds right to me. Just that I believe it's optional behaviour. I think you _could_ import v2.x.x of `foo/bar` as just `foo/bar` if you wanted - though I'm really not sure about that yet. In fact, now I think about it more, I hope I'm wrong about that part haha.
If anyone's interested, I made a couple of examples of [worker queues](https://github.com/Xeoncross/go-concurrently) to help show uses and one [more general one](https://github.com/Xeoncross/goworkqueue/blob/master/goworkqueue.go) I've been using in production for a while.
Thanks that seems to be a good read.
The link /u/Justinsaccount provided below is I think exactly what I need to get rid of this. Thanks for the insight.
I can see the appeal. There are pros and cons to each approach, we prefer to install fresh, despite the risk of an outage. We're hoping to get a proxy set up in the future.
GopherJS is an amazing tool, but I've always been frustrated by the size of the output JS. All the packages in the dependency tree (including the standard library) are compiled to a single JS file. This can cause the resultant file to be several megabytes. I've always thought a better solution would be to split the JS up by package and store it in a centralized CDN. This architecture would then allow aggressive caching: If you import "fmt", it'll be delivered as a separate file "fmt.js", and there's a good chance some of your visitors will already have it in their browser cache. Additionally, incremental updates to your app will only change the package you're updating, so your visitors won't have to download the entire dependency tree again. Today I'm announcing jsgo.io, which is this system. Here's how it works: Visit https://compile.jsgo.io/&lt;path&gt; to compile or re-compile your package. Here's a very simple hello world - just click Compile: https://compile.jsgo.io/dave/jstest After it's finished, you'll be shown a link to a page on jsgo.io: https://jsgo.io/dave/jstest. The compile page will also give you a link to a single JS file on pkg.jsgo.io - this is the bootstrap loader for your package. Add this URL to a &lt;script&gt; tag on your site and it will download all the dependencies and execute your package. Ths compile server should be considered in beta right now... Please add issues on https://github.com/dave/jsgo if it's having trouble compiling your project. The package serving framework (everything in pkg.jsgo.io) should be considered relatively production-ready - it's just static JS files in a Google Storage bucket behind a Cloudflare CDN so there's very little that can go wrong. If you'd like to chat more about the project, feel free to file an issue, email me or post in the GopherJS channel of the Gophers Slack. 
That‚Äôs possible as well, via a directive.
Any popular SaaS company should have ports to golang. Just doing some google-foo, I found some for [twilio](https://github.com/sfreiberg/gotwilio), [slack](https://github.com/nlopes/slack), [stripe](https://github.com/stripe/stripe-go), and [discord](https://github.com/bwmarrin/discordgo). Some of these libraries will have a mix of pure HTTP and other lower level bindings.
It likely solves the same problems as viper but with a different interface. It's also part of a larger ecosystem of tooling for distributed systems development.
Another primarily PHP dev here: I'm learning Go right now, since it seems a nice way to add something much closer to the hardware to my tool-belt.
How does it handle code-changes/upgrades/breakages? That is the reason Go and GopherJS link statically, that you don't have to worry about versioning your things.
All the compiled JS files have the SHA1 hash of the contents in the filename (check out the network tab in your browser as one of the demo pages loads), so versioning is pretty neatly taken care of. If you change code, and it causes a change to the output JS, then the filename of the JS file on the CDN will be different. If the JS output doesn't change, it'll continue serving the same file. Even better: if you change code in a dependency, normally everything in the dependency tree above the change would have to be rebuilt and change... However with this architecture, if the contents of the compiled JS file hasn't changed, then clients will keep using the same file. 
So, when trying to explain this to my co-workers, I noticed that this entire system seems designed for libraries that will be used by many people. That fine, but what about executables instead of libraries? It seems that the incentives are different there. A big point of contention, for example, was the mandatory semver. "So, if you remove a struct, you have to increase the top-level version number. But not if you remove an network API endpoint," I said. But the users, in this case, people who download the executable and run it, probably expect the semver to represent the API stability. They don't care about the code. In fact, nobody cares about the backward compatibility of the code, since it's all effectively unexported by virtue of being an executable instead of a library. But it's still all tangled up, because a module (the unit of versioning) can and often will contain both executables and libraries. So now we're bumping versions on executables for breaking changes in libraries. This feels weird. We can't reasonably break it up, though, because the source control repo defines the module and the tooling is built around the repo. I'm trying to wrap my head around it, but the more I think about it, the more it feels like it mucks with stuff that we need for other things. :/ We're likely to wind up with a totally separate "go version number" that owns the repo tags and a different one for marketing and community reasons. That feels strange, but the tradeoffs might be worth it.
For those who want to see the whole set of benchmarks. https://www.techempower.com/benchmarks/ 
Yes, you should! It will help you thinking on how to better structure your projects :)
What do you want to do with the spidered content?
All Go code is ugly as far as I know.
If there's zero public API... you're ok. You can't disappoint users that don't exist. But it's not always that simple. Consider a repo with both a server and a library for interfacing to that server. Usually, we'd just keep both in sync and expect that if you want to access server version X, you will use client version X to do that. Here, though, we need to semver all the things in the expectation that users will want to mix and match client and server versions, right? Ok, that's sorta fine, but it means we're going to be bumping the major version for an awful lot of bug fixes. (Not all fixes break compatibility, but many do, especially security ones.)
I learned with PHP. I loved it, and I really loved Laravel. It made making apps so easy, and I really felt I understood what was happening. I swiched to Go after ~3 years in PHP and it took about a week until I was much more productive than in PHP. This is mostly because there is no magic. Go standard library feels almost as powerful as Laravel itself, with none of the "magic" behind the scenes, and all the benefits of a more wisely designed language. Go will make you a better programmer, and will make sure your programs better.
It definitely helps you to become a better programmer. This applies to learning any new language, not just Go. As for using it. Go opens up some applications development that you won't do with PHP, such as a command line or background service that runs on the client side. For my own projects I would never use PHP over Go. Go just have a much better development experience. But most business are slow at adopting new technologies when what they knew already work well enough for them. Advantages of Go: * Easy of deployment without worrying about the run time environment. This rules out PHP, Java, Ruby. * Easy to learn and get started on for a PHP developer. This rules out Rust, and purely functional programming languages. * Cross-platform with the least amount of pain. * Security, memory safety. This rules out C. * Security, updating the dependencies. All depends are statically compiled, including TLS, and even the database if you choose. So you just have to keep your development environment up to date, and everything will use the latest version. Disadvantages: * Native application UI development, 3D UI development. No good Go libraries for them and unlikely in the near future. Most Go programs use an http interface for UI. Which usually offers enough features and is cross-platform. * Missing some features available in other languages. This is an advantage for learning Go and reading other people's code, as it forces everyone to be minimalistic over cleverness and over-design. This produces better code in the short run, but how will you learn without shooting your own feet once in a while?
Which of these languages are you more familiar with? This would perhaps be the one to use. Both languages are general-purpose language, and Web spiders do not require any special sort of language or language ecosystem, AFAIK. But if you want to know my (of course *completely* unbiased) opinion: Use Go. The language is lean and clear, the compiler is darn fast and cross-compiles your code for a truckload of different platforms(*), the binaries need no virtual machine pre-installed in order to run, concurrency is built in, the standard library is comprehensive, and even Microsoft supports Go by building and maintaining the Go plugin for Visual Studio Code. (*) "platform" here means: combination of OS and architecture.
FYI you can make CLI apps in PHP just fine. I wouldn't, but you can ; )
I'll be trying it out soon. No clue why noone is using it in my office :)
Having written a search engine for Go, I can say it's easy and works great. However, the content I was crawling was not Javascript-heavy. If you are crawling big sites (like reddit), sites with paywalls (like news), or single-page apps (SPA) then you really need to use a client that handles Javascript like headless Chrome. Now the backbone worker system can still be Go.
Please excuse the ignorant question, but why would you want to compile go to js? I understand this would allow it to run in the browser but, what is the purpose?
Delete your go.modverify file. It will only do hash checking if that file exists.
Copy the vendor directory to the docker image, that way you don't have to download anything.
https://golang.org/pkg/net/http/httputil/#NewProxyClientConn is deprecated: Deprecated: Use the Client or Transport in package net/http instead. 
No, you also need to change your go.mod to identify the module as v2 by sticking a /v2 suffix on the end. But, much more importantly, all of your consumers must import your package as foo/bar/v2. That also includes your own packages, if they reference other packages within foo/bar. For example if foo/bar/baz imports foo/bar/quux, it has to specify import "foo/bar/v2/quux" and, when you bump to v3 eventually, you have to change all your imports to import "foo/bar/v3/quux" 
I mean https://golang.org/pkg/net/http/httputil/#ReverseProxy
Hmm I see. That changes the workflow quite a lot and it's definitely not as invisible as I thought. :-/
Amen
Nice!
For me personally: Writing Go makes me happy. Writing Javascript makes me angry. I've been a web developer for 20 years, and have never liked Javascript. I love clean, elegant, well designed, type-safe languages, and I especially love Go.
Hopefully we can specify the vgo cache directory in some way. We check in all external deps in to our monorepo using a custom `go get` like tool and I would like to continue to check them in globally. 
Looks like right now you can't, here's a snippet: &gt; $GOPATH/src/github.com/golang/vgo/vendor/cmd/go/internal/vgo/init.go list := filepath.SplitList(cfg.BuildContext.GOPATH) if len(list) == 0 || list[0] == "" { base.Fatalf("missing $GOPATH") } gopath = list[0] srcV = filepath.Join(list[0], "src/v") `srcV` being where everything gets downloaded to right now, which is `$GOPATH/src/v`, as has already been revealed!
https://medium.com/@mlowicki/http-s-proxy-in-golang-in-less-than-100-lines-of-code-6a51c2f2c38c allows to do TLS handshake between client and destination origin server. Proxy itself doesn't have access to unencrypted data exchanged between client and destination if client wants to reach e.g. https://google.com https://golang.org/pkg/net/http/httputil/#ReverseProxy has full access to request and response.
I'd maybe not delete the whole file! For most, you'd probably notice the failed build, then go to check why, see a dependency didn't match what was expected, and then go and investigate. Maybe you discover something malicious? Maybe it's a bug in vgo? Maybe someone did re-release a Git tag and it's all fine really so you just delete that one line in your go.modverify file that referred to the old hash, and then continue. You'd not want to delete the whole file, because then you wouldn't know if all of your other dependencies had changed in-between you doing that and re-generating it!
Right, but the problem is that you have nothing to compare it to. It's like if your house alarm goes off, but the doors are closed and you don't see anything amiss. Did someone sneak into a window? Did the alarm malfunction? Did wind just push the front door? You have to carefully investigate with no starting point. And even when you're done, you worry that you missed something. If I could type `vgo diff library` or a similar command and see the changes, I could make an intelligent decision. As it stands, I have to make a decision, but don't have good information on which to base it. 
I'd recommend taking a look at Vecty which is similar to React, but built in pure Go: https://github.com/gopherjs/vecty
I use both in general, and my choice for a web spider would probably be chosen for the other code in the project or familiarity of other developers rather than for the language itself. As jerf mentioned, what you're doing could be the deciding factor. You might decide to interpret the JavaScript so that you can render script-heavy pages and you might decide you like the phantomjs in one language better than the phantomjs library in the other. I think the concurrency primitives in Go are lovely, and I prefer to run server stuff on Linux* when I can, so I might tip more towards Go when I have the choice. I've also run into management that won't buy a product not written in .NET (which is probably already a minority of the things the manager buys--but they don't know that) so as annoying as it is, I've written things in C# just to make something more pleasant for the bureaucrats.** *I very much prefer Go to C# on Linux despite the rapidly advancing dotnet core which I HAVE deployed to Linux in production. That is a much more mild concern than it used to be, but you'll need to check dependencies to make sure you can use the assemblies you need. **And yes, I'm talking about a place with 25% of app dev writing Go... I just don't know.
&gt; Please don‚Äôt use reflect.DeepEqual. &gt; The go-cmp package was designed to avoid this problem. Then how about adding go-cmp in the standard library? This way we won't have to add yet another dependency in our projects. And while on the subject, an easy way to compare structs and print where they differ would also be amazing if it was provided by the standard library.
Slices can't be used with == because they are a form of indirection and Go doesn't do deep comparison with basic operators, hence why type-specific functions like `bytes.Equal` are used. The slice itself is just a pointer with a length and capacity, so in theory == would test whether two slices point to the same place which is almost completely useless but would definitely mislead beginners into thinking it is testing whether the *contents* are equal. Making it a syntax error avoids confusion. Regular pointers and pointer-like types (channels and interfaces) can be compared with == because, while it's still not doing any deep comparisons, it is less likely to mislead people. It's also sometimes useful to use pointers and channels as a map key, for example tracking clients that connect and disconnect from a server. On the other hand, I can't think of a good reason to do a shallow comparison of two slices.
TOC for previous articles: https://research.swtch.com/vgo.
&gt; **The End of Vendoring** &gt; &gt; Vendor directories serve two purposes. First, they specify by their contents the exact version of the dependencies to use during go build. Second, they ensure the availability of those dependencies, even if the original copies disappear. On the other hand, vendor directories are also difficult to manage and bloat the repositories in which they appear. With the go.mod file specifying the exact version of dependencies to use during vgo build, and with **proxy servers for ensuring availability** . . . Proxy servers are not a general solution. An open-source project hosted on GitHub that wants to ensure reproducible builds cannot use a proxy server and must vendor its dependencies in one form or another. Please keep first-class support for vendoring. Without it, vendoring will be re-implemented by people who need it, badly; or it will simply not be used, making packages generally more fragile.
Only the MySQL-backed implementations appear to exhibit this behaviour, maybe start your investigations there.
It should probably be noted that Vecty is still in the very early stages of development, and serious breaking changes are still expected before it should be considered for production use, but by all means give it a try.
This gets us halfway there. May be if it still supports vendoring, we can have the other half or we can have some other solution for the other half.
For something similar to Pandas in Python take a look at github.com/kniren/gota/dataframe. Gonum provides matrix/linear algebra and statistics support backed by BLAS. You can try github.com/james-bowman/sparse for sparse matrix support (disclaimer: I am the author).
Jessie mentioned in the roundtable that Microsoft was donating engineering resources to help build tooling for this, such as proxies that could pull data from a source-not-github. Any vendoring solutions will probably be based on similar tools.
The one scenario that I prefer exceptions is when I have some set of operations that I want to guard against and I don't care about the result. For example, if I had an app that needed to send some messages about failures, but it was not critical: try { # do a bunch of operations and eventually send it somewhere } except err { fmt.Println(err) # just make a note but don't take action } Outside of that use case, I've generally found that anytime I've tried to catch exceptions from more than one operation it is always trouble. Even from a logging perspective, it is better to do: try { x := A() } catch e error { fmt.Println("error doing A " + err) something(e) } In Python, for example, if I were to create a directory and then write a file, I could try both operations, but as both could return the same type of error, the only granularity I had reporting to the user was "error creating the directory or writing the file", which isn't very helpful. Another thing to realize is that if you have exceptions, you often *have* to catch them. For example, if I had a send it and forget it type function where I didn't care about if it worked or not, that is much simpler in Go. sendStatusMessage("yup, things are working") When you have exceptions there is good chance the library might raise an exception if there were a network error and you'd be forced to add a try/catch block in order to protect against a panic. I do understand that when you're used to exceptions it can be frustrating to seemingly expand the check to each individual call, but I think the Go authors realized that there were few use cases where it made the code better. That obviously doesn't make it an easier pill to swallow, but when I think about the number of times I had to unpack a try/except to multiple blocks, I can appreciate that Go essentially just avoids it all together. That doesn't make the boilerplate any easier to write though! For that it is worth thinking about how you can add some helpers via your editor. It might be just what you want.
"form of indirection" "go never does a deep comparison" "just a pointer with a length" strings. 
This post is really great and I feel a tiny bit vindicated about my [nitpicky 2015 blog post](https://blog.merovius.de/2015/07/29/backwards-compatibility-in-go.html). I was kind of excluding reflect-induced breakages at the time, because I considered that too theoretical (like, who would actually use reflect in production, right?) - but I didn't consider testing at the time. But this post really recontextualizes this and makes the issues I so far (deep down) considered mostly theoretical actual *practical* issues for Go maintainers. It makes it so much more important that we get good tooling to handle changes for the Go ecosystem at large :)
I do not think so. same behavior for postgresql.
&gt; the package author, still wanted to be able to adjust the API if it didn't work out. Being in the standard library would limit that growth. Okay. So how about adding it now? :)
[Clean architecture + better package names](https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1).
Personally, I'm not convinced it should live in the stdlib. I think it could live somewhere in `x/`, but really, it should probably just be made okay enough to add a dependency (and it might become okay enough with vgo).
Dep does not work great.
Hello, go-cmp maintainer here. &gt; So how about adding it now? :) Not yet. 1) There are some API extensions that I would like to still explore. One of which is the ability to have filters be composable. Adding it to the standard library binds development to the Go release cycle, which moves slower than is helpful for the project in its current state. 2) Adding cmp to stdlib would still need to go through a formal proposal process. Acceptance would only occur if cmp is an obviously better solution for comparing values in tests as opposed to reflect.DeepEqual or other solutions. I believe there is still not enough data to say that cmp really solves the problems it set out to solve.
To be fair, I only compared fasthttp, and I didn't read the code: fasthttp-postgresql 33,416 71,507 77,884 101,400 124,514 89,580 fasthttp 31,197 39,162 65,152 51,016 46,264 36,677
I don't think tooling or the availability of a third-party caching proxy helps the use case I highlight, which is an open-source project hosted on GitHub (or Bitbucket, it doesn't matter) that wants to ensure reproducible builds by consumers. Those projects must include all of their dependencies in their repos.
Fair enough.
Do you expect everyone to justify why they upvoted? I simply upvoted since fanboy in me approves rewriting stuff to Go, no matter who did the rewrite.
I think the link of the `overview post` should be changed from https://research.swtch.com/vgo1 to https://research.swtch.com/vgo-intro.
All your questions will be answered if you read these: * https://blog.golang.org/slices * https://blog.golang.org/go-slices-usage-and-internals
I don't find this question answerable the way it's phrased. Slices *are* like `std::vector`. Unless you are more specific about what you dislike about them or what specific difference it is you are wondering about, we won't be able to help.
&gt; For small teams, caching proxies are not a good solution, because it's another thing to maintain. I'm not sure I buy this argument. Running a static Go binary on some vm somewhere and setting an environment variable sounds like *far* less work in practice than managing any vendoring-solution‚Ä¶
Technically, you can still vendor using a rewrite rule in the .mod. 
What? If OK Log (for example) ever wanted to ensure fully-reproducible and accessible builds, you think it's simpler for me to run a dependency proxy on a VM that all of my contributors pull through, than to commit my deps to a vendor folder?
It's a go success story post about a rewrite without a single line of go in it and in general is very thin on go-specific information. how do you respond to that? sure go is nice, have an upvote, but what do you comment on?
Because apart from 0-capacity slices the contents (elements) of a slice are mutable, and mutating them when they were in the map would make a mess.
I think it's neat to see how they wrote a new thing to replace a slow thing and how to make sure switching to the new thing doesn't screw anything up. I'd upvote that sort of thing. I don't really have anything to say _about_ what they did, though, so I wasn't about to comment.
&gt;Again, you are trying *very hard* to make sure to interpret OPs question in a way that makes it impossible, instead of answering the question they actually tried to ask. Here is the original question. This is the question I answered, not some hypothetical one that they supposedly "actually" tried to ask. &gt;Citing the golang wiki (https://github.com/golang/go/wiki/MethodSets#interfaces): &gt; &gt;"The concrete value stored in an interface is not addressable, in the same way, that a map element is not addressable." &gt; &gt;The question of map values not being addressable is explained here: Why are map values not addressable? &gt; &gt;However, it is not clear regarding the interface. [Why are they not addressable?](https://stackoverflow.com/questions/38168329/why-are-map-values-not-addressable) Is this because of some hard design assumption?
More than two years ago, I was the initial committer of that [fasthttp-postgresql entry](https://github.com/TechEmpower/FrameworkBenchmarks/pull/1820). Aliaksandr (the creator of fasthttp) then took over and did some optimizations. I will begin by paraphrasing Tim Fox, the creator of Vert.x, who, at some point, some years ago, said something along the lines: "you don't look at the Techempower Benchmarks results to argue over who does 1 million rps vs 1.1 million rps. You will most likely be bottlenecked somewhere else in real life. You go there to weed out the truly bad performers, and know not use them." Here are some of my thoughts which may help you understand those numbers: Infrastructure has changed, and latencies are possibly lower now. This puts more pressure on how each framework performs in regards to allocations and usage of reflection under heavy concurrency. When I committed fasthttp-postgresql, I used my preferred Postgresql driver, Jack Christensen's pgx, which is not using reflection. Also, fasthttp is very good at preventive excessive garbage. So it may be a combination of all these factors. I haven't followed their benchmark since 2016, but keep in mind there are always lots and lots of moving targets with it. Server infrastructure seems to be changing quite frequently, mostly for the better. If you have time, you can always download their benchmarks project from github and try it on a local server of yours. I have a hunch results will vary wildly. Plus you know the world of benchmarks... today's ruler, tomorrow's buffoon. Cheers
The proxy specification can be implemented locally against a local cache of downloaded modules, so you could run a local proxy or one for your company, too.
Where is the proxy specified?
Odd. We use it for over a dozen projects and we haven‚Äôt had any problems. Several of them are far from small code bases too...
I might be reading too much into these posts but it seems that the theory behind what should be the package manager/versioner is becoming too much of a "trying to engineer the perfect solution" problem. 
https://research.swtch.com/vgo-module implies that the `go.mod` files for every version have to be (also) stored outside of the zips (along with other metadata), wherever they are hosted, with Github et al special cased to work with whatever APIs they already expose. So this is less severe in one way (less to download), more severe another way (requirements for hosting are stricter).
I've got that notion too.
[removed]
Not yet specified, but using a filesystem backend was mentioned during the talk as a solution for k8s symlink hell.
Hi all, I ran into an interesting performance issue while working on a side project, and I wanted to share my experience here. This is my first time posting here, so please feel free to let me know if r/golang isn't the right place for this sort of thing.
So beautiful, it makes me wish this was one of the first examples I had read
It does work decent for us, but for anyone reading, make sure you're using the latest version of go! Dep doesnt work pre 1.5ish (it doesnt acknowledge the damn vendor folder dep generates!)
But does it support generics?
The tldr this article needed.
Feels like trolling. 
I am interested in your content, but I've decided I will not read web pages that put a modal popup in front of content after loading. If you could summarize here that would be great, or arrange for medium to not put a modal on top of your content. 
I do not know but I know you can run Go on a Raspberry Pi running Linux, if that is useful info. 
Thanks for the suggestion. I would be a help if you could tell me which resources I should use to learn GO?
But Raspbian runs a 32bit kernel (with 32bit apps) while FreeBSD is aarch64.
True, it works on Linux both on 32 and 64 bit, but not on FreeBSD
The title is missing an article (e.g. the, a, one)
that's great, thank you
This is my first post using RelayPro app, I didn't have, or I didn't saw, the option to comment, just put the link. I'll comment properly when I've a pc near me. But you're right
I would say that way of approaching it is wrong. If you have two goroutines accessing a variable, and atleast one of them is a write, then the behavior of your program is fundamentally undefined. It's not that sometimes you'll read the wrong value. It's that you may have mysterious segfaults in completely unrelated portions of your code. In general, the mental model that a race condition only affects the value of a given racy variable is incorrect. The compiler is not necessarily maintaining your notion of what variables are (nor is the CPU). In other words, let's say that you have a variable X, and you perform some pattern of reads and writes to that variable. The compiler is not obligated to have a single storage location corresponding to X, nor is it obligated to perform the same pattern of reads or writes to it. The only guarantee that the compiler makes is that if you have a race-free program, then it will appear that way to you. As an example, assume you have a racy integer variable, and the compiler is running low on registers in some portion of your code. Let's say that in a few instructions, there is a write to the racy integer variable, and there are no fences. From this, the compiler can infer that no one else can be reading/writing this variable (given that the compiler is free to assume that your code is race-free). So, it can temporarily save some register space by spilling one of its registers to your racy variable. As long as it restores this spilled register before the next read or before the next fence, it "knows" that you can't tell that it did this. Now if you have a racy write on that variable, note that the code may now corrupt some arbitrary data. This could, for instance, be a pointer value in a completely unrelated part of your program that happened to be in a register. Suddenly, the next time you access this unrelated pointer, you might mysterious get a crash. Or if instead of a pointer it spilled a boolean, you can get a corrupted boolean that is both true and false simultaneously. The described optimizations above may or may not exist in the current version of the default Go compiler (I have no idea), but I know from painful experience that they are certainly there in C++ compilers (so presumably they are there in gccgo, as well). The race detector errors are telling you that you are depending on an implementation detail by breaking the rules, and are dancing on thin ice. Just use a lock, they are incredibly fast (particularly if uncontended). Or if its something dead simple and very very performance sensitive, then maybe consider atomics (but honestly, a lock is better even in this case). And make absolutely sure that you pay very very close attention to Thread Sanitizer (i.e. the race detector) and keep your program's race reports empty. tsan has no known false positives; any issues you see represent a real issue in your code. (Sorry for the delay in replying, I didn't notice the notification icon)
Depends on your goal. If you really like go and want to use it on the job then yes of course. But I wouldn't pursue Go currently just to get a job because there are not a lot of options currently, it will take time to grow. But I develop PHP on the job and go for fun in private. I have a lot of fun with it. And I hope to bring it up at work when I can where it's really useful, maybe a websockets service of sorts. 
fixed, git pull please
Sorry to hear that! The basic idea is that I expected my program's bottleneck to be IO, but after profiling, I found the surprising result that the bottleneck was actually channel writes / context switching.
I am already working as PHP Developer and a little bit confused that learning GO could help me in building my better career and of course package too!!
[removed]
you can try this: http://www.raspbsd.org/raspberrypi.html.
Why separate into processes? Seems to me that running single process with http server and handlers should be simpler for you. Don't complicate the design before really hitting any bottlenecks.
&gt; Its not equivalent at all Are you just explaining to me how what I suggested works? Read again, what I phrased please.
Then ask your questions in a way that they are answerable. It is fine asking questions, it's fine questioning the design of Go or certain aspects of it - but "why is $thing bad" just isn't a question that *can* be answered.
You could also use the Int function in the crypt/rand package: https://golang.org/src/crypto/rand/util.go?s=3070:3132#L96 Something akin: https://play.golang.org/p/4-SKnslDLRU
&gt; That whole git history is very efficiently packed Not that I'm aware of. Git stores whole files, not deltas and by default stores them as "loose objects", which means they are only compressed as individual files. Git *also* has a way to do delta-compression and only store diffs, by packing a set of objects into a single packfile and that mechanism is used when pulling - which is why a git-pull *is* more efficient that pulling all intermediate full versions. *But*: As far as I know, git *also* requires that packfiles are self-contained, so the delta can only be applied to another object in the packfile. As a result, every packfile needs to contain at least one full (compressed) source tree, which would imply that every pull needs to pull at least one (compressed) source tree. It has been a couple of years, so I am not *entirely* certain of this. But I seem to remember that that's one of the discoveries I made when I last worked on reimplementing git in go. In any case, I do seriously doubt that this will make for a significant difference in practice; unless you pitch "releases *all the time* and constant usage of exclusion of very fresh versions" against "I pull almost never". I just doubt that that's what's going to happen in practice.
&gt; I don't now exactly how to solve this problem, but I do have a few ideas Ideas are good but after reading the ideas I couldn't understand how they could possibly be implemented in practice. &gt; This means you can write code that operates over a specification, and for example, automatically generate the boilerplate necessary to forward from one implementation to another. How does that even work? This is the [json:api specification](http://jsonapi.org/format/) and it's supposed to be simple compared with other specifications. I dare anyone to write a prototype that produces the necessary boilerplate as mentioned in the idea. &gt; Instead, you would just make the edit, and expect the system to figure it out. This would be great yes but if no system in existence is able to do that then it sounds like an awfully hard problem. If it was easy to do then all package managers would support it. &gt; there is no reason a single unit of code cannot provide multiple interfaces, and package tools should make this possible. But how? We need something more concrete than this.
 This article feels like it was written 6 years ago. I find /u/4ad 's [comment](https://lobste.rs/s/7oxlm4/i_do_not_like_go#c_yq5uyn) much more interesting. &gt; Yes, Go is an operating system, almost. &gt; I wouldn‚Äôt want to change Go, I just want a different language for distributed systems.
There is no such thing as a "cryptographically secure uint64". I know that it sounds tempting to combine the two `rand`s, but it doesn't make actual sense in practice‚Ä¶
The author says very little about how Go made/makes him productive. He‚Äôs still writing Go so I guess this is second degree issues along the lines of: now that everything is working well for me what shall I complain about? And then he nitpicks, but that‚Äôs fine. 
Interesting article, pretty negative perspective as expected from the title. I guess that's what gets clicks these days. An article titled _areas where go can be improved_ wouldn't get the same attention. I note this community didn't take the bait and post equally negative comments which is promising. Enough meta, to the content. I feel like the author missing half of ianlancetaylor's comment was a bit disingenuous. ianlancetaylor goes on to suggest that the proposal requires more details. Rob Pike weighs in with additional information on past discussions. Personally I like enum types, but all these small things add up to increased compiler complexity, and compile time which is a trade off against one of the very specific goals of fast compile times. Additionally, warnings have been left out on purpose which is IMHO a great decision. Perhaps a linter could potentially provide the information required. &gt; On the unskilled programmers side, the language forbids features considered "too advanced." Go has no generics, no way to write higher-order functions that generalize across more than a single concrete type, and extremely stringent prescriptive rules about the presence of commas, unused symbols, and other infelicities that might occur in ordinary code. This is the world in which Go programmers live - one which is, if anything, even more constrained than Java 1.4 was. I feel like _too advanced_ doesn't acurately portray the situation. Commas is a design decision that doesn't really matter. It has them or it doesn't. JSON for example forbids them - an area that generates much more noise. If go didn't allow them, I imagine the IDE would take care of that too. Go doesn't have warning, it has errors instead. Unused symbols are an error. It makes rapidly working with code difficult at times, but a quick `_ = x` removes this hindrance immediately. Pretty minor IMHO but this is pretty damn subjective. I've seen awlful code bases that over use the hell out of generics, and it's been discussed in great detail that the go authors are looking for a suitable solution that plays will with the design goals of the language. Sure it would be nice to have them, but like all things they are a trade off. If I happen to be working on a problem that simple is unfeasible without generics - I'll use a language that supports them. I look forward to the day that Go does have generics or the equivalent. In general, this is an article from someone who wants to use Go to program like Java. They're different languages, and it's best to work with the language rather than force it to behave against its design. I can understand their frustration, but I think a different perspective would make their life easier.
Change your `Run` and `Shutdown` methods to have a `*ServerConfiguration` receiver, instead of `ServerConfiguration`. You are modifying a copy of the config in `Run` (because you are not using a pointer), so the `serverN` fields on the original don't get populated.
That worked! Thanks! Seems like I have a lot to learn. ^^
[removed]
&gt; The Java team took criticism in this vein to heart, and Java can now emit this warning for switches over enum types. Go tools could do that too, without any language change. If anyone would bother to write them. And that someone doesn't have to be on the Go team, it could be any sufficiently motivated programmer taking a day off work. There either is no one willing to do the implementation to make it happen, or that tool is unknown to me because no one is adopting it. This suggests to me that this is *not* an obviously useful feature. &gt; Packaging and Distribution of Go Code &gt; GOPATH It's a bit amusing (in a Schadenfreude sorta way) how these two sections got invalidated a couple of days before they published this post. &gt; nothing other than good sense stops a programmer from returning an error in some other position, such as in the middle of a sequence of return values, or at the start And yet, the fact that I've never seen that happens seems to suggest that "good sense" is actually pretty reliable in practice here. --- Overall‚Ä¶ well, all of this has been brought up before and all of it has been justified before (well, maybe except Rob's dismissive attitude). Not acknowledging the validity or even existence of the other side of an argument makes this come off as a useless unproductive rant. This article sadly only opens up two avenues to react to it: Fully just do what they say because they said it or outright dismiss it in full. It frustrates me, that the Go community is painted as dismissive or arrogant, because it refuses to do the former in reaction to articles like this.
For future reference, discussion with Russ on this issue: https://research.swtch.com/vgo-module#comment-3771676619
Discussion with Russ on this issue, for future reference: https://research.swtch.com/vgo-module#comment-3771676619
It's possible to use uncompressed .zip. This way, git will be able to compute minimal diffs. And git already compresses everything. That said, even storing compressed .zip, I'm not sure it's a big issue in practice, according to Facebook experience report with Yarn (https://yarnpkg.com/blog/2016/11/24/offline-mirror/).
The idea is to move $GOPATH/src/v/cache to a project subdirectory, for example go.modcache or go.modmirror, and commit this in git. It sounds simpler than having to install, run, monitor and backup a proxy (whose storage needs to be durable and available since this is the whole point).
Discussion with Russ on this issue, for future reference: https://research.swtch.com/vgo-module#comment-3771676619
A more concrete proposal here: https://research.swtch.com/vgo-module#comment-3772245118
 I don't want to find a big struct with multiple props anothertype b | string | nil No thank you. If you are going to be witty then spell it out for the next person down the road. I suppose maybe if you did this with interface types it might be interesting. iread iwrite | ibuff But again anyone looking at that is gonna be spell bound. So it's a big no for me. 
Summary: the author complains about lack of templates and lack of exceptions in Go. Uninteresting article. 
Summary: A Java programmer complains about the lack of templates and exceptions in Go.
I believe the Raspberry Pi 3 can support a 64-bit system just fine though.
I think medium does it randomly when you aren't logged in, and I'm never going to log in.
Syntax highlighting on playground: https://go-sandbox.com Everything else is meh
Can you explain?
very low effort
&gt; whose storage needs to be durable and available since this is the whole point Does it? IMHO not if it's about providing a service exclusively to your small team. At that point, I'd argue that it's trivial to keep the storage durable and available enough. Because it's mainly a fallback if the outside storage is unavailable. You'd likely be fine with whatever a random cloud provider gives you for a VM. And maybe a daily rsync to something more permanent. (Arguably, that will make you still more durable and available than what most companies out there do for their actual production data). And also, you'd need exactly the same level of durability that you'd need either way to host your repo: Anything that might nuke your cache would also nuke the storage of your repo. And anything that would save your repo in that case would also save your cache. I'm not convinced that there is a significant reliability difference.
"one" is not an article :p Yah, this is not really that newsworthy.
[removed]
&gt; Personally I like enum types, but all these small things add up to increased compiler complexity, and compile time which is a trade off against one of the very specific goals of fast compile times. Additionally, warnings have been left out on purpose which is IMHO a great decision. Perhaps a linter could potentially provide the information required. A reduction rewriter is not at all complicated, in fact, that is what compilers do - reduce. COMPILERS ARE REDUCTION REWRITERS. The go compiler is a reduction rewriter. Your argument against an enum-like type borders on hypocrisy considering the compiler already does this for slice and map(not to mention just the rest of the language...). The same goes for 'generics'. Mostly what people are asking for is simple reduction, nothing complicated, just simple authoritative reduction introduced into the compilation pipeline. The key word here is 'authoritative' as this feature MUST be standard. &gt;In general, this is an article from someone who wants to use Go to program like Java. They're different languages, and it's best to work with the language rather than force it to behave against its design. I can understand their frustration, but I think a different perspective would make their life easier. Please, be less rhetorical. Coders are forced to use languages they don't like or that aren't familiar all the time, it is a part of life as a coder. 
Your link doesn't appear to work.
&gt; Please, be less rhetorical. Coders are forced to use languages they don't like or that aren't familiar all the time, it is a part of life as a coder. I honestly don't think what I wrote is rhetorical. I'm an Engineer first and a programmer second - as such I design solutions appropriate to the problem. I am forced to use an incredibly wide variety of languages day to day. I don't try and write Java as if it were C, and I don't try and write Go as if it were Java. Go is different, and your approach should be different too.
I studied the effects of different data structures on caches of numa systems.
[removed]
Hey, You must learn GO. I think every PHP Developer should learn Golang. You want to know why? Read here:- https://www.laitkor.com/php-guys-learn-golang/ It will help you in taking correct decision. 
Sorry bud, #1 rule in Go is no bike shedding. The quote from rob that the writer took offense to is explaining exactly that. Any discussion that diverges from a realistic ability to push production quality code into production is met with a terse answer. I know there are those out there that love to customize their editors to high hell and IDEs are very popular, but neither get and keep code in production. Go isn‚Äôt for everyone. But the it is so popular because it‚Äôs design principles are aligned with that of the business which is to make more money then you spend. That is why other high level concepts which are implemented poorly are ignored (i.e. generics). That isn‚Äôt to say that won‚Äôt ever exist. But they won‚Äôt exist in the the way they are thrown into other languages. You can see this with channels and go routines being fundamental while threads and mutexes are not. As a person that has build and deploy to production I am well aware of the shortcoming of almost ever programming language. Go deployments are the only ones that don‚Äôt suck. Debugging go in production also generally sucks less as well. At first I missed the ability to crack open and edit scripts on the fly; but I have been bitten far too many times by uncontrolled changes in production. It‚Äôs a crutch. Also, go runs faster and with less resources then most other languages save C. When you are talking about PC code maybe not a huge deal. But when you deploy to AWS and pay per second CPU and memory costs it adds up.
Regarding the somewhat condescending attitude of core team, well, yeah, he's right, obviously. Which is both good and bad, actually. They do their own language, based on their own needs and own philosophies. They don't give a f*ck about their users, but that's actually one of the reasons why the language is good IMO: it is very opinionated, and doesn't try to pile features upon features just to make everyone happy. They are making a tool for them, and since they are good engineers and their needs match mine, I can use their tool and be happy with it. They don't care about me? That's ok, I'm not there to make friends anyway. This is how C was conceived, and despite its shortcomings, almost everybody agrees C is a simple and pragmatic language. Regarding errors, it's always the same, people complaining about how error management is a sequence of "if err != nil { return nil, err }", while I've never written such code (or when I did, it was a mistake): just returning the error is useless, you have to either deal with it, or return it encapsulated with a significant error message, like "if err != nil { return nil, fmt.Errorf("could not perform foo: %v", err) }". Once you really deal with your errors, all those fancy try/catch mechanisms stop looking so neat and simple (this is not necessarily the case with union-type-based error mechanisms). And if your error-handling philosophy is "let the program crash with a stack-trace on the first error I meet", then you can use "panic" everywhere and call it a day.
Thanks, you could suggest that to the Go team to check what is wrong.
&gt; coders have wrote many rewriters. I was talking about a linter - specifically one that "emit this warning for switches over enum types". &gt; It is nearly impossible to get something into the standard tooling Why would it need to? Most Go tools do not live in the standard toolchain. `errcheck` is a widely used linter outside of the standard toolchain; why would this linter be any different? &gt; so what is a necessary but almost impossible task The first step should be "demonstrate that this is a check that is useful". You can do that, by writing the tool and show that it finds meaningful bugs. Integrating it upstream can come after. One of the main stated benefit of sum types is that the compiler can find bugs that would otherwise be missed, when type-checks are not exhaustive. The reply is: You can reap that benefit without actual sumtypes in the language and without the complaint coming from the compiler itself; you can use interfaces and have a linter and *it is literally impossible to stop you or anyone from doing that*. And even if you are disappointed that this is not giving you *all* the benefits you want from sum-types - if it's such an important benefit, I'd argue that, given how simple it is, someone should just *do* it and demonstrate it. But here we have something that is a) almost trivial to do, b) no one seems to be actually doing, but c) people are complaining that no one is doing it for them. And I find that incredibly frustrating. I find it frustrating to be expected to tolerate phrases like these &gt; okay, we have done this before in the past when all I'm pointing out is, that there is this person on the internet calling the Go team and community "dismissive" - but is then completely unwilling to spend even the tiniest amount of effort on the feature they are requesting *that they could implement themselves and get the benefits of and prove all those idiots disagreeing with them wrong in literally one day of work*. And that maybe, if all those people so heavily advocating for this thing, who are *so convinced* of its importance - are unwilling to spend this effort, that it seems the cost/benefit analysis of this feature isn't actually justifying the 8 engineering hours and thus *should* not to live in the language proper. Prove me wrong.
I've read a lot of bad takes on Go, and this article is the worst in recent memory.
To be honest I don't really understand your complaint here, specially how you consider malloc to be an improvement. The zero values in Go are well defined and a much bigger improvement over zero values in C. You are also always guaranteed to get zero values, unlike malloc which requires explicit initialization over every single field. I'm of course ignoring the fact that memory management is the single largest source of software vulnerabilities, you can malloc exactly once or leak, you may free exactly once. If you double free you will "..." oh- undefined. What platform are you on? What compiler? We haven't even't touched on the fact that you are often dealing with abstractions on top of malloc because of course people are clever. You can't use malloc in real world applications either, because of fragmentation many tiny mallocs like your posting here aren't even an option. You end up allocating all your types through arenas which just adds more complexity and room for error. Malloc nor the C type system provide more flexibility, I see no merit to your points here. It almost seems like your sole complaint is based on the fact the zero value for a map is nil- you want a ready to use zero value map to make your life better than make one. C does not have a ready to use hashmap either. The entire C ecosystem is pretty awful, the last thing the world needs is malloc or anything close to resembling it. Post a real piece of software that you feel was "cumbersome" to write, maybe there is something that can be done in your designs. Maybe you are using too much struct composition as an artifact of your C experience instead of interfaces. If you are often refactoring structures it's possible you are passing around too many concrete implementations across pkg or even inner-pkg abstraction boundaries. Post code next time and see what kind of suggestions come up. &gt; Fundamentally, my point was that the compiler already knows the type of the thing it needs to allocate in all of these expressions (including the compound literal return type in NewNamedMap!), so why not just use it instead of making me type it everywhere? Seeing this it's almost as if you don't want to type NamedMap in either the functions return value or the composite literal. You want to elide &amp;NamedMap to look like: func NewNamedMap(name string) *NamedMap { return &amp;{name: name, m: member{}} } I really don't find that very readable, in this simple example it's "ok" because it's so close in vicinity to *NamedMap- but don't forget that this is not some special construct in Go. This would have to apply to all function declarations in the entire language. So suddenly we omit types in the return values in every function body in all Go code. It would make you have to constantly reference function declarations at function exit- something people already complain about with named returns. But in the end you have to type this in c as well, even if you are one of those people that abuses void all over you eventually have to write the type out in some place to access it's fields. I don't see any advantages to malloc idioms or c here, honestly. Try to use interfaces more and less struct composition, you're doing something wrong to feel so much resistance. My Go code is usually a fraction of the size of my c code, or any high quality secure, safe and correct c code I have seen.
Nobody is really putting together a complete enough proposal. Or maybe people have in the past but this isn't one. Showing the syntax you would like to see is a good step, but you also need to flesh out how it would be implemented, discuss what would be the run time and compile time tradeoffs of that implementation be, whether you can constrain the generic types, and how. How do you express that you want a nil or zero of a generic type, can you type switch on them? How will the generics functionality interact with the already generic slices and maps? Would they need to change in a breaking way for the language to be consistent? Then show some examples of practical things you could implement, both with your generics system and without to show the difference. (A few high performance generic containers and maybe a vector math library are obvious choices) There is a lot to work through. I would love to have generics in Go, maybe I will try to do a real proposal, but if there have already been a few good ones I don't want to just add more noise. 
I advocate for this approach and want to see it adopted everywhere. It is essentially a hybrid of the opt-in approach (pinning) and opt-out approach (floating), where you can get essential security updates while not having interface changes forced on you. Semver is not enough. This week I spent the better part of an entire day hunting down an obscure bug that couldn't be replicated by someone else on the team. It was a new build environment, so it was unclear where the problem existed. The dependencies were pinned‚Äîbut some of the dependencies' dependencies were floating (e.g., "^1.2.3"), and one of those dependencies introduced a subtle breaking change in a minor version. This could have been resolved much faster if the locked dependency list was checked in (poor practice in an inherited codebase), but the problem remains. It's not surprising that Russ Cox has been thinking about this. Just like Go's explicit error handling forces the developer to consider every error condition, "imver" forces the interface designer to explicitly consider older versions, instead of letting versioning implicitly handle it. The result is, hopefully, a more thoughtful change management process.
&gt; I was talking about a linter - specifically one that "emit this warning for switches over enum types". NO, you were talking about tools. The rest of your comment is rabble. My opening comment addressed the unwillingness of the core go team to openly communicate and cooperate with the greater go community. I agree that the core go team is dismissive, arrogant, stubborn, hypocrites, and to clarify things, that is what I argued. &gt;Prove me wrong. dep vs vgo already did. 
I thought I should do what I preach and also explicitly point out where I believe the article makes *good points* (even ones that I don't necessarily completely agree with). &gt; To pick one example, Rob Pike has been repeatedly and openly hostile to any discussion of syntax highlighting on the Go playground. I think this is a fully justified criticism. I don't use syntax highlighting myself and I *do* believe its benefits are hugely overstated - but I also think it almost definitely poses no significant harm and as such the request deserves a more respectful, serious response. There might still be good reason not to do it and they might still be considered to outweigh the benefits. But I agree that Robs response has been overly dismissive and disrespectful. Unfortunately, as much as I respect him, understand that he doesn't *mean* to and tend to agree with most of what the says on a technical level, he *is* pretty abrasive. I also have the impression that he recognizes that himself and has lately become much less front-and-center with these remarks and I think is making an effort; but that's just interpretation. &gt; In Go's case, the language embodies an extremely rigid caste hierarchy of "skilled programmers" and "unskilled programmers," enforced by the language itself. I don't think that the divide they are perceiving here really is about "skilled programmers vs. unskilled programmers", or "trustworthy vs. untrustworthy programmers", but more about certain ideas about programs, abstractions, code and architecture. But I can well understand why it *seems* that the distinction is about good vs. bad programmers (and I'm sure there are quotes of me, personally, out there where I may even literally say something to that effect) There is some level of understanding of what makes good *programs* and what makes bad programs and the language and tooling do reflect that to a degree. I think that the community makes a bad job agreeing, codifying and communicating what that understanding *is* and to advocate it and onboard more people to it. But I also have to admit that there are people in the community that I definitely trust to represent "good, Go-like ideas" and to write "good Go code" - and that there are people who I don't necessarily think that of. An example (for me) is dep vs. vgo, where admittedly, I prefer vgo, because its ideas feel more "Go-ish" to me, not least because they come from Russ - whereas dep feels heavily inspired by other languages and other existing solutions and a way of thinking that I have trouble identifying with. It is bad, that I have biases about people though and that, while I believe there *is* an actual, technical merit that would make me choose vgo even disregarding the source, I also can't really describe or justify it. &gt; Error Handling in Go I think what they say is factually correct and it's a real valid reason to be frustrated. This is a perfect example of where I completely agree with the author on the facts and even on the complaints, but I come to very different conclusions. Which is that I, personally, prefer the boilerplate and annoyance - if I get understandable, robust, local error handling in return. *I* got frustrated with Haskell, where I had to fight the language to actually handle the errors where they occurred. I also get frustrated with Go, but I like the result more.
&gt; NO, you were talking about tools. Can we please stop for a second to acknowledge that I probably know best what I was and was not talking about? Please see the line I quoted in my first comment in this thread - above the piece of my comment you quoted when you replied to me.
On the topic of error handling, I am writing low level protocol handler and, boy, so many things can go wrong. I have a lot of custom errors and a lot of error handling boilerplate. Does not look neat but... It actually helps. I don't need to sprinkle my code with logging and tracing. Errors tell me exactly what went wrong and I even can recover from some of them. I now really appreciate how Go forces you to do error handling right.
&gt; I would expect community-run proxies to be fully sufficient. This is crazypants. No way.
There are actually very few languages that offer: 1. garbage collection 2. native AOT compiled EXEs with a top notch compiler 3. big/good ecosystem 4. fast compile times The closest you can get are some of the non-standard ways of compiling C#/Java, which is a bit limited, and OCaml, which doesn't have the big/good ecosystem, no threading, etc. There is Nim but it is still a bit beta, and may always be. Though I do really like it! Dlang fits the bill I suppose. Maybe I should try it =) But anyway a lot of people come to go for #2 and #4 and wish they still had their generics or sum types. 
Well it didn't work. Go complained about a wrong header. On the plus side, I can print the email by reading it with a "strings.NewReader" but it's not possible to search for a string. It never finds it, even if I copy past a string
Can you explain why it's "crazypants"? Like, I'm the last person who'd want to defend other languages here, but it seems this is working quite okay for other languages (and distributions, FWIW, which isn't that different).
I've been writing in Go for the past year and a half. My background before that was using C++. I like Go, but I don't know that I can honestly say I'd use it for everything. I suppose it depends on what type of system you're building. Instead of "switching" to Go, I would say learning Go is certainly valuable in the sense of adding another tool to your toolbox.
I agree! I love C++ but the problem is that everybody only knows a different subset of it, it is really difficult to write idiomatic C++ on a team environment, code reviews are always a pain and hard to explain. I hear that go is so simple that writing idiomatic go is the default. About switching, I was thinking about doing some side projects in go and then try to find a job writing go... :)
Looks simple enough to replace my bloated Chef install recipes which blow up whenever i use them twice a year! Nice 
You could think of Go as C with garbage collection and a big standard library Or you could think of it as C++ without templates or metaprogramming, less mess, and no header files. faster compile times (sometimes) 
Gals you like it ;)
ITT: the typical defensiveness you'd expect in any other language subreddit. I agree with some of this but I generally refrain from these types of discussions, the flamewar potential is high and the downvotes just come pouring in. And yes, upvotes and downvotes are worth a lot in Monopoly cryptocurrency but they do clearly display that the Hive disagrees with you. tl;dr there's no way this can end well. FWIW, the Go founders do come off as elitist dictators.
Precisely this. The whole `vgo` design appears to have decided that development will be happening "in companies". Lots of discussion about caching proxy servers and security audit... but none of this can reasonably apply to an open source project. Open source is by and large decentralized. We manage that decentralization by mirroring each other's code. Right now, the vendor solution does an adequate, albeit not great, job of that. /u/rsc says "Caching proxies ensures availability". It might, if you work at google. In some future universe, there might even be an official `proxy.golang.org` proxy or something. But all this violates the isolation principle from the next post. Whether my build succeeds (or which code it builds with) depends on the state of the Internet and/or any proxy configuration I may have set up. If I download a repository for an executable itself, it should be capable of providing for me all it's dependencies. I should git clone anywhere-but-github.example.com/coolrepo/solve-everything &amp;&amp; \ ifdown eth0 &amp;&amp; \ cd solve-everything &amp;&amp; \ go build &amp;&amp; \ ./solve-everything and it should just work, assuming `solve-everything` chooses to include all it's dependencies (transitively, ofc.) It shouldn't have to, of course. I should be able to get up and running on a new project by just including the import lines and letting go build get all the right stuff.
I think we could manage to do something similar with interfaces and reflection in Go =)
Whilst I appreciate the motivation to keep things simple it seems like the discussions become totally unreasonable with the constant assertion that adding generics would inevitably lead to Go being as complicated as C++ It's just such a ridiculous argument 
I wonder if the people who worry about that have only been exposed to C++ templates and not to generics in C#, or F#, where they are pretty pleasant in comparison. 
Ohhh, I just looked back at the chart. I forgot that the full range of architectures is really only supported for Linux. Do you know if gccgo would have much chance in succeeding with that combo of bad and arm64?
Would this not be compiling go code every execution? While I think the systemd info is cool, I am under the impression that compiling your binaries and pushing those to a server is the correct method of deployment. A binary made in go shouldn't require packages to work from what I can tell, so 1 binary instead of having to do two things, install go and run it with go. 
please see if you can comment in a constructive manner, you're more likely to receive a constructive reply. 
What do you mean by "as it is"?
https://play.golang.org/p/9DkWGRJuPt8
Just about to start coding a email confirmation myself and I was just going to have the link in the email use a cryptographically secure pseudo random number which was linked to the the email address. When the link is clicked (or code typed into web page) the backend just marks the linked account as having the email address verified. I was thinking I only needed to confirm the user has access to the email and the login itself provides the actual security. 
I'm interested in the answers. I work on networking devices. We are always going to switch packets in C, but for many of the features, including much platform, protocol, and device management (we do much of this in C++ and a bit of Python for a RESTAPI), Go would seem like a natural fit.
ive seen it done in a tutorial with just a link &lt;a href="www...."&gt; .seems to make sense to just do it as a GET. i was doing is with a key (same as your psuedo random string/number). then sets the user to "verified" (sets boolean to true) once the link is clicked. thought it was fine? then i read in a book on go microservices that you shouldn't pass data in the url params? just info on paging filtering etc? this threw me a bit. id like to adhere to rest guidlines as much as possible. 
&gt; You will be remunerated ¬£25,000 - ¬£35,000 per year (dependent on experience). Surely I'm not the only person who thinks this salary is a joke?
Open-source software is generally developed and hosted entirely for free, modulo development time. Asking maintainers to own and operate a running piece of software incurs an entire universe of costs: VPS, DNS, maintenance, upgrades, resiliency, monitoring, etc. The jump isn't incremental, from cost to cost+1. It's fundamental, from zero to nonzero. It's not a reasonable ask for something that can, and should, be provided by the code repo itself. (To say nothing of the fact that a proxy is a new, separate point of failure.)
 $ vgo list -f {{.Dir}} rsc.io/quote /Users/rsc/src/v/rsc.io/quote(v1.5.2) The actual form implemented looks like `.../quote@v1.5.2`, sounds like the article predates a source code change?
Do yourself and the safety of your users a favor and do not implement your home grown user system. Use what's out there, Firebase has a free service for this, and you own your user data.
&gt; and the safety of your users a favor and do not implement your home grown user system. Use what's out there, Firebase has a free service for this, and you own your user data. in using https , password encryption and oauth tokens so im thinking its relatively secure. for me this is an educational exercise also, so i dont see an issue with rolling my own.
The map key stores a copy of the struct, you can't mutate that. You can mutate the array a copy of a slice points to.
Imagine this working: m := make(map[[]byte]int) // not actually valid k := []byte{'a'} m[k] = 42 k[0] = 'b' You've just changed the underlying memory pointed to by the map key. Lookups of `[]byte{'a'}` wouldn't find the value 42 anymore. Go does not allow that, because it would not work right. P.S. There's no such thing as a "raw type".
I agree with much of your post, but I don't buy arguments like this: &gt; Personally I like enum types, but all these small things add up to increased compiler complexity, and compile time which is a trade off against one of the very specific goals of fast compile times. This is boilerplate that is used to oppose every addition to the Go language, but it tells us nothing about the actual cost. In the case of enum types, I would be surprised if the compiler performance cost weren't marginal.
I don't agree with all your points but it was well written and well argued.
**Cuisenaire rods** Cuisenaire rods are mathematics learning aids for students that provide an enactive, hands-on way to explore mathematics and learn mathematical concepts, such as the four basic arithmetical operations, working with fractions and finding divisors. In the early 1950s, Caleb Gattegno popularised this set of coloured number rods created by the Belgian primary school teacher Georges Cuisenaire (1891‚Äì1975), who called the rods r√©glettes. According to Gattegno, "Georges Cuisenaire showed in the early fifties that students who had been taught traditionally, and were rated ‚Äòweak‚Äô, took huge strides when they shifted to using the material. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/golang/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Prove it. Add enum types to golang, and convert a reasonably large project to use them. Measure compile times before and after.
&gt; better error handling You're trying to imply your opinion is fact.
I came to Go [from C++](https://dmitri.shuralyov.com/about); very happy with it. I have a great experience developing and maintaining software written in it. That wasn't the case for me with C++.
json.RawMessage might help. Try json.Unmarshal()ing into something of type var array []json.RawMessage and then pick through each item in the array, passing it through your clean-up-a-single-item function, which would cast it back to []byte and Unmarshal() that. If you truly need streaming functionality then I don't have any easy answer. But 8k records like the ones you show in playground are not going to take much ram.
One thing I saw is that in mutating operations there was frequently this pattern: select { case ce.quCh &lt;- struct{}{}: default: } What this effectively does is to unblock the queueWorker(). However, the issue here is that if the queueWorker goroutine is already busy doing something the default case in the select statement will qualify and the message will go unanswered. This seems to be likely in a highly concurrent situation. Does this matter? It helps that the `quCh` chan is buffered, but it seems like a weird mechanic to me. 
This article commits the common fallacy of thinking they can respond with errors after a handler has already done things to the ResponseWriter. HTTP has no backsies for headers.
I recently had to solve this exact problem. This was my solution, which avoids the casts: https://gist.github.com/jackmott/6f17dfed4c16c705e50a43c1b06c4bb9 But I suppose does not let one add new functions over the data type without recompiling, per the challenge? Another way is just to use a Node struct with a tag indicating the operation, and then a big switch statement inside of Eval. This is how they do it in "The Go Programming Language" 
really? please elaborate 
Passwords should never be encrypted, ever. Hash them. bcrypt or PBKDF2 are the standard. There's also Argon2 and scrypt. Pretty much anything else isn't acceptable by industry standard.
Nope, it's a password hash. Google "difference between hashing and encrypting" and have a read.
https://www.youtube.com/watch?v=I3NIAj8MtpY
rsync?
I have been programming in C/C++ for the last 20 years. The newer additions to C++ made it rather complex and heavy. Looking for a clean and simple language, we decided to use Go (instead of Java). It was a very pleasant experience learning and working with Go. There is a place for generics in a programming language, I hope to see it in Go. I just hope it does not lead to an abuse and excessive over complications trying to build DSL using compile time code generation (C++ meta programming) The fact that Go binaries are native format, you get to use a lot of familiar tools for debugging and troubleshooting. Overall, I find Go to be the best language with a runtime and garbage collection. For those requirements where you do not want an overhead of a runtime and GC, you have Rust and C++
I agree with this logic. Hashing the password rather than encrypting it is far safer for your users. There is no good reason to store a password.
Oh ya that isn‚Äôt my data , thanks like I said was trying to avoid iteration because it‚Äôs 15 sources every 5 min or so with bulk inserts etc ... If no solution exists I have iteration, and a dirty hack in mind and I‚Äôll test both and report back with performance of each method lol thanks for suggestion 
I don't understand why it's not just type Expr interface { Eval() float64 } If you are going to do a `.(Eval).Eval()` on every Expression, then why wouldn't you just say an Expression has to have an Eval function?
You can say a hash is encryption. However hashes are not as directly reversible as an encrypted value. Rainbow tables exist for all standard hashes but using strong hashing (bcrypt) + good salt makes rainbow tables almost impossible.
The description of the problem linked in the article (https://eli.thegreenplace.net/2016/the-expression-problem-and-its-solutions/ and https://en.wikipedia.org/wiki/Expression_problem ) basically prohibits doing this.
Thanks!
Real article title: Testing Go: Mocking Third Party Dependencies Using Third Party Dependencies [But in fact you can do all the above without a single dependency.](https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1) Also some great techniques here: [Mitchell Hashimoto - Advanced Testing with Go](https://www.youtube.com/watch?v=8hQG7QlcLBk)
Pretty like to let GOPATH go.
Cool, thanks. I will point people who are interested in sum types to that in the future. :)
To be clear: When I talked about "public proxies", the assumption was, that they'll exists and that they'd fulfill similar durability and reliability as crates.io does today (which I'd call "roughly similar to github"). Is, what you describe as "crazypants" *this assumption*, or is it the opinion that that should serve your need? Because AIUI Microsoft has already promised at least some resources towards the former (and given that crates.io. exists, I can hardly agree that that assumption is crazy). And if it's the latter, I don't understand why it wouldn't be.
&gt; That's not an accurate description of the packfile logic. For example, it'll pick different versions of files with similar names to delta against each other, to get a file to compress against its own previous versions. This is true, but I don't see how it contradicts anything of what I said. The claim you might try to attack is that packfiles are self-contained: That is they can not store an object as a delta to another object, unless that other object is also contained in the pack. If that is true (I'm pretty sure it is, but the packfile format is terribly underdocumented, I only found this out by accidentally creating one that violates this and trying to have git use it), there is no way around the fact that every packfile also has to contain at least one full (compressed) representation of all the source. Which is why that's the interesting claim you are trying to debunk, not how git is figuring out which objects to store verbatim and which to store as deltas. &gt; Pack files also get recombined; you don't end up with one pack file for every time you ran git fetch. Yes you do. They get recombined *after you downloaded them*, but the [git-protocol](https://git-scm.com/book/en/v2/Git-Internals-Transfer-Protocols) works by a) figuring out what objects you need and b) putting them into a new, ephemeral packfile. For transfer, there will still be a new packfile created. Which, again, if my claim from above holds, would mean that every git-fetch transfers at least one full (compressed) representation of all the source, just like when you download a zip. [edit] It turns out that while packfiles do indeed have to be self-contained, git violates that during transfer for efficiency. So I was wrong and git indeed only transfers the delta. Good to know. Leaving the rest of this comment up for posterity [/edit]
See [here](https://www.reddit.com/r/golang/comments/7za8g7/researchrsc_minimal_version_selection_go/dup649w/) and [here](https://www.reddit.com/r/golang/comments/7za8g7/researchrsc_minimal_version_selection_go/dur061o/) for why I don't believe that changes the fact that you pull down a full compressed source tree every time you run git-pull.
1)LAN-enclaved module publishing So intuitively within go.mod I replaced github with 192.168.x.x pointing a git server within the lan. Vgo build complained and failed to build. FAIL. 2)LAN-enclaved code hosting repository Vgo build currently imposes modules to be on github. Extra effort must be made by system admits to serve up web pages within the LAN that contain vgo friendly go-import meta tags. FAIL. I shouldn't be required to a web server. A git server should be sufficient. Both of these issues are related commented code since Vgo breaks my current package builds and effort to correct this to migrate to vgo is a PITA. FAIL.
&gt; https://golang.org/pkg/cmd/go/#hdr-Import_path_checking is totally unintuitive. I have never used a comment such as this nor do I believe I should. Import path comments have been a part of the ecosystem for a very long time. Same with build tags such as `// +build linux`. Maybe you should read the documentation and learn about these features before you speak about intuitiveness.
 &gt; Extra effort must be made by system admits to serve up web pages within the LAN that contain vgo friendly go-import meta tags. FAIL. I shouldn't be required to a web server. A git server should be sufficient. This is a totally separate issue. You might want to open a new issue or find an existing one about this. &gt; Both of these issues are related commented code since Vgo breaks my current package builds and effort to correct this to migrate to vgo is a PITA. FAIL. You do realize that vgo is just a prototype right? 
&gt;It will also allow the workflow of "getting last workable build That's not exactly what -p would do. The last workable build (according to semver rules) would be not just patch upgrades, but also minor version upgrades. The -p flag from the specifically excludes those.
awesome community desu
&gt; EDIT: and github.com/kylelemons/godebug does a pretty good job of pretty-printing, too. My request was for a standard library solution.
&gt; Use fakes, not mocks. If a part is hard to use under test, then either write a fake or redesign your part. From my point of view, this is not very helpful. It would be better if the article showcased the difference between fakes and mocks with some code.
&gt; Use what's out there, Firebase has a free service for this, and you own your user data. Interesting... I am using firebase auth but as far as I can tell the firebase email confirmation is only available through javascript and not Go. Do you have a pointer to how to do it from Go?
Isnt the real problem here, closed source libraries?
Tripped me up just now. So just wanted to share as a heads up.
Sorry if I wasn't clear - by last workable build I meant the `go get` without flags. So you get exactly what you left, and then, after making sure it still works, you apply security patches using `go get -p`.
Thanks, appreciate the heads up!
quCh needs to be buffered. Because it is a trigger for starting to process first hash-map and it have to keep information. queueWorker() is triggered async. Set operations have been blocked by first cache mutex (quMu) already. So i used select pattern. If value in tr(b-tree index) or qu(hash-map), there is the value in cache. The value can not be in both indexes. It is protected by quMu and trMu. ce.quMu.Lock() ce.qu[key] = item{Key: key, Val: val} ce.quMu.Unlock() select { case ce.quCh &lt;- struct{}{}: default: }
Even more fun is `"1.10.x"`, which will give you Go 1.7.4.
This has been fixed now. Just use `1.10.x`.
Now it works as expected : https://travis-ci.org/samonzeweb/godb/jobs/344754058 
Neat, I've been wanting to write something similar for prometheus.
[removed]
This sounds more like a critique of spin locks. I think that is a common way to do that.
What about AMD infinity fabric?
Yes, thanks. This link really helped. It has some good points. 
Oof, that makes the post even more painful.
You could take a look at this package: https://github.com/ugorji/go/tree/master/codec It has been quite a while since I've used it, but I remember that it had some good dynamic JSON handling, including streaming objects in arrays through channels.
How does mockery compare with mockgen/gomock?
I kind of hate yaml for this reason. Lulls you into a false sense of type literacy and common sense and then backfires on you at moments like this.
It sounds like there simply is no Go binding for that call. Depending on how tricky it is to implement you can write the Go to C binding yourself for this one. That is of course assuming the Python function is backed by the C implementation. EDIT: I assumed wrong, it's some sugar implemented in Python, here is the implementation. It makes sense this is not present in the Go version. https://github.com/python/cpython/blob/3.6/Lib/sqlite3/dump.py
How does this relate to gopherjs? Does it reuse some of it's code? Or does it use a compiler written from scratch?
It looks like vgo fork includes all of the 1.10 release. This is the last commit before the 'rc2' was dropped from the go version https://github.com/golang/go/commit/0b3f04d5f945bc8f4764e77d34c6fedf3327478f These changes are in the vgo source https://github.com/golang/vgo/blob/master/vendor/cmd/go/internal/work/exec.go#L955-L959 
I'm fairly certain that the compiler performance cost is negligible and the tradeoff is a good one, but that's really beyond the point, which is that design goals are meant to be weighted against each other, and it's ~~mind numbingly stupid~~ such a bad argument to invoke one or two against a particular feature proposal without looking at how much value the proposed feature delivers toward other design goals. It's valid to make the argument, "enums make the compiler *considerably* slower and they only *marginally* improve usability". You'd be wrong, but but forgivably so compared to "enums could make the compiler slower and therefore we will not discuss them no matter how more usable they make the language". I'm not saying you're making the latter argument, only that you appear to be driving in that direction.
Json schema to validate these things.
Yep, someone else had already pointed this out. Maybe the handlers could return a response struct/interface instead to get around this issue.
We were mostly talking about a private proxy, one that I would need to run myself. An e.g. gopkgs.io doesn‚Äôt solve the problem of ensuring accessibility of builds. It can be down independent of the code repo, and packages could be removed or modified, even if it is nominally immutable.
had this issue with openshift-cli on brew too.
# cd /home/xxxx/go/src/github.com/matcornic/subify; git submodule update --init --recursive fatal: no submodule mapping found in .gitmodules for path 'vendor/github.com/google/go-querystring' package github.com/matcornic/subify: exit status 128
We needed this implementation using our httpproxy library for a week ago. We wrote in our app exact this :) https://github.com/go-httpproxy/httpproxy
How on earth does that happen? All I can think of is it's either parsed as base 7 or is just a straight up obscure bug
Thanks, I created an issue: https://github.com/matcornic/subify/issues/6
[removed]
&gt; WatchdogSec=30s Does your application ping the systemd watchdog? If not systemd will be killing and restarting your application every 30 seconds.
Gimme didn't know what 1.10.x was so it fell back on 'auto' which, on this platform, was 1.7.4.
Watchdog is only really useful for applications that have a tendency to lock up and stop responding, with go's goroutines, it can be tricky to implement correctly. I would just remove the setting and not use the watchdog at all, it is generally better to panic if your application gets in a twist and let systemd restart the process. I don't think I have ever seen a program in the wild use this feature.
When downloading changes, the client informs the server what commits it already has, and the server can send just the missing objects. I have no interest in arguing the details of the git pack file format with you, and that has very little to do with Go. You seem to be confusing a data format with one of its applications.
Jsgo uses GopherJS as it's compiler... Sorry I should make that more clear in the readme. I forked the GopherJS "build" package and added tweaks to split the packages up, but the GopherJS "compile" package didn't need any changes: https://github.com/dave/jsgo/tree/master/builder In fact the compile package already has a function for writing the individual package: https://github.com/gopherjs/gopherjs/blob/fa8fb1b0e439b027e84c5af269725890b8cebadc/compiler/compiler.go#L178 
Update: I've improved the readme to make it more obvious it's using GopherJS to do the compiling.
Sorry if that sounded accusatory! I was just interested in the answer.
I was hoping to see a 1.0 release with function evaluation support. Unfortunately it seems it has been pushed back
That makes sense I'm new to using Go so all of this is a bit confusing to me Thanks for your help!
&gt;So when you say ‚Äúgenerics‚Äù you are really just asking the go compiler to add the code so you don‚Äôt have to check it into SVC. This is the silliest thing i've read on a programming forum this year. 
[removed]
yes sorry forgot to add to OP after posting solution found and accepted as well 
&gt;A reduction rewriter is not at all complicated, in fact, that is what compilers do - reduce. COMPILERS ARE REDUCTION REWRITERS. The go compiler is a reduction rewriter. Your argument against an enum-like type borders on hypocrisy considering the compiler already does this for slice and map(not to mention just the rest of the language...). &gt;The same goes for 'generics'. Mostly what people are asking for is simple reduction, nothing complicated, just simple authoritative reduction introduced into the compilation pipeline. The key word here is 'authoritative' as this feature MUST be standard. You are factually correct. It's amazing that you are being downvoted.
Secondary question, should the `Add` func on the `Users` interface accept a `user` struct, or a list of arguments?
The "communicate to cache" design will have significantly lower performance. Using a mutex would be higher performance. A more common (and higher performance) design is to protect the cache with a mutex. Then implement the cache with a map for lookup and some other mechanism for ejection. A common ejection mechanism is to use a list to keep track of lru. Here is an example. https://godoc.org/golang.org/x/build/internal/lru 
This looks neat! We just finished writing a tool in Go to monitor all our APIs and ship uptime/response time to influxdb. Keep up the good work!
I've felt this pain to many times
‚ÄúGoLang‚Äù
Nice article, I have felt this pain too, but I'm happy with the work around that are used the the sql package. It is effectively what you'd get with discriminated unions but with out compiler support. You'd have to know to use the special type eg `sql.NullableInt`, but that is no different from the sum types either `int | nil`. You'd get a compiler error if you don't unpack the discriminated unions, but you can't access the value without unpacked at least the value. Apart from not being part of the language there are only a few differences: * syntactically, discriminated unions use less code, and thus are less subject to error. You'd have to create all of your own in go * compiler enforced unpacking of them with a switch statement could be annoying if you just want the zero value anyway. 
Should mention https://mholt.github.io/json-to-go/ which generates struct from JSON. Give it a try by pasting some JSON.
Well shit. Just wrote one in Python last week. I‚Äôll check it out!
It looks like [goav](https://github.com/giorgisio/goav/blob/master/README.md) may do what you want. It's a go library that wraps libffmpeg. I haven't tried it myself, but I do know that ffmpeg can handle rtmp. Also nginx has an [rtmp module](https://github.com/arut/nginx-rtmp-module) that may work way better for your use case.
Neat -- seems very similar to Checkup: https://github.com/sourcegraph/checkup
Well I've also been coding in Go for 6 years. If you are telling me that after 6 years in Go you've never heard of even *seen* import comments and build tags then sorry my friend, you are either lying or you are a lousy coder. But no, it's not any of this, you are just refusing to see them on purpose, maybe refuse to use them even. No matter what you choose to do it is a fact that Go has [hidden pragmas](https://dave.cheney.net/2018/01/08/gos-hidden-pragmas), like it or not. I would argue that these comments are much more aligned with KISS than real pragmas but I digress. Now, I am not saying that these comments are great. In fact Rob Pike himself is growing wary of them: &gt; The cost here is continued proliferation of magic comments, which are becoming too numerous already. ‚ÄìRob Pike What you need to understand that is that vgo is a prototype. During the transition period, the tool might make some assumptions about GOPATH or use some magic comments. I don't think they will be necessary once the tool becomes part of the language for good.
Only comment i'd add for the author is to use the string literal syntax, it's a lot easier to read JSON then. https://play.golang.org/p/AWBhKCD9_Gm
What are you saying. I dont understand. 
Although using a linked list to implement the stack is correct, using a slice will often be both simpler and more efficient, as the data is contiguous and you're not spending all your time following pointers.
How about a shell function? ``` godoc() { godoc "$@" | less; }; ```
Thanks, hadn't thought about that. I'm using fish though so I'm using this instead function godoc --description 'go doc wrapper using less' go doc $argv | less end
The readme explains why `rsync` does not meet the needs of the author. (That being said, I wonder if [restic](https://github.com/restic/restic) might have been the tool the author was searching for?)
Should also mention https://github.com/galeone/rts that does the same, but as a library &amp; cli application directly from the JSON server responsse: create the structures for your API client in a few seconds
Why would it do that . . . clearly returning an error would be the logical thing to do in that situation.
There is Iceland conference in May. https://gophercon.is/
Or June!
Here's a good demo of the power of GopherJS + jsgo. I made a couple of small tools that make use of Go standard library packages. Remember this is all client-side code: This one compresses a drag+dropped image to a 50% jpg: https://jsgo.io/dave/compress/image This one zips drag+dropped files: https://jsgo.io/dave/compress/zip Here's the source: https://github.com/dave/compress/blob/master/image/main.go https://github.com/dave/compress/blob/master/zip/main.go
Yes, but AFAIK it is re-branded as GopherCon UK. Should be first week of August.
Improvement: It makes more sense to accept an io.Reader instead of a string filename, or at least have a second function that enables this...
Also, you should almost always check for an error first, before the defer close. If there's an error then the file will usually be nil, and won't need closing.
@davebrophy Thanks! 1. I agree with your comment that the needs of accepting an io.Reader. I am going to add it. :) 2. I will fix it. Thank you for the review! 
&gt; I am forced to use an incredibly wide variety of languages day to day. I don't try and write Java as if it were C, and I don't try and write Go as if it were Java. Go is different, and your approach should be different too. Well, i'm an engineer too and I also need to use a wide variety of language. Both Java and Go are imperative languages based on static type systems that are mostly similar (and very distinct from a type system of, say, an ML-family language). Moreover both are based around dynamic memory management. Thus, there is a set of problems and obstacles that will appear on such similar programming languages, and when they happened in Java, java overcame some of them by the introduction of **Generics**. There, i said it. It seems that *some* Go developers have some fear of this word. Generics, generics, generics. &gt;I don't try and write Go as if it were Java. So this is a very weak argument, since we're talking about a problem with the type system that will happen on both programming languages because of their both similar characteristics, and that has been solved just fine in the past by the introduction of Generics in java. Additionally, i don't know if it was you, but somebody made the argument that the compiler shouldn't be there to "write code for you". This premise is failed from the beginning: The compiler, by definition, is a tool that *writes code for you* by definition. The compiler writes machine language (in the case of Go) or bytecode (in the case of Java), and to achieve this it just requests you to tell it what do you need to be written. So basically if one is doing many times copy/paste of blocks of text to make sure interface XX is implemented for types int, float, a,b,c,d,e, and f; then he/she is doing a job that the *compiler* should be doing, and that, *obviously* the compiler could do with *zero* perfornance overhead on the compiled code. It's a no-brainer really. The fact that Go did not support generics shouldn't be justified with "you don't need them", because years of Java code shown it was needed and thus was added afterwards; nor "go generate is OK", since the end result, Go code, will still be full with copy/paste, making the code less readable and less maintainable. And readability and maintainability are values I'm sure you appreciate, since you are an Engineer first and programmer second. 
An better approach to custom json marshalers would be to use `string` tag and implement just a text marshaler for the type stored as stringified json. Example: https://play.golang.org/p/cJP3gYJ5_fo It does not unmarshal twice, is not limited to only json and is more readable.
&gt; People didn't find the need for generics in ALGOL, and that did fine for 21 years. ALGOL was created in 1968 and from 1968 to 1989 (21 years you mention), software wasn't expected to be written in sych a short amount of time as it is today. Thus, the need for more abstractions and more powerful language constructs. One of them being generics. So if you think Algol is just fine then, yeah, it's fine as long as your customers expect you to deliver software in years, not months; just as in the 70s.
This is how it looks in action - https://pbs.twimg.com/media/DWtJd0zUQAE3Zfn?format=png&amp;name=large
Now, I update WhatFromReader for accepting io.Reader
Here's your package running in the browser - drag+drop a file onto the browser it'll tell you the type: https://jsgo.io/gist.github.com/f13d749068f32ba667d0a921594ea4c0 ... here's the source: https://gist.github.com/dave/f13d749068f32ba667d0a921594ea4c0 You can recompile it here: https://compile.jsgo.io/gist.github.com/dave/f13d749068f32ba667d0a921594ea4c0 
Man you're right, I'm sorry. If only we'd realised all we need was generics in ALGOL, and we'd have been pumping out products in a tenth of the time...
Nice, but I think the package has a lot of work to do still. I'm all for tools that make proper encryption easier. (I haven't audited or reviewed the code here, just leaving this comment in passing, so take it for what it's worth.) One nit: &gt; making it convenient for things like storing sensitive data in memory in your applications. What is the value of this, exactly? I presume the data will have to be decrypted to be used; and the key will also be in memory at some point(s). If an attacker can access your process' memory, isn't it game over?
~~is Magical~~ kind of sucks
&gt; where in reality plain AES is very rarely safe by itself. Oh? I was unaware. Can you point me to some reading? I think it only makes it worse in as far as how bad or good an application's key management is. You are right however that I should probably clarify in the repo this package is not nearly in a state where people should blindly rely on it (or anything). Good call on that, I'll make a note there now.
I'm quite new to Go, and thus thoughts, issues, and PRs are more then welcome!
&gt;Man you're right, I'm sorry. If only we'd realised all we need was generics in ALGOL, and we'd have been pumping out products in a tenth of the time Well, generics in Algol (plus a host of other niceties) exists as "Free Pascal" and indeed it allows rapid application delivery by using the Lazarus IDE and frameworks. 
Compiling the binary and pushing it to a server is definitely the correct method of _deployment_ - as in, of an application. But when it comes to scripting, running them ad-hoc (compile + run) is what languages like Python/Ruby do by default, I don't see why Go would be any different. If anything it compiles faster ;)
&gt;Yet strangely I've never heard of "Free Pascal", Well, google it and download Lazarus. It is currently one of the few plataforms that can give you free multi-platform rapid GUI creation, the other ones being Qt, Electron, and Tcl/Tk. 
Surprisingly nice package! Don't be, however, surprised to not meet much enthusiasm about it in the Go community. Gophers are not to blame, the reason is: Go is a very imperative language. Functional concepts don't fit it so well. Usually, they're not worth the struggle. That's why most Gophers don't like them so much. I love functional programming. But, I still agree, that functional concepts don't find much use in Go. Though, I still admit your library is very nicely designed.
Sounds like a great idea, seeing as I've already heard of and used Tk, Qt, GTK, Electron, and wx.
I've been looking into getting solar panels on my house for years and I recently started working on a tool to help analyze my historical data, hopefully to make it easier to see what kind of a setup I need. This program takes historical kWh usage from my power company in csv format, then calculates averages and tells me the peak/low usage as well. Any feedback would be welcome. &gt;Total kWh usage: 593.241 &gt;Average hourly kWh: 0.458 &gt;Average daily kWh: 10.986 &gt;Lowest daily kWh: 6.299 on 01/12/2018 &gt;Highest daily kWh: 26.439 on 01/14/2018 &gt;Total days: 54 &gt;Total data points: 1295
(1) For as long as the go command writes to or reads from GOPATH, you will be able to set GOPATH. (2) Import comments are an old thing. The point of the example was to show how the old thing is automatically used to set up the new thing. I apologize for assuming everyone knew about the old thing. Once everyone is using modules, import path comments will be redundant and unnecessary. You don't have to use them to use vgo. You have to put a module line in go.mod. That's all.
Fixed all these, thanks. 
I guess ya gotta start somewhere. Keep working on it; keep learning, it's worth it if you like it.
research.swtch.com/vgo-module outlines those specifics. You can import mywebsite.co/left-pad as long as that URL has a &lt;meta&gt; tag pointing at archives.mywebsite.co/left-pad. 
Project is already quite overloaded. "The Go Project" arguably describes many different modules, for example. And GitHub has a very specific meaning of project that people have a strong association with; this is not that meaning. 
That's fine. If a user asks for upgrades I have no problem delivering upgrades. It's when the user _doesn't_ ask that I object. 
I don't get what you're saying here. The main reason you don't see functional concepts in go is because they tend to heavily rely on generics. For example, something as basic as writing an efficient `map` function simply isn't possible in go. A language doesn't have to be full on functional to have functional tidbits in it. Even java has an `Option`.
A done deal? No. We will certainly fix things. For example it seems clear that we cannot completely drop vendoring. But yes the proposal is that we do something more like vgo than like dep for the eventual go command integration. 
&gt; Using "Optionals" like in Java might save a few lines but ultimately it hides information. The intended control flow is obscured. Just take the example with filtering you created. It's not immediately clear what the return value will be. What do you mean here? Optional types in any language don't hide information, because the type is part of the optional type, eg (`Optional&lt;int&gt;`) is either an integer or a null value. Saying it "hides control flow" sounds a bit overly dramatic..they're just functions that wrap common logic. I do agree I would never use something like this, just because it would be a pain in the ass since it's not how any library or the base language works.
not sure what you mean, just use runtime.Breakpoint()
You can use runtime package, but IÔ∏è prefer using gdb. It works natively nowadays. IÔ∏è don‚Äôt know which version of GO and which version of gdb started being compatible. But it works well. 
Thanks; that enumeration is perfect. vgo looks great so far, can't wait to see the impact it has on the Go community when its ideas are brought into the main tool. 
Friendly reminder to add `defer r.Body.Close()` :)
On which part out of interest? I thought they closed themselves?
Unless gdb received support for green threads recently, gdb is still not support. I recommend using delve instead.
Thanks I ended up opting for creating a MJPEG stream and parsing that instead using [this](https://github.com/mattn/go-mjpeg) handy little library I found.
I don't see anything wrong with MSG. MSG is a outlook save file format and not a e-mail file format and its used for storing more than just e-mails. Also, the format is well documented as its a open specification format. https://msdn.microsoft.com/en-us/library/cc463912.aspx
sorry. what i mean is i want to set a breakpoint at the end of a function, so I need a no-op statement to set the breakpoint at.
True! It might not be worth it in the long run...
What do you mean by 1) save, and 2) stack status? You can get the stack when a panic occurs if you use `recover` to catch this eventuality, and at that point call `runtime/debug`'s `Stack()` function. You can at this point save this to a log file, or somewhere else if you like. Here's a [playground example](https://play.golang.org/p/4KfSz68BdD1).
Not in the US, but ravelin.com is an amazing place to work. They use machine learning and network analysis to catch fraudsters. Amazingly bright team and of course Go! 
How about you see what [Up](https://up.docs.apex.sh) is and understand some context here? Gist also links to the blog post - https://blog.ashfame.com/2018/02/deploying-lambdas-multiple-regions/ If you are really interested, go ahead &amp; give this a go yourself &amp; you will see whether the tiny script gives you the same output or not. Something doesn't become useless if you don't know what it does.
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://up.docs.apex.sh) - Previous text "Up" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
I also don't like Go. Here are my top reasons: - Pretty much no immutability (just like Java) - **Both** language _and_ stdlib backward compatibility guarantees have been broken _repeatedly_ (mostly in corner cases, but still, this to me is a red flag) - Bad error handling (no sum types) - No RAII, no defence against copying of resource holders (you need a linter to tell you you accidentally copied a mutex) - Composition. In its single-struct form it's OK, every other form (multiple, by-interface) is a broken afterthought that IMO should never have made it into the language - Interfaces, in that they are non-explicit and not covariant with implementors (cf. the famous pitfall when comparing interface pointer for `nil`) Interestingly, all these issues are solved in Rust rather well: - Immutability: First class support in Rust, it is the default, mutability i opt-in - Rust's error handling is the best you can get outside of purely functional languages - First class support for RAII - No composition, instead there's [`Deref`](https://doc.rust-lang.org/std/ops/trait.Deref.html) - Interfaces, called traits, are explicit, can express dependencies, can have associated types, work well with generics OTOH, Rust has no coroutines and kind of sucks when it comes to async I/O, especially in terms of developer experience / productivity. This is were Go is good. What's the takeaway? Use Go when you quickly need to come up with some sort of a network service, esp. web services. For stuff that is more systems-related and needs to be reliable and needs errors handled correctly, Rust is IMO a much better choice. For some purposes, using both languages for different layers might make sense (I am considering this for a project). 
bcrypt is one option, other good options found in golang.org/x/crypto are argon2, scrypt and, if you need, to follow nist recommendations, pbkdf2. But whatever you choose, you should implement cryptoagility. You should store the algoritm and parameters along with the hash in order to be able to change those later on
Or Common Lisp. There is a library which implements an HL7 client.
I hope you aren't *completely* limiting yourself to Go jobs. Like if you got an opportunity to work on something amazing (Doom 5! Self driving cars! Cancer curing protein folding!) and it was in C or Rust or Java, I would still take it! Code is still code. 
Again, just use runtime.Breakpoint()
One of many reasons I like go is startup time. Especially after working with JVM based language for years now on. I don't really want to bring JVM based bulid tool in to my stack. Are there any similar alternatives written in go?
Just use https://godoc.org/github.com/pkg/errors#example-Wrapf instead.
This is nice! TY!
Thank you
I agree that go really should have started with a better interface then ‚Äúfunc Error() string‚Äù. Not a full exception mind you, but something where you could add addition details. At least separate the type of error from the details. 
Say the limits in the description, which gets printed by -help. Then in your program, check for them and error out if any flag is out of the range. Do not convert them to the acceptable values since it's a configuration error.
I'll probably stick with that. It would be cool tho to have limiting option. 
I really like optionals, but as you've already probably found, it can be quite painful working with libraries and code in general that uses reflection and returns things that require casting or more reflection. This is a cool library, and probably something that was fun to make, so props for making it and giving it a go (and succeeding at doing what you set out to). But I have to agree with others here that I doubt many Gophers would use something like this, because it is sacrificing things that don't need to be sacrificed (namely, compile-time type-safety, performance, and some readability you could argue too).
[removed]
&gt; juju/errors was created for big, complex projects like Juju. Most projects are nowhere near that scale and have no need for stack traces. I beg the differ. If I have 15 layers of structs with methods and layer 1 yields an error I can't handle until layer 15, you bet I want to know where the initial error comes from. &gt; fmt.Errorf("opening project %q: %v", projName, err) is brittle. I want to know the exact code path that led to the error when looking at my logs. The easiest way to do that is with a stack trace, not with comments decorating the error. That's how all other languages except C work. I don't think pseudo error codes, because that's what basically go errors are, are useful in anyway. So back to my 15 layers of structs with method, I don't see myself decorating errors at every layer with comments. It all comes down to the fact that go errors aren't first class in the language. Panics are.
[removed]
[removed]
Not quite. The whole point of key-stretching algorithms is that they've got a flexible computational cost, to compensate for hardware improving as it does. Storing the key-stretching parameters would let them narrow down their attack but they still have to hash crack the password through the key-stretching iterations which for your average attacker should be "enough". If you were using a hashing algorithm that you aren't supposed to ( like MD5 ), making it obvious that you're using MD5 could make a significant difference in cracking those yummy hashes, but if the application is using MD5 in the first place, other bad decisions were probably made elsewhere and there probably isn't much hope anyways.
I don't think it's really that insecure, [and neither does the person answering this stack exchange question](https://crypto.stackexchange.com/questions/43471/storing-parameters-in-argon2-hash-as-potential-security-issue/43473)
What do you have in mind? On one project I've been involved with, errors include information about the user who caused the error. But that is domain-specific and it would make no sense for the error interface to have a `User()` method. Many apps have no need for the concept of a user.
&gt; If you are really interested, go ahead &amp; give this a go yourself No thanks, I use proper orchestration tools for my deployments: Ansible and Chef. These are much more suitable for automation, in part because they are more graceful in the face of failures. (Hint: cloud deploys don't always go smoothly, can take a long time and consume a lot of bandwidth. Don't build something that can't tolerate failure!)
I assume OP meant technical reasons lol
Context is a great example of one in the stdlib 
I assume you haven't read the "Gotchas to keep in mind" section, have you? With the simple approach you suggested, we'd be unable to reliably get the original error back, and often we do want this.
&gt; With the simple approach you suggested, we'd be unable to get the original error back in a uniform way, and often we do want this. But by using the simple fmt.Errorf() approach, you are making the decision that the best way to handle that specific error is to simply return some human readable message explaining the error. If you want to be able to return both a message and the cause you again need to use the custom error solution.
At least one reason is that `errors.Wrapf(..)` is easier to type than `errors.Annotatef(..)`.
&gt;it's more likely to have maintainers and potentially outlive the original contributor / project. That's not a technical reason. Also, juju uses their errors package in their Juju project which has 55k commits, so I suspect the juju/errors package will be maintained adequately.
Thanks! I didn't know such a spec exists. Now only a pure Go OLE implementation is needed :) (I know no such thing)
Do a network penetration tester, it's open ended but you can also quickly get some features going, like a port scanner.
Why would you ignore the problem of maintainers / users and insist on a technical reason? Lol?
The stack trace should be sufficient to point you to the file / function where the panic occurs. From there, start with a read-through of the code. You should be able to make a list of suspect variables. Trace those variables back through the call chain to see if you can find a potential path where one is never populated. Much debugging is dependent on your runtime environment. You'll have to tell us more before we can advise you: add logging messages, print to the console, etc...
What are the differences compared to twirp? What are the benefits of using xservice over twirp?
I have no idea what that means, but I will look into it before choosing it as our project for the bet. Do you feel this will be harder in either of the languages, or highly doable for both? Also no a side note, thank you very much for the opinion/suggestion.
The "Network Error" is on another side of the spectrum: it's too broad. The `juju/errors` and similar packages allow us to create specific error messages with the higher level context.
As stated, `juju/errors` looks like it's really useful if you don't bother to check the type of your errors. That is usually how applications start out anyway.
I use a log wrapper which has a `log.Errorf()` method which does a `fmt.Errorf` and also logs the result along with source code location.
In your code, I do not see a reason for continuing if an error happens. For example: data, err := ioutil.ReadAll(resp.Body) if err != nil { errs = append(errs, err) } If `ReadAll` fails there's isn't anything you can do. Same with all other cases. You can simply return the error or add context with `fmt.Errorf` or `errors.New`. So basically return early. If for some reason it is really important to gather all the errors that happen in the function, then you could gather them as you do but instead of `[]error` you could return a custom error as `error` so that you do not alienate the users of the package. Perhaps something like this: type ErrorResponse struct { Response *http.Response Errors []error } func (e *ErrorResponse) Error() string { ... } You could even add the data in the struct if it is important.
&gt; insist that somehow juju won't maintain their own packages? That's not at all what I asked. Lol.
Smart + saves you an unnecessary dependency. üëç Just be careful you do not end up [handling the error twice](https://dave.cheney.net/2015/11/05/lets-talk-about-logging). &gt; If you choose to handle the error by logging it, by definition it‚Äôs not an error any more ‚Äî you handled it. The act of logging an error handles the error, hence it is no longer appropriate to log it as an error.
Why? That sounds like a bad idea.
Indeed. Typically the error is logged and returned, and then the code receiving the wrapped error issues a more general http.Error to the user. The whole approach is probably only suited to web applications and system daemons.
Fun link to try: https://gopherjs.github.io/playground/#/UdyEyAtguS.
Ask your network admin friend, he should know. It's basically his job, but from the hackers point of view. I think threading does Go an advantage, but if you can do async IO in Python, it might work out even better than Go.
At least people clearly said that generics and dependency management are badly needed.
bcrypt follows the [crypt(3)](http://man7.org/linux/man-pages/man3/crypt.3.html) tradition of output formats. Most password hashing (key derivation) functions in go just return a byte slice: https://play.golang.org/p/_Aw6WeWC42I From the byte slice it is impossible to tell which kdf was used, so that information needs to be stored. Also, bcrypt provides the CompareHashAndPassword() -function, that will parse the parameters from the bcrypt output (the salt and the cost parameter). With the other kdfs you need to store the salt and other parameters (but not the password itself) to be able to perform the same key derivation operation and compare the outputs.
A framework is a reusable set of libraries pieced together for a specific purpose. So you may quickly find yourself using or building one.
Indeed you may. You could argue any collection of libraries that you might use commonly in similar way could be considered your own personal framework. There are certainly those that actually set out to be frameworks for other people to use as frameworks though, not as groups of libraries.
The architecture is quite simple: It opens a single ICMP connection (per address family), and correlates incoming packets (either destination unreachables, or echo responses) with previously sent echo requests. It contains a [simple](https://github.com/digineo/go-ping/tree/master/cmd/ping-test) and a [more complex](https://github.com/digineo/go-ping/tree/master/cmd/multiping) demo application. Tests are still a bit lacking, mainly because testing low-level socket connections is hard :-) Any idea suggestion, how to improve this situation?
&gt; Like last year, we took the results of the statement ‚ÄúI feel welcome in the Go community‚Äù and broke them down by responses to the various underrepresented categories. Like the whole, most of the respondents who identified as underrepresented also felt significantly more welcome in the Go community than in 2016. Respondents who identified as a woman showed the most significant improvement with an increase of over 400% in the ratio of agree:disagree to this statement (3:1 ‚Üí 13:1). People who identified as ethnically or racially underrepresented had an increase of over 250% (7:1 ‚Üí 18:1). This is awesome and a sign for the right direction this community and the go team is heading! Way to go!
There's no way that 64% of people develop Go on Linux. I'm guessing those people mis-read the question in that they develop Go *for* Linux. 
why do you think that? go doesn't seem to have any tools that tend to drive people towards windows like C++ (debugger) /C# (visual studio) do
[Meanwhile at google](https://i.imgur.com/DjGVnnJ.gifv)
Where are the authentication frameworks? I've looked in the past and I haven't found much. At least nothing that looked easier than rolling my own one-off system, and I really, really hate writing my own auth.
Just been saying to a friend that's had a read through this; I'm not sure if anybody will find this interesting, but I wanted to finally actually use my blog, and figured a bit of history would be a decent place to start.
In case someone is having this problem, I fixed it by manually overwriting .bash_profile
Aka the rise of VSCode. 
I‚Äôm not so quick to dismiss this. I regularly develop Go code on my MacBook by writing the code in vim and then using Docker to build and test it, so technically I‚Äôm developing on macOS **and** Linux every day. (And, relatedly, I checked both of those options when I filled out the survey.)
Authentication, etc. would be part of a larger framework. On its own, that would just be a library. I'm new to Go, myself, so I can only comment on things that are true of other languages or that transcend language. A Web framework for Go would have many of the things that I mentioned. A proper Web framework for Go may not exist yet for all I know.
&gt;You're not explaining why there is or isn't a problem. You've claimed that maintenance is a problem, and the same people that maintain/use juju/errors maintain/develop [this](https://github.com/juju/juju) project. The project has been actively maintained for the last 3 years at least, so I don't see why there would be a concern with maintenance of their own packages they use in their main project. &gt;You're so focused on Juju (whatever that is) you seem to have lost the bigger picture... That would be the main project of team that built the package the original question was about... Bottom line is juju is clearly actively maintaining juju/errors for their larger project, so using the popularity of the library as a reason to use or not use it is just silly.
&gt; if you don't need those problems solved Who does not need these problems solved in any non-hello-world real world app?
I've been using Go for 5 (almost 6) years and I don't know of any frameworks that support authentication in any reasonably friendly way. The ones I've looked at seem so general and so configurable that one needs quite a bit of expertise in auth just to use it; on the other hand, I can write my own auth code that meets the needs of my application even if it doesn't "solve auth" for every conceivable application. That said, maybe there's something good that I'm now aware of, hence my question.
Not every HTTP server is a web app, and not every web app's backend is pure-Go. Many don't even have a concept of users much less sessions, and many others' backends have an auth service that serves as a gateway to a user- (or at least auth-) agnostic backend (or back-backend?).
&gt; Not every HTTP server is a web app Over 99% of users coming to this sub asking 'which web framework in Go' are trying to build a web app. This question keeps coming at least once a week. The state you described is a minority.
"raises funds to send diverse participants" Diverse doesn't mean what you think it means, here you're using it to say "send women". Diverse could be to send a poor dev, or an Australian dev, or a Kiwi dev, or a dev from a different language background. Might start a Fundraiser for "Aussies who go" and see if we can get a couple of "diverse" attendants from Australia...
Because of business. Who supports Unity now?
Why not fork nats.io and upgrade it instead of rewriting a entire thing?
Nope, nope, NOPE. They're not the minority. People who build web apps using http are not in minority. This denial needs to be removed out of the Go community. They finally accepted to have an official dependency management tool. Generics for Go team and web frameworks for Go community are only a matter of time, it's just that it will be a long time due to their adamancy. Remember, people who build web apps using http are not in minority.
When Microsoft pays shills to aggressively market their product in every tech forum it's no wonder. 
[Yes](https://play.golang.org/p/TB0UP0ox-y6)
Personally I won't donate to any cause that doesn't aim to help three-legged men from Portugal who are fluent in 3 languages but misunderstood by society.
&gt; The thing was... I didn't feel like that at all. I still felt like other people's code was extremely confusing and difficult to read. (about Scala) That's also my experience on other languages unless the team has very strict guidelines that are constantly enforced. However, with Go there's often only one way, the idiomatic Go way and code is much simpler to follow albeit sometimes verbose. I take verbosity over complexity anytime. After decades in the trade one starts valuing simplicity and clarity highly.
Show the code that sends the alarm time to the server. Log the received alarm time and current time in serverReceive to ensure that the times are as you expect. The loop that waits for the alarm will use most of CPU while waiting. Consider computing a time to sleep and sleep for that amount of time. 
https://research.swtch.com/vgo-tour
I'm telling ya. *2018* is the year!
Yup, my team is Linux only and we do primarily Go development. I've tried on Windows and it's kind of a pain.
I thought `dep` was going to be their approach to dependency management.
Funny, I've just realized that Russ Cox have family resemblance with Jim Carrey. 
https://github.com/richardlehane/mscfb
Error handling is not bad. 
The `alarmSub` and `wsInit` functions open separate connections to the server. The alarm may be triggered on the connection opened by `alarmSub`, but there's no code to receive it. Nothing triggers the alarm on the connection on opened by `wsInit`. 
I suppose that Nats still can be used inside Centrifugo as PUB/SUB broker and nats-streaming for message history/recovery - i.e. be an Engine like current Memory or Redis engines. The only thing that will be missing is presence feature - which is now simple to do with Redis. As far as I understand there is no need to fork Nats - it's just a question of integrating Nats.
Commenting for later. Looks useful
I find it hard to believe that many people would disagree, I [certainly don't](https://www.reddit.com/r/golang/comments/5i47ha/the_saga_of_go_dependency_management/db6bn20/). Despite it being clear that versions are problem zero from every other successful language in existence, dep engineered it's way through from the other end. It's odd because I remember they later said [versions are important](https://www.reddit.com/r/golang/comments/65vlmg/dep_status_week_of_april_17/dgeqbt0/) but continued to work through building dep without them. I would have been entirely okay with this personally, but what began to irk me is how aggressive the dep contributors particularly Sam Boyer was trying to literally force the adoption of dep through activism. Which led us to the point of the unusual dialog where he was saying it would be in Go 1.10- forcing RSC into an unfortunate position to correct him publicly. But I never saw any real attempt at clarification- the wording was always very vague on github and you could still find links about trying to make it to mainline. Spreading misinformation feels like it was a very calculated strategy used by the dep team and it caused some real damage and splintering in the Go community. People are upset because they truly were led to believe dep would be the official tool. The Dep team even hunted down and publicly pressured other package managers to terminate their projects under the disingenuous guise that it was all in the greater good for the Go community. I say disingenuous because you can really feel the undertone of discontent in the post the Dep author created, if the only goal was to unify dependency management then that is now officially underway, why the salt? &gt; i also have process concerns. vgo, as currently conceived, is a near-complete departure from dep. It was created largely in isolation from the community's work on dep, to the point where not only is there no shared code and at best moderate conceptual overlap, but a considerable amount of the insight and experience gleaned from dep as the "official experiment" is just discarded. One of the many reasons I dislike dep is because it was developed in a silo without community collaboration early on, when all the critical design decisions and user interactions were being defined. Many including myself had concerns about things like using two files for versioning, command names, use of toml and many other things. The issues were closed without debate or community discussion for either "bike shedding" (how dare users have concerns about user facing portions of the design)- under the premise that those things could be discussed at a later date. But myself and everyone with these concerns knew such things could not change at a later date and sure enough as dep acquired (notice the term, acquired, as we went over this above) adoption backwards compatibility suddenly was a viable defense to justify the contentious early design decisions. First time I tried dep I immediately loathed the fact the command itself felt completely foreign to anything in the Go toolchain, but I still tried to give it a fair shot. What did it do? Deleted my existing vendor folder I just got done updating and validating and shit out **two** Camelcase.**toml** files.. **toml** files in a language with arguably the best language parsing / toolchain library support I've ever seen. Went to the Slack channel and asked why it deleted my vendor folder, they wrote a bug. Asked why there is two files and why are they toml. Was told I was bike shedding, explained what bike shedding actually means and why the issues I was raising were important to me and was told they were already discussed to death. Searched the issue tracker saw nothing but the Dep author closing issues with "there will be time to change all of this in the future" or "this is just a prototype", or "this has already been discussed", but prior discussions were: "this is just a prototype".. lol. I still appreciate the work done on dep even if I disagree in the way everything played out. I feel like if they had focused less on getting dep integrated into the toolchain, and more on what the original message was said to be things would have played out much better for everyone. The Go team certainly doesn't deserve any flack for vgo, in my opinion.
I don‚Äôt even own a windows machine, and only open my work issued MacBook for zoom meetings. Most every engineer I know runs Linux at home as well, care to join us? :)
I think one of the core go idioms is something to the degree of: *don't panic* ( if i am not mistaken...)
Oh! Are we playing this game? I‚Äôd like to play! &gt; One of the many reasons I dislike dep is because it was developed in a silo without community collaboration early on, when all the critical design decisions and user interactions were being defined. You mean exactly like vgo? &gt; Many including myself had concerns about things You mean like with vgo? &gt; First time I tried dep I immediately loathed the fact the command itself felt completely foreign You mean like vgo, compared to literally any other dependency manager ever?
Just a heads up, I see a lot of people commenting this, but you can save a post for later with the "save" link under the post. On the official mobile app it's the little bookmark on the top right of a post too. I'm not sure if other apps hook into that functionality though I guess.
In regards to the posts mentioning that a framework may be beneficial in the sense that is solves a lot of common problems for you... Learning how to solve these problems I think is very valuable. For instance, implementing proper CSRF protection isn't too difficult, and in implementing a solution, you hopefully understand why it's needed in the first place. I'll be writing an article in the next few weeks here that will go over building a web application in Go without using a framework, while solving all of these common issues (some using third party libraries, such as for authentication via JWT). The structure that will be explained there should cover most use cases.
Women who go. I is impressed
I haven't used anything but linux as my primary development/work-workstation os since windows XP.. I only use windows/macos when I need to actually develop for those platforms. Linux distributions are in many ways easier OS'es for developers since those OS'es are centered around being able to recompile any system package from source and so on having everything development related at arms length. Since I am a developer and I know what I want/need I can develop a system suitable for my work flow using any window manager i want.. Nothing I've seen doesn't even remotely beat that, it makes any potential drawbacks with Linux insignificant in comparison. 
I have tried D and I even implemented some kind of primitive web stack on it for my needs.. it was in 2011-2012 I think.. and at that point there was very little support in D for web programming IIRC. But then I discovered Go and it became quickly evident that it was a better tool for what I was interested in.. web services, web clients, db apps, easy concurrency, things like these.. and I have never looked back.
Agreed.
[removed]
I think that error handling could be more useful without getting in the way of application-specific use-cases. In my opinion, it basically boils down to having something in the standard library that lets you wrap an error without losing information. That's why libraries like `pkg/errors` and `juju/errors` exist (and many, many more). It seems like if there was just a "previous" field that you could set, or that was set on errors then it would help. That's the core of many of these error wrapping libraries; a custom type with a previous field to make a singly linked list of errors.
Even then (2011-2012) there were more libraries for Go (for my domains of interest) and the language looked fast moving (growing adoption rate and growing number of libraries). Go is quite easy to pick up (compared to D) and looks well supported by a big and diverse team of experimented developers - so it looks like a safe bet when it comes to investing time in learning it. D is a good language too but it's domain of suitability is different than Go. Although Go does not have some of the features I would like, I'm still very productive with it at what I do and that's what counts in the end.
Does it answer any of the questions?
I tried it and I like it, but I can't use it for work (not my decision). So....
I love D, but its ecosystem and community is still too C-minded; while the language allows for more modern ways of OOP abstraction (without extreme performance penalties), a lot of people and libraries are occupied with implementation details. Another thing that put me off is that there are a lot of ways to do one thing; this increases language complexity immensely. But this is not why I never use it. I never use it, because there is no job security in it -- yet. Go, on the other hand, strives for simplicity by leaving a lot of things out by default.
I tried D back in 2009. I was trying to migrate away from C++ and Python. It was great, save for the 2 competing standard libraries, a closed official compiler, gcd lagging behind and a LLVM-based one just starting off. Also, Linux was a second zone citizen. It didn't pan out in my case (Linux was and still is the primary OS in scientific circles) Then, Go came along...
"Golang‚Äôs tooling is inconvenient" and "Learning difficulties". Are we already the 1 April ? A joke ? 
Cross-posted to https://stackoverflow.com/q/48953803/720999
I very often throw up a quick static informational site on a subdomain. None of those things are necessary except logging (which is always good), but you can solve logging with VERY simple code on a static site because you aren't going to need to log much and might not even be running the static site for long. For example, it took me only a few hours to create [a static site generator](https://github.com/PaluMacil/visit) from scratch using only the standard library with a mere 150 SLOC. It's super simple because it just shows people how to visit my house, but all I need to do it add a gohtml file containing the Bootstrap 4 page body to the content folder to get a new page on the nav bar. It's a lot of functionality for doing it from scratch, on my first time using html/template (normally I write Angular apps), and it isn't meant to be an example of perfect code by any means, but it didn't need any of the features listed previously. You're right about a lot of people needing auth and such, but I don't know what the split is, and I'd guess that a lot of people are also making internal sites that might just need to look at a static token in a header to authenticate, since a lot of Go web apps are just services talking to other services behind a firewall in some companies.
And your first issue was addressed by go??????
The one thing keeping me from using D for any project is its garbage collector.
A laundry list of statements, with no explanations. Quite a useless amount of resources. Wasted my time on this one.
Nice use of semaphores! 
I tried D about 7 or 8 years ago, I think. I vaguely remember liking a number of things about it, but, if I remember right, dealing with `dmd`, Phobos vs. Tango, and the lack of libraries just made it more annoying than I had a reason to deal with.
It's an thinly veiled advert for their terrible software house by the looks of it.
I always sort of hate having to write this sort of feedback, because I know it feels like I'm dumping on your work, but there's two important reasons to write this even so: First, to help you learn how to write Go in a better way, and second, to ensure that bad practices don't take hold in a community because someone put up a library with questionable practices and nobody said not to do it that way. (I've seen it happen in other communities.) Compare your library to the idiomatic way of writing a Go-based TCP server. I'm just bashing this out for comparison purposes, so it won't compile and may have errors, and I'm going to skip error handling for clarity. func StartServer() { listen, _ := net.Listen("tcp", ":8080") for { conn, _ := listen.Accept() go HandleConnection(conn) } } func HandleConnection(conn net.Conn) { defer func() { conn.Write([]byte("QUIT")) conn.Close() } conn.Write([]byte("WELCOME")) bio := bufio.NewReader(conn) for { line, _ := bio.ReadString('\n') if line == "QUIT\n" { return } conn.Write([]byte(line)) } } Event-based APIs are hacks for runtimes that aren't capable of maintaining many run contexts at the same time. When you have Go in hand, you really want to just... _write_ the servers. While I hope you learned a lot writing this for yourself, and you can and should count that as a big win for yourself, you really shouldn't write any real servers with this library. Again, I'm sorry that this feedback has to come, but I think even if it hurts less in the short term, it's ultimately even more harmful to not feed back to you that this is not really a good path to go down.
&gt;Make TCP socket programming easy. Seems like this package makes it more complicated, actually. One of the things I love about Go is that I *don't* have to write code like this. No [complicated](http://lucumr.pocoo.org/2016/10/30/i-dont-understand-asyncio/) event loop frameworks, no callback soup, just straightforward code. What problem does this package solve?
For what it's worth, glide.sh is phenomenal. As far as I can tell it has optimal behavior for basically everything you could want. Takes maybe 15 mins to figure out, then you're set with a proper human file &amp; lock file.
I think you misread my posts, or you're deliberately arguing against straw men. In either case, I've lost interest.
Honest question, what does dependency management have to do with the language itself? I don't know of many languages with built-in dependency management. Usually the community creates a tool that everyone uses, but that is separate from the compiler itself (see npm for js, gem for rb, composer-php, cabal-haskell, maven-java, conan-c++... I could go on).
Hmm, I like the tooling and thought it was the easiest language I've ever learned. If you need debugging, I recommend using Delve (and the VS Code Go plugin makes great use of Delve). Otherwise Goland is becoming popular fast and is from a company with a good track record for fully featured IDEs. I suppose if you're afraid of commandline, the tooling could be considered very limited, and if you've never used a language with pointers, it might seem complicated to learn, but in the end, I think having only value types where some values are pointers hides less complexity than having some things be reference types and others be value types.
Yeah, but they don't hide anything in a way that seems deceptive to me. I don't think it bothers me even if it didn't say anything useful. You're right, but... companies gotta advertise somehow.
I wrote a blog post about it a while back outlining some of the pros and cons for anyone interested https://medium.com/@joeybloggs/gos-std-net-http-is-all-you-need-right-1c5555a9f2f6
It's beautiful! I can't decide if it's too beautiful with so much color (thus distracting for me), but I really like it aesthetically.
link is dead
go dep / glide works fine.
Dependency management has nothing to do with the language, but everything to do with the platform. Having a standard way to share packages means the community will make their packages compatible with it, which eases development and encourages people to make libraries available. I came to D and Go from Node.js and `npm`, as well as Python and `pip`, so I had expectations coming in. I started with D back in 2011 and continued until 2013 or so, and Go 1.0 was released in 2012. At that time, there wasn't really a "standard" package manager. The most popular one was [orbit](https://github.com/jacob-carlborg/orbit), but as you can tell it didn't contine, and [dub](https://github.com/dlang/dub) was still in the planning stages. I even took a stab at creating my own, but that kind of failed when there wasn't even a standard http library, so I started writing one as well. The JSON library in the standard library sucked, so I made some improvements to it and after discussing it with the community, realized that the standard lib json library was essentially deprecated in favor of [orange](https://github.com/jacob-carlborg/orange), which was still very much a work in progress. So I realized that I'd have to do a ton of work to get anything done, and it just wasn't worth it. When Go came out, it had `go get`, which was a way to fetch and compile any github project. It didn't do version management, but it wasn't long until projects popped up to fill that gap. I found that I could find projects that met my needs quite easily and used [glock](https://github.com/robfig/glock) (still do) for reproducible builds. The Go community was focused on building tools for servers, whereas the D community seemed focused on bindings and games, so Go was a natural fit in package selection. The situation is a bit better now in D since `dub` took over and `orange` is a bit more full featured (and I think it's still the go-to solution), but unfortunately it didn't fit the bill when I needed to make a decision. I was looking for a replacement for Node.js for our products because we were running into CPU and memory limitations, and Go was able to solve pretty much all of our problems with Node.js, and it was easy to learn to boot. I have no problem with package management being a completely separate project (though I think it makes sense for the language developers to either bless or develop one), as long as it's high quality and standard. I've used a lot of browser package managers (browserify, requirejs, webpack, etc), and it's a nightmare finding the perfect mix of package support and packages you need for a project. Programming languages *need* to have a plan for this from the get go, and D just didn't deliver when I needed it.
`go get` was good enough until other solutions arrived, like [glock](https://github.com/robfig/glock) and [gopkg.in](http://labix.org/gopkg.in). Go came with far more batteries included than D, which was enough to get me to switch our company to it at the Go 1.0 launch. I was tempted to pitch D, but it just couldn't deliver on the things we needed. Go was good enough, and it continues to get better.
Awesome! I've been using the fastping package for a few years and recently ran into scaling/feature issues. I've been dreading writing my own package to replace it, but now I can try this one. Looking through the code, it is pretty much exactly what I was planning, so thanks for saving me the time! My main problem with fastping is it doesn't handle adding a lot of hosts at once and just locks up. 
Perhaps those libraries don't do it well, I've only used `juju/errors` personally, and only briefly before deciding it wasn't entirely what I needed. It is possible to generalise it by making errors exhibit certain kinds of behaviour though. You wouldn't be able to rely entirely on a wrapping library to handle your application-specific use-cases like fetching the user form an error, but a library could help you avoid the error wrapping boilerplate that you'd need for it, and provide you with utilities to find and extract information from errors of a specific type. By generalise it there I mean, you will still be making an error that fulfils the standard error interface, but you might also be implementing your own error interface that includes a `User() *User` method or something. The error wrapping library doesn't need to know about that to help you find it, it just needs to provide a utility for finding an error in a chain of errors that fulfils that interface. I created an error wrapping library for internal use where I work, and it does this. In a past iteration, it had a function that took an error, and a function to test the behaviour of the error (basically a `func(error) bool`. It would return the first error it found in the wrapped error chain that made that function return true, traversing it for you. You could make a function that returned every match, the fist, the last, etc. You could make more utilities around those to do things like quickly return the user (or users) that caused an error. All with the help of the error wrapping library, letting you avoid writing that part again.
&gt; A library sounds great, but something that still seems opinionated enough about error handling that I'm not sure that it fits the standard library. A more concrete example of what it might look like could change my mind. Is there a reason you feel it should go directly to the standard library before being tested as a third-party one? I mainly feel that the stdlib could benefit from errors just being more useful out of the box. I don't know what the solution to that looks like specifically. Maybe it's similar to the current error wrapping libraries, maybe they'd think of something else. It'd just be nice to have something to handle this quite common requirement. I'm happy enough to continue using a library really, because whichever way it went I'd still have to manually be calling things to decorate the errors as long as it was Go 1.x. &gt; The entire error library in the aforementioned app, including line breaks, comments, import statements and everything else, is only 100 lines of code. Realistically, it is a pretty small lift to recreate. But in reality if I had an application with similar error needs, but with different metadata, the vast majority of it could be easily copy/pasted. I'm not sure it needs to be any more complicated just to cut down on a few lines of code later. The library I've made for internal use is a little bit more complex it seems, coming in at a 1,764 lines total (including tests). I could definitely simplify it now I know what I need from it better though. I'd still like to improve parts of it's API too. It's been really useful in every application we've put it in so far. Many of which define their own custom errors types too.
Are you using the standard library sql package? You should be passing around a `*sql.DB`, not an actual connection.
I really like the api.
&gt; it‚Äôs ecosystem and community are still too C-minded This is why I still use it. I use D and Go for different types of projects. Hope to never touch C++ again. 
&gt; I mainly feel that the stdlib could benefit from errors just being more useful out of the box. I don't disagree in theory. Something would be nice, but what that something is has not made itself clear. There was originally some talk of whether `pkg/errors` should be included out of the box or not, but I think that it has proven to have some shortcomings in practice. This is why it may not be wise to go straight to the standard library. You might not like what you get. If has proven itself to be generally useful in the real world, like `golang.org/x/net/context` did, then it makes sense to consider adding it to the standard library. If there is a good way to generalize error handling in Go, a third-party library will eventually appear with those patterns. &gt; (including tests) To be fair, I skipped the test lines. But even including them it is *nowhere near* the depth of yours. It is certainly tuned to the specific needs of the application, and that yours is substantially larger suggests to me that different applications do have vastly different error handling needs, which does not come as much of a surprise. Which I think is the problem with trying to find a one size fits all solution here.
I'm ambivolent. On the one hand, I like that Go imposes few architectural requirements, but on the other hand I often find it tedious (and repetitive) to just "*write* the servers", as you say. I've often wanted utilities to help write protocols. I'm not sure that this is the best way to do it, but the OP has definitely identified an itch that needs scratching.
This looks very neat. Databases are still quite a rough edge in Go; not terrible, just less user-friendly than they could be. I'm excited to explore this further!
&gt; Oh! Are we playing this game? I‚Äôd like to play! ? &gt; You mean exactly like vgo? RSC has published 28k words across 7 documents containing extensive details on the design of vgo, for the sole purpose of rigorous public review like any other very serious Go proposal. But lets pretend they are more like the Dep team and simply started writing code while ignoring feedback, then started apply public social pressure to current package managers in the name of collaboration to close their own projects. Even in this fictional reality we have created it certainly would make more sense for the Go team to do it, they are responsible for the vision and success of the Go project. Their collective experience has delivered what the Go we have all came to know and love is today despite it's shortcomings. So no, not exactly like vgo. But if it was I would much rather it be the group of people responsible for the project, who have a proven track record of delivering tooling I've already determined is high quality and meshes well with the unix eco system. &gt; Many including myself had concerns about things 90% of all concerns I've seen has been purely non-technical "but I thought dep was going to be the official package manager" which was caused solely by the spread of misinformation by the dep team. The other 10 percent has been non-technical skepticism "i dunno about |versions|projects|[a-z]+" without any technical backing. **Would you like to provide me an example of serious technical feedback that is within contention of the vgo design- that has been closed due to "we can change it later" or "this is bike shedding" or "this has been discussed to death"** ? Hint- you won't because that is not how design on the Go team works- the winner of contentious technical topics emerge from merit not social pressure or dictatorship which is why I wanted the tool designed by the Go team. &gt; You mean like vgo, compared to literally any other dependency manager ever? You must be misunderstanding why so many people and I said dep feels foreign. It does not feel "Go like", "unix like", or "package manager like". There is nothing familiar about freezing random moments of time for your project and calling it dependency management. It's leaving out literally the primary thing that dependency management does- adhere to the developer created versioning contract. Instead it crapped out a couple incomprehensible and near identical files in my otherwise clean workspace. I know this was mostly just an emotional response you put very little effort into, but I figured I would respond seriously using deductive reasoning because that is how experienced software engineers deal with technical issues. Have a good one.
Whatever floats your boat. I think D is great for OOP-based and contract programming and would rather see less of the lower level details.
Thanks!
/u/Slavos17 seems to be a corporate shill for K&amp;C. His/her only.posts are either posting similar blogs posts to this one or commenting other other posts going to the website. This garbage doesn't belong here.
Zero lines of code to use [complex128](https://golang.org/pkg/builtin/#complex128), with an extensive library in [math/cmplx](https://golang.org/pkg/math/cmplx/).
&gt; My main problem with fastping is it doesn't handle adding a lot of hosts at once and just locks up. Have you tried using a local caching resolver, like unbound? Your upstream DNS might have throttled you unexpectedly. I once locked myself out of Google's DNS service, because I ignored the TTL and repeatedly asked for the same records... Also, `go-ping` might have an issue with a large number of parallel pings. In `multiping`, I needed work around this by [introducing a small delay](https://github.com/digineo/go-ping/blob/master/cmd/multiping/main.go#L98) before actually sending the echo request. Come to think of it, the [underlying `icmp.PacketConn`](https://github.com/digineo/go-ping/blob/master/sending.go#L78) might need to be locked.
[removed]
I've [introduced a lock](https://github.com/digineo/go-ping/commit/c697d6ad5f5314717c362d81e94930adb7a822fd) around the critical section.
&gt; Why does this exist? Probably for the bit at the end. "Strangely enough, Go has built-in support for complex numbers." z := 1+2i fmt.Println(z + z) But three line blog posts tend to not go over well.
Stack Overflow had a project for a while where they wanted collaborative creation of documentation in a more organized way that just Q/A. It ran for a while but then they abandoned it (see https://meta.stackoverflow.com/questions/354217/sunsetting-documentation). And yes, part of it was addressing commonly asked questions on SO in those docs. 
the language D is really great and I personal like it but I just find it lacks very much of libraries. If there are libs then they're really old. I'd rather use rust than D now since it seems the community is more active... but then again I don't know. I feel my time would be wasted.
Nope, it's not *sql.DB. It's a `net.Conn`.
&gt; In any case, I've often found myself wanting utilities that help write protocols. I've written a lot of little helpers for myself. But they aren't "frameworks" in the sense that they expect to take control over and call your code in an event-driven manner. There's too many possibilities for what you may want to do "next" for that to work sensibly, and in the absence of generics you can't abstract this out well enough to make it make sense, any more than "just writing code". Writing in this style puts you back in a box... what I want to receive a line which will have the number of bytes the next binary message is, and then I want to receive that many binary bytes? You can return some special value or switch out handlers or whatever to get around the fact that you made this event-based and now you want to go back to thread-like behavior (going down to the fourth level of doing and undoing abstractions), but it's all going to be more complicated than the call to `io.ReadAll` I'm going to make in my code. Go is already in some sense a DSL for writing network servers. The Go "framework" already takes care of events for you; layering an event-based mechanism on top of the Go runtime that is already turning event-based code back into threaded code for you is just undoing all the hard work the Go runtime did for you. At that point you're paying the negatives associated with being in Go (as no language is perfect) and not getting any of the positives.
All languages are rough around the edge when they get started. That's why IMHO new languages should start small and concentrate on quality over quantity. Start with a solid core, while thinking about the future. The core includes the std lib that will basically define how the language will be used and the tooling (testing,code quality tool,...). The problem is that D 5/10 years ago just didn't work properly. The language was extremely buggy, the licensing archaic, and the tools inexistant. You can have the most elegant language in the world, if the ecosystem sucks if the compiler has bugs nobody's going to use it in production. While Go has its share of crap, Go designers focused on accessibility and ease of use and practicality. And actually delivered something that "just worked", Apple style. It makes me think about Ocaml a bit. A very good language that should have had a broader adoption yet a pain in the ass to get started with or to use, and let's not even talk about its ecosystem. Imho language designers should spend as much time as working on feature than working on a strong std lib, even if it means shipping half the features they intended to ship. 
Thanks, I've added those suggestions to my list. Exercises are an interesting idea. It would be possible to build a custom playground where it shows unfinished snippet of code, asks you to complete it and automatically checks if the result is correct. Would require bunch of programming work to build the playground but certainly doable. As to mobile format / donations - that's another interesting idea. My immediate focus is to get it to v1.0 i.e. finish editing the second half of the book, do another editing pass for typos and such. 
&gt; Memory consumption of ~25Mb on x86-64 3.1MB isn't too bad. However, if that's a typo and it actually needs 25MB, that's a whole different kettle of red herrings.
[removed]
Awesome! If the playground is a lot of initial work - even linking to a repo asking users to `go get` this solution and work on it to get it to do "x" would be neat. Yeah I enjoy reading on mobile devices and would love to drop a donation to go towards the development/work on the book. I'll keep an eye out for 1.0 :)
It doesn't solve anything. I didn't want repetitive code, and i wrote it just one time to satisfy our needs. And i share it :)
[removed]
You're wrong on every point of fact about dep. - dep initiative was announced on golang-nuts and go-pkg-mgmt lists - Committee was announced on same lists - Meetings were recorded with public minutes - All decisions were exhaustively documented - Continuous feedback occurred on Slack #vendor and mailing lists - Anyone in the community who wanted to take part was able to - Once we had a prototype it was open to contributors immediately - Sam has done an incredible job building a community
When did websites become books?
Thanks for the info, good to know.
I've been using [upper](https://upper.io) recently. This seems quite similar, in case anyone is looking for alternatives.
Unfortunately this makes me think of the Monty Python "Nudge Nudge" sketch.
[removed]
Good catch, that certainly is a major difference. I've added it to the list of differences. Thanks!
Would be nice to not forget the error returned on f.Close(): https://www.programming-books.io/essential/go/a-2795-defer I think it's better to know if there is something wrong appending during the defer.
"pointers are real, someone has to deal with them!" 
I narrowed the problem down by using just ip addresses, and adding more than 10 or so at once would cause fastping to stop pinging/responding. I settled on adding a delay, but i've also had to design around other fastping "features" so a package like this is exactly what i wanted to make. I already have sophisticated schedulers and everything else, I just need a package that can send a ping when i tell it to.
not quite similar, since upper is a real ORM and loukoum is only here to offer a DSL on top of your SQL. Thank you for the reference anyway, I wasn't aware of upper which seems to be a good project.
&gt; There's vgo code, written in isolation before anyone knew there was even a discussion to be had. Yes- the initial protoype implementation was provided with this **Feature proposal[1]** just like every serious, high quality feature proposal before it. Also included is 28k words across 7 design documents with in depth design rational. &gt; rsc: &gt; [1] This post sketches a proposal for doing exactly that, along with a prototype demonstration that you can try today and that hopefully will be the basis for eventual go command integration. &gt; https://research.swtch.com/vgo-intro &gt; And it seems to me that vgo is being pushed into Go whether the community wants it or not, which is even worse than trying to achieve domination through influencing. You would have features added to Go by processes akin to political campaigns than rigorous technical review? I don't share this belief and part of my willingness to commit to projects in Go is that the [proposal process](https://github.com/golang/proposal#process) is will defined. &gt; You can criticize the dep team for many things, but please let's not pretend that vgo is more of a community effort. Again the [proposal process](https://github.com/golang/proposal#process) here is _NO_ different then all feature proposals before it, dep was unique in that it was made without the Go team and actively campaigned against existing implementations to gain market share. While making promises about official toolchain integration that were very clearly not the intent of the Go authors. By Go authors here I mean the ones responsible for maintaining the quality standards and vision of the Go project, a vision that has set the foundation for the success of Go. In closing people even if this regurgitated &amp; provable false statement that vgo is being "developed in private / forced" was true, only a single person has the right to do so and that would be RSC, not the authors of Dep. **"If clear agreement cannot be reached, the arbiter (rsc@) reviews the discussion and makes the decision to accept or decline."** Luckily this is not the case and the Go team is being reasonable and actively engaging with the community as we speak. **The Go team is owning the outcome of a Go feature, following the documented [proposal process](https://github.com/golang/proposal#process), period.** I won't participate in any more dep dissenters banter, sorry for your loss but it's for the better, deal with it.
Then this should do fine. We've been using the code in a simimar form in production for ~1¬Ω years now and ping 500 hosts an a regular basis (albeit a very slow rate, only every 2-3 min).
[removed]
Honestly, I think a lot of people who are new to go, especially those coming from languages like PHP, are the ones who are: 1. More likely to be using a framework in their existing language, and thus more likely to want one in Go. 2. More likely to actually be asking a question, full stop, rather than making up their own mind by seeking out information. The _vast_ majority of Go applications I've seen or heard of are _not_ web applications, they're web services. I don't doubt that people do go and find a framework and make their web app in Go. It's not something I feel that I'm likely to do, but I can get why people would try. Hell, I may be totally wrong, this is only based on what I've seen, and the other Gophers I've spoken to in various places.
First you forgat "\n" at conn.Write. Your code is just proof of concept. Errors is skipped as you said. How you close your code gracefully? How you handle connection errors? Do you always do copy-paste your repetitive codes? I hope you learned anything while writing this code. I think you didn't understand anything from this library.
 type UserStore struct { db .... } func (u *UserStore) Get(id int) (User, error) { conn, err := u.db.NewConnection() defer conn.Close() user, err := conn.FindByID(id) return user, err } in your example, i'm assuming you have some thing called `db` that can create connections, so it can be injected via the struct
Simple utility for monitoring goroutines during development. There's a lot of room for improvement; feedback, suggestions, and PRs are most welcome!
&gt; Event-based APIs are hacks for runtimes that aren't capable of maintaining many run contexts at the same time. you're undermining your own message by shotgunning insults at everything in sight. Event-based APIs are perfectly reasonable abstractions for transaction systems. If an event captures the entirety of its state and events are order-independent, an event-based API is perfectly reasonable. HTTP is intentionally stateless. You can handle an HTTP request entirely based on what's contained in that request. The whole concept of an event server makes sense for HTTP because an HTTP request is a singular event. But a TCP socket is not a stateless transaction: it is an ordered, stateful stream of bytes. An event-driven API for a TCP socket is always going to be awkward because you wind up having to write synchronization logic to handle the original stream in the order it was presented in. Great, this textproto thing gives you a callback that's executed on each line of text, but how do you know in a callback that all _prior_ lines of text have already been processed? You have to write your own synchonization logic just to recover the ordering guarantees that you already had at the transport layer that this library has thrown out on your behalf. There's a lot of code that is redundant. Like the implementation of [TrimCrlf](https://github.com/go-tcpserver/tcpserver/blob/ebe790768dbfd6f50a091db9961b0b5ca8015d33/utils.go#L26) could all be replaced with one invocation of [bytes.TrimSuffix](https://golang.org/pkg/bytes/#TrimSuffix), like `bytes.TrimSuffix(b, []byte('\r', '\n'))`. [ReadBytesLimit](https://github.com/go-tcpserver/tcpserver/blob/ebe790768dbfd6f50a091db9961b0b5ca8015d33/utils.go#L6) has a bunch of redundant append operations but most of its logic can be avoided by using an [io.LimitedReader](https://golang.org/pkg/io/#LimitedReader) instead. 
I disagree. For me as Go fan it presents interesting point of view.
This looks great. Are you looking for collaborators at all? At any point do you think you'll provide other formats including perhaps selling books (physical or digital)? I find projects around building books on programming really interesting.
I will also say the following: 1. There are definitely people using it. 2. There is a very large group of people (your truly included) that consider sum types and corresponding exhaustiveness checks to be very obviously useful. 3. I did not come to the conclusion in (2) until I used sum types in anger.
Collaborators: very much so. See https://github.com/essentialbooks/books/blob/master/how-to-contribute.md The book will always be free on the web and open source. Selling a printed copy is a possibility.
Nice read, thanks for sharing, and agreed on repository structure and configuration. I wanted to ask about the `Resolver` concept, because I swear by dependency injection in my projects and I currently use the "all dependency wiring in main" approach as the simplest and least-magical way to do it, which scales well over time. Consider an example where you need to resolve multiple copies of a single type of dependency, for sake of argument, you have two clients that need to be configured with a `Host`: clientA := NewClient("host-a") clientB := NewClient("host-b") How do you see that situation playing out with the `Resolver` concept? If the solution is to pass in the `Host` as a parameter to the resolver, then isn't the resolver just hiding the implementation of the constructor itself? Would love to hear your thoughts! Thanks again for the article.
Wut.
I love Go's error handling.
Grmon is a tool by Cobham Gaisler for Leon 2+ processors over their Debug Support Unit (DSU)... Not that these two would ever overlap. Just thought it was funny.
What about vim üò¢
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/full360] [honeytrap ‚Äî advanced honeypot framework : golang](https://www.reddit.com/r/full360/comments/80t7u0/honeytrap_advanced_honeypot_framework_golang/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Thanks for your help, but I figured it out.
&gt; I note this community didn't take the bait and post equally negative comments which is promising. I wrote this post. I endorse this remark wholeheartedly. I've been consistently impressed with the quality of the feedback I got on it, both privately and publicly.
- C - NewSqueak: https://swtch.com/~rsc/thread/newsqueak.pdf - Oberon-2: https://en.wikipedia.org/wiki/Oberon-2#Example_Oberon-2_code http://talks.golang.org/2014/hellogophers.slide
Thanks!
Go maybe has 2 "frameworks" (revel, beego), only one being actively maintained. So it's not like you have a lot of "frameworks" to try in order to make a decision for yourself. And even these absolutely cannot compare to Spring or MVC.net . Go doesn't really have any framework.
Damn this looks comprehensive, nice.
I recommend this one https://www.youtube.com/watch?v=0ReKdcpNyQg if your interested of the origins of go
That is... Quite.. Bright. 
There are few approaches to this you could take, depending on what you knew about the clients. If I had a finite amount of them I'd consider just making a separate resolver method for each client, even if it returns the same type. If I had an unknown number of them then I'd consider making a factory type or something similar, or if it was more applicable, maybe making all of them and returning a map to pick them out of. In that first case, with the finite number of clients, if your constructor was more complex I'd consider making an unexposed method that just accepts the things that changed (e.g. configuration) and then making the exposed resolver methods call that unexposed one to build them. func (r *Resolver) ResolveClientA() *Client { return r.resolveClient(r.config.ClientB) } func (r *Resolver) ResolveClientB() *Client { return r.resolveClient(r.config.ClientB) } func (r *Resolver) resolveClient(config foo.Config) *Client { return foo.NewClient( r.ResolveSomething(), r.ResolveSomethingElse(), r.ResolveSomeOtherThing(), config, ) } Another approach would be to just expose the `resolveClient` method above, and pass the configuration in from somewhere in your app when you need the client. I've tried to avoid that situation so far and keep all of the configuration in the resolver when it starts - but I can definitely see how you might want to configure something when you want to resolve it.
I'm kinda new to blogging, so if you're planning on downvoting I'd really love to get some feedback. Whether it's just that it's a boring article, or whatever, I don't mind - I'd just rather know so I can improve in the future.
What do you mean ? Go is very C-like, with its braces everywhere, its semicolon-separated "for" loop, and the use of weird operators (equality sign for affectation, "++" for increment, etc.). Of all the popular languages, python is the most foreign. Never have a look at ada, haskell, lisp or, god forbid, APL. You might have a heart attack.
But what if I need to interactively debug my goroutines on a satellite?!
We should make a tool called Grmon^2 that‚Äôs just better!
If you think Go and C have weird operators, don't look at Perl.
Women are underrepresented in the field, so I'm supportive of initiatives to have more women. I also have no hope that groups formerly were underrepresented will remember others in the future... Hispanic and black people are way more underrepresented but you hardly ever hear about us. Let alone people who fit multiple categories. I'm pretty sure I've only ever worked with one black woman. It's all about network effects.
Neat. I was planning on making something to read the dataset itself (and avoid sending off data to their service). Just some code feedback after scanning through your code: - Your `Do` function doesn't actually propogate the `Close` error anywhere, as the returned error isn't named, and even if it were it'd be shadowed by the internal `Do` call's return value. Example: https://play.golang.org/p/c_VmV6yPiyG - You'd probably benefit from a few more linters (which would yell at you for naming functions like `_hashString`). Shadow checking may have warned you about some of your error checks. - That usage you have in your README would be a good idea to stick into its own `cmd/something` directory to be go-gettable. - As much as caching the data is good, the cache you've implemented will grow infinitely if used in a long-running process, and there's no way to opt-out or prune it. IMO, you'd be better off making the library, and leaving the caching method to the user. You could limit your cache size, too (and there are libraries which serve this purpose). - Your types/structs and how they interact is odd. You have an underlying `service` type, which both `PwnedService` and `CacheService` are the same as, which only contain a `Client`, then the client has pointers to both `PwnedService` and `CacheService`, except that the client doesn't ever use those "services". - Your structs export things that shouldn't be exported, and don't export things that should. For example, you export a `url.URL` pointer from `Client`, which a user can set to `nil`, which will break things. On the other hand, `Client` doesn't export its underlying `http.Client`, forcing the use of the default HTTP client, which isn't a good idea for usability purposes ([among other issues with using the default client](https://medium.com/@nate510/don-t-use-go-s-default-http-client-4804cb19f779)). All-in-all, cool to see a Go lib for this, as I hadn't seen anything yet.
Setting up a websocket backend in Go is trivial. You'll be up and running in no time. I'd definitely recommend giving it a try. Go is amazingly productive for networked servers, which is exactly what you're making. 
that's what I'm reading. What is the library, websocketd? I can write my server in bash if want, lol. It does seem trivial with everything I would need almost built in to to Go. Just double checking... thank you!
Can you pretty please do a chapter on apis without a framework. Like doing a 'select' is easy and so is a templated post as a crud request but doing a restful API with crud to a URL with jQuery is really time consuming. I really don't know how many books or articles I read where they say they are explaining restful crud app and just do select... It's emense. https://docs.google.com/document/d/1Zb9GCWPKeEJ4Dyn2TkT-O3wJ8AFc-IMxZzTugNCjr-8/edit?usp=drivesdk&amp;ouid=116246055924160986923 Please do chapters on that! 
Gorilla Toolkit WebSocket at https://github.com/gorilla/websocket Highly recommended.
The most actively used and maintained is https://github.com/gorilla/websocket One of the nicest things about go is that it has a Reader and Writer interface that are standard across the entire ecosystem, and most communication mediums implement it. It's abstracted down to just reading and writing bytes, but it can be used for reading or writing to a websocket or to a file or to a local TCP socket. Network servers create a "connection" interface that wraps the common paradigms for a network connection, and so you can run, for example, RPC over websocket or TCP or whatever you want the transport to be, just by plugging in a different connection object. godoc.org is the canonical website for documentation in the Go ecosystem. Everything that exists in Go exists on godoc.org. You can just add the import path to the end of the url, and it brings you to the documentation. for example https://godoc.org/github.com/gorilla/websocket Godoc.org is also the best place to go find libraries. So if you want a websocket library, go to godoc.org and search for websocket. It'll bring up the most popular ones at the top.
Additionally, I really like [gobwas/ws](https://github.com/gobwas/ws).
Please read ["The Origins of Go" section](https://books.google.ru/books?id=SJHvCgAAQBAJ&amp;lpg=PA1&amp;pg=PT12#v=onepage&amp;q&amp;f=false) of [the GOPL](http://gopl.io).
There's also a section on syntax in [the "Go for C++ programmers" document](https://github.com/golang/go/wiki/GoForCPPProgrammers#syntax) on the wiki.
Excellent! A good online compliment to the classic physical Go textbook by Donovan and Kernighan.
&gt; TL;DR: FU Ruby, Go is amazing, fast and scales exceptionally well. Priceless. lmao
Thank you for sharing. I had some initial trouble working with iris and firestore data last night and this was a great example. 
Check out Rob Pike et al's earlier work on Sawzall: https://research.google.com/archive/sawzall.html Seemed to have a lot of influence on Go.
I have never been a fan of DSL abstractions over SQL (which is already an abstraction). builder := lk.Update("news"). Set( lk.Pair("published_at", lk.Raw("NOW()")), lk.Pair("status", "published"), ). Where(lk.Condition("id").Equal(news.ID)). And(lk.Condition("deleted_at").IsNull(true)). Returning("published_at") query, args := builder.Prepare() vs UPDATE news SET published_at = NOW(), status = :arg_1 WHERE id = :arg_2 AND deleted_at IS NULL RETURNING published_at It's not like SCSS which is actually providing new features over CSS.
&gt; JSON objects only support strings as keys: a map must be of the form `map[string]T`. A couple of versions back, the json library developed the ability to have non-string keys as long as they implement encoding.TextUnmarshaler, and encoding.TextMarshaler for the trip back out.
Weird, I'm not getting this (but i know where it comes from). Would you be so kind to lmk on what link this happened for you?
Happening to me too: https://scene-si.org/2018/02/28/interfaces-in-go/
Should just be fixed, apologies.
&gt; pkg/shop/interfaces/private/http/products.go Clean architecture ladies and gentlemen! But seriously this looks like Java with Go syntax to me.
Any reason I would use this over the pprof web UI?
I would go with something like: func normalize(s []byte) []byte { return append(s, make([]byte, 8-len(s))...) } 
Using [reflect.New](https://play.golang.org/p/vQ3O38QnE9j)
Jesus, too much color, I'll be staring at words forgetting what I was doing! My favourite has slowly become solarized dark in VSCode - aquatic and lets you focus without being distracted by colors
&gt;(of a system or machine) achieving maximum productivity with minimum wasted effort or expense. &gt;"more efficient processing of information" By using a slice, you use less memory overall (one pointer total, rather than one pointer per entry), you perform fewer pointer indirections (one per operation), and the data is contiguous in memory, so you use the CPU cache more effectively.
I've been using [gabs](https://github.com/Jeffail/gabs). I find it to be much easier to understand and use with my limited mental capacity.
In the computing world today, main memory often dwarfs the cache, data sets are often very large, and often the efficiencies you speak of don't relate to real world work loads. So in conclusion, often should actually be seldom.
I'm far, far from being sold on vgo's supposed advantages over dep, but I'm looking forward to this series. This video doesn't count as it has no dependency management in it :) 
&gt; I tried changing the code to `reflect.Zero(reflect.TypeOf(a)).Interface().(Foo)` but then I just have a `(*main.Foo)(nil)` No, definitely not. The result of the first expression is *definitely* a `main.Foo`, not a `*main.Foo`. You made some sort of copy-paste error or the like here.
&gt; I want a common panic catcher that can e-mail the details while letting the server continue to run, without any of the goroutines dying off This is a *bad idea*. If you have a panic, your code is buggy and should be considered to be in an inconsistent state. Doing complex things, like sending out an E-Mail, is a bad idea in that situation. Let the process die and restart it; capture the panic-details from your service manager, reading its stderr. There's also another significant issue with this, which is that this means a bug in your server will, with even moderately high traffic, get blacklisted by your E-Mail provider. Because you will try send out thousands of E-Mails per second. Use something like [prometheus](https://prometheus.io/) to monitor the number of failures and notify when they pass a threshold - and make sure to deduplicate those alerts. &gt; or should I look outside of Go, perhaps at the supervisor level Yes. This.
Murphy's law: for each "performance optimization" patch you write, I'll write a caller with an O(n^2) algorithm where only O(n) is needed.
Came here to post the same thing: a panic should not be considered part of any workflow, in that you're planning around panics. Panics shouldn't happen, and all that work should be focused on preventing them. At the same time, you do need something in place to ensure your services don't fall over from the failure of a single process. For this you have a few tools... [runit](http://smarden.org/runit/) can be used to supervise processes. Docker offers a set of restart policies if your service is containerized. Autoscaling is another option, if your service is rubber-stamp replicable (ex. Chef recipe). As /u/TheMerovius points out, you need to use these tools with caution, as you can end up spamming yourself to death in some freak error conditions. Make sure that you have some form of rate limiting in place first, and test it to ensure rate limiting persists across application crashes.
Rewrite in Rust!
now that's responsible software versioning :)
Maybe you should watch video put here last week where Russ, Sam, Jess discuss about dep, vgo and dependency management in general. Russ's core idea of Minimal version was not really liked by dep community and the problem teams at google faced did not have solution via dep. So they have to try out their idea independent of dep and Sam was made aware of that.
Luckily the majority of what I'm discussing (so far) is not Go-specific, and even then much of it will apply to other languages. In terms of compiler optimizations, you'll only get a factor of 2 or 3 due to LLVM vs. what Go currently does. Much larger improvements as possible in both languages with smarter programming.
I've found it easier to evaluate at a glance than the web UI when tracking goroutines in realtime; e.g. to follow the trace of one specific goroutine with grmon you can [p]ause, expand the trace with &lt;enter&gt;, unpause and the trace will be updated at each refresh. Sorting by goroutine number or simple state(sleep, IO wait, etc.) are helpful at times, and if nothing else grmon parses and exports the bultin pprof goroutine profile as JSON to use however you'd like.
tehe that's what I said... cept we don't really need the vendor dir, I'm happy to risk the latest versions - if I wanted vendered versions I could always download dep, cd into the dir, then use dep to get those versions. Committing vendered code just makes repos bigger, and I'm in australia where the internet is government mandated rubbish.
I could have used this 3 weeks ago, I'm penalising you for not anticipating my needs sooner. However, downloaded and installed it yesterday and solved one problem almost immediately, so thanks.
i too am looking for a good golang and Algorithms and Data structure book or blog. Ideally, someone from the core team write up something so we can study the code. So far, I found this guys blog great http://tobin.cc/blog/ Very clean and easy to understand. 
Thanks for the video Brian, it was well done and helpful to me (I‚Äôm too lazy to try vgo myself because I don‚Äôt need it, but I want to know more about it).
FWIW, I mostly agree with /u/themerovius. We have global recovers that send uncaught panics to Sentry. That is built for high volume and we do alerting from there. Because the system is in an unreliable state, we then panic again with the original error and let the process die and restart.
Why don't you need it? Don't you have versioning problems with packages stored as gists? 
Really appreciate the json packages people are suggesting!
First of all, not everyone has access to a computer, boys are much more likely to get access to a computer, too. Solid internet access is also vital. And a ton of undisturbed time.
I think vgo and dep are both gross :D
could you please link the video?
hello, have you perhaps thought about using [cu](https://github.com/gorgonia/cu) 
Pretty hard since go does not allow instrumentation like java, you have to place your tracers manually. https://github.com/DataDog/dd-trace-go
I think it's this one? https://www.youtube.com/watch?v=sbrZfPgNmfw
Interesting personal corollary - I write go every day at work, and I‚Äôve worked on many go applications, mostly deployed in docker images. I don‚Äôt really know what ‚Äòinstall‚Äô does, and I haven‚Äôt bothered to look. We simply use ‚Äòmakefile‚Äôs and ‚Äògo build‚Äô.
Hi there, Actually I did take a look at your project and I will probably utilize it when I go back and redo this. As my code stands now it is a rewrite of my fully functional CUDA C version to take advantage of go's concurrency. Currently I am just trying to get it to work so I can compare the GPU utilization against my C implementation. Talking about `cGo.cuh`: In my project directory I have 3 folders: main, cuda &amp; library. Within library I placed cGo.cu &amp; cGo.cuh. This is also where I specify the output of the shared library from NVCC `(myLib.exp, myLib.lib, &amp; myLib.so)`. Now the reason why I have `#include &lt;cGo.cuh&gt;` is because I assumed I need to specify the header file like I do with the cuda runtime library `cudart`. I have passed the path location of the library folder to both the `LDFLAGS` &amp; `CFLAGS` statement so the compiler should be able to see it. Also I just tried with `#include "cGo.cuh"`and it gave me the same error. No doubt there is something I am not understanding, but I'm not sure what. Also I just updated my post with new information.
When you hit encoding/json library's performance ceiling, try one of the json marshaling/unmarshaling by code generation. You can get a easy 5x speedup on unmarshaling that way. Personally I reach for [easyjson](https://github.com/mailru/easyjson) once I'm unmarshaling the same struct a few 100 times/sec. But it has been a year since I benchmarked the competition. (And they all say they're the fastest :-)
But I doubt there is a 2-3x difference when comparing Go to Rust (more like 1.5 and less). Except maybe for large memory allocations.
Email me (my Reddit user name) at gmail. I will walk thru your code with you 
&gt; Consider this Yes, you are deferencing a nil-pointer here. You can add an `Elem` (to get the element type of the type of `a`) and use `New` to allocate a new thing: https://play.golang.org/p/6TcoFCCK_tx In the end, using reflect just means thinking about what the Go-operations are that you'd like to do an do them :)
https://www.programming-books.io/essential/go/a-5818-csv might be helpful
It was discussed one year ago between Sam and Russ https://groups.google.com/forum/#!msg/golang-nuts/PaGu2s9knao/Bq4vmFh7AgAJ &gt; Hi Sam, &gt; &gt; You make it sound like dep is just going to become 'go dep' in Go 1.10. That's not the plan I thought we discussed. &gt;...
Thanks, your speaking is very clear for non english speaker.
[removed]
Im gonna start using this again if it's in Go now.
Meh. I really wanted to like Rust, but it lacks proper tooling for a lot of things. E.g., the Rust SDK for AWS is really slow (compared to go), apparently due to slow XML libraries. Rust is great when you don't have to talk to other programs...
..maybe? I think I better spend a few more days digging through the resources. Pretty sure I don't understand the language well enough yet to do what I'm trying to do. 
`go install` compiles and puts your binary in `$GOROOT/bin`
Once the mod file contains the import path, can we delete the comment on the source? Seems like a hack...
Great! Just curious, how does this compare to gometalinter? https://github.com/alecthomas/gometalinter
We have had Jenkins for a while. But we are actually trying to switch over to Gitlab CI
I love how "blazing fast" today doesn't seem to mean 0-1ms, but what, 28-29ms for the fastest request? It's like my job got 30 times easier now. But in all seriousness, Lambda/CGI is useless if your performance targets are in the 1ms range. Even a dedicated PHP-FPM server will beat those measured response times possibly several-fold. Without predictive scheduling, you'll never get any kind of real performance with this stack. Think of it like this: Curently each lambda requests triggers an `os.Exec` as the request comes in. A predictive scheduler may trigger the `os.Exec` before the request comes in, and then pass the payload via stdin. The second method uses more system resources, complicates billing and stuff, but would be several orders of magnitude faster as long as you can reliably call os.Exec faster than your requests are coming in. If you could reliably freeze the process while it's waiting for the payload, it would be a possible approach to solving at least the billing of long-running processes. Docker supports something like that [via docker pause](https://docs.docker.com/engine/reference/commandline/pause/), which could be adapted into this kind of predictive scheduler. Here are the [kernel docs for the freezer subsystem](https://www.kernel.org/doc/Documentation/cgroup-v1/freezer-subsystem.txt). Obviously, there are ways to optimize performance here from the AWS side, but the obvious motivation here is to move from lambda into docker or EC2 instances as your request volume grows. If lambda was 5x faster or even more, there would be less incentive to migrate to stacks that support larger volumes / faster responses. 
[removed]
[removed]
Is this even when compiling C code with gcc and Go code with gccgo and then combining the resulting .o files?
Do you know of any helper app to detect panics in Go stderr and forward each of them to some external sink?
&gt; For now I'd like to set a variable to "First Title" using Go Why not: https://play.golang.org/p/u_Mr6DPI4JH
The video is very straight forward but I was hoping to see it actually fetch some dependencies, and how it locks them, you know? 
I would suggest using [opentracing](https://github.com/opentracing/opentracing-go), and then injecting the actual APM tracer. This way you can move across different APM providers without (much) code change.
As an aside, are there any linters you recommend that can be used in PRs and such? A lot of linters come with a bunch of false positives, and say that they shouldn't be used in automated PRs and such. So are there any linters with a limited scope, to reduce the amount of automated PR issues?
"It depends." Go's emphasis on fast compilation does not come without cost, and there are times and places where the more thorough optimization that Rust does can be a significant win. I think if you've been programming for a while it can be a bit difficult to wrap your head around Go's performance. For a long time there were two types of languages: Fast like C/C++ and its system-level friends, and slow like Python and Perl and PHP. Go ends up slotting sort of inbetween those things... it is not, strictly speaking, "as fast as C" in any sense. But if you're coming from Python, it's going to feel blazingly fast. It's a relatively new slot on the cost/performance/effort curve, which I think is secretly a great deal of Go's appeal... it isn't _quite_ as easy as Python, no, but it's at best only slightly harder in my opinion for a lot of tasks, and for that "slightly harder" you get performance that isn't as fast as C, but is at least within spitting distance, whereas Python is on the other side of a mountain from C's performance. Doing better tends to require a huge leap in effort required. When your only choices were Python or C (more or less), more things were worth the trek over to C, but if you've got something only 2-3x slower than C, the window for such things is much smaller. It's not zero, and there's been some good articles about companies pushing Go to its limits at scale, but it's still a much _smaller_ window; Go will take you a lot farther before gassing out than the scripting languages can.
Savage. 
I am annoyed that the dep team used [activism and misinformation](https://www.reddit.com/r/golang/comments/80fm2v/go_2017_survey_results/duw9yhn/) to gain market share so people like you wouldn‚Äôt be uninformed. I mean come on, Sam knew over a year ago this wasn‚Äôt going to get into the tool chain but left the README link to the [Roadmap](https://github.com/golang/dep/wiki/Roadmap) saying they expected official toolchain integration for 1.10. You think it was an oversight? No it was calculated and all the little things like that have caused this mess. The Go teams behavior is exactly what people like me want. I want the hundreds of years of collective top tier engineering talent they have to carefully vet and ensure the vision of the project. Stop publicly shaming them for doing the one, [documented](https://github.com/golang/proposal#process) damn thing that has ensured Go‚Äôs success. Dep did not follow that process. They tried to circumvent it through acitivism instead of technical merit and the outcome was righteous. Deal with it.
Even the code itself reads like a Java application, especially with respect to the naming. I cannot argue much with the theoretical concepts applied though. The pattern does work well in Go.
https://medium.com/@eminetto/clean-architecture-using-golang-b63587aa5e3f
Thank you for the link. However it does not answer my questions in the repository. Example in the post suggest I'd have to have one huge Repository interface with *all* methods for all the tables. When having 20 tables, this interface would have *at least* 40 methods (Get/Save for each), in reality more like 100 methods. That is terrible. Or have I misunderstood?
I'm no expert but I'd imagine the idea is to have the methods in the interface be generic enough that the method signature swould be implemented by each repo struct ( one for each db). So one get(id), getAll() []*User, etc 
 To be honest idk the best design for your code or what you want the code to do in the future. That's a personal decision but every choice is gonna be a trader off. Just depends on how you want to leverage things now or for the future. It really depends on your future goals. https://blog.gopheracademy.com/advent-2016/go-and-package-focused-design/ https://www.ardanlabs.com/blog/2017/02/package-oriented-design.html https://www.ardanlabs.com/blog/2017/02/design-philosophy-on-packaging.html 
&gt; Another valid concern is that since errors are variables and exported they can be changed in other packages, that is true however it sounds to me more as a culture problem than a problem of the approach itself. Here is a suggestion. Define some error behavior: type notFounder interface { NotFound() bool } And have your error with the context implement that behavior on the producer: type userNotFoundError struct { // Go nuts with all the context you want user *user.User cause error } func (e *userNotFoundError) Error() string { ... } func (e *userNotFoundError) NotFound() bool { ... } As you very well mentioned in the article, checking for certain error types such `ErrUserNotFound ` causes coupling between packages. Since in Go interfaces are satisfied implicitly you can keep your packages decoupled by [checking the error's behavior](https://dave.cheney.net/2014/12/24/inspecting-errors) instead. func isNotFound(err error) bool { type notFounder interface { NotFound() bool } nfe, ok := err.(notFounder) return ok &amp;&amp; nfe.NotFound() } Then your handler code becomes: user, err := repository.getUser(userID) switch { case isNotFound(err): // Returns 404 case err == nil: default: // Returns 500 } Which is much nicer I think and you have context and decoupling on top.
`go install` and `go build` are very similar. I wasn‚Äôt sure which to use in my example, I picked a random one. If you‚Äôre familiar with `go build`, then `go install` is just like that, except the puts the compiler binary in GOPATH/bin rather than cwd. In previous versions of Go, it saved the results of building libraries instead of throwing that away, so it was faster.
Thank you for doing this. I will follow the linked repository as I am starting to get to that point as well.
this is a corporation. dude
Ever heard of mvc?
What does that have to do with it? Just format all your source files.
&gt; and I'm having trouble with "clean" architecture of this application. Yes you are having trouble because you are most likely copying the implementation of the pattern from other languages. My suggestion is to think how the pattern should be applied in the context of Go. Have a look at [this](https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1). 
The post basically just partially rehashes the (more thorough) Dave Cheney post linked at the end.
To be clear, a repository should be a higher-level abstraction than tables. It models what you want to store and retrieve, not *how* you store and retrieve it. 20 tables does not necessarily mean 40 methods. Join tables, for example, have no business in a repository API. Necessary under the hood for implementation in many circumstances, but the caller needn't and shouldn't care about them. I will add that you should not be thinking in terms of tables at all. Design your repository so that you could theoretically swap out the implementation for flat files, or a third-party REST API, without the caller seeing any difference. In fact, it is incredibly useful to provide a repository implementation that does not rely on any external services for testing. So, with that said, I would recommend multiple repositories that each deal with different storage concepts, not a monolith repository that concerns itself with everything you could possibly want to store.
[removed]
Thank you so much. So in the end, I just didn't have the order right: b := reflect.New(reflect.TypeOf(a)).Elem().Interface() // ptr, invalid b := reflect.New(reflect.TypeOf(a).Elem()).Interface() // ptr, struct 
you know these days. internet companies usually have a single repo for a language with all projects in it
Seems I have one last hurdle to jump. I need to overcome the compiler thinking the `Foo` is an `interface{}`. https://play.golang.org/p/o08LXSMCV4U
errcheck, megagcheck, ineffassign and go vet are almost always correct. With gometalinter you can filter out errors that you don't care about from them. Appart from this, I really like reviewdog for the other linters with false positives such as golint: https://github.com/haya14busa/reviewdog
No one. Who is failing to gofmt the files they change when they change them? Every editor out there will let you format on save. There is no excuse for winding up in a situation where you have thousands of files modified just for gofmt.
What you want is not possible (and internally contradictory). As I mentioned, if you don't know the type at compile time, you won't get compile time type-checking. You can still [call useFoo via reflect](https://play.golang.org/p/ue_tl4TzM08), but static types require static types, that's literally their point.
There's no reason FaaS needs to be slow (high latency). You can build a FaaS that takes apps as `http.Handler`s and builds longrunning containers that wrap them in `http.ListenAndServe()`. The FaaS engine will increase or decrease the replicas based on some heuristic, and this can be predictive so no inbound request needs to wait for new containers to become available. FaaS can trivially be both low latency and high throughput.
Guess who baldie. This is the only warning you will get, back off now, you are about to trigger me. If you think I fucked with you too much in the past, then just push me some more. I don't like you and you don't like me. I don't like how you approach people on reddit, and I am at my tolerable limit. So fuck off before shit hits the fan.
out of the box not a lot run go fmt by itself. Visual Studio Code is the perfect example. If you don't have the Go plugin it won't fmt on its own for you. And running personally i tend to forget it aswell a lot, when I'm not using the on save feature. So yeah there are decent excuses for that.
[removed]
The comment isn't required if you setup a go.mod file
Thanks for repeating it for me. Bold text helps but please use ALL CAPS too.
There are several great use cases for Lambda, even user facing ones. But as others have said, microservices which you can similarly scale with docker/k8s will be more effective where latency is king. Handling 20x traffic spikes is an interesting challenge however, and lambda solves it well if you‚Äôre aiming for throughput over latency. It takes 9 months to deliver a baby, and lambda gives you 1000s of pregnant women üòÇ
Okay, I will repeat this again and again just for you. **THIS TYPE OF BEHAVIOR SEEMS LIKE A RECURRING PATTERN WITH CORE GO VS GREATER GO. CORE GO JUST DOESN'T PLAY WELL WITH OTHERS.**
Just a thing I did yesterday that I thought was worth sharing. TL;DR: * Clone the go playground * Build the docker image * Mount your $GOPATH/src to /go/src The playground will now import your local packages, to get a shareable play.golang.org link, update the share handler to make a POST request to play.golang.org/share 
This sort of thing is one of the first things I wrote in go. [my version](https://github.com/justinazoff/http_flood) a bit more complicated though. I don't think websockets were supported everywhere when I first wrote the web interface side of things and I couldn't get JS to run fast, so I ended up using flash via haxe. I've been meaning to rip out the flash stuff and replace it with gopherjs or whatever. Yours looks kinda ok.. I'm not sure why you use atomic all over the place though. Some neat things I implemented for mine are the random readers, so it just does randomreader.LimitedRandomGen(m*consts.Megabyte) or randomreader.TimedRandomGen(seconds) and then just calls io.Copy with the http.ResponseWriter and the random reader. It's neat because the protocol is just http so if you want to test it remotely you can use use curl or wget, you don't even need the client: $ curl -o /dev/null http://box:7070/flood?m=200 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 200M 100 200M 0 0 65.8M 0 0:00:03 0:00:03 --:--:-- 65.8M
totally offtopic and somewhat mean but you kinda look like richard from silicon valley. props for that lmao
A reading list without books?
Well the issue is I'm trying to teach myself Go. The best way I've learned in the past is to build an indexed reference with parsable by *me* explanations and examples. Yes, basically rewriting godoc as I learn. For instance: `func Println(a ...interface{}) (n int, err error)` ..might as well be in Latin atm. I'd like to put together a database with each field explained in "I've never coded before" terms. So, say I want to learn/refresh on what `Println` does. I'll be able to punch "Println" into my little jalopy search and it'll return: &gt;`Println` `func`(link) `Println`(link) `[(a`(link)` ...interface{}`(link)`) (n int`(link)`, err error`(link)`)` //Or whatever I should be linking to.. A quick explanation for dummies on what a `Println` is/does/represents/constraints etc. Basically, my little go-index-of-data(the csv(or two)) is going to fill up fast. I'm just learning to manipulate variable (the bytes/string bit keeps screwing with me) at this point, and I imagine there's a pretty big learning curve between that and reading/writing to a file. I like the lang, but I'm really in over my head at the moment. Lots more basic learning to do ya know :) 
[removed]
Please let me know if there is something I can approve or something I've missed out on. Your feedback is appreciated
Thank you @shovelpost
Thank you for writing the article. Maybe you could also include /u/peterbourgon 's [oklog](https://github.com/oklog/oklog) in the benchmarks.
I too found the articles and demos about Clean Architecture in Go to be either lacking or icorrect. I think /u/JidokaUS isn't wrong. I began with an MVC but then went on to steal a lot of ideas that went into a set of Clean Architecture variations of it, called VIPER. [Here's an article that helped me a lot.](https://brigade.engineering/brigades-experience-using-an-mvc-alternative-36ef1601a41f) However, I've been using this in a Go project for a couple of weeks now (contract work, can't show) and I think it was an incorrect decision for two primary reasons. The first one is maybe innate to VIPER or any MVC: [Object-Relational Mapping is the Vietnam of Computer Science](https://blog.codinghorror.com/object-relational-mapping-is-the-vietnam-of-computer-science/). Maybe having a single huge repo and selecting this or that set of closures via interfaces is not such a bad idea. The second has to do with the Go ecosystem: You'll find yourself rewriting perfectly fine libraries and you really do not want to rewrite `gorilla/sessions`.
if you happen to want them to be zero values, just make a slice, and they will be zero values, the language spec gaurantees that. Otherwise, for i := range mySlice { mySlice[i] = default } 
 It's tucked away in the Readme. 
go-kit/log is the more analogous package.
I'm a big fan of having the appropriate data structure for the job to improve performance AND insertion/querying ergonomy (I see it as a warning sign when my sql queries exceed 500 loc.) The big problem is the shared global mutable that most relational dbs invite you to do. If you can ‚Äî give the Kleppmann/kappa way a try: use a log (yeah that would be Kafka) as a WAL and then construct views into the appropriate data structure, be it a full text index, a col store, a kv store, a ts db or even a SQL db.
&gt; There are few applications that aren't actually storing relational data I'm not sure this is true. While many applications need a relational database, I believe there are many that do not. At the beginning of a project I always try to model the solution with a KV store. More often than not, a KV store is sufficient. It simply requires different thinking and a bit of compromise. And if you can model your solution using a KV store, then you get all the advantages that come with it: speed, scaling across a practically infinite number of machines, no database setup, very simple code that's a fraction of the size, etc. And since KV stores are so simple you can abstract them quite easily, which makes your application truly storage agnostic. 
do a ctrl + f and paste: "./bin/go-wrk -c 1000 -d 10 -T 20000 http://0.0.0.0:8080/zerolog100", below it theres a wrong test result
awesome. as a noob in golang im lurking around this subred quite some time to find something like this!
Agreed. Also don't forget: https://medium.com/@benbjohnson/standard-package-layout-7cdbc8391fc1
i happen to need a different value. for-loop needs too much typing. ideally it's declaration + initialization in one statement like other languages
Of course, and people often take logs for free, in production they have a big impact on performance, especially if you log a lot, stack traces, json payload ect ...
While Almost 100% of applications need a relational database at moderate scale, my guess is that 97% or more of Wordpress installations do not need a relational database for anything they do. If you could run WordPress without installing MySQL and if it ran on something as light as go instead of PHP, nobody would complain. There are tons of applications in the world that could fit their entire database contents into a single big HTTP response without the server timing out the connection. At the core we probably agree with each other on the importance of relational databases, but there is another segment where simplicity of usage and deployment can trump everything.
Doesn't exist and can't make one because no generics. If this sort of situation is not workable for you, Go is not a good language choice, because this comes up a lot. You can make a slow generic fill function using interface{} and reflection, see: https://groups.google.com/forum/#!msg/golang-nuts/8WSkwEs4YkU/s-BYdfLQszIJ
How is it significantly more typing than other languages? Minimal difference
Even in an Enterprise environment, many developers find them self frequently writing small applications that will have only 1 to 6 maximum concurrent users at a time. Even when needs grow beyond that, I have successfully engineered applications with a combination of relational databases and Bolt. I've also made the mistake of using bolt when relational databases where the correct choice, and I would generally agree that if you are working somewhere with plenty of build infrastructure and server resources, you should always just use a relational database, but if you are doing freelance for people who would otherwise go to a WordPress developer bolt might be best. Outside of web, I think we forget that there's a full world of desktop that somebody somewhere still develops code for... Though that's usually an electron app these days.
After writing this, I hope this doesn't come across as defensive, I don't mean it to be. I have just seen and experienced many devs falling for the shininess of NoSQL when they would have generally been better served by an RDBMS. I have operated multiple NoSQL stores in production and at high volume, so I'm not just defaulting to MySQL because that is what I have always used. &gt; While many applications need a relational database, I believe there are many that do not. We can agree to disagree. I think for most of the applications are referring to, people are ok giving up relations for ease of implementation. That's a fine tradeoff if chosen for the right reasons and compromises like you mentioned, but most of the time ends up being a lazy decision that causes problems later. &gt; you get all the advantages that come with it: speed Like I pointed out before, NoSQL databases aren't inherently faster at k/v lookups and aren't inherently faster. I've had MySQL outperform Bigtable, Cassandra and Couchbase for k/v lookups. &gt; scaling across a practically infinite number of machines You can get the same scaling benefits by using something like Spanner, Vitess (MySQL) or Citus (Postgres). Granted the open source options aren't quite as easy to operate as some of the NoSQL solutions, but that's getting better. &gt; no database setup IMO, this is generally the worst selling point for NoSQL, that somehow your data can be "schemaless". All this means is that you have to manage all schema versions inside your application logic as opposed to in a database that is built to enforce schemas, data types and relational integrity. &gt; since KV stores are so simple you can abstract them quite easily This sounds better in theory than in practice. It is certainly nice for unit testing, but even if you choose to use a NoSQL db, abstracting it completely also means that you give up any custom functionality that your db of choice provides. Companies rarely actually switch out the entire db layer without also refactoring code, though making stores swappable sounds sexy. There are also significant cons to moving away from an RDBMS. You have to give up transactions and relational integrity, which are underrated these days. The tooling around MySQL/Postgres is far superior to NoSQL options, where you typically end up writing code to run any type of analysis. TL;DR - NoSQL has its place, but devs should default to using RDBMS and only switch to NoSQL for very specific use cases.
I think we're seeing that problem solved at a different layer with static site generators. That lets you store relational data in a relational way, but compile it essentially into a giant denormalized k/v store, also known as html files. :)
[removed]
Oh yeah I totally agree. Those are what I call "small projects" and I love using BoltDB on those. I had no idea about blevesearch though. I hope I can remember about it next time I use bolt. I am also interested in what the article said about Badger. Is it really so much faster than Bolt? Then again I am more interested in Badger because of the value expiration feature. It always makes me wonder if I could replace Redis with Badger. :)
[removed]
thanks.
&gt; While many applications need a relational database, I believe there are many that do not. Specifically, I'm referring to applications that require relational relationships in a single query. If a record simply contains the ID to another record that will be looked up in a separate operation, it isn't necessary for that relational relationship to be defined at the data layer. &gt; Like I pointed out before, NoSQL databases aren't inherently faster at k/v lookups and aren't inherently faster. This hasn't been my experience. SQL databases always have some sort of overhead. In addition, NoSQL forces you to think about **how** queries are being executed, which generally leads to a faster application. Denormalization is also common with NoSQL, which will always be faster than accessing normalized data. &gt; You can get the same scaling benefits by using something like... I've tried several of these. They're a maintenance nightmare compared to a NoSQL backend. &gt; All this means is that you have to manage all schema versions inside your application logic Which to my mind is a good thing. You have one code base instead of two. Your application defines everything it needs to run and therefore can be deployed anywhere at anytime with no prior setup. &gt; This sounds better in theory than in practice. It is certainly nice for unit testing, but even if you choose to use a NoSQL db, abstracting it completely also means that you give up any custom functionality that your db of choice provides. Dynamo, Azure Table, Cosmos, Bigtable, and Cassandra all provide very similar feature sets. Granted, you could use some of the more specialized features, locking you in, but I'd argue that you probably don't need those more specialized features either. Or at the very least, you can limit yourself to features provided by multiple providers, like secondary indexes or TTL. &gt; Companies rarely actually switch out the entire db layer without also refactoring code, though making stores swappable sounds sexy. Mainly because it's impractical. In my experience, once that refactoring becomes a matter of days or hours you're far more likely to see an application change its storage provider. Especially early in development. Hell, I'll frequently implement multiple backends from day one just so I can benchmark the differences. &gt; Devs should default to using RDBMS and only switch to NoSQL for very specific use cases This is my primary difference of opinion. I believe devs should default to using NoSQL and switch to relational databases when they need it. Like you said, you can easily implement a KV store in a relational database if you need to. In practice, it's far easier to upgrade from a KV store to a relational database than it is to downgrade from a relational database to a KV store. Therefore, I believe devs should start with a very simple KV store solution and migrate to SQL **if** it becomes clear that they need relational database functionality, since their KV store tables can almost always be plopped into a relational database with very little effort. Being stuck with a relational database like MySQL when it comes time to horizontally scale can really really suck, especially if you never really needed a relational database in the first place, it was just the go to default. 
Mac: $ grep --color=always "gopher" /usr/share/dict/words Ubuntu Linux: $ sudo apt-get install --reinstall wamerican $ grep --color=always "gopher" /usr/share/dict/words 
Know about windows too by any chance ;)?
am i shadowbanned?
A lot of the simple sites a previous team of mine worked with were Boltdb wrapped with Storm for simple queries and were not denormalized like a document store but were backing fairly dynamic Angular frontends that simply didn't need to index thousands of lines in any table.
&gt; Specifically, I'm referring to applications that require relational relationships in a single query. If a record simply contains the ID to another record that will be looked up in a separate operation, it isn't necessary for that relational relationship to be defined at the data layer. Now you've pushed the burden of relational integrity onto your app developers. Now they have to manage writing multiple records without the benefit of transactions, along with handling cascading updates, deletes, etc. This is a virtual guarantee that you'll have data integrity problems. This is the number 1 reason that most devs should avoid NoSQL. &gt; In addition, NoSQL forces you to think about how queries are being executed, which generally leads to a faster application. This is no different than an RDBMS. It's not uncommon to see several orders of magnitude difference in a well formed / indexed query. Whether NoSQL or SQL, understanding query execution is vital to performance. &gt; Denormalization is also common with NoSQL, which will always be faster than accessing normalized data. Again, this has to do with architecture over technology. Oftentimes I will materialize a denormalized view for performance reasons. The source data stays securely relational, while still reaping the benefits of denormalization. In any event, I'm not going to change your mind in this forum, and I appreciate the nuances of your points. I think it's useful for devs making a db decision to see both sides of the equation.
What's windows?
My main point is that many applications won't need immediate consistency or complex transactions or fully mutable databases. Developers have a tendency to make things far more complex than they need to be. KV stores are the simplest possible solution to data storage. Start there. And if you need more complex features **then** spin up a SQL server. I've lost track of the number of times I've asked, after giving some guidance, "Did you end up needing a SQL server?" and the answer is "No." I've also lost track of the number of times I've seen a team ripping their hair out trying to get an old SQL database to scale across multiple machines when they probably didn't even really need a SQL database to begin with. Keep it simple.
Yes, you should be thinking about the domain entities, not the representation in a particular store. It should encapsulate the storage so that SQL vs graph vs document vs test is invisible.
tl;dr zap https://github.com/uber-go/zap/blob/master/README.md
As far as reads are concerned, Bolt is an in-memory mmap b+tree that gives you direct access to the stored data without extra marshaling/unmarshaling steps. That's a pretty tough design to out-perform, especially with something that resides on the other side of a network connection. (As for writes, well, that's not what Bolt was designed to be fast at ;)
&gt; Now they have to manage writing multiple records without the benefit of transactions, along with handling cascading updates, deletes, etc. That's true for many of those scale-out SQL solutions you were recommending earlier, too. Also from what I've seen, these days it's more common to flag things as deleted than to delete them from the database immediately, and the distributed nature of typical systems means there's some sort of references living all over the system, outside the scope of your DBMS.
[removed]
&gt; Bolt won't wait at all If you have multiple concurrent goroutines doing updates, use [DB.Batch](https://godoc.org/github.com/boltdb/bolt#DB.Batch) to coalesce the updates together.
I spend far more time thinking about what code to write than I do actually typing. The way to do this with go is a for loop, and this is not a big deal.
In C++ it would be something like: std::vector&lt;char&gt; my_vector(5, 'a'); Beyond that I'm not really sure--I think most other languages do actually solve this with a for loop.
1. There's only one channel. It's capacity is the length of the wordList. This is to ensure the goroutines don't block waiting for the last loop to consume elements. 2. Each goroutine is computing the letter frequencies for a single word, putting it in a map and sending the map to the channel. So if the word was "abca", the map would look like: map[rune]int{a: 2, b: 1, c: 1} 3. The final loop is looping over all elements in the channel (all the maps the goroutines are sending over) and merging them into a single Frequency map. The outer loop iterates over all the maps in the channel, and the inner loop iterates over all the key/val pairs in each map.
I think its some kind of spyware?
F# let myArray = Array.create 10 default 
I inherited a project from someone else, and they had already started using the root bucket for all the data. When I needed to start storing another collection, I found it a pain to have to start accounting for sub buckets in the middle of the production data in the root. 
For sure, if you're read heavy and aren't storing relational data, Bolt is a great option. We use it fairly heavily via etcd inside of Kubernetes.
Of the tools I mentioned, I'm least familiar with Citus, but I believe that it, along with Vitess and Spanner, still can provide full transactional capabilities and relational integrity checks, even while sharded. I contribute to Vitess, and if you design your sharding keys appropriately, most of your transactions stay within a single shard and use vanilla MySQL transactional guarantees. If you need cross-shard transactions, it allows for those too as a two phase commit, though there is a more significant performance penalty there.
Thank you for reading, I'll have a look at it and will definitely correct the mistakes 
Thank you for reading /u/Thaxll