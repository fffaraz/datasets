http://strangepaths.com/reversible-computation/2008/01/20/en/ http://www.complang.tuwien.ac.at/anton/euroforth/ef09/papers/wodni/wrongforth.pdf http://forth.wodni.at/wrongforth
I think it may be a mistake to base it in ans. People have tried that before and not been successful.
That's preposterous logic. Anyway ... my view is that with a few definitions one dialect can be transformed into another. They're not that different. Maybe apart from how the dictionary is constructed, but that's not necessary for every application.
Just as anecdotal evidence, I have brought old code (from different systems I've written) into new projects using the quick and dirty approach of writing an adapter and protecting the rest of the application using a simple wordlist. It's not hard and it works. I'm talking about standardizing that.
[removed]
Perhaps you could talk more about this process and how it worked for you - bringing old code and making it work in adapters
I think that with the use of adapters based on a reasonable standard, code sharing could be increased without everyone having to adopt a singular standard dialect.
So if I understand what you are saying you are taking the core wordset that makes up your previous forth and defining those words in your next forth so that they behave the same way.
I missed that, thanks for letting me know!
[removed]
For the components I mentioned ... 15, 20 minutes? A comprehensive adapter would of course take more time, perhaps a few days.
[removed]
[removed]
http://mindprod.com/jgloss/abundance.html
Paging John Shutt, http://fexpr.blogspot.com/ Fexprs can apparently play nice with lexcal bindings.
But *at the time*, as Pitman notes, it was easier to persuade Lispers to give up fexprs than to embrace lexical binding; macros were deemed sufficiently expressive, and played well with both. Shutt's work didn't appear until several years after both issues had been resolved, and even had the question still been open, I wonder whether it would have changed much. It should be noted that Lisp *users* were getting on just fine with fexprs and dynamic binding. It was the *implementors* - specifically, compiler writers - who had the headaches. And compilation was only important because Lisp interpreters were slow - although running a Lisp interpreter on a modern machine is still orders of magnitude faster than running compiled Lisp code on even a Lisp machine. Beckons^(no, not begs, which is quite different) the question of whether it was wise to spend all that effort on compilation. **edit**: on quickly scanning the first post on Shutt's site, it appears that he's reached a rather similar conclusion - that practicality of compilation, rather than the possibilities afforded by first class functions and the lambda calculus, came to define what was desirable in Lisp. But then, Lisp was always defined more by pragmatism than by theory - or to put it another way, Lisp evolved as a way for people to figure out what they were doing, and by the time they realised how much they'd got wrong Lisp was just too useful to be retrofitted.
[removed]
[removed]
[removed]
&gt; However, CISC processors persist due to momentum, like the Intel x86 family, and in the microcontroller area where raw speed is not an important factor. Written in 1995 -- wonder how the author would feel about how things went since then. I think not many would have expected another 22 years of dominance by x86.
Thanks! Glad you liked it.
The social aspects of it are fascinating. I wish to know more about this topic in other languages, niches or domains.
GOTTEMMMM
Just that everything you need to learn forth fits into a meme.
even cant recognize those letters in the picture
For those who don't get it (myself included) what are those things? :)
&gt; everything you need to learn forth "everything you need to learn forth"
the two links
When people tell me Forth is so unintuitive, I use shell pipelines as an example of concatenative programming that many find comfortable. The big difference to me is that the implicit data structure is not a stack, but a stream. Recognizing that the implicit data structure could be anything, I've been trying to imagine a concatenative language where it is a graph or something really crazy.
[removed]
I've asked something like this in the past; My understanding is that the companies that rely on Forth systems and the maintenance of them are something of an insider secret, as the "old guard" has no motivation to pass the torch onto a new generation (why give up your livelihood?), and there aren't many new Forth jobs being created, if any at all. So, yeah, no. Nobody that doesn't already have all the programmers they need. Your options are: wait for them to retire or die out, or startup your own Forth powered thing. If something ever popped up, I'd be first in line to apply for it...
The company I work at use Forth extensively in our products. We will be hiring more Forth programmers at some point. But in general, as /u/mcsleepy said, nobody that doesn't already have all the programmers they need. Forth is somewhat of an insider secret, or in our case, secret weapon.
looks not that old 
Thanks for replying, I'm not specifically looking to get hired, just curious to know more about the application domains. Could you share more about what your company does? Or would that violate the secrecy clause?
Algorithms that would take several pages of C code were one-liners in Forth. We could also update algorithms on devices without having to recompile and ship a new firmware. Algorithm developers (who know MATLAB and statistics) could modify their algorithms without the firmware engineers having to worry about how the C code got accidentally mangled. For instance, in this embedded system, there was no FPU, so all calculation had to be done in fixed point. But the compiler would happily emit floating point calculations by including a soft fp library, which was very slow and consumed way too much code space. We couldn't disable the fp lib completely because it was required to calculate compile-time fixed-point constants. And the algo devs didn't know anything about firmware, so they didn't have their toolchain set up or hardware to run their changes anyway. Basically, it's the same reasons you'd use any high-level language. Except Forth would actually fit on a device with 48k of RAM.
My best guess is that the system goes along with great beard, and it's cheaper to keep paying him to maintain it and put off rewriting it until he goes...
We're not. My desktop machine has a 64-bit ARM CPU.
&gt; Python (to generate Forth code from a machine-learned classifier specification) Could you expand on this?
Well a classifier is just a tree of "if feature1 &gt; threshold1, if feature2 &gt; threshold2, ... classify as type 1" with machine-learned features and thresholds. If the features themselves are mostly simple vector/matrix transforms, then you can decompose them into a list of concatenated operations. So if you have everything in the right format, it's a short script to turn the classifier output into "SIGNAL_DATA VARIANCE 3.238 &gt; IF SIGNAL_DATA SOME_CONST_MATRIX M* MSUM 2.455 &gt; IF 1 CLASSIFY THEN THEN" and making the code very boilerplate and kind of stupid simple, while allowing a rich set of feature operations and possible tree structures.
I enjoyed this quote from the linked article: &gt; If C gives you enough rope to hang yourself, Forth is a flamethrower crawling with cobras. 
Flamethrowers are too compex. Forth is just a very sharp knife.
Consider how stupid one has to be to manage to accidentally hang themselves. Article author is guilty of repeating a pretty meaningless metaphor and then accusing Forth of being worse in the same aspect. At least Forth lets you test interactively.
[removed]
Great minds think alike ;)
Because the different voltage levels need to be separated somehow after reading and this will require additional logic effort. I think this can be an option for quantum computing probably.
Are your Forth products small? How large do you think your products would be written in other languages? 
Agree. Stack is wack. It just makes everyone focus on the stack. What's in the stack? Pop off the stack. Duplicate what's in the stack. You are the stack.
Unfortunately there was no video for this talk, but here's the [slides](https://docs.google.com/presentation/d/1wL2eqf7eHGEybsK0C4MUB4ibP1z6voyhgYfUSNhuBQA/edit#slide=id.p).
Hi, I remember you rainbowforth in the past is it still mantained i like colorforth Bye
 I wonder how much sourceless Forth systems can learn from other traditionally sourceless programming environments, like APL.
Cost, mostly. It's all about the BOM.
[removed]
I wonder whether the primary thing we should learn from sourceless environments is that they solve the wrong problem. As someone (who escapes me) once observed, programming languages are only incidentally for directing computers; their primary purpose is communicating with other programmers (including the original author at some later date). Perhaps the real problem with sourceless (and maybe even all image-based) systems is that they streamline the incidental, whilst abandoning the primary altogether.
[removed]
thx, but could you update the sample arch for those principle articles? its hard to get 6809 these days, can you use arm/avr/mips as the target arch instead?
Hmm... considering some these article are pretty old, I suspect something like that would have to be left to others to do. For those with the interest, it would be a cool and relatively easy project to translate some of these to modern architecture. 
[removed]
It took me one day to write an assembler for ARM. And another day to use that assembler to port my Forth to ARM.
Sounds like something Djikstra or Knuth would have said.
&gt; I think it might have been Fred Brooks, actually, but don't quote me on that. ~ */u/thamesynne*
...of course there is. \**sigh*\*
well maybe you are "everyone", but for me, i am eager to practise what he said in moving forth on a modern cpu
Cool, that's what I do with embedded platforms as well. I try to find or port or write a Forth first before working with something. Much easier than assembly/C. And mostly we work with assembly as we need to work in *very* small spaces.
Your selfy isn't as sexy as FORTH. Not even close...
There isn't much Forth in this report, but it is about doing much more with much less, so I think Forthers will find it interesting. I found it [in this comment](https://www.reddit.com/r/ReverseEngineering/comments/5t244s/generating_a_thumb2_disassembler_from_the_pdf_spec/ddkcun8/).
of course that's just an example, but you need to know that archtecture to practise these examples. and 6809 is really hard to found . and except to understanding the example i dont know what else i could do for using that. i already have some x86 asm experiences from learning "programming from the bottom up". so if its written in x86 i think that would be very helpful for understanding , also even if its written in arm/mips its still helpful cause i could learning them not only for this book but also for other purpose
I added generic words to Lifoo (https://github.com/codr4life/lifoo) with specified argument types, the compiler uses that to select the right word from the types on the stack or signal an error if the right combination is not available. It's been a life saver for chasing down stack errors. As long as they're optional/gradual, I say adding types improves the Forth experience.
Go for it! A couple of stray observations: Firstly, 64-bit address space is signed; 0..4GB isn't at the bottom, it's right in the middle. This probably doesn't matter very much, since you need a *lot* of memory before it wraps around; but it might be worth noting. Secondly, only the indirection needs to be in the 0..4GB range, since you're using 64-bit addresses for the jump. You could structure things so all that needs to be under the 4GB boundary is a [CF,PF] pair, with PF pointing at a list / data structure anywhere in memory. Thirdly, since you probably don't want misaligned accesses in your inner interpreter, why not take advantage of qword alignment to stretch your reach to the 0..32GB range by using `jmp [rax*8]`? Fourthly, lodsd is slow, and sticking it right next to the indirect jump risks both the usual indirect jump misprediction (although that isn't that bad in modern processors) and an AGI stall. You'll probably see better performance taking a leaf out of gforth's book, and separating it into movzx r14, dword [r15] add r15, 4 ; your primitive code goes here jmp qword [r14] which (a) frees you from having to use those exact registers, (b) reduces pipeline stalls, and (c) spaces out your primitives enough that they'll land on in different 16-byte rows, which will help with branch prediction. It does have a longer encoding than lodsd/jmp [rax], but hey, you're making a 64-bit Forth... **edit**: One last one. It's worth separating out code from everything else, so that the code, once it lands in the i-cache, never gets evicted because something just wrote to a variable in the same cache line. Fortunately indirect threaded code pretty much gives you that for free. 
Okay, so your suggested change list is this: * You're correct, but since the 32-bit addresses are zero-extended, this isn't of any concern. * This is something I thought about, and I like the idea: I'll split definitions into dictionary and `ALLOC` space. * This is a clever idea that I wish I'd thought of. `jmp [rax*8]` is 5 bytes longer, though, including a dword-size zero displacement. Combined with your next tip, the size of our inlined next would be pretty hefty. I don't think my dictionary space even use a fraction of the 4GB space anyway. * Replace `lodsd` with an equivalent, but larger sequence. I agree, the savings are paid for already. This also frees up `eax` and `rsi` for me; I can use any pair of registers for W and IP now. One thing to look out for, yourself, is that x86_64 doesn't have `movzx` from a 32-bit to 64-bit register; `mov r14d,[r15]` automatically does the sign extension. Also, I wouldn't use `r8`-`r15` in my inlined code because of the VAX prefix they require on each opcode. With this, so far I have 4 memory sections: * Dictionary, 0 - 4GB * Code space * `ALLOC` space * Stacks *edit*: This makes me wonder if I can leverage the FS and GS registers for anything. But I think they're only really useful if you're multithreading, which isn't even on my mental roadmap at this point. Have any ideas?
As you plan a bare metal port both registers can be useful. Dependent of the processor architecture (AMD or Intel) MSR registers C0000100h and C0000101h hold physical (64 bit) base addresses for the FS and GS registers. It is thus possible to access the entire address space from unreal and protected mode without entering 64 bit long mode. This can be of advantage because long mode requires paging. If your Forth system does not profit from the additional register set (which is only accessible withing 64 bit long mode), running in a 32 bit mode allow a more compact encoding, requires lesser effort and you can still access the entire address space at demand (Please note, some programmers call this possibility Unreal 64 mode).
Anton Ertl [did some benchmarking of Forth systems](https://www.complang.tuwien.ac.at/forth/performance.html)
This article finally got me started on writing the language I've been dreaming into shape over the years. I've had a couple of false starts, but this time the design really took off. Seems like Forth was the piece that was missing from my puzzle. https://github.com/codr4life/lifoo
It's both, really. The stack-based architecture makes it natural to pass items implicitly in a 'flow'. That turns into an advantage in that you code *can* be easier to understand and to code.
Wow, I've been reading a bunch of interpreter / compiler implementation papers, and just read about Duff's Device this afternoon. And here's an article on forthit! Thanks for posting, looks neat.
I agree; this is not intended as competition to Forth, more a celebration of it's power. The lack of immediate words is compensated by Lifoo's embedded nature, implementing words in pure Lisp that act on token streams is supported in the API; I did consider going all in with first class macros but deemed it not worth the effort right now.
The dictionary can mean a lot of things; there may be as many variations in the dictionary as there are in compilation. Most implementations end up confounding multiple concerns. The dictionary is typical used for both storage and discovery, forcing a certain structure on the dictionary and making it difficult to unpick the system and the solution for deployment [0]. The system that we use at my company has a very unusual dictionary structure, which among other things enables vastly simpler and more efficient compilation, and naturally supports "turnkey applications". None of the Forth's that I implemented have used FORGET. Long ago I came to the conclusion that if compilation is instantaneous then it's easier to recompile everything, to return the system to a known state. The result is that we bootstrap constantly during development. Not only does this provide a flexible alternative to FORGET, it ensure the integrity of the system while reducing the barrier to entry to modifying the kernel. The source code in that system isn't text. When words are renamed the change is effective everywhere and visible instantly. This is a nice feature, but I don't know if it's much better than doing a search and replace in the source code[1]. It is somewhat more useful in the interpreter but isn't really essential; an alias would work just as well in this case. Where the classical Forth approach of accreting code and data into a space called the dictionary shines is it's simplicity. You can usually keep piling more "stuff" on the dictionary without much thought to where in memory things are going. Overall (in my admittedly biased opinion) the classical Forth approach works very nicely, but it's antiquated. Many different implementations have been explored over the years, and many of these have proven to have useful properties, that are absent in the classical Forth approach. tl;dr The state of the art has moved on a lot over the last few decades and and I find it increasingly difficult to recommend the classical Forth implementation[2] [0] Depending on the kind of problems that you work on this may or may not be a concern but there's something beautiful about being able to ship a solution without dragging along the entire system. [1] Without complicated scoping rules, search and replace is much more effective as a tool for refactoring Forth code than it is in other languages. [2] In my experience the Forth systems that are being employed in industry today are quite different from classical Forths, exemplified by implementations like gforth, or JonesForth, and the hundreds of other Forth's that have been implemented and abandoned on GitHub and the larger web.
&gt; No, memory can have lots of partitions. But each partition has two ends - the top end and the bottom end. And partitioning memory comes with problems of its own. My forth, gelFORTH is going with a sparse dictionary. Words *overlap* where they have identical semantics (common factors). They *overhang* where they have different semantics. The common factors overlap as literals -- not through indirection.
[removed]
[removed]
For references I'm using gforth for the time being. You're right about stack juggling being symptomatic of some issues. &gt; Words like `dup` are a symptom of words that consume more information than they use. I'm unsure of what you meant here but in that case, I needed a 'dup' because 'rshift' is not powerful enough as it discards the carry while I need it for computation. I should define my own 'rshiftc' word. Also I place on the stack values which I use all over the place when named variable would be better. Thanks you, without your comments I would not have realised those points. &gt;I use conditionals only at the leaves of a program for informational messages to the user. Everywhere else I go with a formula. Makes sense. 
&gt; the chunkier ARM processors perform awfully with threaded code. As far as I can tell, that's because they predict any indirect branch as though it were a return Just wanted to note that AArch64 has separate instructions to give a hint about whether it's a subroutine return or not.
Thanks. Yes, that'll definitely help! Even if indirect branches not predicted as returns are only predicted to go where they went last time, that'll still cut the misprediction penalty of threaded code in half (assuming the inner interpreter is inlined).
[removed]
&gt;(forth) "And let there be light..."
This isn't about Forth but the structure of the code and the arguments for that structure, and working with it, may be interesting to Forthwrights.
&gt; this is exactly what I meant about CREATE being poorly factored! :-) If by that you can accept that the "poor" factoring makes this harder to do, which in some opinions is a good thing (for much the same reason that many people think parse words should avoided.) &gt; is that a syntax you can live with? I spend about 3 years working exclusively in Lisp - so I'm very familiar with let - and I personally find the second form much easier to follow, requiring no look ahead to understand. Sometimes a deficiency - lack of flexibility- is a strength.
To be honest, I find the second form more aesthetically pleasing too.
[removed]
 #define BEGIN { #define END } 
I don't understand the implication.
See the benchmark code on the MPE web site http://www.mpeforth.com/arena/benchmrk.fth Stephen
Anonymous code blocks is a great idea, actually. The way Lua does it is that every function is a lambda that gets assigned to a named pointer (basically). It would be almost trivial to implement a Forth with this functionality (maybe even returning functions from another function), but this also seems to imply some form of memory management scheme. Dynamic function compilation might get tricky, but I'll revisit this and maybe have better avenues of attack in the near future. Taking another page out of Lua's book, it would also not be difficult to implement its Table data structure for our dictionary, which would be yet another trivial expansion to nested/linked dictionaries.
There are no immediate words in our system, just words that emit code when called; we've inverted the normal relationship a little. Everything happens immediately unless you state otherwise. This would be clumsy in a classical Forth but it can be handled cleanly using an appropriate source code format. Think colorForth, if that helps :-). Needless to say coming up with an appropriate source code format took us a long time and many iterations. I'm sorry if that's more vague than you wanted but that's all I can say for now. We'll be open sourcing our core system later this year and I'll happily go into detail then. There will be a private beta before the initial public release and we still have a few spots available if you'd like to be involved.
:-) while I agree to some extent I would like to point out that Charles Moore's later Forth's are significantly less flexible; there are no defining words or parsing words etc. You can still produce really beautiful, efficient, expressive code but you have to work with the "natural" word order (of the computer.) This has a number of advantages, not least that code is easier to read, and you have to write less of it (since you're not massaging it into an "unnatural" word order, just to make it look like something else.) To be absolutely clear: do what feels right. There is no "right" answer here.
Ah, I see. So input from the keyboard is a lambda that is compiled and executed right away? Then other words during the normal compilation phase act more or less as calculation words or macros.
Do you think it would be uncouth (at least in Forth) to build in some sort of word naming that could operate as a regex? Although I suppose at that point it would be not too far away from some bastard BNF with lisp-style macros.
Although even Moving Forth is 24 years old now - and the most prominent commercial Forths (FORTH Inc's SwiftForth, MPE Ltd's VFX, Marcel Hendrix's iForth) have abandoned threading altogether in favour of native code generation, of varying complexity. Even gforth has stepped away from the traditional threading model. This may be because modern CPUs impose [a much higher cost](http://www.complang.tuwien.ac.at/forth/threading/) on threaded code than old minicomputers and 8-bit micros did - for two reasons: Firstly, pipelining and branch misprediction hit threaded code particularly hard; secondly, Forth primitives tend to be implementable in only two or three machine instructions on modern CPUs, in such a way as to allow them all to run in parallel; on old minicomputers and 8-bit micros, each primitive took several instructions to implement, meaning that the overhead of threaded code really wasn't that high in comparison.
https://www.reddit.com/r/rust/comments/5wa6ly/pumpkindb_an_event_sourcing_database_engine/
I'm not sure about that. Once you have a written one Forth why would you go back to using a third-party assembly language? I bootstrapped my first Forth system several years ago and every Forth I've written since has been written in the preceding Forth. I start from scratch, and then write only what's needed, but not from assembly. Starting from scratch doesn't take very much time at all in my experience; I've done it in a day or two. The real problem's come when you "need" a lot.
 &gt; abandoned threading altogether in favour of native code generation, of varying complexity Have any of the Open Source implementations done write ups on how they're pulling that off?
The irony is that if anything needs the speed of something like VFX, it's a resource-constrained platform. And in fairness to MPE, they've taken the same view; the free Cortex-Mx and MSP430 cross-compilers they offer on their website include the same VFX technology as their native x86 compilers (as, of course, does their commercial cross-compiler range). I assume it was easier to develop VFX in the comfort of a native environment, even if it was primarily intended as a cross-compiler technology.
It can, you know. Lisp has been used for systems programming for well over 40 years; the first Lisp machine was proposed in 1973, and PDP-1 Lisp ran on bare metal over a decade before that - and in about the same kind of space as a basic Forth. In modern systems, SBCL and Corman Lisp are two examples, but far from the only two, of systems that compile everything straight to native code; indeed, there's nothing that prevents a Lisp having the same kind of embedded assembler as most Forth systems do. Two things get in the way. First, there's Lisp's type system; unlike Forth, which lets words interpret data as they please, Lisp rigidly types data objects (at runtime). That typing can be overridden (and in a standard way, in Common Lisp), and in early systems was very far from foolproof; but it's easier to work with than against. And it's also tied intimately together with the second impediment, pervasive dynamic allocation. Lisp is basically written as a system with infinite memory, which - given its genesis on systems that had anything but infinite memory! - meant that it was garbage-collected from day 1; and for garbage collection to work effectively, it needs to know the type of every object in the system - at least to the extent of knowing where all the pointers are, and what they might be pointing to. Whereas core Forth doesn't even *have* dynamic allocation as standard; new data is only ever declared from the interpreter loop. However, those two assumptions are open to question without fundamentally changing Lisp's character. One might start by observing that most Lisp functions work with particular types of data - for example, CAR only accepts lists, and + only accepts numbers - and either use type inference to detect collisions of intent, or simply declare that anything passed to CAR must be a list. Likewise, garbage collection doesn't *have* to know types exactly. If memory is abundant, it can be turned off altogether; otherwise, conservative algorithms like Boehm's can be used just as well with Lisp as with C (and in fact, some Lisps do). Indeed, Lisp has been [proved to not need garbage collection](http://home.pipeline.com/~hbaker1/LinearLisp.html) at all. The inherent typing of Lisp data is secondarily a safety question, and primarily a question of polymorphism - if you want to use the same functions to act on different kinds of data, your have to be able to tell what kind of data they've been called with. However, if you can live without safety and polymorphism, there's no real reason why Lisp couldn't be as low level as Forth.
Well, yes. Aside from anything, native compiled Lisps like SBCL are *very* fast. Lisp has been something of a testbed for compiler technology over the years, and they've got good. There's definitely a Lisp influence to Forth - Moore says as much in a few places - but in a contest between SBCL and VFX Forth, I'd put my money on SBCL. And SBCL is free software, and portable; there simply isn't a portable free Forth that can lay a glove on it, speedwise. In summary, it's probably truer to contend that Forth is just slow, static, inflexible, unsafe Lisp.
Your understanding, I'm afraid, is somewhat flawed. Firstly, reverse Polish notation and Polish notation are the same thing, just the other way around. In terms of abstraction and parsing complexity, they're at exactly the same level. Secondly, you're assuming interpretation. As I've repeatedly pointed out, both Lisp and Forth have moved on from direct interpretation. Forward or reverse polish only matters at parse time, and for that, see above. Thirdly, even in naive interpreters, the differences in efficiency between Lisp and Forth have nothing to do with forward vs reverse Polish notation, and everything to do with the different semantics of the languages.
Actually, if you tweaked the Forth interpreter to interpret by character than merely space-delimited words, it would be able to interpret Lisp expressions just fine. I see Forth and Lisp as dual languages. EDIT: To add on, both could do each other's jobs, but they come at tasks from opposite ends.
&gt; That is not correct. It damned well is! You are conflating Polish notation with s-expressions. Look it up. In fact, basically, follow the advice of your own bloody username. \**plonk*\*
I think it's the exact opposite. You have polish notation when you have parens and you have reverse polish notation when you don't have parens. The point of RPN is to eliminate parens. Writing it backwards or forwards makes no difference. If you don't have parens you simply don't have lisp, even though it may map to lisp.
[removed]
Hmm, I wonder how much it would take to create a Lisp interpreter for Forth, so that something like this: (+ 2 3) would be parsed like this: 2 3 + I think you'd make the parens execute the rest of the words in the sequence, then the first. Making operators expand to arbitrary arity might be difficult; Forth interpreters don't usually know the arity of an operator. You'd want to transform this: (+ 1 2 3 4) into this: 1 2 + 3 + 4 +
Here's one approach to arbitrary arity from comp.lang.forth. https://groups.google.com/forum/#!searchin/comp.lang.forth/lisp$20in$20forth|sort:relevance/comp.lang.forth/gBXAQ30WSmg/KMBcb78BaosJ : ( depth 1+ r&gt; 2&gt;r ; : cond depth j &gt; ; : done 2r&gt; rdrop 2&gt;r ; : +) begin cond while + repeat done ; : *) begin cond while * repeat done ; ( 1 2 3 4 5 6 7 8 9 10 +) . ( 1 2 3 4 5 6 7 8 9 10 *) . ( ( 6 ( 3 3 +) *) ( ( 4 4 +) 8 *) +) . To get the prefix s-expressions you'd need to do some parsing. Perhaps you could leverage `evaluate` or `execute-parsing`.
I think it comes down to where they came from. Lisp came from big iron and academia, and ended up being a testbed for new software technologies; Forth came from one guy who wanted to do his job better. So Lisp ended up with really good compilation technology because it was where those techniques were developed, and Forth ended up with the simplest compiler that could possibly work. There's also a difference of philosophy. In Forth, it's expected that the programmer will do as much work as possible, in order to make things easier for the system. In Lisp, the system is expected to make things easier for the programmer. It's funny, really, how two systems whose aims and origins are so diametrically opposed ended up with so many parallels between them. I'm interested in fast Forth, too, but not at the expense of its essential character. I like indirect threaded code; I like its potential for homoiconicity - there's no reason why Forth programs can't manipulate themselves in exactly the same way Lisp programs can, if one presumes ITC. I think that just-in-time compilation of lists is likely to be a much more productive way of speeding up a Forth system... and it can be done without losing the inherent reflectivity of ITC by simply rewriting the code field: let colon definitions start with doNEST, but let that be a routine which, as well as pushing the interpreter context, replaces itself with the address of doCOMPILE, which will in turn compile the list it's passed into native code (which can happen very quickly indeed) and replace itself with the address of that code. When the word is edited or manipulated, the address is reset to doNEST. Et voilà - the simplest possible JIT - only a word called more than once will be compiled - and a fast fast Forth which nonetheless remains indirect-threaded. (This isn't too far removed from what gforth already does with dynamic superinstructions, except that gforth does it for everything at compile time.) As to *how* to compile stack code so that it's reasonably speedy... fortunately, most of Forth's primitives have fixed stack effects (?DUP is the obvious exception), so it's relatively easy to analyse the overall stack effect of a basic block and allocate registers accordingly; unfortunately, basic blocks tend to be pretty short, perhaps even more so in Forth than in other languages because of the intense factoring that Forth rewards. And even more unfortunately, even the most complex Forth compilers tend to assume no lookahead, because of Forth's incremental, interactive nature. Moreover, stack code is so serial by nature that opportunities for parallellising operations and sharing intermediate results can be hard to shake out - much harder, ironically, than with languages that do everything with local variables. VFX and FLK work by basically modelling the stack in registers; VFX will also inline short routines at the source level, rather than trying to inline compiled code. There are optimisations possible without going to native code, though; Wil Baden proposed combining common runs of primitives into longer words, a process he termed pinhole optimisation (like peephole optimisation, but on primitives). For example, DUP &lt;lit&gt; @ + &lt; IF could be compressed into a single primitive: ldmia rIP!, {r2,r3} @ in ARM code ldr r4, [r2] cmp rTOS, r4 addeq rIP, rIP, r3 ldr rTOS, [rSP], #4 next which executes much more quickly than the six individual words would, and avoids much of the stack traffic. Oddly enough, the bits of compiler literature on parser techniques will be the most helpful here.
&gt; and a fast fast Forth which nonetheless remains indirect-threaded. (This isn't too far removed from what gforth already does with dynamic superinstructions, except that gforth does it for everything at compile time.) Is there really a huge penalty to the indirect threaded jumps in a Forth if the core dictionary is held in cache? You'll just be jumping in/out of the cache anyway, which will make it only a shade slower than hitting into your register file.
Depends what you call huge; my testing indicates that on sieve, indirect threaded code is 3-4 times slower than relatively simple native code (specifically, gforth-itc vs mxForth) on a Core 2. Going from relatively simple native code to densely optimised native code (mxForth to VFX) gives another 3-fold speed increase.
I think my point was more, would the gains be larger if we perform "stack" manipulation on registers instead of trying to JIT instructions and remove the jump calls there?
&gt; which is slow not because of code cache effects, but because a non-trivial percentage of indirect jumps get mispredicted in threaded code (in the worst case - processors like the Cortex A53 (in AArch32 mode), which appears to predict any indirect jump as a subroutine return - 100% of them), which causes a pipeline bubble. That's what I was looking for! Thanks, that makes total sense. Wouldn't JITing in Forth words make the dictionary grow potentially very large? Also how would you handle allocating/reallocating space in the dictionary in order to make space for the native code?
Polish notation was invented to remove parens which are necessary with infix. https://en.m.wikipedia.org/wiki/Polish_notation
Non-Mobile link: https://en.wikipedia.org/wiki/Polish_notation *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^38475
I don't know if you'd have to write your own parser, just redefine the left and right parenthesis. A left parenthesis would parse the next word and push its xt on a stack (perhaps the return stack), and a right parenthesis would pull it and execute it. The `(+` and `(*` words might be a bit tricky, though.
In the original mathematical notation he may have sometimes written it without parentheses, but there is a VERY important difference between polish notation and reverse polish notation. In polish notation each function may take an arbitrary number of arguments. In reverse polish notation the number of arguments and their types are fixed, unless you have an end condition. In polish notation you can do thins like this (3 3 3 3 3 +) In reverse polish notation you can't do that. Because what if I want my result to be 8 6 and I type 8 3 3 + RETURN. Well plus in parentheses would eat everything to the left and so you get 8 3 3 + =&gt; 8 6 + =&gt; 14. Clearly this operation yields different results in polish notation. This has implications. The key difference here is that in LISP the output of one function becomes the arguments of the next function. In FORTH all of the inputs and outputs of functions are stored in one big linked list. That is very important to keep in mind. In lisp you can not steal the arguments that another function was going to get, not by intended design anyway. In forth you can write a function that eats up every input and output on the linked list and push it to a database. You can accidentally give too many arguments to another function or take away an argument that another function was supposed to have. This is much like haskell's type system. You must know the type of the two functions you are paring up in haskell. In forth you must know the stacks of the functions you are pairing up. In lisp you do not need to know in the least bit what the arguments of a different function looks like. All that you care is that your inputs are of the right type and if they are you transform them and spit them back out. This is the key difference between polish and reverse polish. It makes no difference if you write it from the left or the right.
[removed]
You could push the + as a tagged piece of data, and push another tag that tags "1 2 3 4" as a list for + to operate over. All-in-all, a pretty minor modification, but then you'd probably be better just writing the Lisp interpreter from the start for all the trouble.
You clearly did not properly read the link I supplied, or you would have seen that a synonym for Polish notation is prefix notation. The "pre" in "prefix" means "before." It also says that Polish notation's "distinguishing feature is that it places operators to the left of their operands." (3 3 3 3 3 +) is clearly reverse Polish notation (RPN) with arbitrary arity for the + operator. RPN is also known as postfix notation. The "post" in "postfix" means "after." If you can be bothered reading the below link, you will see that it says " Reverse Polish notation (RPN) is a mathematical notation in which every operator follows all of its operands, in contrast to Polish notation (PN), which puts the operator before its operands." It also says "an example of a unary operator whose standard notation uses postfix is the factorial." An example would be 5! or the factorial function applied to the number five. As you can see, the operator (!) comes after the operand. https://en.wikipedia.org/wiki/Reverse_Polish_notation
I am not sure if you understood the point that I was making. Lisp operates by taking the outputs of one or more function as the inputs of another function. Forth stores all outputs and inputs onto a linked list. Do you understand what the difference between those two things is?
Both Lisp and Forth can be implemented with the functions/words taking the outputs of one or more functions/words from the stack as input. The main differences between the two implementations being that the Lisp stack usually has a stack frame, whereas the Forth stack doesn't, and Forth allows/requires the coder to use the stack explicitly, whereas Lisp requires the coder to use the stack implicitly. (A minor point, Forth's stacks are almost always implemented as arrays with stack pointers, rather than a linked list.) 
Is there a version of lisp with no garbage collection or automatic memory management?
This is what I mean is that unless you want to make some serious compromises lisp must logically be a little slower than forth. Now that being said if I were to make a forth I would probably put garbage collection in it but that is what I mean to say.
Are you familiar with Postscript? It is kind of like a cross between Forth and Lisp; Forth with Garbage Collection. 
Yes I am roughly familiar with postscript and consider it to be the most successful forth there is. I have never investigated it.
How is this post related to Forth?
Thanks. Yeah, I learned long ago not to "optimize" too early in the coding process. My current rash of performance improvements is entirely in response to real-world problems. I definitely agree that the Forth testing methodology yields rewards, in particular when you need to see the inside of something at run-time.
Yeah, that's definitely a powerful advantage. Whenever one of my friends sees me testing Forth code, they're horrified when I mention that I might change some features of how polymorphism is handled in my code. But hey, that's just how it goes; they can't wrap their heads around the methodology. "You can't handle the truth!"
What on earth are you writing that would require an implementation of "polymorphism" in your FORTH code? Your own, unique language?
Yes, more as a proof of concept/teaching tool than anything else. Polymorphism is trivial if you have a few hash tables, built-in strings, and sane garbage collection for the previous two items (I'm using a copying tracer). People have trouble wrapping their heads around OOP, and it's much easier to demonstrate static and dynamic binding in action (and in step-by-step slow motion) when the teaching language is running on top of Forth.
Neither, I would say. But Lisp predates Forth, if that is what you mean. 
Forth is very lisp and lisp is very forthy.
I feel like we care more about Lisp than /r/lisp cares about us
I think a general patern in the universe is the movement of things between very monolithic versions and very distributed versions. For example in nature hive like creatures seem to behave much more efficiently than big beast type creatures, and yet we humans as big beast type creatures have grown to such numbers and had such communication skills that now we act like a hive and so we have gone from a state of a monolith to a distributed state. I think this is just the natural order of things. And it makes sense that if this is the correct way that both the forthers and the industry would tend towards this. But yes definitely I agree with you completely.
Offtopic: &gt; I'm fairly sure that back when the benchmarking shootout still included Forth, it demonstrated that the Lisp implementations were generally rather faster than the Forth implementations. It is a shame that there is no Forth included anymore. Why is that? 
Why does the RPN feel so right and unix feel so awkward &gt;_&lt; 
Pipes are streams between concurrent processes. the lisp example is of a function call. the forth example is words being interpreted. The unix example is spawning three processes and setting up streams of communication between them. The comparison is unfair because Forth is interpreting the words sequentially, and there are no streams of communication between them.
I think you're confusing syntax with semantics.
So I agree from the perspective of how it works internally that pipes are fundimentally different. My point is that pipes, lisp, and forth are all about redirecting the output of one program into the input of another. It is this redirection of IO that interests me, and for me personally I don't care how it's implemented as long as IO is redirected.
The industry will have to move that way. I heard that server electricity consumption in the USA was 20% of the total, but I cannot find a link to confirm that. Nevertheless, server requirements are growing faster than the grid capacity growth so a day of reckoning is coming for efficiency.
I must correct myself. "US data centers consumed about 70 billion kilowatt-hours of electricity in 2014, the most recent year examined, representing 2 percent of the country’s total energy consumption, according to the study. "http://www.datacenterknowledge.com/archives/2016/06/27/heres-how-much-energy-all-us-data-centers-consume/
There is a limit and the owner might sell their shares after a short time profit.
And speaking of beasts, the top-down, formal, doctrinal method of coding has clearly hit the wall. Even something simple like a smartphone game now must import many thousands of libraries. Obviously the designer has no f**ing idea what's in those myriad black boxes. Why not just give up on that paradigm and adopt nature's algorithm: - try everything - keep and replicate what works - repeat 
What is the license for these?
I'm not redirecting anything. Unix pipes are not a stack. They don't even resemble a stack. There isn't even a superficial similarity between piping the output of a command to another command, and stack-based operands.
Unix pipes are not implemented as a stack but you can redirect information with a stack in a similar way to how you can do it with unix pipes.
foo x | bar | baz x foo bar baz
That's not an explanation, that's some random metasyntactic words. Other than they in nearly the same order, there is no similarity. They don't mean the same thing.
Between TI-85 hacks and Sun bootloader fonts, was there anything substantial?
Not really but it was nice to hear different opinions about Forth :-) I thought the range, from "kill me" to "mother's milk" was quite fun. Overall my experience from talking to them is that the FreeBSD developers hate Forth and would love to replace it but they have found out the hard way that there is no practical alternative. There was some talk about replacing Forth with Lua but that seems to have fizzled out as more and more features have been added. This opinion is easy to understand if you take a look at the Forth code for the FreeBSD bootloader... it looks like it was written by thousand or C programmers hacking away on a thousand teletypes. This is some of the worst Forth code I've seen but good luck telling that to the FreeBSD guys who openly consider themselves the bastions of clean code ;-). "We know how to write good code and we wrote this so by definition it must be good code and since it's terrible and unreadable Forth must be terrible and unreadable" The code may be crap but it's integral to booting tens of thousands of FreeBSD and now IllumOS boxes. At one point I was thinking about getting involved and helping them improve it but the overall hostility towards Forth put me off. The IllumOS guys are much less hostile towards Forth as you can see and they chose to adopt the FreeBSD bootloader despite its poor code quality. I'd argue this is because it can do things that no other bootloader can do right now and that this is a direct result of it being Forth but I think it would either be preaching to the choir or it would fall on deaf ears :-). Maybe the IllumOS guys will hep clean it up. It's a real shame that one of the biggest uses of Forth in an open source project to be such a poor example of Forth code.
Where exactly is the Forth bootloader code? Github?
Conceptually, it would probably not be very hard to borrow from either of them for some of your own special sauce. Which chip are you targeting, btw?
I'm attempting to create a self-contained device, so this approach doesn't work for that. However, the idea is interesting.
It's been a few years but if I recall correctly you should find it in the FreeBSD source tree under /sys/boot/ :-)
&gt; I'm attempting to create a self-contained device So do you really need BLOCK support, given that either Forth system would essentially be a closed environment anyway? Getting source code in would be as simple as just squirting it over a command line; getting it out is a problem that doesn't need solving, as long as you're obsessive about console logging.
thanks :)
&gt; Forth is not a requirement, it's just something I know already And certainly, if you want a programming language that can double as a transmission protocol Forth is probably the easiest route. Hence, a redirectable SEE. &gt; I want to avoid a laptop-dependent system But you need to have a keyboard and a screen in order to program... I suppose a touch screen obviates the keyboard requirement, after a Small Matter of Programming, but that still means at least a tablet.
Nothing wrong with that. There are many mindsets in the world that I don't understand either.
Well, Amforth does seem to adhere reasonably closely to Forth standards rather than trying to go it alone, so if you want a more standard Forth then yes, it would be better. Which common words are you feeling the absence of in FlashForth?
Crashes in what sense?
gForth freezes and shows nothing. On android it seems to cause a resource crash. It doesn't return any errors or dumps that I am aware of.
Yes, that's because it's a tight loop with no exit nor any pause in it. The contents of the tib are frozen at the point at which you call 'hello', so if you run it interactively it'll do nothing forever, and if you somehow manage to call it with an empty tib it'll print "Hello" forever. And nothing else will ever happen. In Forths which support co-operative multitasking, things *might* be improved by putting a PAUSE before #TIB, which would give other tasks a chance to change the state of the tib. gforth doesn't include PAUSE out of the box, but there's one in tasker.fs; however, it doesn't help unless you have another task running that will clear the tib somehow. However, is there any reason why you can't just use the interpreter loop as is, rather than trying to duplicate it?
And, you saved my day. 
Same thing happened on my Pentium III daily driver sans crash. I am not diagnosing what's going on. To Duke Nukem from DasBoschitt's Foreverquest, "I've got shit to do".
Walk through the code. What does it do? In particular, what would "tib# @" tell you?
- strong type-checking in 8th - application stored as signed, encrypted blob in 8th - all data-types as single stack items in 8th - transparent numeric (int,float, bigint ) in 8th
Here are some that I think are important, and found their way into Able * headless definitions * great for doing turnkey applications, where you want to be able to strip off all the supporting language structures and deliver the smallest possible binary. * multiple entry points and exit points. * together with tail-call optimization this allows you to express all sorts of different patterns without having to invent and name new control structures * I call this semi-structured programming. In many cases, it leads to code which is shorter and clearer than the equivalent structured program where the natural flow of the program is perverted * extensible multi-dimensional dictionary * the multi-dimensional design of our dictionary allows us to layer meta-data * O(1) access to any part of the dictionary * we use a single worldlist and avoid needing more using `bind` * extensible multi-dimensional source * the same applies to our source code * we use color to show the semantics, but unlike colorForth we allow new colors can be defined. * this extensibility applies to the editor, as well as the compiler, which in colorForth were effectively immutable. * present-time execution * like compile-time execution allows arbitrary code to be executed at compile time, present-time execution allows arbitrary code to be executed when the code is presented to the user. * names as first-class data types * like the `bind` word this feature drops out of our dictionary structure and allows us to do symbolic programming * this is how we get by without parsing words * with colorForth Chuck Moore ended up making definitions one of the magical colors and so didn't need parsing words * this reduces the flexibility of the language quite significantly. * stateless compilation * since the compiler has no state none of the words have to consider it * no intermediate words, or special wordlists * you can execute any word at any time; this is explicit in the source code, and distinguished through the use of color. * region-based memory management (of code and data) * sort of anyway, it's a bit more nuanced than that EDIT: some additions * alternative to `create does&gt;` * colorForth didn't have anything like `create does&gt;` so had no defining words * it also had no way of controlling execution e.g. no postpone or equivalent. * if a word was defined in a special wordlist then it was executed, otherwise, it was compiled. * using explicit execution and first class names you can easily write words that define words * so there's no need for the `create does&gt;` dance; defining words are just normal words, composed of normal words like `:` (which takes a name from the stack), and written in exactly the same way. * alternative to "recognizers" * we don't have a traditional outer interpreter at all; our interpreter is quite a bit simpler, and there's no ambiguity e.g. data and names are easily distinguished by color * you can define new input words that wait for user input and do something with them * these words ace called explicitly from the interpreter so are given short names (think sh, ed and/or Vim), like * `ib` (insert blue), which allows you to insert a blue name into the source code -- call name during execution * `im` (insert magenta) which allows you to insert magenta data into the source code -- load data (base16) during compilation * `em` (evaluate magenta) which allows you to evaluate magenta data at the interpreter -- load data (base 16) interatively. * etc. * separate compiler and interpreter * our compiler is a wholly separate program to the interpreter (because UNIX got some things right ;-)) * the compiler acts on area's or pages of code * the interpreter acts on words entered interactively * the compiler is invoked from the interpreter * This leads to a much simpler compiler and system overall NOTE: the editor (think Vim and/or Emacs, only aimed at simplicity[-1]) is the least developed part of the system. Several months ago we moved from the mouse-driven environment that we'd been using since the system was first designed, to a more traditional keyboard-driven environment. This was no easy task since the language and environment were so closely integrated[0], but we finally have something that we're happy and moving forward with. There are a bunch of other things I'd love to discuss that I can't, unfortunately. Soon hopefully :-) Idea's I like which didn't make it into Able * smart compile, * rather than make the word know how to compile itself we put that information in the source code but it's a similar idea. * always compiling (FreeForth) * another approach to removing state I might use both of these idea's if I ever write a plain-text Forth again. As it stands the rich multi-dimensional source code allows other (different) solutions, and has other (different) advantages[1] Idea's which are becoming more widespread which I don't necessarily like * quotations and stack combinators * initially very useful but they allow you to push too much under the rug * all data-types as single stack items * implies that everything is a complex data structure on the heap * introducing not significant overhead and limits system applicability * strong type *checking* * implies a runtime doing even more runtimey things * more complexity and overhead EDIT: Idea's from /u/8thdev that we plan to borrow without asking ;-) * application stored as signed, encrypted blob (as an option, where useful) More seriously, this is a great idea! Hopefully, imitation is still flattery [-1] This is one area that we'll be looking at improving over time, and would very much welcome contributions of idea's and code once the system is released (the whole system will be released under the terms of the permissive opensource ISC license -- https://en.wikipedia.org/wiki/ISC_license -- the simplest and most Forth-like license we could find ;-)) [0] To put things in perspective, we didn't have any use for `find`, and names could (and did) have spaces in them. [1] Everything is a tradeoff, right?
&gt; had no way of controlling execution e.g. no postpone or equivalent Nitpick, but colorForth does have a colour that does the equivalent of cmForth's \\ . &gt; names could (and did) have spaces in them I'm reminded of Thinking Forth's advice re hyphenated word names (to paraphrase, they carry a whiff of poor factoring).
&gt; Nitpick, but colorForth does have a colour that does the equivalent of cmForth's \ . Thanks for the correction. What was the color? EDIT: I found it this the arrayForth manual &gt; (Cyan text) enters words only. Not relevant for F18 code, but analogous to POSTPONE in ANS Forth when compiling x86 code. :-) for some reason I had it in my mind that cyan words were macro words that were inlined by the compiler. I stand corrected. I don't know why this wouldn't be relevant for F18 code. EDIT2: Reading a little more :-). &gt; x86 colorForth has two wordlists (forth and macro), where the macro words behave like IMMEDIATE words in classical Forth. It is an optimizing system so the Compiler and Interpreter make different uses of the two lists and there are rules about which has precedence; for example, If the same word is both a macro and a forth word, the macro will always be found when compiling (green or cyan) and the forth word will always be found when interpreting (yellow). Macros are only relevant in x86 code. http://www.greenarraychips.com/home/documents/greg/DB004-131030-aFUSER.pdf &gt; I'm reminded of Thinking Forth's advice re hyphenated word names (to paraphrase, they carry a whiff of poor factoring). I'm inclined to agree but this was quite a different environment and we initially thought that the use of longer names (which we didn't have to type every time due to the heavy use of the mouse) might help in documenting programs. It didn't really work out that way, and the majority of words were short and without spaces. An example of where this might be useful (for self-documenting) open the file at path for read and write NOTE: that was name of a word That's not the wackiest thing that we've done with this codebase honestly ;-). EDIT: but there are far more interesting things about the system than this and the decision was made to drop these oddities, to, hopefully, make it more inviting.
The last three points are largely solved in Able, which is an image-based system with extensible multi-dimensional source code, makes use of semantic colors and does defining words without create/does&gt;. I'd love to tell you more about our approach to interoperability but that'll have to wait until I get the final all clear. Caveats: The syntax is naturally much more rigid than in systems with parsing words (but you easily define our own colors and there's nothing special about them; Their use is mainly to reduce clutter. You could do everything with compile-time execution and the first class data if you wanted but that quickly gets unwieldy.) Personally, I consider that a feature :-). In giving up this syntactic flexibility you get easier to read programs and compilation which can best be called instant. You can completely destroy the image. The image can be in a partial state of failure, with certain "programs" being broken and corrupt or inoperable. That's part of the freedom that Forth provides and we maintain. What I find very interesting is that from a handful of words we can bootstrap a clean system, within the image, and use foundation to repair/rebuild the rest of the image, and any or all of the programs. Because of the speed of the system, this whole process can be done in between keystrokes. As I wrote elsewhere: it's always a tradeoff.
Certainly the pros and cons of all these ideas could, and should, be discussed at length. I don't parciularly like all of them myself. But I wanted to bring out the full range of novel ideas, brilliant or crazy.
My Retro dialect has: Prefixes (somewhat like colors in colorForth, but which can be replaced/added to by the user without changing the kernel), quotations, combinators, word classes (words which handle other words based on system state). It's also rigidly RPN with no exposed parser.
A nested comment wouldn't really be a comment would it?
You could use an editor which let's you comment/uncomment a region with a since key command. On a slightly different note and of the many advanges of arranging code into blocks is that you can simply not load a bit of code, or load an alternative. This solves the problem well enough that I never comment out code.
lisbeth@memebox:~$ gforth .gforth-history in file included from *OS command line*:-1 .gforth-history:11: Undefined word : /* 1 begin &gt;&gt;&gt;next-word&lt;&lt;&lt; 2dup 2&gt;r s" /*" compare 0= - Backtrace: $7FC49FACBA68 throw $7FC49FAE1C68 no.extensions $7FC49FACF3A0 compiler-notfound1 lisbeth@memebox:~$ This is what I get no matter how I run it. I am sure the rest of my file compiles fine. It seems to not like the word next-word. edit: nevermind apparently the word for this in my forth was parse-word not next-word. It works now.
No, `next-word` isn't a standard word. That's why I described it to you. But here's an implementation that probably works in gforth: : must-refill refill 0= abort" End of file when parsing word" ; : next-word begin parse-name dup 0= while 2drop must-refill repeat ;
I believe you want to make `/*` and `*/` work just like `0 [if]` and `[then]`. No, I don't think there's a reasonable way to use `[if]` or `[then]` to implement your `/*` and `*/`. You would have to make those words yourself. You *can* modify the "parse area", which is where the input stream is stored when the interpreter reads it. But it's strongly discouraged. And the parse area normally only contains one line of text.
If you're okay with having only one comment exit point you can effectively nest comments like so: ( ." hello0" ( ." hello1" ( ." hello2" )
Well, `source` gives you the address and length of the parsing area. And `&gt;in` is the address of an index. That index is the current position of the text interpreter. You *are* allowed to change this index, to e.g. skip over text, or go back and re-parse things.
As /u/larsbrinkhoff said, you can use SOURCE and &gt;IN to explore the input buffers. But you can't alter this input, only the &gt;IN value. See 3.3.3.5 and RFI 0006
One day I hope to write Forth code like someone who does know Forth.
I was looking for an example I saw a few years ago of PDF generation written in the factor programming language. I like this particular example because I remember so much nesting that I actually had to get out a pen and paper to figure out what block went with what. I can't find this particular example and it seems that Factor has pretty much dropped off the web but just look at how quotations are used in factor code and I think you'll see what I'm talking about. (We're talking very large words with lots of nesting and dozens of different stack combinators -- which you need to learn and fully understand in order to read any of this code. Our Forth has this, that, them, swap, drop, push and pull. Once you've internalised these operations then you can read and reason about any code written in the language. They might not be as super pretty but I'd rather put a few simple operations in the right place than burying everything under layers of single purpose stack competitors because it makes the code prettier.) You could never get away with this in Forth. Not only would it be considered very bad style but it would be even more unreadable. That is an important feature because it forces you to factor (no pun intended), factor, factor and make your solution as simple as possible. quotations and stack combinator are great but they remove too much of this pain and the result is much more complexity. Using quotations and stack combinators you can take whatever ugly code you have and make it look nice to feel good to work with -- they allow you to push the complexity under the rug where you don't have to deal with it. And there it sits until some poor soul wants or has to understand or debug it. In my experience, this doesn't really happen in Forth code because of a hard limit on the complexity of your solution. If you don't deal with that complexity then you're not going to get very far. This is a mixed blessing; Forth is not a panacea -- nothing is. After years of dealing with unnecessary complexity, I've come to love how Forth *forces* you to remove complexity. Languages like factor allow you to get a lot done quickly but they do this my making it too easy to pile on complexity.
The source word makes sense to me but the &gt;in word seems a little harder for me to understand (sorry I am not a low leveler I am pretty dumb compared to you guys). Do you think you could explain the &gt;in word to me a little more, or perhaps point me to some literature about how the parsing area works and why not to manipulate it?
Would those page numbers be for the ans spec? I'll check it out and see if I can understand the text at that location.
Those are the section numbers. * https://www.forthworks.com/forth/standards/DPANS/dpans3.htm#3.3.3.5 * https://www.forthworks.com/forth/standards/DPANS/a0006.htm 
`&gt;in` returns the address of a number. This number is a index, or offset into the parsing area. It's just like a variable defined by `VARIABLE`. I.e. it might be defined as `VARIABLE &gt;IN`. You get the index with `&gt;IN @`, and set it with `&gt;IN !`.
So by index do you mean like the length of an array?
I'm not a Forth expert, not even moderately competent. But I read Starting Forth in one sitting, and it made so much sense that I wrote a Forth virtually the next day. All the things you listed are easy to understand if you understand how Forth works internally, but I see how it can be hard to understand if you only know high level languages.
No, by index I mean a number from 0 to the length of the array. An index indicates one particular element in an array. (Or possibly the end of the array.)
I never learned lisp macros unfortunately. So that clears up , for me but I have more questions about the create word, specifically that it only works at runtime. What is confusing to me is that in lisp I can take just about any piece of lisp that runs at runtime and put it inside of a lambda definition, whereas there are some words which only really work at runtime and can not be used inside of a :noname definition. What I would really like to take words that are supposed to work a certain way and trick them to work another way. So trick words to use their compile time definitions at runtime and especially to trick them to use their runtime definitions at compile time. That's because not only do I like to take words that already exist and create duplicate words with the same name using the old definition, but I like to take these words with special definitions and use them within my :noname definitions. Could you perhaps explain what the word does&gt; does? And why is it that postpone doesn't work on all words?
* `POSTPONE` takes one word (parsed from the input stream), and compiles it into another word. It does this differently depending on whether the word is a regular word or an immediate word, but we can skip those details. Example: : foo ." FOO " ; : bar postpone foo ; immediate : baz bar ; What did we do here? `foo` is a regular word which prints FOO. `bar` is an immediate word which compiles `foo` into another definition. `baz` calls `bar`, which makes `baz` call `foo`. So the end result is: baz FOO 
* Now `DOES&gt;`. It specifies the run-time action of the latest defined word. (And in Standard Forth, that word must be created with `CREATE`.) Since you know Lisp, I'll explain CREATE DOES&gt; like it's a named closure. This Lisp snippet: (defun foo (n) (lambda () (incf n))) translates to: : incf (a -- n ) 1 over +! @ ; : foo ( n -- ) create , does&gt; ( a -- n ) incf ; Except that Lisp returns an anonymous closure, and Forth defines a named word. Look closely at `foo`. It has two parts: the compile-time part, and the run-time part. The compile-time part uses `CREATE` to make a new word. And then `,` to store a number in its parameter field. The run-time part is defined with `DOES&gt;`. `DOES&gt;` itself performs some magic, but the end result it that is modifies the word created by `CREATE`, and makes it do something different than the default action. The new action comes after `DOES&gt;`. This action gets the address of the parameter field as its input. So let's try it out: 0 foo bar \ We run foo, which through CREATE makes bar. bar . 1 bar . 2 When `bar` runs, it fetches the contents of its parameter field and increments it by one, and returns the result.
That should be the opposite, right? : baz foo bar ; BAR ok baz FOO ok Since `bar` is the immediate word, not `foo`.
Space characters are normally no problems.
The token delimiter may not always be a space. Words like s" will parse for other characters (like "). So a parse buffer filled with: 1 2 + s" hello world" Technically has five tokens: 1 2 + s" hello world The basic text interpreter doesn't necessarily know what any particular word is parsing for. It just uses &gt;IN as an index into SOURCE to find the start of the next token. Assuming use at the interpret time, in this example, the interpreter would extract the first token, stopping at space. 1 Determine that this is a number and push it to the stack. Then it does the same for the next token (2). On the third token, +, it finds it in the dictionary and call the word. When it gets to the fourth, s", it calls the word which then parses ahead for a ". It moves the parsed string into a buffer of some sort and returns the address and length. s" will have advanced &gt;IN to the end of the parsed string, so future tokens will begin after the closing ". 
I do not quite understand how postpone operates on a regular word. How about this example? : blah2 2 ; : blah postpone blah 2 ; blah As for the s" issue is there a way to explicitly tell postpone or some other word whether or not you want to postpone the runtime or the compile time definition? Perhaps I can get a pointer to either/or definition and then postpone that pointer?
No, `,` does not make a word official. `CREATE` already does that. I explained what `,` does in another reply. Your're right that the runtime action is not variable. The words created by `foo` always do the same thing. What is variable is the data, the input to the runtime action.
What I don't understand is why you would want to put whatever is in the stack in the dictionary right after you use the word create.
I believe, in my humble or infuriating opinion, that you have trouble understanding because you do not yet understand the very basics of Forth. It would probably be better if you had a very specific application in mind, and then we could help you implement that.
I'm also compiling such a list, but it's on an offline computer. I have often seen `'EXAMPLE` used as "address of example". Your list has the tick at the end. I have seen two variants for "operates on string": `$EXAMPLE` and `"EXAMPLE`. I have adopted the latter, because the $ reminds me too much of BASIC and Perl. `?EXAMPLE` means "optionally do EXAMPLE". Whilst `EXAMPLE?` usually indicates a boolean return. `#EXAMPLE` - number of examples. `EXAMPLE#` - example number. The difference would be that the first refers to cardinality, and the second a single number. `/EXAMPLE` - size of example ("(bytes) per example"). `+EXAMPLE` - add to example, increase example. Adding to a list, for example. `-EXAMPLE` could mean the opposite, or it could mean "disable example", "negate example", "not example". I have seen `0EXAMPLE` as meaning "zero/clear/initialize examle". This is supposed to be an action. On the other hand `EXAMPLE0` could mean "initial value of example", as in a value. The standard have some examples of `EXAMPLE@`, `EXAMPLE!` and `EXAMPLE,` operating on values of type example. There is less agreement on what `@EXAMPLE` and `!EXAMPLE` should mean. But I see them used.
 R&gt;, R&lt; This should be: R&gt;, &gt;R 
`&gt;EXAMPLE` could mean "convert to example" (as in &gt;NUMBER or D&gt;S), or "push onto EXAMPLE" (as in &gt;R). Or many other things. `&gt;IN` is kind of an anomaly, I think.
Sometimes you must do a conditional jump in Forth. There use to be a non standard word like ?BRANCH for that. But after the code for ?BRANCH there should be an address loaded. When programming you don't know that address. In Forth all jumps are done by structure words IF ELSE THEN BEGIN WHILE REPEAT etc. These words are marked IMMEDIATE and while the source code is loaded as executable code in the memory, these words do all the counting and load all addresses. I guess very few forthers think that this kind of programming i trivial, but if you know what you are doing it is possible to create such words. 
One thing I would say is that in ans forth if you hit the tab key enough times it will print out every single word in that forth, which is useful to get a glimpse at all of the funny characters.
It is sort of like "eliminate the unnecessary." I think it is a saying which goes long before even forth. It has to do with minimalism. Though to Moore it is very important in his forth programs.
Here is real example for a simple music player: \ note object creator : note: create hz , \ compile time: compile OSC divisor into the note does&gt; @ play ; \ run time: fetch the value and play the tone Now we can use it to define notes, that play when they run: 131 note: C3 147 note: D3 165 note: E3 etc.... So at compile time create makes a dictionary name, HZ converts a frequency parameter into a OSC divisor value and ',' puts that value into memory, typically right after the name. At runtime, the DOES&gt; part, returns the address of the data, then it uses @ to fetch the divisor value from memory onto the stack and finally PLAY picks it up and plays the note. In object oriented terms, CREATE makes a data structure, and DOES&gt; assigns 1 METHOD to each data structure. Does that help? 
I would also like to ask what is the difference between the code field and the parameter field of a word? If you don't mind. And again thank you for your time, I really appreciate it.
So your word has worked great up until now but I realize that it hangs when you try to do a comment across multiple lines like if you do /* newline newline newline */ Is there a way to fix it so that it will account for newline?
You are on your way now with this I can see from your reply. In this example Hz is a real function that takes a value in Herz at COMPILE TIME and computes the correct number that would adjust a real oscillator to run at that number of Herz. Simple. The comma operator in Forth is one of those innocent looking but quite clever little things. It "compiles" an integer into the next available memory cell in the dictionary and then increments the "dictionary pointer" variable by the number of bytes that your Forth system uses for an integer. (2 4 or 8) So the value computed by Hz is "compiled" into the dictionary by the comma. It's NOT punctuation. It's a Forth WORD that runs reals code! Something to watch out for is there is no "standard" Forth implementation. This was done by the ANS 94 committee to try and satisfy all the divergent developments that were occurring at the time on how you COULD build a Forth system. Threaded code was giving way to native code generators more like you see with conventional languages. So in an old fashioned Forth systems our "note:" word would make a HEADER with text label, a link and few other fields, and right after it the Code field and the parameters follow until the word is complete and the address of "EXIT" is compiled onto the end. See next entry....
For the second you want vocabularies. Something like: \ Create a new vocabulary and switch to it vocabulary my-dsl my-dsl definitions : + ." hello?" cr ; \ Back to the Forth vocabulary only forth \ Add my-dsl words: also my-dsl 
That works splendidly for my second question thank you.
, (comma) stores a value into data space. This may be part of a word's parameter field, or possibly somewhere else. E.g., in my Forth there's a big linear memory space for data. There's a variable named Heap that holds the next free address in this space. ("HERE" is the typical name for accessing this variable). When , is used, the top value on the stack is stored HERE and the value in Heap is increased by the size of one cell. You can see this in gforth by trying something like: here . 199 , here . 
Well that at least gives me a piece of usable code with which I can use , in a macro technically answering my first question but , still seems pointless to me in terms of macros.
That no longer nests.
Ah you're correct.
The code field has the code for the word (either the machine code itself, or a pointer to it). It defines the runtime behaviour of the word. The parameter field has the data for the word. However, for ordinary colon definitions this can be confusing. Because the code field refers to a special code snippet called `docolon`, or `enter`, or `nest`. (It has no standard name.) This piece of code is responsible for executing the threaded code of the colon definition. And for a colon definition, the parameter field holds the threaded code.
Your list says that `2EXAMPLE` does something twice. That's not how I see it. `2SWAP` doesn't swap twice, that would be a noop. :-) I think the prefix `2`, `3`, and maybe even `4` says how many cells to operate on. More than that would be unusual and/or unweildy. There's also `DEXAMPLE` which says to operate on a double. The `D` prefix differs from `2` in that a double is a numerical type, but 2 cells can be any data.
Maybe read chapters 8 and 9 in Starting Forth: https://www.forth.com/starting-forth/8-variables-constants-arrays/ https://www.forth.com/starting-forth/9-forth-execution/
The first version I posted works across newlines. You switched out `next-word` for `parse-name` which broke it.
Words marked to be IMMEDIATE do not appear in new definitions. They do something with the code block for the definition, eventually put som other execution code there.
I posted an implementation.
&gt; Forth *forces* you to remove complexity. Yes! Forth is a sharp knife, and you'll cut yourself it you don't handle it well. But I'm sometimes tempted to make the code maybe *too* simple. Taking shortcuts to avoid a few extra lines, or reduce stack juggling.
&gt; My third question is to ask if parsing the input stream and expanding it based on syntax rules is the standard way that people write DSL's in forth, or is there another way people do it? Not at all. Indeed many Forthwrites dislike these tricks, with parsing words. Parsing words are only useful for changing the word order and once you get used to reading and writing Forth you'll come to realise that the order of words in your program is the natural order for your program. Everything else goes against the grain and will naturally introduce more code and more complexity. To me Forth is about removing complexity. The Forth I use every day doesn't have parsing words and I don't miss them. Forth already has a perfect fine syntax, without us turning it into Javascript or SQL and whatnot. I create problem-oriented languages in Forth by using appropriate names. Nothing else is required in my experience.
Good point. I was thinking more about words like `2DROP`. I will update the post here shortly.
Whoops! You're right. Thanks! Will fix shortly.
&gt; I understand what , does. It takes something on the top of the stack and stores it here and then pushes here forwards by one cell. I just don't understand what it is for or why it would use it. Have you ever played around with Logo (or with turtle graphics in general)? `,` is the pen you use to draw in memory. The structures you paint with that pen are what the computer sees, processes (data) and executes (code). Words can be written which [help] draw all sorts of fantastical things but in the end it all comes back to `,`. 
Thanks!
Why use Forth when you don't want Forth? Perhaps something like PostScript would be a better model for your language?
Forth serves as a better base for the language I am trying to build. And in general forth is very good at building languages themselves so any time I built a language I might want to build it in forth. Furthermore sometimes I do like to program in the pure forth way. For this particular application I know my users will not want to do that.
It seems my forth doesn't have _ Do you know what the equivalent of _ is in ans forth or gforth?
 : _ 3 5 + ; : test _ _ + 7 - . ; `_` is just a short name that I use in place of :noname and passing execution tokens around on the stack unnecessarily (as it is in the majority of cases.)
Nooooo! :/
I seem not to be able to reach the abort condition. How can I tell when the input stream is empty? 
Depends on what you mean by "the input stream is empty". If by that you mean "everything in the buffer for the input stream has been read", that is source nip &gt;in @ = but that doesn't necessarily mean that there isn't more input *available*. For that, you'd use `refill`, which as the name implies will refill the input buffer from whatever the current source is and leave a flag indicating whether it succeeded. If you use this and it returns false, it seems reasonable to assume that there isn't any more input. If you mean "how can I tell when the input buffer contains a string of length 0", that should be pretty straightforward. `source` gives you information about the entire line and is independent of the value of `&gt;in`. If `source nip` is zero, then it's empty. It should be possible to reach the abort condition in lars' definition of `pop` by having the source be empty at the time that `pop` is run. Note that this can't happen as long as you run `pop` by itself from the interpreter, because the source will contain "pop". I think it's assumed that `pop` will be used inside some other definition that will ensure that when `pop` is used, there is stuff in the input buffer and `&gt;in` points to a valid character index in the input buffer. In some sort of loop.
&gt; Forth uses a *Stack* to pass parameters to, and get results from operations. Entering numbers will put these numbers on the Stack. Enter two numbers. Example shows entering three numbers... &gt; Numbers enter and leave the Stack in a *First in, First out* fashion. I'm not entirely convinced...
I haven't read it, but based on the TOC and sample chapter it looks like it is mostly for people who don't know Forth at all, and they prefer to learn it by stepping through a single example code.
&gt; I think :noname is supposed to be useful as a building block inside other words where you may want to compile some code but not be forced to give it a header or a name. I don't think I've ever seen it used like this. In my experience :noname ends up being used like lambda by people who think higher-order functions are a must have and can't be bothered to learn Forth. This usage leads to more execution tokens than anyone wants or needs flying around with the data on the stack, which makes it practically impossible to get by without no-no words like `pick`. Needless to say, I'm not a fan of :noname :-) I've used `(` ... `)` in programs where it's desirable to write complex queries interactively but I'm not overly taken with it. If the problem is that you don't want to give something a name then `_` and `_n` solve that problem very well; words can be immediate, and recursive, referred to multiple times in context and don't take up any space on the stack. It also effectively flattens definitions. Maybe it's a bit low-level but I see no reason to choose any other approach. In our Forth there's no cost for headers, so :noname wouldn't help there either. I don't know but you're probably right: design by committee... does it ever work out for anyone other than the committee? :-) 
`_` doesn't solve the problem if you want to create something which hides the use of a colon definition. Here's the one and only example I can find in my own code. Suppose you want to add an action `foo` to a deferred word `bar`, while still calling the old action. The obvious way would be : foo ... [ action-of bar ] literal execute ; ' foo is bar You may want to give this a simpler syntax, such as : foo ... ; ' foo +is bar One way to do this is to have `+is` make a :noname definition which calls the `foo` xt and then the old `bar` xt. And then `bar` is set to the :noname xt. Yes, this could probably be solved in other ways. But it's an example of why one might not want to have a named definition. Here's a message discussing the intruduction of `:NONAME`, initially called `START:` https://groups.google.com/forum/#!original/comp.lang.forth/SmsEZaT35h8/myKmuLOw7yMJ
&gt; (We're talking very large words with lots of nesting and &gt; dozens of different stack combinators -- which you need &gt; to learn and fully understand in order to read any of this &gt; code. This is the result of bad factoring in either a classical Forth or one with combinators and quotations. My conditionals and loops are combinator based: &lt;flag&gt; [ true ] [ false ] choose &lt;flag&gt; [ true ] if &lt;flag&gt; [ false ] -if &lt;count&gt; [ code ] times [ code &lt;flag&gt; ] while [ code &lt;flag&gt; ] until value [ ... ] case These correspond to: &lt;flag&gt; IF true ELSE false THEN &lt;flag&gt; IF true THEN &lt;flag&gt; NOT IF false THEN &lt;count&gt; FOR ... NEXT (or &lt;count&gt; 0 DO ... LOOP) BEGIN ... &lt;flag&gt; NOT IF EXIT THEN AGAIN BEGIN ... &lt;flag&gt; IF EXIT THEN AGAIN DUP value = IF DROP ... EXIT THEN And a limited amount of stack flow. E.g., #1 #2 [ n:inc ] dip #1 #2 [ n:inc ] sip Which would correspond to: 1 2 &gt;R 1+ R&gt; 1 2 DUP &gt;R 1+ R&gt; I have others (bi, tri, (and variants of these), and a lot of ones operating on strings and data structures) for specific use cases. A case might be something like: :eol? (c-f) [ ASCII:LF eq? ] [ ASCII:CR eq? ] bi or ; :eol2? (c-f) [ ASCII:CR eq? ] [ ASCII:LF eq? ] [ ASCII:SPACE eq? ] tri or or ; In a more traditional sense I would have done something like: : eol? ( char -- flag ) DUP ASCII:LF = SWAP ASCII:CR = OR ; : eol2? ( char -- flag ) DUP &gt;R ASCII:CR = R@ ASCII:LF = R&gt; ASCII:SPACE = OR OR ; Or implementing `words`: :words (-) [ d:name puts sp ] d:for-each ; Without combinators this would be (in my Forth): :words (-) &amp;Dictionary repeat fetch 0; dup d:name puts sp again ; I personally find the combinator approach to be a bit cleaner to read as it lets me think about the problem and less on the mechanics of shuffling stack items. It does require a different approach though. I have an advantage in that I've implemented everything in my Forth, so I have a deep understanding of the combinators and can use that when deciding when to use them.
That's a pretty funtastical use case. I can't say I've ever needed to append behavior to the front of an existing definition at runtime but far be it from me to tell you is or isn't useful in your solutions :-) What I'm unsure about of is the cumulative effect of repeatedly rebinding `bar`. It seems like you'd end up with a definition foo1 foo2 ... fooN bar with a horribly inefficient calling pattern. (foo1 (foo2 (... (fooN bar)))) Was that the intended understanding? In Able I would probably solve this like (translated to a kind of pseudo-Forth.) : foo ... bar ; ' foo ' bar bind Then assuming you want to defer the call to `bar` you'd write something like : ... defer-call bar ... ; The key difference here is that `bar` is a normal definition and it's up to the user of `bar` to decide when to resolve it. The resolution of any word can be deferred until runtime if that's desirable. Now that's not to say you couldn't implement defer yourself but I think there are some nice advantages to this approach.
I appreciate your considered response! :-) I'm not going to reply to it directly because I don't necessarily disagree with anything that you wrote here. &gt; This is the result of bad factoring in either a classical Forth or one with combinators and quotations. Perhaps you're right but looking at Factor code this is what you're going to see: many large definitions with all sorts of weird and wonderful stack combinators nested deeply inside them. This isn't to say that you can't write good code that uses quotations and stack combinators but I haven't really seen any :-). Our Forth has a handful of stack operations and even fewer control structures this that them swap drop push pull then ... else ... And a few more just for good measure[0] with ... loop from ... next What's interesting is all the ways in which these few words can be combined. Once you internalize these dozen or so words you'll never have to stop and ask yourself what is implied by the names nip, tuck, dip, sip, keep, bi or tri, or any of their seemingly endless variants. And please don't get me started on all of the domain specific combinators :-) Besides heavy nesting, this is by far my biggest issue with stack combinators. Is it really an improvement to replace a handful of very simple and generally useful operations with quotations and dozens of new and exotic combinators? &gt; I personally find the combinator approach to be a bit cleaner to read as it lets me think about the problem and less on the mechanics of shuffling stack items. I can certainly respect your preference and I can even believe that you can write good code with these things :-). After years of reading and writing code with only these simple words I don't really have to think about it anymore. I've used them so many times that I can recognize and implicitly understand all of the most common phrases swap under swap drop "nip" swap that "tuck" push f pull dip this push f pull sip (keep?) swap f swap g bi etc. But naming them took quite some effort ;-). I could give these patterns many names. Not unlike how the symbol `-` is called subtract and difference, distance etc. depending on the context. I like to keep thing simple. I don't usually use go deeper than 4 items in any definition and that's as far as these words will take you before you start to coil up from the pain. Rotating 3 items like this is enough of a hassle to make you stop and reconsider what you're doing. Then there are many other times where having these words exposed helps to reveal useful patterns. Consider [ u1 = ] [ u2 = ] [ u3 = ] tri or or Which I would write like this this u1 = that u2 = or swap u3 = or You would probably call this ugly, but let's look at what happens when we want to compare the TOS with 5 values :-). this u1 = that u2 = or that u3 = or that u4 = or swap u5 = or There's a the pattern forming there. I'm honestly not sure how I'd do that with a combination of tri's and bi's so perhaps you could enlighten me as to what this pattern looks like with them? There's are other important considerations; the stack combinators version uses O(n) space on the stack while the version using a repeating pattern of simple words uses O(1) space on the stack. How would you address this problem using your stack combinators and at what point do you have to fall back an the fundamental stack operations? In cases where you don't have a stack combinators that just does what you want do you write one? What do you call it?[1] At what point do you stop? Pease don't take this the wrong way :-) I've wrestled with these questions, and perhaps due to the fact that I really hate naming things I've forced myself to make do with the simple and minimally powerful set of words that time and repetition has revealed to me. This is a mixed blessing because now I can't see any reason to add more ;-) [0] These are strictly optional if you have tail-call optimization and recursion but as infinite and counted loops are so common that it doesn't hurt to have them. [1] I'm assuming magical stack combitor 55 -- msc55 for short -- isn't taken? :-)
Sure, it's debatable whether this is useful, or how to do it in other ways. I was trying to come up with a use for :noname rather than _. I had a hard time doing that, and this was the only actual example I found. As for understanding. I have a deferred word initialized with a default behaviour. I may not want to replace that behaviour, just have a chance to do something first and then proceed with the default. It's similar to a hook or function advice in Emacs Lisp. Or vaguely resembling:before/:after/:around methods in CLOS. It's even more funny that I actually don't use +is. But if I did, I'm glad I didn't have to supply a _! ;-)
That's 4 problems, dlyund.
Try recursion.
We replaced Forth's traditional loops with tail recursion, and because they're so common added an infinite loop, written `with ... loop`, and counted (count down to zero) loop, written `from ... next` EDIT: some more details in case this isn't as obvious as I'd hoped: This is only one use of the word `with`, which marks the beginning of a block, and `from` is just an abbreviation for `push with`. `loop` is just jump and `next` is decrement and jump while not zero.
I think something like this would be appealing. Do you also have address registers like in MachineForth?
for..next !p&gt;r r&gt;p p@ pc@ p! pc! p+ p2+ p++ It is a simple for..next loop combined with a reentrant pointer register. I have never missed do loop in flashforth. Altough do loop can be compiled in afterwards for people who need it for copy/pasting forth into flashforth. Most of my loops don't need anything but a count down to zero. for..next is perfect for that. The TOS and P register can handle most stuff.
I also implemented rsa (and zkp) using your bignum library some time ago: https://hub.darcs.net/pointfree/forth-crypt
the author has lots of interesting Forth related posts, hope he doesn't mind me posting this to reddit
y, prob preaching to the choir here, reposted [/r/programming](https://www.reddit.com/r/programming/comments/6c355p/swiftforth_review/)
:) should have posted with original title except left the Forth part off ("The Unreasonable Awesomeness of Swift....."), think most people really (never) do anything so complex that the limitations of there ideas come to the fore without blaming some other factor or ruling the problem impossible, Forth thinking made the impossible possible for me (c++ program)
&gt; the long lost Lisp Genera [Found it!](https://github.com/ynniv/opengenera)
They'd only have complained that Swift is a [different language entirely](https://en.wikipedia.org/wiki/Swift_(programming_language%29).
&gt;[**Hunters &amp; Collectors - True Tears Of Joy (Under One Roof) [4:28]**](http://youtu.be/WcroHVCdwMU) &gt;&gt;Music video by Hunters &amp; Collectors performing True Tears Of Joy (Under One Roof). &gt; [*^HuntersCollectorVEVO*](https://www.youtube.com/channel/UC0nTrc86TA4lAufp5it21aQ) ^in ^Music &gt;*^4,616 ^views ^since ^May ^2013* [^bot ^info](/r/youtubefactsbot/wiki/index)
I think it makes as much sense as using colors does. More, actually; since plenty of people are color-blind.
it has the advantage of emphasizing that IF ELSE and THEN are compiling words, which a Forth newbie might not know. what are the lower case versions of those words used for?
I'd prefer this over using colours, I think. I never liked syntax highlighting. I usually turn it off at first chance.
Yeah, I was thinking about this (also numbers are a problem), so probably some other mechanism is needed besides this. Let's use roman numerals for executing at compile time and arabic numbers for compiling literals! Just kidding. :)
&gt; colorForth does, in fact, have immediate words; words compiled into the MACRO vocabulary are executed where an ordinary word would be compiled. Hm, interesting. I thought this isn't needed if you can control the compiling with the colors. &gt; Why add another, much less flexible, over the top of it? This looks more flexible to me than the regular immediate words. If you can mark the word at the place where you're using it, instead of where you're declaring it then you won't need to decide upfront which words are immediate and which aren't. If you wan't to compile an immediate word in a traditional Forth, you have to use postpone. I find this confusing. Here you just write if instead of IF. I don't want to convince anyone this is a good idea, because I'm not 100% convinced either. I'm just trying to think it through, how this would play out.
&gt; colorForth doesn't have immediate words in a traditional sense, Words don't have an immediate bit, no, but it has something like `postpone`. colorforth for x86 has `cyan` words, which allow you to compile calls to words that are defined in a macro wordlist. Words in the macro wordlist are executed by the compiler. &gt; the programmer decides when to compile a word into the definition and when to execute it at compile time by using different colors. I think you may be thinking of Able :-). Ignoring the controversy surrounding the use of color this works really well. I see only one reason that you couldn't use case and it's that the there is no maximal case for numbers and symbols, and therefore no way to distinguish their use at compile time and run time. You could use a prefix but this quickly becomes unwieldy because almost every word will be preceded by one. The code ends up looking like APL, without its elegance. We've played with a number of alternatives and we always went back to color. This isn't to say that you have to use color, and weight has often been used when publishing colorforth code in print, but this causes other problems e.g. how do you handle the ambiguity between names that look like data and data. The common approach is to just ignore it, but that leads to mistakes while reading programs etc. It's better to be explicit. On things that I think Moore got wrong with colorforth was to give transitions between colors semantic meaning, which leads to situations where you have to insert invisible colored spaces and code which you can only read by moving your cursor around. It's better to be explicit. Now let me play devil's advocate in the defense of "color" for a moment ;-) There are a number of reasons that we ended up choosing "rich" text for our language but the important two for this discussion are efficiency and simplicity. While it's not the case that this "rich" text solves every problem you might have it significantly speeds up compilation to the point that we can do a complete rebuild of the whole system in between keystrokes and you won't even notice, and it allows us to simplify the language, compiler, and system as a "whole". Having used Smalltalk professionally for several years I know the advantages of working with the system from within; having and being able to make tools which take advantage of the wealth of information that this makes available was always a priority. I don't mind at all that I can't use my usual emacs setup[2] It's all about the power to weight ratio. If it were just a small improvement here or there then we'd have probably just implemented something like cmforth, which seems to exist in the transition between classical and modern Forth[0]. I have a lot of love for cmforth and I tried to squeeze as much insight from it as possible but in the end, I think it highlights the complexity that comes from weaving your parser throughout your system. The classical Forth is about parsing text. The language is given form by the parsing words it supports. This is beautiful, but it leaves a lot to be desired[3] colorforth removed most of that flexibility, defining a hand-full of words magical colors and transitions for you to work with. The colors were set in stone unless you had the nerve to start ripping the tightly integrated compiler-editor apart. I like to think Able takes the middle road; you can define new colors as easily as you can define parsing and immediate. More than that, you can easily add new ways or modify how words are shown. My hope is that those who really don't like our use of color will be able to propose an alternative but in the worst case maybe choosing colors and contrasts that work for them will be enough[1] :-) If you don't like it then you can change it. That's one of the great things about open systems like Forth. [0] modern in the sense that it's new; whether modern is better is up to you :-) [1] We didn't banish the use of color from the world entirely because some people are color blind. [2] In my opinion this is the thing that most people bark at when it comes to things like Smalltalk and colorforth and I'm the first to admit that our editor needs a lot of work before it is even remotely comparable to Vim or Emacs (if that's a goal) but there was a point when Vim and Emacs weren't comparable to Vim and Emacs :-). [3] People have done crazy things like going so far as to implement BNF in Forth so they can write parsing words for horrifically complex languages. Other's have proposed things like recognizers as an alternative to parsing words, but the end result is yet more complexity. Parsing words seem to be a local minimum. With colorforth Moore showed us a much lower local minimum; a way to build a [Forth] system which is even simpler overall. Able is an evolution of this idea, which I hope nudges the ball enough that someone else will be able to roll it down the hill a little further.
dlyund, this is going to sound unduly harsh and for that I'm sorry, but you really do need to stop using a system nobody else can even see yet as a starting point for generalisation. I'm sure Able is a fantastic system for you, but as closed systems go it's the North Korea of programming environments right now. Moving on: &gt; Is it really another, less flexible syntax? Yes, at least the way colorForth does it, which is what was under discussion here. colorForth even has syntactical rules attached (the yellow/green transition). &gt; To me [color is] a way of enriching source code by attaching useful information to it Augmenting source code is great; but if the augmentations are necessary to get the source code to behave properly, then they're not just augmentations - they're part of the source code itself. &gt; What I hope I've illustrated is how much complexity we have to introduce in order to achive what the colors achive But, referring back to the yellow/green transition, the colours still need to keep track of history (which is what state is) and modify behaviour accordingly. And they do it much less well than traditional Forth, being necessarily one-dimensional. Similarly, colorForth doesn't change the execute-if-immediate-otherwise-compile action of the compiler; it just explicitly invokes it - as long as you didn't type a yellow word first, because if you did whatever's on the top of the stack gets compiled as a constant... and bad luck if it was the IF address. colorForth doesn't actually reduce the complexity of Forth at all. It just redistributes it, makes part of it unalterable, and then asserts that this is a reduction. &gt; you are free to disagree Well, that's generous of you(!). &gt; or [assert?] that parsing words are much more flexible than these colors Well, they are, but that's kind of beside the point. &gt; The only thing that is stopping you from adding parsing words (which consume this rich source code) is the inherent complexity they introduce. But the Forth interpreter is *built* on parsing words. They introduce no additional complexity whatsoever, because their complexity is already paid for. And if we tokenise source - which is also orthogonal to the question of syntax augmentation - then parsing becomes quite trivial: `@+`. But removing the ability to define parsing words essentially means that the Forth interpreter - historically as wide open as an interpreter can be - is reduced to a little black box, closed and unalterable. &gt; What we do have is a way of attaching presentation information in the code When Knuth did that, he called it "literate programming". But the source still worked perfectly well as source when shorn of its literature. colorForth's doesn't. Does Able's?
&gt; If you can mark the word at the place where you're using it, instead of where you're declaring it then you won't need to decide upfront which words are immediate and which aren't. Except you still will, because immediate words behave differently. You'll just end up with buckets of confusion for all concerned - confusion that the original Forth (and even colorForth) tried to address, specifically by adding immediate words in the first place. &gt; If you want to compile an immediate word in a traditional Forth, you have to use postpone. In fact, POSTPONE does what you'd expect whether the word it's handed is immediate or not - it causes its compile-time action, rather than its run-time action, to be compiled. It's one of the better-thought-out aspects of ANS Forth. 
&gt; This is going to sound unduly harsh and for that I'm sorry, but you really do need to stop using a system nobody else can even see yet as a starting point for generalisation. No offense taken. You make a fair point :-) &gt; as closed systems go it's the North Korea of programming environments right now. That's true but I'm slowly disseminating information through these discussions, ahead of its eventual but imminent release. I don't think anything that I've said is out off topic since Able does exactly what /u/attmag and others thought colorforth did. As I detailed recently in this thread https://www.reddit.com/r/Forth/comments/695oik/advances_in_forth_language_design/ Which may have caused some confusion for those who didn't realize I wasn't talking about colorforth ;-). &gt; colorForth even has syntactical rules attached (the yellow/green transition). I completely agree but I don't see that this has anything to do with the colors or the use of rich source code in general, and you said, you remain unconvinced about the use of color -- presumably not just colorforth or you'd have said that. The complicated and often problematic semantics of colorforths color transition are not a requirement and many colorforth derivatives abandoned this idea. Do those languages replace Forth's perfectly good syntax with another much less flexible one? I think I've made a good case that there's nothing about the use of colors (typographic hints, if you prefer) that prevents you from doing parsing words. It's not done (at least in Able) because it's not necessary and I place very little value on being able to e.g. embed SQL. &gt; Well, that's generous of you(!). I get the impression that I might have offended you? That wasn't my intention, and I'm sorry if I have. All I meant was that you're entitled to your opinion. &gt; colorForth doesn't actually reduce the complexity of Forth at all. colorforth may only move the complexity around but having seen parsing words abused for all sorts of things, taught Forth many times, and talked with more people than I can count about Forth I concluded (personal opinion) that parsing words make[0] things more difficult to understand. How do you read/scan of code when you can suddenly end up in a context where word order, syntax, and semantics, are nothing like the rest of Forth? By virtue of it's lacking this flexibility colorforth code is arguably much simpler. &gt; They introduce no additional complexity whatsoever, because their complexity is already paid for. I completely agree. Parsing words introduce little additional complexity in the implemenatation, but if you want to remove the complexity that is alread paid then parsing words certainly aren't free. &gt; if we tokenise source - which is also orthogonal to the question of syntax augmentation - then parsing becomes quite trivial Very true but if this is as far as you take it, I think you've missed the opertunity to simplify the language. We don't need to agree as to whether this is worthwhile and it's fair to say that traditional Forth is already pretty simple. &gt; emoving the ability to define parsing words essentially means that the Forth interpreter - historically as wide open as an interpreter can be - is reduced to a little black box, closed and unalterable. I don't understand this line of thinking. In Able (sorry) you can define new "colors" as easily as any other word. You can even bind words which weren't written to be "colors" to the source code. Even ignoring that, it's an open system with full source code. What exactly is closed and unalterable? &gt; When Knuth did that, he called it "literate programming". But the source still worked perfectly well as source when shorn of its literature. colorForth's doesn't. Does Able's? If you mean can source code be compiled and executed if the presentation information is removed, then yes. [0] Or can make... but if you're not going to use them you surely can't argue that they make things more flexible :-). 
What is an actor here? I'm guessing you're not talking about "actors" in the sense of concurrency? 
Does it have anything to do with this? https://en.wikipedia.org/wiki/Actor_model
Not directly, no. You're reading too much into my choice of the word 'actor' for the concept in 8th. All it is, is a convenient way to have a kind of (word,data) pair.
It's an easy mistake, because the word "actor" comes with a baggage. No complaints from me though, many words are overloaded.
True enough. Like "word" :)
I believe this is approximately how the Forth Inc systems used to work (or maybe still do). They call it the "compiler loop", because the compiler is a loop which exits when the definition ends. It's not exactly stateless. True, there is no `STATE` variable, but the state is implict in where the system is executing: inside the compiler loop or not.
I don't think there's much Lisp Machine software that's really missing. Just hiding well. The one thing is the CONS software.
I found some info about polyForth in [this](http://www.forth.org/fd/FD-V17N1.pdf). It looks like it used a similar technique.
Yes, polyFORTH should be the one. Forth Inc also had chipFORTH and microFORTH. Not sure about those.
Here is how I implemented it in punyforth: : if immediate ['] branch0 , prepare-forward-ref ; : then immediate resolve-forward-ref ; The prepare-forward-ref word just compiles a placeholder 0 into the next cell, and leaves the address of that cell on the stack. : prepare-forward-ref ( -- a) here 0 , ; The resolve-forward-ref calculates the relative address by substracting the current here from the address on the stack and ovewrites the placeholder zero with that address. : resolve-forward-ref ( a -- ) here over - swap ! ; edit: fixed "cell -"
&gt; tldr: it is Forth, you already have a stack at hand so use it. Ah, silly me. Good point -- as long as it's valid code, everything will balance out. On the assembly point, I'm working my way up to native code generation :-). Someone I had read said that Jonesforth does a few things oddly, and not to use it as an example. That implementation makes perfect sense, though. Thanks!
so when in nested if case, how you ensure which then you will jump at the first if?
each IF puts the address of its forward reference on the data stack at compile time. each THEN resolves the topmost forward reference.
there must be some difference between forward reference and ordinary data right? other ways how can THEN get to know the current item on the top of stack is a reference?
They shouldn't; and if one were to use an immediate word with a stack-effect on the data-stack, during an IF..THEN which used the data-stack for keeping track of code... well, that wouldn't work well.
As an aside, in 8th we use the "r-stack" to keep track of the data for control structures... but the data is tagged so we can tell if (for example) one typed "then" without a matching "if".
IF and THEN works at compile time, so there's no conflict with the contents of the runtime stack.
Some dialects of Forth check control structures for balance and match, most notably figForth. It costs nothing at run time and can save a few headscratchy moments, but at the expense of flexibility - specifically, you can't pull cmForth-like tricks like this one: : MAX ( n n - n) OVER OVER - 0&lt; IF BEGIN SWAP DROP ; : MIN ( n n - n) OVER OVER - 0&lt; UNTIL THEN DROP ; without adding an extra word or two of code to let you shuffle the control words around on the stack - eg. : MAX ( n n - n) OVER OVER - 0&lt; IF BEGIN SWAP DROP [ 2SWAP ] ; : MIN ( n n - n) OVER OVER - 0&lt; [ 2SWAP ] UNTIL THEN DROP ; Given the relative frequency of forgetting to properly nest constructs vs intentionally subverting them, though, along with the ease of doing the latter with such protection in place - one might consider it a shame that such checking has fallen into disuse.
so if i have a user-defined compiling words that has some stack effects , there might be problem? this design is too broken 
by tagged, you mean that data has some extral flags or fields indicate its tag?
Does that mean that your cell size is bigger than your pointer size, or are you tagging with doubles or something?
What kind of stack affecting word are you thinking about? It has to store in the middle of a control structure, and later read it outside the control structure. This doesn't seem like a common situation? The only thing that's gotten a little irritating for me is a result of how I implemented quotations. Say I want to do a quotation like this (note that I'm not using `[` and `]` to change state, and they are instead used to define and terminate a quotation): : main begin key? [ ." Done" exit ] when loop ; In this case, since I implemented `exit` and `]` as `ret` instructions (basically), I can't `exit` from `main` because the `exit` just returns early from the quotation. To get around this, I made a (probably stupid hackish solution) separate global location used in compile mode that stores the return stack depth when running `:` so that `exit` knows how much to pop off the stack to return properly. 
Actually, it works quite well in pratice.
Yes, the data is actually pointers to the real data. Items have tag fields so the type can be checked.
ok thx very much , you just solved my problem that bored me for years :D
Is there a GC in 8th because of this?
8th uses reference-counting. When an item's reference goes to zero, it's deallocated (resources released, and the item is put on the 'free list' of those items).
over and tuck are very commonly used, for example when you need to save a value before you consume it. pick is useful if you have a stack deeper than 3 items. In particular, if you get called from a system (C) routine with a lot of parameters. In plain Forth code, you should normally avoid having long parameter lists. roll is very rarely used. I have a sample which demonstrates it (in 8th) and in fact I've implemented rot and -rot using roll; but otherwise I don't use it.
`TUCK` examples: \ Add n to the cell at address a. : +! ( n a -- ) tuck @ + swap ! ; \ Store x in the cell at address a, return a' pointing to the next cell. : !+ ( x a -- a' ) tuck ! cell+ ;
Funny enough, I've just recently read the article in "lambda the ultimate" comparing the quadratic formula in Lisp &amp; Forth.
Exactly what i was looking for, thanks!
Interesting, thanks! A little rarer than I thought.
Nested definitions are only accessible within their enclosing definition. When starting a definition, push the pointer to the head. It is otherwise the same, just updating LATEST as normal, until you close a definition, then pop the header pointer to LATEST EDIT, example below : thisword : nextword ; ; In this definition, nextword is only accessible during compilation of thisword.
 : next-fib tuck + ; 1 1 next-fib dup . next-fib dup . next-fib dup . next-fib dup . Relatively useless but somewhat interesting use of tuck to dramatically simplify iterative fibonacci. I try to follow the rule of thumb that if you're using more than one stack manipulation in a row where at least one of them is rot, tuck, -rot, or nip, you're probably doing it wrong. Sometimes it really is necessary, though.
&gt; Whatever it is you're doing with Forth must translate very straight-forwardly to a stack. I don't necessarily believe that FORTH (by itself, at least) is always the appropriate solution. A lot of the time it might be; but it's educational to understand just how similar a stripped UNIX shell environment actually is to FORTH. At a core level, UNIX just accesses certain services of the hardware which are sufficiently complex that they would be tricky to implement in FORTH in some cases, that's all. If all I need is what I can usually get straight from assembly anyway, then I'd be silly not to use plain FORTH, because it's basically scripted asm with nicer control structures. If I want to play with my hard drive and use things like multitasking, however, (and sometimes I do; if only because implementing networking is a headache without multi-user support) then a bare FreeBSD with a Korn shell is nice; and from there I can get several different FORTH implementations if I want them, anyway. Being able to go in and out of GNU FORTH from a UNIX shell is insanely powerful; it really needs to be seen to be believed. The system word essentially gives me access to UNIX's services directly from FORTH programs.
In 8th there's something a bit similar, in that an anonymous word defined with "(" and ")" can be (and very often is) inside a colon-def word. Those anonymous words are typically used as callbacks for iterators, e.g.: : munge-array \ a -- a ( s:uc ) a:map ; Which would take an array of strings and create a new array with uppercased versions of the original strings.
Recently I was thinking about the implementation of locals in the following way: : max ( n n -- n ) -&gt; a -&gt; b a b &gt; if a else b then ; [Kitten](http://kittenlang.org) uses a similar syntax than this. My first though was to add nested definitions first, then implementing the *-&gt;* won't be difficult. It's just a local constant that is accessible from the current definition only. Basically *-&gt;* pops off one item from the stack and creates a local word (constant) that returns that item. Probably not the most cost effective way to have locals but I like this syntax.
&gt; I implemented Nested Definitions in my Forth, which actually was just a small modification of the compilation flag from False (== 0) &amp; True (&lt;&gt; 0) to False (== 0) &amp; Nesting Level (&gt; 0). How does the *[* and *]* work after this modification? The *[* switches back to interpretation mode in the middle of compiling, and *]* switches back to compilation mode.
I feel you bro. I basically use Forth as a jumping-off point for a lot of stuff. It feels "impure" to do it in Forth, but I have on occasion used a stack-frame structure for variables.
Would you believe that I just push the compilation state to the return stack with [ and then pop it back with ] ? I figured that if it went anywhere else it would either interfere with whatever branching information was on the parameter stack, and if it was saved at a static memory location (like a word-defined variable, say STATESAVE) then it wouldn't nest.
You could do that with this if you wanted to. But I think you mean Lambdas, right? You could do those too, but that implies garbage collection of some kind, or careful memory management.
http://lambda-the-ultimate.org/node/900 turns out it was Joy and not Forth, there might be some similarities as both are concatenative I believe 
I guess I fail to see why one would even bother with anonymous words unless they were literally objects that could become garbage (passed as an argument or return value, and subsequently thrown away), otherwise the property (or lack of) having a name to reference becomes a meaningless distinction. Although in Forth, passing functions as arguments or returns are a questionable necessity, when compilation is easy to do anywhere.
I suppose by "lambda" most people would mean "anonymous closures". Closures can also be named, for example by using nested functions.
Whatever the case may be, nested definitions could address both use-cases. I really do feel I should implement some garbage collection soon, as returning lambda functions from a word may soon be too convenient to ignore. Also, looking back at your previous comment, I'm inclined to argue that "making this up as I go along" is exactly within traditional Forth territory, whatever it may be that I'm doing.
[Quotations](http://elasticdog.com/2008/12/beginning-factor-shufflers-and-combinators/) are basically anonymous words, they can be used together with combinators. I also use them to make exception handling easier. { \ this is an anonymous word that can fail } catch
Yes, it is very useful with local variables. And generally, any form of local variables in Forth are frowned upon by the creator, Charles Moore, and so some Forthers consider them to be uncouth.
How does Moore manage to avoid them?
By using global variables instead. I'm not saying Moore is right or wrong, but I will state that there are hard ways and easy ways to shoot yourself in the foot.
Exactly.
&gt; Why bother with exception handling when you can return multiple values? I don't think the two approaches are comparable. There are many programming languages where you can return multiple values (either directly, like in Python or Groovy, or via out parameters like in C#) and they still support exceptions as well. &gt; Try-Catch would, for me, simply be an extra structure on top of an already simple system. Try and Catch are about 5-6 LOC words each. It doesn't make the system more complicated because they're not built into the language. If you don't want to use exceptions just delete or not "import" those two words.
Well, ask yourself this question. With object oriented languages, how much of a person's time is spent writing inheritance crap, as opposed to actual code? With shell script, you don't even need an include statement. I am predicting, or at least hoping, that once the OOP insanity dies, people will go back to how we did it in the beginning.
I'd like to find a good stream of Forth-related content anywhere, whether it's a journal or not. 
I believe most Forths distinguish between compile-time and interpret-time. "Run-time" starts as interpreting, but usually code is compiled and then run. Almost always, anyway.
They're very convenient. 8th uses them more extensively than most other Forths, I think, since they are used in iterators and in gui definitions of event handlers for example. It saves you from having to come up with mostly meaningless names for bits of code.
The German Forth Dimensions quarterly is still going and accepts articles in English: https://www.forth-ev.de/filemgmt/viewcat.php?cid=2 James Bowman and myself discussed bringing back the English Forth Dimensions magazine by translating the German language articles to English and exchanging between Forth Gesellshaft and SV-FIG so we can both benefit. I'd also like to get the magazine on [magcloud](http://www.magcloud.com/) so that nice, glossy, full color, and bound copies are available to those who want them. There were a number of reasons the American Forth Dimensions magazine ended, one reason was the www. Perhaps another reason was the declining use of Forth in North America. FIG once had two house numbers of offices. At some point FIG(?) wanted to discourage "hobbyist use" in favor of "professional use" because they believed the "hacker reputation" hindered professional Forth adoption. Hobbyists don't make huge volume orders, but I think they are the initial conditions that can persuade the industry to go in a particular direction. IMO, there are still some benefits to a special interest magazine/journal even with the www, namely the editing process pushes up the quality, the nice graphic design, the pacing of content, etc.
That'd definitely be interesting. I'd fall into the beginner hobbyist category, but can usually make my way through most scientific whitepapers. Although I must admit without any significant experience in Forth most of the articles would be a challenge. I'd like to get passed that bump in the next year or so. It's odd that I've always thought Forth to be a tiny community, but the activity on this board seems to suggest (along with some other factors), that it perhaps is more lively than I thought which is a good thing.
There is good content, but it seems to be all over the place. If you google for ACM Forth papers you'll usually find a bunch stuck behind a paywall. Fortunately, a lot of professors also post on their university's website, so once you know a paper's name and google it, you can usually find a more accessible PDF that should be legal. Note that I'm not talking of when someone illegally posts content.
&gt; What I mean to say is that, theoretically, Forth could be done in such a way that it could be purely run-time, and actually "compiling" a new definition would be a necessary annoyance rather than a feature. Assuming I understood, that's how we do it.
I see. Still, having names even for throwaway use-cases makes for easier debugging, assuming the naming convention is sensible and consistent, which admittedly is a huge assumption.
Remember that the dictionary itself is a stack structure, so whatever issues might come up can be handled by re-defining an older word, thereby "hiding" it.
Point taken. I have never experienced a more fluid transition between different languages than with Forth. Even what little OOP I have done with Forth has never felt easier to do, to the extent that other languages feel too restrictive with OOP.
Well, as I said, there's nothing preventing you from using a named word instead of an anonymous word. You just can't create one inside another (in 8th, and I think in most Forths)
Blurry? Interpret means look-up and execute; compile means add new code which can be looked-up.
[ and ] as per ANS Forth don't exist in 8th. Instead, the [ symbol begins an array as per JSON. Your point stands, kind of. However, I would say that "doing one in the middle of the other" does not erase the distinction between the two; you're simply stopping one and starting another.
The concept of polymorphism certainly doesn't hurt, but yes, the primary usage I see with OOP is managing the program state within some narrow domain.
But that is already so far removed from the Compile-&gt;Execute cycle that is present in, say, C, that I'm willing to say that it doesn't matter much. BTW, if you don't use [ and ] to change the execution state, what do you substitute for that capability?
Interesting.
Some do, some don't. VFX does pretty good optimizing. 8th doesn't do any except tail-call elimination.
Traditionally Forth [cross alias meta] compilers are non optimizing to my knowledge which conform to the principle of making code generation explicit, seem to be seen as a useful feature. Indeed for processor architectures like Itanium where hand optimisations in machine code can lead to a performance increase of factor 10 upwards this makes sense in my opinion.
Flashforth does some optimisation like inlining, peephole and combining literals with arithmetic and logical operations on TOS cached in a register. And tail call elimination.
Anton Ertl has touched on it, I think; it's certainly evident when comparing the indirect threading performance of, say, the Core 2 (^**e** which does have multiple-target indirect branch prediction) and the Atom (^**e** which doesn't). Basically, the impetus to improve indirect branch prediction has come from the reliance of Simula-descended OO languages on virtual methods; the previous mechanism - predict that indirect branches would go where they did last time - worked fine for most virtual calls (which turn out to go to the same code every time), but a few are stubbornly polymorphic. It also turns out that the previous mechanism for prediction gives a 50% hit rate for threaded code, assuming a copy of NEXT per primitive (reflecting the tendency of primitives to occur in runs); however, because each primitive will, in practice, only be followed by a few possible others, a mechanism designed for predicting polymorphic indirect calls also benefits virtual machines. As for the detail of how those predictions are made... I have only the vaguest idea. Reading Agner Fog's microarchitecture guide might be useful, though.
What are the performance penalties for taking that approach? I imagine Forth (and your version 8th) is pretty fast by itself.
a good article, **The death of optimizing compilers**, [pdf](https://cr.yp.to/talks/2015.04.16/slides-djb-20150416-a4.pdf), [audio](http://cr.yp.to/talks/2015.04.16/audio.ogg). summed up well on [twitter](https://twitter.com/hashbreaker/status/576748897772650496?lang=en) &gt; for unimportant code, you don't need them, for important code they're not enough think he make's a good point with examples to back his view up
previously havn't heard of him, he looks impressive
Yes, those are simple optimizations, which does some good, without taking too much memory. 
Are you the author? Nice work!
but how to play it? any words documents?
cool, over my head but seem to confirm the point
thanks
If I had the time, I would add more videos. More sample code comes out with each new release.
Thanks!
As my understanding this is much more restricted than functional programming. Lambda calculus is turing complete, but utterly unpractical because the only things you have and use are lambdas that take exactly one other lambda as a parameter and return exactly one lambda as an output. There are no numbers, booleans, control structures and variables. You can invent most of these things and encode them as lambdas. This is called Church encoding. Building everything from ground up using lambdas is a great fun and very interesting, but still not so pratical. One example of a partical usage of Church encoding I came across is how booleans work in Smalltalk. They're essentially Church encoded booleans, or at least a very similar concept.
Not everything needs to be pratical, sometimes fun is enough.
Okay then, have you got a practical example of where functional programming would be useful?
It would be useful in writing multithreaded code, writing immutable code, writing code that has higher reliability and guarantee of a correct answer, and writing code that is easier to debug.
How so? You can't actually write any useful code with it Can you give a practical example of where functional programming would be usable?
"You can't actually write any useful code with it." I think you will find you can write any possible useful code with lambdas. Perhaps go on youtube and type in lambda and educate yourself a bit. Furthermore, the point of this exercise was not to prove to you the usefulnes of the krivine machine but to debunk some of the false claims that DGASAU had made in the chatroom.
I don't watch videos. All the articles I see about lambdas suggest they're mostly useful for solving toy problems in academic Comp Sci, and don't lend themselves to real-world things. If you reckon Krivine machines are worth investigating, go and write one and show us it working.
Have you ever seen a good collection API?
Nope. What would it look like?
I came up with this: create arr 1 , 3 , 7 , 6 , 3 , : nth ( n -- a ) cells arr + ; : show 5 0 do i nth ? loop cr ; : @! ( a1 a2 -- ) over @ over @ &gt;r swap ! r&gt; swap ! ; : exch ( n i -- ) nth swap nth @! ; : odd? ( n i -- n i f ) over 1+ 1 and ; : switch ( n i -- ) odd? if drop 0 then exch ; defer permutations : permute ( n -- ) 1- dup 0 do dup permutations dup i switch loop permutations ; : (permutations) ( n -- ) dup 1 = if drop show else permute then ; ' (permutations) is permutations I hardcoded the array size, because the original specification called for five elements.
You're right, that's wrong. I'll fix it.
He was claiming it would be better than forth and that everyone using forth should switch to the krivine machine. But sense all a krivine machine is is a lambda interpreter it can not to everything that forth does. It could complete any of the same mathematical tasks but it would not be able to completely redefine itsself or have mutable state or anything like that because it can only run the lambda calculus and nothing else. And so that ends the debate of if krivine machine does more than ans forth. Ans forth does more than the krivine machine does. That being said if what you are trying to do is implement a basis for a functional programming language using lambdas in forth then the krivine machine allows you to do it in a very small amount of functions. So it does solve a problem but a very specific problem and not a general problem like common lisp and ans forth do.
ASau would prefer to use something more mathematically pure and abstract than Forth. But it doesn't have to be a krivine machine, which is what I was trying to point out with this excerpt. Mostly he just tries to discourage use of Forth: From November 20, 2013: forthworks.com/forth/irc-logs/13.11.20 &gt; 03:23:12 &lt;ASau&gt; I'm here to help other reasonable people who've got fascinated by Forth to get rid of illusions. &gt; 03:23:44 &lt;ASau&gt; In particular, by providing inconvenient evidence that Forth isn't actually as good as it is painted by Forth lovers Just for the record, the full log from the day of my excerpt from the last post is available via gopher or http at: forthworks.com/forth/irc-logs/16.09.14 
Forth philosophy is generally said to be "solve the problem at hand, not the problem that you might have tomorrow". So if the problem required that it be reusable then your solution might be better. You can see how many assumptions we make when you really start pruning the problem down to what is needed. A possible solution to Lars's code to make it more general purpose could be to make all those words DEFERed and change their behavior later when the need arises. I think the lesson to learn from Lar's code is the extreme factoring that he used. It created a little language to solve the problem.
That generate word is a little bulky. I would put the two different sets of parameters for 2dup, into two different words. FORTH is more about calling words which have their parameters encapsulated in, rather than calling a more primitive word and adding parameters in the middle of a routine. : cellswitchbundle 2dup 1- cells over &gt;r + r&gt; i cells + switch ; : csb2 2dup 1- cells over &gt;r + r&gt; switch ; This is a quick and bad example, but hopefully you get the idea. If you're worried about your vocabulary getting too large, then you can specify dropping those words at the end of the given program. One other manner in which this isn't really particularly good form in my mind, is in the sense that you have so many words in the same file. Different words should ideally be treated as their own executables; parameters are baked into said words, and so the only time you should really be using multiple words in the same file at all, is if you're doing branching, which granted you were here. : STAR ( Emits a single asterisk ) 42 EMIT ; : STARS ( Allows printing of multiple asterisks ) 0 DO STAR LOOP CR ; 5 STARS Something else to understand about Wikipedia, is that it is run by people who have very conventional mindsets about lots of things, and they have a tendency to try and impose said mindsets in places where they don't work. So for languages like FORTH and Brainfuck, they really aren't the best source of information, because you'll likely have some object oriented elitist refactoring code from those two languages into a horrible mess, and then assuming that s/he is doing the world a favour.
&gt; literally everyone is teaching the opposite right now - make your solutions general They are indeed teaching that, because those languages are not interactively compiled languages. In most languages compilation is separated from runtime and as a consequence your program will require something to fill the need for receiving inputs... a separate "user interface". Forth doesn't separate compilation from ui. As a consequence, the compiling functionality needs to be simple like a user interface. The words as disposable abbreviations workflow is great, but, if forth is to be better at Unix-cli-style unplanned&amp;ad-hoc computing it needs better primitives for filtering. I used to shun parsing in forth, but parsing is happening every step of the way regardless. So what I actually dislike is off-line parsing. Now I'm remaking my forth around parser-combinators ...but with fine-grained interactivity. You can already parse a csv file with the comma compiler, but now I'm talking names that are more than just string literals. To tie this into forth as the ui, your keyboard can code, could your mouse code too?
Maybe 30 minutes. Many things in Forth goes against the grain of mainstream languages. In my opinion, that what's makes it interesting. It's a fresh perspective. If you're out to prove that programming in Forth takes longer than in other languages, there may be some truth to that. It takes time to learn Forth well. It's like a sharp knife that can do wonders in the hands of a master chef, but a lesser cook will only risk hurting himself.
&gt; If you're out to prove that programming in Forth takes longer than in other languages, there may be some truth to that I don't have such plans :) Reading books or blogs about forth people say stuff like "10x faster development than XYZ" or "secret weapon". I don't get that feeling when struggling as a beginner of course. I'm just trying to find out if the community thinks that too and what gain comes from using the language.
From a theoretical standpoint, state less programming have the advantage of implicit parallelism. In practice this advantage is limited though state-bound IO operations which may require serialization.
I'm curious: how many items do people keep on the stack in 8th? In my Forth dialect, I use a configuration with around 500k items as a max, though the largest set of values I've actually stored there is around 20k. (Using the stack as a scratch space while unpacking a large data set during some prototyping).
In Stack Computers, Koopman, the research showed that a typical Forth program seldom has more than 20 items on the stack and even recursive program examples needed 36 items. https://users.ece.cmu.edu/~koopman/stack_computers/sec6_4.html#641 In Forth the stack is more like a dynamic register set so well designed Forth words are never manipulating a large number of stack items. Using the stack as temporary buffer space is a huge environmental dependency, since many systems have less than 1K allocated for the Parameter Stack. 
"It depends". Normally, just a few. But recursive algorithms need more stack (usually). So I settled on 64 being a reasonable size, with the size increasing by 50% each time an overflow would otherwise have occurred. It seems to work nicely, though it does exact a small speed penalty when pushing to make sure the stack is big enough (and to resize if needed).
In particular, in 8th you can create arbitrary stacks, buffers etc as you wish (without a lot of work and worry). However, there are some times when it's useful to be able to recurse 10000 times and put data on the stack. It does happen, and now it's easier to do that.
Or calculating the [Ackermann function](https://en.wikipedia.org/wiki/Ackermann_function)
I sometimes use the stack for data storage as, in my vm, it's sometimes faster to work with values on the stack than to fetch/store values in memory. It's certainly not an elegant approach, but for quick one-off tasks I'm not opposed to doing this. I don't really worry about the environmental dependence since my code isn't really portable to other forth systems anyway due to a heavy reliance on things that aren't present in ANS or older standards.
My Forth is based on 12 required primitives (plus four I/O words). I recently took these primitives and made a virtual machine with 11 instructions, of which one is a noop. There is also a Verilog implementation. http://github.com/larsbrinkhoff/nybbleForth
Nand, Subtract, Store to mem, load from mem, load instruction pointer. That's 5, without going absolutely crazy minimal, like [One Instruction](https://en.wikipedia.org/wiki/One_instruction_set_computer).
I think I have one. Has no ALU though.
Thanks for the feedback. I was hoping that it would be simple and easy to understand.
https://github.com/rufig/spf/blob/master/docs/readme.en.md &gt; SP-Forth is a reliable and comfortable forth system producing optimized native code for the Intel x86 processors. It runs on MS Windows 9x, NT and Linux. &gt; SP-Forth is free software, see LICENSE section for more information.
"it has a Forth-inspired stack-based language, PumpkinScript"
the supplied count is the number of cells, e.g. `arr 4 show`. So I guess your code would need to have a `cells` call before `over`. Thanks for the tip, I'll try to remember this idiom :) Edit: actually that would be wrong, would need to be `1- cells`.
Ah, [starting forth](https://www.forth.com/starting-forth/6-forth-do-loops/#Indefinite_Loops) said `0 until`, didn't know about `again`, thanks :)
what kind of problem?
The code above does not work when I try to use the word butler-definitions
You mean the Usenet comp.lang.forth group?
&gt; I would list this language on my resume To bad this is listed at the poorly side. I would definitely put Forth in my cv. Gilad Bracha (a language designer, currently works at Google) said that he always asks new software developer candidates what programming languages do they know. He always get the same answer which is something like, Java, C#, C++, Ruby, Python, PHP. He finds this sad because this is a very narrow slice of the full spectrum of programming languages. Yeah, these languages are quite different if we forget about, Forth, APL, Haskell, Lisp, Smalltalk, Prolog, etc. But they are fairly similar if we consider the full scale of languages.
Sure, I tell recruiters and everyone else who'll listen that I'm into Forth. Most have never heard of it, unfortunately. I'll also mention Lisp, and at least they have the decency to have vague misconceptions about it.
:-) awesome find. Are you going to add them to the wiki here on /r/Forth?
Good idea. I made a new section under "Further Reading".
Literally none of this makes any sense whatsoever. I'd rather not be rude but I think you ought to know it. Like, this kind of thinking leads only to Time Cube. If you have +, 0, and -, then + is the symbol you've chosen for the digit 1. It's as simple as that. The three symbols are arbitrary. You could use 1, 0, and - instead if you wanted. Or even 1, 0, and 2 if you don't find 2 confusing as a symbol for -1. You could use three Emoji... it won't make the system more or less "complete".
Here's the prequel from the year before to avoid making another top level post: [Algebraic Specification of Stack-Effects for Forth Programs](https://www.kodu.ee/~jpoial/teadus/EuroForth90_Algebraic.pdf)
Jaanus Pöial has a lot of interesting formal analysis of stack languages https://www.kodu.ee/~jpoial/teadus/ 
Neat stuff. I've been kicking around some ideas about type inference and stack effect notation, so some of this is interesting.
In particular, the Forth: http://www.canoncat.net/cat/Cat%20tForth%20Documentation.pdf WARNING! IT'S A PDF!!!!!! ;-)
Be careful to check all your status bars because next time I have a bad day I'll link you to a self-extracting tar bomb :D
I don't know if my blog is a good read, but i't about Forth and mathematics: http://forthmath.blogspot.se/
It doesn't have to be good, just about Forth. :-) Added to the wiki.
I once badly wanted to buy that thing. But it wasn't available in Europe at the time.
Can we see it online?
http://forth128.blogspot.com/
I think [that's the one I was thinking about](http://www.excamera.com/sphinx/fpga-j1.html), not the same author though.
i3wm and a nice Forth. Me sitting behind Visual Studio not having a good time envy the author. 
I am not surprised. *Enter ye in at the strait gate: for wide is the gate, and broad is the way, that leadeth to destruction, and many there be which go in thereat: Because strait is the gate, and narrow is the way, which leadeth unto life, and few there be that find it.* -- Matthew 7:13-14, KJV.
The URL gives a 404 error. This should be the correct one: https://marketplace.visualstudio.com/items?itemName=rickcarlino.forth
What's the github Repo ?
&gt; You have to do something to ensure that different execution paths (e.g., via `if ... then`, loops, etc.) net a consistent stack effect. Or you could have the control structues be compiling words. Then maybe you could use them outside a colon definition and count their pushes and pops at runtime.
https://github.com/RickCarlino/vscode-forth
Not aiming to hijack the discussion about this VS extension, but the [Acme text editor](https://research.swtch.com/acme) and its mouse chording is awesome for microcontroller forths without built-in tab completion or prompt history. All I have to do is highlight code from the scrollback with the middle mouse button and Acme will send it to the prompt below and run it. This way you can also execute any substrings of the scrollback or middle click single words that you are too lazy to type out. You'll want to configure acme from plan9ports with a decent font and some handy plumbing rules according to your needs but it's worth the effort. https://github.com/evbogdanov/acme http://www.mostlymaths.net/search/label/acme I have a plumbing rule for register addresses so I can right mouse button highlight the high nybles of an address and get a list of all the registers under that base address in a new pane. type is text data matches '0x([0-9A-Fa-f]+)' plumb start rc -c 'grep -n '$1' '*' | plumb -i -d edit -a ''action=showdata filename=0x'$1'''' The downside is you'll need a 3-button mouse and not one with a scroll wheel. I'm using [this one](https://www.amazon.com/HP-Optical-Button-Mouse-accessory/dp/B0002Y5LZ8).
thanks!
Thanks for catching this one, @pointfree @eloraiby - I need to update the README!
Interesting. I have sometimes temporarily moved the dictionary pointer so I can use the usual compiling words somewhere else in memory. cmFORTH has the `{` and `}` words to set the dictionary pointer for metacompilation.
Renaming `c@` may not be the best solution, because you may well want to access memory elsewhere too. Consider e.g. copying strings. I have sometimes felt a need to fetch sequentially from a pointer, like in a disassembler. I called my words `^` and `c^`, but I never was too happy with those names.
Forth macros are ordinary words executed at runtime. Basically, `IMMEDIATE` is just a convenience to omit square brackets. : MYMACRO ... ; IMMEDIATE : SOMETHING ... MYMACRO ... ; over : MYMACRO ... ; : SOMETHING ... [ MYMACRO ] ... ;
My understanding though is that forth macros allow you to extend the language such that it can have arbitrary syntax. Is that not the case?
They are. But they are not "macros" as in textual/lexical substitution and templates as in C/Lisp. https://www.forth.com/starting-forth/11-forth-compiler-defining-words/ 
It is not Forth "macros" that extend the language, but the words that extend the compiler. Typically this is done with CREATE / DOES&gt;. But these are not macro creators. They are more analogous to extremely simple Object oriented words which create a data structure that has only one method. Example: make a word that adds "constants" to the compiler : CONSTANT CREATE ( n --) , \ at compile time, compile number from stack to memory DOES&gt; ( addr -- n) @ ; \ at run time fetch the number from memory to stack Now the compiler will understand: HEX 00FF CONSTANT BYTEMASK 
I may have shown this technique in another post but here it is again. In ANS Forth to do a text macro we can use a string literal and the word EVALUATE. : MYMACRO S" OVER + SWAP" EVALUATE ; Now when we use MYMACRO it will expand to OVER + SWAP. 
You might also want to look at the late Dr. Julian Noble's tutorial. See section 4. http://galileo.phys.virginia.edu/classes/551.jvn.fall01/primer.htm#intro As I think about your request for macros it occurs to me that in some ways the entire Forth language, except for CODE primitives, is simply a collection of macros. A forth word created with colon/semi-colon simply expands to a set of other words. Is it possible to do what you want by just creating new Forth words?
"arbitrary syntax" is possible. For example recursive descent compilers have been written in Forth. However, typically creators of "domain specific languages" on top of Forth create something that is inherently Forth interpreter friendly. Meaning space delimited tokens, RPN parameters. But you can create simple "monitor" type commands by using parser words to create &lt;command&gt; &lt;paramenter&gt; &lt;parameter&gt; type script languages. It becomes more challenging to compile those commands but if you only want interpretation it works fine. I once created a script language for an election system that looked like this: DATABASE RIDING HAMILTON-WENTWORTH NAME John Doe, CONSERVATIVE, ENCUMBENT NAME Jim Smith, LIBERAL NAME Mary Jones, NDP NAME Frank Garcia, GREEN An admin typed up the database for me on a word processor and the system read the file and created the database. Can you give us an example of what you are trying to accomplish?
What is interesting to me is creating a dsl that is not parsed by whitespace delimiters but that has arbitrary parsing rules.
At which point you have to ask the question, "why Forth?". Maybe something like META II or OMeta are what you want.
Because forth is one of the only languages that can begin parsing code in a completely different way while it is running.
But from what I can see you're not interested in Forth's semantics, philosophy, or other qualities. That is patently false as any language which exposes the input stream at runtime will suit your needs but if you want to be able to embed complex sublanguages then something like META II and OMeta are obviously a much better place to start. I don't want to discourage you from using Forth and it won't stop you if you want to use it this way, but, honestly, it seems like you're starting from a weird place and you're only going to make life more difficult for yourself :).
It is not necessarily that I don't like the forth syntax. I usually prefer to program in the forth syntax. But I desire the feature of being able to change the syntax to whatever I want.
Cool. Ok. So you write a parsing word[0]? You'll have to do all the parsing by hand but there's nothing stopping you. I'm really not sure what you're asking at this point? [0] Not "macros" in any sense of the word, parsing words are just immediate words (words which run during compilation) which interact with the input stream.
OK. So rather than make your own parser generator from scratch, there are 2 that I know of. Anton Ertl has created the GRAY parser. (http://www.complang.tuwien.ac.at/forth/Gray5.zip Or a simpler and less efficient BNF parser was created by Dr. Brad Rodriguez. It can be difficult to implement, but I see a version here. http://compgroups.net/comp.lang.forth/difficulty-with-brad-rodriguez-screenful/2099429 I cut and pasted the code and into an old DOS Forth that I have "ANSified" and it seems to work, but I did not test it very hard. I had to remove some control code artifacts (3D 09 etc...) 
Interesting. Where do the local variables get kept? Parameter stack? Return stack? A specific local word stack?
Thanks. Local variables (e.g. word-locals) get put in a 'map' which is pushed on an auxiliary stack on word entrance and popped off on word exit. Words which don't have locals don't have that overhead. An interesting side-effect of the manner in which this works is that any word invoked from a "local context", e.g. a word using locals, can access that word's locals. So it's possible to access a word's locals from its factors. Once a word is declared to use locals, though, it can only access its own scope. Task locals are put in a map which is held in the task structure.
Is the local context creation part of the compilation process or is it created at run-time?
The support for local context is part of compilation. If a word is labeled as using locals (by putting **locals:** before the colon) then the support words are compiled in so that the word entry and exit do the right thing. Then at run-time, the word pushes and pops the context.
Ah, that makes sense. I recently implemented locals on my own Forth, but its really just a base pointer on the stack that gets saved on the return stack. I know some dislike accessing the stack in an array-style, but I'll be damned if it isn't useful.
Yep. I prefer utility to slavish adherence to standards...
;-) there are some good reasons for not liking local variables that go beyond a "slavish adherence to standards...", but, ok, If you want to look at it that way.
Ah, no; I don't like locals in fact. But my users clamored for them, so what could I do?
What reasons are those?
Good find! There is more at http://trinary.ru/kb/ but, again, in Russian.
Someone suggested a name change, so now they're called "listeners" instead.
This is pretty neat. daf.txt shows the thoughts of someone who had a specific need and profound insight into the solution.
Do the listeners poll constantly for changes? Or do they replace the variable IO with a subroutine that submits a notice to whoever placed it?
The appropriate "!" words are modified so that if the container has any listeners, they are invoked just after the store. So no, there is no polling.
Ah, so basically a Set or Get method for an object property. Interesting.
You could think of it that way, though the listener only fires on a 'set'
Too academic for me, I prefer hands-on problem solving: https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md
I'll go in this direction. Here's my set of words to use the dictionary pointer in loops: : &gt;h here &gt;r dp ! ; : h&gt; r&gt; dp ! ; : @h here @ ; : c@h here c@ ; : @+h @h cell allot ; : c@+h c@h 1 allot ; Typical use case, copying memory. \ Standard Forth. : cmove bounds ?do c@+ i c! loop drop ; \ Using dictionary pointer. : cmove swap &gt;h for c@+ c, next drop h&gt; ; 
I think you are trying to reinvent Smalltalk :)
Not quite since I've never read a line of smalltalk.
Try it and report back. You'd think that after many years of researchers researching ASTs and the like that if this was truly easier, it would be the default way of building compilers. But you can try it and let us know how it works out. Maybe you are on to something. 
The way you design the parser resembles Smalltalk. 
I'll put smalltalk on my list of languages to look into. Thank you.
I am not sure how suitable it would be for writing a JIT compiler but on a language that is left to right that is interpreted I think it could work well. If you wanted to compile it I think there is a way in gforth that you can compile forth code so what you'd need to do is insert the spaces into the code without evaluating the code and then tell gforth to compile the code into a binary.
Rather than try to guess where to put in spaces, just do a closest-match style parser. Spaces would then be nothing but a word that consists of the space character and adds nothing to compilation.
You could probably write a cross compiler to target this, but the flash and ram is likely too small for a hosted system.
You might be able to squeeze in a variant of Forth, some cut-down stack-based virtual machine, but any form of interpreter will be impossible if you also want to store any other useful program.
Just stick some external serial ram and flash and you are golden.
Keep in mind, the built-in resturn stack is only two levels (IIRC) deep.
I'm currently developing a cross compiler to target this class of small microcontrollers. My target is to require a few hundred bytes of program space, and a few tens of RAM for the stacks. PIC10 is on my radar, but it would be a tight fit. This particular device has 768 bytes flash (or 512 instructions really) and 24 bytes RAM.
For arrays, APL and J are the two I've seen the most. For trees, there's not much I'm aware of. See http://www.cs.columbia.edu/~sedwards/classes/2015/4115-fall/lrms/PLTree.pdf for one, and I remember seeing an esoteric language once using trees as a primary data structure. 
Far too limited for my purposes :)
What about lisp? They even have some lisp machines: https://en.m.wikipedia.org/wiki/Lisp_machine
**Lisp machine** Lisp machines are general-purpose computers designed to efficiently run Lisp as their main software and programming language, usually via hardware support. They are an example of a high-level language computer architecture, and in a sense, they were the first commercial single-user workstations. Despite being modest in number (perhaps 7,000 units total as of 1988), Lisp machines commercially pioneered many now-commonplace technologies – including effective garbage collection, laser printing, windowing systems, computer mice, high-resolution bit-mapped raster graphics, computer graphic rendering, and networking innovations like Chaosnet. Several firms built and sold Lisp machines in the 1980s: Symbolics (3600, 3640, XL1200, MacIvory, and other models), Lisp Machines Incorporated (LMI Lambda), Texas Instruments (Explorer and MicroExplorer), and Xerox (Interlisp-D workstations). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Non-Mobile link: https://en.wikipedia.org/wiki/Lisp_machine *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^101785
Lisp is more or less structured around trees (which are formed by linked lists). Quoting [from Wikipedia:](https://en.wikipedia.org/wiki/Linked_list#Related_data_structures) &gt; A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. The result is that each node may include a reference to the first node of one or two other linked lists, which, together with their contents, form the subtrees below that node. Basically, when you nest linked lists, you essentially wind up with trees.
**Linked list: Related data structures** Both stacks and queues are often implemented using linked lists, and simply restrict the type of operations which are supported. The skip list is a linked list augmented with layers of pointers for quickly jumping over large numbers of elements, and then descending to the next layer. This process continues down to the bottom layer, which is the actual list. A binary tree can be seen as a type of linked list where the elements are themselves linked lists of the same nature. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
&gt; Yet in the quest for simplicity I wonder if there is not some trick that we do not know about that will produce an even more elegantly small system. Part of the reason Forth is as small as it is, is because all operands are implicitly addressed. You don't need to specify the source or destination register, which requires something like 5 bits each of a 32-bit RISC instruction for example. x86 did something similar, they had registers that were used implicitly for certain instructions, and their code was (relatively) compact. Why not any other data structure? I think ultimately because the stack is most easily implemented in silicon. If you had specialised hardware e.g. in a Lisp machine, a linked list could perhaps be as fast (not a compsci guy, don't know). But Chuck started on commodity hardware and didn't start designing chips till late. So, stacks.
What you write resonate a lot with my own experience. As such, I want to answer you, but I have a hard time to find how. This is my fourth attempt. I do not write Forth code professionaly. Most of what I do is play with Forth, and tinker with it’s implementations. I experiment a lot with the language. This gives me some insight which, I hope, you will find value in. However, my answer will be long, because for me, your question is at the same time too broad (touching different aspects of the conception of the language), and too precise (demanding specifics in each aspect we will discuss). But first, I should rephrase your question as I understand it, because my answer would not make a lot of sense without that. So… It seems to me that you are asking, exactly, “What if… We replaced the data stack with another data structure…?”, in the context of “I want an elegant and small system”, “I want my code to be succinct and compact” and “I want an efficient way to compute”. Because you speak of arrays, and want succinct code, \_crc pointed you to APL and J. His answer is _for my understanding_ off topic. They do not replace the stack. They use arrays as arguments, but the arguments themselves are perfectly normal (we could reimplement the APL operations in Forth by putting pointers to arrays on the stack, for instance). The succinctness of the code is achieved in another way, by abstracting the functions _a lot_, up to the mathematical level, and using powerful abstract tools like [currying](https://en.wikipedia.org/wiki/Currying) and [partial application](https://en.wikipedia.org/wiki/Partial_application). These abstractions do not tell us what data structure is used, and chances are, they use a stack, or a stack frame. It is still a good answer, in that it shows a different way of achieving the same kind of conciseness as Forth, using a different paradigm. [This article](https://en.wikipedia.org/wiki/Tacit_programming) shows some more ways. But if I am to say that he is off-topic, I have to try and give an answer on-topic. And to avoid any judgement and hard feelings, I will insist on the fact that I have a specific understanding of the question and only use “off-topic” in this light. My answer is not better, just different (in fact it is too long for my taste, but at this point I decided to give up trying to shorten it). And so, after this too long introduction about why my answer is too long (ho ! The irony ! When discussing a subject such as Forth, which encourage brief and to the point expression of our ideas !), let’s review your questions in more details. &gt; Yet in the quest for simplicity I wonder if there is not some trick that we do not know about that will produce an even more elegantly small system. No, there is not. That may be the shortest and most unsatisfying answer you will find in this post. This very short and abrupt answer is the reason I feel the need to argue in such details. &gt;Now I know ans forth is generally not implemented with a linked list as a stack but I am going to use the word stack and linked list interchangeably. This is in my opinion a mistake. Yes, you can use a linked list like a stack. It _can_ behave in the same way. But a linked list can do more. It is not implemented the same way (that, you now), and while it could be of no importance in another context, if you talk about “small system” and “efficient way to compute”, the data structure used is very important. At this level, a linked list is not a stack, and it is not an array. As well, an association list is not a hash map, even when both can be used to implement a dictionary. &gt;There are two sort of datastructures that you can really create in forth. One is an array and one is a list. A list being a value and pointer pair and an array being a continuous series of bytes. Here, you confuse me. Is the list you talk about the Forth dictionary, or are you still talking about linked list while thinking of stack ? There is only one data structure in Forth, the array. But since we can implement _every_ data structure with these arrays, we do not think of them as arrays. We think of them as… Well, what we want, really. If I decide to implement a lisp like leaky cons cell as `: cons here rot , swap , ;`, `: car @ ;` and `: cdr cell + @ ;`, `cons` does not create an array. It creates a cons cell. There is no linked list by default in Forth, unless you implement it. Well, the dictionary may be a linked list, but that does not mean you can “create a linked list”. You can’t even “create a stack”. There are two stacks. There is a dictionary. They are there, but they are their own, and you cannot “create them”. You can only implement your own version of a stack/linked list/whatever by creating a vocabulary which will, under the hood, manipulate arrays. &gt; You can get fancy with it by manipulating these to be shaped like trees or by using functions/words to add complexity to it, but ultimately as far as the datastructure is concerned these are the two key elements. Either continuous arrays or pointer based storage. Yeah, pointer based storage. Pointers are not lists ;-). &gt;Although forth is often represented with a stack as an array I would say a stack really belongs in the list category. A stack at it's essence is a linked list where we push and pop, but there are advantages to implementing it as a long array. I refer you to what I said earlier about a list not being a stack. This is not a futile correction, and that is because of what you say right after : &gt;Is there not another better way to do this? Is there not a more efficient way to compute […] ? Here. A stack is more efficient than a list. Because it is simpler. On modern hardware, registers are also more efficient than the stack. However… &gt;Is there not […] a more compact and succinct way? A while ago, I did it, just to see. I implemented a Forth which did not have a data stack. Only the return stack, and three registers, which simulated the data stack. They where named T, N and L, for Top, Next and Last of stack. Literals where always put in T. There was no push/pop on this fake stack, as it was just registers. The words “n” and “l” would swap the content of the named register with T, so `1 2 +` in Forth was `1 n 2 +` in my experiment. The `1` was not popped, and so the signature of “+” was `( a b - a c )` (“c” being “a+b”). The words “t!”, “t@”, “n!”, “n@”, “l!”, “l@”, would push and pop the registers on the return stack, so we could preserve them if needed before or after a call. And it worked. I could do without the data stack. It was like an assembly language, but without the need to specify “call” everywhere, and I could even omit the register for each “opcode” most of the time. It was really a Forth, more than an assembly language (in the sense of the Intel syntax). But I had a bigger need for words to manage the registers, than words to manage the stack in Forth. Well, even in this `1 n 2 +` example, I need to manage the registers. So, here, the stack is the best compromise : it is better than a linked list in terms of efficiency (It takes less memory, and is simpler to implement. Plus, a linked list does not work very well without memory management (alloc/free or garbage collector).), and it is better than registers to write compact code. (continued in child comment)
**Currying** In mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a single argument. Currying is related to, but not the same as, partial application. Currying is useful in both practical and theoretical settings. In functional programming languages, and many others, it provides a way of automatically managing how arguments are passed to functions and exceptions. *** **Tacit programming** Tacit programming, also called point-free style, is a programming paradigm in which function definitions do not identify the arguments (or "points") on which they operate. Instead the definitions merely compose other functions, among which are combinators that manipulate the arguments. Tacit programming is of theoretical interest, because the strict use of composition results in programs that are well adapted for equational reasoning. It is also the natural style of certain programming languages, including APL and its derivatives, and concatenative languages such as Forth. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
 &gt;By using an array I do not mean using a stack that is in an array. I mean making a computation system that is random access instead of Last In First Out (LIFO). What would this language look like? I could not tell you because I have not invented it. But perhaps there is a system out there even simpler than forth that can exist using arrays instead of a pushdown stack. There is a system simpler than Forth. Simpler in its implementation, at least. Let’s see what happens if I replace the stack by an array. My arguments are provided in that array, and my words read them in there. So, I need a way to access these arguments… Let’s say, by an offset into the array. And some offset have a “standard” signification. Like, offset 0 and 1 are always used for binary operators like “+”, “-” and such. Now, I need to be able to move these results (like with my registers example). I need to fetch and store them ? Wait a minute… I already have this array in Forth ! It’s the heap ! So, all we need to do is to… Discard the stack completely, and define some standard offset for our arguments, and a way to handle literals. So, if you look about it that way, a Forth where the data stack is replaced by an array is just a forth without a data stack. Since we removed one “complex” element, the system is simpler. It is also more cumbersome to use. Now we need to define some standard offsets, because we do not have the “sliding” effect of the stack any more. And we run into problems when writing recursive/re-entrant functions. This model of programming, with just the memory for arguments and local variables, is what was used at the very beginning of computer science. They pretty rapidly invented the stack. They pretty rapidly learned to juggle their arguments on the return stack, too. Then, the second way of seeing the “stack as an array” showed up. The stack frame. The stack frame is _kind of_ a stack of arrays. So, we still have a stack (but we put our return address in the stack frame, so we only have one stack), _but_ we access our arguments with an offet into the stack frame, as an array. But using offsets is not practical. So, we invent variables names to hold these offsets. You guessed it, that is how C works. How most languages work, in fact. The problem is, we cannot use implicit arguments any more. So, we lost our succinctness. Looking at APL (or even Haskell for the matter), we can, of course, get back this succinctness. But it is through using high level abstraction, and even if we manage to remove this abstraction at compilation time, this still means the compilation is a arduous task. So, _if_ we get back our succinctness, we definitively lose our simplicity. Also, the stack frame is problematic when we want to implement proper tail recursion, which can be natural in Forth (ColorForth uses it for some loop constructs and to survive with a very small return stack). (ANS Forth does not require proper tail recursion, and recursion is not really in the mindset of this dialect.) &gt; The other option is trees. Again what would a tree based forth look like? I haven't the slightest idea. All I can say is that it is an avenue to be explored. A tree based forth would have a branching structure instead of a stack. If the “array instead of a stack” immediately made me think about C and its stack frame (and then about Forth without a stack at all), the “tree instead of a stack” makes me think about Scheme. Well, _here_ I am off-topic. But it is worth looking at it. In scheme “theoretically”, a function has three inputs : a list, which is its arguments, a tree, which is its environment, and a function which is its continuation. Continuations are not the subject here and I will not talk about them. What interest me here is the environment. It is a list of dictionaries. Well, it is _a_ dictionary, with all the function variables in it, plus a link to the parent environment (where the function was defined). So, the Scheme functions does use a tree for “implicit variables” (here, the closures), like a C function takes an array for it’s local variables. We could also argue that the three arguments of the Scheme function can be put in a list, and so the Scheme function takes just one argument, a list, which contains more arguments, and the local variables, and the parent environment, and the continuation… And all that in a well structured maneer. I highly encourage you to [read SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html), or to [watch the lectures](https://www.youtube.com/playlist?list=PL8FE88AA54363BC46), as they teach a lot about computer science and different programming paradigms and data structures (in addition to showing how environment and closures work ;-)). A note about the environment : you may implement vocabularies in Forth by creating multiple dictionaries, putting them on a stack, and then searching word in the order of the stack. Or, you may (and it may not be standard compliant) decide that a _new_ vocabulary will link to the _previous_ vocabulary. So, instead of having a stack of vocabulary, where each dictionary end with a null element to stop the search, you can have many vocabularies, were instead of the last element of the dictionary being null, it points to the word of an older vocabulary, and ultimately to the core vocabulary, which is the only one ending with null. _Then_, your vocabularies are implemented as a reverse tree (you have access to the leaves and can go back to the root, instead of having access to the root and being able to walk up to the leaves). (Aaand… That's a digression if I know one…) Now, the more interesting idea is of course to really replace the stack with a tree. You would then need some more words to manage this tree. Like… `fork` to create a fork ? `fall` to go back to the last fork, `jump` to switch to another branch…? would you always be on a leaf, or at the root of the tree…? I concede this is a very interesting idea. However, I come back to your requirements. “Efficient”, “simple”, “concise”. Can you argue that implementing your system with a tree will be more efficient…? It will not be simpler. It _may_ be more concise, but… It may be less concise, depending on the need to do “branch jumping” in contrast to “stack juggling”. I will refer you to some quotes from [this page](http://www.ultratechnology.com/1xforth.htm) : &gt; A Forth word should not have more than one or two arguments. This stack which people have so much trouble manipulating should never be more than three or four deep. And : &gt; But as to stack parameters, the stacks should be shallow. On the i21 we have an on-chip stack 18 deep. This size was chosen as a number effectively infinite. If you decide to follow these principles (which maybe you should if you strive for simplicity), then what is the point of a tree…? It’s fun, but is it _really_ useful…? And more than all, is it worth the added complexity…? (continued in child comment)
 &gt; What drives me crazy is that no matter how I investigate these two potential forths, I can not justifiably say that I have exhausted the possibilities. Don’t get crazy over that. Play with the thing. Get a feel for it, but keep it as a game. You may exhaust all the possibilities. Or you may grow bored of it. Or you may find a really exciting and new way of doing things. Just… Don’t invest yourself in it so much that it has an impact on your health ;-). Just to be sure, did you only think about it, or did you try to implement it…? If not, I encourage you to try to implement a Forth. Not a standard Forth, not a full featured one. A simple one. Maybe read [jonesforth](https://rwmj.wordpress.com/2010/08/07/jonesforth-git-repository/) if you want a low level view of how it could be implemented, or just try to come up with your own implementation with a high level language (when I want to test my ideas, I rarely write an implementation in assembly, but I go for Python or JavaScript depending on my programming environment and goals). Once you have done that, you hopefully gain enough understanding of how Forth work to tinker with it. Then you can try implementing your questions. Take your implementation, replace the stack. See what problems come from it. Do you need new words to replace swap/dup/drop, that are more adapted to, let's say… An array…? Or you can even try to follow what Chuck Moore did. When [he first started to build his Forth](https://web.archive.org/web/20060615025259/http://www.colorforth.com:80/HOPL.html), he did not have a stack (“_A data stack was unnecessary, and probably unknown to me._”). Scrap the stack from your Forth implementation. How do you do things ? Get one register, which would normally be the stack pointer, but use it as a simple accumulator (a register for a number, where your result will go and where your words may take their argument). Turn it into a tree pointer, or an array pointer. What do you get, what does it do ? How does it influence your code ? A fun fact… Chuck Moore did, in a way, exactly what you did. He replaced the stack. And he found some new possibilities. So… What did he replace the stack with…? A __simpler__ version of the stack (for his problem). A circular stack. On the F18A, the stacks are 8 deep, and circular (there are “register caches” which change the thing a little, I’ll omit them for simplicity's sake). What does this mean…? It means, no stack overflow/underflow. If you pop 8 times, you will get back in your original state. If you push more than 8 times, however, you will overwrite some previous values (in the stack itself, predictably, as opposed to arbitrary parts of the memory). What did he find ? Well, that sometimes, he really did not care about what was on the stack. Sometimes, he could say “from here on, I work on an empty stack”. And instead of cleaning the stack, with the right amount of drops or something like “quit”, he could just continue pushing things on the stack. This is true for the data _and_ the return stack. So, replacing the stack can indeed lead to discovery (even when it is replaced with a simpler version of the stack). &gt;Let's imagine a world where Chuck Moore got hit by a truck and never got to deliver forth to the world, so we did not know about it. Perhaps we knew the concept of a stack machine but not quite such a complete picture. Well… I think we would have a Forth. Not with this name, and maybe not _this_ good, as Chuck had brilliant insights. But if you followed the links about DSSP, you may have gathered that they had invented their own Forth without knowing about Forth (they learned about it later and really liked it, but they already had a good base). The [first edition of the PostScript manual](https://books.google.fr/books?ei=nyF8SMm3MKXmtgOSu6CmDQ&amp;id=6x4nzbwzxtEC&amp;dq=0201101742&amp;q=forth&amp;pgis=1&amp;redir_esc=y#search) stated that it was _not_ influenced by Forth (although since it appeared later than Forth, there may have been “thought contamination”). We don’t know, but all the ideas for Forth were invented independently of Chuck. Chuck himself like to say he “discovered Forth”, and I think it is accurate. As all the pieces are there, the chances of putting them together, even by coincidence, were not that low. We may have missed his will for simplicity, however. If Forth is words and stacks, we may have it, if Forth is _the simple thing_, maybe not. &gt;In the same way there may be something to having an array based forth with random access or a tree based forth with branches, and it just eludes me. And no matter how I play with those structures I can not prove that such forths do not exist or that if they did that they would not be better, because some lucky person may come along and figure out that which has slipped through my fingers. Have you though about putting stacks on the stack…? ;-) (in your world view, it’s a tree, or lists in lists, but there it is stacks, so with a limit in how many items they can hold and a simple/efficient implementation). &gt;Why stacks? Are stacks the best way, or is there an even greater and more simple way of doing things which is eluding me. If I find that such a way does exist then my question is answered. But if I never find it then that question is never answered. You may find some elements of response in these quotes : [About why stacks and not registers](http://www.ultratechnology.com/fsc2000.htm) : &gt;The Pentium is basically a register machine. It has a stack and you use it as the return stack and you have to do something to get a data stack. Looking at these various architectures it is clear to me that the stack is essential, you must have a stack. Registers are optional. If you had some registers you might be able to optimize the code for ways where doing it on on a stack is clumsy or difficult. &gt; &gt;But to program a register only machine is much more difficult than to program a stack only machine. I think stacks are the important concept. Registers are a nice thing to have if you can afford them. It may not feel related to your question, because they discuss a low level construct (registers vs stack), while you ask about higher level structures (tree vs stack). But it is in fact an element of response. Forth is though of as a low level language (that can reach high levels if you want to). So, the design is done with this in mind. A stack is the best low level construct you can come up with. Other constructs are too complex (lists, objects), or too cumbersome (array, registers). [About the necessity of stacks](http://www.ultratechnology.com/1xforth.htm) : &gt; Forth is highly factored code. I don't know anything else to say except that Forth is definitions. If you have a lot of small definitions you are writing Forth. In order to write a lot of small definitions you have to have a stack. […] Stacks are not a solve all problems concept but they are very very useful, especially for information hiding and you have to have two of them. For Chuck, you have to have two stacks. But note that it is not “having a stack for the sake of it”. His goal is not to have stacks. His goal is to have lots of small definitions. And for this, the best solution he found was to have two stacks. He could have a stack and juggle parameters and return address, or create a stack frame, or use a tree, or swap between registers like in my experiment. But of all these possibilities, the only ones that can really give short definitions are the stack and the tree, and the stack being the simplest, it wins. (I do not say here that Chuck did experiment with trees and decided they were not better. But given his mindset, I would not be surprised if that was his conclusion ;-).) So, there, my too long answer, which I hope gave you some ideas. Now I need to go to bed :-°. Good day to you. 
Fawkin' Chipped ya babe!
If you replace stack with array, you get [brainfuck](https://en.wikipedia.org/wiki/Brainfuck). It's simpler that FORTH, but it's practical?
**Brainfuck** Brainfuck is an esoteric programming language created in 1993 by Urban Müller, and notable for its extreme minimalism. The language consists of only eight simple commands and an instruction pointer. While it is fully Turing-complete, it is not intended for practical use, but to challenge and amuse programmers. Brainfuck simply requires one to break commands into microscopic steps. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
I disagree. You may get something similar to brainfuck in design but you may have a much richer syntax than brainfuck does.
Other Forth content by Ting https://www.amazon.com/s/ref=dp_byline_sr_ebooks_1?ie=UTF8&amp;text=Chen-Hanson+Ting&amp;search-alias=digital-text&amp;field-author=Chen-Hanson+Ting&amp;sort=relevancerank
This is a great little virtual machine but I have to ask how you've implement `lshift`? I can't seem to find the definition anywhere, which makes me wonder about the status of the project?
The ANS Forth standard is not finished IMHO. Commercial Forth systems have lots of libraries. Take a look at MPE Forth products for example. The Forth standard represents all that the standards team could agree on in a reasonable period of time given the fact that the language has no syntax and Forth people are VERY independent thinkers. Forth Inc was running over 20 terminals on an IBM PC in 1980s, but all the multi-tasking stuff did not get into the standard. Maybe one day... but it is pretty cool to roll-yer-own. 
`lshift` is not implemented in the Forth kernel yet, but it should be something like : lshift begin ?dup while 1- swap 2* swap repeat ; If you're wondering about `lshift` in the definition of `0&lt;`, it's only used at compile time. The status is somewhat stalled, because I've shifted my attention to a cross compiler for tiny microntrollers. I haven't tested the nybbleForth kernel yet, so it's definitely a work in progress.
And Forth derivatives like 8th or 4th have a lot of features which traditional Forths don't have.
Excellent. When you have something written where you can show people, post it here. 
Factor does all these things!
And as a result, Factor is huge (bordering on bloated) resource hungry and very complex :-)
Factor is an example of what I was talking about in terms of crossing a line. To get those kinds of features you simply have to give up the ability to run on a microcontroller.
Oforth ? 
Most of the object stuff is addressable quite easily by nested vocabularies, or sub-dictionaries.
yes its very huge, and i dont know why they cut the support of arm version
but i love forth just because its a thin abstract layer. for eg, on whatever hosts, you could use less cost to build that abstract layer, and then write the code you were familliared with
Yeah you'd want all of these features to be in vocabularies so the underlying forth would be intact and compliant with ans.
&gt; To get those kinds of features you simply have to give up the ability to run on a microcontroller. If you're proposing a trade-off than an argument should be made for why the features are necessary and/or beneficial; as written, the reader is asked to just accept your premise. Let's take on example: &gt; Such a forth would probably have nestable version of :noname because passing a function as a variable and nesting functions is pretty important. Why? Having worked with languages like Lisp and Smalltalk for many years before working with Forth for the last several years, I wholeheartedly deny that nested higher-order and anonymous functions, and the implied closures and lexical scope are necessary and/or beneficial. Not only are they not needed in Forth, but the cost in resources and complexity (both introduced by these features and as a result of applying these features) is hard to justify in any language[0]. Forth has other mechanisms for accomplishing these tasks and I'm wholly unconvinced that anything would be gained, even on paper, by adopting "true" higher-order and anonymous functions[1], I would argue, for familiarity and syntactic convenience. &gt; This stack would probably be an array, a vector, or a linked list. So not a stack. You state that your ideal "high-level" Forth would have built-in support for statically typed, imperative???, functional and object-oriented programming, and have a rich library of mutable and immutable data structures with a Lisp-like syntax and associated macros system. I won't deny that, as a language enthusiast, I'm quite curious to see such a language would be like but from my perspective that's not Forth in any sense. You and I clearly have a very different understanding of what the term high-level means, in relation to Forth. Personally, I consider Forth code high-level once (in the context of some particular problem) the vocabulary has developed to a point where the solution can be fully described in a handful of easily understood pseudo-English phrases[2] This problem-oriented language approach (as prescribed by Chuck Moore) to Forth seems to be the antithesis of the unabashedly general-purpose high-level language that you have in mind. There's nothing wrong with that but, speaking for myself, I would like to see more of a justification for your desire; particularly, why you think that your understanding of Forth is better[3]. It might come as a surprise to you but many of your claims are less "self-evident" than you may think :-). Especially to a Forth programmer. [0] I'm a big fan of Scheme but early Lisps have a character all of their own; one which has been sadly lost to history, with the sustained push for efficient compilation and functional programming. [1] I'm not talking about quotations here. I've written about the problems with quotations in this subeddit in the past; I'm talking about something even more "powerful", as implied by the claim "passing a *function* as a *variable* and *nesting functions* is pretty important". See Full Lexical Closures in Factor. [2] And not all problems need to get there. It's perfectly acceptable for a problem-solution to be low-level if that's what's required. [3] That your [Forth] kung fu is strong ;-)
My understanding was that /u/ummwut was saying that much of the need for object-oriented programming is already addressed by nested vocabularies, rather than stating that a word set supporting object-oriented programming should be provided using vocabularies.
First of all I don't see what is necessarily wrong with closures and higher order functions. Just because you may not like them does not mean there are not other programmers out there who can make good use of them. Plenty of lispers, javascripters, and haskellers all make common use of higher order functions in their programs. Second of all I think it is evident that if you add enough code to your forth eventually it will be too big or slow to fit onto a microcontroller. There is simply a limit to how much you can pack into a microcontroller and when you are talking about doing high level things like object oriented you run into a wall in terms of what an mcu can support. You can try to dance at the edge of it and make it small enough to fit on an mcu but not all programmers are going to want to have to do that. Some programmers naturally write bulky code and that is not necessarily bad or wrong. You say that a stack is not an array so I am curious to know what your definition of a stack is. Also I can agree that creating a pseudo-english wordset to work with is ideal, but this I think is only suited to domain specific languages. You eventually run into the problem where when you combine words in a way that works computationally it sounds very unnatural verbally. Or else you run into a situation where someone strings some words together not knowing how they work under the hood only to find that computationally they do not work out. Nobody has been able to come up with a wordset where you can just type in english and get out something that works. I think the closest anyone has ever come to this is cobol.
Sure that is one way to implement object oriented. There are in fact many ways to do it.
&gt; [all we need to do is to] give up the ability to run on a microcontroller. I wanted to address this point separately because I think it speaks to a widely held belief among programmers that computer resources are unlimited today. Nothing could be further from the truth. The problem-solutions have grown and resources are just as constrained on today's hardware as on microcontrollers. Problems which could be solved in Forth on a [what today would be considered a] microcontroller ~50 years ago with an overhead of a few KBs now commonly require tens or hundreds of MBs and computers are continually overloaded. Modern software is super-massive, grossly complex, and infested with bugs. Ignoring the operating system overhead, Factor required ~80 MB just to start, the last time I looked [1] Ignoring the operating system overhead[0], you can still write a Forth system, which you fully control and understand, and solve real problems in a few KBs! tl;dr the features you astound aren't free and even ignoring the rise of mobile devices resource usage is still of paramount importance when solving real problems using today's computers. [0] Assuming that you're not targeting a microcontroller today then you're probably running "hosted". [1] In my experience, Lisp and Smalltalk, Haskell, Python, Ruby, Javascript etc. aren't much (if any) better in this regard. There is no alternative to Forth today if you value efficiency/performance, correctness/reliability+security, portability, simplicity, understandability, and interactivity. That's why we use Forth (I can't say why other people here are using Forth). From that point of view, all these proposals do is turn Forth into "everything else". Properly applied, Forth allows you to quickly and easily produce high-quality solutions which are orders of magnitude more efficient than equivalent solutions using other technology (and are correspondingly cheaper and easier to operate). I wouldn't give that up easily.
:-) Indeed. I wrote my dissertation on object-oriented programming. If you want to maximise the bang for your buck, vocabularies solve the problem well and do so much better than alternatives. They're not perfect. Engineering is about making tradeoffs, but the cost is so low that compared to the benefit, they're a no brainier. Most alternatives have much higher costs
&gt; Just because you may not like them does not mean there are not other programmers out there who can make good use of them. It's not about whether I like them or not. When I worked in Lisp and Smalltalk I enjoyed them very much. What can't be denied is that (unless used sparingly) they lead to higher-overheads, greater complexity, and reduced transparency. Having worked in Forth for years now, I can't think of one situation where I would choose these 'nested higher-order and anonymous functions, and the implied closures and lexical scope', over the efficient, simple, and obvious approaches that emerge from Forth. When I write in Lisp and Smalltalk today (which is much less than in the past) I use the mechanisms that they provide. Generally speaking, they are the best (and arguably only) way to resolve the internal forces which these languages make manifest in code. Forth has very different mechanisms, which are objectively and provably better along these dimensions. &gt; Second of all I think it is evident that if you add enough code to your forth eventually it will be too big or slow to fit onto a microcontroller. If you waste resources in any solution eventually the solution will become too big and too slow. There are alway limits. &gt; Some programmers naturally write bulky code and that is not necessarily bad or wrong. There is some truth to that but a systems programmer should not, as a rule, produce systems which are difficult or impossible to use properly or efficiently. Can you imagine how popular a firewall that can't handle 1000 rules effectively (where you need to handle even 1 order of magnitude more) because the developer wanted to provide a high-level interface to what is otherwise a very simple system to program? All high-level languages today are like this. They make it practically impossible to do anything well or efficiently in the name of being familiar and comfortable. &gt; You say that a stack is not an array so I am curious to know what your definition of a stack is. Formally, a stack is an First-in Last-out Abstract Data Type with two defined operations, usually called, push and pop. There's nothing more to it and you only confuse yourself when you talk in terms of possible implementations. It simply does not matter. A stack is a stack. It doesn't matter whether that stack stores numbers, or large binary objects, or pointers to structures in memory. These are trivialities. &gt; I can agree that creating a pseudo-english wordset to work with is ideal, but this I think is only suited to domain specific languages [...] Chuck Moore originally described Forth as a technology for programming application or problem-oriented languages. Every vocabulary/lexicon in Forth defines a problem-oriented language. Any problem that can be solved in Forth is solved by a problem-oriented language. All problems that can be solved by a computer can be solved in Forth. Ipso faco all problems can be solved by way of a problem-oriented language. &gt; You eventually run into the problem where when you combine words in a way that works computationally it sounds very unnatural verbally. The degree to which these problem-oriented languages follow English is neither here nor there, but names are usually chosen from English, hence pseudo-English. &gt; Or else you run into a situation where someone strings some words together not knowing how they work under the hood only to find that computationally they do not work out. This is patently false. You don't need to know how a word is written under the hood in order to use it. That's the whole point. You do, of course, need to know what the word does/what the effect of the word is, but this is true of any anything. &gt; Nobody has been able to come up with a wordset where you can just type in english and get out something that works. I'm not surprised but that's a non-problem.
My point is that there are some very large tasks which take large chunks of code and to write that code you have to cross the line where it won't fit on an MCU. For example implementing a SQL database, implementing a linux style distribution, making a web browser that runs javascript, making a video content streaming website, and etcetera. Let alone writing all of those things. Eventually you get to a point where you write more code than will fit on a microcontroller, period. There are no if ands or buts about it. Certain programming goals can not be achieved in a microcontroller's spec. You won't write the videogame crysis on an MCU. You've got to write a huge bulk of code to make a game like that.
It's is not a deep or difficult point that you're making here. It is obvious that there comes a point where any given solution will not "fit" in a microcontroller (or any other arbitrary set of constraints). Nobody has disagreed with this. What you're yet to explain is why you think that the simple and powerful language and techniques which work so well on microcontrollers aren't applicable outside of microcontrollers, and need to be replaced by a kitchen-sink language, with built-in support for almost every buzz-word feature of the last two decades. Why do you think that you can't write a database engine without these? And why not an OS, a web-browser or a content streaming video website etc? Are we just supposed to accept these claims as self-evident without thinking? There is plenty of examples of database engines and operating systems, and video streaming solutions, written in Forth. Indeed, plenty of desktop applications and even an indi-games which have been written without them. So I'll ask again. Why do you think otherwise? What exactly is wrong with the facilities and techniques you intend to replace? What problem are you trying to solve which can't be solved without them?
&gt; You couldn't build vast libraries in this forth, because it would be such an obscure dialect that nobody but you would write in it. You wouldn't build the videogame crysis or a sql database in this forth because we are not trying to build something so large. Simple code to achieve a simple goal. Why do you believe that simple code, written using a simple and powerful language with simple and powerful techniques can't achieve great things? Why can't the Forth that you'd put on a microcontroller be the exact same Forth that you'd put in the cloud??? You're drawing completely arbitrary distinctions to justify your claims that Forth isn't applicable to these kinds of problems, and so needs to support [the features you are advocating but haven't justified e.g.] static typing, nested higher-order and anonymous functions with full lexical closures, a presumably conventional object system, a rich standard library providing immutable and mutable data structures, and Lisp-like syntax and macros. Well, ok, but can you please try and justify this thinking without the strawmen? For interested readers, this is a continuation of a discussion going on here. https://www.reddit.com/r/Forth/comments/6uuit5/high_level_forth_the_libraries_you_wouldnt_put_on/dlvqj7j/
I don't know Mr. Moore personally but the last I heard he was alive and well. He is/was working at GreenArrays Inc. on the F18/GA144 Forth chips and seemingly enjoying life. http://www.greenarraychips.com/ There is no mention of his death or departure by the people who know him so he's probably still there.
Tsss. Good one, babe.
From a c.l.f. posting in July: &gt; Greg Bailey from GreenArrays promptly replied to let me know that &gt; &gt; [M]ore than a few people have become concerned about [Chuck's] &gt; decision to make colorforth.com go away. &gt; &gt; My understanding from Chuck is that this was a conscious decision &gt; on his part. Maintaining a website and interacting with people &gt; consume lifespan, and colorforth.com was an experiment for Chuck &gt; in both of those activities. Chuck concluded that the experiment &gt; had run its course and took the indicated actions. 
Good to know that about his website. Thanks.
Ok, I'll keep an eye on that site. Thanks for linking to it. Fyi, for anyone who's interested, I also found his daughter's [twitter profile,](https://twitter.com/Methidoc) which could be used to find info about her father or to ask her how he's doing.
The reason I don't think it could have libraries is that if you write your own custom forth tomorrow then by definition there are no libraries written for that forth. And if that forth does not ever become popular then there will never be libraries written in it. It's not a bad thing, especially on the microcontroller scene. And there are some limited ways around it. Generally though I think if you are going for the most minimal forth then libraries are not super important and often add bulk or are an extra appendage.
I outright disagree that using moore's programing techniques that you could build something such as the game crysis. That is at least one solid answer which I think can not be validly disputed. I would challenge you to write something of that scale in forth or show something written to that scale.
I have a young higer-level Forth with parameterized types, generic words, coroutines and more that checks some of your boxes. Mostly aimed at scripting applications for now so no user-defined types/macros from the inside yet, but if you would consider writing a tiny bit of C++ to hook in what's missing it might work for something. https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md
I'll keep your forth in mind.
Fascinating information in there. I haven't even gotten past the first part, but his initial discussion about words alone is very intriguing.
This was a really great read. Thank you for sharing.
I started writing an Emacs in Forth: http://github.com/larsbrinkhoff/fmacs
It should be Vimforth...
&gt; Chuck: Teamwork—much overrated. LOL!
agree, look at vim's key stroke, they just like inputing forth code for eg, `11j` means move downward 11 lines
The Forthmacs name is already in use. It is/was a FORTH-83 system for 68K (and possibly other RISC) architecture. IIRC it was at least partially the basis for OpenFirmware. * ftp://ftp.taygeta.com/pub/Forth/Reviewed/forthmacs.arc * https://www.forthworks.com/mirrors/taygeta/Unreviewed/forthmacs.msg * https://www.forthworks.com/mirrors/taygeta/Unreviewed/forthmacs.arc 
Do you have a digital copy of your dissertation available? I'll PM you my email address if you're willing to share.
That is very cool. It makes me wonder what it would take to write CONC in Forth. It provides a more general solution language that could be exposed to the end user for specific project needs or used by the developer to write internal modules. I am writing this before looking at Github.
Actually I figured out I was at 95% in a way to use a proper parameter stack to pass things around so it is done. A master loop removes the eventual problem of some recursion depth overflow. Maybe when my forth VM is done I can seamlessly mix them together, which should be fun.
Perhaps it would help if you shared your source code.
There's no standard wordset for interacting with a mouse. Which Forth and OS are you using?
https://github.com/giltheb/design/blob/master/route/example/route.html
This is not a comment to you but to /u/GitiB. What helps me in my javascript forth is to begin by making a function push(x){stack.push(x)} function pop(){return stack.pop()}
GForth, Win7. Have not tried Linux yet but if that's all you know I wouldn't mind references for the future.
I'd recommend looking into the FFI gforth has and using it with SDL. Some possible starting points: * http://rosettacode.org/wiki/Call_a_foreign-language_function#Forth * ftp://ftp.taygeta.com/pub/Forth/Archive/tutorials/gforth-sdl-opengl/ * https://github.com/schmx/sdl-forth * https://wiki.libsdl.org/CategoryMouse 
Oh yes I saw [/u/_crc](https://www.reddit.com/u/_crc) doing that too in his retroforth12. Well I tend to be verbose for little things like that, I'm a bit oldschool but I can see its usefulness, thanks. edit: I have added push pop fetch store, I agree it is easier to read.
I'm wondering if there's an issue with the system interface. What happens if you try running: s" dir" system s" set" system 
Thanks for the reply, I won't be able to test the code on Windows untill tomorrow.
So I think the main thing for getting forth working well as a windows programming language is to hook it up to c#. Alot of programming functionality in the design of windows is made so that it should be done in c#, so making your forth compatible with c# objects and libraries will be a good step. One thing that can help you to get started is to install bash for windows.
It's not clear if the issue is that it can't find string.h or gcc. If the issue is with finding gcc, I can't explain that, but if the problem is in not finding string.h, it might be that you're using the wrong environment variable to tell gcc about where to find it. Try putting `C:\MinGW\include` in C_INCLUDE_PATH instead of PATH. Good luck, who knows, maybe if the gforth experience on windows improves enough my brother will finally try it...
There are binary packages available for Windows for those who don't want to attempt compiling themselves at http://www.complang.tuwien.ac.at/forth/gforth Note: 0.7.3 is the latest release, but it's 9 years old and I can't find binaries there for anything newer than 0.7.0. Personally I like to use the snapshots, for which binaries are available. Check here http://www.complang.tuwien.ac.at/forth/gforth/Snapshots/current One of those should be what you're looking for. Disclaimer: I don't know much about Windows. If I'm completely misunderstanding the problem, my apologies.
Thank you, I will try that. 
Light is not something you see, it's the medium of seeing. https://www.reddit.com/r/Forth/comments/6ucu8q/three_forths_make_a_hole_a_story_about_using/
You may have to concede a point: C may produce faster code. Forth is usually fast enough, though. And if it's not, you can make it so. What I see EE people rave about is usually Forth interactiveness. The ability to type commands and test things directly in the device.
Thank you! You haven't misunderstood -- however, I also noticed that gforth hasn't exactly been churning out the updates; is there another choice you'd recommend? I mean, there's no reason it *has* to be updated, I suppose, and I liked that it's supposed to be "classic" in feel and features (and too, not least, compatible with the code in my Forth books), but maybe something else has come along in the meantime.
Thanks! &gt;One thing that can help you to get started is to install bash for windows. For some reason, I've honestly never even considered such a thing; in the process right now. (Related tangent: any opinion on the "Windows trifecta" of SwiftForth, Win32Forth, and gforth?)
Ok, I installed the latest Gforth snapshot (2017-05-07) and reinstalled cygwin and msys32. For what it's worth here is the output of the forth phrases above: 14 [main] dir (4128) C:\msys32\usr\bin\dir.exe: *** fatal error - cygheap b ase mismatch detected - 0x612E8408/0x61298408. This problem is probably due to using incompatible versions of the cygwin DLL. Search for cygwin1.dll using the Windows Start-&gt;Find/Search facility and delete all but the most recent version. The most recent version *should* reside in x:\cygwin\bin, where 'x' is the drive on which you have installed the cygwin distribution. Rebooting is also suggested if you are unable to find another cygwin DLL. ALLUSERSPROFILE='C:\ProgramData' APPDATA='C:\Users\uros\AppData\Roaming' BASH=/cygdrive/c/msys32/usr/bin/sh BASHOPTS=cmdhist:complete_fullquote:extquote:force_fignore:hostcomplete:interact ive_comments:progcomp:promptvars:sourcepath BASH_ALIASES=() BASH_ARGC=() BASH_ARGV=() BASH_CMDS=() BASH_EXECUTION_STRING=set BASH_LINENO=() BASH_SOURCE=() BASH_VERSINFO=([0]="4" [1]="4" [2]="12" [3]="3" [4]="release" [5]="i686-pc-cygwi n") BASH_VERSION='4.4.12(3)-release' COMMONPROGRAMFILES='C:\Program Files\Common Files' COMPUTERNAME=TEST COMSPEC='C:\Windows\system32\cmd.exe' DIRSTACK=() EUID=197608 FP_NO_HOST_CHECK=NO GROUPS=() HOME=/home/uros HOMEDRIVE=C: HOMEPATH='\Users\uros' HOSTNAME=test HOSTTYPE=i686 IFS=' ' LOCALAPPDATA='C:\Users\uros\AppData\Local' LOGONSERVER='\\TEST' MACHTYPE=i686-pc-cygwin MOZ_PLUGIN_PATH='C:\Program Files\Foxit Software\Foxit Reader\plugins\' NUMBER_OF_PROCESSORS=2 OPTERR=1 OPTIND=1 OS=Windows_NT OSTYPE=cygwin PATH=/cygdrive/c/Python34:/cygdrive/c/Python34/Scripts:/cygdrive/c/ProgramData/O racle/Java/javapath:/cygdrive/c/Windows/system32:/cygdrive/c/Windows:/cygdrive/c /Windows/System32/Wbem:/cygdrive/c/Windows/System32/WindowsPowerShell/v1.0:/Skyp e/Phone:/gforth:/cygdrive/c/msys32/usr/bin:/cygdrive/c/cygwin/bin PATHEXT='.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY' POSIXLY_CORRECT=y PPID=5156 PROCESSOR_ARCHITECTURE=x86 PROCESSOR_IDENTIFIER='x86 Family 6 Model 58 Stepping 9, GenuineIntel' PROCESSOR_LEVEL=6 PROCESSOR_REVISION=3a09 PROGRAMFILES='C:\Program Files' PROMPT='$P$G' PS4='+ ' PSModulePath='C:\Windows\system32\WindowsPowerShell\v1.0\Modules\' PUBLIC='C:\Users\Public' PWD=/cygdrive/c/Users/uros/Desktop ProgramData='C:\ProgramData' SESSIONNAME=Console SHELL=/bin/bash SHELLOPTS=braceexpand:hashall:interactive-comments:posix SHLVL=1 SPAWN=' gforth untitled1.fs' SYSTEMDRIVE=C: SYSTEMROOT='C:\Windows' TEMP=/cygdrive/c/Users/uros/AppData/Local/Temp TERM=cygwin TMP=/cygdrive/c/Users/uros/AppData/Local/Temp UID=197608 USERDOMAIN=test USERNAME=uros USERPROFILE='C:\Users\uros' WINDIR='C:\Windows' _=sh autoclose=0 Gforth 0.7.9_20170705, Copyright (C) 1995-2016 Free Software Foundation, Inc. Gforth comes with ABSOLUTELY NO WARRANTY; for details type `license' Type `help' for basic help I have fount two cygwin1.dll files - versions 2008.1.0.0 and 2008.2.0.0 and with either of them I get that cygheap mismatch error. So, still not working. Thank you both for your help, I'll experiment some more with this setup and report here if I find out anything useful.
I might ask about reliability and security; assuming your friend hasn't written his own C compiler, and he doesn't read all of the assembly output, ask him why he thinks that he can trust the C compiler and all of the nasal demons that come along with it? I've written quite a bit of C in my career and while I'll use it when I have to, I *always* lament the need to fight the C compiler to get it to do what I tell it, rather than what it thinks should be faster, but which changes the meaning or behaviour of the program in subtle ways. It's not uncommon to come across a problem-solution which is easy to explain and to implement, but which is impossible to express in C, without running into undefined behavior, making your solution non-portable, or using piles of non-standard extensions. At which point I would note how many errors, and particularly, how many serious security vulnerabilities have been directly caused by the C compiler thinking that it's doing you a favor, or how much time is wasted trying to get just one specific version of a specific C compiler to output the code you want[0] One of my favorite things about Forth, besides its simplicity, flexibility, and expressive power, is that Forth does what you tell it to do. The result is that, with a little thought, you can produce code which is reliable, secure, and portable, and efficient. Writing C feels like taking a Sunday afternoon stroll through a mine field. If you don't know that you're in a mind field you may feel be perfectly comfortable, but once you realize (know not think) that your next step could literally kill you... well... it tends to spoil the mood a bit ;-). [0] And could presumably just write yourself, with relative ease.
I don't think gForth is the way to impress someone to try Forth. It is a great public domain Forth kernel, but it is not built out like a commercial product. The 2 major commercial Forth systems offer free downloads of trial versions. SwiftForth has a nice IDE that makes exploration a little more like other contemporary development systems. My 2 cents. Disclaimer: I have no connection to either Forth Inc. or MPE aside from having met some people who worked there.
There are some great commercial Forths -- I hear a lot of good things about SwiftForth, from Forth Inc. and apparently supports Windows. There's also VFX, from MPE, which, among other things, allows you to create GUI apps. The only other options practical (free) options are, get GForth working, or write your own. GForth is better than nothing, but I really can't say that I'm a fan.
Try http://hub.computersninternet.org/hub/forth 
There may be an ans forth which is ported specifically to windows which may be the best option.
It's hard to convince someone who doesn't want to listen in the first place, wouldn't you agree? If he didn't ask for your help don't try to force him, he will only resist more. He might get intrigued one day and ask you himself.
swiftforth should be easy to install on windows (the free trial version) which should be more than enough to tinker with the language and get some examples working. There's also a couple of forth-on-browser options if you just want a live interpreter. For serious programming work I recommend to make the switch and try out a linux distribution. You can put it in a virtualbox vm to get a feel for it without breaking anything. IMHO don't bother with the linux subsystem and similar crap. Just follow the good devs, they all use linux or bsd whenever possible.
Is there a free Forth that you would recommend? I have pForth and Win32Forth installed alongside Gforth.
Among the free systems Win32forth is at least reasonably complete with an editor, assembler, de-compiler and the Forth console window. But it uses a lot of OOP code in a specific dialect. That's pretty cool in that it shows how Forth is adapted to make an OOP language but it's not "standard" Forth throughout. Most other free Forth systems require a ton of effort to get them to a "commercial" grade system. So I guess I don't have a good recommendation. Nothing to lose IMHO in downloading VFX Forth or Swift Forth for trial purposes.
Thanks!
I was under the impression gforth is GPL, not public domain.
You could consider Forth as a well organized macro assembler.
I finally have a chance to try this approach, so I thought I'd dig out the paper. I'm hoping to write a little about my experience over the next week or so :-).
RIP
Any favs?
http://www.ultratechnology.com/levels.htm
I do not know much about embedded hardware but it seems to me you could eliminate the subroutine instruction. From what I know about subroutines they are pretty much a location in memory which is followed by instructions, some of which may cause you to do a jump to another subroutine. So in theory you could write your subroutines in a way where they routinely check a specific bit to be flipped. If that bit gets flipped then it looks in a special place in storage for the area to jump to. Then all that's necessary is to do one initial jump to get that subroutine going.
any newer articles? 
Well, the author (Jeff Fox) is dead, so, no?
Neat idea, but it doesn't look like Forth specific to me. You could do the same with any other programming language (and say I have a 3-instruction, embedded whatever programming language). What I would consider interesting is to implement a Forth on the host machine that compiles everything what you type in to the memory of the target machine. This doesn't sound difficult if you have an assembler on the host that can compile to the target.
sorry, i mean newer articles about forth
What you describe is just a regular tethered Forth cross compiler, isn't it?
I didn't get around to thanking you for this advice when I should have, because I've been too busy messing with virtualbox and SwiftForth. (Belated) thanks -- not just for the IDE advice, but for the encouragement to give Linux a try!
You're welcome, enjoy the ride!
Hey, you are thinking like Chuck Moore. He did this with his early Forth CPUs. See: https://users.ece.cmu.edu/~koopman/stack_computers/sec4_5.html The other thing to consider is how to return from a subroutine, so before you jump, you have to save your current location. That typically is done by pushing the CPU program counter + 1CELL onto the return stack. The "return from subroutine instruction" then just pops that address off the return stack back into the program counter and the program continues. 
http://video.fed.wiki.org/view/welcome-visitors/view/federated-wiki-videos Federated wikis have no need for continuous consensus and so they must be a good fit for Forthers.
Well if you've got a fetch from storage routine you can add a return bit and if that bit gets flipped you write to the return location then fetch storage from there.
https://wiki.forth-ev.de/doku.php/events:tagung-2017:federated-wiki [video, in German]
Why use enums? You could just as well use the xt of the relevant state-handler, and use execute (or whatever it is in ANS Forth) to invoke that word's xt. Then you don't have to have more code just to provide enums which you don't really have to have.
The use of enums is so that later on if I'm in the repl and I want to set the machine into the dog state I don't have to go look through my history and see which number was dog. If you wanted to though you could skip the enum part. It only adds a few bytes of bulk.
' dog state !
I do not understand what you mean by your last remark. I don't doubt it can be done without enums. I simply like to use it with this style of enum.
I mean to say that that is how you would set the 'state' (I know, a different name is needed there) to be "dog-state" for example. Since the state is implicitly the action (e.g. xt) which will be performed, simply using the xt directly as in "' dog" is exactly the same as declaring an enum to represent that action.
The RTX2000 did the return operation for free. Any instruction that had the return bit set just did it. BTW the way the storage for CALL is in the instruction itself (15 bit address). The storage for return is the return stack so that is how it can be implicit in the return instruction. I don't think anyone had ever done it like that before Chuck.(not 100% certain about that)
In your second example, it'd be better to do: create my-array ' state-one , ' state-two , ' state-three , This is more readable and less prone to errors due to manually counting offsets.
You're using the word once every time through the loop, but you're not duplicating it. Try : color-code ( addr -- ) 3 0 do .( i = ) i . DUP i cells + @ . cr loop DROP ; (capitalised additions) There are other ways to solve that problem, but that's the most direct.
Thanks, I appreciate it.
A macro like you want for "state" will likely be difficult to achieve if you want to do this portably. In theory you could try using EVALUATE to do this, but you'll run into issues with the conditional structures. Specifically, control and defining words can leave things on the stack until the definition is finished; working around these will be problematic. It may be feasible to construct a string for EVALUATE, but this runs into text manipulation, which is not one of the strengths of ANS Forth. (IIRC, Forth200x extensions to the standard have some words that may be useful with regards to parsing and manipulating the input stream, but I'm less familiar with that spec) If I had to do something like this, I'd prefer the second approach of using a lookup table; it's a lot less complex; likely faster; and works extremely well for values within a limited range. References: 3.2.3.2, 6.1.1700, 6.1.1310, 6.1.1360, 6.1.2270 (in ANS spec, see [http://forthworks.com/forth/standards/DPANS/DPANS.txt](http://forthworks.com/forth/standards/DPANS/DPANS.txt) or [gopher://forthworks.com/forth/standards/DPANS/DPANS.txt](gopher://forthworks.com/forth/standards/DPANS/DPANS.txt)) One other thing: if you preface your code with four spaces it'll make reading future posts easier as the code blocks will be formatted differently than the body text. Without this things are a little harder to parse.
Thanks
Welcome to r/Forth. u/thamesynne mentioned other ways to solve your problem, so I will contribute the one I prefer. A common iteration idiom is to make your loop index the same as your addresses. Add your beginning address to the loop bounds up front. It's more efficient and not conceptually harder. : color-code ( addr -- ) dup 3 + swap do .( i = ) i . i @ . cr loop ;
Thank you.
Except that that code will only work if CELLS = 1; and the ability to print I is disrupted, because it's now an address.
A common way to do this is to use the phrase "OVER + SWAP" Given: create red 255 , 0 , 0 , You can show the contents with : RED 3 OVER + SWAP DO I @ . LOOP This short phrase is often defined as: : BOUNDS ( adr u -- limit index) OVER + SWAP ; 
Thanks, I'm still trying to make sense of all this :-)
That will also only work when CELLS = 1. On almost any system that gforth runs on, that's not the case, and you'll end up either fetching cells a byte apart or causing a misalignment exception.
You are correct. My code only works when advancing a byte at a time rather than a cell at a time, and so is not exactly right for u/Joe_Anonimista's purpose. Correct code for advancing a cell at a time would be: : color-code ( addr -- ) dup 3 cells + swap do .( i = ) i . i @ . cr 1 cells +loop Thank you for pointing that out.
&gt; Computers at their core are register machines. Registers, or memory, are designed for extremely fast and sequential random access and random write. This is the fastest part about the computer and should be utilized heavily. I agree with much of what you wrote about the use of the stack and how we should approach programming in Forth, but the statement above is not true. It's certainly the common case today, but it's simply not necessarily the case that computers, in general, are register machines at their core. Real stack machines do exist. Much of the Forth hardware (which is admittedly a niche) falls into this category. 
I think people bring over conventional anti-global variables opinion to Forth directing them to try the opposite, mistaking the stack for a heap.
While a deep stack does sound like an anti-pattern, I don't understand the argument that using it would be slower unless it requires many additional stack operations per element. Or are you just talking about unnecessarily copying into the stack data that is already accessible elsewhere? If I had words `range ( u -- n1 n2 ... nu )` and `sum ( n1 n2 ... nu u -- n )` that pushed then summed numbers 1 to u, what would be the performance penalty for working from the stack? I'm still trying to grok forth, so be gentle.
A good example of what I mean is how programming languages like haskell scheme and lisp are a little bit slower than languages like c and C++. C and c++ work on mutable data, whereas when you program immutably using functions you end up doing more processing per computational result. This processing slows things down a bit depending on how abstracted you are. That being said immutable programming can be fast. i am simply saying that it is good to take advantage of the fact that memory is random access and is very fast to mutate.
Supercomputer with Forth in PROM - the highest point ever reached by human civilization.
Haskell has a heavier runtime than C++ which has a heavier runtime than Forth. Copy semantics (as in immutability or map/reduce chaining) is overhead. Both are sacrifices made to the Gods of Correctness, and the debate over their ROI is longstanding. What has that got to do with your original stack versus register argument? You can't keep an array in registers -- elements will always be fetched from memory -- so why would it make a difference in Forth whether than memory was the stack or cells? And in MPUs (though not MCUs), processing is MUCH faster than memory. Confused. 
Is there a performance penalty for overusing the stack, or is it just not idiomatic?
Memory is made out of registers, first of all. Secondly my point is not that the stack is slow but the stack does not offer as immediate of a benefit as a table lookup.
&gt; Memory is made out of registers, first of all. That is a very machine dependent assertion. Not every/many machines support the sort rich set of memory-memory instructions required to say all memory locations are registers. You might have to copy one or both arguments from high memory into proper registers just to be able to add them, then move the result back into high memory to free the register(s) up. A table/array is certainly more flexible than working from the stack (which pretty much only lends itself to a single pass), but if the latter is all you need, it seems like it will be faster than the table due fewer memory operations per element (unless forth allows you to exploit base register indexing for arrays in a portable way that I don't know). In fact, it seems like it would be easy to make a non-portable assembly word that uses a base register to copy an array that already exists in memory onto the stack to be reduced in a way that saves a few memory operations BECAUSE it uses the stack. 
Let me just say that it would be very easy to modify the dictionary to a tree-like structure, so that finding words would be very possible and easy; in other words, spaces would no longer be required at all, and in fact may need to be defined as no-op words. Depending on how strict your matching criteria is, it could even throw errors if it encounters an incomplete word. Although this means the dictionary would also lose the stack-like property of being able to "cover up" previously defined words, to me this merely indicates that a word's name and function become uncoupled. Whether you care to juggle word definitions (and their names) then becomes a matter of style. If this leaves you with a bitter taste, a clever search function can still find a best match in the traditional dictionary structure (single-linked list), but the search then becomes highly inefficient due to the non-linearity of the search.
One thing that sets Forth apart from other languages is it doesn't spend much time with parsing theory. Yes it is possible to use Forth to make a compiler or interpreter using Forth. But I think Chuck would say, just use it to solve a real problem, not a created problem.
I think that this feature could be built on top off ordinary forth. You could create a separate dictionary which works as you proposed and make an interpreter which parses the input stream and uses that separate dictionary. The way I was planning to get rid of spaces was to use a finite state machine but perhaps your way would work. The reason I was considering a finite state machine is because it is quick decision making and because when you design it you must make sure there are no syntax conflicts. But this could similarly be done by the computer. It could check your separate dictionary when you define a new word to see if there will be a syntax conflict.
I agree that you can use forth to solve problems but I would say for some people the thing they want to do is to create a programming language. And forth is an excellent way to do this.
I find this idea very interesting. When will it ship?
This isn't always true. RAM is typically implemented differently from registers and is generally slower. A couple of potentially useful articles/papers regarding memory and registers: * [https://mikeash.com/pyblog/friday-qa-2013-10-11-why-registers-are-fast-and-ram-is-slow.html](https://mikeash.com/pyblog/friday-qa-2013-10-11-why-registers-are-fast-and-ram-is-slow.html) * [https://www.akkadia.org/drepper/cpumemory.pdf](https://www.akkadia.org/drepper/cpumemory.pdf) 
In other words: the problem you're trying to solve is the creation of a programming language? EDIT: maybe /r/concatenative is more receptive to discussion?
I brought up a similar idea in the #forth irc channel a while ago. The idea is that words can be named after things other than passive text strings. http://forthworks.com/forth/irc-logs/17.02.03 - A word could be named after a keypress. This makes implementation of text editors much simpler. It's also why I'm doing away with line-buffering-by-default in my forth. Words are executed as soon as they can be executed. Buffering can still be done explicitly. - There's a lot of other sensors and input devices that could be directly writing forth code as well. Accelerometers, water level, temperature, whatever. I don't think the outer interpreter need only be for the keyboard. - You could write a word named after whatever seisomograph readings a seismologist would identify as an earthquake. I know nothing about seismology but I would think there is some interpretation required and it's not a single literal. This is where parse-time execution comes in.
The great thing about FORTH is that you can mold it into anything you need or want.
&gt; A word could be named after a keypress. This makes implementation of text editors much simpler. This sounds pretty cool &gt; I'm doing away with line-buffering-by-default in my forth. Words are executed as soon as they can be executed. I'm not sure if I understand correctly: do you mean for human input? Because that sounds a bit risky: remember when Swift came out and wasn't sandboxed, [and the run-as-you-type feature was pretty dangerous](http://www.mactrast.com/2014/06/careful-apples-new-swift-playground-run-type-can-dangerous/)? Also, what about typos?
The outer interpreter need not be just for human input. I might have a water level sensor that talks to the outer interpreter as well. I might have a word named after sensor outputs corresponding to flood levels. When the sensor sends that output to the outer interpreter it's writing a little bit of forth code that will turn on the flood alarm. I wouldn't want that to be buffered. For human input at a prompt I would want to use line buffering. The idea is that how something is buffered should be under the control of the user/developer/code/peripherals. For a prompt I would want line buffering that is terminated by an unbuffered enter key word. For a text editor I would want the arrow keys to be unbuffered. Naturally I would still want to do some kind of buffering of the file in the text editor, maybe it would be a gap buffer or a rope -- but it wouldn't be line buffering. That kind of buffering/state is for a forth prompt. I want to do my own buffering so it fits the problem at hand.
Forth already does this.
Well any concatenative language should be able to serve as a syntax machine for creating languages. I think that forth is particularly suited for this because forth is good as a runtime.
don't know, imagine it depend on the implementation. y, for me it would just be idiomatic, i wouldn't be able to get the other style to work, be interested in any conclusions you come to if you follow up on this style, wonder if this is the style of the other concatenative lang's 
If you're implementing Forth in software, you can set the stack pointers and have as many stacks as you can manage, *without* needing a dozen more stack specific or parameterized words. This is how Forth has worked forever. That said it's trivial to implement separate application specific stacks if needed. I've said it once, and I'll say it again (and I understand that maybe this is just your way of learning): maybe you should learn Forth before you try to improve it. Or maybe implement your idea's so you see the tradeoffs you're making.
I think that this is just a difference in the way that we would design it. It may be more efficient, though, to use some underlying feature designed to create more stacks.
&gt; It may be more efficient, though, to use some underlying feature designed to create more stacks. Why/how would it be more efficient? We're talking about updating a pointer. You'll be hard pressed to come up with something more efficient than that. In your proposal you mention searching for and reclaiming unused cells before you can push a value to some stack. Do you honestly think that doing that is more efficient than setting a variable?[0] This is why I suggested that you should try and implement some of your ideas. As it stands you're arguing from ignorance; many of your beliefs about Forth (from the perspective of someone who uses Forth every day and has been working on and in Forth for several years) seem to to be based on unfounded assumptions, or are outright assertions with (and hopefully without being too harsh here) little to no basis in reality. [0] Or often, a register
Then by all means show your implementation. I have shown mine in my edit of the post. If you can implement the spool and unspool words with your implementation by all means do it.
When I read your post I find myself wondering why you need deep stacks in Forth. Research has shown even complex apps seldom need no more than 20 items on the parameter stack, however it's an interesting exercise. Regarding 2 item strings, this is a relatively recent development in Forth history. Previously strings in Forth were very much like Pascal or Modula using a count byte at the front with the text following. These are used with just the address to the string on the stack. ANS Forth still assumes this structure in memory but uses the addr,len format on the stack. The 2 item system provides some interesting speed increases when manipulating strings because you just move pointers to the address and len to get sub-sets of the string. I like to use both methods to best affect in my code. Manipulating 2 items for a single data element in Forth is a pain IMHO. Check out the words: ( I am used descriptive comment names) COUNT ( string -- start-addr len) PLACE ( addr len string_addr -- ) \ put 2 item string into memory with count Also check out the late Wil Baden's (AKA Neil Baud) tool set and string words for some ideas on using strings. I find them helpful. http://www.wilbaden.com/neil_bawd/tool2002.txt
/me sigh Deleting posts and making another again? Is criticism that hard for you to deal with that you have to censor any disagreement and waste the time of those who have taken the time to reply to you? Now you have the nerve to demand that I waste more time implementing things which you didn't even bother to implement yourself? This will likely be the last of your posts which I bother to read or respond to because I find this behavior both disingenuous and childish. If I had this problem then I would do: variable sps SOMEWHERE sps ! : pushs ( sp -- ) sp @ sps @ ! sp ! sps 1 cells +! ; : pops sps 1 cells -! sps @ @ sp ! ; Here you have a stack of stack pointers, used to manage multiple stacks, which you could use for local storage[0]... except that no Forth programmer in their right mind would do this because it's not a problem that ever comes up. If you want random access to different stacks, then all you have to do is store the stack pointer in a variable and restore it. You just activate the stack you want, push or pop the values, then activate another stack. Best wishes. [0] This is based on my understanding and recollection of what you originally wrote and has now been deleted. You can, of course, use this technique for other things. Copying multiple values from one stack to another, or more concretely, swapping multiple values is a simple extension etc.
One the coolest pieces of Forth code for FSMs was created by the late Dr. Julian noble. It uses the Forth principal "create a language to solve the problem". Dr. Noble's solution creates a mini language that lets you write your FSM as a table of text and then Forth compiles the FSM from the table. It's genius. :-) http://galileo.phys.virginia.edu/classes/551.jvn.fall01/fsm.html This is how you "Use the Forth Luke" Warning: the paper does not use ANS 94 Forth code. It looks more like Forth 83. Needs translation.
This is a continuation of a deleted thread. Sadly /u/read_harder's words are lost to history, but I'd like to preserve mine: https://www.reddit.com/r/Forth/comments/6ylx9n/how_to_have_many_stacks_instead_of_one_deep_stack/
I deleted it because after implementing what I was talking about I felt it needed to be completely reworded.
Is there a particular reason that the two explanations can't live side by side?
What is the significance of the magic numbers 8 and 16 in this?
Thank you for preserving this
How does "idle" compare to Forth's PAUSE word? PAUSE is not in the standard because the committee could not agree on multi-tasking but it has been used for 40 years or so as the context switcher in multi-tasking Forth systems. From Wikipedia:"The word PAUSE is used to save the current task's execution context, to locate the next task, and restore its execution context. Each task has its own stacks, private copies of some control variables and a scratch area. Swapping tasks is simple and efficient; as a result, Forth multitaskers are available even on very simple microcontrollers, such as the Intel 8051, Atmel AVR, and TI MSP430" 
They're not related, Snabel turns the scheduler inside out to allow tasks to be put in lists, passed around and processed like any other coroutine, they give up control by YIELDing. The reason IDLE exists is all about IO; when you're running a non-blocking IO-loop you need a way to wait for activity, otherwise you'll trash the cpu doing nothing.
`idle` looks like `wait` in our Forth system. `wait` causes the system to sleep until an external process kicks it. While the system is asleep, an external process is free to mess with its state, and this is how we implement all I/O. Unlike `idle`, `wait` doesn't switch tasks (although it could[0]), it simply `waits`, consuming no "energy" (meaning time, in a software implementation). [0] We don't have a task abstraction; where this is desirable, multiple isolated systems communicate.
Cool. Just wanted to note that like WAIT, IDLE doesn't switch tasks; not that Snabel is aware of at least; but since it may call poll with a timeout, anything can happen on the OS-level.
:-) Thanks for the clarification about `idle` and scheduling; it seems that our `wait` *is* (exactly?!) your `idle`
I'm fine with that as long as we can agree to disagree on the name :)
cell produces 8 and 2 cells produces 16
It would clog the subreddit.
Then do what everyone else does and clarify your position in the comments? You're going to make mistakes. Leave your posts in place. You may have raised the topic but the discussions you're deleting don't belong to you alone. When myself and others spend our time considering your positions and writing replies and you just delete our posts, not only are you wasted our time but you're preventing others from benefiting from these discussions in the future. There's no good reason for deleting them and posting the same topic, with a different introduction, and none of the discussion.
So it's hard coded for a specific cell size and not portable. 
:) Absolutely. The names we choose are very personal things, and there's no reason that we can't use the names we like. I like `idle`, but I've been calling it `wait` for years, so I'll stick with that for now.
Noted. I'll change it.
it looks the actor model was very suit forth
re: ANS core and words not specifically needed: section 3 of the standard explicitly allows for the core words to be provided as separate sources able to be loaded in as desired. You could have a very non-standard forth that becomes ans compliant when desired. (In my forth's 9th incarnation this was done). Edit: forgot to include the section number that allows for this. (That's what I get for replying at midnight...)
Perhaps a good strategy is to make a kernel which starts out suitable for small micro controller but which can become ans forth complaint in as quickly as possible. 
I did have an ANS-compatibility layer in Reva Forth. But in 8th I figured that most of my users were actually *not* Forthers but people new to the language, and it was needless effort to provide an ANS layer. After all, Forthers could adapt easily to 8th; while an ANS layer would give no benefit to the rest of my users. Just my 2p.
So what words from CORE would you drop? [http://quartus.net/files/PalmOS/Forth/Docs/stdref.pdf](http://quartus.net/files/PalmOS/Forth/Docs/stdref.pdf) lists the words in ANS in a concise format. It might be worth looking at other bases if you want to build from a small starting point. eForth typically builds from around 30 primitives, and there are at least a few others in a similar base size. There's no reason to stick to ANS FORTH as a base if you dislike it. Forth dialects offer lots of options. That said, while I personally have no use for ANS FORTH as a standard, it's not that big. the CORE wordset is 133 words, and many of them can be provided separately if desired. Looking at my old retro9 sources; the binary was about 12k for linux. It could run in a very small amount of RAM. Adding in ANS support increased memory by about 10K. I'm sure it'd be feasible to write an aNS compliant Forth supporting all of CORE and at least some of the optional wordsets in 32-64k of RAM if this was needed.
An example of a word that could be cut from the core is 2drop. AFAIK 2drop can be simply implemented as : 2drop drop drop ; Any electrical engineer could add this word back in if they wanted to, but it isn't absolutely necessary to have the core, so why include it? Keep it in for the high levelers but make a core without 2drop. Some other examples: 0= 2* cell+ ." Any of these examples can be re-implemented and so the core doesn't truly need them. What the core needs is words which if you took them from the core it would not be able to be re-implemented by just writing a word.
A lot of things like this are there for performance purposes, which you imply support of: &gt; I also think that there should not be compromises in speed in order to &gt; gain smallness. In other words if it takes some extra code to make a &gt; faster forth I think that extra code is worth it. You might find this paper (pdf) interesting: [Minimal Forth by Peter Knaggs and Paul E. Bennett](http://www.complang.tuwien.ac.at/anton/euroforth/ef15/papers/knaggs.pdf) It presents a proposed cut down core wordlist (down to 69 words).
I suppose it depends on how much performance is lost. It's a careful game, making standards.
This is a game which can only be won when you know the state of the board, which is an argument against a standard, but if I were going to try then I'd start with machine Forth[0]. I'm also a big fan of /u/larsbrinkhoff work on "nybble" as a very minimal Forth, from an intellectual point of view :-). But to be clear, I don't think that a new standard is necessary, and I feel like we're only new dragging ourselves out of the rut that ANS forth put us in. I would need a much better argument. My $0.02 [0] And the ISA developed by Charles Moore and Jeff Fox for Forth hardware. It was the starting point for our Forth, which has a core of 32 words that were chosen to take advantage of typical mobile, desktop, and server hardware. These words are sufficient but numerous extensions are added for efficiency.
You're free to discuss it in the new thread. I don't feel bad for wasting the possibly three minutes it took you to write a reply.
&gt; I don't feel bad for wasting the possibly three minutes it took you to write a reply. In this particular case, but this isn't the first or only time that you've done this and I'm not the only person whose thoughts that have been lost in the process. It's not my intention to make you feel bad, but I'm sure we would all appreciate it if, in the future, you could refrain from starting discussions just to delete them and repost the same topic with a different introduction. Without getting defensive, Is that at least understandable?
That is understandable, however I think in this situation it was better to delete it. In my other discussions I kept them up because the ideas I was talking about where very difficult to implement in an explanatory way. Whereas once I had implemented my idea here I knew I could explain it better. My goal was not to preserve the state of things but to demonstrate my idea as clearly as possible so that it could be used.
I have always wondered why the Forth community has not created a pair of words like: FROM &lt;filename&gt; IMPORT &lt;wordname&gt; as seen in Modula 2. I can see this being very beneficial in a cross-compiler, less valuable to an embedded Forth Compiler/interpreter due to complexity An alternative approach would be to define a clear hierarchy of words that starts with the minimum required to bootstrap the system and then defining a set of file names that bring in more words in a dependent order. However this approach could never satisfy ever situation optimally and so Forth will be Forth I suspect. 
I started reading it. FORTH's internal behaviour is likely to be very difficult to understand for anyone who hasn't used GOTO and is forced to rely on canned loop constructs or if-then-else, which is most programmers these days. Assembly doesn't have if at all; subroutines are chosen as a result of cmp tests and whether or not the zero flag is set as a result. The jz function runs if it is, and jnz runs if it isn't. Loops are formed from two functions bouncing back and forth with GOTO, like a metronome in music. Every FORTH word is a seperate subroutine; all "threading" basically means is that each subroutine is called by NEXT, one after the other. NEXT pulls the memory address of the next word to run off the return stack, then calls lodsw or the equivalent assembly code to put it into memory. This is voodoo for modern programmers; it's extremely empowering, which is why the psychopaths don't want to let you do it. I use something vaguely similar in Lua programming in Minecraft, although Lua is deeply broken and doesn't go close to really letting you imitate FORTH. I consider [this](http://www.forth.org/fd/FD-V13N6.pdf) to be FORTH's answer to the Rosetta Stone. It's a side by side comparison of nineteen different FORTH implementations. If you really want to understand FORTH's internal workings, it's a good article to read; for me it was better than the FORTH standards for figuring out which baseline words are in pretty much every FORTH. It has code fragments for most of the standard words, as well. They aren't in x86 assembly as such, but if you're at all familiar with asm, translation should not be difficult.
I appreciate the lengthy post and link! Thanks!
I've been reading it after picking up a used copy of it. I too wish there was a new edition that covered ARM, but Z80 assembler isn't too hard to read. My goal is a good baremetal ARM Forth for the Beaglebone Black. 
That's really cool. Is there anyway you could post the code on github or do a YouTube video explaining your design choices? I bet I would learn a quite a bit!
Why, thank you. You're right that it's mostly an intellectual exercise to reduce Forth to a minimal set of primitives. It's not very useful in the real word.
I haven't read it, but I've played around with Forth in many different assembly languages. Anything I can do to help? No, I won't translate the entire book. ;-)
I have written a Forth assembler for ARM, and then used it to port my Forth to ARM Linux. I think the ARM was quite pleasant to work with, so your goal is very reasonable.
I like the beginning chapters of *Threaded Interpretive Languages*, but for explanations of how each word is implemented [*Forth Encyclopedia*](http://odroid.0xffffffff.in/~deploy/fig-book-of-the-month/) was useful. The latter book is more architecture neutral because everything is shown with flowcharts in high level forth except for a few primitives like `DROP` which were explained without code.
I would love to see that, if you did it as open source! That would help with a few things I'm getting stuck on (specifically implementing DEFER and POSTPONE properly)
Thanks for the link. Can one still buy those? It looks like the PDF was only a few pages (might have been my phone though).
I've read a bit of Forth tutorials, so I'm aware of some of the concepts, but mostly need help in the bootstrap phase. If I'm starting with assembly or C, what do I need to implement first? I know there is an inner and outer interpreter...etc. If you search for Forth on Youtube there are only ~3 videos. One is a speech by Chuck and the other two show people building things in GForth. I'm surprised there isn't anything showing more of a step by step this is how you build a very simple Forth. Implementing all of the basic ~100 words needn't be done of course as long as it's shown how to add one as in the primitive language like assembly or actually in Forth. The folks on here have all implemented many Forth systems and I'm sure are some amazing software developers. It's completely self serving on my part to ask for this of course :) but I think this would help Forth get a lot more exposure as most have never heard of it and those that take a peek into it find a much much higher barrier to entry than in any other language I can think of (not counting GForth, which is a simple install from the package manager). Let me know if you're interested in something like this. If not, no biggie. If we could email some notes, I could even do a video myself if I can just get past the bootstrap phase. User Dlyund has been a massive help in answering a ton of conceptual questions for me in the past on a wide range of programming topics from Forth to Smalltalk. I wonder if I grabbed a few of the more prominent posters in this group and gathered some of the available wisdom it might serve the community. Sorry for the rant!
Yes, if not on Github I would even be happy with it included in an email or PM as long as you are ok with that. I'll probably have some questions of course.
I haven't seen any on amazon, but if you're interested, I have a few hundred copies of it in storage. I could send you a copy for the price of shipping + the price of the padded envelope + $10 (only because I'm trying to recover some of the costs of keeping 120sqft of Forth books in storage). Or, if you happen to be in the SF Bay Area you could come to a Silicon Valley Forth Interest Group meeting :)
Unfortunately I don't live that close to civilization, but I'll keep your offer in mind. Do you also sell the other Forth books on that site? If not, is there a list somewhere?
https://github.com/16Bitt/forthstrap/blob/master/src/generic.forth this is pretty terrible (this is precompiled into the binary so some words aren't available) but eval and interp handle this step in my forth
Sure! The ARM bits are in https://github.com/larsbrinkhoff/lbForth/tree/master/targets/arm There's a small assembly language nucleus. The rest, including DEFER and POSTPONE, is in Forth.
See answer to /u/retrodaemon. Do ask away. Maybe start a new thread?
I can think of two tutorial-like resources for building a Forth from scratch. One is "jonesforth" (find it on GitHub), which is an actual implementation with plenty of comments in a literal programming style. The other is Brad Rodriguez series 'Moving Forth". http://www.bradrodriguez.com/papers/moving1.htm I don't have to be an amazing developer to write your own Forth. Take it from me. I was a very bad Forth programmer indeed when I started out, but it grew on me.
Depends on what you mean. ANS Forth and 8th both allow you to restrict the words available to the REPL (and therefore eval), so you can do a form of sandboxing.
&gt; Bonus credits for implementations that take sand-boxing into account. Sandboxing and Forth are kind of antithetical. However, EVALUATE is a member of the Core wordset, and in fact forms the core of the Forth interpreter (which is conceptually not much more than BEGIN tbuf DUP maxin ACCEPT EVALUATE ." ok" CR AGAIN where tbuf and maxin are not standard variables). Wil Baden used textual evaluation extensively in his programming, and other Forth coders were somewhat critical of that, noting that the results it gave would be dependent on the definitions of the given words at the time EVALUATE was called. I'm not sure he saw that as a disadvantage...
Sorry, I edited it extensively &amp; didn't see you'd replied until too late. Replacement comment is [here](https://www.reddit.com/r/Forth/comments/6z8omj/any_references_for_implementations_of_eval_as_in/dmtebuo/).
Here's an implementation of `evaluate` for Retro: * HTTP: [forthworks.com/temp/evaluate.forth](http://forthworks.com/temp/evaluate.forth) * Gopher: gopher://forthworks.com/temp/evaluate.forth It's very much not standard.
Thank you! I'll start a questions thread once I get a chance to dive in.
I have everything that's on that web page. I've also been putting up a new title every month.
Thanks! I guess you're just trying to do the Forth community a service?
Yea, [the materials came from Glenn Haydon, and before that Mountain View Press](https://goo.gl/photos/kEN9WW7oYZmYfyAE9). I'm just about breaking even with it.
me too, want to read a modern assembly lang version, or maybe in some popular educational assembly language like MMIX or DLX
You can also see how Brad implements a generic Forth system called CAMEL Forth. There is code in Assembler for 8086, MSP430 and others. Here is the 8086 link. http://www.camelforth.com/page.php?7 Also I have made a cross-compiler that builds a Camel Forth system for the TI-99 for my own education, which is heavily commented. It let's you see how I implemented code primitives and then compile the rest of the system as Forth code. It might be helpful (?) Note: The assembler code is Forth Assembler meaning the instructions are on the right side of the operands. The ASM primitives are here: https://github.com/bfox9900/CAMEL99/blob/master/9900FAST.HSF The high level Forth is here: https://github.com/bfox9900/CAMEL99/blob/master/CAMEL99.HSF
Mine is incredibly simple compared to what /u/larsbrinkhoff has built, but you can see it here: [Plaid FORTH](https://github.com/projectplaid/PlaidFORTH)
Simple is good!
We salute you Chuck. May the next generation take on the banner.
Why not link the whole discussion where this answer is questioned? While it might be true don't believe everything on the internet on first sight :) https://groups.google.com/forum/#!msg/comp.lang.forth/LmthCiZ4Cjk/f-e03GHXAQAJ
The other people seem to question Elizabeth Rather's claim however I don't think Rather has any reason to lie. Elizabeth Rather was the second person to ever learn Forth after Chuck introduced it to her, and they have worked closely for many years.
Because that would appear to give equal credence to Liz Rather, who worked closely with Moore for many years and might well be assumed to know him socially too, and some random shitflinger whose only identity is a gmail address. ~~Unless the shitflinger in question is...~~ no, never mind.
I only see one person questioning the claim. I trust Elizabeth more than that person.
I guess that's to be expected at this point but it makes me sad to think that somewhere, out there, the creator has retired from his creation. Still, I wish him all the joy life brings
I see, thanks for the explanation
Indeed.
Somebody smart has got to step into his shoes and be the new leader of a dwindling people.
With the **[mindforth source code](https://ai.neocities.org/mindforth.txt)** and its sister **[Perl AI](https://ai.neocities.org/perlmind.txt)**, Forth has a role to play in artificial intelligence. * [MainLoop](http://ai.neocities.org/MainLoop.html) * --TabulaRasa -- wipe memory clean * --[MindBoot](http://mind.sourceforge.net/enboot.html) of English and Russian vocabulary * ----KbLoad -- load the Knowledge Base * --ReJuvenate -- recycle memory space * --Sensorium (SensoryInput) * ----AudInput -- auditory input * ------[AudListen](http://github.com/kernc/mindforth/blob/master/wiki/AudListen.wiki) -- listen for input * --------AudMem -- English auditory memory * ----------[AudRecog](http://github.com/kernc/mindforth/blob/master/wiki/AudRecog.wiki) -- auditory recognition * --------RuAudMem -- Russian auditory memory * ----------RuAudRecog -- Russian auditory recognition * ------[OldConcept](http://github.com/kernc/mindforth/blob/master/wiki/OldConcept.wiki) -- recognize a known old concept * --------Parser -- expect first a noun, then a verb * ----------[InStantiate](http://github.com/kernc/mindforth/blob/master/wiki/InStantiate.wiki) -- create conceptual engram * ------------EnParser -- preposition? (in)direct object? * ------[NewConcept](http://github.com/kernc/mindforth/blob/master/wiki/NewConcept.wiki) -- create new concept * ----FileInput -- read files on a server * --[Volition FreeWill](http://mind.sourceforge.net/volition.html) * ----[Emotion](http://github.com/kernc/mindforth/blob/master/wiki/EmotiOn.wiki) as influence on thought and free will * ----EnThink -- think in English * ------[InFerence](http://ai.neocities.org/InFerence.html) -- automated reasoning * --------[AskUser](http://github.com/kernc/mindforth/blob/master/wiki/AskUser.wiki) -- validate an inference * ------[EnNounPhrase](http://github.com/kernc/mindforth/blob/master/wiki/NounPhrase.wiki) -- English noun phrase * --------[EnPrep](http://github.com/kernc/mindforth/blob/master/wiki/EnPrep.wiki) -- insert English preposition * --------[EnArticle](http://github.com/kernc/mindforth/blob/master/wiki/EnArticle.wiki) -- insert English article * --------[EnPronoun](http://github.com/kernc/mindforth/blob/master/wiki/EnPronoun.wiki) -- substitute a pronoun for a noun * --------[ConJoin](http://github.com/kernc/mindforth/blob/master/wiki/ConJoin.wiki) -- insert conjunction * ------[EnVerbPhrase](http://github.com/kernc/mindforth/blob/master/wiki/VerbPhrase.wiki) -- English verb phrase * --------[EnAuxverb](http://github.com/kernc/mindforth/blob/master/wiki/AuxVerb.wiki) -- insert auxiliary verb * --------[EnAdverb](http://github.com/kernc/mindforth/blob/master/wiki/EnAdverb.wiki) modifies a verb * ----------EnAdverb -- modifies another adverb * ------[ConJoin](http://github.com/kernc/mindforth/blob/master/wiki/ConJoin.wiki) -- insert conjunction * ----[RuThink](http://ai.neocities.org/RuThink.html) -- think in Russian * ------RuNounPhrase - think with a Russian noun * --------RuPrep -- insert a Russian preposition * --------RuPronoun -- substitute a Russian pronoun * ------RuVerbPhrase - think with a Russian verb * --------RuAdverb -- insert a Russian adverb * --------[RuVerbGen](http://github.com/kernc/mindforth/blob/master/wiki/VerbGen.wiki) generates a Russian verb-form * ----------[AudBuffer](http://github.com/kernc/mindforth/blob/master/wiki/AudBuffer.wiki) stores Russian phonemes * ----------[OutBuffer](http://github.com/kernc/mindforth/blob/master/wiki/OutBuffer.wiki) manipulates phonemes * ------[Speech](http://github.com/kernc/mindforth/blob/master/wiki/SpeechAct.wiki) -- output to screen or loudspeaker * ----[PsiDecay](http://github.com/kernc/mindforth/blob/master/wiki/PsiDecay.wiki) lowers conceptual activation * ----[SpreadAct](http://github.com/kernc/mindforth/blob/master/wiki/SpreadAct.wiki) -- spreading activation * ----[MetEmPsychosis](http://ai.neocities.ord/MetEmPsychosis.html) -- AI soul travel * ----MindMeld -- merge two AI Minds * ----[Motorium](http://github.com/kernc/mindforth/blob/master/wiki/MotorOutput.wiki) -- for robots 
 &gt; Forth almost died. The powers that be who had hired Moore sent in &gt; Elizabeth Rather to try to understand his code so that it could be &gt; rewritten in a different language. To Rather's surprise she grew to &gt; love forth and see that it was a superior computing system. She and &gt; Moore went on to promote forth in the computer industry. Do you have a source for this? It doesn't match the notes in "Forth - The Early Years" (Chuck Moore, 1991), where he wrote: &gt; The system was well-received in Tucson, where Ned Conklin was in &gt; charge. It did advance the state of-the-art in on-line data reduction. &gt; Astronomers used it to discover and map inter-stellar molecules just &gt; as that became hot research. &gt; Bess Rather was hired to provide on-site support. She had first to &gt; learn the Forth system and then explain and document it, with minimal &gt; help from me. The next year I reprogrammed the DDP-116 to optimize &gt; telescope pointing. The next, Bess and I replaced the 116 and 316 with &gt; a DEC PDP-11. It also doesn't quite match the description in the "Evolution of Forth" (Elizabeth Rather, Donald Coblurn, and Charles Moore): &gt; Edward K. Conklin, head of the Tucson division of NRAO which operated &gt; the 11-meter telescope, found it difficult to maintain the software &gt; since Moore was based at NRAO’s headquarters in Charlottesville, VA. &gt; So in 1971 he brought in Elizabeth Rather, a systems analyst at the &gt; University of Arizona, to provide local support on a part time basis. &gt; Rather was appalled to find this critical system written in a unique &gt; language, undocumented and known to only one human. Her instinctive &gt; reaction was to re-write the whole thing in FORTRAN to get it under &gt; control. Alas, however, there was neither time nor budget for this, &gt; so she set out to learn and document the system as best she could. This makes it clear that Elizabeth was brought in to provide support, not to do a rewrite in another language (which was *her* initial idea). References: * [http://www.wulfden.org/downloads/Forth_Resources/CM_Invention_of_Forth.pdf](http://www.wulfden.org/downloads/Forth_Resources/CM_Invention_of_Forth.pdf) * [https://www.forth.com/resources/forth-programming-language/](https://www.forth.com/resources/forth-programming-language/)
I dig the code format, I was already considering adding the option of doing something similar using markdown and code blocks for Snabel. I can't shake the feeling that we got the priority between code and documentation wrong from the start.
Thanks. I've been quite pleased with the results (in terms of helping to keep code and documentation together and current).
Albert van der Horst posted a bit of code to comp.lang.forth to allow for: &lt;wordlist&gt; IMPORT &lt;name&gt; Which is similar in spirit. Actually pulling specific functions out of a file would be much more complex due to the need to calculate dependencies and discard unused bits. (Not that this is a bad idea though. I've explored adding such functionality to my Forth, but continuing down this path will take more time than I have to spare currently.) References: * [https://groups.google.com/forum/#!original/comp.lang.forth/n4mqn9cXVyM/d5n46mePSZ0J](https://groups.google.com/forum/#!original/comp.lang.forth/n4mqn9cXVyM/d5n46mePSZ0J)
I've removed that part
While this may be interesting, with words over 240 lines long there's no way I'm going to try to make sense of it.
If Chuck really has retired, then congrats on his out of this world (quite literally) career. Inventing a new and groundbreaking language, designing your own CAD software, building your own chips, having those chips orbit a gas giant at the edges of our solar system, founding a company...etc etc. I'm sure it would make a great biography. I just wish he'd write another book. Anyway, as far as the future of Forth, it does seem like Forth has a bit of life in the hobby and microcontroller sectors, but I haven't heard of too many people writing commercial Forth software running on the Desktop or server. That is where the lions share of programmers develop, so it wouldn't hurt to get more material out there for those folks and make it more beginner friendly. Something Chuck said about million LOC codebases (Alan Kay went over this as well in a recent talk) really stuck with me from a philosophical perspective. Code like that leaves a certain impact on society from the energy spent if it is inefficient to the technical debt in maintaining it...etc. I know Chuck is hardly human, but can any other Forthers on here comment on the length of their code in Forth versus what a similar project would be in another language they're familiar with?
forth actually has a role at massive parallela computing, but unfortunately i found those guys always talking about how to implent/improve/other on z80/68xxx/blahblah, its understantable since if i were them, i might also lost in the former glory memory, but we need to move forward. for AI parts, i guess its also an forward too
I believe programs in Forth tend to be smaller, but I don't have any hard numbers. Of course, Forth syntax supports this; e.g. a colon definition is made with just two characters, `:` and `;`, no type declarations, etc. But maybe it's the Forth philosophy more than the actual language. If I write programs in a Forth mindset using other languages, they can be quite small too.
This is Forth. There can be no leader. There are plenty of smart people though.
I tip my hat to your use of Gopher to present Retro.
The implementation of eval (or EVALUATE) kind of falls out of the normal operation of the Forth interpreter. What the interpreter does is just to accept a string as input, and process whitespace-delimited tokens. The input string can come from: * The user, in which case the string is a line from the terminal. * A file, in which case the strings are successive lines from the file. * A block (if you have them), in which case the string is the entire block. * Or a programmer-supplied string. Just process the string as is.
I'd like to think that sandboxing is orthogonal to the interpreter. Sandboxing in Forth can be done with vocabularies. You create a new vocabulary with sandboxed versions of all words. Then you set the search order to use only this vocablulary. Voila, your application can't escape the boundaries you have chosen. Memory references? You create a sandboxed memory space. Allocate a block of memory for your sandbox, and make `@`, `!`, etc refer to that region only. This is much like what a Forth cross compiler would typically do. If you think about it, these have much in common: * Sandboxed Forth * Forth cross compiler * Forth virtual machine simulator
The implementations I've seen implement the dictionary as a linked list, which links from the last entry backwards to the previous ones. In such an implementation, accessing the earlier words will be slower than the ones added recently.
Thank you.
That is very much implementation specific. In my Forth, the dictionary is a linked list, so older definitions take progressively longer to find. I've dabbled with some other structures (hash tables, usage of an sqlite3 database) as well.
Thanks. Since I encountered Gopher earlier this year I've become a big fan of the simplicity. (I have rather a lot of Gopher-related projects ongoing now).
Funny you say that because the reason I asked was to see if I could use vocabularies as a hash table.
This is tricky; are we counting the length of the code in libraries/frameworks the code uses? With some things (e.g., my gopher server), I had previously done a Python implementation. The code was shorter, but depends on a much larger language and set of frameworks. The forth version is longer, but has far fewer dependencies. The Forth version needs: * `tui`: a wrapper to bind stdin/stdout to a socket: 71 lines of C * `rre` (retro for linux): 1198 lines of C (includes inline version of image file, nga virtual machine, and all optional I/O functionality) And the server itself is: * `Atua`: 253 lines w/comments, 92 lines without comments (compiles to 2,186 cells or about 8.5KiB) My original Python prototype was 33 lines (no comments). But the reliance on Python means that it has a much larger code base behind it. So which is truly smaller?
Sure you can. 8th uses hash-tables for its namespaces.
Sure. Hash tables work fine as long as you have the memory for them. Though a linked list and single dictionary isn't that bad. In my Forth: Finding the oldest definition ("dup"): - with 1500 words: .006s - with 5500 words: .021s - with 15000 words: .163s - with 50000 words: .487s (Based on average result of 5 runs against each set) And my Forth is *not* optimized for speed. There's no shortcuts (each entry has to be compared until the match is found, and my string equality check is not particularly fast). Also my Forth runs on an emulated MISC architecture, so it's not able to be tailored to a specific hardware for performance tuning.
AFAIK 8th is not free software though
This is good for medium sized compute but for big compute a hash table is necessary, afaik. The parts I run into trouble with with making my hash tables are making the hash function and in making the memory recycling. Making memory recycling is tough because I want to recycle using a hash table of pointers to hash tables of pointers to recycled hash tables. And I have to implement that before I implement my actual hash table to have the recycling in the first place. That's not impossible but ends up being alot of code. It is painful as a forth programmer to see that much code. The other hard part is trying to choose which hash function to use and then figuring out how to implement it. This ends up being a huge design choice that is actually very few lines.
Not sure what that has to do with the original question. 8th has a free version, in any event.
Could you link me to that free version and the documentation to it and how I can install it in ubuntu?
[https://8th-dev.com/dl.php?sku=A100](https://8th-dev.com/dl.php?sku=A100) Installation instructions are in the [manual](https://8th-dev.com/manual.pdf)
I do not mean free as in free beer. In order for me to use 8th as a dependency there needs to be a version that is free as in freedom, libre, or FOSS as it is also called. This means that it uses the GPL 2.0, gpl 3.0, lgpl, or some other free license approved by the free software foundation. In order for a license to count as a free license it must meet the four freedoms as defined by the free software foundation.
How many words do you ultimately expect to have in the dictionary?
Well part of what I research is cloud computing which absolutely demands fully scalable computing. The O notation of linked lists is simply not suitable for cloud computing. You're talking about arrays hundreds of millions of items long or longer. Customers in cloud computing are willing to pay more for there to be no limit in terms of how much data they can crunch. They often pay for very big virtual machine with dozens of gigabytes of ram.
This is off-topic from the original posting though, which asked about using a hash table, not FOSS implementations of Forth. As /u/8thdev says, 8th uses hash tables. You could still try it out to see how it performs and maybe get ideas from it even if you don't end up using it for any serious applications.
That's a different issue than *vocabularies* though. It's *very* unlikely that you'll ever need a single *vocabulary* with hundreds of millions of word names.
https://8th-dev.com/get.php
I can not utilize 8th's hash tables because it is not FOSS. Nor can I steal the implementation because it is not FOSS.
The goal was not to use them as vocabularies though but to find an easy way to implement hash tables that scale to big data.
OK, I understand what you mean. I did not realize your intention was to lift an implementation detail. FWIW, the hash tables used in 8th are wrappers around SQLite's hash tables. SQLite (www.sqlite.org) is public-domain, so you can do as you like.
Do you have one that is published under a free license? Otherwise I am not interested in it.
Absolutely not; if you've got millions of names in your vocabulary, there's something wrong with what you're doing.
I can consider using SQLite as a backend for the storage aspect but not for processing data live in memory. Thanks for the tip, though.
Nope. No GPL, nor do I use any libraries which are GPLed. Sorry that doesn't work for you.
Isn't it true that your implementation of hash tables uses sqlite?
I doubt that any Forth implementations of vocabularies using hash tables will be helpful since scaling vocabularies to that level is not something that is needed.
Yes (SQLite's hash tables, not the db stuff). But SQLite is not GPL, it's PD.
Well, 8th doesn't have a monopoly on hash tables.
While I don't think this will be of any help, here's a [gforth hashed dictionary](https://github.com/forthy42/gforth/blob/5e18907b2e750503c070da6429fd10aff8bcd868/hash.fs) implementation.
Getting ideas is not equal to stealing the implementation...
Your software depends on the work of others and would not be where it is today without the work of others. Yet you do not share your work so that others may do the same thing that you did. I say shame on you, sir.
Indeed. I'm going to have to implement the hash tables out of forth words.
Sure but I mean now that I know it's based on SQLite I could just use SQLite. And I may indeed do that for a storage based big data. But for something run in ram I can't depend on SQLite as a backend.
Ah, the old "the fruits of your labors should be free". Or, "you didn't build that", to quote someone famous. If you want, you can get my "Reva Forth". It's PD. Completely free to do with as you like, with no restrictions, all source included. Or, you can look up the work I've done over the past 30 years on various FOSS projects. Like Vim for instance. But 8th is a commercial product. So I don't release the source to the general public. I feel no shame whatsoever about that; I've earned my chops.
That's pretty interesting. Does this replace the underlying dictionary system with a new one when you use it?
No idea; I only use gForth sparingly for tests involving ANS code; I can't study the sources due to them using the GPL.
I'll message the creator and find out.
&gt; As I approach infinity is the access to the newer words going to be slower and slower? Yes, but this only affects interpretation mode (when you're playing in the REPL), not compiled words. I don't think it's a big deal.
Sure though I am looking for a hash table design that is roughly equally fast in both the compiled version and the repl version.
I am getting a little curious as to what you actually aim to do with Forth. A while ago it was a bash replacement. Then posts about simplifying the core and microcontrollers. And now some mention of very large data sets. I no longer have any idea what you actually want to use Forth for.
While EVALUATE may be used in a million ways, and I appreciate the simplicity and flexibility of vocabularies; I would guess the most common cases are internal/trusted code and external/untrusted code. Offering a reasonably safe standard vocabulary as an option out of the box sort of solves that problem, but I can see other issues in Snabels case. Lexically scoped environments means I need to limit lookup to a certain depth, opening new scopes should still be allowed inside EVALUATE and they should be able to access all super-scopes opened from within but nothing from the outside. And generic words in combination with the requirement to support defining new words/lambdas from within EVALUATE means I need to be able to somehow tag code as unsafe and perform checks at runtime once I can tell which implementation is being selected based on stack contents. Right now, I'm playing around with tagging all built-in words as safe/unsafe out of the box and providing a SAFE-word that guarantees that what follows within the same scope is safe. There is no going back, once a scope is tagged as safe it stays safe until closed. Words and lambdas are tagged with the current scope's safety-level when defined, and environment lookups are prevented from going outside the current safety-level. SAFE would be usable in other contexts as well, placing a call at the top of any untrusted script is convenient enough for it to actually be used.
So your question doesn't seem to be about EVALUATE in Forth, correct?
It's not about copying Forth literally for the sake of doing the same thing, enough people are doing that. I appreciate the input; and as a token, and in the hope of helping get the discussion started I offer my perspective.
Another very common way of implementing the dictionary is a chained hash table i.e. a hash table of linked lists of words.
Sandboxing is in, eval right around the corner: https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md#sandboxing
So I can try to describe my goals and visions for forth. One of the first languages I learned was c++ and I hated it alot. It's syntax was very punishing and hard to learn. I moved onto bash and in some ways bash was nice but bash was much worse. As I learned about bash I learned about a terrible history. In the 1940s shells were baked into the operating system. So each time you wrote a new operating system you wrote a new shell. For that reason those shells had to be replaced. Then in the 1960s bourne shell came about and this was a good shell for a while. But then they replaced bourne shell with korn shell and they replaced korn shell with bash. And now today people are saying that bash needs to be replaced with zsh. But zsh itself still has problems so we can be relatively certain it needs to be replaced again. So what I wanted to do was to find a language which for mathematical reasons would never need to be replaced because it would be a programmable programming language. A language that could become whatever you want could be eternally fixed. Two languages fit that bill. One was forth and one was common lisp. I prefer forth because it has a simpler design. That being said I am not an electrical engineer by any means. I'm a high level programmer who thinks alot about cloud computing which means using gigabytes of ram in your applications. That being said I want to create high level libraries to go to forth and extend what forth can do in a high ceiling kind of way. This means extending it beyond the micro controller realm and making it usable enough for high level programmers that it can be used in the mainstream. It means turning forth into a python-esque language without compromising the core of how forth was designed to work. The way that I see it, most people who write languages in forth just write a brand new language and use forth as a runtime. This is a very bad design choice. Extensions to forth should be done in vocabularies so that the base ans forth beneath is protected and the user can get access to the low level side of things. So I intend to extend forth in ways that are natural to how forth works. A good example of this is my Read Process Write paradigm and my spooling and unspooling. I think programming techniques like these enhance how one can use forth while not changing the essence of what forth is under the hood. In other words I don't agree completely with Moore about how things should be, but I want to stay as true to his vision of forth as I can while still making forth a higher level language. In the end what my research will look like is a version of gforth which has been extended to be more like the racket language with standard libraries for doing all sorts of things.
&gt;In the 1940s shells were baked into the operating system. So each time you &gt; wrote a new operating system you wrote a new shell. For that reason those &gt; shells had to be replaced. Then in the 1960s bourne shell came about and &gt; this was a good shell for a while. But then they replaced bourne shell with &gt; korn shell and they replaced korn shell with bash. And now today people are &gt; saying that bash needs to be replaced with zsh. But zsh itself still has problems &gt; so we can be relatively certain it needs to be replaced again. There's some inaccuracy here. The bourne shell was released in the late 1970's (1977 I believe, but might have been a year or so earlier). In the early 1970's Unix had the Thompson shell (around 1971), though there was an earlier one (1969?) in assembly language. Shells will be replaced. And they should be; one of the nice things about Unix-syle OSes is that a lot of things can be switched out and alternatives explored fairly easily.
Building towards a higher level set of functionality isn't a bad thing. But there'll be a lot of work to build Forth into something suitable for dealing with complex data sets that are multiple gigabytes in size. If you do pursue this, I think the final language will be, of necessity, different in a lot of ways from Forth today (both ANS and the less traditional approaches).
I think that it can be accomplished if one preserves the forth underneath. One must try to keep the ans forth running their programs pristine so that they conform to the ans standard. Furthermore one must try to not make multiple extensions to the language which are mutually exclusive.
Whatever your goal is, avoid premature optimization. When your program becomes too slow, measure carefully, then rewrite the bottlenecks.
On the one hand I agree with Moore in that you should not try to plan for the future because you can not predict the future. On the other hand I think the science of O notation is decided and tables will always outperform linked lists as data demands get higher.
You can rent thouthand machines, feed them with your words, connect them, make a torrent portal, you got your billions entries hash table dictionary no?
The issue with that is that the network is a huge bottleneck. For some applications people want a single computer with alot of ram.
Well if the people have a torrent client with a forth kernel, they can build their own tailored application from the words hosted by your servers, or ask the cloud to build the application and send the result back. I just pointed the torrent technology because you want something free and it does excatly what you ask.
Torrent technology is good for a distributed system but it is not ideal for what i am talking about for two reasons. First of all torrenting uses file storage and processing data by file storage instead of in memory is slower. Also torrenting must pass everything over the network slowing things down even further. If you want to do rapid number crunching it has to be in memory.
Why stay with the ANS standard if your goals lead to a very different set of needs? If you want to build something for processing very big data sets, you'll likely be delving into lower level details for performance tuning and adding mechanisms for parallelism.
"some inaccuracy" is being very polite. It's nonsense. The one and only digital computer in the world in the early 1940s was Colossus, a top secret, machine for crypto-analysts in UK. It had no shell and in fact nothing that has any resemblance to a shell. It was programmed with switches! You can't know where you are going unless you know where you have been.
&gt; I have seen many of the forth groups in the past. One example is the forth interest &gt; group. This is a mainly non interactive group on flat webpages which is basically a web &gt; 1.0 attempt to be a forth community. Many of the postings on there are multiple years &gt; old. There's at least one FIG still active in Silicon Valley[1]. They have a mailing list[2] and monthly meetings[3]. It's not a web-based group though, which is why the web site is not all bloated with the web2.0 junk. Apart from SVFIG, the #forth IRC channel[4], and this Reddit the other big place for Forth discussion would be the comp.lang.forth newsgroup[5]. &gt; I propose that a new forth community using discord. Here is a link to the discord &gt; community I have created: &gt; &gt; https://discord.gg/sBaeG2a &gt; &gt; Discord is a webservice which is like irc but much more advanced, allowing picture, &gt; video, and voice communication. Anyone with the link can join and be a part of the &gt; community. While I won't be joining this, I wish you luck. Growing a healthy community is hard work. &gt;This group will be divided into three parts: &gt; &gt; The first part is for the advocacy of forth hardware. These will be people who are &gt; interested in making green arrays chips mainstream to replace the commonplace &gt; intel designs for desktop computing. These people will be set in the interest of making &gt; forth hardware a reality. I wasn't aware that Green Arrays had any chips even close to targeting the desktop computing market. Nothing I've seen from them appears to indicate that this is even a goal of theirs. &gt; The third group will be dedicated to application software on the desktop and phone using &gt; forth. This group will focus on advocating for forth in the high level software industry to &gt; try to bring some cash flow and new programmers into the forth community. New programmers would be a good thing, though interest in Forth seems to already be growing slowly over the last decade, as long as you count interest in non-traditional dialects. &gt; This group's mission is to develop a twenty year plan for spreading the use of forth &gt; across the world and then following that plan to actualization. This plan will likely include &gt; producing forth literature and videos, advocating for forth online, and organizing forth &gt; meetings offline aka forth day. Very ambitious. Twenty years is a long time to devote to an effort like this. I've been working on my Forth for over sixteen years now, and have been participating in various communities for most of that time. The #forth irc channel is fifteen years old. While it's had busier and slower times, there's never been a group that coalesced around a single plan to try to grow a specific Forth into a mainstream language. You'll need a clear idea of exactly what you want to achieve, and will need to be prepared to communicate that vision well. You'll also need to promote your efforts through existing communities if you want to find like-minded individuals to help progress things along. It won't be easy, but can be very rewarding in its own way. Good luck! References: * [1] [SVFIG](http://www.forth.org/svfig/) * [2] [Mailing List](http://www.forth.org/svfig/zorkmail.html) * [3] [Monthly Meetings](http://www.forth.org/svfig/past.html) * [4] [#forth irc web client](http://webchat.freenode.net/?randomnick=1&amp;channels=%23forth&amp;uio=d4) * [5] [comp.lang.forth on google groups](https://groups.google.com/forum/#!forum/comp.lang.forth) Edit: fix link #5 Edit #2: There's also a facebook group at [https://www.facebook.com/groups/PROGRAMMINGFORTH/](https://www.facebook.com/groups/PROGRAMMINGFORTH/).
I'm not sure how deeply you have looked into Forth implementations but for example "2*" is not defined as: : 2* ( n n -- n) 2 * ; Typically it is a bit shift to the left which is a faster instruction than multiply and way faster if the machine does not have a multiply instruction. (tiny 8 bit machines for example)
Very true. It was late for me yesterday when I replied, and I didn't register the 1940's bit.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) https://i.imgur.com/bUqsA93.jpg ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dmyju95) 
It would be great if someone who was there or had the lecture notes could tell a little about NDCS as it pertains to Forth.
This is exactly the reason why I want to preserve the ans underneath. having ans at the base of the language gives one more fine tuned control over what is going on.
But ANS doesn't give this. It helps with source portability, but implementation details are mostly not specified. All the low level bits needed for performance tuning and adding parallelism won't be portable anyway. It'd be better to build a foundation that's designed for the tasks you anticipate anf provide an ANS layer above this if you actually need it.
It seems like what you are telling me is that forth is not suited for what I want to do, but I know that by design forth is suited for that.
is that board ga144?
See the paper that went with this: [http://www.complang.tuwien.ac.at/anton/euroforth/ef17/papers/pelc.pdf](http://www.complang.tuwien.ac.at/anton/euroforth/ef17/papers/pelc.pdf)
Thank you!
Non-default compilation semantics. E.g. `+` might have default compilation semantics, `S"` does not.
Ah, so he's talking about completely separating out compile vs interpret behavior, and Immediate being a special case? I thought many people were already doing this.
An extremely important point. I suppose if I actually brought in and counted all the python modules and the language's source it would be massive.
Snabel adds lexically scoped environments which tips the scales slightly in favor or register-style. I find that my Snabel-code is pretty light on stack use as a result. https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md#bindings
Pretty neat. It's amazing that there's so much room for efficiency, but we are stuck with legacy schemes for communication merely due to momentum.
Thank you. I found this interesting, though the color choices the page author used made it difficult to read. (bright green on white background in tables is a *very* bad idea).
&gt; It's no secret that the average microcontroller that can run forth is &gt; the size of a peanut. It's also no secret that the current best &gt; microsd card is the size of a postage stamp and stores half a &gt; terrabyte of data. Do you have a link to this? The biggest microsd card I've seen is 400gb. &gt; It seems to me that these two technologies should merge. There should &gt; be a very simple word set to handle storage on a microsd card. Since you want to stick with ANS, this already exists. Raw access can be managed by the block word set[1] and access to the file system can be managed via the file-access word set[2]. &gt; The interface has 8 terminals so one needs a controller with 8 spare &gt; pins. If one needs more pins than that they need to add a second &gt; controller. &gt; So you've got two controllers plus a microsd card plus the wiring to &gt; connect them. Now you've got a pretty slim machine that is very low &gt; power and has up to half a terrabyte of storage. Which is impressive, but such a machine will be very expensive. Just taking a quick look, the 400gb microsd card I know of sells for around $250[3]. At that point I'd rather invest in a smaller amount of storage and get a more capable processor and a wifi module[4]. With that I could cache relevant data locally and have access to as much storage as I want by using a remote server. References: * [1] Section 7 [http://forthworks.com/forth/standards/DPANS/DPANS.txt](http://forthworks.com/forth/standards/DPANS/DPANS.txt) * [2] Section 11 [http://forthworks.com/forth/standards/DPANS/DPANS.txt](http://forthworks.com/forth/standards/DPANS/DPANS.txt) * [3] [https://smile.amazon.com/SanDisk-Ultra-microSDXC-Adapter-SDSQUAR-400G-GN6MA/dp/B074RNRM2B](https://smile.amazon.com/SanDisk-Ultra-microSDXC-Adapter-SDSQUAR-400G-GN6MA/dp/B074RNRM2B) * [4] [https://www.adafruit.com/product/2999](https://www.adafruit.com/product/2999) Edit: fixed a typo
note that I found this while looking about the espressif ESP-14 module which embeds a STM8 chip along the esp8826 (around 2$)
&gt; The idea is that, instead of searching for whitespace, we start by &gt; immediately searching the dictionary to find the word that matches the &gt; most characters (and makes a complete match) from that point in the &gt; input stream. This is an interesting approach. I wonder if would make it harder to follow the source in general though. If: dup@ Could be parsed as either `dup` and then `@` or as a single word it would require a bit more context to be mentally tracked. I'd welcome seeing some more detailed examples of how this could be used in an actual application.
Someone would have to be a complete maniac to define dup@ to mean something other than literally dup and @, but I see your point. In this particular case (assuming a sane dictionary), dup is processed first, and the @ after. In the post I did point out "h80" as one possibility, although I will go back and change it to "hex80". I considered writing "bin10000000" but felt it was unclear. The examples I have are mostly small things, like Object-&gt;Method with the arrow being an operator on its own without needing to be written Object -&gt; Method, and of course "bin" "hex" and "dec". And there's always the weirdness of S" This is a string" vs "This is also a string." Spacing can make a difference.
I've done something similiar in my Forth. My headers look like: &lt;link to previous&gt; &lt;pointer to compiled definition ("xt")&gt; &lt;pointer to class handler&gt; &lt;bytes of name&gt; &lt;ascii null&gt; The core of my evaluation loop looks like: * find definition * push xt to stack * call the class handler function for this word All behavior of particular classes of words are handled by the specific class handler words. So I have specific classes for normal words: :class:word (a-) @Compiler [ call ] [ compile:call ] choose ; And `immediate` words: :class:immediate (a-) call ; And variables, constants, etc: :class:data (a-) @Compiler [ compile:lit ] if ; I also take advantage of this to have a special class for inlining words that map directly to the VM instructions.
Looks a little like DOES&gt; with some special sauce. What kind of method binding do you use? Early, late, or both?
I'm not quite sure how to answer that. A new header starts with the class field pointing to `class:data`. This is changed by `:` and my `immediate` word, or by manually updating the class field in the header. I suppose that this would be early binding, but it is changable at any time. 
My Forth doesn't have a conventional `DOES&gt;`. I implement `does` like this: :does (q-) d:last&lt;xt&gt; swap curry d:last d:xt store &amp;class:word reclass ; In use it'd look like: :constant (ns-) d:create , [ fetch ] does ; The `does` implementation takes the contents of the xt field for the most recently defined word (`d:last&lt;xt&gt;`) and uses `curry` to create a new function using it and the code passed in. It then patches the xt field and changes the class to `class:word`. E.g., #10 'TEN constant * Creates a new constant named TEN * Uses `,` to place a value into the heap (where TEN's xt field points) * Passes a quote to `does` * `does` takes the xt and the quote and makes a new quote like: [ address fetch ] * It then assigns the address of this quote to the TEN word's xt field * And finally changes the class to `class:word` so it'll behave like a standard word 
The Object-&gt;Method idea makes the use case for this clearer, thank you.
Ah, I see now. It seems very flexible.
You're welcome. I can see this concept being used extensively in the class definitions you mentioned in the other post about Immediate being outdated.
Perhaps it was 400 gb. We can call 400 gb microsd cards expensive today but it used to be that an 8 gb microsd card was that expensive. Look where we're at now.
My forth not have state, all is compiled, this simplify all, not does&gt; not immediate. Colorforth have the same aproach. 
&gt; it clearly demonstrates that &lt;Object ID&gt; can either be a pointer to an object or a word that switches to a subdictionary for &lt;Method ID&gt; (such as the familiar ! or @) Only if you know what it is at compile time, otherwise it has to parse the next word and subvert it - which isn't very Forthlike (and completely incompatible with EXECUTE). However, that does lend itself to a distinction between early and late bound methods: if you know the object ID at compile time, then you can statically bind the method to a vtable slot; if you know one of its parents at compile time, you can do something like &lt;object&gt; as &lt;class&gt; &lt;method&gt; to persuade the compiler to look up the method in the right class's vtable; and if you don't know what the object is at all, you can define methods to actually do a full dictionary search on the object's class dictionary. So you can choose between Modula-style compile-time binding, efficient (and safe) Oberon-style vtable-based dynamic binding, or completely flexible Smalltalk-style message searches complete with msgNotKnown fallbacks... from within the same language.
I believe Chuck experimented with this kind of parser during the evolution of Forth, but in the end settled on whitespace delimiting everything as both simpler and more flexible. How do you differentiate between two words with a common prefix? How, for example, would you distinguish between `C@+` and `C@ +`, which are common names that mean very different things in Forth? Or, for that matter, between `DO` and `DOES&gt;`? Or are you relying on the programmer to stick spaces in where it matters? To be honest, I'm not sure that what you've solved is actually a problem...
My point is exactly that; Forth has no arbitrary limitations that we must try to fiddle with. Also, usually when I implement objects, the first field is a pointer to the object's class, and from there the class switches the active dictionary to the class's subdictionary. If the next word isn't a valid method, it searches all ancestor classes until it finds the valid method or throws an error if it fails. The prerequisite to all of this is having a clear differentiation between compile-time and interpret-time behavior.
It was more of a minor annoyance than an actual problem. What is it that C@+ does? Also, DOES&gt; would be picked first if it appeared exactly that way, unless it was written DO ES&gt;, then DO would be picked.
: C@+ DUP 1+ SWAP C@ ; 
&gt; How do you differentiate between two words with a common prefix? How, for example, would you distinguish between C@+ and C@ + I would include the whitespace or lack thereof as part of the name.
Ah. Well, if it was written as " ... C@+ ... " then C@+ would be what it would find. That's not really all that different from accidentally mixing those two things up even during normal typing.
sounds like a trie tree
Oh, I went back to some older code where this is used, and apparently I also decided that spaces were dumb when populating an array with values, so I wrote "hex isbase FFE0, E950, AAFF, 27FF, 26AA, 55E9, FDFF, E9FA, FFA7," just as another example. I had commented in the line above "Spaces between the numbers and the comma look ugly."
:) we did something similar in Able, which uses color (like colorForth) to simply say when a word is to be executed -- during compilation or during execution.
Initially, I did something similar, where I'd have a flag to determine if a word was to execute during the compile or the interpret phase, but I found that I would often write a word definition for each phase. Having two versions of some words quickly got exhausting.
That's why have "the flag" in the source code works so well. Execution or compilation are properties of the point of use (the call site) rather than the definition.
Could I have some examples?
I want to do something similar but instead of using colors I'd use uppercase letters to indicate immediate execution. : min 2dup &lt; IF drop ELSE nip THEN ; Here `IF` (uppercase) is executed like an immediate word, but at the definition site of `if` there is nothing to indicate that this is an immediate word. If you use `if` (lowercase) then it will compile without being executed. 
I admire your enthusiam and idealism, but I suspect that without some tempering with realism, you may be heading for frustration and disappointment. There just don't seem to be that many Forth enthusiasts who are engaged in publicising or discussing Forth publically at all, and of those few people, there isn't a shared concept of what Forth is or ought to be. There's some agreement on some general points, such as minimalism, but it falls a long way short of a shared vision of a language uniform enough to have the shared libraries you envisage in one of your other posts. You only have to look at the divide over ANS Forth to see this. And the non-ANS parties are not united around a single alternative, either. Promoting a new standard is likely to only introduce a new division (assuming it has any success at all). And I'm not sure there's many people hungry to take over the world with Forth, either. It seems to me for a lot of people it's an intellectual exercise or a hobby, rather than something that they feel a burning need to evangelize. If there was a shared vision and a hunger to bring it to the world, what you're trying to start would already be happening. It seems to me that what ends up making a difference in popularizing something among programmers is working software. Moore didn't set out to found a movement, he just wanted something to make programming easier. Torvalds didn't set out to found a movement, he just wanted an operating system kernel for his PC. Stallman *did* set out to found a movement, but he also wrote a C compiler. Less notably, but perhaps more directly relevant, Paul Graham brought some increased interest in Lisp by using it in a successful start-up. He has also been a strong advocate for Lisp, but such advocacy has a lot more clout when it's on the back of a succesful software project. So it seems to me that the best bet for increasing Forth's public profile is a high-profile piece of software written in Forth. That would achieve a lot more than any number of speculative proposals posted on Reddit. Making something successful and high-profile depends on a lot of things outside of your direct control - luck, in other words - but at any rate, having a few working forth programs or a useful library or two would both have some value in its own right and also mean people would be more inclined to listen and want to cooperate with you. So, what are you going to write? (Also, what are you doing being all Free software purist with Ron, yet promoting the use of a non-free chat program? )
Yes, they are. But there's no consensus on how to handle all details. E.g. some would like a "smart COMPILE," whilst others oppose that.
What about `:` and `&lt;`?
`:` doesn't need to be immediate. But it's true that symbols (and numbers) and the mixture of symbols and letters will be trickier.
I'll probably try this soon.
I'm tempted to buy one too.
You'll need a cheap board, a "TTL" serial interface, and an ST-Link (V2) programmer, i.e. items for $3 (and some patience - budget delivery from China takes time). It also works on a recent development version of uCsim, for free, and without delay :-)
Well, [512GB microSD cards exist](https://www.amazon.de/Micro-SD-512-Speicherkarte-Smartphones/dp/B01NC30Z84/ref=sr_1_3?ie=UTF8&amp;qid=1505586878&amp;sr=8-3&amp;keywords=micro+sd+512gb). The [µC embedded in some SDcards](https://www.bunniestudios.com/blog/?p=3554) should be more than sufficient for running Forth ;-)
I'm well versed in china delivery latency (except itead.cc which had the box in Europe the morning after it was insane). Never heard of uCsim, what is it ?
[uCsim](http://mazsola.iit.uni-miskolc.hu/~drdani/embedded/ucsim/) is a multi-target µC simulator that's used in the [SDCC](http://sdcc.sourceforge.net/) test suite. Here is a brief [STM8 eForth uCsim howto](https://github.com/TG9541/stm8ef/wiki/STM8S-Programming#sdcc-the-simulator-ucsim).
oh nice, thanks
Some sd cards on amazon have shown to be fake though
I am not a free software purists I just don't use proprietary software as a dependency if I can avoid it.
You can use all uppercase, bold, and italics if you need to convey different meanings. I like the idea of tagging words, but I felt that it wasn't worth the trouble of thinking about all of that while writing in code.
Free Software chat programs exist, so this is a dependency you can avoid, no? Anyway, that's the parethentical addenum you're answering, not the main question. What software are you writing, or going to write? I agree with you that the Forth web presence seems rather web 1.0, and I would go further: the impression given to someone looking into the matter is that Forth is practically moribund. I don't see the existence of static websites as a problem per se, but there's also a dearth of interactive and new material, the static websites seem stuck in 1999 in both style and content, and of the few blogs that exist a fair proportion haven't been updated for several years, it does not look like there's a going concern here. It's not quite that bad of course, but to someone newly Forth-curious (because an evangelist like you has given them the Good News, maybe), it's easy to get the impression it's dead in the water. So a modern, interactive website with fresh content every day would be a significant contribution that would promote Forth in a way that would make a difference today. It's not going to produce a bright and shining future for Forth on its own, but it is a necessary step that could be taken right now. If no-one is prepared to undertake say a 2-month project, then there's no point in having a 20-year plan. There's quite a bit of work involved to get it set up, and there would be ongoing work to maintain it and keep it stocked with new content, but again, much less work than your 20-year plan requires. There's all sorts of content that you could consider. Get input from some Forth experts and assemble a list of 20 'must reads' from all the articles and posts that have been written over the years. Index every open source Forth project that's more than a hobby project and is actively maintained (or isn't, but is otherwise significant). Interview Forth luminaries. If you play your cards right, maybe you could secure an interview with Chuck Moore himself! Hell, even having a comp.lang.forth feed on a website with the pointless trolls filtered out and a rating system would have quite a bit of value. I'd love to be able to go somewhere I could get to the worthwhile comp.lang.forth content easily. There would be some advantage to such a site being written in Forth, in a kind of 'eat your own dogfood' sort of way, but that's not necessary, and it would be a lot more work. Even Forth afficionardos often think text processing isn't Forth's strong point (although people have written blog frameworks in Forth (minimalist and simple ones, of course), so it's not impossible). So I would start with a framework that already exists, and see how it goes. 
I do not see using discord as a negative thing. As for what I am writing my specialty is in systems programming and language design. So in my free time I try to think of ways to extend forth beyond what it can currently do. I see myself making forth into something more like factor that has standard libraries for things like scientific computing and etcetera. I really hope to extend forth in such a way that does not tamper with the way that forth works under the hood. I also have been considering making a shell in forth though this is a huge undertaking.
There are some limitations: * the speed with uCsim is in the order of 1/10 of the real thing * no STM8 timer support -&gt; no background task * very limited support of peripherals * you need to use ctrl-h as backspace There is a [script](https://github.com/TG9541/stm8ef/blob/master/tools/codeloadTCP.py) that uses uCsim for compiling STM8EF code with e4thcom-style *#include* / *#require*, and a [second script](https://github.com/TG9541/stm8ef/blob/master/tools/simload.sh) for creating binaries with target code, e.g. in a CI environment.
Here is a short asciinema recording of [STM8EF on a $0.60 STM8S103F3 "Mindev" board](https://asciinema.org/a/4VnvFnCcRlQOLZL5HCh4qiCrF): [![asciicast](https://asciinema.org/a/4VnvFnCcRlQOLZL5HCh4qiCrF.png)](https://asciinema.org/a/4VnvFnCcRlQOLZL5HCh4qiCrF) For some reason asciinema doesn't render e4thcom keyboard input correctly.
Why do you not have any trouble using discord, but reject 8th due to it being non-free, to the point of berating Ron about it? This seems quite inconsistent to me, if not hypocritical. As it happens, Julian Fondren has [just encouraged the production of libraries to make Forth more attractive](https://groups.google.com/d/msg/comp.lang.forth/e7-zIls37OU/iHmr8dGaBAAJ), even if they're less than great. So writing libraries sounds like an important and useful task. Are you working on a library right now? 
that's superb, thanks !
for a newbie that will do (also I use emacs, not using the usual keys is our bread and butter)
Where do you compile the code that was entered by the user into the repl? Is there a dedicated space for that or you just compile everthing to `here` and retrieve that space after the code was executed? Do you execute the compiled code everytime the user pressed enter?
You might be interested in this paper. It is old, but will give you references that you can search for if you need details on other efforts in this area. http://www.bradrodriguez.com/papers/oofs.htm You could also study the source for Win32Forth as it uses a form of OOP built on Forth. Not to show you the "ideal" but rather to allow you to compare and contrast your ideas with others. 
r4 is not interactive, the code is compiled (in bytecodes or in asm) and then executed 
When I write software I license that software as open source. 8th is not open source so if I write code in 8th I can not license it in open source. Discord is a closed source website. I did not write it so I am not worried about it's license. Here is the sum of my current published forth code: https://github.com/johnmorrisbeck/gforth-config-nightly/blob/master/.gforth-config
Which allows generic words with different implementations for different stack contents. Which in turn means it makes even more sense to add new implementations last since it allows overriding/overloading in order of appearance. That's what I ended up doing for Snabel: https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md
Snabel is GPL3, but prefers ordered maps based on the C++ STL: https://github.com/andreas-gone-wild/snackis/blob/master/snabel.md
A few quick observations from looking at the code: - no comments (stack or otherwise) make it difficult to follow the code. - `safe-pop` is defined twice - using the name `list` in a global lexical scope hides LIST from the Block Extensions word set. - `cell +` could be replaced with `cell+` (potentially a more optimized solution than using them separately) RE: licensing You should be distribute 8th code as open source and allow users to obtain a license for the free (or otherwise) versions if they see fit to build/run for personal use. Discord might be a good service (I've never used it). But it's a closed source platform, so what plans exist for moving a community from it if the platform changes? (E.g., if discontinued or if the service goes paid only, etc)
Ah, great resource! I read his articles "Moving Forth" and found them enlightening when I was just starting out with Forth. I'll give the OOFs a look. Thanks!
I'll keep snabel in mind.
The way i see it by the time discord ever fails another clone will rise up in it's place. I don't agree that people should never use any proprietary software anywhere. I simply believe that most software projects (at least programmers tools) should be open sourced. Also thank you for the code comments I fixed those issues.
&gt; So first of all I need to know about what sized microcontroller I need to fit ans forth on &gt; it and still have a decent amount of space for a program to run. i am a pretty high level &gt; programmer so what is decent for an electrical engineer in terms of space is probably a &gt; little too cramped for me. The guy who is buying the microcontrollers is willing to spend &gt; a bit in that regards. I've been considering PIC32 as an option if for some future hardware projects. Something like the PIC32MZ2048EFG064[1] has plenty of flash and RAM and isn't too expensive. Though a Raspberry Pi Zero W[2] is similar in price and much more capable. For a Forth for even smaller MCU's, maybe look at FlashForth[3]. It aims for ANS compat., runs on multiple PIC and Atmega microcontrollers and is GPL'd. &gt; edit: it seems the part that I am talking about is called a GPIO expander but from what &gt; I've seen it seems like I could just build that out of microcontrollers. Am I wrong? Singe an I/O expander is cheap[4], why build one from scratch using microcontrollers? References: * [1] [http://www.microchip.com/wwwproducts/en/PIC32MZ2048EFG064](http://www.microchip.com/wwwproducts/en/PIC32MZ2048EFG064) * [2] [https://www.adafruit.com/product/3400](https://www.adafruit.com/product/3400) * [3] [http://flashforth.com/](http://flashforth.com/) * [4] [https://www.adafruit.com/product/732](https://www.adafruit.com/product/732) 
It seems to me then what I really need is an rpi and an expander, and I can program the project itself in forth. What I am curious about though is that the expander you linked seems to just be a micro controller to me. It seems that I could make one myself for cheaper by writing a little c code.
For $5 or so per month you could setup a VPS from DigitalOcean or Linode, run an IRC server with a web interface, and have facilities for hosting project data and repositories. This would free you from a dependence on a relatively young closed platform with no clear path to profitability. Relying on something like Discord and having it fail would be damaging to any community you manage to grow. Whereas with something like a VPS, if the underlying host goes away you can migrate your entire setup to a new host with little to no impact on your group of users. 
Here's a somewhat cheaper source ($1.18 instead of $2.95) for that I/O expander: [https://www.alliedelec.com/microchip-technology-inc-mcp23017-e-sp/70046138/](https://www.alliedelec.com/microchip-technology-inc-mcp23017-e-sp/70046138/). In either case, if you do manage to do something functionally identical that's cheaper by using an MCU, I'd love to see it.
The reason I chose discord was because it is a little more complex than irc. It allows photos for example and it can do voice chat.
Do you think this one would work? http://www.microchip.com/wwwproducts/en/MCP23017
That's the same one I linked to, except directly from the manufacturer. The only reason I'd see not to order from them is that they have various MOQ's depending on the chip packaging, and I don't often have a reason to order more parts than I need for a specific project. 
What is an MOQ?
Minimum Order Quantity
There is [Riot (client)](https://github.com/vector-im) &amp; [Matrix (server)](https://github.com/matrix-org/synapse) as an open source option. As with Discord, I have no experience with either of these, preferring to stick to simpler, text-centric communications.
I recently stumbled across this while doing some research for an ongoing language project. The text can be [read in English by using Google Translate](https://translate.google.com/translate?sl=auto&amp;tl=en&amp;js=y&amp;prev=_t&amp;hl=en&amp;ie=UTF-8&amp;u=http%3A%2F%2Fwww.asu.ru%2Ffiles%2Fdocuments%2F00002990.pdf&amp;edit-text=&amp;act=url). 
Oops you are correct. If you create a constant that is 1 CELL bytes then I believe this is better. RED 3 CELLS OVER + SWAP DO I @ . CELL +LOOP 
This works in GForth create red 255 , 0 , 0 , : .color-code ( addr cell# -- ) cells bounds do cr ." i = " i . space i @ . cell +loop ; red 3 .color-code 
To be honest, that's what I expected :-) I watched the fireside chat with Chuck Moore last year, and I'm looking forward to the next Forth day.
With that pic32 chip you showed me is it possible to implement the usb protocol and hook up that chip to a usb wire and plug that into a computer?
Hooray!
Yes. The data sheet[1] lists USB support as one of the features. (Details on this are on pages 197-246 of the data sheet). References: * [1] [http://ww1.microchip.com/downloads/en/DeviceDoc/60001320D.pdf](http://ww1.microchip.com/downloads/en/DeviceDoc/60001320D.pdf) 
Thanks for posting
Here is a toy OOP implementation that used to be included with GForth \ Mini-OOF Bernd Paysan 12apr98py : METHOD ( M V "NAME" -- M' V ) CREATE OVER , SWAP CELL+ SWAP DOES&gt; ( ... O -- ... ) @ OVER @ + @ EXECUTE ; : VAR ( M V SIZE "NAME" -- M V' ) CREATE OVER , + DOES&gt; ( O -- ADDR ) @ + ; : CLASS ( CLASS -- CLASS METHODS VARS ) DUP 2@ ; : END-CLASS ( CLASS METHODS VARS "NAME" -- ) CREATE HERE &gt;R , DUP , 2 CELLS ?DO ['] NOOP , 1 CELLS +LOOP CELL+ DUP CELL+ R&gt; ROT @ 2 CELLS /STRING MOVE ; : DEFINES ( XT CLASS "NAME" -- ) ' &gt;BODY @ + ! ; : NEW ( CLASS -- O ) HERE OVER @ ALLOT SWAP OVER ! ; : :: ( CLASS "NAME" -- ) ' &gt;BODY @ + @ COMPILE, ; CREATE OBJECT 1 CELLS , 2 CELLS , 
Here is some example code for how to use it. https://www.complang.tuwien.ac.at/forth/gforth/Docs-html/Mini_002dOOF-Example.html
Thanks! This is great for minimal objects to do encapsulation.
If you chose not to open source it, that's your choice, don't listen to the entiteled ones, I don't have the need to get a pro lisense, and I don't have the means to, but I do feel that people like you should be allowed to get to live off of something that they have put so much work into.
Thanks. Obviously, I agree...
Preach!
:) I just wanted to share my 2 cent, so that people don't believe all of us open-source dudes are entiteled a-holes :)
&gt; ...and looks forward to seeing some of you there "Some". The others, however...
Is there any difference between the PIC32 instruction set, and vanilla MIPS32?
None that I'm aware of.
Good job! This is interesting. I would suggest it might be good to line up identically named words (or words that are named differently but do the same things). This way, it would be easier to compare the different wordsets.
I agree; I'll add that to my todo list for the first update :)
This is really cool. I had no idea eForth was so minimal. I also had no idea ANS was as large as Forth 83. BTW, I'm very interested in Retro. Do you have any documentation anywhere?
ANS really is an interesting blend of the earlier standards. RE: Retro documentation; what exists is in the repository and snapshots. It's also browsable via [http](http://forthworks.com/retro/s) or via gopher at gopher://forthworks.com:70/retro/s The documentation files are under the "doc" heading. Note that most of the sources (apart from the I/O layers) are written in a fairly literate style with commentary intermixed as part of the source. I'm slowly working on a book/manual that will eventually cover the language in significant detail, including tutorial material. This is slow going though and isn't public yet. (I'm hoping to complete a draft by late October, depending on how things with my other projects).
Nice. I really appreciate the work. The more slim Forths exist, the more we all benefit. And Retro presents its own unique ideas which are sure to inspire others as well.
Why not support android ?
I have a few reasons. The main one is that I hate dealing with Java. As awkward as it is sometimes, I much prefer working with Objective C and Apple's frameworks than Java and Google's API's. A secondary one is that I don't use any Android devices. I use an iPad as my primary computer, a Linode server for backend processing, and an iPhone for work. I have an aging MacBook Air for use when rebuilding the iOS apps, but I do all my writing and non-app update work from the iPad. The only Android hardware I have now is a very old Amazon Fire tablet, which is slow and annoying to use. I keep it solely for occasional use with streaming video from Amazon when traveling. There's also a distinct lack of demand. The iOS version is profitable enough to let me break even on the annual developer fees for having an app in the Apple App Store. I get a good amount of feedback and see steady interest from iOS users. But in the last four years I can only remember a couple of inquiries about Retro for Android. So ultimately, I'd rather devote my limited time to working on building things for the systems I use, rather than one that I don't and for which I've had almost no requests to support.
I'd say comparing eForth primitives to ANS CORE words is apples to oranges.
Someone is still using Gopher. God bless you, sincerely. This is valuable work. Looking at this list almost makes me inclined to view FORTH '79 as the proverbial One True FORTH, to be honest.
Agreed. I'm removing them from the followup chart which will group based on words that are equivalent.
Thanks. I've become a big fan of Gopher and hope to share more of my Gopher related projects in the near future.
Thanks!
Just one point. Is U/ in F79 really equivalent to U&lt; in everything else? (And a formatting nitpick; something seems to have gone a little awry at SAVE-BUFFERS.)
Thanks! I fixed the formatting of SAVE-BUFFERS and corrected U/ to U&lt; (this was due to an OCR issue - sorry!)
Seems like ANS has a significant group of "quality of life" words.
More like "not every Forth is a 16-bit indirect threaded Forth running on bare metal" words. Also, ANS went to considerable lengths to try and consider Forth-79 and Forth-83 equally valid predecessors (or make them equally painful to switch from, depending on one's perspective) - hence SM/REM and FM/MOD. (Having said that, ANS Forth still shows some disdain for figFORTH in its insistence that all words have the same size of CFA; hence, gforth choosing to use two words of CFA for every word, just because it needs to for DOES&gt; words - figFORTH inherited a way of doing DOES&gt; that added an extra word for the DOES&gt; thread, because nobody had yet thought of doing it the Forth-79 way... although the figFORTH way does allow for a ROMmable Forth environment like the Jupiter ACE's, or a Forth that can run on a strict Harvard architecture like AVR, whereas the Forth-79 way assumes that one can write machine code at compile time.)
Fascinating, I hadn't considered that at all. I mean, I understand the quality of life stuff, but I didn't think about how ANS chose to address Forth-79 or figForth. That might have rubbed quite a few people the wrong way.
actually you could use android's NDK to do native developing on android. but since your secondary reason, i could understand that, its just a pity that such a great stuff cant be use on that platform
The NDK still has some shortcomings for Android development. Unless it's changed recently, it isn't really suitable for building a complete application. (The last time I read up on it the NDK appeared to be more useful for interfacing a library in C/C++ with the Java code for the UI bits). It also sacrifices some portability. I actually find it odd that there's not more choices for Forth on Android. Both Android and Google's play store is much more open than Apple's offerings, so it should be easier to get Forth and other languages running there.
actually there's 3 forth apps on play store. and yes, NDK still cant build a complete app. i guess that's why there's not much forth there. we all dislike java
It looks like two of them are basically abandonded: last updated in 2011 and 2013. Only the gforth has recent updates.
I just removed a bug in the code for adding new words to the Flash memory. Literally a single bit had to be flipped to fix it but yet it was one of the more sneaky bugs I have seen in a long time (not as good as [the bug in the original STM8EF code of COMPILE](https://github.com/TG9541/stm8ef/commit/ae43d090addd38b1e7ec8b6b215f12074b8bed39), though)
See also: HP 48 series (i.e. graphing calculators that use HP's RPN stack-based notation for all operations including calculation, graphing, and programming) Available as inspiration for you, in physical form and smart phone apps! Helpful?
A mobile phone that runs forth as its OS is totally doable (although it would be somewhat bulky). http://www.instructables.com/id/ArduinoPhone-20-an-Open-Source-Mobile-Phone-Based-/ EDIT: since I last looked at this the SIM808 is available with GSM, GPRS, bluetooth, and GPS. https://www.banggood.com/SIM808-GSM-GPRS-GPS-BT-Development-Board-Module-For-Arduino-p-1080013.html
What is the phone at walmart? That sounds interesting.
I don't know the model name but it is a very crappy phone.
Could it be reprogrammed?
STM8 eForth can now be built and tested in a Docker container. See https://github.com/TG9541/docker-sdcc and https://hub.docker.com/r/tg9541/docker-sdcc/
So I think that it has a rom that prevents tampering with the os but it should still be able to be used as an embedded device.
I assume the phone is build around a SoC, system-on-chip. Are there really such devices with ROMs these days?
What I mean is I think that the boot process is so dependent on the rom that it is very difficult to rip out the OS because I think the whole OS is on rom.
This is *not* portable. Tested only under Linux on a gForth install. : system-capture ( addr count -- addr2 count2 ) s" 2&gt;&amp;1 &gt;/tmp/_" s+ system \ capture results to /tmp/_ s" /tmp/_" slurp-file s" /tmp/_" delete-file drop ; SYSTEM, S+ and SLURP-FILE are not defined in ANS, but are provided by gForth. DELETE-FILE is a standard word.
Reading the final paragraphs I have a sense that these engineers had never done a project in Forth before. "Because Forth code is almost unreadable, as in other low-level assembly-like languages, we had to strictly document each complex Forth “hex word.” So, the inline documentation of the source code was several times as big as the code itself." Wow! I have never heard of this in Forth. This sounds like they did not get the concept of writing the application specific language that would make their job easier. They stayed at the stack machine level of abstraction all the way through the project. Does anyone know if what I am thinking has any substance?
On twitter, someone, presumably one of the engineers, told me they used MPE Forth and LMI Forth for "most of the experiments" although I'm not sure if that means anything with regards to your question. https://twitter.com/kabunkie/status/554249871482703872 
i doubt you cant understand my forth code easily 
That's an interesting comment. I am just starting to learn Forth so it would be great if you could give an example of going beyond the 'stack machine level of abstraction.'
The RTX2010 is a stack machine so he's just saying that they wrote everything in machine code (hex numbers); possibly because they didn't know any better.
Thank you for clearing that up.
That's what I took it to mean as well and that seems crazy. If it's the case then the first thing they could have done was use MPE Forth for a PC to create an assembler and a Forth compiler.
Update. Got some clarification on comp.lang.forth Looks like an LMI cross compiler was used and the opinion of someone who was partly involved was that the writing/writer of the article is the problem.
Oh boy. I'm from the Roseville area. I wonder if I should go? Traffic through Sacramento and the Bay Area are awful almost every hour of the day.
In gforth there is also 'pipe-open'. Combined with the ANS 'file-read' you should be able to write your system-store word.
Hmm, interesting. This would probably be a much better way to do this. [https://github.com/forthy42/gforth/blob/master/dis-gdb.fs](https://github.com/forthy42/gforth/blob/master/dis-gdb.fs) has an example of this in action.
Using `open-pipe`: : system-capture ( addr len -- addr len2 ) r/o open-pipe throw dup slurp-fid rot close-pipe throw drop ; 
This is neat! I was able to access it earlier, but forthworks.com seems to be down now.
Should be back up now. The server got overloaded for about an hour during the night :(
The [Jupiter Ace Resource Site](https://www.jupiter-ace.co.uk/) contains also a list of emulators. I started to learn Forth using one of them. I really enjoyed it.
You mean something like a trie?
You might find it interesting that they're starting to re-release hobby computers from this era. Unfortunately, I can't imagine the Jupiter Ace ever being re-released, but it sounds cool. https://www.specnext.com/ https://thec64.com/
Re-released, as in, a box that emulates the original hardware?
Not all of it, e.g. you can't connect a floppy drive to the "THE C64".
No I do not mean a trie. A trie seems to be for searching for things like strings whereas what I am talking about is for storing arbitrary data. Also with a trie you still have multiple pointer lookups for very deep tries. Whereas this is instant lookup except where there are collisions just like a hash table.
I use 3 header flag bits: : compile-only ( -- ) interpretation -flags ; : interpret-only ( -- ) compilation -flags ; : immediate ( -- ) precedence +flags ; : directive ( -- ) immediate compile-only ; When compiling, dictionary lookup only considers words whose compilation bit is set. When interpreting, dictionary lookup only considers words whose interpretation bit is set.
Would it not be simpler at that point to have two separate dictionaries? Or is there some utility I'm missing? I'd greatly appreciate knowing your reasoning for doing it like this, if you don't mind.
I'm not sure what you mean by "two separate dictionaries", do you mean two separate vocabularies one for compilation and one for interpretation as in, IIRC, cmForth or Pygmy Forth? I see the vocabulary subsystem as orthogonal to the issue of special interpretation/compilation semantics. 2 bits per header allows me to deal with all the various special compilation/interpretation semantics scenarios, whether or not vocabularies are implemented in the particular Forth. I did consider adopting the dual vocabulary approach but found it less natural (to me), I can't remember my reasoning in detail, it's been about 20 years since I switched to my current method. I also use two versions of 'tick' so that I can explicitly request either the interpretation xt or the compilation xt as desired. For a typical word with default semantics, they would return the same xt, but for special cases where the interpretation and compilation semantics differ from the norm they will return different xts (or throw an exception if the requested semantics don't exist). : ' ( -- cfa ) interpretation defined cfa ; : ^ ( -- cfa ) compilation defined cfa ; : ['] ( -- ) ' postpone literal ; directive : [^] ( -- ) ^ postpone literal ; directive
I understand. Very clever and seems straightforward.
&gt; hereas this is instant lookup except where there are collisions just like a hash table. To be fair, this is nothing, because all you've done is state that it would be nice to have this and that awesome property, which nobody would or could object too because you haven't given us any details. We're forced to accept that this mythical data structure is simply perfect. It's akin to proposing to implement a word DOIT, whose stated purpose is "does whatever you need" -- Sounds fantastic! You'll deliver tomorrow? Where do I sign? -- it's the everlasting gobstopper of software engineering/computer science. 
&gt; I wonder if there is not a way to trick a tree into being instant look up like how a vector/dynamic array is. Why? What do you get for doing this? You've already told us that a hash table gives you instant lookup so what problem are you trying to solve? Tree's have their properties. Hash tables have theirs. If you can make a tree behave like a hash table by computing the position of the data item in the "tree" by hashing it then you've made a hash table (with an arguably over-engineered memory layout). The properties that make it a tree are no more. EDIT: Having read this several times now I do wonder if what you want isn't just an open hash table; a hash table which handles collisions by growing a new arm and storing the new value at the same offset, but I'm doubtful, because you would have learned about open and closed hash tables in your "introduction to algorithms" class. \*shrug\*
Well we can imagine that in javascript we can have an array like this: myarray=[1,[2]] To access the one I must type myarray[0] To access the 2 which is deeper in the array i can say myarray[1][0] This use of brackets is a universal syntax for traversing trees. I can write a location of a string like this [1][2][3][4][3][2] and if I have an array with an object that deep I can access it's value. I can also represent this as a string like ." [1][2]" Because I can represent the location in a tree as a string I can hash any tree location. And because I can hash a tree location I can store a tree location as a value in a hash table. This gives me near instant lookup of the tree at it's leaves except where there are collisions in the string. Of course you wouldn't want to represent it as a string. There are more efficient ways to notate it and to implement it in memory.
The benefit is that for very deep trees you are able to save yourself some pointer lookups while still having it behave like a tree.
So you're saying you want to use a [hash] table as an index from key to the address of a value stored in a tree? Ok. That's a thing, and you can certainly do that if you wish. Personally, I wouldn't call that arrangement a fundamentally new data structure. &gt; I can represent the location in a tree as a string I can hash any tree location. You can also represent the location as an array of offsets and avoid the hashing. &gt; [1][2][3][4][3][2] Assuming you have an array of a known and uniform shape then you can precompute this and apply it with `+`. If you can't know any of that because you refuse to apply reasonable constraints (aka design or engineering) to your data structure for your problem then we're back to using a [hash] table an index.
What properties of the tree, in particular, are you after, if you're using if you plan to do lookup through a index/[hash] table? I'm guessing that it has something to do with the dynamic nature of something or the other that you haven't explained yet?
The properties of a tree that you can work on it as if it was a tree datastructure even though it is a hash table.
I am not sure what you mean by using an array of offsets to avoid the hashing. And anyway I am not claiming it is a new data structure merely that I thought of it while falling asleep.
An interesting paper, but incomplete. Bookmarking in hopes that the author fleshes this out further someday. Links I found relating to this: * github: [https://github.com/suhr/obsc](https://github.com/suhr/obsc) * Esobsc: the esoteric version of ␣; at [https://github.com/suhr/esobsc](https://github.com/suhr/esobsc) (As a side note, the naming of the project is rather difficult to search for; even more so than many other languages due to the fact that it consists entirely of punctuation/whitespace).
I submit that, by the time you are finished with your infix library, you have probably learned Forth well enough not to need the library any more.
Agreed. This is a lot of work for very little payoff.
The lengths that some people will go to, to make Forth look like other languages, boggles the mind. I'm not ashamed to admit that these kinds of syntactic tricks hurt my head, so if it's all the same to you, I'll stick with postfix. 
I don't want an infix Forth. I think that changes the nature of the language in ways that make it fundamentally different. I do sometimes want to translate an infix expression to rpn though. This isn't difficult, but being lazy I am now automating it. I just setup a Gopher server running a shunting yard algorithim against the passed selectors. It returns a text document with the expression converted to RPN. Then, in Retro a couple of words to access this server: {{ :SERVER 'forthworks.com #7900 ; ---reveal--- :s:infix-to-rpn (s-s) s:empty [ SERVER 'abcd 'bcda reorder gopher:get drop ] sip s:chop ; :s:evaluate&lt;infix&gt; (s-) s:infix-to-rpn s:evaluate ; }} And now I can convert or evaluate expressions without needing to add things that fundamentally change the language: '(_b_+_a_)_*_(_c_-_d_) s:evaluate&lt;infix&gt; This is a much better approach IMO than extending Forth into being yet another infix language.
I was also uncertain what the intended benefit was, but the last slide says: &gt;If we’re going to translate from FORTRAN, C, etc, to Forth for a standard algorithms library, this is a much better way to do it than translating from infix to postfix by hand. It’s easier to do and easier to check. That seems somewhat plausible. 
Except that there are already plenty of algorithms that do this, e.g. [The Shunting Yard Algorithm](https://en.wikipedia.org/wiki/Shunting-yard_algorithm). The fact that the author isn't using this indicates to me that he is out of his element doing comp-sci work like this. The fact of the matter is that building an X -&gt; Forth compiler should be almost trivial; even a stripped-down compiler for X has already done all of the heavy lifting for you. As it happens, writing a compiler in Forth itself is dead-simple for the reasons stated above, with the interpreter working on a character-by-character basis, rather than merely on space delimitation.
**Shunting-yard algorithm** In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the "shunting yard" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/Forth/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
&gt; Computers are register machines that are optimized for programming random access memory. As explained before, computers need not be register machines. This is the current fashion but it isn't necessarily true. &gt; The @ and ! operators are some of the most efficient operators you can use in forth. It is very quick and cheap to do a pointer lookup and it is very quick and cheap to write to a pointer. This seems like a strange statement. Accessing memory is one of the most expensive things a computer does. They're necessary. In what sense are you proclaiming that @ and ! are efficient?
&gt; Linked lists are some of the most easily recyclable datastructures [... and] excel where memory is precious I almost feel like I'm reading the journal of someone who has just returned from a long trip in opposite land... Linked lists waste 1/2 of the space you allocate to them and if you want to reuse cells in a remotely efficient way then you have and painstakingly maintain a list of used and free cells. Otherwise, you're stuck doing tracing... which even if computers are fast, is anything but efficient, algorithmically. It's much easier and more efficient to allocate a reasonably sized array for your data and blow it away when appropriate (region-based memory management) It could be helpful to define some terms; you seem to be using the same words but coming to the opposite conclusions. I'm not sure how.
In a computer optimized for fetching and storing from memory such as most computers out there, this is true.
Fixed arrays are not good for dynamic and growing data. With a linked list you can grow your data up to the capacity of memory in a fluid way. When you grow your data past the size of the array you have to either throw away that array space or try to recycle it. Recycling arrays is not very efficient. Recycling linked lists is much more efficient than recycling arrays. You simply keep a list of the recycled cells. Nothing gets wasted.
What is true? Accessing memory takes time. A lot of time. https://gist.github.com/jboner/2841832
What you propose as an alternative to accessing memory?
Resources are always limited and there are there is always an upper bound on what is possible with those resources. If you use an array you can store twice as much useful data in the same space. Allocating 1/4 more than you need gives you flexibility, while also saving space. Arranging this space as a series or regions for specific purposes gives you more flexibility and recycling is free. You get all the great properties of an array. NOTE: I'm assuming that you're bounding the number of cons cells, and if you're not, you will crash unexpectedly at some point; usually in production, and under load, when your clients are most depending on your solution to work. Better a little [temporary] "waste" and well-defined behavior than half-assed argument about flexibility; flexibility which is only needed if you refuse to pin your problem down and define limits.
Andrew Haley is active on comp.lang.forth, and he seems like a competent Forth programmer. So I'm surprised he wrote this paper. Maybe it was just an intellectual exercise, and now a solution in search of a problem.
Have you actually done any low level coding and dealt with the issues that go into optimizations at that level? In my experience, on most of the processors I've done assembly for, memory reads and writes are relatively slow operations. Comparisons are also, and branches can be troublesome, especially with increasingly complicated pipelining and cache strategies. Just assuming that computers are register oriented machines with fast read/write to memory is a flawed view. If you are actually trying to do fast, lower-level code, you really need to learn about your targets and choose your implementation strategies around their design limitations. You make a lot of blanket assumptions here with nothing to back you up. &gt; Constants are very cheap. This really depends on how they are implemented. E.g., if you make a constant using something like this it won't be cheap at all: : constant create , does&gt; @ ; &gt; Writing languages that work closer to forth will make things faster. Or just code *in forth* and avoid the headaches and need to try to find ways to make it faster. &gt; Linked lists are not very cheap in execution time but they are very &gt; cheap in terms of memory efficiency. You need 50% more memory than a static array. More if you want to keep track of allocated/free cells. &gt; Linked lists are some of the most easily recyclable datastructures Sure, if you keep a map of the cons cells that you have used/freed. If you clobber a pointer to a cons without going through whatever allocate and free routines provided then you'll run into a much bigger set of hassles trying to identify them again. You also seem to be quite focused on linked lists. Perhaps LISP would be a better base for whatever you are trying to do?
In defense of register machines, they do avoid a hell of a lot of stack shuffling. Strictly speaking, it is not always true that algorithm XYZ will smoothly translate to stack-based manipulation. On the other hand, then you have to juggle registers, but more often than not is straight-forward.
I'm not proposing an alternative to memory. I'm asking you to justify your claim that arguably the most expensive operation that a computer has to do is somehow "one of the most efficient", because, frankly, you seem to have a completely different dictionary to me and I would like to know what *you* mean when you use words like "efficient", and "cheapest", to describe things which are neither.
&gt; Fixed arrays are not good for dynamic and growing data. As much as I love working with linked lists, your claim is simply false. Arrays are more efficient for almost all use cases.
:-) Absolutely! I have nothing against register machines per se. My comment was simply meant to point out the fallacy that computers are, by their very nature, register machines. Both approaches have positives and negatives and we have to take the good with the bad. Stack machines may not be the best for handling complex mathematical expressions, with a great many variables, but as I'm sure you know, they shine in other ways; they can be made extremely simple, reliable, and resource[0] efficient, offer extremely fast context switching and interrupt handling, and are great for pushing and pulling data around etc. Those are some of the things which make me prefer stack machines. That being said it may be useful to have a healthy number of registers too, if only to provide "instant" access to temporary values(?)[1] [0] time, space and energy (and $$$) [1] At some cost in all of the thing that make stack machines awesome. Tradeoffs, everywhere.. EDIT: Corrections thanks to /u/tlaboc073
Linked lists are memory efficient? Compared to what? They have one or two machine words of overhead per cell of data.
&gt;... **Register machines** may not be the best for handling complex mathematical expressions with a great many variables... Did you mean to type "stack machines" there? It sounds like you are describing stack machines in that sentence. Also, and I apologize for being pedantic, its "per se", not "per say". :)
:-) Thanks for the corrections. Always welcome. Yes, and yes.
An interesting feature of some stack-based processors is combining a sequence of Forth "primitives" into a single instruction to reduce the amount of stack shuffling that actually needs to be done in hardware. For example, the RTX 2000 [1] has an instruction for "@ SWAP alu_op", which is pretty close in functionality to an x86 register-memory operation. [1] Table 4.3(a) at https://users.ece.cmu.edu/~koopman/stack_computers/sec4_5.html#453
Yeah, I've been thinking about supplementing the Forth VM with a bytecode interpreter. Maybe fancying it up with some sort of implicit register file on the Parameter Stack.
Yeah, there's nothing particularly striking about doing any sort of lengthy calculations, but even without any complex algebra I often find myself having to push a lot of things to the Return Stack to get to stuff underneath. I don't think it has to do with particularly bad factoring, as much as it has to do with the need for words like 2dup, 2swap, etc etc. Holding even 2 strings on the stack and a shared index between them is already 5 elements; it can add up quickly to a lot of data.
&gt; Fixed arrays are not good for dynamic and growing data. With a linked list you can grow your data up to the capacity of memory in a fluid way. You can do the same with arrays: Just reallocate the array when it's too small and use an exponential size increase. This way, you still have amortized constant append time.
What do you mean when you say that recycling is free. And how can you know for certain how much array size you need for a given problem?
What do you do when you have an arbitrary amount of data and do not know what array size you need?
This is what I call dynamic arrays or vectors. They are highly efficient.
Well what are some operations that are more efficient/fast than @ and ! ?
&gt; What do you mean when you say that recycling is free In the simplest case you reset a pointer. In the worst case you clear the area. Other approaches are also possible. Forth's dictionary is a great example but the principle is much more general. &gt; how can you know for certain how much array size you need for a given problem? You look at the problem specification and decide on reasonable sizes, limits, and tolerances... it's called engineering? The alternative, which seems to be increasingly popular these days, is chaos; don't try and understand your programs, just let everything grows out of control then act surprised when your solution eats all of the resources on the system (which is bad in itself!) and then causes things to fail unpredictably through some sort of spooky-action-at-a-distance. To be clear: there are situations where you might want to use a linked-list but you need to put a limit on that list too. If you're going to do that then you can probably get away with using an array. Lists are more flexible than arrays (particularly multi-dimensional arrays) but this doesn't have anything to do with your not wanting to think about limits. It's not that hard. Learn.
"reset the pointer". "clear the area." What do these terms mean and what do they have to do with getting free recycling of arrays? I ask you again. If there is no specification and you don't know what size your data is going to be in how do you choose how big your array should be?
Ah, I see, so you're from the brain-damaged school of thought which states that if your best solution is inefficient and expensive, then it must really be efficient and cheap? Give me a break. Is doing a linear search on an very big, unordered data set suddenly "efficient" and "cheap" because you didn't think or didn't know there was a better solution? Jesus Christ man, do you listen to yourself when you make these comments?
&gt; "reset the pointer". "clear the area." What do these terms mean and what do they have to do with getting free recycling of arrays? The irony of you asking me to define terms when you yourself refuse to do tell me what you mean by "efficient" and "cheap" is not lost on me. As I wrote above, there are many ways or organizing memory. One of them is to keep a pointer to the next free position in the list so that you can "cons" new values onto the end. This pointer is updated to point to the next free cell when a new value is added. If that sounds familiar it's because that's precisely how Forths dictionary works, and as with the dictionary you can freely move the pointer back to some point. This can be used to temporarily add values and then to instantly discard them. If you structure your problem this way -- storing values and then blowing them all away when you know they're not used -- then you pay nothing for cleanup. Using a few such spaces for specific cases, where, by definition, the values stored within them have the same lifetime, then you have even more control. More generally we can say that during a lifetime time you are free to use the space however you like. It may or may not be ok to treat the space as "noisy". If it is then you simply reuse the space, not worrying about what *was* there. If it isn't then you have to clean up first. This usually means filling the space with a uniform value. Circular storage spaces are also possible -- the older values simply vanishing when they reach the end of their lifetime. This is similar to the rings (circular stacks) found in some Forth implementations. I could go on but you should get the idea. ALL of these are vastly more efficient, in almost every sense, than linked-lists. &gt; I ask you again. If there is no specification and you don't know what size your data is going to be in how do you choose how big your array should be? If there is no specification and you have no opportunity to ask the client what their requirements are (because I know some people don't like writing things down, cos' like, dude, that's not agile!!! and like, the accountability, and shit :P) or perform experiments to unveil them, what you're doing is not engineering! Can you imagine being asked to build a bridge to span an unknown distance that supports an unknown load and time, where there's nobody to ask these basic questions and where you're not even allowed to see the site yourself? This's utter lunacy! Would you use that bridge? As an engineer, would you take that contract? How would you know when the job is done and you should expect payment? Now to be clear, there are times when you know that the resources you have are vastly more than you'll ever need. It's a safe bet to say that "Hello, World!" (running on any modern laptop without competition from other programs etc. and implemented in a straigt forward way) will never exhaust the resources, but even then there's probably no good reason not to define healthy limits -- you know that only need 14 bytes. And if you're working on your own things, hell, experiment. Think. Start off with more space than you could need and then work your way down to a reasonable level once you understand the problem better. Monitor your program (if it's appropirate) and see how it performs under normal stresses. Build in some leg room, and use this as a "red area", so that you can decide whether you need to raise the limits. It's still better to agree on limits before hand if you can but your question is predicated on not knowing anything... which I still think is, unlikely, and insane, but there you go.
That is one reason I really dislike strings in standard Forth. I prefer to have strings where I can just pass a single pointer, whether to a stored count + data or null terminated data.
In some sense the shuffling in a stack machine is quite similar to the register selection bits in register machine instructions. The shuffling is all about getting at the inputs. The two approaches just put the information in different places: stack machines in shuffling instructions, register machines in dedicated instruction fields. Granted, some things that are easy to express with a register machine are harder to express with a stack machine.
Memory access: expensive. ALU operations: cheap. Conditional and indirect jumps: expensive, unless predicted correctly. Worrying too much about what is cheap or expensive: expensive.
This has been done in many Forths, including, I believe, Chuck's own cmforth, going back 30+ years. So it's a good idea, but definitely not new.
I'm really enjoying exploring Gopher! Thanks for bringing it to my attention :-)
One of the things I said in my original post is that it is not efficient for linear searches.
It sounds like what you are talking about is that if you only manipulate the here word for your array then you get recycling for free because you don't have to allocate a new array. To me this is not a good solution for recycling because then you can not define new forth words. As for setting limits I think you are simply refusing to admit that there are some times in engineering where you don't know how much data you will get.
I didn't say it was. My posts here are mainly for feedback, and inspiring others to improve their own Forths. The more Forths exist, the more we all benefit from it, both in better implementations and visibility of the language. I've seen at least one other person take this idea and go to the extreme, with an interpret, compile, and execution behavior. That guy was probably some sort of alien :p
I've been mulling over a dedicated instruction set that manipulates an indicated section of the stack like a register file. Say, one bit flags it as either Load or Store, and the remaining 7 bits select the register (in native bit width, not a byte offset). Maybe have other flag bits to signal the use of primitives. Of course, since we already have amazing virtual machine models to reference and work with, it wouldn't be that hard to intermix bytecode with regular Forth code either.
I got a chance to read that link in more detail, and I'm struck by the similarity with that to the bytecode system I devised a day ago to mitigate stack juggling. Although I basically abandon the idea of stack juggling by making the Parameter Stack into an extended register file.
Yeah, I'm still not really sure what to do with them. The Return Stack just doesn't feel right for this type of stuff.
Please, don't be so deliberately obtuse. I don't believe for a second that you're so stupid that you really think that's what I said
&gt; To me this is not a good solution for recycling because then you can not define new forth words. \*sigh\* I related a general concept to a specific instance that you will be familiar with. The forth dictionary is very simple. You can implement a new one anywhere in memory in one line. And you're not limited to one pointer or the dictionaries specific properties. With a few more lines you can create regions with very different properties, all of which make cleanup really easy, efficient, and cheap[0]. &gt; As for setting limits I think you are simply refusing to admit that there are some times in engineering where you don't know how much data you will get. Not at all. I explicitly stated that if you don't know then you should find out and you can always find out if you really care. Unfortunately, you've made it very clear that you don't. With a little forethought you can write a text editor that easily handle a 1 TB file (assuming that the hardware, operating system and file system can ;-)). When you don't think about it you get the current crop of text editors which can't load a few MBs without hanging. Knowledge is power. Why wouldn't you want to set limits? Limits are your guide. They're what let you make informed decisions about you design... I've been clear. If you don't want to try and understand and you won't clarify your bizarre opinions then there's nothing more I can say. [0] particularly compared to using linked lists
It seems to be part of what you were saying. Perhaps you could clarify.
It seems to me that you have not found a way to get free recycling of arrays. You have not demonstrated how it is done. "and you can always find out" This is blatantly false. You can't always find out how much data you are going to have to process.
&gt; It seems to me that you have not found a way to get free recycling of arrays. You have not demonstrated how it is done. I've explained in depth how it's done. If you'd like more information search for region-based memory management online. It's a pretty straightforward idea and it happens to be very easy to do in Forth. &gt; This is blatantly false. You can't always find out how much data you are going to have to process. Come on. Think. I can't do that for you. There's always an upper bound and that is most obviously set by the hardware that your solution must execute on. You cannot do more than the machine allows; you can't process more data than the machine can process and you can't handle more requests per unit of time than the machine can handle etc. Any solution has a number of things it must accomplish to solve the problem. Work backward and, whether you like it or not, you will find the limit imposed by the reality of that machine. Thus you know (or have discovered) the limits. NOTE: if you have a requirement to process a certain amount of data or to handle a certain number of requests per second, a certain number of users etc, then you can work the other way. Choose or design appropriate hardware and software to solve your problem. No matter how you arrive at the realization, there are always limits and you will find out what they are, one way or the other. You either work with them or don't. You can make it easy for yourself, or you can make it hard.You either find them in development or, at some point, you find them in production, perhaps when you get an angry phone call at 4 am telling you to get your ass into the office or you won't have a job when the sun rises. Wishful thinking and youthful ignorance will only get you so far in this world. Confidence is useful. Arrogance is not -- because it is unfounded [confidence].
What do you mean by region based memory management and how does this get you recycled arrays for free?
In a vacuum, no data structure is better than another. Name a specific problem you want to solve, and then it will be possible to concretely discuss the pros and cons of dynamic arrays vs. linked lists vs. other approaches. Here's a sample concrete problem: managing a buffer to store the contents of a file in a text editor. What operations must be efficient? Primarily, inserting single characters at random points in the file. It should also be efficient to search for substrings. You may also want to be able to edit files that are too large to fit in memory. Also, operations only need to be fast enough that a human can't perceive a delay -- this gives you perhaps as much as 100 milliseconds just to move a cursor! Human interaction time scales are much more forgiving than some other timescales in computing, and that matters greatly when you are designing a solution to a problem. A historically popular solution to this problem is the gap buffer. A solution that would be a terrible idea is a linked list of characters. Other solutions are possible. I hope you see my point: With specific requirements and a clearly identified set of operations that must be efficient, it is easy to compare the merits of various data structures. Without a goal, you will just talk in circles, because there is no way to measure success. 
I guess the way I think about it is the opposites. I try to come up with efficient solutions without having a problem to solve.
And how do you decide what is efficient with no problem or specification? Are you aiming for efficiency in terms of performance? code size? or something else? What are the limits your hardware and tools provide? There's no universal best approach. You *need* to understand the problem domain and constraints to implement an ideal (or at least acceptable) solution for the issues you face.
But how can a solution be efficient all on its own? As an example, take dynamic arrays (a.k.a. vectors). They are great for random access: O(1), but they are terrible for random insertion: O(n). Linked lists have the opposite properties: O(n) random access, O(1) insertion. If you want to make a more sophisticated analysis, you also need to consider the cache behavior that is so important in modern computers. Which is more efficient? Neither is more efficient in general; one will be more efficient for any given usage. If your program spends 99% of its time inserting and deleting items and only 1% of its time performing random accesses, then linked lists look very attractive. For a different workload, a different structure will be most efficient. I hope you see what I'm saying. A *solution* is a way to *solve* a problem. Solutions only exist in the context of a problem.
You should probably look into the Mill architectyre if you haven't already.
That seems extremely similar to a stack-based register file. I already wrote down several ideas about the bytecode processing, and now that I look at this Mill architecture, it looks very very similar. I'm not gonna win any awards for an interesting virtual machine, but I'd like to see this Mill see some success. New CPU designs are great, and the more, the better.
I've explained this several times now. I've been clear. You seem to be the only one who doesn't understand and I don't know if that's intentional or if you just lack the knowledge necessary to understand what I've written. In either case, and with all due respect, go do your own basic research. If you still find it impossible to understand then I don't know what to tell you. It's really not that difficult of a concept... But then neither are ideas like "efficient" and "cheap"
That explains a lot... So to put it plainly, you don't have a clue what you're doing and your diatribe doesn't make sense, not because of our misunderstanding of the words that you chose here, but because, well, it's founded on nonsense :P. A solution cannot exist without a problem and while masturbating over Big Oh notations can be fun, it is, in the end, fruitless.
&gt; O(1) insertion Once you've found your place in the list :-). Prepend is O(1).
I try to find as universally good of an approach as possible.
I would say you should aim not to insert into the middle.
I disagree with you.
I think that solutions can exist without problems.