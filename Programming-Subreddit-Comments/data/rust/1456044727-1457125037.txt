And the first post is about Nicholas Matsakis' Parallel Iteraters! As someone who is Chinese and started trying out Rust sometime ago, I would love to show some support where I can.
I want a way to comfortably type ‚àß and ‚à®. Using normal mathematical notation in programming languages instead of strange sequences of ASCII characters would be a lot nicer.
That's great! We need help, you could join our chat room from https://chat.rust-china.org . And @zonyitoo who wrote [coio-rs](https://github.com/zonyitoo/coio-rs) and [context-rs](https://github.com/zonyitoo/context-rs) holds *coio-rs* and *context-rs* discussion there, someone interested could join us, those two channels are **in English** And to someone who might think we're just a static blog site. We're not just a blog, we only use the blog as our homepage and entrance to chat, wiki, translation etc. Actually we have setup the chat room and wiki for about 2 months and communicate with each other there. But to let more people know us and join us, we setup the blog as our homepage and link to our chat/wiki. The static blog site was just setup these two days
The meaning is "all references in `T` are valid for the duration of `'iter`". If there are no references in the type `T`, then this is trivially satisfied (so, e.g., `i32: 'static` is true).
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/botsrights] [Mod refuses to delete bot post, quoting \/r\/botrights](https://np.reddit.com/r/botsrights/comments/46ur4j/mod_refuses_to_delete_bot_post_quoting_rbotrights/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Hand in your available times in April and we schedule it on a date you can ;)
I will be sure to tell people about clippy in your honour!
I have a sort of "master" object, struct M, and it has a pointer to a file reader (which is in reality a BufferedReader&lt;File&gt;), like this pub struct M&lt;'a&gt; { pub reader: &amp;'a Read, Now I'd like to write a function bytes() which returns an iterator that gets a stream of bytes from the file, limited by a certain position and length. I'm quite stuck there, though. I think I know how to implement the iterator state in the struct itself, but I'd like to have the possibility to have multiple, independent iterators, and I can't quite get there. I've tried returning boxed objects that contain a pointer to reader, and FnMut closures that use reader to get a byte from a given position, but I always run into lifetime problems. So my question: is there an idiomatic way of returning an iterator object, or is it really better to implement the iterator on the M struct itself?
Well it works with a Response&lt;Empty&gt; and that's what made me think that it could be an issue with Response&lt;Box&gt;. However I don't know whether it's an issue with Response or Box that causes the crash, and I'm not sure if I'm even going to pursue this bug. Anyways, I don't really think it's a bug with Box itself, since [this](https://ebvalaim.mydevil.net/en/2016/02/03/rust-applications-with-plugin-api/) guy here is using a Box and he doesn't seem to be experiencing crashes
Mobile web is a crap: http://imgur.com/a/ReoKE üòû Edit: by "crap" I didn't meant Pijul homepage specifically but rather mobile browsing experience in general.
&gt; (we'd need to replace the calls to mmap with their Windows equivalent). I think you should be able to just use the [`memmap`](https://crates.io/crates/memmap) crate and get this for free.
It's not at all, no. this is one of the first discussions about it I can remember in a very long time. As said elsewhere in the thread, it will be in the new book though.
Of course! It's always great to see people who have come to their ~~senses~~ train station to join us, have a great time, and some K√∂lsch!
The problem I see with small libraries such as rust-jwt depending on sodiumoxide is just the dependency situation: It requires the user to install libsodium. * *ring* statically links against its C dependencies AFAIK. * Octavo is another option because it doesn't have any non-Cargo dependencies at all (in that sense it would be closest to a rust-crypto replacement) I know octavo is in a very early stage of development, but so was rust-crypto (according to the README). Why not Octavo then?
We don't have any plans or anything, I'm just asking: Do you have anything planned on the first Wednesday in April, /u/llogiq, /u/badboy_? :)
I agree about the API that libsodium exposes. What I rather meant to say is that it'd be nice if one could statically link against libsodium using sodiumoxide.
Can you give some info on the features missing from lmdb and that you're looking into implementing.
I think Github is blocked in China, that might be a big hindrance as the rust community is so reliant on it.
[rustdoc looks nicer than a directory listing without a Readme file :)](http://pijul.org/sanakirja/doc/sanakirja/)
You could always submit an RFC for this, and see what people think.
I think it's a bit too late for this, to be honest. The syntax is stable, and no one in their right mind will switch to the new operators, breaking compatibility with older Rust versions in the process. Maybe I'll try an RFC for 2.0, whenever that happens.
Unfortunately, I'm doing relatively tricky stuff with my mmaps: I need precise control over where the mmaps take place, and when they are unmapped. the memmap crate seems to be a bit too high-level (I understand the intention of providing high-level bindings to mmap, though).
I think the only thing stopping the following impl from happening: impl&lt;B, A: Mul&lt;B&gt;&gt; Mul&lt;Complex&lt;A&gt;&gt; for B { type Output = Complex&lt;&lt;A as Mul&lt;B&gt;&gt;::Output&gt;; fn mul(self, rhs: B) -&gt; Self::Output { unimplemented!(); } } is the coherence rules. Which makes sense, because it's a blanket impl and if two such impls clash, we have a problem. This would not be fixed if binary operators were overloadable two-input functions or anything else. However, this problem should go away once specialization lands; and that is something that is being actively worked upon and seems to be high priority. In that case this would just be a `default impl`, though I'm unsure how well that works with associated types. &gt; They're busy worrying about adding "higher kinded types" and "negative traits" I've yet to see any serious discussion amongst the rust developers about implementing HKT, fwiw. Everyone seems to agree that if we ever get that, we'll get it in Rust 2.0 or something. All I have seen is people bringing HKT up when some type gymnastics are not easy with the current type system. I haven't seen discussion about negative traits in like a year. The implementation is the same bare-bones implementation (which is useless for doing complex type system things) that existed when OIBIT was first implemented, and there's no indication that that's changing.
LMDB is an awesome library, and should definitely be used instead of this crate whenever possible. - The main thing that was missing for us (and we asked the authors of LMDB to add it) was the possibility to duplicate a base and share all but the difference, just like purely functional key-value maps. In Pijul, we need this for efficient branches: we want to have two different copies of a single repository in the same database, sharing everything but their differences. - Another thing is, Pijul needs to allocate cursors recursively, which is not the most efficient thing when repositories grow in size (number of files). Sanakirja opens the possibility of iterating a closure on the database instead of using cursors. Our solution is clearly not as powerful and versatile as cursors, but is more memory- and time-efficient in our particular use case. - B-trees are more compact than B+-trees, with no real difference in cache misses in our specific use case (we almost never iterate over more than one page). The amount of bookkeeping needed is also heavily reduced, which might make it faster (but again, with a difference in versatility. In our case, we compensate this by using closures, and the stack). - Another concern is, while LMDB is highly portable in the sense that it can be compiled everywhere, we don't have many guarantees for using it as a file format, a typical use case being x86-64 users using a Pijul repository from a SPARC server over NFS. (both page size and endianness are different between those two architectures). 
Ah! Pijul and licenses... Thanks for the link, we're not yet using Sanakirja in Pijul, and it might take some time (a month?) before we can do so. This is a license experiment, I like the MPL. If it works for Sanakirja, and people don't complain too much, or even contribute to Sanakirja, we might release libpijul under the MPL.
Octavio's head is working on Redox more then Octavo. 
I'd recommend filing an issue: https://github.com/danburkert/memmap-rs --- Perhaps your use cases could be accommodated!
Basically, its a design decision that the move/borrow distinction in such cases should be explicit, (to make reasoning about the code easier - you generally can know if a function call argument will borrow a value or not) so you need to explicitly use a borrow operator at least once to go from a value to a reference.
Done!
Oh well, it was worth a try. Good luck with your project! And, uh, Box has been a core primitive of the language for more or less forever, I would be absolutely astounded if there was this basic a bug with it. But it is possible that tiny-http screwed up in some unsafe code and did something like create two copies of a box (both of which would be dealocated, causing a crash). Unlikely but not something I would categorically rule out.
The "raw" module is an alias for "libgit2_sys" create. Command line for rustc does not contain reference to libgit2_sys crate (i.e., "--extern libgit2_sys=/path/to/libgit2_sys.rlib").
Like WhiteOak said, YCM's Rust completion is powered by racer. And yes, it was _originally_ designed for vim. The core of it has since been split out into *ycmd* which powers clients for Vim, emacs, Atom, and Sublime Text. You get a lot more than just Rust completions; I elaborated on why [you should use YCM over plain racer](http://blog.jwilm.io/youcompleteme-rust#differences-from-vimracer-emacsracer-and-othe) in the [Rust for YCM announcement](http://blog.jwilm.io/youcompleteme-rust). 
Your question is not very clear. What do you mean by "plugin"? Rust compile-time plugins (which are unstable and work on nightlies atm), or crates as plugins, or...? There is [dynamic_reload](https://github.com/emoon/dynamic_reload) if you are interested in loading your libraries dynamically.
From reading the GNU one, it looks like MPL 2.0 is only incompatible with GPL 3.0 if the MPL'd files use the "incompatible with secondary licenses" declaration. I think the Tom Hull one is talking about MPL 1.x.
Can't they use an outside proxy for the connection?
Or mirror it on a local git instance.
Yes, perhaps you could be charged for that though since they don't like Github.
As I wrote all that out without access to the internet, I found this: https://huonw.github.io/blog/2015/05/finding-closure-in-rust/ Which explains how rust's closures work in terms of C++ closures, but pretty much implies that the syntax is the way it is 'because'. There isn't an advantage or disadvantage to either, just that the rust developers chose the 'ruby-style' syntax because they could.
`move` does not disallow you to call the closure more than once; `move` means that the closure should capture its environment by value instead of by reference, which is the default. Whether the closure may be called more than once or not is determined by the set of traits (`FnOnce`, `FnMut`, `Fn`) it implements, which are usually determined from the signature of a function you're passing this closure to. Note that because `move` closures in Rust capture their environment by value, and because references, unlike in C++, are first-class entities in Rust, it is always possible to tweak how exactly values from the environment should be captured: let a = 42; let b = 88; let c = 0; let b_ref = &amp;b; let c_ref = &amp;mut c; let f = move |add: i32| { *c_ref = -4; add + *b_ref - a }; let r = f(5); Semantics of this code mirrors semantics in your example exactly. Rust just doesn't need some special syntax to allow one to capture different variables differently. As for why Rust uses bars instead of something else for arguments, I believe that there were no special reason. As far as I remember, this was inspired by Ruby, back at the time when we had `do` keyword. It was equivalent to calling a function with a closure as its last argument: whatever(|x| { ... }); do whatever() |x| { ... }; `do` is also used in Ruby for similar purposes, so I guess it may be somehow connected to the chosen syntax of closures, but I may be wrong, of course.
Can you explain what you mean by &gt; `move` does not disallow you to call the closure more than once; See this playpen: http://is.gd/FHCfiM It makes sense that `move`d closures cannot be called more than once, because any moved variables can themselves be moved further out and become invalid the second time. In the playpen I cannot call it more than once either.
Learn something new every day (you're using rust parameter syntax btw, been there too when switching back to C++) :) That's really ugly though, and prone to error if you use x afterwards. Still a +1 to rust.
To rephrase, not all `move` closures disallow calling the closure multiple times. In most cases `move` closures will be only callable once, though. http://is.gd/zfMIiM is an example where a move closure is being called more than once. `move` means that any variables referenced internally will be captured "by move", whereas without `move` they will be captured by reference. However, both kinds of closures need not themselves move the capture clause when being called. `FnOnce` closures like the `drop` one you link to do move variables (in this case `vec`) out of the capture clause. `Fn` and `FnMut` closures `move` determines if the environment is moved into the capture clause when the closure is _created_. `Fn`/`FnMut`/`FnOnce` (which are automatically figured out for you) determine if, when _called_, the capture clause is borrowed/mutably borrowed/moved out. It's the latter (specifically, `FnOnce` closures) that restrict you from calling closures multiple times, however you usually need the former (a `move` keyword) to get the latter.
Unfortunately that's not it. The libgit2_sys crate is picked up in the search path given with the -L parameter. Just to be sure I did try it again, but the same 248 errors pop up.
Instead of modifying the existing traits, it's also conceivable to add new RAdd::radd, RMul::rmul, RDiv::rdiv traits too. This would help avoid breaking existing code.
Shouldn't it be possible to make a mirror for Rust on an accessible site?
We actually had this already. When unboxed closures first landed, the closures would be specified like `|&amp;: x: i32| {...}`, `|&amp;mut: x: i32| {...}`, and `|: x: i32| {...}`. This would set which `Fn*` trait would be implemented by the closure. Thing is, this is entirely redundant, so it's inferred now. On the other hand, `move` still exists and gives you the ability to determine how the environment is captured. Given the above inference, this is the only thing you need.
Note also that while you can capture one variable by move, you cannot capture a parameter pack by move in C++14 (and very likely also in C++17) because lambda captures don't work with variadics: template&lt;typename... Args&gt; auto capture_args(Args&amp;&amp;... args) { return [xs = std::move(args)...]() { do_stuff(xs...); // won't compile because you cannot // capture argument packs } } so you need to do stuff like: template&lt;typename... Args&gt; auto capture_args(Args&amp;&amp;... args) { return [xs = make_tuple(move(args)...)]() { // &lt;- pack into a tuple std::experimental::apply([](auto&amp;&amp;... args) { // &lt;- unpack tuple into a function // your original stuff goes gere: do_stuff(std::forward&lt;decltype(args)&gt;(args)...); // &lt;- original code }, xs); } } where you need two things. First you need to pack the variadics into a tuple, capture that, and then explode the tuple into a function. For that to work you need to move your original code into a different lambda. That's quite a contortion. You might think that C++ lambdas "are nice", but they are only nice until you use them with the rest of the new C++ features. Notice also that using `std::forward` from within a generic lambda is pretty ugly, and until C++14 you couldn't capture by move at all (without incredible contortions). Right now you still cannot use lambdas with constexpr, they still don't play well with variadics (as shown above), they don't play well with attributes (there is no way to mark the `operator()` of a lambda with an attribute)... OTOH Rust lambdas work pretty well with the rest of the language. The only ergonomic issue I know of is the lack of `abstract return types`, but that is orthogonal to lambdas itself, wasn't also possible in C++ until C++14, and it remains to be seen how well will C++14 solution play with the Concepts TS.
Seen that but I also have https://crates.io/crates/bcrypt/ for an easy to use hashing
It sometimes gets blocked, but is currently accessible. There are a large number of mainland Chinese projects hosted there so it does actually get used.
&gt; I think the only thing stopping the following impl from happening is the coherence rules. Which makes sense, because it's a blanket impl and if two such impls clash, we have a problem. No, it doesn't make sense. It's just the way it is, and it's arbitrary (apparently favoring replaceable hash functions [1]). You're used to it, so you seem to think it's common sense. Ignoring the mixed type non-commutative thing for now, and using just a single type - The following impl compiles just fine, and it is every bit as much of a blanket impl as your disallowed example: impl&lt;T: Mul&lt;T, Output=T&gt; + Copy&gt; Mul&lt;T&gt; for Complex&lt;T&gt; { type Output = Complex&lt;T&gt;; fn mul(self, other: T) -&gt; Complex&lt;T&gt; { Complex::new(self.re*other, self.im*other) } } In both cases, one of the two arguments is Complex&lt;T&gt; and the other is T. There shouldn't be any "clash" problems. It's very inconsistent for the compiler (and language design) to say one is fine but the other is a problem. I think the Rust syntax for this is confusing because the operands and types are in mixed order, so forgive the following example in C++: template&lt;class T&gt; Complex&lt;T&gt; operator *(T a, Complex&lt;T&gt; b) template&lt;class T&gt; Complex&lt;T&gt; operator *(Complex&lt;T&gt; a, T b) Rust has simply decided the first one isn't allowed. Oddly though, I can manually create it for any specific type: // Complex&lt;f64&gt; operator *(f64 a, Complex&lt;f64&gt; b) impl Mul&lt;Complex&lt;f64&gt;&gt; for f64 { type Output = Complex&lt;f64&gt;; fn mul(self, other: Complex&lt;f64&gt;) -&gt; Complex&lt;f64&gt; { Complex::new(self*other.re, self*other.im) } } So if there was some overarching philosophical reason why I shouldn't be able to add operators to types I didn't create (outside my crate), that rule isn't enforced. I just have to create macros to manually do what templates can't. &gt; However, this problem should go away once specialization lands; and that is something that is being actively worked upon and seems to be high priority. Do you have any particular links I could read? [1] - http://smallcultfollowing.com/babysteps/blog/2015/01/14/little-orphan-impls/ 
That seems like a good idea, but the devil is in the details. It's really nice to be able to clean up everything by just deleting the "target" directory. No need to worry about caching, old binaries compiled with different Rust versions, etc. - not to mention the cache filling up space as packages get updated again and again. Apart from that, there are a lot of different configurations that can be selected for the build (opt-level, feature flags). I don't mind the initial compile time so much; what I'm hoping is that we'll get incremental rebuilds soon. Rebuilding the whole crate every iteration can get quite a bore, and eats a lot more time than the first build after checkout.
This post is amazing. Thank you.
The Dyon language (previously Dynamo) is more robust now, but requires more work! Contributors are welcome.
Nice! I've left you some comments on some safety issues ([#13](https://github.com/Immington-Industries/rust-wlc/issues/13)). I recommend you read the [Rustonomicon](https://doc.rust-lang.org/nomicon/) if you plan on doing anything with the `unsafe` keyword.
I generally recommend that everyone use GPLv3, LGPLv3, Apache 2.0, or MIT licensing, or at least dual licensing with more than one of those or those and another license. Picking from those 4 licenses gives you a reasonable choice from most restrictive (in the copyleft sense) to least, while reducing licensing proliferation and headaches caused by it. Those licenses are widespread enough that pretty much everyone who cares about licensing already knows about what restrictions they each have, and how compatibility works with them. That particular set of licenses is ordered in terms of compatibility and restrictiveness. That means that if you choose one of those licenses, your work can be used in pretty much any other free software project, though with different levels of copyleft depending on your personal preferences in that area. If you choose MIT or Apache, you will pretty much never have any further discussions about licensing, as they pretty much permit anything that people want to do (unless someone wants to use your Apache licensed code in a GPLv2 only project; however, any GPLv2+ or GPLv3 project is fine since the GPLv3 was explicitly written to be compatible with the Apache 2.0 license). If you choose LGPL or GPL, then the only real discussions will generally be from people who disagree with copyleft; but anyone else who is on board with copyleft is almost sure to be either using one of those or dual-licensing under one of those. Anyhow, sorry for the derailment onto licensing discussion, I just really want to get to a place where we don't have to worry about license compatibility issues other than where on the copyleft continuum it falls; I find that copyleft and permissive licenses can have their place, but too many of them and you start getting combinations that don't work together even if the projects agree on how far copyleft or permissive they want their code to be.
&gt; It's just the way it is, and it's arbitrary (apparently favoring replaceable hash functions [1]). You're used to it, so you seem to think it's common sense. C++ has even more arbitrary overload resolution rules to solve this problem. I do agree that the orphan rules are one solution amongst many to solve this problem. My comment there was that it makes sense that there _is_ a problem in the first place. I don't think the orphan rules are common sense; they're quite involved and confusing (though usually that part doesn't come up so often). I do think that their necessity makes sense. &gt; In both cases, one of the two arguments is Complex&lt;T&gt; and the other is T. Yes, but in the second case you're saying that `Complex&lt;T&gt;` implements a trait, and in the first case you're saying that all `T` implements a trait. When defining a function elsewhere parametrized over `T`, you _can_ say `T: Add&lt;something&gt;` in the parameter list, but you cannot say `Complex&lt;T&gt;: Add&lt;something&gt;` in the parameter list (you can do so in a where clause, but those work differently with coherence). It is this asymmetry that leads to the asymmetry in the orphan rules. &gt; So if there was some overarching philosophical reason why I shouldn't be able to add operators to types I didn't create (outside my crate), that rule isn't enforced. I just have to create macros to manually do what templates can't. Let's say you had an impl of `Add&lt;Complex&lt;U&gt;&gt; for T`. At the same time, some other crate decided to impl `Add&lt;U&gt; for T where U: SomeTrait`. These two crates are used by a third crate, that defines concrete type `Foo` and impls `SomeTrait` for `Complex&lt;Foo&gt;`. Now we have two conflicting impls. This problem does not appear when you restrict yourself to impls to concrete types. The issue with a blanket impl is that it applies to every type that satisfies the constraints, even types from reverse dependencies that you haven't thought of. Not a problem with concrete impls; they only apply to known types. The core philosophical thing here is that Rust believes that APIs should not leak their internals. This happens all the time in C++ with templates (which are more of a fancy macro system since they don't actually type check anything), where a change in the internals (but not the API) can break a template, and where template errors often require you to understand the contents of the template itself. Rust actively avoids this; the impl/fn generic signatures are enough to tell you how something will behave. Which is why it considers impl conflicts a problem; since a method of arbitrarily choosing an impl would be brittle (i.e. would change if you perhaps move around your dependencies without any APIs changing). Specialization actually breaks this philosophy, but in that case you're explicitly opting in to that behavior ("I'm okay with this impl conflicting with others, just choose what makes more sense") so it's okay. &gt; Do you have any particular links I could read? https://aturon.github.io/blog/2015/09/18/reuse/ https://github.com/aturon/rfcs/blob/impl-specialization/text/0000-impl-specialization.md I believe there's been some further discussion and tweaking after this (how it works with associated types is a rather interesting problem, for example), but I don't recall what. 
You know, clippy breaks everyone's builds every time there's a Nightly update that changes the compiler...
It does, but it's easy enough to update clippy. And we usually have a new version within a few hours of a breaking rustc change.
There's nothing weird or particularly low level there. If you want to talk to C you'll probably have to learn some C. Look at other FFI bindings to pick up Rust-side conventions. I was learning by hacking on rust-openssl and rgtk (which has become gtk-rs since).
For your first question: because the entire point of the borrow checker in Rust is not assuming anything involving pointers. If you want to do something that can potentially lead to use-after-free, you have to convince the compiler that it won't, or your code simply won't build. It may sound like a bad thing, but it makes it very hard for you to accidentally shoot yourself in the foot, which happens all the time with manual memory management (*cough* C *cough*).
Unfortunately, this is currently less tractible than it might seem. First of all, libraries compiled with different versions of the compiler (as in, even successive builds) aren't compatible, so every time you change the compiler, you have to rebuild the world. Then there's the fact that *all* the compilation flags have to be the same. Feature flags, optimisation level, etc. The killer, though is: position in the dependency graph. No, really, the result of compiling a library directly, then compiling it with exactly the same compiler and settings but as a dependency of some other crate, are *not* compatible. Which is insane, and a large reason why I gave up on my hot reload crate. :P But yes, given that I use Windows (and thus have to sit through `winapi` doing a 5 minute compile over and over and over and over and over and over and over again), I'd love to see this, too.
There are some pretty great alternative operating systems that can help with this problem :)
That looks like an excellent resource! It's a lot more comprehensive than the other things we looked at. 
We don't have anything currently built-in.
 &gt; Anyway, specialization is a less brittle solution which should be easier to use (it also gets us a lot more than just fixing this problem), and since we're getting that soon, I don't think this solution is necessary. Are you guys going to loosen up the rules on templated traits, add new right-handed trait operators (RMul::rmul), or something else? Will specialization allow complex numbers and matrices work as we've discussed in this thread? 
Specialization lets you loosen up the rules with the tradeoff that your impl may get overridden by someone else. It should allow that, in its final form (i.e. with some handling for associated types; not sure how that's going to work). Oh, also, don't call them templates :) The correct term is generics; templates are C++s substitution-based answer to generics.
You're totally right, I didn't really realise that the content of this post is super lacking. I've spent a lot of time working on this and definitely need to make it easier for someone to understand what the hell I'm asking! I'll make some updates to my initial post to try and make it clearer. With regards to the SO responses you mentioned. The SO question ended with the issue on the rust-lang repo that I posted. It was then pointed out that this was more a design fault than a language issue. Though I have to admit that I'm not sure why there is different behaviour when lifetimes are involved. Oh and thanks for taking the time to write this response - it's appreciated :). More information coming soon!
Hey, as someone who commented on licenses the first time I just wanted to say that the MPL would be more than liberal enough for my use cases. Thanks for making awesome software!
That error you're getting seems to be a bug in 1.6, that has been fixed in later versions of the compiler (it works in 1.6 if you just don't specify the type of self, which you normally don't because there's no need). Try the "beta" or "nightly" versions instead.
I'm not super familiar with the use cases for intrusive collections, but I'm wondering: is there a way to create a `Link` structure which owns the `T` and is 'anchored' by also owning a reference into itself, preventing moves? This would also prevent you from mutating the items, which is probably not what you want, but it would be safe.
I lost some of the drive to work on [my GBA emulator](https://github.com/Ketsuban/rustboyadvance), but I want to concentrate a little on refactoring and documentation rather than implementing more instructions right now. My medium-term goal is to get the bare minimum going to escape the BIOS into ‚Äúuser code‚Äù and then start soliciting test ROMs to fill out the instructions I'm missing‚ÄîI'd like to get a full CPU core going before I start looking too much at the I/O registers. I spent some time over the weekend following some of /u/phil-opp's OS tutorial; I'd like to find out whether Cargo can do the bit which currently requires a Makefile.
I have deployed a couple of sites that use the iron web framework. I use [supervisor](http://supervisord.org/) to keep the web server running on a local port and use nginx to proxy that local port to port 80 or 443. In my nginx configuration file I have the following proxy settings: location / { proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-NginX-Proxy true; proxy_pass http://127.0.0.1:3000; proxy_redirect off; } I also serve all static files through nginx rather than the iron static file middleware: location /static/ { alias /path/to/static/; autoindex off; }
Building the assembly should be easy with a [build script](http://doc.crates.io/build-script.html) and [nasm-rs](https://crates.io/crates/nasm-rs). But I'm not sure if it's possible to override the linker script.
It does do hashing: https://briansmith.org/rustdoc/ring/digest/index.html. It also does PBKDF2: https://briansmith.org/rustdoc/ring/pbkdf2/index.html. In fact, it does them very well (see the tests) and very fast (see the benchmarks). Note that rust-fastpbkdf2 currently does PBKDF2 faster, but it requires OpenSSL. *ring* doesn't do bcrypt, intentionally, because I think bcrypt is probably a worse idea than PBKDF2 given the current state and amazing future state of SHA-256 implementations. Why waste time with bcrypt when we could instead spend that time on building the best implementation of Argon2?
GitHub was once blocked by GFW, but soon got back. What annoys Chinese Rust programmers most is AWS S3 service is blocked. And we know GitHub project release packages are stored on S3 and so it is with crates on https://crates.io and rust installation packages. Only archive files can't be easy accessed. So we made an efforts to let [USTC](https://mirrors.ustc.edu.cn) to mirror [rust archive files](http://static.rust-lang.org/dist/index.html) and https://crates.io If you're interested, see our [wiki](https://wiki.rust-china.org/ÂõΩÂÜÖÈïúÂÉè) So now network is not that hurt for Chinese rust programmers **Edit**: *Disclaimer*: Our wiki page are all in Chinese
Brilliant!
In general, scripting usually also serves as a way to sandbox code which rust isn't designed to do. Eg. multiplayer games such as Garry's Mod and Natural Selection 2 allow the server to send lua scripts to the client, you obviously want to sandbox that (although I've no idea about the actual sandbox security properties of lua). But if sandboxing isn't a desired property then I agree (I didn't look too much in this Dyon language to see if this is the case).
&gt; Hope that gives you a little better background on the design history, and the advantages of the current design over a C++ style design. It very much does! Thank you for taking the time to write all this out.
Right, why not just use the timeouts on keep-alive, read and write in iron/hyper? It doesn't seem that different from what nginx does to mitigate DDoS attacks. Then, load balancing is certainly an issue, but it doesn't seem that hard to add to Iron, and the main Iron developer seems to react to requests and comments really fast. I'm also not too sure of how the nginx proxy works, but it seems to copy the responses to a buffer, and from that buffer to the kernel's buffers (instead of just one copy from iron to the kernel's buffer): http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering 
I'm looking into building my own RangeSet implementation, which we could end up using in clippy. No public code yet, though.
You should add a link to your twitch to youtube videos. Also here. :)
One can somewhat sandbox code of compiled languages as well. 1. Restrict dangerous features such as pointers or `unsafe`. 2. Restrict module imports. 3. Compile and link against a sandboxed API. 4. Run the stuff. Space Engineers does something similar with C#. There are, however, few difficulties, depending on whether you use scripting for the game-specific code or for ingame-programming. Sandboxing in Lua is btw. done like in most other scripting languages: Just don't give it direct access to non-sandboxed functions and data structures.
This is interesting: it seemed to me that memcache (of static contents) could be done very easily in iron, and even more easily using include_bytes!, which would make deployment on several machines even easier (basically, this uses the CPU cache as the server cache).
&gt; ring doesn't do bcrypt, intentionally, because I think bcrypt is probably a worse idea than PBKDF2 given the current state and amazing future state of SHA-256 implementations. Can you expand on this? From my lay perspective, I thought bcrypt was considered one of the best password hashing functions to use. Is that not the case anymore? And how does SHA-256 play into bcrypt vs PBKDF2?
Actually it is not quite true. To be honest I currently work on either. This is due my day to day job and there is currently no time to work on these Rust projects. But I am trying to do so with every free time I have. Recently Octavo was splitted into smaller crates in monorepo (at least for now). Now I am working on cleaner version of rust-crypto bitslicing AES implementation. 
&gt; I see you are using cmark, so I guess you are currently limited by what this supports. Tables and footnotes are supported by pulldown-cmark (if that is what you were talking about). Maybe the limitations are elsewhere in the toolchain? :)
bcrypt was designed with the idea that it would be difficult to write custom hardware to attack it because it requires the use of some writable lookup tables, which used to be difficult to do in hardware. However, FPGAs now commonly include RAM, so bcrypt-specific hardware can now be developed relatively easily and cheaply, just like custom PBKDF2-SHA256 hardware. This basically removes bcrypt's theoretical advantage over PBKDF2-SHA256 from the attacker side. At the same time, Intel and ARM are now adding SHA-256 instructions to their processors, meaning that PBKDF2-SHA256 will be (is) much more efficient for defenders to use than bcrypt. In other words, it seems like going forward, attackers will be able to do bcrypt as efficiently as they can do PBKDF2-SHA256, but defenders will be able to do PBKDF2-SHA256 much more efficiently than they can do bcrypt. Thus, it hardly makes sense to use bcrypt over PBKDF2-SHA256. I haven't, however, calculated the exact inflection point for any particular implementations. If you really care about such things, and you don't have external constraints, then there are other alternatives like Argon2 which should be considered.
Damn, I really need to speed up the release my wayland-server bindings, so people don't need to use such a big C library (while it's a really nice one) to write a rust compositor!
Not really, is it? Literally it only applies to a trait and a lifetime.
Why would you want to memcache static content? That is precisely what your file system buffers do, which is why statically serving from disk from a proven webserver like NGINX is the way to go. Possibly using things like sendfile: https://www.nginx.com/resources/wiki/start/topics/examples/xsendfile/# One of the reasons why reverse proxies are important in HTTP deployments is that they are a major element of _indirection_. Pushing static files to the wire (even from a networked file system) is a very solved problem. Unless someone writes an apache/NGINX alternative in Rust, I would not go without them.
Oh yeah, I was advocating scripting languages in general, rather than Dyon. I've never used it and can't speak for it. &gt; And I would argue that most of Rust's inconvenience comes from immaturity. I disagree. Many inconvenient features aren't going anywhere, and can't be obviated using autocompletion: * Implicit immutability * By-value vs by-reference vs by-mut-reference function invocation (most scripting languages just treat every non-word-sized object as an unbounded-lifetime mutable reference) * "Ownership" in general (`struct Foo&lt;'a&gt; { bar: &amp;'a Bar }`) * Static name and member lookup (`use x::y::{z, a, b}`) * No mutable global variables * No implicit polymorphism (writing a polymorphic fn requires you to entangle yourself in generics, traits, etc.) * A standard library focused on performance and absolute low-level control (which massively complicates the documentation compared to a scripting language - think of `str` vs `String` vs `OsStr`, for example) * Typed arithmetic (`((i as u32) + 100u32) as f32` - most scripting languages are just `f64` everywhere) * No arbitrary shared access to mutable data (possibly the biggest inconvenience of all) To be clear, when I talk about "iteration time", I'm talking about the time spent wrestling with these features (including the time spent *thinking* about their complex model, as well as the time spent writing them out), rather than the time spent sitting waiting for the program to compile.
Interesting, thanks. I will have a look at Argon2 as well
Well, it doesn't try too hard to be readable...
I had to read this comment half a dozen times to really understand it, but now I feel like I have a much more coherent understanding of how rust coerces and flattens references instead of viewing it as just "rust doing magic rust things". Thanks for the explanation!
Since I can't play around with Vulkan directly right now, I've been working on a library for dealing with SPIR-V modules. I want to get some reflection capabilities up first, but ultimately want to have validation, modification and possibly even code generation in the long run. 
Give this man a reddit gold!
If someone decided to write game in Rust, he would be ok with most of these anyway. I guess we will see. 
If you have an ECS managing your object ownership, Rust code can be quite lightweight - see the compiler for an example. 
This is my favorite series right now. Bar none! (draws level with Better Call Saul though). Truly, I'm so into this. Never stop.
With the context that generics provide this `+` operator which applies to multiple things and lifetimes uses that same op which also has an extended form regarding traits, it isn't that big a jump to skip it. Having said that, it really should be explicit. The fact it isn't mentioned directly somewhere (if not there) is probably just an oversight.I guess I didn't look that carefully before I posted.
Which in the long run won't be a niche.
Feedback on the API would be very welcome!
It isn't a big jump, but I have learned that with rust you have to take things rather literally. The problem, of course, is how to make things understandable and detailed at the same time.
Macros &lt;3 https://gist.github.com/jFransham/017bd5bd8f693a04749a
Thanks. fixed.
Not enough to be able to give definite answer, but from my shallow understanding, it could help. It features relational-level operators like group_by and join, but also focuses on integrating incremental changes to a computed result, which comes to a cost. This may actually be the right experiment for the next post of the series :)
You might also want to think about compiling. As far as I know you have the following options: 1. **Compile on dev machine and copy over everything.** That's the worst idea, because you need to ensure that libc (especially) and all dependencies of your system and the VM are compatible. 2. **Use the production VM for compiling as well.** With cargo that's rather easy but it's not a very good process to use resources of the production system to build your project. On the other hand it's easy for very small projects. 3. **Use a VM clone for building.** Use a VM that mirrors the same system as your production machine for building. That works well, but don't forget to copy the right files (esp. dependencies) afterwards. `ldd` might be your friend. 4. **Build a static variant on your dev system.** You can build a [musl version of Rust](https://doc.rust-lang.org/book/advanced-linking.html#static-linking) and then emit static binaries of your project. That's awesome, easy and works very very well. Be careful with processor-specific optimizations or you get `illegal instruction` errors.
Wrong Rust. You want /r/playrust.
Another way to say "moi Rust users in Helsinki! Interested in a meetup?" ;-)
Over the weekend I've been working on a [Travis/AppVeyor setup](https://github.com/japaric/rust-everywhere) that builds your Rust program on Linux, Mac and Windows and then uploads it to GitHub releases. I got the technical part working: Just `git tag v1.2.3 &amp;&amp; git push --tags` and [ta-da!](https://github.com/japaric/rust-everywhere/releases) Your program has been released for all [tier-1](https://doc.rust-lang.org/book/getting-started.html#tier-1) platforms. I'm now writing some docs explaining how to use this setup in your project. Next, I hope to release the first chapter of [copper](https://github.com/japaric/cu) (a wiki/book/blog/thing about using Rust to program Cortex M microcontrollers) this week, but [this Cargo PR](https://github.com/rust-lang/cargo/pull/2241) must land first. The first chapter will be about setting up the development environment (DE). What I like about the DE is that the build system is 100% Cargo and 0% Makefile nosense. Also Cargo subcommands! `cargo debug` to flash your application and start a remote `gdb` session on your microcontroller. Also releasing [criterion](https://github.com/japaric/criterion.rs) 0.1.0 on crates.io this week. I've been sitting on it for too long.
You are missing a `)` at the end of `.scoped` ;) fn main() { timely::execute_from_args(std::env::args(), move |root| { let peers = root.peers(); let index = root.index(); root.scoped::&lt;u64, _, _&gt;(move |builder| { [...] }; } } Great post as always
Keep in mind that in the majority of cases, objects will be allocated on the heap rather than on the stack. So for example: static mut l = LinkedList::new(ObjAdaptor); let a = Box::new(Obj::new()); unsafe { l.insert(Box::into_raw(a)); } // later let a = Box::from_raw(l.front_mut().remove().unwrap()); `l` is a static, and so can't have a lifetime. `a` has no lifetime because it is allocated on the heap and converted to a raw pointer. The idea with intrusive containers is that you should create your own safe wrapper around it that maintains the required guarantees.
I also have to get my hands dirty and try it out. Will give you feedback if I have a chance.
That's a separate concern, the calling mode of the closure. It is orthogonal to capture mode.
"I'm curious, what are the main differences with mdBook?" - the objective was to quickly have PDF and Epub output, even if it meant not supporting or partially supporting some markdown elements. - it doesn't use the `Summary.md` format (which comes from gitbook, I think?) which I found a bit verbose and redundant (i.e. instead of having `- [chapter name](chapter_file.md)` the name is taken from the content of `chapter_file.md`) I guess the fact that it's more targeted at "novels", ot at least books that have a simple structure. E.g. in mdBook you can specify a nested structure, whereas I just handle level-1 chapters. As a result, listing the markdown files to include is a bit less verbose than mdBook's (or gitbook's) Summary.md. "Did you start this project because of some features lacking in mdBook or rather for learning purposes?" Definitely more for learning purposes :) Honestly at first I just wanted a way to list various markdown files and output one markdown file that could be processed by `pandoc`, but I got a bit carried away when I started toying with pulldown-cmark... "Also, especially if you target longer format books, you will probably want to split up the book into multiple HTML files to decrease loading times :)" Yeah I should definetly have an option for that at some time :)
&gt; The key is that it is a lazy DFA. Right, you perform lazy computation like RE2. All of my attempts to date have been eager DFAs, so there was my source of confusion. &gt; [RE2 does](https://github.com/google/re2/blob/master/re2/set.h), which is where I got the idea. :-) And of course I would forget that. :) It's been quite a long gap between me looking into what RE2 offers as an API and how RE2 actually works under the hood, so it seems in the interim that slipped my mind. &gt; While it's technically slower, I think isolating the set of regexes down to a very small number using `RegexSet` and then running those again to find captures is probably good enough. :-) That was the general idea. Looking back at the API docs again, the library consumer just has to maintain a reference to the vector used in the constructor, and you do have capturing group syntax demonstrated, so it looks like RegexSet already can do what I want. 
Yes, the limitations are mostly because I was lazy and I felt it could wait :) 
To clarify, you can use capturing group syntax in regex sets, but they won't actually capture anything and you can't extract any information about them. More precisely, in regex sets, `(re)` and `(?:re)` are precisely equivalent.
No need for a whole VM clone, you can just use a chroot environment for building that has the packages installed that you need to build against. Here's [Debian's instructions](https://wiki.debian.org/chroot), which should work well for Ubuntu as well; there are probably similar instructions for Fedora and other distros out there.
Adding system clock support to our unikernel Crust, and laying the foundation for network support.
You should try and spin off your network stack into its own crate! I'm sure I'm not the only one who'd be interested in a `#![no_std]` Rust network stack :)
May I suggest [movecell](https://crates.io/crates/movecell). I made a sketch of a library that uses it to build intrusive graphs. https://github.com/bbatha/movecell_graph
yeah, I probably need to be more consistent with that. There's a link on the project repo atm; either way it's http://www.twitch.tv/ferrisstreamsstuff
haha, awesome! thanks! :D
How does one begin on programming neural networks? Do you have any resources you might not mind sharing? 
Yeah, it's quite similar, but I plan in allowing any PartialOrd-bound type as range bounds. Using it in clippy would imply taking arbitrary Constants as bounds.
That is what im using but it gives signal 11 in osx
Be nice if the set elements could be named. edit: Wait does that just tell you which regex matched? Oh, I thought it would return the set of Captures.
It's a question about rust programs loding code to do other things
Nope. The regex-dna benchmark needs positions for all matches, which this can't give. This can only tell you which regexes in a set match. (Interestingly, the regex-dna benchmark is likely going to get slightly slower when it starts running with the lazy DFA. It's a weird case of overlapping optimizations... *sigh*)
There is nothing wrong with writing entire games in Rust! However, there are many benefits with a domain specific language. It is easier to get stuff done with less typing and more instant feedback. For example, last year I made a meta parsing language: https://github.com/pistondevelopers/meta. Before, I could not wrap my head around parsing, now it is much easier! In theory I could write all parsing logic in Rust, but it would take a lot of work. Having a domain specific language makes it faster to try new ideas, so when you get a big idea, it is no longer that scary because you have a better understanding of that domain. Of course this dynamic changes greatly across team sizes, project structure etc. and there are downsides with every language. I guess that is why programmers like to discuss them. Language design is itself a domain that I love, and Dyon is a way for me to explore it. I make it simply because it is fun! If you thought Dyon is even a choice for you to consider when developing a game, then I take it as a compliment, considering the language was started one month ago...
I had an idea about a game with programmable computers inside the game as part of the gameplay. This lead to the idea of dynamic modules, which could be used to script a virtual computer running its own script. It is not perfect yet, need a way to opt out of standard intrinsics.
The answer is, unless you're running a production system that will be serving major traffic, you don't really *need* one. Rust/Iron will do well enough in the vast majority of cases. It's a good practice to use it though, since it's unlikely your application code will be better at handling static files and juggling sockets.
As one of the Crust guys, I'd definitely be interested in your network stack being usable from other kernels ;)
I'm 'stuck on nightly' as you put it for quite some time, and the amount of breakage I've witnessed is entirely cromulent. ;-)
I've been translating octave stuff from the Machine learning course in Coursera. It's not that hard once you start working with a good linear algebra library like nalgebra. I would link the course but I am on mobile lol. Just look up machine learning course Andrew Ng and it should get you there. 
You'd almost certainly need to hack on rustc directly in order to implement something like this. I'd recommend speaking with the developers at the #rust-internals IRC on irc.mozilla.org (https://chat.mibbit.com/?server=irc.mozilla.org%3A%2B6697&amp;channel=%23rust-internals) or at the developer discussion board at https://internals.rust-lang.org/ .
I believe Servo/WebRender use the default "give us an OpenGL context please" thing from glutin and don‚Äôt do anything specific to Optimus or other hybrid GPU system.
One more question though. If I remove the derives of Clone and Copy the compile error will not mention anything about unsuccessful cloning, clone() returns a reference. Why? This feels wrong for some reason. https://gist.github.com/561cf6d066a73ca2e022
I (think) I need `Model` to contain `Algorithm` because of the way I'm using the Algorithm in this setting. It will look something like this: impl Wing&lt;'a&gt; { pub fn optimize_wing(&amp;mut self) { // Will use the algorithm as well as compute_fitness to update the layer_sizes. } } The idea behind this usage is that it allows me to choose a default algorithm (with settings on the algorithm) that will generally work best for this model. The user can then simply create the model and optimize it. That said, you may be right that it is just easier to force the user to explicitly define the algorithm and use that to optimize the model. My initial thoughts were that for a lot of users this may be daunting. I wanted to achieve simplicity whilst allowing users to be very specific about how things are handled. As for the `Inputs` and `Outputs` you're right that they make things messy and should probably have been excluded. They are an additional obstacle to overcome (I want my algorithms to work for models with generic inputs and outputs, but the inputs and outputs must be a part of the optimization function). I'll leave them out for now. I hope that's clear? I'll have to think a little about your given example - it certainly is clean enough but will require a big overhaul of my library at the moment.
&gt; (FWIW, I'm marching towards 1.0, so we'll have one shot at making revisions to this API in the RFC.) If the `RegexSet` API is not quite finalized, it may be possible not to stabilize it for 1.0 and add it later instead. Adding a new type should be backward compatible, no? 
External crates don't have access to `std`'s stability system. But I mean, sure, I imagine we could devise something in the spirit of your suggestion, like adding a sub-module called `unstable` or something. But it would live in a weird place outside of semver. And of course, we could just bump the major version of the crate, which we would ideally not be afraid of doing more frequently outside of `std`.
&gt; External crates don't have access to `std`'s stability system. Ah sorry. I thought that by push to 1.0 you meant integration in the standard library for some reason... 
Is there a tutorial anywhere for using the sysroot thing? Or existing examples?
&gt; I've read somewhere that using clone() is cheating the borrow checker. It's not so much cheating the borrow checker as it is making tons of copies. Lots of copies? No problems. It may be significantly less efficient than a version which doesn't copy, yet properly manages references though.
No, `clone` is not at all cheating the borrow checker. Obviously it can be used as such (often at the cost of allocating a lot, depending on the types involved), but that doesn't mean that all clones are bad. In the code I presented the `clone` does not allocate, it merely increases a refcount and performs a memcopy. The clone is also the most obvious solution here, because we cannot keep a reference into a named wire while simultaneously mutating the hashmap.
Making lots of copies? I dare you to write a solution that allocates less :p
Why another scripting language ? What Dyon offer that other scripting languages (like Lua) doesn't have ?
Thanks, that's what I suspected.
Thanks, that's what I suspected. I've been digging through the rust internals to try to understand how everything is implemented. It takes some work!
All of your points are valid. I agree that it's not at all clear which would be superior. There's something to be said for your dead simple approach. KISS and all that.
&gt; However, this problem should go away once specialization lands; and that is something that is being actively worked upon and seems to be high priority. In that case this would just be a default impl, though I'm unsure how well that works with associated types. This is playing with fire. Taking the route of allowing default implementations and specializations that may be used, but maybe not, is dangerous. I've written plenty of C++ code that compiles and I think it should use my specialization but I need to jump through some casting hoops to make sure my type is used. The following is way easy to get wrong and it would be unfortunate if Rust gained a similar foot gun: #include &lt;iostream&gt; #include &lt;functional&gt; template&lt;typename T&gt; void f(T x) { std::cout &lt;&lt; "general impl" &lt;&lt; std::endl; } template&lt;&gt; void f(long x) { std::cout &lt;&lt; "long impl" &lt;&lt; std::endl; } template&lt;&gt; void f(void(*x)(void)) { std::cout &lt;&lt; "void void fptr impl" &lt;&lt; std::endl; } template&lt;&gt; void f(std::function&lt;void(void)&gt; x) { std::cout &lt;&lt; "std::function impl" &lt;&lt; std::endl; } void some_func() {} int main() { f(1); f(1L); f(some_func); std::function&lt;void(void)&gt; my_some_func = some_func; f(my_some_func); } 
very excited for the ```std::sys``` refactor, whatever path is taken for it (in tree refactor if that is what is necessary for best performance or external crate for easier/more cargo-esque tooling), it will make rust OSes like redox and robogalia have a much easier job fully supporting libstd early on.
ISTM that a separate crate, and not traits, is the way to go for this, but I haven't really put much thought into it.
The PR contains an example, pulled [straight out of AveryOS](https://github.com/AveryOS/avery/blob/master/sysroot/Cargo.toml)
Quick test: http://ddg.gg/?q=!rustn+Vec
The `Link` needs to be part of `Obj` to allow one `Obj` to contain multiple links so that it can be inserted into multiple collections. The idea is that an `Obj` gets inserted into multiple lists, then forgotten about. It can later be recovered through one of the lists.
As of today, you cannot constraint `I` like you want. You're now like many of us, waiting for something like [abstract return types](https://github.com/rust-lang/rfcs/pull/1305). For now, you don't really have any other option than returning a `Box&lt;Iterator&gt;`.
I think I'm more of the mind, "all approaches are inferior to just getting the design right and stabilizing it." :-) But yeah, I probably like your approach the best if it comes to it.
This is true of floating point in every language, and is not actually accuracy being lost; it's just that the `f32` is rounded to begin with, and the formatting takes that into account when printing an `f32`, but once you cast to an `f64`, it tries to print it out to greater precision, even though you've already rounded it to 32 bits. The problem is that 1/5, or .2 in decimal, is a repeating fraction in binary. That means that any multiple of it, like 4/5 or .8, is also a repeating fraction. Any time you try to represent it in binary, you will have to round the last bit. When you convert to `f64` and print it out, you are able to then see the magnitude of the rounding error.
Hmm. I think I'm thinking of `f32` as being what is a `double` in other languages. Actually it's a `float` and `f64` is a `double`, which explains why I'm expecting it to be twice as accurate as it is.
Oh, yeah, no, I'm just a lazy sack of crap.
&gt; I think I'm thinking of f32 as being what is a double in other languages. Could you elaborate on that? So far, I haven't seen a language where `float` was not 32-bit or `double` was not 64-bit. 
My problem is not with Nightly releases having breakages, my problem is being STUCK on Nightly for so long.
Before I jump into a fumbled attempt at clarification I want to thank you for your patience and help. Your contribution so far has been very meaningful and you've offered me one out at least. Thank you! ---- The models I'm referring to are models used in machine learning. It's probably easiest to explain with a real example: Logistic Regression is a Model (in the machine learning sense). The idea is that it takes some `Inputs` and produces `Outputs`. But we can "train" the model by giving it some sample `Inputs` and `Outputs` (hence the reason why `Outputs` is used). It implements the `Model` trait (named `Optimizable` in my library) which computes the gradient of the model given it's current state. This gradient is used during optimization - the details aren't particularly important. This state depends on the parameters (`start_vals` in your example), the `Inputs` and the `Outputs`. You are correct that `Inputs` and `Outputs` remain fixed and so can be closed over - however I was advised that this is not very Rust-like, though this may have been a misunderstanding on my part. You can see that discussion [here](https://www.reddit.com/r/rust/comments/3ycsx9/can_i_get_some_high_level_guidance_please/) - it's the same topic but equally long winded! At a high level the optimization will modify the parameters using the gradient. We do this iteratively until we have an optimized model. So the user creates a Logistic Regression model, and they have some `Inputs` and `Outputs`. Right now (in my library) they do something like this: let log_mod = LogisticRegressor::default(); // This picks some sensible default algorithm for Logistic Regression log_mod.train(&amp;inputs, &amp;outputs); // The model has now been trained and can be used to predict new inputs (that don't have associated outputs). The `train` method then uses the algorithm to optimize the model as described above. The user should also be able to choose a different algorithm to the default. It may well be better to set things up as in your example, where the user interacts with the algorithm directly. The issue for me is that in my library ALL models can be trained (though there are different traits giving the train function), but not all of them need an algorithm to be trained. I don't want the user to interact directly with the model in some cases but then use an algorithm in others. It may be a worthwhile trade off though... If anything there isn't clear (and you still have the patience to continue) please let me know. I really don't want to make this any more difficult for you than necessary!
ah yeah, that'd do it ;D
Note: Servo has a couple of GSoC projects too! https://wiki.mozilla.org/Community:SummerOfCode16#Servo
Note that specialization is opt in; you're explicitly declaring that you don't care if an implementation is overridden. In which case you stop caring which type is used. I don't know what footgun you're talking about in the C++ code.
You may or may not be aware of this, but on your Readme, you put `(Linux)` next to `x86_64-pc-windows-gnu`, just thought I'd let you know! :)
Whoops! Fixed, thanks!
&gt; \o/ this is amazing! \o/ &gt; So what you're telling me it's that I shouldn't have bought that Mac mini to compile Mac releases? :P Yeah, I'm sorry. All computers can be used as heaters though, so your investment was not in vain. Just kidding. I'm sure you'll find some creative way to use your Mac :P.
There isn't any automatic switching on Linux; especially not on nouveau. You may install bumblebee and manually run Servo with the discrete processor.
For the floating point number, it is important to distinguish *inaccuracy* from *inexactness*. FP arithmetic is inherently **accurate**; but its limited size gives you the inexactness and that's why you think it is inaccurate (while it's not). Let's approach the problem as follows. Define two functions `f32(x)` and `f64(x)` from any finite real number to the closest FP value in that type. When you store 4.8 into `v`, the actual value stored to `v` is `f32(4.8) = 4.80000019073486328125`. Actually, it is true for any real number between the following two numbers: 4.7999999523162841796875 4.8000004291534423828125 Since this range obviously contains 4.8, it would be convenient to *pretend* that it is rounded from 4.8. That's a glimpse of the [sophiscated algorithm](https://github.com/rust-lang/rust/blob/master/src/libcore/num/flt2dec/strategy/grisu.rs) to print the FP number. It is no longer true for f64, however, since `f64(f32(4.8)) = f32(4.8)` (as f64 is a superset of f32) and the range of numbers rounded to `f32(4.8)` in f64 would no longer contain 4.8: 4.800000190734862837160790149937383830547332763671875 4.800000190734863725339209850062616169452667236328125 In this way we can explain a lot of seemingly inaccurate FP operations. For example, `0.1 + 0.2 != 0.3` in f64 because `f64(f64(0.1) + f64(0.2)) != f64(0.1 + 0.2)` (a consequence of double rounding).
Using Rust 1.6.0: Have a function which takes a closure as an argument. That closure then gets "stolen" by the [`sort_by_key`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.sort_by_key) function. Pretty sure I understand what is happening. The value is being moved, makes total sense. What I don't know is how to work around it. pub fn partition_by&lt;F&gt;(&amp;mut self, mut partition: F) where F: FnMut(&amp;usize) -&gt; usize { // code omitted here ... // next line moves partition because sort_by_key takes it by value, see docs self.elements.sort_by_key(partition); // next line "error: use of moved value: 'partition' [E0382]" let mut current_partition = partition(&amp;0); // more code with more calls to partition omitted here ... } This seems a totally reasonable thing to do. Most API functions take their arguments as borrow references whenever possible it seems specifically to prevent problems like this. But all the examples I see take closures by value so they are moved. Why is that? Makes it impossible to implement more complex functions that take closures and call other functions taking closures. All the ways I can think of working around this involve making extra copies of elements into a more complex structure. Then manipulating it. Then using it to go back and setup up elements (add the other things that need done in the function). Seems like a real mess when I should just be able to call `partition` repeatedly. Hopefully that is clear enough. If not please tell me what I can answer to make it clearer.
Everyone's favourite "how to learn a new language" project, a Forth interpreter.
&gt; I want to thank you for your patience and help No worries. I don't know much about machine learning, but it's an interesting topic. Maybe we both can learn something! However, we might have to go a bit slower and take the long way around to bring me up to speed with terminology. I started by taking some of your words and mixing in some Wikipedia reading: // The idea [of a model] is that it takes some Inputs and produces Outputs // Note: Simplified to singular with the assumption you can call multiple times trait Model { type Input; type Output; fn evaluate(&amp;self, input: Self::Input) -&gt; Self::Output; } struct ExamPassing { intercept: f64, hours: f64, } impl Model for ExamPassing { type Input = f64; // hours type Output = f64; // probability fn evaluate(&amp;self, input: f64) -&gt; f64 { (1.0 + (-(self.intercept + self.hours * input)).exp()).recip() } } I think that `Model` matches what you said pretty closely. `ExamPassing` is a concrete example of logistic regression that we can try and start from before we go N levels deep in abstraction. It only allows for a single dependent variable (correct terminology?) &gt; computes the gradient of the model given [...] the parameters, the Inputs and the Outputs. I get lost here. How do the inputs and outputs contribute to the gradient? More Wiki reading suggests that you need to have the derivative of the function (a.k.a. `Model`), but presumably that isn't just provided to us. Can the gradient be computed from a model with one set of coefficients, an "old" input/output pair, and a "new" input/output pair? Once you have the gradient (which I'm assuming basically ends up as two amounts to adjust the model coefficients, in the example case above), then you need to construct a new model with the updated parameters and iterate until the error of the training data is sufficiently small (or you run out of iterations)?
Let's be cautious of over-exuberance. There are still good reasons for writing software in C, so let's not start denouncing people who still choose to use it. A better title might be "Start Rewriting Security-Critical Things in Rust".
Yo listen to this dude, he birthed std's [mind blowing implementation](https://github.com/rust-lang/rust/pull/24612) of float formatting. Did you think that printing floating point numbers correctly and efficiently requires 3000 lines of code, *two different algorithms*, **and another 1000 lines of tests**? Because it does. Time, strings, and float formatting, man. Bullshit problems.
All Hail WebRender! ;-) Makes me wish it ran on my mobile...does it work on Android btw.?
As a counterpoint to the recent vague "Rewrite everything in Rust" article, here's a more concrete example of someone actually giving it a try. :)
Thinking about adding support for more transports to [tarpc](https://github.com/Google/tarpc).
This feature is called "higher-rank trait bounds" and is sufficiently obscure that the book doesn't even seem to bother mentioning it. :P I've never bothered to understand them myself, beyond "they're needed to make closures work". You may find the original RFC enlightening: https://www.reddit.com/r/rust/comments/4758e7/til_the_for_syntax/ though note that it's from 2014 and may mention things that don't make sense in the context of modern Rust.
Here's something we urgently need: a drop-in replacement for OpenSSL. Something like [miTLS](https://mitls.org/) would be ideal, but unfortunately depending on .NET isn't acceptable for most projects. Here's an opportunity for Rust to make a measurable difference in the security of millions of devices. Replacing Linux is a tougher sell. Perhaps writing some modules in Rust would be okay, but they wouldn't ever be merged (I presume).
I see, thanks... (Psst [the link](https://github.com/rust-lang/rfcs/blob/master/text/0387-higher-ranked-trait-bounds.md)!)
&gt; Extend Rust's guarantees. We can extend the classes of bugs that Rust prevents. For example, we should try to make Rust release builds check for integer overflow by default. I think that, as hardware is becoming faster and cheaper, some day we will treat integer overflow like we treat bounds checks. It's a performance penalty, but disabling it is probably only worth for very specific pieces of code. Rustc should at least make it easier for projects to enable overflow checks even on release builds.
Rust allows you to [omit lifetimes in some common circumstances](https://doc.rust-lang.org/book/lifetimes.html#lifetime-elision). Here Rust assumes that the return value has the same lifetime as the `self` parameter: get_val(&amp;self, k :&amp;K) -&gt; Option&lt;&amp;V&gt; That should be the same as: get_val&lt;'a, 'b&gt;(&amp;'a self, k: &amp;'b K) -&gt; Option&lt;&amp;'a V&gt; Which is sort of like saying, "unless you tell me otherwise, I'm going to assume that this reference you're returning is pointing to something inside of `self`." But in this signature, there's nothing for Rust to work with to determine the lifetime of the return: xx&lt;T&gt;(s :Vec&lt;T&gt;, i :usize) -&gt; &amp;T Note that the `s` and `i` arguments don't have lifetimes at all. Instead, they're passed in by value, and they'll get cleaned up when the function returns. If you tried to return a reference to `s`, Rust would complain that it doesn't live long enough. In general, there no way to return anything that lives long enough for this function, unless you return something `'static`. In this case, you probably meant to pass `s` by reference (as `s: &amp;[T]`). If you made that change, Rust would go back to assuming the lifetime for the return, and you wouldn't need to specify it.
It's not just for closures; it's for any trait and `fn` pointers. Anywhere where `Trait&lt;'a&gt;` is allowed, `for&lt;'a&gt; Trait&lt;'a&gt;` is also allowed (same goes for functions). The benefits for functions and the function traits are probably obvious, they let you abstract over functions which are themselves lifetime-parametrized. For other traits it's not always so straightforward; but sometimes you need this for similar reasons. This only works with lifetimes, however. Lifetimes are different from regular types because you cannot impl on any concrete lifetime other than `'static`.
Libressl seems to be doing well in that front.
Perhaps refinement types could be used to avoid overflow checks. Not sure if in practice people would use it.
Also, remember that a lot of the openssl interface is macros reaching into structures. To be a drop-in replacement you'd have to implement precisely the same structures and internal implementation. Likely, you'd have to reimplement a lot of bugs.
Because it's an implementation detail
Fine. Build aa library that fas a sane, clean interface and then provide crazy API wrappers. :)
One of the OS projects written in rust. http://www.redox-os.org/ Front page is not so informative, this one is better. http://www.redox-os.org/news/this-week-in-redox-11/
I misunderstood. What's the sticking-point for you?
&gt; I believe this is already possible? Last time I checked, overflow checking was just implemented as the automatic insertion of `debug_assert!` calls, leveraging the existing flag to remove debug assertions in release builds. If this is still the case, it would be nice to have a flag to turn on overflow checking without having to leave all debug asserts on. Sounds like an easy RFC.
&gt; There are still good reasons for writing software in C Anything that needs a portable macro assembler without memory space for fitting the runtime library of something better, basically.
&gt; You can easily set up a specialization which doesn't get called in C++. Yes, and that's the point. With specialization in Rust you're explicitly declaring that you don't care if this happens. It's opt-in. The coherence system will still exist by default and prevent these errors; but you can turn it off for a particular impl and say that you're okay with it getting overridden. (Also, Rust's typesystem is a bit more concrete than C++s, where pointers/constness/by-ref aren't exactly types in the regular sense and coerce more easily.)
I'm kinda dreaming of a refinement-range type that would use compile-time const evaluation and type level integers to achieve this ergonomically (=without hand-specifying the ranges at every point) and with dynamic checks only at operations that would dynamically try to constrain the range. let a : RangeInt&lt;5, 10&gt; = RangeInt.new(7); let b : RangeInt&lt;5, 10&gt; = RangeInt.new(8); let c = a * b; // c's type inferred to be RangeInt&lt;25, 100&gt; match c.split_at::&lt;50&gt;() { // dynamic check happens only here SplitRange::Lower(x) =&gt; { /* do something */ }, // x : RangeInt&lt;25, 49&gt; SplitRange::Upper(y) =&gt; { /* do something else */ }, // y : RangeInt&lt;50, 100&gt; } let big : RangeInt&lt;10, 470000000000000000&gt; = RangeInt.new(11); let wont_compile = c * big; // compile-time error! RangeInt&lt;25, 100&gt; doesn't implement // multiplication with anything that is possibly so big that the result might overflow. if let Some(not_too_big) = big.restrict_to::&lt;10, 20&gt;() { // dynamic check let it_compiles = c * not_too_big; // it_compiles : RangeInt&lt;250, 2000&gt; } One can dream at least...
k
&gt; I need this to overwrite the file-pointer with zeroes to delete any reference to the file. Is it to use `write_bytes` on the pointer? The answer is you can't convert an *immutable* reference to a *mutable* pointer. Get an `&amp;mut File`: https://doc.rust-lang.org/book/raw-pointers.html#basics `&amp;Foo` can be cast to `*const Foo`, and `&amp;mut Foo` to `*mut Foo` (and `&amp;mut Foo` to `*const Foo`, incidentally)
That said, the last part of the explanation is entirely fair. &gt; The final step is to set the results [...] &gt; &gt; `q = p = results[order[0]].dest_addr;` &gt; `q = q-&gt;ai_next = results[order[i]].dest_addr;` &gt; &gt; ‡≤†_‡≤†
Little do you know, our apartment is so small that we do indeed use my machines for heat! :-)
&gt;I need this to overwrite the file-pointer with zeroes to delete any reference to the file. What are you actually trying to do? Do you just want to close the file handle? If so just `drop(file)`. Why do you need to overwrite the file pointer with zeroes, especially since that won't actually close the handle? This looks like a classic case of http://xyproblem.info/
Develop a nicer API, and provide a wrapper to match the existing API.
Just as a heads up, the MIR tracking issue is here: https://github.com/rust-lang/rust/issues/27840. There have been a couple of regressions that have been fixed along the way. My personal (slightly pessimistic) *guess* is that MIR will land in the 'end of July' release (which would be 1.10). This is just an educated guess, based upon the time spent on the issue so far, and regressions encountered so far. There's also HIR. (tracking issue is https://github.com/rust-lang/rust/issues/28139) I'm not sure how much of this has to be completed before MIR can land. The main idea is that the Rust compiler is being transition from building an Abstract Symbol Tree (AST) of the code to compile, and performing all of the compilation steps from the AST (well, until the compiler generates LLVM MIR code), to: * build the AST as before, do syntax checks * build a (slightly simplified) High Intermediate Representation (HIR) from the AST do type checks * build a (further simplified) Mid Intermediate Representation (MIR) from the HIR, do everything else before generating LLVM MIR, * build LLVM MIR from MIR (which should be simpler, because MIR is a simplification of the original AST) The reason for all of this is that having a single AST representation of the code being compiled makes changes pretty hard, with changes in one area of the compiler affecting others. This change to three representatons helps compartmentalize the impact of future changes.
More importantly, lifetimes don't change or affect the generated code, which makes it possible to have concrete values polymorphic over them, and which is the reason why it can't be as easily supported for type paramters.
Thanks, but &amp;mut file as *mut c_void gives the same error.
If you want to do things such that no traces in memory are left of the operation that you've done, Rust is not the language to be doing it in. Besides, if someone is spying on your computer and is able to read the memory of your process, very likely they also have the ability to trace syscalls and bam they'd be able to see every single file you open and the operations you perform to them. A `File` is nothing more than a handle or file descriptor, which, once closed, is a completely meaningless value due the operating system deleting its entry in the object manager or file descriptor tables.
Ah I hadn't seen you were casting to `*mut c_void`, why are you casting to `c_void` instead of `T`? `write_bytes` takes an `*mut T` (as internally it writes `sizeof::&lt;T&gt;() * count` bytes to the pointer). I agree with /u/retep998 that this doesn't seem very useful though, the File object is just a c_int fd (unices) or a HANDLE (windows), and you've got the issue that File has a nontrivial drop.
Keep in mind you must mark the binding `file` with `mut` to be able to overwrite it, too. `File` has a destructor (that closes the file and does whatever it needs to do), and I don't completely see how you can overwrite the `file` without also skipping to run the destructor -- which surely you don't want).
btw, not meaning to put you under stress or something, I just want to ask: How is the progress going? Do you have any target date where you want to release a first draft? Thanks! :)
Hehe, you can't possibly add to my stress ;) Progress is good, though last week was an exceedingly slow week. I had a root canal and was on _strong_ antibiotics, and so my writing output was more minimal, I focused on other work. Yesterday was the last day of taking them though, and I'm already feeling a bit better this morning. So I'm hoping to get a ton done this week. Let's say "the first half of the year" for when a reasonable draft should be ready-ish. I expect it to get easier as it goes along, the beginning part is much harder and takes much more time, the ending chapters are much easier; you've already learned most of Rust by then! As always, you can check in at https://github.com/rust-lang/book to see how it's going.
&gt; and I don't completely see how you can overwrite the file without also skipping to run the destructor -- which surely you don't want Let the destructor run then write all over the dropped/freed memory expecting nobody else is reusing it.
This can be done with a Box-like wrapper or so, at least. Using a box seems best to avoid the data being moved / copied around anway.
Just came here to ask the same. I'm getting: thread '&lt;main&gt;' panicked at 'called `Option::unwrap()` on a `None` value', ../src/libcore/option.rs:367 playpen: application terminated with error code 101 
Thanks :)
Ada has this since its first version. It can be a bit hairy in places. GNAT even has a non-standard mode where the generated code evaluates conditions in `if` statements using infinite precision (by widening the types if necessary). Detecting overflows is one thing, you also need to give programmers a way to cope with somewhat expected overflows, without terminating the whole task.
`const char*` means a pointer to const, assigning to such a parameter is no more of a no-no than assigning to an int. Maybe the author confused it with `const char* const` ? 
It also tells about the error: &gt; ld terminated with signal 31 [Bad system call] This is because of the secure sandbox the compiler and the code is running in. Either the sandbox changed or ld/gcc changed.
Just to add to retep998's comment, what information do you think is in the File instance? Is it secrecy or integrity that concerns you here?
**EDIT: I initially thought a modified version of your suggestion would work, turns out it wouldn't quite.** It does work except that I need my base models to be public. This isn't really a problem I guess, though I would prefer to hide this at the API level. I'm sure there's a way that your idea works better so I'll pursue that for now. *Here is my [implementation](https://github.com/AtheMathmo/rusty-machine/blob/restructuring-optimization-traits/rusty-machine/src/learning/logistic_reg.rs) - link likely to be outdated.* ---- From what you've said I think I have a solution, thank you! I need to look over each of my `Model`s but I think this will work! It also feels very natural and Rust-like (of course, as always when a solution is presented I wonder how I didn't try it already). I will separate the models and the algorithm as you describe, so I will have struct BaseModelA { // Contains some data } impl Model for BaseModelA { } struct UserFacingModelA&lt;M, A&gt; where M: Model, A: Algorithm&lt;M&gt; { baseModel: M, alg: A } The UserFacingModel will then implement the user facing functionality I had before whilst allowing different algorithms to be chosen. With Specializations this would work perfectly - I can get it working well enough without. Thanks again! 
https://github.com/rust-lang/rust-playpen/issues/178
It seems to me that zeroing out the File handle will just leak a file descriptor, which would seem *MORE* likely to give an attacker info about the file.
Does Ada widen the ranges of result of multiplication, sum etc? Because if it doesn't, it's quite a different system from what I had in mind. The point in the example I posted is that overflows can't happen at runtime ‚Äì all the possible places they could happen are detected statically, and the typeck wouldn't allow them to compile. This enables having safety without checks at runtime.
This is a tangent, but a word of warning: if you need real security, I'd recommend against using anything implemented in Rust (the rust-crypto README says this too). Instead, I'd use bindings to more established crypto libraries, ones that have gotten owned and been patched a bunch of times.
I'm really excited to see what impact specialization has. I haven't been following the the topic in all its details (there are a lot) so it's still fuzzy to me. &gt; In conclusion, thanks everyone for an edifying RFC discussion. This thread has consistently been of very high quality, with many interesting twists and turns. A completely different point: After reading more criticism of Github Issues lately, it's very refreshing to see this. There are a lot of comments, but the signal to noise ratio seems really high. 
Nice, although `multirust-rs` has been doing this for a while now ;)
&gt; In the short term, the answer is "Rust will get faster." Remember arguing over .to_string() vs .to_owned() on literals? They'll be the same speed now. There has only ever been `String::from("foo")`. 
I would like to see this some day. Perhaps in Rust 2.0? One can dream..
Since `FnMut` is also implemented for mutable references you could avoid the extra closure by passing the function by reference. self.elements.sort_by_key(&amp;mut partition);
Note that iterators are always lazy ‚Äì or at least, I don't know of a language where they are not. https://en.wikipedia.org/wiki/Iterator
I think [this](http://www.boost.org/doc/libs/1_60_0/doc/html/intrusive/intrusive_vs_nontrusive.html) will give you a good overview of intrusive containers.
I have a great response but it violates rule 3 so.. oh well :) (and yeah I agree)
Yes, I would hope so as well.
I must admit I sometimes have a bit of pain with understanding the nuances of RFC-s, personally to explain these kinds of things I tend to think in animals: so a quick stab. Lets say mammal is a trait with "talk" taking another mammal to talk to, dog of course has mammal trait. When a dog talks to certain other mammals (cats, squirrels, etc) it wants to say &gt;I'm gonna get you! But to Humans &gt;Let me out or the carpet is doomed. and to all the other mammals: &gt;woof So specialization would allow the different impls for "talk" and pick the best one based on the other mammal passed? Whereas before the dog would have to more work to identify the creature before talking? (Maybe use smellable trait?)
Consider that even Servo, a somewhat new project, is using OpenSSL. OpenSSL may have an unsafe interface, but Servo is (presumably?) using a safe wrapper: any safety bug during the use of OpenSSL shouldn't come from Servo, even if Servo itself is buggy (that's the premise of Rust safety encapsulation anyway). But the problem here isn't just the API. Even if you use the OpenSSL API correctly, without triggering any unsafety, it can still be memory unsafe because of some internal bug of the library. I would think that a Rust library would be less likely to have internal bugs that lead to memory unsafety. I think that writing an OpenSSL-compatible library that is memory safe in its internal operation would measurably improve the memory safety of any program that uses OpenSSL correctly.
Is this equivalent to universal quantification over lifetimes? Methinks I could probably use that to implement existential types :p
Speaking from the point of view of C++ and other languages that allow for specialization. I assume Rust will gain similar capabilities with this RFC. With specialization you can implement a generic vector, but when you want a vector of booleans, you can have a specialized version that only takes one bit per element instead of the default boolean size, for example. But to the outside world it behaves like the generic vector with boolean as type argument.
Is this any better than .to_owned()?
I used copy semantics when I did `let xs = (x, x);`, but I *also* used copy semantics when I did `let x_refs = (x_ref, x_ref);`. All non-mutable references have `Copy` semantics, and as such must also implement `Clone`. And if you check the [source code](https://doc.rust-lang.org/src/core/clone.rs.html#55-60) for `Clone`, you'll noticed that it is implemented for `&amp;'a T`, for *all* `'a` and `T`. That is, when you have a type `T`, then `&amp;T` implements `Clone`, returning `&amp;T`, regardless of whether `T` implements `Clone`. In summary, all non-mutable references have `Copy` semantics and implement `Clone`. [Demo.](http://is.gd/KT28eu)
**Edit:** Solved! See [this comment](/r/rust/comments/46zhff/hey_new_rust_users_got_an_easy_question_ask_here/d0b0w9z) for the question I should have asked. But, I think this may be a case of "docs that haven't been written yet". I spent many hours looking for an example of how to use vectors in let statements created from a struct. ---- [Push the vector Victor!](https://www.youtube.com/watch?v=fVq4_HhBK8Y) How do I push and pop to a vec that has been created from a struct **from within a let statement**? All the examples I can find online about using structs show every data type except a vec. I can work with strings, bools, etc., but the syntax for working with vecs escapes me. struct PrimWords { name: String, cp: Vec&lt;u32&gt;, } let mut prim1 = PrimWords { name: "test".to_string(), cp.push(): 256 }; After prim1 is created with a `let`, I could use: prim1.cp.push(256); but all of the combinations I've tried **within** the `let` gives a compile error like: "error: expected ':', found '.'" 
You can't do that because this RFC allows for specialization of the implementation of methods only. The underlying data type is set at that point. Also, `vector&lt;bool&gt;` is a leaky abstraction, that has a subtly different behaviour and interface to a real vector of bools. It was a mistake to put this specialization into place.
Perhaps not that specific example, as you wouldn't be able to make a &amp;bool from a single bit ;)
Thanks for these comments, it's remarkably difficult to find people who have experience with Ada.
Well, just now I open sourced my automated theorem prover, [Serkr](https://github.com/mAarnos/Serkr). Unfortunately, it is still far away from a proper release as it is inefficient, has a few very annoying bugs and cannot do proof output. Hopefully it won't take too long to fix these problems though. If you'd like to see some kinds of problems ATP systems can be used for, check the folder 'examples' and read some of the files. Oh yeah, a big thank you to the developers of LALRPOP. 
 prim1.cp.push(256); should do it.
That would work after the `let`, but within the `let` the compiler complains "unresolved name 'prim1'". Probably because it is still in the middle of creating prim1 and can't point to it because it doesn't exist yet. I'll edit my OP to hopefully make it more clear.
Watching the WebRender presentation I couldn't help but grin broadly at the fact that Patrick *very calmly* showcased the most revolutionary thing in web client tech in this millennium (at least so far). :-D
I'm on mobile, so a proper review will have to wait. However one suggestion: Try [clippy](https://github.com/Manishearth/rust-clippy) on your code. Yes, you'll need a nightly, but with multirust-rs it's easy to get.
Both the ones that panic for you work for me, maybe try a hard refresh?
Is it? There is a Usenet group (comp.lang.ada), and there are various mailing lists. FOSDEM typically has an Ada devroom, too.
Not so subtle, indexing doesn't work at all :) When someone told me about `vector&lt;bool&gt;` I was intrigued. I was like "that's impossible; the vector API lets you index, and this will lead to unaligned indexing which won't work.". Then I tried it and realized that indexing doesn't work at all, which probably breaks tons of template code and is horrible for everyone.
Let me qualify that prior statement with "who are also willing and able to comment on analogous features in Rust".
I tend to use &lt;String as Into&lt;String&gt;&gt;::into(String::from("a".to_owned().to_string()))
&gt; Both the ones that panic for you work for me, maybe try a hard refresh? I did that the first time around, and again now; still a panic with Debug/Stable, but when I modify the program ("Hello" -&gt; "hello") it compiles, so it must be some weird caching issue *somewhere*. Never mind, still better than before.
And we can finally retire the str_to_string lint from clippy ;-)
You forgot to put a `format!("{}", _)` in there.
&gt; All computers can be used as heaters though, so your investment was not in vain. Reminds me of my students days, cram 3 computers running 24/24, 7/7 in a 30m^2 studio and you're opening the window in the middle of the winter because it's too hot inside oO
Are you crazy, going through the `format` machinery?! That would be super inefficient! Rust is a _systems_ programming language, goddammit!
Do you know how much the overhead of Rust is, once stripped of its standard library? I would expect it not to take more than C.
No need to wait for 2.0, it's not a breaking change ;)
Joke explainer: `format!("{}", x)` expands to `x.to_string()` at compile time. Edit: formatting. Pun as a bonus.
&gt; I'd argue that default-on things like integer overflow checking is a change in philosophy (for better or worse). Actually it's not. Rust very carefully defines the result of underflow/overflow on integers as *unspecified*. For now: - it's a `panic!` if checking is enabled (default in Debug) - it's the result computed via wrapping arithmetic otherwise (default in Release) And the latter is *only* because there's no efficient implementation at the moment: if not for a technical limitation, Rust would have overflow checking in all compilation modes.
I think nwyndo is joking also.
This is definitely not client-side. For me the default hello world program fails only with debug/stable regardless of browser. Yes, that includes trying multiple browsers in private/incognito mode and clearing cookies and local storage.
The canonical example is iteration. Suppose that you write a trait `IteratorAdvance` that you implement for any iterator which advances the iterator by N steps instead of one. trait IteratorAdvance: Iterator { fn advance(&amp;mut self, n: usize) { for _ in (0..n) { self.next(); } } } Pretty cool right? But then you look at the `Vec` iterator and you're like: What! It's so wasteful to go 1 by 1 when I could just add `n` in one shot and be done! Well, that's what specialization covers: - you provide a default implementation with minimal assumptions - the implementation can be refined (specialized) when more options are available Note that in theory, I think that you should be performing the same *function*, because the function is determined by the trait... but maybe it's just my limited imagination :)
You can rewrite your method like this so that it supports both static and dynamic dispatch: http://is.gd/Vhww6u
From *The Rust Book* &gt; We can create an instance of our struct via let, as usual, but we use a key: value style syntax to set each field. I guess what I'm really asking is: what key:value syntax do I use to create an **instance** of a struct via a let that has a vector as one of its fields? let mut prim1 = PrimWords { name: "test".to_string(), cp: ???? }; In my example above, the **key** for the vector field would be "cp", what would the **value** be? BTW, thanks for helping a newbie trying to cram a new language into a very old brain. 
&gt; BTW, thanks for helping a newbie trying to cram a new language into a very old brain. Any time! &gt; what key:value syntax do I use to create an instance of a struct via a let that has a vector as one of its fields? One easy way is to have an empty vector: let mut prim1 = PrimWords { name: "test".to_string(), cp: vec![] }; If you want to put some things in the vector first, then you might instead use some variable binding: let mut v = vec![]; v.push_str("okay"); // possibly do other stuff to v let mut prim1 = PrimWords { name: "test".to_string(), cp: v }; Does that make sense?
Yes, if you want a vector with a single `256` in it, `vec![256]` is the way to go.
well, `cp: vec!(256)` Gives: "warning: struct field is never used: 'cp', #[warn(dead_code)] on by default cp: Vec&lt;i32&gt;," `cp: Vec::new` gives: error: mismatched types: expected `collections::vec::Vec&lt;i32&gt;`, found `fn() -&gt; collections::vec::Vec&lt;_&gt; {collections::vec::Vec&lt;T&gt;::new}` (expected struct `collections::vec::Vec`,
Is there a real difference between using brackets or parens or braces with macro invocation (`vec!(...)` vs `vec![...]`)? I just read the section of the book on [macro syntactic requirements](https://doc.rust-lang.org/book/macros.html#syntactic-requirements) but all I could quickly gather is that they must match if they're present, not if there was any difference. The bit about needing `{}` or `();` for "items" was also unclear to me - what even are "items" in this context.
&gt; Not so subtle, indexing doesn't work at all :) &gt; [...] &gt; Then I tried it and realized that indexing doesn't work at all, which probably breaks tons of template code and is horrible for everyone. What happened? Indexing is one of the few things that actually works just fine with `vector&lt;bool&gt;`: auto v = std::vector&lt;bool&gt;{true, false, true}; assert(!v[1] and v[2]); The main thing that doesn't work with `vector&lt;bool&gt;` is that its iterators are not C++ iterators. They work fine 99% of the time, but when they break its unexpected (most implementations support them pretty good anyways). If the Concept TS gets merged into the standard as it is `vector&lt;bool&gt;` won't type check, but the Ranges TS has a pretty good solution that makes proxy iterators first class C++ iterators fixing this and whole lot of problems. C++11 introduced a couple of gotchas with the interaction of `vector&lt;bool&gt;`'s proxy references and automatic type deduction. Sometimes the behavior is the desired one and sometimes it isn't, but when it isn't the solution is to be explicit with the types that you want: auto r = v[2]; // this is a reference (unexpected) auto&amp; r2 = v[2]; // this won't compile (unexpected) auto const&amp; r2 = v[2]; // this is a const reference (expected) bool val = v[2]; // this is a value (expected) bool&amp; r3 = v[2]; // this won't compile (unexpected) bool const&amp; r4 = v[2]; // this is a const reference (expected) Those are 6 cases, 3 expected, and 3 unexpected, and from the unexpected cases, only one compiles. If one doesn't use it with type deduction, there is no unexpected case that compiles which is pretty good. There are proposals to make it always compile and always result in the expected behavior, but those haven't got much traction yet. Probably the only issue that will remain in the near future is that `vector&lt;bool&gt;` is a "leaky" abstraction in that you should know that it is not an array of bools. But even if you don't know it, its actually pretty hard to shoot yourself in the foot with it. Things like serialization/deserialization work just fine, and that is the only edgy situation i can think of in which the memory representation of `vector&lt;bool&gt;` is actually important. Still, `boost/dynamic_bitset` is just better in almost every single way than `vector&lt;bool&gt;` for too many reasons.
Posting so that other Android devs are aware of it :)
I think you are right, however the problem is convincing the said developers. Last month I watched a few talks trying to sell using modern C++ to embedded developers that do use C as a portable macro assembler for micro-controllers and such. Just using it without runtime, using the safety features C++ has over C, but I doubt they got sold into the idea.
&gt; Doing this as text since it was already submitted: Like, a recent repost? Because if it was last posted a year ago I don't see the issue. (There's also the `?resubmit=i_love_karma` option.)
No, lifetime specialization was rejected as a distinctly Bad Idea. At first glance, it seems desirable to be able to express that e.g. to_owned for a string literal is a no-op, but when it comes time to actually resolve implementations, you suddenly have the realization of "dear god, trans is dispatching implementations on the values of lifetimes" (lifetimes don't even make it to trans today, and it's not clear what the even means).
It was the one last year, and that trick didn't work for some reason, I dunno.
its yucks and chuckles all the way down
&gt; I think that you should be performing the same function, because the function is determined by the trait... but maybe it's just my limited imagination :) Based on my limited following along, I think that a large chunk of the discussion focused on this, under the term "parametricity". Also see references to the `Reflect` trait. As a programmer with limited brain power, I hope that most instances of specialization are the "make it go faster" kind, not the "change what this does" kind. 
Si this is scripting language with better performance and safety ? If this is the case, this is definitelly usefull ! But will it be more difficult to write than python ?
Edit: SOLVED. I've been attempting to teach myself Rust on and off for a while now, and I recently decided to try to implement a Rust version of a variety of simple graph searches. I've run into a problem, though, and, after beating my head into the wall, I've decided that I can't figure out a solution myself without investing more time and effort than I would like to. Any advice that someone could offer would be helpful, even if it's 'Your premise is wrong. Rewrite everything from scratch.' My latest attempt, after a fair number of rewrites and different approaches: struct Chain&lt;'a, T: 'a&gt;((T, Action, i32), Option&lt;&amp;'a Chain&lt;'a, T&gt;&gt;); trait Problem { type State : Hash + Eq; fn start(&amp;self) -&gt; &amp;Self::State; fn is_goal(&amp;self, &amp;Self::State) -&gt; bool; fn successors(&amp;self, &amp;Self::State) -&gt; Vec&lt;(&amp;Self::State, Action, i32)&gt;; fn heur(&amp;self, &amp;Self::State) -&gt; i32 {0} fn dfs(&amp;self) -&gt; Vec&lt;Action&gt; { use std::collections::{HashSet, LinkedList}; let mut goal = Vec::new(); let mut visited = HashSet::new(); let mut stack = vec![Chain((self.start(), Action::None, 0), None)]; while !stack.is_empty() { // The borrow checker doesn't like this. My guess is that // the value popped off the stack becomes scoped to the // loop, resulting in it living for less time than cur. let ref cur = stack.pop().unwrap(); if self.is_goal((cur.0).0) { for s in cur { goal.insert(0, s.1); } break; } visited.insert((cur.0).0); for s in self.successors((cur.0).0) { if visited.contains(s.0) { continue; } // This line moves cur to the outer scope, breaking things. stack.push(Chain(s, Some(cur))); } } goal } }
Lifetime Woes I'm implementing a specialized iterator into a part of my collection. The collection is a bunch of elements partitioned into sets and within those sets some elements are marked. This iterator goes through the marked elements of one set. pub struct PartitionMarkedSetIterator&lt;'a&gt; { set: usize, partition_marking: &amp;'a mut PartitionMarking&lt;'a&gt;, location: usize } The method that is supposed to give you one of these is the problem. impl&lt;'a&gt; PartitionMarking&lt;'a&gt; { pub fn marked_in_set(&amp;mut self, set: usize) -&gt; PartitionMarkedSetIterator { let location = self.partition.first[set]; return PartitionMarkedSetIterator { set: set, partition_marking: &amp;mut self, location: location }; } } The compiler error states "error: cannot infer an appropriate lifetime due to conflicting requirements [E0495]". The notes focus on `partition_marking: &amp;mut self` and `location: location`. I've tried adding an explicit lifetime parameter and also using the `'a` lifetime. Obviously the goal is that the iterator hold onto a mutable borrow of self. So as long as the the iterator is in scope the mutable borrow continues. I'm confused by the compiler complaining about location since it is just a `usize`. Given that `usize` implements `Copy` shouldn't the value just get copied into the struct and not cause problems with the lifetime? Thanks in advance for your help! **EDIT:** I was mis-reading the notes. They don't talk about location. Just about `partition_marking: &amp;mut self` and the return lifetime. Returning `-&gt; PartitionMarkedSetIterator&lt;'a&gt;` seems to make the compiler not complain, but then I have trouble where I am trying to call it. Is there a more detailed explanation of the current version lifetimes than the [rust book](https://doc.rust-lang.org/book/lifetimes.html)?
Nah, one a day is too much (remember, scale it by number of users). One a week may be better. Or maybe you need a proof-of-work, like landing PRs. I think bors should be able to post as many memes as they want.
Now it's time to see if this fixes typenum's division. I keep meaning to get around to it, maybe this blog post will finally push me over the edge. It still seems odd to me that this works, though.
Other than iterators, and optimizing around indexing vecs, where will we see improvements in std from specialization?
Despite running on nightly itself, it would be nice to tell clippy what older version of rust I need to keep supporting.
Is there a template for variadic generics?
Now the question becomes: how long until we actually have a concrete implementation to play with? 
Blargh, yeah I meant RFC, brain fart.
It probably won't be backported to the stdlib, but I plan to use specialization in [buf_redux](https://github.com/cybergeek94/buf_redux) to determine whether or not a particular `Read` impl can be trusted with an un-zeroed buffer, with something like unsafe trait UninitSafety: Read { fn uninit_safe() -&gt; bool; } unsafe impl&lt;T: Read&gt; UninitSafety for T { default fn uninit_safe() -&gt; bool { false } } unsafe impl UninitSafety for std::fs::File { fn uninit_safe() -&gt; bool { true } } unsafe impl UninitSafety for std::net::TcpStream { fn uninit_safe() -&gt; bool { true } } *et cetera*, and then at https://github.com/cybergeek94/buf_redux/blob/master/src/lib.rs#L129: let new_len = self.buf.capacity(); if T::uninit_safe() { unsafe { self.buf.set_len(new_len); } } else { self.buf.resize(new_len, 0u8); } That will save a few cycles zeroing the buffer when it's just going to be passed directly to the OS anyways. 
Very cool, thanks for the example.
[Quite soon.](https://github.com/rust-lang/rust/pull/30652)
So.. never trust someone who doesn't publicize their tests?
I think that's a problem with doing *any* kind of generic programming. There's always *something* right at the edge of your abstraction that you wish were doable and is *almost* like what you already have. 
Come on man, get serious! This scheme is completely vulnerable to upvoting rings.
&gt; The underlying data type is set at that point. trait VecItem { type Impl: VecImpl; } impl&lt;T&gt; VecItem for T { default type Impl = GenericVecImpl&lt;T&gt;; } impl VecItem for bool { type Impl = BitVecImpl&lt;T&gt;; } struct Vec&lt;T: VecItem&gt; { data: T::Impl } Done.
&gt; Once we have variadic generics Is this something that's being worked on currently? The RFC discussion hasn't had contributions in a year... (A lot has changed since it was posted though, it would require a bit of rework I think)
I'm not aware of any active work on it.
I thought so, but who knows. I don't need the karma anyway :)
&gt; At some point, you need to trust that the other programmer isn't doing terrible things Note that this is already a concern, e.g. you have to trust that the library you're using doesn't try to delete all your files. Of course, specialisation possibly allows more subtle confusing scenarios, or tempts people to create them in away that doesn't currently exist.
You can use `#[cfg(target_arch = "x86_64")]` and `#[cfg(target_arch = "arm")]` to automatically choose the correct version of the struct and the value of `NCCS` for the target platform. The attributes available for `#[cfg()]` are listed [in the reference](http://doc.rust-lang.org/reference.html#conditional-compilation). [Example usage](http://rustbyexample.com/attribute/cfg.html)
Brian Smith is doing a bunch of SSL rewriting (slowly picking away at BoringSSL) that we might want to switch over to. &gt; Servo is (presumably?) using a safe wrapper Sure, and consumers of RustSSL that are in Rust and use the Rust safe wrapper won't have to worry. In fact, the unsafe OpenSSL-compat API would probably be a wrapper around the safe Rust one, instead of the other way around like it is now. However, all the other languages do not get the benefit of this since they can only consume the unsafe OpenSSL-compat API. An API which is unsafe not just because it uses raw pointers, but because there are nontrivial interactions between said pointers that you need to account for. I think that's the point being made here -- a RustSSL is very useful for Rust itself, but the marginal utility is much less for other languages. I'm pretty sure you can implement a RustSSL with no unsafe code (maybe a little for perf) except in the C API wrapper (which would probably be a rather thin wrapper).
This is half-way towards a solution, but unfortunately, it doesn't work in this case. The problem is that it requires coming up with a rule that predicts what the C header file will say, and encoding that rule into conditional compilations, but there is no simple rule that always gets it right. I just checked the platforms I had handy (Linux, OS X, Cygwin) and no two were the same. This isn't a problem for C programs, since they can just include the header.
How would this allow function overloading in a way the current system does not?
Currently every "overload" of rust's "overloaded functions" must take the same number of parameters. I'm thinking of something more like this: trait&lt;Args...&gt; GrowApples&lt;Args...&gt;{ fn grow_apples(a:Args...)-&gt; Apples ; } impl GrowApples&lt;&gt; for () { fn grow_apples()-&gt; Apples { emerge_apples(import_dirt(), import_seeds()) } } impl GrowApples&lt;Dirt&gt; for () { fn grow_apples(d:Dirt)-&gt; Apples { emerge_apples(d, import_seeds()) } } impl GrowApples&lt;Seeds&gt; for () { fn grow_apples(s:Seeds)-&gt; Apples { emerge_apples(import_dirt(), s) } } impl GrowApples&lt;Dirt, Seeds&gt; for () { fn grow_apples(d:Dirt, s:Seeds)-&gt; Apples { emerge_apples(d, s) } } at which point you could let dirt = import_dirt(); let seeds = import_seeds(); let a1 = GrowApples::grow_apples(dirt); let a2 = GrowApples::grow_apples(dirt, seeds); let a3 = GrowApples::grow_apples(); It comes out kind of ugly, so I guess I'd be surprised if anyone actually used this over just giving the variants of the functions different names, but I might be missing something, there may be something in these RFCs that allows you to do this in a far nicer way. The main point is, once we have these things, the team has to admit they've already done most of the work underlying full function overloading, all that's left to do is to add a dash of sugar.
I don't know much about RawTable, but an `isize` is going to be an address-sized integer, so it's what I'd use to store an address. `*mut`/`*const` are still pointers.
Explicit return can make sure this is parsed has an expression i guess? (i never understood this distinction since now, huh..): fn test1() -&gt; usize { return if true { 1 } else { 0 } + if true { 1 } else { 0 } } 
The reason to put in PhantomData is to avoid the compiler yelling about unused type parameters (it yells because it infers variance of generic parameters from their usage). `PhantomData&lt;(U,V)&gt;` says "please pretend this struct contains values of type `U` and `V`". But `u64` is a concrete type, not a parameter, so no special handling is necessary. (In fact `Unique&lt;T&gt;` contains its own `PhantomData&lt;T&gt;`, so it's ghosts all the way down in this case.)
I have a private GitLab server for personal use and one that we use at work. It's very nice software, better than GitHub in my opinion. The only reason I still keep my public projects on GitHub is because I'm concerned nobody will look at them if they're hosted elsewhere--which is something I hope changes. (I'm also somewhat forced to use it anyway if I want to contribute to Rust or 99% of its crates)
&gt; There's also HIR. That issue seems to be a bit out of date. Some of the checkboxes have been completed. At least, HIR exists in the compiler and most(?) desugarings (e.g. for loops) are done during the AST-&gt;HIR lowering pass.
Indeed. I found C's syntax regarding pointers confusing until someone finally explained that you're supposed to read them right-to-left. So `const char *` is read as a `pointer` to a `char` which is `const`ant. Changed my life. Function pointers are still a bear though... Consequently I now write all my types in C so they make sense when read that way. So I write `char const *`, which is more comfortable to read aloud: "a pointer to constant char".
`const char *` pointer to a character that is constant (can change the pointer but not the data the pointer refers to). `char const *` constant pointer to a character (can change the data the pointer refers to but not the pointer). `const char const *` constant pointer to a character that is constant (can't change the pointer or the data the pointer refers to).
We don't ‚Äì we simply target a superposition of current stable, beta and nightly.
It's not the detour that does it, it's the fact that the Output type is fixed by a trait that is implemented on a concrete type.
I think so! [pcwalton's talk](https://air.mozilla.org/bay-area-rust-meetup-february-2016/) mentions mobile platforms a few times between the 15 and 16 minute marks.
Trait bounds get monomorphized at compile-time, e.g. a function foo::&lt;T&gt; gets generated for each type implementing T, whenever it is necessary (i.e. it won't create a variant that is never used.) This produces faster, but bigger, binaries. Trait object store the v-table of a value at runtime, which contains the address of the correct function to call. This is mildly slower, but much more versatile: you can, for instance, iterate over a Vec that contains values of different types, as long as they implement some trait.
I'm still working on [notty](https://github.com/withoutboats/notty). We've made a lot of progress lately and now it works well enough that there's a screenshot in the README!
&gt; looks a little less like a hack with template specialization I don't see this either; your example doesn't use specialization at all. What specialization does get you is overload specialization, i.e. the ability to have an overloaded function where one of the overloads is generic and others are on concrete types. But that's just "specialization begets specialization"
Yeah I probably tried to be too generic (which is ironic, because that's exactly the problem this technique is attempting to solve). I may want to add some example that trips the compiler when I get around to it.
Or even fn test() -&gt; usize { 0 + if true { 1 } else { 0 } + if true { 1 } else { 0 } }
No worries! That's why it's an easy questions thread :) Next level answer: create a newtype instead: struct Address(usize); Now you can know which ones are addresses and which ones are not. This may or may not be worth it, but it can really help in cases!
[Lowering!](https://github.com/rust-lang/rust/blob/master/src/librustc_front/lowering.rs#L11)
You don't read them right-to-left, you read them [in spirals](http://c-faq.com/decl/spiral.anderson.html)! (But actually, you just use [cdecl](http://www.cdecl.org/).)
There are times when trait bounds are more versatile. You can have a function `foo&lt;T: SomeTrait&gt;(T, T)`, and know that both arguments are the same type and that type implements SomeTrait. If you use trait objects, all you know is that both arguments implement SomeTrait, but you don't know whether they're the same underlying type or not.
Exciting. One question: How does this interact with default methods? It looks to me like this almost makes default methods redundant...
It makes default methods basically a syntactic sugar for one kind of specialization ‚Äì the RFC acknowledges that, and states that the keyword "default" isn't "default" by chance, but it draws deliberately parallels with default methods.
&gt;So there seem to be two options. And you then proceed to list four options Option #2 is typically terrible as it requires the user have libclang hanging around which is frustrating for those platforms that don't have the correct libclang in their package manager or don't have a package manager at all. Option #3 works pretty well with the `gcc` crate. You can write a simple portable C shim and it'll compile on nearly every platform without issues. 
Why not try [Rayon](http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/)? use rayon::prelude::*; fn compute(a: &amp;[u32], b: &amp;mut [u32]) { // Rayon b.par_iter_mut().zip(a.par_iter()).for_each(|(bi, ai)| { *bi = compute_value(ai); }) }
No. I mean, you *can*, but you have to pass the trait's *entire* definition into the macro as input. Rust macros have *absolutely no reflection capabilities whatsoever*. They have the literal tokens they're passed as input, *and that's it*.
Yeah, it actually might make sense to have a feature added to rust doc to force hide certain implementations for brevity (and possibly allow adding something like this manually)
When you say "write an implementation of a trait" do you mean create a new trait, or just impl a trait? If the latter, then do you mean something like this? trait TraitA { fn a_fun(&amp;self); } trait TraitB { fn b_fun(&amp;self); } impl&lt;T: TraitA&gt; TraitB for T { fn b_fun(&amp;self) { self.a_fun() } }
Well it is zero logic in terms of the language itself. You cannot explicitly define functions logic in Polly. This gives a Polly a singular syntax, purely about templates, and the function calls aren't logic in terms of traditional logic, as they will always be called. Functions are required to be defined in Rust, and then can be called in Polly. You do loop, and conditionals through these functions. You can see [here](https://polly-lang.github.io/Polly/polly/fn.std_functions.html) there is already std functions for these. This provides the advantages of having the logic being compiled with the program allowing Rust to optimise the logic, far better than if Polly invented it's own syntax for logic, and tried to optimise, and run it at run-time. 
Polly might not be the best name, because it is used for a long time by [this LLVM project](http://polly.llvm.org/)
This technique really does work. Just today I got it working for a statically type-checked BNF grammar representation, with a few `macro_rules` for boilerplate. For example if you want a parser that takes a list of numbers and adds them up: let parser = parser! { list = or![Tok(NUM), map_seq!([x in list, y in Tok(NUM)] x + y)]; }; This successfully works with my Earley parser implementation (so it works for all context-free grammars). Only caveat at the moment is that it relies on `#![feature(optin_builtin_traits)]` for type level lookup tables.
I know. But I doubt frontend developers would be familiar with low level framework for generating polyhedra. 
I've since revisited my interface to require `Ord` bounds, too. What function could I use to add or subtract a range of values?
I've always struggled to find a place where the struct-like notation for enums came in useful; perhaps it'll be useful for you here? enum Termios { SixtyFour { c_iflag: tcflag_t, c_oflag: tcflag_t, c_cflag: tcflag_t, c_lflag: tcflag_t, c_line: cc_t, cc: [cc_t; 32], c_ispeed: speed_t, c_ospeed: speed_t, }, Arm { c_iflag: tcflag_t, c_oflag: tcflag_t, c_cflag: tcflag_t, c_lflag: tcflag_t, c_line: cc_t, cc: [cc_t; 19], } }
I could add convenience functions for that (which would be based on `intersection` and `union`), but they would be O(n) -- the intended use of `range-map` is to collect all the ranges you want in an iterator and then construct a `RangeSet` all at once using `collect`. If you're going to be regularly adding or removing single ranges, you'd need a different data structure (e.g. something tree-like, whereas `range-map` is just a `Vec` of sorted ranges).
What is the convention for when to put functions in traits/classes vs. free functions in Rust?
Right, you have `self.inner.read(&amp;mut self.buf)` So you write an expression of `&amp;mut Vec&lt;u8&gt;` where `&amp;mut [u8]` is expected, and it coerces by itself the same way `&amp;mut self.buf[..]` does. But `&amp;mut self.buf[..]` (slice the full length of the vector) is a slice of length 0, when vec is of length 0.
It also introduces another indirect call.
To mutate the value then pass it back out again, most often by changing the type (for things like pseudo linear types).
I don't think it will be a replacement for Python, which has a broader usage. Yes, it is bit more difficult to write, but I have not written enough code in it to make a direct comparison. I would think there is a smaller gap between Dyon vs Python than Dyon vs Rust. Dyon might end up having comparable performance with Lua, but also does some extra runtime type checking on assignment which can be a challenge to optimize. In some ways it might be a safer language, in other ways more unsafe. There is an unsolved problem concerning safety when using first class functions. So far, no silver bullet. The easy integration with Rust makes it trivial to optimize parts of it in a way that fits gamedev, so this is the space I am aiming for. It is a lot easier to design a language for a narrow domain.
Each benchmark is compiled as separate crate. So you just import your crate as an external library, with an `extern crate` statement.
No. Macros are lexical and have no reflective capabilities. I've always wanted some kind of compile time reflection / code generation that looks something like "for every method in ThisTrait, make a method..."
If your function's return type is Self, you can return self, or create a new object of type Self, and return that instead (in this case self is deallocated). If you have to mutate self in this function (eg. increment a counter), you can write mut self in the function signature, so you don't have to write something like this: fn foo(self) { let mut self = self; // make self mutable ... }
Dankcoin.
I originally considered this, and was going to suggest argon2, but collision/bruteforce ostensibly doesn't matter for the algo itself in this scheme. You need to worry about sybil attacks and other activity flooding techniques. Mods can deal with comment flooding but not upboats. 
Please go to /r/playrust and take your "funny" clip with you. Thanks.
&gt; I've always struggled to find a place where the struct-like notation for enums came in useful Strange; as soon as a variant has more than one, maybe two, members, I feel it's not readable anymore to use the tuple form.
Habits die hard... and FUD.
Why is Gitlab preferable? I've had my gripes with GitHub, but I'm curious about yours. The main thing that comes to mind is the separation of issues from commits.
permutohedron's changes should be correlated with llvm upgrades I think. Without looking I'm guessing it's related to how by-reference captures in closures are optimized.
I think you might be right. The spike corresponds pretty nicely with this commit: https://github.com/rust-lang/rust/commit/e8c398d05c92d4cdd7b3528b892355e0c000131b But from `git log src/llvm` I don't see any commits that would have impacted that folder in the late January timeframe, when the reversion to the mean occurred. I assume something changed elsewhere in the compiler to account for the change LLVM made?
The million dollar question: Can you build rustc itself from your own fork? (using the new, cargo-based build system of course)
Oh, I should have looked into the LLVM IR. Thank you for clearing up my misconception.
Vec keeps track of that itself - might as well leverage it. fn extend_by_reading(v:&amp;mut Vec&lt;u8&gt;, r:&amp;mut io::Read) -&gt; io::Result&lt;usize&gt; { let orig_len = v.len(); let max_len = v.capacity(); if orig_len+1 &gt;= max_len { return Ok(0) } unsafe { v.set_len(max_len); } let mut result: io::Result&lt;usize&gt;; { let (_,mut buf) = (&amp;mut v[..]).split_at_mut(orig_len); result = r.read(buf); } let n = match result { Ok(x) =&gt; x, _ =&gt; 0 }; unsafe { v.set_len(orig_len + n); } result }
I don't know if the creator of the site intends to keep it running for the foreseeable future, but your crate's docs are already available at https://crates.fyi/crates/pelite/0.1.0/ You can read more about the project [in this thread](https://www.reddit.com/r/rust/comments/43o9fj/cratesfyi_documentation_of_almost_every_crate_in/). [Repo link](https://github.com/onur/cratesfyi)
Maybe you should substitute the *"What‚Äôs everyone working on this week"* thread on Sunday with a *"What's everyone is memeing?"*.
What do you mean by pseudo-linear, and how do they differ from linear? Does rust support the latter?
&gt; you have to trust that the library you're using doesn't try to delete all your files This is the moment all Haskell zealots* live for. Monads to the rescue! *) I'm not one of them.
Oh you are so very right, from the [c-faq](http://c-faq.com/ansi/constptrconst.html) and in section 6.5.15 (subpart 8) of the C99 specification this is very clear. Thanks for the correction as I clearly haven't been using const often enough X(.
Can you import Github issues?
&gt; Oh...so *not* zero logic then? The logic is defined in Rust (a programming language), not in Polly (a templating language).
No problem, I was really confused by the same issue :)
I'm just starting work on a side project that will be using mio, would you recommend waiting until after this change, or should it be fairly straightforward to change code to use the new API?
So, I started implementing this in typenum, but it hasn't helped yet, as you can see in these hastily done test results: http://i.imgur.com/TBraHHK.png Still linear and still sad. It may be that it's a somewhat different issue at play here.
This may or may not be relevant but I once encountered a proprietary object database that called `getrusage` excessively due to debugging being (mistakenly) switched on in the application. What made this interesting was that the database was running 200+ threads and the `getrusage` call would chew up excessive CPU time as "system CPU". Why? Because `getrusage` calls a spin lock. And get enough threads trying to `getrusage` at the same time and you're going to have massive contention.
I would start now... building a shim layer from the current API to the new API if the proposal lands should be pretty easy.
If I'm just using epoll (or kqueue) directly, I can set the fd to be nonblocking and get notifications when there is data waiting. I meant to ask if that's possible under mio :).
That helps, yeah, but... if we're talking about trust, you still have to trust that any `IO _` (or similar) actions don't delete your files, and, you have to trust there's no bad `unsafePerformIO`s.
Any plans to copy D's wchar &amp; dchar types so that string slices can be done on unicode code points?
Remove the `*` from the register callback call in `main`. The `&amp;mut * x` idiom is used to convert smart pointers into the pointer they wrap; you're just trying to make a reference, so you just need `&amp;mut x`. The more serious problem, though, is that `register_callback_for_interrupt` is not safe to call, because you're handing it a raw pointer (which contains no lifetime information) and can then free the object at any time. It should take a `&amp;'static mut`, since that means the object must not ever be freed.
didn't realize there was an argument over `.to_string()` vs `.to_owned()`. Somehow I've almost always used `.into()` because it was less characters.
Couldn't mio abstract blocking away with something like `io_submit` in event loop and secret thread doing `io_getevents` and resubmitting events back to event loop? On linux. And just start a thread per file descriptor on platforms where AIO is not available?
Potentially relevant thread: https://www.reddit.com/r/rust/comments/3hkd0w/til_fnself_can_also_be_fnmut_self/
&gt; so it only blocks for a few cycles at most before returning an error Uh, this doesn't make sense. If the mutex is locked when the CPU enters the interrupt handler, nothing can reasonably unlock it before the interrupt handler returns. (In principle you can get preempted by a higher-priority interrupt, but unlocking a mutex this way would be a bug factory.)
Well, it'd *have* to be full signatures because you need the type of the arguments, and the return type.
Right, in ordinary kernel programming, this situation would call for disabling interrupts (or at least the interrupt in question). Otherwise, an IRQ in the middle of this critical section would deadlock the CPU, as you say. (Sorry, not Rust-specific, but then the essence of this question isn't really, either.)
Check the deploy section of the [.travis.yml](https://github.com/japaric/rust-everywhere/blob/master/.travis.yml#L68-L83) and [appveyor.yml](https://github.com/japaric/rust-everywhere/blob/master/appveyor.yml#L53-L67). In this repository both are configured to upload the artifacts to GitHub but both [Travis](https://docs.travis-ci.com/user/deployment/) and [AppVeyor](https://www.appveyor.com/docs/deployment) support uploading the files to different hosting sites. They also support running a custom script that you could use to upload the artifacts to a custom website, or email them, etc.
I'm not sure I follow. Do you mean building rustc with `cd src/rustc &amp;&amp; cargo build`? What do you have in mind?
&gt; Rust doesn't support linear types due to unwinding. I'm not fully sure what that means. How do panics (I'm assuming) interfere with linear types. Any reference material to look at?
Oh, wow, I didn't realize that. I knew that about "mut" qualifying parameters in general but I didn't know that holds for self as well.
Some high-level thoughts about creating a safe interface around an interrupt handler: * Since you can't use `Box`, I think using closures will be difficult. That would have been the easiest way out otherwise. * The callback must be `Send` to enable Rust's compile-time check for thread safety. Rust will now complain in case you have data races between your main thread and your interrupt. * Unless you use a spinlock like DroidLogician suggested, you need to have the interrupt off when you set the callback. The callback can be a fat pointer, so setting it is not atomic. * Have you considered what happens after `main()` quits and the interrupt keeps running (and `interested` is no longer on the stack)? The easiest way out is probably to add a `'static` bound to your trait (in addition to `Send`), but that in turn means `interested` needs to be a `static mut` variable too.
The way I understand it, one of the requirements for linear types is that each value *must* be consumed by something; *i.e.* you can't just implicitly drop a value. Panics in Rust, however, make this a non-starter: you can't guarantee that a value won't be dropped during a panic, as opposed to being explicitly consumed.
&gt; nothing can reasonably unlock it before the interrupt handler returns Not even threads executing on other cores? The case I had in mind was the interrupt triggering while attempting to replace the callback. 
Makes sense. Can we say that, in the absence of panics, then they act as linear types? Seems that everything goes out the window during a panic anyway.
I just can't wrap my head around how you can get by without a simple `if` statement. Even something simple like {% if user.logged_in %}&lt;a href="logout.rst"&gt;Log Out&lt;/a&gt;{% else %}&lt;a href="login.rst"&gt;Log in{% endif %} You're supposed to do what..? Put some crap like this in your Rust: let logInOutUrl = user.logged_in ? "..." : "..."; let logInOutText = user.logged_in ? "Log out" : "Log in"; &lt;a href="{{ logInOutUrl }}"&gt;{{ logInOutText }}&lt;/a&gt; And that's supposed to be easier to read, and better contained and...? You're just moving all the display logic into Rust for...what benefit? I know Polly isn't the first language to do this, but I've just never heard an explanation.
Yeah but you're just putting that logic in a different place and then passing it through to the view which makes it even harder to follow.
I switched to the rayon library and and I'm getting almost perfect utilization during the parallel portion! Rayon uses work stealing to better exploit the parallelism available.
Maintaining my PRs which are, respectively, mired in controversy ([`#[derive(Copy, Clone)]` optimization](https://github.com/rust-lang/rust/pull/31414)) and [as neglected as the nine button on a microwave](https://xkcd.com/1103/) ([inclusive range syntax](https://github.com/rust-lang/rust/pull/30884)). Published a few macro crates: [unborrow](https://crates.io/crates/unborrow), [static-cond](https://crates.io/crates/static-cond) and [named-block](https://crates.io/crates/named-block). Thinking about how the unstability of `#[macro_reexport]` means that macro crates can't really participate in the dependency system on stable.
[Image](http://imgs.xkcd.com/comics/nine.png) [Mobile](http://m.xkcd.com/1103/) **Title:** Nine **Title-text:** FYI: If you get curious and start trying to calculate the time adjustment function that minimizes the gap between the most-used and least-used digit (for a representative sample of common cook times) without altering any time by more than 10%, and someone asks you what you're doing, it's easier to just lie. [Comic Explanation](http://www.explainxkcd.com/wiki/index.php/1103#Explanation) **Stats:** This comic has been referenced 41 times, representing 0.0406% of referenced xkcds. --- ^[xkcd.com](http://www.xkcd.com) ^| ^[xkcd sub](http://www.reddit.com/r/xkcd/) ^| ^[Problems/Bugs?](http://www.reddit.com/r/xkcd_transcriber/) ^| ^[Statistics](http://xkcdref.info/statistics/) ^| ^[Stop Replying](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=ignore%20me&amp;message=ignore%20me) ^| ^[Delete](http://reddit.com/message/compose/?to=xkcd_transcriber&amp;subject=delete&amp;message=delete%20t1_d0cy3ba)
Yeah, I mean building rustc via CI and downloading the tarball.
Interesting, did you have access to the source code of the proprietary database or did you just keep poking it from the outside different a bunch of different ways till you narrowed down the issue?
I've used it in [Rust GMP bindings](https://github.com/fizyk20/rust-gmp/blob/4caee45c9786c3c19eaa50554c6e2ff93d60065c/src/mpz.rs#L478) to optimize arithmetic operators. This allowed me to reduce allocations - I don't need to allocate an additional structure for the result, I can just reuse `self`.
As someone who writes low-level embedded systems software in C every day, I would really like to discourage the notion of C as a "portable macro assembler". It really isn't; its language semantics have their own definition separate from that of any machine, and it has strict rules that must be followed if you want to have the "portable" part of that description. Following those rules keeps the language at a much higher level than assembly, and this is to the great *benefit* of the systems programmer! It isn't even *historically* true. C is a direct descendant of a really old language called CPL, which was based on Algol 60 but extended with a lot of fairly complex features. It was developed as a joint effort between London and Cambridge; one of the main developers was Christopher Strachey, who is one of the fathers of the study of semantics of programming languages. He used CPL as an example in his '67 lectures on [The Fundamental Concepts of Programming Languages](https://www.cs.cmu.edu/~crary/819-f09/Strachey67.pdf); if you read through those, you'll find they're very relevant to understanding C, C++, and even Rust. Because CPL was a large language for the day, it took a while before complete implementations were finished. A student of Cambridge, Martin Richards, wanted a language with the nice properties of Algol and CPL, but with the complicated parts removed so that it would be easier to write a compiler. This he named "Bootstrap CPL" and later "Basic CPL"; it was intended as a portable system that could be used to build compilers. It was first implemented by Richards while visiting MIT's Project MAC in 1967. He has continued to maintain and develop things with BCPL; you can get a recent version along with extensive documentation and learning material from his [home page](http://www.cl.cam.ac.uk/~mr10/). Project MAC was one of the main collaborators on the MULTICS project; Bell Labs was another until 1969, at which point they pulled out of the project, but not before Ken Thompson was exposed to BCPL. He liked it quite a bit (certainly more than the main language of MULTICS, PL/1), and because it was a relatively simple language he was able to make his own implementation of it with a somewhat different syntax. He called his version B, though he never developed a full compiler for it (BCPL was portable via an intermediate code representation that could be directly interpreted as well as compiled; this is the forerunner of JVM-style and LLVM-style byte-code). When he and Dennis Ritchie got access to a PDP-11, which unlike the machines they'd used previously was byte-addressable, Ritchie decided to write a compiler for B. To account for smaller-than-word sized data, he added multiple data types and structures to the language, borrowing ideas on the type system from Algol 68, but tried to keep the language as faithful as possible to its B (and thus BCPL) roots. Eventually 'New B' became C. Anyway, I don't want to derail too much from discussion of Rust, but given that C programmers are a target audience of Rust, I think that a better understanding of where the ideas embodied in C came from and how they're different from a macro-assembler is a valuable point of discussion. C and Rust share the semantic heritage of CPL, and Strachey's work on explaining a model for reasoning about the meaning of imperative programs (the lecture notes linked above introduce the concepts of "l-value" and "r-value" along with the concept of parametric polymorphism) is part of what allows compilers today to make efficient machine code out of our C and Rust programs, far more so than they'd be able to do with the output of a naive macro-assembler. Rust just embraces *more* of Strachey's concepts, along with other ideas in programming language semantics that have been developed in the intervening 50 or so years. :)
Isn't `&amp;mut self` better though? Then you have a choice whether you use it fluently or non-fluently.
Neat, but I'll still look into getting them on github pages. That one's missing some of the windows only stuff (because doc generation is platform specific).
Exactly what SirOgeon said; sometimes it is simply more convenient, especially when chained as part of construction of something. Look [here](https://github.com/Nercury/little-rs/blob/master/tests/interpreter.rs#L249) for example, note that it would take much more space to do it with `&amp;mut` borrow. However, in that particular case I also have [a variant of the same methods to do same things without taking ownership](https://github.com/Nercury/little-rs/blob/master/src/template.rs#L47). Might be overkill, but I like convenience.
This shows how to reasonably sort a vector of floating point values, but what's a nice way to find the minimum (here the minimum by abs())? fn main() { let mut xs = (-5 .. 5).map(|x| x as f64).collect::&lt;Vec&lt;f64&gt;&gt;(); xs.sort_by(|a, b| a.partial_cmp(b).unwrap_or(std::cmp::Ordering::Less)); println!("{:?}", xs.iter().min_by_key(|x| x.abs())); } Currently it gives: ...\test.rs:5:32: 5:42 error: the trait `core::cmp::Ord` is not implemented for the type `f64` [E0277] ...\test.rs:5 println!("{:?}", xs.iter().min_by_key(|x| x.abs()));
As `&amp;mut self` is basically sugar for the (illegal, `self` is a keyword) `self: &amp;mut Self`, `mut self` is basically `mut self: Self`.
I eventually found `strace -c` (which shows a count of the system calls) along with doing `gdb` dumps of the stack of every thread in a program along with monitoring the thread count of the program. TL;DR I poked the black box from outside.
that was important to note. The `mut` (unlike `&amp;mut`) is not part of the signature at all. You can define a fn taking self in a trait and then impl it with mut self, and the compiler will happily accept it: //you can define a fn taking self in trait trait Foo { fn foo(self); } struct Bar; impl Foo for Bar { // and then impl it like this fn foo(mut self) {} } the `fn foo(mut self)` is practically the same thing as: fn foo(self) { let mut s = self; // bind this mutably for the scope of the function //.... } the rationale being that when you move something (including self), you're saying that the ownership of that thing goes to the function. The function can then do everything, including mutate it, so the `mut` does not give any additional power in that sense.
/u/sophrosun3 I know it takes a bit more work, but without labels on the y-axis I have to search within the whole text to figure out what you are plotting and in which units if any (I guess I made this same comment about your previous post). It would also be nicer if the figures would have a caption with a label for the figure name, if the text will use this name when talking about the figure, and if the caption would have a one/two line description of what is plotted in the figure. Even if the caption repeats something thats already in the text, repeating important stuff twice doesnt hurt. I think that the work you are putting into benchmarking Rust is both awesome and of vital importance to the Rust community (which is very performance focused) but to allow a wider technical audience to participate into the discussions it is also very important that the data is presented in a water-proof way that leaves no room for misinterpretations. Some people might just skim over the blog post directly to the data and if they see something interesting they might read the whole thing and contribute some new insight to it, but if they don't understand whats been shown and the cost of entry is reading a page of python code they might not even participate at all. This would be a shame.
I know well that story, as I was already programming when C only mattered to those using UNIX. C is just the next step one would take if picking a powerful macro Assembler like MASM and replaced the processor opcodes with something else more portable. In fact TI has such an Assembly language for some of their processors. Extended Algol served Burroughs well in 1961 in hardware way more basic than any of the PDPs that C ran on. The fact is that any of the C designers was at the same level of the compiler writers for Algol, PL/I and other older systems programming languages. C's success is married to UNIXs adoption by the enterprise.
The problem is that `f64` is only partially ordered because of NaNs. You can either use a wrapper type that implements `Ord` or reimplement your own `min_by_key(_)` function (or both ‚Äì take a no_nan_f64 type that wraps f64 and disallows NaNs and return that in your closure, mapping Nan to infinity).
Being open source is a huge plus. For all the big reasons of preferring open source, but also the small stuff goes a long way, for example if documentation is out of date, or incorrect. I can update myself, and make a pull request, rather than make a support ticket, and wait for someday someone to care about the fact that this one document has the wrong windows commands.
in my years of experiencing web apps, this is a false equivalence. Of course, others's experiences may differ. :) Logic is significantly tougher in templates than it is in your actual language.
Currently I think this is not reasonable situation. Do you know one real example where not considering f64 Ord leads to safer code or some other improvements? How much f64/f32 processing is done in Servo/Rustc?
There was so much helpful feedback on my previous post, I was bound to miss something! Thanks very much for the reminder and for the compliments. What do you think a reasonable y axis label would be? After normalizing the data and taking the geometric mean, it's no longer really expressible in its original units. I guess I could call it "runtime as a percentage relative to [first day in series]" but that might be disingenuous because changes affect the geometric mean slightly differently AIUI. I agree that captions would be nice. Time to learn something new about matplotlib :).
Ah, that makes sense, that is what I am doing as well. Are you doing it the "double url in remote" way? (if you do then just ignore the rest of the comment) I found out about this a couple weeks ago, but if you set up both `github` and `gitlab` remotes, you can go into `.git/config` and add another remote manually, with 2 urls, like this: [remote "gitlab"] url = git@gitlab.com:user/repo fetch = +refs/heads/*:refs/remotes/gitlab/* [remote "github"] url = git@github.com:user/repo fetch = +refs/heads/*:refs/remotes/github/* # Then this to push to both: [remote "origin"] url = git@gitlab.com:user/repo url = git@github.com:user/repo and then you just have to `git push origin master` and it will simultaneously push to both.
Forgot to post in last weeks thread, but I finished on the weekend, so kinda valid here? ;) Anyways, finished the http://adventofcode.com challenges, which I had been working on casually in rust. This definitely helped me learn more about rust times, various collections in rust, and things about the language. Not the best of code in many cases, but it was fun to learn. https://github.com/agmcleod/advent-coder
This will be great! Can't wait to learn more about what others have been doing (and of course shamelessly steal things from Leaf to use in my own crate!).
On the contrary, I think it's perfectly reasonable. There is no globally valid definition of 'minimum' without a total order, and you cannot rule out NaNs without some work. So what should a `min` function return? What should it return for other types that have no total order where even less element pairs are within the ordering relation? Edit: And I'd like to add that Servo does a lot of floating point arithmetic, with all that CSS wrinkles...
Ooh, pretty. OK, I'll take a closer look :).
It's a property of `Ord` types that `a == b || a &gt; b || a &lt; b` _must_ be true. ~~Unsafe code can assume this invariant.~~ Therefore, there's no way to make floating point types `Ord` without violating the IEEE standard, because any comparison involving `NaN` evaluates to false. Whether this is a good design or not is perhaps a question, but it's IEEE's fault, not Rust's. Another option is to panic on comparisons involving `NaN`, but that can be done with a wrapper type as well.
Ah, nice, I didn't know about that one!
If you want to slice code points you can use a `Vec&lt;char&gt;`, which you can get from a `String` or `&amp;str` by `s.chars().collect::&lt;Vec&lt;_&gt;&gt;()`.
I briefly considered adding a footnote about SMP systems, but frankly, there doesn't exist a problem that blocking inside an interrupt handler while waiting for another core to release a mutex could possibly solve. If you want a responsive system, or even a system not prone to regular deadlocks, you do not block in interrupt handlers *period*.
I completely agree that C's success is almost entirely due to the adoption of UNIX, which was in turn almost entirely due to the fact that it was small, portable, and had licenses practically available for free to universities as minicomputers were becoming common at universities. What better way to teach an OS class? I'm not sure I agree with your characterization of early PDP systems vs. Burroughs B5000, though. I think, like many mainframes of the era, the B5000 was far more [architecturally advanced](https://en.wikipedia.org/wiki/Burroughs_large_systems) than the cheap minicomputers that followed. It has more in common with today's JVM than the PDP systems UNIX and C were developed on; it was basically custom-designed to run Algol, which is why a variant of Algol was the lowest-level language available. That doesn't mean Algol was low-level, it means the architecture was high-level. I also still don't agree that C is anything like a macro assembler. I've used macro assemblers; they are nothing like C. Early C was certainly a naively-compiled language, but that was more a function of the size of the machines it was running on than the language itself. It's clear by the way the C specification is written that it was intended to be able to generate sequences of instructions *not* designed explicitly by the programmer, which is the hallmark of a macro-assembler.
I wish there was a more ergonomic approach to allocation-free function passing.
You can call it "normalized run-time []" (without units) and link the label to the section of the text that explains how it is normalized. If I know that it is the normalized run-time, even if I don't know how it is normalized, I at least know that higher than 1 is bad, and lower than 1 is good, and can make a preliminary interpretation of the data. I for example just from looking at the graph was wondering if you were plotting speed-up or slow-down. Both seemed plausible :/
I wouldn't go with max/min values but with using standard deviation for the error bars. There are some other things you can use like a 95% confidence interval. See [wikipedia error bar](https://en.wikipedia.org/wiki/Error_bar) for a startup point. Whatever way of computing the error bars you decide to settle on, don't forget to mention it. I think with labels and error bars your graphs will tell a very solid story. The error bars might also explain why `getrusage()` gives different results.
The `num` crate is helpful with this kind of thing. There's no good way with just the standard library; `num` was in there, but wasn't good enough to save forever. It was the fourth or such attempt. Numbers are tough!
Right, so it always comes down to `panic()`, correct? Hypothetically, if one were not to use the standard lib (say, for writing a kernel), and avoiding the use of `panic()`, then Rust would have linear types. Am I correct in my understanding? Or did you mean something else when you said "implicit drop" (I'm not quite sure what that is).
Thanks. I'll have a look at num while waiting for the one true solution to numbers...
Cygwin offers LLVM packages, also LLVM should be compilable within Cygwin. Have you tried using a native LLVM? :.:.: Also your issues seem to from the intrinsic library? If your compiling i386 you need to compile i686. Rust 32bit expects all i686 symbols. Rust expects SSE2 on i386 but *strictly* i386 doesn't include MMX/SSE/SSE2 
Well, `Eq` and `Ord` aren't marked unsafe so maybe not. But that would mean you can't ~~compare values~~ depend on the results of comparisons for safety in generic unsafe code, which is kind of unfortunate.
It's not really about panic, it's just that panics will unwind, and unwinding runs destructors. Destructors are the crux of it, and the reason why it's pedantically correct to say that Rust has "affine" types instead of "linear" types. If Rust had linear types, it would have to enforce that all values are explicitly consumed, e.g. via an explicit call to `.drop()`, even if those values don't actually have any work that needs to be done in a destructor. Failing to call `.drop()` on 100% of the values in your program would be a compilation error. It's a lot more verbose than affine types, but it avoids some of the undesirable semantic consequences that are introduced by having implicit destructors.
I'm reminded of a [comment](https://www.reddit.com/r/rust/comments/3znc2w/my_thoughts_on_rust_in_2016/cyof04t) I saw a while back: &gt; 2016 is the year of the implementation detail. All else must fall at the hands of the dark lord MIR. It's very exciting to see this make it's way forward so quickly!
One way to do it is using a fold: `[1.0, 2.0, 3.0].iter().fold(std::f64::INFINITY, |a,&amp;b| f64::min(a,b))`
(In their paper, Talpin and Tofte present a safe system that works completely without GC, but I guess it didn't work quite well in practice, since the current ML Kit can have dangling pointers)
&gt; For example, you could implement mio::Evented on a thread pool to notify you when the pool has available threads. Isn't this how kqueue works? Seems kind of like implementing kqueue on top of epoll with rust. Which doesn't seem like such a bad idea. I'm woefully uniformed though.
[Here's the tracking issue](https://github.com/rust-lang/rust/issues/27840)
The Cygwin i was using is a 64bit one. I'm compiling an x86_64 MinGW GCC compiler on that with crosstool-ng, and then using the resulting compiler to build Rust. I'm trying to skip the issues with 32bit GCC altogether; on Windows Rust requires a specific threading and exception model (threads=win32,exceptions=dwarf/seh) and as far as i understood the 64bit build doesn't suffer from this restriction.
These were introduced to abstract out the common logic of manipulating borrows, allowing the `Ref`/`RefMut` types to focus on the actual data they hold, https://github.com/rust-lang/rust/commit/5048953debe9eb44c672f3c453e894bd64a1301a This wasn't a factor when it was introduced, but now, this breakdown is especially helpful with the `map`/`filter_map` functions, as they can just pass the borrow around, and move semantics help reduce the possible mistakes (e.g. someone editing `cell.rs` can't easily accidentally make multiple `RefMut`s to the same data, because each one needs a `BorrowRefMut` and there's no way to duplicate it). (BTW, for your edification, I answered this question by using [`git blame` on the source](https://github.com/rust-lang/rust/blame/master/src/libcore/cell.rs). :) )
As I recall, the Talpin Tofte system worked without GC, ... ... but (and this is a big "but"), the region inference system was free, in principle, to allocate objects to the region at the base of the stack if it could not infer a more precise extent for the value. I.e. such storage wouldn't be reclaimed until the program exited. At that point, you're relying on a combination of the compiler doing a good job at region inference *and* on the programmer to write their code in a way that doesn't force the compiler to make bad decisions. I'm happy with the Rust approach, where we tend not rely too much on such magic (especially not via a global program analysis).
Thanks :)
Here's what I had to do to get `gtk` to build *outside* of an MSYS shell (which I *loathe*). 1. [Install MSYS2](https://msys2.github.io/). 2. You will need to `pacman -S mingw-w64-i686-pkg-config` and `pacman -S mingw-w64-i686-gtk3`. If you're targetting 64-bit, replace `i686` with `x86_64`. Possibly more, but that's what it took for me. 3. Make sure `...\MSYS2*\mingw32\bin` is in `PATH`, or `mingw64` if you're targetting 64-bit. You only get to pick one (unless you write environment setup scripts for both and run those first). 4. Make sure `...\MSYS2\mingw32\i686-w64-mingw32\lib` is in the `LIBRARY_PATH` environment variable, otherwise Rust's linker won't be able to find all the win32 import libraries that `pkg-config` will tell it to pull in. If you're targetting 64-bit, change to `mingw64\x86_64-w64-mingw64` (probably). After all that, it *might* work. At this point, you will discover that GTK3 looks *even worse* on Windows than GTK2 does, and seriously consider just giving up on the whole enterprise. :P
My Masters was based on MLKit (see http://research.microsoft.com/pubs/67471/afl-pldi95.pdf). Basically, we tried to make the analysis more fine-grained, for example discovering temporaries that could be freed at the end of a "loop" represented as tail recursion. In retrospect, I would say it's accurate to say it didn't work very well, for a variety of reasons: 1. It wasn't precise enough that you could rely entirely on static memory management. A big part of the reason why is that we hadn't nailed down move semantics back then, though there certainly were people thinking about linear types. 2. Mixing malloc/free and gc in the same runtime doesn't have particularly good performance. A really good gc should be able to free short-lived garbage as fast as an explicit free, and you still have to trace O(everything), so there's just not that much win. The poor performance of JNI is more evidence it's hard to mix the two approaches and get good performance. 3. (And, what I think is ultimately the nail in the coffin of the idea for real use) All these types of "magical" analysis are very fragile. It's common to experience that a small change in one part of the program means that in another part, a formerly efficient loop has turned into a huge memory leak. Even for programming language grad students, it's impossible to keep a model of exactly what the type checker will do in your head. Rust basically solves all of these problems, but there is friction in having to make everything explicit. I do think in the back of my heart that there's a place for "easy mode" Rust, where you can (for example) use strings just as if they were numbers, and the compiler fills in clone and/or reference count operations automatically as needed. But that would definitely be a topic for future research.
Rust is a programming language. It's a general programming language, so it's good for a large variety of things. Different people have different reasons for using Rust. There are three large constituencies, as I see them: systems programmers, functional programmers, and scripting language programmers. Before I get into details: all generalizations are false ;) Systems programmers come to Rust because it's able to do the lowest levels of software: operating systems, device drivers, stuff like that. Where it improves on existing systems languages is "safety." The Rust compiler does a lot of compile-time checking to make sure you don't do things wrong. Existing languages force you, as the programmer, to double check your work. We have the compiler double-check your work, and force you to get it right. Functional programmers see all that static checking, and it feels like home. But a home where they have significant speed gained. Functional languages can be fast, but not always C level fast. Rust is. It's easier to get that level of performance out of Rust. Scripting language programmers come to Rust for two reasons: the first of which is to write extensions to their language for speed. You can write a Ruby gem in Rust, rather than in C. This is useful because this low-level stuff is not their expertise, and so the extra checks are really nice. Related: we have a lot of the nice tooling that scripting language folks have come to expect from their language. No need to write makefiles, we have Cargo. The second is sort of related, but these checks also mean it's easier to learn low-level stuff, so if they want to expand their universe, Rust is a nice way to do so. We're seeing a lot of people for whom Rust is their first systems language. I didn't know where to put this one, but Rust focuses on a concept called "zero-cost abstractions." This means that a lot of our features that feel very high level are extremely efficient. You don't write manual for loops, you se iterators. You don't deal with pointers directly, but with references. Structures like Box and Vec do memory management, but you don't need a garbage collector, and you don't need to manually manage it, but you also don't need to explicitly call `free()`. These are very broad strokes. Lots of other people have other reasons too. But that's some of them.
[Here's](https://github.com/google/tarpc/pull/27) how it's shaping up.
Awesome! Thanks!
Thanks for the update tikue :)
&gt; but it avoids some of the undesirable semantic consequences that are introduced by having implicit destructors. Like? I always thought that writing `.drop()` calls yourself at the end of a scope or having the compiler automatically write them for you would be semantically equivalent. 
The only thing I don't like about the async/await approach is that they polute the code base making it hard to reuse code between stackless and stackfull coroutines as well as threads. I like a stackless "down" suspension model better. Its a bit more verbose when you want stackfull coroutines, but then these can be implemented as a library and be made pretty ergonomic with lambdas. Not perfect, but then I guess this is a trade-off between performance, reusability/flexibility and ergonomics.
`foo` doesn't return anything, while `simple_return` does. If we wanted to be able to `await!(foo())` we could add `fn foo() -&gt; () {` and the generator would add a callback.
Do you have any examples of this?
Terminals are like very simple cursors that point somewhere (ex:`/.../... $`) in your harddrives file system, if you make a program that outputs a file, its going to need to choose where the file output goes or just guess (usually dropping it where you called it from, because its the most logical guess, if you're a user, you might not even know where the program is). This is why you can send arguments to programs via the command lines usually. Doing so to select the output location in terminal programs. `$ program path/to/file` or even more arguments `$ program arg1 arg2 arg3 arg4 ....`, in most terminal spaces separate arguments. In Rust you can get arguments with [`std::env::args`](https://doc.rust-lang.org/std/env/index.html) or other `env` functions for other things. Anyways, by this we know the applications can only know something when they are spawned. If you can pass him an argument, this is going to be when you use [`Command.arg("arg")`](https://doc.rust-lang.org/std/process/struct.Command.html) otherwise, you are going to have to make him believe you're somewhere else, looking at the docs you can probably use this [`Command.current_dir("/path/to/file")`](https://doc.rust-lang.org/std/process/struct.Command.html#method.current_dir) i am guessing.
You are almost there :) This is indeed an application for a scoped threads API which is currently not in `std` (not even in nightly; the version that was in std was removed as unsound), but several crates come to the rescue. For example, [crossbeam](http://aturon.github.io/crossbeam-doc/crossbeam/struct.Scope.html) provides a scoped API. Others like [scoped-pool](https://github.com/reem/rust-scoped-pool) and [scoped_threadpool](https://github.com/Kimundi/scoped-threadpool-rs) provide a thread pool that can be reused. In general for such parallel iterating stuff you will want to look at crates like [rayon](https://github.com/nikomatsakis/rayon) and build upon work-stealing `join` or parallel iterators. [jobsteal](https://github.com/rphmeier/jobsteal) is another often recommended crate.
[P0114r0 "Resumable expressions (revision 1)"](http://open-std.org/JTC1/SC22/WG21/docs/papers/2015/p0114r0.pdf) is the C++ proposal that competes with the async/await proposal. Then there is [this very long thread](https://groups.google.com/a/isocpp.org/forum/#!searchin/std-proposals/resumable/std-proposals/L5ZsY1SYnrA/rReGgjMvCwAJ) in the C++ std.future-proposals list discussing async/await vs resumable expressions. And finally I just discovered this paper [P0171R0 "Response To: Resumable Expressions P0114R0"](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0171r0.html) from the main author of C++ async/await proposal: [P0057R00: "Wording for Coroutines (Revision 3)"](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0057r0.pdf). Have to still read it though. EDIT: that last paper is good and makes some strong points for async/await. 
I guess [this](http://is.gd/olyl4t) is the reason (variables with same name in an inner scope (or in the same scope) hiding the previous one -- but all of them should be listed at start in MIR).
Fair enough.
&gt; I'd also like to reuse a pool of threads to amortize thread creation overhead That's what the two pool crates do. &gt; so I'm not sure if that would still count as "scoped threads" The "scoped" part is orthogonal to whether a pool is used or not. The scope just ensures that the closure you're running can borrow stack variables from the parent function.
&gt; Rust basically solves all of these problems, My trouble is that the borrow checker rejects some common idioms (eg. internal references in structs) - the alternative being dynamic borrowing, that panics on error instead of failing to compile. I want static guarantees dammit. :( edit: so I welcome any and all modifications to the borrow checker to accept sound programs that it previously rejected (eg. [non-lexical borrowing](https://github.com/rust-lang/rfcs/issues/811)).
I am creating a binding with a C static library. After compiling the C library I am left with a BeaEngine.lib. The rust code is very simple as based on [the book](https://doc.rust-lang.org/book/ffi.html) use std::os::raw::c_char; #[link(name = "BeaEngine", kind = "static")] extern "C" { #[no_mangle] pub fn Disasm(disasm: *mut DISASM); #[no_mangle] pub fn BeaEngineVersion() -&gt; *const c_char; #[no_mangle] pub fn BeaEngineRevision() -&gt; *const c_char; } The problem is that it never mentions where to put the static .lib where rustc can find it? error: could not find native static library `BeaEngine.lib`, perhaps an -L flag is missing? I have tried to create the following build.rs (and annotate it in the project's Cargo.toml): fn main() { println!("cargo:rustc-link-lib=static=BeaEngine.lib"); println!("cargo:rustc-link-search=native={}\\bin\\x86_64-pc-windows-msvc", env!("CARGO_MANIFEST_DIR")); } (related, when I used `env!("TARGET")` as advised by [this doc](https://github.com/rust-lang/cargo/blob/master/src/doc/environment-variables.md#environment-variables-cargo-sets-for-build-scripts) it complains that it doesn't exist, no big deal I hard-coded it as shown in the previous code snippet). Anyway, I have a C static .lib, where do I put this file to make it link correctly?
So basically this concurrency pattern can't be implemented without using unsafe code and unstable language features? In this case, wouldn't the best approach be to simply use raw pointers? It's very simple and works on stable.
Drop variable tmp4 and jump to block bb5
This is going to become much simpler with gtk 0.0.7: either make sure `pkg-config` from msys is in `PATH` or set `GTK_LIB_DIR` to `...\MSYS2\mingw32\i686-w64-mingw32\lib` to skip `pkg-config` altogether.
Those links are very helpful, thanks.
The state machine code appears to bear some resemblance to the new MIR intermediate representation: bb0: { tmp1 = Eq(const 1, const 1); if(tmp1) -&gt; [true: bb2, false: bb3]; } bb1: { return; } bb2: { var0 = const 1; tmp0 = (); goto -&gt; bb4; } bb3: { var1 = const 2; tmp0 = (); goto -&gt; bb4; }
&gt; No unstable features involved here. See [`FnBox`](https://github.com/rust-lang/rust/issues/28796). I thought it would be stabilized sooner, but after going through the issue, I'm getting a feeling of later...
That's what the two pool crates do. Crossbeam is not one of them :)
This looks like the exact issue Rayon was intended to help.
As others have pointed out, I think we have to choose between either moving the input vector into an Arc or depending on one of the crates that implements scoped threads. This [seems to work](http://is.gd/YhjI9N) with Arc: use std::sync::Arc; use std::thread::spawn; fn multithreaded_map&lt;T, U&gt;(input: Arc&lt;Vec&lt;T&gt;&gt;, f: fn(&amp;T) -&gt; U) -&gt; Vec&lt;U&gt; where T: 'static + Send + Sync, U: 'static + Send, { let mut threads = Vec::new(); for i in 0..input.len() { let cloned_arc = input.clone(); threads.push(spawn(move || { f(&amp;cloned_arc[i]) })); } threads.into_iter().map(|t| t.join().unwrap()).collect() } fn main() { let input = Arc::new(vec![1, 2, 3]); fn f(x: &amp;i32) -&gt; i32 { x + 1 } let output = multithreaded_map(input, f); println!("{:?}", output); } This takes a static function instead of a closure, because I wasn't able to figure out all the inscrutable compiler errors the closure was giving me. But maybe someone can suggest how to fix it so that it takes a closure? Edit: I found [a way to do it with a closure](http://is.gd/4VdkRR). I have no idea whether this is idiomatic though. Edit again: Reading OP's question again, I think I understand it better. The real challenge here is letting the different threads write to chunks of the same vector, so that when all is said and done we have a complete vector of outputs, without having to reallocate the whole thing at the end to put it together. One problem with doing that in safe code is that Rust is going to insist that you don't pass around uninitialized slices. A `Vec` will let you preallocate some uninitialized capacity if you're only inserting from the front, but I don't know of a way to insert at 5 different points all at once, unless the whole `Vec` has been initialized to some default value.
In the code for "fn sum_producer", I think there is a typo in the line where it splits the producer: ``` let (left_producer, right_producer) = iter.split_at(mid); ``` it should be "producer" instead of "iter" 
Thanks. For reference: [LLVM invoke instruction](http://llvm.org/docs/LangRef.html#invoke-instruction), takes a normal label and an unwind label.
Just to save this knowledge for the future; I was compiling the HEAD version of bit flags, whereas [version 0.1.1](https://github.com/rust-lang-nursery/bitflags/commit/78241d80cb9d0e1d64976afd8be6d35dd58824b0) is explicitly required. 
As far as I can see, `crossbeam` just implements it in `lib.rs`.
Whoops! I think I'm going blind! :)
Hey, neat! And you can use the same mechanisms to statically check that mandatory parameters are set _exactly_ once before `.build()`. I'm definitely stealing this idea.
Hey, author of jobsteal here. I'd just like to note that it's still pretty experimental and that although it seems to work, there may be safety holes that I haven't caught yet. It also doesn't handle jobs with large environment very well (or at all). For anything serious, I would stay away from it for the time being. However, I have also put a lot of work into making it performant, and I think in anything more complex than microbenchmarks will see significant yields in performance due to the emphasis on contiguous memory. For toy projects, like the one I created it for, jobsteal should be fine.
Will these videos be online? (Please say yes)
As you note, the lack of RAII would be my primary concern. fn main() { let mut x = &amp;Mutex::new(5); { // ML Kit infers that y is referenced by x, // so it makes it live longer let y = Mutex::new(6); x = &amp;y; } println!("{}", x); } Automatically extending the lifetime of `Mutex` basically kills any human analysis. Magic ruins the day :(
I wanted to draw a parallel between `async fn` and `const fn`... but found myself wondering about the semantics of `async fn` suddenly. I suppose that an `async fn` may be called from a regular function, because otherwise by transitivity from `main` none could be ever called. I also suppose that `async fn` may call regular functions, because otherwise the set of operations they could perform would be quite limited. While the scope of `const fn` is clear: - necessary to initialize `const` items - a `const fn` may only call `fn` that are also `const` I find no such limitation in an `async fn`, except the fact its call site should use the `await` keyword. Thus the only limitation I find is that an `async fn` would require run-time support, which the various bare-metal communities might not particularly appreciate (especially if more and more libraries switch to such calling conventions)... ... yet your premise: &gt; The only thing I don't like about the async/await approach is that they polute the code base making it hard to reuse code between stackless and stackfull coroutines as well as threads. makes me think that a lot more is at stake and that I missing something. Could you please expand and enlighten my poor self? *Note: I've not used those facilities in C# nor Python so that may be why my understanding is so limited.* 
http://pcwalton.github.io/blog/2013/06/02/removing-garbage-collection-from-the-rust-language/ is very good reading for (1) and (2). An interesting statement is "Actually, we were confused on this point for a long time as well‚Äîit wasn‚Äôt clear whether ~ [owned polinter] or @ [GC'ed reference] would become dominant." The answer only emerged after considerable use of the prototype language. Also see "Is Rust garbage collected?" in https://www.rust-lang.org/faq.html For (3) it's a core philosophy of the language that when a construct has a potential performance problem, you have to invoke it explicitly, ie there are no hidden performance "gotchas". I haven't easily been able to find a clear statement of this philosophy written down, though.
&gt; If I have to append benchmark output to a CSV once more I'm going to drown the world in a bath of fire. Haha! It seems like the benchmarking stuff is on at least one other person's mind (I've been tinkering with things the last couple of weeks), which is nice to see. EDIT: Related, I've been using [this script](https://github.com/dikaiosune/rust-runtime-benchmarks/blob/master/bench-suite-linux/run_nightlies.py) to run different nightly compilers for tracking performance. It seems like this is something that the community needs in a more standard form than just "write a bit of python each time you are curious about performance."
You're missing fortunes.nki and original.nki in the src directory.
I had completely forgotten that. I was remembering how things were *ages* ago.
The authors are over in /r/programming, you might want to re-paste this there too :)
&gt; I do think in the back of my heart that there's a place for "easy mode" Rust, where you can (for example) use strings just as if they were numbers, and the compiler fills in clone and/or reference count operations automatically as needed. But that would definitely be a topic for future research. Currently Rust is split in two languages, the regular safe one, and the unsafe{} dialect (and there's the asm too). Creating another Rust dialect like a auto{}, that clones and reference counts heap memory automatically, seems possible and I think it's not too much hard to do. Such auto{} could be handy when you're writing code that doesn't need maximum performance (some GUI code?). But how do you interface regular "manual" Rust code with auto{} Rust code?
As you mention, raw data beats rankings. There are lots of other metrics that can be extracted from data. * number of stars * number of commits * projects with more than 1 committer * post, vote and subscriber activity on the subreddits for a language * number of posts with that tag on careers.stackoverflow.com and the half-life of a posting * edit, ratio of work week commits to weekend commits to gauge amount of rust done "on worktime" Github is a hard one because forks are often counted in queries. And to get adequate projections we would need to know the derivative and integral of all of these curves.
In general, we're interested in custom test frameworks, which would allow for people to write ones which output results in whatever structured format makes sense.
Re: serde, it turns out it's not *that* painful to handroll the code for most fairly simple structs. I've been using nightly while iterating heavily on the layout of the data in my current project, but have plans to manually implement the serialization/deserialization code once the struct definition stabilizes. At that point, I'll be able to build the program on stable without buildscripts. It's not ideal, but I've already tinkered with it a bit and it's generally OK. Granted, my current project reads from and writes to a very large index that doesn't have many fields, so I imagine that for projects with many structs that need serde it won't work. Maybe there's a way to capture the output of the syntex build script and save it in a file that can be copypasta'd into the main source?
Exciting! Even more performance settings for low-level use cases. &gt; disabling unwinding in a principled fashion provides far higher quality error messages, prevents erroneous situations, and provides an immediate benefit for many Rust users today. I don't see how this follows. Surely disabling unwinding produces worse error messages?
I tried using 0.7 but the json serde thing does not work with it. Also how are you using it? I tried the cargo overlays but the inter dependencies of the other modules make it nearly impossible to use without hackery.
&gt; requires that all fields are specified could you please explain it little bit? (I have not tried Serde because it doesn't work with stable Rust). Does it mean if on input we have string where not all fields of JSON are mentioned, then it will not be parsed into structure, even if values for fields are Option?
Regarding serialization, it seems both serde and rustc-serialize half-conflate two concerns that are often separate. Fully reversible serialization on one hand and read/writing JSON on the other. Both libraries have even chosen to prioritise writing nice JSON over being able to serialize all kinds of rust types (because map keys must be strings). Users have to use bincode to get "real serialization". "Real" reversible serialization happens when a Serializer can handle any type that implements Serialize. (adjust trait names per library).
Totally unfamiliar with mio, but I see the word "channel" and wonder if Rust+mio would therefore offer the same capabilities as Go with its channel and goroutines.
Hi thanks for the note. I fixed it, if you want to try it again. Sorry about that. It speaks to how difficult it is to put out a fully working thing by yourself the first time even for a tiny thing like this. I tested for all the stupid edge bugs and read the rfc to check if the behavior was right, then I downloaded and checked the releases but I forgot to do a clean src checkout. My possibly love of include_str and possibly stupid packing of files into the binary explodes the files in the current working directory on first use, with cargo run it explodes it in the top level directory so I added them to the gitignore but didn't make it top level only.
Looks like it just landed 40 minutes ago.
I *just* released [Serde 0.7](https://www.reddit.com/r/rust/comments/47tuow/serde_07_many_many_changes_including_the_long/), which finally switches over to ignoring unknown fields by default, plus a whole host of other features. I hope that'll help with your frustrations! (PS: thanks for flask! having a lot of fun with it at work!)
You can create a newtype wrapper around these big slices and provide these impls yourself. You'll have to do it for each slice size, unfortunately, but you can create a macro that will eliminate a lot of the boilerplate: http://is.gd/rlviw8 Essentially, [this has already been done in the stdlib](https://github.com/rust-lang/rust/blob/master/src/libcore/array.rs), but only for array types up to 32 elements. It creates a lot of code bloat so I can understand the Rust team's unwillingness to add more than that. 
In the now old 0.6 version of serde_codegen, it was much more pedantic, and would error out deserializing a value like: struct Foo { x: usize } But the server sent you the JSON value `{"x": 1, "y": 2}`. You could always write a `Deserialize` implementation by hand to ignore these fields, but that's annoying, so in serde_codegen 0.7, the default is now to ignore unknown fields, and a flag, `#[serde(deny_unknown_fields)]` to get the erroring out behavior.
&gt; Serde now by default ignores unknown fields when deserializing Thanks a lot for this. We have been stuck with *rustc-serialize* because we needed to ignore unknown files.
Sorry it took so long. There were a whole bunch of breaking changes I've been meaning on doing, and it took some time to get them all over the finish line. I hope this works for you!
[Serde 0.7!](https://www.reddit.com/r/rust/comments/47tuow/serde_07_many_many_changes_including_the_long/)
Those numbers don't make much sense. Stackoverflow frequency is a weird choice for popularity: a difficult language will score higher than an easy one, and I suspect that e.g. SQL scores high because it's something many programmers encounter now and then without having a proper background and then just head off to stackoverflow to ask the obvious. In SQL's case it's also not really a language, but rather a whole bunch of platforms with their own idiosyncrasies, so the questions range from "group if in mysql" to "how to restore SQL server". SQL won't be popular on github, since it's not a language you'd write a program in. Rather, it is usually generated within another programs, and even then only takes a few lines. And why VimL is there, is beyond me. It doesn't seem to be used as a language but rather as the subject of parsers and compilers (clojure-to-viml, I kid you not). So even when the numbers are accurate, it's still comparing apples to orangutans. Edit: it basically irks me that people pull numbers from something and believe that they are useful without even a basic attempt at validating the interpretation.
... and is currently used to measure the performance of the rustc stable versions (1.0.0 till 1.6.0, beta and the current nightly), with all four opimization levels. The measurements should be finished till Monday or Tuesday.
Try /r/playrust. This subreddit is about Rust programming language, not game. 
As someone who's never done anything with machine learning in their lives, would this crate be a good introduction to the field?
Omgggggg, thank you, Erick! I will be switching my projects over right away. This looks great.
I think there may be a bit of a false equivalence here, because other than the fact that both languages have relatively stronger type systems than most mainstream languages, neither Rust nor Haskell have very much in common. (And just because Rust traits have some similarities to Haskell typeclasses doesn't itself justify the comparison, any more than the presence of closures in Rust warrants a comparison to Lisp). Given the text in the OP, I'd say the more interesting question is, "as someone who uses $SOME_LANG, why should I bother learning Rust?" In the case where $SOME_LANG = Haskell, well, intuitively I'd say that there may be little to interest you... but then again, I do have multiple Rust-using friends who were former Haskell devotees, so clearly there's *some* appeal that I'm just not aware of. :P Dissatisfied with laziness, perhaps?
It's hard to compete with geth, on a lot of levels.
I don't know if I should be commenting, as I haven't actually written that much code in Haskell (or Rust, although these days I'm actually contributing small libraries, some Haskell-inspired), so maybe the experience isn't what I'm worried it would be, but, some things that Rust "fixes" about Haskell, from my perspective: * No Cabal-hell and no wondering if some set {A, B, C} of packages are mutually incompatible because A wants version 1 of C and B wants version 2 of C. A whole bunch of people seem to be rallying around a curated list of Haskell libraries as a way to "fix" this. * The names of fields of records don't leak out of the namespace. In Haskell, you can't do `data Dog = { name :: String }; data Person = { name :: String }`, because that's defining `name` twice. * Using monad transformers to structure programs. Monads are great, but they don't combine so nicely. The order which you combine monads makes a difference. And it seems to be a popular way to structure everything, one monad based on monad transformers for a program (e.g. a Snap application). And here's one reason for a Haskeller to look at Rust more than other functional languages: * Inherited mutability. It reduces action at a distance. If you squint, `&amp;mut` looks sort of like the State monad (EDIT: Or maybe ST monad's STRef?) . You can pass a value to something expecting `&amp;mut T` and get a value back, but things other than the owner can't observe the mutation that occurred (modulo globals and unsafe and IO and a whole bunch of stuff).
Eh, I never had much trouble, but admittedly that depends on your [difficulty level](https://etherscan.io/charts/difficulty).
&gt;I do have multiple Rust-using friends who were former Haskell devotees For me, it's about control and performance. It's hard for me (but rewarding) to optimize Haskell programs, relative to, say, Rust.
Yes, those are some really good advice. I would call it a must read. Clippy is actually an impressive project and a good example of dealing with forgetfulness and easy mistakes. I just wish it was as easy to use with stable Rust...
I'd rather have it easy to use with nightly Rust, actually. Just today I had another PR extending the `LintPass` traits merged. Waiting 12 weeks to use code is not an attractive perspective. Making multirust-rs the preferred setup method would go a long way in dealing with this, IMHO.
Except that, as far as production software is concerned, nightly is a no-go. This ties well into the article. The subset of libraries that you can actually use with a stable version shows that the ecosystem is not mature enough in some ways. I think we should strive for stable compatibility at some point to really call the language production-ready.
Just for point 1, cabal hell has basically been solved by stack and/or nix. If you're getting in to Haskell today, you shouldn't need to worry about it at all.
IIRC `Range&lt;Idx&gt;` can only be `Copy` if `Idx: Copy`. I'm not sure if it makes sense to implement that.
Oh, don't get me wrong. It's just the lazy side of me speaking ;) I'm primarily using stable, for various reasons, and jumping between stable and nightly is a bit of a barrier. I haven't investigated too deeply into the issue, though, so I'm sure I'll figure out a good work flow once I do.
Comments, criticisms, &amp; improvements very much welcome! This is my first Rust project, so I'd kinda like to know how I did ;P
I'd like to see an example of what it does, I don't quite get it
Note that `RangeFull` and `RangeTo` both implement `Copy`, while `Range` and `RangeFrom`, which implement `Iterator`, do not implement `Copy`. My recollection says that this was deliberate, that there were cases that could be pointed to where the copyability of `Range` and `RangeFrom` had caused mistakes, but I don‚Äôt have time to try to find such evidence now, and it‚Äôs not documented. (As I see it, either `Copy` implementations should be added or explanation of why they‚Äôre not there. And `RangeFull` and `RangeFrom` should have explanation of why they don‚Äôt implement `Iterator` (no sensible starting point).
Haskell: When you want to feel smarter than everyone else in the universe Rust: When you want to feel like an idiot because even a mere compiler is smarter than you. (kidding, kidding :P ) Really though, whilst learning Haskell^1 you get all these wonderful epiphanies that feel awesome. Learning Rust is the opposite feeling; the compiler is very nice and helpful, but trying to work with the borrow checker is arduous and not-awesome work. Pays off in the end however, especially once you internalize it. Neither of these crop up when _using_ (as opposed to _learning_) the language, though, so it's not really a major concern in this context. ^1 I guess this applies to any functional language? Not sure.
Yes, you can do that for clippy, but when it comes to libraries, you don't really get that choice. We have a great Raft implementation, nightly only. The full-fledge serde? Nightly. And the list goes on.
&gt; Edit: it basically irks me that people pull numbers from something and believe that they are useful without even a basic attempt at validating the interpretation. At the very least, the Redmonk guys are up-front about how they generate this ranking and its limitations. It's hardly their fault if people attempt to read too much in the presented data ;) That being said, a single snapshot is not very helpful, it is much more helpful to look at the serie of snapshots to try and see whether things are accelerating or slowing down.
&gt; that's defining name twice Because record syntax is syntactic sugar for definition of pattern matching functions: name :: Dog -&gt; String name (Dog name) = name name :: Person -&gt; String name (Person name) = name So solution would be to define them in separate modules and have `Dog.name`, `Person.name` or use type class for eg. `name :: HasName a =&gt; a -&gt; String`. 
Fixed, thanks!
Some human eyes are needed to debug things.
Thanks! This is a neat solution. The stdlib code you linked to also led me to find [arrayvec](https://crates.io/crates/arrayvec).
I'm reasonably confident that basic serde support can be implemented in `macro_rules!`, given the two extra built-in macros I have kicking around... need to finish writing the RFC for them.
I'd love to see another thing that automatically downloads what's been said in this subreddit, and feeds it to inebriated-rs, and publishes the "weekly punch of drunken Rust" :D
Yeah, /u/yupferris was the impetus for starting the project to begin with. I was following his methodology of implementing each instruction as it came up, though some stuff (e.g. B/BL and LDR/STR/LDRB/STRB) made sense to implement as a block. Decoding all the instructions at once before thinking about implementation at all doesn't sound that bad in principle, but the instruction set is in the Architecture Reference Manual, and I can only find that for ARMv6. I'd rather let the hardware tell me what instructions I need to implement than constantly be working out where the manual is talking about things I don't need to be concerned with.
Maybe I'm just too stupid, but I just can't figure out how to develop Haskell stuff under NixOS ‚Ä¶
Three comments: ~~First: I was looking at the serialize_with example and when I try to create the MySerialization trait it yells that Self is not Sized. ~~ (Figured it out) ~~Second: Is there somewhere which shows how to write a serialization for a struct by hand? I have one struct which is from an external crate that I would like to do by hand. Also I might need to do one by hand to handle the problem in comment three (below).~~ Third: Right now serde_json doesn't like BTreeMap&lt;u64, _&gt; it yells that the key must be a string. Unfortunately I need a sparse map of IDs or I would use an array.
https://github.com/rust-lang/rust/pull/27186 and Alex's comment therein I agree with you in that we need to improve the documentation.
Hm it might not be as deprecated as it used to be. Its repo was https://github.com/rust-lang-deprecated/num, but it since has been moved into https://github.com/rust-num/num. If I recall correctly, in the past it was considered pretty incomplete and the BigNum implementations didn't perform that well. Anyway, I moved out the num impls into the num crate to cut down on the required impls needed to use serde. 
https://gfycat.com/CoolShrillBighorn I've worked on a self driving car demo. A neural network (in its early stage, after around 10 generations) is driving the car in the gif.
In contrast, I'm obviously using nightly, but have Travis configured to build my crates with stable+beta too wherever possible. I also have multirust to run the benchmarksgame entries on stable, or to run clippy against a stage1.
Rust has channels in libstd and for coroutines there are hell lot of libraries (coroutines and mioco is what I can recall right now). Also there are some work-stealing libraries (i.e. Rayon) which provide coroutines-like feel to Your code. 
I think you're dead on. This junk pops up a lot in enterprise Java applications too. 
I don't remember where I got my reference PDF from, but this here looks pretty decent as well: http://morrow.ece.wisc.edu/ECE353/arm7tdmi_instruction_set_reference.pdf I does not handle the GBA's co-processors or the THUMB instruction set, but it's all the ARM state instructions the GBA handles, so there will be no need to filter ARMv6 instructions out. ;) And an implementation tip for the instructions: Do some register renaming in your code when handling MUL/MLA and their MULL/MLAL cousins. For whatever the reason, they swapped Rd and Rn. So if the manual tells you to do `Rd = Rn * Rm` or whatever, do `Rn = Rd * Rm` instead. This way, decoding register addresses comes without any exceptions, and less exceptions while decoding instructions obviously means way simpler code. At least that's what I do. You could go with `instruction.mul_Rn()` as well. That's the only piece of ARM I know of that I don't like much.
Is there a deeper meaning to what `'static` represents? I thought it just means "valid for the lifetime of the program", but I don't think I understand why this compiles: fn works&lt;T:'static&gt;(_: &amp;T) {} fn main() { let x: u32 = 0; //let no_work: &amp;'static u32 = &amp;x; works(&amp;x); } But the `no_work` binding doesn't work when uncommented. I'm not sure how exactly they're different. I mean, I know I can't have `works` take a `&amp;'static T` either, but again I'm not sure why. It seems to me like none of these should work because `x` is only valid for the lifetime of `main`.
Following up on my own thing here. I think part of the explanation is that the `T` in `works` is referring to the type itself (which in this case will be `u32`), not to any particular instance of that type (like `x`, which indeed only lives for `main`). But I'm not quite sure what it means for a type (rather than an object) to be valid or not for the lifetime of a program. I tried to think of some way to make the bound fail, and did indeed come up with this: #![allow(dead_code)] struct Foo&lt;'a&gt; { x: &amp;'a u32 } fn no_work&lt;T:'static&gt;(_: &amp;T) {} fn main() { let foo = Foo { x: &amp;0 }; no_work(&amp;foo); } So maybe the rule is that a `'static` type is a type that transitively contains no non-`'static` references? (Is there any other reason that a type would be parameterized on a lifetime?) That would sorta make sense, it would mean that any value of that type is something that would be valid for the lifetime of the program. Is this roughly right? I'm trying to figure out why something like `Any` would care if its type was `'static` or not. I guess it's because Foo&lt;'a&gt; and Foo&lt;'b&gt; are different logical types, but presumably don't monomorphize, and thus don't get their own `TypeId`? Is this on the right track?
This has been a concern for me as well. I wrote Serialize impls for enums that assume bincode, but at some point I want that library to be serialization protocol agnostic, and it's not immediately clear to me from the traits which methods I am required to implement.
As far as I am aware there is not better way to do this at this time. However, it would be possible to create a safe wrapper that chains two commands, so that you do not need the entire code to be `unsafe`.
smaller size and faster execution, less memory footprint, easier to maintain for one guy ;). And Wayland will replace X11 in 5-6 years on most Linux distributions anyway, so, my project is a longterm effort, so, why not to grab the new stuff, right. It's not commercial, so, I don't care that I will have not so many customers. I just don't want to use X11 anymore. It's a mess. Some things are not solvable with X11, e.g. screensavers and controlling music are problem for X11 and I am doing something similar to that.
&gt; Haskell makes me feel like the biggest idiot in the world. I'd venture to say that it's in large part due to most materials being really bad. If you're interested in learning Haskell I'd really recommend /u/mob_of_one's book, [Haskell Programming from first principles](http://haskellbook.com/).
&gt; So maybe the rule is that a 'static type is a type that transitively contains no non-'static references? This is exactly correct. `T: 'a` means both that values of `T` can be kept alive at least as long as the lifetime `'a`, and that means they can't contain any references with shorter lifetimes. Therefore a `'static` bound means the type contains no non-`'static` references at all. You're also right about `Any`. It refuses to deal with lifetimes because all lifetimes are erased before codegen.
Which of these functions are actually unsafe?
Once numeric generics are added, the standard library will have `impl&lt;T, N&gt; Trait for [T; N]` instead of its current long list of `impl&lt;T&gt; Trait for [T; up to 32]; ...`.
The problem for the first bullet was that I didn't have "where Self : Sized" after declaring my deserialize_with function in the trait . For the second I scrolled down on the repository. :)
`Stdio::from_raw_fd`
There are different types of memory, you might know about stack, heap, but static is different, it means its being hard coded in your binary, it should be known at compile time (be fixed/frozen), you can use the `const` keyword if you only need literals in your struct: struct Foo&lt;'a&gt; { x: &amp;'a u32 } fn no_work&lt;T: 'static&gt;(_: T) {} const foo: Foo&lt;'static&gt; = Foo { x: &amp;0 }; fn main() { no_work(foo); } This is the most normal thing to do if you want a static, it can be global, it doesn't really matter since it will never change. Otherwise you can do this if you want scoped usage: static foo: Foo&lt;'static&gt; = Foo { x: &amp;0 }; no_work(&amp;foo); String literals `&amp;str` are static because they're a reference to where the string is stored in static storage. Using `static` instead of `let` hard codes this in static memory and the static lifetime ensures you it respects that, another nifty thing is to make your static mutable with `static mut`, except, this is going to require unsafe.
Interesting! The downside with this approach is that one cannot do `#[path = concat!(env!("OUT_DIR"), "/generated_foo.rs")]` because macros cannot be substituted in a literal at the moment. I filed this [rust-lang/rfc issue](https://github.com/rust-lang/rfcs/issues/1516) to see if we can add that functionality.
:) By the way, you don't have to use a trait if you don't want to. It could save some effort with generics, but functions work fine too. Then you wouldn't need to add that `Sized` constraint :)
I don't really understand this... &gt; other than the fact that both languages have relatively stronger type systems than most mainstream languages, neither Rust nor Haskell have very much in common On what basis would you qualify two languages as being similar or having much in common? Static type safety, algebraic datatypes, generics with type classes, first-class functions, absence of nulls, and type inference are things which relatively few of today's languages have. Haskell and Rust both have them, and practitioners of both tend to value them highly. The two languages are of course targetted at different domains, and it makes perfect sense to me that someone who already likes one of them might look to the other when faced with a task it would be better suited for.
Why, when in `match` variable `end` is set to `true` then out of the match it stays `false`? Here's my code: extern crate sdl2; fn main() { // initialize SDL2 // SDL STUFF OMMITED M8 let mut end = false; // make new window // SDL OMMITED loop { // display the window window.show(); for event in event_pump.poll_iter() { use sdl2::event::Event; match event { Event::KeyDown {..} =&gt; {println!("Hi!");}, Event::Quit {..} =&gt; {let end = true;}, // &lt;-- why this no change end to true? _ =&gt; {} } } if end { println!("end"); // this no happenino even when i do an Quit event. break; } } } #SOLUTION: ^^^^because ^^^^someone ^^^^is ^^^^blind ^^^^prob EDIT: Solved, I should have not used let in `Event::Quit {..} =&gt; {let end = true;},` I made it `Event::Quit {..} =&gt; {end = true;},` and it works now. 
Hmm, yeah, I'm seeing a large performance regression from 0.2 -&gt; 0.4 as well. Not sure what caused it, but I'm taking a look right now. Edit: Well, the only performance regression I saw was actually one introduced when refactoring join not to use `unsafe`. I fixed about a week ago on a local branch but never pushed. I'll run the microbenchmark you're suggesting as well. Edit 2: I can't say I'm seeing the same results. Cargo.toml: [package] name = "simple_pool_bench" version = "0.1.0" authors = [""] [dependencies] jobsteal = "0.4" scoped_threadpool = "0.1" src/main.rs: #![feature(test)] extern crate jobsteal; extern crate scoped_threadpool; extern crate test; #[bench] fn jobsteal_simple(b: &amp;mut test::Bencher) { let mut pool = jobsteal::make_pool(4).unwrap(); b.iter(|| { let mut vec = vec![0, 1, 2, 3, 4, 5, 6, 7]; pool.scope(|scope| { for e in &amp;mut vec { scope.submit(move || *e += 1); } }); }); } #[bench] fn scoped_threadpool_simple(b: &amp;mut test::Bencher) { let mut pool = scoped_threadpool::Pool::new(4); b.iter(|| { let mut vec = vec![0, 1, 2, 3, 4, 5, 6, 7]; pool.scoped(|scoped| { for e in &amp;mut vec { scoped.execute(move || *e += 1); } }); }); } output of `cargo bench`: Running target/release/simple_pool_bench-fd07647b8d68836e running 2 tests test jobsteal_simple ... bench: 3,084 ns/iter (+/- 3,002) test scoped_threadpool_simple ... bench: 47,393 ns/iter (+/- 9,387) 
Hi, maintainer of wayland-client here! This crate is a bit more than a wrapper around the c library (it's wayland-sys that is): it includes the code generation for the core protocol, and soon for various protocol extensions as well. It's basically the rust version of the huge .h files generated by wayland-scanner (for people friendly of wayland), with one major change: it replaces the callback-based event handling of the C code by an iterator-based interface, which is much more rust-friendly. :) But indeed it's still a very thin shim around the libs and protocol, nothing near a full-featured windowing toolkit.
Yeah, it does work. Check my edit above! They could also be written to a tmp directory or something that is ignored entirely.
I think it'd be really great to do it in a way that wasn't just a one-time survey. Something that can keep up-to-date stats about the demand for various unstable features and what is causing the most pain. Is there anything in crater that does something like this?
They usually do, but I don't take them, so I'm not sure how they get there.
Ah, right. Arrayvec basically does the same thing, and it has a lot more array sizes covered. [It's still limited to an arbitrary list of sizes](http://bluss.github.io/arrayvec/doc/arrayvec/trait.Array.html), but at least 48 is one of them so I guess it covers your use-case.
Hot damn you're right https://crates.io/crates/serde_json you just made my weekend. I have no holdups about serde now!
your unsafe block doesn't reach far enough :P
Oh wow, I just realized the "ser" and "de" in the name! Is the name then from "serialization" and "deserialization" and not a French word?!
It's not enough to list off shared features (especially since, of those features that you listed, Rust and Haskell offer different kinds of type safety, Haskell offers GADTs where Rust doesn't, Rust requires monomorphic generics where Haskell doesn't, first-class functions in Haskell favor currying and partial application whereas closures in Rust are relatively clunky, and Haskell performs whole-program type inference while Rust only performs function-local type inference). By that same token we could consider Ruby to be roughly equivalent to Java, given that both are garbage-collected VM-based Smalltalk-inspired everything-is-an-object languages. But in practice the act of writing Ruby emphasizes dynamism, metaprogramming, and a culture that celebrates brevity and programmer ergonomics, while the practice of writing Java is typified by class hierarchies, design patterns, and a culture of conservative maintainability and an aversion to magic. This matters because the people who consciously choose to write in these languages (as opposed to those forced to write in these languages for their jobs) will usually have made their choice based on how the prevailing use of the language conforms to their personal preference and intended use case. Likewise, the act of writing Haskell is unavoidably concerned with managing purity and the prevailing culture favors extreme genericity and a distinctly mathematical sympathy. Meanwhile, I don't know if I can argue that the act of pleasing Rust's borrow checker is really comparable to designing around purity in Haskell, and the Rust culture tends to repeatedly emphasize zero-cost abstractions at the expense of expressiveness. So given that, like Rust, Haskell is very rarely a language that is foisted upon one unwillingly, it is reasonable to assume that people who are proficient in Haskell *enjoy* its tremendous expressiveness and extreme abstractive capabilities, and have made peace with Haskell's resistance to allowing one to reason about the time and space complexity of their programs. Of course, it's reasonable to expect that a Haskell programmer who's looking to expand their experience into the systems programming domain would be interested in Rust and may be willing to trade off a great deal of expressiveness in order to operate at the systems level. However, that's not a prospect that's unique or tantalizing to Haskell programmers in particular; *any* programmer who's proficient in a high-level language (in the modern sense) is likely to prefer a systems language that enforces the same sort of memory safety as the languages that they're accustomed to, and the modern features of Rust (relative to C) are just icing on the cake. Of course this is all just my opinion, YMMV, etc. :)
Actually, I was more intending to contrast "pure Rust" and "any C, regardless of how little or how much" (hence my mention of `xlib-python`). That aside, I *am* very glad to hear that. Coming from Python and having used things like wxPython, I'm *well* aware of how important it is for a wrapper to adapt the API to the language conventions.
While dismantling this gentleman's defense of my question, you have managed to give me one of the best summaries of the two ecosystems and languages that I've found. I'm very grateful.
I wouldn't exactly characterize it as dismantling, as I suspect that glabhoerl is easily better at both Rust and Haskell than I am. :P My expertise is just in guiding people who are learning the language, so I have a decent outlook on how it is perceived by people coming from a variety of language backgrounds (as well as the misconceptions that they often have, which is why I might seem particularly spirited regarding this topic). EDIT: As a final word, a TL;DR of my opinions here would be "there are certainly similarities between Haskell and Rust, but there are more potent similarities between many other languages and Rust (especially C++ and ML), which is something to keep in mind when framing an explicit Rust/Haskell comparison."
Rust binaries are self-contained, which means that you don't need to install a rust runtime. The build process should just be `cargo build--release` and installing simply consists in copying the resulting binary to /usr/bin.
Could you point me to any examples of bincode-specific implementations? I'd be curious to see how they differ from generic serde implementations.
`Packet`, defined [here](https://github.com/google/tarpc/blob/master/tarpc/src/protocol/packet.rs), passes the test in that module that uses bincode, but fails this test: #[test] fn serde_json() { use serde_json; let packet = Packet { rpc_id: 1, message: (), }; let ser = serde_json::to_string(&amp;packet).unwrap(); let de = serde_json::from_str(&amp;ser).unwrap(); assert_eq!(packet, de); } Output: running 2 tests test protocol::packet::bincode ... ok test protocol::packet::serde_json ... FAILED failures: ---- protocol::packet::serde_json stdout ---- thread 'protocol::packet::serde_json' panicked at 'called `Result::unwrap()` on an `Err` value: Syntax(invalid type: Map, 1, 1)', ../src/libcore/result.rs:746 note: Run with `RUST_BACKTRACE=1` for a backtrace. 
Well, until we get stable compiler plugins done, that will be the case, right? My current guess is mid-2017 at the earliest. There's so many things to work through: the Mid Intermediate Representation (MIR) refactor, stabilizing an Rust Application Binary Interface (ABI), figuring out how panic unwinding works across Rust processes, building infrastructure for compiler plugins, and stabilizing all of these. It's going to be a fairly long journey, but the destination will be worth it. Edit: now that I think about it, if we were willing to use C style FFI for compiler plugins, it could get done faster. I'm not sure how we would like the end product, though.
This has me wondering. If I use a simple binding for the `for` loop, I still get an error with: fn dub(vex :&amp;Vec&lt;(&amp;'static str, &amp;'static str)&gt;) { for p in vex { let (i, v) = p; println!("{:?} =&gt; {:?}", i, v); } } But I don't get an error with either: fn dub(vex :&amp;Vec&lt;(&amp;'static str, &amp;'static str)&gt;) { for p in vex { let &amp;(i, v) = p; println!("{:?} =&gt; {:?}", i, v); } } or fn dub(vex :&amp;Vec&lt;(&amp;'static str, &amp;'static str)&gt;) { for p in vex { let (i, v) = (p.0, p.1); println!("{:?} =&gt; {:?}", i, v); } } I'm guessing that this has to do with automatic dereferencing of references. I'm wondering: why does this limitation exist? It is intentional (say for performance), or something that will be automatic in the future?
I think it would be fair to say that Rust is _more_ like Haskell than it is like most other well-known languages - and even more fair to say the reverse of that. That doesn't mean they aren't decidedly distinct, just that Rust is more alike to Haskell in my opinion than either is to, say, Python. I think this is consistent with, perhaps corollary to, your tl;dr in a niece comment of this one.
WRT footnote 1: I had my share of ephiphanies when reading the Wizard Book and dabbling with Scheme. Then I had ANOTHER bunch of them while learning the basics of Haskell. The rabbit hole of goes deep and delving there is not without its side-effects!
It was around 2010 when I was playing with Haskell, so not unlikely. I think there was just one http library and it was a pretty old version provided by Debian.
tl;dr: The core essence of `async/await` is an automated compiler transformation of a function to a state machine. &gt; I suppose that an async fn may be called from a regular function, because otherwise by transitivity from main none could be ever called. I also suppose that async fn may call regular functions, because otherwise the set of operations they could perform would be quite limited. In C#, an `async` function can be called from anywhere; the fact that a function is `async` is not visible to the caller! The caller just sees a normal function that returns a `Task`, `Task&lt;T&gt;` or other awaitable^1 type. If the caller doesn't use `await`, the caller just gets that `Task` object immediately, and at that point it behaves like a `future` from many other languages; if you had it in `main` you would call `asyncfunction().Wait()` which would then block the main thread until the task is complete. If the caller uses `await`, the caller is itself transformed. A function async Task&lt;int&gt; foo() { return await a() + await b(); } becomes something akin to Task&lt;int&gt; foo() { return new FooStateMachine(); } where `FooStateMachine` contains logic to store local variables and intermediate values into itself (instead of the program stack) and to query the nested `await`ed expressions for when they are complete. &gt; an `async fn` would require run-time support Nope! In fact it would require no run-time support whatsoever. &gt; &gt; The only thing I don't like about the async/await approach is that they polute the code base making it hard to reuse code between stackless and stackfull coroutines as well as threads. &gt; makes me think that a lot more is at stake and that I missing something. `async/await` means stackless coroutines: instead of storing local variables on the actual program execution stack, they are stored (after a program transformation) into state machine objects. Stackful coroutines, on the other hand, allocate an entire separate stack per coroutine for the CPU or VM to use as a call stack, and resuming a stackful coroutine involves a somewhat expensive [context switch](https://en.wikipedia.org/wiki/Context_switch). ^1 see [Async/Await FAQ](http://blogs.msdn.com/b/pfxteam/archive/2012/04/12/10293335.aspx) 
To be accurate, `p`'s type is `(&amp;'static str, &amp;'static str)`. The value in `p` *is* owned, but it's not owned by the *loop*; it's owned by the variable binding `p`... which is inside the loop, but you could say the same of `i` and `v`. This works because `(&amp;str, &amp;str)` implements `Copy`, meaning it can be implicitly copied around. When you destructure the element, you copy the value behind the reference and bind the result to `p`.
&gt;... if you *really* want speed without compromising on readability then pre-compile Polly into pure Rust and then let Rust compile that. This is what Twig and other not-logicless languages already do. Shameless plug: [Maud](https://github.com/lfairy/maud) does exactly that.
That would be great. I think we need more than 2 major branches (Linux, BSD) of a "free-and/or-opensource" OS branches that could be used in daily life on server or as a desktop/tablet or even phone ;). And maybe it's time to move on from C/C++ to something more "recent". The creator of Redox is very active. There are changes on Github all the time. He has created a GTK like toolkit OrbTK https://github.com/redox-os/orbtk and a lot of stuff for Redox. But he needs more people who will help him and the few other programmers working on this project with the code. I am not a very good programmer so I cannot help much, but maybe this post will bring some other programmers who could help
Is there an iso somewhere to try it out? I'm feeling too lazy today to compile it myself.
I'd love to see some ideas from Andrew Zonenberg's Antikernel work incorporated, it just looks like a really good fit for Rust. 
&gt;I would like to start a discussion on Redox vs Linux in 10 years. Redox will stay a niche OS, given its license. Nothing to discuss there. Generally, same goes for any other OS that won't have the right license. Shame that people don't seem to be able to learn from the mistakes in the history.
It's MIT, is it not? How is that the 'wrong' license?
Good to see this written down. Lastly I followed [the "Need help with emscripten port" thread](https://internals.rust-lang.org/t/need-help-with-emscripten-port/3154/) and was able to compile one of my crates (with a few minor hacks and manual editing). More changes are about to land in Rust and emscripten (some changes in Rust are blocked by the emscripten part), so soon we should be able to easily get it going without too much of manual editing.
Thanks for reminding this has to be put in the changelog, which still is incomplete. To reiterate [the commit message](https://github.com/gkoz/gtk-rs-gtk/commit/38bba79be12c62270c48e1bdf6a80b60c08e9896), you still have access to the low-level structs via `as_ref` (see the [examples fix](https://github.com/gkoz/gtk-rs-examples/commit/a75c2a4c0dd5f3e71a1dd233986da3f9bf19b367)). PRs that add safe getters and setters would be appreciated.
Yes I see that there are more changes to be landing soon, and that is where I got some of the manual file changes from. Frankly I'm surprised that there is so little to be done manually right now to get it working.
Care to elaborate? What's wrong with the current license, what do you think it should have picked instead? 
I do agree with much of this. Culture matters. If I wanted to pick the language most similar to Rust, I wouldn't choose Haskell, and if I wanted to identify the language most similar to Haskell, I wouldn't land on Rust. That's basic sense. But of course languages can be similar without having to be *exactly the same*. And the context for the question -- the other prevailing languages (maybe the "evolutionary environment") -- matters too. A feature which two languages have in common, and which most other popular languages do *not* share, carries a lot more weight. And I feel like languages which have the features I identified in my previous comment (modulo the choice of ML modules vs. type classes) are their own little family in the evolutionary tree. Within that family, Haskell and Rust are kind of far apart, as you laid out very well. But they are both *in it*, and most other languages aren't. I also *would* say that Rust is the systems programming language that's *most* similar to Haskell. (Out of the ones which have any semblance of being mature, popular, and well-supported, at least.) Which again justifies a Haskell programmer looking to Rust if she wants to do some systems programming, although the same thing isn't necessarily true in the reverse direction. (Haskell isn't *the* higher-level language that's most similar to Rust, just one candidate out of many.)
I think "Everything is a URL" is a nice idea. Why? "Everything is a file" is nice too, but it's kind of limiting: files are expected to be laid out in a categorical hierarchy ‚Äì the file system has a strict tree structure that is expected to be traversed in a certain way. (At least normally, let's not get into hard links) For some use cases, this is nice, but for some, it's not. But URL is more diverse, with a particular scheme you can have a tree-like hierarchy, and with some other scheme some other kind of "addressing method" for system resources that fits them better.
It's not a copyleft license, nor it protects from issues created by some of current patent systems. Note that there are good reasons why Linux is licensed under GPL. Now, if you ask about my *opinion* what it should use, the obvious answer is GPLv3+ :P And if you're looking for a word of wisdom from the past, there you go: http://www.onlamp.com/pub/a/bsd/2006/09/14/netbsd_future.html Relevant part: &gt;If I were doing it again, I might very well switch to the LGPL. I'll just note that it didn't exist at the time.
&gt; It's not a copyleft license Care to elaborate what's wrong with this, especially in the OS space? This doesn't elaborate on "Redox will stay a niche OS, given its license" at all. &gt; a word of wisdom from the past, I dunno, as that page states the issue with non-copyleft is that with a non-copyleft license companies aren't incentivised to contribute back. However, now, there's a growing trend of companies using and contributing to open source software; so non-copyleft might just work.
Heey. [We're](https://github.com/redox-os/redox/graphs/contributors) many people working on Redox, 40+ developers.
That's actually not quite how I develop in Nix (but you can do it that way). I have a sort of [global set of packages](https://github.com/mjhoy/dotfiles/blob/a4947aea94751ba3bce3b5087ba7668ee3e5eacd/nix/nixpkgs/config.nix#L88) that I'd like to use for projects, which Nix installs, and I don't jump into the nix shell often. I just use cabal, which can only work with the packages that Nix has installed. (And nix pulls from a set of packages proven to compile with each other, much like stack; in addition to this it pulls down /binaries/ so I don't need to compile anything.) Nix also builds a hoogle database for all of those packages, this is very nice and it's what I want rather than a per-project hoogle database, because I'm often looking for a function defined in a package I haven't pulled in to my cabal file yet. When I need to install a project I'm working on, at that point I generate a nix expression and build it in isolation.
&gt;Care to elaborate what's wrong with this, especially in the OS space? This doesn't elaborate on "Redox will stay a niche OS, given its license" at all. And then you basically go into answering your own question with &gt; (‚Ä¶) non-copyleft might just work. **`might`** is a good word ‚Äì precisely expresses low chances of success. What you have with copyleft, is "feel free to contribute back", and as bonus you have a way to get back stuff. Without copyleft, you're left only with "feel free to contribute back", and no real way of getting *anything* at all back. So the real question is whether you feel like OS development and success can be sustained by your own work without getting anything back, or not. There's also this thing, where people always assume overly optimistic scenarios - that's a trait of how minds works, there's nothing that could be done about it (yes, there is research on that, feel free to google it). So when your overly optimistic scenario is that at best it "might" work, it's most likely simply going to fail. &gt;However, now, there's a growing trend of companies using and contributing to open source software (‚Ä¶) The real question is, "where are money leading". Feel free to ponder on that. Also, not sure if you're seeing it, but there's a lot of companies (their number also increases) that contribute to libre software projects.
&gt; Shame that people don't seem to be able to learn from the mistakes in the history. Sorry to say, but restrictive licenses are quite a big disadvantage for OS projects. MIT opens up a lot of possibilities, which are simply not plausible with, say, GPL.
I'll add one this week.
&gt; I disagree with you on this one. Rust has enormous advantages, because for operating systems safety matters. A lot, actually. How much `unsafe` code is in Redox?
Well, the downside of wayland is that its integration with mesa-egl is pretty tight. Too tight actually : you must use the C libraries if you want opengl support. That's why I gave up on a pure rust implementation of these libs, which are actually mostly serialization wrappers around the compositor socket.
Well, `unsafe` is inevitable, but lately we have trimmed the kernel from `unsafe`s. The kernel has 16.52% unsafe code, a 50% improvement in the last three weeks. Userspace has roughly 0.3%.
No need to say sorry. You asked a legitimate question!
[removed]
I don't think anything will happen with it. Breaking into general users marking share should be seen as impossible at worst, exceedingly difficult at best, especially without ecosystem around it to drive adoption (selling DoxBooks that can only run Redox, a la OSX) and any end user usability gains. People don't care about what language it's in or what new technologies it uses: just look at Plan 9, Inferno, Oberon, and the literal dozens of other OSes that should have been the future. The fact that it's not POSIX compatible is also a kiss of death. You cannot run applications built for BSD or Linux on it without porting, which creates incredible friction for switching.
We discussed this on our internal chat. @stratact put this pretty well: &gt; the only thing GPL is good for is forcing people who make changes to the source to contribute back. This would be okay if you developing an app, but never ideal for libraries, because GPL forces any code that even remotely uses or links to the GPL source, to become GPL'd So in other words, GPL is upstream-centric, MIT is downstream-centric. We happen to prioritize downstream more than upstream, since downstream is what really matters: the userbase, the community, the availability.
i think url is more general than URI, as URI are URL with file:// predicate, but i may be wrong
Thank you for links and answers! May I suggest adding those links to the site's main page. From newcomers perspective it's not obvious that the wiki even exists. This issue https://github.com/redox-os/redox/issues/361 resonated with me heavily. After reading through the wiki and other sources I still find myself confused about design of Redox. I can't stress enough the importance of oss projects having at least high level design documents, especially for such complicated project as OS. Imagine a possible early contributor stumbling upon Redox repository. He'd wanna know a lot before reading source code, like: what is the idea behind the project? is this a serious project or just another hobby OS? And he'd like to figure this out not by reading generic statements like "this is serious OS", but by reading design documents and evaluating them for himself. As for now it's very hard to understand what is Redox about without reading source code. I think a great sources of inspiration for such documents are Joe Duffy's posts about Midori (http://joeduffyblog.com/2015/12/19/safe-native-code/) and Rust's RFCs. I hope I don't sound too harsh. My goal here is to suggest a way how initial experience with Redox can be improved. &gt;&gt;how does it solve underlying problems of that use cases. &gt;Could you expand on this question? Sure, lets take for example a network stack and as a use case "platform for HPC applications". We'd like to deploy a high throughput application, for example some nosql database, and we'd like to be able to saturate 40gbit network (given that we are network bound). Default Linux network api will fail to deliver required performance, because of high cost of syscalls and buffer copying. The solution would be to use userspace network libraries like DPDK. ScyllaDB would an example of this kind of an application (http://www.scylladb.com/) While there are some mentions of user-space networking at wiki, there is no substantial enough description to make even preliminary conclusions about viability of Redox for this kind of usecase. &gt;I disagree with you on this one. Rust has enormous advantages, because for operating systems safety matters. A lot, actually. Without doubt safety matters, but I would argue that first: it's not enough to make it viable for end user. New OS have to deliver significant improvements in either performance or usability to justify costs of switching from old one. And second: I would like to point out this post as example (http://dictator.redox-os.org/index.php?controller=post&amp;action=view&amp;id_post=17). At some point it was possible in Redox for a process to access kernel or other process memory, which proves a rather simple statement that implementation matters. So if hypothetically in future Redox'es slogan would be "the most secure os", then it would not be enough to say that it's safe because it's written in Rust, you'll also have to explain your design (process isolation, sandboxing etc) My point is not that Rust doesn't provide any benefits for OS development, quite the contrary, I think Rust is awesome for this task, but it's not enough to be "just written in Rust" to become either safe or viable. 
This is a very good critique, much appreciated. Would you mind putting in a github issue, or however many you want, so we can track our completion of the design documents?
I think you're misunderstanding things here. The problem with Linux is that these legacy drivers are mandatory parts of the kernel. In Redox, drivers will run in userspace. In other words: the Linux kernel is bloated with long-outdated drivers. Redox will not be like this, instead the drivers run, like other programs, in userspace.
I just want to remind everyone: if you can think of anything you want to see from redox, add a github issue!
&gt; Use Rust instead of C. Use Haskell instead of Java. Use Scheme instead of Python. Summarizing one of the commenters over there.
[removed]
Keep it civil.
&gt; Actually, in 10 years we will be living in a pretty censored/monitored/hacking-(cracking)all-the-time world, It seems pretty unbased to think so.
Although the sibling post is blatantly out of line, I have the same view: those accusations are completely unbased and sensationalist. Even if what you say is true, then Redox would only be used for security critical systems instead of general use, but Redox has only Rust's benefits for that. It's not formally verified, it's not capability based, it's not inheritally hardened like seL4 or Mirage are. In fact, being a security critical kernel is pretty orthogonal to being a generic consumer OS since it is runtime expensive and hard for consumers to use with no benefits to the everyday user. I'd also advise against change for the sake of change. Linux does "old stuff" for a reason, and probably has a reason for not switching it out for untested "new stuff" 
Well, http://tools.ietf.org/html/rfc3986 &gt; A URI can be further classified as a locator, a name, or both. The term ‚ÄúUniform Resource Locator‚Äù **(URL) refers to the subset of URIs** that, in addition to identifying a resource, provide a means of locating the resource by describing its primary access mechanism (e.g., its network ‚Äúlocation‚Äù). Emphasis mine. But I get this confused like everyone else does :)
What do you mean by Mac-ish?
free functions and functions in traits are practically interchangeable, with Universal Function Call Syntax. If you have: struct Foo; struct Bar; and you want to write a function that does something with a Foo and a Bar, you could have written it as a free function: fn frob(foo: &amp;Foo, foo: &amp;Bar) { println!("frobing foos and bars"); } or a method on Foo: impl Foo { fn frob(&amp;self, _: &amp;Bar) { println!("please foo, frob me this bar"); } } depending if you want to call it as `frob(&amp;foo, &amp;bar)` or `foo.frob(&amp;bar)`. Rust is not very opinionated and lets you choose whatever you like best; even better, the second form is even expressible as the first. Given the impl above, these two calls are equivalent: // called as a free function Foo::frob(&amp;foo, &amp;bar); // method style foo.frob(&amp;bar); and this is not only syntax sugar. If you have a function that takes a function pointer `fn(&amp;foo, &amp;bar)`, then `Foo::frob` will satisfy the required signature. 
OP is not a Redox developer. We do not think that we will replace Linux in the future. That'd be pretty crazy.
Gotcha. Thanks :)
&gt; This renders Redox (as a rule of thumb) non-POSIX. Any ideas or inspiration from Plan 9?
Many concepts of Redox comes from Plan9. For example, everything is an URL. When I say non-POSIX, I mean in the strictest sense. We support many of the Linux syscalls, making applications insanely easy to port. Examples of ported programs and libraries are: SDL, FreeCiv, ed, dosbox, Lua, zlib and many more.
We don't intend to break userspace (that is, applications which relies on stable API). We will just be more wary at stabilizing things. So the release model will look something like Rust's.
This is not correct, I'm afraid. URIs are _either_ URLs _or_ URNs; sum, not product. /u/steveklabnik is right. URNs are a rarely used scheme for essentially UUIDs which contain some semantic info about the resource under identification.
I've done a few utilities with `optparse-applicative`, etc. My impression was that, if you make abstraction of the advanced features, Haskell is a very unpleasant/un-ergonomic language. I'd use F# or Rust over Haskell any day, except in places where Haskell's additional features make a significant difference. But I'm not smart enough to make the best of these features.
My kingdom for an LLVM Xtensa backend
I'm too new to know the idioms, but how about: vec.extend(std::iter::repeat(val).take(k)); ?
You're right.
Looks cool! I've got a few short questions though if you don't mind ;) It seems to me as if the order in which transformations are applied is disregarded; is that intentional (i.e. is it the same in SVG)? I also wondered why `Fig` has the variants `Multiple(Vec&lt;Fig&gt;)` and `Shared&lt;Rc&lt;Fig&gt;)`, why are these necessary, and why isn't just having e.g. a `Vec&lt;Fig&gt;` enough? Since I'm already asking, why does e.g. `Styled(Attr, Box&lt;Fig&gt;)` keep the `Fig` inside a Box, i.e. why isn't it just `Styled(Attr, Fig)`? Thanks!
Sure. This is just so that I could generate some simple illustrations from some rust data structures that I already have. There's a lot about SVG I don't know or missed, and this crate with simple in its name, will never support it all. I didn't think about transformation order, so that's something that can be fixed actually. I guess it is better represented as a sequence (Vec) of transformations (An enum). I have open questions about the `Fig` enum too! But first why the `Box&lt;Fig&gt;` is needed; we can't store a `Fig` inline in itself, the type would be infinitely large. So we need an indirection like `Box, Rc, Vec`! `Multiple` is needed so that we can put many figure children inside the same style or transform. `Shared` is needed, so that it is possible to use a reference counted figure part multiple times in the same figure (as demonstrated in the koch snowflake test, which duplicates itself into 4 parts transforms them; using Rc, it uses less than 1/3rd of the memory). This representation is however not ideal. What if there was a nice way to have an "either Box or Rc" child?
Awesome! However, I'm confused of how much I need to do. Why isn't this easier? Why can't the official Rust compiler use the target `asmjs-unknown-emscripten`? Or lets say: why can't I just tell the compiler to spit out LLVM IR and tell emscripten to compile that into JS? I'd appreciate it, if someone could tell me why this is so difficult :)
The mechanism already exists, but maybe there could be an official Crate Orphanage. If you want to stop maintaining a crate but you don't have another maintainer lined up, you can donate it to the Orphanage where (ideally) some caretakers will at least patch it into building on later versions of Rust, and maybe somebody will even adopt it.
It's so nice to have a community where bad behavior is shunned. Nice change of pace from some other parts of the Internet.
Ok, I'll just do the change kibwen suggested then
We already have several domain-specific organizations which share the maintenance burden of their crates, the two I can think of right now being [contain-rs](https://github.com/contain-rs/) (collections and generic containers with various algorithms) and [PistonDevelopers](https://github.com/PistonDevelopers/) (anything related to gamedev). I'm sure there's several others that aren't coming to mind right now. It's incredibly easy to start one and invite people to it, someone just has to be the one to take those first steps. Projects get started and abandoned all the time, but overall I think the community is still growing. I bet many people are just waiting for certain language features to be implemented before they start the projects they really want to work on: specialization (which will hopefully land soon), higher-kinded types, ~~unsized~~/anonymous/abstract/whatever return types (biggest want is to return closures from functions without having to box them), integer generics (mainly to make working with arrays more convenient), the list goes on and on. Some of these features have people championing them, others have had very little visible progress. None of them seem to have any sort of concrete timeline. They're always blocked on something else which may or may not be implemented soon--MIR seems to be the common denominator for most of them. 
You might enjoy /r/Subredditsimulator then. 
One of the problems with this idea is that the reason crates go unmaintained because there's no time. So like, said authors aren't gonna (in many cases) proactively donate their crates, because that's the exact kind of maintenance they're trying to avoid in the first place!
Something that would help differentiate redox from other OSes is a formal argument for correctness. You probably don't need to go as far as SeL4, but some sort of assurance story beyond rust types and memory safety would help. Perhaps model checking would be appropriate.
And the accompanying LADSPA plugins for all of your audio effect needs: https://github.com/nwoeanhinnogaehr/pvoc-plugins
&gt; But first why the Box&lt;Fig&gt; is needed; we can't store a Fig inline in itself, the type would be infinitely large. So we need an indirection like Box, Rc, Vec! Ah right, of course, I forgot about that for a moment.. thanks! &gt; Multiple is needed so that we can put many figure children inside the same style or transform. Shared is needed, so that it is possible to use a reference counted figure part multiple times in the same figure (as demonstrated in the koch snowflake test, which duplicates itself into 4 parts transforms them; using Rc, it uses less than 1/3rd of the memory). &gt; This representation is however not ideal. What if there was a nice way to have an "either Box or Rc" child? I see, that makes sense. Thanks for the explanations!
It's because rustc and emscripten don't yet share the same LLVM backend. Both rustc and emscripten use LLVM as a backend. rustc compiles emscripten by passing emcc, the emscripten compiler, LLVM bitcode, which emcc then converts to js. LLVM bitcode is not stable, so rustc and emcc need to be on the exact same LLVM commit. This is complicated by emscripten's large out-of-tree patch to LLVM. Eventually the two projects will share LLVM but for now to use emscripten one needs to replace LLVM in rustc.
Thanks for the comment. I did think of this when writing my guide, but I figured if the line numbers change in future iterations, then the changes to apply will also likely change or will be not needed. But what I should do is state that at the top that the guide is valid for rust nightly as at the 26th of Feb 2016, and may be subject to change.
In the Arch Linux community, it is possible to "flag" user maintained packages as unmaintained/orphaned and trusted users can review this status and attempt to contact the owner. This has become an extraordinarily useful feature because then trusted users can transfer ownership of the crate, and there's no reliance on the previous maintainer to do anything. Being able to see at a glance whether a crate has a real owner or not is a small addition to crates.io and I think it's worth it :)
Thanks for your reply /u/tomaka17, and for your list of suggestions. &gt;The actual list of changes to make to Rust is visible here. I think that is the diff that I was referencing when I was playing with things last week trying to get a working build, and my changes are cherry picked from that diff. Note that the changes to rustc_llvm/lib.rs are no longer valid because large sections of that file changed recently. The code in my guide uses the new init_target! macro to initialize the JSBackend component. &gt;I suggest that the version of emscripten you clone should also get a small change as well, to support unwinding. Thank you for this. I see that it will be important, I will add it to my article. &gt;The rustc standard library supports emscripten, so you can just do --target=asmjs-unknown-emscripten I tried that, couldn't get it to work, lots of errors in `libstd/os/linux/fs.rs` maybe some things changed in master recently to prevent it from working. &gt;In theory --llvm-root should work, but if it doesn't you can just make the src/llvm I was going to do it by checking out fastcomp into src/llvm as suggested, but the `--llvm-root` method worked for me, and I think it was easier (less steps) to put into my guide. &gt;You don't need to make install your newly-compiled rustc. If you use multirust, you can do multirust update my-emscripten-rust --link-local /path/to/rust/build/x86_64-unknown-linux-gnu/stage2 Oh nice! I was wondering if I could get multirust to use custom rustc version, but I didn't look into it just yet. &gt;Points 12 to 14 are not necessary if you passed --target=asmjs-unknown-emscripten Yep, if I could get libstd to build, these steps would be totally unnecessary. &gt;If you use multirust, go to your project's directory and do multirust override my-emscripten-rust I will look at adding the multirust integration to my guide, and I will change the build steps at the bottom to match your suggestion.
I‚Äôve filed https://github.com/rust-lang/rfcs/issues/1519 about adding something to the standard library.
Just a reminder: https://www.reddit.com/r/rust/comments/44u4mg/rust_contributorswanted_list/
&gt; Both Rust and Java keep their generics to compile time What does this mean exactly? Doesn't Java remove type information from generics and the actual method look up happens with a vtable during runtime while Rust's generics work more like C++ templates (code generation during compile time)?
It is, in the reference.
Sure thing. So if you'd help me, which order is the natural one to use? Should `f.rotate(10.).translate(20., 20.)` emit the SVG `rotate(10) translate(20, 20)` or `translate(20, 20) rotate(10)` ? Edit: I worked out that the only reasonable thing is `translate(20, 20) rotate(10)`.
I've been tooling around for a few days trying to learn Rust. I'm a kernel guy so everything I do is in C and I'm having a hard time wrapping my head around the best way to do IO. I need to read in a binary format but man it is super frustrating, to the point that I'm pretty sure I'm doing something braindead. So for example if I want to read in a single u32 from a file, I have to do something like this let mut file = try!(File::open("/dev/urandom")); let mut buf = [0; 4]; try!(file.read(buf)); let mut ret: u32 = 0; for b in 0..3 { ret |= (buf[b] as u32) &lt;&lt; (b * 8); } Is this seriously the best way to do this? Please make me look stupid and point out the obvious thing I'm missing. edit: fixed bitshift bugglet so it wasn't distracting from the real issue.
While Rust is not mentioned in this talk, it felt very relevant to Niko's recent work on [Rayon](http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/), [sequential iterators](http://smallcultfollowing.com/babysteps/blog/2016/02/19/parallel-iterators-part-1-foundations/), and [parallel iterators](http://smallcultfollowing.com/babysteps/blog/2016/02/25/parallel-iterators-part-2-producers/).
What's the difference (for the user) between running a driver in userspace and the way Linux does it (kernel space?)?
&gt; neither Rust nor Haskell have very much in common They may have lots of differences, but the experience in programming in them is sometimes similar; some programming patterns are analogous. But perhaps laziness (and not the esoteric type-level stuff) is the thing that sets apart Haskell from, say, Rust. (well Rust has high-order operations on iterators, but it's not the same thing as pervasive laziness) edit: some programs I write in Haskell I could write in Rust. But C++ wouldn't be a good replacement. This despite the fact that Rust and modern C++ programming cultures have more in common than Rust and Haskell.
&gt; these legacy drivers are mandatory parts of the kernel I may not understand Linux correctly, but isn't this untrue for most drivers? True, with most Linux *distributions* you are given a monolithic kernel with infinite legacy support, but that is the decision of the distro maintainers. If you use Gentoo or for some other reason build your kernel from source, you have the option to disable any drivers that you do not use (and therefore not have that bloat on your system).
Would a pre-pass that inserts explicit drops everywhere turn a language with affine types into a language with linear types? (in absence of unwinding).
Of course. I didn't mean to imply otherwise. Sorry if it sounded like that.
It is not different from how X11 works. When resizing a window, the X11 server gets a command from a socket stream. The point here is that resources can be socket streams too! So the design is similar, but Redox generalizes the notion of files.
&gt; llogiq finally reveals his true colors as a Corporate Java Shill! He's a mod of /r/java! We should have known!! :P
There is a difference between receiving a command over a socket stream and navigating to a different URL. If a window is a resource, it doesn't make sense to me that it becomes a different resource because I've resized or moved it. Is this how Redox's display protocol works now? When I last looked at it, which was a long time ago, the window's location was a part of its URL, not something you set by sending messages to the url.
Very nice writeup. Just as small addenda, although the OpenJDK only does JIT, there are quite a few commercial JVMs and Android (ART), that actually do AOT as well.
Steve, are these the same mruby bindings you mentioned on Twitter a while back, or was that a different project?
Neat, would this be something worth contributing to RustAudio? https://github.com/RustAudio
&gt; As it stands now, Java doesn‚Äôt monomorphize (at least not at compile time, though the JIT may synthesize specialized versions of hot code), while Rust does. **This leads to smaller binaries for Java** My experience is the opposite. We deploy a fatjar package (i.e. a single .jar with all dependencies), and it is huge (almost 100M). There are tools like Proguard to remove unused code, but it requires a lot of fine tuning for some of our dependencies. Recently we have been using Rust for one of our tools, and the size of the binary (with --release) is a fraction of the fatjar size.
Great to see this, this was exactly the reason I started [`kugel`](https://github.com/Nercury/kugel-rs/blob/master/examples/gl_triangle.rs) a while ago, but sadly had not enough time to progress beyond trivial examples. I hope to use your library in near future.
I didn't, but it is enough for people like Heroku and Firaxis to use it (Sid Meier's Starships). mruby isn't exactly young... Lua, on the other hand, has quite a lot of problems in my opinion, one of them are a pretty hard to graps C interface (I'm currently trying to work with Mozillas luasandbox and it is a pain).
I can't see how I am not. I think my comment is legitimate criticism. I was just asking for a source on his opinion, sorry for harassing him. Maybe asking him to use a spellchecker causes him mental harm and that was a bit much. &gt; 1) We are committed to providing a friendly, safe and welcoming environment for all I can't see how I am being actively hostile, I am neutral at most. &gt; 3) Please be kind and courteous. There's no need to be mean or rude. 4) Respect that people have differences of opinion. &gt; 5) Please keep unstructured critique to a minimum. I think it was pretty structured.
Probably very little as you are likely to run a distribution created by a third party. The distributor may make it easy to specify which drivers are to be used, but I expect that most people will just stick to defaults
I think that what he meant by throwing generic type information away is exactly that: generic types that does not use internally the anonymous class trick of TypeToken like `Pair&lt;T, T&gt;` and `Pair&lt;U, U&gt;` cannot be distinguished at run time. They can freely be cast between the one and the other without any run time check failing. Their size is also the same. In Rust this is a complete different story. `(T, T)` and `(U, U)` have clearly a different size when size(T) is not equal to size(U).
Until recently the prevailing view was if you want decent performance, but are scared of C or C++, then the only place to turn to is the JVM or Go. We now have another option: Rust.
First you out me as mod of /r/rust, now as mod of /r/java. To preempt any further outing, I hereby put on record that I'm also mod of /r/tonguetwisters. There you have it. :-P
Actually, I'm a huge fan of Glium and tomaka's other work! (And I'm pretty excited to see where [`vulkano`](https://github.com/tomaka/vulkano) is headed). Although early on when I started work on my game, and I was deciding between using an existing graphics library or rolling my own, Glium was failing to compile with an ICE when building for iOS (it was a stack overflow IIRC, which has since been fixed). I'd definitely be lying if I said that Glium hasn't influenced glitter, especially when it comes to program objects, vertex attributes, and uniforms. On top of that, there were 2 other things that drove me to building my own library. First, Glium distinctly diverges from the OpenGL API, going for a bindless design: I was hoping for a library that would more clearly map to OpenGL (where Glium is definitely a higher-level graphics API). Second, while poking around Glium's docs, I noticed that there were lots of uses of `Rc`. Now, I'm fully aware that the performance loss of reference counting is somewhere between trivial and non-existent, but I saw this as a challenge: could you write a safe and simple graphics library using only zero-sized types? And glitter is the result! But, like I said, I still really like Glium, and I definitely see myself using it in the future! (E: formatting)
Tongue twisters and a programming language shill, eh? Here's one for you: She shills C shells shorn of safe code.
I just updated the post.
I got invited to mod /r/java after starting a discussion about the relation of /r/java and /r/javahelp. I had started /r/tonguetwisters earlier after asking someone to post there as a joke and recognizing that it didn't exist. /u/kibwen invited me to mod for /r/rust when I started writing weekly 'what's everyone doing' posts, so I could pin the threads myself. So now everyone knows how I became a mod. :-)
I've been working on my Game Boy emulator again. Currently implementing the CPU instructions one by one. https://gitlab.wambo.at/GyrosOfWar/gameboy-rs
Been working on a template engine based on jinja2/django https://github.com/Keats/tera Missing inheritance/blocks template and any kind of error handling so not usable yet. I'm not planning to do an exact copy of jinja2/django so if you used them and liked/disliked some things, let me know. The current code is quite horrible so I could use some help! (don't look at the ==/!= handling in the rendering, *shudders*)
Not a beginner question but I wasn't sure where else to put this. Do you think we could start another weekly thread for matching up beginners looking for mentors with crate authors looking to mentor, or dual-purpose either this thread or the "what are you working on" thread?
Yea just this, the "everything is an URL" just sold me on Redox. :) Looking at things like IPFS and torrent-mount it's a great way to let new applications rapidly integrate into a system: define a URI, map URI scheme to a handler program in Kernel, and carry on?
i'm implementing a delay as vst plugin. It's pretty basic, without gui. When delaytime gets automated, the cpu usage is grealy increasing ( reason is allocating new array for every change ), first priority now, is to fix this:) https://github.com/stubiklaus/diLay 
We only get two sticky threads, so this would probably be buried. However, I think this is a good idea to just start this here! So /u/DroidLogician, for what project are you seeking contributors? Obligatory plug for [clippy](https://github.com/Manishearth/rust-clippy) where we currently have 25 [mentored easy bugs](https://github.com/Manishearth/rust-clippy/issues?q=is%3Aopen+is%3Aissue+label%3AE-easy).
&gt; True, but you will still get a runtime Error when you try to use a T in place of U unless the types are compatible. IIRC that's just a type assertion on the returned value though, the generic container itself doesn't exist at runtime, whereas I believe it does in Rust (even if that information isn't accessible due to an absence of RTTI).
I'm willing to mentor on [multipart](https://github.com/cybergeek94/multipart), adding support for Hyper's `client::RequestBuilder` and/or server support for Iron, or any other HTTP crates out there. I'm selling it as backend-agnostic but that's a pretty shallow promise if all it works with is Hyper anyways, right?.
X-post from users.rust-lang: Looking for more crates/types to add support to [`multipart`](https://github.com/cybergeek94/multipart) for. Goes without saying that I'm taking requests! First on the list is Hyper's `client::RequestBuilder`, which requires a lazy approach instead of the current eager one--ironically, `multipart`'s client implementation *used* to be lazy, so maybe I'll go back to that or somehow provide both. I want to try static chaining *a la* [`std::io::Chain`](http://doc.rust-lang.org/std/io/struct.Chain.html) to avoid both allocations and dynamic dispatch, but I'll probably have to provide some sort of dynamically dispatched fallback for requests which need to be built dynamically (e.g. adding fields based on runtime data). And another feature I have in mind which I'll need [specialization](https://github.com/rust-lang/rust/pull/30652) for: being able to create sized requests without having to write the entire request into memory to measure it. I'll have a trait inheriting from `Read` which adds a function that returns an `Option&lt;u64&gt;`, with a default impl that returns `None` and specialized impls which return `Some(size)` if they can accurately get the number of bytes in the reader, e.g. like `&amp;[u8]` or `File` for finite-sized files. If all readers added to the request returned `Some(size)`, then the total size of the request must be the sum of all these values plus all the HTTP boilerplate, which is relatively easy to measure.
The design of vulkano is mostly figured out. I still have a few uncertainties, the major one being how to prevent the user from destroying command buffers while they are still in use by the GPU, without adding a too big overhead? I had some precise ideas before the 1.0 release, but some details in the specs ruined my plans. I'm also starting to convert my game to vulkano. The library is far from being finished, but using it in a real-world situation will help me find potential problems with the API. Also writing an article to describe vulkano's design choice. That's probably the most annoying part. 
I will very soon need to handle file uploads in my website written with [rouille](https://github.com/tomaka/rouille). You crate is interesting.
How to specify the return type of the func2 when reusing the func1 of my trait that has a generic return type? struct MyStruct&lt;F&gt; { f: F, value: i32 } trait MyTrait { fn func1&lt;F&gt;(self, value: i32, f: F) -&gt; MyStruct&lt;F&gt; where Self: Sized, F: FnMut(i32, i32) -&gt; i32 { MyStruct { f: F, value: value } } // What is the return value here? fn func2(self) -&gt; ??? where Self: Sized { self.func1(0, |i, v| i + v) } } Playpen: http://is.gd/leHsKg
/r/rust_gamedev may also be interested in this.
I've just published a rust/java comparison, so I'll be watching the discussions and try to defuse any misunderstanding. Which is a diversion from my range set.
That really resonates with me, and I think it's a similar background in dynamic languages and the pain points that can bring that really has made me appreciate the strong typing combined with the expressiveness of dynamic languages that really draws me in. I really like the party about compiler errors saving me from an error I just didn't see; that's my feeling verbatim :)
The simple answer is you can't. ~~Functions~~ Closures are anonymously typed. The long answer, is you need to create a trait object out of the function, and use that. http://is.gd/JqZYYQ
I have a couple of questions: 1. In the book, there is a section about [Diverging Functions](https://doc.rust-lang.org/nightly/book/functions.html#diverging-functions). But it's not explained when they are useful, and why they can be used as any type. We know that in C, `__attribute__((__noreturn__))` is helpful for compilers and static analysis tools. Do diverging functions serve a similar purpose in Rust? 2. Is there a page that documents best practices for signal handling in Rust? For example, someone suggested using `sigaction` from `nix` in a forum discussion. But what about signal functions used in multi-threaded programs like `pthread_sigmask`? What is the right API to use to get this functionality in Rust?
In pre-generics Java, you dealt with the fact that e.g. ArrayList holds an array of objects by casting to the appropriate type on access. String s = (String)stringArray.get(i); With "generics" it's pretty much the exact same at runtime, it's just that now you do it like this in the code: String s = stringArray.get(i); and the compiler will verify that stringArray is actually supposed to hold strings before inserting this cast. The generated byte code is probably very similar to what it was before if not identical. To really see the difference I would recommending doing some C# programming with generics and gain experience with all the things you get. Then try the same in Java and understand why the same strategy won't work. As an example, here is fetching an object from NHibernate in C# (disclaimer, from memory): Person p = nhibSession.get(id); And in Java: Person p = hibSession.get(id, Person.class()); The difference in how generics work is the reason for the difference in interfaces here. 
The Lua C API is one of the best designed parts of the implementation. http://www.lua.org/pil/24.2.html And LuaJITs ergonomic cffi has spread nearly everywhere. 
It basically involves manipulating the LUA stack, which can lead to fun things if you hold it wrong. The API is certainly good, but it _has_ pitfalls. Here's a detailed discussion: https://julien.danjou.info/blog/2011/why-not-lua Also, from a "use it from Rust perspective", things like `lua_pcall` change between being functions and macros between versions, but are documented as if they were something callable...
From what I gathered talking to wayland devs, `wl_drm` is an implementation detail of mesa, and should not be relied upon at all. What I understand is that, basically, an rust re-implementation of the libs would have to match the API of the C lib (and maybe even part of its internal representation), which removes any interest in doing so...
I was thinking in the direction LLVM-&gt;JVM, meaning in theory Rust code could run on JVM
It's not clear to me, to be honest. Usernames are hard to keep track of :)
The byte code isn't identical. In fact, there are synthetic methods the compiler introduces that wrap the non-generic methods and forward generic calls to them. This ensures that both the generic and pre-generic version can be called. However those wrapper methods get inlined by the VM, so there is essentially no runtime overhead.
Edit: this was about Java, not Rust. Rust optimizes up to the crate level, but LTO can go beyond.
Well, the Rust compiler can be considered as a pretty good static analyzer, so I would say `-&gt; !` is similar to `__attribute__((__noreturn__))` in that sense :) It can be useful when you need a guarantee that a function doesn't return, such as providing a panic handler for an embedded system. You can also have a diverging _expression_, which just means the code after it never runs -- control flow can continue somewhere else. For example, `let x: Thing = return;` It doesn't matter what a `Thing` is, or that I didn't write the code here to construct one -- this typechecks because the compiler can see that nothing ever actually gets assigned to `x`. For the same reason, fn diverge() -&gt; ! { panic!() } let x: Thing = diverge(); always typechecks. This is why we say that "`!` unifies with any type".
I'm not really a beginner (:P), but I wanted to see if anyone knew this... Why aren't these implemented: impl AsRef&lt;T&gt; for T {...} impl AsMut&lt;T&gt; for T {...}
Let me rephrase your question: Why shouldn't T be a reference to T? I have no idea how a type that is a reference to itself would look like.
I've always though that Rust is the prime replacement for high performance+low latency Java - Go still has pauses (albeit short) and suffers from fragmentation, and realtime/pauseless compacting JVMs are expensive and sometimes require abstruse techniques to make full use of. Rust improves on the memory safety and productivity aspects of C++ while retaining the predictable high performance.
 fn as_ref(&amp;self) -&gt; &amp;Self { self } it's implemented on `str` and `[T]`, for example.
how does it compare to minix?
The docs say: &gt;The `Borrow` and `AsRef` traits are very similar, but different. Here‚Äôs a quick refresher on what these two traits mean. &gt;Choose `Borrow` when you want to abstract over different kinds of borrowing, or when you‚Äôre building a datastructure that treats owned and borrowed values in equivalent ways, such as hashing and comparison. &gt;Choose `AsRef` when you want to convert something to a reference directly, and you‚Äôre writing generic code. This isn't very helpful :'( `Borrow&lt;T&gt;` is implemented for `T`, but `AsRef&lt;T&gt;` is not implemented for `T`. I am so friggin confused. Why is `From&lt;T&gt;` implemented for `T` if `AsRef&lt;T&gt;` isn't? Is it just an oversight? gragh!
i guess, there is quite some love ‚Äì more love than the server can handle :( ‚ù§
I seem to recall reading somewhere that Java's overloading is only based on arity, not types?
Lol OK I just thought since Rust also produces LLVM IR I'd mention that Java can as well
No, you can have `frob(Foo)` and `frob(Bar)` in the same class. If an argument at call site could be both `Foo` and `Bar` (which can happen with interfaces), the compiler will give you an error and ask you to clarify with a cast.
&gt;I would like to start a discussion on Redox vs Linux in 10 years. Sorry- looks like we all got started without you.
Do you have any ideas on how to word it more clearly? This stuff is really important, so if it's confusing, I'd like to fix it.
This issue also introduces a new section "Call for Participation". Suggestions welcome.
Knowledge in brain did not translate to hands on keyboard, thanks for the catch.
I used it in a student project (game engine) last year. It was interesting, but it definitely had some problems that could be show-stoppers for production-level stuff.
Any time :)
can a link to that be made sticky? or put in the side bar? How does that work?
https://github.com/rust-lang/rust/issues/24355
[tiny_http](https://crates.io/crates/tiny_http)?
I feel like the most important information is at the bottom of the page. I read This Week in Rust to get updates about changes to the language, know about approved RFCs and final comment periods. I don't care much for the updates from the community, especially since most of those have already been posted to reddit during the week. The changes to the language get way less exposure during the week, and they should be at the top.
&gt; but they are relaxed during unsafe blocks I don't think so. The only things `unsafe` blocks allow are calling unsafe functions, implementing unsafe traits, reading/writing to a raw pointer and mutating `static mut`s. Borrow checker works as usual in them. See, for example, [here](https://doc.rust-lang.org/nightly/nomicon/meet-safe-and-unsafe.html).
On Wednesday, we have a Rust meetup in Cologne, which I need to prepare some things for. [More information](http://rustaceans.cologne/2016/03/02/live-coding.html) -- You should come if you are in the area! Also, I want to get some [cargo-edit](https://github.com/killercup/cargo-edit) stuff done. For a while, my goal was to get `cargo add` into a state where we _just_ need a nice TOML crate (which reformats less on edits) and be done with it :)
You would still get a warning from `javac`.
&gt; Obligatory plug for clippy where we currently have 25 mentored easy bugs. Do you have a short(?) description about the build requirements for these bugs? I'd like to play with one if it does not require me to rebuild rustc and a gazillion other things using a thousand steps process. I'm afraid I'd get lost and give up easily, unfortunately I have a short attention span.:(
You do need a nightly Rust (which you can download and install from rust-lang.org or use multirust or multirust-rs), then you can simply clone https://github.com/Manishearth/rust-clippy and run `cargo test` on it to see if everything compiles. No need to recompile rustc. The 'bugs' are usually lint suggestions or suggestions to extend something.
+1 language changes are always my favorite part.
I'd be happy to try to help. In fact, if you'd like, I'll keep some notes that I could share with you once I've made it through the docs and I may be able to help with some pull requests for the docs too. So [here](http://doc.rust-lang.org/book/references-and-borrowing.html#thinking-in-scopes): &gt; What we want is for the mutable borrow to end before we try to call println! and make an immutable borrow. I feel like this could be more natural because when someone borrows something from me and then it's returned to me we don't say that I'm borrowing it. Maybe something like: &gt; What we want is for the mutable borrow by y to end so that the resource can be used by x, the owner, in println!. Perhaps internally, the same process is used to give access to y as when it is returned to x so there may be reason to refer to it as an immutable borrow again when it transfers from y back to x but it could be worded my clearly imho. Thanks for all the work everyone is doing on such a neat new(ish) language.
Notes would be great, PRs are even better :) Ahhh, so I think the difference that's missing here is that `println!`is in fact going to be borrowing its argument. Your new wording is saying a slightly different, yet equally true thing. I would take a PR to use that wording :)
You want /r/playrust ;), this is a subreddit for Rust the programming language.
Do you think redox would be called downstream-centered if some company takes your code and its hardware, writes drivers &amp; everything else in a proprietary manner and you won't even be able to use your OS on that hardware? That's called tivoization and GPL actually protects you from that. Do you feel protected from such a scenario using MIT?
One annoying consequence of this is that you can't overload methods based on generic types. This means that you can't have class FooBert { public void doStuff(List&lt;String&gt; stuff) {} public void doStuff(List&lt;Integer&gt; stuff) {} } Because Java just sees `doStuff(List stuff)` at runtime. 
I'm thinking of buying a Raspberry Pi, is there a particular model that's easier to get rust up and running on or are they all the same? and is there any other detailed blog posts like [this](https://blog.thiago.me/raspberry-pi-bare-metal-programming-with-rust/)?
It is! Thanks a ton.
I rewrote the whole interface, so that log and return values are now passed explicitely. I could get rid of all the magic macros and boxed closures, but unfortunately there are still more `Arc`s than I would like. I also found and fixed a bug, that could lead to deadlocks, improved the unit tests and cleaned up the implementation.
You could argue that means it's actually in the correct order, following the "save the best for last" principle. 
Very cool!
Thank you for the extensive answer! It does make sense to have 2 layers. In fact, for partially the same reason we have `gfx_core` crate separate from `gfx`. It's nice to see Glitter as an alternative to Glium. Folks who know what they are doing and can't tolerate any overhead will surely appreciate your zero-cost approach. As for practical applications, there is a shorter way for you to get something running other than writing your own apps &amp; games: write a piston-graphics backend for glitter (it already has backends for gfx, glium, and raw gl). This way you'd be able to run piston games out of the box.
If we're interested do we just comment on the issue and ask for help? Or is the expectation that we can jump in and deal with the issue mostly solo?
How are you going to avoid the confusion that torments linux right now? With /usr/bin/, /usr/local/bin, /usr/sbin, /usr/local/sbin, /usr/share/applications, etc. In my opinion, it is not clear at all what goes where and is not intuitive at all.
&gt; What classes of errors does this prevent, given that "glitter is designed to statically prevent OpenGL errors where possible, using compile-time checks" is advertised as a feature? So, I'll be perfectly honest: I'm no rockstar programmer, and I'm bad at getting things right the first time. My past experiences with OpenGL have always been frustrating, which stings especially bad, because I think the idea of 3D graphics programming is *really interesting*!. For example, take this code that creates and fills a buffer: float vertices = { -1.0, -1.0, 0.0, 1.0, 1.0, -1.0 }; GLuint buffer; glGenBuffers(&amp;buffer, 1); glBindBuffer(GL_ARRAY_BUFFER, buffer); glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); What mistakes could a person reasonably make while writing this? Here's the list of things I came up with: 1. Using the wrong data type for vertex data (e.g. using `int` accidentally) 2. Calling `glBindBuffer` with the wrong argument order 3. Calling `glBufferData` with the wrong argument order 4. Forgetting `glGenBuffers` after declaring `GLuint` 5. Forgetting `glBindBuffer` 6. Binding `buffer` to `GL_ARRAY_BUFFER`, then buffering data to the wrong target While this is totally contrived in isolation, this is a *necessary* part of building even the simplest "hello triangle" example (past OpenGL 1.x). But, coupled with the dozens of other OpenGL calls that make up "hello triangle" (or the hundreds or thousands or more that would go with building a graphics engine), the list of potential mistakes on any given line grows *quickly*. Like I said, I'm no rockstar programmer, so I make these kinds of mistakes all the time! And the *worst part* about all of that is that, in OpenGL, **these errors compile successfully, then silently fail**. I don't know what percentage of my life was spent writing some OpenGL code, hitting run, staring at a black screen, then staring at code for a while; and I don't really want to know. And the problems only get more subtle and even harder to track down when you start breaking stuff into functions. Now, maybe it's my fault for not using good tools like an OpenGL tracer to point out these errors (although Apple's Instruments works pretty well for tracing on iOS, but lacks OpenGL tracing on OS X). And it really feels like more of a failing of the API if some *special* debugging tool is needed for finding simple problems. Wow, that was a long tangent. Well, maybe I'm the only person in the world to feel that way, but I was annoyed enough by those experiences to try rolling my own OpenGL abstraction. Isseus 1 through 4 above could also easily be solved by using newtypes only, but I also strived for a solution that could solve points 5 and 6. &gt; I think libraries like Glium and gfx-rs are roughly the right abstraction that belong at the lowest levels of any Rust graphics-using application, with higher-level APIs that don't really exist yet serving other users For the most part, I think I agree. But, every library has trade-offs, and no tool will ever be suitable for every problem it's domain. Perhaps you're right, and the lowest level anyone will ever *need* is Glium or gfx, but I still think that there are circumstances where a project needs to be able to squeeze out every last drop of performance, from both the CPU and GPU. Maybe it's only 1% of projects, or 0.1%, or 0.0001% (or maybe it's only crazy people that like doing stuff at the low level; i.e. me), but I think there is *some* subset of projects that need no-overhead OpenGL access. For these projects, most people will reach for raw OpenGL (either in C or using the `gl` crate). But, glitter is designed to explore what an alternative world could look like. Like I mentioned in a few places, glitter is little beyond a prototype right now, but I'm genuinely curious to see what it can be used for, if there really is a niche of people that need to use OpenGL directly, but are willing to pay the price of complexity for a little peace of mind (which is arguably the same trade-off Rust makes in the systems programming space). That said, I think you definitely nailed down what I think the biggest failing of glitter is so far, which is the incredible amount of complexity. Realistically, a low-level OpenGL wrapper shouldn't be *that much* code. But, a lot of code went into making the binding system work, and work across function boundaries. That extra complexity though also gets pushed into the user's face, and it means that the code is easy to read, but hard to fully understand. Was the extra complexity worth it? I'm definitely not the one to say, only the people that would use glitter in large-scale games and game engines could possibly say. Overall, I have to say that I agree with your points, but I think I'll keep working on improving the library to push it to its limits. Even if it ends up being a dead-end, I still think it's an interesting problem, and one where I've been able to learn a lot about both Rust and OpenGL :D
See my second example for the function case :) Concretely, if you run Rust on an embedded system without the standard library, you have to implement several low-level functions, and one of them, `panic_fmt`, is called when the code panics. Its return type is declared as `!`, to ensure that your implementation never returns (simplest implementation: `loop {}`).
You know that linux drivers are in modules that get loaded when needed, right?
Linux does not have multiple drivers for the same piece of hardware. What should the distribution specify?
"Save the best for last" is not a great principle for technical writing.
I've had my eye on that one for sure.
For all the handful of MB that saves you, yes.
One of the principles of web design is that the user is free to leave at anytime (it's the nature of web browsers). It's therefore important to catch the viewer's attention and be straight to the point. Give the user what they want without having to jump through hoops. In our case, assuming people come for the juicy changes to the language, this is the first information the user should see.
Quite possibly. I'll ask them about it.
Just done implementing an ARM state disassembler for my GBA emulator. Quite an ugly piece of code, but I'll beautify it later. Maybe tomorrow. The actual working code has top-beautification-priority. `¬∞^¬∞` And I started implementing a unit test that disassembles raw test instructions and compares the output to an expected string. The test instructions cover all possible branches in the code (read: all possible kinds of instructions + flags), which helps spotting bugs in two points at once: Decoding instructions and disassembling them. Edit: Little nasty `^`...!
Um, you can rebuild the kernel without those modules.
&gt; Note that there are good reasons why Linux is licensed under GPL. Linux is licensed under the GPL because it was the only available copyleft license around at the time.
I too would prefer having rust changes first. Even if I like community stuff a lot
&gt; I think I'll keep working on improving the library to push it to its limits Please do! I'm interested to see how it turns out.
In theory, a userspace driver can be isolated from the kernel such that the driver can't crash or hang the kernel, no matter what happens. In practice, this is not that much of a benefit compared to the way Linux drivers behave today. In many cases, the current modular ones can be reloaded and restarted even in kernel space, and something like a graphics driver or a keyboard driver hanging would still be a pain in the fundament even if it was in userspace (the user would need to work around the missing human input and output capability, which, for a personal system, is likely to be more difficult than a reboot).
&gt; Here the Rust type system could help us, by tracking effects, or at least specifying that a given function or closure must be pure. Unlike haskell rust is not designed to prevent side effects, especially because the functions are not entirely pure. They still mutate Transaction and indirectly TVar. I think it would be helpful to have lints, but they can not detect all mistakes, because they can't know, which functions are safe to use. Still deadlocks are the worst, that can happen, because the library follows all rules for safe rust, even when it is used in the wrong way. &gt; I don't get it, could you elaborate? Failure in stm means that the transaction will run again. `or` chooses the working one out of two possible calculations. Both Transaction::or and Result::or / Result::or_else would do that, but Transaction::or throws away all the now invalidated writes.
Oops - the code blocks are intended to be: enum Formula { Atom(char), Not(Box&lt;Formula&gt;), Implies { l: Box&lt;Formula&gt;, r: Box&lt;Formula&gt; }, Iff { l: Box&lt;Formula&gt;, r: Box&lt;Formula&gt; }, And(Vec&lt;Formula&gt;), Or(Vec&lt;Formula&gt;), } and fn clone(f : &amp;Formula) -&gt; Formula { match f { &amp;Formula::Atom(c) =&gt; Formula::Atom(c), &amp;Formula::Not(ref n) =&gt; Formula::Not(box clone(&amp;**n)), &amp;Formula::Implies { ref l, ref r } =&gt; Formula::Implies { l: box clone(&amp;**l), r: box clone(&amp;**r) }, &amp;Formula::Iff { ref l, ref r } =&gt; Formula::Iff { l: box clone(&amp;**l), r: box clone(&amp;**r) }, &amp;Formula::And(ref v) =&gt; Formula::And(v.iter().map(|ref x| clone(x)).collect()), &amp;Formula::Or(ref v) =&gt; Formula::Or(v.iter().map(|ref x| clone(x)).collect()), } } Hopefully that works the second time around. 
I think you can use Graphviz as a library ( http://www.graphviz.org ). If I recall correctly, it contains tools both for constructing graphs and showing them.
I documented my experience [here](https://internals.rust-lang.org/t/mozilla-is-hiring-for-the-rust-team/2897), you may want to do so as well. I applied, received an automated response saying "get back to you in about two weeks", received no response for two weeks, sent an email, received no response for two weeks, wrote on internals.rust-lang.org, received an automated response almost immediately saying "we're sorry that we were unable to make it work".
I expect array_join, because it's surprisingly fast compared to every other method . Hate the clickbait title though.
For `push_str`, can you try with `String::with_capacity` (as you know the size)
Hang on, why is `&lt;[&amp;str]&gt;::join` so much faster than `&lt;[&amp;str]&gt;::concat`? They should be doing *more or less* the same thing. Also, there's one other variant: using `String::with_capacity` and `push_str`, which is handy in cases where you don't have a nice slice of strings to use `concat` on (but it should otherwise be identical, performance-wise).
I tried to raise some questions on MIT applicability for OS in neighbour threads (check out surrounding comments).
Great question! &gt; Why is unsafe inevitable Because when writing extremely low-level, you simply _need_ unsafes, to build safe abstractions. You need `unsafe`s to talk to hardware, you need `unsafe`s to create a memory manager. So you use `unsafe`s to build safe abstractions. &gt; how can you still call your OS safe if it has unsafe code? We can't. But, having 84% proven safe code is better than having 0% safe code, which Linux has, because of the nature of C. I am working on a complete formal verification of the kernel, to prove that those 16% are actually safe, so hopefully some time Redox is proved completely safe.
&gt; But then again, I do see it did not become hijacked by any single company and stayed alive &amp; independent for so many years. Thanks for the question. To avoid duplicates, I'll answer this with text from the work-in-progress book I am writing: We wouldn't mind if somebody did that. For successfully steal a project, you'd have to do _some_ improvements over the upstream version. You can't sell an apple for $2, if another person stands right next to you, giving them away for free. For this reason, making a (potentially proprietary) fork interesting requires to put some time and money into it. There is nothing wrong with building on the top of Redox. You can't _unfairly_ steal a project. That's simply not possible. For a fork to gain interest, you will no matter what have to put effort into it. Building on the top of Redox, whether it gets to upstream or not, is a thing we appreciate. We like to have a decentralized structure of the project, allowing people to do whatever they want, no matter how they intend to share it. 
I can't say how it is going to look in the future, but currently we have a `/bin` folder, where _all the binaries_ (modulo the app folder) are placed.
I'm not a Rustacean by any means, but it would appear that `[].join()` is using some kind of pre-allocation. It could also be tabulating the size of all elements (since it could know that they're all strings in the first place) plus delimiters so that it only allocates what it needs. But it clearly just goes to the heap exactly once. I'm curious - what do the results look like with large numbers of joined elements vs `concat()`? Edit: do Rust's borrowing/ownership semantics force reallocations on return values sometimes? Perhaps a spurious copy on return is also causing the performance gap?
Going through the formatting infrastructure is bad because it involves virtual calls.
I don't think there's really anything like this in Rust yet. However, I really like the Python library matplotlib. It can do plots, animations, etc. either live or output to files.
I find it weird, but also very interesting. Is there no way to talk to hardware or make a memory manager in safe or does the nature of those things make unsafe needed no matter what?
You might want to take a look at [this](https://github.com/nrc/patterns/blob/master/idioms/concat-format.md). Granted, the idiomatic approach doesn't necessarily have to be the most efficient, but getting people in the habit of using the *least* efficient approach might not be the best idea either, especially when you're trying to maintain a reputation for speed. 
the array_join() version which is more then twice as fast as array_concat() :D
Can anyone explain the side effects that occure when connect() and join() are both in there? https://github.com/hoodie/concatenation_benchmarks-rs/commit/f088fa4c4fe37ce377b5febf41451ad7085f8108
How peculiar that `join` is faster than `concat`. What about let datetime:&amp;str = &amp;[ DATE, "T", TIME ].join(""); ?
You nailed it. Well put!
There are lots of pitfalls with benchmarking like that. If there is only one single use of `join` in the whole program, the compiler might make different optimizing decisions.
And none of these seven ways are accidentally quadratic (nor intentionally so..). That's a nice win right there.
For what it's worth, the second one blew me away too. For once the clickbaity part is accurate :-)
Merged.
Ah okay, that's pretty clear.
Very clear explanation as well. Thanks.
/u/larsberg is the right person to ask here. I think you should continue listing the MPI-SWS RustBelt thing, though, that hasn't gotten enough facetime.
There's https://github.com/brson/httptest
For the `enum Formula` you gave above, adding `#[derive( Clone )]` before it should work fine and give you a deep-copying `Clone::clone` implementation. Also, going from `&amp;Box&lt;T&gt;` to `&amp;T` should just work when the types are available to the compiler, though I'm unsure right now which trait makes that work: let box_reference = &amp;Box::new(23); let reference: &amp;i32 = box_reference;
oops thank you very much
Good read, as with the rest of the series! I have to say that I'm still trying to wrap my head around Differential. I think I understand how/why it works for situations where the predominant motivation is calculating some metric about the collection, driven by changes to the collection itself. Add some values to the collection, Differential represents those as diffs and incrementally updates the metrics about the collection. It's less clear to me if Differential is useful when the collection is relatively static, but you are invoking methods on the collection and care about the side-effects of those methods. For example, those side-effects may be incremental themselves (invoke on one item, it cascades to a small subset of the entire collection), so it feels like Differential would apply. But it seems at-odds with how Differential maintains diffs of the collection. But perhaps that just means the side effects are what should be modeled as the collection (not the collection itself), and I'm thinking about the problem the wrong way. :) In any case, Differential has always intrigued me, I just need to keep wrangling with my mental model of how it can be used :)
I know that feeling. We'll still be here when you have time! :)
Have you tried looking at the generated assembly?
Maybe a compromise would be to have a little ToC at the top, so you can jump straight to "Updates from Rust Core" if you want to skip the community stuff.
That isn't exactly obvious from the posting, I think, maybe a short clarification should be added?
The compiler is telling you that if you removed an `Item` from the vector, then any `link`s that were referencing it would be left dangling. Probably what you want to do is think about who you want owning `Item`. Right now your code is stating the the vector owns each `Item`, so if the vector ever discarded them, the `link` would be bad. You might try changing your code so that `link` was the owner, and the vector stored the references, but then you'd have the opposite problem: any `Item` which got un`link`ed would have invalid references to it from the vector, so you can't do that either. If what you want is the `Item` to be valid if *either* it is stored in the vector *or* pointed to by a `link`, then you're talking about a shared-ownership situation. For that, use [`Rc&lt;&gt;`](https://doc.rust-lang.org/std/rc/), like [so](http://is.gd/yUHn4D). Of course, that still doesn't compile. Now the problem is that `Rc` can only contain immutable data, and we need to mutate `Item` to set its `link` field. There are two options here. Either we can leave `link` immutable by constructing the whole linked chain in one go and then doing a separate pass through the chain where we add references to the vector, or we can just wrap our item in one of the [`cell`](http://is.gd/yUHn4D) types to allow mutation where it's normally not allowed. These types are `Cell` and `RefCell`. `Cell` only works for types that are `Copy`, which `Item` is not, so let's use `RefCell`. Finally, we get [something that works](http://is.gd/6m2MZj).
Try /r/playrust
Surely `+` / `push_str()` should be O(n\^2). (Without `with_capacity()`). And only one of the results is faster than those obvious quadratics...
Also, nom is here too https://teams.railsgirlssummerofcode.org/projects/78-nom!
&gt; Calls to `atomically` should not be nested. Is this a technical limitation? This seems to really hurt composability, as it means that if I were to call a 3rd party function I would need to ensure it doesn't, itself, calls `atomically`. I am wondering if it could be possible to: - either lift this limitation: either have the inner atomically be transparent or be an inner transaction - or reliably `panic`/`abort` to inform the developer of the issue in a deterministic way
I must admit I still do not understand why the winner... is a winner. I checked the array `concat` implementation [libcollections/str.rs](https://github.com/rust-lang/rust/blob/52cb8a9d39d05126a79e7b9a3adc31a5e3cdde94/src/libcollections/str.rs#L66): fn concat(&amp;self) -&gt; String { if self.is_empty() { return String::new(); } // `len` calculation may overflow but push_str will check boundaries let len = self.iter().map(|s| s.borrow().len()).sum(); let mut result = String::with_capacity(len); for s in self { result.push_str(s.borrow()) } result } And sure enough it's pre-computing the length and allocating the right capacity straight up. I suppose some calls are not inlined/some constants not propagated (for example, that the `len` is not computed at compile-time, or the loop), but I am a bit disappointed :(
Two questions about the `Box::downcast`: impl Box&lt;Any + 'static&gt; fn downcast&lt;T&gt;(self) -&gt; Result&lt;Box&lt;T&gt;, Box&lt;Any + 'static&gt;&gt; where T: Any impl Box&lt;Any + 'static + Send&gt; fn downcast&lt;T&gt;(self) -&gt; Result&lt;Box&lt;T&gt;, Box&lt;Any + 'static + Send&gt;&gt; where T: Any 1. Why is the `'static` bound needed? I thought `Any` already required `'static`. 2. This is stable Rust, so no impl specialization yet. Why don't these impls overlap? It seems like any type that matches the second impl also matches the first. Are there different rules for OIBITs?
I was hoping for a tutorial somewhat similar to a book like the rust book which has chapters explaining certain things about iron and so on
Thanks but I was hoping for something similar to the Book but for iron
That does not exist, unless someone has been keeping it secret :) I agree such a thing would be very useful.
I'm sure it was. Doesn't mean I have to enjoy it.
The format string is parsed at compile-time and typechecked, however there is no compile-time evaluation. &gt; Given that the string literal for the formatting string doesn't change, (and you actually have a guarantee of that, which you don't have in C) I would expect quite a bit of that work to be completed at compile time. If it doesn't already do this, it sounds like a potential area of improvement for Rust. The actual effective value (and ROI) seem very debatable.
whenever i see STM i think of intel TSX https://en.wikipedia.org/wiki/Transactional_Synchronization_Extensions - would it be possible to implement atomically() using these?
Came to say the same.
I write a lot of Python these days in my day job. It definitely fails to scale quickly (for example, you start spending so much time keeping type documentation up to date that a typed language could have payed for itself many times over). Not to mention performance, parallelism, etc. From a language perspective, it would be great to have something in the space of Go or Java; Rust is still unsuitably low-level, relatively foreign, and quite a bit more complex than Go or Java. Between Go and Java, Go is a moderately better *language* for its value semantics, error-handling model, concurrency model, type inference, and inheritance-free, modern standard lib. But Go's simple ecosystem really makes it the winner; no complex XML or DSL project configuration files, static-compilation by default, native binaries, no dependency on an IDE, etc. Go is still working out its package management story, but I do find the simplicity of a vendor directory to be more attractive than a Maven repository (having used both). This is only meant to be a 'Go is better for the use cases I encounter' datapoint; this may not generalize well to other use cases.
I am working on [libseccomp](https://github.com/seccomp/libseccomp) bindings. I can do simple things like this: let mut ctx = Context::default(Action::Allow); ctx.add_rule(Rule::new(105 /* setuid */, Compare::arg(0).using(SCMP_CMP_EQ).with(1000).build().unwrap(), Action::Kill)); ctx.load(); [seccomp_rule_add](http://plhk.ru/static/doc/seccomp-sys/seccomp_sys/fn.seccomp_rule_add.html) uses varargs and i haven't figured out how to unwrap a Vec into varargs yet, so multiple comparisons are not yet possible.
Model =&gt; Render Push updates to model, re-render. Probably just take a couple demos from d3.js and port them to Rust and Gilium.
&gt; I recently asked a question on Stack Overflow but was advised to resubmit my question on reddit. SO does have an active contingent of Rust users, even if there's not quite as many as here on Reddit, so I wouldn't interpret that advice to mean that you should always ask on Reddit instead. :) In particular SO scales much better at supporting a large number of questions, its search capabilities are far superior, and it ranks highly on Google so an answered question on SO will be more likely to help someone else in the future. In fact, if you manage to find the answer to your question, I would encourage you to post the answer to SO as well. :)
Thank you for the very detailed explanation! I have one more question :) If I redefine `Item` as struct Item&lt;'a&gt; { name: String, link: Cell&lt;Option&lt;&amp;'a Item&lt;'a&gt;&gt;&gt;, } ... then, this works fine: pub fn main() { let vec = vec![ Item { name: String::from("one"), link: Cell::new(None) }, Item { name: String::from("two"), link: Cell::new(None) } ]; vec[0].link.set(Some(&amp;vec[1])); println!("{:?}", vec); } but this will again complain that `vec` does not live long enough: fn test&lt;'a&gt;() -&gt; Vec&lt;Item&lt;'a&gt;&gt; { let vec = vec![ Item { name: String::from("one"), link: Cell::new(None) }, Item { name: String::from("two"), link: Cell::new(None) } ]; vec[0].link.set(Some(&amp;vec[1])); vec } pub fn main() { let vec = test(); println!("{:?}", vec); } Aren't the two pieces of code equivalent though?
There's one important difference between Rust's [`String`](https://doc.rust-lang.org/std/string/struct.String.html) and .NET [`StringBuilder`](http://referencesource.microsoft.com/#mscorlib/system/text/stringbuilder.cs): `StringBuilder` uses chunks. An equivalent Rust implementation would internally use `Vec&lt;str&gt;` or `Vec&lt;String&gt;`, and concatenate upon materialization (an `Into&lt;String&gt;` call?). 
I've been looking for one for a while, but I haven't been able to find anything more than "Here is an example of how to do Hello World" or "Look at the docs, which are automatically generated from the source code." Maybe I should teach myself Iron an then write a tutorial...
This was a big frustration for me when I started to use it. After learning it, some of it is still pretty cryptic and hard to remember. I should really write a blog post explaining it, cause its docs are not very good and I think this would be of interest to a lot of people.
Basically, `where` clauses for `for`-wrapped types. I think.
For-wrapped type? I'm currently mobile else I'd try to research on my own.
I gave a talk about Iron at a Rust meet-up a few weeks ago, but it's probably not as focussed or as clear as you'd like (or I'd like). There's a recording here if you're interested: https://youtu.be/Sr5QcYyGrCo And slides here: https://michaelsproul.github.io/iron-talk/
Not sure, but I think it's that the borrow `&amp;vec[1]` only lives to the end of the function. That's fine as long as you're not returning anything, but when you try to return `vec`, it then has to be valid at the caller, which it's not. It's a bit easier to see like this: http://is.gd/AhNXcO pub fn main() { let mut vec = vec![ Item { name: String::from("one"), link: Cell::new(None) }, Item { name: String::from("two"), link: Cell::new(None) } ]; vec[0].link.set(Some(&amp;vec[1])); vec.truncate(1); println!("{:?}", vec); } where the call to `truncate()` isn't valid because there's an outstanding borrow.
It has to be a literal, because it's inspected at compile time. Or more specifically, earlier than things like constants are expanded, at least to my knowledge. Even though writing it out may not be great, at least it will be deduplacated in the binary,'s rodata, so you're not using extra memory.
You can use a macro for the format string: macro_rules! my_format { () =&gt; ("{0} {0}") } fn main() { println!(my_format!(), 5); } You've gotta fight macros with macros ;)
Oh okay I see. Thanks 
How soon could I expect servo to be my daily driver (outside or inside of ff)?
Not soon. The ways forward for Servo include: - Moving smaller Rust components into FF (this is already happening) - A browser with its own UI separate from FF (This is also already happening with browser.html) - Perhaps replacing FF on mobile (in the far future) - Replacing FF on desktop would be a very-far-future thing; it's more likely that more and more FF components will get replaced by ones in Rust I'm just guessing here, but by the end of the year I'd hope to have a browser that supports mostly everything for everyday web browsing needs; but may still be missing more niche or modern features.
Care to elaborate about your experience coming from Python/Javascript? E.g. what features gave you the most trouble, how useful our documentation is, and so on.
Is it correct to say it's a benchmark of rust libraries?
No. It's all using the standard library.
Just commenting here once more because I can't think of a better place. After merging those fixes I made to libstd, I've managed to get a "Hello World" build compiled to asmjs-unknown-emscripten with libstd included. It works as expected, thanks for the tips /u/tomaka17 The filesize difference in the output file is huge. The original hello_world.js that I crated without libstd and just libcore (as per the guide in my article) is 219kb, and the one compiled with libstd is 1.1MB! Im pretty sure emscripten has ways to shrink that with optimisations though.
/r/playrust Wrong subreddit. 
Probably the most important first step is to make sure you have good situational awareness. You need to keep an eye on your surroundings; frequently looking over your shoulder to see if someone's trying to come up behind you. It also means looking around for escape paths. If you were jumped *right now*, what would you do? Before you attack someone, consider what you'll do if things go wrong; how do you get yourself out of a potential mess? But, yeah, *completely* wrong subreddit.
The difference with adblocks is that they block requests to specific resources (or resources matching certain regexes), rather than to entire domains. Personally I'd love to switch to a whole system solution like a hostfile instead of having an addon if it were more situational than just redirecting certain domains to localhost.
I'd rather a slightly slower response time I. The browser than every time a new connection opens on my machine.
 &lt;anon&gt;:9:9: 9:17 error: mismatched types: expected `/r/rust`, found `/r/playrust` [E0308]
Are the examples still OpenGL and not ES?
We have CEF bindings, so you can use the regular ways for embedding CEF. Embedding is a proper goal, so it's probably going to get improved in the future.
How about with `join("T")` ? Then iets only 2 elements, might be faster..
It may well be faster to first shift both values together and then use a single lookup, depending on hardware. I'd certainly measure before declaring one faster than the other.
Finally done readin it. Very well written and entertaining, thanks!
&gt; which would be the "intersection" lifetime of &amp;self and 'a `RefHolder` has a lifetime smaller than 'a, so the intersection will always be `RefHolder` lifetime ('b) itself.
Doing this for the first time will take a lot of work, but: - one could then update this as part of the release process (which would take less effort) - TWIR already covers new crates, blogposts, ... reviewing 6 issues of TWIR will help a lot with keeping things up-to-date, - icing on the cake would be some sort of interactive website when people could, using their forum accounts, upvote/downvote packages as well as propose new ones to be added or propose some for removal.
I see, so it is not possible in today's Rust. Do you think I should bring this discussion to the internal forum / try to write an RFC for that?
Check out https://github.com/browserhtml/browserhtml for instructions
Wrong subreddit, try /r/playrust
&gt; I am not a bot. Uh-huh... is that why your name is "Droid"? *I'm on to you.*
This would likely amount to a proper higher ranked trait bound and higher kinded type system, something which isn't going to happen soon. I don't think pushing this is a good idea.
Browser.html is a separate project which is an HTML-based browser. Instructions for building and running are [here](https://github.com/browserhtml/browserhtml#prerequisites-and-setup)
Android, as in the mobile OS. I was pretty heavily into it when I created this account. Go ahead, Turing test me.
African or European swallow?
I'd launch into the Navy Seal copypasta if I didn't think it'd draw the ire of the mods.
longjmp skips destructors. Note that not running destructors isn't undefined behavior
Also, the sponsored project gets paid (a thing often missed).
I don't know if it's been done in Rust, but you should take a look at Matthew Might's essay ["Continuations by example: Exceptions, time-traveling search, generators, threads, and coroutines"](http://matt.might.net/articles/programming-with-continuations--exceptions-backtracking-search-threads-generators-coroutines/), which begins with an explanation of using continuations to implement `amb`, which appears to be similar to CBack's `choice` function.
I originally had that in [0.1](https://github.com/Marthog/rust-stm/tree/v0.1.2#usage), but I didn't like it. The api was ugly, I had to hide implementation detail behind macros, put all the functions into trait objects and it didn't solve any actual problems, because you can still run arbitrary code, even one with side effects. Haskell can prevent all side effects except ones with `unsafePerformIO` and co. There even is a runtime check to prevent calls to `atomically` inside unsafe IO.
Thanks for the explanation.
That's a good principle for telling a story, not so much for a news article. News articles use the principle of the inverted pyramid; most important information at the top, less important stuff at the bottom.
As an ocaml developer and great fan of menhir, I'm so very happy to see a LR(1) parser of this quality being developed in another language (and in Rust in particular). No, parser generation doesn't suck when you have a decent parser generator! :) @author: Are you aware of http://gallium.inria.fr/~fpottier/publis/fpottier-reachability.pdf ? How does error handling works in LALRPOP ?
I'm hunkering down and really trying to learn the language. I want to enter the 7DRL challenge next week, and I'd love to use Rust as my language of choice. I've got most of the syntax down, just wrapping my head around some of the idioms of the language. In that vein, any good books y'all recommend?
[0 allocation json parsing](https://github.com/valarauca/lib_json) Works on the 2-3 API's I use, if it breaks for you let me know.
I didn't want to make a thread for this. I am updating some code to in a template rendering module to remove all unwrap()s and panic!()s. It currently doesn't handle errors well at all. Is there a compiler option or (experimental?) linter which can warn me about dangerous unwraps, panics and the like?
Just pushed a PR with some small improvements.
Those are some nice error messages. 
Yeah, reducing the size of the generated code is definitely a priority. I've not really put much effort into that yet.
Yes. Thanks for confirming. 
In the blog post there was a code passage where ident was defined: Ident = r#"[a-zA-Z][a-zA-Z0-9]*"#;
Alright, thanks! That's interesting, is there any situation in which an auto-deref leads to an auto-borrow and therefor an infinite loop?
The differences between OpenGL 2 and ES 2 are substantial (unlike 1.x where porting is trivial) so, if you'd thought they were interchangeable, that must be the reason it doesn't run on an actual GLES Linux system. 
Heh, that *was* actually what I was building too! Lua grammar is particularly hard to describe due to its reliance on lookaheads.
Rc&lt;FooIter&gt; wouldn't really work because you can't move out of it. Box&lt;FooIter&gt; could work, but the whole point of abstract return types is so you don't have to box it in the first place.
Could someone explain what an "inherent function" is? I think I know what free-standing functions are (functions not part of an impl, is that right?) but I couldn't find anything on inherent functions.
Nice! This is one of the features I'm really looking forward to! This would make working with views of matrices or ndarrays much easier. However I wonder if this can also be useful in a more specific case: in my [sprs library](https://github.com/vbarrielle/sprs), I have views of sparse matrices, which are basically a struct holding three slices and some metadata (shape of the matrix, ordering): struct SparseMatView&lt;T&gt; { indptr: *const usize, indices: *const usize, data: *const T, rows: usize, cols: usize, } It would be nice to have DST semantics on such a view type, but there would be 3 pointers to handle, and I don't know if that's possible with the proposed RFC (or at all, I'm not sure what DST semantics on such a type would mean).
Sorry I didn't give the proper name here, in the library the view type is `CsMatView`. `indptr` and `indices` represent the non-zero pattern of the matrix in a [compressed storage format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29).
The main question with this feature was always the syntax since there's a broad agreement about its semantics and its necessity for Rust. So let's bikeshed a little bit :) Personally I don't like the suggested '@' syntax because it adds another way to express generics instead of expanding the current syntax. It'll overlap with existing notation if expanded to e.g traits. The semantics here are roughly: fn foo&lt;T: Iterator&gt;() -&gt; T {...} except that in the above the ```T``` is an *input* type whereas we want the function *body* to specify an *output* type. How about one of the following suggestions? fn foo&lt;foo::T : Iterator&gt;() -&gt; foo::T {...} fn foo&lt;fn::T : Iterator&gt;() -&gt; fn::T {...} fn foo&lt;Self::T : Iterator&gt;() -&gt; Self::T {...} And for traits it's just syntax sugar. trait Trait1 { fn foo&lt;Self::Out&gt;() -&gt; Self::Out; } is sugar for: trait Trait1 { type Out; fn foo() -&gt; Self::Out; } 
 struct CsfDatum&lt;T&gt; { data: T, index: usize, indptr: usize, } struct CsMatView&lt;T&gt;[R: usize, C: usize] { [CsfDatum&lt;T&gt;; R * C] } unsafe impl&lt;T, const R: usize, C: usize&gt; Unsize&lt;CsMatView&lt;T&gt;&gt; for CsMatView&lt;T&gt;[R, C] { const fn unsize(&amp;self) -&gt; &amp;CsMatView&lt;T&gt; { unsafe { std::ptr::make_fat_ptr(self, (R, C)) } } } unsafe impl&lt;T&gt; Dst for CsMatView&lt;T&gt; { type Meta = (usize, usize); fn size_of_val(&amp;self) { size_of::&lt;CsfDatum&lt;T&gt;&gt;() * self.width() * self.height() } } // see PixelBuffer for implementation of self.width()/self.height() Something like this, probably. NOTE: definitely DEFINITELY up for change, especially the integer generic syntax.
I'm very happy to see these new error messages. I used LALRPOP several months ago on a project to write a Mini-ML in rust. ([See LALRPOP grammar here](https://github.com/losvedir/rust-zoo-miniml/blob/master/src/miniml_grammar.lalrpop)). I actually ran into one of the exact problems mentioned in the blog post, and being a novice wrt to parsing could not decode the error message. I [opened an issue](https://github.com/nikomatsakis/lalrpop/issues/22) about it an Niko was kind enough to give an explanation that got me unstuck. Looks like that explanation is now part of the error message. Cool!
Thanks, that's what I thought. I will try to suggest using JSON. The worst that could happen is that I have to use Python after all ;)
It's an `&amp;` pointer though, does llvm/black box not know about the restriction on writing through it?
Great points, I think FFI is a little more complicated than I want to go for this school assignment. The python proxy could be a viable option though :) I guess I will see when they release the game server.
No, in fact the `black_box` call makes it so that the _address_ of its arguments is marked as "used in some arbitrary way". In fact, if it would do less than that, the optimizer could in fact deduce that we're not actually doing anything and optimize the whole thing away, which would mean that black boxing would be pretty useless for benchmarks ;-)
But... writing to an `&amp;` pointer is undefined behaviour, so "used in some arbitrary way" should not include "writing to it" (sucesfully)... Unless that information isn't (yet) passed along to llvm? Since we allow reading it should still be useful for benchmarks, since black box might print (as an example) the input it needs to be correct...
Yes, I imagine talking JSON over a socket will be considerably less work and frustration than getting FFI to work. The latter would also require `unsafe`, and I imagine your project will be much cooler if you can do it without using `unsafe`. If the assignment includes writing an AI for the game you might have a big advantage over people who used Python (assuming your solution doesn't have a worse time complexity) because you'll be able to evaluate board positions faster.
Personally, I mostly used the official Rust book and http://rustbyexample.com.
Yeah, those are looking like the best options. Thanks!
Great to see good error messages for the programmer, have you also thought about error messages for the user? I've written a few parsers using Menhir and I'm never satisfied of the error messages that I give to users; I add extra productions to catch some errors, but if they cause S/R or R/R conflicts I just remove them and say "syntax error". I'd love if Menhir had something similar to Bison where you could say "found token FOO, expected token BAR or BAZ". Is that planned for LALRPOP?
Nice! Thanks for your answer. Edit: oh. What was wrong with the answer?
&gt; The semantics of a by-value DST is that it can only be "passed down" and never returned. Is there any kind of by-value passing of DSTs at the moment? I thought there was not? I.e. does the no-return limitation apply if/when the by-value DSTs are implemented or is such a system already in place? And... why the no-return limitation? I guess it's because you can't place the "unsized" return value on stack? But what about the box and placement operators, couldn't DSTs allowed to be returned if directly boxed/placed into some backing storage?
I just use websocket in webpage with some js gui framework to communicate with rust. In rust I use the ws crate. It's works great. This way you can use any browser(chrome ie etc..), and maybe servo in the future. p.s I used to use these.. python+ cefpython+nanomsg+protobuf, but as browser support json only, I finally switch to the websocket with json.
It didn't actually work :P This one does, though.
What kind of application do you plan to do? Servo is probably a bad idea, especialy if you need forms. 
Instead of using Servo itself, you could use WebRender independently: https://github.com/servo/webrender
Yeah, that's precisely because we want to leave some leeway for future extensions. I can totally see boxing and doing all kinds of things with anonymized types seeing daylight sometime in the future. Box&lt;Trait&gt; means that you've got a trait object (=some struct, type of which isn't known compile-time, that implements a trait) allocated in heap. You have to use a vtable to call its methods, because the type isn't known statically. Box&lt;@Trait&gt; or Box&lt;impl Trait&gt;, or whatever the syntax is going to be, means that you've got a heap-allocated struct that implements a trait, and its type is statically known by the compiler but "hidden" from the user, so you don't need a vtable to call its methods.
It's not the whole point. Conditional bounds, while not addressed by this proposal, are something that `impl Trait` will almost certainly cover at some point and trait objects do not.
Yeah, the only thing saving the OIBIT name is that they are indeed traits! I liked OOT. I don't dislike acronyms myself, they are easily googable (OIBIT excels in this aspect IMO). See also: universal function call syntax or UFCS.
There's actually some discussion on renaming them: https://internals.rust-lang.org/t/pre-rfc-renaming-oibits-and-changing-their-declaration-syntax/3086/4 Auto traits, default traits, structural traits, inferred traits etc. have been suggested!
Unfortunately some sites that I like do not work well with JavaScript (or Flash), so I'd like to be able to allow it on a per domain basis... would that work too?
Servo might be still bit incomplete right now for rich applications, but in future I'm kinda pining for Servo to be a key to sort of pure-Rust alternative for [Electron](http://electron.atom.io/) 
You are not alone. :-) This is related to just one of the reasons I prefer keywords over sigils: being able to look up an unfamiliar term is pretty important!
Fortunately, Rust has taken the approach of either refusing to compile programs with UB, or doing what the programmer had probably expected (e.g. when int overflow checks are disabled, the values are defined to wrap, rather than blowing demons out of your nose). Still, this is something to keep in mind when dealing with undefined behavior. In particular, I wonder what would have happened if I tripped one of the things described [here](https://doc.rust-lang.org/reference.html#behavior-considered-undefined). (At a guess, UB in the "C" sense, since codegen behavior is mostly defined by what LLVM does).
X-Post referenced from /r/programming by /u/gpakosz [What every compiler writer should know about programmers](https://www.reddit.com/r/programming/comments/48sijo/what_every_compiler_writer_should_know_about/) ***** ^^I ^^am ^^a ^^bot ^^made ^^for ^^your ^^convenience ^^\(Especially ^^for ^^mobile ^^users). ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Thanks! These are all great things to know. I'll work on implementing that. I understand the rationale about the license, but if the main thing is about GPL compatibility, why not use one of the GPL licenses instead?
Haha, thanks for checking it out anyway :)
&gt; when int overflow checks are disabled, the values are defined to wrap, rather than blowing demons out of your nose Is that true? I was thinking it is considered undefined.
Thank you! That worked. Yes you are right, but I don't want to make all `Callable`s `&amp;'static mut`, since that would mean I would have to clutter the code with `unsafe` whenever I'm accessing those objects. On the other hand, what you are suggesting is safer so maybe I'll do that anyway.
Yeah, unsafe Rust can invoke undefined behavior. And yes, we can lint against it (at least in your case, we're on it) in clippy or even rustc. Speaking of which: Has anyone here seen other unsafe idioms that are actually UB we could lint against?
Congratulations to the team and thanks everyone for yet another great Rust version! This release has been stabilizing many APIs and there's been much work behind the scenes to allow even more great things in the future (notably MIR), what with the specialization RFC being merged and implementation underway, renewed interest in custom dynamically-sized types and anonymous return types and much more.
I get the part about the vtable vs. statically known types but I don't know how that would differ in practice. This is probably because I cannot think of an example due to just having done too little Rust. Couldn't the compiler do monomorphization like it does for generic types where possible and use vtables where it's impossible?
Static and dynamic dispatch have both their own pros and cons ‚Äì that's why we want the control over which ones are being used where (and of course control over allocation too). If it wouldn't be for low-level control and performance, we could use heap-allocated trait objects everywhere. That's what Java does, basically.
There's [the Glossary chapter in the Book](https://doc.rust-lang.org/book/glossary.html), but it's decidedly lacking.
The linked benchmarks (http://cglab.ca/~abeinges/blah/hash-rs/) mention a "btreenew" that's supposed to be better than the existing BTreeMap. Where is this, and what is its status? The program I'm writing now spends most of its time inserting into a BTreeMap.
/cc /u/gankro . That post was written a while ago, I wonder if that is just BTreeMap.
The release notes mention: &gt; [`BTreeMap` was rewritten to use less memory and improve the performance of insertion and iteration, the latter by as much as 5x.](https://github.com/rust-lang/rust/pull/30426) so "btreenew" could be the rewrite by the insatiable gereeter?
Haha, indeed, hopefully there are _far far_ fewer than 1,214 errors as a result of this change in all the Rust that exists in the wild.
well done, love the hashmap feature
a redditor mentioned it's in the detailed release notes though: https://www.reddit.com/r/programming/comments/48tgs4/announcing_rust_17/d0mngt8
I wrote a program that takes an integer (by which I mean specifically a number of type `usize`) on the commandline as the first argument. What's the idiomatic way to get at the value in Rust? I used: fn main() { let mut args = std::env::args(); args.next(); if let Some(Ok(n)) = args.next().map(|s| { s.parse::&lt;usize&gt;() }) { now_i_can_use(n); } else { println!("usage: prg n"); } } But that feels very clunky.
No there isn't. It's not safe to use arbitrary f32 and f64 As keys because they have only partial equality instead of total equality due to NaN != NaN. You would never be able to recover a value indexed with NaN. If however you know you'll never have a NaN, you could write a new type wrapper around the float, and implement Hash for it yourself. To protect yourself though, you really ought to do an assertion that the values make sense in your hash function. 
Thanks! It doesn't seem like you need the `into_string`, so I went with the slightly different: if let Some(n) = std::env::args().nth(1) .and_then(|s| { s.parse().ok() }) { something_something(n); } else { println!("usage"); } 
So I guess you are saying that no such helper is planned for the standard library (but in crates.io I think there is a float wrapper). It's an interesting situation, the White Knights of Correctness and Purity on one side, against the dirty messy Chaos Army of the Practical Needs on the other. I'm curious to see how it will evolve in the next years :-)
Another option is msgpack, pretty much drop in replacement for json and serializes and deserializes much faster
No it has nothing to do with longjmp. If you have a set of variables then you will only save the variables that have changed from one state to the other (instead of saving all variables with copying).
&gt; Supported VCS: Git, Mercurial OK.
I don't think webrender is a good idea here; you'd probably want _some_ layout engine for designing apps. Webrender is for the rendering step -- "I know exactly where things should go on the screen, now I need to draw them".
About the same level as [evas](https://docs.enlightenment.org/auto/evas_main.html)? That is, does it come with something like a scenegraph and does retained mode, or is it immediate?
Webrender is fully retained mode.
This is my first "finished" (fully usable) project in Rust. As such, feedback and comments are very much welcome. The program should be stable in most cases and has a lot of integration tests, but since this code hasn't been battle-tested in real-life usage, please backup your images before using this tool on them. It should be considered beta-quality, and I don't want to feel responsible if someone loses their entire library of images because of an unforeseen bug. (If you do use it and find a bug, please submit an issue as well!)
* ZST: zero size type * FCP: Final comment period * ICE: Internal compiler error * HRTB: Higher ranked trait bound (the `for&lt;'a&gt; X&lt;'a&gt;` syntax).
Clearly, SDL is trying to get an OpenGL context and not GLES - there are separate backends for those so it's entirely possible the examples are fine and it's merely about SDL initialisation.
I've implemented your suggestions. The code is a bit cleaner now (:
Ah, *OIBIT*s. Famously neither opt in nor built in! Fortunately OIBITs have a new name: [**Auto traits**](https://github.com/rust-lang/rfcs/pull/1522#discussion_r54885874)!
The big divergence is that Rust focuses on low-level performance at the cost of allowing unsafety. Ada also focuses a bit more on runtime checks, while Rust tries to be as fast as reasonably possible. Overall I think the attitude is inherited, but there's not a particularly strong relation between the two languages.
Congratulations. What's the state of `const fn`s or compiler plug-ins?
Both still unstable. Off the top of my head, not sure what's blocking const fn, compiler plugins are being worked on but there's still lots to do.
Hashmaps are implemented using hash tables, not trees, surely? BtreeMaps require pointer chasing, whereas hash maps could get a hit on the first cache miss (or two).
Well, there's a [few issues tagged](https://github.com/rust-lang/rust/labels/A-const-fn) for `const fn`.
I'm currently a fan of something along the lines of fn foo() -&gt; type I: Iterator&lt;Item=u32&gt; { ... } because it separates the input types from the output types, while allowing you to use the output type in a where clause, or use an input type in the output type, such as fn foo&lt;T&gt;(n: u32) -&gt; type I where T: Clone, I: Iterator&lt;Item=T&gt; { ... } 
Even with a newtype wrapper, I still caution you about using floats with maps. It is really quite easy to have two floats that are effectively the same value but differ by some minor epsilon amount due to how floating point math works. Other data structures may be much better for this. What are you trying to index? Perhaps a space partitioning tree like a quadtree/octtree/R-Tree/BSP/etc would be more applicable?
/u/Gankro correct me if I'm wrong, but I thought you said that the new BTreeMap implementation was faster than HashMaps for certain operations? If so, was this because of the SipHash overhead? Incidentally, did you see some google engineers just released a faster SIMD-tized variant of SipHash, [HighwayHash](https://github.com/google/highwayhash/), at least for strings longer than 96 bytes?
SSE: Streaming SIMD Extensions (e.g. adding four `u32`s with one instruction), a.k.a. something LLVM doesn't take advantage of nearly as much as everyone would like it to.
Hello, Reddit (or perhaps third-party Reddit mobile app) user! It appears you have posted a link concerning the Rust survival-sandbox game. Unfortunately, this is not the correct subreddit. This subreddit (/r/rust) is for the Rust programming language. You'll want to post in /r/playrust instead. We Rust language users kindly ask you to check what subreddit you are posting to before you submit, so that your content appears in the correct place. Please refer to the subreddit sidebar, or quickly glance at the frontpage posts, to verify that you are in the correct subreddit. ^(Example AutoModerator post. I am not a bot, honest!)
Cool! Did you benchmark this against some other png optimizers? I'm on mobile right now, but I'll have a look at the code later.
If we had type-level numbers it would be much more convenient. But true compiler support for bounded integral types might still have a big advantage when it allows the compiler to optimize things such as `Option&lt;SomeBoundedType&gt;` just like it does with `Option&lt;SomeReference&gt;`.
From the main.rs I see a lot of duplication (which you could use functions against). You may also want to look into the combinator functions of Option and Result to simplify the `match` statements, e.g. `some_opt.and_then(parse).unwrap_or(SOME_DEFAULT)`.
No, I mean proper compiler support, not just special-casing for every type that might benefit from the optimization.
It's easy enough to implement `Noned` for any type you want an `Optioned` of, so what exactly would compiler support buy us? Would the benefits outweigh the cost of adding this support?
Looks really interesting. Is there a reason for using libz and not zopflipng?
Well for one, you would be able to use the actual `Option` type from the standard library, and you would have access to things such as the `iter_mut` and `as_mut_slice` methods, and being able to use `match` and `if let`. The `Iterator` trait is defined as returning `Option`, not `Optioned`, and there are various other places in the standard library where `Option` is returned (e.g. `get` on a slice). `Optioned` and similar types will forever be second-class citizens. The only cost of this support that I see would be increased complexity in the compiler and some more syntax. I have no experience with Ada but those who have always seem to be enthusiastic about the bounded integral types, and they seem to fit nicely in Rust's safety story. I'm not saying we must add this now, just that it would be worth investigating. I would certainly prefer to see this feature included rather than the `?` operator.
You mean like a trait object? struct S(Box&lt;Vec&lt;u8&gt;&gt;); impl S { fn deref_ref(&amp;self) -&gt; &amp;Deref&lt;Target=Vec&lt;u8&gt;&gt; { &amp;self.0 } }
Regarding iterator returns, those usually get optimized away anyway, and even if not the costs are negligible. On the other hand I think you underestimate the cost in compiler complexity (well, it may be ok-ish if we had native type numerals).
That initialism was deprecated when "virtual machine" stopped being an apt description of the project. It is now ["The LLVM Compiler Infrastructure"](http://llvm.org/).
UFCS: Unified function call syntax but its not similiar to D feature
I'd say it has the same control, but without ownership it's less safe. Dynamic allocation is discouraged, as in that style guide. Because Ada is focused on safety. Hence the name, `Unchecked_Deallocation()`. The style guide also shows Finalization. So it's possible to define smart pointers. Supposedly they _can_ be [made safe](https://en.wikibooks.org/wiki/Ada_Programming/Types/access#Constructing_Reference_Counting_Pointers), at least for reference counted pointers, although it involves extra refcount manipulation.
Pluggable DEFLATE might help as well (size-wise, it usually costs in compression time). PNGOut uses Ken Silverman's own DEFLATE (written for KZip) and AdvPng/AdvanceComp uses 7z's DEFLATE. For a modern recompressor, zopfli might be an option.
&gt; Ada also focuses a bit more on runtime checks, while Rust tries to be as fast as reasonably possible. AFAIK most or all of Ada's runtime checks can be removed, sometimes [with problematic consequences](https://en.wikipedia.org/wiki/Cluster_\(spacecraft\)#Launch_failure).
Take a look at [Borrow](https://doc.rust-lang.org/nightly/std/borrow/trait.Borrow.html) and [AsRef](https://doc.rust-lang.org/nightly/std/convert/trait.AsRef.html). I think they are more appropriated than Deref fore generic code.
You can also deallocate via pools and the stack. Ada allows for dynamic stack growth, thus using it as a type safe alloca. If allocation is too big, you get an exception and can recover trying to use a smaller size. 
If you don't mind using html + javascript, have a look at http://www.highcharts.com It's very versatile, interactive and pretty.
What does it mean?
I was hoping to add zopfli as an option either prior to or shortly after the 1.0 milestone. I wanted to leave libz as the default however, as I've read that libz is still much faster and I'd like the default case to be fast. I'd also love to have a DEFLATE algorithm written in pure Rust, but I don't believe any exist at the moment that can both compress and decompress.
I haven't yet, although it's been on my radar. It looks like /u/SethDusek5 posted some benchmarks in another comment, however.
Save the date!!!! More details will be coming soon, but I wanted to get on everyone's calendars :) 
http://cglab.ca/~abeinges/blah/hash-rs/ https://github.com/rust-lang/rust/issues/29754#issuecomment-192012561
That's correct.
Yeah it never really came through the way that I wanted it to.
Ada's concurrency model (tasks and "protected objects") is really quite clever, well-conceived, and clearly specified. I haven't come across anything else quite like it, at least not as part of the formal specification of a production-oriented language.
The code looks nice, and it's good to have such a crate. This is exactly the sort of crate I'd like to see as crate of the week. Question: Why JSON? Why not, say, toml?
&gt;Thanks for the benchmarks! I've been wanting to benchmark this against Optipng but have been busy just trying to get everything to work. I knew nothing about the internals of a PNG file before I started this project, so it's been quite a learning experience. Thanks for the project too. I really wanted a multithreaded png optimizer (I even filed a feature request on optipng's sourceforge a month or so ago) and then I just stumbled across this &gt;(P.S. For optipng, did you use the latest stable release, or the latest from Mercurial? They changed the optimization levels between those two.) I used the Hg release
&gt; I even filed a feature request on optipng's sourceforge a month or so ago My initial plan as well was to fork optipng and implement multithreading on it, but optipng, like many C programs, takes pride in using global mutable shared state in every place possible, and I didn't want to try to untangle that spaghetti code to make it thread-safe.
&gt; HighwayHash I implemented some (terrible, Linux- and gcc-specific) [Rust bindings](https://github.com/vks/highwayhash-rs).
Getting it wrong is kind of an in-joke :-)
:) I'm considering asking Pittsburgh Penguins hockey player Bryan Rust to give a talk ;)
Ahh that's what I've been looking for. I knew this was a problem but I didn't find a good solution. Thanks!
Oh, I was sooooo waiting for this! &lt;3
 - PR: Pull Request - MIR: mid-level IR - IR: Intermediate Representation - CFG: Control Flow Graph
Can we get an index of in-jokes too? :-P
Kennywood has the Thunderbolt, largely considered to be one of the best wooden roller coasters in the world: https://en.wikipedia.org/wiki/Thunderbolt_%28Kennywood%29#Awards_and_rankings One of the reasons it's so awesome is that it's built on the side of a hill, so you get an immediate drop, right at the start: https://upload.wikimedia.org/wikipedia/commons/thumb/8/80/Thunderbolt_roller_coaster_start.jpg/1280px-Thunderbolt_roller_coaster_start.jpg Phantom's Revenge is also considered to be one hell of a ride, but I preferred the original Steel Phantom myself.
Maybe! It was sort of intended to be a "background on CS topics that you may not know if you didn't go to school", originally. Sigh.
Not yet, that much is true.
"Static memory" can be changed in rust
It was very popular, but had two big problems: 1. No criteria for what languages to include. Including some but not others isn't a good look, as it implies various bad things. Where to draw this line? 2. It's documentation that's _ridiculously_ untestable. What happens when another language releases a new version that breaks our docs? Do we now have to make CI depend on all of those other languages too? Whose job is it to track every other language and keep them up to date? I know that I can't do it. So sadly, it had to go :(
Ahh, right! Thanks. I will try to go back and do this again to fix it...
I've previously pointed here: http://jakegoulding.com/rust-ffi-omnibus/
Maybe, maybe not. It's hard to say. I haven't gotten that far in the new book yet, am going to tackle that question when I get to the FFI chapter.
Truth, but I usually dont :(
I have been thinking about writing up this for a while, since few other people than me knows how to use Eco. Enjoy!
Oh cool. Thank you.
It's a mild annoyance, if anything. The mods should have AutoMod configured soon, and it can get to submissions before anyone else sees it.
We will sing you happy birthday if you want!!!! :) :) :)
Searching for 'rustlang' has served me well. Alternatively, asking on IRC ;)
Interesting, I wish serde-json had that. But to be fair it does allocate Vec and BTreeMaps. It should have a way to lazily unescape the string references by returning a Cow or something. 
When 1.0 comes out send me your address! Your work on this crate is awesome, someone must send you a medal or something.