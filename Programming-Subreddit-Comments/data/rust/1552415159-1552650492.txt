Actually, [here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1d5ddd605ca7022e7580e6d7116c9a1a)'s a version that mostly just takes the values of the temporary vec
Of course *that* segfaults. Change the unnecessary lifetimed reference `u64` to `*const` and see what happens though.
`safety-contract`
"Assuming" is the key word here
You could run that bit of code on CI for the next five years and I guarantee you it would always do the same thing because of the specific types the struct contains.
see my other comment
Second part: https://youtu.be/dJoZyMVrJrA
As an example of how things like this can change without your code changing, Rust recently changed its default allocator from jemalloc to the system allocator. Any changes in these kinds of behaviors could affect what you're doing. &amp;#x200B; But in the end, again, that doesn't matter. It's UB. It's still UB, even with all of that, and is always going to be UB.
async doesn't make database access faster though
Check out these two playground examples: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cdb8746f88e1357b5ccec3b5f0ddaa09 https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=746d08680b5523fca56cdca1a8f072e5 Exact same code, but the behavior is different in debug mode and release mode.
Please see [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=50c7cbae237d6ec3f6d7d577b879b2d0)
I understand things just fine, thanks. Did any of the examples *I* posted segfault?
They can do literally anything. That they don‚Äôt segfault today doesn‚Äôt mean that they won‚Äôt open up nethack and start playing a game tomorrow.
um.. did you see mine? if any of the types in your struct were pointers, and you used a temporary vec, then used another of similar size, the pointer could not be initialized to null. Impossible for you to detect if a value is set or just random garbage..
Doesn't mean they're right if they don't segfault.
Continuing to work on my Secret Crate Data Mining Project. Maybe someday in a few weeks it will have enough useful info to be worthy of an actual release?
No they won't. I don't understand why people in this thread are seemingly trying to celebrate how things that can in reality only physically really work one way are somehow volatile and unpredictable in the context of Rust.
That is actually a real-life example: https://feross.org/gcc-ownage/
Here's an example of values not being initialized to 0: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=018c33c4c785489c67f5037fd2a0cd88 In it, I initialize another vec which requires the same sized allocation, fill it with 42s, and drop it. When we allocate the vec of `MyStruct`s the allocator reuses the just-freed memory from the first array. When I run the code, it outputs 42 MyStruct { a: 707406378, b: 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000014260258159703532, c: true }
The chart measures requests-per-second, ie. throughput. Async database access definitely does increase throughput.
Yes it does.
Pretty sure you're trolling at this point lol
Open your example and click Tools-&gt;Miri button, and read what it says. Your code is not valid Rust.
What happens when someone implements the trait with a different type than the default associated type, but the default function?
yep, he seems to deliberately be ignoring minor variations that break his assumptions.
You cannot. You if you change default associated type, you must reimplement all methods using that associated type. Thus if you change the NodeRef you must reimplement traverse.
Can static arrays be initialized with `cont fn` lengths? I've never checked. Feel like that would solve the kind of use case OP has in mind though.
I tried to generate things like "arg1 must be less then arg2". This works, but the string is messed up then. #[safety(ne(new_size, layout.size()))] converts to (note the whitespace) /// `new_size` must not be equal to `layout . size ( )` 
 1. You set the capacity to 1000 and thus allocate 1000 integers without initializing them. 2. You then read the first integer. The kernel sees that the memory page was never written into, so it just returns 0. 3. You then write some value into the second integer. The kernel loads the actual non-zeroed memory page to do this. 4. You then read the first integer again. Now the memory page is not completely uninitialized, so the kernel does not simply return zero, but the value in actual memory which is something non-zero.
Like this? ```rust const fn len() -&gt; usize { 5 } pub static ARR: [i32; len()] = [1, 2, 3, 4, 5]; ``` that compiles just fine. I'm not 100% sure what it has to do with the OP though.
You're wrong. Several allocators go 'undefined behavior? Any sane language doesn't care, skip'. And it's appropriate for rust too (and faster, which is why they do it).
async is not faster, just google it, and it could actually be slower.
&gt;[If I'm going to choose my first(somewhat) programming language between C and Rust and learn it by myself](https://www.reddit.com/r/rust/comments/aptix4/c_or_rust/?st=js1xbna0&amp;sh=de066042) &gt;P.S. I've got some elementary ideas of programming, in Java. Posted 28 days ago. 
I know what will happen. You'll have an initialized raw pointer. If you try to dereference it, you'll probably segfault, but the Debug output of a raw pointer does not attempt to dereference (unlike a "borrow" which Debug does attempt to dereference. Also, the fact that *any* code at all can segfault should show you that this is not a good idea. For this reason, Vec::with_capacity_and_len(), if such a function existed, **must** be marked unsafe. I'm answering your question of why/how is it initialized, it's because all you do is reserve space in memory. That memory might contain literally anything, but in many cases it will have been zeroed at some point before you reserved it. That zeroing is absolutely not going to happen every time, so there is no way you can rely on it to provide an "initialized" value. I'll give you that your specific example is not unsafe, since the data is valid regardless if the actual bits. But as soon as you move onto something more complicated, this can become a dangerous assumption.
No I'm not. Did I ever even say I actually planned to *read* from values without assigning to them first? Additionally, the hypothetical function I'm suggesting here *would* logically safely initialize the values.
I think this `#[safety]` is a great idea. It certainly has some elements of DBC but IMO a general DBC feature like this would be yet better still. [Issue 1077](https://github.com/rust-lang/rfcs/issues/1077) refers to this BTW. But we should never let perfect be the enemy of "good enough," so I'd rather proceed with something like `#[safety]` rather than keep waiting for a general DBC.
No. Sorry for suggesting a minor ergonomics change, I guess.
You're talking about speed and the other commenters are talking about throughput. These are two entirely different things. As an example, suppose operation `x` takes 4 seconds to complete an IO bound operation. Perhaps you can optimize operation `x` so that it only takes 3.5 seconds. Over the course of a minute, on a single thread you can now process 17 requests instead of 15 requests. This is an 11% improvement. However, suppose you use `async` to do other things while waiting for the IO to complete. The operation still takes 4 seconds but you can run `n` of them at a time on a single thread. For even a small value of `n` like 2, this is a huge throughput increase: 15 requests/minute * (n = 2) = 60 which a 100% improvement. ---- &gt; I thought diesel's greatness was mostly compiled away (macros and typing). Is there dynamic dispatching going on? There's thread contention/blocking going on. The only thing this has to do with Diesel is that it isn't async yet. The web server spins up some number of worker threads (often scaled to how many cores are on the machine). You have many more requests coming in though. When Diesel executes a SQL query without using async, the thread becomes blocked and can't work on anything else until the query returns. This means that web requests to the web server are starved for threads and can't be processed. This article talks quite a bit about this concept and while it's C#/.Net in the article, the concepts pretty much apply to most `async` implementations: https://medium.com/@frederikbanke/improving-scalability-in-c-using-async-and-await-f97af1466922
I know it's a lost cause but I'm posting this anyway: https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=d65162bfb005590f54f1c3a756c75a0e Run it a few times and check out all the different permutations of "zero" that you get :D
Asynchronous I/O allows for a higher throughout by potentially not blocking other requests. In this case the sync actor could be blocking other requests. You're right that raw speed of running code is not affected but when it comes to requests per second it is indeed a factor. That potentially very fast code is waiting for the OS to get back to it with the data it's requested, even if other data is ready to be handled.
There aren't many examples, but actix has no problem with any of those things. Its flexible enough to allow you to organize complex applications the way you want. Routing can be arbitrarily complex, injecting db connections can be done with middleware (I am using it with R2D2 for connection pooling and it works great). It is very well designed framework.
I'd be interested in seeing a version using `diesel`'s `r2d2` integration to pool and get connections instead of the Actor based `DbExecutor` way. 
A small use example in the README would be nice.
Come on, don't be discourage, one day someone will make a bot to reserve all crate names and the system will change =)
Having done some UI work in the past, some thoughts: 1. There's an assumption that recalculation is bad, and I agree that once your rendering logic becomes sufficiently complex it would be. But right now, is it? And more importantly, do you have concrete plans to make it more complex later? 2. It sounds like you're essentially implementing a view-model over your Nonogram, and the core problem you're facing is that there would conceptually be some cached information based entirely on some core state. Your suggestion to "re-define all the public methods on Nonogram and pass them along" probably wouldn't be bad here -- I think in this case it's just an application of composition. I can't imagine there are more than a handful of operations you'd have to define, and if you do this you can take one of two approaches for rendering later: * Eagerly update the cached state and always be ready for a render * Mark a flag when something gets mutated for a lazy render later in addition to performing the actual state manipulation.
Well gee, I sure am glad that Lord High Gatekeeper /u/dreugeworst has declared this troll Authorized For Downvoting. Wouldn't want to do that kind of thing without permission. How about you stop telling people how to act when you don't even understand the situation, hmm?
&gt; You're talking about speed and the other commenters are talking about throughput. These are two entirely different things. They become the same thing when you have a bottleneck. Actix has a number of sync actors, these sync actors are running the diesel queries against a sync database. Being able to put these diesel queries inside async actors is not going to speed up anything. The speed of which the database can return data is always going to be the bottleneck, not whether its sync or async.
Rather than stuffing code inside the attributes, I'd love to see something like this: #[safety] unsafe fn realloc( ptr: *mut u8, layout: Layout, new_size: usize, ) -&gt; *mut u8 { /// `layout` must *fit* the `ptr` safety!(ptr as usize % layout.align() == 0); /// `new_size` must be greater than zero safety!(new_size &gt; 0); /// `new_size`, when rounded up to the nearest multiple of `layout.align()`, must not /// overflow (i.e., the rounded value must be less than `usize::MAX`). safety!(); // ... } The `#[safety]` proc-macro attibute will look inside the function's body, find the `safety!` calls, convert them to assertions with their docs as the panic messages and add these messages to the _Safety_ clause in the doc.
It's not wrong to suggest a minor ergonomics change. But suggesting something that is actually unsound and refusing to acknowledge the myriad of proofs that it doesn't work is kinda an issue.
This is excellent! However, I seem to get garbled function names like _ZN4core5slice29_$LT... In both debug and release modes. The same happens when I run with vanilla perf and flamegraph though. Any suggestions on how to clean these up?
I was not talking about procedural macros at all, but you just gave me an idea: I can use `include_str!` or `include_bytes!` to read from a file or fifo and therefore rely on invocation order.
The examples you posted all involved you printing the uninitialized values, which counts as reading from them, and you defended those examples as being sound just because they "worked". So yes, you did say that a lot of times. If you really want something that safely zero-initializes a `Vec&lt;YourStruct&gt;` to the desired capacity, this should do the trick: https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=064cac8b1762621a538bfd717cb5d8d0
The speed the database can return data is a latency bottleneck, not a throughput bottleneck. I mean, presumably there's a throughput bottleneck as well, but at a much higher rate of response. If actix workers are written in an async (non-blocking) manner, they can handle dozens or hundreds of concurrent requests on a thread, but if the db query blocks, they won't be able to hand off control to another worker, and the non-blocking call suddenly becomes blocking, and the parallelism is suddenly limited to one worker per thread. 
You're right, this is essentially a view model over the Nonogram. &amp;#x200B; The data I'm suggesting I cache is something that will never change once the Nonogram is created (it depends on the "hints" that never change). The data that I need to display is the contents which are expected to mutate often. &amp;#x200B; It's true that the computations aren't super complex, but I figured I would take the challenge to do it "right" so I can learn some lessons for the next time I face something like this. &amp;#x200B; Right now my idea is to have the NonogramPrinter own a Nonogram and expose it via a public method. Then, if you want to do anything on the Nonogram, you can simply ask for it and call the method on what you get.
Tokio has included an Async await feature flag for months. 
Since your example can be trivially changed to segfault or corrupt memory, I think it is easy to agree that even if `with_capacity_and_len` were to exist, it would have to be unsafe.
Ah, okay, so none of the information in the `NonogramPrinter` will actually change? That does change things a bit. I'd say there's not any real issue with coupling to the `Nonogram` interface -- from the sound of it, you're trying to make this generic.
maybe have the comments be an additional arg to the macro call instead. having magic comments always seems hacky to me
Using `//!` for comments apply the comments to the outer entity. So using `//!` for comments at the very beginning of an .rs file will apply it to the module represented by that file.
&gt; They become the same thing when you have a bottleneck. The bottleneck is that the thread is waiting on a response from the db as such the web server is waiting on things to do. &gt; Actix has a number of sync actors, these sync actors are running the diesel queries against a sync database. Being able to put these diesel queries inside async actors is not going to speed up anything. The speed of which the database can return data is always going to be the bottleneck, not whether its sync or async. I'm not sure why you think this. Using `async` correctly means that the thread will become available to do more work. Actix's sync actors are nice but the framework will not spawn an infinite number of threads to handle work. To keep scaling beyond a certain point, you can't just spawn more threads, you have to use green threads.
Nope, sorry. This does invoke some of the concepts i remember, but the tutorial you've linked is more to the point, and is very Rust-oriented. Also, it doesn't touch the topic of type mismatch errors.
Are we so sure that Virtual DOM is the answer to everything web that we want to bake it in?
thank you. I had it. but made the //! a /// because I had #[forbid(unsfae)] before that, and didn't know why the compiler complained. 
I remember reading that it provided support for async await without true futures 0.3 support, but I can find where it says that. What I did see is that it uses `#![feature(rust_2018_preview)]` despite rust 2018 already being stable (not to mention last year).
 &gt; Using `//!` for comments apply the comments to the outer entity. Could be noted that `//!` are called inner doc comments because they apply to the entity they are inside of.
Thank you. The official name was escaping me.
Libraries can't just switch to 2018. It would be a breaking change.
The intention is definitely not to bake in a virtual dom library that everyone is required to use. It is an explicit goal that one should be able to pick and choose individual crates from the toolkit. I do believe that a virtual dom library (or interface/trait/something) for a virtual dom library should be *available* however.
&gt; Actix's sync actors are nice but the framework will not spawn an infinite number of threads to handle work. It just needs to spawn enough sync actors to load the database
The query it's running is a very simple `SELECT` and there are no queries updating the data so there won't be any locks taken and the data will never be evicted from cached memory. Given the hardware techempower runs on, I'd be very surprised if the DB server is going to be maxed out. 
&gt; The speed the database can return data is a latency bottleneck, not a throughput bottleneck. Not sure how you can make that claim. &gt; If actix workers are written in an async (non-blocking) manner, they can handle dozens or hundreds of concurrent requests on a thread, but if the db query blocks, they won't be able to hand off control to another worker, and the non-blocking call suddenly becomes blocking, and the parallelism is suddenly limited to one worker per thread. actix has async and sync actors. I presume that the sync ones are handled by threads. Nothing needs to block unless you exceed the number of threads. If you match up the number of sync actors with the number of database connections I dont know how async will help you.
\[released as \`safety-guard\`\]([https://crates.io/crates/safety-guard](https://crates.io/crates/safety-guard))
maybe you can answer that too. I am using external crates but only one is relevant for the user of the library. -&gt; bincode::Result. It's facing the implementor of the lib. Is there a way to include external crates that are either used in formal arguments or return types, but not other external crates that are used on one of the dependencies?
I just noticed that, in many cases, you can call a function in Rust code and choose not to do anything with the value(s) the function returns. That is, if you have a function: `fn printfoo()-&gt;i32 {` `println!("Hello world!");` `43` `}` both of the following statements will compile and execute without errors: `printfoo();` `let bar=printfoo();` Is this generally the case? When will not processing a function's return values throw an error in Rust, either at compilation time, or during execution?
Here's a minimal pure Rust wasm-bindgen example: [https://github.com/anderejd/wasm-bindgen-minimal-example](https://github.com/anderejd/wasm-bindgen-minimal-example)
I thought running `cargo doc` would automatically generate docs for the dependencies too. So that you could click the return type you described and see the documentation for it.
&gt;I'll give you that your specific example is not unsafe, since the data is valid regardless if the actual bits. Yeah, I know. Did my initial post not make it clear the sort of use I was talking about for this kind of thing? 
There's an attribute `#[must_use]` and corresponding lint that generates warnings or errors at compile time. https://doc.rust-lang.org/reference/attributes.html#must_use
Why doesn't Rust just... not segfault there? Couldn't it print the address like for regular pointers?
I'm not attempting to actively develop wearte. Since yarte is in violation of its licensing terms I occasionally pull the current source yarte and merge it into my package. That way users can actually use the crate without worrying about DMCA takedowns. Making your repo private doesn't stop me from getting the source since your uploaded crate has all the .rs files already.
/r/playrust
A lot of people in this thread seem to think pointers are WAY more complicated than they actually are.
"It works properly" does not prove "it is defined behavior." The Rustonomicon specifically cites this situation (use of values that are not logically initialized) as something that falls within the boundary of undefined behavior, even though that exact boundary hasn't been specified yet. Later versions of the compiler may break your code however they wish, even introducing exploitable vulnerabilities.
So when you get a stabilization RFC this usually means that the feature is ready to stabilize. Sometimes you have an RFC that proposes a feature, it gets implemented, and works fine, and then stabilizes without another RFC after we're sure it works. This is usually what happens. Sometimes you have an RFC that proposes a feature. It gets implemented and through experimentation changes a lot and is now not the same thing that got proposed. Because it was implemented and shipped in nightly people _have_ had a chance to test it. In such a case it's nice to re-RFC the new design and say "here's what works, we want to stabilize it". Typically once such an RFC lands, it means the feature is ready to stabilize (there may be loose ends to tie up).
it does, but then it does for all of them. for instance I am using rand, but the it has no relevance for you for instance if you would use the library. Therefore, I don't want it in the docs. However, if you use one of functions, that return bincode::Result it would be nice to have that dependency listed in the crate. I know ```cargo doc``` and ```cargo doc --no-deps```. It*s all or nothing.
to demangle the Rust symbol names you can use [rustfilt](https://crates.io/crates/rustfilt)
The simplest case would just be to use \`failure::Error\`. &amp;#x200B; See the \`read\_toolchains(...)\` example here: [https://github.com/rust-lang-nursery/failure](https://github.com/rust-lang-nursery/failure) &amp;#x200B; If you want to make it easier for the caller to figure out exactly which type of error happened, then you can implement your own error type like ToolchainError on that page.
It really doesn't matter what the runtime behavior is at all, the issue is the compiler. The `Vec::with_capacity_and_len()` function you want will give you a `Vec` which is conceptually backed by an allocation of LLVM `undef` values. If you then attempt to read (and printing very much counts as reading) *any* of those values, you've invoked UB and LLVM is allowed to do anything it wants.
People only say benchmarks are BS when their preferred thing is losing in them. "Benchmarks are BS" is BS.
Hah! not sure I appreciate the sarcasm, but I suppose you have a point there. Will remove the bits about downvoting, it's entirely besides the point of the discussion anyway.
Thanks for this! I was wondering if you had a recommended way to generate a flamegraph for the test suite of a library? I was hoping for a command like \`cargo flamegraph test\` My current workaround is to first run cargo test, take note of the output binary that it generates, then invoke \`cargo flamegraph --exec ${path to binary}\`. Any tips on improving that workflow?
Thanks for this. Downloaded the 3-10 nightly and everything appears to be working. üëç
I'm not sure how to get the behavior you're after. Run cargo doc --help and experiment with some of the options there. I hope you find what you're after. 
`perf` is _supposed_ to be doing its own demangling, but it doesn't always work very well, especially on Rust symbol names. I've opened an issue on inferno for this feature [here](https://github.com/jonhoo/inferno/issues/87).
Because it's UB, it's not a legal Rust program. It could do anything. Including not segfault. &amp;#x200B; Since it's not a legal Rust program, there's not a lot of reason to make it have any particular behavior.
&gt;I'll give you that your specific example is not unsafe, since the data is valid regardless if the actual bits. That's not correct; it's UB regardless.
Support for more targets is already [a goal](https://github.com/ferrous-systems/cargo-flamegraph/issues/10). I mentioned your desire for support for tests [here](https://github.com/ferrous-systems/cargo-flamegraph/pull/11#issuecomment-472177641), though I'm sure a PR would be highly appreciated!
Is that because you consider a minimum compiler version part of your contract? Because otherwise, I can‚Äôt think why it would be.
I'll se if I have time to change it out and measure the impact. Would be interesting to know for sure what impact it will have. The overhead of `lazy_static!` should be very small compared to passing a reference if any at all. I would rather start by caching the random number generator and pass that as a reference instead: [https://docs.rs/rand/0.6.5/rand/fn.random.html#examples](https://docs.rs/rand/0.6.5/rand/fn.random.html#examples). But if if you want to try changing out `lazy_static!` and pass it as a reference instead, try it out and let us know the result :) You can fork or clone the code here: [https://github.com/cfsamson/pixar\_raytracer](https://github.com/cfsamson/pixar_raytracer)
Yes
üëç
What's the difference between `safety!` and `debug_assert!`?
&gt;(that can in reality only physically really work one way) So, at the level of machine code you are almost entirely correct. Each instruction has a deterministic effect, even if you do something that's not allowed. Read from linear address 0 on x86? `#GP` is invoked and the OS generates SIGSEGV or whatever. The exceptions to determinism are pretty obvious: hardware bugs, undocumented instructions, new instructions, chaotic timing effects between threads, etc. However, there are conventions for how the CPU must be used. What happens if you forget to preserve `rbx` across function calls? I dunno, maybe it works, most likely it doesn't. What happens if you overwrite the procedure linkage table? Geez, that'll a mess. Overflow a stack allocation? Well, let's just hope that nobody decides to use "return-oriented programming" for nefariously purposes. With assembly you hope that if your code goes off the rails, it'll at least crash before something bad happens. But you're almost always dealing with a situation that's too complex to make guarantees instead of hopes. Because you don't understand the rest of the system perfectly, it's hard to tell when you have to write defensively and when you can rely on the rest of the complex system to function properly. Anyway, high level languages *don't* work on physical hardware. They run on something called an *abstract machine*. The behavior of this AM is not defined for all situations. Sometimes it really is "compiler's choice". Sometimes the compiler has a constrained choice (multiple local variables bound in the same statement are dropped one after the other but the compiler chooses the order) sometimes it's literally allowed to execute NetHack as a joke. Reading uninitialized memory is most often destructive when that memory belongs to the stack. The compiler is within its rights to assume that particular code path *will never happen* and therefore the most efficient way to implement it is to simply *not*. If you read an uninitialized value and then write it, llvm can omit both instructions. If you read an uninitialized value and then branch on it, llvm assumes the test doesn't happen, that subsequent instructions don't happen, and that preceding instructions don't happen as far back as the last branch that is actually well-formed. Currently rustc isn't clever enough to do this for heap-allocated memory. Currently. But the Rustonomicon specifically says to expect undefined behavior, so there's some potential for the compiler to change. The exact reasons why UB is allowed to exist are rather complex and I don't understand them well enough to explain them briefly. The nutshell is that a 100% watertight abstract machine has been tried many times and it tends to force the compiler to generate extremely pedantic code that doesn't perform as well as when compilers are given a little bit more freedom. In fact, that freedom is a big part of how optimizers actually work. Detecting and propagating undefined behavior makes it possible to detect and prune code that is not necessary in an appropriate situation. The best approach may be to design a language that prevents UB caused by programmer error but allows and copes with it during optimization steps. Rust is closer to striking that balance than any language yet. Until you use unsafe. 
Is your mind going to be blown if I told you that the async db example uses a single async actor?
&gt;So the "path" field accepts either a package or a path? This didn't seem obvious to me with how packages and paths are shown in the UI. It accepts a regex matching a path, but the paths we match agains actually look like `kicad_parse_gen/7.0.2/src/footprint/data.rs` and not just `footprint/data.rs`. This would be nice to clarify. Issue: [https://github.com/aelve/codesearch/issues/244](https://github.com/aelve/codesearch/issues/244). If you want to specify both, try something like `package_name.*file_name`. &amp;#x200B;
This has already happened and they've rolled back only the names created by that bot
It's very much intentional, just not in the way you think it is. An operating system when providing new pages to a program zeroes values in those. This prevents the program from reading others' program memory with passwords and other secrets once they get closed, which would be a security issue. Note the keyword "new". If a page is deallocated by your program, and then reallocated by your program, the memory won't be cleared, as the owner is still the same, and there is no need to clear the memory. If you are going to assume all allocations are zeroed, your program will explode when it turns out to be to not be the case. So you may want to not do that.
The Tide link 404's.
It's not switching. It already is 2018, but it's still treating 2018 as an unstable feature that needs to be enabled.
Yeah, thanks. We [fixed it](https://github.com/rustwasm/rustwasm.github.io/commit/04f1292b8cace5870bbe6467ccd1dbcec6d8014f) but gh-pages is taking forever to deploy today.
Historical note: Rust used to have this in the compiler as typestate. I can't find any introductory blog posts on it anymore, but https://pcwalton.github.io/2012/12/26/typestate-is-dead.html is around still.
I guess I'm just not sure I agree with the idea of not avoiding problems that you know you could avoid.
Sure! This is a good reason to make as much behavior well-defined as possible. That's why safe Rust contains no UB. But once you get into the realm of unsafe, full behavior can't always be fully specified. And that's the case here. At some point, something cannot be specified, and you run into this problem. 
await/generators make self-borrows practical to use in async code, and generators couldn't be realistically implemented outside of standard library. You could in theory use `rental` crate for self-borrows, but combining `rental` and `futures` seems like a recipe for having way too many problems.
\&gt; something extremely simple that many other languages manage to do safely is somehow impossible and also dangerous to theatrical extents in Rust It's extremely simple and safe in Rust as well. You write `vec![MyStruct { a: 0, b: 0.0, c: false }; returns_a_number_somehow()];` and you're done.
You were arguing that Vec::with_capacity_and_len() should be a function availabe to end users, and that it doesn't need to be marked unsafe. So that's what I was arguing, not necessarily if your *specific* use case is okay or not. I just used your specific example and expanded on it because it was really easy to create an obvious memory problem.
Ho interesting, maybe a bot making bots that register a crate each? (I'm kidding) Anyway, a lot of people agree that there is a problem but the politic is that it's not. It's not a bug it's a feature? If at least the politic was that the current architecture is flowed and a new one is in the making. I'm curious about all this, I saw a comment about a discussion before the release of crates.io maybe I'll understand after reading it.
&gt;No perl or pipes required &lt;3 This means a lot to me! I get unreasonably annoyed at having to run perl scripts.
Actually, collecting into a `Vec&lt;(Vec3, Vec3)&gt;` my time went down from 5.14s to 4.39s, so that was definite improvement!
&gt; That's not correct; it's UB regardless. I know you are technically correct, but I hate this argument. It is pretty clear that if you call Vec::with_capacity() with any non-zero value, Vec is going to allocate. Therefore Vec's pointer will point to owned-but-uninitialized memory, every time, no matter what. Only exception would be if the allocation fails, but I'm pretty sure that is handled by std, not user-written code. Knowing this, despite what some documentation says is UB, *I* know that accessing this memory using types where every possible bit-pattern is a valid value of that type will not cause UB. Obviously the values I get can be anything, and if I take those values and use them elsewhere, I'm propagating that unknown value around, but there is no possible way, given the rest of the program is safe code, to cause unsafe or undefined behavior. All that said, looking at the original example again, there is one big problem: a `bool` type is *not* valid for every possible bit-pattern, so I take my original statement back - this example is NOT okay.
I'd wager it's an extra layer of versatility. Imagine a scenario where you want to test safety invariants without debug asserts/ in a release build which may alter runtime behaviors.
UB is all about technical arguments. This does cause UB, even if you think you know the codegen the compiler produces. You have laid out what behavior you believe this particular compiler has when this particular form of UB is encountered, and you may be right, but that doesn‚Äôt mean there‚Äôs no UB, only that you know the particular code of this particular compiler revision. That can change at any time. (And yes I forgot about the book too. But as I said, that doesn‚Äôt matter because the UB happens before we even get there.)
How good does it work for you? I have exactly the same problems as op, something it works, but most times its not that goos
Thanks!
The real reason is that `impl&lt;'a, T&gt; Debug for &amp;'a T where T: Debug` Basically, Debug is implemented in std for any "borrow" where the underlying type (T) is also Debug, and that implementation explicitly dereferences &amp;T. The Debug impl for raw pointers does not dereference *T, since dereferencing a raw pointer is unsafe. In safe rust code, it is not possible to create an invalid &amp;T, so it is always safe to dereference.
Actually, I take back my original statement. Your original example has one flaw that prevents it from being "okay" despite what everyone else here has said. The flaw is that you are using a bool type in the struct. `bool` only has two valid bit-patters, 0x0 and 0x1. Therefore, if your allocation does not zero the space used by that bool, the bool may contain an invalid value, and from there, who knows what happens. (If std was programmed assuming `bool` is always 0x1 or 0x0, then unless you're willing to comb through all of std, you have no way to know what happens when a bool is not one of those values)
Make it a static global.
Could you write arbitrary size arrays there? How would you guarantee that it wouldn't write over other data? Or would you reserve a large buffer and support arrays up to that size?
Thanks for putting this out. Here's hoping that it either gets adopted or inspired others to make versions that are more easily adopted.
&gt; even if you think you know the codegen the compiler produces Well, if rustc ever produces code from the given example that doesn't both allocate and then read from the allocated memory, then at that point I probably won't be using Rust any longer, as I would consider that wholly illogical and having to deal with that kind of weirdness isn't worth it to me.
You can not deal with that weirdness: don't write code that invokes UB! (This is also true of C and C++ compilers, even more so since there's so much more UB.)
`safety!`'s panic message gets into the function's documentation, in the _Safety_ section.
I‚Äôm not exactly sure what you are unhappy about. 
These comments only affect two things: 1. The documentation - which is exactly the purpose of doccomments. 2. The panic message - which I consider as another form of documentation.
I see that \`Futures\` just got accepted into the standard library. Once Async/Await gets finalized too, does that mean there isn't much of a purpose for Tokio? Or is Async/Await just syntax which I would use in conjunction with Tokio?
Pin makes self-borrows practical *period*. Generators are a fantastic DSL built on-top of it. Async await is just fairly simple transformation to take advantage of the benefits provided by generators and Pin. Personally, I think generators are far more important than async await and I'm somewhat disappointed that there still isn't any integration with iterators. (They're a strict superset. Iterator&lt;Item=T&gt; = Generator&lt;Yield=T, Return=()&gt; + Unpin) In practice, they could conceivably be used for many forms of suspendable and monadic computations. People are so fixated on async/await that the features they're built on top of don't get the attention they deserve. For example, placement new was abandoned a while back, but with Pin it would now be incredibly useful to initialize Pin values without unsafe. Same with clone_from without clone.
It's enabling a flag that does less than nothing. If it weren't for cargo hiding the warnings of dependencies, anyone who uses it would see a warning about enabling an already stabilized feature. Simply removing the feature would be trivial, but nobody seems to care.
I wrote that in a bit of haste, and was definitely oversimplifying things. Actually meant to delete it but somehow didn't, I guess. &gt; don't write code that invokes UB! The problem (I think) is that it's extremely easy (intuitive, even, I mean just look at OP) to write UB in an unsafe block without realizing it. Extending that, there more than a handful of situations in Rust where the cost for performance is dealing with unsafe.
It's all good! &gt; The problem (I think) is that it's extremely easy (intuitive, even, I mean just look at OP) to write UB in an unsafe block without realizing it. Yeah, I mean, I was being a bit glib. I agree that it's easy to accidentally write UB, but once you find out it's UB, it's not hard to decide to change the code to do things in a well-defined way, that's what I was getting at.
It looks great! But darn it, us game engine devs will someday rebel if everyone keeps writing their own game engine instead of using ours! If only making WebGL and wasm work weren't so darn *easy*... :-p
Yay! I was thinking of something like this the other day, and I'm glad somebody is trying to tackle it. 
Is Gloo affiliated with the Rust or WebAssembly teams?
Using the doc-comments in the function would work, however, the compiler will probably emit a warning: safety!(true, "a, b, c"); ------------------------- rustdoc does not generate documentation for macro expansions 
Thanks for the actual explanation.
Futures and async/await are just basic building blocks for making asynchronous functions. Tokio is a full-fledged framework providing stuff like task scheduling, event dispatching, asynchronous network sockets, and more.
Thanks for your work on this. You may want to look at what the author of [Display As](https://github.com/droundy/display-as) does in that templating crate. It also makes it possible to intermingle pure Rust code within the template instead of relying on a DSL. I also think it's worth keeping tabs on the approach being taken in the new [Ramhorns](https://github.com/maciejhirsz/ramhorns) templating system. It loads and processes templates at runtime, but it achieves rendering speeds comparable to, and sometimes better, than pre-compiled template systems.
Works great! And keeps getting better. I also happen to prefer JetBrains ides. 
A bot making bots that register names?..
&gt; The kernel sees that the memory page was never written into, so it just returns 0. Is this a thing a Kernel will actually do in practice? This seems so bizarre to me. Does that mean the Kernel maps the desired page to a pre-zero'd frame in physical memory? How would the kernel know when it needs to be swapped with the real backing-memory? Is this an attempt to reduce swapping, or is there some other motive behind it?
Amazing! I can see that having one of the core tools in performance optimization more neatly integrated will very likely improve the performance of the ecosystem as a whole! Setting up the traditional flamegraph for a repository and maintaining a set of scripts to invoke it has always been a bit of a hassle. With it now only requiring a simple local crate install the barrier to entry has become a lot lower!
"Gloo" looks like a Seussian word (this is not a criticism)
ah yes, the ol' "typestate and sigil" days of rust
You want /r/playrust.
Thanks fam
It doesnt improve anything and it may harm performance, since a sync actor is a single thread, so at most you use one connection per actor. That means creating connections when starting the server and never bothering again. That said I use a pool in this situation with a connecion per thread anyway.
I may extend/or replace it with this, but not with doc comments, but with optional string literal as last parameter (other than string literal won't work as it's not known at compile time to put it on the docs). Two reasons against the doc-comments: - warning for doc comments on macros (can only be disabled for whole function) - doc comment above empty macro feels weird
(Slightly off-topic, but I think this is cool.) My perspective is that those limitations are temporary and there's no _real_ limitation preventing the use of WSGI in systems like Lambda‚ÄîI think the current lack of support for things like WSGI is prioritization &amp; staffing. It should be possible to have a Python runtime that uses the [Lambda Runtime APIs](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html) (which the Rust runtime makes use of!). The rest is up to language/ecosystem, which requires somebody to drive it. Regarding lock-in: cloud providers make _significantly_ less money on serverless platforms than things like VMs or databases. Speaking for myself and prior experience, serverless platforms are platforms that cannibalize other business units _and_ were designed for and by _extremely_ operationally conscious engineers. I think there are _far_ better ways to get customers to stay with a cloud provider than twisting their customer's arms.
You can run into trouble even if all bitpatterns are valid for an uninitialized variable. I posted this example in another comment showing how branching on the value of the uninitialized `i32` member can give you a zero that's also not zero: https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=7d4064fb124a6a29eb24b8f41f4aff94
Thats the point you need threads using CPU to handle each connection while with async actors they are forgotten and don't use resources when idle (except ram). You can't debate that async db connections improve throughput by making connections that are waiting basically no cost.
&gt; Similarly, OP could use one of the many non-UB ways to get the behavior they want. It's actually easier! I get the feeling they're talking about really high performance use cases where actually filling the Vec with something that isn't the final values is undesirable, though. Like, they just want to allocate the Vec, and assign things directly into the indices exactly once each.
It's a bit heavyweight, but proc-macro authors are typically expected to reach for [syn](https://crates.io/crates/syn) for anything non-trivial. You would use `parse_macro_input!()` to parse the tokens to a `LitStr` which has a `value(&amp;self) -&gt; String` method. This does have the advantage of good automatic error reporting, as `parse_macro_input` will issue a compiler error with proper spans if it fails to parse the string literal. The default features of `syn` are good enough for most proc-macro cases, but for this you can turn off all of them but `parsing` and `proc_macro`. For 2018 edition: extern crate proc_macro; use syn::{parse_macro_input, LitStr}; // or however your macro is declared #[proc_macro] pub fn my_macro(tokens: TokenStream) -&gt; TokenStream { let lit_str = parse_macro_input!(tokens as LitStr); let str_value: String = lit_str.value(); // the parsed value of the string literal } For 2015 (only the import style changes): extern crate proc_macro; #[macro_use] extern crate syn; use syn::LitStr;
Because it would break Tokio‚Äôs minimum supported Rust contract. 
Sure! You can do that with ptr::write, without UB as well. This use case is why set_len exists in the first place!
&gt; You can't debate that async db connections improve throughput by making connections that are waiting basically no cost. I really dont know what you are talking about. You do not increase throughput of a limited resource by making it async. Here's a thread that makes a lot more sense then the comments in this one https://www.reddit.com/r/rust/comments/814sfq/whats_the_state_of_async_db_access/
Yea, I saw that right after I posted, and realized shit makes no sense. I'd love to know what specific mechanics cause that to happen (is it optimizations? something with the OS/kernel? some complex combination of things?) and if whatever causes it is worth whatever benefits it presumably has.
&gt; My personal policy is to use stable for libraries I intend to publish on [crates.io](https://crates.io), and nightly for everything else. Fair policy, but most of my Rust code *is* on Crates.io, and that happens to include the crates that would most benefit from std futures, so I have felt stuck between a rock and a hard place for some time on this issue. I don't have what feels like enough free time to maintain nightly and non-nightly branches of a library if the changes are drastic enough for conditional compilation to handle.
Yes. I use the SDL2 crate and the specs crate but build everything else myself. It works quite well and I have not run into any major issues. As long as you're ready to do the work/research required to do everything, it's really fun and rewarding! I have not personally used gfx, but if you use it, be ready to do some low level graphics programming. You may want to look into the recently released rendy crate that makes some of that easier. I use SDL2 because it is a very mature and well documented library, but it definitely doesn't give you as much control as gfx does. It also binds to a C library, so there's that. :X
yes, ggez is a great example of this
I've been using futures-preview 0.2 (you know, the non-nightly one we were explicitly told *not* to use...) and from what I've seen the nightly version isn't drastically different. I think it's pretty solid at this point.
&gt; not sure I appreciate the sarcasm And I'm not sure I like being in the company of one who apparently takes moralizing as a way of life, but *here we are*!
But without an event loop, there isn't the ability to have async functions, right?
It's an optimization thing. uninitialized memory is considered by Rust's "abstract machine" to be `undef` regardless of the bitpattern that happens to be in memory at the time, so reading from it and making a decision based off of that read is also undefined. I don't really know what sorts of optimizations that actually enables in practice, but I'm pretty sure that C and C++ compilers operate under the same assumption too.
It's worth noting that `vec![0u8; returns_a_number_somehow()]` has an optimization in libstd that makes it call `calloc` under the hood. That only works for pointers (null) and integers (0). It doesn't work for the cases where None is represented by a 0, and it would be great if it did. It also doesn't work for tuples/structs integers, etc.
Hmm... I think much of the boilerplate could be reduced through the use of proc_macros. To answer your question about overlapping typeclasses, I'm assuming you're referring to class instances, in which case it is a problem for Rust right now, but there is work on `specialization` feature to improve the situation. I recommend taking a look at [`typenum`](https://crates.io/crates/typenum) and [`type-operators`](https://crates.io/crates/type-operators). They don't try to implement higher kinded types, but they do provide ways of performing forms of arithmetic at the type level, which does seem useful in this context. After thinking about what proc_macros could do to simplify type arithmetic, I think that proc_macros don't technically need to receive valid rust code, so it's conceivable that it may even be possible to write Rust in an ML-like syntax using them.
The bottleneck is number of blocking concurrent connections you can have, when you use async that bottleneck is gone. And it is a bottleneck, at least for me, I've tested, but without diesel it's just too much of a pain, It's better to pay for better machines than to spend that money in development time.
Of course your natural language that you are most fluent in is easiest to think through problems with. Thinking by coding is also not saving you time.
&gt; The bottleneck is the number of blocking concurrent connections you can have, when you use async that bottleneck is gone. Its a database on a server, how many connections do you think its going to be able to handle? You are talking like its a endpoint for thousands of websocket connections.
&gt;You are talking like its a endpoint for thousands of websocket connections. That's literally my use case.
But thats not thousands of connections to your database.
Sure, but the idle connections are using CPU from the server and they are limited to the number of threads, which is limited too. When you have thousands of users connected at the same time doing concurrent requests at the same time your blocked connections matter to the server. I can put the database in a better machine, but I shouldn't need to increase the number of server machines.
&gt; Sure, but the idle connections are using CPU from the server and they are limited to the number of threads, which is limited too. Idle connections do not use CPU, and connections are not limited to the number of threads. Using an async type server you can support thousands of websocket connections on a single thread. Connections are handled by the operating system, and async servers use a multiplexing thingy, probably epoll, to inform your server process when more data is available for all those connections, and the data flows into your app, which handles it in an evented type way (node), or a green thread type way (Go). Either way its considered concurrent. Now when you reach out to your database to handle each of those connections there are only so many database "connections" available, whether they are async or not, the database itself isn't, and in fact, mysql will probably handle each connection in a thread, and postgresql will handle each of those in a separate process. 
Also makes some sense to have the whole codebase in C++ rather than having to work with two languages.
&gt;Idle connections do not use CPU I meant blocked connections, which do use because you can't know the connection returned unless you wait for it, so you have to schedule it every time to check if it's done. It doesn't matter inside the database, if it's the bottleneck I will upgrade it, but I don't need another server machine to wait for the responses because the other has all threads blocked and using CPU while at it. My point is, to handle more users I may need a better database server, but I should not need more web server machines, because they are not the bottleneck...
It's not just `Pin` - I have been trying to find how `Pin` could be useful with `futures-0.1` without success. If you have any hint, I would be grateful.
I don't think any of this matters, because I **do not want or intend** to intentionally read from uninitialized variables, as that is pointless for very obvious reasons. It was just a trivial example. I just want a Vec where length and capacity are the same right off the bat, to be assigned to by its elements and never "pushed" to, that I don't have to fill up with some specific empty or default value first. I *do* however think people are going way out of their way to attach an element of mystery to certain things that just doesn't exist. Pointers are not complicated.
Sure thing! Will add this to my todo list.
There are a lot of crates for 2D graphics on crates.io. I rarely can‚Äôt find what I‚Äôm looking for.
Makes sense to keep the code base the same language the whole way through. The [mesh generator](https://github.com/huxingyi/dust3d/blob/master/src/meshgenerator.cpp) has zero tests that I can find, and has a lot of null pointer usage etc... It does use the Qt types quite a lot, and I know that is painful to use these in rust. If the author is more comfortable writing this code in C++ it makes sense to use it, but I feel like there is a long way to go to make this code "safe" regardless of language. Rust just happens to bring these safety decisions to the front, and can be disguised as friction from the compiler.
Another good usecase for this would be detecting features used in your crate, but only enabled transitively by dependencies. Thats a bit annoying to run in to, so a tool to detect that would be great.
Wrong sub reddit. You want r/playrust
I think one measure of a language is how many widely adopted products are written in it *without any kind of forcing*, and especially without being produced by the same org that's producing the language. Docker and Prometheus were written in Go. Homebrew, Vagrant and Fluentd were written in Ruby. What does Rust have? The only major non-Mozilla success story I can think of is ripgrep. This isn't all there is to a programming language. Rust would still be mind-blowing if zero real-world code was being written in it. But it's part of the story.
This is amazing!
Chucklefish used it for some of their games, most notably Starbound I think.
Honestly? Wrap a generator. (The same as std futures). Unfortunately, while pin enables self-borrows, the rest of the language and stdlib don't have many facilities to support it. Trying to do so manually either requires an Option or unsafe to initialize the reference in place after its already been moved to its location. Self Borrows are technically possible in Rust without Pin, it's just not very useful without unsafe; all it does it communicate which operations are in fact safe and which are absolutely not safe.
That's not even a good proxy for popularity. The fact that Homebrew was written in Ruby doesn't reflect on Ruby hardly at all: it tells you that one person liked Ruby enough to write a tool in it. It doesn't matter that some application is famous and written in Rust. What matters is that, if you want to write world-class software, Rust should be an obvious choice.
No one usually polls connections. If you need to wait for data, you just do recv() or read() and kernel blocks untill new data is available. That's just one syscall and context switch to kernel and back. Now compare it with async: epoll_ctl() to indicate that you want to wait for read, epoll_wait() to wait for it, and then recv() to actually read the data.
It's still in development.
Yes this was planned as part of our working group's 2019 roadmap, as described in the post.
Number of concurrent connections is limited by DB. Postgres, mysql, etc are not async, they allocate large buffers for each connection and I believe use a lot of locking internally so they can't really scale that much.
I'm not talking about popularity, I'm talking about the learning curve of contributing to an open-source project assuming you're already comfortable with the language.
I stand corrected.
I encourage reading https://blog.burntsushi.net/rust-error-handling/ to get a good understanding or how to handle errors. Then there are a few crates to remove some of the work.
I have hundreds of DB connections and it's ok.
I figured it was something to do with optimization. It makes me a bit uneasy though, that an optimization can take what has a pretty clear meaning (to any logical human who doesn't know better) and make it something that is impossible by the original definition. Also, it seems insane to me that with the possibility of pointer arithmetic (especially in C/C++) the compiler can have any idea of what memory is undef and what has been written to. Like, if I take input from a user, and dynamically choose an element to write to based on that input, the compiler can't possibly know which elements in the array are undef and which single element has been written to.
Whoa, I've never seen the `where for&lt;'a&gt;` syntax before. Neat!
It's certainly possible, but look at those random graphs on how pg scales with number of connections: http://i.stack.imgur.com/EMjet.png https://www.percona.com/blog/wp-content/uploads/2016/12/scalability_point_hl-1024x614.png This stuff depends on workload a lot and may or may not apply to your setup.
Is there an easy way to resolve this on OSX, searching online says no. &gt; dtrace: system integrity protection is on, some features will not be available My flamegraphs are missing tons of info and calls which I know are happening, which I assume is being caused by this. I'm running the process under `sudo` because that complained also :)
They haven't used Rust in their game for a while.
Working on a RESTful API framework, due to the sad demise of other REST frameworks. I'm planning to publish it, but still needs a few polishes
The server code for Wargroove was written in Rust, iirc.
Same 
/r/playrust
The `#[safety]` proc-macro could strip the doc-comments out of the output code.
As a newbie I'm trying to stay as much in stdlib as possible - but I discover new crates like you mention here every day. How does one stay on top of all of this? 
It honestly isn't in a lot of cases. For instance take a look at https://github.com/ryansolid/solid. If you go and see how it performs in benchmarks you'll find a library that is smaller and faster than most out there. He also has a fairly good blog around the lib.
&gt;Better questions is why some languages feel a need to have those "basic" modules to begin with? There are some advantages I can think of: * Lack of something as convenient as cargo, so dependencies are more of a hassle * Discoverability * Easy to know it's a solution that can be "trusted" (even if it's not the best) * Convenient to be able to distribute just a single dependency-free file for simple scripts The first point obvious doesn't matter. Making sure the accepted and trusted crates are discoverable to users (particularly new users) is perhaps something that could be improved; but probably not by moving them into std. I'm not sure what's best for that. The last point might seem unimportant, but in a language like Python it can be quite convenient for certain purposes; it's less useful for a compiled language and becomes irrelevant for larger project that need many files.
Not really sure what language this is. But the error seems to be saying you are missing a token of sorts in your macro argument in (statement!($stmt),)+ To me this seems like its saying that you need something in place of the , just a pure guess though 
Not automatically that I am aware of. You can give `-p` flags to specify the crates manually as in `--no-deps -p othercrate`. In order to do it automatically, I think something like [public/private dependencies](https://github.com/rust-lang/rfcs/blob/master/text/1977-public-private-dependencies.md) will need to be implemented.
It may not yet be widely adopted, and frankly unless you run some sort of cloud service there's not a lot of reason *to* adopt it, but I consider [firecracker](https://github.com/firecracker-microvm/firecracker) to be probably the most important open source project to come out of 2018.
&gt; Makes sense to keep the code base the same language the whole way through. I agree with what you've said here. However, I think the author takes the wrong approach towards coding complex algorithms or methods. If anything is sufficiently complex, you should write it out on paper and make sure everything makes sense before trying to code it, so that you're not battling the language. I don't think that's unique to Rust, though. Just because you can bang out a script in Python in 10 minutes and mash "up-enter" until you find all the bugs doesn't mean that that sort of programming is the most effective/efficient. I've coded my fair share of complex algorithms using both Rust and Python, and I don't think I've ever had the same problems the author describes.
Here is lots of new blue goo now. New goo. Blue goo. Gooey. Gooey. Blue goo. New goo. Gluey. Gluey.
If you're interested in firecracker, check out `rust-vmm` which is going to try to standardize a lot of the kvm wrappers and virtio interfaces for rust based virtual machine managers. Amazon, Google, and a couple other groups are working on it. 
Checking out [Amethyst](https://www.amethyst.rs/). Just finished the Pong tutorial, I love the [book](https://www.amethyst.rs/book/latest/intro.html) so far! I've been looking for crates to contribute to, would love to help out if I'm able. Graphics and games are a fun but foreign concept to me at this point. I came across [this talk](https://www.youtube.com/watch?v=aKLntZcp27M) by Catherine West about game dev in Rust, and I had actually already watched it once, but I watched it again for a recap of Entity Component Systems and it's just generally an entertaining watch. I recommend checking it out if you haven't seen it. I may soon begin a long-term game development project... We'll see.
great advice!
We don't have to go over to the JavaScript world to find alternative approaches. [`rust-dominator`](https://github.com/Pauan/rust-dominator) does fine-grained change tracking as well.
Before I started learning rust in earnest I read this article from someone who already tried to learn rust. The main idea of the article was that learning rust makes you a better programmer and even if you decide not to use rust, the patterns you've learned can be transferred to C++. When I was learning rust I remembered that idea every time I had to fight the borrow checker. The reason I fought the borrow checker was that the patterns I thought in were not safe. It might be presumptuous, but I think this 'fighting the borrow checker' period of adjusting is when we adapt from the old way of not caring about memory safety of our algorithms to the new way of having it at the forefront of our minds.
The language is Rust.
Yes. I'm currently working on a JSON reader for the terminal and this is just the motivation I need!
&gt; I know the friction is greater because I am still a Rust learner, not a veteran... I'd be curious if this is a pretty generic case of the steep learning curve, or if the mesh code in this project is hitting one of the especially painful "unclear ownership" cases that safe Rust has a hard time with.
This fabulous utilities you mentioned really are incredible!
I also in fact chose a Rust CLI as a first project. Once completed, where is a good place to get some constructive feedback on the code style etc.?
If you dont know where to start, take a look at https://github.com/uutils Just pick something that is missing there. Then contribute your changes to help make uutils more complete.
Sweet, glad to hear it and good luck!
I know, right? I'm pretty in love with `bat`.
I've seen people ask for and get nice code reviews on this subreddit before.
You can also view + download existing implementations straight from (e.g.) GNU or BSD repos if you want to: 1. Check your work 2. Get an idea for how to implement 3. Get an idea for how not to implement 4. Find out all the crazy edge cases you may not consider in a first implementation 5. Go mad with the power of old wizards Some places to start: GNU coreutils: http://git.savannah.gnu.org/cgit/coreutils.git/tree/src FreeBSD /usr/bin: https://svnweb.freebsd.org/base/head/usr.bin/
For anyone looking to get into rust and webassembly, I read through [the most depended upon packages on npm](https://www.npmjs.com/browse/depended) to try to find some nice low hanging fruit and [made a list of things I thought might be good targets.](https://gist.github.com/heyimalex/6e93d2abb2113d6d93e7e28b159d8b02) It's short, but maybe could give you an idea for your next project!
Then they ported it to C++.
It's totally reasonable if you're comfortable to work with Rust Similar to @sunjayv, I used SDL2 but build my own ECS for a simple 2D game engine. It hasn't finished yet but it works quite well, I use it to learn more about Rust and my paint points are mostly when I tried to construct data structure in Rust the same with what I did in other languages and sometime lifetime issues. So most of them can be avoided if you really know Rust well. I recommend to read this [article](https://kyren.github.io/2018/09/14/rustconf-talk.html) about how to start building game engine in Rust. It cleared many of my doubts,
I gave up on some projects because they would require native code in pypy. Compiled code portals are a nice idea as long as its the same platform for the coder and the user *or* the language is completely portable if done right (like pure python or java).
That approach isn't going to work. I think it's like, once you have `$thing:expr`, you can't do further unpacking of `$thing` by passing it to other macros: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4fc34afb7dd714b71a78149cec3e8d72 (Except for `$_:tt`, which can IIRC). You're getting an infinite loop because the 2nd rule is expanding to itself, the macro system doesn't even consider the first rule to even be a possible match. I recommend changing the second rule towards `(let $first; $(let $second)+)`. If you're going to be adding lots of different enum variants to `Statement`, you should consider using a procedural macro instead, because complex `macro_rules` can suck in a big way. But that might require nightly.
I took computer science but i'm still confused about the experts and professors insistence on this particular bit of pedantry. Probably just to discourage people from thinking 'smaller is always better' without considering input size or something. As i said, i don't understand it. Been some years though.
This clarifies things for me too...
bat?
It's like cat, but with syntax highlighting and git integration: https://github.com/sharkdp/bat
If you're writing an application, failure::Error will work well. If you're designing a public API, wrap them in your own error.
Go was released in 2009, Ruby was released in 1995. These languages had much more time to mature, including their tools, libraries, and general ecosystem. Rust 1.0 was released in 2015, just a few years ago (before 1.0, the language was very unstable). Give it some time before judging.
You can disable SIP to get DTrace to work again on macOS I believe. 
This makes me a little sad, but thank you (if you are a reddit user?) for releasing [https://github.com/huxingyi/meshlite](https://github.com/huxingyi/meshlite) as part of the Dust3d + Rust experiment.
Thanks for linking to structopt. I've used various command-line argument parsing libraries in the past, but I think I like how this one works the most, the style is very clear.
Good for you! For me sadly its not that great, its come to the point where i dont expect it to work at all and when it works its nice, but that doesnt last long :/ also thought to switch to vscode but intellij is a damn good ide
This flew straight over my head. I don't understand what you are doing or even why. It looks interesting though. How do you think I should approach it? Do you think I have to become proficient with Haskell first?
If I try to run this, it tells me that `associated type defaults are unstable (see issue #29661)`, so maybe you could mention this on the appropriate issue here: https://github.com/rust-lang/rust/issues/29661 
When you say "compile-time", do you mean that I can use this in my rust-only wasm web app without using the html!{} (by yew) macro all over the place? (i.e. you dont need the templates at runtime?) The major problem with templating engines is that they require file access at runtime, something thats not possible in wasm. So if I can use your crate for this, then I love you and I will start rewriting my wasm client asap :)
You're correct, the template file is not needed at runtime. I'm planning to implement a way to reload parts of the template at runtime when compiled in debug mode (without \`--release\`) when the logic hasn't changed between two instantiations of a page. This would make it possible to iterate faster on a design.
DisplayAs looks indeed very similar to Cuach. I just couldn't find it when I went looking for a new template engine (is DisplayAs on [crates.io](https://crates.io)? Also, I couldn't find any documentation). The kind of design Ramhorns is probably very useful for some projects, but my particular use case ([nest.pijul.com](https://nest.pijul.com), especially when displaying patches) involved tricky logic to display pages, and I really needed Rust in some places. Also, being able to implement some parts directly in Rust, outside the template file, in the harder cases, was pretty useful.
Can confirm. I wrote [a clone of GNU `tr`](https://github.com/Shnatsel/rust-tr) as a first project, learned a lot.
If you look at GPL code (the one GNU implementation is under) wouldn't you have to release your code under GPL as well? You are basing your implementation on the code you've read, after all.
After you're done you might want to contribute to https://github.com/dflemstr/rq
Oooo, a classic.
btw \`cat\` originally is for concatenating several files like \`cat 1.txt 2.txt &gt; 3.txt\` and 3.txt will have text from both files
\&gt; no CGAL and no QT &amp;#x200B; \+1000000
You can find a bit more about Display As in [this Rust forum post](https://users.rust-lang.org/t/display-as-compile-time-template-crate/24097) announcing it. It is on [Crates.io](https://Crates.io) ([this is its entry](https://crates.io/crates/display-as)), but I suspect it's hard to find because of uncertainty surrounding its official name (is it "Display As"? "display-as"? "DisplayAs"?). It's also new and isn't being promoted very much, which is a shame, since it's simply great how it allows you to embed arbitrary Rust code in templates. It would be great to see the Ramhorns approach combined with the approach you and the author are taking with Display As to make it possible to have templates with embedded Rust that also can be updated at runtime. It seems a shame to have to re-compile templates every time they change even a little bit. Thanks again for your work on Cuach (and, of course, Pijul).
:) I even didn't consider using separate game engine. It draws on canvas, whole graphics data is contained in single png file, there is single rust function for drawing tile on canvas - that's all :)
For the GPL (and any license) to apply, the implementation must be a _derived work_, which has a specific, legal meaning. In general, you have to actually copy code verbatim or with only cosmetic changes in order for a work to be derived from another, and thus be subject to terms of the license of the original work. Just looking at and taking inspiration from another implementation probably isn't enough. In fact, even copying small snippets might be covered under fair use.
\&gt; I know the friction is greater because I am still a Rust learner, not a veteran &amp;#x200B; I'm pretty sure that's the case. When I began to write some Rust code, it was a real pain, but after 1 year or so, it feels so easy to satisfy the borrow-checker that I don't really think about it anymore.
I tried this, the warning is generated before 
There have been a lot of arguments around this, with some lawyers insisting that if you use information from the GPL'd code, your code is a derived work.
Do you have some links? I certainly could be wrong, and if I am I'd definitely like to correct myself before I make a costly mistake :P
Unclear ownership is a design smell
I'm pretty sure they're still using it there, they only moved their upcoming game, Witchbrook, to C++ to reuse their engine
I found another downside: macros could also be called like `safety_guard::safety!(...)`. How do you determine, if this is the safety macro from the crate? If you would have had another `safety!` macro imported, this would be overwritten. It basically bypasses the rustc parser. Don't get me wrong, I'd like to have your notation as I also prefer to keep code in the function, but I don't think it's possible without unexpected side effects. If you have a solution, feel free to [open an issue](https://gitlab.com/tdiekmann/safety-guard/issues/new) or [crate a merge request](https://gitlab.com/tdiekmann/safety-guard/merge_requests/new) :)
From https://copyleft.org/guide/comprehensive-gpl-guidech5.html, the copyright act says: "In no case does copyright protection for an original work of authorship extend to any idea, procedure, process, system, method of operation, concept, principle, or discovery, regardless of the form in which it is described, explained, illustrated, or embodied in such work." See your lawyer for advice, but my opinion is that means using information is fine.
Unfortunately there's no "document public dependencies only" mode yet. There's a small bit of push towards eventually classifying public/private dependencies to make this use case work better, but for now it's not possible. If you're feeling really self-conscious about your docs' dependency on bincode's, you can re-export the type (`pub type Result = bincode::Result;`) and mark it as `#[doc(inline)]` and rustdoc will pretend it's defined inline (though the type will still unify properly).
&gt; (pub type Result = bincode::Result;) and mark it as #[doc(inline)] Will try that. never done that before. thank you.
You're assuming the database is a more limited resource. In practice, you may be running a fully distributed database where that's not the case, or you may be running a more typical database but on a much beefier machine. In either case, making your database access asynchronous will greatly improve throughput. As a real life example, at work I use kubernetes to run a fairly typical microservices architecture: the services are mostly stateless and have limited memory usage. Any solution that involves using a thread-pool to do database access is going to have lower throughput because threads use a lot of memory.
I do not have links right now, no, but phoil has, and it seems I recalled incorrectly.
It isn't something the deals with untrusted data so it isn't super important that it be safe. &amp;#x200B;
So the meshes that are loaded from a file can't affect this behaviour? What about any edge cases in the algorithm? I don't know what it does well enough to make those judgements, but that's the point: even the best of the best make mistakes. I get that this project probably values developer efficiency over safety or correctness, and probably makes sense in this use case. But I don't think it's wise to dismiss safety as a concern by claiming it doesn't deal with untrusted data
It's already there in unstable rust -&gt; [struct.Graphemes](https://doc.rust-lang.org/1.3.0/std/str/struct.Graphemes.html) &amp;#x200B;
Hey, there is a [Rust talk about exactly this](https://youtu.be/ApOUBBOvZDo). It might be useful to you as a comparison or reference.
I think it's like the distinction between a logging facade and a logging implementation. Language-level `async fn` just means that the compiler will do the heavy lifting of compiling imperative code to a state machine and state transition function. The programmer still has to figure out when to invoke that transition function. This is similar to how the closure syntax does the heavy lifting of compiling the high-level concept of upvalues to a concrete struct and corresponding function. That task is very much possible in C. There's even a pattern for it: the `userdata` argument to a callback. But it's all done manually and is just a pain in the butt. Language level async is intended to make the API of sitting like `tokio` much easier to design and use, much like how closures make many functional programming patterns that are technically possible in C easy enough to be worth using.
&gt;So the meshes that are loaded from a file can't affect this behaviour? If you planned to wire this thing up to the internet and let unknown people send meshes in, then yes, importance of safety would increase. My point is not that people don't make mistakes, its that a safety error with a program that doesn't deal with untrusted data is just going to crash your program, maybe corrupt a mesh file, not give someone root or leak financial data of users. 
The problem is that same "paper" implementation can look vastly different in different languages.
oh, cool
Oh my god. I need to try this tonight. If it works, then I'm gonna rewrite my entire client app! You cant believe how much I love you! I waited ages for something like this!
Couldn‚Äôt you run rustfmt on the output to at least make it look sane-ish?
I love and hate uutils at the same time. This is beautiful project but impossible to use as a library
&gt; When you implement an algorithm using C++, you can write it down without one second of pause, but you can‚Äôt do that in Rust. As the compiler will stop you from borrow checking or something unsafe again and again, **you are being distracted constantly by focusing on the language itself instead of the problem you are solving**. (emphasis mine) While I have nothing to say about the decision to switch, which looks to be done for good reasons (and it their project, they can do what they want), I strongly disagree with the way this is formulated, as if it was some general truth. And this is something that we see in many blog-posts (not only about Rust). What this quote is really saying is "I am much more familiar with C++ than Rust", and nothing more. I personnally would say the exact opposite thing about C++ : even though I have a few years of familiarity with it, I feel that as a language it requires me to think much more about small things that are not my algorithm than Rust. So C++ gets in the way, while Rust allows me to focus on the essential. Actually, likely any programmer can make the same statement about any programming language that is not their language of preference. I believe the programming language you use profoundly affects the way you think an reasons about programs, so it is expected that switching to an other language has a mental overhead. But if you feel that a language is resisting your attempts to program without distractions, then it's probably not due to the language's natural friction, but rather due to the mismatch of you mental model compared to the language. It's perfectly fine to not want to change your mental model (I honestly tried haskell before deciding I didn't like it), but let us be honest about it : it is often more a matter of personal preference than some objective shortcoming of a language (there are of course some languages that are ill-suited for some tasks, but I don't think this kind of problem applies when comparing Rust to C++). 
I don't know if this would be possible, but this would be out of scope of a single small attribute IMO.
Great project! Fork `uutils` and start rewriting stuff to split the core functionality into a library crate.
Very interesting, thanks for sharing!
You should check project panama http://openjdk.java.net/projects/panama/
I was already planning to run a Hackathon / Coding Retreat around sensor networks and processing this June or so as part of a project I'm involved with, so‚Ä¶ Thanks much for posting this inspirational and informative account! That TI board looks pretty nice. I may have to try playing with it further.
I'll point out that `cat` works for any files, not necessarily for text ones. Do you can `cat` binary files as well.
&gt; Clones are often compiled away. So, clone() away instead Ooo, really? Is there any place I can read more about this?
Um, no. Those bitshifts aren't correctly decomposing the 128-bit number. `a = b = c`. Why are you implementing chacha20 yourself? Even if your code matches your interpretation of the specification, you should use test vectors. Or better yet, use [cryptol.net](https://cryptol.net)'s verification methods to formally prove the implementation matches the spec and has no obscure bugs. I'm not saying anything else about the rest of the function as I don't have the time just now to read the spec and compare it against other implementations (like my own; verified with test vectors but not Cryptol).
I know what all the words and references mean in your question, but I still have NFI what you're talking about.
I would also recommend adding a compile fail statement to prevent recursion like so https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e88ddc7674d8502b211aa6931a6bf2ac
I have a question about when to use an Option vs a Result. I'm working on a tool that has a settings file. I have a `get_user_config()` function that checks if there's a config file, then opens it, deserializes it with serde and then returns either a `Some()` containing a `Settings` struct or it returns a `None`. Now this works just fine, but if I think about it I could just as well be using a Result as well. After all there could be an error occuring. (File cannot be opened, deserialisation goes wrong). So should I be using an Option or a Result here? And if I think about it: I could in theory always use a Result, so why is there even an Option? Results can do everything an Option can, correct? I can just pass back a `Ok()` instead of a `Some()`. So what am I not seeing here? How do you guys decide on when to use an Option vs a Result? Thanks so much in advance!
I'm working on a Full-Domain-Hash crate: https://github.com/phayes/fdh-rs This is a nice cryptographic construction that can extend the length of a fixed-length hash function to an arbitrary size (turning it into a variable length hash function). For example, we can extend SHA256 from 256 bits to 1024 bits. This type of hash function is useful when using RSA with no padding. 
The qtr_round macro looks correct to me at quick glance of it and the rfc. If it were me, I'd make it a real function, and then write a test or 2 for it (the rfc provides at least one test case to use). As someone else pointed out, your enceypt_block function doesn't make much sense. 
Hi! Great article, but I have some questions: &gt; Keep in mind an important hidden property here: because you accept values to be moved in while you just have a read-only contract on them, you also accept clients values to be dropped by corollary. My attempt to restate what you wrote: a side effect of the client `move`ing the thing into your API is that the passed-in value might get dropped as your method is running. Is my understanding correct? If so, doesn't this happen anytime you move ownership as a result of calling a function? Can you give an example where this might be important?
I don't see how that reflects on the language either. That's a function of the project complexity and the willingness of project organizers to document and explain their code and build systems...
Cases where the caller does not care details of the error, I use a `failure::Error` type. In cases where caller might need to know some details or display some form it to user I use a `ErrorKind` pattern from `failure`. An example below(omitted details). Here `find_by_username`, `save_user`, `send_welcome_email` all return `failure::Error` and `sign_up` returns more specific error. I also have some helper code and macros to easily convert ApiError to Json response with appropriate error code. pub fn sign_up(conn: &amp;Connection, user_ac: &amp;UserSignUp) -&gt; Result&lt;(), ApiError&lt;SignUpErrorKind&gt;&gt; { debug!("User to register {:?}", user_ac); let maybe_user = find_by_username(conn, &amp;*user_ac.email)?; if maybe_user.is_some() { Err(SignUpErrorKind::UserEmailAlreadyExists)?; } let user_record = UserRecord{...}; //creating db record save_user(user_record)?; send_welcome_email(user_record)? } #[derive(Copy, Clone, Eq, PartialEq, Debug, Error, Serialize)] pub enum SignUpErrorKind { #[error(display = "User already exists with same email")] UserEmailAlreadyExists, #[error(display = "Internal error")] Internal, } impl From&lt;failure::Error&gt; for create::ApiError&lt;SignUpErrorKind&gt; { fn from(e: failure::Error) -&gt; Self { e.context(SignUpErrorKind::Internal).into() } } I am not sure if this is the right approach, but this appear to work for me for now.
I should have known someone has already started doing this
Maybe also make them 100% POSIX compliant as IIRC it was Theo de Raadt's main complaint about memory-safe languages like Rust.
I always thought it was the last n, but no idea why that stuck in my head. Feels like that would be the way to abbreviate it.
I'd lean on result here. Option fits better when it's a exists or doesn't exist thing. Getting a element from a Vec from example return a option because either it exists or it doesn't. Results are used where there are many non obvious "errors" 
I don't know that I've ever considered it consciously, but I think I assumed that fn was abbreviating fun, so the "n" would be the first one.
You could try putting the `//!` doc comment above the `#[forbid(unsafe)]`. 
Though wasn't valid because the Rust implementation of coreutils is POSIX-compliant, with GNU extensions, considering it's set up to pass the busybox test suite.
It depends whether the function is fun enough.
Honestly the bit you highlighted is really saying "C++ lets me do things that _might_ be correct, but probably aren't, while Rust makes me prove it's correct, and I'm too lazy to figure out how to prove to the compiler that it's actually correct".
Yeah none of the cached information that is exclusively in `NonogramPrinter` will change. But one of your previous comments made me think of something...what if there was a situation where a user wanted to use multiple "views" into the same `Nonogram` object? E.g. both a "terminal" printing view and a GUI display view. In this case, having the view own the `Nonogram` object wouldn't work. Is there any way to do something like that in Rust? I guess I would probably have to pass in the reference to the object every time. I just didn't like that idea as much because then you could pass in a reference to a different `Nonogram` than the one you initialized it with.
I always thought was the first. Like you abbreviate "function" to "fun" and then drop the "u" to just say "fn" without a vowel.
That's pretty cool. It would be nifty to see it added to the frontend framework benchmark.
C and C++ don't have macros. Macros aren't a necessity. But I were to create a small compendium of my flawed knowledge of metaprogramming* (i.e. programs that write programs that executes at the same time) goes like this: * C. Has no macros. It's metaprogramming faculties are essentially super complex C preprocessors, which can be rather intricate but on the flip side, they become super hard to maintain, since you are essentially doing string based replacement. * C++ - Doesn't have macros but has templates that reduce boilerplate and possibly compile-time functions, which can do some intricate things. Can they do compile-time checking with error reporting? All signs point to no. Again haven't touched C++ in decades. So it's *might* be possible, but my last touch with C++ ended with template errors so arcane and dense, I gave up on any attempts to metaprogram in C. * Lisp - Your entire language is essentially a macro, and you can create arbitrary syntax for your language. *It's possible to write programs that write programs in another language. But that can be done in any language.
It's both Ns, of course.
fnn
I'm curious about where you want to take this series, scientific computing means different things to different people.
That's awesome, cruelly lacks a table of content for quick navigation though! 
&gt; Anyway, a lot of people agree that there is a problem but the politic is that it's not. I don't think that's true. I have not seen anyone disagree with the statement that name spamming is a problem. There just is no agreement on what a good and realistic solution for this could look like -- realistic in the sense that it can actually be implemented with the current team size.
I skimmed the Zeit docs about lambdas and I've heard of similar features from the bigger cloud players. But I still don't know how people use it in practice. What are some existing commercial uses of lambdas/serverless code?
Here's an example of a success story using Rust, even [https://andre.arko.net/2018/10/25/parsing-logs-230x-faster-with-rust/](https://andre.arko.net/2018/10/25/parsing-logs-230x-faster-with-rust/) [https://andre.arko.net/2019/01/11/parsing-logs-faster-with-rust-continued/](https://andre.arko.net/2019/01/11/parsing-logs-faster-with-rust-continued/)
Yes.
I find that many classic CS data structures require extensive rethinking when implemented in Rust. I do a lot of work with zero copy data structures and local references, and almost nothing in that realm can be made to look like idiomatic Rust without a lot of unsafe... the fact that containment precludes move, while obvious, breaks a fair number of the classic algorithms if you want to get efficient memory layout... these are all consequences of reasonable decisions, but the fact is, if you're using Rust for actual systems and core programming, it requires you to learn some new ways of thinking about problems, and that's a step beyond merely learning a new syntax. The most common example is implementing a graph-like structure.
&gt; As a real life example, at work I use kubernetes to run a fairly typical microservices architecture: the services are mostly stateless and have limited memory usage. Any solution that involves using a thread-pool to do database access is going to have lower throughput because threads use a lot of memory. Thats a very unusual situation, normally that would not come up and I doubt all the 'async makes everything faster' crowd here is thinking of that. Here's a more sane thread about the subject here https://www.reddit.com/r/rust/comments/814sfq/whats_the_state_of_async_db_access/
FooNugget 
I think it's just short for fun, which is short for defun, which is short for define function.. which is short for a whole bunch of assembly :P
That is what probably makes the most sense, the API for Rand or Regex isn't yet as standardized and hence we can't just push it into the std. That's what I needed. So it's okay to say that we'd be getting lots more in the std, in the years to come?
Never really thought about it, but in my head I think of it like a shortening of 'func' so I guess for me it's the first n.
How do I use for-loop with mutability and recursion: ``` fn enrich(window: &amp;mut Yaml) { for (key, value) in template { if window[key.as_str()].is_badvalue() { // if field is missing window[key.as_str()] = value.clone(); // fill it from template } } for w in &amp;mut window["windows"].into_iter() { // this does not compile enrich(w); } } ``` I tried all kind of variations, placing `&amp;mut` at ridiculous spots, but it always ends with compile error. Any idea how to do it properly? Thanks!
This is just friggin impressive!
Second/last "n". Just like "i18n" is referencing the last "n" in "internationalization".
I was thinking of things like doubly-linked lists or graphs, where the ownership issues are kind of inherent to the problem and not the programmer's fault.
pretty sure this will divide the community in half. It is obviously the first n
Yes, `Result` is strictly more powerful than `Option`, but that doesn't necessarily make it better. Even though their implementations are rather similar, the difference is semantics is vast. There are lots of cases where something not existing or something returning nothing is not an error, but an expected and common occurence. For example, you can have optional configuration settings or function parameters, which would be represented as options - missing the setting is not an error. Similarly, it doesn't really make sense to return an `Err` when looking up a non-existent key in a hashmap, because there is exactly one reason an item couldn't be found: it doesn't exist. Use `Option&lt;T&gt;` when there's a possibility that a thing does not exist. Use `Result&lt;T, E&gt;` when an operation can fail, preferrably with an error message/type. 
I'm adding new features to [custom\_error!](https://github.com/lovasoa/custom_error), my error management crate. It now supports custom derives and doc comments. I also contributed a few commits to the great [broot](https://github.com/Canop/broot) file manager.
`f6n`?
@CUViper, stop trying to make \`f6n\` happen. It's not going to happen.
Or **fu**nction with an upside-down 'u'?
I gave a related talk at the most recent Rust Meetup in Denver. Slides here if you want to see some other cli utilities and some historical context. https://peterlyons.com/rust-cli-2019/
But why that font
Good Q. I wanted it to look informal, which I think has succeeded, but probably at the cost of legibility. Am considering changing it :-)
This should be the accepted answer 
The [GNU Coding Standards](https://www.gnu.org/prep/standards/standards.html#Reading-Non_002dFree-Code) has some good examples of how to rewrite code (remember that the original GNU tools were just rewrites of the AT&amp;T/BSD versions!): &gt;If you have a vague recollection of the internals of a Unix program, this does not absolutely mean you can‚Äôt write an imitation of it, but do try to organize the imitation internally along different lines, because this is likely to make the details of the Unix version irrelevant and dissimilar to your results. &gt; &gt;For example, Unix utilities were generally optimized to minimize memory use; if you go for speed instead, your program will be very different. You could keep the entire input file in memory and scan it there instead of using stdio. Use a smarter algorithm discovered more recently than the Unix program. Eliminate use of temporary files. Do it in one pass instead of two (we did this in the assembler). &amp;#x200B;
Incorrect. Game code is in C++, they still have other pieces of infrastructure that don't ship to players in Rust.
f6n
Most 'fixes' involve namespacing, but if that were an acceptable solution, it is worth pointing out that "tdiekmann_safety" is not an existing crate name. So...
The idea is to provide an easy gateway to become a proficient user of the libraries currently available in the nearest ecosystem and work with them, write new ones and generally move the ecosystem forward :)
But it isn't `f6n`, so while I understand your logic, I don't agree that this is the authoritative actual answer for this reason. It might be the actual correct answer, sure.
I first thought of the first n, because I had also used Go at the time, which uses `func` for the same purpose. But actually I believe it's the final n, because that's the one that actually makes an /n/ sound. The first n makes an /≈ã/ sound, so it doesn't make sense to separate it from the following c.
The translation of the QotW is super weird. I believe is something more along the lines of: &gt; Yes. Ownership is hard. Until now, this hard stuff have been dealt with by hand without machine checks (intentionally)
Thanks for your explanation. So you would agree that a Result would be the more fitting choice here?
I had to change it through the developer tools before I could read the post :)
[https://media.giphy.com/media/1eKbAKgocJCta/giphy.gif](https://media.giphy.com/media/1eKbAKgocJCta/giphy.gif)
Its just this: &amp;#x200B;
&amp;#x200B;
My head asplode
fnsn
Well, you need to handle the potential errors anyway - either by returning a result or panicking on error with `unwrap` or `expect` - so I think Result is right choice here. Generally speaking: if your function calls other functions which return Results then your method should also return a Result.
Rust is basically a form of C/C++ and in C++ we have ``mem_fn`` and ``ptr_fun`` so following the same pattern it should be the first n; however if there is a reason why those two names are different, then it should be the second (and hence match with ``f6n`` below).
Rough heuristic: Use \`Result\` to represent failure, \`Option\` to represent absence. If a file can't be opened, that's a Result::Error. If an optional config value isn't set, that's an Option::None. You can indeed always return a Result, but sometimes that doesn't express what you mean cleanly, and makes life harder for the people using your function. For that matter, you could model every single variable as a \`futures::Async\` enum that's always \`Ready\`, but why would you?
Given Rust's ML ancestry which has the "fun" keyword, I'd argue it's the first n.
it has to be the second, if you say "function" out loud and then "eff nn", it's clear how they line up
r/inclusiveor
Here's my take: &gt; Ownership is hard. &gt; It indeed is. And you managed to do that exact hard thing by hand, without any mechanical checks. (Or so you think.)
Rust isn't intended to be used in a "std only" mode. Crates are your friend.
Whoah. In my head, I totally read it as: &gt;**f**^(---nn)**n do\_thing()** &amp;#x200B; The book-ended, blurred-in-the-middle variant of this: &gt;**f**^(unctio)**n do\_thing()** &amp;#x200B; Which is nothing like: &gt;**f**^(u)**n**^(ction) **do\_thing()**
There's no fun without u.
That sound reasonable. Thanks! Currently my get_user_settings() function handles the errors via expect(). It seemed/seems like a reasonable thing to do as I didn‚Äôt want all the error handling code in my main function. How would you handle this? I‚Äôm the function that returns a result or on the ‚Äûupmost level‚Äú, meaning my main function?
üòç
Dropbox?
&gt; Rust is basically a form of C/C++ and in C++ Not even remotely.
Looks like I‚Äôve been overusing Option/underusing Result. Thanks for this explanation! Result it is then :-)
Is Dropbox FOSS?
Given the similarities between Rust and ML, and Rust and Swift, I'd say it is the first n.
We do /u/ in here.
I don't think there's an universal answer for this. It depends on your application and how do you react to the error case - by crashing or by trying to recover in some way. I can't give much better advice than try out a few approaches and see what works best.
They are implementing some cryptographic primitives, but they aren't confident whether the implementation is correct. "Chinese room" refers to a thought experiment where a person just follows instructions without understanding the intent or the big picture.
I will definitely keep playing around! :-) Thanks for your help!!
First-Ndians, and Second-Ndians (see Gulliver's Travels)
I think it should be shortened to just `f` to resolve this ambiguity and make the language a bit less verbose. Let's get this on the docker for the 2030 edition.
We were proposing removing all consonants and just using vowels in a rust discord server. Function would be uio Module would be oue use would be ue Pub and struct would be u
My [`wc` clone](https://github.com/Freaky/cw) was a lot of fun. Starting with the basics, adding optimized code paths, infrastructure to make the sprawl of that more manageable, and then making it multithreaded.
Don't kid yourself, all functions are fun.
`u u U {}`
On second thought, if we're going with the full versions of at least some of the words then the last 2 would be Public becoming ui Structure becoming uue
Nobody but you in this thread is making the claim that async makes things faster. The test you are asking about is a measure of requests per second. That's throughput not speed which is what everyone in this thread has been trying to tell you.
I've poured a lot of work into real time audio programming with Rust and I am very comfortable with the language at this point. That is a great article, reading it now. Don't agree at all with the OOP ideological bits. God classes and is-a/has-a subsumption are the things you are supposed to not do in OOP, but are somehow the exclusive recurring points in anti-oop arguments. Myopic design also seems to stick to anti-oop programmers. Why have a player object with properties for health and attack? Because there's a player thing in the real world? That's the extent of the design? I would make a DamageLayer object and wrap it with a specific Renderable object representing a player. Not everything needs to go into one object or be an object because that's your real world mental model /rant
Great idea!
"docket", presumably. Please tell me we're not using Docker for the 2030 Edition?
Have you declared `mod scene` in main.rs?
I always read it as "f¬¥n¬¥", as in "Look at that f¬¥n¬¥ abomination!" fn abomination()
I always thought it's the first `n` but I now that you mention it I prefer to think of it as the last `n`. It just seems neater all the omitted letters being together and the remaining letters being the first and last.
This is going to come up in my mind every single damn time I declare a function from now on Thanks
Woah, looks like a lot happened in the compiler this week.
FYI, I don't think that looks formal to anyone. Formal = legible and boring.
Thanks for the tips!
Ah that fixed it. Thanks so much!
ah, the humble funsion
I do what I want.
I know it's definitely the first n! I asked my stomach and he said so.
Well of course when I say 'faster' I mean 'performance', which is what the graph of the benchmark I linked to measures. The common claim here is that async database increases throughput, which is wrong, and I will never accept.
Huh, TIL!
"*in*formal"
Rust is secretly the language for really angry programmers. Unless it's a macro, then we're just really excited!
I see, I missed the „Äå„ÅÇ„Å™„Åü„ÅØ~„Äç part &gt;.&gt; I adjusted it considering this. Thanks for correcting me.
Looks like I need to take first grade reading and reading comprehension again. Thanks for pointing that out. My apologies, /u/marcusklaas.
Haskell is cool. I like it. I recommend it. But you don‚Äôt have to learn it if you don‚Äôt want to. It is my jumping off point of familiarity. The Rust trait system is very similar to the Haskell typeclass system, so it was helpful for me to translate tricks and hacks I know there to Rust and learn the syntax. It‚Äôs always nice to have an interesting learning project and fancy types interest me. This post was experimental and probably not worth anyone‚Äôs time. TBD. It is not clear that GADTs are even something a rustacean would want. Probably depends on how ergonomic they can be made. GADTs let you enforce more things at compile time in the type system, catching more bugs. They are quite nice, although confusing at first (at least to me) It is considered an intermediate to advanced haskell topic although actually quite not scary once you grok.
Yours are some high-quality posts! I‚Äôm looking forward to your upcoming articles :)
Actually, in hindsight, it is the regular data type declaration syntax that is more confusing. Data Constructors are just functions, although they aren‚Äôt just functions. Odd isn‚Äôt it?
Big Ndian and little Ndian?
I was considering it and even started analyzing. Definitely too much work, not even sure if it is possible right now. I think that such a HUGE change in the code layout would require freeze for a couple of months. Otherwise, changes in the master would affect and ruin changes in the 'lib' branch over and over again. + I definitely don't have enough time to start such a huge side project :( I've workarounded this issue by developing macro which allows me to write and run bash as a rust function. Ugly (I mean solution, because the code looks good), slow, but easy to use and brings the power of core utils and all other shell utils to rust.
Prior art: C++ abbreviated [`mem_fun`](https://en.cppreference.com/w/cpp/utility/functional/mem_fun) into [`mem_fn`](https://en.cppreference.com/w/cpp/utility/functional/mem_fn) when they have to choose a name for the better replacement. Rust as a spiritual successor of C++ inherited the same approach to abbreviations, so the `n` is obviously the first one.
####BIG NDIAN and ^little ^ndian
[False](http://thedailywtf.com/articles/it-takes-one-function)
I pronounce fn in my head as fun, so I'm gonna say it's the first n.
Since I'm not able to change font on my mobile phone I just can't read the article. Being informal doesn't mean you should take the most unreadable font. I'd like to read the article but I merely can't
# From vs Into I read the docs of From&lt;T&gt; and Into&lt;T&gt; and it says From&lt;T&gt;: "Simple and safe type conversions in to Self." Into&lt;T&gt;: "A conversion that consumes self, which may or may not be expensive." Can I take from this that one would implement From&lt;T&gt; for safe and cheap conversions whereas Into&lt;T&gt; would be implemented for an expensive conversion? Since if something that implements From&lt;T&gt; automatically gets Into&lt;T&gt; but not the other way around it would mean that if expensive conversions only implement Into&lt;T&gt; you could ensure that types that implement From&lt;T&gt; are always cheap (unless of course someone doesn't follow this standard).
I just see that Into&lt;T&gt; states: "Library authors should not directly implement this trait, but should prefer implementing the From trait." But it feels like that doesn't make sense, since that would mean that we should only implement From&lt;T&gt; which means that all conversions are only simple (cheap) and safe conversions. Or do I misinterpret "simple" for "cheap"?
It was actually quite some time after I started learning rust that I learned that data constructors in Rust are also functions. I thought that to, for example, box every item of an iterator in an Option, that you'd have to do `iter.map(|x| Some(x))`, but instead you could treat the enum conductor like a function and write `iter.map(Option::Some)`. This blew my mind when I first saw it until I thought about how it's exactly the same in Haskell.
&lt;3
Not really angry programmers, just those that manage their anger *by* programming. Leaving all the rage on the page. Coding as catharsis.
Short version: Closures have unique types, so you can use those as type parameters to make other types unique, which lets you tie an index type to a particular instance of a tree-in-vec type, so that you can't use the index on the wrong instance. But downside outside of use in a single function is that you'll need to add type parameters everywhere. I think there may be a flaw in the logic, though. The same closure at the same point in the code always has the same type. So two calls to the same function will create interchangeable index types. Now whether they could leak from one call to the other, I don't know. (This is interesting because it is kind of related to the kinds of problems I was looking at in the `qcell` crate, and GhostCell.)
Go not using libc is exactly my reason to hop on this crappy platform with full confidence that musl won't affect me (hell, CGO_ENABLED is on by default, but fuck that). gcc-go is a thing as well, but it's not as optimized.
haha funny
Thanks for your summary and input! I think you are totally right with regards to the flaw in logic. Not immediately clear how to best solve that. Maybe if we change the trait bound to something like `FnOnce(Empty) -&gt; ()` so it may never be cloned/ copied. We could store a `PhantomData&lt;*const F&gt;` in the indices. You've given me something to think about! ;-)
Eh, this is just poor wording. Everything that implements From implements Into, thanks to a blanket implementation in libstd. Why two traits? Well, they're forumlated slightly differently, and that means they're more useful in different situations, I believe, but I forget the exact details.
The standard library will have two things: Futures, and Executor. Executor is the API that an event loop needs to provide. However, the standard library does not provide an implementation of Executor, only the trait itself. Tokio's role is to be one of those possible implementations. Thanks to this way of constructing the API, any async/await calls turn into Futures, which can use any Executor. Does that make sense?
Should be the same as calling a C ffi function. In this case, the overhead provided by the JNI might actually exceed the speed up of Rust's native code, meaning you might get better results from using Graal, which can directly execute the LLVM code that Rust emits without the overhead of leaving the JVM. While it probably wouldn't be worth it if you already have a large portion of the project already written, there's always the possiblity of rewriting the rest of the codebase. :P
It must be fate :) I was just looking on how to contact the Documentation Team (you specifically ;)), if I can maybe help to improve the documentation here and there.
That'd be great! A good first step would be to open up an issue, and say that the descriptions are confusing. You've already basically written it with the above comment. We can then go from there :)
I will get working on it right away :) Thanks!
What does the 'Option&lt;usize&gt;' end up as? I guess that's 64bit for the usize and then.. hmm an ugly extra bit somewhere or what? It would be nice to be able to squash that down to a single 64bit.
I'm interested in what you come up with. GhostCell uses a closure to create a unique lifetime with the execution of the closure. This seems sound as far as I can tell (it's not my idea, or code). But downside is lifetime annotations everywhere. TCell (in qcell crate) needs a unique marker type to make it distinct from all others, but checks that at runtime by using a static type set, to avoid two instances using the same marker. If using closures like you did offered unique types at runtime for each run through, I could maybe have used that instead in TCell and avoided the runtime type check. But so far I can't see a way to make it work, unfortunately. So I think the closure is effectively working the same as using a unique marker type, just with less typing.
It's totally possible using [NonZeroUsize](https://doc.rust-lang.org/std/num/struct.NonZeroUsize.html), but I decided not to introduce it in the post to keep it at least somewhat focused ;-) You are correct that `Option&lt;usize&gt;` will be a wasteful 16 bytes on 64 bit machines.
I'm all in for all the fancy fonts, as long as I can read them without my eyes hurting. I think a slightly more generous letter spacing could do the trick. The text does look squeezed.
Here's an example of triggering the panic by reusing the same closure in two different trees: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0ac931b35baf19551ffe99c025c88f14
Thanks for writing these, the first one was very helpful for learning about ownership. 
Funky.
Right.. it seems the idea above didn't prevent that either. Back to drawing board I guess :$
Is there a way to run tests under this?
Okay, how about this: by taking a mutable reference to the empty value, the closure can't be cloned/ copied any more! https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=7b4be2ac99626a75714265dd63608fad
`Option&lt;usize&gt;` ends up being twice as big as a `usize`, because it needs to be a multiple of `usize`'s alignment.
I opened one over [Here](https://github.com/rust-lang/rust/issues/59163
so it might be de*f*i*n*e function?
I'm not sure how to get that example to build, but I don't think it's going to work the way you want. Having a closure take an `&amp;mut` reference as an _argument_ isn't the same as closing over an `&amp;mut` reference in the _body_. Arguments are passed in when the closure is called, so copying the closure itself doesn't copy any arguments. See https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=acfd87d46ba35accd662660e30222249.
Yes, thank you!
Oh you!
Yeah, my reasoning would also be function -&gt; func -&gt; fun -&gt; fn
Ah yes, I see now. Thanks for the pointers!
Yeah, when I think about scientific programming I think about MPI and Fortran. But others think of MATLAB and Julia. I've even spoken to people who consider Spark and Hadoop to be scientific computing tools.
Or **f**un**c**tion with a sideways c and an unintentional smudge?
Sorry, but in that font the article reads like: AbUsInG RuSt'S TyPe SyStEm FoR SoUnD BoUnDs ChEc ELiSiOn.
Exactly! But there you can‚Äôt pattern match on any old function foo, so there is some kind of distinction. And you can restrict the type signature of ordinary functions to be less than the absolute most general one that makes sense, while you can‚Äôt for the raw constructors. This is where the GADT comes in.
You can `impl Trait for ()`, and call the functions without any data.
f2n
We are not allowed to have fun
Ahahahahaha....I found this way too fnn-y
Only it‚Äôs not an *f*, it‚Äôs the *t* flipped upside down!
I think the issue is that although statute encodes the letter of the law, and case history provides examples, ultimately the outcome of a case depends a lot on interpretation on the part of the judge. And, lawyers are paid to make claims like, "The defendant has a `for` loop in their code, thus it is clearly a derivative of the original, which also has a `for` loop, and you should award us $10mm of damages." So even if something seems cut and dried, it's hard to be sure, and even if you would win a case if you went to court, it might not be worth the time and money to fight it.
Well, probably the people who wrote it had some idea it existed. :)
Nice talk, thanks for sharing! (I LOLed at the "U WOT M8?" part pretty hard.)
Or, hear me out here, is it *f*unc*t*ion with the t turned 90 degrees and mirrored?
While a nice achievement, they couldn't really use ripgrep with exactly their use case. So, it is likely that rust would still be faster. So, the title is misleading, the article itself seems interesting.
I'm not sure what you mean. They couldn't use it because of Haskell FFI constraints. The Rust implementation was slower for UTF-16.
Stndrd prgrmmr spllng. Drp th vwls nd rmv lttrs frm th nd. Vwls r dmb.
we have a letter 'm' (nn), a 'double-U' (uu).. could we add a 'un' with the two inner vertical lines merged
My first thought was to make the a wrapper that allows read access to the actual enum, but not write access. Users would be able to instantiate illegal instances of the inner enum, but functions that use it would be made to take the wrapper which can only be instantiated though the wrapped constructors. That might work: both types are public, but only take the outer one and implement `Deref` but not `DerefMut` to allow pattern matching.
&gt; MPI and Fortran. Me too. I think about PDE solvers, other things about machine learning and R and data science.
`cargo new` will spin up a Windows XP QEMU instance. For maximum consistency, we will run QEMU in Windows Subsystem for Linux, with other platforms running QEMU in WSL in Wine.
There's only fun in F#...
function (JavaScript) -&gt; func (Swift) -&gt; fun (Kotlin) -&gt; fn (Rust)
Only in F#
Thanks for posting this, I thoroughly enjoyed riding along with your thought process instead of just seeing the final idea. I was confronted with somewhat of a similar problem in a project I was working on. So much is so perfectly safe and guaranteed at compile time, using indices that might panic on range violations just felt wrong. I was curious if a safe abstraction was possible. It seems like the jury is still out!
Why post in /r/rust and not just in r/haskell? Other than click bait title, not sure how relevant it is to Rust?
Appreciate the kind words. Thank you!
dear god burn it with fire
In my case it's mostly ML and numerical analysis. I usually refer to MPI/cluster computing as HPC.
That's really cool, and well explained. Thanks! I feel this is potentially useful in other cases (where it might be more of a gain too). It'd be really nice if there was a supported way of expressing this in a way that made the intent obvious. Bonus if it also supports creating more helpful error messages. Second, I also wouldn't be so quick to dismiss this approach for this use case. Yes, branch prediction shines at making the costs for (some of) these kinds of bound checks magically go away. On the other hand, it's becoming pretty clear that the concept of branch prediction in processors was a big mistake from a security point of view, making it impossible to actually isolate different tasks on a processor. It'll take us a really long time to wean ourselves from that, but I wouldn't be too surprised to see processors in the near future with the _option_ to disable branch predictions, if, say, you're running really security-sensitive workloads and it's worth it to trade away some performance against immunity from all the Spectre-like attacks. Then, the ability to gain some of that performance back via the type system would be interesting.
&gt; There are neither good bindings to these Unfortunately, C++ is binding adverse :/ The only languages which seem to have good bindings for C++ are languages compiling down to C++, and those are restricting their semantics to be expressible in C++. Rust does not have type-based aliasing, like C++, so cannot compiled down to standard C++. *(And to be honest, type-based aliasing is a work-around against the lack of proper aliasing information to start with, so I'd rather NOT have it)*
https://gtk-rs.org/
That's real smooth.
Shouldn't that be `fm` then?
I think a better option would be just to put a disclaimer at the start of the article to denote the same. 
&gt; E: holy downvotes! Am I rubbing people the wrong way? I think it's just a way to show disagreement, either with the premise, the presentation or the conclusion. I think the premise is fair, personally, and that having open-source products/libraries in a language are one of the measures of success. However, there are *many* open-source products/libraries written in Rust (`exa`, `pijul`, `redox`, ...) that are you not mentioning: Why? Are you unaware of them? Unconsciously ignoring them? Purposefully ignoring them because...? Moreover, looking at the list of Go projects mentioned and the list of Rust projects mentioned, I see 2 vs 2, with Rust having had 3x less time (3 years vs 9 years). I would naturally conclude that Rust is doing better than Go. You may want to review your arguments; they don't seem to support your conclusion.
Rust being used as the baseline for an optimization instead of C is pretty impressive to me. It shows that the language is becoming established as a better systems language.
It's east-const vs. west-const all over again :O
Fury, Nick
See [this comment](https://www.reddit.com/r/rust/comments/b08oex/cargoflamegraph_generate_flame_graphs_directly/eidp5e3/). It's not currently supported, but hopefully [soon](https://github.com/ferrous-systems/flamegraph/pull/11#issuecomment-472178617).
Good plan :) don‚Äôt forget the data importing/exporting part since it‚Äôs quite often the painful part, especially if you‚Äôre integrating some scientific rust stuff into a larger existing ecosystem
Are you using [yaml-rust](https://crates.io/crates/yaml-rust)? I'm not seeing trait implementations that would allow you to do this as written; firstly, there's no `IndexMut&lt;&amp;str&gt;` implementation that would allow `window[key.as_str()] = value.clone();`, and secondly there's no `IntoIterator for &amp;mut Yaml` that would make the `for` loop work as-is. You would need to match on the `Yaml` manually to mutate the inner map: fn enrich(window: &amp;mut Yaml) { let window = match *yaml { // this match is done internally when indexing by a string Yaml::Hash(ref mut map) =&gt; map, _ =&gt; return, }; for (key, value) in template { // `window` is now a `LinkedHashMap` which has an entry API similar to stdlib's `HashMap` // it requires an owned key which requires cloning but saves a hashmap lookup // which is probably more computationally expensive anyway window.entry(key.clone()).or_insert_with(|| value.clone()); } // simultaneously get the "windows" key and check that it's an array, otherwise this doesn't make sense if let Some(&amp;Yaml::Array(ref mut windows)) = window.get_mut("windows") { for w in windows { // `w` is `&amp;mut Yaml` enrich(w); } } } The trait implementations that are missing seem like they would be sound enough and they would avoid this papercut. Perhaps you'd like to [open an issue on the repository](https://github.com/chyh1990/yaml-rust/issues/new)? 
Or f(n) 
Am I missing something or is there no `--exec` argument?
Back in the days I remember toying with lifetimes to achieve the same effect, which unfortunately was not possible: there is no guarantee that the compiler will not merge two lifetimes for "open" scopes. This is why the API for `scope_thread` etc... force you to pass a closure which receives an argument with a lifetime :/ The idea of using closures is pretty clever, given their unique types. There is a first hurdle that you hit here: if the closure is `Copy` (or even `Clone`) then it can be passed around easily, as demonstrated by [u/oconnor663](https://www.reddit.com/r/rust/comments/b0nqp9/abusing_rusts_type_system_for_sound_bounds_check/eig82io): fn main() { let closure = |_| (); let mut tree1 = our_tree::Tree::new(closure); let ix1 = tree1.insert(our_tree::Node::new(5i32)); let mut tree2 = our_tree::Tree::&lt;i32, _&gt;::new(closure); // try using the index of one tree for the other tree2.index_mut(ix1); } But there is a second hurdle which is even worse, in a sense: even if the closure is not `Clone`, two closures created by at the same "spot" have the same type: fn creator&lt;T&gt;() -&gt; Tree&lt;T, impl FnOnce() -&gt; ()&gt; { Tree::new(|| ()) } fn main() { let mut tree1 = creator(); let ix1 = tree1.insert(Node::new(5i32)); let mut tree2 = creator::&lt;i32&gt;(); // try using the index of one tree for the other tree2.index_mut(ix1); } If you can figure out an ergonomic way out of there (borrowing would be annoying :x), then you may have found something pretty rad!
As a neophyte I fnd myself wondering why we don't abbreviate it `fun` for jovial symmetry with the other front-of-the-line TLAs: `pub` `mut` `let` `mod` `use`. We still wouldn't know which `n` it is, but it'd be more funner.
I bet that's the decompiler's fault..
Is the "n" in "fun" the first or second "n" of the word "function"?
What is this r/keming
[#56732: Make the rustc driver and interface demand driven](https://github.com/rust-lang/rust/pull/56732): that's one hell of a PR! And I love to see progress on the `const` front :)
Lemme get uuuh
Does the [GhostCell](https://github.com/ppedrot/kravanenn/blob/master/src/util/ghost_cell.rs) approach look sound? The claim here is that the `'id` produced in one closure will be different to any other closure *at runtime*. So it's a step beyond a unique marker type or a unique closure. I tried creating two nested `'id` lifetimes using GhostCell and the two were checked as different by the compiler. So maybe this would give the OP a solution.
Try your browser's reader mode, should use its own font
Ripgrep is distributed with every version of vscode iirc
As of [0.1.9](https://github.com/ferrous-systems/flamegraph/commit/b39f56872b0b4234e03dfb137971f21112b570c0), the command for running arbitrary binaries is now: ``` $ flamegraph [-o my_flamegraph.svg] /path/to/my/binary --my-arg 5 ``` See also the updated README.
And also because Rust users ‚ù§Ô∏è Haskell, of course.
Thanks!
If it was the second the abbreviation would be "fcn". Or maybe "ftn" would be better even if not as regularly formed, after all, we're not naming fsck, here.
Make an RFC.
A true UTF-16 version, working with `u16` code units, may be faster than the 2-`u8`s version.
I was trying to find a CLI option parsing library the other day. Web search and crates.io search were both giving me unpopular libraries that I did like. I asked a friend and they were like, "why not clap?". Some how both clap and structopt were not showing up in my searches. So we need to get the word out more I guess? Anyway, I ended up using structopt and was done in about 15 minutes.
One simple thing you can do is run clippy over your code. You don't necessarily have to make the changes it suggests but it should give you a sense of things that people cared enough about to turn into a lint.
This is only really a limitation of the way the Text datatype was constructed, not of the Haskell FFI in general. If there were using the ByteStrings datatype no conversion would be necessary.
Oooh. That‚Äôs interesting. I didn‚Äôt realize that dereferencing was overrideable in Rust. I‚Äôm not sure I can get Gadt‚Äôs out of this, but it is an angle of attack that would have never occurred to me. Thanks!
Abstractions that don't have a runtime cost compared to eschewing the abstraction and writing the code without it. Usually requires a compiler that that chew through the layers of abstraction. For the most common example in the Rust community, our iterator chains often compile down to the same assembly as simple c-style for loops.
Even further back, BASIC had DEF FN https://en.m.wikipedia.org/wiki/User-defined_function
Desktop link: https://en.wikipedia.org/wiki/User-defined_function *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^244110
Part of the fanciness of the gadt pattern match is that in each branch you learn something different possibly about type parameters, but then the results in each branch re unify kind of out side of the match. It‚Äôs not clear to me that I can achieve that this way.
The most real fu I've ever seen 
Nice. What are you planning to do with the mutations?
I'm going to go against the grain here and say that `Result` tends to be overused in the Rust ecosystem. Think of how your user will handle errors and use `Result` when two things are true: - This operation may fail - This is a convenient time for the caller to deal with failure If there is almost never a meaningful way to recover from an error, panic instead. Also panic if an error condition is primarily due to a bug with how your interface is being used (like an index out of bounds). If an operation requires multiple method calls to meaningfully perform, avoid reporting errors at each step. In that situation you already have a stateful object-like thing; don't make it a *hard to use* stateful object at the same time! In that situation it makes sense to define sane error states, so that if one step fails the code can fall through to a point where it makes sense to collect the error. So in this situation you have `get_user_config()` which can return errors which have different levels of badness. This is tricky. - Probably the tool wants to continue silently if it can't find a user config file. That's the least-surprising thing. - But if the configuration file is badly formed then it should fail loudly and early. It would surprise the user to ignore their configuration if it exists. So not only should you use `Result` (one-call interface, error occurs due to I/O, the best behavior of the program depends on error detection) you should also think about the `Err` type and how it communicates the difference between an error that may/should be ignored and one which should be fatal. The `Result::or_else` method can be used to filter out errors, but it's a bit messy. I'm considering an extension method: impl&lt;T,E&gt; OrMaybeExt for Result&lt;T,E&gt; /// Filter errors, ignoring them by returning `Ok(None)`, by introducing a fallback value of /// type `T`, or by transforming them to a new error type. /// /// `Ok(x).or_maybe({...})` evaluates to Ok(Some(x)) /// /// `Err(e).or_maybe({...})` is evaluated by the provided closure fn or_maybe&lt;F, O&gt;(self, op: O) -&gt; Result&lt;Option&lt;T&gt;, F&gt; where O: FnOnce(E) -&gt; Result&lt;Option&lt;T&gt;, F&gt; However, that might be too generic too early for what you need.
Maybe some kind of trait function that allows for reboxing up the revealed type variable on the right hnd side of the match.
I gave the new `flamegraph` and `cargo-flamegraph` tool a try on SmartOS/illumos and it worked out of the box now that DTrace support has landed. This is awesome, thanks for the awesome work!
I hope that's a joke. If someone writes that way with modern editors, 
&gt; ptr_fun Implying pointers are ever fun
What about javascript's `Immediately-invoked Function Expression` (function () { console.log("kill me"); })(); 
From this [HN post](https://news.ycombinator.com/item?id=12270428) &gt;A "zero-cost abstraction" really means that abstraction doesn't impose a cost over the optimal implementation of the task it is abstracting. Some things‚Äîlike chaining a dynamic number of (arbitrary) closures‚Äîfundamentally require some sort of dynamic allocation/construction, and so a zero-cost abstraction would be one that it only does that dynamic behaviour when necessary.
It's explicitly undefined in stable Rust. However, you can **force** it to be the last 'n' in nightly with `#![feature(fn_final_n)]`. Just be careful when writing production code that relies on that as that feature might not ever make it to stable.
Been working on driving strings of Neopixel/WS2812 RGB LEDs from a Gemma M0 board; once I learned that `cortex_m::asm::delay` existed, it became pretty easy to bit-bang the output pin from there. &amp;#x200B; I think I worked out a [nice abstraction](https://gitlab.com/Tangent128/cortex-lights/blob/master/lights/src/lib.rs) for the light strip; might clean up and publish it if I hear of anybody else interested.
Unfortunately due to the dependency on VS2017 you cannot use msvc under Linux. I think that you can still use the gnu tool chain, however, by installing mingw-w64 x86_64 under Linux. You also probably need to change the linker in your cargo config file to be the ld linker used by mingw. Unfortunately i cannot provide the exact packages by I hope I have pointed you in the right direction
If the pattern match reveals an appropriate type casting function in addition to the regular data, I suppose this becomes roughly the design of the leibnitz equality based fake GADT. No sane programmer will ever dare tread here.
&gt; [...] or, worse, it may have been assigned to a different process, in which case dereferencing my_reference would be an illegal operation and crash our program. Am I missing something or is this not at all how virtual memory works?
Yea, I kinda feared that msvc would be too dependent on Windows exclusive programs. Good news as you said, the other option is likely still possible. Do you know if there's a flag that I can set to avoid permanently modifying the cargo config? I do appreciate your tip, thank you.
It is likely that you are not missing anything and that the misunderstanding is mine. I shall review that bit.
Possible X/Y problem insofar as the comparison to UTF-16 rust is concerned. `aho_corasick` crate works on raw byte slices, so when working with a UTF-16 haystack, instead of converting it to UTF-8 so your UTF-8 needles work, one could instead leave the haystack as is convert the needles to UTF-16, probably in less time than converting the whole haystack.
Here's a [GhostCell example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=55fae45f2093de5065c4c00ea1dd9ffe) on the playground to try out. It's the same code from that GhostCell link, just cut down and with types renamed. As it is, it fails to compile because the wrong owner is used to access the cell. Commenting that line lets it work. This is for owners and cells, but adapting it for indexes and trees should be doable.
Of course it is! :D How else would there be 5 different smart pointers in the standard library? (Not including guards, built in pointers and references, and specialized smart pointers.) The doc page for [`Deref`](https://doc.rust-lang.org/stable/std/ops/trait.Deref) has a total of 20 implementations.
hp s 2
If the context of this piece is indeed on a operating system that implements virtual memory (such as Linux, Mac OS and Windows) then accidentally dereferencing memory belonging to another process will not happen. At least not in such a trivial way as the example (it would at least involve some system calls).
Typo? s/let path/let reader/ ???
Oh, haha that's a typo in reddit, but it's correct in the actual code ü§¶‚Äç‚ôÄÔ∏è
You could always just run an http crate like warp, then do your UI in react or something.
My impression is that they did indeed do this as part of their benchmarking. They also benchmarked the UTF-8 variant. But I haven't read their code yet.
Looking a bit closer, I think you're correct. I went searching for some code as well, but couldn't find the benchmark code.
Fiiiiiixed. Thanks :)
This isn't a question per-se, but I would appreciate a review of my crate before I publish it to crates.io: https://github.com/phayes/fdh-rs
You can modify the .cargo/config file once per machine.
Performance, bare-metal capability, error-handling unity and syntax constitute nearly the sum total of Rust's advantages over Haskell - thus it is interesting to see significant progress on one of those items.
Right, it's not allocated to another process, but it's outside your own allowed range? Appreciate the pointers.
No, it is just Fn, the key in your keyboard
I'll post more in the Gloo issues, but I think top priority is setting up Gloo's endpoints/interface, to accommodate the decoupled components. Maybe with traits?
Just get rid of the `f` entirely . We Haskell now 
Highlights since v0.2: - Complete rework of routing system - Complete rework of fetch module - Complete rework of the update system for things other than (click etc) events - State updates can either re-render, or not - Attributes and Events now use enums by default, instead of strings - More flexible text node handling - More utility functions - More examples, eg websockets - Lots of breaking syntax / fn sig schanges
Performance and bare-metal capability are givens. Syntax is subjective; Haskell's is simpler but is also less familiar for those used to C/C++/Java. You'll need to elaborate on error-handling unity. Haskell has a pretty outstanding error handling story when you combine monads &amp; their friends, purity, and laziness. For example, `sequence :: (Traversable t, Monad m) =&gt; t (m a) -&gt; m (t a)` can be used to convert `[Maybe Foo] -&gt; Maybe [Foo]`. Laziness can be used for things like `when :: Applicative f =&gt; Bool -&gt; f () -&gt; f ()` to invent your own control flow, e.g. `when condition $ throwError Foo`.
I'm not sure what your question actually is. I THINK, like others here, that you're asking what zero-cost abstractions actually are. Is that so? Also, do you mean "no assembly"? In answer to the supposed question, I would point you to Bjarne's famous quote (about C++, but applicable here for Rust) that is generally accepted as a definition of "zero-cost": &gt; C++ implementations obey the zero-overhead principle: What you don‚Äôt use, you don‚Äôt pay for. And further: What you do use, you couldn‚Äôt hand code any better. ‚Äì Bjarne Stroustrup, ["Foundations of C++"](http://www.stroustrup.com/ETAPS-corrected-draft.pdf), pg. 4 
Are you using "high level" to mean more advanced? Because, to my understanding, "low level" means more detail oriented. C and Assembly are low level while JavaScript is high level. I would say JavaScript is much higher level than Rust because you don't have to think about memory management or types or a variety of other things with JavaScript. Even going from JS to Rust, you need to learn what iterators are and what `collect()` does since in JS you can just call `map` or `filter` or `reduce` directly on arrays without needing to collect anything or needing `iter()` for anything. That being said, I'd say Rust takes things as low level as C and makes them much closer to JS while it takes the upper end of JS's spectrum and beings it lower a bit.
It's both
Thanks! I've now updated the quote with your translation.
Oh, sorry, I didn't see the OSS requirement there. If you want OSS, then how about librsvg?
&gt; JavaScript is much higher level than Rust because you don't have to think about memory management or types That sounds like you've never programmed a large project in JavaScript. I've spent *days* hunting down memory leaks in code written in that language. I've also experienced bugs like 1+1 = 11 because the PHP programmer on the server decided to return numbers in JSON as strings. Making these issues more obscure doesn't absolve the programmer from caring about them. It makes it even harder to fix them (serde_json would have bailed out immediately with that type issue, for example). Anyways, I'm aware that it's a bit off the tangent on the actual topic.
Remember when C came out it was considered a high level programming language. It still sort of is. I think the bigger distinction is between modern language features and the previous generation of languages. Rust gives us the ability to do OOP or functional programming, and we aren't stuck with structs with function pointers or other strange things. We can use generics and nicer enums, but we still have the zero cost abstractions that earlier languages like C gave us. We have memory management where we can tell the compiler when we are going to handle things ourselves, and otherwise it should watch our memory use for us and guide our hands a bit. It fills the same gap that C and C++ can fill but with all those modern features. The worst part of C is having to re-implement the same scaffolding shit over and over, or import it from another project then learn halfway through that it isn't going to work quite the way you need because the original implementation was geared towards one particular use.
Just let them build it themselves. Why would you not? 
`f` and `n` are the first consonants in `function`.
Sort of. I‚Äôm thinking along the lines of internal vs external complexity, and logical vs algorithmic abstraction. In theory, you don‚Äôt have to worry about memory in JS, just like you don‚Äôt have to worry about pointers in Java. But in practice these are *very* leaky abstractions. You have to know an Object in Java is actually a reference, and in JS you have to think about types, you just can‚Äôt write them down. I‚Äôm battling some inexplicable memory leaks in Node.js ATM. In Haskell, the story is a bit complicated. It turns out that people often annotate that types should be unboxed. And one of SPJs visions for Haskell (regarding the ‚Äúopportunities for optimization‚Äù I mentioned) didn‚Äôt quite come to fruition. Namely, the idea that functional purity would enable implicit data-parallelism. I‚Äôm not really sure what I‚Äôm getting at. Maybe that‚Äôs the best takeaway of this post...
How do you know that? Why couldn't it be FUnctioN?
No, I actually don‚Äôt remember when C came out! Must have been exciting.
I think the terms ‚Äúhigh‚Äù and ‚Äúlow‚Äù originally just referred to the level of discourse above the native machine language. C was considered to be a high level language, yet it also abstracted the absolute minimum amount of working features over assembly - ie. no first-class booleans, strings because these are higher level concepts or abstractions. 
What's the value in projecting languages onto a single dimensional axis for which people are extremely unlikely to agree on the definition of? Qualifying a language as high or low level seems to have no value. If you tell someone you're proficient in using high/low level languages, it doesn't convey any real information to them unless you've already agreed on what that means. Languages have many dimensions. How many users or projects are currently using them? Which features do they have? How many lines does it take to write fizzbuzz? How long does it take to be proficient at a new language if you already know C? Or if you already know Javascript? How many times do you need to slam google to figure out how to write a webserver in it? To write tests in it? tldr: rust is a \~high\~ level language üå≤üå≤üå≤
While reading, had a feeling that I've already seen this approach described and used in Rust before, and, indeed: * Reddit thread: https://www.reddit.com/r/rust/comments/3oo0oe/sound_unchecked_indexing_with_lifetimebased_value/ * Crate utilising that idea: https://github.com/bluss/indexing Your post is explaining the approach really well though!
shes mad sexy too just an fyi
Have Travis build tagged commits for Linux and OSX, AppVeyor build for Windows, and push the resulting artifacts as GitHub Releases. Both platforms have excellent documentation for the process.
this sub name is brilliant
Don't push binaries to github. If I get something from github I expect to have to build it myself.
I tend to think of panics a bit differently. My conception of panics is that they always represent a bug. Any occurrence of a panic in the wild means that someone, somewhere, needs to fix their code. Panics should be used in two situations: 1. Signalling that there's a bug in the crate the produced the panic. 2. Signalling that there's a bug in the crate that called into the crate that produced the panic. In this second case, a crate should document proper and improper use of all functions that might panic due to inputs, and detail that a panic might occur if the function is misused with bad inputs. My two cents on panics. :) 
Regarding your Node memory leaks, I always use const unless I reaaaallly need to reassign, in which case I use let. I think this helps a lot with memory management because of stricter scoping rules, plus it's just a good practice since immutability should be default like Rust does it.
What if instead of using a raw closure, you used some type wrapper with an unsafe constructor and a `F: Fn()` type parameter? You wouldn't be able to construct such a type safely, but you could provide a safe *macro* that could construct such a type by always passing in a fresh closure. Sometimes I wish Rust had generative types that weren't a side effect of something else, but it's kind of surprising how many things it has that can be used to emulate it :P
It /should/ be possible to set \`CARGO\_TARGET\_X86\_64\_PC\_WINDOWS\_MSVC\_LINKER\` to \`lld-link\`, and have the .dlls and .libs from MSVC and the Windows SDK somewhere where lld-link finds them (possibly using the \`LIB\` environment variable). 
Can't understand what exactly is the problem that rustc is complaining about here. I want to load a URL using reqwest and load the result into a struct using serde. ``` use serde::Deserialize; use reqwest as r; #[derive(Debug, Deserialize)] struct S { attr: String, } fn fun&lt;T: Deserialize&gt;(url: &amp;str) -&gt; r::Result&lt;T&gt; { r::get(url)?.json()? } ``` The compiler is saying things like... ``` 23 | r::get(path)?.json()? | ^^^^ the trait `_IMPL_DESERIALIZE_FOR_ClusterInfo::_serde::Deserialize&lt;'_&gt;` is not implemented for `reqwest::error::Error` ``` Any pointers? Much appreciated :)
Both the haystack and the needle are UTF-16. In Rust we treat both as byte slices when matching, and we discard matches at odd offsets.
You might be interested in Firefox's reader mode.
What if it could be `f`unction `n`amed?
Seeing it on mobile makes it unbearable. I won‚Äôt be sleeping tonight 
You can use std::mem::uninitialized if you want to leave memory uninitialized - mind you ,this WILL fuck shit up if you're not careful, since the magic of RAII will try to free any data owned by the uninitialised struct, which will of course probably point to some random unallocated piece of memory
Rust is lower-level than JS, definitely. Generally the "level" is how close to the metal you are - e.g. assembly language compiles into pure machine code almost directly, so it's higher level than machine code even though it abstracts little aside from giving you more than convenient names for things (e.g. MULtiply becomes opcode 0x64). C would be higher level than asm because it abstracts a lot more, mainly functions and control flow, and the compiler will make more intelligent judgement-calls about the code to make (e.g. code inlining, dead code removal, loop unrolling, etc.) . Above that is languages like C++ that heavily abstract data structures with things like classes. Above that's Java, mainly for the garbage collection. Above that are the "scripting languages", like js, py, rb, php, swift, etc. Then you get the languages so high-level they try to abstract what most of us would traditionally think of as "coding". What you're describing is really the "abstraction penalty", but the thing about abstraction is it's only as valuable as how understandable and applicable the abstraction is to what you're using it for. This is the big reason rust devs will constantly champion their "zero-cost abstractions": because many things that drew people to higher level languages like js have equally clean perhaps clearer, semantics in rust. e.g. chaining a bunch of \`.map\` and \`.filter\` calls in rust will often get compiled down to equivalent machine code to a simple loop. You do that in js, and there's a penalty, but js devs will still be adamant that it's worth paying, because "it's easier to optimize clean code than it is to clean up optimized code". I'd put rust at maybe a hair above C. The main abstraction it adds is the memory ownership system - malloc and free get abstracted out, but not to the point where there's any runtime penalty. In fact, for code that \_works\_, I'd say you should be allocating and freeing memory exactly as much as you would in C, so it's barely more of an abstraction as ASM is to machine code. So yes rust is much lower level than JS, but that doesn't mean it's harder to reason about or more complicated. It may have a higher barrier to entry, but if your goal is to write equally semantic, low-defect-rate code as rust, you'll probably actually be slower writing in JS. Just look how many JS devs gladly pay an even greater abstraction penalty with something like TypeScript. Yet rust has those types out of the box, and does the opposite: they let you build faster-running programs, not slower.
'cause it's a huge turn-off for anyone who wants to use your tool, especially if they have no idea what a compiler is (nevermind needing rustc installed)
I mostly only use *let* now, and started using *const* for toplevel constants and requires. Kinda mixed feelings about const, since reassignment doesn‚Äôt prevent mutation. How can const prevent leaks, though?
If you‚Äôre trying to make a product for and end user, create a website for it and provide the binaries there. If the user doesn‚Äôt care about the source code they‚Äôve got no business being on GitHub anyway.
I've programmed extremely large complex Node projects. With any language, you'll run into problems if you don't use it in the right way. The thing about JavaScript is due to its idiosyncrasies it makes it ripe for abuse. I think also Rust also concatenates strings with the + operator.
If OP used that technique then we've come full circle and that's [this](https://www.reddit.com/r/rust/comments/3oo0oe/sound_unchecked_indexing_with_lifetimebased_value/) again. The idea of using closure types instead of lifeimes is kind of cool because it might not require placing code that uses the type itself into a closure where it has access to a fresh lifetime, which might improve the ergonomics a bit?
what about adding --sync flag to force waiting for response?
http://www.randomhacks.net/2007/03/10/haskell-8-ways-to-report-errors/
This looks like exactly what I wanted to build myself to learn more about the rust wasm story, will definitely be checking out the code and exploring how you built it all! Thanks for sharing!!
That sounds like about the right thing, but the docs say that mem::uninitialized is deprecated, but its replacement is unstable :/
Since const and let are block scoped (var is function scoped) they will probably both be dropped more quickly. Also, I would imagine stuff like `thing += something` could be responsible for memory leaks sometimes. I have no idea without looking at the code. My point is, IMO, being as strict as possible with scoping and assignment helps.
Sorry... I'm not a native speaker of either English or Japanese, so my translation would likely be terrible. I just thought that tweet was great and it's worth being included in QotW.
91 contributors and non-Mozilla... very nice.
I‚Äôm not aware of any further performance penalties over JS for using TypeScript
Oooh it would be cool to write an alternative frontend for rust as a proc-macro
Each repo has a releases section you can add your binaries to
This blog post is from 2007 and not representative of modern Haskell. 1. Use `error` &gt; The *most popular* way to report errors in Haskell is error, ... I'd like to see this backed up with numbers. From my perspective of using the language for several years is that this is emphatically wrong and represents unidiomatic Haskell. Moreover, this corresponds directly to `panic!("Division by zero")`. The `catch` function corresponds to `catch_unwind` in Rust. This is a fact of life when integrating with FFI. 2. Use `Maybe a` This is just `Option&lt;A&gt;` and is also used in Rust for exceptional behavior sometimes where the origin of the error is uninteresting. It is isomorphic to `Either () a` so I don't see this as another way. 3. Use `Either String a` Same as `Result&lt;String, A&gt;`. 4. Use `Monad` and `fail` to generalize 1--3 This (`fail`) corresponds roughly to using `Try::from_error(..)`. This is a design mistake and will become the same as `throwError` a la 5. 5. Use `MonadError` and a custom error type This is being generic over your error monad (`Maybe`, `Either`, `RandomEither`, ...). This is probably corresponds more directly to `Try`. 6. Use `throwDyn` in the `IO` monad 7. Use `ioError` and `catch` Both are deprecated. (I've never used these...) 8. Go nuts with monad transformers This is just 5 with explicit types instead of being generic; it hardly deserves to be mentioned as a different way. I note that 2, 3, 5, and 8 are all forms of `MonadError` where you just use do-notation and `throwError` and `catchError`. They just vary in their flexibility and genericity. If you want to count in this way, Rust also has at least panics, Option, Result, and Try. Oh, and this is before we go into differences between error-chain, quick-error, failure, `std::error::Error`.
Yes, I agree... [knpw.rs](https://knpw.rs) //shameless plug
AFAIK, Rustc will let you know if you let a `Result&lt;&gt;` return from a function without consuming it. This is the only type that, built in, behaves this way.
function in-lining is a good example. &amp;#x200B; normally, with a function call, there is all this overhead around calling a function. parameters are pushed on the stack or put in registers, previous register values are saved on the stack and popped off afterwards, etc etc. Their is some kind of code that is needed to get the job done. &amp;#x200B; With in-lining, this goes away. The code is just copied into the location you were going to use it and the code is then optimized in place to make it even better given the surrounding code. &amp;#x200B; The abstraction is the function call: a unit of reusable well defined code. This abstraction is reduced to nothing (and in some cases, \*less\* than nothing, actually removing code from the function since it won't be used!) in specific cases. &amp;#x200B; There are all kinds of these things in rust and other languages. Iterators are usually converted into loops for example, the abstraction of iterators are narrow enough that the smart compiler can go in and simplify things since what you can express with iterators is restricted in a specific way that the compiler can reason about it.
Check out https://github.com/japaric/trust, it's a template / tutorial for using free CI platforms to compile &amp; release binaries on all main platforms, without having to cross compile on your own computer, and doing things automatically on new releases. As others have said, you'll want to upload releases to the "releases" section in GitHub rather than pushing to the repo itself so that your repo doesn't become bloated. But trust's templates will handle that for you too, as long as you give Travis/AppVeyor access.
Not an answer to your question, but `opt.take()` is a shortcut method for `mem::replace(&amp;mut opt, None)`.
I have seen production tables with 300 columns. 100+ is not rare in the enterprise space.
The maximally correct way to do this (without using unsafe) is to use `Default`. Obviously this only works for types which have a default value, and in practice you'd only want to do it for cheaply defaultable types anyway (which is the case for most `Default` implementations). Dipping into unsafe, you'd use `ptr::read` and `ptr::write` and be VERY careful about you code paths: pub fn push_front(&amp;mut self, item: T) { unsafe { // If this panics (for instance, if we're out of memory and Node::new fails) // we're in a double free situation ptr::write(&amp;mut self.head, Node::new(item, ptr::read(&amp;self.head))); } } &amp;#x200B;
It's not generally legal to do this using uninitialized memory, as the compiler expects all memory in structs with references to them to be initialized at all point. At least I'm 90% sure this is a guarantee we need to uphold. The general solution is still to use an Option, or if your struct is huge, `Option&lt;Box&lt;YourStruct&gt;&gt;`. MaybeUninit will make using unutilized memory sound when it's stabilized, and you can duplicate it by making a 1-variant union, but I wouldn't recommend it. Even if your field is a union wrapping a struct and it's thus sound to set it to unitialiazed memory, you need to do a lot to ensure that either your code never panics while its uninitialized, or every other function is fully prepared to deal with the field being uninit. An option makes it safe, makes mistakes panics rather than memory corruption, and in practice shouldn't be too expensive.
These days, C++ people refer to them as "zero-overhead abstractions". The basic idea is that there's always going to be some cost to performing a task, but there may be added costs on top of that, depending on how you go about solving the problem. Historically, you've had to write ugly or tedious human-readable code in order to give the compiler enough hints about how to produce the most efficient machine-readable code. A zero-overhead abstraction is a way of writing pretty human-readable code that has been designed so that the compiler can still understand it well enough to produce the most efficient version of the machine-readable code.
Thanks! Thats good to know
Not to be confused with 'fmt'. 
Thanks for the link. So there's prior art to GhostCell's approach. That's very useful to know! I don't think using closures is any different to defining a `struct Marker;` in the code at the call site and using that as the type param, because it's just a private type which is local to that bit of code. It's only unique if you don't use it twice. You're certain that no-one else can use it, but you can't be certain that you yourself haven't used it twice without runtime checks. Effectively for things to be safe something somewhere must act like a singleton, which harms code reuse. It looks very similar to the `qcell` crate / GhostCell choices: fully dynamic, checked at runtime with an ID; type-based, but forcing use of singleton-style checks to make it safe; lifetime-based within a closure, reusable and free of runtime checks, but annotation-heavy. I'd be really interested if there was another way.
An abstraction penalty doesn't necessarily mean a performance penalty... it just means an additional layer you have to work through when you want to reason about the fine details involved in how things are behaving. For example, if you're transpiling TypeScript classes to a version of JavaScript with no direct equivalent or debugging in a context where you can't use source maps. It *can* imply a performance penalty but it doesn't have to. (eg. I just can't get excited about Haskell because it's an additional layer of thinking about how the abstraction between the physical machine model and Haskell's machine model will affect the real-world performance of my code.)
I'm not gonna lie, this is not even rustjerk- this is a genuine question which comes to mind when you're trying to make a mental model of things.
In this case github is not the right place to distribute binaries. Of course you can abuse git for that purpose but this it really not what it was made for.
Same, @p\[name=seamsay\]
Rust. We take the u out of fun. 
&gt; How would serde_json fixed the type issue? Probably the same way Python would fix it, except at compile time. As soon as the `"1"` from the server encounters a numeric type, you'll get an error about mixing types without an explicit conversion. Both PHP and JavaScript are weakly typed in the sense that they're eager to perform implicit conversions, compared to Python and Rust being strongly typed. (eg. JavaScript lets you do stuff like `alert("Your number is " + thing);` and it'll Just Work‚Ñ¢ whether or not it does what you expected.) (Python may be dynamically and implicitly typed, but it's strongly typed like Rust in that your conversions need to be done explicitly.)
The doc says it will be deprecated in 2.0 and by that time MaybeUninit should be stable. If you want to use something other than an option, put it in an option. Dealing with uninitialized memory is like walking in a UB mine field.
The most sensible answer so far (though it assumes that `fn` was taken from ML which might not necessarily be the case). Rust's original author is probably the only guy who can say for sure.
I thought the same when I read the title, but barsoap's answer seems more likely: https://www.reddit.com/r/rust/comments/b0mmz5/shower_thought_is_the_n_in_fn_the_first_or_second/eifvz6y/
Common, not even just a little? 
This is awesome!
Who is going to install software off github but doesn't know what a compiler is?
Travis has a Windows runner, is that not sufficient for publishing artifacts?
https://areweideyet.com/ is going to list most of the major options.
Abstractions will result in code being written differently - if you write js the exact same way as you would ts, then by design ts isn't going to add any runtime checks or map things to poor-performing implementations. What it will do is add an abstraction (the unusually backwards typing to an untyped language) that doesn't exist in the build target (plain js). A ts dev is going to lean on classes a lot more heavily than they would without it, because they're a much more useful concept with strong types. But you don't get any of the runtime optimizations for it, and are really just mapping to a loose-typed prototype, so it'll be slower than what a non-ts js dev might write with plain objects. I think it's a big part of why fp with simple data structures and occasional immutable class objects is popular for pure js devs, while ts people seem to like more complex classes whose objects are mutated procedurally. Going a level down, a js dev doesn't strictly have to use callbacks and things like `Array#map` or `Array#filter`, since that could probably be written slightly faster using simple loops. Even lots of break and continue might make things quicker. But you have better abstractions with the array methods and callbacks. Iterators too. It'll be interesting to see what direction this takes with wasm and ts. TS with JS as a build target makes typing purely an abstraction, but with wasm as the target it would actually build different, possibly more efficient code. Not quite at the same level as deciding how many bits are in your numbers, but fixed sizes on collections, knowing a class object's property will only ever be a number, true constant values (not just constant references), etc. 
RIDE but still in early stages.
My favorite is going "ooo I wonder how this library works" and opening it to see it is one file with all of the code within a giant one of these.
- [Tag releases ready for compiling](https://git-scm.com/book/en/v2/Git-Basics-Tagging) - [Setup Travis to build those releases](https://docs.travis-ci.com/user/deployment/releases/) - Figure out platform-specific ways to publish your package. For example, bat is published on Homebrew with [this script](https://github.com/Homebrew/homebrew-core/blob/master/Formula/bat.rb) - Looking at popular Rust projects is probably a good way to figure out how to distribute yours. ripgrep, fd-find, bat, and exa are all mature projects available from a lot of places, and you can probably use their scripts as a template for your project.
Ah you mean the struct in Rust would be strongly typed and wouldn't be able to serialize it into that. Gotcha. Yeah that would prevent it. Yes, JS does badly need a type system. They're actually working on that now (incorporating something like Typescript into ECMAScript).
FnAye!
Please don't take anything I said as criticism of a fine project. The difficulty in the code change is why I suggested a fork ‚Äî the lib split could be completed in a frozen repo and eventually "caught up" and re-merged. Anyhow, it would be a fun idea for a new Rust coder to work with, maybe.
This looks great!
There's a crate called [take_mut](https://crates.io/crates/take_mut) that does unsafe trickery so that you don't have to.
Oh no pointers aren't but *the adventures of dealing with them* are, hence, "pointer fun".
&gt; because the PHP programmer on the server decided to return numbers in JSON as strings. I've had to do this before because 64 bit numbers will get truncated in js. It caused a lot of weird issues
You need to disambiguate the type parameter of `.json()`: r::get(url)?.json::&lt;T&gt;()?
Intellij is pretty decent. 
I've had issues with people having to install dependencies.
Well GitHub here serves as free and simple hosting.
You still can? This is not for developers.
pages.github.com
Thanks, that is *exactly* what I was looking for.
Thanks, will do.
Damn, these all look amazing. Thanks!
That's still a non-zero amount of work. I'll probably make a website at some point, if I have more than 3 users.
I have to go with the same conclusion you came to: "lower along one dimension, and higher on another." It's type system and static checking are closely related to Haskell, but the same system is used to present low level machine details. Programming languages are simply too complex to order from "low level" to "high level" and there will always be outliers that don't fit anywhere until 1 or more additional dimensions are added to the equation.
LLVM IR is a form of C/C++. Rust is a form of LLVM IR. Form is reflexive and transitive (?).
For me this seemed to work perfectly for cross compiling 1. `sudo apt-get install gcc-mingw-w64-x86-64 -y` 2. `rustup target add x86_64-pc-windows-gnu` 3. `cargo build --release --target=x86_64-pc-windows-gnu --verbose`
Writing closures in Rust was mind bending for me, because I had never had to explicitly think about what closures bring into scope before. It's no surprise that closures are the #1 source of unintentional leaks in JavaScript and while it's great that functions are first class it's surprisingly easy to get bitten by bringing things into scope that you don't mean to.
Wait. This isn't stack overflow.
Or perhaps it's 'fn' so that you could bind the fn key on some keyboard to type fn for you?
What's so bad about it?
&lt;3
`hexyl`
Just ignore that one. It's there so that library-scoped globals don't leak out into the actual global scope.
Or, new function but function new.
http://areweguiyet.com/
If I understand correctly, it's because it's a strange and verbose way of defining scopes in JS. It's beautiful as a hack, but messy as a feature.
Read reddit description before posting (applies to whole reddit).
**Which Rust web framework is most similar to Python's Flask?** I've decided to improve my skills in Rust by porting a few projects from various languages. I'm going to start with a Python project that uses Flask. I am familiar with a few [Rust frameworks](https://www.arewewebyet.org/) (hyper, actix-web, and nickel) and I know of some others (rocket, iron, gotham, warp, tower-web). I know that none of these frameworks are quite as advanced as Flask, but in the context of porting an application, which framework would be the easiest to use? Thanks!
Awesome talk and (it looks like) an awesome library!
At work there's code where someone has used shortened forms of words as part of the function prefix. It's seems to have been done such that each prefix is same length. So for `programmer`, `function`, `spelling`, it would become something like: ``` PRGRMR FUNCTN SPLLNG ```
Another great project are encodings. I personally did start writing Rust code, by implementing a base64-de/encoder (it was so slow, that I didn't even have a chance against other cratesüôÑ). It was a fun project and helped me understand base64 a lot more. Implementing some kind of Cryptography should be interesting too, like Sha1/2/256, MD5, AES,.... Another project I can think of would be algorithm, for example finding duplicates of multiple bytes in a binary (very hard, not language wise but brainpower üòï)
Gtk-rs is your best bet today. That said, I'm excited about Azul, someday...
it's at the very least pleasing
What about it? I imagine one could implement such a thing locally by adding a loop which sleeps, then checks whether your update has been processed (and breaks when this is the case).
Shameless plug: https://crates.io/crates/runner. Similar use case to cargo-script, different strategy - keeps a cache of compiled crates that can be shared. With default dynamic option (i.e. compile against the Rust runtime shared libraries) can take &lt; 300ms to compile and run smaller programs. These may be in the format of 'snippets' inspired by rustdoc
Yeah, the only benefit closures have is that it's anonymous and unnameable, but it's not really that much different than like a macro generated identifier or something. Generative lifetimes also have the benefit of preventing the type being smuggled out through globals, TLS, Any, etc which for lots of uses is really important for soundness, and 'static types don't have this benefit. If you need to add a lifetime, then it's not any less boilerplate really and still requires a closure so the borrow checker doesn't unify them improperly, so you could just only use the lifetime. For the language boundary work that I've done, that generative lifetime trick has been really useful and it's one of my goto tricks, but it's NOT terribly ergonomic, and the error messages kind of suck sometimes, but I don't have a great idea of what a better replacement would look like.
GitHub has a releases section for every repos for publishing binaries along with a changelog. https://help.github.com/en/articles/creating-releases
That‚Äôs the right way to do. Remembering to set it under the correct cfg (ie x86_64-Windows-gnu).
Testing that all of our tests actually would catch errors in our code.
I wrote this post a good while ago and forgot about it until recently. It appears that one of the last Rust versions changed `resolve`. Alas, I don't have enough time to bisect.
Thanks, fixed.
If the software itself is not a development tool, then anyone who googles it and finds the github repository. Even if it IS a development tool, it might still be useful to host binaries and/or installers if the tool is potentially useful to developers using other languages.
I'd lean on Rocket here
The pointers you're dealing with are virtual addresses. They only mean something within your process. They can be mapped to actual memory (in which case dereferencing "works"), or they can be unmapped (in which case it crashes). If you have a valid virtual address, and then it gets unmapped (making it invalid), there's no physical memory backing it anymore. Whatever happens to the physical memory it was mapped to before is irrelevant. Other processes don't factor into it. On a side note, you called accessing unmapped addresses and crashing worse than accessing mapped addresses that some other part of your process is using. That's wrong. When you have an address that you're not supposed to be accessing, crashing is the *best* thing that can happen. Everything else will silently corrupt data or crash at some later point, making it much harder to debug.
Pretty cool to see Rust on these little things :) Out of curiosity, what was your background on developing embedded software? (oh and I really appreciated your personal infrastructure and Raspberry Pi badge posts, thanks!) 
I propose we change the others to pb, mt, lt, md, and s!
It would be \`f6n\` if "n" stood for the last one. 
I‚Äôm a hobbyist but I worked with microcontrollers at high school and university... a long time ago. A few years ago I brushed up my skills and learnt more about ARM microcontrollers and played with what was the fairly early Rust support then and have been tinkering on and off since. 
It is a breaked 'o' in functi**o**n, which means you won't be trapped in Reference Cycle.
I know it's a joke, but Semitic languages, like Arabic and Hebrew, are actually written that way.
Hi, I have trouble describing my problem in words but I think it it is obvious what I want to achieve in this example: [`https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=418eb626b41d7a77544245575b282b6c`](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=418eb626b41d7a77544245575b282b6c) . If I understand the issue correctly &lt;I&gt; gets bound to the iterator type of the argument and because chain is a different iterator the types in the struct definition don't match up any more. How can I solve the problem? &amp;#x200B; Thank you very much in advance. &amp;#x200B;
Remove the f next and get back to good old B days. History is repeating. 
Something I've notices is that after making a few small changes in the project I'm working on and rebuilding it for whatever target, all of my dependencies are regularly recompiled. As a result, a single test can take a minute to complete rather than a fraction of a second. Is this normal? I've never really noticed this before but it has become rather frustrating. I feel like the compiled dependencies could be reused, but I might be mistaken.
I'd simply change the type of the `iter` field on `Test` to be compatible: use std::{iter, vec}; pub struct Test&lt;I&gt; where I: Iterator&lt;Item=usize&gt; { // it's idiomatic to use the module when referencing an ambiguously named type // like `Chain` (because of `std::io::Chain`) // or `IntoIter` (because every collection type has an iterator type named that) iter: iter::Chain&lt;I, vec::IntoIter&lt;usize&gt;&gt;, } impl&lt;I&gt; Test&lt;I&gt; where I: Iterator&lt;Item=usize&gt; { pub fn new(iter: I) -&gt; Test&lt;I&gt; { Test { iter: iter.chain(vec![4]), } } } If you want multiple constructors of `Test` that use different iterator adapters then that's a different solution entirely. I'd do something with `impl Trait`: pub struct Test&lt;I&gt; where I: Iterator&lt;Item=usize&gt; { iter: I, } impl&lt;I&gt; Test&lt;I&gt; where I: Iterator&lt;Item=usize&gt; { pub fn new(iter: I) -&gt; Test&lt;impl Iterator&lt;Item = usize&gt;&gt; { Test { iter: iter.chain(vec![4]), } } pub fn new_evens(iter: I) -&gt; Test&lt;impl Iterator&lt;Item = usize&gt;&gt; { Test { iter: iter.filter(|i| i % 2 == 0) } } } This does prevent the user from storing `Test` in a struct without hiding it behind `Box` or a generic parameter, though. You could create a private `enum` with a variant for each iterator type you intend to construct and implement `Iterator` on that, but that's probably a bit more in-depth than you're interested in right now.
Swift is not a ‚Äúscripting language‚Äù.
Thank you for the solution and the extended example!
This is probably because `MyStruct` implements `Serialize`, so the blanket implementation `impl&lt;T&gt; From&lt;T&gt; for MyStruct where T: Serialize` would also implement `From&lt;MyStruct&gt; for MyStruct`. That would conflict with the implementation in core `impl&lt;T&gt; From&lt;T&gt; for T`, which also implements `From&lt;MyStruct&gt; for MyStruct`.
Thanks - haven't really used it.
You've got to do the same thing in C++, but the compiler doesn't watch your back, so it's real easy to have a use after free.
The link to the online telegram team meeting seems to be dead. 
That it has actual use in js.
I thought 100+ columns are rare. But I was surprised to see a table with 121 columns at work the other day.
I believe it's based on `fun` from ml, which would make it the first `n`
is there a way I can fix that?
The missing ::&lt;T&gt; type argument was not the problem. The compiler seems to be able to infer the type argument for the json function just fine. Instead, the json function expects T: serde::de::DeserializeOwned, but unfortunately the compiler's error message is super bad here. The ? at the end was an issue, though, which lead to the appearance of reqwest::error::Error type in the trait impl error message. So thanks for your comment, it helped me figure it out! Cheers ps. Follow-up question, maybe someone knows: &amp;#x200B; What does the for&lt;...&gt; here signify? I've done some Rust programming about 2 years back and I can't remember seeing anything like it in any docs or error messages: \`the trait \`for&lt;'de&gt; serde::de::Deserialize&lt;'de&gt;\` is not implemented for \`T\`\`
i keep spelling fn as fun even though i don't use any languages that spells functions like that &amp;#x200B; and then when i write javascript i spell it "fn" every time not "function" help
Doesn't GitHub allow you to publish releases which can contain additional files like binaries?
gnome builder is awesome if you are on linux
Have you tried making a release build with embedded debug symbols, like this SO answer? [https://stackoverflow.com/questions/38803760/how-to-get-a-release-build-with-debugging-information-when-using-cargo](https://stackoverflow.com/questions/38803760/how-to-get-a-release-build-with-debugging-information-when-using-cargo)
[There is a Rust plugin for Intellij IDEA.](https://intellij-rust.github.io/)
Always assumed it‚Äôs the last n kinda like internationalization is abbreviated as i18n. function -&gt; f6n -&gt; fn 
Yeah, I think that's the way to go. Specialization would also work, but is still unstable.
Sweet to have an update to the fantastic series!
... kind of... some vowels are indicated by consonants that do double duty. The earliest examples we have texts in Semitic alphabets do indicate __only__ consonants--though Ugaritic has three different alifs which indicate different vowel qualities. Over time, certain consonants started to be used to indicate vowels /w/ for /o/ and /u/, /y/ for /i/ and /e/. At first only at the ends of words, then later to indicate long vowels, and today even some short vowels are indicated, but it's always ambiguous, and the fact remains that you can't really pronounce words written in Hebrew or Arabic just from looking at the spelling. (Of course, the pronunciation of English words isn't particularly obvious from their spellings either) Source: I used to teach Biblical Hebrew.
I get as far as /usr/bin/ld: unrecognized option '--enable-long-section-names' during the build (Debian buster). Any suggestions?
I know that, it just seems like a hack that having a good import system would have solved but now we are stuck with it.
This is a normal debug build. So yes, the debug symbol should be there. The backtrace is present when running from the build folder but not when copied. I have copied everything from the target/debug.
conflating language and implementation.
Really interesting series! Are you planning to cover some python interop (especially numpy)? That and internode communication like MPI would be super nice.
All programming languages are high or low level depending on how you look at them. When F77 was introduced, "functions" were considered to be super high-level and have a lot of overhead when compared with writing raw assembly. With functions you have to copy stuff into registers according to the ABI, etc. but when writing raw assembly one did not have to do any of that. I've to program in assembly every now and then for a couple of weeks at a time. When that happens, I am often happy that for some bits and pieces I am allowed to use a super high level language like C. 
The title is click bait but the article is well written and fair. "We did not benchmark our own implementation on UTF-8 strings, as it is specialized to 16-bit code units for Haskell‚Äôs Texttype. " They showed integrity by acknowledging this. Good on them for doing so. Unfortunately, oranges were compared to apples. 
One of previous attempts http://cis198-2016s.github.io/schedule/
I had a short experience teaching Rust at my university, but unfortunately schedule was very tight, so I am not sure if I was able to do a good job. I've based my [slides](https://github.com/newpavlov/rust-isp-2019/) on [CIS 198](https://github.com/cis198-2016s/slides) slides. It was quite outdated, but proved a good starting point. The hardest part was that in my case students had very different backgrounds, including C++, Java, JS, Python, Matlab and some weren't programmers at all. I've tried to explain things in low-level terms of memory representation, stack/heap, references, etc. and it was not easy to understand for those who programmed mostly in Python/JS, referencing numpy helped by only a little. I guess the main advice will be to prepare a large number of small exercises (and also give them as homework), this not only will help students to get a better understanding, but also will allow you to see hard to understand parts better.
How does it compare in performance against vanilla Javascript dom operations?
The talk is about extending Redis with Rust using its modules facility and cffi. This work is analogous to work done by /u/bluejekyll for extending postgres with Rust extensions. This would make for an interesting blog post. 
I bet it's a simple answer but every article I read and every answer here is, no pun intended, abstract. If I have a loop that's always performed ten times, would other compilers increment a counter and perform jumps? A zero cost abstraction would just put ten operations in a row? I could be completely wrong, and in fact I would be surprised if I was even close.
Ok, I like this answer. Are you able to go into an example of machine code that does versus doesn't have zero cost. A link to some good examples would work. My success with Google on this topic has been scarce to say the least.
Nice, I like this. My concern is does this make executables bigger or smaller?
hahaha I will never tire of seeing this completely unexpected joke.
You could try to use ciopfs to avoid the case sensitivity issues 
I've held a four-hour course. I just used the book directly. It worked fairly well. The biggest troubles were with variables and their names. Variables aren't mutable by default, but you can declare a new one with the same name and different content that overrides the previous one. Also, four hours wasn't nearly enough. I had to stop before traits and borrowing were introduced. There are a lot of basics you have to go through before you even reach the good parts.
Thanks for the detailed answer! I've recently discovered proc-macros and I'm shoving them in places where I probably shouldn't. To balance things I'm trying to keep the additional dependencies to a minimum :) My particular use case is compile time parsing a custom DSL which you provide as a string literal in a proc-macro, eg. `parse!("hello world")` (const eval does not apply as I want it to return a fixed size array where the runtime version returns a `Vec&lt;T&gt;`). Another one I made just spits out UTF-16 word array for a string literal. For both of these full blown parsing seems overblown. I know I shouldn't reimplement basic stuff like escape sequence parsing but it's simple enough and I don't want to overwhelm users with too much stuff to compile for an utility which seemingly shouldn't be needing it...
If you change the target then all deps must be recompiled, unfortunately. If you update or change the compiler toolchain, everything must also be recompiled. If you touch Cargo.toml or update deps or profiles you will likely see recompilation of deps. These seem fairly fundamental issues that can't easily be fixed due to Rust's preference for static linking. I try to delay and batch changes I know will trigger a large rebuild to moments when I'm prepared for it, but it doesn't take away the unpleasant feeling when it's unexpected.
thanks. still learning.. Often I do things twice, because I haven't really understood the consequencies of what I do in rust . Lots of refactoring etc.. 
Ah, that's too bad. I was dealing with some performance issues, so my workflow was as follows: 1. Make changes 2. Cargo test to see if nothing is broken 3. Cargo bench to see if there are significant changes 
Python interop is definitely going to be a team. I will touch on parallelism, but I don't I think I will cover MPI, I have never actually used it :/
I'm not sure why but when I load the .org site with cache disabled, in network tab of dev tools the wasm(small TTFB, long download) and favicon(very small file, long TTFB, small download) files take several seconds to arrive, while everything else is good within a few hundred ms. I take it that's more to do with where it's hosted though(along with my own location and connection speed). Netlify provides a free static host(if you don't need a backend for serving the site) and is pretty good on speed.
Agreed, I work on a pure rust implementation of \`ls\` tool at the moment, great way to get used to rust and its ecosystem.
It's not that there are types of machine code that do or don't have cost. It's that the compiler may or may not be able to see the most efficient way to translate higher-level code into machine code. (eg. It may generate more instructions than necessary because it isn't smart enough to recognize that some of the instructions are superfluous when the bigger picture is looked at.) A zero-overhead abstraction is an abstraction specifically designed so that the compiler will see an optimal translation for it. For example, the compiler doesn't need to do bounds-checking on every array access if it can prove that it's safe to only check that the last index you want is within the bounds of the array, then trust that the rest will be valid.
Thanks for sharing, I always enjoy reading your posts. In this case though, you lost me about half way through. I understand what you are saying with the first block in the code example, but I'm not sure how to interpret the rest of it. &amp;#x200B; Maybe it would be helpful to define the difference between a \`bound identifier\` and an \`assigned identifier\`? I'm also not sure what you mean by \`Given that the number of identifiers for most expressions will have one or two digits..\`. &amp;#x200B; It may just be that I am not the target audience of this post, as I don't have any experience working on the Rust compiler, and only minimal experience working with the AST via clippy.
It depends on the size of the function being inlined. You *are* making copies of the function, but the machine code boilerplate to set up for a function call and clean up afterward takes up space and you don't have that when you copy the function body into its call site. If you want to see what I mean, take a look at the [Godbolt Compiler Explorer](https://godbolt.org/). It'll colour code the higher-level language and the assembly so you can see what assembly corresponds to the start and end of the function.
It depends. What you described is called loop unrolling and it's used when the compiler estimates that there are few enough iterations that the cost of fiddling with loop variables and jumps is less than the cost of just putting a bunch of copies of the "do something" code in a row. If you have a loop in a language that does bounds checking on every array access, a zero-overhead abstraction might recognize that it only needs to check the length of the array once and compare it to the highest index you intend to access. If that check passes, it doesn't need to check any of the others but you still get the same automatic safety.
&gt; But you don't get any of the runtime optimizations for it, and are really just mapping to a loose-typed prototype, so it'll be slower than what a non-ts js dev might write with plain objects. That's actually not true. Modern JS JITs makes speculative optmizations based on inferred static types. These then deopt into a slow path if those assumptions turn out not to be true. As such, if using TS results in your writing more monomorphic functions, then it will likely be faster than pure JS code that didn't have that discipline. You can of course achieve this speed in pure JS (it's why libraries like lodash and bluebird are so fast), but TS makes it easier. &gt; I think it's a big part of why fp with simple data structures and occasional immutable class objects is popular for pure js devs I think FP with simple data structures is popular in JS for the same reason it's popular in Rust. 1. The language supports it and 2. It makes it easier to reason about control flow. &gt; while ts people seem to like more complex classes whose objects are mutated procedurally. IMO one of the best things about TS is that you don't have to do this. You can use most of the same patterns that idiomatic JS uses, with the benefit of having it strictly typed. People doing this mostly seems to be people who have come to TS from Java/C#. 
That shouldn't mess with anything, at least, it should only compile them twice, one for each of those two profiles, and then re-use them.
[https://doc.rust-lang.org/nomicon/hrtb.html](https://doc.rust-lang.org/nomicon/hrtb.html)
Try [this](https://github.com/rust-lang/rust/issues/33465)
Using Netlify. I was building in debug mode; switched to release. Is it better now?
I don't know - haven't benchmarked. A comparison to vanilla JS is based on how web\_sys performs, rather than this framework. &amp;#x200B; A better comparison for this framework is something like React, Angular, or Elm. I suspect poorly, since the vdom is not optimized. Would like to fix this; one of the biggest weaknesses.
I disagree. They compared the UTF-16 benchmarks directly. The only difference is that the Rust implementation retains its bytewise quality, where as the Haskell implementation uses pairs of bytes (utf16 code units). That's a totally fair comparison. Choosing a good title is hard. People are too quick to cry "clickbait" these days.
Would love to see that on github
Thanks!
The Blogpost is very good at explaining the todos and whatnot. What it is for me missing is the basic knowledge of rust, and why these explicit version numbers are used: cortex-m-rt = "0.6.7", cortex-m = "0.5.8", panic-halt = "0.2.0" But in the end very detailed for flashing and so on. Only the pinout for the stlink connector to the bluepill would be an addition. Thanks mate.
lmao how many of these "are we x" sites are there?
For back end website development I've mainly just used JavaScript and PHP. Does Rust in any way protect against or make it harder to do SQL injections? Is that something an ORM would do instead? Or would I also have to manually guard against that?
A fair few haha.
&gt; Is it better now? [Nope])https://i.imgur.com/qgEDgon.png)(`8 requests | 620 KB transferred | Finish: 5.05 s | DOMContentLoaded: 1.25 s | Load: 1.24 s`). I don't remember the wasm being 2MB so I must have already viewed it when after you'd done those changes. Not sure why it's slow for those two files(especially the favicon). I've noticed some similar TTFB issues with my own Netlify site, but they're only 500ms. The TTFB for the wasm is actually not too bad(225ms), I believe my connection here caps at 1MB/s too, so perhaps it's just a false positive :) Perhaps Seed could try build a static payload for the html to deliver first with inline CSS? Like Gatsby for React does?(static site gen, then rehydrates into a React app once the JS is loaded in). [This is with cache](https://i.imgur.com/mIwwVUD.png), just the long TTFB issue delaying the site all data is there, about 1KB transfers from checking if those requested assets have changed(which you might not want to cache, or have a low cache age on)
An interesting Rust features is that it doesn't *need* a course to learn the language: there's the book which you can read, do some exercises, and, voila, you know the language in a sense that you can get things done. This is in contrast to, say, Python, where, as far as I know, there's no single definitive way to learn the language. So in the Rust course I am teaching I leave "how to use the language" to homework and instead focus on "what can we learn about programming languages by studding Rust". For example, in the lecture about modules and crates I've spend relatively few minutes on the actual code organization, but spend quite some time explaining why "anonymous crates instead of global shared namespace" is different from other languages.
No - the implementation is a form of the language...
You could try [enquote::unquote()](https://docs.rs/enquote/1.0.3/enquote/fn.unquote.html). That's about as lightweight as it gets. It's not really documented but it does process escapes in the string as well. It at least seems to be reasonably well tested, but there's no CI config so you might want to check it yourself.
This isn't crying. This is pointing out a misleading title. If you were tasked with writing this need/haystack program entirely in Rust, you wouldn't use UTF-16 units. Evaluating Rust on something it isn't meant for and then contrasting its performance is misleading.
No, from some functions you just want to return ASAP fn nofun() -&gt; i32 { return return return return return!!!!!!!!!!!!!!!11111; } &amp;#x200B;
`macro!` is just an abbreviation from `macaroni!`, you realize that? As in, within these braces, beware of macaroni of perl-like syntax and other horrors.
Wow, this is indeed quite impressive! Although, is stack usage of three functions that call each other repeatedly really 0? Don't they have to store at least return address pointers?
Only locally.
Only locally.
Okay, I will look into it. Thank you!
Using idiomatic diesel (http://diesel.rs) should prevent SQL injections. Of course there are still ways to abuse it and shoot yourself in the foot. 
Yes, thats what they are proposing `cargo publish --sync` would internally (locally) recheck that it has been processed until it's done.
and this has to do what with rust?
There's nothing misleading in this blog post as far as I can tell. The benchmarks are entirely fair in their context.
Just looking at the source here - https://github.com/Freaky/cw/blob/c3e37ceee016ee8c54b9f344072cf3fc8aa69043/src/count.rs#L35 I'm not too familiar with using Self in the (sort of ) constructor. Also what's the ..Self syntax?
Is there a guide to do this?
/u/sbenitez taught (still teaches?) Rust at stanford and can probably answer some of this
On linux, `cargo install cross`, and `cross build --target= x86_64-pc-windows-gnu` works just fine. If you need to target msvc, you are going to need a VM with windows installed.
I can't promise that it's interesting but I did write a little bit about it [here](https://gsquire.github.io/static/post/rust-redis-modules/). I'd be happy to answer any questions too.
As someone who has used `Yew`, and has experienced build times for larger projects approach three minutes, how does Seed stack up? I've cloned the source and built some of the examples, and they took between one and two seconds on a quad-core laptop. How does this scale when working on larger projects, like for example [seed-rs.org](https://seed-rs.org)? An another notable difference from Yew is the lack of self-contained components with their own state. Instead Seed has one large Model and one large Msg enum which is responsible for all state and updates respectively within the app. I think this noticeably cuts down on the complexity of the framework, at the expense of making integration of third party components/crates hard (because they don't have access to the main Model / Msg types). Without looking too closely at the code, could it be possible to change the signature of update to: ``` fn update(msg: Into&lt;Msg&gt;, &amp;mut model: Model) -&gt; Update&lt;Msg&gt; {...} ``` and somehow wrangle the `El&lt;MsgFromThirdParty&gt;`s emitted from third party `view()` functions to work with the main application assuming that the developer writes the `impl From&lt;MsgFromThirdParty&gt; for Msg {...}`?
The second match of the macro is going to result in infinite recursion, as the macro arguments are passed along without modification. What I would do is something like this: macro_rules! statement { (let $var: ident = $expr: expr;) =&gt; { Statement::Let }; (let $var: ident = $expr: expr; $($rest: tt)+) =&gt; { Statement::Chain(vec![ statement!(let $var = $expr;), statement!($($rest)+), ]) }; } That way, in the second match you're taking out one statement from the macro arguments.
How do I implement fmt::Display on a type alias? I wanted something like type Grid [[char; WIDTH]; HEIGHT]; I ended up having to embed it in a struct to satisfy the compiler. 
A quick glance at the unquote lib you linked showed it is not usable: Rust has slightly different string escape conventions eg. `\u{hexdigits}` instead of the more usual `\uXXXX` and `\uXXXXXXXX` convention. The Rust debug formatter likes to fallback to generic unicode for `\0` returns `\u{0}`. All in all I'll keep my parser (it's not that hard to write these) and when trying something more complex seriously consider just depending on `syn`, now that I take a closer look at it it does tries very hard to feature-gate things.
While you're giving your class, it'd be very useful to take note of places where students require your intervention/help to understand what the compiler is telling them or how to accomplish something. Part of why some of the errors are good is because we use common mistakes that seem reasonable have very targeted messages. The more real work human gathered information we can get the better we can make the diagnostics. The ultimate goal is to make rustc behave almost like a tutor.
You cannot (unless it's an alias for your own type). If you could it would be possible to have orphan impl problem - for example, what would happen if `core` decided to add `impl Display` for this type?
Or in the case you want either a single `Let` or a single `Vec`, you could use this: macro_rules! statement { (let $var: ident = $expr: expr;) =&gt; { Statement::Let }; ($(let $var: ident = $expr: expr;)+) =&gt; { Statement::Chain(vec![ $(statement!(let $var = $expr;),)+ ]) }; } I think the macro expansion won't let you open up a `stmt` into `let ident = expr`.
I think we need to be a little more nuanced than the binary high/low. What about middle-level languages? I'd say Rust, C, and C++ can be considered middle-level languages because it's relatively easy to see the mapping to machine code in these languages.
This is very interesting, thanks for sharing
You should seriously consider making your course in English. I really like it. 
We used trust as an example to set up automated builds for [Torchbear](https://github.com/foundpatterns/torchbear), and it worked out very well (huge thanks to Darius and also Sharaf).
Thanks to the wonderful reddit rust community. I have managed to write a program that actually does something. It can be easily extended (with some constraint in 0.1.0), at least I hope so. I have improved the way I code, at least I hope so, and I have learned a lot, definitly. Constructive criticism appreciated. :) Thanks.
`Self` acts like an alias to the name of the struct. It's handy because it lets you do things like change the name of a struct without having to change the return type of every constructor function that you've made for it. The `..` part of the `..Self::default()` line is called struct update syntax. You can see how the function is initializing `self.path` to a particular value and then initializing all of the other struct members using `Self::default()`.
Spoiler...you should post this to r/playrust.
looks really cool. thanks! missing link about Decentralized sync, see [Syncing ¬∑ Issue #35 ¬∑ ActivityWatch/activitywatch ¬∑ GitHub](https://github.com/ActivityWatch/activitywatch/issues/35)
maybe u/DebuggingPanda has some advices. He made a [nice german course](https://www.youtube.com/watch?v=lQ36K1htRDY&amp;list=PL0Ur-09iGhpwMbNiVTBeHmIjs0GuIXhNg)) 
check the hyper docs for basic setup of your web server: [https://hyper.rs/](https://hyper.rs/) Once you have your endpoints, check out serde\_json for serializing your responses as json: [https://github.com/serde-rs/json](https://github.com/serde-rs/json) Once you're that far (e.g. a \`GET /users/1\` responds with \`{"id": 1, "name": "John Smith"}\`), you can start running those actions in react. [https://reactjs.org/](https://reactjs.org/) Often people use a components library like [http://material-ui.com](http://material-ui.com) to look google-ish, or [https://ant.design/](https://ant.design/) if you want something a bit simpler/less opinionated. If you have no webdev experience this is probably a lot to take on at once. What's nice is it's easy to write the UI separately from your rust app. Get all your buttons and fields looking good with some dummy data first. If you need to write something \_really\_ simple, you may not need a js lib at all. You could use plain old HTML and a little bit of JS to load/save the data, e.g. on a form: [https://developer.mozilla.org/en-US/docs/Web/HTML/Element/form](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/form)
&gt; If you have no webdev experience this is probably a lot to take on at once. Yeah, zero web-experience. I have a raspberry pi controlling my brewing equipment, and I currently set it up by `scp`ing a file with instructions to the pi. But I would like to change this into a server/client system, where i can handle the inputs on my laptop, and be able to interact with the server in case something needs to be changed on the fly (which is pretty much always the case).
Honestly, this might be a case where just doing a heap allocation and an `Arc`/`Rc` wouldn't be a bad idea. I'm not sure why you'd want to use multiple views into something at the same time, save for debugging -- wouldn't you just want to do `println!("{:#?}", view)` at that point?
**fu**n**c**tion fuc?
I think the identifier bit is saying that in most cases, the number of identifiers will be less than 100. I think the bound vs assign difference is that `let mut x = ...` is called a (let) binding whereas `x = ...` is an assignment. Bindings are also introduced in lambdas. So I think the point being made is that if you're shadowing the variable in a local scope (using let or let mut), it is alright to get rid of subsequent nested assignments as they're not visible outside.
I'm planning to take a bottom up approach, from how memory works to introduce the type system and the borrow checker to more high level features
sane or MFC, choose one :)
I'd fancy to know this too. Maybe LLVM will mark the local size 0 only if it manages to do a tail call optimization for that function?
I lead a book club at work where we read The Book. It went pretty well. The audience was mixed - some were coming from Python and/or JS, others from Java. Rust has a fairly complete type system, which was new to almost everyone. JS and Python are dynamically typed, and Java's generics are so messy that they almost don't help in learning a type system with real generics. So there was some friction there. There were a lot of questions about how things actually worked. Coming from super-high-level languages with interpreters/VMs, the very concept of writing code that compiles directly to something that *just runs on the CPU* was weird for some people. As a part of that, the stack/heap distinction was new, and there were questions about how that worked. Python/Java/JS all use heap allocation for nearly everything, and are all garbage collected. Because those languages offer no choices, they achieve a certain amount of simplicity, which isn't the case with Rust. Rust makes you think about memory, and that will be new for many. Of course, ownership, borrowing, and lifetimes were new. That's pretty obviously the most unique feature of Rust. 
Is there a particular reason to avoid using results of some data flow analysis such as use-def/def-use chains? More implementation burden? Poorer performance? Doesn't work well with deferred initialization? It seems that using those would allow you to express things in a clearer fashion.
It's actually f**u**nc**t**ion with both the u and the t flipped upside down and swapped, and the t is mirrored.
Yarte 0.1.0 does not violate any law. With your persecution you are damaging the name of our community without any kind of proof. I insist that you desist in your behavior. The repository is public and you can visit it and collaborate respecting the rules that have been set. [https://gitlab.com/r-iendo/yarte](https://gitlab.com/r-iendo/yarte) In any other case we reserve the right to exercise the appropriate measures. &amp;#x200B; Regards,
Well there is a good import system now, but if you're transpiling to es5 you gotta do this.
Unless that's PublicRelations for Grand Misters. On a serious note readability is more important than saving ~3 characters. Even if it's a style issue - how is he using all caps for a function name at all, let alone mixing it with snake case. If you were for overthrowing the military on Mars base -- I'd be with you. But that kind of naming would make me defect to the traiters.
Hmm, that's odd. I'm sure I didn't call cargo clean... Is it possible that it's due to compiling to wasm? I did use cargo-web a few times
I know a year or so ago people were talking about changes to the macro system in Rust. What is the state of that? Are "Macros 2.0" finished? Or are there upcoming changes / improvements to the macro system?
You've done a good job obfuscating the derivations of yarte, but it is still possible to find evidence of the origin of segments of the code. For example: the parser module shows interesting similarities to the askama parser. There is the unexplained `Ws` type which has an identical name to the one in askama. There's also the odd `Input` function which violates normal Rust conventions which would point to a similar origin. It is also not enough to simply change the names of these items, the origin is still the same.
There (were for a long time) no modules or namespaces in Javascript. Scope is function-level, **not block level, not module level not file level.** They finally added let, which helps some. If you didn't use a IIFE, then function foo() { for(i=0; i&lt;5; i++) { } } Would create a **global** variable `i` with the value `5`. That means other scripts in the browser -- either yours or others can interact with that variable. If you use `var` correctly, 
I highly suggest you look into TypeScript for future projects. It transpiles into vanilla JavaScript.
A feature that is not zero-cost: calling a method on a trait object. This involved reading a value from memory and performing a call to that location. An example of something that is zero-cost: struct Deg(f32). The optimizer will almost always treat it as if it were a bare f32, even though Rust does not. It's an abstraction, but it gets peeled away when you compile. Iterators can also often be unwrapped to be equivalent ASM as a normal for loop without bounds checking.
What exactly about it is duplicated? Just the three lines to log and set the environment variable?
regarding Rust syslog and log crates, following an example [here](https://rust-lang-nursery.github.io/rust-cookbook/development_tools/debugging/log.html) but not seeing anything in my log stream (mac os). Not sure what's going wrong code sample: ``` #[macro_use] extern crate log; extern crate syslog; use syslog::Facility; fn run() -&gt; Result&lt;()&gt; { syslog::init(Facility::LOG_USER, log::LevelFilter::Debug, Some("My app name"))?; debug!("this is a debug {}", "message"); error!("this is an error!"); Ok(()) } ```
Ah so, maybe yes. It would need to compile them once for each target, and you‚Äôre using multiple targets. But still just once for each target...
Initialize the globals should be similar also. I guess it's just that there is Wayland server init stuff in a function for setting up a display and surface.
Well, it seems pretty different overall, and the winit code is only there for testing anyway, so I guess they just didn't bother building a common infrastructure there. That definitely does not mean code duplication is unavoidable in Rust, so don't worry about that.
Do you have recommendations on the "helper crates" once a person learns the standard way to handle errors?
using this command to track the logs &amp;#x200B; log stream --info --debug &amp;#x200B;
&gt;Rust. We take the u out of fun. The "u" in "fun" is for "undefined behavior".
This is really cool! I appreciate the detailed explanations, e.g., on "bump" allocation and the code samples to demonstrate the API.
I would not use this since the Speck algorithm is pretty controversial: the ISO first declined to standardize it, and it was removed from the Linux kernel shortly after it was added. In the kernel, it was replaced with HPolyC, maybe that could fit your use case, too? 
Thanks :)
Meson allows you to control how your project is built, how you build run your test, you can manage packages in subprojects and you can download external crates that you may require or just ended the binaries in your project. Crates are allowed since thy are really libraries and libraries are just sub-projects. I would heavily recommend reading the documentation for [***Mesonbuild***](http://mesonbuild.com/) *for more information*.
I am the worst person in the world for soundness: my background in PLT is so close to zero it could freeze in a light breeze. My only contributions, therefore, are to give examples of brokenness. Proving that things are sound are beyond my ability :/
This series is probably the best intro to bare metal PC programming one could ask for. You've really done a fantastic job of balancing good explanations of PC features and showing off the payoff of Rust's abstractions.
This is the style used by scoped_threads following the Leakpocalypse. Unfortunately, it's only really applicable to cases where the number of elements to juggle is statically known. As soon as you try to juggle a dynamic number of `GhostCell`, you'll feel the pain, because of this closure approach.
Nonce reuse is a very big no-no: https://github.com/aspera-non-spernit/bacon/blob/dev/src/ciphers/chacha20.rs#L15
This is really cool! Can you speak on the possibility of this being used server side as well? Ie, a lot of code is likely swapped between a DOM and Server implementation - meaning an identical interface allows a developer to write a single templating implementation which can be rendered on both server and DOM. Would this interface have to be defined above Dodrio? An example of this might look like two different implementations of a `!html` macro system - one where the backend uses plain string templating, another where the template system uses Dodrio. Do you think this DOM/Server abstracting interface would happen above Dodrio or within it? .. hopefully this is clear :s
I'm just starting to work with `chrono`. If I want to store the current (UTC) time in a variable that has the type NaiveDateTime, is this really the right way to do it? `let curr_time = Utc::now().naive_utc();` It seems like there should be a more direct way.
Thanks! We could implement `Display` for `dodrio::Node` that emits an HTML string (or just a regular method) that could be used to do server side rendering. Lots more little design decisions to investigate and dig into, but I think that would be the big thing.
Sometimes just sprinkling clone can go a long way and can help you learn better
LLVM IR is LLVM IR. It is not intrinsically tied to C/C++. Clang is merely one front-end that targets LLVM IR. Saying that Rust is a form of C++ because *one particular* implementation of Rust and *one particular* implementation of C++ both use LLVM as an implementation detail is a fallacy. 
I like the idea of building up macros for types that need to be encrypted and serialized but I would recommend making the macro a very thin wrapper around already built encryption. This will make it more modular, and definitely more correct (as one person has already pointed out a very serious flaw) - you can allow users to configure which algorithms they want. I think there's a lot of potential in creating a macro that just gets crypto right by default - it could generate Nonce types that are single use through move semantics, and even generate type-specific code based on the input. But the crypto itself should be \*dead simple\* and drawn entirely from standardized projects.
Using `$item` twice like this can cause an expression with side effects to be evaluated twice. let byte_doc = bincode::serialize(&amp;$item).unwrap(); drop($item); You should capture it in a local variable instead, to make sure it's only evaluated once. --- I agree with /u/sfackler that any encryption intended for real world usage needs to have authenticity built in. This is especially true of high-level libraries like this one, because the idea is that applications can just "call this one function and not worry about the details." Most applications that need privacy also need authenticity, to prevent padding oracle attacks and all sorts of other mutations. But most non-crypto-developers don't know about those subtleties 
There's Once is Nonce.
Thanks. Fiddled with it a while and gave up. Not urgent in any case.
CLion with Rust plugin is slow but supports the most features and the only one that just works out of the box.
The cargo-call-stack [README](https://github.com/japaric/cargo-call-stack/tree/v0.1.2#cycles) also contains this "cycles" example but includes the disassembly and the logs of a GDB session that show that the stack usage is indeed 0. The return address (for the whole cycle) is stored in the link register (LR) not on the stack. The function calls in this case use the B (branch) instruction which only overwrites the program counter (PC) register. If the function calls had used the BL (branch link) instruction the LR would have needed to be pushed onto the stack as the BL instruction overwrites the LR with the PC of the next instruction. The B instruction is used to implement things like if conditionals and for loops within a subroutine so in this case the foo, bar, baz cycle looks like a single subroutine where the function calls are just the branching you would see in a for loop that contains if blocks. cc /u/GolDDranks
&gt; Does Rust in any way protect against or make it harder to do SQL injections? Not really. SQL injection can happen in any language/library that lets you build SQL queries from strings. &gt; Is that something an ORM would do instead? Yes, using a library that hides the "strings of SQL code" generally prevents these problems, in any language. Pretty much the same is true of HTML and XSS attacks.
I see you tried to fix it with the [recent commit](https://github.com/aspera-non-spernit/bacon/commit/d7fd2a10b7e81718bbc28f24bb7571a53faabdf9), but you forgot an else branch on your if-statement. Right now you are discarding any user-supplied nonce and overwriting it with random data.
cc: /u/staticassert /u/sfackler Thank you very much for the good input. I followed your advice fixed the local variable in the fry! macro. For now (v.0.1.0). As workaround, and minimal improvement for the static nonce \`\`\`{Cipher}::new(k: Self::Key, n: Option&lt;\[u8; 8\]&gt;\`\`\` now takes an optional nonce. If non provided it's been generated by std::rand. It's probably not the best random generator out there. I may change the crypto libs I am using. However, I cannot evaluate if there are flaws in the source code of those libs. There are several crypto libs available for rust, no clue which one is "the best". Actually, it wasn't my goal to implement "so many" Ciphers: 1. I cannot evaluate the libs I am implementing 2. Users of bacon should be responsible for which ciphers they implement However Chacha20-Poly1305 is probably a good idea. Maybe it helps, that anyone is using bacon one day. I have to read about the authenticator. Since I am not really able to follow the RFC without having too many errors, I am relying on libs that provide ciphers. I think I could write something like this [https://upload.wikimedia.org/wikipedia/commons/b/b9/Authenticated\_Encryption\_EtM.png](https://upload.wikimedia.org/wikipedia/commons/b/b9/Authenticated_Encryption_EtM.png) Doesn't seem too complex, once I have understood what it means. Thank you very much for your help.
&gt; This is in contrast to, say, Python, where, as far as I know, there's no single definitive way to learn the language. Python has an official tutorial that is comprehensive enough. When I learned the language two decades ago it was front and center on the website. These days it's buried deep in the documentation so most newbies don't find it.
Nice, how would you describe it relative to petgraph?
Super awesome! This has really been a long time coming. I had some crazy thoughts about using generator state-machines to leverage breadth-first vnode trees that never came into fruition due to lack of time. Dodrio seems to be in that spirit. I also wrote a keyed list [diffing library](https://github.com/axelf4/lis) that uses the LIS algo, maybe take a look at that.
I have to write an algorithm which involve a potentially multi-rooted graph which is constantly under churn i.e. edges and vertices get removed and are modified a lot. I became tired with petgraph and the StableGraph api because I had to keep track of the roots outside of the graph and this became an annoying source of bugs. The api of petgraph is quite leaky and hard to understand. Graphlib strives to serve a simple api that resembles the api of other data structures in rust i.e. Vec, HashMap, VecDeque, etc.
Eagerly awaiting the js-framework-benchmark results, since the first benchmark is mostly useless.
If I was you, I would teach Rust alongside a systems programming course --- my university calls this class "Computer Systems Organization", and they use C and assembly to teach concepts like the stack/the heap, memory representation, alignment, and control flow. I bet you could teach the same class using mostly Rust, using inline assembly or `global_asm` where necessary.
Allowing the bump-allocated vdom nodes to point into components (the `'a: 'bump` stuff) seems unsound to me- the tree continues to exist through the event handlers, which call `unwrap_mut` and then mutate the components.
Any disadvantage/limitations that one should be aware of? You present the library stating *"optimized for high churn environments (where the graph mutates often)"*, may we conclude that other libraries are faster for low churn environments?
I can't comment on a speed comparison to Yew, or compile time on complex projects; seed-rs is straight-fwd, as it's a collection of text and some basic routing. Compiles in a few secs for me. &amp;#x200B; Great idea re supporting other message types: Fire away with that PR. 
Are there any 2D graphics/animation libraries at a similar level of abstraction as Pixi.js?
&gt; I think I could write something like this &gt; https://upload.wikimedia.org/wikipedia/commons/b/b9/Authenticated_Encryption_EtM.png &gt; Doesn't seem too complex, once I have understood what it means. **Don't roll your own crypto.** These flaws are not in the library, their in the usage of it. Use a high level API designed by an expert, like NaCl's `crypto_secretbox`. The sodiumoxide crate provides Rust bindings: https://docs.rs/sodiumoxide/0.2.1/sodiumoxide/crypto/secretbox/xsalsa20poly1305/index.html
Thanks! I'll give that a try.
&gt; As workaround, and minimal improvement for the static nonce `{Cipher}::new(k: Self::Key, n: Option&lt;[u8; 8]&gt;` now takes an optional nonce. If non provided it's been generated by std::rand. It's probably not the best random generator out there. Randomly generating an 8-byte nonce is going to be insecure, even if your random bytes are good. The problem is that there are only 2^64 possible 8-byte numbers. Because of the [birthday problem](https://en.wikipedia.org/wiki/Birthday_problem), you're likely to reuse a nonce after just 2^32 (4 billion) messages. That's why ciphers like XSalsa20/XChacha20 use 192-bit nonces; those are large enough that generating them (or part of them) randomly has some security margin. Algorithms with a smaller nonce than that are usually intended to be part of a protocol that prevents nonce reuse in some other way, like by generating ephemeral keys as a first step and then just using a sequential nonce. This leads into the topic of "forward secrecy", which is also something that high-level crypto APIs need to think about providing for its caller, in situations where it's possible. The constant refrain in the crypto developer community is "don't roll your own crypto." Some people feel like that's too harsh on beginners, and gets in the way of learning, which is fair enough. It might be worth coming up with some modified refrain like "if you roll your own crypto, you will make tricky mistakes that leave you with no security, even if you are an expert." This stuff is really hard, and even more importantly, it's very difficult to notice when you've made a mistake and learn from it. It's worth being quite clear about when something is a learning project, vs when it's intended for other people to use in production. &gt; However, I cannot evaluate if there are flaws in the source code of those libs. There are several crypto libs available for rust, no clue which one is "the best". If I was doing a project like this, my first instinct would be to use Libsodium to do all the heavy lifting. There's a `sodiumoxide` crate that provides bindings. Something like that is going to handle a lot of the tricky AEAD details for you. Another high quality Rust crypto library is `ring`, but note that it's designed for implementing TLS, and TLS uses small nonces. &gt; Doesn't seem too complex, once I have understood what it means. Famous last words ;) This entire StackExchange thread is interesting reading: https://crypto.stackexchange.com/questions/202/should-we-mac-then-encrypt-or-encrypt-then-mac
Any sense in how this compares in terms of compressed size for sending over the wire? Unless the app is *really* large (or average visits are long), it's hard to make up the difference in initial page load for a larger framework. But maybe web assembly would help there too?
It still doesn‚Äôt have a shortest path between two vertices iterator and it doesn‚Äôt yet support edge weights. But I have opened issues for this on the [repository page](https://github.com/purpleprotocol/graphlib/issues). What I meant is that the api supports such an environment because this is the use case that I wrote it for.
I actually did run into exactly this bug! The event handlers must now be `'static` so they can't close over borrowed references.
Code size and network transfer is a difficult topic that deserves its own full discussion. On the one hand, wasm code size tends to be larger than JS because it is talking about more low-level details that JS leaves implicit. On the other hand, it is designed for streaming compilation, and can be compiled faster than it is transferred over the network, where as JS parsing is regularly a bottleneck for page load / time to interactive. More info here: * https://hacks.mozilla.org/2018/01/making-webassembly-even-faster-firefoxs-new-streaming-and-tiering-compiler/ * https://rustwasm.github.io/docs/book/reference/code-size.html
What about other references? The first example in the post looks like it's directly storing `&amp;self.who` in a text node, which could be invalidated by an event handler calling `unwrap_mut` even though the closure itself is `'static`.
I beleive one is coming, but it's not available yet.
See this [http://smallcultfollowing.com/babysteps/blog/2019/03/01/async-await-status-report/](blog) &gt; Finally, we come to the question of the await syntax. At the All Hands, we had a number of conversations on this topic, and it became clear that we do not presently have consensus for any one syntax. We did a lot of exploration here, however, and enumerated a number of subtle arguments in favor of each option. At this moment, @withoutboats is busily trying to write-up that exploration into a document.
Thanks for those links‚ÄîI didn't realize that wasm could be complied faster than it can be streamed in many cases. That does change the game!
If you copy open source code, even if you change some of it, and even if they are the same license. You are required to retain their copywrite notice and license somewhere in your project. https://github.com/aspera-non-spernit/bacon/blob/dev/src/ciphers/speck.rs https://github.com/redox-os/tfs/blob/master/speck/src/lib.rs
I've taught Rust twice now to a mixed audience of senior undergrad and MS students. It was a [ten-week course](http://moodle.svcs.cs.pdx.edu/course/view.php?id=30) the first time, and eight weeks the second (sadly, Google Classroom won't let me share the course with y'all). I'm about to teach it a third time. My approach is to work straight through Blandy and Orendorff's *Programming Rust*: I can highly recommend this, even compared to The Book: it gives really thorough coverage with great explanations. I'm jealous of semesters, though: at ten weeks it's a pretty heavy load, and at eight weeks its a bit much. I use exercises from the web, such as the [Exercism Rust Track](https://exercism.io/my/tracks/rust), as early homework, then start a course project (individual or small-group) halfway through. This gives some practical exposure. Lifetimes and borrow checking have been the big sticking points. The module system is pretty confusing. Closures are tough. Iterators are so boilerplatey that they present a challenge: I'm so ready for generators. Trait objects take a bit of getting used to. None of this is likely a surprise to anyone. I really enjoy this course: I'll be teaching it for the fifth time in the summer. One would think I'd eventually run out of students, but we have huge enrollments right now, so I should be good for a while yet. Anyway, then I can do Advanced Rust Seminar.
Hello everyone; The language team had, as usual, a meeting today (Thursday). More information can be found in: https://internals.rust-lang.org/t/lang-team-working-group-sync-meetings/9573/2 Today we primarily synced up with the [Unsafe Code Guidelines Working Group](https://github.com/rust-rfcs/unsafe-code-guidelines). We have also started recording the meetings for posterity and for everyone's benefit.
Also, Clippy and rustfmt seem to be in an easily-usable state now: this wasn't really the case when I last taught. I will try to make sure that students post difficulties understanding the compiler messages to the issue tracker.
The README.md of the compiler source should explain everything. If it doesn‚Äôt, please file a bug!
Hm yeah, actually I think you're right. Thought I'd found a way to avoid interior mutability, but it looks like I need to remove the ability to get mutable references to the rendering component.
I'm not sure that helps, which is what I was referring to in parentheses above. The diffing algorithm needs to see the old values in the old tree, or it won't realize that the new tree has changed, right? So giving the event handler an immutable reference and expecting the components to use `Cell`s will break that. I suspect you need to copy everything into the arena, like you do for the counter example, for vdom diffing to work.
FFI yes, sorta, its tricky. Its been some time since I did this kinda work, so things I mention here come with a C / Fortran bent, but I think its possible to make rust do better JNI. JNI has the problem of being a great interface that is the wrong way around for the JIT. So what you wind up with is something that has the following properties from the viewpoint of the JVM: * It is an opaque node in a compile graph, so how should I (the JIT) call and optimise it. This leads to de-optimisations in certain places. * Since I cant see into it, I have to assume I can inline, or play with the calling convention to my advantage. This leads to bridges that have to mangle the stack back into shape. * What precautions do I need to take to honor the JVM invariants * ... and the JNI API invariants * ... and C's invariants * How is memory laid out between C / Java * Are you going to play nice with concurrency (safepoints) * Are you going to play nice with the GC (going to steal pointers to my memory?) All in, JNI is a weird pain. It feels like it should be faster than Java "Well you know its C right?" but winds up being on par with some form of RPC. There are some tricks you can deploy around this, panama and graal are examples. Other tricks involve: * pass off to background threads, painful to get right and not what I think you want in this case, it involves cooking a message passing framework (hello channels) and use of AttachThread DetachThread. * Use the parts dealing with MappedByteBuffer / DirectByteBuffer from java.nio. These give you safe(ish) hooks on mmap and malloc that you can use as buffers for I/O. These buffers despite the I/O connotation are also usable for passing memory to and from without getting the GC's attention as much. Its been some time since I did high perf JVM numerics, but this was the trick that one of the blas binding libraries (I think jBLAS but I could be wrong) did to speed things up. * If you are on 9+ You can use VarHandles to get much of the benefits of the Java NIO stuffs, with a nicer API that is faster. * ... Dont use Unsafe, it feels low-level but it can compile into the JVM IR in funny ways and can actually put barriers and other stuff in places your are not expecting. Sometimes passing things with Unsafe is like turbo skates, sometimes its just slow as the slowest thing. * Avoid JNI Global references, these things play strange hell with the GC and can cause slowdowns you didnt expect (ask me about the fun I had when every fast JNI call was coupled to a full stop the world GC straight afterwards). * Abuse Critical JNI, this will change the programming environment for JNI, it removes preemption, safepoints and return-to-vm features of JNI. The idea of critical JNI is that your code needs to behave like it is a critical section. That is for the thread you should treat it like you took a lock over JNIEnv* and only hold that thread out from the VM as short as possible. If you go Critical JNI you cant poke the VM, so no calling objects or methods, and no using JNIEnv. If you can wait for project panama, then it covers my first statement. Panama is bindings done backwards, that is most of the bindings are done in Java which gives the JIT the information it needs to ultimately compile the callsite down into `JMP` / `CALL` in x86 speak. See http://cr.openjdk.java.net/~vlivanov/talks/2016_NativeCode.pdf for some gory details, it leads up to Panama but covers stuff wrt native calls in general. I _think_ your best bet for your code is probably the critical API while staring at how to send the memory back and forth.
If there are different ways to fail, that a user might conceivably want to deal with differently, then you probably should use Err to store the data needed for that disambiguation. If there is only one possible non-value behavior, then None is fine.
One thing I don't see often discussed: if you do common networking code in your Rust lib, the `NSURLSession` bits on iOS supposedly do a lot regarding waking up and managing the radio, and going around them can create situations that'd avoid VPNs/cause power management issues/etc. I'd love to be wrong, but this is my understanding of it based off both writing the code and reading over a good chunk of iOS documentation + release notes over the years.
I toyed with this last year and I successfully had an hello-world : https://github.com/kennytm/rust-ios-android/blob/master/README.md. I believe that for each technical difficulties you will encounter with creating a shared library in C/C++ for mobile, you‚Äôll encounter pretty much the same in a Rust library, but with less specific examples on how solve your problem: - bridging to ¬´ native ¬ª - OS specifics - building libraries - linking libraries - UI specifics (if I were you, I wouldn‚Äôt attempt to share UI logic too closely: start with a thin layer on server communication, shared data objects, shared compitation logic, that‚Äôs already plenty) On a side note, I have worked on a project using java2objc (https://developers.google.com/j2objc/), with that approach, through a common java codebase, and a UI layer doubled for specifics to android and iOS.
added. :)
It would let users make the choice between swapping out different references to immutable strings vs copying mutable strings into the bump.
I'm on the same boat doing a proof of concept to move a fairly big .NET Xamarin project. &amp;#x200B; 1- I use html for the UI, and a rest server on .NET, inside the iOS/android app. I plan to replace the server. So, no UI with rust. Maybe later rewrite in native swift/kotlin the UI, but I don't see rust to be a viable contender for UI in this decade 2- Expect to have trouble finding how exactly cross build all, but is doable 3- You lose the ability to debug the rust code on mobile. But is possible to run tests: [https://docs.rs/crate/dinghy/0.2.16](https://docs.rs/crate/dinghy/0.2.16) 4- Some libs will fail to cross compile with weird bugs, mainly if use assembly. 5- Is not clear (to me) how pass complex structs: [https://www.reddit.com/r/rust/comments/b02sf1/how\_take\_a\_handle\_using\_rustandroidjni\_for\_wrap\_a/](https://www.reddit.com/r/rust/comments/b02sf1/how_take_a_handle_using_rustandroidjni_for_wrap_a/). But because I plan to use REST as communication layer this concerns are reduced 6- Setup all the components take time. Use android studio instead of manually setup the android toolchains as in some examples I found. This because: 7- Android development sucks. Sucks. Sucks. Everything about android sucks. In fact, the reason of use Xamarin/rust is because android. All my problems are with android and their weird ways. P.D: Android is as far as the C/C++/rust world as you could imagine, and have a very poor FFI. I even consider the possibility to use nanomsg or similar to bridge and keep the JNI to the minimum. &amp;#x200B; I also recommend to collect all the dependencies you need (and predict the future ones) and see first if exist a rust answer to that. I need a way to generate excel files, and the only option so far is with C/C++, so soon or later you will need to go that route (maybe). &amp;#x200B; Automate the build from the start. You will need to redo everything often. &amp;#x200B; Make a proof of concept and try to do the hardest kind of task(s) you expect to solve. Easy is easy. Hard is where you could hit a dead-end &amp;#x200B; &amp;#x200B;
This is worrysome. Do you have links that talk about that? I need to do REST and part of the allure is to put all of that on Rust (where I also wish to put DB layers and business logic)
Hi, sorry the late response. &amp;#x200B; I use the android\_logger and nothing show up. &amp;#x200B; gdb allow to debug into the rust code???
Why isn't this more accessible? This solution is far simpler than even the official guide
I can't wait to see how that goes. There might be bugs (I found one today), but I was still able to rewrite [nest.pijul.com](https://nest.pijul.com) entirely with it.
Thank you very much. &gt; Randomly generating an 8-byte nonce is going to be insecure, even if your random bytes are good. The problem is that there are only 264 possible 8-byte numbers. Because of the birthday problem, you're likely to reuse a nonce after just 232 (4 billion) messages. That's why ciphers like XSalsa20/XChacha20 use 192-bit nonces; I am not sure if I can just change that. The problem is, every crypto library implements things differently and not all libs support every variant. At the moment I am happy with what I have and I hope I can gradually improve bacon so it implements only the "highest" standards. &gt; This leads into the topic of "forward secrecy", which is also something that a high-level crypto API needs to think about providing for its callers, in situations where it's possible. &gt; The constant refrain in the crypto developer community is "don't roll your own crypto." That's not my approach. If I cannot evaluate which crypto lib is safe and which is not, I could do a really big mistake by choosing the wrong one. So I can do my own mistakes and have the benefit of learning. I will probably have a mix of custom made and external crate and learn as I go forward in the project. I am going to read about that too. Thanks for the input. &gt; It's worth being quite clear about when something is a learning project, vs when it's intended for other people to use in production. v0.1.0 stage: dev in red :) This is something I came up with to begin with authentication. Not sure if that's safe or deeply flawed :) impl Authenticate for ChaCha20 { // TODO: Hash not impl for HashMap&lt;String, String&gt; of Bacon fn hash(&amp;self, bacon: Bacon)-&gt; MAC { let mut hasher = DefaultHasher::new(); self.key.hash(&amp;mut hasher); self.nonce.hash(&amp;mut hasher); let kh = hasher.finish(); let o_k: u64 = kh.bitxor(0x5c * 64); let i_k: u64 = kh.bitxor(0x36 * 64); let mut b_k: u64 = 0; for block in &amp;bacon.data { block.hash(&amp;mut hasher); b_k ^= b_k | hasher.finish(); } bacon.data.hash(&amp;mut hasher); ( o_k | (i_k | hasher.finish() ) ).hash(&amp;mut hasher); MAC( hasher.finish() ) } }
Congrats! I look forward to upgrading to this! &gt; Another substantial way to improve performance is to never copy or allocate memory when it's not strictly necessary. For example, text nodes will usually be just be references into the source text. Copying those references around is much cheaper than allocating memory and copying the text itself. The same is done for most other text-like data, like link URLs or their titles. Generally, only when text needs to be (un)escaped, memory is allocated to store these strings. The new release introduces a new copy-on-write string type that can store small strings inline, further reducing the need for allocations. Since this type is exposed in the public API, this does constitute a compatibility break. However, it behaves very similarly to the Cow&lt;str&gt; that was previously used and we expect that most users will experience no breakage. I'm assuming this is referring to [tentril](https://crates.io/crates/tendril) which looks like a really handy crate! &gt; To make the construction and traversal of the AST fast, the crate uses an arena-backed tree implementation similar to the indextree crate. That means that all the nodes of the tree are laid out contiguously in memory and all references to nodes are just indices into the backing vector. This is super good for cache locality and insertion speed ‚Äî we can just append the vector. Tree indices are then essentially just values of usize, where 0 represent the Nil pointer. To prevent using nil pointers by accident, pointers are encoded in the type system as Option&lt;NonZeroUsize&gt;, which gets optimized to a single word! Would [it](https://github.com/raphlinus/pulldown-cmark/blob/master/src/tree.rs) be useful to others as a crate or is it too specialized? &gt; An important goal of new_algo branch was to provide a better foundation to tackle these problems. This branch introduced a two-phase design, with the first phase constructing an abstract syntax tree (AST) of the document, which an in-memory datastructure representing the blocks (paragraphs, codeblocks, lists, etc.) that make up the document and their nesting. The second phase then traverses this tree, parses inline elements in details and generates the right events along the way. The idea is that by doing a quick first pass, we have a context which we can use in the second pass, eliminating the need to look ahead since we already know what's there! Some of these denial-of-service vulnerabilities still exist in 0.3, but they should be much easier to iron out. It sounds like the AST has a very specialized API and only does some of the work, so I'm assuming this isn't generally useful?
[Are we yet?](https://wiki.mozilla.org/Areweyet)
hahahaha I love Rust so much.
But what do you say, if the parser in askama is with nom and in yarte it is with memchr, nom and syn. Do you see the parse::&lt;syn :: Expr&gt; and parser::&lt;syn::Stmt&gt;? The ast? You are telling me that because these 5 lines [https://gitlab.com/r-iendo/yarte/blob/master/yarte\_derive/src/parser/mod.rs#L79-84](https://gitlab.com/r-iendo/yarte/blob/master/yarte_derive/src/parser/mod.rs#L79-84) , which is a constructor of the type nom::types::CompleteByteSlice. Am I obfuscating the code? Seriously? 5 lines? From a constructor of a type of nom?
Sorry we only allow compiled binaries here. Can you compile yours?
&gt; Does Rust in any way protect against or make it harder to do SQL injections? A library can use the type system to guarantee that you'll remember to sanitize input. The language isn't magic, it just has really good features for balancing paranoia against productivity and performance.
Or `Serialize`
I have forgotten the Dena team and suckless comunity. Thanks also to them, I do not think they will go through here but thanks anyway.
64-bit MACs don't provide security in the same way that random 64-bit nonces don't provide security. They're just not big enough. It looks like you might've taken the `0x5c * 64` notation [from Wikipedia](https://en.wikipedia.org/wiki/HMAC), but in the pseudocode there it represents repeated elements in a list, while here it's multiplication. They're not doing the same thing. Whether that masking step in HMAC is necessary for a modern hashing function is a different question, but in any case it looks like you're kinda sorta trying to implement HMAC here. Other than that, this code is doing a lot of things that just aren't what HMAC says to do. It looks like it's taking the output of a hash state and feeding it back into itself as additional input? (Rather than starting with a second fresh instance of the hash, as HMAC does.) You might want to find another implementation of HMAC that you can understand, and study it more carefully. This subreddit will be helpful as you run into problems with Rust itself, but it might not be the best starting point for learning the basics of crypto.
/r/playrust is that way *points*.
&gt; I'm assuming this is referring to tentril which looks like a really handy crate! No, it's not tendril, it's house made. In general we tried not to rely on too many extra dependencies. &gt; Would it be useful to others as a crate or is it too specialized? I think too specialized. We do a lot of splicing in the tree during parsing, and have struggled how to abstract those operations, so end up doing it as low level accesses. Perhaps that can be rationalized though, at which point it *might* well be useful. The splicing operations are interesting, think [a, *, many, words, here, *, b] to [a, emph: [many, words, here], b], with that operation in constant time. Standard AST representations would have trouble with that. &gt; It sounds like the AST has a very specialized API and only does some of the work, so I'm assuming this isn't generally useful? There's an [issue](https://github.com/raphlinus/pulldown-cmark/issues/160) open about how to expose that. I think it's a good idea, but we want to be careful how we do it so we don't overly constrain further work.
I'm a bit new to allocators, so please excuse if this is a silly question. Is a bump allocator the same as an arena allocator? Or is there a difference?
Another interesting thing is to see the difference between the control of Whitespace in handlebars "{{\~# \~}}{{\~/ \~}}" and in jinja "{{%- %-}}". As you can see the sign is wrote with another syntax so it is impossible for the implementation to be the same.
Based on the documentation, that looks right to me.
maybe i find another source. However, I wonder why I need that? How can an encrypted message be manipulated, and still decrypted, wouldn't decryption with the same key, but different data fail to decrypt? For instance in my bacon project, I cannot decrypt an encrypted bacon object if only one number of the block is different? Is it possible to change a message and have all blocks remain the same?
Thanks. The link to old discussions that kyrenn posted really helped me to understand better what's going on in GhostCell. I'll check out scoped threads too.
You can look at the [stable PRs](https://github.com/rust-lang/rust/pulls?utf8=%E2%9C%93&amp;q=is%3Apr+base%3Astable) to find [release 1.28.0](https://github.com/rust-lang/rust/pull/52864), then for Windows follow the link to [status-appveyor](https://ci.appveyor.com/project/rust-lang/rust/build/1.0.8375). The release builds have `DEPLOY=1`, so here is [`i686-pc-windows-gnu`](https://ci.appveyor.com/project/rust-lang/rust/build/1.0.8375/job/9o7ur628u891kk0h). You can see the configuration in the full log [here](https://ci.appveyor.com/project/rust-lang/rust/build/1.0.8375/job/9o7ur628u891kk0h?fullLog=true#L168).
&gt; How can an encrypted message be manipulated, and still decrypted, wouldn't decryption with the same key, but different data fail to decrypt? https://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Error_propagation https://en.wikipedia.org/wiki/Padding_oracle_attack https://en.wikipedia.org/wiki/Chosen-ciphertext_attack For a broad introduction to these and many other attacks, I can recommend: https://cryptopals.com/
Ironically, C++ has two features that aren't zero-cost: exceptions and RTTI. Rust's alternation type is ironically much better at this.
Well, in general, doubly linked lists are inherently very interior-mutable data structures. As such, in some sense, a doubly linked list is hard to mesh with safe anything unless it's through a container abstraction (e.g. std::list). Graphs too. And there is always the argument that it would be just as difficult in C++, since idiomatic RAII C++ is also strongly about ownership, except that C++ would make it harder to reason about.
&gt; Even if it's a style issue - how is he using all caps for a function name at all, let alone mixing it with snake case. I think I didn't word myself well enough - the capitals portion of the function tells you which module the function is from, and the rest of the function is the actual function name. I think this is fair as it can be difficult to tell where a function came from without using the IDE with our code base. The main part that gets me is the inconsistency/use of abbreviations. I think the guy has a really awesome memory so from his point of view it's not a problem, but for me I can never remember what the abbreviations are and have to look them up. Finally, it's more or less a one-man-show at this point - that person is effectively the only person working on that particular codebase (except that one time I had to look at it). 
bump &amp;#x200B;
Thank you for the feedback! I decided, not to give an tutorial of Rust, as there are already plenty available. But ideed, when you are new to Rust, the versions might be confusing. The explanation however is quite simple : These are the latest versions :-D While I agree that it is somehow a bit missing, basically everything is written to the PCB and the STLink ;-) so I skipped it. This is something that can be found online quickly.
i will look into that, ty.
Compression seems like it's very relevant, and might differ between js and was wasm substantially. I think you tried to just pick out the framework size portions, which I didn't bother (doesn't seem to change that much anyways) React: 286.67 KB transferred, 1.22MB uncompressed Dodrio: 89.25 KB transferred, 235.78 KB uncompressed (this is after running wasm-opt on the wasm file, gzip compression) Preact: 29.98 KB transferred, 47.85 KB uncompressed The big takeaway here for me is that gzip helps react *a lot*, but not enough to make up the difference.
&gt; I use the android_logger and nothing show up. May be you forget to initialize logger? &gt; gdb allow to debug into the rust code??? Yes, as I know this was the first debugger that was supported by Rust team. And gdb team integrate patches for rust long time ago. So I hope in the last version of NDK there is gdb with rust support.
given that you gave it the general name of graphlib it would be nice if you had 2 modules in it 1 for a graph that undergoes frequent change(your use case) and another for a relatively static graph(with less memory overhead and other benefits a more static graph may provide) I don't expect you to do this given it's for your use case rather than a project you created to stand on it's own but one can hope
I haven't done webdev seriously in a couple of years, mostly been doing data analytics, but I've heard good things about TypeScript. I'll probably give it a try if I need some JS in the future.
Thanks for the information. I haven't studied any of those languages, but I have relatives who have, so I just know what they told me. Also, my understanding is that, due to Semitic roots being based only on consonants, it is not that much of a problem. An Iranian guy who I met many years ago told me that it was much worse for them, because Farsi is an Indoeuropean language so vowels are really important, but they don't write most of them.
What is an "inherent" function or method? I was reading this [RFC](https://github.com/rust-lang/rfcs/blob/master/text/1522-conservative-impl-trait.md) on `impl Trait` and didn't understand the term.
I have a total beginner question. My code: fn main() { println!("\nwoodpecker JMAP test!\n"); let mut response = reqwest::get("https://httpbin.org/status/200").expect("Failed to send request"); println!("{}", response.status()); for element in response.headers() { let one = element.0; let two = element.1; println!("{}", one); println!("{}", two); } } The error I'm getting: error[E0277]: `http::header::value::HeaderValue` doesn't implement `std::fmt::Display` --&gt; src/main.rs:13:24 | 13 | println!("{}", two); | ^^^ `http::header::value::HeaderValue` cannot be formatted with the default formatter | = help: the trait `std::fmt::Display` is not implemented for `http::header::value::HeaderValue` = note: in format strings you may be able to use `{:?}` (or {:#?} for pretty-print) instead = note: required because of the requirements on the impl of `std::fmt::Display` for `&amp;http::header::value::HeaderValue` = note: required by `std::fmt::Display::fmt` Now I know that I can just use `{:?}` but that just means I'm working around the problem. What if I want the variable `two` to hold an actual String and not some `http::header::value::HeaderValue` type? I hope it's clear what my problem is. 
A bump allocator would be a very simple version of an arena allocator. Both allocate from a given chunk of memory (and "arena"), though an arena allocators (typically) provide methods to reclaim memory without reclaiming the entire arena. With a bump allocator, the only choice is to reclaim the entire arena. This is done to keep the allocation code time-deterministic and overall simpler. Said differently, a bump allocator is an "allocate only" arena allocator.
It's just a method on a concrete type, e.g.: impl String { pub fn len(&amp;self) -&gt; usize { ... } } This is contrasted to a method on a trait definition or in a trait implementation: trait Foo { fn bar(&amp;self); } impl Foo for Baz { fn bar(&amp;self) {} } 
Thanks. I was giving the uncompressed file size partly out of laziness (I didn't bother running gzip on Dodrio, since it wasn't compressed in their demo site). But also gave the uncompressed size partly because, with JavaScript, the uncompressed size can matter just as much as the compressed size‚Äîthe JS has to be transferred *and parsed* before it can be executed, and the parsing depends on the uncompressed size. But what u/fitzgen's first link pointed out is that *this isn't true for wasm*. Wasm can be compiled as it's streamed in (which I didn't know) and is compiled so quickly that it's usually done at basically the same time that the transfer is over. Bottom line: I agree that compressed size is most relevant for looking at Dodrio, though I think uncompressed matters a lot for comparing React to Preact. &gt; 235.78 KB uncompressed (this is after running wasm-opt on the wasm file) Interesting. I got it down to 184 KB with `wasm-opt -Oz`. I wonder where the difference came from? 
So `HeaderValue` isn't trivially convertible to a string because the HTTP specification allows header values to contain arbitrary bytes. There is the fallible `.to_str()` method which checks if the `HeaderValue` is ASCII and returns it as a string slice: // you can destructure in `for` loops for (name, value) in response.headers() { println!("{}", name); match value.to_str() { Ok(value) =&gt; println!("{}", value), // convert the value to a string, replacing unknown sequences with ÔøΩ // or however you want to handle a non-ASCII header value Err(_) =&gt; println!("{}", String::from_utf8_lossy(value.as_ref())), } }
As a follow-up on this, I also ran the numbers on the `Hello, World` example, and I got a gziped size of 27 KB (24 wasm + 3 JS), which should represent something of a lower bound for the current implementation. (It was 67 KB uncompressed (57 wasm + 10 JS), just in case that's relevant)
I'm curious - is there any accepted way to create an application-wide Singleton for a cache or something similar?
Thanks so much for your help. Seems like this is more complicated than I thought. &gt;because the HTTP specification allows header values to contain arbitrary bytes. This sounds like a horrible concept. Why would they do this?
Probably that I didn't know to use `-Oz`, I just naively assumed that it had a sensible default... Oddly I get 172 KB with `-Oz`... which still isn't quite the same. Updating numbers in my original post.
Somehow it just doesn't seem right. What bothers me is that these two lines of code give exactly the same results: \`let curr\_dt\_naive\_utc = Utc::now().naive\_utc(); let curr\_dt\_naive\_local = Utc::now().naive\_local();\` See [this playground example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fcf7513b3bac862549f17406b507d54d) as a demonstration. It seems like there should be a \`to\_naive()\` method, but I don't think there is one.
Possibly to support alternate text encodings but I'm not sure. [This StackOverflow](https://stackoverflow.com/a/4410331/1299804) answer attempts to explain it.
Thanks
[removed]
Ah OK. Glad to hear it. I will just ignore this and happily use `to_str()`. Thanks again!!
You're looking for [https://www.reddit.com/r/playrust](https://www.reddit.com/r/playrust) :) 
Actually I just realized that this is advertising a cheating service, not trying to report a cheater. Guess that's what I get for trying to be nice lol
Is that the production React build? React has a whole bunch of code in debug builds that is compiled out in the production version...
No clue, it's literally just what the firefox debug tools network tab tells me when I click on codesections react link
It's the nth n.
I was mucking around with VPN-related stuff a month or so ago, and [this document](https://developer.apple.com/library/archive/documentation/NetworkingInternetWeb/Conceptual/NetworkingOverview/CommonPitfalls/CommonPitfalls.html#//apple_ref/doc/uid/TP40010220-CH4-SW2) was the one that made me remember this stuff. Like many pieces of Apple's documentation, it has a label that says it isn't being updated anymore... but it's full of interesting info, and it'd be really great if they'd label what's still useful/relevant/etc. They won't, because they're Apple. But it'd be cool. Anyway, like I said... happy to be wrong, really. But worth looking into. :)
The crate lazy_static lets you easily initialize a static on first use, and you can wrap it in a Mutex if you need it to be mutable.
They‚Äôre not done, but a significant chunk was shipped. The main things left to do are: 1. macros by example 2.0 with the macro keyword 2. Proper hygiene for procedural macros 
Thanks for the answer. Now that you mention, it I have seen the `macro` keyword somewhere before and the syntax looked really good at first impression, as best I can remember.
Checking it out. Cheers
This is a [Redis Module](https://redis.io/topics/modules-intro) embedding SQLite.
That's really awesome! I always wondered how a virtual DOM based framework would look like in rust. The benchmark results are quite interesting but they're a bit unfair due to using very old versions for the other frameworks. Would love to see an updated benchmark with recent versions of React, Angular and Preact.
For me Rust RLS plugin for VS Code works much better than other options. Gives much more warnings and errors highlighting out of the box. IntelliSense, doc previews and tasks (build, test) also work. https://marketplace.visualstudio.com/items?itemName=rust-lang.rust
Yo I've been looking for a good side project to do with gaming. Maybe I'll give this a go, I fucking loved Minecraft back in the day
Elm 0.16 and 0.17 are very old now. I'm not sure there is still some people using these versions. How does it compare to 0.18 and 0.19?
Can you go more into detail on what you want to achieve? A test suite can be more than one thing, depending whom you ask, from a set of simple unit tests (like `#[test]`) to a test framework that handles setup, teardown, an out-of-process runner (so that a crashing test doesn't kill everything), or a test harness like one a compiler or browser might use, with inputs and expected outputs, stored in files.
Is it always safe to `.unwrap()` the first item when spiting a str? More specifically I want to delete every character after a match like: `"stay:delete".splitn(2, ":").next().unwrap()` So is this safe? And is there a better way to do it?
&gt; # Tunable for every use case &gt; &gt; RediSQL inheritanve all the SQLite knobs and capabilities, if the out of the box performance are not enough you can tune it for your specific use case. s/inheritanve/inherits/ s/performance are/performance is/
Hi! Here is the author, if there are any questions I am glad to answer! 
Thanks! I will fix it!
Also, here is the GitHub repo for savings clicks: https://github.com/RedBeardLab/rediSQL
It's working well, but is there a cross-platform way to tell Travis to install SDL2 ?
That was a very interesting watch. So glad this exists
Did you fill an issue on petgraph about the problems you had with it?
cross is maintained by the rust-embedded working group, you'd have to ask them
Unfortunately I'm not allowed to give too much detail, but I currently have this done for production. The core of the product is a library with a well defined set of functions exported as an FFI API, this is used on both iOS and Android to great effect and runs very very well. It is also compiled to Wasm for use in HTML client, and yes, also used for a few AWS Lambda services paired with the new rust runtime, rusoto, and aws\_lambda\_events. &amp;#x200B; So yeah, entirely possible to do what you want. the iOS app uses it with an objectiveC wrapper, while the Android app uses it with a Kotlin wrapper. The microservice runs it as a rust app compiled via musl and is very very fast with low memory use.
Is petgraph even maintained anymore?
pulldown_cmark 0.3 [broke the About page on a pastebin I'm working on](http://localhost:8080/about). Specifically, it appears to break autolinks.
The React project uses a three-year-old, unminified version of React. For a fair comparison I would use the latest production build instead.
you'd have to ask /u/bluss
Neat, the update works on my website, aside of one issue: https://github.com/raphlinus/pulldown-cmark/issues/213, which hopefully will get fixed soon.
I was under the impression that the biggest draw for using arena allocators was specifically so that they could be reclaimed all at once! But I guess occasional housekeeping is preferable?
Great tool! Really want to see \`cargo flamegraph bench\`. &amp;#x200B; So much better than a bunch of loose scripts
For point 5, I pass pointers to the structs, and any operations on the structs requires passing the pointer back to Rust in the function call. The general pattern is called out-args or similar I think - pass a pointer sized value to the rust function as an arg, and have the rust func write the pointer to the struct in that.
The difference might be negligible for all I know. It's just that I haven't tested this yet to know for sure. My use case involves graphs that are acyclic and that are maximum 100-150 vertices large, always. &amp;#x200B; The reason for the memory overhead is not only for mutation but for fast accessing as well so it also helps static graph analysis. &amp;#x200B; My reasoning with this library is that it should provide the default functionality that one should expect from a graph library. If one has a specific special use case that involves **REALLY** large graphs with special use cases or some other exotic stuff I think it is foolish from the beginning to rely on a generic library and one should implement his own custom case. 
It seems that even with an empty string, the `SplitN` iterator will always yield at least one string, but it can be empty.
I finally and already for the community to see the programming level you have. How is the code obfuscated by removing lines? Askama parser [https://github.com/djc/askama/blob/master/askama\_derive/src/parser.rs#L894](https://github.com/djc/askama/blob/master/askama_derive/src/parser.rs#L894) Yarte parser [https://gitlab.com/r-iendo/yarte/blob/master/yarte\_derive/src/parser/mod.rs#L611](https://gitlab.com/r-iendo/yarte/blob/master/yarte_derive/src/parser/mod.rs#L611) &amp;#x200B; **Can you explain to me how to obfuscate something by removing 283.** &amp;#x200B; Do you know this in Microsoft? Because I advise you to see as the example of yarte for actix\_web in actix/example It seems to me that one of your main senior software engineers does not think like you. Because it will be? &amp;#x200B; I already warned you many times. I understand your feelings, but you're wrong, think again now.
Activity is very low in recent months. In fact, he seems to be looking for maintainers for some of his crates.
I may have to improve my writing here. Thanks for letting me know.
In our app, we have \~3K LOC rust code as shared native iOS/android logic like a static web server, unzip etc. But mobile is a second class citizen to rust because of issues like [https://github.com/rust-lang/rust/issues/35968](https://github.com/rust-lang/rust/issues/35968). I wish Rust community put some efforts to mobile just some portion of the efforts has been put into wasm for example. 
.02 kB is 20 Bytes, right?
You know, regular databases are not that slow once you know how to use them. Plus, it saves you having a gazillion different systems + their interfaces if you know how to use one properly.
I need to write a consensus algorithm, not wait for incomplete and buggy crates to get fixed. I believe the moment when I decided to write this was when I saw that petgraph was 4 years old and still didn't have Breadth-First Search and multi-root traversal. &amp;#x200B; &amp;#x200B;
Implement the trait syn::visit::Visit, even if it is blank, and follow the documentation is an excellent exercise to learn the syntax of rust. https://docs.rs/syn/0.15.29/syn/visit/index.html#functions
*Ok, this is epic.* It's your **7th Cakeday** cbarrick! ^(hug)
I'm one of the organizers of a Rust course in Sofia University: https://fmi.rust-lang.bg/ (It's all in Bulgarian). The Rust book is an excellent starting point for organizing your lectures -- it's very well-thought out. We added material from other places as well, like [Learning Rust With Entirely Too Many Linked Lists](http://cglab.ca/~abeinges/blah/too-many-lists/book/) -- I implemented some of these lists semi-live, explaining interesting details. [The Little Book of Rust Macros](https://danielkeep.github.io/tlborm/book/index.html) was also helpful with some of the weirder workings of macros. The biggest challenge for us in structuring the early lectures was how to teach a coherent set of things while avoiding "we'll explain this later". A lot of the concepts are connected -- You can't learn error handling before knowing what an enum is, there's no point in learning about Traits if you don't know about generics, iteration needs both generics, traits, possibly closures, though you could split it out. Sometimes you just *have* to show something without fully explaining it -- even static strings have a lifetime in their type, which you'd probably need to handwave away a bit. The most difficult thing for our students to grok was, I feel, `Deref`, and deref coercion. I think a big problem is that terminology is a bit iffy -- "Deref" is not the same as "dereferencing", even though they seem the same. The trait is a conversion between pointers references, while the verb implies accessing a value through a pointer. Add to that smart pointers, where `RefCell` gives out a `Ref`, which reads like a shortened version of "reference", but isn't, and it can get a bit confusing. We try to be extra slow in these lectures, especially after we get a collective WTF-sound from them seeing `&amp;**node` in the linked list examples. Regarding structure, we started with the Rust book, occasionally had "filler" lectures, where we talked about useful/popular types, traits, crates, etc -- anything we didn't feel matched under a specific topic, but could be useful to know for homeworks or projects. The later lectures were project demos, like a simple GGEZ game, a website, a basic GTK app. Students needed to build their own project, so the goal was to give them ideas, show them what's possible, so they can pick one of them and research on their own. The lectures are all uploaded on [the site](https://fmi.rust-lang.bg/lectures), and the titles are in Bulgarian, but if you hover over the urls, those are in English :). So, if you're very interested, you can take a look at how we structured things at least. Best of luck, in any case -- hope you inspire a good batch of new Rustaceans :)
What I describe here is in fact a crude variant of data flow analysis. My goal is to build up the analysis from small testable parts in order to always have a working system.
The other half of this comparison has been missing a little for me in these discussions ‚Äî how quickly do JS engines parse and baseline-compile JavaScript? How will I know if a 600kB gzipped WASM file is competitive with the 150kB gzipped JS file it‚Äôs replacing? I would time it myself but it hasn‚Äôt seemed obvious in the Chrome or FireFox flamegraphs when this takes place.
The sublime plugin is handy for showing in line warnings/compile errors on save. Visual studio code isn't bad either but neither are heavy ides
There are quite a few other spelling and grammar issues on the page. I don‚Äôt know where you can hire a copy editor or technical writer on a contract basis, but it might be worth it.
I'm loving this blog! &amp;#x200B; P.S. What tool do you use for generating those visual diagrams?
It may definitely be worth it. Even though I am about to redesign it following the same narrative in the GitHub readme
Thanks so much!
Great to hear that! I created the diagrams using https://draw.io/.
&gt; # ~~Carefull~~ Carefully engineered &gt; RediSQL is written in Rust which provides more ~~guarantess~~ guarantees ~~agains~~ against common bugs. Then we carefully tested it.
Awesome! Thanks! 
Hey guys, can we just thank him for sharing his/her work with us instead of thinking about how he/she could've done it differently to help the community. He didn't do it for the community, he did it for himself; he just seems to be sharing in case it's useful for anyone else. Not everyone feels the need to contribute to the community as much as you guys, and him just doing this much is enough. Anyway, thanks OP.
Whoa thats huge - is there an RFC for this? This has some pretty great implications for IDE integration.
You can use UDP. Below that you have IP, and your concern is how to get operating system to allow you to use it. See https://github.com/libpnet/libpnet and search for "how to sendf ip packets" or "raw network access" for your OS.
This is cool but the obvious comparison is [node version manager](https://github.com/creationix/nvm), which many, many people I know use. What are the pros / cons?
Unfortunately, I have no time to help, but I like your project. Keep going!
I switched to notion exclusively because nvm was taking upwards of three seconds to load when you open the terminal. This is noticeable when you start opening tmux panes and have to wait.
Heh I've never seen it be that bad; but thats a big benefit. You should describe that on your website - "Like nvm but way faster (graph)"
Could we get access to the AST itself in future releases? I've built myself a syntax highlighter that is basically a mapping iterator for the events, but that means I have to do one of the following: 1. Buffer events for all elements that go into `&lt;pre&gt;&lt;code&gt;` block and emit those, or 2. Emit a single `InlineHtml` event, in which case I've to allocate a temporary buffer for the whole syntax highlighted HTML. Being able to write directly to the output buffer without temporary allocations would be obviously better than either option. A push parser adapter is something I've been considering, as that allows you to transform 1 event into `n` events, but if there is an AST with an arena, doing a quick second pass on it with transformations to insert extra nodes would be an improvement too, I reckon.
If you're using TcpStream, you are a level below HTTP already. If your server only serves HTTP, you have to use HTTP to talk to it.
What is the purpose of the hash? Integrity or uniqueness?
No personal experience with the crate. But the murmur hash is the fastest one you are going to find; https://crates.io/search?q=murmur
Hi there ! I am trying to implement a **sequential** program that listens to `stdin` while periodically writing unrelated things to demonstrate atomic operations for an assignment. I used a mutex to synchronize threads but the program just acts like a single-threaded one: the lock is almost never released before the `for` loop ends. ``` fn main() { let lock = Arc::new(Mutex::new(true)); let lock2 = lock.clone(); let millis = time::Duration::from_millis(1000); thread::spawn(move || { for _ in 0..50 { let _guard = lock2.lock(); // .unwarp(); println!("before wait"); thread::sleep(millis); println!("after wait"); } }); let stdin = io::stdin(); stdin.lock().lines().for_each( |line| { let _guard = lock.lock().unwrap(); println!("{}", line.expect("Could not read line from standard in")); }); } ``` Any thoughts ?
The hash is used to check whether a single file or the contents of a directory has/have changed, so it needs to produce the same hash from the same values consistently, but also avoid producing the same hash from different contents. 
Thank you. When I understand it correctly. I am still having the TcpListener on the server side, but when the client sends data from a UdpSocket, the server receives the plain buffer and I have to return a UdpSocket up to my liking? Is that right?
https://youtu.be/bzja9fQWzdA
Normally it‚Äôs possible to have two different versions of a crate coexisting in the same binary (though Cargo tries to deduplicate where they‚Äôre compatible), but when you have code that links to native libraries it‚Äôs not possible, because they operate in the same namespace. So your trouble is that gtk 0.6.0 needs a version of gdk-sys that is incompatible with the version of gdk-sys that webkit2gtk 0.5.1 requires. Your only recourse is to upgrade one, or downgrade the other, so that they require the same version of gdk-sys.
&gt; not wait for incomplete and buggy crates to get fixed. You claim that petgraph is incomplete and buggy, and claim that petgraph API is "quite leaky and hard to understand". So you decided to invent your own library to solve this problem. &gt; I decided to write this was when I saw that petgraph was 4 years old and still didn't have Breadth-First Search But petgraph does have BFS traversal: https://docs.rs/petgraph/%5E0.4/petgraph/visit/struct.Bfs.html which makes me suspicious about your claims. I want to believe you, which is why I asked this question above: &gt; Did you fill an issue on petgraph about the problems you had with it? Often, these questions are answered with "this is how you solve this problem with petgraph", "there is this other crate that builds on top of petgraph and supports multi-root traversal", "you are right, we should implement this, approaches X or Y would be a good place to start, feel free to send a PR".
[Fxhash](https://crates.io/crates/fxhash) is usually brought up when fast non-cryptographic hash functions are discussed. Its one of the fastest for large inputs. [https://docs.rs/fxhash/0.2.1/fxhash/type.FxHashMap.html](https://docs.rs/fxhash/0.2.1/fxhash/type.FxHashMap.html)
Don't worry about duplication!! Worry about cognitive load. Worry about incorrect abstractions. Worry about not understanding. Worry about not practicing reading code. Worry about big surface APIs. Worry about the big problems you're programming to solve. Worry about lifetimes. Try inlining most of your code and only create new functions for the sake of ergonomics, NEVER just to make your code more compact. This is all true for any programming language. There is so much to say about this but I don't have the energy to preach forever. &lt;3
You can take something like [`crc32fast`](https://github.com/srijs/rust-crc32fast) which will give you several GB/s per second which will be faster than almost any storage device (well, if we'll forget about tmpfs of course). For other hashers you can take a look at this comparison: http://cglab.ca/~abeinges/blah/hash-rs/
Hello, i'm neither a proper Rust dev(read the book, did some examples, did a small project but i didn't wrote more than ~1000 lines of rust yet, so a Rust newbie) nor do i have a lot of free time(40h work week), but i'm interested in using&amp;learning Rust and could spent a few hours of my free time with programming little bits.
You are right about petgraph having bfs traversal. However this only became clear to me now by you pointing it out, it wasn't clear in the documentation and there were absolutely no examples of bfs traversal which made me assume (incorrectly) that petgraph does not have bfs traversal. This proves my previous point. A library user shouldn't have to file issues and ask the tired devs who don't answer anyway in order to understand how to use it. It should be clear and simple. At the same time, my main problem wasn't bfs traversal but multi-root graph traversal so I didn't thoroughly check the documentation which was giving me headaches at that time. I just thought it weird that it didn't have bfs so it was another reason for me to write this.
This is super cool, but I'm lukewarm about news of a new virtual-dom implementation because nobody seems to address the problems suspense &amp; time slicing changes in react aims to solve ([which was one of the things that motivated the react 16 rewrite](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html)). IMHO those will have more of an impact to FE web dev than just having another vdom implementation, even with the potential for perf improvements from using wasm/rust.