I'm not even half way through. I'm afraid TFS will be like 50,000 lines, even though I'd like to keep it minimal.
More libraries are always good. I don't have a particular need for this one, but better to have one and not need it than to need it and not have it, IMHO.
&gt; It could be constant time and fetch the char that starts at byte offset 5 This doesn't work, because you wouldn't get a `char`. That is, you _could_ get four bytes, and whatever they happen to be would make up that `char`, but it's not going to be whatever letter you had. That is fn main() { let s = "abcd"; let r = s.as_ptr() as *const char; unsafe { println!("char: {}", *r); } } does not give you something coherent.
&gt; This is a question that so many developers/engineers struggle with, too, in my experience. We need, not just as a Rust community, but as a software engineering community, better material (and better education in the college level, IMO) about this. I would be extremely happy to add something specific to the docs about all of this. I try to cover it, but it's such a huge topic...
I've explained this in another comment. You're stuck with an `Index` that returns a reference, but with `IndexGet -&gt; char` this would be perfectly possible. Edit: Looking closer, I've explained this in the comment you replied to.
Whoops my b
Great work!
What's Redoxes purpose or intention? I ask this without intending to sound critical: it seems like a cool project, I'm just wondering if there's any goal outside of proof of concept.
An Index trait that doesn't return a reference, but a value from its method. Then `foo[i] -&gt; char` would be implementable. (And it solves the `BitSet[i] -&gt; bool`'s conundrum too, among other things).
https://doc.redox-os.org/book/introduction/what_is_redox.html 
Ah yes, I see. Sorry, since that doesn't exist, I got confused. It's early here, my bad.
Not yours, but mine. "Fix the old book to not say UFCS" has been on my to-do list for over a year...
is it expected behavior for the game to exit when I crash? edit: it seems to crash after a few seconds no matter what on win x64
No, writing operating systems in GC enabled system programming languages goes all the way back to Mesa/Cedar at Xerox PARC.
Yes it is, as many have been confused about this, I'll try to printout an alert :) If there is no error messages, it seems that you've hit a pale box (grey box with small alpha channel)
The problem is that you'd need latency-tolerant hardware and users for this to work in practice.
&gt; For pointers, you mean that adding a pointer like in `*v` is a move operation ? I don't understand the question. In the context of `*v` the `*` is not "a pointer" but a pointer dereference, and the `v` is (in this case) a reference, which is a type of pointer. Dereferencing a pointer can be a move operation (but it need not be, e.g. when the pointed-to thing is `Copy`, or when you do something like `&amp;*v`). If you have trouble with these concepts I again recommend reading the book, or you could try asking on #rust on irc, which I suspect might be better because it is a bit more interactive than messages on reddit.
Native Oberon did not had any issues doing video editing in mid-90's hardware, other than having some Assembly on the codecs, but that even fmpeg has. It also had a big chunk of ETHZ IT department as its users. And how latency-tolerant do you think ARM Cortex-M3, Cortex-M4, Cortex-M7 and Xilinx FPGA Systems are? http://www.astrobe.com/default.htm
In an OS written in bare metal Go, the native libraries are Go libraries. :)
The second edition on rust-lang.github.io is newer, but incomplete.
Is there reason to believe packages from crates.io wouldn't be malicious?
Reminds me of Tempest. Great work!
actually, i found out that over 3/4 of dependencies are available either via cargo or are small enough to be snipped out from something larger code and ported 
Ah, you're right.
This subreddit is for Rust the programming language. I believe you're looking for /r/playrust
Ok thank you
Oh sh*t xd
Hmmm this could be a really nice gametype for osu/guitarHero like games.
&gt; Native Oberon did not had any issues doing video editing in mid-90's hardware Video editing is not necessarily sensitive to latency. Video playback is. Beside, I am not familiar with Oberon, but video editor sounds like user-space application, not part of an OS. Or was it implemeneted as part of kernel? &gt; It also had a big chunk of ETHZ IT department as its users. There are many uses for applications using GC, I am not denying that. Its just not suitable for an OS. I conclude this from the fact that all popular operating systems do have several places where any form of unpredictable overhead is unacceptable. &gt; other than having some Assembly on the codecs Which I presume does not implement any form of GC at all.
Never played those games before :&gt;
assuming that you're not using any non-Rust code in the fault-tolerant code, and you avoid introducing new `unsafe` code, you can use [catch_unwind](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html) to absorb any crashes that happen and keep going, optionally attempting to run the same exact computation again, but I don't know why it would work any better the second time around. Many existing pieces of Rust code use `catch_unwind` to isolate along thread boundaries and prevent a single thread from taking down the entire application. If you use an FFI to interface with C code and the C code segfaults, the situation gets a lot... stickier. However, I would make the (possibly dumb) argument: why not just make your code avoid crashing in the first place? handling crashes is a last resort, and I think defense-in-depth would suggest it is a necessary piece, but I believe Rust makes it much easier to write code that won't crash than a language like C or C++.
Signedness doesn't exist in the popular notion of hexadecimal. This is correct behavior. The hexadecimal output is emitting a compacted representation of the bits. It is not altering the radix on an abstract mathematical number. The bit pattern `0b1000_0001` is considered +128 + 1 for unsigned, and -128 + 1 for signed. Printing any number as hex prints its bit pattern, not its abstract numeric value.
Specifically, that's because Atmel mints ARM chips as well as AVR. The Due runs an Atmel SAM chip, which uses ARM ISA in the kind of package for which their AVR line is known.
I built an npm proxy that only fetches what isn't cached - and packages are cached forever. This lets us easily keep up to date - but only for the packages we need (which is &lt; 1% of all npm packages) It would be nice to have something similar for Rust. I have been putting this off for Rust/Cargo and don't have anything working for production code yet :-( 
I see. That makes sense. Git doesn't *necessarily* have releases like Crates does. Would using commit hashes, while not as elegant as releases, mitigate this problem?
Significantly less often, though. The only time taking a reference will crash is when it's hidden behind an indexing operation, like `&amp;a[6]`. Taking a reference or dereferencing with `&amp;` never panics.
&gt; easy_strings I'll check that out. Thanks!
So this has to do with [object safety](http://huonw.github.io/blog/2015/01/object-safety/#references-self) -- any trait that references `Self` is not object safe because the true `Self` type is erased. However, the error doesn't seem appropriate in this case. I think this warrants a bug report at least to fix the error message, if not to relax the object safety check (because there could be something I don't see preventing it still... can't say I fully understand object safety).
With Rust you can achieve a great level of stability on your software thanks to no exceptions, ADT, exhaustive pattern matching... You can run some threads and rerun them if panics (even integer division by 0), it's quite easy. But Erlang is far away with thousands or millions of processes, linked processes, supervisors, OTP caring and managing the state and restarting...
&gt; I am a firm believer that is an historical accident of the rise of UNIX and political decisions where to spend money. In some aspects, maybe. But I believe that the fact that people are extremely sensitive for interruptions in sound playing, or that you *have to* probe some device n times a second to work correctly is just practical requirement, regardless of historical choices. &gt; I guess you missed the reference to ffmpeg also requiring Assembly, even though it is heavily optimized C The problem with ising GC is not that it slow, but that a) its unpredictable and b) it has time and memory overhead (not to mention the fact that it heavily affects design of everything it touches) you may not want to accept in many situations. If the parts of ffmpeg use assembly for speed (over C doing the same thing), than it has nothing to do with GC. If it would happen to avoid GC (which might be the case with Go), than it would just prove Go not being suitable for system programming. &gt; I will cheer for anyone that takes the time and effort to prove people wrong in regards to using GC enabled system programming languages. Many things can be done, but it just makes no sense to do them. There are no practical benefits of using GC for low-level programing, while there are many problems that it causes. Sure, someone may bravely fight them, but ... why do that?
It boils down to that in the first one, we own `x` so we can choose if we want it mutable or not. In the second one, someone else owns `x` so we can't choose to change our immutable access to mutable access.
Also, please let me know if you have any problems or have suggestions for improvements/missing features.
When you call a method, for convenience, Rust will effectively automatically insert as many `&amp;` or `*` as is necessary to make the method call. Bare in mind that when you see a `*x` it isn't necessarily a load. At compile time the compiler balances out the `*` and `&amp;`, meaning that for `&amp;*x`, although it looks like taking the address to the dereferenced value of `x`, both operations just cancel out and it becomes `x`. Method calls automatically take a reference to `self`, so in `(*x).push(102)`, `push` takes the address of `*x`, so both cancel out and `push` just gets passed `x` as `self` which is already a reference.
What I am trying to get across is that you can write bare-metal Go code, and can even compile it using the normal Go compiler. In that regard it is just like Rust. You can use as much or as little of the runtime as you like. Now, to be fair, and based on my admittedly limited experience with Rust, Rust *does* make that a little easier than Go does.
Based on the crate description and your 3GB input this smells like you are swapping. How much RAM do you have? 
It's running in an AWS server, and the server has 32 GB RAM.
Thanks. After looking around a bit, I think a cache miss really makes a lot of sense. I'll look into optimizing that. It's been a year since I got into systems programming (and Rust), and there's still **so much** to learn!
Wow, thank you so much! I would never have discovered this myself; is there documentation that covers this?
That link is really interesting! And no, I can't sort the data. The values at each position are *special*. They're the positions of each prefix in the original string.
Since it is very likely to be related to cache misses, this might be the first time I've seen a situation where buying a processor with a larger cache would lead to visible increases in performance. A standard Intel processor only has about 8MB of shared L3 cache, whereas the top-of-the-line i7-6950X has a whopping 25MB of L3 cache, and some Intel Xeon processors can have 45MB of L3 cache or more. It could _significantly_ increase your cache hit rate, or it might not, since I think you said you're already using an AWS instance. That instance _is_ sharing with other instances though, so there are a lot of variables to consider. It would be interesting to see just how much impact cache size would have on performance on this particular problem.
I can't really watch a video where I am right now. Is there documentation somewhere on the kernel characteristics? I'd love to see a kernel that merges the security model of SEL4 with the IPC model of QNX. Probably a bit much to ask, but I'm still curious as to what other directions are being taken in the microkernel space. 
Not directly answering your question, but I had much better time profiling with valgrind as desrbied here: https://jbendig.github.io/fix-rs/2017/01/24/how-to-optimize-rust-programs-on-linux/ . It is heavy and affects your program performance, but it's just so much easier to work with... I wouldn't worry about that `unkown` (might be just an artefact/glitch of release build that confused `perf`). The `self time` question is unclear to me too.
&gt; Amtel Atmel
The other answers seem a little pesimistic: AFAIK a `read()` syscall on a socket that has been closed on the other end [will return 0 bytes read](http://stackoverflow.com/questions/2416944/can-read-function-on-a-connected-socket-return-zero-bytes), but no error, which **should** translate to `Ok(0)` in Rust. But this may only apply when the other end closed the connection cleanly. Anyway, you should not ignore the return value of `read()`, it will give you useful information. Your current implementation won't let a NUL byte to occur in the stream as this will be treated as "no new byte received".
Awesome, good work! 
Can you somehow take advantage of the fact that you could pack four values in a byte? I assume you can't, but it's worth a shot.
&gt; Many existing pieces of Rust code use catch_unwind to isolate along thread boundaries and prevent a single thread from taking down the entire application Isn't catch_unwind about isolating *within* a thread? An uncaught panic within a single thread doesn't take down the entire application anyway (unless it's the main thread). Across threads, you have to deliberately use JoinHandle::join to detect termination, or run into a poisoned mutex or something equally unfortunate. I ask this as someone who this afternoon rewrote some code from using spawn and join to contain panics, to using catch_unwind, and is really hoping he's understood what they do and not screwed up a program which is being deployed into production as he types this ... 
Yup, I also use this method in practice and it is quite reliable (at least on Linux).
I want such badges on crates.io. There are a lot of outdated/abandoned crates.
thanks for the pointer. I'm actually more accustomed to valgrind so i'll give it a try for sure tomorrow. the article you linked enabled me to find something interesting: i was using in Cargo.toml : [profile.release] debug = true but not: [profile.bench] debug = true adding this last one, the behavior of the profiler changed dramatically: one of my "unknown" disappeared and now the Cutter::run is at 68% 
Rust also supports linear-ish types via `#[must_use]` or `Drop`.
Right, I should have looked at the code :(.
cc /u/formode
Yeah. Linear-ish is definitely the way to describe it I think. 
It is certainly worth watching the whole interview. My only complaint is that the interviewer kept interjecting while Jeremy was talking, but overall it is interesting and enlightening.
upon further research, it looks like you might be right, and I may have misunderstood.
thanks, this seems to be what I need along with timeouts
Oh man, makes me think of the training levels in TIE Fighter
Not easily possible. You need to compile for the .NET VM to use .NET libs, and Rust compiles to machine code.
Sounds like JFrog's Artifactory. A Cargo plugin would be cool.
Is it possible to directly build apps on top of TiKV using the protobuf interface? It seems like a powerful distributed KV store in its own right which could be useful even without TiDB on top.
Be careful with idle connections though. If you are not sending any data, you may not find out that the remote end has crashed long time ago.
Here are a few resources which might help: * [How-to Optimize Rust Programs on Linux](https://jbendig.github.io/fix-rs/2017/01/24/how-to-optimize-rust-programs-on-linux/) guides you through using Valgrind and KCachegrind to gather and display a profile with Valgrind's virtual CPU collecting cache statistics. * [How profilers lie: the cases of gprof and KCachegrind](http://yosefk.com/blog/how-profilers-lie-the-cases-of-gprof-and-kcachegrind.html) points out some caveats in how KCachegrind visualizes the profile data. * While I haven't incorporated what I learned from the first link into it yet, the justfile in my CLI project boilerplate has [a task](https://github.com/ssokolow/rust-cli-boilerplate/blob/master/justfile#L91-L97) for streamlining the "run-inspect-edit" loop a bit and adding the "enable cache stats" argument to the valgrind call would be trivial. (I may have done it by the time you read this.) ...and, if you haven't watched [code::dive conference 2014 - Scott Meyers: Cpu Caches and Why You Care](https://www.youtube.com/watch?v=WDIkqP4JbkE), I recommend you do to insure that we're all at a common baseline, knowledge-wise.
&gt; The consistent hierarchy of Local &gt; External &gt; Std I feel helps mitigate that a bit. I think I've actually seen the exact opposite more frequently. Also, I would personally be inclined to start any style guide with "run `cargo fmt` on your code". :-) 
...which does in fact exist, with that name: https://github.com/dikaiosune/rusl :)
Or you could reimplement the [CLR in Rust](https://github.com/paavohuhtala/clri), which would be even more awesome. (It's not usable in any way yet though)
Theres a page on these attributes in [the Serde user's guide](https://serde.rs/attributes.html). 
Not in it's current state (and most likely not in any future state either), but if it was a finished, polished project you could in theory load &amp; execute .NET assemblies from Rust. :) I'm very slowly working towards a working interpreter; I can load CLR assemblies from .exe files on both Windows and OS X, parse all of the metadata (for a simple hello world program), and load &amp; parse the CIL bytecode. The "only" thing missing is the runtime.
What does "UB" stand for?
&gt; However, I would make the (possibly dumb) argument: why not just make your code avoid crashing in the first place? handling crashes is a last resort, and I think defense-in-depth would suggest it is a necessary piece, but I believe Rust makes it much easier to write code that won't crash than a language like C or C++. This is quite the opposite: things that would cause the application to start acting wildly in C(++) are often reliable panics in Rust. See: index out of bounds, null dereference, concurrent modifications (Refcell), and many integer overflows. The primary reason Rust *has* panics is to exactly enable Erlang-style "die fast, spin back up" designs. Servo has long been built around this (parser panics? treat it like a resource that didn't load).
I submitted an [issue](https://github.com/rust-lang/rust/issues/40533) in case you're interested..
It's like a 3D Audiosurf! :O 
Yes! Seems like you can already do it by packaging your app into a container. Has anyone tried it yet? On a related note, Rust should provide official docker images.
It's not on by default, you [have to pass `-Z mir-opt-level=2` or `-Z mir-opt-level=3`](https://github.com/rust-lang/rust/pull/39648/files#diff-04789deb41c6e9576b3942e6a92e5551R51).
The point is linear types can be introduced to Haskell in a backwards compatible way, and only when it's necessary for performance or safety reasons. It's opt in. Rusts borrow checker isn't opt in (right?)
I was thinking of writing Rust bindings to Mono or CoreCLR, but I don't have any real motivation yet.
Great to know! Thanks. Let's say I do that - what might I notice from the MIR inlining vs letting LLVM deal with it? Lower codegen times? Faster release builds?
Makes sense - I'd never looked at the innards of journald, only the features (which I think are pretty nice when you get used to the break of tradition).
I think it is supported in the sense that you can bring a Docker container with your Rust applications.
Upvote for the Rust images. Currently using someone's third party images to test stable, beta and nightly on Gitlab CI :)
All the work on my quotes has paid off ‚Äì between dozens of limericks, one off-hand remark takes the cake... üôÉ
Nice! Gave me something to play with on the commute to work :) Had to change the main loop though: // Do things that have to be done every frame while let Some(e) = window.next() { rect.update((640.0, 480.0)); window.draw_2d(&amp;e, |c, g| { clear([1.0; 4], g); // Clear to white rectangle(rect.color, // Color rect.position, // Position/size c.transform, g); }); } Also updated to depend on version 0.64.0 of piston_window. (Only had time to get the square bouncing... not the control part.)
No, Rust is not directly supported. The link is to the documentation on how to make your own support, so I thought there would be some Rusaceans interested in exploring this option.
&gt; In Rust just use cargo fmt and advise other people to use it. The last time I tried `rustfmt` I really disliked its rightward drifting. Has there been changes about this? 
Sure, there is a go client driver. You may use the raw API to get/set key values. Ref: https://github.com/pingcap/tidb/blob/master/cmd/benchraw/main.go
It would be helpful to include the abbreviation style, e.g. `HttpParser` instead of `HTTPParser` and `Vector3dIter` instead of `Vector3DIter`. Edit: This is not usually picked up by automatic formatters.
I guess it could have gone into rustc fork of llvm. Not sure why that wasn't considered.
My bad, Idk why I thought you were responding to [this other comment](https://www.reddit.com/r/rust/comments/5zbiaa/question_about_mutability_and_references/dewzf9a/). Yes you are totally right, i figured it out later, when I was able to understand (partially) how Rust interprets dereferencing syntax.
Just to close the loop on this: I implemented @retep998's suggestion, which is good but not as good as having the compiler let me checked that a type is #[repr(C,packed)] for example. My use case is exactly what @bbatha describes.
You can design your program let's say a web app so it's stateless, by inlinining all it's assets like templates,CVSs etc basically have no access to file systems. It becomes like a function, because rust is a language that "prevents almost all crashes", your program should run for a long time [see skylight.io case]. If it does crash , have systemd or other supervisor program restart it and it will carry on being a stateless app won't effect. 
&gt; What happens when you use a unicode regex in python2, or a bytes regex in Rust? On my machine setting `re.UNICODE` and matching against a unicode string instead of a bytestring makes the Python version's runtime increase by ~10% (4.5s -&gt; 4.8s). The Rust version is around 10s on the same system. Also FWIW using named capture groups yields no difference.
Sadly, this is expected. Resolving the capture groups is what's killing performance here. While the regex engine is very fast at determining where a match is, the DFA that does that can't also determine the location of capture groups. So, when your regex is executed, it's actually executing the DFA first to find the bounds of the match and then executing an NFA engine to find the location of each of your captures. In your case, since the match itself and the string are roughly equivalent, running the DFA is actually hurting us here and not helping us. [If I fix that](https://github.com/rust-lang/regex/issues/348), then I get a sizable speedup (about 22%), but it's still not faster than Python. The regex crate has basically two methods for finding captures using an NFA engine: it either uses a full NFA simulation (called the "pike VM") or it uses *bounded* backtracking. Backtracking can actually be rather fast in its non-exponential cases, but in order to satisfy the O(n) search time guarantee, we need to do some bookkeeping to keep track of which states we've visited so that we never visit the same state more than once. In a profile of your program, this book-keeping shows up. If I remove the book-keeping, then I get a 21% boost (39% total including the previous optimization). But the book-keeping fundamentally has to be there, so this is more of a curiosity. So what can *you* do to make it faster? Not much, unfortunately. The ball is really in the regex crate's court: * [Stop running the DFA](https://github.com/rust-lang/regex/issues/348) * [Implement a one-pass NFA engine](https://github.com/rust-lang/regex/issues/68) (It *looks* like your regex is "one-pass", which I think qualifies it for this optimization.) * Experiment with other things, like bit-parallel NFAs. The latter two fixes are probably a bit far off at this point, but the first is readily fixable with a little experimentation to figure out the right threshold. There may also be places in the code where I've done things sub-optimally, so more eyes on it couldn't hurt. :-)
There's no need for your struct there. You can just use `std::panic::AssertUnwindSafe`.
Unicode is a first class thing in the regex crate, so you generally shouldn't lose any performance because of it. In fact, in the NFA simulations, byte based regexes might be slower because the compiler can sometimes move UTF-8 decoding into the FSM itself. This works *really* well for the DFA, but no so much for the NFAs because there's a lot of overhead associated with it. Classical backtracking engines do, in my experience, pay for Unicode support. Sometimes the price is pretty high.
you could also implement a parser combinator instead of a regex, since /u/burntsushi has chimed in and pointed out that your situation is exceptional. [nom](https://github.com/Geal/nom) is exceptionally popular, but it is also really challenging to understand for many people. [pom](https://github.com/J-F-Liu/pom) might be easier to get started with, since it has a distinct lack of macros. [combine](https://github.com/Marwes/combine) is a good option to look at too. A parser combinator could easily be faster than any regex too, I think, so that might be encouraging.
For what it's worth, I actually don't think this case is exceptional. It seems like a very standard thing someone might want to do with a regex, e.g., churn through lines of a log file and extract structured data from relatively-unstructured text. Resolving capture groups quickly is probably the biggest weakness of Rust's regex crate. I can't remember where I saw it, but someone at Google did an analysis of where RE2 spends most of its time within Google's codebase, and the answer was overwhelmingly the one-pass NFA matcher (which RE2 has, but the regex crate doesn't have). The one-pass NFA matcher isn't that useful on its own, since it's restricted to fully anchored regexes, but that's precisely the case we want after a DFA has run (since we already know the extent of the match, we can anchor the regex). But yes, sorry for the niggle, parser combinators would be a totally valid approach here too of course. :-)
you are perfectly right. I'm on manjaro (arch variant) and actually libc is fine but libm has been stripped of debug symbols. after re-compiling the libc, re-running my tests everything is now fine with valgrind. perf however is still stuck at 68% with unknowns. thanks !
yes, i know them. however perf's results are not ok so the flamegraph is not helping here. thanks anyway for your pointer !
How you've structured the test is a little odd. In your real world example you'd be reading in a log file, rather than carrying out a repetitive action against the same string. If you're going to realistically benchmark stuff, it's best to try to get as close to a real world scenario as possible. Compilers are very clever and will optimise stuff you might not want to be optimised in your benchmarking. I'm somewhat surprised the rust compilation process didn't do something very clever there given it can see the whole of the picture. By way of example, if you run pypy against your benchmark code, it comes out about twice as fast. If you switch to reading in a file that consists of that same line repeated over and over, pypy comes out as comparable with python on the scale you demonstrated. Making a fake log file: for i in $( seq 1 1000000); do echo '13.28.24.13 - - [10/Mar/2016:19:29:25 +0100] "GET /etc/lib/pChart2/examples/index.php?Action=View&amp;Script=../../../../cnf/db.php HTTP/1.1" 404 151 "-" "HTTP_Request2/2.2.1 (http://pear.php.net/package/http_request2) PHP/5.3.16"' &gt;&gt; log; done Depending on your log file size, you may find pypy better suited anyway. Making a 2Gb version of that log file (multiplying its length by 10), then using a benchmark script that: $ time python longpylogreg.py 1510000000 real 0m11.939s user 0m11.420s sys 0m0.396s $ time pypy longpylogreg.py 1510000000 real 0m8.058s user 0m6.372s sys 0m0.440s (there's still a chance that pypy is being clever, given the log file is just all the same data).
How does waitid help? Is there some identifier other than pid you could use? The Linux manpage only mentions pid, pgid, and all. Those same options are available with waitpid.
&gt; I consider detailed style guides harmful unless they are automatic. I tend to agree. Perhaps this should be thought of as more of an organization guide? I would consider this best effort basics, as it's certainly not my intention to create a restrictive "follow-this-or-die" document, but I find it useful for myself though I understand why others may not. I'm also an idiot who forgets `cargo fmt` exists every time he starts a Rust project. :/
My "real life" test case indeed reads and processes a huge log file. The speed difference between the Python and Rust implementation is exactly the same. I made this simplified example so it is easier to test and run without having a logfile. 
I recommend writing a rustfmt.toml that satisfies you and committing it into your projects. That way your style guide is automatically maintained. 
`waitid` has a special (kind of confusingly named) `WNOWAIT` flag, which lets you leave the child in its zombie state after the wait is finished. That's exactly what you need if one thread is waiting and another thread might kill. You do the first wait with `WNOWAIT`, and kills are allowed during that time. Then, when the first wait returns, you mark the child as exited to prevent further kills, and finally clean up the zombie with a regular wait.
Good to see that Armin &amp; Team loves Rust üòÅ (One of the few weakness pointed was very long compile times)
My point was not to come up with a "proper" benchmark but to demonstrate the speed difference as simply as possible ;)
After generating the docs I found out this also works: while let Some(e) = window.next() { match e { Input::Update(_) =&gt; { rect.update((640.0, 480.0)); }, Input::Render(_) =&gt; { window.draw_2d(&amp;e, |c, g| { clear([1.0; 4], g); // Clear to white rectangle(rect.color, // Color rect.position, // Position/size c.transform, g); }); }, _ =&gt; {} } } Seems they've changed the name of the type returned from Window::next to Input rather than Event.
I think what /u/burntsushi meant is that Rust doesn't check whether Rust functions have side effects, so it can't optimize multiple function calls that will have the same return value out.
Serious developer is serious. Serious developer no laugh.
Pricy font!
manually-anchored regexen are not that rare on their own (e.g. this example).
"The current implementation of FM-index is a memory killer, since it stores positions of all bytes in the given data. For the human genome (~3 GB), it consumed ~27 GB of RAM to build the index (in ~4 mins)." You are probably just starting to swap depending on what else is running. 
I guess I meant faster compile time release builds because of LLVM doing less work, not faster run time release builds; I'm pretty confident LLVM does good work there. I just don't know if it would be enough of a difference that we could tell.
Makes sense. Thank you!
Ah well. I rarely ever see or use them.
Maybe I'm naive in my understanding of the compilation chain, but wouldn't LLVM have enough context, even if the rust compiler doesn't?
Does anyone know if there's a video that goes along with this? Also, I would love to have heard about Sentry's experience with Rust.
I came from arch too. Use the overlay! It works awesome! I use stable rust wherever I can, but when I need nightly, I use the overlay.
Why can't capture groups extents be determined by a DFA? I'm not 100% clear on that part. Or does it just make the DFA way bigger?
I feel like F#'s Units of Measure deserve a shoutout here. After having them save my ass a couple of times and assisted me with algorithm development many others, it makes me wonder why more safety oriented languages haven't adopted them. 
Thank you for explaining why my company's code base has so many fully anchored regex. I always thought it just improved readability.
Sadly not but I was considering doing a version of this talk at polyconf. 
Just to throw something out there, [fancy-regex](https://github.com/google/fancy-regex) has a backtracking implementation, and by slightly tweaking the regex (putting in empty context like `(?=)` for example) it's possible to switch it to that rather than deferring to regex. I'm not suggesting people put that into production, but it might be worth experimenting with to see if it can improve performance. Right now, fancy-regex aggressively delegates to regex, but I've thought about doing some kind of analysis to make it more selective. For example, `"abbababbabaa".match(/(([ab]*?)(b*?))*(a*)$/)` (example due to Vyacheslav Egorov) gives different results in different regex implementations. Backtracking in fancy-regex matches v8's results (which is desirable), while the captures from regex don't.
Imagine a graph represented as an indexed array. You're going to have one index stored away, as the program's entry point into the graph. You're also going to need a free list, so that it can add a new node to the graph. Any positions that are neither in the free list nor reachable from the entry point are leaked memory.
/r/osugame
&gt; I may have misunderstood what you were looking for. For me, composite meant dividing meters per seconds to get a speed (speed being the composite). No, that's exactly what I want. If you multiply or divide two unit-aware numbers, the resulting number should have a different unit than either of those two numbers. 9 meters / 3 seconds should give you 3 (meters / second) or 3 mps or 3 ratio&lt;meter, second&gt; or however it should be represented. The challenge is if you do this series of calculations: 12km / 3hr -&gt; 4 (km/h) 4 (km/h) * 21min -&gt; ??? In `dimensioned`, every length unit is represent by a `Length&lt;T&gt;` which internally represents every kind of length as a meter, I think. This same property is what would allow it to multiple hour^-1 * min and cancel those units. However, if hours and minutes are completely separate types in the Rust type system, how do you know that the `ratio&lt;km, hour&gt; * min` should yield just a `km` newtyped number? At compile time, you have to be able to calculate that you can simplify rather than getting a `mul&lt;ratio&lt;km, hour&gt;, min&gt;` unit type which would not be useful. If and when Rust type specialization lands, that might be helpful. I might have to generate `mul` trait implementations for every likely-to-be-encountered composite unit type, which would be less than fun, but with macros, it might not be unthinkable.
&gt; you could also implement a parser combinator instead of a regex Regular expressions are a subset of parser combinators, no? Using user-supplied regex is the only time I could think they should possibly be slower. 
&gt; However, if hours and minutes are completely separate types in the Rust type system, how do you know that the `ratio&lt;km, hour&gt; * min` should yield just a `km` newtyped number? I would argue that you should probably not take this decision, actually. Because of the way precision works, I would punt on this and involve the user here: would the user prefer (1) converting `km/h` to `km/min` and then multiplying by `min` or (2) converting `min` to `h` and then multiplying `km/h` by `h`? By NOT implementing the composite multiplication you force the user to decide what is the best scale to use for this computation. Anything else is likely, I am afraid, to cause confusion by silently losing precision and require painful tracking because it would not be obvious what went on. Any kind of deduction that you try to make will just end up being more confusing. What should be the result type of `km/h * min + m/s * year`? Should everything be reduced to `/s` first? What if it was `/ns`, would things still fit? Only the user knows the magnitudes of the number involved, and therefore I argue that the user should be the one to decide. --- Regarding the typing issue, however, note that the ratios I presented completely solve the problem in C++. If you look at the C++ chrono library you'll notice there is a *single* duration type, parameterized by the internal representation (i64, f64, ... or i16 if you wish) and a ratio. km, m, nm, feet, yards, miles, would all be an instance of `Length&lt;&gt;` with an appropriate ratio compared to the SI unit (`m`) and adding `km` would just work (ratios being identical). I feel like you could do the same in your case; though as mentioned given the restrictions on type-level computations, you'd better stick to powers of 10 for the ratios...
/r/playrust
Just for fun, the target I have at work (SPARCv8, GNU toolchain, VxWorks system) doesn't have a `stdint.h` and I hate everything about that.
TLDR of the other two responses, `main` doesn't mutate `foo`; `main` gives it away and is no longer able to see what happens to it. No mutation within `main` means the compiler doesn't want you marking it as mutable within `main`
C++ is officially supported for CLR
I've been pondering of doing a similar thing for F#. There is lots of value in remembering what you need to call by heart, as opposed to googling everything one by one
There was [this blog post](https://blog.sentry.io/2016/10/19/fixing-python-performance-with-rust.html) a while back.
You could create your own `stdint.h` where you declare those types and then include your header everywhere you need it.
Why would that help a DFA or NFA implementation? Unless you mean a pushdown automata or something.
I loved Flask the last time I was doing web dev with it (thanks). I wonder if you've been involved with any Rust web development libraries/frameworks, and if you had any thoughts on how Rust compares to Python in that domain, and whether Rust efforts are going in the right direction in your opinion? 
I get it. I was more asking (poorly) why `\w+$` is easier to match against than `\w+a` or something. But I see how `$` is special in that you know its location? I think I get why that is true under various circumstances. thanks
How about adding two more pointer in the automaticaly generated vtable for CloneMammal, one for each implemented trait? let clone_mammal: &amp;CloneMammal = cat; // clone_mammal is a fat pointer (&amp;vtable_CloneMammal, &amp;data) let clone: &amp;Clone = &amp;clone_mammal; // clone is a fat pointer (&amp;vtable_CloneMammal.vtable_clone, &amp;data)
Right. Because if the regex doesn't match in reverse from the end of the string, then you know for sure that it never matches. It's basically the same optimization that (probably all non-toy) regex engines implement for the `^` anchor, but being able to match regex in reverse takes a little bit extra effort (for FSM based engines anyway, for backtracking engines I'm not sure if it's possible at all). For `\w+a`, it could match anywhere. Now, it is possible to search for the `a` literal ("suffix literal optimization") and then match `\w+` in reverse. But there are some gnarly corner cases to handle, otherwise you end going quadratic. I can expand on any of this if you're interested, but figured I'd stop here for now!
I would say this is preferable, and would be modern C.
First and foremost, Russ Cox's series of articles on regex implementation is the canonical source for how to build a production grade regex engine based on FSMs: https://swtch.com/~rsc/regexp/ --- Rust's regex engine is heavily inspired by this. For example, it would be fair to say that Rust's regex engine, Go's regex engine and RE2 all implement roughly the same algorithms. (I could spend days talking about the differences between them, but those are details.) Other bits: * [Performance tips for the regex crate](https://github.com/rust-lang/regex/blob/master/PERFORMANCE.md). * [Benchmarks with some explanations I use](https://github.com/rust-lang/regex/blob/master/bench/src/sherlock.rs). * [Heavily commented DFA implementation (read Cox's third article first)](https://github.com/rust-lang/regex/blob/master/src/dfa.rs). * [Heavily commented SIMD multi pattern matcher](https://github.com/rust-lang/regex/blob/master/src/simd_accel/teddy128.rs).
Great article (only read the GitHub link, so far). One question; doesn't the `long` / `float` punning break C's strict aliasing rule? I come from a C++ background and as I understand this has always been discouraged as strictly undefined behaviour
~~Nope~~ Only if you use `-Wall`. It's a straight up copy: the `*(long*)&amp;orig` part compiles out (or at least it should) so that the only ~~a~~ effect that entire line has is to copy a bit pattern into a region the compiler will treat as an int, not as a float. It could actually have been done better via a `union`, as that doesn't require duplication. I don't know offhand if C permits unions in function parameter positions -- even my masochism has limits -- but this would have worked equally well: int func(float orig) { union { float f; long l; } i.f = orig; i.l = magic - (i.l &gt;&gt; 1); } I have yet to run into any UB from throwing (struct) pointers around like confetti in C, honestly. That may be because I am stuck with C99 being the most modern standard I can use due to my constraints at work. ---- The text is identical on GitHub and my personal site; my site just adds a little formatting and framing. No reason to visit both; its not like I have any burning desire to rack up visit impressions.
If you're gonna need fancy stuffs on your website you'll need javascript for sure, you could use a framework as well and minifity it in production with webpack ;)
Ah, you're compiling with `-Wall` Get out of here with that "obedience to rules" and "hang on what if we looked before we leapt" crap :p Though I must say I enjoy that the warning message *explicitly* calls out this hack. It works fine without `-Wall`, and swapping it for the `union` line yields identical behavior and compiles under `-Wall` It makes sense that C has opinions about doing this on primitives, but even under `-Wall` I have never seen it raise an eyebrow about doing this to structs. I'll edit the text to use unions and explain this, though; good catch! ---- I put an aside right below that section detailing your note and how to do this in a way that the compiler will accept. Thank you and /u/cakefonz for your notes!
Absolutely. Non-explicitly-sized types was a travesty.
I know someone who created a tagged DFA engine. Unfortunately it's in his own language, but he has some notes about it [here](https://github.com/samrushing/irken-tdfa). I can get you in contact with him if you want, I'm sure he'd be happy to talk about it. From what I remember, it does create a large number of states, but for relatively simple regexes, it works quite nicely. Someone has also created one for Haskell [here](https://github.com/ChrisKuklewicz/regex-tdfa).
`union` is the compliant way to do this in C ([but not in C++](http://stackoverflow.com/a/11996970)). Alternatively, you can use `memcpy`, which can be optimized out by modern compilers and works for both C and C++. --- Some other things: &gt; struct Device* pdev = (void*)pdh - sizeof(struct DevFuncs); Clang says: warning: arithmetic on a pointer to void is a GNU extension Also, regarding padding, it's safer to use `offsetof(struct Device, info)` here. &gt; Addition is reflexive Addition is commutative, meaning `a + b == b + a` for any `a` and `b`. Equality is reflexive, meaning `a == a` for any `a`. Edit: Correction regarding type punning.
Can I have you be my editor more often \*looks at compiler flags\* yup I'm using `-std=gnu99` Re: offsetof; I was thinking that `sizeof` couldn't possibly be safe enough and remarked that a means of getting the full offset with padding included would be better. I didn't know offsetof existed, honestly. &gt;s/commutative/reflexive It's a good thing none of my math teachers know where I live holy shit
More like "validation and parsing of everything regular". Which is a surprising amount of things (e.g. URL routing).
iff i buy stuff from the shop on a family shared account will it stay when i buy the game?
Good &amp; well written text, but I'm not in favor of doing this in Rust. Its quite a bit of complexity to add an inherently unsafe feature that would exist to work around what seem to me like badly designed APIs. Obviously you can't avoid having to work with C APIs that expect unsafe things, but I wouldn't want to encourage it. Instead I'd prefer to push toward a stabilized offsetof feature so that you can abstractly (but statically) introspect the actual layout of the struct in order to perform the transmute. Equally unsafe, but much more *obviously* unsafe.
&gt; I wonder if you've been involved with any Rust web development libraries/frameworks Not involved, but I'm tracking rocket with interest. &gt; thoughts on how Rust compares to Python in that domain At the moment it's pretty bad. Iteration times on Rust are horrible because of lack of incremental compilation. Also the whole field just moved too much last year. Let's see if it is going somewhere now.
Should be `ssl::SslConnector::connect(...)`, probably?
No idea. Perhaps you'll have more success asking in /r/playrust?
thanks
Yeah, I think that might work. Nice idea!
I have rustfmt set in vim to run every time I save a `rs` file.
seri-rust developer?
Anybody who knows more about this stuff know what the current state and timeline for incremental compilation is?
Also, there was no Flask like framework for Rust with any momentum till recently. I've been following Rocket.rs on github and it seems like its got the necessary momentum to incentivize improving the development cycle for web in Rust (incentivizing creation and improvement of libraries, more literature, docs etc...).
libco bindings for Rust would be pretty easy, but *safe* bindings would probably be pretty difficult. Since co-routines have separate stacks and one can jump into the middle of another, which returns further up its own stack and not to the co-routine that called it... the lifetime annotations would probably get pretty scary. Maybe you could re-use the same Send/Sync marker traits that are used for kernel-level threads, but then I guess you'd need mutexes so state could be shared between co-routines and that seems like overkill‚Äîthe whole point of co-routines is that control-flow only switches at specific, pre-ordained points, so there's no possibility of lock contention.
I'm probably missing something, but I just can't find any decent way of parsing a simple CSV-like string into a structure. This is what I'd do in C: struct pair { int a, b; }; int parse_pair(const char* s, struct pair *p) { int n = sscanf("%d %d", &amp;p-&gt;a, &amp;p-&gt;b); if (n &lt; 2) return -1; return 0; } And this is what I've ended up with in Rust: struct Pair { a: i64, b: i64, } impl FromStr for Pair { type Err = (); fn from_str(s: &amp;str) -&gt; Result&lt;Pair, ()&gt; { let mut iter = s.split_whitespace(); let p = Pair { // I have no idea if the order of evaluation is defined for this. // Maybe I even have to split 'iter.next()'s to separate 'let's // to guarantee the correct order. a: match iter.next() { Some(s) =&gt; match s.parse() { Ok(a) =&gt; a, Err(_) =&gt; return Err(()), }, None =&gt; return Err(()), }, b: match iter.next() { Some(s) =&gt; match s.parse() { Ok(b) =&gt; b, Err(_) =&gt; return Err(()), }, None =&gt; return Err(()), }, }; Ok(p) } } &lt;rant&gt;I appreciate the proper error handling, but this is rediculous. Isn't there something as simple as sscanf() in the standard library? I mean, even OCaml has it.&lt;/rant&gt; Plese help me re-write it in a concise and idiomatic manner.
Instead of a meme week, can we have a rabbit week instead?
that's what happens when you stockpile meme magic for a year straight
Ah gotcha. So the API calls AsRef::&lt;Child&gt;(&amp;SomeParent) to get its info, and hands back &amp;SomeParent to us? Definitely safer, but I'm not sure how I feel about virtual calls down close to metal where I'm living and coming up with this. If it's a zero, or at least decently low, cost abstraction though then yeah that's a much better plan
And then unwrap() it all at once
Bunny memes.
Something like this will probably work, but it's best not to roll your own CSV parsing. There are crates for that. If you're just mocking stuff up yourself, use unwrap(), or expect() to skip past the error handling stuff. https://is.gd/r0PXm2 I think the next() calls in the struct will go in the right order, but I don't have a citation for that.
Type punning via union is just as undefined as it is via pointer casts. (But memcpy is indeed fine).
&gt; This is a shared object file. It might be called .so or dylib (for ‚Äòdynamic library‚Äô). I am not sure of the distinction if any between these suffixes, but the basic idea is that the file contains machine code designated by symbols that can be linked into other programs. If you mean the difference between .o and the others, I wasn't able to find a clear, concise answer. Both `.o` and `.so` are ELF-format binaries (as is your final executable) and I remember hearing somewhere that certain rules are more lax for `.o` files since they may be destined for static linking and `.so` files contain some extra metadata. As for `.so` and `.dylib`, if that's what you were referring to, `.dylib` a [Mach-O](https://en.wikipedia.org/wiki/Mach-O) shared library (OSX/iOS) while `.so` is an [ELF](https://en.wikipedia.org/wiki/Executable_and_Linkable_Format) shared library (every other modern, mainstream UNIX and Unix-like OS).
Never tried unions in C++ but yeah in C it's a free-for-all
Just what I was looking for! I spent the last like 3 years practicing C# at work and this was the best search result for "LINQ in Rust"
First pass: struct Pair { a: i64, b: i64, } impl Pair { fn from_str(s: &amp;str) -&gt; Result&lt;Pair, ()&gt; { let mut iter = s.split_whitespace(); let fst = if let Some(s) = iter.next() { s.parse().unwrap() } else { return Err(()) }; let snd = if let Some(s) = iter.next() { s.parse().unwrap() } else {return Err(()) }; Ok(Pair {a: fst, b: snd}) } } That's better, but still about as annoying as Reddit's code-block syntax, so... impl Pair { fn from_str(s: &amp;str) -&gt; Result&lt;Pair, ()&gt; { let lst: Vec&lt;_&gt; = s.split_whitespace() .map(|itm| itm.parse().unwrap()) .collect(); if lst.len() &gt; 1 { Ok(Pair {a: lst[0], b: lst[1]}) } else { Err(()) } } } Not really shorter, but definitely nicer. If you wanted to do better error handling, it turns out that you can actually use `.collect()` to turn a sequence of `Result`'s into a `Result` containing a sequence, like so: fn from_str(s: &amp;str) -&gt; Result&lt;Pair, ()&gt; { let lst = s.split_whitespace() .map(|itm| itm.parse()) .collect::&lt;Result&lt;Vec&lt;_&gt;,_&gt;&gt;() .map_err(|_| ())?; if lst.len() &gt; 1 { Ok(Pair {a: lst[0], b: lst[1]}) } else { Err(()) } } Still not as simple as scanf, I admit.
PyPy includes a tracing JIT, which compiles and optimizes particular execution paths at runtime. LLVM needs to consider all execution paths.
I wrote it. I know it takes 27 GB of RAM, that's exactly why I chose that server. I think cache misses make more sense now :)
They're not specifically for that use case, but it certainly can be done with them. Their [potential use cases and capabilities are pretty vast](https://fsharpforfunandprofit.com/posts/units-of-measure/). 
Even worse: _sequential_ cache misses. The loop can't be unrolled/vectorized/pipelined because each iteration depends on the previous one. That is, you can't fetch `lf_vec[i]` until you know what `i` is, and it came from the previous iteration's fetch. I'm also not familiar with the problem domain, and just from glancing at the code I can't think of any way around this. But I do see some promising hits on Google for `parallel fm-index`, so maybe there's a reasonable parallel algorithm.
&gt; The real issue, and what I'm still struggling with, is how to do composite units with zero-cost abstractions. I'm really interested to see if you can come up with an elegant solution for this. For dimensioned, I gave it some thought, and unit systems was the only solution that I could come up with that allows arbitrary unit composition (e.g. meter^6 / second^12, or anything else you may want without defining it ahead of time). For what it's worth, I have tried to make it easy to make unit systems in dimensioned, so if you need to operate at e.g. megaparsecs and nm, you could create a unit for each in the same system. Then multiply or divide by something like `megaparsecs_per_nm` if you need to convert between them. Alternatively, you could use two different unit systems, but that would not allow you to use the different units in the same expression.
Someone asked a parsing question a couple weeks ago and [this](https://is.gd/T9Gchv) is what I came up with for them ([context](https://www.reddit.com/r/rust/comments/5weild/hey_rustaceans_got_an_easy_question_ask_here_92017/dedrm35/?context=3)). The gist is that Rust tends not to conflate multiple errors like C and C++, so either create some helper functions or use a crate for the common bits, as others suggested. But also, the implementation is only half the story. You have to consider usage as well. In C you'll likely have something like this at every usage point struct pair p; const char * s = get_string(); if (parse_pair(s, &amp;p) &lt; 0) { /* free string maybe? */ return -1; } In Rust you'd have something like let p: Pair = get_string().parse()?; So even if the implementation is more verbose I think it's worth the automatic resource management, unambiguous error handling, and simpler usage; especially since you'll probably have more usages than you do implementations.
Honest question, when is Servo expected to be mature enough to put in real products? We've been getting these screenshots for two years.
I use servo's rust-url a lot in my project .I am sure other components like webrender and pathfinder can be used as libs.
This might be a little off topic, but I think it is because of my lack of understanding of conditional compilation and features. I'm trying to use the crate [conrod](https://crates.io/crates/conrod) to implement a basic GUI for an application. Only problem, it seems I'm not able to tell the compiler to use the `conrod::backend::glium` and `conrod::backend::winit`, which should be conditionally compiled by using the `--features="glium winit"` flags with cargo. A basic example would be this: #[macro_use] extern crate conrod; use conrod::backend::glium; use conrod::backend::winit; fn main() { println!("Hello world!"); } In the Cargo.toml I added: [dependencies] conrod = "*" glium = { version = "0.16.0", optional = true } winit = { version = "0.5.9", optional = true } And finally compile with: cargo run --release --features="glium winit" which results in 3 | use conrod::backend::glium; | ^^^^^^^^^^^^^^^^^^^^^^ no `glium` in `backend` and the same for winit. What am I doing wrong? How do I get to compile this? 
Sounds cool! One issue I can see immediately is that building the DFA up front is an immediate non-starter. It has to be built lazily to avoid exponential memory growth due to state blow up.
[Dimensioned does!](http://paholg.com/dimensioned/dimensioned/dimensions/index.html)
That seems to be a lot of extra complication of application logic. Maybe using a separate thread (or a runtime provided part of an async main event loop) for this is the easiest work around of too-fast-wrapping-PIDs. A thread-safe hashtable + waitpid + WNOHANG in a loop. When the runtime starts a process it registers it in the table, the loop waits on it, and when it dies it saves the exit status in the table. The actual PID is not even important for consumers of this service, because the key in the table can be anything (likely the memory address of a struct), and entries should get removed as consumers release the references to the delegated child.
There's also [libfringe](https://github.com/edef1c/libfringe)
Thanks, I'll take this in stride :)
I've provided a link to the developer docs in the description which explain what they do
iirc it hit alpha quite recently.
I've only used GCC honestly. I've never had reason to do primitive-level type punning; I included the Quake FISR purely as a common example. I've seen and used struct punning, including the void* cast to prevent implicit offset multiplication, plenty of times at work and am more familiar with that. Since the meat of my post was about the expansion of a pointer from an arbitrary child to it's parent item, I didn't really dive in to making sure my example C was fully compliant with specs and compilers, especially because it's supposed to illustrate the dangerous and stupid things that kinda work in C and really shouldn't in modern languages. Rust has a way to accomplish the float/int transmutation directly with `std::mem::transmute` and that is sufficient for punning types without altering physical properties of target or length. Rust does *not* yet have a way to do what I am describing in the main body; the RFC about fields marked in the vtable is pretty solid and mostly overlaps with my ideas here. I still like the idea of a compiler-assisted `offsetof`, and dislike the idea of using the C method of ptr - offset in favor of a descriptive pattern that can be type checked, but that's just me.
I"ve been having fun with nom, and I'm impressed with the composability of nom parsers. However, I can't find a crate (or even a repo) that maintains a useful list of named! parsers so no wheel-reinventing has to take place, like floating-point numbers, single &amp; double quoted strings, etc. 
And down we slide, the slippery slope towards bugs and problems.
Maybe I should be more precise... In the [examples](https://github.com/PistonDevelopers/conrod/tree/master/examples) there is an example called [image.rs](https://github.com/PistonDevelopers/conrod/blob/master/examples/image.rs) where they explicitly use use conrod::backend::glium::glium; use conrod::backend::glium::glium::{DisplayBuild, Surface}; and later // A type used for converting `conrod::render::Primitives` into `Command`s that can be used // for drawing to the glium `Surface`. let mut renderer = conrod::backend::glium::Renderer::new(&amp;display).unwrap(); To start with conrod I wanted to use the example and modify the code for my needs, but I can't manage to compile it as I get the error mentioned before. If I compile conrod examples directly in the conrod crate with cargo run --release --features="glium winit" --example image it compiles without any error.
Interesting. How does lazy building avoid exponential memory growth? Couldn't you build an exponential number of states lazily? Are you relying on the assumption that this probably won't happen in practice?
Ord exists for a reason. If you want that as a bound, use it as a bound!
A lazy DFA builds at most one state per byte of input and it can always build a state even if it doesn't exist. Because of this, you can keep a bounded cache of states that can be flushed when it gets too big. The worst case is that a state really is created for every byte of input, which causes the cache of states to thrash. But memory growth and search time are still bounded. Afaik, this how all production grade general purpose FSM regex engines work.
On Linux there's signalfd SIGCHLD, so you know when to wake up. Also, if you don't directly expose the "child process state table", then the work of checking the wait queue can be done when someone queries the state of child processes. (Of course that means there must be a small piece of logic that makes sure not to check the wait queue on every call, only when there was a new child registered.) I found this discussion about the gory details of userspace races relating to subprocesses, it might be of interest to you: https://patchwork.kernel.org/patch/1716/
`cargo check` is in stable? Yes. Also now I just have to wait 6 more weeks for `pub(restricted)` to land on stable!
The patches [landed in LLVM](http://reviews.llvm.org/rL278015) in August! Specifically it was patches to add support for certain PIC strategies the were introduced in GCC a few years back. The barrier now is waiting for those changes to bubble up to the version of LLVM that Rust uses, which I believe means also waiting for Emscripten (since Rust uses their LLVM version as a baseline in practice). Since getting Rust apps working hasn't been a high priority in practice, we haven't found the cycles to push this ourselves, and kind of figure it will just get there eventually...
Thanks for the quick reply! And no worries, I love the Rust update posts. I don't think it would hurt to mention as an aside that the solution in this case is impossible as is (or that Name should be refactored to own its own copy of &amp;str), so new devs who are still struggling with the lifetime concept don't feel like there's a solution just out of sight?
Any ETA for SVG support? Saw that it wasn't in your last roadmap. I'm really interested myself to see how fast Servo could render https://www.flightradar24.com/ on busy locations, as that tends to cripple both Chrome and Firefox. But, as it stands, that site requires (at least) SVG support.
can you provide an example of such a calculation that involves both scales at once? legitimately curious. I believe it's entirely possible, but whatever it is has to be interesting.
&gt; (Also, I noticed an inconsistency/mistake: diesel's initial build goes from 15s to 13s for a speedup of 0.015, but the secondary build goes from 13.5 to 12 for a speedup of 1.1.) woo i cannot do basic math apparently
thanks. That was my question! üòä
I just realized that 1.17 should mark Rust 1.x 2nd birthday... will the release on Thursday work, or will some shift need be applied to make it true?
The [docs of parse](https://doc.rust-lang.org/std/primitive.str.html#method.parse) say: &gt; parse() can parse any type that implements the FromStr trait. So, implement it and you're good.
This is the first time I've heard about pub(restricted). I have a bunch of things that are `pub` which I don't really want to be, because I have whitebox unit tests.
Seems so
Seems weird to make that `&amp;str`-slicing is byte-oriented, instead of character-oriented.
How many bytes are in a length 4 `&amp;str`? Byte oriented means the answer is 4, character-oriented would mean who knows or always some huge number.
No ETA, but it will likely be supported by WebRender in the next few months, which would then make it much more realistic for a non-core team member to add support for it in Servo proper.
This is an incredibly simple thing to do that's easy to forget. That's why I have a template :)
Well, not one to leave myself hanging. I haven't solved the first part, reading the body of a request to the server into a string. However, I solved the client timeout problem by selection over two futures: // Retrieve a device home page. fn get_device_home(location: &amp;str) -&gt; Result&lt;String, error::Error&gt; { let url = hyper::Url::parse(location)?; let mut core = tokio_core::reactor::Core::new().unwrap(); let handle = core.handle(); let client = hyper::Client::new(&amp;handle); // Future getting home page let work = client.get(url) .and_then(|res| { res.body().fold(Vec::new(), |mut v, chunk| { v.extend(&amp;chunk[..]); future::ok::&lt;_, hyper::Error&gt;(v) }).and_then(|chunks| { let s = String::from_utf8(chunks).unwrap(); future::ok::&lt;_, hyper::Error&gt;(s) }) }); // Timeout future let timer = Timer::default(); let timeout = timer.sleep(Duration::from_secs(5)) .then(|_| future::err::&lt;_, hyper::Error&gt;(hyper::Error::Timeout)); let winner = timeout.select(work).map(|(win, _)| win); match core.run(winner) { Ok(string) =&gt; Ok(string), Err(e) =&gt; Err(error::Error::from(e.0)), } } 
Not trying to be difficult. :) I have lots of Lattice-based partial orders, so end up with a bunch of things implementing `PartialOrd` and `Ord` for which the `PartialOrd` impl isn't total. I lose sleep over whether that is ok. :)
I would very much like a PR for this, if you have time :)
/r/playrust
(as evidenced by that thread, I don't actually mind being pinged like this, just amused that it happens :) )
Well, it would always be less than or equal to 4, regardless of whether "character" means "grapheme cluster" or "codepoint", unless you're talking about NFDd code points, in which case there is a bounded (by I think `4n/3` and provided future unicode changes, `13*n / 3`) but often larger size. Edit: misinterpreted comment
No, didn't change anything. Same errors
Gotcha! I know there is a current push to get LLVM 4 support in the compiler right now, so this maybe a lot closer than expected.
It seems a couple of interested people might already be working on the GH issue ‚Äî I have already explained the steps I took there just in case, but could also help if anyone needs it, sure
Figured out I installed rustc to /usr/local/bin at some point, so that was overriding rustup's default in `~/.cargo/bin` deleting those binaries fixed it.
I think you've flipped it: it sounds to me like the hypothetical in the parent is "what if the length isn't measuring bytes", so a string of length 4 could mean 4 codepoints (i.e. the storage is anywhere from 4 to 16 bytes) or 4 graphemes (4 to &amp;infin; bytes&amp;mdash;you can always tack more combining characters on the end). And I think normalisation is at most an 18&amp;times; length difference, never an "asymptotic" change (i.e. there's no upper bound of the number of code points in a single grapheme, even after normalizing).
thanks! looks like we have a PR https://github.com/rust-lang/blog.rust-lang.org/pull/157
In C++ you can call new/delete for the system allocator, but that's it, the language does not provide any facility that can be used to inspect its data-structures. Same for Rust which just gives you `Box`. If you happen to know that the system allocator is malloc or jemalloc, you can in both Rust and C++, use C FFI bindings to access the allocator C-API, which depending on the allocator might or might not provide a way to poke its data-structures. Even if you were able to do this, you would loose type-safety in the process, since just because you know that there is some memory stored somewhere you don't know the type of the objects that live on that memory. Loosing type-safety is "a bit" worse that leaking memory though...
This messed with me today. I was looking at the std API docs today for a way to split a string at an offset. I found [split_off](https://doc.rust-lang.org/std/string/struct.String.html#method.split_off) and got a compiler error when trying to use it. My reaction was basically: 1. Looks at the docs 2. Looks at the error 3. Looks at the docs 4. Wait...what? At that point I ran `rustc --version` and realized there was a new stable release. As always, thanks to everyone who helped!
I bet a lot of these wheels exist in [syn](https://crates.io/crates/syn).
You don't need to put the glium and winit dependencies in your crate (unless you are going to `extern crate` them yourself as well). You need to pass those feature flags along to conrod. Try this: # Cargo.toml [features] glium = ["conrod/glium"] winit = ["conrod/winit"] [dependencies] conrod = "0.51" # wildcard dependencies are bad Actually since your `use` statements are not conditional, maybe you don't need to put feature flags on your crate at all, and you can just do this: # Cargo.toml [dependencies] conrod = { version = "0.51", features = ["glium", "winit"] } 
That's fine, but it seems like the way they went is worst-of-all-worlds. If indexing into a `&amp;str` can't reliably be done on "characters", why is it erroring slicing into the middle of a code point? Why doesn't it just return the byte at that offset? Instead it's trying to do both: slice bytewise, but error if your byte happens to be in the middle of a code point. If code points "don't matter" (which I agree with), this should not be a problematic operation. Pick one, yeah?
Uh, acting as a library yet being responsible for subprocesses, that'd be a pretty hard place. But signal handling is not black magic. I mean, yeah, it's fucked up that you have to manually mask, and unmask on fork, and that coalescing happens, but ... it's just a checklist of things to look out for. The thing that drives me mad is that there isn't even a project on the horizon to address this gimpness of the Kernel. Searching for signal coalescing returns only the dead kevent mails. ([CLONE_FD](https://lwn.net/Articles/638613/) was aiming to solve the SIGCHLD issue at least, but that [did not get merged](https://github.com/torvalds/linux/search?q=%22CLONE_FD%22&amp;type=Code&amp;utf8=%E2%9C%93) as far as I can see.)
/r/playrust
/r/playrust
Except for the last year or so, but we just weren't going to stabilize `-Z no-trans`. Also, it was more painful for Cargo to track those artifacts (since they had the same file paths, i.e. `.rlib` with no object files inside) than the current scheme exposed through `--emit`.
How are they distributing the ripgrep binary?
In which case, why doesn't indexing work on *code points*? Like I said, worst of both worlds. If it won't let you divide between code points anyway, what's the point of pretending to slice by bytes and failing? It's clearly already doing the work needed to do codepoint boundary detection regardless. 
Yep, I flipped it. Oops.
https://www.npmjs.com/package/vscode-ripgrep and https://github.com/roblourens/ripgrep/blob/vscode/VSCODE_BUILD.md TL;DR - Looks like they build their own binaries and distribute them using Github releases: https://github.com/roblourens/ripgrep/releases
yeah no i misinterpreted your statement and flipped it -- "how many characters are in a 4 byte string" :)
&gt; Why doesn't it just return the byte at that offset? That's when you do `s.as_bytes()[index]` &gt; Why doesn't it just return the byte at that offset? That will not be a valid utf8 `str`/`char`. That's not how code points work. &gt; Instead it's trying to do both: slice bytewise, but error if your byte happens to be in the middle of a code point. If code points "don't matter" (which I agree with), this should not be a problematic operation. The use case for this is a pretty specific one -- you're iterating through the string and want to cache locations of points of interest in a local variable for indexing later. Slices handle a lot of this for you, but sometimes you want to be able to get a finger to the code point from which you can peek both ways. Almost all string processing will involve iteration. If it doesn't, there's a very high chance you're going to choke on international text (if your application deals with only ascii, don't use `str`, there's an `Ascii` type for this). So indexing is not very useful. But the one time you do need it will be when you've iterated and noted down some points of interest which you want fast access to. This can be done via byte or code point indexes, but byte indices are faster. &gt; [from child comment] In which case, why doesn't indexing work on code points? Like I said, worst of both worlds. That's `O(n)`. You can always do it explicitly via `s.chars().nth(..)`. This has multiple benefits. Firstly, it forces you to explicitly acknowledge the cost. Secondly, the explicitness of the iteration makes it easy to roll together any related iterations here. In _most_ cases you're iterating through a string anyway -- the use cases for directly indexing are rare as I already mentioned, so you can collapse these into your regular iteration. ----------------------- Rust's solution is far from the worst of both worlds, it's a solution I find to be one of the best given the constraints. It forces you to think about what you're doing -- in most other languages, you just end up randomly splicing code points or grapheme clusters and a lot of text/emoji breaks badly. A possibly better solution would be what Swift does with dealing with grapheme clusters as the default segmentation unit (and abstracting away the storage), which may not work so well for rust since you want clearer abstractions with explicit costs. This is debatable, but ultimately we can't change this now.
Whoa that was fast.
Because the programmer has to explicitly state his intention, otherwise there'd be ambiguity. This is from the docs: Indexing is intended to be a constant-time operation, but UTF-8 encoding does not allow us to do this. Furthermore, it's not clear what sort of thing the index should return: a byte, a codepoint, or a grapheme cluster. The bytes() and chars() methods return iterators over the first two, respectively. Edit: I've now realized again that checking the first two bit of the indexed byte(s) is enough to trigger the error condition. I agree that having to use `(into_)bytes()` to opt out of O(1) boundary checking and `chars()` to opt in to O(n) codepoint indexing is weird, but see the point in `[]` by default giving preference to neither, given that in the first case you'd be better served with a `Vec&lt;u8&gt;` to begin with and the second would cause unexpected hidden runtime cost. At least that's how I understand it right now. 
It seems like it would be easy to write one, assuming you just wrap float in a newtype. Checking for NaN on every operation would probably be expensive though.
You always know how to make me blush. :-)
I ran `rustup update` and when I check versions I have rustc at 1.16 and cargo at 1.17-nightly. I don't have nightly toolchain installed (only stable) and I have never installed a nightly version. I'm not sure how I got in this state (I don't believe the versions were consistent before upgrading). When I check `rustup toolchain list` it shows only stable. When I run `rustup which cargo` it shows `~/.rustup/bin/...` but when I run `which cargo` it shows `~/.cargo/bin/cargo`. If I remove `~/.cargo/bin` from my path then neither cargo nor rustup work. Any idea what I messed up?
Oh I didn't realize! Thanks!
Hmm, I have a [crate](https://github.com/Hyperchaotic/weectrl) with a Conrod UI example. In Cargo.toml I have: [dev-dependencies.conrod] git = "https://github.com/PistonDevelopers/conrod.git" features = ["glium", "winit"] Remove the "dev-" part if not example. In the app.rs file: #[macro_use] extern crate conrod; use conrod::backend::glium::glium; use conrod::backend::glium::glium::{DisplayBuild, Surface}; I compile with a simple 'cargo build --example weeapp' 
Ah, first thought was to check crates.io 
It makes more sense if you realize that &amp;str is literally just &amp;[u8] with the additional restriction of being valid utf8.
Yes, it wrap rustc to build rust program and supports android target.
FWIW, style guides generally don't recommend having lines everywhere: it ends up being unnecessarily busy. (Also, the speed-up is still wrong, and the number of significant figures is still unnecessarily large.)
Huh, maybe I'm too used to zebra stripes. So much CSS work to do on various Rust web properties... I'm taking care of the speedup stuff tomorrow.
LOL, that is incorrect, it is 2017!!, not 2017!.
[Here](https://crates.io/crates/hc) is a similar bitwise calculator I found a few days ago searching for tiny binaries on crates.io. Both could use some love :)
This is so awesome! Does anyone know how the VSCode release train works? I.e. what kind of process or timeframe does this go through to make it into stable?
Exciting and there is currently a 30% coupon PREORDER. Is the checkout page really not https or did I get a bad link? (Edit) please update link to be https. Checkout did not auto redirect. Thanks
It looks super cool! :D
I'm so proud of our community, Rust and friends will go far.
From https://code.visualstudio.com/updates/v1_10 looks like releases are monthly? Not sure other than that.
It is; there are like 5 crates that will do this for you.
You have to add the racer binary to your $PATH environment variable. cargo doesn't do that by default. Otherwise - post the error message your getting. In my experience, Atom was too slow for efficient development, especially building would take like 20 seconds. There is Visual Studio Code, which has been faster in my experience but setting up Rust is a bit of a pain. Sublime Text the fastest, but commercial.
Maybe rustc should check if it's been exactly 6 weeks since its release and let you know about features that were stabilized for the beta when it was released ;)
Seems like `cargo check` and `cargo watch` would a great pairing
Somewhat confusingly, it's actually smaller (approximately the square root): https://en.wikipedia.org/wiki/Double_factorial
An algorithm by Dijkstra for handling arithmetic. Works best with Polish or reverse Polish notation, but can run with infix as well. Wikipedia has more information
Yeah. I even have a `// lol` where I do the whole integer macro thing because, well, lolarrays. :P
can confirm. monthly iterations followed by a ship. 
**Go to https://www.nostarch.com/Rust, not the http link in the OP.** Multiple people have run into a bug on the http version of nostarch where the shopping cart still shows up empty after trying to add the book to the cart. I successfully ordered through the https site. Very exiting! =)
stickied above
The work to detect codepoint boundaries doesn't run in O(n) time; it can be done by checking the leading bits of some octets at the beginning and end of the slice. Indexing by codepoints requires knowing not just whether something is a codepoint but where a codepoint exists relative to other codepoints; requiring O(n) time. Indexing not being O(1), imo, is tricky. It's something people reasonably expect to be O(1). Slicing a str in rust is not something I see often; it seems like a side effect of it being &amp;[T], not something necessarily correct to use except in contexts where you somehow know the codepoint boundaries already.
In terms of learning after reading the Book: are there good screencasts or writeups of people writing Rust programs idiomatically? Preferably using some features like Arc, macros, etc.
Ah, I see, my initial error indeed was that I didn't tell cargo explicitly to pass the feature flags along to conrod!! Then I saw the optional dependencies in the conrod Cargo.toml and got stuck there. I do understand now, thanks! Could you explain me why wildcard dependencies are bad? Sure, they might break stuff... But I get the newest version, with possible bugfixes... And if I have to adapt my code anyway sooner or later, I don't see why not doing it right away. Ok, probably one might stick to fixed releases in production code.
Is this a totally new book or the published version of the new The Rust Programming Language book?
I tried a dozen combinations of things entered into $PATH and the two directories the Racer plugin requests (in Settings). I even added the directory containing racer.exe to $PATH. But then I added the racer.exe binary itself to $PATH as you suggested, and also pointed the plugin at ".../lib/rustlib/src/rust/src" instead of ".../lib/rustlib/src" as someone else suggested. And that seems to have fixed the problem. At least, I no longer get error messages when I type. I'm not sure how to verify that Racer is doing what it's supposed to, but generating a flood of error messages clearly wasn't its correct function. Since I'm only an amateur programmer, high performance is not particularly important. Thanks for your help.
Getting "Not found!" when trying to look at https://nest.pijul.com/pijul :(
That's the best thing about it :) /r/unexpectedfactorial/
Congratulations to everyone, yet another step for increasing Rust's adoption.
When you make an account, it works. Just the welcoming page seems to be missing.
https://nest.pijul.com/pijul_org/pijul
Ok, I'm installing window now!
[removed]
Is there a specific design reason that `evaluate` returns an `Option&lt;i32&gt;` rather than a `Result&lt;i32, String&gt;`? It might be useful to have information about why it failed availible to the consumer.
Can't recommend this combo enough. Makes checking code a breeze
The latter.
Say I have this function that consumes a string and returns a new one: fn add_thing(s: String) -&gt; String { s + "thing" } Then I have this vector: let mut v = vec!["one".to_string(), "two".into(), "three".into(), "four".into()]; Is it possible to map add_thing over v in-place so it doesn't have to re-allocate v? It should be possible because add_thing takes a string and also returns a string, so the input and output types are the same size. This obviously doesn't work because elem is borrowed by the assignment: for elem in &amp;mut v { *elem = add_thing(*elem); } Same with this: v.iter_mut().map(|elem| add_thing(*elem)); It seems like there would need to be a custom method on Vec for this to work. EDIT: I'm not sure if it would be safe or feasible, but maybe I should also be able to wrap add_thing in a function like this, which would solve the problem above: fn add_thing_mut(s: &amp;mut String) { *s = add_thing(*s); } **EDIT 2:** I devised a function that does what I want, but it's pretty gross and I have no idea whether it is "safe" or not: fn apply_pipe_to_mut_ref&lt;T, FT&gt;(func: FT, val_ref: &amp;mut T) where FT: FnOnce(T) -&gt; T { let valcopy: T = unsafe { mem::transmute_copy(val_ref) }; let mut valnew = func(valcopy); mem::swap(&amp;mut valnew, val_ref); mem::forget(valnew); } Then you just apply it via `apply_pipe_to_mut_ref(add_thing, elem);`.
Maybe a minimal example on https://play.rust-lang.org/ would help us. You could also try adding `'gloop` to `&amp;mut self`.
Whoops sorry about that! NoStarch just redid their site, they warned that it might be a little buggy.
You can add a `parse` method to your struct like this: impl Color { fn parse(&amp;self) -&gt; u32 { ((self.red as u32) &lt;&lt; 16) | ((self.green as u32) &lt;&lt; 8) | self.blue as u32 } } Though you could name this method anything, since it's not related to `str::parse` aside from being named the same.
Why not just have add_thing take a mutable reference to the string?
This works, but I'm not entirely sure if it's what you're going for. for elem in &amp;mut v { let ans = add_thing(elem.clone()); *elem = ans; } My (not very skilled) understanding of the problem you're running into is that you can't move an individual String out of the vec (because that doesn't make much sense) and the borrow checker isn't smart enough to see that doing a map puts another string back into the space that you moved the original out of before anything else can "see" the empty space. 
Just a small VsCode tip, if you start typing in the right pane, you actually get completion options for the settings. So in this case, just put your cursor in the right pane and type something like `"ripgrep` and you should see an autocompletion for `"search.useRipgrep"`.
 cargo new awesome_stuff --vcs=pijul
Your `eval(tokens: Vec&lt;RPNToken&gt;)` is extremely repetitive, it feels like there ought to be a better way to write it.
You can accomplish what you're trying to do with mem::replace. It's not elegant but it does avoid reallocating the vec. https://is.gd/qsjOnp
I almost want to order it just for the adorable cover art!
Very happy to see that the license is changed to stock GPLv2. 
It seems we came up with different API surfaces, though. Mine just assumes external state, whereas /u/Manishearth's version gives you the index as a function argument. Also `array-init` uses `mem::uninitialized` + `mem::replace`/`mem::forget` whereas I literally expand `[init(), init(), init(), ...]` in a macro. (I did something like this, with `ptr::write` instead, in an earlier version of `init_with`, but switched to the macro version to remove the unsafe code and the chance to leak items.)
Update: I think I managed to solve the issue with the lifetime indicators. However, now I have a borrow checker fight in progress, specifically in this piece of code: while !root.window_closed() &amp;&amp; !quit{ game_loop.poll(); { quit = handle_input(&amp;mut root, &amp;mut game_loop, &amp;mut player, &amp;map); } map.render(&amp;mut world_console, &amp;player); player.render(&amp;mut world_console); blit(&amp;mut world_console, (0, 0), (SCREEN_WIDTH, SCREEN_HEIGHT), &amp;mut root, (0, 0), 1.0, 1.0); root.flush(); } Rust complains that `player` and `map` cannot be borrowed mutably in `handle_input`, as they are already borrowed immutably in the `render` calls. However, those functions return before the `handle_input` culprit is called, so I must be missing something very important here. Compiler output: error[E0499]: cannot borrow `player` as mutable more than once at a time --&gt; src/main.rs:46:65 | 46 | quit = handle_input(&amp;mut root, &amp;mut game_loop, &amp;mut player, &amp;map); | ^^^^^^ | | | second mutable borrow occurs here | first mutable borrow occurs here ... 55 | } | - first borrow ends here error[E0502]: cannot borrow `map` as mutable because it is also borrowed as immutable --&gt; src/main.rs:49:9 | 46 | quit = handle_input(&amp;mut root, &amp;mut game_loop, &amp;mut player, &amp;map); | --- immutable borrow occurs here ... 49 | map.render(&amp;mut world_console, &amp;player); | ^^^ mutable borrow occurs here ... 55 | } | - immutable borrow ends here error[E0502]: cannot borrow `player` as immutable because it is also borrowed as mutable --&gt; src/main.rs:49:41 | 46 | quit = handle_input(&amp;mut root, &amp;mut game_loop, &amp;mut player, &amp;map); | ------ mutable borrow occurs here ... 49 | map.render(&amp;mut world_console, &amp;player); | ^^^^^^ immutable borrow occurs here ... 55 | } | - mutable borrow ends here error[E0502]: cannot borrow `player` as immutable because it is also borrowed as mutable --&gt; src/main.rs:50:9 | 46 | quit = handle_input(&amp;mut root, &amp;mut game_loop, &amp;mut player, &amp;map); | ------ mutable borrow occurs here ... 50 | player.render(&amp;mut world_console); | ^^^^^^ immutable borrow occurs here ... 55 | } | - mutable borrow ends here error: aborting due to 4 previous errors
[This is a version](http://play.integer32.com/?gist=554bbe1a30a6ba9901ef23d07fdd21df&amp;version=stable) without any clones or mem::replace and which doesn't do anything of questionable/non-obvious safety. Probably marginally slower than the other version burkadurka posted, though. (How good's the optimiser?)
Not really, but it's probably a good idea to change it to Result.
Now, I'm just missinig the `awesome_stuff`.
&gt; the major purpose of the tokio-io crate is to provide these core utilities without the implication of a runtime. With tokio-io crates can depend on asynchronous I/O semantics without tying themselves to a particular runtime, for example tokio-core. The tokio-io crate is intended to be similar to the std::io standard library module in terms of serving a common abstraction for the asynchronous ecosystem. The concepts and traits set forth in tokio-io are the foundation for all I/O done in the Tokio stack. Nice!
AGPL
In order to help better we'd need the method signatures of: - `handle_input` - `map.render` - `player.render` Because this looks (mostly) sane, but the duration of the borrows depends on the return lifetimes.
Email went to my spam inbox, make sure to check if you're not getting it!
It doesn't look like it, so maybe you'll just want to use that /reimplement it yourself? 
Implementing Rust with far too many linked lists deserves a mention, if only because it steps through the thought process to an idiomatic solution after starting with a naive one
That's very nice, it does look like adding new IO to tokio would be easier. I would like to rewrite my `zmq` services using tokio but I still don't understand how to add support for `Select` - it seems that tokio is based on polling, but zmq is based on select and notifications. I am quite sure I am overlooking something simple that would let me rewrite my zmq select in the tokio `Polling&lt;u8,Error&gt;` trait.
This sounds like [SPF](http://www.openspf.org/) and [DMARC](https://dmarc.org/) should be set up for the website. (Check with your domain registrar.)
This is getting solved. The whole stack is relatively new: pijul itself, but also my smtp, postgresql, http (+https via rustls) and ssh (via thrussh) crates.
It'd be awesome to get zmq integration! If you need any help feel free to drop by [gitter](https://gitter.im/tokio-rs/tokio) and we'll try to help out. If you're working with a completion based system (e.g. a callback runs when an asynchronous operation completes) rather than a poll pased system (e.g. you check to see whether an operation can complete) then no need to worry, that model is still adapt-able to the `Future` trait. I'd recommend closely reading [`Future::poll` documentation](https://docs.rs/futures/0.1.11/futures/future/trait.Future.html#tymethod.poll) to get started. Basically the only requirement is that `Future::poll` finishes "quickly" and if it returns `NotReady` then a notification (via `task.unpark()`) is scheduled to happen in the future. For a notification based system (or completion based) then this typically means that when the work is created (such as when the future is created) then a completion is configured. This completion is then scheduled to attempt to unpark the the associated future (if the future is blocked on). This is really just a fancy way of saying, though, that you should probably use [`futures::sync::oneshot`](https://docs.rs/futures/0.1.11/futures/sync/oneshot/index.html) internally. When a future is created it has a `oneshot` behind it and then `Future for MyType` just calls `Sender::poll`. The only tricky part about that is handling cancellation. With futures we typically interpret `drop` as "cancel this future", so you'll need to implement `Drop for MyType` and appropriately cancel the corresponding operation (if possible). Let me know though if any of that's confusing!
Same here!
Just to add, if using `oneshot`, implementing cancellation can be done by having the producing half (the part that holds the sender) also watch for cancellation on the oneshot using: https://docs.rs/futures/0.1.10/futures/sync/oneshot/struct.Sender.html#method.poll_cancel
So it matters, isn't it ? If git was AGPL, does github interface had to be freed ?
Neat project. I enjoyed reading through it. I have a few remarks: https://github.com/squiidz/yard/blob/master/src/eval.rs#L3 Usually, unless you are going to be mutating the `Vec&lt;_&gt;`, it's idiomatic to take a `&amp;[RPNToken]`. It probably doesn't matter much here, however. https://github.com/squiidz/yard/blob/master/src/eval.rs#L8 The `unable to pop` message is probably due to an unbalanced binary operation? It might be better to express it as such, like an "unbalanced addition", etc https://github.com/squiidz/yard/blob/master/src/parse.rs#L4 There actually isn't any need to `.collect()` into a new `Vec` if you are only going to be iterating through it. https://github.com/squiidz/yard/blob/master/src/parse.rs#L18 I understand why you used `clone` here, but it's not necessary. Another option is to use `cloned()`, which is clone each element as you iterate through. When you call `last()`, it then will only clone the last item. `clone` here will eagerly clone each element before you call last. Alternatively, I would probably recommend giving your RPNToken* types the `Copy` trait. Then you can rewrite what you have to something like this: ``` let qe = match queue.last() { Some(&amp;v) =&gt; v.value, None =&gt; { queue.push(rpnt); continue } }; ``` https://github.com/squiidz/yard/blob/master/src/parse.rs#L33 I'm not sure if similar reasoning holds as before, since there is some mutation going on while iterating through the queue. I guess I'd need to study the algorithm a bit more to better understand how this works.
This creates a similar problem to the one I was first having (in the OP): error[E0478]: lifetime bound not satisfied --&gt; src/main.rs:66:29 | 66 | game_loop.push((actor, Event::Move(Direction::N, map))); | ^^^^^ | note: lifetime parameter instantiated with the anonymous lifetime #3 defined on the body at 57:133 --&gt; src/main.rs:57:134 | 57 | fn handle_input&lt;'root, 'gloop&gt;(root: &amp;mut Root, game_loop: &amp;mut EventQueue&lt;'root, 'gloop&gt;, actor: &amp;mut EventActor, map: &amp;Map) -&gt; bool{ | ^ note: but lifetime parameter must outlive the lifetime 'gloop as defined on the body at 57:133 --&gt; src/main.rs:57:134 | 57 | fn handle_input&lt;'root, 'gloop&gt;(root: &amp;mut Root, game_loop: &amp;mut EventQueue&lt;'root, 'gloop&gt;, actor: &amp;mut EventActor, map: &amp;Map) -&gt; bool{ | ^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:66:29 | 66 | game_loop.push((actor, Event::Move(Direction::N, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:66:36 | 66 | game_loop.push((actor, Event::Move(Direction::N, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:70:29 | 70 | game_loop.push((actor, Event::Move(Direction::S, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:70:36 | 70 | game_loop.push((actor, Event::Move(Direction::S, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:74:29 | 74 | game_loop.push((actor, Event::Move(Direction::E, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:74:36 | 74 | game_loop.push((actor, Event::Move(Direction::E, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:78:29 | 78 | game_loop.push((actor, Event::Move(Direction::W, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:78:36 | 78 | game_loop.push((actor, Event::Move(Direction::W, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:82:29 | 82 | game_loop.push((actor, Event::Move(Direction::NW, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:82:36 | 82 | game_loop.push((actor, Event::Move(Direction::NW, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:86:29 | 86 | game_loop.push((actor, Event::Move(Direction::NE, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:86:36 | 86 | game_loop.push((actor, Event::Move(Direction::NE, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:90:29 | 90 | game_loop.push((actor, Event::Move(Direction::SE, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:90:36 | 90 | game_loop.push((actor, Event::Move(Direction::SE, map))); | ^^^^^^^^^^^ error[E0495]: cannot infer an appropriate lifetime for automatic coercion due to conflicting requirements --&gt; src/main.rs:94:29 | 94 | game_loop.push((actor, Event::Move(Direction::SW, map))); | ^^^^^ error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'root` due to conflicting requirements --&gt; src/main.rs:94:36 | 94 | game_loop.push((actor, Event::Move(Direction::SW, map))); | ^^^^^^^^^^^ | help: consider using an explicit lifetime parameter as shown: fn handle_input&lt;'root, 'gloop&gt;(root: &amp;mut Root, game_loop: &amp;mut EventQueue&lt;'root, 'gloop&gt;, actor: &amp;'gloop mut EventActor, map: &amp;'root Map) -&gt; bool --&gt; src/main.rs:57:1 | 57 | fn handle_input&lt;'root, 'gloop&gt;(root: &amp;mut Root, game_loop: &amp;mut EventQueue&lt;'root, 'gloop&gt;, actor: &amp;mut EventActor, map: &amp;Map) -&gt; bool{ | ^ error: aborting due to 17 previous errors Modified method signature: fn handle_input&lt;'root, 'gloop&gt;(root: &amp;mut Root, game_loop: &amp;mut EventQueue&lt;'root, 'gloop&gt;, actor: &amp;mut EventActor, map: &amp;Map) -&gt; bool
[removed]
yep. I also got same error on windows 10 :(
Hmm, I thought so. Honestly this was just my very iffy attempt at having a modular, abstract way of handling events: any entity that can process events is stored along with the event it processes, and then you just do "actor.execute()" when polling for the next event (in a turn-based context, for instance).
already done :)
This release works just fine in Hyper-V FWIW, besides networking.
It's all good! I expect this to happen for new things.
I have to confess that I'm completely lost with Tokio, and that every time a new piece is added to the puzzle, it seems to make it even less approachable. Having a lot of examples for all these crates would *really* help. Right now, it's hard to know where to start. For example: how to make a simple TCP acceptor, that would drop the oldest client when a new connection comes in and more than N clients are already connected? How do I change a previously set timeout because I want to adjust it to the system load? That kind of thing is trivial when you deal with sockets directly, yet I have absolutely no idea about how to achieve this using Tokio.
This is fixed in the new release: https://www.reddit.com/r/rust/comments/5zyujb/redox_os_release_012_fix_virtualbox_hopefully/
Thanks!
Not yet. I will work on it
Not one bit! https://twitter.com/steveklabnik/status/842736139467345922
Good question! In theory, Rust should be able to target any platform supported by LLVM. Platforms that do not support the standard library can use the `#![no_std]` directive and run on the bare metal. See [Redox](https://www.redox-os.org) as an example of this mind of usage. In practice, however, things are a bit more complicated. Upstream LLVM gained support for AVR microcontrollers a little while ago ([merged repo](https://github.com/avr-llvm/llvm)). This allows LLVM to target a host of microcontroller platforms, including the Arduino, but the Rust compiler hasn't officially exposed this target yet AFAIK. See Embedded Rust RFC [#2](https://github.com/rust-embedded/rfcs/issues/2) and [#3](https://github.com/rust-embedded/rfcs/issues/3) for the current status. Could someone else shed some light on the current status of AVR support in Rust?
Not very sure how to explain this one, basically I want to to get as an argument to a function and return type a reference with lifetime 'a associated type of trait implemented for some type. The problem is the trait has a lifetime itself, and while I "know" (at least they are declared as such in practice) that both the associated type lifetime and the reference in the struct have virtually the same duration, the compiler doesn't and won't compile. An example of what I'm trying is here: * https://play.rust-lang.org/?gist=46a24c655940972fa54c81871fabf598&amp;version=stable&amp;backtrace=0 * https://gist.github.com/anonymous/987123c0e3965a54d21485275bff983d There is anyway to make the code commented out work? Maybe is not possible, maybe I'm missing some syntax, in any case there is an alternative (aside of moving a method to the trait definition itself). 
&gt; Could someone else shed some light on the current status of AVR support in Rust? Waiting for the LLVM 4.0.0 stuff to land first, at least.
ARM appears to be much better supported than AVR, thankfully. There are examples online of Rust code being successfully built for the Cortex M4, for example.
A direct answer to your question is https://forge.rust-lang.org/platform-support.html in terms of officially supported platforms. https://github.com/japaric/xargo lets you create your own builds for additional ones as well.
Yours also assumes initialization order, mine enforces it. This was an important bit of the design :)
Oh you know what - I'm just an idiot - I forgot that I had recently stopped allowing 3rd party scripts and that broke the playground's gist integration. The original link is fine. Sorry.
Could redox provide a userspace compat layer that allows linux drivers to run? That would be a goldmine of hardware compatibility, but it's quite the technical feat
Rust: infiltrating every system under the sun! --- I'm only half jesting too. To pick only 3 usage points (blissfully ignoring Firefox): - VSCode: Rust infiltrating Windows! - libresvg: Rust infiltrating Linux (via Gnome)! - Dropbox: Rust infiltrating Cloud services! At this rhythm, soon enough it'll be difficult NOT to use Rust, even indirectly.
unexpected /r/unexpectedfactorial best link all day
yeah, or having module-wide default-derives
No need to kill the mood, man. Cheer up, it's friday and we have a new shiny toy to play with.
Here's the list of platforms that Rust is known to support: https://forge.rust-lang.org/platform-support.html As far as cross-compiling goes (which is mandatory for the more limited platforms), japaric is the expert: https://github.com/japaric/rust-cross
I feel exactly like /u/jedisct1 except that I probably understand even less. I think I don't get the whole purpose of async IO and the apparent increase in complexity. Sure something something light user threads something something performance and so forth. But I have yet to find a tutorial/explanation which really makes me understand. The async libraries look a lot more complex than the sync IO thingies. I also don't know whether or not all this stuff is important for most users or rather only for those writing the core of big libraries, like HTTP servers and stuff. It would be so great to understand this async hype \^_^
Yea you are definitely right about the terminology there. For some reason I just instantly associated system with low level platforms. Probably in part due to the fact that I am taking an embedded 'systems' class right now in school.
That second link seems awesome. Thanks!
Yea I am definitely getting that feeling lol. Especially given my limited amount of knowledge with low level stuff. Mainly wanted to see if I could use Rust for a project at school where we have to program a LPC4088 board. The board is mbed enabled so that looks like my best bet right now, although doing it in Rust would have been cool.
Try this to start fn bigger(&amp;self, other: &amp;&lt;V as First&lt;'a&gt;&gt;::Arg) -&gt; &amp;&lt;V as First&lt;'a&gt;&gt;::Arg That will at least ensure the input and output lifetimes match. The body of that function won't compile as-is. I wasn't sure what you wanted to do with it but this compiles, for example fn bigger(&amp;self, other: &amp;'a&lt;V as First&lt;'a&gt;&gt;::Arg) -&gt; &amp;&lt;V as First&lt;'a&gt;&gt;::Arg { self.test.bigger_works(other) }
&gt; But I have yet to find a tutorial/explanation which really makes me understand. I am waiting on a build, so let me try. You're working at a pizza place: Blocking Pizza: the best pizza on the block! (tm). You're the cashier, it's your job to take the orders. You're standing at the register. Three people walk in, and get in line. The first person goes "Hmmm I'm not sure, do I want pineapple or not?" They think about it for five minutes. They decide yes (fight me), and then you move on to the second person. "I dunno, pepperoni _and_ sausage, or just pepperoni?" They take ten minutes to decide this. Then the third person says "yes I'd like cheese please." immediately. Five minutes plus ten minutes plus zero minutes equals 15 minutes of time to take their orders. This is not great. Especially for the third person, who actually had their stuff together and was prepared to actually order their damn pizza. They had to wait forever thanks to the jerks in front. You say "this job is a joke" and move to Async Pizza across the street. Same three people come in the door. The first person says "Hmmm I'm not sure, do I want pineapple or not?" After a few seconds of waiting, you say "Can you step aside and think about it? When you're ready, let me know." They step aside. The second person says "I dunno, pepperoni _and_ sausage, or just pepperoni?". Same deal: step aside. The third person says "yes I'd like cheese please" immediately, and you take their order. A few minutes goes by, and the pineapple lover is ready to order. Five more minutes goes by, and the pepperoni and sausage person is ready. In this scenario, you've taken ten minutes to process everyone's orders: that's five whole minutes faster, even doing the same amount of work! Not only that, but the cheese pizza order-er was able to get in and get out, and not get held up by the slowpokes. Ordering pizza is a process, this is it sync and async. Does that make sense? (Also, the first line was a subtle joke, did you get it now?)
Alright!! That was quick!
I would be very impressed too, although since that method isn't doing anything complicated, I don't think anything in the language says you *can't* do it, at least at a machine code level. (Since when you spit out machine code, you can ignore the language-level source playing musical chairs with bindings and just assign the last one to the first one directly.) Also, I've only just seen your discussion with /u/gregwtmtno, and didn't realise you weren't using literal Strings. Does my solution still work in your context? (I can't immediately think of any limitations it has, apart from requiring 3-4 bit-level moves of whatever value you're using as `T`, hence the "marginally slower" comment.)
Why would you ever want pizza to be non-blocking? I want to block everyone from eating my pizza.
That sounds nice. Though I guess in the end, the coupon probably makes it the same total price once it's there anyway, assuming it'll sell for around the same cost. Maybe I'm just making excuses to not end up with yet another programming book on my shelves that gather dust. I swear dear Java, C, C++ etc. books; I will read and learn you soon... üôÑ 
what about cgi-bin pizza where you use cans and string to ask your neighbors about their pizza
&gt; (fight me) 
This can be done pretty easily with macros.
The short wikipedia definition pretty much nails it down: System software is computer software designed to operate and control the computer hardware, and to provide a platform for running application software. System software includes software categories such as operating systems, utility software, device drivers, compilers, and linkers.
Not directly Rust related, but Rust is one of the included languages, and seems to be doing pretty well by this metric, within the top 10 languages sorted by "future probability" based on number of people migrating between languages. Go takes the top spot, which is an interesting result, while older languages like C, C++, Java, Python and C# are still doing well. Lisp, Perl, Visual Basic, Fortran, and Lua(!) don't do particularly well on this metric.
If you'd like to see a full real-world example of tokio usage, I was fortunate enough to recently have /u/acrichto rewrite the core of [sccache](https://github.com/mozilla/sccache) to use tokio instead of raw mio. (Pro-tip: if a core Rust contributor offers to contribute to your project, always say yes, even if it means you have to spend a few days reviewing their code!) Most of the interesting bits are in [server.rs](https://github.com/mozilla/sccache/blob/master/src/server.rs), which handles client connections and doles out work. I have a number of not-yet-landed changes that I've written atop those changes, and one thing that I've noticed is that it makes writing functions that need to do some work asynchronously *way* better. It's especially noticeable when a function might be able to immediately return an answer, or might need to do some long-running work to produce an answer. With futures+tokio you can just return a `Future` and the caller calls `and_then(...)` and everything works. Previously the code had a hodgepodge of thread spawning and whatnot and it was just generally not as easy to work with. 
You may also be interested in [`rayon`](https://github.com/nikomatsakis/rayon) which has [experimental `futures` support](https://github.com/nikomatsakis/rayon/blob/master/Cargo.toml#L19). In general futures should definitely cover this use case, and if something is missing just let us know! It may not be quite as abstract as you'd find in Java, but you can also let us know about that :)
You folks are killing it! Awesome work.
I believe it's only GCC and probably clang that will definitely work with the union trick. Memcpy is the fully standard way of handling it in C, and often the compiler turn the memcpy into what you want anyway. The bugs you can get from the type punning in C can be nightmarish to hunt down, because the majority of the time it doesn't cause a problem so it sits there until some point in the future some change entirely unrelated to the actual cause of the bug causes completely insane bugs that only really affect release builds. Then you're stepping the debugger through the assembly since the issue is obviously impossible in the the C code. I had to hunt down a similar issue last weekend, and it was extremely frustrating. I got the info that th union only works on the more GCC/Clang compilers from https://www.cocoawithlove.com/2008/04/using-pointers-to-recast-in-c-is-bad.html
There is a blog post about it: https://pijul.org/2017/03/16/sanakirja.html
It's still not clear to me why you need the function to consume the enum value instead of taking it by mutable reference.
You seem to have grasped the fundamentals already: 1) Exactly. If rust didn't have lifetimes, when the compiler came to check `test_box`, it would also have to look inside the `bar*` function to determine whether the return value lived long enough. (Effectively inferring the lifetimes for you) This rapidly becomes infeasible as the program becomes larger, and would mean that changes to the implementation of a function could inadvertently break code using that function. To avoid this, rust has lifetime annotations, which lock down the signature, and no inference is needed. It is sufficient to separately check that both the caller and callee abide the rules set down by the lifetime annotations. When you omit lifetimes in rust, it is still not doing that kind of inference - there are just some simple rules which determine what the lifetimes of the argument and return type default to if left unspecified. 2) Lifetimes have no concrete "value" attached to them. If there is no bound relating 'a and 'b, then not only can the compiler not know whether one outlives another, there isn't even an answer to that question. What happens is that at the call site, the compiler looks at the signature, sees no bound on 'a or 'b, and so allows the calling code to pass borrow checking. At the callee, the compiler sees you trying to pass something with a lifetime 'b as a result with a lifetime 'a, is unable to prove that 'b: 'a, and so fails borrow checking. 3+4) It fails not because the types don't *match*, but because the variable's type cannot be coerced into the return type. (In a covariant context, such as a return statement, types can be implicitly coerced when the lifetime of one type outlives the lifetime of the other) 
Please see Rule 4.
Threaded Pizza works except that all the employees behind the counter can bump into each other trying to take orders and get pizza from the kitchen. It works up to a point where employees trying to get around other employees takes up more and more time.
They're two different axes. You can do sync or async io with native or green threads.
The net stuff relies on the tokio reactor core, whereas the io stuff didn't.
Fortunately the ownership system protects you from having your pizza stolen, but please be aware, that `forget` is still possible in safe rust.
Yep I've seen that one. If a chat based thing is a good example, then perhaps a simple custom chat protocol. Like new user packet and chat packet etc.
Project properties - Rust Build targets - "Check" target - set to "${CARGO_TOOL_PATH} rustc --bin &lt;binary_name&gt; --message-format=json"
Some companies won't touch gplv3 code.
And to answer the question you had... The differences between both are quite subtile. The current common understanding ist that green threads are to be used when you want to reduce jitter in response times (i.e. a 9x% percentile as low as possible). But due to the overhead of managing all those coroutines and switching between them you'll often find that pure tokio-style async I/O has a slightly better throughput. So in the end it's mostly a question about what you consider more important: Throughput or latency. You can't have both though. As Steve said both, green threads and tokio, are just styles of writing a program with async I/O. In most use cases the above difference in performance is too subtile to be relevant so it becomes a question of which programming style you favor.
https://github.com/weld-project/weld/blob/master/Cargo.toml
That's a known Hyper issue, come add your voice in the https://github.com/hyperium/hyper/issues/1075
Yep. Like any survey based on Google searches for particular words or phrases, there are all kinds of flaws with this methodology. It's more for fun, and to get a rough sense of some trends, than anything scientific. None of these ranking systems (TIOBE, Redmonk, PYPL, Benchmarks Game, Stack Overflow Developer Survey) are particularly accurate reflections of the whole programming community or the full story of what language is best, fastest, or most popular. But they do say something, about some fraction of the community, or about some aspect of performance, and can be interesting to watch over time.
Totally. It's just important to understand exactly what's being measured. That reminds me, time to check RedMonk...
[If only there was some other way...](https://github.com/rust-lang/rfcs/pull/483) ü§î
That is good.
It's at this point I lament the lack of good support for async file I/O in the various operating systems. Sure they have APIs that seem asynchronous, but they all end up cheating in one way or another (looking at you, *Linux*). It seems like the best way to do async file I/O is to just have a thread reading block-sized chunks at a time from a rotating queue of files so it's (hopefully) not blocked on one for too long, but that means a lot of time wasted on disk head movement.
@MaxwellTheWalrus the link doesn't exist anymore :/
Could you explain (or point to some information) how Linux APIs are cheating please? I don't recall hearing that before. 
If you say "system software", my mind generally thinks of what you said. If you say "systems programming" (as in "systems programming language"), my mind generally thinks of what *I* said: that is, systems software plus other large systems, for some definition of "large".
IANAL As I understand it it's pretty ambiguous. The intent of the AGPL is that yes, all of github would have to be open source as it is (presumably) now architectured. I believe the intent becomes less clear if github didn't use git as a library, and used it as a binary. Github could possibly also partition their code into a "VCS handling portion" and a "front end portion" with the front end portion not being a derivative work of the VCS handling portion, and then only the VCS handling portion would only need to be AGPLed. The reality is more complex though. The AGPL claims to only be a license (granting more rights than you already have), not a contract. I've seen actual lawyers arguing that this means that, since you don't need a license to use a binary you possess on your server, it doesn't actually do what it wants to and you're free to use AGPL code the same as you use GPL code. Either way, github would have been free to reimplement git, creating a non AGPL version, and keep it closed source.
You mean, on the physical hardware (Thinkpad)? Or somwhere in the VirtualBox settings?
I know that when https://github.com/mr-byte/tokio-irc-client was posted a few days ago carllerche gave it a thumbs up. It might be more complicated than you're looking for, but it's a real world example.
The physical hardware. Go into the BIOS and enable virtualization.
Look out, VisualBasic! Rust has your number!
I think most people just assume file I/O is irredeemably slow and thus cache as heavily as they can. Even the OS developers seem to have given up on improving it.
Of course, it gets wiped every so often, here you go: https://hastebin.com/woviqenimu.toml
Finally a reason to use beta!
Look at the link.
Some distros (like arch) just symlink them all to /usr/bin, which I think is the right way to go
Heresy
tokio-irc-client was created a few days ago, so it just needs a bit of time to mature and get all of that yummy documentation.
Referring to rules by number but not numbering the rules means accesses are O(n). Why are they not numbered?
I am still working on resizing in many applications. Currently the browser and image viewer resize.
This new analysis may be overrepresenting Rust, but the old one was (AFAICT) underrepresenting it. Every time that I tried to replicate their previous Github rankings I was getting data that would have put Rust somewhere in the 30s (by their ranking) rather than the 40s. And of course, Rust isn't the only programming language developed on Github, e.g. Swift and Typescript both are as well (and both saw large jumps in the rankings too).
Isn't there this tool by the core team that periodically tries to build (all?) crates on crates.io for stable (and I think nightly)? Something quite close in name to crate(s), cargo or something like that.
As mentioned, there are already rust bindings. I've actually used them and they are quite good.
\*chuckle\* But it's TypeScript that Rust should have its eye on, given the kind of people who are likely to be attracted to them. (People who want a stronger type system and a language well-adapted to making large solutions more feasible that can still interoperate with the incumbent language for the problem domain.)
From what I remember from reading about this topic in the past, the recommendations are to basically schedule as many reads in parallel as you can and let the lower-level IO systems worry about optimizing the actual disk reads.
FWIW, I've been tracking the number of active repositories on GitHub for Rust and a few other languages for the last few months. We did see growth that puts us between Haskell and Scala in terms of active projects, which is what's reflected in the Redmonk survey as well. My take-away is to not take any one survey too seriously, but it's nice to see the growth happening in Rust reflected nevertheless.
Welp. I did this on my old library and immediately found a bug in cargo. Thanks.
Yep! That might be the only thing in the whole project where theory is of absolutely no help :( Hopefully this will be resolved soon.
Check out this: http://berb.github.io/diploma-thesis/community/042_serverarch.html
You may be wondering how the compiler knows which implementation of the `FromStr` trait to use. Basically, when `addr` is passed to some function (like `connect` maybe) later, that function will only accept a specific type, thus `addr` must be of that type. The compiler will then use the `FromStr` implementation for that type.
This morning. (For those of you not in the loop: Rule 7 on this subreddit is just a bit of fun which changes constantly)
Cool, I am a little more confident I'm not losing it now.
It would also need to account for people entering programming on a particular language, and people leaving programming completely (or changing to a language not in the model).
This is now my primary focus - to fix this issue. I will be tracking progress here: https://github.com/redox-os/redox/issues/868
it would be nice if the encoding/decoding could be abstracted away, probably by accepting things that implement the serde traits as argument.
I really can't say. I imagine with mass adoption of solid-state media driving down prices and forcing the obsoletion of hard disks, we might see a new interface standard develop that allows the disk controller to field many requests at once and serve them asynchronously, and then chipsets cropping up that accommodate that, and then OSes developing support for it. Asynchronous/reactive paradigms are very rapidly being adopted across the board so I can imagine demand for true async file I/O is going to do nothing but skyrocket in the coming years, and the first OS/architecture to answer that call in earnest is going to become very popular indeed.
Note that if your crate is still in `0.x.y`, you do *not* need to go to `1.0.0`; a `0.x.y` -&gt; `0.x+1.y` upgrade does not need to be backwards-compatible. I've made that mistake before.
I'm not sure if you're kidding, but: Rule 7 on the sidebar has always been a random obviously-joking rule. The community has been okay with this so far.
A good tip for this kind of thing is to search for `Rust parse` on google. The first result explains it [here](https://doc.rust-lang.org/std/primitive.str.html). Basically, parse will convert a string slice to any type if that type implements `FromStr`. What type is chosen? That is decided by the usage of the parsed object later in the code. From a C++ background, type inference only occurs on one line: auto x = "127.0.0.1:17653".parse::&lt;SocketAddr&gt;().unwrap(); //Can't omit the type whereas Rust type inference can work on multiple lines: let x = "127.0.0.1:17653".parse().unwrap(); foo(x); //foo takes a SocketAddr so we can infer the type of x, //therefore we know which parse method to call Rust therefore allows for fewer type annotations. This is cleaner but I find that it is often harder to understand. The `unwrap` call is used because parsing a string can fail, `unwrap` simply panics at runtime if there is a typo. Ideally you have a `constexpr` function with a `static_assert` so that you can do those checks at compile time and handle a `SocketAddr` directly, not a `Result&lt;SocketAddr&gt;`. I expect this will one day be possible in Rust.
Why GPLv2 and not GPLv3? GPLv2 is incompatible with Apache-2.0.
I'd heard of NVMe but I didn't realize it was asynchronous. I also got the impression that it was highly proprietary.
Indeed what might be the best solution is to have exactly as many registers as you have counter space, and to have them balance the work among themselves use some heuristic. That is - a rayon-like threadpool with a workstealing algorithm. My understanding is someday tokio hopes to have such an engine.
The post itself contains such disclaimers^1, and I thought it was pretty clear from the intro of the post itself^2 that this was an idle musing of "huh, this is an interesting question, let's see how much data I can collect with a free afternoon," not "this is the be-all, end-all analysis of teams moving between programming languages." I feel like a lot of people are taking this way more seriously than it was intended. ^1 "What about the diagonal elements? There is of course a really big probability that people just stay with a certain programming language. But I‚Äôm ignoring this because (a) turns out search results for things like stay with Swift is 99% related to Taylor Swift (b) the stationary distribution is actually independent of adding a constant diagonal (identity) matrix (c) it‚Äôs my blog post and I can do whatever I want :trollface:." ^2 'I was reading yet another blog post titled ‚ÄúWhy our team moved from \&lt;language X\&gt; to \&lt;language Y\&gt;" (I forgot which one) and I started wondering if you can generalize it a bit. Is it possible to generate a N * N contingency table of moving from language X to language Y?'
&gt; Cycles are avoided by not storing reference counts less than or equal to 1. This seems very haphazard
If you're sustaining a really high op count that can work very well, particularly if the the threads get pinned to cpus to prevent swapping, and the device io supports per-cpu queues. You still likely want async io in each of the threads though. Edit: BTW this is a change in scheme from the synchronous-io per thread model to a a thread per CPU driving async ios in each.... 
It may have been just the usual complaints about closed-source firmware blobs or something like that.
thanks for this... just enabled weekly.
IMO IRC is one of the most valuable resources: https://www.rust-lang.org/en-US/community.html. 
Being compatible with linux userspace/syscalls isn't terribly hard by comparison (Microsoft did this, and I think FreeBSD supports it too), but the internal kernel API is pretty massive and a different beast altogether. Even just to support a few basic drivers would probably require a lot of effort.
You can even tell Travis to ignore build failures with nightly, as... breakage is kind of expected there: language: rust rust: - stable - beta - nightly matrix: allow_failures: - rust: nightly It's described in more detail [in the official Travis tutorial for Rust](https://docs.travis-ci.com/user/languages/rust/). They even mention this exact issue: &gt; The Rust team appreciates testing against the `beta` and `nightly` channels, [...].
Are there plans to add trait like a `tokio::Session`, that behaves like `tokio::Service`, but adds an additional method that can poll a backend for messages and pass messages to the client? This could be useful for irc-servers and game-servers.
&gt; Lisp, Perl, Visual Basic, Fortran, and Lua(!) don't do particularly well on this metric. That makes sense: who would want to move from C# to Fortran? Or from Rust to Perl? There are a few reasons to change programming language (change of team/environment, portability, performance), and Lisp, etc. are not targets there, only in really rare cases. Rust OTOH is almost never an initial language (if only because it didn't exist when the project started), so can only be a target.
I actually thought that Block Pizza would require to first accept #1's order, then cook his pizza completely, and only *then* accept #2's order. And your Block Pizza is actually a coarse-grained Async Pizza. Kind of?
I like the ideas of patch based version control and would like to see it get more widespread usage. A more established license will make this more likely IMHO.
I feel like this could be better accomplished by a blacklist than a whitelist. I definitely don't want to have malware linked on crates.io, but I also don't think we should be forcing developers to use one of a curated list of 'safe' community-maintained sites which may or may not be just as likely to be dropped as the developer's personal site. I've hosted docs for the very few crates I publish on my website, and if they had been using rust-ci (which was popular when I was first developing them), the links would be in a much worse state than they are now.
FreeBSD actually has a bit of Linux kernel compatibility too, in the kernel ‚Äî that's used for the graphics stack. And there's also `webcamd` that uses Linux drivers in userspace to support USB devices like touchscreens, DVB and, well, webcams.
Interesting, I wouldn't have expected it for graphics since that seems relatively complicated, although that is the natural thing to go after. For a long time I remember people using ndiswrapper on linux for networking drivers from Windows, which is I guess a similar idea.
Can you link to this implementation? If not, why not? It feels like I keep seeing references from the tokio/mio inner cabal to half-finished repos that are secret or whatever, and it gives me a bad feeling about the community. Why not really work in the open?
there's another issue then: the OS might batch a few reads together, so from its point of view, the request was fast, but in fact it just added more latency to every read. There's a good [talk](https://www.infoq.com/presentations/scylladb) about this by ScyllaDB's CTO.
I believe the popularity of Rust will only start to skyrocket once we have a culture transition in computer science. If it does not matter how many vulnerabilities your program has then C is good for systems programming. If it matters, then C could be excluded as a tool for everyone since decades of experience has proven that human capability is not enough to master C to a sufficient degree.
Nice work. It's unfortunate that it also inherit lmdb biggest drawback of having to be preallocated.
I've noticed the same thing. Here's another example of the secrecy you describe: https://github.com/hyperium/hyper/issues/894#issuecomment-282811785.
No, it's deliberately unergonomic to try to prevent people from using `catch_unwind()` too generously. Why that can't be enforced at the paradigm level, I don't really know. 
It's GPLv2 or later, so you can use it as GPLv3.
Wrong sub
Great; http2 is definitely complex enough to prove the point. Looking forward to see how it turns out.
Because GPLv3 contains strong political statements about what kind of hardware you may and may not use (tivoization). My opinion might be different in a perfect world with good and cheap devices (especially mobile devices) on the market. It is GPLv2+, by the way.
That gives context to explain things. The quoted statement didn't explain that. Still not explained is how you know no cycles can be created with reference counts of 2+
It depends on your learning style. I prefer learning more theory parts first and then implement some toy projects, instead learning by doing. For me, It was the [reference](https://doc.rust-lang.org/reference.html) , [Programming Rust Book](https://www.safaribooksonline.com/library/view/programming-rust/9781491927274/) and [Rust Programming Course of University of Pennsylvania](https://cis198-2016f.github.io/schedule/) You can find good resources for each section from [ctjhoa/rust-learning](https://github.com/ctjhoa/rust-learning) I prefer [Go Bootcamp](http://www.golangbootcamp.com/book) type reference. Because so far we don't have that type of resource in Rust, I tried to create my own one for learning purposes; [Learning Rust](https://medium.com/learning-rust). Hope these things will be helpful for you. Good Luck!
Indexing rules in a non-latin scripts rarely makes sense, see the excellent http://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/ by /u/Manishearth
No need to shout here üòâ That said, Rust steers you more in a direction of *data-oriented programming*. You define your data structures to hold the state of whatever computation you are coding, then write `impl`s (and traits + trait `impl`s to abstract out common interfaces) for your computation. Also Rust lets you mutate state happily, as long as there's no other reference to it (which could lead to data races and invalidation errors).
I got commit access to leveldb-rs and an open issue. Once I got a computer some time on my hand, I will fix this.
Blacklists and whitelists are the same amount of effort for responding to a good site turning bad - add to the blacklist or remove from the whitelist. The whitelist is more effort to mark a site good in the first place but that's arguably a good thing, it's a nudge to use an already-whitelisted source that the community already tacitly endorses. And if the whitelist add policy is liberal, it's not a significant obstacle to people who feel strongly about self-hosting.
Is this problem occurring again in `rustc 1.17.0-nightly (a559452b0 2017-03-17)`? The backtrace says that `cargo::version` is the cause of the panic. thread 'main' panicked at 'called `Option::unwrap()` on a `None` value', /checkout/src/libcore/option.rs:323 note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace. stack backtrace: 0: cargo::version at /checkout/src/libcore/macros.rs:21 at /checkout/cargo/src/cargo/lib.rs:263 1: cargo::execute at /checkout/cargo/src/bin/cargo.rs:156 2: cargo::call_main_without_stdin at /checkout/cargo/src/cargo/lib.rs:128 3: cargo::main at /checkout/cargo/src/bin/cargo.rs:91 at /checkout/cargo/src/bin/cargo.rs:84
http://shop.oreilly.com/product/0636920040385.do
Because sometimes you don't feel like developing in the open because: 1) You're still experimenting with the API and implementation 2) Work is in progress, so everything is messy, may not compile, etc. 3) People just shouldn't use the code, or imagine it's ready for use 
Mine is definitely an unpopular opinion here, but, having written servers extensively in both sync and async contexts, in a variety of languages, I believe that: 1) Sync APIs and models are dramatically easier for people to reason about and maintain 2) Go got it right by exposing a sync API for IO and handling the scheduling and the blocking under the hood I understand that async is a better fit for the context that Rust is meant to be used with.
It's all good.
I would prefer including a comment to state that the `.unwrap()` call is intentional, rather than something to fix later.
&gt; Another great thing to do, if you're using Travis CI, is to include beta as one of the Rust versions tested against: Hijacking the top comment to tell people that this is "equally" easy in GitLab CI; [Example .gitlab-ci.yml file](https://gitlab.com/rubdos/ntru-rs/blob/master/.gitlab-ci.yml) (this uses `build-essentials` because I use the `ring` crate), [example GitLab pipeline output](https://gitlab.com/rubdos/ntru-rs/pipelines/7037658).
My only point is that if the goal is for users to be confident they can click a link on crates.io without landing on a malware site, a blacklist isn't effective, as it can only address links that have already been demonstrated as bad. There's no prevention, just reaction. It's completely legitimate for other concerns to outweigh this one, but the tradeoff should be acknowledged. A middle ground might be to decorate links with some kind of whitelisted icon. Or, conversely, you could do an icon or insert an interstitial page for non-whitelisted sites. 
[WireGuard on FOSDEM 2017](https://fosdem.org/2017/schedule/event/wireguard/)
I was really impressed with the typed database thing, it allows some neat optimizations! The most basic being storing fixed length types with less overhead.
&gt; It's completely legitimate for other concerns to outweigh this one, but the tradeoff should be acknowledged. Absolutely, and acknowledging the tradeoffs is what I was attempting to do in my comment. We've got multiple problems to solve here, with different stakeholders: - Crate users should feel safe and confident when clicking on a link from crates.io that they won't end up on a page with malware/ads/spam. - Crate users should be able to access legitimate documentation. - Crate authors should be able to host their docs on a custom domain without having unreasonable barriers (discovering the barriers, logistics of navigating the barriers, waiting for approval) placed in their way. - Crates.io maintainers and/or mod team should not end up spending a large portion of their volunteer time checking/approving/denying links. I'm not sure if it's possible to satisfy all of these constraints! I'm currently liking a little warning icon for links of unknown quality, that could link to an explanation about whatever policy we end up going with. I've also duplicated a bunch of these awesome comments on [the issue](https://github.com/rust-lang/crates.io/issues/626) since we might get different people weighing in there. 
Is there any point to testing on beta if you're already testing on nightly? To be honest, beta seems fairly pointless at the moment. Everyone uses either stable, if they want stability, or nightly, if they need to use unstable features.
I am seeing this error now: error: failed to run custom build command for `miniz-sys v0.1.9` process didn't exit successfully: `C:\Users\bla\appdata\local\temp\cargo-install.bI4gKx6HwRWq\release\build\miniz-sys-0d42c5494ba5bc47\build-script-build` (exit code: 101) --- stdout TARGET = Some("x86_64-pc-windows-gnu") OPT_LEVEL = Some("3") TARGET = Some("x86_64-pc-windows-gnu") HOST = Some("x86_64-pc-windows-gnu") TARGET = Some("x86_64-pc-windows-gnu") TARGET = Some("x86_64-pc-windows-gnu") HOST = Some("x86_64-pc-windows-gnu") CC_x86_64-pc-windows-gnu = None CC_x86_64_pc_windows_gnu = None HOST_CC = None CC = None TARGET = Some("x86_64-pc-windows-gnu") HOST = Some("x86_64-pc-windows-gnu") CFLAGS_x86_64-pc-windows-gnu = None CFLAGS_x86_64_pc_windows_gnu = None HOST_CFLAGS = None CFLAGS = None PROFILE = Some("release") running: "gcc.exe" "-O3" "-ffunction-sections" "-fdata-sections" "-m64" "-o" "C:\\Users\\bla\\appdata\\local\\temp\\cargo-install.bI4gKx6HwRWq\\release\\build\\miniz-sys-18005000ddedadf4\\out\\miniz.o" "-c" "miniz.c" 
Well, that doesn't seem to be coming from Pijul. Maybe from flate2?
Can I ask the obvious question and ask why you *want* to do this? FFI is one thing, but pure Rust is high-level enough to cleanly support composition in a way that doesn't require weird pointer arithmetic and casting.
This seems like one of those situations where the correct solution is to be mildly opinionated. "Everything on crates.io has auto generated docs on docs.rs with a corresponding link" seems a better answer. That way everything has at least a type description of what it makes public. While at the same time developers are encouraged to use the standard source. If they want to have a link to their own docs elsewhere, that would be a separate matter.
I don't have VC, so I have stable-gnu installed. I would like to try stable-msvc but my internet connection is _very_ weak right now.
AFAIK, not until field offsets are stored in a vtable it's not :/
But ... but... that's the point, isn't it? Rust is generally unergonomic around stuff it wants you to think about, that's deliberate. That's why it's annoying to codepoint index strings, for example. I don't find this "adding another complaint to the 'Rust is needlessly unergonomic' pile" because it isn't needlessly unergonomic. There's a reason behind this. What this does is give some more fine-grainedness; there's a regular "catch_unwind" and then the AssertUnwindSafe catch_unwind. Both can be code smells (depends), but one is much smellier.
I think, I mostly agree -- as I hopefully mentioned -- though I'm certainly no expert. Thank you for this comment. It's a bit unfortunate that everything -- even a blog -- has to be picked apart/nitpicked to death these days, but I feel it's actually a good thing, collectively, even though it may hurt the individual who "dares speak out". There's just **so** much "native advertising"[1] that looks like actual "real people doing real things", but is really just shilling. After a while it becomes impossible to separate the real stuff from the shilling and I think (hope) that this extreme-nitpicking effect is a counter-reaction. [1] No, I'm not making that term up.
I've written embedded rust for some ARM processors. It's not a supported target triple (or wasn't at the time), so you'll have to write your own llvm target file, but that's a one time annoyance and overall the experience with xargo is pleasant.
 1. I understand that the pledge is made about the return value. But what is the implication of that? Is it something like if you return y, does the error that is thrown imply something like the following? foo says: &gt; "I can only guarantee that 'a values will be valid until return, but you are returning 'b and I don't know if 'b was deallocated somewhere in my body" But the compiler should be able to know if 'b values were deallocated within its body, so my assumption seems to be incorrect. So what is the rationale behind complaining that 'b was returned when 'a was expected? 2. By lifetime of the function, I was trying to say &gt; as far as main() is concerned, it looks at foo() as a function whose return value guarantees 'a lifetime. So in this example, main() counts on the fact that whatever lifetime x in main() will have, will be the lifetime of the value returned by foo. 3. I was confused about this due to the error message displayed: &gt; lifetime of reference outlives lifetime of borrowed content. So I'm misinterpreting this. I'm going to play with this example a little more to understand this point further. I'm also aware of the Rust syntax that does let you establish some relation between lifetimes, &lt;'b, 'b:'a&gt; for example. 4. Assuming the future holds more lifetime elision, I'm still trying to grasp what the goal would be for the return value of foo to have a lifetime specified. I agree about reddit's numbering system, it made me restructure my post too!
&gt; Is it because main() needs to know what the lifetime of the return value of foo() is? In general, every reference has a lifetime. So strictly speaking, you need to annotate all of them in type signatures. However, this was a pain, and so we added "lifetime elision" https://doc.rust-lang.org/stable/book/lifetimes.html#lifetime-elision This lets you not write the lifetimes in certain circumstances, where the vast majority of the time, a default makes sense. (We actually calculated this with numbers.) This signature is not one of those situations, and therefore, you cannot elide the lifetimes, so you have to write them out. It has nothing to directly do with "not being able to figure them out", other than the rules for not writing them (which is different in a sense than "figuring them out") are related to there not being a pattern. There is another related bit of perspective here, which is that Rust makes you write out the type signatures of your functions as a statement of intent. It infers types elsewhere because they're not as important as the signatures of functions. So Rust won't look at the body of the function to determine its signature, it looks at the signature to determine the behavior of the body. Does that all make sense?
Because having the project open means it takes a lot more time / effort to make progress. People are excited and try using it, then I have to spend time debugging their issues / answering questions. If I don't, I'm being a bad open source maintainer. I may also decide to delete and start over, etc... And once it is open, there also is the group of people who end up saying how terrible it is, how they don't understand the point, etc... and that also takes time to respond to :) Keeping it private until I have some degree of confidence that it is good is faster. There is nothing nefarious about it.
Thanks for the response and all the write ups, your docs have been very, very helpful overall. That said, I did go through the lifetime elision link, and it makes sense, but I guess I'm not able to pinpoint the exact goal of return values having explicitly specified lifetimes specified. The idea of 'intent' makes sense. How about this modified example: fn foo(x: &amp;u32, y: &amp;u32) -&gt; &amp;u32 { x } fn main() { let x = 12; let y = 42; foo(&amp;x, &amp;y); } In this example, lifetime elision doesn't work. Could you please explain why? Sorry, I know it's right there in front of me, but I'm missing it. I'm going to re-read these posts and other articles until I get this right.
Original author here. You are of course correct on that point and I glossed over that distinction. I probably should have investigated the specifics further but I didn't; the comments here are very useful though!
Yes, this is exactly what I would have written.
The comment about codepoints seemed half-facetious. You know there's better reasons why Rust strings aren't trivially indexed by codepoints. I can concede the difference in smelliness but it brings to mind parallels with `unsafe {}`, which I'm sure have already been hashed out *ad nauseum*. I just don't think the difference is significant enough to warrant it, because it depends on context. `*p = String::new();` in isolation looks perfectly innocent, but in an `unsafe {}` block it's highly suspect. I just don't understand why we can't trust people to look at `catch_unwind() ` in the same light. If `Send` and `Sync` properly implied `UnwindSafe` and `RefUnwindSafe`, respectively, then I might have less of an issue. The RFC that introduced `PanicSafe` hand-waved this away due to coherence and it doesn't seem that anyone's in a hurry to re-address this in the context of either specialization or lattice impls because wanting to make `catch_unwind()` easier to use is apparently synonymous with wanting to implement full-on exceptions in Rust.
Thank you for this and the other response, I think it clears things up a bit. 
Compiler optimizations tend to be a bit unreliable. The [playground](https://play.rust-lang.org/) will let you see the assembly output of whatever Rust code you enter. For this example, I'm pretty sure that both C++ with `a` and `b` declared as `const` and Rust will be able to pull constant calculations like that out of the loop. `strlen` is different because, unless it's hard-coded into the optimizer, the compiler doesn't know that `strlen` is a pure function. It could have `return len + rand();`, for all the compiler knows. Rust doesn't have a way to mark a function as pure either, so it wouldn't be able to perform the optimization in the stackoverflow post. However, since Rust strings store their length, if the `len()` call is inlined it would just be a plain member access, so the optimizer would be able to tell that it doesn't change.
Thanks for the hint: I will try the minimal code in the playground ;)
Redirecting to https should be fixed now! https://twitter.com/billpollock/status/843145740247474176
Some prominent community members are strongly against forcing docs.rs only because they do extra to make their docs good, and want that control.
thanks!
I am writing a free book for beginner programmers. It assumes you have done a one-semester course in C language or in a language derived from C, and have no other programming experience. Here you can read the first 147 pages: https://www.gitbook.com/book/carlomilanesi/rust-programming-step-by-step/details
I see that /u/steveklabnik1 has linked to the first edition of the book, which you mention you've read. I'm mostly curious-- have you read [the section about lifetimes in the second (in-progress) edition of the book](http://rust-lang.github.io/book/second-edition/ch10-03-lifetime-syntax.html#generic-lifetimes-in-functions)? There's an example that's very similar to your example, and I'm curious to know if the second edition's explanations help at all.
I just checked and it is, thanks! That's cool.
Pijul stores most things in a Sanakirja database, which can be used in parallel. The current Pijul is not really using these features, though, because we preferred to focus on an understandable user experience (right, we're not there yet‚Ä¶). In a future version, we won't have to change much.
Ah alright thanks.
Ah, okay. I suspected it, what with the whole "blatantly wrong statements" (instead of "blatantly misleading" or whatever in case you were actually being serious), but it's good to clarify in case you actually did have a problem with it and had your voice ignored because it was too close to being a joke. You're welcome! I'm really happy with the atmosphere here so far :)
Yes, but there's no individual incentive to use it.
Wow, this is impressive. The logo reminds me a lot of the Dungeons &amp; Dragons ampersand logo, though :) http://www.underconsideration.com/brandnew/archives/dungeons_and_dragons_40_ampersand_flat.png
If you happen to be German (just to add this), the best resource is this video playlist: https://www.youtube.com/playlist?list=PL0Ur-09iGhpwMbNiVTBeHmIjs0GuIXhNg I plan to translate it to English soon (in a few months). It explains a lot of things that the Rust book does a bit poorly (like Fn vs FnMut vs FnOnce, etc) from start (simple variables, ownership) to finish (closures, multithreading, lifetimes). IMHO lifetimes aren't explained well enough in "the book".
Thank you, that's awesome, got it working! This actually completes my project, looking forward to push to crates.io when new hyper get there.
&gt; That doesn't seem like a very good rationale for the use cases given. For instance, in the cases given, there's a single identifier followed by arguments enclosed in delimiters, so parsing that would be pretty easy. But this is valid rust (because attributes can be applied to expressions): #[foo(bar)] (bar) By always requiring the delimiters we can avoid parsing ambiguities like this.
I wouldn't say that eschewing codepoint-based indexing was deliberate, it was more of a logical consequence of using UTF-8. In retrospect, I agree that the byte interface causes more problems than it solves. However, hiding cost in indexing isn't exactly foreign to Rust either, given `HashMap` and `BTreeMap`. The `char` story is interesting because Rust has renamed primitives before, but `char` just matches established convention too well--ignoring C, of course. The expectations are well enough defined that there's generally been not much issue, but that's purely conjecture on my part. I wasn't only pointing to the one comment, but also the one before it condescendingly referencing the documentation of `catch_unwind()`, which somehow garnered some +1s even though it didn't contribute anything. The sentiment is clear, even if the sample size is small and doesn't necessarily even represent the designers (which I don't consider to be exclusively the Rust core team even though that's what it sounds like in this context). The pain point statement I guess is a hyperbole considering I can't find much mention of `UnwindSafe` anywhere. But it may be that people just throw in `AssertUnwindSafe` and move on without commenting on the invariants they preserve. I'm not at my computer, but a [precursory search of Github](https://github.com/search?l=Rust&amp;q=AssertUnwindSafe&amp;ref=reposearch&amp;type=Code&amp;utf8=%E2%9C%93) (edit: link now for just Rust code), based on the number of pages alone, suggests that *maybe* the escape hatch is a little easier to reach for than the designers expected.
The documentation tool could add a robust identifier and include it as a meta tag or comment. You could even dual purpose the identifier to include information about the tooling used to generate the docs to help recognize potentially stale projects (i.e. if you could automatically recognize docs produced with, say, the 1.10 toolchain, those might be indicative of somewhat dead crates.) I'd also advocate some form of a 'report crate' link on crates.io to put as much burden for recognizing malicious links on the community rather than the crates.io maintainers.
rather than call into ripgrep directly, they could make many of the internal algorthms (like the FSM regex engine's impementation) generally available through an interface for other search-based features (fuzzy matching files, for instance). it would take a bit more work but they'd be able to unify the search featureset across the editor. it's just a thought, tho - I'm sure there's all sorts of details &amp;amp; complications I'm missing :)
in **"Run Configurations" Main** tab , replace *${CARGO_TOOL_PATH} test --no-run --message-format=json* with *${CARGO_TOOL_PATH} build* and then click **Run**
My point is that the extra noise wouldn't be necessary if you confer that responsibility on the use of `catch_unwind()` itself, which is the center of the entire issue and where the burden truly lies. But instead, the current API design dances around it and requires acknowledgement in hardly relevant places, or the use of a liability waiver/escape hatch which people prefer to reach for even though it ignores everything and requires the same enforcement as if `UnwindSafe`/`RefUnwindSafe` didn't exist; and as the cherry on top, if you try to be finer-grained with `AssertUnwindSafe` and your captures, you're just rewarded with more noise. It just smells.
&gt; if you confer that responsibility on the use of But, that's what it's doing already? Yes, it isn't perfect, but IMO the "perfect" version of this would still have the UnwindSafe trait and AssertUnwindSafe escape hatch, and still have an explicit slightly-awkward opt-in. Or maybe a different design, but still with an explicit opt-in (not "enforced at the paradigm level")
AXM or A\^M: Aliasable xor mutable
Half-tempted to remove this post solely to preserve the treasured sanctity of INWHTAMTPAMA.
Well, I can concede that `UnwindSafe` makes it possible to reason about unwind-safety in a generic context, but that's just not the major use-case, by a wide margin.
Have you actually checked that GCC doesn't optimize it? It's a lot harder to optimize a function call (like strlen) than addition in this way. The problem is that the compiler would have to know that strlen is pure. It can easily tell that addition is pure, and that the operands don't change, so the problem doesn't exist for addition.
Ah, I see. Yeah, that would make sense.
Or SWMR, SWXMR, SW/MR. Rust enforces single-writer / multiple-readers at compile time, just like a RWLock does at runtime.
Ah awesome, thanks for that clarification!
I want to write a function fn map&lt;A, B, F: FnMut(A) -&gt; B&gt;(mut v: Vec&lt;Vec&lt;A&gt;&gt;, mut f: F) -&gt; Vec&lt;Vec&lt;B&gt;&gt; that maps an FnMut closure over a nested vector. (This is a reduced version of a more realistic example I want to do.) This works fine when I implement it using loops. But when I try to implement this using iterators (`drain` and `map`), I get a borrowcheck error: error[E0507]: cannot move out of captured outer variable in an `FnMut` closure --&gt; &lt;anon&gt;:14:45 | 14 | v.drain(..).map(|mut w| w.drain(..).map(f).collect::&lt;Vec&lt;_&gt;&gt;()).collect::&lt;Vec&lt;_&gt;&gt;() | ^ cannot move out of captured outer variable in an `FnMut` closure Here's the [playground link](https://is.gd/EbAJlF). Thanks in advance!
Can I ask: Why is Tokio-Proto called Tokio-Proto and not Tokio-Protocol? Is saving three characters in the crate name really worth the cost of introducing the ambiguity between Protocol and Prototype?
I assumed you meant [ABS](https://en.wikipedia.org/wiki/Anti-lock_braking_system) and [PET](https://en.wikipedia.org/wiki/Positron_emission_tomography) before I stopped skipping your parenthetical expansions. UFCS I always have to backtrack from [UFC](https://en.wikipedia.org/wiki/Ultimate_Fighting_Championship) and then expand to ensure I'm not mixing it up with [UAP](https://en.wikipedia.org/wiki/Uniform_access_principle). I'd recommend names like [No free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem) not like [TANSTAAFL](https://en.wikipedia.org/wiki/There_ain%27t_no_such_thing_as_a_free_lunch)T. That feels too much like trying to remember intentionally confusing secrets like [IDSPISPOPD](https://en.wikipedia.org/w/index.php?title=IDSPISPOPD&amp;redirect=no) instead of just using [IDCLIP](http://doom.wikia.com/wiki/Doom_cheat_codes).
Using stable-msvc together with Visual Studio 2013 I see also a problem with ring: https://gist.github.com/fabric-and-ink/43398e7410b3535a6ff6e15bc354734b
I don't understand the requirement that poll_cancel must be run within a task. Is this a limitation due to the implementation or is there a good reason for it? I have a project which has a separate thread doing a blocking pop on multiple redis lists. I use futures::sync::oneshot to wait for items to be popped off each list in the event loop. It works well, but I can't poll cancellation from the redis thread as it isn't running inside a task (nor do I see a reason for it to do so).
Ok, I need VS2015 :( Sadly I don't have an admin account, so I cannot install it. https://github.com/briansmith/ring/blob/master/BUILDING.md Edit: Could you please offer windows builds? It seems to be kind of a pain to compile *ring* on windows.
Cool, thanks!
I got it to work by changing the line to v.drain(..).map(|mut w| { let f = &amp;mut f; w.drain(..).map(f).collect::&lt;Vec&lt;_&gt;&gt;() }).collect::&lt;Vec&lt;_&gt;&gt;() But I'd still like to know why the original code doesn't work. So if someone could explain, that would be very nice.
&gt; what if allocating a string in Rust fails? I would expect Rust to throw a panic! but how do I log where the error originally came from? By default, Rust will abort, not panic here. So your program straight-up terminates, not panics.
Yes, exactly what Steve said-- string slices is exactly where people first run into lifetime issues since integers are `Copy`. So this is a more realistic example that will better match the problems people are having in their own code. You're welcome! &lt;3
&gt; Also, I've noticed many crates (and I am currently doing the same) to just have a big enum for error handling? Is this considered good practise? It's at least not considered bad practice. It really depends; there are lots of advantages of doing it this way, though.
Is Tokio (and Futures) an implementation of a Reactive programming paradigm for Rust?
Nice write up! I will say that from my previous experience with zmq, I've found it a lot easier (in python anyway) to use zmq.select. This creates a Poller internally but returns 3 lists of which sockets are ready to read, write, or have errors. The really interesting use case is checking if a socket is ready to write. In a lot of cases this means something has connected on the other end, although it isn't applicable to all socket types (SUB can't write, for example). That might be useful for you to check if the server or client has crashed. 
You can create the implementation for From and Into automatically by using my [derive_more](https://jeltef.github.io/derive_more/derive_more/) crate 
How about [The Law of Exclusivity](https://github.com/apple/swift/blob/master/docs/OwnershipManifesto.md#the-law-of-exclusivity)? 
Thank you!
Might be interesting to open an issue so we can fix this instead of crying here. :-/
Thanks! It's nice to see we don't do it for nothing.
 pub struct Db&lt;K: Representable, V: Representable&gt; and pub trait Representable: Copy + std::fmt::Debug 
I've had mixed results with this... meaning nightly failures, fail the overall build...
The details are a bit subtle, however there definitely should be a way to poll for cancellation w/o being on a task. There is no fundamental reason why it shouldn't be possible. Just a missing API. I created an issue to track this: https://github.com/alexcrichton/futures-rs/issues/419 If you want to look into how to implement it or work around the limitation, I would take a look at how `Future::wait` is implemented.
Hmm, Separated Aliases and Mutations? "SAM"
Btw, how can I read_to_string if I want to write something back that depends on the string? I need to break the connection to read and then make another to get the response? I think I was mixing EOF with EOL, but how does that work anyway? [Here is the page in the docs: TcpStream](https://doc.rust-lang.org/std/net/struct.TcpStream.html)
/r/playrust?
Oh damn yeah sorry
https://pijul.org/sanakirja/doc/sanakirja/struct.Db.html apparently not in the docs, can you provide a link to your source?
That did't stop Rust from using acronyms HRTB/WF/DST/ZST/Arc/MIR/HAIR :)
https://docs.rs/sanakirja/0.8.3/sanakirja/ And the repository, using Sanakirja itself: https://nest.pijul.com/pijul_org/sanakirja
in that case the information on crates.io seems to be out of date thank you
Right, our mistake. OTOH, we're only two, with main jobs, to maintain four big crates as part of pijul (libpijul, pijul, sanakirja, thrussh). Excluding the nest, obviously (that would add at least five others ;-)).
This feature was explicitly called out as a non-goal in the LMDB design.
The tokio.rs documentation is great, but after reading it, I'm still stuck on obvious things. For example, it includes examples on how to set up a simple TCP server using tokio_proto::TcpServer. Awesome. Now, how to listen to two IP addresses simultaneously, without duplicating everything and ending up with twice more threads? How do I attach a BPF filter to the socket? I probably just need to get used to it but these abstraction layers seen to make everything way more complicated than using mio directly.
would it be helpful if I told you, what it would always overwrite the first four bytes, regardless of the buffer size? I did take it even one step further and manually added `+4` to the pointer, yet still the first four bytes of the buffer are set to zero.
This is, I think, primarily an issue with how traits are handled in the documentation. A `TcpStream` has very little functionality outside of the [Read](https://doc.rust-lang.org/std/io/trait.Read.html) and [Write](https://doc.rust-lang.org/std/io/trait.Write.html) traits. This definitely needs to be made more explicit in the documentation, not just for `TcpStream` but in several other places as well, because the traits themselves have *wonderful* documentation. Edit: I always forget Markdown's hyperlink format.
What disadvantages would this have?
Preallocation is an asset, not a liability. Especially in high performance servers, where your max resource requirements are known in advance. It gives you deterministic performance with no wasted CPU cycles and no unexpected pauses.
Now do it the other way around so that I can compile OSX binaries from our linux-based CI server?
r/playrust
thread 'main' panicked at 'index out of bounds: the len is 7 but the index is 7'
There's an unstable api to override the handler, see https://doc.rust-lang.org/stable/alloc/oom/fn.set_oom_handler.html Also note that overcommit, etc, will all play into if this routine even gets hit.
I'm not an expert in Reactive programming but I believe it is.
Wow, that was easy. Just enabled a CRON job for the master branch for my crate as well!
thank you, it works now
&gt; asset I'm sure it is in some cases, but it's extremely annoying when it's not. 
The inner `.map()` is trying to take `f` by-value, but it can't do that because `f` needs to be around for the next invocation of the outer closure. By explicitly capturing by mutable reference, you're making sure `f` isn't consumed by `.map(f)`.
/u/DroidLogician gave the explanation of what is going on, but note that instead of using the `let` statement you can just change `map(f)` to `map(&amp;mut f)` and the capture inference works then.
Nice one for offering an ebook and paper book option, unlike certain [other publishers](http://shop.oreilly.com/product/0636920040385.do).
According to [this](https://docs.microsoft.com/en-us/sql/odbc/reference/odbc-64-bit-information), it should be. Seems like the issue isn't actually in OP's code, but rather in the `odbc_ffi` crate defining the types wrong. Edit: this is even more confusing: the [crate source](https://docs.rs/crate/odbc-ffi/0.2.3/source/src/lib.rs) does define #[cfg(all(windows, target_pointer_width = "64"))] pub type SQLLEN = i64; #[cfg(all(windows, target_pointer_width = "32"))] pub type SQLLEN = SQLINTEGER; Could it break in the compiler somewhere?
Hint: unless doing `cargo run --release` it would take longer then you expect to do anything. I tried debug build first, and it took like two hours to resize.
 use std::thread; use std::rc::Rc; use std::sync::{Arc, Mutex, mpsc}; use std::marker::{Send, Sync}; use std::cell::RefCell; pub struct Monitor { receiver: Arc&lt;Mutex&lt;mpsc::Receiver&lt;i32&gt;&gt;&gt;, sender: mpsc::Sender&lt;i32&gt;, listeners: RefCell&lt;Arc&lt;Vec&lt;Box&lt;Fn(i32) + Send + Sync + 'static&gt;&gt;&gt;&gt; } impl Monitor { pub fn new() -&gt; Monitor { let (tx, rx) = mpsc::channel(); Monitor { receiver: Arc::new(Mutex::new(rx)), sender: tx, listeners: RefCell::new(Arc::new(Vec::new())) } } pub fn start(&amp;self) -&gt; thread::JoinHandle&lt;()&gt; { let rc = self.receiver.clone(); let cbs = self.listeners.borrow_mut().clone(); let handle = thread::spawn(move || { loop { { let value = rc.lock().unwrap().recv(); for i in 0..cbs.len() { cbs[i](value.unwrap()); } } thread::sleep_ms(100); } }); handle } pub fn send(&amp;self, val: i32) { self.sender.send(val); } pub fn addListener(&amp;mut self, cb: Box&lt;Fn(i32) + Send + Sync + 'static&gt;) { let mut c = &amp;self.listeners; c.borrow_mut().push(cb); } } My 2nd attempt at rust, I gave up 6 months ago whilst fighting the borrow checker and I'm determined not to give up this time. I'm trying to move ownership of closures to another thread and I think I'm almost getting there. I'm not sure if the CellRef is necessary. c.borrow_mut().push(cb) gives a compilation error: cannot borrow immutable borrowed content as mutable any help would be appreciated https://play.rust-lang.org/?gist=edcc0786e87fd97f6e1e8df1e77cc413&amp;version=stable&amp;backtrace=0
I know most of the folk working on this project, so happy to field any questions or pass them along.
I think you are on to something here. The code did not break on my 64 Bit Windows, but it did on the 64 Bit Linux used by travis. When writing the ffi crate I used [the headers supplied by Microsoft](https://github.com/Microsoft/ODBC-Specification/blob/master/Windows/inc/sqltypes.h#L63) as a guideline. Yet then I look at the analog unix ODBC headers I did find [this comment](https://fossies.org/dox/unixODBC-2.3.4/sqltypes_8h_source.html) indicating that the author hopes 64Bit SQLLEN is safe on unix now. I think I'll patch the SQLLEN length and see what happens.
Yeah, this is exactly what had happened.
Yes, declaring CPU as mutable is necessary. Rust is pretty pedantic about this, so that anyone reading the code knows that the value in `cpu` might change. This matters for the programmer. For example, reordering operations that only operate on immutable data is safe, but the instant any of them can mutate the data, this becomes a problem.
Could the lazy evaluation in Haskell ever have a runtime expression optimizer like this?
There are loads of Rust acronyms that you forgot, such as `Rc`, `Arc`, etc :P
You should try out error chain! I think it'll help with standardizing your Results between crates.
Playing around with a futures-based GraphQL server implementation... 
Looks nice! Haven't used the library (yet), but in the docs I'd like to see the following answered: * How/when does Updater::start terminate? Is there any way to make it quit from outside its thread? * Command handlers as free functions are simple, but that's a simplistic example: in practice, you'd like to share some struct between handlers. How to do that? * (minor) Why does the token have to be an owned String? * (minor) Is `Option&lt;Vec&lt;&amp;str&gt;&gt;` for arguments necessary? (i.e., is there a semantic difference between `None` and empty `Vec`?)
Mutability can't just be cared for in the internals. `load_rom` changes the CPU, and most likely future calls will do so too (changing the memory, for example). This fact is visible to the outside using `&amp;mut self` arguments, which is critical to keep Rust's memory safety guarantees. For example, if you could call `load_rom` or `execute_code` with a non-mutable reference, Rust would allow you to do so at once from many threads, which leads to chaos. Python doesn't give you this guarantee. CPython, due to the fact that bytecode execution is essentially single-threaded, at least won't crash, but the results would be undefined and quite likely infinitely looping. In systems languages that don't have builtin synchronization or Rust's guarantees, concurrent modification is a big cause of crashes. BTW, looks like you could use [`copy_from_slice`](https://doc.rust-lang.org/std/primitive.slice.html#method.copy_from_slice) here.
OK, thanks. Didn't know that would work.
Well, what do you say, I'm still working on the [LLVM 4.0 upgrade](https://github.com/rust-lang/rust/pull/40123). However things are starting to shape up with [several](https://github.com/rust-lang/rust/pull/40617) [PRs](https://github.com/rust-lang/rust/pull/40612) [sent](https://github.com/rust-lang/rust/pull/40581) in preparation of the upgrade and even [a patch](https://reviews.llvm.org/D31116) being submitted to LLVM.
The pages are all 4 KiB? What happens when you store values larger than that? I happen to have a table with values that average just over 4 KiB, so I've been looking into how different databases deal with this. LMDB apparently uses overflow pages, where each overflow page is dedicated to a single value, so a lot of space ends up wasted. I'm using SQLite now. It has a chain of overflow pages for all the values in one leaf BTree entry, so it's space-efficient but needs a lot of seeks. It also lets you change the page size, so I just use 16 KiB pages. LevelDB/RocksDB also let you change your block size, and (given that they're based on ststables+logs rather than B-Trees) don't need to pad anything to the block size. The block size basically just controls how much stuff gets compressed together. Kyoto Cabinet...not sure. Haven't found anything that talks about this, and I haven't tried it out yet. I'm a little curious about it; this is the only B-Tree-based database I've found that supports compression, and I'm not sure how that works.
I have the following piece of code for y in 0..lines { threads.push(thread::spawn(move || { Message::text(grab_img(0,y*lh,width,lh)) })); } for t in threads { let msg = t.join().unwrap(); client.send_message(&amp;msg).unwrap(); } The idea is to process some data, and return a Message when it's ready. This implementation works well, but I'd like to send the message immediately when a thread is over, rather than waiting as in queue for other threads. Is there a way to do this?
No problem :)
Does anyone have any good resources to learn file IO? I looked through the Rust book, FAQ, and Example section on it, but was hoping for something more in depth.
It works for basically everything in the Cortex M family. (https://github.com/japaric/copper) Currently there is a lot of development for the stm32f3discovery with https://github.com/japaric/f3 Nice to see you here :)
Thank you so much! I really appreciate the quick feedback! 
&gt; BTW, looks like you could use copy_from_slice here. Alternatively, read-ing directly into self.mem would usually work (even without looping around), especially as "base" chip8 ROMs can't be more than 3.5k and are often significantly smaller (many roms are in the hundred-bytes range) Which incidentally is a mistake of yours /u/PatMcCrackit, roms should normally be loaded at 0x200 ;) And I'd recommend hex constants, 
After we remove the last C++ code, it will probably be easier to support VS 2013. Until then 2015 is required. Personally I think 2015 is much, much better anyway.
After reading your and other people's comments on the multitude of similar threads I think I will start with python. I was recently learning Java but have gone back to Linux and python just fits better there. I will eventually head back to. A C like language such as rust in the future however since python doesn't really fit what I want to accomplish in the long run. If nothing else I may try to learn concurrently as in "ok I can do this in python now, let's see how I would make it work using rust" it we'll see. 
I did just verify that it does build when VS2015 is installed.
F# is a good compromise between the two IMO &amp;ndash; strongly and statically typed with decent type-inference, light syntax, and (obviously) a very comprehensive standard library.
You lose the light syntax and comprehensive standard library. Keep in mind we're talking about a language to learn programming with. And let's not forget tooling; VSCode+Ionide is pretty great for learning with, since it shows you the inferred types of everything inline and automatically.
I've been working on yet another Parser Combinator library. I've written dozens of prototypes in the last 2 years (since `nom` came out) and finally settled on an API I really like. There is only one macro in the library and it's rarely used. There are only 3 operators I've overloaded (`|`, `&gt;&gt;`, `&lt;&lt;`) as they really make writing parsers elegant. Here's an INI parser I ported from [this nom version](https://github.com/Geal/nom/blob/db94701564aae254b494e1769993b9ab2ec56685/tests/ini_str.rs#L9-L89): // https://github.com/utkarshkukreti/munch.rs/blob/62907a57d0d598bd5570c4726bbd0251bd66152f/examples/ini.rs pub type Value&lt;'a&gt; = HashMap&lt;&amp;'a str, HashMap&lt;&amp;'a str, &amp;'a str&gt;&gt;; pub fn parse(str: &amp;str) -&gt; Result&lt;Value, (usize, munch::str::Error&lt;'static&gt;)&gt; { use munch::str::*; let s = || P(TakeWhile(|ch| ch == ' ' || ch == '\t')); let ws = || P(TakeWhile(char::is_whitespace)); let header = P('[') &gt;&gt; TakeWhile1(|ch| ch != ']') &lt;&lt; ']' &lt;&lt; ws(); let comment = P(';') &gt;&gt; TakeWhile(|ch| ch != '\n') &lt;&lt; ws(); let key = P(TakeWhile1(char::is_alphanumeric)); let value = P(TakeWhile(|ch| ch != '\n' &amp;&amp; ch != ';')); let kv = (key &lt;&lt; s() &lt;&lt; '=' &lt;&lt; s(), value &lt;&lt; s() &lt;&lt; Optional(comment) &lt;&lt; ws()); let kvs = kv.repeat(..).map(|vec| vec.into_iter().collect()); let section = (header, kvs); let sections = section.repeat(..).map(|vec| vec.into_iter().collect()); match (ws() &gt;&gt; sections &lt;&lt; End).parse(str, 0) { Ok((_, output)) =&gt; Ok(output), Err((from, error)) =&gt; Err((from, error)), } } In my benchmarks my parser is about 50% faster than the nom one after [this PR](https://github.com/Geal/nom/pull/461) applied. My version does a bit more work in some places, e.g. it checks for Unicode `is_alphabetic` while the current nom one only does that for ASCII, and less in other, e.g. the error payload is most likely smaller in my version right now. I'm sure the nom version still has room for optimization. There is no documentation yet (coming soon!) but every line of code is heavily tested. Here's the repo: https://github.com/utkarshkukreti/munch.rs and some example parsers: https://github.com/utkarshkukreti/munch.rs/tree/master/examples if anyone wants to try it out or has any feedback!
Is it normal that when I add the bot to a group chat, all the messages I receive have no user ID ? Is this a privacy thing Telegram does ?
it works fine though output js is a little bit large , about 1mb.
And [here](https://github.com/utkarshkukreti/munch.rs/blob/62907a57d0d598bd5570c4726bbd0251bd66152f/examples/arithmetic.rs) is a reimplementation of the Arithmetic parser from [Nom's module documentation](http://rust.unhandledexpression.com/nom/#example), with added support for negative integers.
what exactly is is that confuses you about file io?
yes, this is possible by using something future-like (an abstraction over a computation that might or might not be done yet) and something select-like (allows you to wait until one out of a set of things made progress)
Nothing in particular. I'd really just like to see a bunch of examples of best practice Rust code working with files or various types. I'm coming from Python where if `with open` isn't what you need there's a module for that.
Added quantity descriptions, unit abbreviations and descriptions, and even a bunch of [non-si units](https://github.com/iliekturtles/uom/commit/d1b969b810a4b3298d4cf93d0a795d922261735b#diff-86b1cf98ca31184ca5fd0033df427f38R39) to [uom](https://github.com/iliekturtles/uom). I know everyone is excited to start measuring length in chains. This week I'm trying to find time to some tests together.
I've looked into it, and it's an [issue](https://github.com/voider1/teleborg/issues/8). Thanks for bringing it up! I will try to solve it as fast as possible. Edit: Solved it! I have pushed a new version to crates.io. =)
I know that "Go vs Rust" has been written about before. The perspective I wanted to take with this post was to examine specific ways I think Go could improve.
I haven‚Äôt changed the file, probably it is true that rust trips up Emscripten.
Dense but with a lot of seeks, then. Thanks. Hmm...I'd prefer a larger page size, but that might be good enough. I'll likely eventually migrate away from SQLite (too much CPU overhead for the SQL layer) and will look at Sanakirja. Compression that I can implement myself on top would have to be value-by-value compression. That isn't real interesting to me. RocksDB and Kyoto Cabinet compress multiple values together and get a much better compression ratio as a result. I don't know how this would work for a B-Tree database (that is, how to achieve a fixed block size post-compression with not too much waste) but apparently Kyoto Cabinet has a way.
I released a few versions of [relm](https://github.com/antoyo/relm), an asynchronous GUI library based on GTK+ and tokio, inspired by Elm (now more than even :p). You can find it on [crates.io](https://crates.io/crates/relm). **If you intend to use nigthly, it might be better to wait for a next release before using relm, because I plan to add some attributes to make it easier to use.** Last week: * I added a readme (finally) with a quick start guide. * I added the error handling (a message can now be sent when there is an error in a `Future` or `Stream`). * I updated the library to run the `tokio` event loop in another thread (which was a big structural change). * I improved the usability (while it was somehow degraded by the fact that relm now uses 2 threads). This week, I plan to add some attributes to make it more declarative and easier to use. You can now try this library. Look at the [readme](https://github.com/antoyo/relm/blob/master/README.adoc), the [examples](https://github.com/antoyo/relm/tree/master/examples) and the incomplete [documentation](https://docs.rs/relm/). Any feedback is welcomed!
Hey, I'm going to respond to your questions here and I'll add them to the docs later. * It doesn't terminate until the user ctrl-c's or something akin. I was struggling with how to make a built-in function for quitting the bot. For now I've decided to not have one, and maybe later when I or someone else has figured out something it gets implemented. * I haven't looked into this yet tbh! If you have any ideas I'd appreciate your input. I will look into this shortly too. * I haven't really given this thought except for "no lifetimes". Should it be an `&amp;str`? * Yes, I've chosen for this on purpose. A `Command` which takes no arguments by default always gets None, but a `Command` which takes arguments, but gets none will get an empty `Vec`. Thank you for your questions and input, I enjoy answering them and thinking about how I could do things better or another way. If you have more feedback/questions/etc, I'd love to hear it!
Oh, a trick! But, alas, no cigar. While this works for [singly-borrowed data](https://is.gd/WGKhiw), it still fails for [non-borrowed data](https://is.gd/71nMS4) and [doubly-borrowed data](https://is.gd/oI7WKy). As such, it doesn't really change anything.
Thanks! I added that link.
You forgot to post a link.
Alright, I've just built https://pijul.org/pijul.exe, but on a clean windows install, it complains about a missing DLL: vcruntime140.dll. What's the standard way to fix that on windows? I guess I don't have the right to distribute the DLL, right?
It looks like you're just supposed to read the raw markdown files, right? Just making sure.
Have you seen these? http://rustbyexample.com/std_misc/file/open.html http://rustbyexample.com/std_misc/file/create.html Like Python, a `File` object closes itself in its destructor. But unlike Python, its safe to rely on that destructor getting called at the end of a function. That's a first class feature of Rust, and not an implementation detail like in CPython. Though if you want to explicitly close a file you can use `drop`.
Cool. Good to hear and once again, thanks for all the good work.
Offtopic: Your rustlings repo was a great help to get kickstarted!
&lt;3
Would this work for you? https://play.rust-lang.org/?gist=317ab565e5c6d2fff359d7cca297660d
&gt; How easy is it to transform an unknown JSON structure? Do you have to define the whole schema in order to derive Serde functions? Serde will ignore unknown attributes by default, so say your json has "x" and "y" keys, but you only define "x", it will parse it no problem. You can turn on a "strict" mode that will make you do all of them if you want. &gt; In native Rust structs, how easy is it to drill into data like obj.prop1.prop1a.thisThing.that. Is there a null-aware way to do this like in Ruby? Rust doesn't have nulls, so in some sense, you can do just that. However, if these are Option types, there's a variety of things you can do, it just depends. It's not technically in the language, but in libraries, so there's no syntax at the moment to do this; there are methods though.
I've recently written an ETL layer in Rust. Or rather, more of just an extract layer. Some thoughts: - Mine was parsing a custom format rather than JSON. Python is probably more useful for standard formats as it can defer to libraries written in C. But for my use Rust was a LOT faster than python (100x, easily). - Arbitrary JSON can be transformed using `serde_json`s `JSON::Value` type which has serde functions derived. http://kbknapp.github.io/doapi-rs/docs/serde/json/value/enum.Value.html - There is no `null` in Rust. Using JSON::Value, you could do obj.find_path(&amp;["prop1", "prop1a", "thisThing", "that"]). Otherwise you will need a custom struct with lots of nested Option types. - Using enums and Rust's static types catches a lot of business logic errors, which is pretty nice for ETL as it means a lot of stuff gets caught at compile time rather than halfway through a run through gigabytes of data. - As far as I know, there isn't a dataframe crate. But I imagine you could handle this pretty efficiently in Rust using an array of structs (where each struct represents a row, and each field of the struct represents a column).
Haha, I love it! An excellent example of [the fundamental theorem of software engineering](https://en.wikipedia.org/wiki/Fundamental_theorem_of_software_engineering): &gt; We can solve any problem by introducing an extra level of indirection I think this idea solves a different problem I am actually having which has to do with how `use` works/fails to work in different cases. However, it still runs afoul of the naming conflict I express concern about at the end of my post. See my [example conflict on the playground](https://is.gd/a4vBff). 
How to implement the `From` trait for a `struct` holding a private value `T`? It works for `String` but not for `Vec&lt;T&gt;`. Why is this the case and how to implement it, if it is possible. Code: [play.rust-lang](https://is.gd/ZFiGA3)
Awesome!!!! &lt;3
&gt; Some older languages that make heavy use of channels (e.g., Erlang and Scala) support tagged-union types in combination with pattern matching Erlang does not have statically checked "tagged unions" in the same way that ML languages do. You can fake it with tuples and typespecs, though.
I have written a messy, but stable [ETL tool](https://github.com/rrichardson/lout). It is meant to replace logstash, it can route from inputs to outputs, where both are sort-of pluggable. The bits of it that are relevant to ETL are [this module](https://github.com/rrichardson/lout/blob/master/src/output/postgres.rs), which leverages a schema defined in json to extract data from inbound events and put them into a postgres data warehouse. It leverages serde_json and Rust CSV extensively to perform it's duties. Using the schema-driven extraction approach, you can gain flexibility and also tolerate poorly formatted events (as long as they're correct JSON ) Regarding dataframes, [Rust CSV](https://github.com/BurntSushi/rust-csv) has some indexing features which allow it to offer quite a few features that are similar to dataframes. If you need to load up large datasets and then perform aggregations and joins against them. I'd recommend starting there. 
Pivoting a little bit from work related to the [openai crate](https://crates.io/crates/openai) and related AI. Trying to keep track with upstream OpenAI projects. They started doing NLP with AIs in a blockworld type setting, though they haven't open-sourced that yet so I'm building [my own block-world game](https://github.com/andrew-lucker/RYB). Also, trying to make it fun to play as an RPG video-game.
Just because an implementer of the trait needs to specify the associated type doesn't necessarily mean they have access to a value of that type. So if you want ensure they do have such a value you'll need to write a method they're required to implement in order to access that value, e.g. fn inner(self) -&gt; Self::Inner; // or fn inner(&amp;self) -&gt; &amp;Self::Inner; At some point we might get the ability to have "abstract fields" so that you could call `self.inner` and know it's backed by value, but for now you need the method.
I don't think a dataframe structure would be all that useful in a language like Rust, since it's compiled and all.
Thanks for those suggestions! And I'm glad that you liked the post! Yes, the paragraph of me describing my uncertainty about race conditions is in there partly to illustrate that not being able to abstract those details is burdensome. But I was genuinely uncertain about whether indexed slice access is thread-safe. So I appreciate your impromptu code review! I have the function returning one non-deterministic error because I initially wanted to mimic the behavior of the Rust version, where the function returns immediately on the first error that is encountered. But I ended up spending more time than I wanted to on that function, so I never got around to implementing the early return. Since the function is going to wait for all results anyway, you make a good point that collecting all of the errors is a good idea. &gt; This is correct, but Rust does support variadic macros, and so you could use a make!() macro to dispatch those function calls to the underlying Make::make() and Make::make_with_capacity() methods, if so desired. That is very interesting! I may make that change!
Whoops! Thanks for that catch!
To be fair, I probably should have said "correct" instead of "safe". Rust doesn't consider it unsafe to leak memory (though it's usually incorrect). One of the downsides of destructors in the GC world is that really tricky things can happen when your program is exiting, as all the globals are getting cleaned up in a difficult-to-predict order. It might not be safe to call library functions in a destructor, for example, if that library's globals get finalized before yours do. As far as I know, Rust makes this sort of issue impossible.
I earn my bread working with Java. To learn something new I wanted a fresh idea and a new language. So I've picked Go. ‚Ä¶ and disliked it very fast. I wanted something safer, not something that takes away new features from Java 8. I really couldn't find anything good about nil, mutability by default, lack of generics or lack of declarative code structure. Then I picked Scala which I liked very much but lost interest after too many hours spent struggling with Play! poor documentation. Then I choose Rust and now I'm spoiled enough to feel how I could rewrite my Java code at work to something more readable and safe.
Ah! I should have looked at the source rather than going by the docs.
That may be. But on my work machine, I don't have a choice...
 use std::thread; use std::rc::Rc; use std::sync::{Arc, Mutex, mpsc}; use std::marker::{Send, Sync}; use std::cell::RefCell; pub struct Monitor { receiver: Arc&lt;Mutex&lt;mpsc::Receiver&lt;i32&gt;&gt;&gt;, sender: mpsc::Sender&lt;i32&gt;, listeners: RefCell&lt;Arc&lt;Vec&lt;Box&lt;Fn(i32) + Send + Sync + 'static&gt;&gt;&gt;&gt; } impl Monitor { pub fn new() -&gt; Monitor { let (tx, rx) = mpsc::channel(); Monitor { receiver: Arc::new(Mutex::new(rx)), sender: tx, listeners: RefCell::new(Arc::new(Vec::new())) } } pub fn start(&amp;self) -&gt; thread::JoinHandle&lt;()&gt; { let rc = self.receiver.clone(); let cbs = self.listeners.borrow_mut().clone(); let handle = thread::spawn(move || { loop { { let value = rc.lock().unwrap().recv(); for i in 0..cbs.len() { cbs[i](value.unwrap()); } } thread::sleep_ms(100); } }); handle } pub fn send(&amp;self, val: i32) { self.sender.send(val); } pub fn addListener(&amp;mut self, cb: Box&lt;Fn(i32) + Send + Sync + 'static&gt;) { let mut c = &amp;self.listeners; c.borrow_mut().push(cb); } } My 2nd attempt at rust, I gave up 6 months ago whilst fighting the borrow checker and I'm determined not to give up this time. I'm trying to move ownership of closures to another thread and I think I'm almost getting there. I'm not sure if the CellRef is necessary. c.borrow_mut().push(cb) gives a compilation error: cannot borrow immutable borrowed content as mutable any help would be appreciated https://play.rust-lang.org/?gist=edcc0786e87fd97f6e1e8df1e77cc413&amp;version=stable&amp;backtrace=0
After starting the work in [waltz](https://github.com/killercup/waltz) again, I wanted a nice crate to test CLIs‚Ä¶ but my old [assert_cli](https://github.com/killercup/assert_cli) didn't cut it. So I'm currently rewriting that as well üòÑ *Edit:* Released waltz 0.2.1, waltz_cli 0.1, and assert_cli 0.4! Have a look at [these](https://github.com/killercup/waltz/blob/4076fac12590da28a8860cd21b72dca45b4f4dba/waltz_cli/tests/simple.rs) nice tests!
When I tried this strategy it *decreased* performance. Maybe there was something wrong in my implementation of it.
I've been working on integrating [rustls](https://github.com/ctz/rustls) with [tokio](https://tokio.rs/) (And it works!). I'm going to get it into a presentable state this week.
I'm of the same opinion... I would actually rather write Java 8 then Go in most cases. The most annoying thing about Java 8 right now is its patchy library support for new features: lots of popular libraries are still architected in a "pre-generics" style, and library support for functional programming and completable futures and such is patchy at best. (With the way lots of crates have embraced tokio, Rust is rapidly surpassing Java in some ways, though I am still a bit worried about backpressured streaming). But Go can't ever have most of those things, by design, due to its lack of generics.
You probably want Result::and_then. let handshake = socket.recv_bytes(0) .and_then(String::from_utf8)?;
And if they do something to address lack of generics you will get patched language like Java 5 or transition nightmare like with Python 3.0.
Unfortunately, `and_then` requires both errors to be of the same type, which is not the case here.
Yeah `Option&lt;T&gt;` is a `(ssize_t, ssize_t)` which might as well be a `(void *, void *)`. In which can you can return a `(null, void *)` which would be a `(nil, data)` or a `(1,null)` which is `(!= nil, ==nil)`. Either one of which is better represented by `Option::None`. 
I just tested it. It's unlikely we'll go back to supporting VS2013 because VS2013 has limitations regarding UTF-8 support, and some of the C code we inherited from BoringSSL has non-ASCII UTF-8-encoded symbols.
Or to be convertible using Into.
It's more like this: pub enum EmptyInterface { Nil, Func(Func), Ptr(Ptr), Chan(Chan), Slice(Slice), Map(Map), String(String), Int(isize), SomeUserStruct(SomeUserStruct), } pub enum Func&lt;R, T...&gt; { Nil, Func(fn (T...) -&gt; R), } pub enum Ptr&lt;T&gt; { Nil, Ptr(Gc&lt;T&gt;), } pub enum Chan&lt;T&gt; { Nil, Chan(mpmc::Channel&lt;T&gt;), } pub enum Slice&lt;T&gt; { Nil, Slice(Gc&lt;[T]&gt;), } pub enum Map&lt;K, V&gt; { Nil, Map(Gc&lt;HashMap&lt;K, V&gt;&gt;), } pub struct SomeUserStruct { field1: Type1, ... } This means the interface itself can be non-`nil`, while the underlying value is the zero value. In many cases, the zero value is also called `nil`, but `Func::Nil` is not the same as `EmptyInterface::Nil`.
If you want to try Scala again I really recommend a stack of http4s, Doobie, and Argonaut/Circe. Play benefits very little from scalas power but makes you pay the language price anyway
Use `AsRef::as_ref` inside the call to `into_iter`. This is actually even more generic than you asked for, but only in ways that seems relevant anyway. EDIT: Never mind; this doesn't work because of type resolution ambiguity. :/
Just built my first Rust project: [StackParam](https://github.com/cretz/stackparam). It injects method parameters in Java stack traces. The code is probably ugly and I spent too much time in unsafe code to get a great grasp of Rust, but it worked great for my use case. Feedback welcome!
I don't have anything to add, but I just want to say thanks for writing the book! I've purchased the early access edition and despite it still missing some chapters, I already find it incredibly useful! Can't wait to read the final chapters :).
Since we got rid of `~` `&amp;mut` should be called an owning reference. Incidentally, it occurs to me that something of the following should be possible with nonscoped threads and nonsharable data. fn abort_and_send(chan : Chan&lt;T&gt;, x : &amp;mut T) -&gt; ! { chan.send(transmute(x.swap(uninitialized())); abort() }
It seems to me that the problem is unrelated to networking... The string produced by echo is "100\n", while the string in your client is just "100". That is, your client is missing the newline character! I guess your server is using something like `read_line`, which will block until a `\n` is received (in this case, blocking forever :D). You should try the following: `out_buffer.write_all("100\n".as_bytes());`
Thanks, it's working fine now!
If I use Rust on Windows w/ the MSVC toolchain, and I build a DLL, can that be distributed standalone for use w/ the more modern Windows versions? Or do I have to distribute a VC runtime?
&gt; Your c reference in addListener is a shared reference, but you're trying to call borrow_mut with it, which requires a mutable reference [`RefCell::borrow_mut`](https://doc.rust-lang.org/std/cell/struct.RefCell.html#method.borrow_mut) accepts `&amp;self`
Are you targetting just Linux or also other platforms? Edit: Okay you mention a x-platform spec, so I guess crossplatform. If for Windows, I'm interested in what you're going to do for a TUN/TAP driver, e.g. use OpenVPN tap-windows or WinDivert approach or something else. Windows seems especially problematic here.
Oh wow, yep, I'm an idiot :-D I misinterpreted the cause of the `cannot borrow immutable borrowed content as mutable` error. It's not the `borrow_mut` method that's the problem, it's the `push` method, because you can't call that on an `Arc&lt;Vec&lt;_&gt;&gt;`. The `Arc` can't give you a mutable reference. 
Erlang doesn't make heavy use of channels. Each process can receive messages and block waiting for messages but there's no first class channels.
I've got a c library that I'd like to call from Rust but the c lib requires a couple of init setup calls to start. I'm not totally clear on how to approach this (first time binding foreign code from Rust), from a quick search it looks like it would be easier just to write a wrapper in c and then call that from Rust but I'm curious to know what the recommend approach would be. #define VARA 123 #define VARB 456 extern custom_type VARB (another_type_struct*); struct_foo ifoo; init(&amp;ifoo); ifoo.version = VARA ifoo.config = VARB Sorry for the pseudocode I dont have the real thing in front of me but I remember the structure well enough. 
I translated a prolog interpreter from ocaml to rust. You might find my code useful as an example of what to do (or not do!): https://github.com/dagit/rust-prolog
No, what I said is accurate. [Click here for proof](https://is.gd/kwQx5X).
I don't think the `#[deprecated]` attributes do anything on impls, but at least the module being deprecated will show up in docs. I may add a `warn!()` message to the impl so it shows up in logging as well. Addendum: this will be published as `0.10.2`
When did it change? I swear it was opposite before.
I'm pretty sure it's always been this way. Otherwise, how could you call the struct's version when the trait is in scope? Edit: You may be thinking of generic code? Continuing the example I started, fn test&lt;T: Trait&gt;(x: T) { x.print_me(); } will use the trait's version, even when `x` has type `S`.
It's a useful abstraction. Under the hood it could really just be an array of structs. But the dataframe provides useful tools that are commonly needed for manipulating that data structure.
For those of you like me who are curious to know what happened to /u/munificent's "little language" that they described in their blog post, it appears that although magpie hasn't been updated recently, there is a more recent language of theirs named [wren](https://github.com/munificent/wren) that appears to be actively maintained.
tcp is a stream protocol, so what gets exchanged on connections is unstructured streams of bytes. it seems to me like a datagram protocol, where messages get exchanged would fit your use case much better.
At [Faraday](http://www.faraday.io/), we're working on a new ETL system that involves a lot of Rust. We're building it around the [Pachyderm cluster scheduler / data lake](https://www.pachyderm.io/), which handles getting data onto the correct cluster nodes, and which allows us to do all our processing inside Docker containers. We read files from `/pfs/$INPUT_REPO` and write to `/pfs/out`, and Pachyderm takes care of map/reduce and data management. Pachyderm's sweet, and they're going to be releasing some improved documentation soon, which will make it much easier to deploy a production cluster to AWS. (Hint: Use `kops`.) On the Rust end of things, we make heavy use of [`scrubcsv`](https://github.com/faradayio/scrubcsv) to clean input CSV data, and we use /u/BurntSushi's [`xsv`](https://github.com/BurntSushi/xsv) for basic transforms and analysis. Typically, tools based on the Rust `csv` crate can handle about 75 MB/sec per core on typical machines. We're also using [`szip`](https://github.com/BurntSushi/rust-snappy) to compress intermediate working data. If we need to load a lot of data into PostgreSQL using Rust, we've gotten better performance from `diesel` than the `postgres` crate, even if `diesel` does have some pretty terrifying compile-time error messages. `serde`'s cool, though we use it more for configuration files and other little stuff, and less for full-scale ETL, where we do our best to touch the raw data as little as possible. &gt; How does it compare with Python ETL scripting? Rust is very, very fast. This counts for a lot once we're paying for cluster time. :-) &gt; How easy is it to transform an unknown JSON structure? Do you have to define the whole schema in order to `derive` Serde functions? You can declare certain fields of your structure as `serde_json::Value`, which can store anything. It's harder to deal with a single object that has a mix of known and unknown keys unless you're ready to throw the unknown keys away. &gt; Is there a dataframe crate for managing huge tables of data efficiently? I don't know. Our files are too big to fit into RAM, so we assume that higher-level libraries are less useful for our use case. &gt; In native Rust structs, how easy is it to drill into data like `obj.prop1.prop1a.thisThing.that`. Is there a null-aware way to do this like in Ruby? You probably want something like: obj.and_then(|o| o.prop1) .and_then(|p1| p1.prop1a) .and_then(|p1a| p1a.thisThing) ...possibly with some `ref` or `.as_ref()` scattered around appropriately.
IMO concurrency is just as easy, if not easier in Rust. Need some parallelism? Slap Rayon on it with just one line of code (plus imports). And as a bonus, you don't have to worry about race conditions and undefined behavior like you do with Go.
You could also make a method that takes an Option for capacity. Seperate methods is more idiomatic though.
nil has no type of its own, it is a keyword which magically coerces to the zero value of any type.
Agree with this very much. The typelevel stack feels like a better use of Scala to me. Finch with Circe and Doobie is also really great.
What's the "billion-dollar mistake"?
It's a quote from [Tony Hoare](https://en.wikipedia.org/wiki/Tony_Hoare): &gt; I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years. I think he's probably right ;_;
Developing a tiny Lisp implementation using `Rc` to avoid coping nodes for practice https://github.com/komamitsu/tiny-rust-lisp. I benchmarked it with a Fibonacci number function. But it's slower than a Fibonacci number function written in Ruby... Maybe I should use arena allocations to improve the performance (throughput) of it.
Is it intentional that you mention a lot of different languages at the very beginning? On the Basics page, you have (mostly brief) comparisons with C, C++, Java, JavaScript, Python, Go and Haskell for many little things. IMHO, if your target audience is someone who finds the Book too hard/intimidating then this might be a turn-off.
 extern crate url; use url::Url; use std::collections::HashMap; fn main() { let url = Url::parse("a://b?option1=false").unwrap(); let queries: HashMap&lt;_, _&gt; = url.query_pairs().collect(); let option1 = match queries.get("option1").map(|s| &amp;s[..]) { Some("no") | Some("false") | Some("0") =&gt; false, _ =&gt; true, }; println!("{:?} -&gt; {}", url, option1); } (For your information, [alternative Rust Playground](http://play.integer32.com/) supports common crates and you can directly paste above code there.) Probably one of the confusion you've got is why `queries.get("option1")` does not give *something* resembling a string. In Rust most strings are typed according to their lifetimes, e.g. `String` for owned, `&amp;str` for borrowed, and (in this case) `Cow&lt;'a, str&gt;` for optionally borrowed (common when the library does not want to pay the allocation cost). A catch is that while the conversion (to borrowed) is mostly automatic when the type is used standalone, it's not if the type is a part of larger structures (in this case an `Iterator` returning two `Cow&lt;'a, str&gt;`s). In this case you need to convert to a common type: `.map(|s| &amp;s[..])` is a quick way (you may be heard of `.map(|s| &amp;*s)` or similar, but this may not work if `s` is a double indirection [1]). Using `vec![...].contains(...)` is not recommended too; a pattern match pretty much covers all such cases and much harder to get wrong. I personally hope that it actively dereferences `Cow&lt;'a, str&gt;` so that a plain `queries.get("option1")` can be directly `match`ed against patterns, but currently it isn't. [1] This is because... `Option&lt;T&gt;` is also a structure. So comparing `Option&lt;T&gt;` against `Option&lt;U&gt;` with different but convertible types `T` and `U` is not going to work well. There are several technical arguments, but I cannot deny it's cumbersome ;)
Offical solution on route :) https://github.com/golang/dep
Please try https://github.com/redox-os/redox/releases/tag/0.1.3 and see if the problem persists
What's the intended use of an OS for microcontrollers? I'm no expert, but I thought that the big use case for microcontrollers was their real-time nature, which would seem to preclude an OS.
I believe this would be counter-productive, by annoying people enough that they opt to either close the issue prematurely (which is undesirable) or to have everyone on that issue simply unsubscribe from receiving notifications (which makes it harder to actually fix the bug). Instead I think it would be better to identify the users who have filed the most open bugs, who are almost certainly members of the Rust community, and compel them all to sit down once a year and run through all their open issues to see which ones can be closed (and potentially provide updates on the ones which cannot yet be closed). For the remaining long tail of issues filed by drive-by contributors, I imagine that a casual email-based triage process could keep that under control.
Welcome to Reddit. In future, you may want to *check* what the subreddit you're posting to is about. - [Exhibit 1](http://imgur.com/xCPcX1e). - [Exhibit 2](http://imgur.com/gw3cMin).
Yes it should basically run on every operating system. Yeah I see the windows problem, but it should be doable anyhow. 
Something which could be useful would be encouraging test cases (possibly having triage add them) which a service could regularly run √† la CI, and if it finds the case succeeding post a notification to the tracker. Alternatively, "known failing" test cases in the project though I don't think the builtin harness has a concept of unexpected success. 
Have people discussed getting rid of the word let on variable bindings and just using := like go does when creating new names? Is there a strong reason it's there? 
Absolutely, test cases are essential to effective bug reporting, and the lack of them is what's most likely to get an issue closed outright. Having a mechanism to automatically run test cases contained in issue comments would probably be useful, though there's still plenty of issues that wouldn't benefit (e.g. the OP's example of #1563, which involves generating debuginfo that gdb can interpret).
Good point. My enthusiasm for programming languages could be a liability here. It's all about not _quite_ knowing the audience. Maybe I should tone the comparisons down a bit.
I just tested it on Hyper-V and it seems to work :)
In what ways do they differ from a typical OS in order to retain realtime guarantees?
The "billion-dollar mistake" and default mutability makes go(and most other statically typed languages) hard for me to reason normal sequential code let alone concurrent or parallel.
You just create a "reactor" (like the goroutine scheduler) and start feeding it tasks. That's... pretty much it actually. In Rust you can easily do: - concurrency (tokio stack): solving as many problems at the same time (throughput). You can specify arbitrary dependency graphs between the task, and let the reactor advance them as fast as possible, e.g., once this task finishes, start these other tasks, when the first one finishes, cancel the others and start some new tasks, if the whole thing does not complete it less than 100ms, cancel it and retry, ... - parallelism (rayon): solving one problem as fast as possible using all of the machine. The threads cooperate to split the problem in such a way, that the latency is low. Ideally you should combine both, e.g., using parallelism is just passing it a task to the reactor that includes a parallel computation, but this is "brand new" in rayon, so I don't know how well it works. Your Go 10 liner its ~11 LOC in Rust (one extra line for the reactor, and that's it). The difference between Go and Rust is that Go gives you one single reactor, while in Rust you can choose one. Do you have an HTTP server? Choose one that maximizes throughput, lots of job stealing, memory locality isn't important until you allocate a buffer, but from that point on you don't want to move the tasks between threads to avoid NUMA problems when accessing memory. Do you have a scientific application? Choose a reactor that minimizes latency, the tasks are kept thread local, the number of reactor threads is constant, threads are pinned to numa nodes, some job stealing but not too much, the reactor allows you to control the task granularity, ... So IMO the main difference between Rust and Go is that in Go you have only one scheduler with some knobs, but in Rust, _if you want or need to_, you can write an scheduler tailored to the problem you want to solve, and use that instead (code using futures is scheduler agnostic). 
That doesn't seem to get me anywhere, I haven't been able to write anything with Borrow that solves any of the above problems. According to [the book](https://doc.rust-lang.org/book/borrow-and-asref.html): &gt; Choose Borrow when you want to abstract over different kinds of borrowing, or when you‚Äôre building a data structure that treats owned and borrowed values in equivalent ways, such as hashing and comparison. &gt; &gt; Choose AsRef when you want to convert something to a reference directly, and you‚Äôre writing generic code. It seems like AsRef is appropriate, except I don't know which type I want a reference to, so it cannot work for types like Vec that implement AsRef for multiple reference types :/ Thanks anyway! :)
Getting my [HashMap PR](https://github.com/rust-lang/rust/pull/40561) merged Spend a few hours on my [sugary Riak clone](https://github.com/arthurprs/sucredb) 
Many firewalls/middleboxes block anything that isn't TCP or UDP, so if you want reliable transportation, you need to roll your own on top of UDP, which isn't a good idea ‚Ä¶
This could work too! My main point is to try to do something about this and not hide behind the fact that this is in normal thresholds: 3000 issues do look huge to me and I don't believe it is maintainable. Please do not take it wrong, I really admire the rust team for what they do and how they manage to do it so far! I'm merely trying to understand what could be done ...
&gt; Otherwise, how could you call the struct's version when the trait is in scope? `MyFtruct::foo()`, I guess. But maybe my memory just failed me.
Maybe you will need to create your own trait then (possibly with associated type).
The struct would have to be scoped in order for it to be safe because you could leak destructor. There was a time when scoped threads worked just like that (scoped to the join guard) but they were removed because they were found unsound. The way you could probably implement it would be writing `with` method for your struct. This would probably involve some `unsafe` code.
Seems like you're looking for /r/playrust. This is the subreddit for rust-lang, the programming language.
That's a shame. Is there any plan to bring this functionality back correctly?
Forgive me good sir
There are probably good reasons why you have decided not to do the following, so take this for what it's worth :) If you're creating said struct at the start of main, and it lives until the end of main, does this not mean that your thread should have the same lifetime? Could it be scoped inside of main?
My [relm library](https://github.com/antoyo/relm) is actually based on futures and tokio.
I suppose so but it would be much cleaner to have them created in the constructor. 
Functional = No side effects Reactive = Side effect hell Contradiction? What do you think? Edit: I am happy this is not supported and nobody better break core rust principles forcing it to. Edit2: I asked a question, please answer it before downvoting, or I will just go on knowing all the downvoters know I'm right :) - but seriously, I want to know why you think it's not a contradiction more than I care about internet points.
Working on my static site engine, I've reached a point where I was able to switch my site to it today and is only missing a couple of small things to replace all my Hugo usage. Hopefully it will be in a good shape for (alpha) release later this week!
if there are a lot of duplicated or fixed issues then some help with triage might be in order _if_ diagnostic issues are not wished for, which i don't think is consensus, as good compiler messages are incredibly helpful, especially, but not only if you are new to the language, then this too would call for more help with triage. if not then more help with implementing.
this is a great example of when making things implicit makes people trip up kinda hard from time to time. you seem to come from managed languages, so i would recommend you to get a good understanding of how borrowing, references and the stack and heap work. i would have explained it to you using your current problems as examples, but your "if you don't help me here i will leave for go!" attitudes does not really motivate me to do it, so just go and use google.
&gt; So IMO the main difference between Rust and Go is that in Go you have only one scheduler with some knobs, but in Rust, if you want or need to, you can write an scheduler tailored to the problem you want to solve, and use that instead (code using futures is scheduler agnostic). With Go, what the gp wants to do is shown in the getting started guide, then any beginner is able to do it. The main problem I have with Rust for this kind of use case, is that its quite difficult to find example about how to do it. Most of the time, all I found in Go vs Rust discussions are hand-wavy comments which explain that's as easy to do in Rust and you have better control etc. It would be really kind and helpful if you could provide an example of your simple ¬´~11 LOC¬ª (I don't mind if it's 30 !) program using rayon &amp; tokio altogether with the ¬´reactor¬ª pattern you talk about.
had you not tried to pressure people into helping you because otherwise rust will loose a user, there would have been no reason to even mention that you are about to leave for go in the op in the first place, eh? and that is exactly the point, no one is upset/cares about your choice of language.
Why does Rust lose a user because one task is better off in another language? Why do you see this as a competition between Go and Rust?
i never indicated that i see any competition between go and rust, in fact i explicitly said that i don't care. all i said is that you tried to pressure people into helping you using a pointless argument, and that this attitude is not something i can or want to support, easy.
Also I didn't pressure anyone. &gt;I wanted to give up and switch to Go for this little task, but I'm still curious is just me saying "I have evaluated I can probably better solve this problem in a managed language as it is merely a URL Handler, but curiosity got the better of me and instead I decided to ask here". It's an anecdote about how I felt at the time of writing, not some psychological Rust-Go-circlejerk.
I'll consider this option, since unnecessary or irrelevant details interrupt flow. What I'll do now is examine each comparison to other programming languages and see how relevant they are. I suppose _ultimately_ we're assuming that people 'already know how to program' before tackling Rust, and then people will assimilate new knowledge using the languages they're familiar with. It's an open question whether we can make it gentle enough for first-timers.
Interoperability with exisiting applications or libraries.
&gt;&gt; expected struct std::string::String, found str (pointing to "yes" string) &gt; &gt; okay uhm... aren't they basically the same thing for this purpose? okay, whatever, let's add an .into(): Unfortunately no, `String` and `&amp;str` aren't basically the same thing, and this is part of Rust's learning curve that we're trying to work on improving. I'd be interested to know if [the ownership chapter in the second edition of the book](http://rust-lang.github.io/book/second-edition/ch04-00-understanding-ownership.html) is helpful to you-- I think the slices section will help the most, but the previous sections in the chapter lead up to it. &gt; Also, I can't figure out a way to insert to_lowercase without breaking the whole thing... &gt; `match queries.get("option1").map(|s| s.to_lowercase().as_str())` &gt;&gt; temporary value dropped here while still borrowed The reason this doesn't work is that the `to_lowercase` method allocates a new `String`, since it's creating new data. Then with `as_str`, you try to get a reference to the newly allocated data, but the new data goes out of scope ate the end of the closure inside of `map`, so you can't return a reference to it. The version without `to_lowercase` works because the references are pointing to data owned by `queries`, which is still in scope. Then the problem in your examples and in the match is that you want to compare the `String` to string literals, which are `&amp;str`, and these are different types for the purposes of `contains` and `match`. However, `==` (aka `PartialEq`) is implemented on `String` to compare it with `&amp;str`, so you can use `==` to compare `String` and `&amp;str` values. For that reason, I'd probably write what you're trying to do [like this](http://play.integer32.com/?gist=48395f86fba61867b8bff988cec532e0&amp;version=stable): let option1 = queries.get("option1") .map(|s| s.to_lowercase()) .map(|s| !(s == "no" || s == "false" || s == "0") ) .unwrap_or(true); so it's more like you're transforming the parameter value into a boolean. Does that make sense? &gt; p.s. the pattern-matching solution is unfortunately not reusable if I check for multiple options. Do you mean you have multiple query parameters that all have the same logic about "no"/"false"/"0" meaning false, and everything else meaning true? If so, I'd recommend extracting whatever solution you go with into a function so that you can reuse that logic. If that's not what you mean, please clarify! 
`extern "C"` exists in both languages for this reason. 
quick question, why they didn't use Cargo build system instead of make/makefile ? Do they thinking about migrating to build.rs system in some point?
[OK :-)](http://benchmarksgame.alioth.debian.org/u64q/regexredux.html)
Most components are not precisely encapsulated and defined. Good C(++) interop means less places to introduce horrible bugs where Rust and C(++) disagree on the ABI of things.
You'd create a future for thing you want to do and execute it on a threadpool. I haven't tried it myself, but I expect it would take roughly the same amount of code, perhaps one or two lines more because you have to explicitly create a threadpool instead of relying on an implicit global pool. There's a library called tokio that's designed to handle stuff like this.
https://github.com/rkusa/graphql
Still helping with developing [smithay](https://github.com/vberger/smithay), just started [some libinput bindings](https://github.com/Drakulix/input.rs), if anybody wants to join, that will be necessary for smithay in the long run. And all of that to move my window manager [fireplace](https://github.com/Drakulix/fireplace) over to a *mostly* pure rust framework in the *far* future.
That's a lot less gory than I expected it to be. I've run into the same C++ interoperability issues myself, with others languages like Go and Nim. Both of those languages seem like pleasant languages to do game development in, until you realize you're heavily restricted in the number of actual frameworks you can use. This could go a long way to make Rust feel more like an all-purpose language and less of a niche language for masochists (I do love the pain though). 
Same here. We're still figuring out how best it'll be to put up a mirror for both of crates'.io and rustup's sources
no.
&gt; hard until someone makes it easy https://xkcd.com/1349/ maybe?
It's not necessarily an either-or! [npm](https://www.npmjs.com/) offers public hosting of open source packages as well as private packages, and they have an enterprise offering that runs within a company's firewall. The npm client can install packages from any of these.
&gt; That probably isn't very useful to know. On the contrary! We're a very small company right now, and if this problem isn't worth spending our time on because companies have already figured out solutions on their own, then we should be spending our time elsewhere :)
Looks to me like the Makefile is full of cargo calls and includes other Makefiles that have more cargo calls!
Exactly the same here. I would love to see Rust adopted and I'm trying to find places to get it in the door for evaluation. Here are (roughly) some of the requirements that drive archive management for us: * Ability to control permission to upload to certain repositories (usually LDAP/AD group based). We wouldn't have one big bucket for the whole company, each project would need its own repositories. Similar to the way Artifactory lets you have mirror and local repositories, and then combine them in any way through virtual repositories. * Ability to control the "release" of an in-development version of a library or product. There's a post-build vetting process, so something like 1.0-RC12 needs to be able to become 1.0 without doing a rebuild. * Be able to run the build in no-Internet zones, very common for us * Mirror Internet repositories so we don't abuse them by having teams of devs downloading the same crates ad nauseum
&gt; Is there a strong reason it's there? Remember that `let` does a whole lot more than bind simple variables: it supports full pattern destructuring: let Point { x, y } = p; Regular `=` does not. Part of this means that the grammar is simpler, which helps in a number of ways. I bet `:=` would enable the same simplicity, but it wouldn't be nearly as different for those reading the code, IMHO. Agree wholeheartedly with /u/DroidLogician's comments about sigils as well.
Excellent - this is exactly the kind of careful feedback every author needs. Good changes.
Some of the terminal commands like `head`, `less` does not work. Not sure if that is expected
How would it hide the cost? `Rc` doesn't implement `DerefMut`, the API to get an &amp;mut would be the same as it is today.
&gt; Rust 1.0 was basically the Death Star from Return of the Jedi: fully armed and operational, but none of the tiling is done in any of the bathrooms and sometimes when you go to flush you just get sucked out into space. That is begging to be a quote of the week. cc'ing /u/nagisa.
Wouldn't that be very difficult? I thought the C++ was a bit of unstable. So the rust compiler would have to be compatible with the C++ compiler you are using. Please correct me if I'm wrong. 
Usefulness of this probably depends on how strict the company is. Our team is fine with having internet access on CI provided we commit lockfiles and just ship statically linked binaries to artifactory and sometimes into docker images. For internal dependencies, you can rely on internal git repos in Cargo.toml.
I haven't used libloading myself, but have you tried making the type of your Symbol be `Symbol&lt;extern fn()&gt;` instead of `Symbol&lt;fn()&gt;`? It should affect the calling convention used iirc
Can you elaborate on 2 a bit? What did you have to do that took so much time? All `Cargo.toml`s of the dependencies or of all your projects? Did cargo-vendor and cargo-local-registry not do enough?
To ensure it only gets initialized once, you could use something like use `lazy_static!` ([crates.io link](https://crates.io/crates/lazy_static)) to initialize it instead.
Thanks! This project was a blast to write. Rust is an absolute joy to write in.
Well you could define a build.rs file, which generates your top level lib.rs or main.rs file. Just source in your original one and add the allocator line, or not.
This seems really similar to https://github.com/google/rustcxx, no? I'm surprised no one has mentioned it yet.
No idea, since I can't read the mind of the crate author, so I can just make a guess. Looking over it, note that all the ones in the `BitVec&lt;u32&gt;` are constructors. This means that when you call `let x = BitVec::new()` or whatever, you don't have to disambiguate types, since it just automatically provides the intelligent default of `u32`.
I have a little log extraction utility in rust that has been working nicely, but today for the first time I tried piping it to 'head' and am getting a panic on writing to stdout (presumeably when head had receives enough lines, its closing the pipe, and the utility). It's not clear to me if this is the intended behavior with from all the comments in: https://github.com/rust-lang/rfcs/pull/1014 Its also not clear from the println!/print! documentation what to do to handle the error? $ util | head ... thread 'main' panicked at 'failed printing to stdout: Broken pipe (os error 32)' Edit: it looks like I have to trap EPIPE (32) and shutdown... the absolute simplest thing was install a signal handler that just exits (using the "sig" ffi library). It seems a little OS specific to have to handle it that way - I wonder if there are other alternatives? (I know there are other signal crates, I'm hoping for a good generic solution...).
Cool!!!! Glad I could help :)
IMO this is a rather simplistic view of how rewriting a codebase goes. You can't just replace bits wholesale in Rust, C++ has different semantics and a lot of those can't be easily replicated in Rust. If I have to write code that does a lot of juggling of C++ objects I will just write a reverse binding function and call into it, not try to emulate what C++ does with constructors and stuff. Stylo has a bajillion places where it calls back into C++. Most of these would take a lot more effort to rewrite in Rust. Internal APIs aren't always well-defined one-way streets like library APIs, often it goes both ways, and you need to have easy FFI in both directions.
That adds an extra layer of impedance mismatch, and requires yet more work beyond just "rewrite C++ into Rust", adding more barriers to adoption. It's great as a minimum/last resort, but a pain for working with anything that's more featureful than C.
Yep, it's the same idea, though rust-cpp came first I think.
The best way to replace/rewrite components in a legacy C++ codebase is to be able to do it progressively, and with as much tooling assistance and impedance mismatch as possible. Any extra work required like "write a C API for the C++" (and don't get the, e.g., ownership semantics wrong in the wrapper) just discourages it from happening.
Temporary software fixes have a history of becoming permanent. 
Thanks for mentioning that. I'd forgotten that types can have a default because function arguments don't. However, I guess that'd mean mean that all the code which used Arcs would either have to be generic or would implicitly depend on whatever my default provider class is, right?
Would the following not work? `queries.get("option1").map(|s| s.to_lowercase()).map(|s| !(vec!["no", "false", "0"].contains(&amp;s.as_str())) ).unwrap_or(true);` Edit: Just finished reading your comment I guess you figured it out.
I don't understand. The contents of any Rc are inherently immutable because we can't statically determine the refcount.
That sounds like something people would buy if they used Rust at their workplace. One neat thing about this kind of stuff is that you can take a hash of all the sources and upload it to an anonymous and secure server (it can also be publicly available as the hash itself has no information leak.) You could also upload it to the BitCoin transaction log. If the service does not need to serve a high volume of requests you can then manually write the hash down on a piece of paper, sign it with a bunch of witnesses and lock it in a bank vault.
Right, but that means that‚Äîin combination with lifetime elision‚Äîyou can imitate the source-level syntax of `std::sync::Arc` rather closely. You could even have the ‚Äúdefault‚Äù `ArcProvider` type be a dynamic container of an appropriate sort, so you got the third capability for free anyway.
I *was* planning on finishing up the first release of my Windows GUI library, but ran into the issue where some widgets would draw with the Windows 98 style! Long story short, fixing that involves adding [isolation aware mode](https://msdn.microsoft.com/en-us/library/windows/desktop/aa375197.aspx) to the winapi crate so that's what I'm doing right now. 
Also I would prefer modded around x5
Hello! This is the subreddit for the Rust programming language. If you're interested in the Rust game, you can find it [here](/r/playrust). Alternatively, if you are interested in learning to program in Rust (and you should!), [here's a link](https://www.rust-lang.org/en-US/) to the language homepage.
You can `mem::forget(dylib)` before `load()` returns
Check the Rc/Arc documentation for get_mut() and make_mut(). This allows you to mutably borrow the inside of the Rc/Arc (and if not possible safely return None or create a clone() instead). It's allows to do runtime-checked COW, while cloning is only done whenever needed (if refcount &gt; 1). And this can be done safely because if refcount == 1, nobody else will be able to increase the reference count and you know that you're the only owner. And this is why implicitly cloning Rc/Arc would have potential performance effects that are not easy to track down if the cloning happens implicitly.
Yeah, I have a similar feeling towards it. I get bit sometimes by the borrow checker, but I'd rather get bit at compile time with a friendly warning and an active, helpful IRC channel then at runtime with an active, malicious attacker.
My company would require the entire infrastructure (cargo, rustc, any packages, etc) to be checked into version control. (Although technically we've "adopted" rust anyway, for some small stuff. But large-scale adoption would require us to lock to specific versions of everything and distribute it all without running any installers.)
This sounds like a good reason to have that feature in cargo config. You should open an issue on Cargo to see what other people think.
Progressive rewrites are not "temporary fixes", it's an incremental process. Also, tauntauns have a history of not shaving. If you perfect-is-the-enemy-of-the-good everything and try to write it all in Rust, your yak shave becomes a tauntaun shave and you'll never get there. I say that as someone employed on Servo, which is indeed a "rewrite IT ALL in Rust" solution. Servo will take a long time to be ready, but incrementally rewriting Gecko is something we can also work on in the meantime and get immediate results. That's not "temporary software fixes", that's "incremental improvement"
Here's one way to create a string from a `Vec&lt;&amp;str&gt;`: `vec[2..4].join("")` [playground](https://is.gd/vj7SKX)
only the part used to specify the allocator needs to be generated not all the `lib.rs` file, it would be annoying.
I wrote this demo a few weeks ago and said back then, I will try WASM again when the MVP is finished but now Firefox ships WASM but still I can't get it working. I first updated compiler and emscripten then I did not load the wasm module correctly. (You need to load it with JS fetch or XmlHttpRequest yourself because Emscripten is does not load it itself for some reason in the browser.) Then the version of the module was mismatched. The produced .wasm file was version 0xd but the version identifier was reset to 0x1 for the MVP. I even tried to change the version in a hex editor but this just surfaced another parse error later in the file (in both Firefox and Chrome). So unless someone else gets this to work I will wait some weeks before trying again.
It's not in prod yet, but we did successfully replace one of our dev environment severs without anyone noticing! We have a couple of hurdles left before we start writing new services in Rust. I'd like to do some more extensive interop testing of this library, I need to make a pubsub library for our internal implementation, and I'd like to see at least one more developer get up to speed with Rust before we start writing new things in it. I'm definitely going to fill out that form the second something goes live though.
The output is a few weeks old. Rust or Emscripten have seen some improvements in file size. Now the output is 476_912 Bytes so less than a half mb. wasm is even smaller (168_146 bytes) but needs some boilerplate emscripten (220_123 bytes) which consumes the saved space again. (I don't even know what the emscripten code does, except it does not work for me). In theory the WASM file should be called only with a few lines of JS.
A asm.js file build today is "use asm". Looks like the bug was fixed.
Does it have to be an actual server? For my Perl work, I have a local CPAN mirror that is just a directory with a certain structure; using it instead of the official online repo is just a matter of passing, e.g. "--mirror /path/on/my/local/hdd" to the packaging tools.
Yet the copying overhead will often come about anyways if you're moving the value around Implement `Copy`, suggest users use `Box` or `Rc` or `&amp;` in docs Don't implement `Copy` when you don't want modification of the struct to no longer be `Copy` compatible to be a breaking change
It's such an apt description of Rust at that time. Definitely QOTW material
`img_hash` developer here. There's no open or closed issue for JPEG 2000 in the Image crate so I've gone ahead and opened one, but I can't guarantee any immediate movement on it. In the meantime, you can use [OpenJPEG](http://www.openjpeg.org/) with the [`opj_decompress`](https://github.com/uclouvain/openjpeg/wiki/DocJ2KCodec#opj_decompress) executable to decode .jp2 files into another format; I would probably recommend PNG. You can also try using [rust-bindgen](https://github.com/servo/rust-bindgen) to generate bindings for the C API, though that's a non-trivial task for sure. The good news is that OpenJPEG is BSD licensed so it's not copyleft or anything.
&gt; Plus, typing --feature malloc every time I want to compile burns precious calories that I could be using for fighting the bourgeoise. Have you considered using [just](https://github.com/casey/just) to automate not only that but other tasks? It's a pure-rust automation tool (`cargo install just`) with a scripting syntax that's similar to Makefiles, but less footgun-y. (And, when you need power, it supports starting any task with a shebang to embed a snippet of some other scripting language.) I use it for [everything](https://github.com/ssokolow/rust-cli-boilerplate/blob/master/justfile) and it does one better than `cargo` in the "run it from anywere" department because you can type `just path/to/project/command` as a shorthand for `cd path/to/project; just command`. (And the README even suggests `alias j=just` if you really want to get the character count down.)
Problem code: https://gist.github.com/devnought/6e99169f37560fa57471f78f7563ba64 I'm attempting to wrap an iterator (specifically a git repository status iterator from the git2 crate) inside a struct that implements iterator. I eventually want to return the struct from another function and I don't want to expose the verbose iterator signature. Map is being called on the iterator, and filter will be in the future. I want that abstracted away inside the struct. The immediate problem is lifetimes. The iterator struct owns the repository struct, and the repository struct returns the status iterator which I want to wrap. The compiler says that the repository struct doesn't live long enough and by extension the status iterator doesn't either. I thought that making my custom iterator struct own the repository would solve the problem, but now I'm at a loss.
It's neat, but I don't see the practical reasons. If I need to interface with c++ code, I'm either using bindgen-rs, or writing my own simple wrapper around it. Inlining c++ code usually not enough (especially if you have to setup bunch of linking and compilation options, is that even possible?), especially if the 3rd party library gets complex. Contexts, all sort of things that have to be done outside the scope of one function and shared between many. The possible option is to optimize certain things, but besides vectorization, rust is on par with c++. If I need vectorized code, I would rather use ispc.
Aha! I was looking for a tool like cargo-local-registry a little while ago, but I searched for "cargo local repository" instead of "cargo local registry". Thanks!
run strip --strip-all $binary
Perhaps you want to look at the `mioco` library? It uses the same underlying async I/O library as tokio, but you write your code in the sequential style like that article you linked.
If I understand that article correctly, "Green Callbacks" rely on preemptive scheduling. You can get preemptive scheduling in Rust using threads. The Rust standard library has access to threads in `std::thread`, as well as blocking I/O between `std::io`, `std::fs`, and `std::net`. You can simulate Erlang's mailboxes, sort of, using `std::sync::mpsc` (it doesn't provide "selective read," unfortunately, but you can just shove messages in a `Vec` and process them when you're ready). There's also nothing stopping you from sending boxed closures over a channel, provided they're `Send`. 
[Betteridge's law of headlines](https://en.wikipedia.org/wiki/Betteridge's_law_of_headlines)
Not sure why pointers would be a better storage size alignment then 4 bytes I guess?
I think it's in another class than the crazy sigils rust used to have. I find let ugly personally and haven't found := confusing in go. Maybe it's not worth it but I thought I'd throw it out there while people are thinking about us. I feel like needing to skim over lets is a worse skimming experience that occasionally looking back to find where cars are introduced.
It's difficult to answer your question, because you are asking a loaded question. Functional programming is not side-effect-free programming. It is programming where functions are first-class values and the primary tool of abstraction. Functional-style programs almost always include side effects, and, depending on the language, those side effects might be reflected in the type system. Reactivity has nothing to do with side effects. Reactive programs are simply ones in which the progress of the program is driven by consuming a stream of events from various sources that are outside the program's control. Without the right abstraction, this might result in a "side effect hell". _Functional_ reactive programming is just a paradigm for implementing reactive programs in a functional style. There is no contradiction; "functional" and "reactive" describe totally separate concepts. 
Well either way, it'd be a breaking change reserved for the (eventual) push for 2.0. That's probably a couple years down the road still.
Thank you, this is a very good response. It sounds to me that FRP had very little to nothing to do with F, since pure functions, immutability, etc. are foundational to it, and only using monads can you introduce side effects. Like the other comment misrepresents what I call functional programming, but is probably accurate to FRP, but not to my understanding of F. Edit: my experience with FRP (specifically the R) has been one which overused Observer patterns or similar event/exception/interrupt oriented systems that are one of the most horrific things ever to maintain because of side effects. I see all code as declarative, so I don't quite follow you on that point, and wholefully disagree with you on two way data binding, because HTML is a data format. It was designed to be used as a data format. XSLT (if your data doesn't conform in a way it can be aesthetically serialized without a template and also being a great representation of declarative programming) &amp; CSS provide more than enough capability to do all of that, and in a more functional way.
You can actually just shadow `print!()` with a custom macro that exits instead of panicking, just put it in your crate root before any `mod` declarations: https://is.gd/balLgE
In some cases, data races are undefined behavior in Go.
I've been working with sodiumoxide to create a 'password manager' of sorts. This particular example in the documentation has me stumped: use sodiumoxide::crypto::secretbox; use sodiumoxide::crypto::pwhash; let passwd = b"Correct Horse Battery Staple"; let salt = pwhash::gen_salt(); let mut k = secretbox::Key([0; secretbox::KEYBYTES]); { let secretbox::Key(ref mut kb) = k; pwhash::derive_key(kb, passwd, &amp;salt, pwhash::OPSLIMIT_INTERACTIVE, pwhash::MEMLIMIT_INTERACTIVE).unwrap(); } Specifically the line: let secretbox::Key(ref mut kb) = k; I don't quite understand the syntax of this or what it's doing. An explanation would be great. Thanks.
There isn't a division between red and green with Tokio. If you write your code to be non-blocking, meaning that it handles EWOULDBLOCK on reads and writes by saving any necessary state and returning control to the caller, then you can choose to either use the function synchronously or to plug a Tokio-aware socket into it and use asynchronous utilities. That's why tokio-tls can use native-tls underneath, even though native-tls isn't aware of tokio in any way.
You are most likely looking for /r/playrust
I also recommend running [cargo fuzz](https://github.com/rust-fuzz/cargo-fuzz) on it, a good way to find bugs in something that takes a bunch of bytes as input (which this seems to do).
Are you looking for r/playrust? Although food, tools and protection sounds like a good deal...
Basically, a pure FRP app (like people write in Elm, not thin imitations like React) has this module signature: type State; fn init() -&gt; State; fn update(Event, State) -&gt; State; fn render(State) -&gt; View; As you can see, all of these functions can be side-effect free because the update function takes the old State as an argument and produces a new State as it's result (instead of mutating it in place). It reacts to the events, while simultaneously being a pure functional program. Hence "functional-reactive." P.s. usable versions need to have this signature, at least: type State; fn init() -&gt; State; fn update(Event, State) -&gt; IO&lt;State&gt;; fn render(State) -&gt; View; Real FRP frameworks get more complicated than this, but it never requires your app to have side effects.
Hi /u/Cetra3! I'm [working on](https://www.reddit.com/r/rust/comments/60eru9/whats_everyone_working_on_this_week_122017/df5zkan/) a [parser combinator library](https://github.com/utkarshkukreti/munch.rs) and I was curious how fast a Combined Log Format parser written using it would work (I've had this on my examples TODO list for a while). I decided to extract the pom parser you wrote into a standalone benchmark and then ported it to munch for comparison. Here's what I came up with: extern crate munch; type Value&lt;'a&gt; = (&amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str); fn parse(str: &amp;str) -&gt; munch::Result&lt;Value, munch::str::Error&lt;'static&gt;&gt; { use munch::*; use munch::str::*; let space = P(TakeWhile(char::is_whitespace)); let untilspace = || space &gt;&gt; TakeWhile1(|b| b != ' '); let ipaddr = TakeWhile1(|ch| '0' &lt;= ch &amp;&amp; ch &lt;= '9' || ch == '.'); let betweenbrackets = || P('[') &gt;&gt; TakeWhile(|ch| ch != ']') &lt;&lt; ']'; let betweenquotes = || P('"') &gt;&gt; TakeWhile(|ch| ch != '"') &lt;&lt; '"'; (ipaddr, untilspace(), untilspace() &lt;&lt; space, betweenbrackets() &lt;&lt; space, betweenquotes() &lt;&lt; space, untilspace(), untilspace() &lt;&lt; space, betweenquotes() &lt;&lt; space, betweenquotes()) .parse(str, 0) } Note that this is not exactly equivalent to the pom parser, but it's suitable for your use case as far as I can see. My parser works on `&amp;str` directly and therefore does not need to call `String::from_utf8`. It also returns slices into the original input instead of allocating new strings. pom probably has a way to do these 2 things and that should speed up its benchmark. Here are the numbers: test bench_pom ... bench: 5,303 ns/iter (+/- 382) = 29 MB/s test bench_munch ... bench: 279 ns/iter (+/- 50) = 566 MB/s Source: https://gist.github.com/327de90d279986e9e7a37fa00859b617 Again, the numbers aren't comparable: pom allocates 9 Strings per line while munch does none. &gt; I had a look at nom but didn't feel comfortable with using macros for parsing, and couldn't understand some of the syntax. This is one of the main problems I saw with nom that I've tried to address in munch. &gt; There are a few oddities however with pom, as the parser does return nested tuples, which makes for some really ugly lines. munch implements the Parser trait for tuples upto size 12 for this reason. (I'm curious how `nom` and `regex` would perform here, if anyone is up for it!
Beginning the process of rewriting the shell expansion logic within the Ion shell. Once complete, arrays will be first class citizens, performance should be improved, several corner cases that aren't handled will be handled, and the code will be easier to maintain.
offering good tools, safety, and food for thoughts. he might be in the right place!
That's even faster! I wonder how much time is saved by not allocating Strings on the heap and instead leaving as &amp;str. Initially I was going to pass the ApacheLog struct to another future and decouple the postgres submission, but couldn't get it to work easily which is why I left it there. I'll rewrite the code with munch and see if I can get the time down again! 
Yeah the postgres submission could definitely be optimised more. I am not sure how to break up the futures here so that we can submit a batch. Maybe [Sink](http://alexcrichton.com/futures-rs/futures/sink/trait.Sink.html)?
Coroutines/fibers scale quite well, especially up. They might over-allocate memory for stack, but on modern operating systems (especially 64-bits) it's harmless due to lazy-memory backing.
Thanks so much actually, I spent alot of tonight switching from dein to vim-plug and it's great so far. Il try implimenting alot of this stuff thanks :).
Willingness to learn is all you need. :) And if you want to hack on the compiler itself, a willingness to endure its unusually long compilation times. :P Check out the bugs tagged with "E-mentor" on the issue tracker to find bugs that people have volunteered to walk others through solving: https://github.com/rust-lang/rust/issues?utf8=%E2%9C%93&amp;q=is%3Aopen%20is%3Aissue%20label%3AE-mentor (and you might want to intersect "E-mentor" with "E-easy" for a gentle introduction).
&gt; I wonder how much time is saved by not allocating Strings on the heap and instead leaving as &amp;str. I just tried this: changed `parse` to return a tuple of 9 `String`s. The throughput went down about 50%: test bench_munch ... bench: 543 ns/iter (+/- 254) = 290 MB/s &amp;nbsp; --- a/munch.rs +++ b/munch.rs @@ -2,7 +2,7 @@ extern crate test; extern crate munch; -type Value&lt;'a&gt; = (&amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str, &amp;'a str); +type Value = (String, String, String, String, String, String, String, String, String); fn parse(str: &amp;str) -&gt; munch::Result&lt;Value, munch::str::Error&lt;'static&gt;&gt; { use munch::*; @@ -23,23 +23,34 @@ fn parse(str: &amp;str) -&gt; munch::Result&lt;Value, munch::str::Error&lt;'static&gt;&gt; { untilspace() &lt;&lt; space, betweenquotes() &lt;&lt; space, betweenquotes()) + .map(|(a, b, c, d, e, f, g, h, i)| { + (a.into(), + b.into(), + c.into(), + d.into(), + e.into(), + f.into(), + g.into(), + h.into(), + i.into()) + }) .parse(str, 0) } Honestly, I expected a bigger performance hit than that.
Just added `regex` to the gist with the regex lifted from an older commit by /u/Cetra3. Here's what I get on my machine: test bench_regex ... bench: 3,530 ns/iter (+/- 560) = 44 MB/s
Made a [tiny verilog simulator](https://github.com/Harnesser/tiny-verilog-rs), just to fully grok nonblocking vs blocking assignments once and for all. Writing this in Rust was quite fun, even if I used error-message driven development for the most part! Having things like HashMap quick at hand, and being able to make a minheap from a BinHeap on a custom struct with just a few PartialEqs and whatever (?!) (after a quick internet search, natch) helped me move fast. RustbyExample was a resource I leaned on heavily - sometimes you just need a quick example of how things are used rather than reading a big list of methods. Also, enums, OMFG! Using these to model different kinds of events, or different types of assignments felt really natural. Big thanks to everyone (&amp; everybot) involved in making Rust! 
It's surprising that that's faster than pom. It might be that some of the lines I have in my access logs are what is slowing it down. I have historical benchmarks in the older [readme commit](https://github.com/cetra3/apache_log/blob/266254265802f2b569d745601146a5a7c02a2e08/README.md) 
I'll give that a shot. There is a serial option at the moment `-m s`, but that also doesn't batch the postgres inserts.
Babysitting [`global_asm!`](https://github.com/rust-lang/rust/pull/40702) through CR and testing! Then getting back to work on [porting musl's `open` and any necessary pthreads pieces](https://github.com/dikaiosune/rusl/pull/24) to rust. Woot! So much assembly; coming soon.
I'm not really familiar with the libraries you're using, but might be able to point you in the right direction with a little more information. What OS are you on? What have you tried? Are you getting an error? 
I think they always are. My point is Go allows data races (unsynced read and write) while Rust doesn't, but both allow race conditions.
I'm working on a simple library to parse wikipedia pages. I have this: fn infobox_for(name: &amp;str) -&gt; Option&lt;InfoBox&gt; And this: impl InfoBox { fn parse_nationality(&amp;self) -&gt; Option&lt;String&gt; { ...} fn parse_born(&amp;self) -&gt; Option&lt;String&gt; {....} } And I try to use it as this: let nationality = Wiki::infobox_for(name) .and_then(|infobox| infobox.parse_nationality().or_else(infobox.parse_born())) .unwrap_or("Unknown".to_string()); Which looks perfect to my eyes but I get and error on that `or_else`: the trait `std::ops::FnOnce&lt;()&gt;` is not implemented for `std::option::Option&lt;std::string::String&gt;` What am I doing wrong? 
I would like to help since I will need GPU computing in the next few months anyway (working on mapping software with lots and lots of data). I have a CUDA-enabled GPU (GTX 960) running on Linux right now. I am very happy to see the Collenchyma not going down the drain. I have done OpenCL computing in C++ before, it would be a nice opportunity to learn CUDA. However, for right now, I am busy enough. I hope this repository gains some traction.
yes
Worldwide 2nd most top paid technology http://stackoverflow.com/insights/survey/2017/#top-paying-technologies http://stackoverflow.com/insights/survey/2017/#work-salary-and-experience-by-language
There was also https://github.com/jtomschroeder/cedar posted recently.
No, that's creating my worker pool when my program starts. Set x to 20 and you get 20 concurrent worker listening on the channel and doing function Y when they receive an element on the channel. That's 100% controlled.
I don't need DDos protection from the outside but from the inside. I make the (http) requests, not the other way around in my use case.
You are promoting bad engineering, just like the Rust standard library documentation on files does! Drop on files ignores close failures. Freeing most resources *can fail*. Closing a file can fail. The Rust standard library lacks a function to explicitly close files and react to failure. For any non-trivial, well engineered program writing files there is no alternative to reacting to a close failure, and be it only aborting and informing the caller. I've had such a discussion before, so I want to pre-empt the suggestion of using some sync call - sync and closing are different operations that are not interchangeable.
The only way `close` can fail meaningfully (on Linux, without a bug in the program) is because the sync call failed. Calling `sync` before `close` will catch any error `close` can return. (`close` can return `EBADF` (bug in the program), `EINTR` (nothing to do here, file is closed on Linux anyway) and `EIO` (if the sync fails). The sync can only fail if there was outstanding data, which is what a `sync` would have prevented.) I'd still like an explicit `close` function. There might've been an RFC for that one.
The new ISO works very well, and I also noticed that the UI is a lot more responsive now.
Yes, the two posts below this are exactly the same...
It's also number 10 on the Wanted list.
Looks like Mioco is abandoned? https://github.com/dpc/mioco
Did you press submit a bunch of times on mobile because it looked like nothing happened?
I wasn't suggesting that we use linux package distribution systems. I was suggesting that we design crates2.io to be similar to linux distribution systems; that is, easily clonable (in part, or full), authenticated, and decentralized. Apologies for my poor wording.
Do'h! Looks like GitHub doesn't support a "partial" search. I was looking for "depr" in crates.io issues... "Maintenance" is not the same, really. I'm talking about crates like `time`. It has an explicit notice, that you should use an another crate. But you should read a readme for this. And I want it on crates.io itself. Yes, I notice that some people uses `description` for this, but it's more like a hack and doesn't impact search results.
&gt; Yep, I think this falls under RFC 1824's Maintenance labels, I see "deprecated" crates being labeled either "as-is" or "looking for maintainer". I have several crates that I'm tempted to deprecate. This isn't because I don't have time to maintain them, but because I think there's a _better_ crate that people should use instead. A typical example might be https://crates.io/crates/snappy_framed, which I believe should be ignored in favor of https://crates.io/crates/snap for almost all common use cases. I'm not especially looking for a new maintainer, and I don't think it's necessary to yank all versions of the crate: Somebody might want to the C version instead of the native Rust version for some reason, for example. I just thought I'd mention this as possible use case for when you're designing. :-)
[prior discussion](https://www.reddit.com/r/rust/comments/5o8zk7/using_stdfsfile_as_shown_in_the_docs_can_lead_to/) [highly relevant comment](https://www.reddit.com/r/rust/comments/5o8zk7/using_stdfsfile_as_shown_in_the_docs_can_lead_to/dchowa8/?context=3)
Yea going to delete 
&gt; Out of curiosity, what are you using it for ? 1. High-throughput data munging like [scrubcsv](https://github.com/faradayio/scrubcsv). Node is far too slow for this, and C++ has too many footguns. In some cases, I want to be able to entirely eliminate memory allocation from inner loops, which makes Java and C# less attractive. 2. Programs like [cage](http://cage.faraday.io/) that involve a ridiculous number of tricky edge cases that we want to be "aggressively correct" about handling. Rust is _excellent_ for this because of the ecosystem-wide emphasis on correctness. Also `enum` types rock because they force you to explicit about all the possibilities. :-) 3. Programs where we want statically-linked, single-file binaries for common platforms, such as [credentials_to_env](https://github.com/faradayio/credentials_to_env). &gt; I'm genuinely interested in the language, and have been following it for a while, but I can't find a compelling reason to use it at work instead of Node. This morning, I'm dealing with a piece of Node.js code that uses complex streams and callbacks, and I'm occasionally seeing a few blocks of data go missing during a big cluster job. I don't know whether Node.js is somehow forgetting to flush a file or not. I'm swapping in a piece of very fast Rust code that uses no callbacks and no streams, and I'm going to see if data stops going missing.
Yeah GitHub search really isn't great as a tool. Not surprising you couldn't find issues related to it.
Citing the man page: &gt; Not checking the return value of close() is a common but nevertheless serious programming error. It is quite possible that errors on a previous write(2) operation are first reported at the final close(). Not checking the return value when closing the file may lead to silent loss of data. This can especially be observed with NFS and with disk quota. Even if sync is used, ignoring the return value of close is a violation of the API contract. The standard library documentation examples don't use sync, and neither does this article. Bad engineering. Edit: formatting 
It is an issue if the objective is writing reliable software.
I was trying to say that OS threads don't necessarily scale, not that coroutines/fibres wouldn't. I could have enunciated that better.
If you want an explicit `close` method that returns any errors from `sync`, you could do trait Close { fn try_close(self) -&gt; std::io::Result; } impl Close for File { fn try_close(self) -&gt; self::io::Result { self.sync() } }
Wow, I just looked Maidsafe up - looks very interesting!
That's some crazy speedup. Do you mind sharing some details of the kind of computation that caused that?
&gt;package manager for Java and Javascript libraries, and have continued to use it for Rust. How in hell is that even remotely usable?
Typically the iterator borrows the struct which owns the data being iterated, so you avoid the self-reference problem.
I was recently in the same position and a `brew install csfml` was all I needed. Of course, this assumes you're using macOS. More generally, you'll need to place the downloaded artifacts in your path in order to use them (e.g. `/usr/local/`)
Out of curiosity, a few coworker devs and I were wondering why US developers look like they get paid almost twice that of other countries. What's the explanation for that?
It's to cover healthcare costs.
There has been a _lot_ of discussion on /r/rust about using QT with Rust. I highly recommend searching around, because it's unlikely those people are going to see your post and come have a big discussion. `rust-qt` might be the defacto library. I would recommend checking out `qmlrs`. GUIs in Rust currently suck, for the most part. Using GTK with Rust is one of the least painful options, along with Piston as another less-native option. QT is okay for very simple UIs in Rust, but because it is a C++ GUI library, it doesn't map to Rust very well at all, which is very unfortunate, and there have been many discussions about how great it would be to have good QT bindings in Rust. It's a rather depressing state of affairs for me, so this is all I'm willing to say about it. I _want_ a good GUI library for Rust so badly.
I've always been under the impression it had to do with the cost of living for where the developers are being hired -- most notably in the bay area. Though I'd be curious to see these salaries normalized for cost of living since I don't know for sure.
I don't think that Qt (!) bindings is a great solution for rust. Bindings to C++ is a nightmare by itself and binding to a library that leverage C++ a bit (MOC) is a hell. Also, the only things that needed are QtGui and QtWidgets. We don't need QtCore at all, but we still have to ship it. The ideal solution is to port QtWidgets to rust. It's enormous amount of work, but it's still the best solution. I can't imagine a rust bindings to Qt. It heavily uses inheritance and virtual functions, witch will be a hard part to translate.
Good to hear
Seriously? I decide not to go to C++Now this year due to multiple issues, one of them being that I'm now more interested in Rust than C++, and they pull this? :-(
Plus usually contribution to a state pension and unemployement benefits (which are usually around 60% of your last salary for a year) But it does not explain all the difference, I believe that generally there is more demand for SW engineers in the US.
On a related note, I'd love to see a cross-platform "video input" crate. Something that allowed a user to enumerate the available video input devices on their system (i.e. webcams, forward facing cam, etc), list the supported frame rates and dimensions, and request frames from it in real-time. As an example, openFrameworks provides something like this via an [ofVideoGrabber type](http://openframeworks.cc/documentation/video/ofVideoGrabber/). Super useful for playing around with CV stuff or working on projects like OP mentions.
I would agree with this. If you check out some site I forget the name of, Money per cost of living Austin and Denver make more money (by spending power) than anywhere else in the US, to the point it almost doubles the value of the money compared to the Bay area. So glad I live in Colorado and not California heh.
When I start the Iso in Virtualbox the resolution is really low. How can I increase the resolution to 1080p?
&gt; a few years back, steveklabnik went through and tagged every untagged issue in the tracker, though we could use that treatment again Yeah, slowly there were some that I wasn't sure how they should be tagged, and then two, and then 30, and then I said "fuck it." Today, every bug should have a team tag; many many many don't.
Are y'all hiring? I couldn't find a jobs page.
That depends on the use case. In the most cases, it will be catastrophic, meaning the right thing to do is to stop the task (process, job, request handler, ...), log an error, and return an error from the task (exit with error code, return a negative http response, set the job state to failed, ...).
Not every application needs the guarantees of fsync. It has a big performance impact and should thus be avoided if not required. And there are many use cases where fsync is not needed.
Given that you have performance issues in Python, I take it you're using [imagehash](https://github.com/JohannesBuchner/imagehash) or something else? I'd recommend switching to [py-phash](https://github.com/polachok/py-phash), which is based on a well-optimized C++ library. Said library uses ffmpeg for image decoding, so format shouldn't be an issue. Based on the other comments, this would probably be the path of least resistance / wait-time.
Great! Dive in whenever you want. I'll be around if you have any questions!
Thanks that worked beautfully! Wish there had been a little breadcrumb leading to that from the print!/println! documentation. Maybe I'll have to figure out a way to submit a doc update...
Well, I'm not saying about class-to-class/method-to-method port. But the general ideas of the QtWidgets, aka widgets, layouts, etc. &gt;Qt itself, on the other hand, is mature, solid, popular and actively maintained. Sandy - no. And I'm saying it as a paid Qt developer (not Qt itself, but Qt-based apps). QtWidgets is basically deprecated. There are tons of bugs, even with P1/P2 levels. All the focus is in QML and embed. Desktop is not a priority anymore. &gt;Starting a project in 2017 to fill the same niche as Qt doesn't make sense to me. Yes. But if you want a crossplatform (desktop) app - you will choose Qt. There are no alternatives. And I don't think that trend "put everything in web" will live long. At least all applications I'm working on can't be ported to web, or even mobile. So I'm very interested in Qt alternative without C++ legacy.
I found couple issues that has already been solved by OP bug creator, and it hasn't been closed at all. I would like to participate to clean up issues. 
More speakers will be announced the next few days: https://twitter.com/RustFest/status/843780837938253824 Tickets are already available: http://2017.rustfest.eu/#tickets Grab yours now!
Great! One complication is that we can't just go around giving people the ability to close issues, because Github's permissions system is so coarse that anyone with the ability to close issues also has the ability to commit code (which we obviously can't give out wantonly). If you have a list of issues that can be closed, feel free to either poke someone on #rust-internals, or just PM the links to me and I'll take care of it. :)
You can just use hyper or whatever. Tokio is what you use when you want coroutines, the other ways will continue to work. The red-green thing doesn't apply to Rust at all, because it's not a single thread.
It's not all about looks. Serious applications need a full-featured widget set that has worked out the nitty-gritty details of how the more complex ones must work in order to make your job easier, not harder. I mean, everyone can draw a button, but think of tree widgets that display a model with multiple columns, editable and whatnot. By the way, without Qt/Gtk it's not going to get better with the current trend of "HTML+JS everything, we'll just roll our own, flat design doesn't need widgets anyway".
I'm personally using KDE and I don't see any problems with GTK+ rendering. Yes, `breeze-gtk` doesn't give the best result possible, but it's not catastrophic. At least in this 3 GTK+ apps I'm using. Also "unifying" and Linux are completely different things (in a good way). Diversity, even throw it's painful sometimes, is the main reason why people using Linux.
I think a "Needs Maintainer" badge would be nice. Things like "Deprecated" might be a self-fulfilling sentence for the crate's fate.
You misunderstand. Green threads are not rate limiters.
[Relevant XKCD](https://xkcd.com/927/) Even if we managed to make up for over a decade head start it would have to be monumentally better than Qt or GTK to get people developing *new* code in it. In order to become the defacto standard it would have to have tons of existing codebases switch to it. For a sufficently large codebase that is near impossible.
&gt; What companies are actually hiring Rust developers at the moment? We kinda are. We don't care what language our programmers prefer. Our software stack is modular enough that if someone wants to write a Rust component, it'd be approved just as easily as our Scala and R and Node.js components were approved. 
I hope Rust isn't leading the way in becoming the next "most liked but least used" language. That does tend to be a problem. Hopefully the ergonomics push really makes difference and isnt just bike shedding or even worse (it really needs to).
I use goroutines to create a static amount of workers (x) for a worker pool who then handle the incoming objects in the channel. Which guarantees I only have x connections at the same time maximum. For me this is a rate limit?
Smalltalk is the second most loved language. No good.
Not all of it, as vim doesn't include stuff with other plugins, so I had to strip a bunch of that stuff out. But stuff like `rustfmt` integration, treating `cargo`/`rustc` as a compiler, syntax highlighting.... all that stuff.
&gt; It's also worth pointing out that there are downsides to Erlang-style message-passing concurrency, too‚Äîmaintaining all those mailboxes and the state for all those processes takes time and effort at runtime Eh. Pony has shown that actors can be *very* cheap - Pony actors are a mere 240 bytes &gt; an object. I wouldn't expect actors in a rust world to add significant overhead over tokio. Right now there isn't a great story for async. Tokio is solid, but young and only one paradigm for concurrency. I wanted to build actors in rust but it was a real pain. I may go back to that project at some point though.
I would actually expect Mozilla to pay less than other companies, given that they're a non-profit.
But crate might be deprecated in favor of another crate, so why should not it be an option?
&gt;Yes. But if you want a crossplatform (desktop) app - you will choose Qt. If you want a _native_ crossplatform desktop app. Otherwise you may just use electron or even, heck, JavaFX which is pretty usable, contrary to popular belief. 
If you're manually making a thread pool, then you can do precisely the same with 1:1 threads, with the same amount of work. Green threads are a red herring.
Taxes too. US salaries are typically listed pre-tax, so a good chunk of our salaries is money we never see.
I'm pretty sure the Python example is wrong. Python uses both reference counting and a more traditional garbage collector: this means that when the last reference to an object is cleaned up, any destructors are called, which in the case of a file includes closing it. So no resources are leaked in this case unless you leak them explicitly. The danger here is that you do something like close over f, or throw as stacktrace, or something else that keeps some kind of pointer to the file alive accidentally. Of course, its not a best practice to not close file (using a with block), but in many cases it doesn't leak in the same way Java or something might if you aren't careful.
Me - no. I hate electron. And I don't have any java application on my PC. Also both of them doesn't support native look and feel.
One simple option is to use a derive annotation of some kind, if the code is simple enough. However, I suspect this may be an XY problem, and you might be better of explaining why you decided to try a design like this.
Calling close on drop is a sensible default, but I agree that it is a valid use case to need to check close's return code without having to call fsync. An explicit close method that preempts the destructor from closing again, and returns the error code, is probably the best compromise.
I think you should just use mio directly. It's still a very nice library when your use case doesn't boil down into chains of futures
Anything's a package manager if you're brave enough. The way it works is that there's an FTP server of tarballs. In your project, you have a config file that looks a bit like: retrieve jars lib/ retrieve crates rust/lib/ include junit @ 4.12 include crossbeam @ 0.2.10 When you run the tool, it does something like: 1. Fetches ftp://wobbly.initech.com/junit/4.12/resources.tar.gz 2. Unpacks it 3. Somehow decides that there is a jar file inside (this bit is a mystery to me) 4. Copies the jar into lib/junit-4.12.jar 5. Fetches ftp://wobbly.initech.com/crossbeam/0.2.10/resources.tar.gz 6. Unpacks it 7. Somehow decides that there is a crate inside (still a mystery) 8. Copies the crate into rust/lib/crossbeam If there was a package with a jar and a crate, it would fetch it once and copy both things into the right place. It knows about dependencies, and can resolve them transitively. It has a local cache in ~/.wobbly. it's then up to you to write the relevant things in your build.gradle (or in practice, shell script) or Cargo.toml to reference the downloaded libraries; the package manager sadly doesn't feed metadata about what it's downloaded into the build tools. If i was starting the company from scratch, i certainly wouldn't use this. But it's not all that bad, and it's got all our in-house libraries in it, so i just get on with using it. 
No idea.
Weirdly you can pay your employees a lot when you're not focused on exploiting them to extract value for investors and executives.
Nice to see FromStr go :)
Nice work as always. Do you folks have a public roadmap we could look?
Yes.
That doesn't matter as much compared to *where* these developers are, i.e. not distributed over the world, which is what brings everything else down. It's why Rust doesn't show up in any of the regions, it's mainly relevant in the US and *there* it's below the top 20. I should've stated this in my original comment, sorry I wasn't clearer.
Per patch, afaik. https://github.com/vim/vim/releases
&gt; One simple option is to use a derive annotation of some kind, if the code is simple enough. Would this be something like a procedural macro then? I was reading about that, but I'm not sure if that's the best way to do this ... &gt; However, I suspect this may be an XY problem, and you might be better of explaining why you decided to try a design like this. My use case is quite similar to my example. I want to implement some sort of a trait that represents a range / interval with a name: trait NamedInterval { fn start(&amp;self) -&gt; u64; fn end(&amp;self) -&gt; u64; fn name(&amp;self) -&gt; Option&lt;String&gt;; } The `name` function is only there as a simple attribute, but the more interesting parts are `start` and `end`, which I can then use to implement functions on the trait like `fn make_intersection(&amp;self, other: &amp;NamedInterval) -&gt; Option&lt;NamedInterval&gt;` or `fn is_adjacent(&amp;self, other: &amp;NamedInterval) -&gt; bool`. These structs will be created from an external source where validation is quite minimum, though (e.g. the `start` value may be larger than the `end` value). So I was thinking of having something that can be used like this: let inv = ImplementedInterval::default().with_name(...).with_coords(..., ...).validate() Where `fn validate(&amp;self) -&gt; Result&lt;NamedInterval, E&gt;`. Now, there are several flavors of this interval, and they will carry different information which will determine how they can be used. Some of the flavors may also have `Vec&lt;OtherInterval&gt;` as its field (so they are containers of other intervals). What they all have in common are just those three `Interval` fields. Does this make sense?
We're at a nice kind-of mid-point in the style guidelines RFC process where it would be great to get some feedback from the wider community.
I try to switch to Firefox every few months or so and I can't quite justify it yet. Firefox has definitely closed much of the performance gap (from a users perspective) in recent years, but I still prefer chrome enough to use it as my primary browser.
I love the many calls to participation this week; hopefully we can keep that going!
Whoa rustfmt integration! Like running cargo fmt on :w?
To be clear: 1. Mozilla Foundation (mofo) and Mozilla Corporation (moco) are different. 2. Mofo is a non-profit, Moco is not. 3. Moco is a wholly owned subsidiary of Mofo 4. All employees working on Rust and Servo are part of Moco, not Mofo.
I believe neither Mozilla Foundation nor Corporation offer stock/equity as part of one's compensation, and thus the dollar salary number is higher than it might be at companies that do include some equity. (The survey methodology isn't clear if they wanted people to include or exclude equity, etc.)
Not as scary as it sounds. Like our Scala and R projects - if they're using the best tool for the job, it's easier to maintain them than to maintain a re-write in an inappropriate language. Scala was used because it was a logical choice for Spark, and R because the CRAN provided 98% of what we needed for that project. When a great tool is found, multiple developers express interest, and could take over maintaining it. We also have components in Java (custom Solr filters/tokenizers/analyzers), C (a postgres extension), Python (devops scripts called by Ansible), and probably a half dozen other languages. **TL/DR: It'd be absolutely insane to port all of those to One Standard Language just because of some misguided language standardization policy.** 
You sure you don't just want me to delete this submission right now? Surely you understand the magnitude of bikeshedding this will inevitably result in. :P
Well, I've worked for a non-profit before (a university), and the pay among their technology department was (and still is) about 15-20% lower than similar positions in our city. I had thought that many non-profits were the same due to a limited amount of funding, but I'm glad if Mozilla pays its employees fairly.
That is a fair question. 
Why you don't follow rust codding style? Also `pub fn print (cboard: &amp;chessboard)` should be implemented via Debug trait.
Hmm..what about the multiple flavors of `NamedInterval`? For example, I need to have these (simplified) types too: struct SimpleInterval { start: u64, end: u64, name: Option&lt;String&gt;, } struct ContainerInterval { start: u64, end: u64, name: Option&lt;String&gt;, other: Vec&lt;NamedInterval&gt;, mark: u32, } struct YetAnotherInterval { start: u64, end: u64, name: Option&lt;String&gt;, map: HashMap&lt;String, SimpleInterval&gt;, other_value: i32, } /// How to best do `impl NamedInterval for {interval-flavor}`?
I've been thinking about writing a short guide about advanced macro tricks that I've thought up or come across, there's some cool patterns out there. 
I'm pretty happy with the style guidelines, except: 1. I feel fairly strongly that attributes should go between doc comments and code, and not before doc comments. :-) 2. Those `where` keywords on their own line feel really weird, especially for single-item `where` clauses. My eyes keep getting older, my fonts keep getting bigger and laptop screens keep getting proportionally shorter and wider. I dislike losing that extra vertical line to gain only two horizontal lines. I'd be happier with: # args here ) -&gt; Result&lt;()&gt; where X: Y, A: B { Everything else looks basically reasonable, even when it's not my preferred style. Nice work!
Someone is offering $65714 (per year I assume) to Rust developers? Where can I find him?
Or, to be more technically precise, RCs are merely a CPython implementation detail and are not part of the Python language spec.
You might want to consider changing the name. 
I certainly agree with #1. I prefer keeping things short enough so function signature fits one line, so I don't have much experience with #2. My first feeling is that I'd prefer **not** having `where` on separate line.
I make like 15k pre-tax in central Europe :D, it is amazing how big the difference can be.
Universities have a lot of money
&gt; I feel fairly strongly that attributes should go between doc comments and code, and not before doc comments. :-) I'm interested in the background of the decision to do this the other way, I've personally never seen attributes before doc comments, and it seems to be separating a declaration from its attributes, which are "high-density" information that's more relevant to someone currently reading/editing the raw source code than the doc comments.
And lot of expenses too.
 #args here ) -&gt; Result&lt;()&gt; where X: Y, A: B { Tabs for life As for the example code in the RFC, needs an example of a long chain of iterator adaptors with multiline lambdas sprinkled in. Good to see block indentation taking root As an admitted messy coder, here's a couple personal outliers: [Kelxquoia, go deep](https://github.com/serprex/Kelxquoia/blob/master/src/main.rs) &amp; [ailogic, match match mat..](https://github.com/serprex/rg/blob/master/src/ailogic.rs)
"Use the product you don't like that much, so you have the option to use it in the future" isn't a compelling argument. 
Most likely half year later than rolling release like Arch ;)
I was addressing the attitude of "I'll use it again when it meets some abstract notion of goodness goodness in the future". It doesn't really make sense if everyone thinks that way. 
/u/jtomschroeder Unfortunately, I'm using windows 10. I'm trying to use sfml in a test project to see if sfml is successfully installed, so I can use it to began starting a project. [Screenshots](http://imgur.com/gallery/7le3U) [For the prerequisites](https://docs.rs/sfml/0.11.0/sfml/) it said I need SFML and CSFML installed on my computer (which it is not). I went to the links and downloaded them with the correct version/VC++ as instructed by the sites. [I have two folders named SFML and CSFML with a bunch of DLLs in them.](http://imgur.com/gallery/tXXK8) I simply do not know how to "install" these files. I barely even know what a DLL is. I have however, installed SFML strictly following a youtube videos instruction on Visual Studio 3-4 years ago for a basic snake game project using C++, but I cannot remember any of it. I do however remember getting the DLLs or something and putting the path of those "DLLs or something" inside of a link parameter in a setting box with visual studio. 
Looking at [the OpenJPEG C API](http://www.openjpeg.org/doxygen/modules.html), it doesn't look like too big of a challenge to create bindings for it, at least the bit that decodes `jp2` files to a raw bitmap in-memory. (I might take on that task myself, actually.) Once you have that, you can convert it to an `ImageBuffer` from the `image` crate for further processing: several of the hash types in `img_hash`, with the exception of the Blockhash.io algorithm, require downsampling the image first, and the `image` crate provides the most convenient way to do that. Coincidentally, and I forgot to mention this earlier, the blog you link in the OP is actually where I got several of the original hash types in `img_hash`. The dHash that Krawetz describes is the same as `HashType::Gradient`, and the variant mentioned later that uses both rows and columns is `HashType::DoubleGradient`. Similarly, the aHash algorithm became `HashType::Mean` and the pHash algorithm became `HashType::DCT`. In my own testing, I found them all to be reasonably accurate and varying primarily in performance. The DCT algorithm is the slowest by far, but precalculating the DCT matrix gave a nice speedup that brought it back to within an order of magnitude of the other algorithms. I'm actually working on [a project of my own](https://github.com/abonander/img-dup) which is intended to find and collate images in a directory tree based on similarity. The ultimate goal is to have a nice GUI frontend to present the images to a user so they can decide what action to take for duplicate images.
*edited: my initial comment was agressive/offensive and did not passed the ~~borrow~~ civil checker, I corrected that. Both Scala and Rust have great language abstractions - but sometimes at cost of additional complexity. Good to see them evolving together! Let's steal some Scala users, come to Rust! we have good functional features &gt;:D
It looks you're trying to mention another user, which only works if it's done in the comments like this (otherwise they don't receive a notification): - /u/dbaupp --- ^I'm ^a ^bot. ^Bleep. ^Bloop. ^| ^Visit ^/r/mentionhelper ^for ^discussion/feedback ^| ^Want ^to ^be ^left ^alone? ^Reply ^to ^this ^message ^with ^"stop"
This is a good suggestion. In my experience with ETL the database insert is often the rate limiter - I'm actually rather surprised you were able to drive any performance benefit out of parsing the logs faster given that an insert statement has to make the round trip to the database with each statement. With COPY you can effectively just throw every line into a TCP socket as soon as you parse it. I've seen well over 10x speed increases by using COPY over INSERT.
How so? Smalltalk seems interesting, albeit *way* too OO heavy for my taste. When I started programming I learned to do so in an OO style, using PHP. (Well actually to be way too specific, the first book I read about programming was about C++ but I read it when I was 12 years old and using Win XP and when I compiled Hello World and tried to run the exe a window opened and closed and I thought I had made a mistake but actually what happened was that the book hadn't explained that I was supposed to run my exe from cmd, not by double-click and so I gave up on programming and instead learned about HTML and CSS and it was not until the age of 15 or 16 when I got a Texas Instruments calculator for high school that I tried my hand at programming again by writing some things in TI-BASIC and shortly after that I began learning PHP from a book that my father had bought because he needed to learn PHP for his job.) After a while and after being exposed to C (at the university at age 19) I understood that I had been building inappropriate abstractions previously. I rejected OO and even though Python has now been one of my favorite languages for a good bit of time (since 18 and now I am 26) and OO is natural to Python, I've refrained from OO as much as possible and during the course of time I've also investigated functional programming by reading SICP and writing some functional code both in Scheme and a little bit of Clojure and also in a way when I've been writing Python and I am beginning to understand that OO has it's place so currently for a side-project of mine where I'm writing some custom buildtools with Python, my code for that part is turning out to be very OO and I'm quite happy with what I'm doing because I think that by exploring how to not think in terms of OO only I am now able to see some places where OO is appropriate.
?
The Mozilla Foundation is a non-profit organization, which means they have restrictions on the types and amount of revenue they can have. The Mozilla Corporation, which isn't a non-profit, doesn't have to abide by these restrictions. This means that they can re-invest all of their profits back into Mozilla Corporation projects and other revenue related things. The goals of non-profits and corporations do indeed conflict, but in MoCo's case that (arguably) doesn't matter because MoCo is a wholly owned subsidiary of MoFo. Does that make sense?
Seems to be integration with rustfmt, cargo and rustc as well as some other things that perhaps someone else can explain.
https://en.m.wikipedia.org/wiki/Clitoris (female sex organ)
Non-Mobile link: https://en.wikipedia.org/wiki/Clitoris *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^47004
i still don't really get what the problem is
I linked the above thread because I saw a good, useful and educated discussion going on above both goods and bads of both languages and where other people that use more of the functional paradigm would like to see Rust going or at least where some of the current pain points exist for that use style :). Note that I was not the one that created the question. Although I use Scala professionally and am very interested in Rust for a long while now, I can totally understand the point of view being discussed. For me the most exciting thing is actually the cross-pollination between the languages and the future Dotty and how Rust has already influenced it. And using Scala was what made me aware of HKTs, how they can be used and the powerful abstractions and patterns that they enable.
For better or worse some people may not be comfortable with it, that's all.
I mean that's pretty much exactly how people choose between competing free products. Paid products generally lower their cost compared to competitors when they have an inferior product. Firefox offers "we collect less data about you" as their counter offer to worse features. Which just isn't enough incentive for many.
&gt; ergonomics push What is this about? I haven't been following too closely of late. 
I agree! Both languages are great, with top-class language researchers. I think both next-gen Scala (dotty) and Rust (post-ergonomic initiative and a well-sorted-out-mordenized tokio/futures/async) will be fantastic languages.
That quote of the week doesn‚Äôt fit the meter for a limerick (even after substantial convolution and bending of syllable emphasis, lines one, two and five are a syllable short).
I for one absolutely *love* the new `where` style, and think it's vastly superior to putting `where` on the same line as the first constraint. It's more logically consistent with the rest of the syntax, it introduces a more reasonable and standard indentation level for the constraints in the where clause, I find it much easier to visually scan, and it's easier to edit in editors without special rust support.
Good to hear!
It will ask you for the desired resolution in the bootloader
... huh?
/r/playrust maybe? Idk what this is tho
https://rocket.rs/ is also quite interesting, the type-conforming deserialization from HTTP to function calls seems especially nice
As a Swede, it always amuses me to see this. Something like 70% of the money our employers pay ends up going to taxes, and you have Trump going on about the US having the highest taxes in the world. Hah!
It's not all about widgets either. Serious applications need their UIs to be accessible.
Will these changes be overridden for those of us who install the plugin? Can't imagine going 6-12 months between updates if forced to live by vim/release schedule.
&amp; IronPython uses the CLR's GC
This week I've been overhauling the `Message` struct of my IRC library to be more efficient with allocations, while also adding the ability to have strongly typed access to both tags and the command/arguments of a message, in a way that's extensible by consumers of the library to allow for support of any tags or commands specific to their use case. I'm also working on adding more tests, improving the parser, and adding more message constructors and command parsers for the core set of IRC commands.
I don't think it's actually possible to make this work. If you know statically how big data will be when you initialize Foo then you can use CoerceUnsized (https://doc.rust-lang.org/nomicon/coercions.html) to accomplish something similar, but otherwise I don't think it's possible to initialize dynamically sized types. struct Bar&lt;T: ?Sized&gt; { info: u32, data: T, } type Foo = Bar&lt;[u8]&gt;; fn main() { let x: Box&lt;Bar&lt;[u8; 3]&gt;&gt; = Box::new(Bar { info: 0, data: [1, 2, 3] }); let y: Box&lt;Foo&gt; = x; } 
Thanks for the nice words! Here it is: https://github.com/pingcap/tidb/blob/master/docs/ROADMAP.md
So, 1 is basically incidental, just happened to be that the sample code was written like that. Not a guideline. We discussing if we should do something in https://github.com/rust-lang-nursery/fmt-rfcs/issues/72. I'm a bit partial to visual indent for where clauses, saving a vertical line being one reason in favour. However, we did feel the advantages of block indent outweighed the disadvantages - primarily it is more consistent with other formatting and works well in a variety of situations in which visual indentation is a bit awkward. We are currently in FCP on the issue at https://github.com/rust-lang-nursery/fmt-rfcs/issues/38
You can make `Foo` work as originally defined, but it requires unsafe code. For starters, you could keep the original definition of `Foo` alongside `Bar` and then simply transmute `Bar` to `Foo` (even though field ordering between two different structs isn't guaranteed, if you only have one statically sized field then it's always going to come before the dynamically sized field), but this doesn't support a dynamically sized slice because you have to start with an array anyway. You can also start with a `Vec&lt;u8&gt;` and then push the value of `info` as bytes, then call `into_boxed_slice()` which gives you a `Box&lt;[u8]&gt;`, and transmute to `Box&lt;Foo&gt;` from there, or alternately transmute the `&amp;[u8]` from `Vec::as_slice()`. to `&amp;Foo`. You'd have to subtract 4 (the size of `info`) from the second value in the fat pointer to get the length right, though. Correction: in the `Vec&lt;u8&gt;` case, the bytes of `info` need to be at the start of the vector, then the bytes of `data` come afterward.
are you saying it's not intentional?
Rayon is a parallelism library, not a concurrency library. Don't use Rayon for network servers. Likewise, Golang's runtime is a concurrency-based runtime, not a parallelism-based runtime. Don't use Go for parallel workloads.
In my defense, it is generally admissible to shorten the meter of the 1-2-5 lines, and I often have to reduce text to fit in Twitter's 140 characters. Much worse is that often the first line is neither introducing a person nor location.
`close` without `fsync` doesn't provide any guarantees. If it returns error, you've lost data. If it returns success, you may still lose data. Why do you care about catching the former type of data loss but not the latter? If you care about your data being written, you need the guarantees of `fsync` (or possibly `fdatasync`). If you don't, you're gaining nothing by checking the return value from `close`.
Are you trying to brag, but you're really bad at it?
If it's NSFW, it's not a good crate name.
&gt;Is some work being done to fix the problem in Rust? What problem? From reading the blog post it looks like rust is behaving as you'd want it to.
&gt; To elaborate on this, the Rust community strives to include minorities, and in the IT sector that distinction includes women. well, you know, the author seems to be exactly that, a woman, and seems to have found the name funny. the name is now apparently changed to tw-rs.
I like it too! I've been using what rustfmt does by default just because it's what rustfmt does, but if it changes to this new style I'll be very happy because I've never really liked how it formats where clauses. It wasn't worth arguing with rustfmt over though. I'm too old for extensive style arguments these days!
I actually don't have a problem with this. Which surprises me because I have a long history of shaking my head at the fmt command. This is actually how I used to format complex functions by hand. Then I just mostly stopped writing them. &gt;.&gt;
This comment is unnecessarily insulting, please observe rules 1 and 2 in the sidebar.
I actually have to use Scala at $dayjob, after all this time with Rust... Overall the FP stuff translates quite well, and because of that I enjoy it overall. But there is plenty of smaller problems. Scala embraces FP and wants to have all the complexity possible included. This sometimes comes with total disregard for real-life practicality. Eg. `implicit`s. Stuff can start misbehaving because of some random import. Plus AFAIK, the compilation time suffers because everything can interact with everything else. First, the Java ecosystem is annoying. Things are always slow, JVM is running out of heap, and it's hard to just get stuff done with Vim. And xml ... xml everywhere... Documentation and UX is nothing like Rust. The tooling is much worse with the exception of the IDE. Too bad for me, that I don't like IDEs, and would like to stick with Vim... :D The null is still there... Tiny thing that infuriates me is that foo( bar, x, ) won't work (trailing comma). Trying to have a DSL everywhere... nope. I don't want to. http://www.scala-lang.org/api/rc2/scala/sys/process/package.html import scala.sys.process._ "ls" #| "grep .scala" #&amp;&amp; "scalac *.scala" #|| "echo nothing found" lines IMO, terrible idea.
Did those people explain why they found it uncomfortable? I can't imagine why. The last time I searched Metacollect I had to search old articles with the keyword 'NSA' because I couldn't remember the new name. Funny names have additional benefit of being easier to remember (having more than one association in brain). BTW I think that if people are put off because of names of sex organs, that is a huge manifestation of consequences caused by shaming sexuality.
It depends on Bram's mood. 
&gt; I hope Rust isn't leading the way in becoming the next "most liked but least used" language. My other main squeeze is Haskell so rust is mainstream by my perspective :) Simon Peyton Jones gave a lecture on Haskell's relatively unique status as a research language which neither flopped nor exploded shortly after its release. I think there's still a place for slow, deliberate and intelligent growth. Rust is obviously going to have an easier time than Haskell because its use cases align with C/go/swift, and because the tooling is better. Anyways, making good design choices still matters, even in the mess that is today's tech world. And that will bear out over time. 
I always wondered why the `{` goes on a new line when `where` is used. For example fn test&lt;T&gt;() { } vs fn test&lt;T&gt;() where T: Foo { } Wouldn't it be more consistent if functions without `where` would also put the `{` on a new line? fn test&lt;T&gt;() { }
 for i := 0; i &lt; 20; i++ { go foo(...) } This will always launch 20 workers. Compare to the JVM where you can do (Scala syntax): for (_ &lt;- 0 until 20) { Future(foo(...))(executionContext) } Will be controlled by how many threads were defined in `executionContext`. For example, if you define it with 4 worker threads, then you will have 20 green threads multiplexed over 4 system threads.
Please check the discussion below. Closing a file descriptor without checking the return value of "close" does not look like a good idea. The issue is a bit complicated and I believe people have not yet reached a consensus as to what is the "right" way to do it. If you are on a Linux system, "man 2 close" will give more information.