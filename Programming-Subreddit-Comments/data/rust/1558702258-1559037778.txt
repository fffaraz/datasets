Adopt makes it seem like the called function takes the value. This is not a good replacement, as "move ownership" is generally seen from the position of the caller.
The "owner of a task" is also standard SCRUM terminology.
Donating (if you have the means, and specially if you use it commercially) is also very important.
Great project! I‚Äôve got the feeling the coloring \`-c\` should be default.
Most of those scripts evaluate to commands *shorter* than calling the script. Why would you use `cargo cmd prepublish` instead of `cargo test`? And why would you only be running tests before a release? You should be running `cargo test` nearly every time you change something.
OP‚Äôs post was an attempt to evaluate and discuss said risk.
To be honest, I'm still with you there.
&gt; When you pick a piece of software, you need to ask if your life is better using it, even if that means you might be left stranded in a couple of years, or if the risk of an unmaintained project is too great, in which case you should find an alternative. I'm not sure why you left out one of the obvious alternatives: "When you pick a piece of software, are you ready to maintain it in the future?"
Yes, it's a weird situation where API churn is considered a sign of health - it is a good sign while a project is young, of course. If there are open bugs and 'no-one is home' then there's a problem, but a maintainer has a right to resist feature requests that go beyond the intended scope.
And Github is making that easier now.
\- I don't mind about writing 3 words more, I use zsh autocompletion of the up arrow key for that. The point is to keep order in the project and automate as many things as possible. \- I never call \`cargo cmd prepublish\`, it's automatically called when I call \`cargo cmd publish\`, this is exactly the point \- The same happens for \`cargo cmd test\`. I can't see why you suppose I'm only running the tests before a publish. I run the tests normally, but using my alternative to have the linting and rest of the checks automatically run.
That's what I tried to say [here](https://www.reddit.com/r/rust/comments/bsfpp7/tutorial_feedback_to_move_ownership_is_not/eomx6li/), but the OP said I was wrong, so I'm not actually sure.
I think if something's working as a one-man-show for now, that also makes it more likely that someone else would be able to pick it up in an emergency. Or also that someone might be able to make a new one-man-show replacement eventually. It's when something is developed by a team of people, *and* one or more of the people on that team are subject matter experts, that's when I start to get worried about the bus factor. Because you can't train a new expert without the team, but the team might fall apart without the experts.
Yes I see the resources right now. VSCode is using less ram and running more quickly. It doesn‚Äôt depend on Java that‚Äôs why.
&gt;Quick question though, how can I syntax-highlight `await`? It doesn't seem to use the "Keywords" highlights, and I haven't seen an option in the other "Color Scheme" settings either. I have created [the PR](https://github.com/intellij-rust/intellij-rust/pull/3893) that solves the problem.
You also "adopt an attitude" or "adopt a position" which is more like a clone and a drop. "adopt" seems less permanent than moving. Also "adopting pets" seems like a rather specialised idiom for just one situation. What else can you adopt that is like a "move"?
Thanks! I'm not sure that if coloring feature match for all sort of terminals (especially something like \`Windows cmd\`, and \`powershell\`). So for ensurance, I just make it not enabled by default. But we can easily achieve this by making alias by \`alias hors="hors -c"\`.
In Clippy there is ongoing work on the cognitive complexity lint, replacing the cyclomatic complexity. This is an improved version of this lint. https://github.com/rust-lang/rust-clippy/pull/3963
Sounds like you might want to look at how [stdsimd](https://github.com/rust-lang-nursery/stdsimd) does it, or potentially just _use_ stdsimd directly.
Can someone explain to me why I can have a reference to f1 and mutable reference to f1 in the same scope. // Calculate large fibonacci numbers. fn fib(n: usize) -&gt; BigUint { let mut f0: BigUint = Zero::zero(); let mut f1: BigUint = One::one(); for _ in 0..n { let f2 = f0 + &amp;f1; // This is a low cost way of swapping f0 with f1 and f1 with f2. f0 = replace(&amp;mut f1, f2); } f0 }
It's certainly a problem with [mio](https://github.com/tokio-rs/mio/issues/953).
Reducing the bus factor is exactly why [Code Shelter](https://www.codeshelter.co/) was created. Maybe ask your favorite libraries to join, so the risk is diffused?
Use the `packed_simd` crate.
Let me know if you have any questions or hit any problems implementing it.
Let's look at where the references are: ``` fn fib(n: usize) -&gt; BigUint { let mut f0: BigUint = Zero::zero(); let mut f1: BigUint = One::one(); for _ in 0..n { let f2 = f0 + &amp;f1; &lt;- temporary reference to f1 // This is a low cost way of swapping f0 with f1 and f1 with f2. f0 = replace(&amp;mut f1, f2); &lt;- temporary reference to f1 } f0 } ``` In both cases, the "scope" of these references are temporaries, so they go out of scope immediately. They're never active at the same time.
I think you could use https://docs.rs/cached/0.8.0/cached/ It has all sorts of good stuff, including memoization.
Yup, that's what I've heard as well. However, I really don't understand it. If they want to maintain compatibility with the output format, they can always transform it to camelCase in rendering XUnit output or whatever, while keeping everything internally as snake_case. It just feels out of place.
Oh yeah, I understand the reasoning, I just don't agree with it. They can always translate snake_case to camelCase in the xUnit output, so it's really just the in-code API that would be different, which really isn't a big deal.
How do you know how it compares to Intellij Rust if you are using only VS Code ? Electron based applications( example Atom Editor, VS Code etc.) tend to be more resource heavy and less stable than Java applications because there is a difference in managing resources. For example, VS Code adds around 50+ mb of RAM for an empty terminal panel, the extensions are heavy because a lot of them open trees of background processes. I don't say Electron is bad, but is not the right choice if you want performance. Jetbrain's Java environment based IDEs are using less memory than showed in the task manager/system monitor/htop etc. as they are reserving some RAM, not much. I am running Clion with the Rust plugin I don't even pass 400 mb. I have 4 gb RAM and I don't complain. Use the tools that suit you the most.
This is really cool, keen to run it on my projects when the new rule lands.
Links: - [github](https://github.com/myelin-ai/mockiato) - [crates.io](https://crates.io/crates/mockiato) - [docs.rs](https://docs.rs/mockiato/0.6.2/mockiato/)
Or its sub-option: "maintain your own local fork of the project". Taking over an open source project when its original loses interest is a lot of work, but in the commercial environment you can just keep using an old version, fixing bugs as they arise.
I think you have a typo in your `RangeToInclusive` example - shouldn't it be `..=3`, with `=`?
Good catch üòÑ
This is why Cargo.toml's `[badges]` section allows you to report the maintenance status of a crate as `passively-maintained`, i.e. the author is willing to do maintenance work, but the crate is effectively "finished" and doesn't need a whole lot of ongoing work: https://github.com/rust-lang/crates.io/issues/704
Are you using Amethyst for this game, or your own engine?
 I am in the 300MB range with all the rust plugging right now with VSCode. Electron is written in c++ which is more efficient than java . So it can certainly compete with java based frameworks even with the JavaScript layer. Java is a resource hog. A security weak spot .
I couldn't agree more!
I think part of it might just be the process of libraries maturing and people exploring systems. Case study: I maintain a game framework crate, `ggez`. There's lots of contributions, some major, but when I asked publically if anyone wanted to co-maintain it I got resounding silence, so its still basically a one-developer show. There are at least two other crates that do more or less what ggez does, `quicksilver` and `tetra`. The important bits of their APIs is VERY similar to ggez, and they both list ggez as inspiration. Both are basically one-developer shows at all. At first I was a bit miffed at these maintainers going off and doing their own thing instead of wanting to make an existing system better, but after talking to them I realized that they wanted to do the same thing ggez did but explore different technologies and tradeoffs. Their API's have also started diverging from ggez, including things ggez deliberately omits or making different tradeoffs. Meanwhile ggez has gone the other way, trying to aggressively consolidate with its chosen tools. So, mostly when I work on ggez these days I end up working on its major dependencies instead. I'm currently playing with `rendy`, a rendering crate made by and for the Amethyst game engine, and it's looking like it will get used in the next major ggez version. But it's sure as heck a lot more complicated than using plain OpenGL like `tetra` does... Anyway. Idk where I was going with this, its just a story of how and why a (not major but not small) crate ends up remaining with only a single maintainer. It would sure be nice for someone else to share some of the load, but the people most qualified are off doing things that are just as interesting, so here we are.
simdeez can take care of a lot of this for you super easy: [https://github.com/jackmott/simdeez](https://github.com/jackmott/simdeez)
I will say, though, that I've had much more occasion to separate parts of the unit test case names: `testFooBar_InvalidArgument`. You might argue that that's an anti-pattern but it has come in handy.
Actix and actix-web are different things, they are not even using the same runtime... actix-web has 114 contributors, but sure fafhrd91 has 2070 commits while the rest has 1/100 of that, but realistically if they fafhrd91 stopped maintaining (he would tell us) and a bunch of companies would intervene, because their software is without support. It's really expensive to refactor everything, maintaining a library is much easier, even if there are less features being added. Eventually someone would fork because it's years of work that works very well and is used in the wild...
The problem is that transferring ownership and actually moving data in memory should not be conflated because, when optimizations are functioning properly, the actual movement gets optimized away.
As a native English speaker, I have to say that, if it had said "adopt", I'd have found it confusing. "move ownership", "pass ownership", or "transfer ownership" makes it clear that it's an operation similar to the design of a token-passing network protocol and "ownership" is the token being passed around.
Unless things have changed I don't think stdsimd abstracts away SIMD instruction sets of differing width.
I have never used the phrase "borrow a value" before encountering Rust, either. Nor have I used the phrase "allocate a menu class" before encountering OOP and GUI programming, despite all of those words being relatively commonly used English lexemes... Jargon is to be interpreted within the domain it belongs to, and it's perfectly fine for them to make no sense outside of it...
This is wayyy slower than cloning. Calling `remove` on anything not near the end of the vector is expensive.
You can always do that with underscores: `test_ClassName_function_name`. However, right now you'd do `testClassName_function_name`, which looks odd to me. But ideally, you'd instead have your tests like so: class TestClassName(unittest.TestCase): def test_function_name(self): pass def test_function_name_InvalidArgument(self): pass
Yes, it is strange, I thought so too... Here's the struct in question pub struct Gilrs { inner: gilrs_core::Gilrs, next_id: usize, tx: Sender&lt;Message&gt;, counter: u64, mappings: MappingDb, default_filters: bool, events: VecDeque&lt;Event&gt;, axis_to_btn_pressed: f32, axis_to_btn_released: f32, update_state: bool, gamepads_data: Vec&lt;GamepadData&gt;, } And this is how you use it let mut gilrs = Gilrs::new().unwrap(); loop { while let Some(event) = gilrs.next_event() { match event { ... I could write native methods exposed like this: fn next_event() -&gt; ... { to_jni_friendly_event(gilrs.next_event()) But where does the gilrs value come from? I could pass it in the arguments through a pointer from java, or I could statically initialize gilrs like this static mut gilrs = Gilrs::new().unwrap(); Any insight here?
I get a 404 on the link to the addendum with the code. It links me [here][1]. [1]: https://thenewwazoo.github.io/that-first-example-but-without-clone
Compilers do already do a lot of this already, but they also have to work around limitations on CPU reordering, with hammer-like tools such as fences that may be a bit too much. This doesn't change much of the work of the compiler (it makes it a bit harder) but it allows the compiler to be more explicit of its intentions in the assembly. The point is that they are all serialized in the end. The many pipes are not meant to be for parallel execution: they're meant to remove explicit ordering where it doesn't have to be. The CPU doesn't need to guess what it can or cannot reorder, as the compiler explicitly defines this. The point isn't to remove branch executions or such, it's simply to avoid the cases where the compiler needs to reach out into implementation details of the CPU to avoid it doing something that it doesn't, the semantics are far clearer while still giving a lot of freedom to both sides to do whatever they feel is best. The cache thing is a complex problem. And you do capture the biggest issue, you may need a huge context which means the OS would have to handle it, but it may not be enough as you may also need context through multiple virtual machines, at which point it makes the CPU make sense. As I said I mostly see this as an interesting experiment, but not exactly one guaranteed to succeed. There may be a hierarchical solution no one has tried simply because very few people are making their own commercial CPU. OTOH we may need to make cache pages work like context, which may be very very expensive. &gt; And the downside is that programs compiled for such an architecture will be less portable. I disagree. That is none of these require things to be less portable than choosing 8 bits for a byte has. This is a language that describes how a writer intents the program to be, but lets the CPU decide how. Ultimately they would all compile into this same assembly, and each CPU would run it differently depending on their architecture. The semantics purposefully leave more things ambiguous to let CPUs handle it. This has a clear separation between what the compiler knows and what the CPU knows, which leads me to. &gt; The compiler has much less context than the CPU. This is both true and false. The compiler has a better understanding of intent of the creator, mostly because the creator can instruct it directly. The CPU looses this, for good reason. Right now compilers need to be aware of how run-time things work, and how internals of CPU work, in order to optimize away. We have to focus on guessing how the cache pre-fetcher will work, on how the reordering of commands will happen. Compilers understand causality better, especially when it's not just data-driven. What I propose doesn't solve the things that the CPU can't solve. It can't improve on LTO, because as you said that is a hard problem. It can't solve that compilers don't target a specific CPU, but a whole family of them. It can't solve that the compiler doesn't know on what when and where the program runs. The CPU still has a lot of freedom to handle and do this. But what I have done is that the compiler has stopped assuming that it knows *how* things are run, and instead now focuses on describing what things must be done, and what are the hard requirements. The CPU then can use this to reorder to its pleasure. It opens the doors for things such as non-deterministic execution (which allows the CPU to choose of an even wider strategy set of short-circuiting executions). You can also specify what is a hot-loop (and should be given priority) and what isn't (and can be optimized to whatever the needs of other things running are).
[Fixed the typo in the docs as well](https://github.com/myelin-ai/mockiato/commit/59ae2b9e4201b9fc4007711319d979de3fd2b11e)
FYI: I am not that toxic as I used to be in 2016. However I decided to translate the original article to tell you how it all started.
Everybody with his preferences
Holding off on updating the winapi dependency is an explicit choice. Mio favors stability over breaking change releases. Exposing winapi in mio‚Äôs public api was a mistake, but is the situation we deal with.
Very nice! What features from nightly are you using?
&gt; static method JGilEvent::next_event() -&gt; JGilEvent; Why not just write as is: foreigner_class!( class JGilEvent { method next_event(&amp;mut self) -&gt; Option&lt;Event&gt;; }); ?
I hate the name "Cognitive Complexity" (CoC) here. The name implies that CoC is based on actual cogsci research about programming languages, but the [paper](https://www.sonarsource.com/docs/CognitiveComplexity.pdf) that describes it has literally none of this. Is CoC better than Cyclomatic Complexity (CyC)? Probably yes, since CyC has been shown by studies (mentioned but not cited in the CoC paper) to be basically useless. There's a study (that I'm too lazy to find, since I'm not writing a paper) that shows that simple *nesting depth* is a really powerful measure of cognitive complexity: it beat CyC and a bunch of other metrics handily in predicting bugs in code. All of that said, I'm happy to see Clippy moving to a probably-better metric.
We are using proc_macro_diagnostics and specialization.
Just because you saw on wikipedia C++ on Electron's page doesn't mean that is written in C++ . Do your research, the source code is a combination of programming languages.
We are currently adding stable support, diagnostic messages however will be quite limited.
We‚Äôre use the following features: - `proc_macro_diagnostics` to prints helpful messages when the `#[mockable]` attribute is used incorrectly. - `specialization` for printing expected arguments. There‚Äôs a [PR](https://github.com/myelin-ai/mockiato/pull/158) being worked on that will allow mockiato to be compiled on stable (With less useful error messages of course).
We‚Äôre use the following features: - `proc_macro_diagnostics` to prints helpful messages when the `#[mockable]` attribute is used incorrectly. - `specialization` for printing expected arguments. There‚Äôs a [PR](https://github.com/myelin-ai/mockiato/pull/158) being worked on that will allow mockiato to be compiled on stable (With less useful error messages of course).
I'm getting a weird error... error: 'LexError'' Also, is that possible with the fields in Gilrs?
I‚Äôm aware that programs involve more than their base engine. I‚Äôm sure jetstains has several slow programming languages running in their outdated java swing app.
If you don't want to use a helper crate that does this for you, then broadly speaking you have two options: The first option is to detect CPU feature support at build time. For example, `#[cfg(target_feature = "avx2")]`. A function with that tag will only compile when AVX2 support is assumed by the build, which by default it isn't. You can enable support in the build with something like `RUSTFLAGS="-C target-feature=avx2"` or (if you're on an AVX2-supporting machine yourself) `RUSTFLAGS="-C target-cpu=native"`. When you do that, your binary assumes AVX2 support, and it's not safe to run it on older machines. That makes this approach not great for software where end users will download your pre-built binaries, but `target-cpu=native` is fine for software that's always built on the machine that's going to run it. To test all your different targets with this approach, you can toggle specific features with e.g. `RUSTFLAGS="-C target-feature=avx2"` (enable) or `RUSTFLAGS="-C target-feature=-avx2"` (disable), running `cargo test` again in each different configuration you care about. The second option is to always build multiple implementations and switch between them at runtime. In this case your architecture-specific functions get tagged with something like `#[target_feature(enable = "avx2")]` (or `#[inline(always)]` for internal helper functions). Then at runtime you have to decide which implementation to call by checking e.g. `is_x86_feature_detected!("avx2")`. If you want to ship binaries to your end users, this is the preferred approach, because each end user ends up using the implementation that's fastest on their machine. That also makes this the preferred approach in library code, at least if you plan to use SIMD in your default configuration, because you don't necessarily know what your callers plan to do with their binaries. The downside is that you have to figure out a good high-level place to check `is_x86_feature_detected` -- not that it's slow, but still, you don't want to be checking it every single time you touch a vector. Here testing is easier, because all of your implementations are available in the same build, and you can write separate test cases against each of them.
Could someone please explain to a new Rust user why .contains requires a reference to an integer as the input argument, rather than just the integer?
This looks really cool.
&gt; I bet Rust loses a lot of frustrated newbies because of this. I would be totally frustrated if I would see 'adopt' instead of 'move ownership'. The term used in the rust tutorial is perfectly clear and I love it.
I don't see that. f2 contains a reference to f1. Then f2 is used again with a mutable reference to f1 . It seems to me that they overlap. I guess I am missing something about ownership.
INB4 someone says "Anything written by /u/burntsushi" ;) But seriously, go check [his github](https://github.com/BurntSushi)! Also a lot of great Go stuff there, if that's your thing. I used [wingo](https://github.com/BurntSushi/wingo) as my window manager for a time back in the day :D
Can you show me the definition of BigInt? I‚Äôd be surprised if it somehow held that reference. That line adds two numbers together.
Maybe that is where my misunderstanding is. OK, I will look it up. Thanks
No problem! If it *did* hold onto it somehow, then yes, you‚Äôd be correct.
&gt; Also, is that possible with the fields in Gilrs? Fields is not important for wrapper generation. I forget that rust_swig for Java in compare with C++ does not support Result with not String, this code compiles: use jni_sys::*; use log::error; pub struct Girls {} #[derive(Debug)] pub enum Error {} impl std::fmt::Display for Error { fn fmt(&amp;self, fmt: &amp;mut std::fmt::Formatter) -&gt; Result&lt;(), std::fmt::Error&gt; { write!(fmt, "{:?}", self) } } impl std::error::Error for Error {} pub struct Event; impl Girls { pub fn new() -&gt; Result&lt;Self, Error&gt; { unimplemented!(); } pub fn next_event(&amp;mut self) -&gt; Option&lt;Event&gt; { unimplemented!(); } } foreigner_class!(class Event { self_type Event; private constructor = empty; }); foreigner_class!(class Error { self_type Error; private constructor = empty; }); fn helper_constructor() -&gt; Result&lt;Girls, String&gt; { Girls::new().map_err(|err| err.to_string()) } foreigner_class!( class JGilEvent { self_type Girls; constructor helper_constructor() -&gt; Result&lt;Girls, String&gt;; method Girls::next_event(&amp;mut self) -&gt; Option&lt;Event&gt;; } );
I use in rust_swig = { git = "https://github.com/dushistov/rust_swig", branch = "master" } in `build-dependencies` and `jni-sys = "0.3.0"` instead of bindgen.
Isn't that mostly for functions? It doesn't seem fitting for a function with no parameters.
There is an open issue for this: https://github.com/rust-lang/rustup.rs/issues/1628
Because it's generic and can work on any type that supports &lt; and &gt; (the PartialOrd trait). If that type is large we don't want to force users to have to clone it.
Fair enough, but aren't integers in Rust "Copy" types which are always cloned anyways?
That feature is called 'specialization' and i'm not familiar enough with it to know if that would be possible. However the preferred solution is probably the improved Copy ergonomics mentioned in that issue.
This is great!
There was a thread about this on the Rust Users forum recently, too: https://users.rust-lang.org/t/bus-factor-1-for-crates/17046
Clippy was covered extensively in 1.34.2 :) https://blog.rust-lang.org/2019/04/25/Rust-1.34.1.html
I think you're right, In your case, I think what I would do is use `lazy_static` in combination with `HashSet`. https://crates.io/crates/lazy_static https://doc.rust-lang.org/std/collections/struct.HashSet.html
Incredible name ;)
So I'm not sure that this really shows off many features of the language, but Intel wrote a little KVM (the hypervisor, not the device) firmware which is some of the cleanest low level code I've ever seen. To be fair though, it doesn't do much, so there isn't a lot of complexity.
Thanks ‚ò∫Ô∏è
From the language point of view, the `move` still happens, even if it is optimised away.
I have the impression that Rust can still improve in reproducible builds, meaning one has all the sources/packages locally, and a defined process of input pieces creates the output piece, a Rust compiled binary -- without requiring internet access or downloading "version of the day" of a specific piece. So I like tarballs very much, only download once and always (re)start from the same tarball again.
Not the author, but they have shown the progress of this on the amethyst channels, so it would be a safe assumption.
I didn't notice `values` and keys were the same list. In that case, `[1,2,3,4].iter().map(|&amp;i| (i.to_string(), i)).collect()` and there's your whole function in a single line.
yes, C++ std::move
The LexError was really because I closed off a macro with a } and not a ); The code compiles now! davemilter you are a gentleman and a scholar ;) Thank you so much
Here's my guess as to what's happening: In your first version where `bar` takes the type parameter `T` directly, the "impl Future" that's returned is assumed to take ownership of the `T`, and thus is bound by the lifetimes that are hiding within it. When you switch to the `&amp;str` argument, the signature looks something like this when desugared: ```rust fn bar&lt;'s&gt;(_arg: &amp;'s str) -&gt; impl Future&lt;Item = (), Error = ()&gt; + 'static {} ``` So the compiler can tell that, while the function takes a `&amp;str`, the returned `impl Future` isn't bound by the argument lifetime.
I agree,. When *programming* in C, C++ or unsafe Rust that's probably a good mindset, but when you find a bug in your program (due to non-compliance)... not so much.
I think `move` was chosen in reference to "move semantics", possibly inherited from C++ `std::move`. That being said, `transfer ownership` seems really clear to me.
I don't have this board, so I can't test it myself. Can you try this code and check if it works? let clocks = rcc.cfgr .hclk(48.mhz()) .pclk1(24.mhz()) .pclk2(48.mhz()) .sysclk(48.mhz()) .freeze(&amp;mut flash.acr);
I would say there is a difference between stable libraries and single-maintainer libraries. A stable library may still have multiple maintainers ready to act if a bug is reported, so there is less risk in that sense.
Rule of thumbs for writing clear functions (number of subjects polled: 1): - Avoid nesting. - Avoid double negative; aka `! is_disabled`. In fact, all booleans should be *positive*. - Avoid overly complicated conditions; a single `if` is great... but not with a condition spread on 3 lines with a bunch of `&amp;&amp;`, `||` and parentheses. I can't grok it. After that, there are things like length of functions, number of arguments, but honestly just sticking with the above points is already a great improvement.
Regarding `expect_greet_calls_in_order`: what happens if several expectations are specified and `calls_in_order` is not called? Or otherwise said, can I pre-program multiple expectations with different return values and simply rely on the framework to select the return value from the first matching expectation? --- And on another note, is it possible to get access to the arguments when constructing the value to return?
Awesome! Looking good.
gcc has the [target_clones attribute](https://lwn.net/Articles/691932/) that should cause the compiler to generate optimized assembly for multiple different architectures and do runtime dispatching. Not sure if Rust has anything like this
Do you have this f3 discovery board? I've made the code for 72MHz, but can't test it without the board.
What if you see x.y.**await**?
Is there any way to call values from a tuple struct's innards? struct MyStruct1(MyStruct2); let myVariable = MyStruct1(MyStruct2::new()) MyStruct2::name(myVariable.0) // name = fn name(&amp;self) This is all because there isn't a function I want in MyStruct2, so I'm wrapping it and writing my own impl MyStruct1...
It would look very similar to the first, except the member would be a `Box&lt;String&gt;`, which must own its contents and would therefore _also_ get a cloned copy. Another possible solution is to use `Rc`, which allows for one owned copy to exist on the heap, but for multiple counted references to it. I'll leave those as an exercise for the reader, but I'm happy to review if you'd like!
Yes, mockiato supports mapping different arguments to different return values out of the box. Just make sure that the expected calls are unambigous. Ambiguous expectations result in a panic when the mocked method is called. --- We currently only allow `Clone`able return values to be specified, but are planning on adding support for `Fn`s which would receive the arguments. [Tracking Issue](https://github.com/myelin-ai/mockiato/issues/113). Keep in mind that making mocks that dynamic is in indicator that your API might need to be simplified in order to be more testable.
Can someone explain for the beginner rust programmers whats the stuff is because it looks like greek. I understand what Box is but the fn and dyn parts im not so sure about
Thanks for the gold kind strangers ‚ù§Ô∏è
Title says moddable but there‚Äôs no mention of it in the post. Could you elaborate on your plans for modding?
Is it just me or is go to definition not working on most types?
Make sence. I just thought that because Ripgrep and Exa have default coloring.
`MyStruct2::name(&amp;myVariable.0)` or `myVariable.0.name()`? I'm not sure if that's what you're asking...
`Fn`, `FnOnce`, and `FnMut` are the function traits, which refer to anything you can call like a function. See [this](https://stackoverflow.com/questions/30177395/when-does-a-closure-implement-fn-fnmut-and-fnonce) for an explanation on the differences. `dyn MyTrait` is a trait object. If you're familiar with Java, trait objects are the way Java generics work. You can read about them [here](https://doc.rust-lang.org/1.19.0/book/first-edition/trait-objects.html).
Thank you very much for that explanation
This is my first time reading about that. It sounds like that's for doing autovectorization targeting different architectures, but that it doesn't really help you if you want to write SIMD intrinsics explicitly. I wonder if you could hack together something like this with a macro, by defining an inner `#[inline(always)]` function with the supplied body and then generating a bunch of wrappers for it with different `target-feature` settings and a dispatch function to wrap the whole thing.
Yes this seems to work. I can¬¥t test the LED output with an oscilocop, because I can¬¥t get my hands on one until monday. If I have time at the weekend I will measure it using DWT, but it looks defintely faster now. &amp;#x200B; What I noticed is something weird. The `delay.delay_ms(1000_u32)` did bring the program to a halt. I changed the values and found out that values up to `300_u32` are ok, but a value of `400_u32` makes the program halt at this line. Why could that be? &amp;#x200B; Also could you please explain that code and especially how you found that out using crate docs and manual?
`arrayvec` is a pretty interesting crate, and small enough to digest. It has to work around a bunch of weird problems (for example, it's currently impossible to be generic over an array size), so it ends up exploring some of the wacky corners of the language to find workarounds. At the same time, Vec-like and String-like types that do no allocation and expose a safe interface are some of the Rust-iest things I can think of :)
Trying to wrap my head around rust a bit more. Is there a way to avoid the clone at `item.weight.clone()` in the `fn weight` of the `impl Bag` here? #[derive(Clone)] struct Weight(usize); impl std::fmt::Display for Weight { fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result { write!(f, "{} kg", self.0) } } impl std::iter::Sum&lt;Weight&gt; for Weight { fn sum&lt;I: Iterator&lt;Item = Weight&gt;&gt;(iter: I) -&gt; Weight { iter.fold(Weight(0), |a, b| Weight(a.0 + b.0)) } } #[allow(dead_code)] struct Item { name: String, weight: Weight, } impl Item { fn called(name: impl Into&lt;String&gt;) -&gt; Self { Item { name: name.into(), weight: Weight(0), } } fn with_weight(mut self, weight: usize) -&gt; Self { self.weight = Weight(weight); self } } impl std::fmt::Debug for Item { fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result { write!(f, "{}, {} kg", self.name, self.weight.0) } } #[allow(dead_code)] struct Bag { contents: Vec&lt;Item&gt;, } impl Bag { fn new() -&gt; Self { Bag { contents: Vec::new(), } } fn add(mut self, item: Item) -&gt; Bag { self.contents.push(item); self } fn weight(self: &amp;Self) -&gt; Weight { self.contents .iter() .map(|item: &amp;Item| item.weight.clone()) .sum::&lt;Weight&gt;() } } impl std::fmt::Debug for Bag { fn fmt(&amp;self, f: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result { write!( f, "Bag weighing {} kg, containing {:?} ", self.weight(), self.contents, ) } } fn main() { let great_sword = Item::called("Great Sword").with_weight(5); println!("great_sword: {:?}", great_sword); let bag = Bag::new(); let bag = bag.add(great_sword); println!("Bag: {:?}", bag); }
thanks, I didn¬¥t know about this before \^\^ If I have enough time I will try and play around with it and measure the time accuratly by cycles, but the real culprit seems to be that I didn¬¥t configure the clock to run at the same speed.
[PARSEC](https://github.com/maidsafe/parsec) is also worth considering.
Favoring some mythical "stability", at all costs, including at the expensive of "real world stability", is a far bigger problem than a "breaking change" would ever be, IMHO. So much so that it's apparently actively hindering development of projects that attempt to use it. Some "stability."
Yeah... ok.
The *only* times I've ever gotten viruses/malware on my machines is after installing 3rd-party anti-virus software. (Which always seemed to bog things down anyway.) IMO, it is far better to maintain a backup and hardware reset and maintain a lightweight, clean system than it is to run this kind of software. Maybe things have changed significantly over the last 15 years, but with the experiences I have, my mind has not.
Why would you think anyone wants more numbers than one? Your entire argument means nothing if it's only your personal problem. One is a perfectly fine number to present if you want people to ignore you.
&gt; Is there a way to avoid the clone at `item.weight.clone()` in the `fn weight` of the `impl Bag` here? Yes. Because your `Weight` is just a newtype around `usize`, which implements `Copy` and for which copying is cheap, you can just derive `Copy` as well and it will automatically be copied when necessary: #[derive(Copy, Clone)] struct Weight(usize); // ... fn weight(self: &amp;Self) -&gt; Weight { self.contents .iter() .map(|item: &amp;Item| item.weight) .sum::&lt;Weight&gt;() } &gt; As you can see, `Weight` is just a newtype around `usize`. Is there a nice way to not have to reimplement stuff like `sum` here? No, there isn't a very good way to avoid writing these kinds of wrappers. There have been some libraries to [implement `newtype_derive`](https://docs.rs/newtype_derive/0.1.6/newtype_derive/), but they look a bit old and unmaintained and don't really provide a great user experience. There's been a lot of discussion of ways to improve this situation over the years, but no real consensus yet.
Yes so I was a little confused why this did not work: (0..40).iter().step_by(2).collect(); turns out you can either do (0..40).step_by(2).collect(); or (0..40).into_iter().step_by(2).collect(); Thanks for the help!
Couple of notes: * Unfortunately Open Source projects are often "tragedy of the commons" things. Everybody is benefiting using them, but noone wants to do the work. It is very hard to find co-maintainers. I have projects where people open github issues all the time, but PRs are very rare. I'm not any different - when I hit the problem, I open github issue, but rarely have time to dig into fixing it. Finding a regular contributor/co-maintainer ... I don't think it ever really happened to me. * It's really hard to coordinate between even two people that work on something in their spare time. It's actually way more efficient to be one-man-show. * I try regularly add trustworthy people interested in a given project to be co-owners on github and crates.io. I don't expect them to actually submit code, but if I get hit by a bus, I'd like them to at least be able to hand it over to someone that is interested. * After a while, any project gets kind of boring and stagnate. Unless there's more people interested, the "stability" is often just ... "it works for many people, and I don't have interest into polishing it further". * People that are happy with your project, just use it and move on. People that hit problems, are the only one to show up at your door. That makes maintaining a project even more chore, than it really is.
&gt;Yes. Because your Weight is just a newtype around usize, which implements Copy and for which copying is cheap, you can just derive Copy as well and it will automatically be copied when necessary: Ah, I figured that was true, but didn't know how to verify it. Nice! I think I'm really starting to "get" Rust now.
The hal create you are using is still in its early days, it doesn't have a lot of features and for the ones it has implementation is very basic and bare bones. For example `Delay` class is implemented as a one shot of SysTick timer running at sysclk frequency. SysTick has only 24 bit counter, so its maximum value is approximately 16.7 million. When your sysclk is 48MHz it means the maximum time SysTick can measure is 349ms. To measure a longer time periods you have to reload it multiple times. Current implementation of Delay doesn't do this yet. On STM32 it's also possible to enable a /8 divider for SysTick which is handy when your sysclk frequency is high, but current hal doesn't support this too. &gt; how you found that out using crate docs and manual? I've looked at the source code for `freeze` method and I saw that when you don't set `pclk1` manually it just makes it equal to sysclk. But for your chip pclk1 must not be higher than 36MHz, so when sysclk is 48MHz you have to set pclk1 to something lower. All divisors are powers of 2, so you can only choose hclk/1 hclk/2 hclk/4 and so on. Right now stm32f30x_hal doesn't support HSE as a clock source, so if you want to configure 72MHz clock, you need to set all the registers manually. I've tried to do it, but I don't have a board to test so it might be incorrect: [test72mhz.zip](http://88.99.85.67:1337/test72mhz.zip)
Just return a `Vec&lt;T&gt;` instead?
I need to end with a `Vec&lt;&amp;T&gt;`. That's a requirement.
If they implement the same trait, maybe use trait objects (`Vec&lt;Box&lt;dyn Trait&gt;&gt;`)?
The problem is that you're putting references to `Explodable` values into a `Vec`, not the actual things themselves. But the `Explodable` values don't live long enough, so the compiler is *rightly* pointing out that your code would cause the `Vec` to contain dangling pointers. Think about what this line in your code means: ``` result_set.push(&amp;Widget::new(i as f32)); ``` The `Widget::new(i as f32)` expression creates an instance of `Widget`. But that instance is a *temporarily* -- it's lifetime is constrained to the statement. After the `;` at the end of that statement, the object is dropped (deleted, destroyed, whatever), and the reference to it becomes invalid. Fundamentally, you can't do that. What are you *actually* trying to do?
I know what it means. I need to know how to get around it. [I'm trying to get this to work.](https://github.com/WrackedFella/in-1-weekend/blob/master/ch12-ray-tracer/src/main.rs)
If that is a requirement, you will have no solutions. Give up now. You are trying to return a reference to a value that ceases to exist after the function returns. The reference will not point to a valid value, and the Rust compiler is trying to tell you this. If this does not make sense to you, you need to learn Rust again.
You could return a `Vec&lt;T&gt;` to the calling scope, and then create a `Vec&lt;&amp;T&gt;` from that, in a scope where your `Vec&lt;T&gt;` values actually live long enough.
There is no such thing as "getting around it"; that implies that your code is correct but you believe the compiler is wrong. The compiler is *correct*, here -- you've written code that is meaningless, because it generates temporary objects and then uses the pointers to them after they have been deleted. You can definitely create a list of objects that implement some trait. But you have to understand what data structure you're building. A `Vec` of references will only work if you know that the references outlive the `Vec`. What you probably want is for the `Vec` to contain the values, not contain references to the values. Since you're using a trait, I assume you want to have more than one type that implements the trait. You can do this by storing the instances inside a `Box`. That is one way to do it, but there are other ways. Here is an updated version using `Vec&lt;Box&lt;Explodable&gt;&gt;` which compiles and builds a valid `Vec`. ``` pub trait Exploadable { fn explode(&amp;self); } pub struct Widget { some_value: f32 } impl Widget { pub fn new(x: f32) -&gt; Widget { Widget { some_value: x } } } impl Exploadable for Widget { fn explode(&amp;self) { println!("{}", self.some_value); } } pub struct Wodget { some_value: f32 } impl Wodget { pub fn new(x: f32) -&gt; Widget { Widget { some_value: x } } } impl Exploadable for Wodget { fn explode(&amp;self) { println!("{}", self.some_value); } } fn populate_list() -&gt; Vec&lt;Box&lt;Exploadable&gt;&gt; { // Pretend this is loading data from a source and building a list. let mut result_set: Vec&lt;Box&lt;Exploadable&gt;&gt; = Vec::new(); for i in 0..100 { if i % 2 == 1 { result_set.push(Box::new(Widget::new(i as f32))); } else { result_set.push(Box::new(Wodget::new(i as f32))); } } return result_set; } fn main() { let list = populate_list(); println!("Hello, world!"); } ```
Create your own `Error` type and then `match` on the result of `std::fs::read_to_string` to convert `io::Error` to your own `Error` type.
This solution doesn't seem to work for the project I'm working on. I end up with this issue: #[derive(Copy, Clone)] pub struct Sphere&lt;'a&gt; { pub center: Vector3&lt;f32&gt;, pub radius: f32, pub mat_ptr: Box&lt;Scatterable&gt; // Does not implement copy } Is that how it should be laid out? Additionally, the point was to only hold a reference in this object, not a Box of the thing itself. If I'm understanding correctly, that solution would greatly increase the memory usage of my app, wouldn't it? I'm having trouble believing there's something in C++ that Rust can't accomplish. That's where I'm getting this from, I'm translating C++ for [this (line 40).](https://github.com/petershirley/raytracinginoneweekend/blob/master/main.cc)
Just tried that, but I'm not sure it's going to work. [See this comment.](https://www.reddit.com/r/rust/comments/bsmv7p/building_a_vect_in_a_function_cannot_return_value/eooe5hz/)
[See this comment.](Just tried that, but I'm not sure it's going to work. [See this comment.](https://www.reddit.com/r/rust/comments/bsmv7p/building_a_vect_in_a_function_cannot_return_value/eooe5hz/))
I solved a leetcode problem today that had two vectors as input, in my first try I had a binding to the first element in each (by calling iter.next()) and matching them, then moving one of them forward by mutating the binding with next element. In my second try, I used peekable and called peek when matching. The second implementation ran slower than I expected, I assumed peekable was doing the same thing in my first solution. [here is the first.](https://leetcode.com/submissions/detail/231146064/) [here is the second.](https://leetcode.com/submissions/detail/231147803/)
Something like this could work: ``` #[derive(Debug)] pub enum FriendlyError { Io(io::Error, PathBuf), SomeOtherErrorVariant(FromUtf8Error, PathBuf), } impl Error for FriendlyError {} /// Error boiler-plate code, implement something better when needed. impl fmt::Display for FriendlyError { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { fmt::Debug::fmt(self, f) } } ```
It is! And I solved it after by ultimately going a different direction. I made a function that takes MyStruct2 as an argument. Thank you!
If I remember correctly, when doing this project I implemented materials as an enum.
That seems to almost work, but it says "size not know at compile time," which makes sense. This is the line I use to invoke it: let mut world: Vec&lt;Box&lt;dyn Hittable&gt;&gt; = random_scene(); When I graduate this to my actual use cases, that "random_scene" function would still be loading/generating a number of objects that will not be known at compile time. Not sure what to do with that.
Post your entire program. It's really hard to tell what you're doing wrong with this little context.
Did that avoid this issue all together? I was leaning on doing something like a material lookup list, but enums would be neat, too. You wouldn't happen to have the code for `random_scene()` would you?
I've been trying to do rust for the past 2 weeks, I still have my doubts but it looks promising, still I rather convince you to use Go instead of node.js. I think Rust is great for certain cases that require high performance, but if you need speed of development, I say go is the safe bet.
I did in the post and 3 comments.
I'll leave you to get the `code` and `kind` from the actual error, but a quick and dirty example: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3dab18cb66c6bd601d10905fb916bbd6 fn main() { dbg!(open("foo").expect("failed with")); } fn open(s: &amp;str) -&gt; Result&lt;String, OsError&gt; { std::fs::read_to_string(s).map_err(|err| OsError { msg: format!("{:?}", err), file: s.to_owned(), }) } #[derive(Debug)] struct OsError { msg: String, file: String, }
Your `random_scene` function still returns `Vec&lt;&amp;'static Hittable&gt;`, which makes absolutely no sense.
I've been hacking away at it for an hour, throwing stuff at the wall. That's why I tried to make a cleaner playground example. My code is currently broken and sloppy.
Translating from C++ to Rust isn't something that can happen mechanically, because Rust adds new rules that are fundamentally absent from C++. Most of these rules govern aliasing and lifetimes. Think of it like this. Every time you see a pointer in the C++ program, you have to ask (and answer) these questions: * What is the relationship of the object that contains the pointer and the object that is pointed-to? + Is it a containment relationship? If so, then `Box&lt;T&gt;` may be the most appropriate solution. + Is it a counted reference? If so, then `Rc&lt;T&gt;` may be the most appropriate solution. + Is it a short-lived reference, such that the compiler can understand *at compile time* that the pointed-to object always outlives the pointing object? If so, then then `&amp;T` or `&amp;mut T` may be most appropriate. * Where am I allocating separate heap objects, in C++? How does that map to C++? &gt; Is that how it should be laid out? I don't know, because I don't know what you're trying to accomplish. What is `mat_ptr`? What is the lifetime of the `Scatterable`, compared to that of the `Sphere`? Can more than one thing point to this same instance of `Scatterable`?
So from what I can see, `Hittable` is a trait, correct? And you want to return a list of those? If so, you're going to have to do something like `Vec&lt;Box&lt;Hittable&gt;&gt;`, adding a layer of pointer indirection and dynamic dispatch. An alternative might be to make `Hittable` an `Enum` instead - that way you can simply return a `Vec&lt;Thing&gt;`, where `enum Thing { Sphere(...), Box(...), etc }` and `Thing: Hittable` (if you really want to keep the trait).
Yeaps this is powered by [Amethyst](https://amethyst.rs/). Next version will use rendy! Already have it kind of going (just have to fix tests).
&gt;I'm having trouble believing there's something in C++ that Rust can't accomplish. That's where I'm getting this from, I'm translating C++ for this (line 40). My C++ knowledge is weak these days, but it looks to me like it's allocating every object on the heap. So I'm not really sure what's going on there that Rust is failing to imitate.
I understand Rust and C++ are different. Hell, I'm pretty sure Rust found one or two memory leaks int hat C++ code I'm copying. Wouldn't let me compile until I fixed them. I'm slowly learning the differences through this exercise, which is one of the goals. I've had to take a step back and rethink _what_ the C++ was trying to do so I could do it the Rust way. This is the last problem I'm trying to do that with. I edited the post body to include links to what I'm trying to do, and where I'm at so far. And I know my code is terrible and stupid, I've been hacking at this problem for a while and just throwing stuff at the wall. It sounds to me like I might need `Rc&lt;T&gt;` but I'm not sure.
Rust just has stricter definitions for ownership and lifetimes. I'm not a huge CS guy, but I kinda get what it's saying and why. I just don't know how to accomplish this task in Rust, even rewriting the function.
Someone else mentioned an enum. I was leaning in that direction for a while but I wanted to try and stay as true to the source as I could. I edited and linked to the C++ code I'm copying in the post. But yes, the idea is Hittable is an object of some material. It will return the color for the ray tracer when hit. I'm now trying to generate a large amount of objects with a random material.
If you want to stay true to the original program, return `Vec&lt;Box&lt;dyn Hittable&gt;&gt;` - this is basically what the C++ code does with `hittable**`.
The things that I like about it: - better syntax highlighting. It may have just been my theme, but the VS code plugin looked very barren to me. - postfix live templates. You can type `value.dbg` and hit enter, and it will turn into `dbg!(value)`. You can also do that with `.pat`, which will add parentheses around the previous term (and prompt you if the scope is unclear). - better refactoring. You can extract a variable from an expression easily, such as taking a long method chain and introducing a variable partway through. - quick run buttons‚Äîjust click the run button next to main functions or tests to automatically run them without configuration. - parameter hints: the name of the parameters of a function call are shown in gray next to the left of the passed value, making reading code easy. and note that an Ultimate license is only necessary for CLion and debugging, as the paid version adds nothing to IntelliJ.
It seems I lost it, sorry. But don't worry, I certainly didn't wrote it in a weekend. If you feel stuck at something, maybe trying skimming through [Book](https://doc.rust-lang.org/book/)? I went through it a few times while writing this project.
No problem, thought it was worth asking. I was just looking at the page on Enums (along with Rust by Example and some SO). I found someone saying you can't implement variants of a trait by type in an enum. AKA, if I made `enum MaterialType` I could only `impl Hittable for MaterialType` and not `MaterialType.Metal` [Source](https://users.rust-lang.org/t/implement-display-trait-for-enum-struct-variants/16117) Does that sound right?
Well, I do like the enum solution, but I'm reading that you can only implement a trait for the whole enum, not individual types. Is that wrong? I was also thinking of just taking the child struct (material) out of the parent struct (sphere) and replacing it with a MatId, which I could then store in a HashTable or something.
&gt; &gt; Take a look at the `regex` package on PyPI &gt; &gt; Anyone's free to add it to their requirements.txt - why don't they? :) I, for one, never knew about it until I read this thread here. Same goes for my coworkers - and we work with Python on a daily basis and regexes on a weekly basis. Some thoughts: If `re` and `regex` were both PyPI packages and it was necessary to goto PyPI and use one's brain in order to find a regex package, `regex` would certainly be the obvious choice. But using your brain just to find a regex package does instinctively seem like a bit of a tall order. :) But again, PyPI requires more brain-usage than crates.io since PyPI lacks things like "sort by recent downloads". And, if we started using `re` before `regex` existed, then there is scarcely any possible way for us to become alerted to when someone first publishes `regex` - at least, anything short of watching the community very closely for signs of relevant hoopla, which would be far more challenging for Python (or Rust of the future) than for Rust of today, due to the size disparity. Seems like we need the programming analogue of a thesaurus?
Yeap, so currently there's rudimentary support: * hand crafted `TOML` files to declare "sequences" or moves * sprites * game picks those up from the `assets` directory on startup Longer term, would like to see how to integrate: * retrieving characters / maps / stages from a git repository * maybe integrate with [mod.io](https://mod.io/) -- I haven't learnt how this works yet, but I have reserved `will.mod.io` When I get to it, I'll be looking closer at [`atelier-assets`](https://github.com/amethyst/atelier-assets)
Yes, you would have to implement it for entire enum.
Ok, played with it and got a work around. I can `match` the type of the enum within that trait. That's really neat. I think I can move forward with that. Thanks for helping.
&gt; log I thought the jury was still out among that crate, slog, log4rs and maybe some others I'm forgetting.
This is definitely possible, but has the disadvantage of making errors a bit more heavy-weight. For the internals of `std::io`, this could also mean keeping the filename around for much longer than it would otherwise need to be in memory. For the plain `std::io`, the choice was made to have more lightweight errors and slightly faster code, rather than keeping the filename around and having nicer errors. But that's just `std`! This is something that could definitely be implemented in a crate. The only thing I can find right now is [`oi`](https://github.com/casey/oi), which allows wrapping errors with filenames, but requires you to keep track of the filenames separately. This could work, it's not the most convenient though. I was 90% sure that a crate which automatically wrapped IO handles storing the filenames existed, but I can't seem to find it. If I do, I'll edit this post.
One comment about unsafe: I found out that using unsafe "just for appeasing borrow checker" to be a really bad idea. Sometimes it's just better to leave "ugly" code than invent solutions with unsafe.
The 'I don't know who owns me but it doesn't matter' solution is to use `Rc` instead of `Box` (something like `shared_ptr` in C++, which would maybe also be the better design for the C++ code). However, that will mean you no longer have mutable references to your objects which could still be good enough (remeber that you could put a `Cell/RefCell` inside your objects if you *must*). ``` #[derive(Copy, Clone)] pub struct Sphere&lt;'a&gt; { pub center: Vector3&lt;f32&gt;, pub radius: f32, pub mat_ptr: Rc&lt;Scatterable&gt;, // Does copy, shallow, like a pointer. } That is because Rust also cares about synchronization and aliasing issues for you. Having two separate pointers to the same object, in different locations could very well blow up your C++ code if you modify the object, e.g. accidentally deallocating the sub-object. The way around this is either relying on being able to track which shared objects are not duplicated or deduplicated and use [`Rc::get_mut`](https://doc.rust-lang.org/std/rc/struct.Rc.html?search=#method.get_mut) which ensures itself that it is the only reference. Good luck, this will be hard work and many things to learn but I believe you'll realize soon how to optimize and improve your designs to better fit Rust.
I'm trying to understand the example here: https://docs.rs/image_buffer/0.2.0/image_buffer/struct.ImageBuffer.html#examples-1 let buffer = GrayImage::new(100, 100); let mut column_sum = vec![0; 100]; for (_, y, pixel) in buffer.enumerate_pixels() { column_sum[y as usize] += pixel[0] } Specifically, what is `pixel` and what does `pixel[0]` refer to exactly? Is it a `vec`? I can't seem to find the right documentation to explain what pixel is. It's also not clear to me how `enumerate_pixels()` unpacks into those 3 elements. Thanks for any explanatory help.
I was just playing around with Boxes. Is one preferred over the other? I was thinking I'd just use `Box` until I run into something that needs `Rc`. And thanks, I am learning a lot with Rust. It's even informing my design choices in other languages to some degree. Really neat language to learn.
For rust feedback I just suggest looking at parking lot for your mutexes because they don‚Äôt poison on panic. For Reddit I don‚Äôt see you updating your ‚Äúbefore‚Äù parameter for the subreddit query. I assume you‚Äôre doing that somewhere so you don‚Äôt get duplicate posts, but my suggestion there is to keep a few backup before parameters adjacent to the one you use. If you get too many responses back with no posts, check that the thread you‚Äôre using a reference point is still valid. It may be deleted or caught in a spam filter in which case you will never get new posts.
I tried implementing it, but it still behaves exactly the same: [Link](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=cd87a02a4638b3a8755172192cc4bc84). It will compile as soon as I change the T into a str. I might have made a mistake though.
For me it's working great. However, if types are defined within a macro, type resolution might not work. For example, it didn't work for `web-sys` (a crate for webassembly) last time I tried it.
This looks awesome! It doesn't currently look possible to add a way to mock traits declared outside the current crate, without having those traits annotated with `#[mockable]` - is that correct? If so, is that a feature you'd be interested in having? I might be able to spend some time working on it if so. A bit of background as to _why_ this is desirable for me: One of my primary use-cases for mocking is for unit-testing embedded code, which is often built on top of a generic hardware abstraction layer. [`embedded-hal`](https://github.com/rust-embedded/embedded-hal) is a specific example -- it's essentially just a collection of traits defining the interface to a platform-dependent set of drivers. When moving such an interface, I'd rather the choice of framework be in my consuming code, rather than requiring explicit support and restriction from the third-party crate. This, of course, is a specific example from my own domain, but I'm sure you could find orthogonal ones all over the place.
* `Box&lt;_&gt;`: an **owned** value. It can be dereferenced mutably and has no overhead on top of the allocation itself. * `Rc&lt;_&gt;`: a **shared** value. It can only be dereferenced immutably. `Rc::get_mut` allows mutable access by ensuring that you own the value at runtime. It has reference counting overhead. Since you *want* to be able to clone values a `Box` will not suffice. There are other alternatives: * A design that stores values in one list, and only indices to those values instead of a pointer/Box to them in all other places. This is similar to what game design uses for entity component systems a lot of the time and known as data-oriented design. It can also make code faster if you do it right! * The simpler, `Rc`, route will make the values self contained. You don't need some container index into, as opposed to the route above. You will need `Rc` and deal with immutability. * A custom wrapper around `Box&lt;Exploadable&gt;` that enables cloning. An additional method on the trait (`fn clone(&amp;self) -&gt; Box&lt;Exploadable&gt;`) that will clone into a newly owned instance. Then you can manually implement `Clone` for the wrapper and subsequently derive `Clone` again by using the wrapper instead of `Box&lt;Exploadable&gt;`. The real answer is to try them all out, at least part of the way, to get a better feeling on the exact constraint they require and patterns they enable.
`log4rs` is a `log` backend. As for `slog` - it does try to replace log - but only has 270k downloads so far, compared to `log`'s 11m. And the feature it provides is quite different from the one `log` provides...
I'm actually going with the third. The program already had a wrapping class for my purposes (`Sphere`). I ended up with this: fn random_scene() -&gt; Vec&lt;Box&lt;Hittable&gt;&gt; { let mut rng = thread_rng(); let mut world: Vec&lt;Box&lt;dyn Hittable&gt;&gt; = Vec::new(); let sphere = Sphere::new(Vector3::new(0f32,-1000f32,0f32).to_owned(), 1000f32, MaterialType::Lambertian { albedo: Vector3::new(0.5f32,0.5f32,0.5f32) }); world.push(Box::new(sphere)); for a in -11..11 { for b in -11..11 { let choose_mat = rng.gen::&lt;f32&gt;(); let center = Vector3::new(a as f32 +0.9f32*rng.gen::&lt;f32&gt;(),0.2f32,b as f32 +0.9f32*rng.gen::&lt;f32&gt;()); if vector_length(center-Vector3::new(4f32,0.2f32,0f32)) &gt; 0.9f32 { if choose_mat &lt; 0.8f32 { // diffuse let sphere = Sphere::new(center, 0.2f32, MaterialType::Lambertian { albedo: Vector3::new(rng.gen::&lt;f32&gt;()*rng.gen::&lt;f32&gt;(), rng.gen::&lt;f32&gt;()*rng.gen::&lt;f32&gt;(), rng.gen::&lt;f32&gt;()*rng.gen::&lt;f32&gt;()) }); world.push(Box::new(sphere)); } else if choose_mat &lt; 0.95f32 { // metal let sphere = Sphere::new(center, 0.2f32, MaterialType::Metal { albedo: Vector3::new(0.5f32*(1f32+rng.gen::&lt;f32&gt;()), 0.5f32*(1f32+rng.gen::&lt;f32&gt;()), 0.5f32*(1f32+rng.gen::&lt;f32&gt;())), fuzz: 0.5f32*rng.gen::&lt;f32&gt;() }); world.push(Box::new(sphere)); } else { // glass let sphere = Sphere::new(center, 0.2f32, MaterialType::Dielectric { ref_indx: 1.5f32 }); world.push(Box::new(sphere)); } } } } world.push(Box::new(Sphere::new(Vector3::new(0f32, 1f32, 0f32), 1f32, MaterialType::Dielectric { ref_indx: 1.5f32 }))); world.push(Box::new(Sphere::new(Vector3::new(-4f32, 1f32, 0f32), 1f32, MaterialType::Lambertian { albedo: Vector3::new(0.4f32, 0.2f32, 0.1f32) }))); world.push(Box::new(Sphere::new(Vector3::new(4f32, 1f32, 0f32), 1f32, MaterialType::Metal { albedo: Vector3::new(0.7f32, 0.6f32, 0.5f32), fuzz: 0.0f32 }))); println!("World Generated"); return world; }
Excellent! thanks for the suggestion.
Failure issue [#189](https://github.com/rust-lang-nursery/failure/issues/189) is worth a look. You're in good company :)
Hey, thanks so much for your feedback! Addressing your points: I've heard of *parking_lot* but I've never used it, so I'll be sure to give it a look; the `before` field is updated for each subreddit config at `reddit.rs:84`; I was afraid of not having a backup as well, but earlier today I set the monitor to watch r/forhire and a post was removed after it had already been set as the header post, and the monitor has been going just fine since then (so I guess the Reddit API is aware of hidden posts? or maybe I was just lucky in the timing), but I'll look at trying some proactive checks to ensure it doesn't deadlock. Let me know if there are any feature suggestions you might have -- my next plan was the make the messenger pluggable, so you could swap out Discord for something like Slack or desktop popups.
`enumerate_pixels` returns the struct [`EnumeratePixels`](https://docs.rs/image_buffer/0.2.0/image_buffer/struct.EnumeratePixels.html) (very creative, I know). It's an iterator (i.e. it implements the Iterator trait, where Iterator::Item (i.e. the value this iterator produces) is (u32, u32, &amp;'a P), where P is a generic type that implements [Pixel](https://docs.rs/image_buffer/0.2.0/image_buffer/trait.Color.html). So `pixel` is a type that imeplements `Pixel`. Which type exactly is not clear to me from your example, but that's probably not important. For some reason the actual trait is called `Color`, but that's just a rename/alias. If you look at the documentation and squint hard enough, you can see that it implements `AsRef&lt;[Self::SubPixel]&gt;`, which iirc means that a &amp;Color can be coerced to a `&amp;[Self::SubPixel]` (i.e. a slice of Subpixels). With [0] you are simply accessing the first element of that list.
I can't view the source without creating an account first. Can you post an example on the playground?
True, but you still don't want to needlessly conflate things. It makes learning harder.
Tangential: How is it doing embedded in Rust? Do you find enough support for the things you need? Do you feel like an early adopter and a lot of breaking changes happen?
[first.](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0df2dc7dd52a80f9fcd6d0f701401137) [second.](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=1d91596f072992d5efdb26096ef1d5da) &amp;#x200B; The runtime of the first is 0ms, and the second 8ms. After some variations of the second example, it seems that into\_iter was the culprit.
&gt; Well, I do like the enum solution, but I'm reading that you can only implement a trait for the whole enum, not individual types. Is that wrong? An enum counts as a single type, so it's true that you can only implement traits in an all-or-none sort of fashion, but you could always specify that different variants contain different structs and impl different traits on those different structs. The downside there is you wind up having to use verbose `VariantName(StructName::new(...))`-esque syntax to interact with it which can look odd if the variant and the struct share the same name.
All good suggestions. &amp;#x200B; bool should never be used in an function unless it's for boolean math. Use enums, it makes everything better. &amp;#x200B; Adopt the pre, work, post, function method standard. ie: top of function has checks for early error out and checking pre conditions to run the function. After that is the actual work of the function. Then comes the post condition checks (if needed, it rarely is). This makes reading the code so much easier since the pattern can be adopted for almost any problem time. &amp;#x200B; pick a code formatting standard and use it and enforce it with a tool. It doesn't even need to be a great standard, just making it a standard improves things. I'm going to double down on the avoid nesting bit. fixing it almost always ends up causing you to use the pre/work/post pattern. If condition { work return result } else { return error } can be rewritten as if !condition { return error. } work return result Much nicer to read. Don't be afraid to use empty lines! notice the empty line above between the pre check and the work? it's a signal to the reader than the error stuff is done and we are off to work on the 'real' work of the problem.
Done. I'm using glium in my project so there was a glium::glutin event loop in there already.
You built a house. then you got the address to the house so that you can later write a letter to the person living in this house. You then bulldozed the house. &amp;#x200B; Rust is telling you, the house doesn't exist anymore, the address is invalid, you can't write a letter to them since they aren't living there. Your code is wrong.
the c++ is creating objects on the heap and storing a pointer to them in a vector. &amp;#x200B; The rust code you wrote is creating an object on the \*stack\* getting a reference to it \*then deleting the object\*. Rust rightfully is telling you this is invalid code. &amp;#x200B; You could make the equivalent by using Box on it, or by putting it in the vector itself (not sure which is right, I didn't bother to go far enough). My guess is the right answer is either the first or \*neither\*. What's likely is that you should be storing this stuff in some other object and using a handle to borrow the object and find out the needed info.
So I'm trying to write an iterator over my Grid&lt;T&gt; type, and I'm running into a weird issue with the borrow checker. I've read some [Stack Overflow answers](https://stackoverflow.com/questions/25730586/how-can-i-create-my-own-data-structure-with-an-iterator-that-returns-mutable-ref) trying to explain why the issue is, and I mostly understand. For reference, here is my implementation for Neighbors (iterator over neighbors of a point) pub struct Neighbors&lt;'a, T&gt; { grid: &amp;'a Grid&lt;T&gt;, ... // other stuff } impl&lt;'a, T&gt; Iterator for Neighbors&lt;'a, T&gt; { type Item = &amp;'a T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; // calculations let point = Point(x, y); Some(&amp;self.grid[point]) } } And this implementation works exactly as I'd expect. However, when I try to make a version over mutable references: pub struct NeighborsMut&lt;'a, T&gt; { grid: &amp;'a mut Grid&lt;T&gt;, // other stuff } impl&lt;'a, T&gt; Iterator for NeighborsMut&lt;'a, T&gt; { type Item = &amp;'a mut T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; // calculations let point = Point(x, y); Some(&amp;mut self.grid[point]) } } It gives me a compiler error saying that I have created an impossible situation, where the lifetime of the reference I'm returning can't outlive the body of the `next()` function, but also has to live at least as long as `'a`. The only way I could figure out to remedy this was to use `unsafe`: fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; // calculations let point = Point(x, y); unsafe { let ret: *mut T = &amp;mut self.grid[point]; Some(&amp;mut *ret) } } There is one main thing I am confused about: why does changing nothing but adding `mut` make the compiler more strict about this? Wouldn't it detect a similar lifetime issue with the non-mut version? Thank you so much for any reply, this is really bothering me.
Keep it up!
This works: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=e4cfb24eba7378518460fa6c2c0e3271 Though I'm not sure if it really captures what you're trying to accomplish. And you don't need the explicit lifetimes - they're just for demonstrating what's actually going on.
rust is designed to not allpw of usage of static methods in trait objects for reason - you can make a method that has self and not use it, then if your object has no fields so create empty object as trait object. don't enforce OOP dogma on things that does not have to.
Are there any plans to support structs and enums? Also, how long do you think it will be until you have support for references to generics? I would use this library right now if it weren't for the lack of this very feature.
Two things. First, wouldn't taking mutable reference to a pointer give just that (`&amp;mut *mut T`), not coerce it to a normal mutable reference (`&amp;mut T`)? Second, you should probably just use [`get_mut`](https://doc.rust-lang.org/std/primitive.slice.html#method.get_mut) like normal, since that returns a reference in an option like you want. Unless you want to panic when the thing isn't there. Generally, when you think you need to you unsafe, it's *highly* likely you're doing something wrong. You need to make 5000% sure you know what you're doing when you invoke unsafe. Especially with NLL improving the borrow checker, it's even more unlikely that it's wrong. In this case, you can probably just do what I said. I can't check to see what's wrong with what you did, since my phone's about to die. D:
Sure, but there's a lot of ways to apply that statement. In my book, Optimizations are definitely out of the picture while you are learning.
Who made that logo? Really nice!
Maaaan...I didn't even know this is something I wanted until I read this post. Unfortunately, I've just switched off of the project where I could have used this and it won't be my primary focus for a few months, but I'll download this sometime next week and see if it fits my use case.
I don't think it's toxic to point out that a library that asks people to trust it for critical stuff and claims cryptographic security hasn't done basic due diligence. I think it's responsible. Not turning on compiler warnings and looking at them is what really got me. Thanks for sharing this.
short answer: `T::name()` [long answer](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=a413c512f132ae844e2d05361d42dc2f): ``` pub trait Foo { fn name() -&gt; &amp;'static str; } pub struct Bar {} impl Foo for Bar { fn name() -&gt; &amp;'static str { "bar" } } pub fn print_name&lt;T: Foo&gt;() { println!("{:?}", T::name()) } fn main() { print_name::&lt;Bar&gt;(); } ```
I‚Äôve did it myself with http://hatchful.shopify.com/. Quite nice, and free!
This is so much needed! A very naive question: will it be feasible to mock a trait from a foreign crate?
Great work! This appears to be inspired by RSpec (or perhaps another library itself inspired by RSpec.) I wrote something similarfor Python when I worked at Uber called doubles: https://github.com/uber/doubles One subtlety that I included in doubles that you might consider is distinguishing between a stub and a mock. Specifically, the `expect` and `returns` methods should be mutually exclusive. The reason for this is that if a test double is returning any sort of meaningful value, then it's possible to make a value-based assertion later in the test to prove that the right thing was called. "Expect" style mocking is only necessary when the function under test causes non-local action (i.e. doesn't have a meaningful return value) and this needs to be verified. If you're interested in going this route, you might find the docs for doubles useful, particularly the section on terminology that provides the distinction between stubs and mocks: https://doubles.readthedocs.io/en/latest/terminology.html, as well as the API for `Allowance` and `Expectation`: https://doubles.readthedocs.io/en/latest/api.html. Note that `Allowance` has methods for setting return values similar to Mockiato but `Expectation` does not.
Looking through the examples, I was wondering, whether it is also possible to mock structs? In the examples, I only saw mocked traits. ~~~ #[mockable] trait Greeter: Debug { fn greet_person(&amp;self, person: &amp;Person) -&gt; String; } #[derive(Debug, PartialEq)] struct Person { name: String, age: u8, } ~~~
Is there a nice way one can move a value out of an `Option`, and replace it with `None`? (I recall there was a crate to do this, but I cannot remember it's name.)
I ended up using lazy_static. lazy_static! { static ref PRIMELIST: Vec&lt;u32&gt; = read_list_of_primes(); } fn read_list_of_primes() -&gt; Vec&lt;u32&gt; { let file = File::open("primes.list").expect("Could not read file."); let buf = BufReader::new(file); buf.lines().map(|l| l.expect("Could not read line.").parse().expect("Could not parse number")).collect() } Could you explain where the Hashset comes into play?
There is, [`Option::take()`](https://doc.rust-lang.org/std/option/enum.Option.html#method.take).
Rust is statically typed. The return type of a given function is resolved at compile-time to one specific type, even if code outside of that function cannot name it or assume anything about it other than specified trait impls. If we had a `typeof` operator there would be a way to name the return type: type FooReturn = &lt;typeof(foo) as FnOnce&gt;::Output; // This should all type check: let a: FooReturn = foo(); let b: FooReturn = foo(); Reflexivity(a, b); &gt; If we know that an `impl Trait` type is equal to some other type, then we have leaked additional information other than the fact that it implements some trait. But it‚Äôs not some other type. `foo()` and `foo()` have the same type. Why wouldn‚Äôt they? As you‚Äôve noted `foo()` and `bar()` in your example *are* different types, and `Reflexivity(foo(), bar())` doesn‚Äôt type-check. Internally `foo` calls `bar()` so behind the curtain they have the same memory representation etc., but code outside of `foo` is not allowed to reason based on that fact. &gt; In contrast, Haskell will not match these kinds of types, and will allow for an uninitialized type variable. I have not heard or read the term ‚Äúuninitialized type variable‚Äù before. It seems this is a Haskell concept that is not present in Rust? &gt; `// We want to avoid this: using an object with a different parent.` `impl Trait` in Rust doesn‚Äôt work this way, and that‚Äôs not a bug IMO. Some tricks using invariant lifetimes do sound like what you‚Äôre looking for, though: * https://doc.rust-lang.org/nomicon/subtyping.html * https://github.com/bluss/indexing
What you are suggesting - giving each method call a unique type - would still allow id mismatch since a method call can be invoked multiple times. For example: let v: Vec&lt;_&gt; = (0..2).map(|_| foo()).collect(); let _ = Reflexivity(v[0], v[1]);
I'm not sure how Rust being statically typed will change my argument. With existential types you don't actually create a new type for every invocation of a function. You just reject programs which assume that the types of repeated invocations are the same. This is possible to do at compile time. In your example regarding a hypothetical `typeof` operator, If `let a: FooReturn = foo()` would typecheck, then what's stopping `let b: FooReturn = bar()` from typechecking (using `foo` and `bar` from my examples above)? `foo()` and `bar()` have identical type signatures from the programmer's point of view. if `impl Trait` is equal to `impl Trait` only half of the time and not the other half, why not make them *never* equal? &gt; But it‚Äôs not some other type. foo() and foo() have the same type. Why wouldn‚Äôt they? I think you answered your own question quite succinctly here: &gt; so behind the curtain they have the same memory representation etc., but code outside of foo is not allowed to reason based on that fact. They're not universally quantified, they're existentially quantified. We know that they return the same type internally, but I would say we shouldn't be writing code based on that assumption.
Mocking a trait is about mocking behaviour, which the struct in your example does not have. What are you trying to achieve?
Interfaces, or in rust terms traits are the correct tool for separating different layers of abstraction in your code from each other, as you can easily interchange concrete implementations of a trait. We are therefore not planning on adding support for mocking structs and enums. --- I‚Äôve tried to implement support for references to generics more than once, but have failed to do so. I believe we might need generic associated types to achieve this. GATs however are currently not finished in the compiler.
The words you‚Äôre looking for are *generative* vs. *applicative* existentials. Under those names, this issue was discussed quite a bit!
But, at the same time, you don't want to intentionally risk teaching people conceptual models they'll have to un-learn later if there's a simple alternative to doing so.
I tried looking this up and I think it's what I'm looking for but I could only find resources relating to OCaml. Is there anything related to Rust?
I'm not sure I understand what you're trying to express here. I would argue that the second line shouldn't compile as the types of separate invocations of `foo` would have different types. However, I lack the knowledge to say how this would be resolved internally.
After some lead-up with unrelated things, [this thread got into it](https://internals.rust-lang.org/t/blog-series-lowering-rust-traits-to-logic/4673/17).
&gt; With existential types you don't actually create a new type for every invocation of a function. Yes, that‚Äôs what I was saying. &gt; You just reject programs which assume that the types of repeated invocations are the same. No, as far I understand that‚Äôs not how `impl Trait` works in Rust. Repeated invocations of the same function return the same type. &gt; what's stopping `let b: FooReturn = bar()` from typechecking (using `foo` and `bar` from my examples above)? `foo()` and `bar()` have identical type signatures from the programmer's point of view. I don‚Äôt see this as a contradiction. They use the same sequence of characters in source code to spell their signature, but they have different return types. I think of it this way: each occurrence of `impl Trait` in source code is ‚Äúinstantiated‚Äù to one concrete type. Referring again to that ‚Äúsame occurrence‚Äù (like in `Reflexivity(foo(), foo())`) does not create a new type. But writing `impl Trait` again in the signature of another function does (from the outside point of view). This is consistent with the way [`impl Trait` in `type` aliases](https://github.com/rust-lang/rfcs/pull/2515) are proposed to work.
&gt; No, as far I understand that‚Äôs not how impl Trait works in Rust. Repeated invocations of the same function return the same type. I already understand how `impl Trait` works in Rust. I want to know your opinion on whether or not that's better or worse than my proposal. I'm not asking for technical help to a question. &gt; I think of it this way: each occurrence of impl Trait in source code is ‚Äúinstantiated‚Äù to one concrete type. Again, this is just documenting what Rust *does*, I already understand that. I would appreciate your comments on why you think this behavior is the "right" behavior and what benefits and/or disadvantages it has over what I propose. &gt; This is consistent with the way impl Trait in type aliases are proposed to work. So do you agree or disagree with how `impl Trait` is going to be implemented in type aliases or not? and if you do or don't agree, why?
https://github.com/brson/stdx is what you're looking for
In fact many would prefer if Rust had a way to name `impl Trait` return types to store them in data structures. Most people don't care about generative types(most of them don't even know they exist),so it seems like it would increase Rust's strangeness budget for something almost nobody will use.
This is very interesting, but one thing I will say is that I don‚Äôt think a hugely expressive type system is a high priority for rust overall
I am unsure on how to solve the issue in compiling around line 116. I am also not changing the values below line 116 in the match expression is there easy way to just say copy these values without explictly outlining them? I am guessing a clone is the way to do that but I would appreciate any advice https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4d6a829e752c25300a1a96d7eb104ff4 https://gist.github.com/rust-play/4d6a829e752c25300a1a96d7eb104ff4
To be quite honest, I think Rust's type system is already highly expressive, and for that I'm already grateful.
Yet all other key crates manage to not wait 2 damn years to make updates. mio is not a special snowflake.
I see, you're right. I could easily just build the objects that I need. I just wondered because I only knew mocking from javascript libraries and so I instinctly thought about mocking of objects instead of behavior.
Could I get some clarification? I thought you could store an `impl Trait` in a data structure already: struct MyStruct&lt;T: MyTrait&gt; { data: T, } I'd assume that if we were to instead write something like struct MyStruct { data: impl MyTrait, } that it would desugar to the first example. I'm not proposing that either of these shouldn't work.
No,`impl trait` is only usable in function arguments,and free function return types right now. The motivation for adding `impl Trait` was added to Rust is to be able to return unnamable/long return types. `impl Trait` return types are applicative because that is the least surprising thing to the vast majority of programmers,and nobody made an argument for making them generative.
You can add `#[derive(Copy,Clone)]` to `struct LumberJack`, and create the enum value with `ForestFeature::LumberJack(*l)` https://gist.github.com/rust-play/0867451bb5e38f8b60c2640702ad90ef
I understand that my second example isn't valid rust. Why did you say that naming `impl Trait` return types to store them in data structures was the opposite of what I was proposing?
Basically the semantics of `type SomeType=impl Trait;` or `existential type SomeType:Trait;` are that every place where that type is mentioned it is the same anonymous type.
put everything in main() -&gt; io::Result&lt;()&gt;
thanks your reply is there anyway to not have to write lines 119 and 120 as they don't change anything something like x =&gt; x This questions is more for matches in general
Are you suggesting that to make the sample code easier to read or as a suggestion for the final actual code? The actual code I end up using will probably be quite different anyway.
I read [this](https://github.com/rust-lang/rfcs/blob/f81510c80164bd9a8330934a1a816114e6cd7394/text/0000-impl-trait-type-aliases.md) article which I think has changed my opinion.
Or... [have every nightly build successfully](https://graydon.livejournal.com/186550.html)?
If you were to contrast actix-web with the rest of the web frameworks in Rust, it is best positioned to address key-person risk in the near-term. Microsoft is launching many azure iot-related projects to production this year, some that use the actix libraries/frameworks. Once releases are made and business models are proven viable, concentration risk (of any one-person show) will be spread across teams. Managers are accountable for concentrated risk and so have the proper incentives to address those risks in the short-term. It would have been premature for managers to spend money, time, and effort on spreading knowledge on exploratory work. However, this won't be the case once business is established.
&gt; Keep in mind that making mocks that dynamic is in indicator that your API might need to be simplified in order to be more testable. I'll admit that I often cheat and use mocks beyond unit tests, in which case "generic" expectations to fake answers can be pretty useful.
You are probably looking for /r/playrust
Ohh, ok thanks. :)
Mocking foreign traits and by extension directly depending on a trait defined in an external crate (excluding crates in the same workspace) seems like a clear violation of the Dependency Inversion Principle, which states that one should depend upon abstractions and not details. To quote Robert C. Martin (Clean Architecture, Frameworks are Details): &gt; Don‚Äôt let frameworks into your core code. Instead, integrate them into components that plug in to your core code, following the Dependency Rule. I believe this statement should also be applied to libraries in general. This is even more relevant in your case since the docs of embedded_hal even mention that their API is not yet stable: &gt; NOTE This HAL is still is active development. Expect the traits presented here to be tweaked, split or be replaced wholesale before being stabilized, i.e. before hitting the 1.0.0 release.
Consider this: for i in 0..10 { let a = foo(); something(a); } The type of `a` would have to be dynamic to track the different types resulting from different calls to `foo`. Basically, your types are no longer static and can change depending on runtime state, so it would be vastly different from the current type system and would likely be impossible to statically verify.
no you can keep him
The Haskell `ST` monad does something very similar, and it can perform the kinds of static analysis that you describe. You don't actually have to have dynamic types under the hood. Haskell does this using an uninitialized type variable. It works like this in Haskell: essentially you have some type `T` that belongs to no traits and you *never* give a concrete type. As a result, you can't do anything with it, except pass it around as a type parameter. Because you can't do anything with it, the compiler can optimize it away completely. When we call `foo` we create an object parameterized by `T` where `T` could be nothing, everything, or anything, and could be different each time: // each time we call foo we "create" a new type parameter T here that could be anything. // but because it belongs to no traits, we can optimize it away to absolutely nothing. fn foo() -&gt; MyStruct&lt;T&gt;; When that object is constructed, it can create children, also parameterized by the *same* `T`: // We don't create a new T here, we can just use the old one. fn bar&lt;T&gt;(parent: MyStruct&lt;T&gt;) -&gt; MyChild&lt;T&gt;; When we want to use both the child and the parent together we can check at compile time: // both the parent and the child have the same T as a type parameter. fn baz&lt;T&gt;(parent: MyStruct&lt;T&gt;, child: MyChild&lt;T&gt;); Therefore you probably wouldn't be using `foo` in a loop. You would probably call `foo` once to create one type parameter, and then use `bar` to create children which have the same type parameter. It's not impossible to statically verify, but it would vastly change the type system. Mind you, now that I've read the latest on `impl Trait` I have to conclude that this is probably not the best way to go about that feature.
If you want to be able to call `name()` from a trait object, `name()` will have to take a reference to self, there's no way otherwise for Rust to know at runtime which implementation of `DbSchema` to get the `name` function from
My gut feeling is that you shouldn‚Äôt have fractional types, but something like `Frac&lt;Meter, Second&gt;`. However, I don‚Äôt know whether that makes the type constraints simpler or even more convoluted.
https://medium.com/sean3z/rest-api-node-vs-rust-c75aa8c96343 I moved over both API servers at my last job to Rust without issues. I think it's great for APIs. The only thing I'd watch out for is making sure other devs understand how to edit them as well.
That's it! Thanks.
rust has matured , its not young ! :-p
What you are trying to do is, I think, the units of measure feature built into F#. You might play with that for a little bit and see if that gives you any ideas for how to deal with it in Rust.
Have you checked this crate for ideas? [https://crates.io/crates/uom](https://crates.io/crates/uom)
why the hell post this here in Rust ? Killing Scala was not enough for your corporate profits ?
Can you add the concache crate to the benchmarks?
Didn't have time to look at the code closely but you should be able to do much more than 30k/s. Tokio and actix are doing 7m/s in the http bench, and a counter isnt much work.
What you're trying to do with the syntax of `impl Trait` can already be done with `dyn Trait`. So I'm not sure what the problem is... can you elaborate on why you're not using `dyn Trait` to get the result you want?
Read the whole article that was nicely written
Not simd specific, but if you look in the std code, a lot of times the platform specific code is on an opaque, private "inner" type. Look at std::fs for example.
Looks great!
Will do as soon as I get home
`Box&lt;dyn Trait&gt;` will always type match with another `Box&lt;dyn Trait&gt;`, even if they are different types under the hood. I wanted the exact opposite: types that look the same on the surface but don't match.
Looking at the benchmarks, they don't look too realistic.. I think you may need more than 16 values and scenarios with and without hash collision
I'll add some more. The reason they are like they are is do avoid overhead not directly related to the map. Other benchmarks with collisions have the same scaling towards the other maps.
Also I don't know if you misread but we bench with 20k values. Insv and Insv2 are just large blobs of data.
Just so you know, ccl is a common acronym for "C compiler." Not that I think you should change the name at this point, but be aware that you might have to address some confusion for your users.
Note that Rust memory management imposes some developer productivity overhead, which would need to offset async ergonomics gains over Java/Python, which I doubt happens. Rust is great if you need the benefits of no GC (no pauses, lower memory usage) or ship runtimeless software.
Ah, I derived the name from 'concurrent collections'. Thanks for notifying me
I think your proposed syntax is already correct. `x =&gt; x` will match any value and return it.
I think that the type of `foo()` being the same as the type of `foo()` is good, and I like RFC 2515. The alternative sounds like a serious limitation to what I feel are "mundane" uses of APIs that return `impl Trait`. The example of preventing use of a child with a different parent however sounds like advanced trickery that, if I understand correctly, can already be achieved with equally advanced invariant lifetimes trickery.
Now that's the best solution so far. Thanks a lot!
But Rust will soon also be getting async support too.
Hey, you should try out rust-analyzer's syntax highlighting. It's not great, but not bad either.
I find actix-web very nice to work with and with no nullpointers and enforced error handling (if you do not use unwrap) I feel much calmer releasing code for real world use. I've not yet deployed anything with more users than me and friends but already I can tell there is sigificantly fewer server errors. The ones I get are mostly misscommunication with frontend. (I might also have become a better programmer since I last released some Java springboot to production)
There is also [CCL](https://en.wikipedia.org/wiki/Clozure_CL) (Clozure Common Lisp).
Hey Ralfj! Thanks for taking your time to reply. I tried breaking up the wall of text in topics to hopefully make it easier to read. # About booleans Is the following statement correct? Rust wants to abstract LLVM but because of LLVM's presence some of its semantics leaks through to Rust whether you want it or not. Specifically UB is very hard to contain (see other instances such as `256.0f32 as u8` being UB). In this way you are trying to create a model for Rust that both wants to stand independent of LLVM's semantics but at the same time recognize the value of LLVM (and UB) when it comes to optimization. The especially aggressive approach of LLVM to exploit UB leads to a lot of frustration (even more common in C/C++!) as well as on your side to try to make all of this work :P Whenever I'm dealing with reading eg. binary file formats with booleans in them I've tend to interpret them as '0 = false, anything else = true' (instead of eg. panicking/exception/UB). This is in part because I know this lowers very efficiently to the underlying hardware, tests against zero/non-zero are basically free. From this perspective the value proposition of boolean as 'false/true/undef' is a lot less straight forward. All that said, invalid boolean values have always been UB (even in C++ where I come from) so this isn't that big of a deal since I'm already used to just 'deal with it'. # About uninit/out references Yup, generalizing uninit/out references is hard. Even in languages that have it, C# in my case, the compiler is very quick to call you out: 'hey you must initialize an out variable before returning'. Which I promptly broke by throwing an exception just to see what would happen. The out value turned out to be zero-initialised (as all values are in C#) but clearly this solution isn't satisfying. Trying to fix it by requiring that the code must not panic in Rust doesn't work as Rust has no way to identify and enforce non-panicking code, especially not in a zero-cost manner. It is true then that I don't even try to fix the general case and suggest an approach which I've only thought about in the use case of calling C APIs. Obviously too much special casing is not a good idea for a general purpose language, everyone likes cross-cutting features that interact well with the rest of the language. # About MaybeUninit type inference Hmm you're right that type inference seems to work perfectly on MaybeUninit! I... can't quite remember why I thought this was going to be a problem. I seem to remember it had something to do with raw pointers and arbitrary self types and type inference being dumb: let ptr = 0 as *const _; if ptr.is_null() {} let ptr: *const u8 = ptr; I've always had a similar issue with closure parameter types: let f = |i| i.abs(); println!("Hello, {}!", f(3.1415f32)); So uhm my biggest complaint is then resolved, I think. # About out references in pure Rust code When dealing with pure Rust code I've come to enjoy `&amp;mut Option&lt;T&gt;` to fill output parameters. If I'm worried about having to drop the original value when I _know_ it is None you can use the replace method and forgetting the old value: let mut output = None; mem::forget(output.replace(Some(value))); I encounter this pattern with APIs that take an FnMut closure as argument but do not have a place to return a value: fn action&lt;F: FnMut()&gt;(mut f: F) { f(); // Do stuff } let mut result = None; action(|| { mem::forget(result.replace(vec![42])); }); let result = result.unwrap(); In any case this solves panic safety by the Option representing a runtime 'flag' whether the variable is initialized. My point is that `Option&lt;T&gt;` seems to have some similarity with `MaybeUninit&lt;T&gt;` where the former is runtime checked and the latter is the unchecked variant of this use case. I enjoy this kind of relation because it allows to view the same concept in different ways which often leads to better understanding. # About the MaybeUninit name Have you considered naming the type Uninitialized (or even just Uninit)? Conceptually this type does not represent an initialized value at any time _because_ the wrapped T is further wrapped in ManuallyDrop. It does not behave like a 'maybe initialized' unlike eg. Option. This isn't really a complaint but more about feelings. For what it's worth I feel this more closely aligns its name with its `mem::uninitialized&lt;T&gt;` heritage. # About compiler support My observation is that the mem::uninitialized code can be transformed fairly mechanically to use MaybeUninit which leads me to think, why should I be in charge of doing this mechanical transformation? Can the compiler do it for me somehow? The 'wishlist' syntax actually matches MaybeUninit pretty dang closely except the compiler figures it out for me: let mut x = mem::MaybeUninint::uninit(); C_API(x.as_mut_ptr()); let x = x.assume_init(); vs. let mut x; C_API(&amp;mut x); let x = mem::initialize(x); Comparing to the hypothetical &amp;out or &amp;uninit references: panic safety is resolved by _not invoking Drop_ even when you've assigned a value until you decide the value is properly initialized (3rd line in both examples). Assigning to an out reference does not drop the value behind the reference. Why not also compare it to using an option: let mut x = None; rust_API(&amp;mut x); let x = x.unwrap(); The point I'm trying to make is by comparing these solutions with each other we can see how one method resolves a particular question to better understand the trade-offs in other methods. # Conclusion I'm happy to understand that MaybeUninit isn't as bad as I thought. The mechanical nature of the transformation gives me hope that this can inform the debate about how out parameters can be implemented in the language itself and perhaps lead to better ergonomics surrounding out parameters in general and C APIs in particular. Thanks for taking your time to read!
We are struggling with bad written and bad performatic PHP legacy code and some memory eager java applications (ironically, all python applications are performing extremely well). The company main goal this year is performance, we don't have plans to rewrite big applications, but only critical performance pieces/services we already have mapped. So if the performance is great, we may happily buy the overhead that comes with it.
I can‚Äôt imagine rust being a net win if your applications are I/O bound. If your main problem is the speed at which you can get data from a third party service... rewriting your own code in another language can only go so far.
Ah, okay, understood! Thanks for clarifying. I don't understand _why_ you want what you want. `impl Trait` already does that across functions, which seems counter-intuitive to me, and your proposal dials it up to 11. In OCaml, this is precisely why generative functors are not allowed in paths (which is kinda' what you're proposing be allowed), they must be bound to a module first. (* Generative functor *) module G () : sig type t val x : t end = struct type t = int let x = 0 end let refl (x : 'a) (y : 'a) = () (* NOT OK *) let x : (G ()).t = (G ()).x (* OK *) module M = G () let x : M.t = M.x let y : M.t = x let () = refl x y
I am not convinced about the need to benchmark for hash collisions. If performance of the hash map is a concern, certainly the first step in case of hash collisions would be to solve the root cause (fixing the hash) rather than paper over it (improve the performance of handling collisions).
I like where your head is at. You might also want to think of the other side of the problem that isn't technical. If you make an API using Rust in a primarily python+Java shop, you need to engender that information to the team(s), hope the team gets to journeyman level to make production level rust code then when you hire someone you're now looking for an engineer with rust experience. These are all time sinks that can be easily avoided by using popular languages and tools (Java and Spring).
And then somebody modifies the type used in the hashmap, without even knowing that it is used in a hashmap somewhere, and without think about collisions, and your hashmap starts getting collisions again. Being able to perform well in the presence of bad hash functions is quite an important property of hashmaps for large systems.
This is fascinating. It's heavily over my head, as I've never used existential types, but boy do I want something to "solve" this issue: In many cases I would _love_ to use `impl Trait`, but I can't. I'm not going to pretend to know the best solution, but as a developer being able to throw away type information while still keeping static dispatch seems hugely beneficial. I so frequently run into design patterns that seem fine, but end up not using them because the concrete implementations become huge. Wider usage of something _like_ `impl Trait` would be amazing in reducing verbosity in some patterns.
The readme has not been updated with the results for it yet
Just got home. Haven't had time yet. Expect results in about 10m.
It may not. The value add of rust is memory safety, less memory footprint, and speed of CPU bound operations. For I/O operations you may get a slight performance increase due to more performant code and less memory used (no gc), but that's not really ging to address your bottleneck.
But the advances that Rust brings to the table (esp. compared to Java) are mostly outside the IO/async scenarios. Java can be highly performant with moderate effort. If someone writes poorly performing Java it is not clear to me that the same person will write great performing Rust.
You most likely don't want a temporary vector. Take a look at [chunks](https://docs.rs/itertools/0.8.0/itertools/trait.Itertools.html#method.chunks) and [tuples](https://docs.rs/itertools/0.8.0/itertools/trait.Itertools.html#method.tuples) in the [itertools](https://crates.io/crates/itertools) crate. They yield chunks lazily without requiring linear-size temporary storage.
You don‚Äôt owe him anything. Keep on keeping on!
If IO is a concern I would look at Elixir, It is great for services which integrate with external APIs.
Piecemeal replacement of existing code is something rust is pretty good at, but I do not know how well the Java&lt;-&gt;Rust and PHP&lt;-&gt;Rust bindings work.
This is the second tool I am researching, It's a little easier because I have friends working as full-time Elixir developer.
It seems to me like you could do what you describe with a macro: use std::marker::PhantomData; struct Foo&lt;Tag&gt; { tag: PhantomData&lt;Tag&gt;, } fn foo&lt;Tag&gt; -&gt; Foo&lt;Tag&gt; { Foo { tag: PhantomData } } macro_rules! foo { () =&gt; ({ enum Tag {}; let ret: Foo&lt;Tag&gt; = foo(); ret }) } struct Reflexivity&lt;T&gt;(T, T); fn main() { let a = foo!(); let b = foo!(); // Works: let _ = Reflexivity(&amp;a, &amp;a); // Fails: let _ = Reflexivity(&amp;a, &amp;b); } But this has the exact same caveats as I mentioned earlier. It only works for some static number of calls to foo. If you have a dynamic number of calls (such as in a `for` loop), this will not be able to staticly verify that ids match.
My experience is that rust is great for web! I‚Äôm working on replacing parts of a rest api with a GraphQL API in rust. Even though it is IO bound by the database I‚Äôm seeing several orders of magnitude better performance. The rest api is in Rails and plenty fast, but no competition for rust regardless of being cpu or io bound.
I think a naively written rust app still outperforms a naively written Java application. My first rust app at the company was a rewrite of a Java app that tried to process gigabytes of data quickly and failed big time. The rust version was much simpler and a lot quicker. Obviously the architecture of the Java app was incorrect, I'm not claiming that it couldn't be written in a better way. However it was naively written in a straightforward way and the GC just killed the whole thing because the software generated too much garbage. The rust version was not overengineered either, it was a very simple port that happened to be much faster because the large amount of data was managed in a better way. I can't share details so sorry if this is too vague:)
You can't do this with functions, but you can use macros and closures to make generative existential types.
Fist of all, thanks for having benchmarks! I think it is very valuable to have a benchmark suite designed to showcase the performance of multiple implementations for a variety of scenarios. Speaking of variety of scenarios, the current benchmark is a bit threadbare, so I'd like to suggest multiple axes of improvements. ### Methodology. I love `rayon`, however I would not use it in benchmarking concurrent structures/algorithms beyond a simple "quick &amp; dirty". `rayon` is great for abstraction, when benchmarking something requires controlling as much as the environment as possible to specifically test certain properties. Instead, I'd recommend building up your own multi-threaded benchmarking facilities, allowing you to: - Pin threads, if possible. - Allow mixed work-loads: N cores doing A while N' cores do B. - Allow latency benchmarks, aka "spurts", where you force all cores to start as close as possible as simultaneously to control contention. ### Hardware. The hardware plays a large part in the scalability and performance of concurrent data-structures and algorithms, so it's generally a good idea to try and design a test-suite to show the "breaking points" of the various implementations. Specifically, you'd want: - To explore scalability based on number of cores, with/without using hyper-threading. - Might be a bit difficult to get your hands on a dual-socket host, however there's a performance cliff when memory reads/writes occur from two different sockets. - To explore scalability based on cache sizes: - Fit all keys in L1, L2, L3 or require RAM. - Fit all keys+values in L1, L2, L3 or require RAM. Scaling the number of CPUs essentially exposes contention issues, scaling the number of key/values exposes cache-friendliness -- or absence thereof. ### Scenarios At the moment, you only seem to be benchmarking parallel insertion. That's definitely interesting, it's also very partial. There are multiple operations of interest in a concurrent hashmap: - The typical find/insert/erase found in normal hashmap. - Don't forget to account for the fact that a key can be already present or not. - The atomic in-place-update of the value associated to a key. And therefore there are multiple usecases, consisting each of a mix of the above. There are way too many usecases, actually, especially when considering the hardware dimensions, so you'll want to select a couple scenarios. The simplest are, I suppose, the single-minded workloads: You already did insert-only, what of find-only, erase-only, update-only (same key), update-only (different keys)? And then you can do some mixing: - Balanced find/insert/erase: - Each core with its own set of keys, doing insert+find+erase. - Same as above, but in batch of size N (varying): insert N, find the N, erase the N, move to next batch. Helps busting the cache. - Producer/Consumer: - N core(s) insert/erase, all others find. - N core(s) insert-only, all others erase. There are also multiple types of keys and values: - Simple: i32, i64. - Heavier: arrays. - Clone: `String`. A `find` operation which returns a `Option&lt;V&gt;` for example is easier to implement than one that returns `Option&lt;&amp;V&gt;`, so it is favored by benchmarks with values that are cheap to copy, but will not perform well on benchmark with large `String` as values.
Definitely, and the java developers we have don't want to use anything different than Java, they are starting to thing that maybe worth use Kotlin, but still, there is some resistance. It's unbelievable. [/rant] My team is full python and they are a little resistent to migrate things to Java, we have the liberty to choose the technology as long we assume all consequences that may comes with it. So, we started to discuss new techs to solve the problems (we are seen the company is growing real fast and python soon will not able to handle the job as well it does today). So we are looking into new tech, Rust, Elixir and Go are on the table.
The latter solution (i.e. \`self.layers\[layer\_index\].update\_weights()\`) seems more appropriate and logical from my point of view. &amp;#x200B; In the previous case, where the two accessors \`get\_weights\_mut()\` and \`get\_momentum\_weights()\` were used, you are relying on "external" code to make "internal" changes (external and internal from the \`layer\` type's perspective), which gets clunky when the type only exposes fields via accessors, which is the case here. &amp;#x200B; I usually judge the use roughly via the following criteria \- If a functionality is only meaningful (or well defined) internally, and intermediary computational results are not important to the external code, then it's definitely implemented at the internal layer as external code has no reason to touch any of this \- If the functionality can be implemented (better) at the more internal layer, then I normally implement it internally as well, as implementing functionality outside of the type's core code base (i.e. \`impl\` code base or module code base) can make things difficult to navigate later on \- If it'll be a commonly used functionality, then implementing internally gives a consistent single implementation &amp;#x200B; In this case, the for loop does not really need access to any of the intermediary results (namely \`weights\` and \`momentum\_weights\`), so it fits the second criterion. And unless you're very certain this will not be needed by any other things later on, I'd consider third criterion an additional reason to move it to internal as well.
I don't think that it's counter intuitive across functions at a minimum. At the end of the day, an `impl Trait` is a concrete type, so you shouldn't be able to mix different concrete types with the same "name". I just read part of u/Gankro 's [thesis](https://raw.githubusercontent.com/Gankro/thesis/master/thesis.pdf). One of the chapters describes how you can use lifetimes to perform this exact behaviour, however, it's probably one of the hackiest things I've ever seen. He creates a closure with the type: `for&lt;‚Äôid&gt; FnOnce(Array&lt;‚Äôarr, ‚Äôid&gt;)` for the pure reason to create a new lifetime parameter. He passes around a `PhantomData&lt;Cell&lt;&amp;‚Äôid u8&gt;&gt;` as an object ID. Well, at least I know that it's possible to do, and that you should *never ever* do it.
I would move the logic into the layers. One thing to note about references is that `&amp;mut T` means a unique reference and `&amp;T` means a shared reference. Thinking of it this way, you shouldn't be able to have both a shared and a unique reference to some value, it just doesn't make sense. So you must refactor you code.
Don't do that, really. Why? Because Rust is not a very good language to do that. It can be done, but it will be painful, and the more code you build on top, the more painful it will get.
Thanks!. Currently I'm benchmarking 20k inserts and then 20k mutable lookups which it then replaces the values of. I'll look into this. I do have trouble getting my hands on a really powerful machine though. Biggest one i have is the top tier VM from hetzner. If someone has hardware could test on i would gladly appreciate a DM.
Instead of using traits like this, I would make a generic type that represents a value with units. This will allow for better generic use than traits.
Thoughts: 1. Avoiding a complete meltdown, sure. But at the end of the day, there's only so much you can do when the hash is always 0, and if the counter-measures affect the performance of good hash functions, then it's a loss for everyone. 2. I find poor hash functions to be less of an issue in Rust than in say, C++ or Java, due to the better hashing framework of Rust. This makes the problem less relevant for the Rust ecosystem. 3. If performance matters, there should be a test/pilot in place to catch regressions.
Fair point. Currently ccl uses open addressing with RHH for collosion handling.
Stop discouraging people who are trying to learn, especially when what they are trying to do is definately possible
PHP code is being rewritten by a couple of teams (is a monolith and we are breaking into microservices). So, mostly we would need Rust-Python bindings which looks to work well, we are testing a Json parser made in Rust that is fast than native parser and even faster than the package written in C we previously used.
What's the best way of implementing methods for `Vec&lt;X&gt;` where `X` is some specific type? For example, let's say that I have this: #[derive(Debug)] enum Tag { Big, Small, Shiny, } #[derive(Debug)] #[allow(dead_code)] struct Entity { name: String, tags: Vec&lt;Tag&gt;, } impl Vec&lt;&amp;Entity&gt; { fn big(self: Vec&lt;&amp;Entity&gt;) -&gt; Vec&lt;&amp;Entity&gt; { self.iter().filter(|e| e.tags.iter().any(|t| match t { Tag::Big =&gt; true, _ =&gt; false, })).collect() } } fn main() { let people = vec![ Entity { name: "Anna", tags: vec![Tag::Big]}, Entity { name: "Bob", tags: vec![Tag::Big]}, Entity { name: "Ceasar", tags: vec![Tag::Small]}, Entity { name: "Dartho", tags: vec![Tag::Big]}, ]; for big_person in people.big() { println!("{:?}", big_person); } } (This won't compile)
Updated.
Just stating the fact. You said the **requirement** is to return a reference to a temporary variable. That is impossible in Rust.
If PRIMELIST is large, then it will be faster to check for inclusion in a HashSet than in a Vec.
I'm also in fintech, and primarily deal with io bound constraints (monolithic database and network to third party services bring the two biggest bottle necks). And like you, primarily in the API side of time. I deal with a combination of node, dotnet (framework and core) and Python. You need to ask yourself a series of questions before bringing a new language to the table: 1. Why do you think it's a good fit? 2. Is there expertise in the language, or would you be learning as you go? 3. Can you fit it with the same metrics and monitoring as your other systems? I've asked myself these questions multiple times when proposing doing something new. In both of our situations, the bottle neck is the network and the speed difference between Python and highly tuned assembly aren't really going to matter here. If it takes three seconds for your third party to respond, it won't matter who made that call, it's still going to take three seconds even if your request actually starts several nanoseconds earlier because of less abstractions to go through. So considering rust to solve that is a non-starter. Python is just as well suited to sit and wait as anything else. I've had lots of success with both synchronous (requests) and asynchronous (aiohttp, both client and server) Python when dealing with third party calls. Moving on from that, if you're considering replacing time sensitive operations with rust (say you're doing high speed trading), Rust might actually be a good fit here. Now you need to consider what sorts of libraries are available for you to use. Machine learning, data processing, etc. If they're available and have reasonable public APIs, then you might be able to sell this point. A similar story if your trying to replace memory hungry applications - though I'd recommend tuning your JVM before proposing rewriting the application. Often defaults are good enough until they aren't and I've seen that tipping point be catastrophic. The next consideration is expertise. We're primarily a dotnet shop so that's usually our first stop in the "what to build this with" train and we usually stick with it. Anyone in engineering is familiar with it (sans a few that are dedicated to WordPress and data science, even then they can usually muddle their way through if they have to). The next stops are node and python, depending on what we're doing. We use node to build middleware servers for front end, and Python for our customer servicing and data science applications as well as a few bits of test tooling. I've pitched Rust for some internal, non-critical applications and been turned down because if I was unavailable and this caused an issue no one else would be able to resolve it. We've had similar conversations about Go, though a few more people are familiar with go than rust where I work. For monitoring and metrics, I'll admit I'm not overly familiar with the state of this in rust because I've never gotten to this point before. Depending on your organisation's attitude towards this and what APM you use, you might end up dead in the water here or you might get a free pass. Looks like there's a few statsd libraries available and that might be good enough for your org. Personally, your post reads like "I want to use new shiny rather than old thing" which isn't necessarily a negative but you need to be conscious and considerate of your coworkers before introducing something like this.
Reddit is written in Python and it works fine. You can always continue to use Python but write the performance critical parts in Rust.
Well, I know what community this is and that you love Rust much (which is good and will help you a lot in your further dev path) ... but I must give you my honest opinion regarding this business decision: 1. Vert.x is one of the top overall stacks for what I'm guessing from your usecase. The only real and big drawback is memory usage. This is where Rust really shines. I would not consider anything else like Go or Elixir based stacks. It seems that you're using Spring? Creating a Vert.x prototype for your slowest Java component should take a couple of days at most. 2. I use Kotlin at my main work projects. I must say that Kotlin is the by far most pleasant and productive language I've ever used. Sorry, Rust has no chance here. However: Java &lt;&lt; Rust &lt;&lt; Kotlin. Also, Kotlin is first class citizen for Vert.x 3. JVM applications can be made native ones with impressive memory and startuptime numbers [quarkus.io](https://quarkus.io) 4. Technically Rust is impossible to beat. But you need to consider ergonomics and business demands.
&gt; I don't think that it's counter intuitive across functions at a minimum. At the end of the day, an `impl Trait` is a concrete type, so you shouldn't be able to mix different concrete types with the same "name". Nothing is counter-intuitive if you understand why it is the way it is :). I really don't want to go on a mini-speech about this, but it breaks simple refactoring fn foo(b : bool) -&gt; impl Debug { if b then "true" else "false" } cannot be refactored to fn g() -&gt; impl Debug { "true" } fn h() -&gt; impl Debug { "false" } fn foo(b : bool) -&gt; impl Debug { if b then g() else h() } Similarly, inlining the RHS of a type alias (once the impl Trait type alias feature lands) changes the semantics of the program because of the type equalities that might be lost. Of course, both of these make perfect sense once you understand WHY this is the case. I don't think that means that the behavior is intuitive, it just means that is teachable/learn-able. Those are two different bars.
Honestly, I'd probably look at golang before rust in that case. I really like rust, though.
Great. Thanks for not helping. I figured it out thanks to people better than you.
Note to self: Never use mio, it's crap. Good to know the response to important projects like servo needing working dependencies and having to fork your ***broken crap*** just to continue is "Yeah... ok."
Actually, this was brought to the table by the whole team, we have a couple of teams in the company that are full Java devs and my team is full python. We recently inherited some real bad/unstable Java applications and we don't have the knowledge to tuning, and TBH I don't think It would make a difference, and after a week analyzing the services the team decide it would be faster rewriting (it's giving our user daily headaches) the improve the code, a couple of this service are real simple and we talked to give a try to a new language because we belive that python maybe turn into a problem in the near future because we have performance AND infrastructure cost goals to met, so we can just scale a lot to servers to reach the goal. So we decide to divide the teams to research Go, Elixir and Rust for a month, and then show the results (performance, monitoring, libraries and how would help us in our scenarios) and debate if would be a good fit to use any other tech or just rewrite in python, not only thinking on now, but also in the company goal for the next year.
Synchronization of the counter might slow it down it bit, but I'm not sure by how much.
Probably the best way would be to create a custom trait, like VecEntityExt: trait VecEntityExt { fn func() -&gt; impl Iterator&lt;Item = Entity&gt; { /* ... */ } } impl VecEntityExt for Vec&lt;Entity&gt; { /* ... */ }
Ive been working on switching my library over to Rust2018. Is that a major change according to semver or is that an API-compatible change? I'm trying to figure out if I should capture that in my changelog and if I should make a new release because of it.
I agree with u/Green0Photon. You should just be using get_mut, that returns a mutable reference with the correct lifetime and everything. The problem in your code is that when you mutably borrow `self.grid[point]`, that mutable reference only stays for that scope (i.e. the function). `get_mut` gives you a reference with the same lifetime as the `Grid` itself. The reason `rustc` doesn't give you an error when you use regular references is because `&amp;T` implements `Copy`, so it can be copied to a reference of a higher lifetime. However, `&amp;mut T` does not implement `Copy`, because you can't have two mutable references to the same thing at once in Rust. (Also, you can't have an immutable reference to something when there is already a mutable reference to it.)
Also note here that `into_iter()` and `iter()` do _almost_ the same thing because numbers are `Copy`, meaning that it will get copied anyways.
&gt; Reddit is written in Python and it works fine. This statement is *highly* debatable as I experience issues with Reddit on the regular. Although that might not be Python's fault.
&gt; they are starting to thing that maybe worth use Kotlin You probably already know this, but I hope the Java devs at your company understand that rewriting from Java to Kotlin won't, by itself, fix performance issues since they both compile to the same bytecode that runs on the JVM. They could just as well rewrite/refactor the code in Java to get performance improvements. Just wanted to mention that, but it sounds like they may just want to use Kotlin regardless.
If this is the first Rust project at your place of work, keep this in mind: *any and all problems with the project can and will be blamed on Rust.* It doesn't matter if it's not Rust that's really at fault, it just seems to be a reality that new languages go through. And if the first project fails, it'll be really hard to overcome the reputation of Rust being immature and unstable and use it in future projects. Therefore, do your due diligence and ensure that Rust is a good fit for a project. If your find that it is a good fit, here is some advice that I've learned from using Rust in production. 1. **Only use Rust stable.** Other programmers will frown and be suspicious if the first thing they have to do is install Rust nightly from the 23rd of December 2018. It makes it easier for new Rust developers to pick up a Rust book and contribute to your project. 2. **Vet your libraries.** If you want to use a library from crates.io, take the time to investigate it. Does it perform well under pressure? Is development active? Is it a pure Rust library or a binding to a well-known C library? Read the code even; are there places where it panic? Can those places be reached via user-controlled input? Does it have a lot of transitive dependencies? Like I said, Rust will be blamed even if the fault resides in libraries, so be sure that you feel comfortable with the quality of the crates you pull in. Some libraries that I feel confident using myself: clap, serde, serde_json, bincode, regex, chrono, rayon, log, env_logger. I currently stay away from anything using tokio (complex and difficult error messages) and error-chain and failure (many allocations). 3. **Keep it simple.** Rust is a very expressive language, and it can be fun to dabble with the advanced features, but I recommend keeping things simple. You don't need to build libraries that will be useful to hypothetical users, you need to write a program that is useful to your company. I like to stick with structs, enums, and functions. I rarely need traits or macros. Go is often lauded for how simple it is for new programmers to understand a Go program; aim for that with your Rust program. 4. **Don't stay stuck.** A common way for Rust programmers to become stuck is when they can't figure out their way around the ownership system. You can get out of a lot of jams by using `.clone()` and/or putting objects in a `Vec` and using indices. It's not necessarily the best, but again, if nothing gets done while you're trying to figure out a no-copy approach to a problem, Rust will be blamed for being a language where development is slow. 3. **Mind your ops team.** Be sure that your program is ops-friendly: you need logs and time-series metrics, you need a machine to build the program (careful about glibc incompatibilities), and an ops-friendly way to start, stop, and restart the program. To your ops team, it should feel no different from deploying a project written in C or Java. Sorry for the wall of text, I just hope it was helpful. There was a failed Rust project at my place of work, and when I decided to write my own project in Rust, I had to work hard to overcome the stigma of Rust being an immature and unreliable tool.
Your godbolt link compiles the code with no optimizations at all. Try giving it the option `-C opt-level=3` (-O 3 seems to be broken). Also are you benchmarks run in release mode? While the two versions do generate different asm I dont think that the performance difference would be that hight.
I have 2x Intel Xeon CPU E5-2670 clocked at @ 2.60GHz. It's a bit old, but if you just want to test a dual-socket setup with 16 cores (32 threads), I'd be happy to close everything and run `cargo bench` a few times.
Clever. I was about to implement a binary search, but your suggestion is better. Thanks again.
&gt;\-C opt-level=3 \`cargo bench\` compiles/runs in release mode, so that seems ok. I was also suprised, I would have expected a much smaller impact.
Great Job! Would it make any sense to add [evmap](http://docs.rs/evmap) to the benchmarks ?
I think that your function is only parameterised by a type T. try `calc_ballistic_range2&lt;T&gt;` instead of `calc_ballistic_range2&lt;T,V,A,H&gt;`
Thank you, I think it was the copying that I missed. However, `grid` is not a slice, so I can't use `get_mut()`. It does store a vector internally though.
I suspect compiler may not be able to optimize slice indexing in all scenarios. I don't think there is anything wrong with using `unsafe` as it guarantees you optimized indexing
If it changes your minimum supported rust version, then depends who you ask, look up that debate. If not then I think it is not a semver breaking change.
Thanks! I could add evmap but it doesnt make a ton of sense as it is quite different. evmap in its core makes it so that writes arent visible until a flush which is very expensive. The maps benchmarked here are all general purpose maps whose writes are visible instantly. evmap is also very read optimized and writes are horribly slow since you can only have one writehandle and thus a need to wrap the writehandle in a Mutex. All maps here are balanced and have good both read and write speeds.
That would be great! When you have the results you can either submit them as a PR to the repository or alternatively just DM them to me and i will add them and credit you.
Yeah, I understand, that do not make sense to add it then. You could probably just update the readme then to avoid the same kind of propositions as mine :)
The conceptual model is that there's a `move`, always, so I don't understand what you are getting at.
&gt;I was about to implement a binary search [https://doc.rust-lang.org/std/primitive.slice.html#method.binary\_search](https://doc.rust-lang.org/std/primitive.slice.html#method.binary_search)
If we are trading anecdotes then I wasn't confused and English isn't even my native language.
I agree with the other commenters, moving the logic into layers is proposals a good idea if you have reason to not expose the fields. If you're not sanitizing the fields when they're being set, then restricting use to accessors isn't as important and that could be a valid option. Aside from your main question, I have to ask if there's any reason you're skipping the first layer and extending one past the end? When iterating a list like this, it's generally preferred to use an iterator rather than indexing. For example, your code is equivalent to: `for layer in self.layers.iter_mut().drop(1).take(output_layer_index) { ... }`. Structured like that, it becomes clear that you're skipping certain layers.
`const_generics`, which are still a work in progress. There are some crates that provide workarounds, but due to the lack of const_generics, need to implement their traits manually for each array length using macros. https://github.com/rust-lang/rust/issues/44580
Perhaps you'd need to built some type level representation of normalised products/fractions?
&gt;How can one use features for conditional benchmark compilation? I have a feature that disables a chunk of my code, and I want to conditionally disabled the same chunk of code in my benchmark. \`cargo bench --features ...\`
Yes, that was what I was using.
This is as fast/faster than the unsafe version for me: pub fn insertion_sort_iter(data: &amp;mut [i32]) { for sorted in 0..data.len() { let min = data[sorted..].iter().enumerate().min_by_key(|&amp;(_, e)| e).unwrap().0; data.swap(sorted, min); } } Obviously could be written more nicely.
Don't you think having too many languages, frameworks instead of 2-3 that are used everywhere will create problems rather than solve them? 1. Devs will be stuck in the same team unless they can pickup the new language that the other team is using quickly. It will impact retention. 2. It will be hard to make across the board improvements to frameworks and impossible to have an team that handles engineering systems: build, tools, platform, deployment, etc. I have a few more examples, but these are the main ones.
This loses almost all the overhead but is just a smidge slower than the unsafe code pub fn insertion_sort(data: &amp;mut [i32]) { for sorted in 0..data.len() { let min = data .iter() .enumerate().skip(sorted) .min_by_key(|(i, e)| *e) .unwrap() .0; data.swap(sorted, min); } } LLVM doesn't do a good job fixing the bounds checks introduced by [], best to just avoid them altogether!
Here's my attempt. I solve a simple physics problem where you drop a ball off the edge of a building. The function should return how long it takes to fall to the ground. The workhorse function is `time_to_hit_ground`. The signature is verbose, but it is far more manageable than what you were using. use std::ops::Mul; use std::ops::Div; struct Length&lt;T&gt;(T); struct Acceleration&lt;T&gt;(T); #[derive(Debug)] struct Time&lt;T&gt;(T); struct Time2&lt;T&gt;(T); impl&lt;T: Mul&lt;Output=T&gt;&gt; Mul&lt;T&gt; for Length&lt;T&gt; { type Output = Self; fn mul (self, rhs: T) -&gt; Self::Output { Length(self.0 * rhs) } } impl&lt;T: Div&lt;Output=T&gt;&gt; Div&lt;Acceleration&lt;T&gt;&gt; for Length&lt;T&gt; { type Output = Time2&lt;T&gt;; fn div (self, rhs: Acceleration&lt;T&gt;) -&gt; Self::Output { Time2(self.0 / rhs.0) } } trait Sqrt { type Output; fn sqrt (self) -&gt; Self::Output; } impl Sqrt for f64 { type Output = Self; fn sqrt (self) -&gt; Self::Output { f64::sqrt(self) } } impl Sqrt for f32 { type Output = Self; fn sqrt (self) -&gt; Self::Output { f32::sqrt(self) } } impl&lt;T: Sqrt&lt;Output = T&gt;&gt; Sqrt for Time2&lt;T&gt; { type Output = Time&lt;T&gt;; fn sqrt (self) -&gt; Self::Output { Time(self.0.sqrt()) } } fn time_to_hit_ground&lt;T&gt; (height: Length&lt;T&gt;, gravity: Acceleration&lt;T&gt;) -&gt; Time&lt;T&gt; where T: Mul&lt;Output=T&gt; + Div&lt;Output=T&gt; + Sqrt&lt;Output=T&gt; + From&lt;f32&gt; { (height * T::from(2.0) / gravity).sqrt() } fn main () { let height: Length&lt;f64&gt; = Length(20.0); let gravity: Acceleration&lt;f64&gt; = Acceleration(9.8); println!("{:?}", time_to_hit_ground(height, gravity)); let height: Length&lt;f32&gt; = Length(20.0); let gravity: Acceleration&lt;f32&gt; = Acceleration(9.8); println!("{:?}", time_to_hit_ground(height, gravity)); }
I don't think this code is correct; you need a `+ sorted` on your max or such.
Reddit is the only major website (Alexa Top 25) I've used that has regular, random outages almost every single week. Might have nothing to do with Python, but Reddit definitely doesn't work fine.
I can say that Rust is great for speeding up small parts of Elixir code.
I'll try to get that for you today, but so far the results look pretty similar to what you were seeing. Concache is maxing out my RAM and swapping to disk a lot, which is slowing it down quite a bit.
Things I'd want to know: Is it a novel data structure, or a based on a design from a paper, say? What design/implementation decisions make it "fast"? What other properties does it have: is it lock-free? Wait-free? If neither, what's the worse thing that can happen under contention?
Yeah, this make sense. Is not exactly what I was looking but from the POW of rust is the way to go.
Ah, i see. It's not based on any kind of paper. It's what i drew up in my free time to solve a problem. It's not wait-free at all times but 99% of practical accesses are in fact wait-free. The worst thing that can happen under contention is another thread having to wait for thread 1 to drop its reference if they are in the segment. If you aren't holding long lived references, access is for all intents and purposes O(1).
They are doing that much pipelined requests - where the connection is kept open and the clients sends the next request already before a response to the previous one had been received. Therefore the results will not be comparable. For non pipelined requests one can look at the Jason serialization results. But I think even that reuses the connection. If the OP creates a new connection per request the result will obviously be slower.
Yeah concache was maxing out my ram too which made no sense. All other maps had much lower usage.
C++ is a ratio to itself, as is the rust. I'm still working on this, but it seems the C++ is quicker because it can do parallel initialization. I'm trying to figure out how to do this in Rust, and will come back with better Graphs as soon as I have them : )
Advertise your no_std support upfront more clearly! That's huge for those of us in the embedded/OS space.
Congrats! The implementation looks pretty clean, and I can see how it'd scale well. One quick win for lookups would be to use the `nightly` feature of `parking_lot`. With that turned on, `parking_lot` performs lock elision potentially completely eliminating cache invalidation in read only situations. Very much not apples to apples, but could you add `crossbeam-skipmap`? It's not yet published as a crate, but it's available on github. Scaling graphs (x axis - num threads, y axis - ops/sec) would also be pretty valuable.
I would argue that functions can return booleans if the name of yhe function makes obvious enough, like `object.is_something()`
Will do!
I found a weird variant... &amp;#x200B; This one is safe Rust (but needs \`#\[feature(nll)\]\`, so nightly-only) and compiles to optimal machine code... but only when using \`-C opt-level=z\`. [https://godbolt.org/z/bTIqdO](https://godbolt.org/z/bTIqdO) pub fn insertion_sort_fast(data: &amp;mut [i32]) { let mut data = data; while data.len() &gt; 1 { let (head, tail) = data.split_first_mut().unwrap(); let tailmin = tail.iter_mut().min().unwrap(); if head &gt; tailmin { std::mem::swap(head, tailmin); } data = tail; } }
I honestly thought that this would be a game where you had to figure out the type of some expression as fast as possible. But this is cool, too
&gt;pub fn insertion\_sort\_iter(data: &amp;mut \[i32\]) { for sorted in 0..data.len() { let min = data\[sorted..\].iter().enumerate().min\_by\_key(|&amp;(\_, e)| e).unwrap().0; data.swap(sorted, min); } } Interesting: It manages to optimize away the check in the tight inner loop (not the one in the outer), but that already seems good enough for performance. Weird that it can see the limitation by \`len()\` when using \`enumerate\`, but not when doing \`0..len()\`. Looks like a missed optimization opportunity in the compiler &amp;#x200B; Benchmark numbers: \`\`\` insertions sort (fast) time: \[187.05 us 187.37 us 187.84 us\] insertions sort (iter) time: \[186.87 us 187.30 us 188.00 us\] \`\`\`
I'm not discouraging them from learning, and I've said that doing this is possible. I've just pointed out that the consequence of doing this is that the resulting Rust programs end up being a pain to code, read, modify, extend, refactor, etc. You end up essentially in trait bound hell. Encouraging people learning to end up in trait bound hell does them IMO a misservice. If they are learning, its easier to learn Rust by using it as a C, not by trying to do C++/Scala/Haskell meta-programming.
I thought you added it, but just forgot to update the results section of the readme.
Yeah i know of parking\_lots nightly. I do want ccl on stable. I might add a nightly feature for that though. Ill add skipmap. Ill look into graphs! Thanks!
Also, forcing the elements of the `Vec` to be 64/128 bytes in size would probably help reduce false conflicts on the locks.
I beg to differ, Rust's type system has a lot to offer and I find that exploiting that for learning purposes and pushing the boundries leads to a better understanding of how to use traits and types more effectively in normal code and to limit or eliminate invalid states.
A slice works
But with a slice, doesn‚Äôt that mean a vector could still be passed to it?
This is cool! Two comments: - Having to type an extra space at the end of a passage was surprising. I sat there, having completed my entire passage, staring at the screen waiting for something to happen. I typed an extra space some seconds later, and got my score reduced since it counted that wait against me. - I got a passage starting with this: `That is the inevitable human response. We‚Äôre`. That apostrophe is not an apostrophe, but rather a right-single-quote. I can't type that on my keyboard and it's counting it against me :(
&gt; but needs `#[feature(nll)]`, so nightly-only I naively thought NLL was the default for edition 2018? Is there a more NLL version of NLL?
Please rerun or do a partial rerun to include the crossbeam-skiplist aswell.
Results for the SkipMap has been added.
Yes but why do you need to restrict that?
No, I was just confused for a second because the compiler explorer uses the 2015 edition because that's what rustc does by default. You need to pass the `--edition 2018` flag to rustc.
That's good relevant information. If you're planning on doing a rewrite, I'd honestly recommend sticking to what you know but seem to think your python is gonna tip the way the Java has. You're probably not wrong given CPython's reputation for performance (assuming you're using that and not pypy or another alternative interpreter), though Instagram has some interesting articles on scaling their CPython infrastructure, I'd recommend checking those out as well. I won't comment on the benefits rust could bring beyond the standard memory safety, etc etc since it's more of a "hey this is really neat" for me right now other than focusing on increasing throughput is probably more worthwhile than raw IO numbers.
Thanks! Sorry about that, I got a bunch of the quotes in bulk from what I'm guessing is a really old release of fortune (although I'm not certain, someone just posted it on GitHub). I need to sanitize the quotes a little more, but since the majority seemed good, I wasn't too worried. The space is a little annoying too. I have that as a TODO on the README, but initially made it this way because that's how typeracer operates too. This will keep me entertained during builds at work though, so you should see some more improvements in the future!
I didn't argue otherwise, I just pointed out that if all your functions need where clauses like the one the OP wrote, you end up spending more time playing type tetris than getting stuff done: fn calc_ballistic_range2&lt;T,V,A,H&gt;(speed: V, gravity: A, initial_height: H) -&gt; impl Length&lt;AmountType=T&gt; where T: FloatScalar + num_traits::Num + num_traits::real::Real + Copy + PartialOrd + From&lt;u8&gt;, V: Copy + Velocity&lt;AmountType=T&gt; + Mul&lt;Pure&lt;T&gt;&gt; + Mul&lt;V&gt;, A: Copy + Acceleration&lt;AmountType=T&gt;, H: Copy + Length&lt;AmountType=T&gt;, Pure&lt;T&gt;: Mul&lt;A&gt;, MulOutput&lt;V, Pure&lt;T&gt;&gt;: Div&lt;A&gt;, MulOutput&lt;V,V&gt;: Mul&lt;Pure&lt;T&gt;&gt;, MulOutput3&lt;V,V,Pure&lt;T&gt;&gt; : Mul&lt;Pure&lt;T&gt;&gt;, MulOutput&lt;Pure&lt;T&gt;,A&gt; : Mul&lt;H&gt;, MulOutput4&lt;V,V,Pure&lt;T&gt;,Pure&lt;T&gt;&gt;: Add&lt;MulOutput3&lt;Pure&lt;T&gt;,A,H&gt;&gt;, AddOutput&lt;MulOutput4&lt;V,V,Pure&lt;T&gt;,Pure&lt;T&gt;&gt;, MulOutput3&lt;Pure&lt;T&gt;,A,H&gt;&gt;: Sqrt, SqrtOutput&lt;AddOutput&lt;MulOutput4&lt;V,V,Pure&lt;T&gt;,Pure&lt;T&gt;&gt;, MulOutput3&lt;Pure&lt;T&gt;,A,H&gt;&gt;&gt; : Add&lt;MulOutput&lt;V, Pure&lt;T&gt;&gt;&gt;, DivOutput&lt;MulOutput&lt;V, Pure&lt;T&gt;&gt;,A&gt;: Mul&lt;AddOutput&lt;SqrtOutput&lt;AddOutput&lt;MulOutput4&lt;V,V,Pure&lt;T&gt;,Pure&lt;T&gt;&gt;, MulOutput3&lt;Pure&lt;T&gt;,A,H&gt;&gt;&gt;,MulOutput&lt;V, Pure&lt;T&gt;&gt;&gt;&gt;, MulOutput&lt;DivOutput&lt;MulOutput&lt;V, Pure&lt;T&gt;&gt;,A&gt;,AddOutput&lt;SqrtOutput&lt;AddOutput&lt;MulOutput4&lt;V,V,Pure&lt;T&gt;,Pure&lt;T&gt;&gt;, MulOutput3&lt;Pure&lt;T&gt;,A,H&gt;&gt;&gt;,MulOutput&lt;V, Pure&lt;T&gt;&gt;&gt;&gt;: Length&lt;AmountType=T&gt; {
Add `#[derive(Clone,Copy)]` in all enums and structs, so you can use `x =&gt; *x` in the `match` ([example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=61f52e728ea257f1d9594cd4ea5bdc08)).
Ok, your original comment sounded like you were being discouraging, I agree that these bounds are too much, but there is a way to eliminate these bounds in Rust by taking a slightly different approach, which I commented earlier.
Oh... maybe the default that should be changed now? With a toggle to revert.
This is very cool! If I knew Rust at all, I might help you implement the multiplayer functionality.
Lovely! &amp;#x200B; A skipmap should not be faster than a hashmap... makes me wonder what's wrong with the other hashmaps.
I'm glad, that I'm not the only one :-D
&gt; Rust is great if you need the benefits of no GC (no pauses, lower memory usage) or ship runtimeless software I would also emphasize static checking: - vs Python: well, Rust is statically typed. - vs Java: there's no overhead for using wrapper structs in Rust, so you don't need to use bare i32/f32/String anywhere and instead can properly type your variables. Especially in fintech, I'd really encourage people to lean hard on proper typing, going as far as using phantom types if necessary. Nobody wants to send an amount expressed in JPY (0.0091 USD) to something expecting the amount in USD because a conversion was forgotten somewhere in the pipeline.
Makes me wonder too. Skipmap is really well implemented though and effort has been put in to make it as efficient as possible. Chashmap has a couple of implementation issues and conchashmap is just *bad*.
It‚Äôs for a crate I‚Äôm working on. I need compile time certainty of the size of the type I‚Äôm using.
I've looked at both uom and [dimensioned](https://github.com/paholg/dimensioned). The latter provides an example of a generic function to compute speed from length and time. But I think it suffers from the exact same problem if extended to a more complex example.
A better example might be Instagram which, I believe to be, the largest production deployment of Django. It doesn't have the random outages that reddit does. Haha.
Also YouTube. And before it was sold to Google, it was in PHP...
I'm not sure I follow. The actual type of `KilometersPerHour2&lt;f32&gt;` is `QuantityT&lt;f32, SIUnitsT&lt;SIRatiosT&lt;Kilo, Unit, Hour&gt;, SIExponentsT&lt;P1,Z0,N2&gt;&gt;&gt;`. The traits, such as Velocity and Acceleration, are barely used. In fact they aren't used within the library at all. The trait for Area is: impl&lt;T: Foo, R:Ratio&gt; Area for QuantityT&lt;T, SIUnitsT&lt;SIRatiosT&lt;R,Zero,Zero&gt;, SIExponentsT&lt;P2, Z0, Z0&gt;&gt;&gt; {} I could not use these traits at all. That would be mostly fine. But it would be really nice for users to be able to verify assumptions. For example when doing a complex computation asserting that a value in middle is an acceleration. It'd also be nice just for self-documentation purposes. Finally, it's useful for stuff like I gave. Imagine a dead reckoning function to compute an intersection point. That can be useful when working with Meters and Seconds. But it might also be useful to use with Kilometers and Hours. I could write two versions of the function trivially. But it also makes sense to write a generic version that works for any scale of Length and Time.
Can you offer alternatives? My background is video games. There's a lot of people excited about Rust for games. There's a whole website dedicated to game crates! [http://arewegameyet.com/](http://arewegameyet.com/) Having type-safe units for games would be really, really nice. I think I've actually gotten a lot of the way there. The last step may not be possible. But maybe it is! Or maybe it will be with some cool new feature in 2019 or 2020. I don't know. Which is why I posted.
Obviously I **don't** want to write code like that! The question is is it possible to write code the achieves the same goal without that wall of garbage.
You can write half-decent Rust that will blow away the java services with respect to memory consumption. Many java programmers have reported substantial savings with a Rust port. I don't recommend porting "easy" java services, though. Pick whatever java service is consuming the most memory. If you want to convince your team about the benefits of using Rust, make a big splash.
Go has a runtime (that provides GC, scheduling goroutines, creating Go stack, etc.) that is linked statically. I think you meant, no virtual machine running a byte code. Go does compile and link to produce a native binary with no additional dependency.
By that I mean that one does not need to install any runtime environment.
Thanks friend :) I had a lot of fun learning rust with this project, if you're ever looking for an open source project to contribute to, to learn rust, feel free!
Look into using the [`generic-array`](https://crates.io/crates/generic-array) crate.
There are many languages with great support for unit systems, F# is probably the most mainstream one of them all. &gt; Having type-safe units for games would be really, really nice. Nobody disagrees, but that does not change the fact that Rust, the programming language, does not have good support for units. Can one build crappy unit libraries that are painful to use and only kind of work? Sure, that can be done. Should people do that and use that on production? Depends, masochists would enjoy that. Otherwise, probably not.
&gt; The question is is it possible to write code the achieves the same goal without that wall of garbage. Not with the current language. You can trade the wall of garbage by other walls of garbage, but they are going to end up being somewhere.
\&gt; Rust does not have good support for units. Can one build crappy unit libraries that are painful to use and only kind of work? I think it's possible to build a good units library in Rust. But I'm not yet sure if it's possible to build a *great* one.
What a lovely way to appear working really hard :)
Good means different things to different people, what's not good enough for me might be good enough for you.
Except minimum supported compiler version the choice of edition should be invisible to dependencies, so it's not a breaking change (or any change at all).
Oh cool! It looks like this is the guy who did ripgrep - which the Rust Book references and uses as inspiration to do the minigrep project. Thanks for the great pointer, I'll check it out.
It's funny you mention this, because I wanted to try writing an imitation of numpy as a learning project. Is this crate for rust what numpy is for python?
I've hit an annoying case with the borrow checker. I've condensed my issue down to a small code example \`\`\` **struct** Foo&lt;*'a*\&gt; { **reference**: &amp;*'a* **u8** } **fn** some\_condition() -&gt; **bool** { **true** } **impl**&lt;*'a*\&gt; Foo&lt;*'a*\&gt; { **fn** try\_next(&amp;**mut self**) -&gt; Option&lt;Bar&gt; { **if** some\_condition() { *Some*(Bar { **reference**: **self**.**reference** }) } **else** { *None* } } **fn** next(&amp;**mut self**) -&gt; Bar { **loop** { **if let** *Some*(bar) = **self**.try\_next() { **return** bar } } } } **struct** Bar&lt;*'a*\&gt; { **reference**: &amp;*'a* **u8** } \`\`\` Does anyone know how I can make this compile?
&gt; but still, there is some resistance. It's unbelievable The way I read it is that the java devs probably understand this, and want to stay in the language they are familiar with, but are being pressured by management / other devs to switch to kotlin for whatever reason. Hence the "resistance" part.
I'm not talking about returning, that may make perfect sense. I mean taking them. &amp;#x200B; device.connect(true); Just from looking at that, you don't know what that true means. device.connect(TimeoutSetting::InfiniteWait); Just looking at it we know that we are waiting forever until that connection happens.
You even get the aggressive typing aspect as if you've just solved a difficult problem and a ready to work on it!
And soon it will be for 2015 too.
Surely that would break backwards compatibility?
You are opening,reading,writing, and closing each connection in sequence.
Thanks for your reply I did try that but it did not quite work as I dont have copy clone traits. But I think it will return for. Primitives
Great I see why didnt do that for your initial solution but its good to know thank you
You also wrote: &gt; I also want it to only be of u8s and have the size be the only thing that‚Äôs dynamic. So which is it? Do you want the length of the slice to be static or dynamic? If it‚Äôs static it‚Äôs fixed and known at compile-time. If it‚Äôs dynamic one given piece of code can decide the length of the slice at run-time, based for example on user input.
Unless you mean `std::mem::size_of::&lt;&amp;[u8]&gt;()` rather than the number of `u8` items. The `size_of` for a slice is always 16 (assuming a 64-bit platform) since the memory representation if `(*const u8, usize)`. This is even if the slice borrows from memory owned by a `Vec`, or anything else.
&gt;I need to sanitize the quotes a little more Some other things in the texts to look out for that I've noticed: * sometimes a text contains a double space between sentences * some texts contain ‚Äì instead of - &gt;The space is a little annoying too. I have that as a TODO on the README, but initially made it this way because that's how typeracer operates too. I was also thrown off by the required space at the end, it's definitely not how typeracer behaves for me. Unless I misunderstood :) Some other things: * Some texts are a little long, maybe there should be a way to skip the current text. * The cursor is often not in the correct spot: after finishing a word it doesn't go back to the left, and also after pressing backspace it often stays in place. * Incorrect letters don't always show up in red in the text, sometimes it takes multiple incorrect letters for them to show. Anyway, this is really cool, I'll definitely consider contributing.
No management pressure here, all tech decisions are made by the dev+ops team, when the java devs came in the stack was Python and legacy PHP, they refused to work with both, and as beginners in the company smaller services/tasks were given to them to work on (or python or PHP) and they would rewrite everything in Java because "python and PHP sucks" the company grew so fast we had whole team (dev, management, product) new to the company and this culture of rewrite everything in Java because "python and PHP sucks" (without having any argument) catch up. In one meeting I had to hear that "everyone who develops services in python in the company should be fired because Python sucks". &amp;#x200B; Ironically, all the Python services are the most stable services in the company according to our APM. EDIT: And, everyone was told in the interviews they'll be working with Python and PHP. Almost none of them respected the company stack.
I don't do a lot of fancy math myself, but maybe the "nalgebra" crate? You'll also probably find bindings to a lot of C libraries, if there's one in particular that you'd like to use.
Oh, I see I misunderstood the problem, sorry. For that I would look to a crate like uom and typenum for inspiration.
Kotlin is like the JVM's C#. And C# is very pleasant to work with, so that's very believable
Come now, back in the good old days we didn't have NLL, and we managed just fine (most of the time, YMMV...): pub fn insertion_sort_fast(data: &amp;mut [i32]) { let mut data = data; while data.len() &gt; 1 { let (head, tail) = {data}.split_first_mut().unwrap(); { let tailmin = tail.iter_mut().min().unwrap(); if head &gt; tailmin { std::mem::swap(head, tailmin); } } data = tail; } }
As long as the application isn't super heavy on data processing (i.e. generates a lot of garbage) , I would second Kotlin. It's much more ergonomic than Java and gets you some of the nicer features from rust, e.g. sum types (sealed types), null safety, coroutines. It lets you focus more on the business logic and is easier to onboard new developers with than rust. By far the biggest advantages are clean interop with existing Java code and the huge ecosystem of JVM libraries. It allows you to incrementally improve the legacy java application without throwing everything away.
I wasn't really talking about where the java came from, more that you mentioned them resisting switching to Kotlin (Which I don't think is necessarily a bad thing to resist, since moving languages just for the sake of moving, instead of refactoring existing code bases can be a massive time sink. Java can handle IO bound applications pretty well). Regardless, if what you said is true about them completely ignoring the tech stack and forcing java, that does not sound like the kind of devs I would want to work with.
Yeah, today this is under control after the CTO intervened, but not after seeing a lot of perfectly running applications being rewriting (and not respecting the deadline because of the rewriting) just for the sake to change the stack, when the original task was adding one route to logically delete a database record.
Yes, I spent some time looking at their benchmarking and among other things, this is bound to be a big factor - I am indeed creating a new TCP connection per request. Whether that's a bad idea is something I'm still thinking about with respect to how I think real-world usage would/should look.
enjoy that darkness today at https://github.com/bluss/indexing
Yes, I think you're right. However, what changes could be made to improve this situation? Since I explicitly want to use a single thread for this benchmark, spawning more isn't the answer. The mio/tokio tests will allow me to test non-blocking I/O and event loops/green threads, but for this test specifically, can it be do any better?
An idea for the readme: record a short demo to http://asciinema.org so it‚Äôs easy to see in action
For me these two variations compile out all unwraps and other possible panics, and seem to generate pretty okay code: pub fn insertion_sort_1(data: &amp;mut [i32]) { let mut data = data; while data.len() &gt; 1 { let (head, tail) = data.split_first_mut().unwrap(); let (tailhead, tailtail) = tail.split_first_mut().unwrap(); let tailmin = tailtail.iter_mut().fold(tailhead, std::cmp::min); if head &gt; tailmin { std::mem::swap(head, tailmin); } data = tail; } } pub fn insertion_sort_2(data: &amp;mut [i32]) { let mut data = data; while data.len() &gt; 1 { let (head, tail) = data.split_first_mut().unwrap(); tail.iter_mut() .map(|x| (*x, Some(x))) .fold( (*head, None), |a, b| if a.0 &lt;= b.0 { a } else { b } ) .1.map( |min| std::mem::swap(head, min) ); data = tail; } } The first one basically inlines Iterator::min so it can figure out that it will never return None. The second one is quite strange but seems to generate code that is at least different and possibly better.
The interesting experiment would be to port the Rust version to Java and see what happens. PHP quicksort should be able to outrun C bubble sort, but that doesn't mean much. The OP is in an IO/async heavy scenario. I think that writing a Rust version that outperforms a Java version (given equally skilled coders) will be more expensive. Maybe a language with first class async will yield the most productive result (runtime performance/implementation cost). If you need the absolute best performance Rust might be the way, but that is rarely the case.
Any single thread solution handling multiple sockets is going to be an event loop in some sense. Using mio is the most bare solution that is still cross platform and worth the effort to at least try once in order to understand Futures better. If you want to go crazy, on Linux you can go further with things like sendmmesg, the new io_uring, raw sockets, passing the network device to userland/writing a kernel module. But for these solutions you need real hardware to benchmark it.
I can give an example from personal experience. When I first learned Rust, I was too wedded to the idea that a `move` of the actual bytes would occur (ie. a copy plus invalidation/delete). As a result, I had a preference for C-style code where you pass in a mutable reference as an "out parameter" and then mutate it to return data.
&gt; we are seen the company is growing real fast and python soon will not able to handle the job as well it does today You really sure about that? This can be benchmarked and measured; switching to a different language is very very costly and you might never realize you paid the prize unnecessarily. If you have lots of Python expertise and code, an option is just to move the hotspots to another language. I've seen plenty articles about writing Python modules in Rust and there seem to be ways to do the same in Go (lots of number-crunching Python libs use this approach to great effect). It is also normal for the Java devs to want to stay on Java. It is mighty fast, statically typed, huge and productive ecosystem. Kotlin gets talked around a lot, but I wouldn't be a first-adopter (outside Android). It's starting to get traction, but if in 5 years time a better Kotlin comes, do you really want to have a codebase which mixes Java which was never ported, the Kotlin-ported bits and have new stuff written in new-Kotlin? Kotlin is definitely nicer than Java, but you should stick to languages unless necessary- and Java is a particularly powerful language to stick to.
Not if the compiler explorer just changes its template for new sessions, comparable to how `cargo new`'s template changed.
Here's a super stupid question (probably). I want to load in an indexed png or bmp (preferably png) and for each pixel get the **palette index** for the colour. The Image crate just seems to be able to give me the RGB colour values but that's not what I want. If I have a 16 colour image, I would expect numbers between either 0 and 15 or 1 and 16.
Yes, of course. Static typing yields a lot of performance advantage- I'd always choose static typing if I could. I wouldn't underestimate the JVM/Java's inlining capabilities. They add all the time optimizations which can make even existing code much faster. You have lots of control in Rust you can use to your advantage, but Java's "lack of control" sometimes results in the compiler/JVM being able to optimize stuff you wouldn't expect to (e.g. I believe at some point they made some String concatenations use StringBuilder under the hood, making it faster than code which was written with StringBuffer).
Yes! And probably sooner than Java :) I'm `await`ing that eagerly as it will make Rust the best option in even more scenarios. I think Java still has the edge on productivity but hopefully Rust will become the way to go for some scenarios. I'm very happy with Python/Java for mostly everything. I've added Rust to my repertoire because I really like it, but it's uncommon for me to find fitting use cases, but I feel that with those three + Typescript I can handle 99% of the stuff I might need to do, esp. when Python/Java/Rust ship solid async first-class support. That should help me avoid Javascript and Go :D
This is a subreddit for the Rust programming language not Rust game. I would consider taking your post down.
Additionally, putting your loot on the first floor in the exact center of your base is a huge mistake. It does not make raiders search for the loot at all; they'll know exactly where it is.
Thanks, looking into it.
Rust is lightning fast. C/C++ speed but safer and easier to use than C/C++ but does have some novel concepts to learn. Once those concepts are learned it‚Äôs quite nice. I find myself more productive in Rust than in Go and have switched my web app over to Rust and only took about a day of work to port it. Maybe you should on nights and weekends try writing something in rust and see what you think?
 The first one works because it returns a copy of `Once::val`, as you pointed out. The second one tries to return a copy of `OnceMut::val`, which is `&amp;mut T` and does not implement `Copy`. Why is this? If `OnceMut` returned `&amp;'a mut`, there would now be two mutable references to the same value. One copy would be inside the structure, and one would be returned. There can only be one mutable reference to something at any given time. The following code works because it ensures there is only one mutable reference to the value at one point. [(playground)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2dd3623a71b4b43988134b092a1634fc) ```rust struct OnceMut&lt;'a, T&gt; { val: Option&lt;&amp;'a mut T&gt;, } impl&lt;'a, T&gt; Iterator for OnceMut&lt;'a, T&gt; { type Item = &amp;'a mut T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.val.take() } } fn main() { let mut value = vec![1, 2, 3]; let mut thing = OnceMut { val: Some(&amp;mut value), }; println!("{:?}", thing.next()); println!("{:?}", thing.next()); } ``` The following code would also work, although it's not compatible with `Iterator`: [(playground)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=ed72fa03599f398a87e6832f3db7dd20) ```rust struct OnceMut&lt;'a, T&gt; { val: &amp;'a mut T, seen: bool } impl&lt;'a, T&gt; OnceMut&lt;'a, T&gt; { fn next&lt;'b&gt;(&amp;'b mut self) -&gt; Option&lt;&amp;'b mut T&gt; { if !self.seen { self.seen = true; Some(self.val) } else { None } } } fn main() { let mut value = vec![1, 2, 3]; let mut thing = OnceMut { val: &amp;mut value, seen: false, }; println!("{:?}", thing.next()); println!("{:?}", thing.next()); } ``` Do note that this has a quirk as well: [(playground)](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=9c9226c8c9e76f1361104d6080550b1b) ``` fn main() { let mut value = vec![1, 2, 3]; let mut thing = OnceMut { val: &amp;mut value, seen: false, }; let mutref1 = thing.next(); // first borrow let mutref2 = thing.next(); // cannot borrow `thing` as mutable more than once at a time println!("{:?}", mutref1); // needed so mutref1 doesn't go out of scope automatically } ``` Because we changed `OnceMut::next`'s return value to share the lifetime with what it returns, we cannot use it again as long as its return value exists. That's why when we try to borrow `thing` mutably a second time, it won't let us, since `mutref1` is still in scope. I'm bad at explaining but I hope the examples helped regardless!
My 2 pence: As some one with production applications in Kotlin, Java, C#, Python, Rust, PHP, Haskell, and a tiny but of Clojure + sandbox/toy experiments in Ruby, Elixir and Julia. By far my favourite is Rust, a couple of reasons: * Can create statically linked runtimeless binary, deployment is beautiful. * low memory footprint * damn good performance * no GC * Cargo * Language feels high level, can encode lots of business logic in very expressive way. * Once it "compiles" it will pretty much just works. * Once deployed, it's just keeps working Cons: Learning curve, only for early adopters.
Oh wow! After reading this it seems so clear to me! Thank you very much. This has been giving me a headache for days.
&gt; I know that &amp;T implements Copy, while &amp;mut T doesn't, but I don't see how that works in this example. That is indeed the key to what's going on. I think the errors are confusing here because sometimes rustc is able to do little tricks with an `&amp;mut T` (like "re-borrow" it to make a function argument, without moving the original), but those tricks aren't working out in this case. If you replaced your `&amp;mut T` with a different non-Copy type, like say `Vec&lt;T&gt;`, you'd get a more familiar error: "cannot move out of borrowed content" &gt; Also, why can't the compiler deduce that, because the reference returned is taken from &amp;mut self, that the lifetimes are ok and don't break any rules? The problem is that "not breaking any rules" here depends on your logic. Suppose you deleted the line `self.seen = true;`. At that point you'd certainly be breaking rules; your iterator would keep repeatedly trying to give out the same `&amp;mut T`. The compiler isn't able to inspect all the logic of what you've done and reason that you're upholding the rules. Instead it sees that what you're trying to do _could_ break rules, and it gives up. That said, you can totally make this work if you keep the `&amp;mut T` inside an `Option` and use `take` to get it out. Like this: struct OnceMut&lt;'a, T&gt; { val: Option&lt;&amp;'a mut T&gt;, } impl&lt;'a, T&gt; Iterator for OnceMut&lt;'a, T&gt; { type Item = &amp;'a mut T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { self.val.take() } }
Yeah, I think that's just an issue with doing things this generically. The thing is, the compiler knows the exact bounds that are needed. I wonder if they could be inserted by a procedural macro. Typenum has an `op!` macro that makes expressing some of the bounds a bit nicer, but you still need them. I also think that specialization of associated types may fix this. I think, then, it may be possible to have a trait that you say can performs all the operations and returns a type of the same trait, so that would be the only bound needed. I'm not certain it gives us that, but I think it could work.
Thank you so much! I will have to read into re-borrowing. I wrote those examples as a quick-and-dirty example, but my real issue was with a much more complicated iterator over a container. Because I can ensure the safety of my iterator, that the compiler can't detect, does this mean I'll have to use `unsafe`?
[ndarray](https://docs.rs/ndarray/0.12.1/ndarray/) has a page titled ["ndarray for NumPy users"](https://docs.rs/ndarray/0.12.1/ndarray/doc/ndarray_for_numpy_users/index.html) in its docs.
6 years working in fintechs/banking, never had this kind of trouble with dynamically typed languages. Yes, sometimes we miss something here and there, but never saw any mistake like that going through any production environment. My experience through different languages made me notice that dynamically typed languages rely heavier on tests and this helps avoid this kind of things.
This reminds me so much of my first experiences with rust. Sounds like a cool project!
This is definitely something I'm going to do, actually, I'm already doing, I asked here to gather more info about use cases in my scenario from people with more experience in Rust. I saw some talks about Rust, showed up in some meetups for beginners in my city and now this question here to learn more about the language use cases. Now I'll get my hands dirty and further down the road create a REST API (we all should create the same API in the language we are researching).
I recommend against using unsafe unless you are absolutely sure you know what you're doing. Almost always when the compiler "can't detect what you're doing", what you are doing is not safe. Then again, I suppose ignoring this advice is also part of the learning process! If you can provide a more detailed example we can see if there is a way to explain to the compiler what your intention is!
I was writing a `Grid&lt;T&gt;` class, representing a grid of a fixed size filled with `T`s. I was trying to write an iterator over the neighbors of a point, so you'd say `for (p, x) in grid.neighbors(Point(10, 10) { ... }` to iterate over the elements at (10, 9), (11, 10), (10, 11), and (9, 10). This worked until I tried writing `Grid::neighbors_mut()`, where I ran into the issue I described above. I did some other reading and saw a bunch of people say that, for your own containers, iterators over `&amp;mut` often do need `unsafe`, like in slices.
In `uom`, it‚Äôs simply true that `Length::div(Time)` gives a `Velocity`, all of which have arbitrary units (i.e. arithmetic on these quantities, and all others, are fully defined and generic). We‚Äôve put a *lot* of time into establishing named quantities for a lot of frequently-encountered concepts, and any other quantities can be used anonymously (or with a typealias or named via a PR to `uom` proper). In essence, while it‚Äôs possible that your usecase requires you to roll your own, I strongly recommend using `uom` and filing issues if you find anything lacking. The author is extremely responsive and helpful, and I‚Äôve been waiting for more work to do on it. Plus, it‚Äôs a really cool crate. :)
That does seem reasonable!
So wait. The `DHashMap` is basically just a wrapper that splits up elements into 256 `RWLock`ed sub-`HashMap` based on the hash prefix. And it's competitively one of the fastest concurrent hashmaps (if not the fastest currently) on a decently realistic synthetic benchmark. (Though I suppose it should be noted that it is not lock free.) First question: Why isn't `DHashMap` parameterized over a `S: BuildHasher` like std `HashMap`? Second question: How hasn't this been done already? Third question: How is this better than all the other options? It feels like it shouldn't be. It _is_ a 10KiB static base space cost (should probably use a `Box&lt;[_; _]&gt;` instead of a `Vec&lt;_&gt;` to save an inline pointer since it doesn't need to resize ever), but for a singleton resource at least that doesn't seem like _too_ too much.
Awesome! Thank you for being so helpful.
Medium paywall is preventing me from reading that article of yours üòî
[Quick "life hack"](https://github.com/rust-lang/rust/issues/20866#issuecomment-495962961).
Your example does not contain any code using Foo, can you post a (not-) working playground example?
The `png` crate can give you access to the raw bytes and pallette: * construct a `Decoder` and call `read_info()` * the pallette is on the `Info` struct returned by `Reader::info()` * use `next_row()` to read the image bytes row by row or preallocate the right size buffer and use `read_frame()` to get the whole image (it returns an error if the buffer isn't big enough, which isn't the best for ergonomics but whatever)
Another option would be to use `Option::take`, which returns Some(x) if the Option is Some, and leaves None in its place, or simply yields None. The code would be fairly simple struct OnceMut&lt;'a, T&gt; {
My primary use case is to learn. :) I didn‚Äôt love how uom hides the ratios. I‚Äôm trying a different approach. Which is obviously not without issue! Trade-offs am I right?
Wrong sub.
All right then, godspeed. :)
There's a good chance you can find a way to wrap Slice::IterMut rather than needing to write unsafe code. But I'd need to look at the internal representation you're using to be sure. Take a look at what was done here: https://github.com/carllerche/slab/pull/39
&gt; PHP quicksort should be able to outrun C bubble sort, but that doesn't mean much. I mean, sure, asymptotics are a thing, but I threw together a little [microbenchmark](http://github.com/BartMassey/sort-race) and it looks like the breakeven for PHP quicksort *vs* C and Rust bubble sort is about 10,000 elements. So‚Ä¶yeah.
You don't need to take `&amp;mut self` in `next` and `try_next`, just take `&amp;self`. I think what's going on here is this: 1. The compiler doesn't want to allow a mutable reference to `self` that outlives the scope of the loop, because then you could potentially have multiple mutable references to `self` 2. But the compiler needs the mutable reference to `self` that it uses to call `try_next` to last outside of the scope of the loop because the return value of `try_next`, `Bar&lt;'a&gt;`, needs to outlive the scope of the loop
r/playrust
Came here to suggest the same thing. C++ "transfers ownership" usually, which is also idiomatic English, and refers to titles and deeds and the general concept of legal ownership as I understand it.
One thing that is important to understand is that `&amp;mut T` does not mean "mutable reference" to the compiler, similarly `&amp;T` does not mean "immutable reference". Instead, `&amp;mut T` means a *unique* reference to `T`, and `&amp;T` means a *shared* reference to `T`. Thinking about iterators from this light leads to some deeper understanding. The compiler hasn't got the slightest clue that `self.seen` ensures the reference is only seen once. But it goes through your program and checks what's up. With `Once`, because you are producing `&amp;T` (shared references) it doesn't matter if you make lots of them, so Rust allows you to make as many as you want. With `OnceMut`, because you are producing `&amp;mut T` (unique references) it must be unique. Let's make a small change to your program to see why Rust complains. impl&lt;'a, T&gt; Iterator for OnceMut&lt;'a, T&gt; { type Item = &amp;'a mut T; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; { if !self.seen { // self.seen = true; Some(self.val) } else { None } } } Now, you are returning the same reference forever. So calling next multiple times will yield aliasing unique references! Oh no! You can't have multiple *unique* references. Rust can't tell the difference between the program above and your program, it's methods are too simple to detect the difference (not to say that the methods are themselves simple, they are very intricate and complex, just that they are not sufficient to detect the subtle difference) So, since uniqueness is hard to prove, `&amp;mut T` is harder to implement in general.
rust is apparently lacking in quality concurrent hashmaps. I agree with your summary of `DHashMap`, but IMO it undersells its effectiveness. Grabbing one of N locks (pseudo)randomly is a pretty performant strategy regardless of the data structure. A more nuanced (bucket locking) approach would be faster, but probably not by a huge amount.
:Unfortunately in the real code a mutable reference is needed. I found a workaround, but it involves doing a small amount of work twice.
The website and the way its been handled and discussed is such a blemish on the image I otherwise have of Rust and the community. :S
Then the language specification is wrong and should be ignored, because this behavior is stupid. The complier should produce an error, or produce a warning and translate the code literally instead of deleting it.
Fix is already deploying, we had to revert a CSS refactoring. Please file GitHub issues in the future, this is a clear bug. We're not all reading reddit.
&gt; everyone who develops services in python in the company should be fired because Python sucks I would hope that whoever said that in a meeting was at least suspended for a week. That is completely unprofessional and out of place.
Can you tell me the name of this VSCode fork that includes the server for http instead of electron? That sounds terrific
Thanks for the informative comment! &gt; I currently stay away from anything using tokio (complex and difficult error messages) What do you recommend for async I/O, specifically HTTP?
That's the first thing I suggested though?
You don't seem to go much into what you like about Rust, just what you don't like about other languages. But based upon what you've mentioned here, it seems like Golang might also fit your needs, and I think it has better job prospects than Rust.
Why hasn't rustup adopted this yet?
For serialization, have you looked into [serde](https://github.com/serde-rs/serde)? It separates serialization from output format, so you can easily swap it.
I also noticed that the folder structure grows... downward. In my opinion things like Serializable should be close to the top so its easier to find.
Bucket locking is what Chashmap does but I still beat it. The advantage of DHashMap is its ability to realloc and do map wide ops concurrently.
\&gt; in before "but language specs can't be wrong!"
Great thanks I will give that a try!
1. I don't know. Ive added it to my todo list for the next major release. 2. It seems that rust severely lacks some concurrent datastructures and everyones been busy doing other stuff and just RwLocking their hashmaps instead. I was using chashmap but i knew i could make it go faster. 3. (Sidenote: Just switched the vec for a boxed slice, thanks) Well sharding in general is a very performant way to increase concurrent performance of a datastructure. One of the main advantages here is that \`DHashMap\` can concurrently perform otherwise map wide operations such as reallocation while options like CHashMap cannot. The other thing you have to note is that the base sub-\`HashMap\` is very fast and optimized to a ridiculous level which makes building something slow on top of them very hard.
Those are super good for clear functions, but my impression is this is kind of an orthogonal concern? Your tips help with the details - but if the function's algorithm is to compute 5 different complex things and then combine them, then it's still going to be confusing no matter how nicely it's implemented. Isn't the point of CoC/CyC more to judge each single functions' overall algorithmic complexity, rather than necessarily the details of how it's implemented?
You could write some rust bindings for ROOT as your next problem :)
It would be interesting to see against [evmap](https://github.com/jonhoo/rust-evmap) as well. To be completely fair one would need a read heavy benchmark as well.
That's so interesting. As another data point, I often start with sketching data in Rust---a bunch of structs, newtypes and enums to model the domain. Then U start defining the transformations: \`Foo -&gt; Bar\` functions with sketch implementations. Ten finally fill in implementations, top to bottom, and changing things from previous steps as needed . After all, "all code is a narrative in which a series of confusing and unpleasant things happen to data".
&gt; **Vet your libraries.** The community/dev team should be doing this explicitly, so that each user/PM who's thinking about using Rust doesn't have to. We need a sanctioned "Rust Platform" set of libraries (modeled on the existing Haskell Platform) that's not constrained by inflexible backwards-compatibility mandates (*no "dead batteries" please*; instead, feel free to deprecate a solution when something that's all around better comes along), involves third-party libraries and not just `std`, and that showcases best-case Rust development practices, as well as pointing out useful "batteries" that people might reach for to solve their problems.
If you are doing most of your math within a single unit system, then you can make your variables generic over the unit system instead of making them generic over individual units. This will help you write algorithms, but it requires you to add methods for converting between unit systems. [link](https://gist.github.com/37d486884ec10cd8028ed7b456356490) trait UnitSystem {} struct Si; impl UnitSystem for Si {} struct Length&lt;T, U&gt; { val: T, _phantom_data: PhantomData&lt;U&gt; }
Why was that github issue closed in the first place? I don't see any sense rejecting people's design feedback. It gives a lot of people like me concern that there's a chilling effect going on.
That's awesome. I wonder if the breakeven for PHP vs. PHP is nearby :-p
Out of curiousity. How often do game programmers need to convert between unit systems? What are some real life use cases?
Not as much if you sent it down a channel on another thread
Good job, and congrats on the accomplishment :-) &gt;When I went to do this in rust, I wasn't really happy with any of the linear algebra packages available Could you perhaps elaborate a little on this? In particular, did you look at \`nalgebra\`? What did you feel was missing?
&gt; Isn't the point of CoC/CyC more to judge each single functions' overall algorithmic complexity, rather than necessarily the details of how it's implemented? Supposedly. Often times, though, I've seen them flagging a `switch` statement over an enum with 20+ values because it passed an arbitrary threshold. They would be happy with a map look-up and dispatch, but a switch dispatch is too many branches? Meh. Similarly, I've seen them flagging "long" functions (~100 lines) consisting of some guards at the top, the meat, and some post-processing. The recommendation would be to put all guards in a single "pre-check" function, and all post-processing in another... would that help? No, because now you couldn't when reading the meat ensure that the pre-conditions it assumes are actually satisfied since those are hidden elsewhere. I'm not a big fan of CoC/CyC. The measurements are arbitrary. The weight assigned are arbitrary. They'll assign the same complexity to 3 successive `if` checks than they will to a single `if` check with all conditions cobbled together on a single long undecipherable line. I don't think a single number can capture a function *implementation* complexity. I'd rather have guidelines of how to implement which guide you toward clearer code rather than a slap on the wrist when some arbitrary threshold is passed with no indication of what to do to satisfy the metric.
Why not just remove the quotes that don't end with a punctuation mark of some sort from the database?
&gt; My experience through different languages made me notice that dynamically typed languages rely heavier on tests and this helps avoid this kind of things. I subscribe to the Swiss Cheese model of managing risks: no single layer can possibly catch all bugs, so the best way forward is to accumulate layers, hoping that their holes don't line up. For the software I work on, this means: static typing, static analysis, assertions, run-time checks (at various boundaries), unit tests, component tests, non-regression tests, code reviews, pilot deployments and reporting + circuit-breakers. The goal is obviously to catch issues as quickly as possible: 1. Compile-time; ideally. 2. Unit-tests; pretty good. 3. Code-reviews; not their goal, but I'll take it. 4. CI-time; about an hour too late, not too bad though. 5. Pilot; points at missing test coverage. 6. Production; undesirable, but better late than never. It's a bit paranoid, maybe, but a bit of paranoia seems warranted.
You might also want to take a look at my [tic-tac-toe microbenchmark](http://github.com/BartMassey/ttt-bench). It shows PHP as about 80√ó slower than C. I wrote a PHP bubble sort for comparison with the C and Rust ones just now: it runs about 100√ó slower. Slooow PHP.
&gt; e.g. I believe at some point they made some String concatenations use StringBuilder under the hood, making it faster than code which was written with StringBuffer They did, indeed. Of course the issue in the first place was a slow `String` implementation which failed the principle of making the obvious case fast :/ &gt; I wouldn't underestimate the JVM/Java's inlining capabilities. AFAIK this doesn't help (much) with storing objects instead of built-ins as class members.
Yeah. I addressed evmap in another reply but i decided to not include it since it doesnt have the same qualities as the other ones i benchmarked since all included ones are balanced between read and write. Evmap is in another category with another use case.
I'm confused about the `log` and `env_logger` crates. Looking at the examples and reading the explanations I can't tell what the difference is.
What is happening here is lifetime elision. `Bar` has a lifetime. However, when you return `Bar`, you don't write the lifetime. As such, the compiler tries to be *smart* and inserts the lifetimes it thinks are correct: fn try_next(&amp;'b mut self) -&gt; Option&lt;Bar&lt;'b&gt;&gt; Those lifetimes are not correct, because you know that the `Bar` will live for `'a`. If you explicitly add the lifetime `&lt;'a&gt;` to `Bar` in both `try_next` and `next`, the code will compile: fn try_next(&amp;mut self) -&gt; Option&lt;Bar&lt;'a&gt;&gt; fn next(&amp;mut self) -&gt; Option&lt;Bar&lt;'a&gt;&gt; /cc /u/0332353584
&gt; Servo is a prototype implementation of *the Web* written in Rust. Servo is a prototype implementation of *a Web Browser*. [The Web](https://en.wikipedia.org/wiki/World_Wide_Web) is (courtesy of Wikipedia): &gt; The World Wide Web (WWW), commonly known as the Web, is an information system where documents and other web resources are identified by Uniform Resource Locators (URLs, such as https://www.example.com/), which may be interlinked by hypertext, and are accessible over the Internet.[1] The resources of the WWW may be accessed by users by a software application called a web browser. Indeed, the Web exists outside of Servo, or any particular Web Browser.
Thanks for an entertaining journey :) Specifically, those two passages really resonated with me: &gt; People write code based on the current behavior of the system, and so a block suddenly becomes a feature that additional code will rely on. This happens all the time. As systems grow and become more complex, there is some sort of emergent behavior that nobody really intended for the system to have which just spontaneously appear... and since the system is complex and understanding it fully is basically impossible, features are checked not against the system, but against a set of tests which accidentally depends on the system behavior. &gt; Instead of properly implementing the spec, our ‚Äúfalse compliance‚Äù with it was based on an unintended hack of relying on a blocking operation in an unrelated place. Removing that block exposed this unintended reliance on it, itself exposing the parts of the spec that we still needed to implement in order not to have to rely on that block. I've found myself quite a few times in this situation where a test starts failing, and investigating it you realize it was passing... for the wrong reasons! For example, a computation was wrong somewhere, but magically compensated for later down the pipeline, and now that you are ripping the pipeline on the issue is exposed. Or a test passes... for a much wider set of inputs than expected, because nobody wrote a negative test to ensure that the set of inputs was properly constrained. And then a poor soul comes which has to implement a "simple feature" and accidentally topples that carefully stacked up house of cards without realizing.
&gt; The community/dev team should be doing this explicitly, so that each user/PM who's thinking about using Rust doesn't have to. Sure, I know a couple of capable people that would actually like to work on this. The only thing I don't know is where to find the 100$k/year that they charge.
*The Web* is a set of protocols to produce a standard of internet communication, servo is an **Implementation** of said protocols, thus an **implementation of the web**. It's not a web *browser*, its a web *engine*.
The existence of *some* positive feedback should really be considered in the context of the visible majority response of dismay, or confusion, upon release of the redesigned website. And the frequent fresh criticisms that have occurred regularly since. Also consider that many of us have been politely waiting for the promised post-mortem before weighing in with too much passion. That is the post-mortem which is meant to address how the community was left out of the process and why the design was pushed through with so many regressions, like a complete lack of translations, leaving non-English speakers high and dry. Your flippant dismissal and redirection to some, any, positive response makes me worry the team handling the website is hiding in an echo chamber. I‚Äôm looking forward to the post-mortem, but am increasingly concerned it will be wholly defensive.
\&gt; We're currently not interested in full redesigns, especially not in that mockup &amp;#x200B; Shame.
Shame. I find the color scheme atrocious. Specifically, it makes digesting the actual information really hard because the colors are so distracting, and the constant background color change.
&gt; It's not a web browser, its a web engine. I can argue with that. &gt; The Web is a set of protocols to produce a standard of internet communication, servo is an Implementation of said protocols, thus an implementation of the web. I've never heard of such a shortcut being made. Even [Merriam-Webster](https://www.merriam-webster.com/dictionary/the%20Web) is in line with Wikipedia: &gt; the Web noun &gt; &gt; : the part of the Internet that can be looked at with a special program (called a browser) and that is made up of many documents which are linked together ‚Äîoften used before another noun Do you have any source for calling a Web browser/engine "an implementation of the web"?
The people responsible for the current state of the website are apparently [no longer involved](https://www.reddit.com/r/rust/comments/bqtkv7/skeleton_slayers_for_the_website_needed/eoajz1n/.compact). So let's bury the hatchet and give their replacements a chance, shall we? :-)
&gt; I understand that an owner cannot access its resources when there's a mutable borrow, but y is an immutable borrow An owner also cannot mutate a value when there are any immutable references (alias xor mutation), otherwise the value in the immutable reference could change without it knowing. I don't have an answer to the code you posted yet, just wanted to clarify that bit.
I was referring to the current people handling the website. I‚Äôm not grinding a hatchet, here, I‚Äôm just worried that apparently the only feedback welcomed is in support of those widely disliked design. And that‚Äôs been the case since it was released. The post-mortem is or was meant to address how the community was left out of the redesign process and why this design was hastily pushed through. It‚Äôs not a great look that the post-mortem has been continually delayed and, meanwhile, negative community feedback continues to be dismissed out of hand.
Try adding explicit type annotations to understand what's going on. In your first example, x is a mutable binding to an immutable reference. When reassigning x, you're changing what x is bound to, but not the value 'behind' the reference. In your second example, y is an immutable binding to an immutable reference, thus any change to y is forbidden. Addendum: It generally helps if you post a more concrete example that shows what you can do while you think you shouldn't be able to. For now I have just guessed what exactly you mean when you say you can mutate x.
At first glance it looks like it utilizes fine-grained locking.
Wait-freedom is a specific term when it comes to concurrent algorithms. i.e. an algorithm is wait-free iff there is a finite bound on the number of steps for all threads. Thus if you utilize locks in your implementation, it cannot be wait-free nor lock-free since locks can wait indefinitely under contention.
The database is just made of text files, so manipulating them isn't very easy, just a little tedious. It's in /assets/quote-pack.tar.gz for those looking.
I never really looked into serde, but I was aware of it's existence. When I was writing the json parts, it just seemed silly to use a crate for that, since it wasn't complicated. Then when I added msgpack, I just wanted to learn the format for myself. In the future, I'd probably use serde.
I assure you as a mostly neutral party, about 95% (my purely scientific, double blind study analysis) or more of people using Rust or thinking about using Rust either like the website or don't care. As they should. It's a nice website. The super tiny but very vocal minority of people constantly bitching about it have gone so overboard that I highly suspect their motivations. Usually at this point, someone mentions the accessibility aspect so that they can throw the entire moral judgement of how hard it is to get accessibility right on to the shoulders of the few Rust members responsible for the website. Those people are also in the wrong.
I did look at nalgebra and there definitely wasn't anything missing. It just seemed like overkill for the tiny amount I wanted to do at first, and then I was sucked in adding things to it. It was just too fun to switch!
I could first write a function that does all the transformations on raw data, but then I could simplify it and write a struct to hold all the stuff
I'm so unhappy with the folder structure. But this was the only way I could get it to compile without doing more research than "make rustc shut up" and it was just never at the top of the list. It's definitely on the list, though. It's so ugly!
You really should, it's what the entire ecosystem uses.
Be that as it may, I don't think there is much that the current people can do to make the postmortem happen sooner. Complaining to them about it is unlikely to make their work on the website more rewarding. The new website was controversial, but it's what we have now. The people involved are all gone, and fgilcher is trying to form a new team. I too look forward to the postmortem, but as far as the future of the website is concerned I think it would be best if the postmortem and things that have happened in the past are not brought up every time the website is mentioned. Try and pretend that the website just magically appeared yesterday in the state that it is now and focus on how to improve it. After all, as far as a new website-team is concerned, that is the situation they'll be in.
I can save you time on the post mortem wait! The Rust team wanted to unveil their nice new website at the same time as they released the 2018 edition of Rust. One of the biggest points of the edition unveil is to reintroduce people to Rust and show them all the great, new stuff we have. This is a great motivation and perfectly reasonable. Designing a website by a committee of thousands of Rust programmers would be a nightmare as shown in these frequent threads, entirely overkill for such a thing, and would not have let them release it anytime this decade.
That's pretty neat! I just added a section!
There's quite a couple of accepted issues around the coloring, e.g. the bad code highlighting (https://github.com/rust-lang/www.rust-lang.org/issues/758) the abrupt color jump at some spaces (https://github.com/rust-lang/www.rust-lang.org/issues/624) and the footer adjustment (https://github.com/rust-lang/www.rust-lang.org/issues/621). They are far more actionable then "have you considered going all white/blue again?" and will be covered in due time now that we are working on the page again. I hope that improves things for you.
What you‚Äôre saying sounds like ‚ÄúWe‚Äôre not interested in keeping our promises, and we don‚Äôt care about community‚Äôs opinion‚Äù, but in many more words.
I'm still confused that "Zen and the Art of Archery" got so popular that people would use the name as a template even if it never makes no sense ;). (The book was very particularly about the marriage of Zen and Kyudo and even there is a debate to be had if it does make sense)
&gt; The new website was controversial, but it's what we have now. The people involved are all gone, and fgilcher is trying to form a new team. I too look forward to the postmortem, but as far as the future of the website is concerned I think it would be best if the postmortem and things that have happened in the past are not brought up every time the website is mentioned. To be clear, I also want to be finished with the post-mortem, because its an emotional blocker for many things. I also don't want to it to be released without being complete (and there's blockers there), that also wouldn't to the topic justice.
The closing message is pretty clear here. &gt; thanks for the feedback. if you'd like to open a PR to reaarange some of the color backdrops on other pages that'd be great and i can give feedback on the PR directly. &gt; as i said previously, wholesale removing the colors is not currently up for discussion. closing this, and will give feedback on specific suggestions in PRs. Please note that the message makes it _very clear_ that adjustments will be discussed and accepted. There's multiple followup issues that _do_ state specific issues clearly, which are far more actionable and will be and and have been worked on. All of them are still open and waiting for suggestions/implementations, which we'd happily take. It was completely shut because people were using it as a discussion forum. Issue trackers are not that.
That's not what I'm saying at all. But if you want to stay angry and bitter forever at everything related to the website, *even* if the people responsible are all *gone* and your negativity is now aimed at *new* people who are *not* responsible for your anger and might, maybe, make things more to your liking, then that doesn't seem like a fulfilling experience, both for you *and* the new team. The phrase 'cutting of your nose to spite your face' springs to mind. The teams working on Rust are mostly volunteers. The previous website-team is disbanded and gone. A new team is about to form. I ask you: what can nagging to the new team about a postmortem possibly accomplish? They hold no power over the old team. They cannot summon them and force them to live up to their promises. The *only* thing nagging can accomplish is demotivate them, or convince them that community input is negative regardless of what they do. How is that in *your* interest? How is that in the interest of the community?
\- Unfortunately the texts just need sanitizing. It's slow and tedious, but I'll get to it at some point. \- I just added in a feature to hit \^N to get the Next text! (skip the current one) \- I just removed the requirement to hit space at the end! \- I've got a bug on the second bullet where text processing happens [https://gitlab.com/DarrienG/terminal-typeracer/blob/master/src/game.rs#L285](https://gitlab.com/DarrienG/terminal-typeracer/blob/master/src/game.rs#L285) I've tried manually moving the cursor to the left and even giving it another render cycle after clearing input, but the cursor is very stagnant. My code for input processing unfortunately has similar behavior to the tui-rs user\_input example [https://github.com/fdehau/tui-rs/blob/master/examples/user\_input.rs](https://github.com/fdehau/tui-rs/blob/master/examples/user_input.rs) They get around it by pretty much having a coordinate where they tell the cursor to go rather than using relative positioning, but it isn't very elegant. \- I've noticed the coloring on incorrect lettering isn't always consistent. I'm not certain why. I'll have to take a closer look later. Thanks for all the feedback! It's great to have a second set of eyes on this!
Hi, I am the author of rust_siwg. Returning reference for Java is not supported for obvious reason. All java types is classes and object lifetime is not deterministic. So for such code: //Java FooRef foo_ref = obj.getMeReference(); user can save foo_ref somewhere and no way to prevent it. So you need to write some wrapper around Rust's builder, that return value, not reference and can live as long as possible: https://github.com/Dushistov/rust_swig/pull/203/files
Hi r/rust! For my development workflows, I often use Docker to make my builds more reproducible. But I found it cumbersome to manually build and publish a Docker image for my CI environment every time I added a system package or updated a dependency. I built Toast to automate that process. You can think of it as "Make meets Docker": you define tasks (install packages, build, test, etc.) in a configuration file (like Make), and Toast runs them in containers (with Docker) and caches the results locally and remotely. You can share the same remote cache as your CI system. For me, this means no more writing Dockerfiles and manually building/publishing Docker images for my builds; I can just define tasks and let Toast take care of caching. Toast can also be used for "non-cacheable" tasks like running a development server or deploying an application, so perhaps a better way to describe it is as a "containerized development environment". I had a great time using Rust for this project. The ownership/borrowing system gives me a lot of confidence that Toast doesn't forget to clean up temporary containers or images. I've also enjoyed being able to do CPU-intensive tasks (e.g., computing hashes of lots of data for cache keys) without needing to switch to a faster language. I've switched from Haskell to Rust for most of my side projects now (and it feels great!), though I still reach for Haskell for quick-and-dirty experiments. Feedback on the code or the design is welcome, as are pull requests and bug reports!
The wider popularity of the idiom Can probably be blamed on Robert Pirsig‚Äòs imitation of the title.
Hi, I am author of rust_swig. I delete my previous post, because I made it from my buddy's laptop, and logged into wrong account. I am not often read posts from here, davemilter pointed me to this post, but you free to ask via issues in github, where I can provide more fast feedback. Returning the references is not supported for Java, because of lifetime of reference is not deterministic: //Java FooRef foo = obj.getFooRef(); This reference is possible to save somewhere, and no way to prevent this. So for "builder pattern" you need use something like this: https://github.com/Dushistov/rust_swig/pull/203/files
The `[]` notation does magic. From the [docs for `Index`](https://doc.rust-lang.org/std/ops/trait.Index.html) &gt; `container[index]` is actually syntactic sugar for `*container.index(index)`, but only when used as an immutable value. If a mutable value is requested, `IndexMut` is used instead. This allows nice things such as `let value = v[index]` if the type of value implements `Copy`.
No, `x` is an `i64`, not a reference.
Does this make it possible to run containers without root or docker group access?
Hyrum's law: &gt; With a sufficient number of users of an API, it does not matter what you promise in the contract: all observable behaviors of your system will be depended on by somebody.
The `log` crate provides a generic logging API, but doesn't provide a backend that actually prints the logs anywhere. `env_logger` is a backend for the `log` crate, which logs to stdout or stderr based on some environment variables. Libraries should only ever use the `log` crate, since it should be up to the application how logging is actually configured. Applications can pick one of many different logging backends depending on their needs; `env_logger` can be the simplest if all you need is printing messages to `stderr`, but there are [lots of other options](https://docs.rs/log/0.4.6/log/#available-logging-implementations) if you want configurable logging to log files, syslog, etc.
Those changes wouldn't remedy my complaints in any way. Those rainbow background colors are just not reconcilable with good readability. The green and red are the worst offenders.
And quite likely some expectations will be wrong-headed. I've seen applications which expected a consistent iteration order out of an `unordered_map` whose keys was pointers. It's quite remarkable it worked during the development, though the tests were flaky until that mistaken assumption was fixed of course.
Sure! The original book is (despite all of its problems) still super popular, though, and the second lesser known outside of the US. But I can never keep away from being kind of marvelled by how these processes work :).
No, Toast uses the Docker client to manage containers and images, so for now it needs access to that client as would any user running Docker commands.
Awesome, thanks for the help, and the great work on rust_swig!
This looks fantastic! I see mention that toast can upload tagged images to a registry, how can I use that feature? (I'm thinking of using it to build my personal docker images)
I think you want `Deref`, but you can only have one implementation. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=b83602a84487deb00b7033e0074291b6
Are you familiar with Nix? I'm wondering if and how this differentiates itself from the Nix build tool/package manager as it also provides reproducible builds, but doesn't use containers (by default).
Thank you. Does this have any other knock on effects? By one implementation you mean that I couldn't repeat this process for `inner_a` and `inner_b`? Is there anything in the works to improve this?
Somebody else came with the same idea of using VScode API to modify the syntax highlighting based on tree-sitter parsing. I've asked for Rust support and the author added it this Week end. Some issues but the author is actively listening to feedbacks. He doesn't know Rust so I'm trying to feed him maximum of information to help him here: https://github.com/EvgeniyPeshkov/syntax-highlighter/issues/3 But I'm far from knowing Rust as well as some other ppl from this sub so any inputs are welcome!
Serializing to json by hand is quite easy but with serde it's often just `#[derive(Serialize, Deserialize)]` on every struct and enum and you are done. Then you can easily change from json to something else and even compare other data formats.
After each task, Toast commits the container to a Docker image and automatically tags it with a cache key, for example: `toast-41b74a3f9d4a2969e0ec8bd60645c4074c6a70c4c3430f01167fe256cb48824b`. If you run that task again (assuming nothing has changed), Toast will quickly find that Docker image and skip the task. If remote caching is enabled (via either the command-line options or the config file), Toast will push that image to a remote Docker registry. When you run the same task on another machine (e.g., a CI worker), Toast will pull the image from the registry and skip the task. So that's how Toast uses Docker images for caching. Currently, Toast does not provide a feature to customize the tags, though I am not necessarily opposed to adding it! But it would require a bit of thought. For example, if you wanted to build a streamlined image with just your final application binary in it (not any intermediate artifacts), there is no good way to do that with Toast since each dependent task becomes a layer in the final image. To make this feature useful, we would probably need to implement something like [multi-stage builds](https://docs.docker.com/develop/develop-images/multistage-build/) where we can use files from a previous task but otherwise start fresh from a base image. I think this is doable with the right design. Let me think about it for a bit. Open to ideas!
I‚Äôm a fan of these sorts of things. See also The Unfooable Barness of Baz.
I still make mistakes in module system every so often. I have set up empty project just to play around with files and folders so I can *get it*.
&gt; They did, indeed. Of course the issue in the first place was a slow `String` implementation which failed the principle of making the obvious case fast :/ No, not really. The thing here is that Java's `String` is immutable, which I believe is the most common choice in all programming languages (for instance, for security reasons). In that case: ``` String a = ...; String b = ...; String c = a + b + a + b; ``` The semantics of writing `s1 + s2` when `s1` and `s2` imply that the result of the concatenation is a newly allocated string. This means that if + is left associative, you need to make (((a+b) + a) + b), which builds two intermediate strings, copying all the time. I don't think there's really any way to make `+` fast other than what the Java devs did, which is turn that transparently into: ``` String c = new StringBuilder(a).append(b).append(a).append(b).toString(); ``` Which doesn't need all the intermediate strings and is efficient. So actually, the only thing here is that they did not do this on day 0, but: * Immutable strings is a good choice (for security, for instance) * Having simple semantics means you can optimize a lot of stuff. And if at some point they make a better `StringBuilder`, they can adapt the `+` optimization to use it at 0 cost, while those who optimized to `StringBuilder` won't reap the benefits. &gt; AFAIK this doesn't help (much) with storing objects instead of built-ins as class members. If you have a sizable amount of Java code, switching to Rust to get that benefit would need a few benchmarks to prove you are making an informed choice and porting is worth it. JVM/Java are amazingly optimized and it's common to be surprised.
FWIW I did not know that the website team was disbanded but I have been personally been staying far away from the issue tracker because of how heated it was initially. I'm sure there are quite a few people who would contribute small changes if they felt they are welcome.
&gt; Also consider that many of us have been politely waiting for the promised post-mortem before weighing in with too much passion. I think people (/u/fgilcher?) have been looking into the post-mortem, part of the issue is that to do one right you need to be able to talk to everyone involved, and these people are super busy. As mentioned below, it's also a completely new set of folks doing website things now, and we're trying to tie off a lot of the loose ends left over from the edition first. &gt; like a complete lack of translations, leaving non-English speakers high and dry. I can address this specifically: this is one of the loose ends I'm tying off. The old site was not sustainable for translations. A lot of the translated pages were out of date on format (let alone content). "Let's just make a copy of each page for each language" doesn't scale at all, and is not how translated live resources are supposed to work. We were already facing problems coordinating translations there. One of the explicit goals of the new website -- a Rocket app -- was to integrate an internationalization framework (likely Fluent) into it. Work [had started on it](https://github.com/rust-lang/www.rust-lang.org/pull/732), but as folks involved in the project no longer had time to finish it, it kinda got lost in the noise. The edition ended up being more work than anticipated for everyone, this is not unique to the website. We recently managed to get that working ([here](https://github.com/rust-lang/www.rust-lang.org/pull/792)), and are now working on getting everything converted to using l10n strings ([help wanted!](https://github.com/rust-lang/www.rust-lang.org/issues/798)). We're also setting up Pontoon and have a [translations policy](https://github.com/rust-lang/www.rust-lang.org/blob/master/TRANSLATIONS.md) so we hope to have some translations up really soon!
Yes, this has been a huge challenge :(
Well, I've got a sizable amount of Java and C++ code at hand, with the same algorithm implemented in both (for multiple algorithms). The Java implementations are not slow per se, but not so surprisingly the C++ implementations have a better *performance consistency*. That is, C++ has better tail latency than Java; and of course a smaller memory footprint (not a concern though).
SNAFU is perfectly suited for this: use snafu::{ResultExt, Snafu}; use std::{ fs, path::{Path, PathBuf}, }; #[derive(Debug, Snafu)] enum Error { #[snafu(display("Could not open file from {}: {}", filename.display(), source))] FileNotFound { filename: PathBuf, source: std::io::Error, }, } type Result&lt;T, E = Error&gt; = std::result::Result&lt;T, E&gt;; fn example(filename: impl AsRef&lt;Path&gt;) -&gt; Result&lt;String&gt; { let filename = filename.as_ref(); fs::read_to_string(filename).context(FileNotFound { filename }) } fn main() {}
&gt; ultimately close the issue without addressing anything Uh, what? [Things were addressed](https://github.com/rust-lang/www.rust-lang.org/pull/760), [as discussed in the issue](https://github.com/rust-lang/www.rust-lang.org/issues/612#issuecomment-491857131). It's also pretty disingenuous to call it a 4-month discussion, there was a short week-long discussion after the issue was opened, and it kinda fell by the wayside for four months because there was nobody managing the website, until /u/fgilcher stepped up and took the reins and brought the discussion to completion, further clarifying what the issue with "just" adding the proposed text was, and finding a decent solution that is adequately clear as well as not overly clunky.
It's been a while since I've done anything with this in Rust, so my memory might be a little fuzzy. But, the thing to do is to create a `Field` trait to implement for numeric types. Since the common numeric types already implement the normal field operations it should be very straightforward to do and you can rely on the existing traits like `Add` to do most of the work for you. By doing this you make your code generic over any type that forms a field, including the complex numbers. This is how nalgebra does it. You'll see that it has `RealField` and `ComplexField` traits. So the types that nalgebra supports is any type that can implement those traits.
You might find [this](https://crates.io/crates/shrinkwraprs) interesting.
Yes, of course. C/C++/Rust are unique in not having GC, so you don't eat pauses like in most other languages. I wouldn't discount memory usage- it's an important metric. However, I think that non-GC languages impose a severe developer overhead. Rust is the friendlier here- the compiler does most of the job. For C++ I believe if you follow a lot of rules (basically, what the Rust compiler enforces, from what I understand), you're done- but if you make a mistake you will pay the price. C I'm not sure of the status of the landscape. Developer productivity is also a performance factor!
https://xkcd.com/1172/
Good article! Some hint for not ending up with an architecture that is prone to the described problems: - If you perform communication between two tasks, always have a clear definition of what is the parent task and what is the subtask. The parent task must outlive the subtask. The parent task **can** wait for synchronously for responses from the subtask. The subtask should never do that, since it can lead to a deadlock. I'm not sure if this has been followed in this case: It's not obvious how the relation between constellation and embedder is. - If you perform asynchronous communication, think about whether you need timeouts or not. In the case of this example if the embedder never sends a response then constellation might still want to move on with navigation? On the other hand introducing a timeout for that also makes things more complicated -&gt; Now one would have to deal with the possibility that an embedder might send a response for request that is no longer active/stored. It must be avoided that this response is associated with the wrong request. From the article it also seems like they now built a hand-written state-machine inside constallation. That might be ok depending on the complexity of the component. But it might also be possible to run that as a more synchronous looking workflow with Futures or async/await.
&gt; I wouldn't discount memory usage- it's an important metric. In general yes, in the particular cases mentioned it's small enough not to matter. &gt; However, I think that non-GC languages impose a severe developer overhead. [...] Wholeheartedly agree with the whole paragraph. &gt; Developer productivity is also a performance factor! Sure... but the argument cuts both ways. For high-performance (either high-throughput or low-latency) applications, using a native language can be more productive because you don't have to fight the language you're using at every step to get the performance you need out of it. Furthermore, the focus on developer productivity is understandable, but it's just one part of a whole. I used to work in a company where one application required 500 servers. It took a team of 3 developers 6 months to eek 10% better performance, which was cause for celebration. The cost of running 50 servers, between hardware and electricity bill, is not trivial. And finally, as parting thought, using a GC to support pervasive mutability is a mistake, to me. When two pieces of an application end up mutating the same piece of memory because at some point a shallow copy was made instead of a deep copy... well, good luck. It's really hard to trace down; even more so that a memory error because there are tools to help with memory errors, but here all operations leading to that state are perfectly legitimate and idiomatic.
Maybe it is not the textbook definition. However it is not practical to build another abstraction layer around everything. If there is already a stable \`trait\` in another crate that defines the behavior - or especially if it's a stable \`std\` trait (like \`Read\`), then it should be fine to reuse this in a program with wrapping it in another layer. And often we have to create mocks for those. So being able to mock traits defined elsewhere would certainly be helpful. I think it's even helpful without talking about dependencies: I actually would prefer not to add any mocking related annotations directly to \`trait\`s in my library, since that now moves testing considerations into my \`src\`, and maybe raises questions for other people what that thing is about. I would rather prefer to just keep plain \`trait\`s without any attributes in \`src\`, and only generate mocks inside tests. E.g. via an extension trait, if that would be possible.
1. Not really. Rust has crates that are conceptually like Go packages, but you don't need to denote them explicitly in every file. 2. It is possible to build a production quality RESTful server using just the standard library, but it's not going to be a nice experience - you'll have to build it over raw sockets and handle all the I/O and parsing and such yourself. It'll be a much more pleasant experience if you use an existing web framework. 3. There is no GC whatsoever in core Rust. 4. Not sure what you mean by "reactive computing", but you can definitely use Rust in an RT context, though getting _really_ real time requires more than just avoiding GC. 5. Cargo does not generate any IR, but the compiler itself, rustc, does - the IR is stored in memory though, and never written to a file, unless you ask for it explicitly. 6. LLVM IR is kinda-sorta portable, but not in the same way as Java bytecode. Apple proved that it's _possible_ to build LLVM IR in a way that makes it target agnostic, but Rust doesn't really attempt to do that. It's easier to just cross compile to your target by going through the whole pipeline again.
1. You put all your needed packages in your cargo.toml. Cargo will take care of the rest (downloading, caching, etc.) 2. Rust is the opposite of Go here. Very minimal standard library. You will need to use a third party one. Look on crates.io. That's the official index. 3. There's no GC in Rust. THE Rust selling point is memory safety at "full speed" (meaning no GC is needed). 4. Yes that is possible. 5. Yes Rust is based on LLVM. 6. Rust can be cross compiled to a lot of systems. I think you should really check out the Rust website for more in depth infos: https://www.rust-lang.org/
rust has no gc at all. rust does generate llvm ir first. generating an intermediate representation is not a necessary condition for being cross platform. people were writing C for years and compiling it on many platforms before anyone invented the fiction that a VM or IR provided some unique cross platform benefit. what is convenient about producing llvm IR is that it is easier for the rust language team to target many platforms and with advanced optimizations since you leverage llvm to do the final step of producing machine code.
&gt; the fiction that a VM or IR provided some unique cross platform benefit That's far from fiction. Having a common IR for multiple targets allows you to do optimizations at IR level, and having a VM allows you to distribute the same exact binary to all targets. Those things are not fiction at all.
Thank you so much for a quick reply. If it‚Äôs okay, can I ask few more questions. For 1, if a main() became too big -because of variables, etc, how can I write main() across multiple files? For 2, regarding the linked rust doc‚Äôs multithreaded example, is it good enough to handle as simple hello api as a production level? (Given number of thread has been changed) Thank you so much
Thank you!! Will do
&gt; For 1, if a main() became too big -because of variables, etc, how can I write main() across multiple files? The same way you'd deal with it in any language - decompose your mega-function into multiple smaller functions, or types, or traits, or any other abstractions. &gt; For 2, regarding the linked rust doc‚Äôs multithreaded example, is it good enough to handle as simple hello api as a production level? (Given number of thread has been changed) Depends on your definition of "production level", I guess. Are you going to need TLS? Rate limiting? Authorization? All of those things are probably things you're going to want eventually in production, and your toy server will not do them without some serious wheel-reinventing.
&gt;LLVM IR is kinda-sorta portable, but not in the same way as Java bytecode. Apple proved that it's possible to build LLVM IR in a way that makes it target agnostic, but Rust doesn't really attempt to do that. It's easier to just cross compile to your target by going through the whole pipeline again. Ahh.. Gotcha! Again, thank you so much!!
Thank you!
philosophically explain how distributing C source and a compiler is different than distributing some byte code format (not a binary) and a java runtime? who benefits when distributing a thing this way vs just distributing an a actual executable? in theory the creator benefits a little at the users expense but in practice people usually also have to provide the specific java vm that crashes least for sufficiently complex java programs. as well as specific vm settings, usually ends up packed all together anyway.
IDE support can help a bit with this. E.g. in IntelliJ Rust you would only need to write \`impl Trait for Outer\`, and then choose the \`implement members\` quick action on it and select the methods to generate code for. Then at least you only have to fill in the inner forwarding code. &amp;#x200B; Also: If you defined the trait in your crate, you also don't require the wrapper. It is only required when both the trait and the struct that it gets implemented for are from 2 different places.
Just look at Android or iOS - both of those systems distribute bytecode (in iOS' case, the bytecode is actually AOT compiled by Apple, but that's beside the point really) to targets that can hardly fit a C compiler, much less run it on every app update. This also means you can target those platforms with any language that compiles to compatible bytecode. Also, Android is very much a Java VM, and most applications for it are compatible with, like, the last 5 years of versions - unless they're not "sufficiently complex" by your definition?
http://xion.io/post/code/rust-extension-traits.html
Why should I learn Rust? I‚Äôm coming up on my third year of a Computer science degree. I‚Äôm strongest in Java (3 years of classroom), but also have taken multiple courses in C++, python, and a watered down version of assembly language. I‚Äôve heard lots of chat about Rust, like how it‚Äôs an up and coming language with lots of potential. Can anybody corroborate these statements? what makes rust so great? Why is it worth my (and your) time?
which part of this do you think makes it that case that having a vm allows something to be cross platform? anything can be cross platform! you just have to compile it for each platform. either on a server (ios) or on install (android, so yes compilers fit on these) or every time you run the thing (java normally) somehow it became a truism people would say that compiling a bytecode each time you ran something made it uniquely cross platform. false.
&gt; which part of this do you think makes it that case that having a vm allows something to be cross platform? I never said that having a VM is the _only_ way to make something cross platform. I'm just saying that it's a valid way. &gt; anything can be cross platform! you just have to compile it for each platform. either on a server (ios) or on install (android, so yes compilers fit on these) or every time you run the thing (java normally) Android doesn't actually compile _everything_ ahead of time, and even when it does, it compiles a very simple register based bytecode, not the entirety of the source code. The reason they do it isn't because they _can't_ compile C, it's because they want it to be fast, and it's much faster to compile an IR to native code than it is to compile source code. &gt; somehow it became a truism people would say that compiling a bytecode each time you ran something made it uniquely cross platform. false. I've never said that.
You can probably use [rust-delegate](https://github.com/chancancode/rust-delegate) or [delegatable-derive](https://github.com/dureuill/delegable-derive), but it is often easier to make the inner field public and call the method on the inner field whenever you need it.
These are great questions! It's awesome to see that you're excited to learn more. I would caution you against switching to Rust because it's trendy, instead learning the reasons why it's trendy and making an informed choice. &gt; Go has a "package" syntax that goes on the top of every page of source code. Do Rust has one like this? Rust projects combine the `\[package\]` section of the crate's `Cargo.toml` file and Rust modules. I recommend taking a look at [chapter 7 of the book](https://doc.rust-lang.org/book/ch07-00-managing-growing-projects-with-packages-crates-and-modules.html) &gt; As I've been using Go mostly, I've been a big fan of standard packages Rust's standard library is much smaller. Generally speaking, you will find yourself looking outside of the standard library more often in Rust, than Go. &gt; Is it possible to build a basic Restful server only using standard packages? Probably not at the level of abstraction that you're looking for. Everything is there to build what you want, but the tools will be a lower level than you're used to. &gt; I was told two different stories about garbage collection of Rust. One says there's none There is no Rust garbage collector. Earlier iterations of the language included one, but it was removed well before 1.0. &gt; \[Is Rust\] capable of realtime programming? Yes. It offers you full control over how your program runs. That said, I don't know of any industrial examples of hard realtime programming in Rust. The industries with these needs are typically very conservative. &gt; Cargo automatically generates binary This is the output from `cargo build` and `cargo run`, but these commands actually hide many details. It's somewhat confusing, but `cargo` is acting as a front end for `rustc`, `rustc` produces output for LLVM. LLVM produces object code. Then the linker produces the binary. To reveal much more about what's happening when things are build, use the `--verbose` option to `cargo`. This will show you how `cargo` is driving `rustc`. To produce LLVM IR output, &gt; If Rust can create an IR file, does that means just like Java, it can port to any other system? just compiling the IR file? The IR that `rustc` produces is LLVM IR. If you save that file, LLVM will be able to compile it to a native executable for the target platform (CPU/OS pair). Generally speaking though, Rust programmers do not touch the IR. Another tool, [`rustup`](https://rustup.rs/) can help manage the targets that your system supports. To list all of the targets that Rust can compile for, use this command. You'll see several dozen appear. ``` rustup target list ``` Adding one is quite simple: ``` rustup target add &lt;target&gt; ``` You may need to add packages to the system. Generally, you require a linker that knows about the target's executable format. On Ubuntu, installing Rust from scratch and preparing it to generate static executables that work on all Linux machines looks like this: ``` sudo apt install -qqy curl git build-essential musl-tools curl --tlsv1.2 -fsS https://sh.rustup.rs/ -o /tmp/init-rustup.rs bash /tmp/init-rustup.rs -y rustup target add x86_64-unknown-linux-musl ``` Now we can build our project: ``` cd &lt;project&gt; cargo build --target x86_64-unknown-linux-musl ```
1. The main trait in `num_traits` is `Num`. It's implemented for floats, ints, and the various other types in the `num` family of crates like `Complex` and `Ratio`. It doesn't include casting by default, so you'd need to add `NumCast` or `FromPrimitive` to the type constraints. There are also the various traits in `alga` which are used by `nalgebra`, but they're a bit more technical to use and I'm personally not as confident in their usage. They're also probably not implemented for as many types. 2. `Complex&lt;T&gt;` implements `Num`. It doesn't implement `FromPrimitive`, but does have `From&lt;T&gt;`. It also lets you perform operations with the inner types type directly without converting it. 3. `std` is where code goes to die. By maintaining a small standard library, important code like compound numeric types can be updated without waiting for the compiler's release schedule. It also makes it easier to have breaking changes since the older version is still available to be used, even alongside the updated version. 4. I'm sorry to say I don't do a lot of scientific programming myself, so I'm probably not the most qualified to answer this. I can say that `alga` does provide implementations for `Complex`, so complex numbers should be usable with `nalgebra`. `ndarray` also seems to provide support for complex numbers.
Not often since they can just use an internal unit system and swap labels (ex meters ~ yards), but I don't think this library is aimed at games - OP said they're making it as a learning exercise.
Yes. I think OP might want to be using `nalgebra` instead of `num`. `nalgebra` doesn't seem to provide a `linspace` equivalent as far as I can tell: I don't know why. `ndarray` provides `linspace`, but only for `Float`.
We already got quite a couple over the last week, so it's pretty cozy over there :). The website team has not officially disbanded, no one has been properly forming one because of the reasons you outline and most of the people involved moving on somewhere else.
I also agree with what you say :D It depends a lot on what you're doing. In many places, it's just a few places that are performance sensitive and the rest not so much- you can spend a lot of time optimizing those (even using a different language) and use a productive language with low optimization effort for most code. In some others, maybe a high percentage of what you are writing is performance sensitive. I'm not familiar with those- you might be writing games or HFT or whatever. Sure, go with Rust there. It's always economics. Sometimes it's worth just to vertically scale- you can predict your load and you know the biggest EC2 instance is going to be enough. Sometimes you need to make sure you can scale horizontally and autoscale. Those two cases are "cheap" on developers, "expensive" on hardware. Sometimes you have stringent performance requirements meaning you need to get the last drop of performance from hardware- that will be "expensive" on developers. Make the numbers; they are certainly different in all cases :) ... I wouldn't say that GC is just there for mutability; even Haskell has GC. Of course mutability/immutability are not an easy choice. With immutability you have thread safety, but maybe you have a lot of allocation churn (lots of modifying stuff which yields new objects). Mutability can be more performant in some cases, but makes threading harder. Why is everything so complicated :-p?
&gt; There is one main thing I am confused about: why does changing nothing but adding mut make the compiler more strict about this? The existence of a usable `&amp;mut` value guarantees that it is the *only* one pointing to wherever it points to. `&amp;` are *shared* references while `&amp;mut` are *"unique"* references. This principle is upheld everywhere. This principle is the reason why you can't do this: let mut v = vec![1,2,3]; let p = &amp;mut v[0]; let q = &amp;mut v[1]; The existence of `p` makes the compiler consider `v` "mutably borrowed" already. So, trying that again could result in a second `&amp;mut` that aliases the first one which is not allowed to happen. In the above example, I used a different index. So, thechnically, it would be OK because `p` and `q` point to different things and thus don't alias. But we can't expect the compiler to be able to verify this. It doesn't really understand what `Vec`'s indexing operator does. Consider implementing your iterator in terms of whaveter `self.grid.iter_mut()` gives you instead of a `&amp;mut Grid`.
([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=49896f3bb8aaa7b0e14e194066541de0))
&lt;tangent&gt; As a C++ programmer who is interested in Rust and skims this sub, statements like: &gt; The `[]` notation does magic. Is the type of thing that pushes me away from Rust. I see from the other quote that it's just syntactic sugar and there are well-defined rules for when that kicks in, which immediately made me feel better. I wonder if this is true for many C/C++ programmers, that the idea of a language doing "magic" is off-putting. &lt;/tangent&gt;
It is a learning exercise. But my background is gamedev so it's definitely in the context of "how could a system like this be useful in games".
According to that page, it was tested on the old-ass PHP 5.6 interpreter? If you didn't know, the whole PHP 5.x line was EOL'd last year. The benchmark should be much faster on PHP 7 or Facebook's HHVM PHP interpreter.
This question was asked by someone else in a subthread somewhere but it got buried. Consider the following: fn main() { //#[derive(Debug, Clone)] #[derive(Debug, Copy, Clone)] struct Foo; let f = |a| |b| (a, b); dbg!(f(Foo)(Foo)); } ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2f75ea3267c5b779cefad9a89af509ff)) If `Foo` implements `Copy`, this fails to compile as expected: the inner closure outlives its parent so it cannot borrow `a`. But if I remove the `Copy` impl the code compiles cleanly! *What is going on?*
Not really on-topic, but you should probably pass a Range as argument (of which the right bound can be exclusive or not) instead of manually passing bounds
Hadn't noticed my PHP was out of date, thanks! Looks like PHP 7.3 is about 2.5√ó faster than PHP 5: the breakeven is now around 5000, with PHP running about 30x slower than Rust. I've updated my benchmark with the new numbers.
You already know `[]` has to do magic because in Rust, `f() = 3` is impossible syntax: there is nothing you can return from a function that can be assigned to, so if the `a[0] = 3` syntax is going to work, the subscripting operation can't just be sugar for a regular function call. C++ also has this kind of magic: `operator-&gt;` returns either a pointer or another object to call `operator-&gt;` on, so `x-&gt;y` is magic for `*(x.operator-&gt;().operator-&gt;().operator-&gt;()...).y`. This is like deref coercion. But in general terms, I do like C++'s approach to operator overloading a lot: every operator expression is re-interpreted as a function call. That means from the existence of the `a[0] = 3` syntax, we can _deduce_ the existence of references `T&amp;` that transparently forward to their referent (as opposed to pointers which require syntactically explicit dereferencing). That is, we haven't invented references, but discovered them already in the existing language. In C++, you can also return a value from `operator[]`, which is currently impossible in Rust.
I think that from a perspective of someone that learned a few common languages already, Rust can be quite refreshing and modern while still maintaining a huge focus on **performance** and **memory management** (freeing is handled automatically). It's modern because most of its features combine together, you can have code written in a functional style composed with imperative code, or even shifting to an OOP style for objects that need this (like handles, resizable arrays and general containers). The best parts about Rust are the **trait** and **type system**: they work well together allowing for a good generics system, expressing intention and preventing a lot of errors that are caught at compile time, so much so you often hear from Rust users the catchphrase: *"If it compiles, it runs"*. This is also because of the unique feature the language offers: the **borrow checker**. This is an extension to the type system that provides "lifetimes" bound to each object reference, which allows guarantees about the **impossibility** of dangling pointers and invalid memory locations in **safe** code, while also allowing for a very simple model of threading, since **no data race**s can happen due to the strict aliasing. I personally enjoy it because it's a language providing high performance and as much control as you need when opting into `unsafe`, while also providing a lot of modernity, zero cost abstractions, a good package manager, easy interoperability with C libraries and some strong guarantees that your code is correct on the details you don't want to care about all the time. If you have more specific questions, ask more, I surely may have missed something that you might consider useful in a language!
It's only magic insofar as `operator[]` and `operator[] const` in C++ are magic too. But because in Rust overload resolution is in general much, much simpler than in C++, it's something of a special case.
Thank you so much for detailed reply with examples! I have a bad(?) habit of only using standard library for functions that needs speed, but I guess it‚Äôs a time for me to learn to use 3rd party. Again thank you.
Yes, I'm not speaking about the underlying rules. In C++ world, when someone asks a question like that, usually one would be pointed to the reference and its endless rules. But personally I like that. For example, I always go straight to the best-effort reference docs when I have a question in Rust, and I find that it's much clearer than reading about something in TRPL.
That the Golang team decided to deliberately randomize the iteration order of the maps, is a good example of how far you have to go to leave yourself the flexibility to change something. Also this https://www.imperialviolet.org/2016/05/16/agility.html : &gt; This is about as simple as an extensibility mechanism could be, and yet lots of implementations get it wrong. It's a *common* mistake for implementations to return an error when the client offers a version number that they don't understand. This, of course, means that deploying new versions doesn't work. But it's insidious because the server will work fine until someone *tries* to deploy a new version. We thought that we had flexibility in the protocol but it turned out that bugs in code had rusted it in place. At this point it's worth recalling the Law of the Internet: *blame attaches to the last thing that changed.*
Yeah, I guess that‚Äôs something you can afford when you have a thousand-page specification for your language. I‚Äôm something of a C++ rules lawyer myself, but it‚Äôs just that Rust is a comparatively young language with no formal specification one could cite, as of now.
Rust actually had bits of its standard library replaced with external libraries because said external libraries performed better than the code that was previously in the standard library.
Thanks for your fast response! Also, I should have known better and have filed the issue. I'm not sure that mockup should be considered a "full redesign", as it only changes the color pallete of specific pages, but I get the decision. In some way, we need to finish things first before improving them. The thing is: the color changes are pretty harsh on the eyes, specially where it mixes white with other, darker colors. Also, all those different colors makes searching for stuff really confusing, but unfortunately after you open the page for the 20th time trying to understand why, you're too familiar with it for it to happen.
You could use a `build.rs` file and at compile time parse the JSON and emit Rust code with efficient data tables. I do something similar with [unicode-linebreak](https://github.com/axelf4/unicode-linebreak) which parses LineBreak.txt.
It's in the Rust reference: https://doc.rust-lang.org/reference/expressions/array-expr.html#array-and-slice-indexing-expressions
Right, I've been trying this but have run into problems. First of all the color\_type was always coming back as RGBA and the raw data was coming back as a massive list of R,G,B and A values. I started to question the format of my file but was certain it was an indexed file. So I tried searching through the code in the crate for references to palettes. To cut a story short - I found this: pub fn new(r: R) -&gt; Decoder&lt;R&gt; { Decoder::new_with_limits(r, Limits::default()) } pub fn new_with_limits(r: R, l: Limits) -&gt; Decoder&lt;R&gt; { Decoder { r: r, transform: ::Transformations::EXPAND | ::Transformations::SCALE_16 | ::Transformations::STRIP_16, limits: l, } } and /// Expand paletted images to RGB; expand grayscale images of /// less than 8-bit depth to 8-bit depth; and expand tRNS chunks /// to alpha channels. const EXPAND = 0x0010; // read only */ So, the transformation flags are causing the palette indexes to be expanded out to RGBA values. Now I've tried to create a Decoder without using new but apparently `transform` is private. I had a play with 'extension traits' which was an interesting learning experience but still had the issue with private properties. Any suggestions on what I can do next. In something like C++ I would expect to be able to extend the class and make my own constructor, but I'm not sure how to approach that in Rust.
&gt;Just not wholesale "let's kick over the implementation people have worked on for 6 months in a rush" replacements. Even though it was fine to do that with the replacement in the first place, internationalization be damned.
Ha when I started, you had no answers. By the time I submitted it, there were 5
I'm kind of confused, how is this different from just using a Dockerfile? Not to be a dick, it's just there seems to be a lot of overlap and I don't see a comparison of features anywhere (sorry if I missed it). The DAG solver is kind of neat but you can get much the same thing from Docker using multistage builds and setting DOCKER\_BUILDKIT=1 so it uses the new buildkit backend.
I think the whole "it takes time, plus look its all different people, too late!" is the one of the most cliched way to avoid transparency. However, your background on internationalization is super valuable context that I appreciate you sharing here. Also, I don't think this addresses the fact that this IS a divisive issue. I've seen people jump down newbies throats because they'll ask an innocent question about the website and defensive folk take it as a provocation. It's been what 7 MONTHS and this is still an ongoing topic and issue? Might I suggest that maybe prioritizing any sort of publicly published retrospective might help put this issue to rest, finally?
I just started learning Rust by implementing a Chip-8 interpreter/emulator and I was wondering about a few things. Should I be adding typing to every variable I'm declaring? Or is leaning on the inference better? It clearly gets to be verbose a little too quickly so I assume the standard is add types on more complex variables but leave simpler ones be. Is this a safe assumption? Also, in regards to implementing a screen of pixels for the CHIP-8 (pixels are either black or white, aka on/off), there seems to be many people implementing each pixel as a u8 by default rather than a boolean. I read that they take up the same amount of space due to everything being 1-byte aligned, so would it be better design in Rust to have each pixel just be u8? My brain is telling me booleans are safer but also there must be a reason why many people prefer the u8 approach. I assume this is due to some computations later being easier due to XOR or something but any advice would be great! Thanks yall.
I'm with you. The homepage makes me instra-cringe, just from first glance: https://i.imgur.com/nMUiO8f.png
That is actually something I consider a important good practice: if you return a collection of something in an API, and don't want to guarantee the order, always shuffle it.
&gt; In C++, you can also return a value from operator[], which is currently impossible in Rust. That makes sense. It's a difficult line to tow. There are plenty of times I wish C++ and Rust had more Haskell like syntax to allow me to arbitrarily name operators and functions (ofc that would muddy all the other necessary syntax). In general, I definitely prefer just calling a named function, though, for clarity (in both Rust and C++). Thanks!
Android ditched the Dalvik VM in favor of AOT-compiled binaries (compiled on the user's device during install) on Marshmallow (5.0). Though it still compiles from the intermediary bytecode, and all apps run on top of a fat runtime (ART), which provides garbage collection and dynamic linking.
Think you meant ing.await. :)
Android went back to a mix of JIT and AOT compilation in the newer versions of ART.
Is there any overlap between this and nphysics?
Oh my god, I could cry, I'm not the only person who came here to talk Nix. Yes, Nix would be a better fit for this. And Nix (on Linux, at least) does sandbox builds.
For 6, I would look at using WASM instead of IR code. Rust can compile directly to WASM and you should be able to run WASM mostly anywhere. I don't know the specifics though.
&gt;I've switched from Haskell to Rust for most of my side projects now (and it feels great!), though I still reach for Haskell for quick-and-dirty experiments. Mind if I ask how they compare? Haskell and rust are two of my favourite languages, but I often have trouble deciding between the raw control and linear types of Rust, and the syntax purity, and other type system features of Haskell. If there was a language that had the benefits of both, I would probably fall in love instantly. (And say what you want of Rust's build times, but Haskell's seem even longer and cabal and stack just make me want to go back to cargo.)
sure man
or just emit a raw binary representation of the json by using serde. json in messagepack out for example in build.rs, then load the messagepack at runtime.
The semantics of C are defined on a "C abstract machine", so one could argue that there is a VM in there, even though it doesn't exist in the same way as the JVM.
Wow, the \`toast.yml\` file has a very clear syntax. Well done!
Toast seems to be talking to docker by [shelling out to it][shellout] rather than using the API. That means you could try using [podman][podman] with something like `alias docker=podman` when you invoke Toast. It's probably a bit of an overkill, but Podman runs with no root or daemon and its CLI is pretty much identical to Docker. They are still separate programs, so the podman's images wouldn't show in docker and vice versa (but you can easily migrate them too). Like I said, probably not worth the trouble, but it is a possibility. [shellout]: https://github.com/stepchowfun/toast/blob/3173344367f2b266f2374089b0080aaca9e81a78/src/docker.rs#L545-L552 [podman]: https://podman.io/
Messagepack still serialized the keys, though. Is there any serde format that doesn‚Äôt?
Look around this sub. Does it look like it‚Äôs related to the Rust game?
The [wasmer](https://wasmer.io) project provides a runtime for WASM outside browsers on common platforms. It's written in Rust.
&gt;I would appreciate your comments on why you think this behavior is the "right" behavior Not the parent, but I think it's the right behavior because it allows things like: let mut x = foo(1); if ... { x = foo(2); } or: let mut v = vec![]; v.push(foo(1)); if ... { v.push(foo(2)); } If my understanding is correct, neither would be possible with what you propose. More importantly, the question is **why** one would want to have each call to `foo()` return a different type? If it somehow allowed `foo()` to return differently-sized trait implementations in different invocations, then that might at least be useful (e.g. it might allow returning a different closure in two branches of the same function returning `impl Fn...`). But you don't seem to be talking about that.
you could set it up to not do so. https://serde.rs/field-attrs.html#serdeskip
Soooo, hm! My understanding is that each toastfile's inter-depentent tasks (props for naming it `toast.yml`, despite the tempting name) build up to one image (with cache-able images in between). To support multi-stage builds *and* the tagging that I would like, I think there's about 1-2 config keys you could add to tasks: * `tag`: a tag name to give the resulting image * `import_paths`: a map of tags (identifying the tagged image, from above; or maybe the build step?) and file names to copy into the current container. The topological sort will have to ensure that any task with an `import_path` from a previous `tag`ged task comes after that task, but I feel that would not be too difficult to arrange (if someone wanted to parallelize the builds from that, I'm pretty sure that would work too!) I feel this would be even better than the Dockerfile multi-stage format, as their format has you tag the container on the `FROM` line, which ... I find kinda messy; much better to tag the *build stage* where you're copying from directly, by the actual container image, not just the end result.
(and by "you could add" up there I probably mean "I could add" because I'm almost volunteering for crafting PR one of these days, amn't I)
You want /r/playrust, I‚Äôm pretty sure.
[mp4 link](https://preview.redd.it/f2e0v200mm031.gif?format=mp4&amp;s=cc0e8bbbcf8cbb573ecd6e216690c1c12a8157e0) --- This mp4 version is 94.33% smaller than the gif (773.03 KB vs 13.32 MB). --- *Beep, I'm a bot.* [FAQ](https://np.reddit.com/r/anti_gif_bot/wiki/index) | [author](https://np.reddit.com/message/compose?to=MrWasdennnoch) | [source](https://github.com/wasdennnoch/reddit-anti-gif-bot) | v1.1.2
https://github.com/Gymmasssorla/finshir
GitHub: [https://github.com/Gymmasssorla/finshir](https://github.com/Gymmasssorla/finshir)
It's just a common saying so I doubt it's been codified. It's probably one of those 'it will be a thing in 5-10 years but not yet' things.
Why not a proc-macro?
New to rust. Trying to break code of binary project into separate files. Compiler complaint: "error: main function not found". &amp;#x200B; rustc 1.18.0 &amp;#x200B; Minimal case, or close to it... [**main.rs**](https://main.rs)**:** mod my\_mod; &amp;#x200B; fn main() { println!("Hello, world!"); } &amp;#x200B; **my\_mod.rs:** pub struct MyStruct { my\_str: String, } &amp;#x200B; impl MyStruct { pub fn say(&amp;self) { println!("{}", self.my\_str); } } It builds if the "impl section is not included in my\_mod.rs.
What the Ferris are you talking about? Google has basically one result, this HN thread: https://news.ycombinator.com/item?id=5476643
Looks very cool. As I always ask: what font are you using?
I am always asked about this question :) I use [terminalizer](https://github.com/faressoft/terminalizer) to generate beautiful GIFs.
I'd say the primary advantage is it seems to be a less leaky abstraction layer over building Dockerfiles. I'd be happier to put a \`toast.yml\` file in front of colleagues who aren't Docker fans.
Is it possible to replace Docker with Vagga?
It could be better if support put container running after the task done just like docker compose do.
Foo, bar, and baz are metasyntactic variables. The canonical real example is the Unreasonable Effectivness of Mathematics, but there are a bajillion others.
A few things I miss from Haskell when using Rust: * Being able to define custom monads to describe my application-specific side effects. With Rust, I can never tell what side effects a function does. It feels like writing everything in the `IO` monad, which to me is a huge loss. I have mixed feelings when I see ad hoc syntax creeping into Rust (the `?` operator, `async`/`await`, etc.) for things that are all unified under the general rubric of monads in Haskell. But I read that long Twitter thread and can sort of appreciate why that doesn't make sense for Rust. * Not having to think about ownership and lifetimes for code that doesn't need it. I do appreciate that Rust reifies these concepts in the types, which makes me feel great about knowing my resources are cleaned up properly and then never accessed again (that's one of the reasons I used Rust for Toast, after all!). But for most projects I tend to write mostly pure code, and for that, Rust's type system seems to just add overhead and complexity. For example, Rust has 4 kinds of functions (top-level functions and the three flavors of closures), whereas Haskell only has one. I know there are good reasons for those distinctions in Rust of course. * General functional programming is somewhat clumsy in Rust. [Here](https://github.com/stepchowfun/toast/blob/663bcfddcecf7d58da40e54306be0fa914a0cbdb/src/failure.rs#L38-L46)'s a small example from the Toast codebase where I have to explicitly curry a function so I can partially apply it for use in a higher-order fashion. There's a lot of noise in there which makes the code hard to read (for me, at least). As another example, in Haskell, when I map over a list, I get back another list. But in Rust when I want to map over a `Vec`, I first have to convert it into an `Iter`, then mapping gives me a `std::iter::Map`, which I then have to `collect` back into a `Vec`. I am guessing this enables Rust to monomorphize and inline things so it can be lowered into an efficient loop with no unnecessary allocations, which of course I appreciate. A few things I miss from Rust when using Haskell: * Cargo. Cargo is awesome! The Haskell tooling is not so streamlined. There's Cabal, Stack, Hackage, Stackage, etc. It's confusing and fragmented. Stack's `package.yml` is much less straightforward compared to `Cargo.toml`. * Ownership and lifetimes! Despite feeling like they sometimes get in the way, I really do love being able to encode these concepts in types when I'm dealing with resources that need to be cleaned up or access that needs to be controlled. * General polish. I feel like the Rust ecosystem really cares about quality. For such a young language, there are so many high quality libraries! And the documentation is excellent. All of the hard work the Rust community has put into this language really shows. In the end, Rust and Haskell are two excellent languages. I don't strictly prefer one over the other‚Äîfor me, it really depends on the job. I'm very grateful both exist!
Good article, other than the weak attempt at throwing shade at Matrix. Weird flex given where the path to tox-rs started, but ooookay.
I wasn't worried about this phenomena until you mentioned GnuPG as an example.
You should be able to read a file, but probably not using `std::fs`, if that's what you were trying? WebAssembly has almost no capability for interacting with anything, but JavaScript does. Frameworks like yew, or stdweb, etc. do most things by bundling JS code to do the interactions, then calling into that. So if you're reading a file using yew's APIs, I would expect there to be some JS related to file reading in the output. Your second paragraph confuses me a bit though. What exactly are you expecting to see in the JS? The majority of your rust code should be compiled to a WASM file, packaged alongside the JS file. Only the glue code should end up in JS. Are you not seeing any relevant glue in the JS, or were you expecting to see a direct translation of your rust code as well? (which won't likely be there, since it's in the wasm file).
Respectfully, as an outsider, looking at the various winapi related issues on `mio`, it just looks like no one cares. There's no roadmap, no list of blockers, it's not actually clear what needs to be done beyond removing winapi from the API and revving mio's API. Apparently so bad that the Servo team has decided to fork, and there are exasperated issues talking about this being an issue that are themselves already 14 months old. What's the path forward for mio or mio users? Seems like not having an answer to that question is more frustrating that just that some dep is not up to date.
Beautiful! Will be giving this some POC time.
Came here with a grimace. Fun thread, a lot more interesting than the usual "lol Rust slow" posts. :)
Seems like a great way to have more boilerplate to copy around and not actually learn Cargo. \~\~"for JavaScripters" indeed.\~\~
Proc macros are much more of a hassle than a `build.rs`...
I'm curious how you installed rust, since the current version of rustc is 1.35. I tried your code and it worked fine. What are you running to compile?
Yea I remember when [Rust 1.0 was released](https://blog.rust-lang.org/2015/05/15/Rust-1.0.html)!
I honestly remember when bps were around, idk why but the loading music when you log makes me nervous, like i dont know if i got raided or i didnt, gives me chills
You're on the wrong sub
&gt; Should I be adding typing to every variable I'm declaring? Assuming you're talking about `let` statements, my preference (which seems to track with the general populace) is to never annotate unless the compiler can't figure it out itself. &gt; ...would it be better design in Rust to have each pixel just be u8? Between the two (`bool` and `u8`,) I'd go for `bool` - they take up the same amount of space, but the `u8` has 254 values that make no sense. If you want to be fancy, you could do: ``` #[repr(u8)] enum Pixel { On, Off, } ``` That'd encode your intent better with no overhead. I'm not sure why people would use `u8`, except if they were using `&amp;[u8]` to represent pixel slices (with each pixel being one bit.)
To add to what u/aLiamInvader said, remote caching is one of the reasons I built Toast. Without Toast, you have three choices for your CI environment (depending on how you use Docker): 1. Rebuild your entire Docker image for every build (slow) 2. Rebuild your Docker image for every build, and use your CI system's Docker image cache to speed it up (good, but your CI system might not support this) 3. Remember to manually build and upload your Docker image every time you change your Dockerfile, and do your build/tests with `docker run` (error-prone, because you can forget to do it) For me, (1) is not an option because some of my Docker images take almost an hour to build. Many CI providers, like Travis, don't support (2). (3) is what I used to do, but I don't trust myself to remember to do it. For me, Toast is the best of all these worlds. It uses a Docker registry as a remote cache to only rebuild what is necessary, and it can be installed on any CI system. It also gives me a nice way to declare tasks, even for things you wouldn't normally put in a Dockerfile, like running a development server. When you use vanilla Docker as your development environment, your development process is split into two phases: 1. `docker build` ‚Äî for installing dependencies, maybe also building your application, and maybe linting and running tests 2. `docker run` ‚Äî for running your application and doing one-off tasks With Toast, all these things are just tasks. You don't have to switch between `docker build` and `docker run` for getting work done. Just jump into your project and run `toast` to get going. So if I had to summarize "why Toast": 1) remote caching, and 2) eliminating the awkward two-phase dance that you do with vanilla Docker.
Not that that is a bad place place to ask, but you may be interested to know about the subreddit r/learnrust
I found [this issue](https://github.com/rust-lang/rust/issues/60413) about it. I suppose it's a matter of what the compiler is willing to infer.
Thank you.
So there is a way you can control this but I'm not super happy with this kind of idiom because it's so difficult to discover from documentation alone. Simply put, if you import `png::HasParameters` you can call `decoder.set(...)` with the `Transformations` instance with the flags you want; I'd use the same construction as you found there but omit `Transformations::EXPAND` which should give you a `Decoder` that yields pallette bytes.
Thank you! That about covers my questions. I think I'll follow your let styling, as well as run with boolean. Rust has been fun to learn and a bit tough so I'm glad I'm running in the right directions.
Looks like there are other comments that have already satisfactorily answered this, but they've missed some little bits. Here's the answers I'd give: 1. Researching "go package syntax" tells me that it says what "package" the file belongs to. I don't know Go, but from what I can tell, it's some kind of built-in unit of dependency. Rust doesn't have this kind of specifier; it forces you to separate different "crates" (packages) into different directories. The crates and dependency management in Rust is handled through Cargo, and the details for each crate is stored in "Cargo.toml" in its project root, including its name. 2. This is actually something I can relate to; I don't like using dependencies that implement things for me because: a. I feel like I can write and use a module that is better by my standards, and b. as a student, I like to learn all of the little bits of protocols and then implement them; that has been fun for me in my early years of programming. When writing production-level code instead of my own little helpers, I've had to realize that the community-driven efforts in those crates have gone through a lot more testing and rigor than whatever implementation I could make, and they have done their best to make the simplest API and most efficient implementation. If I have a serious disagreement or concern with the fundamental design, I should probably bring it up with those communities directly. That way, if I'm wrong, I learn something, and if I'm right, everyone else can benefit from it. Specifically regarding the example in the Rust book, that is far from ready for production. Implementing every single route by extending upon that basic design would lead to a lot of duplicates and a lot of inefficiencies, and if you spend the time learning and implementing every little bit of the protocol, you run into the scenario I've given you above. My advice: Build upon the foundation that the community has already given you; use a well-established framework like actix-web for production, but feel free to experiment with alternatives in your free time, and publish it if you find something new! 3. Yes, there are actually two different stories from two periods of time. Early Rust, before my time, did have a runtime and GC similar to Python, where it had an explicit syntax for garbage-collected references (something like `~T`). That has now been abstracted away from being a compiler intrinsic into APIs in the standard library, since there are some target platforms where such an expensive runtime simply isn't feasible (like embedded systems). In modern Rust, you can opt-in to using heap allocation with structures like `Box` and `Vec` and you can use a simple form of garbage collection with the reference-counting smart pointer `Rc`. Some more advanced garbage-collection environments might be available as third-party crates, and you can even make your own. "Smart pointers" (types encapsulating raw pointers that implement `Deref` and optionally `DerefMut`) are a very common pattern to make your own custom ownership model. 4. Rust is definitely capable of real-time programming; there is a group that is working on real-time audio specifically, and it is something I am also learning and researching independently as one of my current interests. There currently is no standard API for interacting with OSes to arrange for special real-time privileges, however, most things will still work just fine with what we have. The most important part is that your threads will not block if you don't call any expensive or otherwise blocking operations (allocations, locks, blocking I/O), and that definitely is possible. Rust is low-level enough that it will do exactly what you tell it to and nothing else is going on behind the scenes, which is great. 5. Yes, Rust does have an IR, though I don't really know much about this. 6. I don't really know if there is a public API compile from IR to native, though I'd prefer just shipping the source code honestly. Rust has facilities for metaprogramming and conditional compilation within its syntax that are often used to compile different code for different platforms, and those might get lost by the time an IR is generated, in which case I'd strongly discourage distributing IR instead of source code. There are IR interpreters out there - for example, miri is a MIR Interpreter that focuses on detecting undefined behavior - though I'm not aware of any that focus on performance.
&gt; "You don't seem to go much into what you like about Rust, just what you don't like about other languages" Scala is totally un-opinionated (you can program either functionally or object orientedly or imperatively and the language doesn't stop you) while Rust is opinionated. Examples: 1. In Scala it is equally easy to make a reference const (using "val foo = ...") or mutable (using "var foo = ..."). If I want to force people to always use val, or to not use far in a particular part of the codebase, I have to configure using "var" to be a compile error in the static analysis tool. See: https://www.wartremover.org/doc/warts.html . In Rust, const is the default ("let foo = ...") and people have to do extra typing to make a reference mutable ("let mut foo = ..."). Some programmers even feel "dirty" when they have to type that extra "mut" in the variable initialization, and I like that they feel this way because it discourages them from doing it. The language kind of makes them program a certain way where as in Scala the "architect" or "senior engineer" has to configure all the compiler flags, static analysis warning, style enforcement, configuration, etc so as to enforce things to be a certain way. Programming in Scala feels kind of like programming in C++, but with functional programming. 2. In Scala, you are never supposed to use null (use Option/Optional Monad instead), but the language doesn't force this. In order to enforce this, I have to configure it to throw an error at compile time when people write "val foo = null". I believe the Rust language is naturally opinionated, so people don't have to consciously choose against using "null". 3. Backend Scala is supposed to be written in an immutable, functional programming style, but the language doesn't enforce this. Instead, the static analysis tool enforces this style. I never even do object oriented programming or use object oriented inheritance hierarchies in Scala, but the language doesn't stop anyone from programming that way unless the "architect" or "principal engineer" configures the static analysis tool to make people program that way. Rust just doesn't have all the object oriented programming functionality that Java does, so programming Java in Rust isn't even an option, where as programming Java in Scala very much is an option. Basically, Scala is a totally un-opinionated mix of OOP and FP, but in general you are not supposed to use the OOP features in your backend code, so you have to configure things to enforce this. Rust is opinionated towards functional-imperative programming. Also, Scala doesn't have backwards compatibility between versions until after the release of version 3.0, which isn't scheduled until around year 2021. Every time a new Scala version comes out, there is a possibility of a breakage in backwards compatibility, requiring code changes and even sometimes the replacement of entire dependencies that were not updated, re-packaged, and re-published using the latest version of the compiler. This makes upgrading Scala versions hell sometimes, especially if you have deprecation or deprecated dependencies in your codebase. It also sucks for library authors such as myself who have to make code changes and re-publish libraries and jar files every time a new version of the Scala compiler comes out. In a Scala codebase, you kind of need an "architect" or "principal engineer" to kind of stay on top of things or else the codebase turns to hell. I am a Scala aficionado, so I basically have (or at least had) to be this person. I basically end up having to guide and direct programmers twice my age, and if I don't they end up stuck in a hellish codebase and a hellish build with dependency hell. Basically, I was looking for Scala, but opinionated and with all the object oriented programming taken out. GoLang isn't even close to that.
You're right, the original WebAssembly spec doesn't provide that kind of thing. The generic WASM target `wasm32-unknown-unknown` does not have any of those capabilities, as most of the standard library is an abstract API built on top of features provided by OSes which the current WASM core does not have. The most common WASM target that implements some features of `std` is `wasm32-unknown-emscripten`, which uses some JS glue code to call browser APIs. I'm not familiar with how exactly this will translate into opening a file, but I'm doubtful it will just let you modify arbitrary files on the host system, as browsers nowadays sandbox their applications and explicitly require user approval for a lot of things (camera, location, etc). There is a new standard called WASI that will define a "system interface" for WebAssembly, which may let you access the host file system depending on the context. If you're running using wasmer then that kind of access is probably going to be allowed, but it will still be restricted when running in a browser.
r/playrust
That's pretty awesome! I saw your note about binary releases in the README, might I convince you to publish to crates.io instead? That's the main way that I download and install other Rust programs currently (using `cargo install`).
I can't wait to try out Toast! As for your experiences going from Haskell to Rust, I've felt *many* of the same things you did, especially concerning do notation. Real briefly, I'll try to address one of the things you brought up: &gt;General functional programming is somewhat clumsy in Rust. [Here](https://github.com/stepchowfun/toast/blob/98f8314a659063b382256581a5df56f6bf9080e3/src/failure.rs#L38-L44)'s a small example from the Toast codebase where I have to explicitly curry a function so I can partially apply it for use in a higher-order fashion. The linked example *could* be represented using several distinct error structs representing each particular failure mode, embedded within a top-level enum exhaustively capturing *all* possible failure modes, with the specific failures being accessed using [Error::downcast\_ref](https://doc.rust-lang.org/std/error/trait.Error.html#method.downcast_ref). I'm not sure you *need* currying here when a runtime type cast would do üòú.
Thanks for the feedback!
Is `DbExecutor` constructed using `actix_web` or `diesel`? If it is diesel, does it support the application of `cassandra` (`nosql`)? Although I spent some time reading the documentation of the diesel, currently do not see the related note ....
Neither. It is a user defined actor. In the linked example you can see where the DbExecutor is defined on line 13, and Actor is implemented for it on line 25. The actor is just a place for your to hold state. In the diesel example the state is Pool&lt;ConnectionManager&lt;SqliteConnection&gt;&gt;, but in your case you will want it to be a cassandra connection pool instead. The DbExecutor isn't anything provided by either framework but just an actor model design pattern. Does this make sense? Also it's worth asking, how strong is your knowledge on actor model? If you don't have a good working knowledge of it, it's worth getting it. It is valuable when working with actix-web
[removed]
I think you want r/playrust r/rust is about the rust programing language
FYI, your screenshot doesn't show the correct fonts.
I guess there isn't any interest in a redesign of a website entirely dedicated to copying a pasting a curl | sh.
&gt; my preference (which seems to track with the general populace) is to never annotate unless the compiler can't figure it out itself The main exception to this in my mind is when you're dealing with very flexible conversion functions. The big daddy of them all is the unsafe `mem::transmute`, where it could potentially be a safety disaster to infer the wrong type somehow, and it's really important to know exactly what you're getting there. But I find that even with `.into()` or `.parse()` it leads to less confusing errors down the road if I go ahead and specify the type right there.
Here's [where we were talking about it](https://www.reddit.com/r/rust/comments/bqr1xh/hey_rustaceans_got_an_easy_question_ask_here/eohu4ju/) :)
Have a look at this as well https://github.com/capnproto/capnproto-rust
I am not familiar with the use of actors, but it seems to be a good time to learn. However, it seems that there is no cassandra to choose to install in diesel features (I am using cassandra-sys-rs). How is this used?or can be replaced with sql?
You are right. Diesel doesn't support Cassandra. I'd only pointed out that code snippet because it shows a way to handle connection pooling/holding state across requests in actix. You'll have to completely replace all the diesel code with your cassandra-sys-rs queries and connection pooling. Unfortunately I'm not familiar with that library so I can't help there.
The plot thickens! Very nice.
You need `RngCore::fill_bytes`.
Essentially, the trait should end up looking something like: trait AlgebraicField: Add&lt;Output=Self&gt; + Mul&lt;Output=Self&gt; { } But if someone already has one in a crate like nalgebra then use that instead since it's more fleshed out already.
If you are willing to take on the work to a) understand the problem, context, paths forwards, and tradeoffs and b) write up a document that can be referenced whenever this comes up, then please open an issue on the mio repository and we can discuss there. Otherwise, the answer has always been the same (and easily findable via a quick search in the mio repo): it will be bumped w/ a Mio 0.7 release which will happen shortly after the next major version of futures is stable. tl;dr: if mio were to release an 0.7 w/ a bump to the winapi project, it will worsen the situation drastically.
This kind of discussion makes me wonder if there's any plans to support something like: let a = &amp;mut foo[x]; let b = &amp;mut foo[y]; if you can prove that `x != y`. This logically should still work out, but the compiler obviously isn't smart enough to realize that even if you have a `if x != y { ... }` wrapped around it. Idris I think would be smart enough to figure this out though.
Depends on what you mean by "simplified" then, I suppose.
I have wanted to create a site that allows me to get across why I'm spending my nights and weekends writing this book for a long time. Rust empowered me to understand how computers work. The Rust community empowered me when I was savaged by [depression and burn out last year](https://www.reddit.com/r/rust/comments/9jv49b/progress_report_for_rust_in_action_early_access/?utm_source=share&amp;utm_medium=web2x). But, you know, I had the book to write! I've finally gotten around to writing a bare bones site. I'll iterate on it over time. It might eventually become a blog where I expand some of the examples from the book into more fully fledged projects. Thank you for all of the positive feedback as the book has developed. And thanks for sticking with it. It's going to be great. p.s. sorry about the lack of HTTPS. I got sick of trying to fight GitLab Pages on this one. If anyone can help me out, please [add suggestions as a ticket on the site's repo](https://gitlab.com/timClicks/rustinaction.com).
Is there any (plan for) integration with [evcxr](https://github.com/google/evcxr/blob/master/evcxr_jupyter/README.md)?
That's just for recording though, isn't it? If so please share the font, also interested. :)
That's great news, and the API looks very approachable. I've been waiting for something like this. One of the blockers for migrating a lot of C++ legacy code to Rust at work. I assume a cairo backend would be very straight-forward to implement. That would provide us a good plotting lib for GTK-based Rust programs.
Sorry to hear about the hard time you had last year. If it's worth anything, I came across this book when you commented on a post (last week I think) and I have to say I'm very excited to see the full version. I never came from a language where I had to think about memory management (to a certain degree) and it's been great coming across resources that fill in the gaps specifically with Rust. All the best with the rest of the book and of course look after your health!
And make sure to emit the proper `cargo:rerun-if-changed` directives to avoid rebuilding your tables due to unrelated changes. [Relevant reference](https://doc.rust-lang.org/cargo/reference/build-scripts.html#outputs-of-the-build-script)
So whether you get the "main function not found" error depends on whether the `impl` is present or not? That sounds strange. Can you give more details about the layout of your source (did you use `cargo new`?), the `Cargo.toml` and the whole `cargo build` output?
Thanks for the kind words! The final book's going to be great. And, I will look after my health. The team at Manning have been very supportive.
We already have the conceptually similar `slice::split_at_mut` implemented with a dash of `unsafe`, so you could do likewise.
It's usually just a question of the threshold for "magic". Some people consider even the most basic things "magic", maybe because they were once surprised or challenged by it. For me, "magic" would be compiler behavior that isn't following clear rules, but applies "do-what-I-mean" semantics, or rules that "randomly" apply. For a computer program, that's quite uncommon, though it can seem that way given sufficiently complex algorithms. I.e., auto-deref with the `.` operator can seem quite magical.
The projects list sounds amazing!
This feels very similar to my tool [lorikeet](https://github.com/cetra3/lorikeet), but appears to be geared towards containers. Great work!
I assume there are requirement that say no server, no javascript should be needed and each object should have its own URL. One file per namespace and just using anchors for each object could perhaps work though
It would've been a little clearer with X, Y, Z. Foo, bar, baz sound a little too much like real words.
Ha and it's not finished yet. I am finishing off the networking chapter this week and the chapter's major example will be implementing the slowloris attack with TCP primitives
I don't think it's a common saying at all...
This is really nice. Looking forward to using it. The motivation paragraph kind of shoots itself in the foot, since the actual plotting work can still happen in a higher level language even if you use rust to do the data crunching/down sampling. Honestly "why a plotting library" is a terrible question, a better one is "why not". /rant
Easily findable, eh? Let's search the \`mio\` repo for \`winapi\`: * 3 open issues * [https://github.com/tokio-rs/mio/issues/658](https://github.com/tokio-rs/mio/issues/658) does not contain any update for over a year, no indication of mio .7 requiring a stable new futures release * [https://github.com/tokio-rs/mio/issues/912](https://github.com/tokio-rs/mio/issues/912) not relevant * [https://github.com/tokio-rs/mio/issues/953](https://github.com/tokio-rs/mio/issues/953) too new, no explanation of why the current situation is the way it is * and then I found these PRs from those issues: * [https://github.com/carllerche/iovec/issues/16](https://github.com/carllerche/iovec/issues/16) no update from you for 10 months, again no mention of what it's blocked on, just that other surrounding deps are at the same version So, while this may be obvious to you, I'd like to ask you to consider that maybe I'm not a troll, or a jerk, or even a \`mio\` user, but maybe just someone who empathizes with people who are desperately dependent on something that they have no visibility into. It's been a year, maybe a reply on GH would be more valuable than another reply to me?
Oh it was intentional. I'm not fucking turning on JavaScript for that page. It's not my fault no one wants to properly test their shit these days.
I've been following the web platform diligently for over fifteen years. You're the first I've seen call a web browser an implementation of the web.
`fill_bytes` works if I just needed a random byte array. But I also need the random byte array to be within given bounds.
[hdrldr](https://crates.io/crates/hdrldr) - rustified version of [HDR image loader code by Igor Kravtchenko](http://flipcode.com/archives/HDR_Image_Reader.shtml). I was trying to keep the code simple while bringing it to the safe side of Rust,
If you want to treat the byte arrays essentially as big integers, this might help: [num-bigint](https://docs.rs/num-bigint/0.2.2/num_bigint/) ```rust let low = -10000.to_bigint().unwrap(); let high = 10000.to_bigint().unwrap(); let b = rng.gen_bigint_range(&amp;low, &amp;high); ``` there are also methods to convert from and to byte arrays (big endian): * [from_bytes_be](https://docs.rs/num-bigint/0.2.2/num_bigint/struct.BigUint.html#method.from_bytes_be) * [to_bytes_be](https://docs.rs/num-bigint/0.2.2/num_bigint/struct.BigUint.html#method.to_bytes_be) You can also use unsigned BigUints
&gt; Cargo. Cargo is awesome! The Haskell tooling is not so streamlined. There's Cabal, Stack, Hackage, Stackage, etc. It's confusing and fragmented. Stack's package.yml is much less straightforward compared to Cargo.toml. Yes, Haskell was quite confusing in this regard, but `cabal new-build` changed a lot for the better. It IMHO feels now pretty close to a Cargo that even shares the build artifacts between different projects!
I just started working on an attribute macro for traits,that derives an ffi-safe trait object constructible from an pointer to a type that implements the trait. By ffi-safe I mean that it can be used in Rust-to-Rust ffi with different versions of the compiler. The pointer has to implement some traits beyond Deref/DerefMut. Context:This going to be part of the [abi\_stable](https://github.com/rodrimati1992/abi_stable_crates) crate.
[removed]
Which font? You can look at the project's settings at [`terminalizer.yml`](https://github.com/Gymmasssorla/finshir/blob/master/terminalizer.yml)
&gt; This is done by craeting sub drawing areas. Looks nice. I often end up wanting a friendly plotter in a program but rarely want to bother making one. This could be handy if I end up there in Rust.
Weird. I've heard like three people say it here in the northwest. By the downvotes I guess people disagree. &lt;shrug&gt; whatever.
should have said slang or something like that. I've heard a few people say it, but I guess I wouldn't say 'common'.
I'm working on an array sorting visualizer like you see in those cool youtube videos. It's coming along pretty nicely. I'm trying to make it as generic as possible where if you have something like sort(&amp;mut [T]) all you have to do to have it work with the visualizer is make a new one with signature sort(&amp;mut ArrayVis). Maybe in the future I'll start working on a way where you don't have to do the 2nd step at all
I see, tyvm. :)
Adding bits to my SDL2 high level crate so that I can get back to avoiding the real work of actually programming the game
Sounds interesting, I'll probably buy it when it's released. It says that you began working on it back in July 2017. Can we expect that the chapters are up to date and using the latest techniques once the book is released? I'm new to rust so I don't know how much the language has changed since then. But I do know that we'll get some major things like async await before the book will be released.
Interesting ideas! I think there are a few things to unpack. 1. When Toast runs a task that has a `tag`, does it only tag the image locally or does it push to a remote registry? Does it depend on whether remote caching is enabled? Is the behavior customizable from the toastfile? 2. Can tasks that don't have a `tag` depend on tasks that do? If so, how would that work? We can't create a container from multiple images, but it's nice to be able to create a task that only lists other tasks as dependencies and doesn't do any real work. 3. For tasks with a `tag`, how do you specify the base image? Does it use the top-level base image from the bakefile, or can you specify a different one? Maybe there should be an optional field that allows you to specify a different base image from the top-level one. 4. Should Toast squash all the layers (including layers from the base image) in the images? 5. How do you set other metadata that Dockerfiles support, like the entrypoint? I'm sure we can come up with good answers to these questions. You're welcome to submit a PR, but it may be productive for us to flesh out the design further before investing too much in the implementation.
At first glance, this thing seems fantastic! I'm gonna try it out.
I didn't quite understand. Can you elaborate?
&gt; There is no GC whatsoever in core Rust. There is no Tracing Garbage Collectors, like in languages like Java or Go, ... There are reference counted pointers in the standard library (like in modern C++), that are a form of garbage collection, but is is not a pausing garbage collection requiring a heavy runtime.
Yeah I was trying to use `std::fs` that must have been the issue. Yeah I was thinking it would have had something in the javascript regarding reading a file or use some of the variables I set out for the value of the content of the file. &amp;#x200B; Thank you for your help!! :)
WASI sounds interesting, I'm definitely going to check that out. &amp;#x200B; Thank you for you're help!!
That's a very fair question. My expectation is that the manuscript will be finished in 8-12 weeks. I'm close to finishing the networking chapter, which leaves 2 to do. As I have learned, nothing is guaranteed though. I have over 100k words that have been removed from the manuscript. Multiple implementations of several of the worked examples are sometimes required. The table of contents has been updated multiple times. Every chapter I submit to Manning goes through (at least) 3 editors before it is released to the public. Often multiple cycles. Will it be up-to-date when it's published? 100% Rust is a stable language, with a community that values stability. I have been deferring the networking chapter because of the async discussions. I've now decided to ignore that and come back to it during a second edition. My readers (and my family!) have been very patient with me, it's time to get them a book.
I'm definitely in a queue for buying your book when it is finished. Fingers crossed for everything working out well for you and the book!
I think the good common practice has become to have a flag with on/off/auto value and default to auto. Check if stdout is a tty and if the TERM environment allows colors, enable them.
I spent parts of the last couple of weekends collecting all the Rustaceans I could find that are accepting financial support and built this page. I hope that some folks get some new supporters out of it.
&gt; std is where code goes to die That's a bit extreme, no? There are editions that can release breaking changes
The constant breaks "Read more stories" make it really hard to read the article. I've read few paragraphs and couldn't continue. There is also problem with many names being mistyped. ("Charge" instead of "cargo", "craters" instead of "crates", etc.)
Thanks bro for highlighting. Same already corrected...
The site is almost impossible to read in a mobile browser. **Way** too many intrusive, unrelated elements and a ‚Äúread other articles‚Äù seemingly every paragraph. It also doesn‚Äôt help that there‚Äôs lots of buzzword repetition and while sections feel like they were generated by a poorly-tuned algorithm, but I might be able to get past all that if the article itself wasn‚Äôt split around so much clickbait.
Neither you nor the [README.md](https://README.md) explain what that is.
I have absolutely no idea how that set method does what it does, but that totally seems to work. Is this a common way to work in Rust? Is there some part of the Rust book I should read to help me understand what's going on behind the scenes? Thank you
&gt; There are editions that can release breaking changes Not really. Rust allows mix-and-matching editions in your dependency graph, so if one dependency uses Rust2015 and has `Vec&lt;T&gt;` on its API, this API can be called from another dependency using Rust 2018. The only way in which that can work is if the `Vec&lt;T&gt;` implementation used by both is exactly identical, that is, the same. New editions can error instead of warn on deprecated stuff, and can provide new stuff that older editions do not, but the source code of the standard library itself is the same. In fact, the standard library is just a normal Rust crate (`libstd`), so it needs to pick an edition and stick to it.
I didn't know you could publish binaries on crates.io! I'll do that in a bit.
Took me a bit to realize this wasn't a Rust port of http://www.toastball.net/toast/ :)
Would you say I should be able to learn Rust just from this book? I have a little C systems programming exp and tons of app programming.
Sorry for my bad English, my question is will toast support running a docker container in background?
The killer advantage that nix has to this use case is the ability to bring your own tooling to an environment. In this sense I'm talking about things that don't really affect the output of the build like linters, debuggers and editor integrations which are not easy to swap in and out if they have to integrate with the same tools as the build environment. Perhaps some of that is answered here by layering the tasks but it seems a bit more fragile. Main benefit of this tool for me would just be removing the requirement to figure out what directory I'm in for my docket volume mount, which is a huge pain for Go projects. Also hi pcj, sorry for serious posting
Been meaning to ask, what happens to all the GPU-based apps, including your library, when there is a game running in background, and games from what I understand love to take all GPU memory for themselves?
will update the readme. thanks!
The 2 last lines of your example are strange: root.close()?; return Ok(()); You only need to put `root.close()` at the end of your main, since it already returns a `Result&lt;(), Error&gt;`. BTW, a `return` in this position is not idiomatic.
&gt; Can anybody corroborate these statements? Yes. &gt; what makes rust so great? It sits in a very sweet spot if you care about performance as well as safety. While other languages tend to give you only one or the other, Rust gives you both. On top of that you have a great community, a rather open development process where you can participate and make a difference, a great package management and build tool, and a lot of smaller pleasent-to-use library and language features. It makes you feel empowered. You get to have a "more direct" access to the metal while the compiler/language/type system/library protect you from disastrous memory-safety and thread-safty errors.
But they do not need another build step. I guess that's a matter of preference.
You could use Asciinema. It makes even smaller files and one can still copy paste commands from it.
OP might have better luck in the WSL as well. But yeah just install Linux already. It's 2019.
You cannot turn reference into owned without cloning
Minor point of correction: `~T` was once equivalent to `Box&lt;T&gt;` in modern Rust (a unique heap-allocated pointer). The real garbage-collected sigil in ancient Rust was `@T` and its library equivalent `Gc&lt;T&gt;`.
&gt;p.s. sorry about the lack of HTTPS. I got sick of trying to fight GitLab Pages on this one. If anyone can help me out, please [add suggestions as a ticket on the site's repo](https://gitlab.com/timClicks/rustinaction.com). Might I suggest Netflify? You can host simple HTTPS-enabled websites for free on it, and you can just point it to the GitLab repo and set up the build command. I've had a pretty good experience with it. My only complaint is that the free tier CDN isn't great, so sites with lots of JS or embedded images load slowly. i would also keep an eye on [this issue](https://gitlab.com/gitlab-org/gitlab-ce/issues/28996) in case it gets added to GitLab proper.
If you are burning out, you are not doing proper "life management".... There is nothing in this existence that can make you work hard enough so you burn out unless you explicitly do it to yourself. Just relax and realize living life is a skill like any others, and you should work harder on that skill instead of other things that don't mean as much as your health and happiness.
I do not think so, as trait objects cannot implement Clone. Maybe if you know the original concrete type you could cast it into that and clone(), but you should probably consider a different method of whatever it is you're doing
This is a lot like asking whether you can turn a &amp;T into a T. As others have pointed out, you can if T is Clone. But the question sounds a little fishy, in that the real answer might be more like "this function that you've designed to take a reference ultimately need to be redesigned to take a value" or something like that. Tell us more about what's going on in your code?
Looks good, and definitely looking forward to the networking chapter! One minor issue on the book's [page on Maning.com](https://www.manning.com/books/rust-in-action?a_aid=rust&amp;a_bid=0367c58f&amp;chan=www) I noticed that it lists the chapter "Signals, Interrupts and Exceptions" twice (for chapters 10 and 12). On that note, what is the real chapter 10 supposed to be about? Just confirming since a preview of the book confirmed that "Signals, Interrupts and Exceptions" is chapter 12.
Awesome idea!
You can use a method to do so provided by a trait. If a trait doesn't have such a method, you can use a wrapper trait that has such a method. trait Trait { fn method(&amp;self); } trait CloneableTrait: Trait { fn into_box(&amp;self) -&gt; Box&lt;dyn CloneableTrait + '_&gt;; } impl&lt;T&gt; CloneableTrait for T where T: Clone + Trait, { fn into_box(&amp;self) -&gt; Box&lt;dyn CloneableTrait + '_&gt; { Box::new(self.clone()) } } #[derive(Copy, Clone)] struct Example; impl Trait for Example { fn method(&amp;self) { println!("Hello, world!"); } } fn main() { let ct: &amp;dyn CloneableTrait = &amp;Example; ct.into_box().method(); }
&gt; trait objects cannot implement Clone That‚Äôs not true of at least *sized* trait objects. Here‚Äôs a simple example: trait Foo { fn clone_as_dyn_foo(&amp;self) -&gt; Box&lt;dyn Foo + 'static&gt;; } #[derive(Clone)] struct Bar; impl Foo for Bar { fn clone_as_dyn_foo(&amp;self) -&gt; Box&lt;dyn Foo + 'static&gt; { Box::new(self.clone()) } } impl Clone for Box&lt;dyn Foo + 'static&gt; { fn clone(&amp;self) -&gt; Self { self.clone_as_dyn_foo() } } (You will observe that the Clone implementation is on `Box&lt;dyn Foo&gt;` rather than `dyn Foo`, so that to be clonable you‚Äôll need a `&amp;Box&lt;dyn Foo&gt;`. If all you have is a `&amp;dyn Foo`, use `.clone_as_dyn_foo()` instead, and lament whatever it is that you needed a Clone bound for.) That‚Äôs roughly how my anymap crate is able to produce a clonable anymap, as a real-life example of it.
I'm trying to port some old Java code to Rust (as an exercise and a way of learning). In Java I use a lot of interfaces and I was under the impression that Traits were the way to go, the method I'm working now looks something like this: public ContentObject process (Node node, ContentObject flag){ // Some code } If you can't process a `Node` you should return the flag object back to the caller. Since `ContentObject` is an interface I'm using `Box&lt;dyn ContentObject&gt;` as the return type and a `&amp;dyn 'static ContentObject` as a parameter. Any way around this?
I'm pretty sure the reason map order is randomized is to resist DoS attacks, rather than anything API-related.
While I still think that it‚Äôs good practice to wrap non-`std` traits, I have to agree with your other arguments. I updated the [issue on GitHub](https://github.com/myelin-ai/mockiato/issues/164). (I quoted you there if that‚Äôs alright)
So this is an initiative to run our already pickled models and their python code with zero to no change on production with faster runtime than python. Also this framework in the future will enable making prediction over unseen raw data with your own code and make use of parallel processing of large csv files, and more. Basically it is going to be more than a framework, maybe more than a toolbox.
Yes - **in this particular case** you can return the `&amp;'static dyn ContentObject` without boxing it: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0a4f3d40e1c347a7c198da67d3e09f5c ```rust fn process(node: Node, flag: &amp;'static dyn ContentObject) -&gt; Option&lt;&amp;'static dyn ContentObject&gt; { if node.can_be_processed { println!("Processing node with content object"); None } else { println!("Unable to process node"); Some(flag) } } ``` But why do you even want to return it? Wouldn't returning a `bool` (or better - an `enum`) be enough? The caller should already have the content object...
Continuing to work on my [Haml](http://haml.info) library [Haml-rs](https://github.com/jhartwell/haml-rs). I may also try working on my [ASC X12](http://www.x12.org/) crate that has more of a focus on health care x12.
&gt; Since the release of Go 1.0, the runtime has randomized map iteration order. Programmers had begun to rely on the stable iteration order of early versions of Go, which varied between implementations, leading to portability bugs. https://blog.golang.org/go-maps-in-action You might be right about the DoS mitigation part too, not sure.
Good bot :)
Check out this crate, it can help you: https://github.com/dtolnay/objekt
Tools problem. Updated with rustup and problem is gone. Thanks.
Updated with rustup and the problem went away. Thanks for the answer!
Just want to say, I picked this up recently and am looking forward to working through it. Thanks for your hard work!
I like the logo, it looks appropriately heavy-metal. :D
How do I run the examples?
Did you try // All DBs impl &lt;DB&gt; FromSql&lt;Integer, DB&gt; for MyType where DB: Backend, i32: FromSql&lt;Integer, DB&gt; { fn from_sql(bytes: Option&lt;&amp;DB::RawValue&gt;) -&gt; de::Result&lt;Self&gt; { &lt;i32 as FromSql&lt;Integer, DB&gt;&gt;::from_sql(bytes).map(MyType) } } ? I had the same problem and solved it by doing that. It seems that when you try to implement the trait for any generic `DB: Backend` but only constrained by the primitive type implementing `FromSql` for the exact same `DB`, rustc can figure it out.
Hi and thanks for a great book. I'm really looking forward to the next chapters! First book I've read that i follow like a TV series :) Do you want to share what we can look forward to in the last two chapters?
&gt; In Java I use a lot of interfaces and I was under the impression that Traits were the way to go That's the right idea, but there's more than one way to do it, and it can be tricky to figure out the best way when you're first learning the language. - If a `ContentObject` can be a few different types, but you know what they are in advance, it can make sense to define it as an `enum` and to use `match` statements to deal with the different cases. This actually doesn't require traits at all. - If you want to allow 3rd party callers to bring types of their own, then it can make sense to provide a trait for them to implement. In that case, usually the simplest way to work with the trait is to use "static dispatch" (i.e. not the `dyn` keyword). This looks like `fn foo(obj: impl MyTrait)` or `fn foo&lt;T: MyTrait&gt;(obj: T)`. - In some cases you can also use traits with "dynamic dispatch", i.e. with the `dyn` keyword. This is usually my third choice, because it comes with some tricky limitations. (For example, the compiler might not be able to reason about the size of the types involved, and so you might not be able to call methods like `.clone()` that give you a new instance.) &gt; If you can't process a Node you should return the flag object back to the caller. Passing objects around by value is often tricky in Rust, because it means you have to have complete ownership of the objects in question. For example, if I have a `File` object that's sitting in a field inside of a `MyStruct` object, getting my hands on that `File` by value means either replacing it with a different `File` or leaving the `MyStruct` in an invalid state. Taking a `&amp;File` or `&amp;mut File` reference to it is usually much simpler. It's hard to say more without reading your code, but there's a good chance you'll have an easier time if you pass references around instead of trying to pass ownership.
I'm interested.
This makes me sad, New Rustacean is speciall to me. It allowed me to get into a programming community when I hadn't have much experiance with the language at all. I haven't seen a podcast like this before. It doesn't help that I had just finished listening to all the episodes yesterday. I wish you a lot of luck and hope you will invest the freed time wisely into yourself and your familie. Thank you for your outstanding work.
Yes, I would love to contribute to this!
The amount of texture memory that the GUI will use is probably minute in relation to texture memory that games would use. I use very little graphic memory in the heap, unless the GUI is a very busy layout. I suppose I could put in a flag that allows you to select between GPU and Raster-based graphics, that would make for a mighty flexible library. Thoughts?
AFAIK, this yew example is the best you can get, file processing is possible, but it has to be done through onchange event. [https://github.com/DenisKolodin/yew/blob/master/examples/file\_upload/src/lib.rs](https://github.com/DenisKolodin/yew/blob/master/examples/file_upload/src/lib.rs)
I can also contribute to this.
Is there (or will ever be) any way to suppress explicit lifetimes on constructs which need subconstructs that really need explicit lifetimes? struct Foo&lt;'a&gt;{foo: &amp;'a str} struct Bar&lt;'a&gt;{bar_foo: Foo&lt;'a&gt;, bar_bar: i32} This often leads (in my code) to a cascade of lifetimes that could possibly be elided, no?
A broader comment: &gt; I'm trying to port some old Java code to Rust (as an exercise and a way of learning). Overall this seems like a difficult way to learn. If you were porting between say Java and Python, you might end up with less-than-perfectly-idiomatic Python, but the vast majority of your code would still run. You might say the same of Ruby, or JS, or C#, or Go. But porting to Rust is usually a bigger leap than those. And when Rust doesn't like the idiom you're using, it tends to flat out refuse to compile your code. The source language that you're porting from will sometimes _actively work against you_ as you try to get your program working, because it'll be based on idioms that _cannot_ be made to compile in safe Rust. Here's an example: In my Java program, I have `Person` objects and `Dog` objects. Each `Person` has a `pet` field containing a `Dog`, and each `Dog` has an `owner` field containing a `Person`. Initially these fields are `null`, but then when a person and a dog get paired with each other, the two fields get set. This is sort of pattern is super common in Java and Python and in many other languages. It looks perfectly clean and reasonable. And it _will not work_ in Rust. Maybe you can have a person contain a dog by value, and get rid of the `owner` field. In that case, things get much simpler. But if dogs can outlive people and move around on their own, that's not a good fit. They certainly can't both contain each other "by value", because the compiler complains about some kind of recursive type having an infinite size. Having them both refer to each other "by reference" makes the compiler very angry, because there's no GC, so the compiler needs to reason about whether those references might outlive the objects they point to. Sometimes people wind up trying to use `Arc&lt;Mutex&lt;_&gt;&gt;` to replicate the semantics of their source language, but that's painful and verbose, and you have to start contending with memory leaks created through reference cycles. The actual solution to this is usually to keep something like a `Vec&lt;Person&gt;` and a `Vec&lt;Dog&gt;` in some global `Environment` object, and to have the people and dogs refer to each other by _vector index_ (or some similar lookup key). Then you're responsible for figuring out what happens when a person goes away while dog is still referring to it, but the compiler is satisfied that at least no undefined behavior will happen if you get it wrong. If you're coming from game writing in C++, this is actually a very familiar idiom. But if you're coming from most other settings, this is unfamiliar, and it's unlikely that the source code you're translating from will encourage you to write things this way. Unfortunately, if you don't already have this idiom in mind when you start porting your code, it's very unlikely that you'll wind up figuring it out as you go. Trying to follow compiler errors and fix things incrementally tends to back you into a corner, where it seems like the compiler wants contradictory things. It's rough. So usually to do a successful port, you need to have an idiom in mind _ahead of time_ that you know is going to work in Rust. Of course, if you're just learning Rust for the first time, that's unrealistic. You don't know the idioms yet. That's why I think porting existing software isn't a great way to learn Rust. There are exceptions of course; command line apps like `cat` and `tar` tend to be more straightforward, and I hear people enjoy replicating those. But any program that manages a world of objects of different lifetimes is going to slam right into what makes Rust so different.
Not to flog my own projects too much, but plotting is definitely one of the use cases I envisioned for [piet](https://github.com/linebender/piet), and this would give you not only cairo but Direct2D as well. The library is still green in some ways, and there's more work to be done on implementations, but I'm hopeful about its longer term prospects.
You can't suppress lifetimes in type definitions. Do you have any other places where you have trouble with suppressing lifetimes?
Ndarray and nalgebra are targeting two different problem domains, so they're not things you'd likely be choosing between to solve the same problem. Their overlap would be linear systems, which nalgebra is more focused on so would be the better choice. Ndarray is much more akin to something like numpy and suited to general n-dimensional data processing, and likely what you want for machine learning (although you might then optimize a linear model from that dataset with nalgebra).
I use both of these libraries for different purposes. `nalgebra` has advanced linear algebra functions, but only on 1D Vectors and 2D Matrices. `ndarray` has arrays of arbitrary dimension, with convenient slicing inspired by Python's numpy.ndarray.
&gt; use a productive language with low optimization effort for most code. This particular phrasing "productive language" is exactly what I was trying to debunk. To me, which language is "productive" or not vastly depends on the context; there's no universal "most productive" language, it depends what you are trying to achieve. And that being said, yes, Java and C# will work for most developers and most situations. High-performance/low-resource environments being exceptions. &gt; I wouldn't say that GC is just there for mutability; even Haskell has GC. That's not what I meant, sorry for the misunderstanding. What I meant is that using a GC in an all-is-mutable language led to aliasing bugs as shallow copies may be made where deep copies were intended; something which languages with immutable values such as Haskell do not suffer from.
It's not idiomatic either for it not to work by RAII.
Thank you! After looking through Yew's examples their [dashboard](https://github.com/DenisKolodin/yew/tree/master/examples/dashboard) example actually reads a toml file. I should mention though, if you try to use it you must compile with `cargo web start --release` the normal debug version won't work. Their is an [issue](https://github.com/DenisKolodin/yew/issues/478) regarding it.
Not really, it is just about that. Then everything that ever dreams about touching "Foo" or "Bar" is condemned to have explicit lifetimes?
Thank you from the bottom of my heart for all the time put into New Rustacean. Your contribution to rust in this way during its formative years is immense, and the world needs rust to flourish. You did a great thing, and I enjoyed every minute of the podcast, which will serve as a standing reference for so many for a very long time. And I will miss that rad synth intro. Good luck on all future rust projects ;)
Well definitions of types containing them are. A method call can still elide the lifetime.
Is there an efficient way to remove items that satisfy a given condition from a `HashMap`? For example, in an IRC server, I want to remove all empty channels after a client as quit. At the moment, the code allocates a `Vec` and `String`s for any channel that needs to be removed... but the borrow checker doesn't want `HashMap::remove` to be called with strings borrowed from the same `HashMap`. ```rust // self.channels: HashMap&lt;UniCase&lt;String&gt;, Channel&gt; // UniCase&lt;S&gt; is a wrapper around S that provide case-insensitive matches. self.channels.iter_mut() .filter(|(_, chan)| chan.members.contains_key(&amp;peer_addr)) .for_each(|(_, chan)| chan.remove_member(peer_addr)); let removed: Vec&lt;_&gt; = self.channels.iter() .map(|(name, chan)| (name, chan.members.is_empty())) .filter(|(_, b)| *b) .map(|(name, _)| name) .cloned() .collect(); removed.into_iter().for_each(|name| { self.channels.remove(&amp;name); }); ```
French here ! I'm interesting.
Ah, didn't realize they specifically mentioned that. Interesting!
But it returns a result. Maybe an approach like [std::fs::File](https://doc.rust-lang.org/std/fs/struct.File.html) could work: &gt; Files are automatically closed when they go out of scope. Errors detected on closing are ignored by the implementation of Drop.
Yeah, `File` is only ever an opened file. If you want to keep it around as closed then what you need is a path, not a file. It makes working with it way simpler as you can't get errors for trying to read a closed file.
There was a JSON output, but literally nobody used it; it was broken for years, and we never got bug reports. I *think* it may have been brought back, though?
Thanks so much for the kind words. I feel a little sad myself, so I understand!
You're so welcome! And for what it's worth, you can listen to that *rad synth intro* any time you like: [it's freely available for download on Soundcloud](https://soundcloud.com/chriskrycho/new-rustacean-theme)!
Can you run one of the operations on a fixed thread pool with static scheduling? Just to see how it compares to OMP.
`s/viendez/vienderions/` (Pour les Acadiens.)
I'm trying to find a way to accept \`Box&lt;dyn Error + Send&gt;\` in places that accept \`Box&lt;dyn Error&gt;\`. [Example playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0b7d96168085240cfe70e0a7aea76247). Is there a way to do that without \`.map\_err(|e| -&gt; Box&lt;dyn Error&gt; { e })\`?
&gt; Parallel Initialisation helps make things faster, You don't even time this part, so how is it to make any difference?
StackOverflow'ed at [https://stackoverflow.com/questions/56330705/how-to-accept-boxdyn-error-send-in-places-that-accept-boxdyn-error](https://stackoverflow.com/questions/56330705/how-to-accept-boxdyn-error-send-in-places-that-accept-boxdyn-error)
If a function takes a `&amp;mut T` as a parameter, why can't you move a value out of it, and then move one back in later? The compiler does this analysis already for moves / initialization checks / etc., so why is moving out of a mutable borrow illegal if the compiler can ensure that you'll move something back in by the end of the function?
You need to define another generic type (`E` in the example below) for the error. Maybe this is what you want: ```rust use std::error::Error as StdError; use std::result::Result as StdResult; type Result = StdResult&lt;(), Box&lt; dyn StdError &gt;&gt;; type SndResult = StdResult&lt;(), Box&lt; dyn StdError + Sync &gt;&gt;; fn fn_returning_result() -&gt; Result { Ok(()) } fn fn_returning_sndresult() -&gt; SndResult { Ok(()) } fn register_callback&lt;CB, E&gt;(cb: CB) where CB: FnOnce() -&gt; StdResult&lt;(), Box&lt;E&gt;&gt;, E: StdError + ?Sized // ?Sized to accept `dyn` trait objects { /* ... */ } fn main() { register_callback(fn_returning_result); register_callback(fn_returning_sndresult); } ```
Cranking away at [Rusoto](https://github.com/rusoto/rusoto), the community AWS SDK. * updating our service definitions to pave the way to adding the remaining 23 AWS services * reviewing bug fix and new functionality PRs * moving more builds to Azure Pipelines We've got a couple easier tasks available for contributions: * https://github.com/rusoto/rusoto/issues/1417 * https://github.com/rusoto/rusoto/issues/1414
Because the compiler has no idea if the code in between panics. In that case, unless you exchanged your `T` with a valid instance (e.g. via `mem::replace`), you could observe a copy of T and lo! you have undefined behavior.
Pr√©sent! :)
There's a warning somewhere about using Deref to emulate inheritance, rather than on a "pointer"-sort of structure. I think this case sounds more like the inheritance case, so some caution is recommended.
Haha, je te crois que tu es int√©ressant mais es-tu int√©ress√©? :-P
Ah, I forgot about panicking. That makes sense, thank you!
I am the author. I have never imagined there are so many people interested about it! There's one thing I want to justify this open/close pattern. It doesn't mean resource occupation. This is used as dual buffer support for real-time rendering Open means start drawing on the new buffer. And close means present the pending buffer. Sorry about the confuse. Previously this is just for my own use. So if you guys have a better idea about it, please let me know.
I've generally made the experience that for balanced workloads, i.e. where it's more or less trivial to give each thread an equal amount of work, such as splitting up an array in equal chunks, work-stealing is generally inferior to simple "static" scheduling, i.e. divide the work evenly for each thread. Anecdotally, I believe that the reason is partly due to the overhead of the work-stealing setup itself, but I would expect a significant contributor to be the following: Given an array of N elements and P threads, with N &gt;&gt; P, if you give each thread \~N/P \*consecutive\* elements to work on, they will: 1. Access data that is close in memory, and moreover can be reliably prefetched. 2. Minimize instances of [false sharing](https://en.wikipedia.org/wiki/False_sharing), since only at the boundaries of each chunk can there be any overlap in cached data. The kind of fork-join parallelism that Rayon implements, while absolutely brilliant for unbalanced workloads, does not generally enjoy the same benefits. From your profiling, it could be that the "helper" functions that are reported are hiding some of the aforementioned effects. Alternatively, it could simply be down to the overhead of Rayon itself. I'd be very interested to see a more thorough analysis! My admittedly limited experience with Rayon for scientific workloads, which are often highly structured like the above, is... somewhat disappointing. Whereas Rayon is generally much more powerful than OpenMP's static scheduling, I did not manage to get the same kind speedups as I would have expected from similar code written with OpenMP. Unfortunately I don't have numbers to back this up, but I am starting to use Rust more at work these days, and hopefully I'll have more chances to experiment with parallelizing my scientific code soon.
Hey oh vous deux ! :)
Basically, not all memory regions are created equal in a complex multi-CPU setup (called "NUMA"). Single-threaded initialization puts all the data in memory that is optimal for the CPU-core that initialized it, and the other threads will have a less optimal path to that memory location. I'm sure someone with a more detail knowledge of memory layout and subsystems could give a more detailed/correct answer.
"the Linux kernel written mainly in C / C ++." is factually inaccurate. Torvalds has on many occasions professed his distaste for C++, and I would be very surprised to see any C++ in the kernel.
Yes, that totally works, thank you!
Agreed, I write it just in case people find my library. If you have any better idea, please let me know. I am more than happy to make the change! Thanks
Very interesting to hear your experience. I will be porting more HPC mini-apps to Rust, so maybe next I will select one using unstructured data, or an event/actor pattern. I'll post an in depth analysis of all this when I finish my dissertation
This comment feels very arrogant. There's definitely many reasons why you could get burned out without having much of a choice. Be it from work or any other source of stress. Sometimes you end up in a situation you didn't prepare for, no matter how well you try to manage your life.
&gt; all the data in memory that is optimal for the CPU-core that initialized it, and the other threads will have a less optimal path to that memory location. I'm sure someone with a more detail knowledge of memory layout and subsystems could give a more detailed/correct answer. I am very doubtful this is how it will work in practice on Linux. If it was, you would have to create the memory on the same core that later rayon will decide to run it at, too, no? Anyway, on my simple SMP box I get absolutely no scaling, and I tried all the tricks I though about: chunking vs no-chunking, differnt chunk sizes, huge arrays (to allow more runaway) etc. Some is very weird locally here. Which is weird because I use `rayon` in other project and always get a decent speedup on my 4 cores / 8 threads box.
There is one rust-specific pattern for open/close if you can't model it as RAII. You use two different types for opened and closed objects. The open / close methods take ownership of the object and return the other type. It's impossible to use your old handle after opening or closing because the borrow checker guarantees it and you can't call irrelevant methods on open or closed objects. All guaranteed at compile time.
Try it with the OMP benchmark too, you'll probably have the same lack of scaling. I couldn't get any scaling on my laptop too, and I think it might be my intel firmware or something like that. The system I ran this on is very highly tuned for workloads like the benchmark too. As for your scepticism towards the the effect of parallel initialisation, the results speak for themselves. I'm unable to go into great detail about it at the moment/haven't fully looked into it, but parallel initialisation means that the data is distributed across the cores through the CC-NUMA set up (https://en.m.wikipedia.org/wiki/Non-uniform_memory_access). The system I ran these benchmarks on is unlikely to be like yours.
What is supposed to handle `RAYON_NUM_THREADS`? Or what should i use to limit num of CPUs?
New Rustacean is an awesome podcast. It will be sorely missed. Thanks for all the episodes and good luck with your new endeavors. It seems all my Rust podcasts has been cancelled or put on indefinite hold (New Rustacean, Request for Explanation, Rusty Spike). Are there any others still ongoing out there that I've missed. I really love the podcast format and would love to be able to keep listening to news and lessons about Rust.
Interesting stuff. I've been going through "Modern Compiler Implementation in ML" and translating to Rust, sticking as much to the source material as the language allows. Given this is my first real Rust project, it's nice to see how others have tackled some problems. Out of curiosity, why did you use `mem::replace` instead of a `Cell` or `RefCell`? I had never heard of `mem::replace` and as far as I can tell, the "Programming Rust" book from O'Reilly makes no mention of it. As such, I'd love to know the pros and cons of each approach.
Nice post. Out of curiosity. I heard somewhere that If it weren't for Niko, Rust would be yet another compiled language with GC. Is this a correct statement?
`HashMap` has a `retain()` method which gives you a mutable reference to the value and lets you return whether or not that entry should remain in the map: self.channels.retain(|(_, chan)| { if chan.members.contains_key(&amp;peer_addr) { // you probably don't need the `if` here as it's just a redundant lookup chan.members.remove(&amp;peer_addr); } // return true if the entry should remain in the map !chan.members.is_empty() }); https://doc.rust-lang.org/nightly/std/collections/struct.HashMap.html#method.retain
That works! Thank you very much! Just needed to remove the parentheses in `|(_, chan)|`.
Gotcha, I am going to think about it. Thanks!
@zoonage I've created a couple issues that you might be interested into... &amp;#x200B; &amp;#x200B; Please, feel free to read, comment, and give feedback about anything that you believe it's important. &amp;#x200B; Here: [https://github.com/14-bits/voik/issues/7](https://github.com/14-bits/voik/issues/7)
Ah yeah it passes two arguments instead of a tuple.
thanks for the suggestion. I'll add to our list, quite interesting.
I hope so. No one has commented saying that they have been lost. The book's ultimate goal is to give its readers confidence. Confidence to explore the language and extend their own knowledge of how computing works. Rust in Action spends lots of time (about 100 pages) teaching Rust. Every language concept used in the examples is fully explained. But it doesn't cover everything. I need to be realistic and not pretend that this is the only book on Rust. I have tried to do the opposite, by showing readers that they can access the free resources with rustup, for example.
Do I generally declare methods of `Copy` types to take `self` instead of `&amp;self`, or are there cases where (for style or for semantics) `&amp;self` is preferred?
The con is you need an &amp;mut reference, the pro is you don't need Cell or RefCell. Basically you should use mem::replace if you want to swap something out you can get a mutable reference to, and cells if you can't get a mutable reference to it.
I just ran the example and even when idling and nothing animating, the application is using up some of the CPU. Is it continually redrawing the UI when nothing is changing?
Did you manage to measure 48MHz version? Was it fast enough?
Yeah, I've had basically the same experience in my parallel programming course experiments. OpenMP dynamic scheduling with a reasonable chunk size works well for some problems, but static sizes and tuning the chunk size seems to work well in most scenarios.
Yup, that's the issue I'm running into right now. It continually draws and refreshes the screen due to me not being able to blit to a master image. I'm going to be working on that for the first release of 0.3.0: I'm already half way through the implementation, I hope to have a release in the next few days to help alleviate the CPU usage.
Shouldn‚Äôt you pick a work stealing implementation in C++ if you want that comparison? Like cilk++?
If I'm not mistaken, what /u/yuma2017 is asking is if it is possible to have a background container running during a task with local network. &amp;#x200B; For example, it would be nice to run a database container on the side as a task while an integration-test (defined as a task) executes. This is already doable using docker-compose, but it sounds like something very cool to have in toast too.
Thank you for insights!! there are a few terms I didn‚Äôt pick up on right away, but I‚Äôm absolutely going to look into this ‚Äúborrow checker‚Äù that Rust has. This seems like a really useful software feature!
I only found your post this morning, and I'm already hooked. I've been writing Dockerfile(s) and Makefile(s) a lot recently, and this kind of gives me best of both world. This is a really good tool as far as user friendliness is concerned. I'll be using lot more of it! &amp;#x200B; One question though: Does Toast cache only the steps or also the files? For example, if I have a step (task) for compiling the code and another for testing the code, is it possible to ensure that the compile step is only executed if the code has changed? This is if the code that I'm talking about is mounted or added as an input path.
I got *very* fed up with the `libusb` crate, so I'm writing my own USB host API abstraction layer in Rust, using `tokio`. At the moment I don't have a particularly witty name, but if there's interest I'll put my code up on GitHub.
Have you tried chunking on Rust (setting the minimum number of elements that are processed as a block).
Ah, makes sense. And yeah, that's what i was figuring. welp, time to write a scraper
Is there a good crate out there that can implement that sort of simple scheduling as easily as one can use rayon? Like given a slice of size N just split the work into N/cpucount and run a Fn over each and return a Vec of the same size (or array once const generics get finished).
This could save me. Is there any chance you have benchmarks? I'm particularly curious to see how it compares to Highcharts in browser with large data sets (100k + points), especially in live updating contexts.
This needs a much better examples (the ones shown only update a dict, run infinitelly or ignore input), because I am missing a point here. There are like 100 ways of deploying a Python model (TF can be used from anywhere, ONNX + Caffe might work, ...). Or even correctly deployed Flask API can be quite good (although correct forking order of stuff can be hard). What should be better here?
Well, I do ;) My model doesn't have crostini (yet)
https://coder.com/
Oh that's fantastic! Awesome work! :D
Cilk++ has been deprecated by Intel. The recommend to use Intel TBB instead (a library not a language).
I wonder if this is also caused by [Rayon not being NUMA-aware](https://github.com/rayon-rs/rayon/issues/319), and so data is being transferred between nodes more than OMP.
It's contentless generic blogspam fluff that I'm really sad to see upvoted (let alone praised) here.
I am really curious about all this, but I'm very distrusted by my family duties. It seems to me that it might be just memory bandwidth limits on my desktop / your laptop. On my desktop everything just tops around 22320MB/s. About the numa - I realize how NUMA architecture works more or less, but the way the code is right now, you still get 3 vectors, in one big chunk - they might have even got allocated together in one big final reallocation, on the main thread anyway. And even if they didn't - what is the guarantee that the CPUs in subsequent tests will be scheduled to do the work on the chunk that was previously scheduled to them. Since this is work stealing, my understanding is that it's all effectively random. So each thread gets 1/N chance of doing work on the local memory one way or another. Maybe that's even the core reason of poor scaling: for each additional CPU you get more processing power, but the chance of working with local memory becomes a bit lower for each CPU.
Thanks for all the hours you invested in the podcast Chris, I‚Äôve thoroughly enjoyed it. I share your sensibilities regarding cross platform UIs and wish you all the best with your research writing environment app.
I have never so many ads one single page. The actual content is very poor quality.
Depending on how things are set-up you can something akin to work-stealing using OpenMP. You just need to tell things to use either Dynamic or Guided scheduling instead of telling it to use Static scheduling.
Nice. I hope to get y'all working this week :).
Hi again ‚Äì still stuck at my `Box&lt;dyn Error + Send&gt;` problem. Llaurence\_'s answer was helpful ‚Äì it made my example work ‚Äì but when I actually want to store callbacks, I can't use the generic type argument for the error return type. So my current impasse looks like this: ``` use std::error::Error as StdError; use std::result::Result as StdResult; type Result = StdResult&lt;(), Box&lt; dyn StdError &gt;&gt;; type SndResult = StdResult&lt;(), Box&lt; dyn StdError + Send &gt;&gt;; fn add_callback(cb: impl FnOnce() -&gt; Result) { let mut cbs: Vec&lt;Box&lt;dyn FnOnce() -&gt; Result&gt;&gt; = Vec::new(); cbs.push(Box::new(cb)); } fn main() { add_callback(|| -&gt; Result { Ok(()) }); add_callback(|| -&gt; SndResult { Ok(()) }); } ``` Is there a way to change `add_callback` such that it accepts both callbacks potentially returning `Box&lt;dyn Error&gt;` and `Box&lt;dyn Error + Send&gt;`?
Peut on le faire pour le cr√©ole aussi?
While I was documenting project examples, I didn't want to give direction to the people for use cases. Python interface of this project is built for models like gazette-like models, and sklearn kind estimators. I don't want to include data, write algorithms, train and test them using the project itself. Moreover, I don't want to use them as test cases. Since it's not directly the project scope. About Tensorflow interface, it's already inside the project as a separate API. ONNX is going to be added soon. About Flask application based deployment, yes deploying Flask application will be good. BUT, (yes there is a but‚Ä¶) To my experience, that kind of deployment is not responding quite fast under the heavy load. I didn't publish the benchmarks yet but a simple deduper approximately takes 100 ms to dedupe a list of values with this project (10k concurrent requests with Rust's current async support) I can finally leave the decision to you. But definitely, I will take project example improvement comment. I will enrich them over time.
I think this is just wishful thinking. The operating system *may* put the memory on the appropriate NUMA core, but the only way to guarantee it is to call the API correctly and explicitly request local memory. On Windows this is the [VirtualAllocExNuma](https://docs.microsoft.com/en-us/windows/desktop/api/memoryapi/nf-memoryapi-virtualallocexnuma) function. I just searched Rust's and Rayon's GitHub repos, and neither of them use this function. Rayon doesn't pull in any dependencies either which might do this. I suspect this is all waiting on allocators to become stable, because without that it's difficult to allocate in specific memory regions...
What did you use to generate the SVGs in the media directory?
Just updated the readme. I am not sure how people feel about it. If anyone has some toughts about the description, please let me know. [https://github.com/38/plotters/blob/master/README.md](https://github.com/38/plotters/blob/master/README.md)
Yep, that probably matters a lot. I'm not 100% sure if it explains the difference, though.
Thank you for making this podcast,good luck on your way.
Wrong sub. You want r/playrust
I mean using async means being up to date. Up to date is not only working examples, but examples using new syntax.
Unrelated, but I don't even have dyslexia and I can guarantee that I would never be able to remember that acronym, haha.
I used [Asciinema](https://asciinema.org/) and [svg-term-cli](https://github.com/marionebl/svg-term-cli). I'll add a note in the acknowledgements section of the README.
The contents of the `input_paths` are incorporated into the cache key for that step. So if you change your code, the compile task should be be executed again as one would expect. The behavior of `mount_paths` is different. With `mount_paths`, the files are not copied into the container at the beginning of the task. Rather, they are bind mounted into the container and any changes to those files on the host *during* the execution of the task are visible inside the container (and vice versa). Toast does not read the contents of `mount_paths` (so they are not included in the cache key), but when you use `mount_paths`, Toast requires that you set `cache: false` for that task. Tasks that use `mount_paths` are not cacheable.
Would something like [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=fcaf321d09ec6b01572e3754e4388f53) work?
I know I've seen this before, but I can't for the life of me remember the trait I want. Essentially, I want to define a trait method which takes `self` in any form (reference, mutable reference, or owned) so I can take `self` for `Copy` types and `&amp;self` or `&amp;mut self` for non-`Copy` types. What I'm currently doing ([playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=2bc1737407d9f4d5294a266e363dbe5b)): trait Trait { fn thing(self) -&gt; usize; } #[derive(Clone, Copy)] struct Foo { foo: usize } impl Trait for Foo { fn thing(self) -&gt; usize { 0 } } // n.b.: non-Copy struct Bar { bar: String, } impl Trait for &amp;'_ Bar { fn thing(self) -&gt; usize { 1 } }
&gt; I have been deferring the networking chapter because of the async discussions. I've now decided to ignore that and come back to it during a second edition. Sorry, I should have been more explicit. I won't be covering async in Rust in Action, 1st edition.
Yes, I got it thanks, was just saying it's contradictory to say it's up to date and that you will make a version 2 to update because current features won't make it to the book.
You're looking for r/playrust dude. This is the subreddit for the [Rust](https://www.rust-lang.org/) programming language.
/r/playrust, This is for the programming language.
YES! I would totally use this. Although I've never dealt with it myself, word is that the usb spec is incredibly cursed, so I hope you know what your getting into. :P
Async isn't stable, so it's not valid to say that it's a current feature.
What does MEAP mean?
This post needs, like, a spoiler tag. I had no idea this was the final episode until I read comments here...THEN I listened. :*( Chris, your content on this podcast has been first-class, and has always been one of my first suggestions for co-workers who are interested in starting Rust. Coming from a former college professor, your teaching style is superb, and until I started listening to your podcast I had no idea it could be a suitable medium for teaching about a programming language. Best of luck building Buttondown -- let us know if you need anybody to hack on it with you! :) I'm anxious to see what you can contribute to the editor space.
Is it possible to have different tasks use different images?
thanks bro
It is incredibly cursed, but knowing someone would use it makes me feel like it'll be useful.
I suppose both will be profoundly impacted once generic constants arrive, and perhaps they'll overlap more. For now, regarding arrays, while ndarray may offer compile-time dimensionality (for lower dimensions), nalgebra may also offer compile-time length for each dimension (for lower dimensions and lower lengths). eg. you may have a 5D array on ndarray such as a 4x3x7x9x9 array - where dimensions have the length of (4, 3, 7, 9 ,9) and those 5 lengths are runtime values, and you may have a 2D array on nalgebra such as 3x2 - where dimensions have the length of (3, 2) and those 2 lengths are compile time values.
Isn‚Äôt the input of rustdoc machine readable by definition? It should also contain the same information.
I have a use of linear algebra for engineering and mathematical purposes. I compared ndarray and nalgebra before I started working on my [orbital mechanics library](https://crates.io/crates/nyx-space), and nalgebra was by far the best choice. Since, I've also become a contributor to a [math library](https://crates.io/crates/dual_num) which already used nalgebra. For these applications, I'd say that the best feature of nalgebra is dimension checking at compile time (using you use a dynamically allocated `DMatrix`) which means that you can't compile code which does wrong math.
Pas mal de reponses deja, mais s'il y a besoin d'un coup de main supplementaire, je peux configurer mon clavier en azerty a nouveau ;-)
You have to go back to very early versions to find the end of the bootstrap chain (I think around 0.8 or so? - there's a revision list in the rust repository somewhere). As for mrustc - it is just a compiler, it requires the rust 1.19 source for the standard library.
Not currently. Each task uses the image produced from the previous task in the topological sort of the task DAG (or the base image, if it's the first task).
Alrighty thanks. Does it require all of it? Or just a specific folder? And if so, which folder?
That would be convenient. Right now if you need several different build tools you have to install them all yourself instead of using pre-made images. Though I understand it would add complexity. I was wondering, how do you manage cache eviction when you remote cache gets full? Obviously it's outside of Toast's scope but it's still something to consider.
I love this book, great job! I know how much work it is to write a book, let alone finish it. For those that want to get some Rust In Action, everything on manning.com is 1/2 off using code WM052719
right, but i don't want to implement macro expansion + name resolution on my own :/
&gt;That would be convenient. Right now if you need several different build tools you have to install them all yourself instead of using pre-made images. Though I understand it would add complexity. I'll have to give it some more thought. &gt;I was wondering, how do you manage cache eviction when you remote cache gets full? Obviously it's outside of Toast's scope but it's still something to consider. If you use Amazon ECR, you can set a [lifecycle policy](https://aws.amazon.com/blogs/compute/clean-up-your-container-images-with-amazon-ecr-lifecycle-policies/). I use Docker Hub, which as far as I can tell doesn't have a way to set a retention policy. But it seems like as of May 2019 even their free plan doesn't have an image limit (???). It's a strange world we live in.
Have you taken a look at my own crate, [abi_trait](https://gitlab.com/zack-emmert/abi_trait)? I haven't published it to crates.io yet, but it does pretty much exactly that
Yes,it's what motivated me to start working on this attribute.
Ah, not currently. Although you could use something like `toast &amp;&gt; log.txt &amp;` to run Toast itself in the background.
If you're interested, I'd be willing to make myself useful and help integrate my work into abi_stable
The way you do trait objects in abi\_trait is different enough from what I am planning to do that I think it's worth implementing a separate solution myself.
I would not suggest using `mrustc` if you're new to the language. To answer your question - It should only need the standard library crates (from the `src` directory), but since those are mixed in with the rest of `rustc` it may be challenging to just extract those.
nalgebra is pure Rust unlike ndarray which has an OpenBLAS backend. Unfortunately that means that some ndarray programs will perform a lot better than their nalgebra counterparts
Just as an FYI, the initial work I did on it results in the following usage: Before, with no optimizations, just using 2D drawing: 28.9% CPU After, only blitting to an image in GPU RAM: 5.2% CPU. Now, imagine how much less CPU there will be after I finish the complete blitting functionality. Right now, I'm dealing with a screen scaling issue (currently double the screen display, due to HiDPI), but I should have that solved momentarily. Keep an eye out for 0.3.0 release - that will be 100% GPU based.
It stands for "Manning Early Access Program". The book is not yet finished, instead you are given access to the manuscript as it is developed. I believe that readers who purchased the book in 2017 received their 9th update this month.
I think you want to make `fn add_callback(cb: impl FnOnce() -&gt; Result) {...}` into `fn add_callback&lt;R&gt;(cb: impl FnOnce() -&gt; R) {...}`. You will also need to change the type in the vector so that the FnOnce return is also R.
Are you trying to bootstrap Rust? Why? Also, if you really want to do it, you'd better have a lot of free time - I don't think anyone knows the exact number of builds anymore, but it's definitely in the hundreds.
Can't you have a default implementation for a given function in a trait? That way you don't have to rewrite it every time.
Not if it does property access.
So I'm assuming that the body of the id function is just a getter for the id field?
Yes, it should be that way. Currently, it generates a new id all the time.
I'm not sure if this is changed in newer versions of Actix, but in my code I don't store Addr, rather I store a Vec of `Recipient&lt;Message&gt;` which I broadcast messages to. Otherwise your options are `Box&lt;dyn ...&gt;` or even [typemap](https://github.com/reem/rust-typemap)
I‚Äôd assume similar. Macro is the best way to go me thinks.
Look into procedural macros, particularly derive macros.
Right. So if I'm getting this straight, because you implemented a blanket trait, you can't do any property access, (because your code has to work for *every* type and you know that not *every* struct will have that field.) A stub implementation of `fn id` will be required for each type. This is essentially you telling the compiler that the `id` field is in fact an id and not something else with the same name. If this boilerplate is too much, I'd advise to write a macro.
Agreed.
The array needs to be a fixed size prior to specialization, although the actual size doesn't matter. Essentially he wants something roughly equivalent to `fn foo&lt;n: usize&gt;(arr: [u8; n])`, but that isn't valid Rust yet.
Yeah, it's fine
Thanks for reinforcing my thoughts.
I think you can use a mutable array by `zip`ping a mutable iterator through it together with the other two slices.
Things happened and I decided to rework on my old rust GUI framework, native-windows-gui. Now that procedural macro are a thing, it makes everything easier. So far everything works better than I predicted, but there's still a lot of pain point that I will have to work over.
Oui
That's the point. Many wise programmers hate 'mathematical' variables because they have no explanatory power without understanding the context or the theory, and and when and if they do they immediately turn into a real name. I find it interesting the culture forced 'real imaginary' names when the variables are actually supposed to be meaningless.
continuing to work on my rust blog [Rubble](https://github.com/Kilerd/rubble).
Yes, thank you very much for the help. &amp;#x200B; The new measurements are: (66,9ms) Rust (basic\_nth) (81,3ms) C (81,3ms) C++ (75,3ms) Rust with ideomatic code (nth) &amp;#x200B; Now Rust is a lot faster than C/C++. This is odd. I guess there are different default configurations set by the Rust crates and the STM32CubeIDE, which affect the speed...
Neat! As an optimization, you might want to call `Vec::with_capacity(2 + c.len() + 3)` at the beginning instead of `Vec::new()` :).
I don't really understand why? What's wrong with the English website?
Aren't you confusing log and program output ?
Yeah, it looks like you can't even log to a file with this?
This code calculates the 300nth prime in 51,3ms Now I'll be working backwards, trying to understand your code and learning to achieve something like that myself Thanks for the guidance
Yeah a derive macro which generates an id for every implementing struct and returns it every time should work
The book has a super awesome section on writing a derive macro like this: https://doc.rust-lang.org/book/ch19-06-macros.html#how-to-write-a-custom-derive-macro Basically walks through the entire process of what you'll need to write a `#[derive(Identifiable)]` or similar derive macro. Note that this is going to be different from `macro_rules!` macros if you've run into those before, but it's not particularly harder, just different. You should be able to use any regular logic in Rust to back the macro to do your ID generation logic. The proc-macro crate could even persist ids for particular functions in a json file or something in the project root! I don't know what your exact requirements are for these IDs, but since it's literally rust code which runs to generate your rust code, you can do a lot of different things to generate things appropriately.
There are some people who think [you shouldn't log to files]( https://12factor.net/logs).
Good point, I'll add it!
Doesn't change the fact that the output should be writable to a file, i.e. not contain TTY control characters which don't make sense out of the terminal. And as of today it's not easy in rust to make a terminal application and send your logs to stdout.
Ok, merci √† tous ! Je pense qu‚Äôon est assez. Si n√©anmoins vous voulez quand m√™me rejoindre le groupe, pingez moi sur Reddit que je vous ajoute √† la discussion Reddit. Merci !
O√π se font les traductions ? Il n'y a pas une interface en ligne pour √ßa ?
OP delivers! Thanks for the interesting results and analysis.
What if I want to grep stuff from the logs?
Thank you Chris, it‚Äôs been an exciting journey. Every time I saw you‚Äôd released a new podcast I‚Äôd get maybe a bit too excited. It‚Äôs going to be hard to fill the void. Without your podcast, I‚Äôd never have dived into the rust world. You‚Äôve been such a positive part of the rust community that I‚Äôm glad to have stumbled upon.
Thanks for the feedback, I agree with you. I had this same doubt but finally, I preferred to go with the word "log" because it's more familiar for developers. At least in the JavaScript/Node.js ecosystem, where I came, people tend to be "flexible" with the concepts xD. An example of the library I use there: [https://github.com/klaussinani/signale](https://github.com/klaussinani/signale) &amp;#x200B; I'm watching in the Rust community the naming seems not to matter too much. But the experience in more mature ecosystems says it does. In fact, there are some nice crates with quite complex names, which surprised me a lot because the crates namespace is still not saturated.
Why wouldn't that work with stderr/stdout output?
&gt; Doesn't change the fact that the output should be writable to a file, i.e. not contain TTY control characters which don't make sense out of the terminal. That I agree with &gt; And as of today it's not easy in rust to make a terminal application and send your logs to stdout. Do you mean "as if"?
I think it would be better to be clear with the purpose and possible good uses of the crate. Most rust developers are quite mature and wouldn't use such a library for logging while they might find it interesting to generate some app output.
My bash/general terminal knowledge isn't that good. So if I have the app running in my terminal and it's outputting to stdout I can keep on grepping stuff from history (as in not just future prints to stdout) without actually saving the output anywhere?
You can't. I thought on add streams support but finally, I decided to keep a lower complexity of use instead. Anyway, I've added an issue to cover this case. PRs are welcome by the way :). &amp;#x200B; [https://github.com/jesusprubio/leg/issues/3](https://github.com/jesusprubio/leg/issues/3) &amp;#x200B; Moreover, there are already some options among the public crates to cover those more complex cases.
&gt;At least in the JavaScript/Node.js ecosystem, where I came, people tend to be "flexible" with the concepts That's putting it politely.
No, you can't change what you grep while the program is running, but you can send it to a file if you want to and grep that. Anyway, the recommendation to not log to files doesn't mean you shouldn't send logs to permanent storage of some kind, it means that it's not the program that should decide where logs end up.
Why not do this as a function?
As I said, I had doubts and you're confirming them xD. So, I'm trying to change the description to better indicate the purpose and keep it friendly. Do you think "Elegant output made simple" is appropriate?
A significant amount of C code is also valid C++ so there ought to be a lot of "technically C++" code in the kernel.
Something like this does not work for you? `echo "Hi world\nno interesting line" &gt; out.txt &amp;&amp; cat out.txt | grep Hi`
would terminal logs affect the performance?
Hi! I agree with most of what you said so this response should hopefully not become too long. ;) ## `bool` &gt; Is the following statement correct? Rust wants to abstract LLVM but because of LLVM's presence some of its semantics leaks through to Rust whether you want it or not. I would disagree. Rust wants to have an abstract semantics, but it doesn't just want to abstract LLVM specifically -- we want to support other backends as well. For `bool` specifically, there are many reasonable choices for what the representation could be. Saying that "not `0` means `true`", as you suggested, is one of them -- it solves some problems but introduces others. For example, now two booleans that are equal (both `true`) could actually have different representations (so `memcmp` would say "not equal"). Comparing booleans requires normalizing them first. Also, `bool` was just the example in this discussion. The same issue with "there are some inherently invalid bit patterns and we want the compiler to exploit that" applies to references (cannot be NULL), and `enum`s (must have a valid discriminant). Using types to reason about which shapes your value can possibly have is inherently useful for program analysis, optimization but also verification alike. ## `&amp;mut Option&lt;T&gt;` &gt; When dealing with pure Rust code I've come to enjoy &amp;mut Option&lt;T&gt; to stand in for out parameters. If I'm worried about having to drop the original value when I know it is None you can use the replace method and forgetting the old value If you are using `Option` anyway (and thus have some run-time checks), wouldn't it be better to panic if there is a `Some` as opposed to leaking it? ```rust let mut output = None; let old = output.replace(Some(value)); assert!(old.is_none()); ``` A helper method would probably help. (I've sometimes wanted to "assert `is_none`", similar to how `unwrap` does "assert `is_some`", when inserting into a `HashMap` and also found that annoying to write.) &gt; My point is that Option&lt;T&gt; seems to have some similarity with MaybeUninit&lt;T&gt; where the former is runtime checked and the latter is the unchecked variant of this use case. I enjoy this kind of relation because it allows to view the same concept in different ways which often leads to better understanding. Yes, there definitely is a relation. Though `MaybeUninit` is in some sense more powerful since it can be "partially initialized" -- it's not an "all or nothing" thing. ## About the MaybeUninit name There was an FCP for the name specifically in [this PR](https://github.com/rust-lang/rust/pull/56138). &gt; Conceptually this type does not represent an initialized value at any time because the wrapped T is further wrapped in ManuallyDrop. I don't understand this point. The use of `ManuallyDrop` inside `MaybeUninit` is an implementation detail.
``` fn concat(a: i32, b: i32, c: Vec&lt;i32&gt;, d: i32, e: i32, f: i32) -&gt; Vec&lt;i32&gt; { vec![a, b].into_iter() .chain(c) .chain(vec![d, e, f]) .collect() } ```
Yep, Linux is pretty bad by default on anything with &gt;16 cores. If you have long-running parallel jobs, you absolutely need to pin them to cores, otherwise the same task will jump back and forth between completely unrelated cores multiple times a second :-(
Maybe you could make it be "leg, beautiful logs on your stderr"? I'm not any kind of unix traditionalist so I'm not 100% sure on this, but when I write shell scripts I write my messages for the user on stderr and my program output on stdout. I would definitely use a library like this for printing stuff to stderr. It looks like your library can only print to stdout though.
You can't do variable length arguments with Rust fn. Only option is a macro
Evgeniy, the author, has released a new version with many of my feedbacks, and I've created some issues to further improve the extension: https://github.com/EvgeniyPeshkov/syntax-highlighter/issues?q=is%3Aopen+is%3Aissue Feel free to add your remarks as well :)
We can (and must?) be the change we want to see in the world. Chris Krycho was a podcasting Voltron. It would be hard for any one person to fill his shoes, but there *are* smaller, independent components that can be broken out from the New Rustacean model. I'd love a bite-sized "What's new in Rust 1.xx" podcast, or a standalone "Crates You Should Know." There's also room for a renewed set of Rust for Newbies podcasts, for developers first coming into this ecosystem as of Rust 2018. So, who's recording what? :)
If you are new to Rust, please ignore mrustc. It is not for you.