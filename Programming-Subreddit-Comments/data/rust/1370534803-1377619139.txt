Start here: http://smallcultfollowing.com/babysteps/blog/2013/04/30/dynamically-sized-types/ In addition, basically every post to Niko's blog since that one has been as a result of that proposal: http://smallcultfollowing.com/babysteps/blog/categories/rust/
Brilliant, much obliged!
OK, I'm by no means a full fledged programmer, and I'm just 19 years old, so I haven't finished university yet, but could someone explain what this whole thing is about like I'm 5? What does a sized type imply? How would this change the code that you write? The posts on Niko's blog are good, but I feel like I'm too inexperienced to fully understand everything, and I'm really interested in how a programming language is made, which is why I've followed Rust since I first saw it mentioned more than a year ago. 
From http://smallcultfollowing.com/babysteps/blog/2013/04/30/dynamically-sized-types/ : &gt; Sized types indicate values whose size is known to the compiler. Unsized types represent values whose size is not known the compiler (this terminology is somewhat imprecise; unsized values do in fact have a size, but it is not known until runtime). You say you're new to programming and you say you're in university, so I'm going to presume that you're using Java. :) Both sized and unsized types are useful in a programming language, and unsized types can be built out of sized types. Java's array type is sized: int[] foo = new int[10]; The Rust equivalent would be: let foo: [int, ..10] = [0, ..10]; // type annotation included for clarity, can be omitted This works if you know the size of the array beforehand. But what if you don't know how much size you'll need until runtime? In Java, you'd use either the Vector or ArrayList classes, and then add elements to it at your leisure: ArrayList&lt;Integer&gt; foo = new ArrayList&lt;Integer&gt;(); Under the hood, what's happening here is that Java creates a normal, sized array of some basic length (say, for example, 10 elements), and then every time you do an insert, it checks to make sure there's room for one more element in the underlying array. If not, it creates a new, larger array (perhaps now 20 elements), and copies everything from the old array into the new array, and changes the ArrayList to point to the new location in memory. This is necessary because Java's type system doesn't have any notion of unsized types (at least as far as I know), and hence ArrayList is simply a pointer (which has a size). Here's how you make such an unsized type in Rust: let foo: ~[int] = ~[]; // type annotation included for clarity, can be omitted ...But how does this work?! We know that `~` is just a pointer, but `[int]` isn't anything at all! let foo: [int] = []; // fails to compile This is because Rust's type system, like Java's, currently has no notion of unsized types. So instead Rust cheats! It specifically makes an exception for `~[]`, because growable arrays are really useful. But the downside of this is that, for any type `T`, the types `~[T]` and `~T` are completely different beasts! This poses a problem for anyone who wants to implement something generically, because now you've doubled the number of implementations you need (actually, since `~str` is strange in the same way as `~[T]`, you've tripled the number of implementations you need). This "weirdness" of strings and vectors is one of the most counterintuitive aspects of Rust. So what Niko's proposing here is to actually make unsized types a proper aspect of the type system. They wouldn't be allowed everywhere that sized types are allowed (hence the reason for requiring the Sized bound in some cases), but it would make it so that `~T` and `~[T]` and `~str` are no longer separate concepts. Disclaimer: I'm an awful programmer and awful at both Java and Rust! Corrections welcome.
Ah, then check out https://github.com/mozilla/rust/wiki/Rust-for-CXX-programmers , which has tables that map concepts from one language to the other (e.g. C++'s `std::vector&lt;int&gt; {1, 2, 3}` is Rust's `~[1, 2, 3]`).
Thanks a lot! :)
I think you have an off by one error in your printing iterator, as the page repeats the content twice.
Worst font size choice possible.
Hey, I self taught C++, Haskell and now am learning Rust. I also am registered in the US as a civil engineer and in fact have no formal education. I am respected as a software engineer within a large company. Don't devalue your knowledge just because someone else didn't deliver it to you. There are many ways to learn. University is a great one, but not the only.
Oh, that's awesome! I have just seen so much horrible code, and especially C and C++, that I feel that no one would ever trust my skills without a degree. I'd like to think that I'm "better than that", but I'm way too humble, hah.
A degree doesn't automatically make your code better. Reading code and writing code makes your code better. I have a BS in Computer Engineering and 10 years in the industry and the knowledge covered in that degree is a fraction of what it takes to build a real app. The rest is all continuous self study. Waiting to try until you've been validated in an attempt not to fail (this is me projecting) doesn't do anything but delay the inevitable. Your code is going to be bad until you've been reading/writing it long enough regardless of your education. Figuring out why it's bad will get you through the phase faster but everybody starts as a beginner. Getting a good chunk of the bad part out of the way while you're supposed to be learning and when nobody depends on your output is way better than doing it on the job when you have to maintain it and/or otherwise deal with the consequences.
&gt; it would make it so that ~T and ~[T] and ~str are no longer separate concepts. That would be extremely useful! There is much duplicated functionality in the `str` and `vec` modules, it would be nice if everything 'just worked'. :)
Rust is *extremely* unstable, most significant programs end up meeting "ICE"'s (Internal Compiler Errors, i.e. bugs) several times, and it's not that hard to write a (supposedly) safe piece of code that segfaults. There is no guarantee of backwards compatibility yet. However, as a point in its favour, there is [unit testing](https://github.com/mozilla/rust/wiki/Doc-unit-testing) built into the language. (Also, the libraries are not very good at the moment, there isn't a HTTP client (or server) library in the standard library at all, for example.)
In general, Rust is not yet stable enough for the production use. It is unstable in terms of both the language evolution and implementation quality, and while the latter can be worked around the former will prohibit any production use. It is possible to write a small program that will only require the minimal modification as the language evolves at this point, however. Most prominent features are in shape and won't change radically; the type system and static analyses, the language's "semantic" portion, are still evolving but hopefully won't make the existing valid program invalid. While the standard library will break a lot (for example, most recently by renaming `core` and `std` crates to `std` and `extra`) one can work around it by introducing the compatibility layer (which is what I've done in my [Rust port of a game written in C](https://github.com/lifthrasiir/angolmois-rust)). It is still not yet feasible to write a large program without closely following the development of Rust.
It sounds like you might be better off experimenting with [Go](http://www.golang.org/). Go is very much aimed at implementing system services, and has excellent networking and concurrency support. Libraries include JSON too. It is stable, and at v1.1 it has already seen widespread deployment. (I'm very keen on developing Rust myself, but it's simply not production ready yet.)
There did not appear to be [much performance improvement for either compiling or the test suite](http://huonw.github.io/isrustfastyet/#5d2cadb)... does rustc not allocate enough for the better allocator to help significantly?
This, nothing against rust, but Go was practically made to solve OP's problem. All the libraries are relatively complete, and modern as well. Distributed server processing? That's what Google does all day long.
As far as the compiler itself goes, it's possible that any gains would be offset by the newfound need to compile jemalloc. I ran highly unscientific tests of "make clean &amp;&amp; time make" twice for both the commit that added jemalloc and the commit immediately prior to it. Before jemalloc: 14m 53s, 14m 49s After jemalloc: 14m 57s, 14m 58s So it may actually be a net loss for the compiler developers. As for the performance of Rust code in general, I'd like to see more benchmarks before I'm happy with this change.
Have you seen this presentation - http://www.slideshare.net/akuklev/scala-circuitries ? It is about Scala, but may be useful.
rustc doesn't do enough (any) parallelism to show the benefits of using thread-local arenas for allocations up to 4K in size and using only mmap instead of sbrk. It isn't long running so memory fragmentation isn't a big issue either. It isn't enough of a microbenchmark of the allocator for a 10-20% reduction in time for small allocations in a single-threaded program to translate into much. A benchmark like `core-map` shows improvements for both small and large allocations and there are bigger gains from lack of contention with multithreading.
Does anybody else hate when people compare Go to Rust?
Right at the bottom it was mentioned that `lmath` has been fixed. Unfortunately whilst it builds on incoming, due to a bug you can't use it in external crates. moonchrome and I am are working on fixing this but it will require us to remove the trait heirachy and use macros to generate each type (`Vec3f`, `Vec3f32`, ... etc.) individually instead. Integer and Boolean vector types (present in GLSL) will also be removed. Also, happy reddit birthday cmr!
They're both systems level languages that should have a lot of overlap in their domain of use, I don't see why not.
brson started doing this about a year ago and then immediately fell off the wagon, let's see if someone can keep it up this time. :)
This is a really great summary of the progression of the DST proposal and closure reform, for those who haven't been following.
My favourite part, JSON "literals": let json = from_str(stringify!( { "language": "Rust", "level" : 9001 } )).unwrap();
I can't imagine how nice this kind of thing would be for testing parsers. The only requirement is that the stuff inside `stringify!` tokenizes, but that's not a high bar to reach.
It's hard to think of Go as a system language actually. I see Go as a server-side language, certainly, but system ? No.
Completely open-ended public opinion survey: how important are `once fn`s / why are they important?
I'm happy that you folks are remaining civil, and this is a friendly reminder to others to keep it that way. I'm quite aware of the fact that Go tends to attract a lot of online vitriol for its design decisions, and that won't be tolerated here.
I'm rather pleased that there's now some good competition between modern languages on the lower part of the stack - finally we can see some good alternatives to C and C++!
I agree, for anything userspace Go really seems to hit a sweet space between convenience (duck typing on interface/possibility of safe upcast) and performance. I think that given time Go has a fair chance of gaining on Python for servers writing (deplacing Django for example).
Yes, you're correct.
I remember when eholk made an XML literal macro. It actually worked :)
Thanks, I made the same mistake elsewhere too and have now fixed it.
I'm pretty sure SQL tokenises, which'd be awesome, e.g. parameterised queries: `sql!(SELECT * FROM table WHERE foo = $my_variable)`, and the driver would automatically use `my_variable` and escapes it. And, as you showed, `{ .. }` tokenises, so `hashtable!({ "foo": 1.0, "bar": 5.0 })`'d work. So many possibilities!
&gt;It is also in the return type, `Option&lt;&amp;'r ~str&gt;`. Ignoring the wrapping Option type, this should be read as "a slice into a string that lasts for at least `'r`". I think the return type should be Option&lt;&amp;'r str&gt; as it is in the code threw me off while reading.
Thanks, an earlier version returned `Option&lt;&amp;'r ~str&gt;`, but I decided to change it for the reasons I explain later. That bit slipped through.
Hah, that's what it was! I knew something like this existed but when I needed it most I couldn't find it or remember what it was...
Thanks! Any idea what the `NO_REBUILD` variable does?
It [appears to disable setting Makefile dependencies](https://github.com/mozilla/rust/blob/88c318d28c7d8fe8346d38d73cb0b1a30f33f278/mk/tests.mk#L294), so that each crate can be built independently without being forced to build the bootstrap.
I agree with your point in the specific case -- having many different kinds of closures would indeed be scary for newcomers -- but not necessarily with the generalization: &gt; Every additional type system feature has a high cost, and at some point we have to draw the line. I think a more powerful type system can be much closer to a pure win if it's handled right. The point is that while it (a hypothetically more powerful type system along some dimension) would allow expressing things the less capable system wouldn't, the things the less capable system *can* express needn't look any different. Additional power only has a high cost if users are forced to be confronted with it in order to use the language effectively. If they only encounter it in use cases which with the less powerful system wouldn't be possible *at all*, what's the downside? Making sure it actually happens that way could be done on the cultural side, by strongly discouraging use of more advanced features than necessary to solve a problem, and setting an example through `std` and `extra`. (Of course, different kinds of closures are something users probably *would* have to be confronted with early on, which is why I'm not necessarily disagreeing in the specific case.) 
And of course it doesn't have to be `std` to work. :)
I have a proposal for the `@` vs `Gc&lt;&gt;` issue: https://gist.github.com/Kimundi/5744578
When I compiled with -j8 build time went down two minutes (jemalloc build is parallel, as is the rest of the rt, and benefits from it). None of the benchmarks regressed. If you find any code negatively impacted, please do share so I can investigate and tune.
yeah, they certainly have their work cut out for them. I think the biggest hurdle will be robust networking, not really sure how well networking works atm.
Added a note about this to the wiki.
I think it's fair to say Rust did pretty well for not being at 1.0 yet.
Why the large binary size (including libraries)? generics?
Probably the bug whereby all generics are reexported. I have a fix for this, but it bounced last time I tried to land it for some reason I haven't tracked down yet.
It's so that the dynamic linker won't try to relink symbols if their signatures change. The hash should *not* change if you only change the *contents* of existing functions.
But the fact that the hash change if you add an author, seem a problemto me.
The "Linear Types" concept to which the author alluded really piqued my interest, but my googling for Rust documentation on the subject hasn't turned much up. Is this still supported in incoming? Being able to detect possible file handle errors at compile time? Sweet!
Oh! Now I understand. The author was referring to dangling references to a once-opened file handle and how the compiler can prevent that. Brilliant. I thought that he was saying that you could use the type system to specify an end-of-lifetime that was defined by something other than the valid scope of the pointer. Something like "For the OpenFile type, the closeAndKill! macro signifies the end of an instance's lifetime." Then you would know that all references to the file handle beyond the call to closeAndKill! (even within the same scope) would be invalid and could throw compile time errors. Now I'm wondering if my misinterpretation is even possible. 
It is called a typestate, and Rust once had it. It was [not a fundamental concept](http://pcwalton.github.io/blog/2012/12/26/typestate-is-dead/) and turned out to be hardly useful anyway, so it was replaced by a linear type which can be used to simulate typestates (although the syntax would be a bit more verbose).
&gt; Any time you see the ~ sigil, that's a linear type. Strictly speaking I don't think that is true because one can borrow references to ~ types whereas with a true linear type you cannot give someone access to it without losing it yourself (though you can get it back if the function you passed it to returns it of course). For an example with true linear types (or more precisely, a variant called unique types) see the Clean programming language.
You're correct, I'm being fuzzy with terminology here. IIRC it's also the case that true linear types must be used *exactly once* (or maybe that's affine types...). We use the term "unique pointer" rather than "linear pointer" for a reason. :)
Interesting reading. As pcwalton concluded, I'd be interested in revisiting it post 1.0. Thanks for the link.
&gt; You're correct, I'm being fuzzy with terminology here. That's cool, I just wanted to make sure that zlsayton's wouldn't accidently get the wrong picture of what the term actually meant. Also you are correct: it is linear types that must be used exactly once, and affine types that must be used no more than once.
So when adding a function it also breaks?
Can't watch. After signing in with Persona I get the following message: Sorry, sign in is only for Mozilla employees or vouched users on Mozillians.org.
&gt; I think [the usefulness of the borrow checker] points to a kind of deeper analogy between memory errors in sequential programs and data races in parallel programs. I will elaborate on this theory in a later post. I'm looking forward to this post.
In case you haven't seen it yet: http://smallcultfollowing.com/babysteps/blog/2013/06/11/on-the-connection-between-memory-management-and-data-race-freedom/
Is there any particular draw back to *not* privileging `[]`? i.e. execute&lt;It: Iterator&lt;&amp;fn:Isolate()&gt;&gt;(iter: It) This would make it a whole lot more flexible, e.g. taking jobs from a channel. It seems like it would be tricker to get `divide` to work nicely with (the current features of) iterators though. Edit: hm, maybe `divide` wouldn't be that bad, the lowest level is just divide&lt;A, It: Iterator&lt;A&gt;&gt;(f: &amp;fn(A), iter: It) and then there are higher-level functions/iterators that achieve the nicer/faster behaviour. (e.g. `vec.chunked_iter(n)` to iterate over `n` elements at a time, as a slice.)
This is great, I probably will use this in one of my next toy projects. It would be nice if you put some more examples or documentation in a simple README.md file. I know it might be boring but it will probably increase the amount of people that end up using your wrapper.
Very cool, though do be aware that everyone agrees that our current net API is horrendous and that it'll likely be rewritten entirely before Rust 1.0.
Will do!
The README.md file has just been updated with (hopefully helpful!) information.
Thanks, much easier for a random browser to make sense of the project now :)
Great, this was the main thing puzzling me about Rust. So, in summary: * A library is named lib&lt;shortname&gt;-&lt;hash&gt;-&lt;version&gt;.so * The &lt;hash&gt; in the library name is there to prevent 2 different authors who pick the same &lt;shortname&gt; from colliding. * For example, a library provided by 0install should set author="0install.net" and never change it. The library's hash will then never change. * In addition, each symbol provided by the library gets a hash which includes its type signature (so incompatible changes become link-time errors) and the library hash (so you don’t get symbol name conflicts between libraries). This sounds ideal.
The ability to make proper combinators would be cool, but closures are such a tricky issue that nmatsakis is probably the only one qualified to respond to this proposal. As for the closure syntax proposal, I'd personally like to avoid C++-style capture clauses if at all possible. Rust's current `|| foo` syntax for a lambda is just so delightfully lightweight.
Super awesome. I don't have a compiler on hand right now, would it be possible to change the macro definition to: ($c).iter().filter_map(|&amp;$x| if $pred { Some($e) } else { None }) // +++++++ + So that we could invoke it like: filter_map!(~[0,x] for x in v if x &gt; 0) This is assuming that we'd only be using this macro on iterables and that filter_map always passes the closure argument by reference.
My own approach with slightly different syntax and the same capabilities - http://en.wikipedia.org/wiki/List_comprehension#Rust
It should support excluding the if part.
Ah yes, that's a good idea. It's possible like this: /* Iterator transformations in Python list comprehension style */ macro_rules! itermap( ($r:expr for $x:pat in $J:expr if $pred:expr) =&gt; ( ($J).filter_map(|$x| if $pred { Some($r) } else { None }) ); ($r:expr for $x:pat in $J:expr) =&gt; ( ($J).filter_map(|$x| Some($r)) ) ) 
I think my current pattern matching implementation is more generic. Since iterators don't themselves have an .iter() method yet (don't know if they should), the .zip() example doesn't work with your suggestion.
You don't need the `filter_` / `Some` in the second case.
(Although, [#5898](https://github.com/mozilla/rust/issues/5898) means that the method that should be `map` is actually called `transform`.)
This is a very limited sortt of comprehension: it doesn't let you omit the if clause, or add another if or for clause. I find that list comprehensions are really worth it when they replace *nested* loops. Of course, full comprehensions should also be implementable with macros.
It seems as if IteratorUtil doesn't have something that takes an `Iterator&lt;A&gt;` and a `fn(&amp;A) -&gt; T` for some `T: Iterator&lt;B&gt;` and yields an instance of `Iterator&lt;B&gt;` that… well, what I mean is sequence monad bind, for doing things like this: &gt;&gt;&gt; [y for x in xrange(5) for y in xrange(x)] [0, 0, 1, 0, 1, 2, 0, 1, 2, 3]
You can omit the if with the modification in http://www.reddit.com/r/rust/comments/1gag3t/list_comprehensions_in_rust_iterator/caifz04
I think that bind should be simple, and I can make your example work but I can't work out the closure lifetimes so that you can use both `x` and `y` (lexical scoping) in the expression. Edit: I think the below method should be called .join(), not bind. Sorry for not keeping my monadic vocabulary straight! trait Bind&lt;A,B&gt; { fn bind(self) -&gt; BindIterator&lt;A, B, Self&gt;; } impl&lt;B, A: Iterator&lt;B&gt;, I: Iterator&lt;A&gt;&gt; Bind&lt;A,B&gt; for I { fn bind(self) -&gt; BindIterator&lt;A, B, I&gt; { let mut it = self; let first = it.next(); BindIterator{ iter: it, cur: first } } } struct BindIterator&lt;A,B,I&gt; { iter: I, cur: Option&lt;A&gt;, } impl&lt;B, A: Iterator&lt;B&gt;, I: Iterator&lt;A&gt;&gt; Iterator&lt;B&gt; for BindIterator&lt;A,B,I&gt; { fn next(&amp;mut self) -&gt; Option&lt;B&gt; { loop { match self.cur { None =&gt; return None, Some(ref mut inner) =&gt; { match inner.next() { Some(b) =&gt; return Some(b), _ =&gt; {} } } } self.cur = self.iter.next(); } } } 
&gt; When we say "list comprehensions" in python, it's mainly a device to both map and filter one collection to another. It's a bit more flexible than that though: * the input is any iterable, not necessarily actual collections * it allows nested iteration on multiple sequences (synchronous can be handled via `zip` of course) * multiple (or no) filters can be provided (they are AND-ed as far as I know, there could be subtetlies) but more interestingly they can be interspersed with the iteration nesting
It's true that full comprehensions should be doable with a macro, but I disagree that this is desirable. IMO, restricting comprehensions to a single `for` is a feature, and any time that I see nested comprehensions in Python I recoil in horror and immediately convert them into loops.
What is still puzzling me is why we don't just use &lt;shortname&gt;-&lt;uuid&gt;-&lt;version&gt;.so? What is the point of the uuid in the metadata otherwise?
Here's mine: https://gist.github.com/hanny24/5749688 It is quite similar to Scala's for comprehension. It supports multiple iterators, multiple ifs as well as multiple lets. There are some limitations though: ifs (if present) must precede lets.
I thought that several flags were also added to the hash (crate one), such as whether it is a Debug vs Release build (or things like that). Are they not ?
kibwen is correct, you want `~[u8]` here. `~` is the malloc operator (although that may change soon to `new`). By default indexes are bounds checked, but in an `unsafe` block you can use `unsafe_get` and `unsafe_set` for speed.
I agree on the qualification issue, my mind melt a little when I read about how about one could bypass the borrow-checker by using sufficiently disguised closures; this was twisted.
Also from_elem and from_fn are very generic and very poor performing (comparatively). If you can get away with it, don't initialize the buffer, or if you need to have it zerod use std::ptr::set_memory.
&gt; ~ is the malloc operator Doesn't the compiler have the option to put it on the stack if it can prove that it can't escape? Or did I imagine that?
&gt; `vec::from_elem` creates an immutable vector It actually creates an *owned* vector, and mutability inherits with ownership, so if the thing that owns the `~[u8]` is mutable (that is, the image struct) then the vector is too. To give an example: let mut foo = Image::new(8,8); let bar = Image::new(8,8); let mut moved_bar = bar; `foo` is mutable, so `foo.data` is mutable. `bar` is not mutable so `bar.data` is also immutable. `moved_bar` is a transfer of ownership, so that (what was) `bar.data` is now mutable (note that after the `moved_bar` line, `bar` itself is (statically) unusable).
Yes, what happens with a piece of code like fn main() { let x = ~5; } is it (should) gets "translated" to something equivalent to int* x = malloc(sizeof int); *x = 5; free(x); (Or `new int` and `delete x` for C++.) i.e. `~` is just a wrapper around `malloc`/`free` that the compiler can reason about statically.
`vec::from_elem` gives you a mutable vector if you put it an a mutable location: let mut v = vec::from_elem(4, 0); v[0] = 42; println(fmt!("%?", v)); // [ 42, 0, 0, 0 ]
I saw different pieces of code where `mut` appears on either side of the assignment operator. E.g. let mut x = 5; let v = ~mut [1,2,3]; // Or something like that Is there a difference between the two forms?
The latter form is deprecated and no longer works in 0.7pre.
Thanks :)
`vec::with_capacity(n)` will create a `~[]` with space for (at least) `n` elements. (Also, see [#7136](https://github.com/mozilla/rust/issues/7136).)
Right, that makes sense - thanks for the translation. So as a type decorator in a `let` declaration, it indicates an owned pointer, and applied to a type, it constructs an instance? So you could do: let background = ~Colour{r,g,b}; and this would allocate a new owned pointer to a Colour instance, yes? Whereas: let foreground = Colour{r,g,b}; allocates the instance on the stack? Sorry for the basic questions, but this is really useful!
Great job. Thanks !!
~Excellent
`[Effect proposal][eff]` ; some link missing, I guess.
Thought I had already fixed it, thanks
I figured I should actually post this in an public place, rather than just talk on irc about it.
Bikeshedding: How about `@&lt;S&gt; T` or `@&lt;S(args...)&gt; T` after the static parameters? The existing `@T` would automatically infer `S` then.
&gt; * Syntactic similar type-, construction- and pattern-matching form, like for the build-in structural types. That's great. How would pattern matching work? &gt; * The syntax for the build-in Gc Smartpointers becomes slightly heavier (@ -&gt; @Gc), thus discouraging its use unless necessary. I disagree that this is an advantage. Garbage collection is inappropriate in some scenarios, and a convenience in others. There's no reason to pre-judge it as something that should be discouraged across the board. If heavier syntax for the default GC is the price of an improvement in some other area or of a simplification in the language, maybe it's worth it. But in itself it's a drawback, not a benefit. (Otherwise there's nothing to stop us from renaming `@` to `@YesIWantToGarbageCollectThis&lt;&gt;` independently of anything else. We can make it as heavy as we like.) I assume that having plain `@T` default to `Gc` while still allowing `@Rc T` would make the syntax ambiguous? If not it would be a pure win. Otherwise... personally I think I'd prefer to keep `@T`, while still extending first-class borrowing support and such to `Rc` and other types (just not the `@` sugar).
Sure, I'm not saying that garbage collection is bad. But in rust thanks to borrowed pointers and linear types (and their use in the std lib), today the rule of thumb for allocations ends up as: - First and foremost, use the stack. - Else if you need a heap allocation, use unique pointers. - If you have more special needs, like cyclic data structures or memory shared across tasks, use one of the library smartpointers or @. That is, tracing gc has an important place, but not more important than for example ref counting, and neither are the recommended way to deal with memory in rust, so why elevate one over the other? Oh, and pattern matching would probably work with either the existing traits or additions to them. After all, if supported by the type it's just borrowing, mutable borrowing, moving out of it, optionally with an comparison.
Yes, that was what I've intended. And I would argue that `@Rc v` is hardly an improvement over `Rc::new(v)` with that argument. ;) I guess that like a lifetime parameter, the allocation parameter should be optional (i.e. inferred) to be practical anyway, so a slight verbosity of allocation parameter syntax would not be a problem.
I'm just going with "count the symbols" as an approximation of noisiness (which is subjective). By that measure `@Rc v` (1) is far less noisy than `Rc::new(v)` (4), and `@&lt;Rc&gt; T` (3) is slightly noisier than `Rc&lt;T&gt;` (2). If you go with "count the characters" it happens to come out similarly. But symbol/character count isn't the only thing, reducing nesting is also valuable for example (on which measure `Rc&lt;T&gt;` &lt; `@&lt;Rc&gt; T` &lt; `@Rc T`, because `T` will often be `Foo&lt;Bar&lt;Baz&gt;&gt;`).
I just don't see the point adding sugar for smart pointer if they are not part of the language anymore. For me "@Gc Type" is obsolutly not an improvement over "Gc&lt;Type&gt;". It just hide the logic. It seems to me that the idea behind the removal of @ (and rust in general) was to simplify the language and this proposition go the opposite way
You raise an interesting point here that I hadn't considered: Because the type on the expression form is basically an explicit type parameter, it would make sense to make it inferable. The problem as illissius already pointed out is that that would mean making the syntax-with-type more verbose in order to not have any ambiguities, and breaking symmetry between type and expression form. But if you would do it like that, it could probably look like this: let a: @Gc = @:Gc foo(); // explicit type on type and expression side let b: @Gc = @bar(); // explicit type on type side let c = @:Gc baz(); // explicit type on expression side let d = @:(ar)Ar bluh(); // in-object-allocation syntax let e = @:(Ar ar) stuff(); // alternative in-object-allocation syntax let f = ::extra::rc::@:Rc things(); // with path 
I see it like this: Ignoring the sugar, as far as I gathered from the devs smartpointers will be special anyway. You will have a special allocation syntax for them, you can automatically borrow them, dereference them, etc. So, to my eyes, it makes more sense to give them a syntactic special case to separate them from other types that don't allow those operations, _especially_ if it means that the result is that using them works just like the build-in ones (~, &amp; where applyable).
Yeah I think I agree. I think my order of preferences is: 1. Keep `@` as it is 2. Remove `@` entirely; potentially also rename `*` to `*unsafe` and `~` to `*`. Now C/C++ programmers don't have to be confronted with any unfamiliar sigils. Yay! 3. Remove `@`, then add it back as generalized smart pointer syntax sugar in the proposed manner Basically I feel that if `@` is removed, simplifying the language syntax (potentially radically, especially for the target audience) is a bigger win than somewhat nicer syntax for library-defined pointer-like types. 
In preparation of Rust 0.7 The 0.7 release notes start to form here: https://github.com/mozilla/rust/blob/master/RELEASES.txt
Also, because many, *many* people are getting random failures (killing stack canaries, mainly) when running `make check`, to the point of not being able to continue working effectively. (This problem is clearly a blocker for 0.7, as you say.)
This is a very enjoyable technical article. I hope the author keeps investigating the cause of the performance disparity with C, because 5x slower is waaay too slow.
Yes, that was my thought too. I wonder if this 5x is the result of one or two pain points or if it is evenly spread on many little points. The former would probably be easier to fix.
 [**@metajack**](http://twitter.com/metajack): &gt;[2013-06-18 02:51](https://twitter.com/metajack/status/346822553089761281) &gt;You can surf wikipedia in Servo now. Progress! [pic.twitter.com](http://twitter.com/metajack/status/346822553089761281/photo/1) [^[Imgur]](http://i.imgur.com/NM1iSAQ.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1glt9a%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://yl.io/S) [^[Translate]](http://translate.google.com/#auto/en/You%20can%20surf%20wikipedia%20in%20Servo%20now.%20Progress%21%20http%3A//twitter.com/metajack/status/346822553089761281/photo/1) [^[FAQ]](http://yl.io/T) [^[Statistics]](http://yl.io/U) ^\(times ^are ^utc) 
SipHash is easily 3-5x slower than the fragile hashing done by most hash tables, especially since it's not optimized to use SIMD in Rust. The string/vector implementations are also ridiculously slow.
Should we add a SketchyHash option that's faster when you know the hash keys aren't maliciously trying to collide?
The Hash implementation this is using is the default, built on the derived IterBytes trait. It should be possible to implement Hash directly, shouldn't it?
I'm almost convinced that is it, since the program with the find_equiv replaced by a counter increment runs in approx. .9 sec. &gt; The string/vector implementations are also ridiculously slow. I haven't noticed anything particularly bad, what do you have in mind?
It was not too hard to find: https://github.com/cmr/rust-bench
I whipped up some [quick plots](http://www.ug.it.usyd.edu.au/~hwil7821/rust-bench/) of the memory profiles of the `test/bench`s (the passes-timing info and the memory profile seem a little bit skewed relative to each other, haven't investigated further yet). cmr and I are looking to get this info into isrustfastyet at some point. Edit: ok, I did a little investigation, and it seems that there is ~0.02s start-up time, or something (estimated by eye: the shift that made the graphs line up best, these new ones are uploaded). Edit2: cmr gave me some more data, [the same benchmarks but without jemalloc enabled](http://www.ug.it.usyd.edu.au/~hwil7821/rust-bench/noje.html).
I suggest http://en.wikipedia.org/wiki/Rust_(programming_language) or http://en.wikipedia.org/wiki/Servo_%28layout_engine%29 for Wikipedia rendering tests. :)
If brson's comments didn't settle your fears, I did much of the work porting Servo from 0.4/0.5 to 0.6, where a _ton_ of breaking changes were made. I am generally much less conservative about adding more breaking changes to Rust than the Rust team is, even though it might mean unpleasant porting for me in the future. This might become a concern eventually, but it's definitely not the case right now. The way we currently deal with it is to use versions of Rust that are recent enough to have the things we really need, but not so recent that we spend every day dealing with language changes. I think right now we're about a month or so behind master, and are currently working on upgrading to latest Rust.
Very nice. I'd be interested to see if you could further subdivide the "LLVM passes" block with the name of each pass that we're doing (unless they all take the same amount of time).
I'd like that, and the information is [easily obtainable](https://gist.github.com/huonw/76e23154d7dba3a34e5f), although it's more annoying to parse, and it doesn't lend itself to a plot against time at all. (Maybe someone knows more about LLVM &amp; what's possible with its diagnostics...)
The passes aren't chronologically ordered either.
Yup, that's what &gt; it doesn't lend itself to a plot against time at all refers to. :)
I agree that we're rapidly approaching a point where such enormous language changes won't be possible anymore (at least, not without pushing back the 1.0 release by six months). 0.7 should come out in a week or so. If the @ -&gt; Gc change, AND the DST proposal, AND the closure reform aren't all done by 0.8, I'll be very worried. Fortunately summertime is the most productive time of the year, thanks to interns.
And thanks to students even who aren't interns! ^(or students who would really like to be interns but aren't... well, I can't anyway)
very nice set of slides indeed.
affect!
I really enjoyed reading them, thanks for sharing.
About the issue with static data: [this comment describes an easy workaround](https://github.com/mozilla/rust/issues/7012#issuecomment-19160239). And I heard the debug info has been fixed some days ago. I haven't tried it yet, though, maybe it still doesn't quite work.
There's a mistake on slide 28: the inlined version iterates backwards.
Nobody likes Nebraska... Also great slides. Rust is awesome!
A Friendly Introduction? How many Python, Ruby, Java or C developers could follow everything in this "introduction"? 1%? 0.1%? If the only target is experienced C++ developers with a background in ML or Haskell, Rust is DOA. Most experienced C++ developers are not going to give up C++, especially for a language that would require them to learn a bunch of new concepts. I think the language is great but first impressions matter. Rust is going to damage itself by calling something like this an introduction. Compare the level of this presentation with a book like C++ Primer Plus.
Good C and C++ developers are experienced people, they will have no issues. For the other type of developers, I keep getting disillusioned with the level young developers show nowadays about how computers work. 
efficient and expressive i/o and scheduling. binary size is also a problem, but it doesn't affect me as much.
[5898](https://github.com/mozilla/rust/issues/5898) makes things ugly. (A lot of methods with trailing underscores because of it. :( )
&gt; Good C and C++ developers are experienced people, they will have no issues. I doubt that knowledge of C helps much when it comes to traits and generics, for instance. But set that aside. Good C and C++ developers have no interest in switching to a different language. It will have to be possible for young developers to learn Rust without having ten years experience with C++.
You've got a contributor flair, meaning you've contributed code to the Rust compiler. I don't think your definition of 'friendly' is necessarily representative of all Python and C developers.
I still barely know rust (the extent of my contributions are a terminfo parser, documentation fixes, and a simple optimization I found using `perf record`, and a few minor things I stumbled around with), I feel like I can set aside my Rust knowledge to fairly evaluate the slides. But maybe not. After all, cognitive bias is the invisible killer. (It is worth noting that though I get paid to do python and C, I also have myriad experience in java, .net, lua, javascript, and c++... so, there's that)
&gt; makes things ugly. (A lot of methods with trailing underscores Indeed, it's why they're working on language/API stability. There isn't much motivation to develop libraries if the underlying language/API will change.
&gt; Good C and C++ developers have no interest in switching to a different language. You're mistaken. Good C++ developers continue to use C++ because it is, for their purposes, the best tool for the job by far. However, good C++ developers acknowledge that the language is inherently, irreversibly unsafe (unless you don't think Bjarne Stroustrup is good at C++ [1]). They also acknowledge that static verification of code correctness is desirable (unless you don't think John Carmack is good at C++ [2]), despite the fact that C++ resists it so heavily. They also acknowledge that programming in a functional style can aid the ability to assure correctness and assist reasoning in a large codebase (unless, again, you don't think John Carmack is good at C++ [3]). See also Carmack's recent twitter feed for an overview of his exploits with Haskell (tl;dr: he loves the purity and expressiveness, but if only it was *closer to the hardware*). You also overlook the fact that Mozilla's bread-and-butter is one of the largest and most visible C++ codebases in the world, and hence is filled to the brim with good C++ programmers, many of which are employed (and enthusiastic) to work on Rust. Graydon Hoare, a very good C++ programmer, began writing Rust out of years of pent-up frustration of dealing with the correctness and security pitfalls that are inherent to C++. &gt; It will have to be possible for young developers to learn Rust without having ten years experience with C++. As a young (and bad) developer, I'm confident in my Rust knowledge despite having written about 100 lines of combined C and C++ in my life (and each of those lines were written with great trepidation). Rust is my first systems programming language (having only ever written Java, Python, and Javascript in my life), and I'm loving every minute of it. [1] http://www.stroustrup.com/except.pdf [2] http://www.altdevblogaday.com/2011/12/24/static-code-analysis/ [3] http://www.altdevblogaday.com/2012/04/26/functional-programming-in-c/
We do at least have a json library in [extra::json](https://github.com/mozilla/rust/blob/master/src/libextra/json.rs) and a syntax extension that writes a serializer/deserializer for you. You can find an example on how to use it by looking at json.rs's tests.
&gt; Good C and C++ developers have no interest in switching to a different language. I have done C++ programming since 1994, with C only when required to do so, as C++ was a better home for me as Turbo Pascal refugee. On my CS degree, one of my three major focus was compiler design, as such I got in touch with many programming languages, mostly for university related work, while keeping C++ for industry related work. Since 2006 I use mostly C++ on toy projects at home, while at work we do mostly JVM and .NET based projects. Some of them are even related to moving high performance code in the telecommunication and medicine enterprise world from "legacy C++" into those platforms. The main problem I see are the C and C++ developers that only know their world without proper CS background.
I agree that the switching costs of moving from C++ to Rust would be stratospheric (Carmack even says as much in the third link from my previous post [1]). However, the switching costs of moving from any established language to any nascent language are going to be similar. You're free to argue then that *all* new languages are pointless (and you may very well be right). However, the belief that C++'s many shortcomings are completely irrelevant because of its mature ecosystem doesn't make one a good C++ developer, it merely makes them a good Blub developer. There are plenty of good C++ developers who are interested in proving Rust's viability for the C++ audience. See Daniel Micay, a.k.a. strcat, who's one of the most knowledgeable programmers I've ever had the pleasure to observe and who's worked tirelessly to bring Rust's containers and iterators up to the efficiency of C++'s, bringing us to near-exact ASM all while retaining Rust's static safety guarantees. None of us are deluded into believing that Rust is the systems-language messiah, come to deliver all of mankind from the darkness in Bjarne Stroustrup's heart. C++ will never "die", not from *any* challenger, guaranteed 100%, and that's totally fine. For many of us Rust is still the superior option, either because we're not inextricably tied to C++'s ecosystem or because we've always wanted to learn a systems programming language but have been too intimidated by the unspeakable vastness of C++. As for Go and D, I don't believe those are comparable. All the former C++ programmers who were willing to endure pervasive GC jumped ship to Java in 1995 (I know that D doesn't necessarily require GC, but its early stdlib had a bad reputation for it and the language simply never recovered from that first impression). Not only does Rust get along swimmingly without GC, both the language and the community actively discourage its use. &gt; Rust is in a worse position because you have to know functional programming in addition to everything else. This is a meaningless statement unless you're willing to define "functional programming". :) [1] *"I do believe that there is real value in pursuing functional programming, but it would be irresponsible to exhort everyone to abandon their C++ compilers and start coding in Lisp, Haskell, or, to be blunt, any other fringe language. To the eternal chagrin of language designers, there are plenty of externalities that can overwhelm the benefits of a language, and game development has more than most fields. We have cross platform issues, proprietary tool chains, certification gates, licensed technologies, and stringent performance requirements on top of the issues with legacy codebases and workforce availability that everyone faces."*
Dear readers, please do not downvote comments simply because you disagree with them. Criticism is welcome as long as the tone of the conversation remains civil.
I'd like to express my gratitude to all of the Rust contributors that so regularly take the time to put their thoughts into publicly viewable blogposts. It's very gratifying to have this kind of insight into the evolution of the language. Not only does it allow outsiders to learn from the challenges being encountered and overcome, it also illustrates the incredibly rapid pace of development going on behind the scenes. As someone that's been following Rust with great interest since it was first publicly announced, seeing the momentum behind the language keeps me glued to the project. From the post: &gt; One major issue is that our final IR is significantly larger than it should be. &gt; However, I’m not here to talk about what is wrong with the functionality in trans. If someone could spare the time, I'd love to hear about why the IR is larger than it should be.
I'm not very familiar with `trans` but from what I gather watching IRC, there are a lot of redundant casts, unnecessary allocations, dead code, etc emitted.
Dunno, I'm a recent CS graduate with no experience in writing C or C++ (I can read them to an acceptable level) and very limited Haskell experience and I understood it fine. Do most programmers not understand how their code is transformed into what is actually run on their machine? at least at a high level, because that's all that is required here.
&gt; `~` is the malloc operator (although that may change soon to `new`). Just a quick question on this -- I'm wondering, is `new` necessary? This is one of my annoyances in C# or Java compared to C++, it's usually somewhat verbose; compare: `List&lt;int&gt; ids = new List&lt;int&gt; {1, 2, 3}; // C#` or even `var ids = new List&lt;int&gt; {1, 2, 3}; // C#` with `std::list&lt;int&gt; ids {1, 2, 3}; // C++` (and that's already giving a bit of an advantage to C#, since `System.Collections.Generic.List` would be the honest equivalent of `std::list` here -- but namespaces are not the point here, anyway; well, that, and `std::vector` being a more comparable data structure, but that's not the point here, either :]). What do you think?
Compile-time lookup table is [#4864](https://github.com/mozilla/rust/issues/4864).
Unfortunately, as I describe in my [comment](https://github.com/mozilla/rust/issues/7012#issuecomment-19768842) there this workaround does not work for my purposes (I need regular rules to generate asset data.)
There is a simple (but hard) solution. Start measuring quality of compiler output (if possible to define) and never allow it to go down just up or remain the same.
in my experience, that's seldom a good idea in large projects. it's far more productive to make things work first, and make them work efficiently later - get your idea correct, pin it down with a test suite, and then start hacking on efficiency while seeing that you don't break the tests. insisting that exploratory ideas work efficiently at every step of the way simply mires you down.
Yeah, it's mostly poor implementation right now. There's no good reason they are as slow as they are right now.
`clang` does this with IR output tests, we're just missing the infrastructure.
This would be cool. LLVM tests that the assembly looks exactly like they expect and we could too, for at least the most important cases. We have a lot of abstractions that are theoretically very efficient but not necessarily in practice. Would be relatively easy to modify compilertest to do this.
The biggest thing to me is the problems and bugs with traits and trait objects. Trait inheritance doesn't work, there is a large variety of bugs with them in general with the borrow-checker, and it's just so disappointing, because there a lot of key things that it inhibits. The thing that really brought these issues to the fore is a tiny little text-based adventure game I was trying to write as a learning experience.
I don't know if I disagree with it, because it's not constructive criticism. There aren't any reasons given for why a C/Python/Java developer couldn't follow it or examples of a concept not in C++.
It's a great presentation. My only criticism is the first slide, where you use the title "Rust: A Friendly Introduction". It's a friendly introduction for programming language researchers or programming language hobbyists. It simply assumes way too much background to be called a friendly introduction. Rust is positioning itself as a language for only the most advanced programmers, and I don't think the Rust community realizes that. A "friendly introduction" is something like: Learn You a Haskell, Practical Common Lisp, Programming in Scala, Programming: Principles and Practice Using C++. My apologies if I gave the impression that I thought there was something wrong with the presentation itself.
I agree that the original comment failed to be constructive, and I wouldn't have intervened if they hadn't shown willingness to make follow-up comments. If we have the chance to clear up misconceptions, I want to capitalize on it. Even if we don't manage to change their mind, we have a thousand other readers who might harbor the same concerns.
Seconded.
In addition to what brson wrote: * missing libraries: https://github.com/mozilla/rust/wiki/Libs * Rust on Windows badly needs some love: https://github.com/mozilla/rust/issues?direction=desc&amp;labels=A-windows&amp;page=1&amp;sort=created&amp;state=open
Can you give an example of what background you thought I was assuming?
There are also some missing containers listed here: https://github.com/mozilla/rust/wiki/Containers#wanted
I totally agree, the old saying goes *"Make it work. Make it right. Make it fast"*. Still, if the developers are considering technical debt, then perhaps the time for optimization is nigh? 
yes, and they're doing it! but even for paying off debt you can't insist on a monotonic metric; sometimes you have to ruthlessly hack and slash code en route to your new stable point.
I'm also very interested in Rust being used for game programming. It's why I tried to create a game a number of months ago, just to get a feel for the language. The result was a simple somewhat pong-like game: https://github.com/FrozenCow/rust-airhockey At the time the language was in the middle of, what seemed like, a number of changes. I stumbled upon a lot of difficulties that were sometimes fixed just a day after I build Rust from incoming branch. I received a lot of help on IRC, so I could in the end finish somewhat of a game using SDL, OpenGL and Rust. I think the language itself has stabilized a lot more, but expect quite a few breaking changes. You might need to refactor things once in a while to be able to build your game with a newer Rust. Also libraries and APIs are often lacking... I'm just guessing that Rust (at this moment) is not practical if you want to create a full-on game. That being said, Rust is pretty amazing and I don't want to discourage you. I'd suggest to just try to create a smallish game. Not only will you learn the language, find difficulties, etc, but you'll also make another example of a game in Rust. This is a useful contribution. If you also document the difficulties you found, it would help others have a go at Rust and maybe even the developers in knowing where difficulties occur.
&gt; Rust does those five things, too. *Will do* is closer to the truth. And I understand Rust does not intend to have much if any template metaprogramming, most of these use cases should be replaced with rust macros. And the current level of performance is often not very good.
&gt; Rust's stdlib is anaemic at the moment Not like C++'s is very impressive, and part of it (e.g. `&lt;memory&gt;`) are part of core rust (not rust core::). Though diffing the C++ standard library and rust's might be an interesting exercise.
I'll try to address each of your points as best and honestly as I can. I may have got some things wrong, but I'm sure my friends will correct me. :) **Amazing performance** Rust uses LLVM as a backend, and so benefits from the millions of man-hours put into optimizing it. Runtime performance is slower than C, but there hasn't been much work put into optimization, so this is expected to improve. In some benchmarks Rust has been shown to be [competitive with C](http://pcwalton.github.io/blog/2013/04/18/performance-of-sequential-rust-programs/), which is quite encouraging considering the current [issue's with rustc's generated IR](http://www.reddit.com/r/rust/comments/1gs93k/rust_for_game_development/canc0y3). Theoretically, Rust might be able to even beat C or C++ in some instances because the compiler has access to far more type information, and therefore has more 'wiggle room' for optimizations. But I'll stress that this is still only theoretical. Rust also monomorphizes generic code and can perform loop unrolling and cross-crate function inlining. **Manual memory management** Rust already beats C++ on this front. It offers the same fine-grained memory management, but is safe by default. Pointers and lifetimes are built into the type system giving you compile time safety without the cost of GC. Safe pointers are also non-nullable, eliminating an entire class of bugs. You can always open the bonnet though, and do naughty stuff within `unsafe {...}` blocks, but that is rarely necessary, mainly for C interop. **Template metaprogramming** This is where things are more difficult for Rust. The language has parametric polymorphism (ie. 'generics'), but unlike C++ or D, parameter lists have a fixed arity and cannot include constants. Rust's compile time computation is limited to mathematical arithmetic. Functions cannot be executed at compile time like in D, or with C++11's `constexpr`. This capability [might be added in the future](http://www.reddit.com/r/programming/comments/1gpyor/phoronix_d_language_still_showing_promise/can5q67), but I highly doubt it is on the list of priorities for 1.0 (don't quote me though). Rust does however have traits which can take up the slack in some instances. They have been likened to C++'s mythical '[concepts](http://en.wikipedia.org/wiki/Concepts_%28C%2B%2B%29)'. They are already very useable, but will be even better once default method implementations are fixed and associated items are added. Unlike the C preprocessor's [textual substitution macros](http://c2.com/cgi/wiki?TextSubstitutionMacros), Rust also has '[Real Macros](http://c2.com/cgi/wiki?RealMacros)'. These can help to reduce repetitive code and be used to create mini DSLs. There are currently some bugs and limitations with them, but they should be fixed in the mid term. You *can* abuse macros in order to go some way to simulating templates, but this is not recommended and leads to brittle, difficult to maintain code (from personal experience). **Everything acts as a primitive type** I am less experienced in this area. I believe Rust moves by default and gives a compile time error if the moved value is attempted to be used later on. Rust's data types map to machine memory predictably (except for enums, I think), and can be easily used for interoperability with C. **Standard library** There has already been a great deal of good work put into the standard libraries, but there is much more to do. The language has evolved considerably, and some APIs need to be modernized. Documentation is good in some areas and sparse in others. This is a low-hanaging fruit for new contributors, so help is most appreciated. **Some things that C++ does not have** I have mentioned some of these above: - Powerful, [Hindley–Milner](http://en.wikipedia.org/wiki/Hindley-Milner) derived type system - Algebraic data types - Immutability as default - Pointers (except unsafe pointers) cannot be null - Light-weight, task-based concurrency - Hygienic macros - Modules - Traits **Conclusion** Rust will be (and in some places, already is) an excellent programming language for high performance game and graphics development. But it is not ready for production use yet. I would however, highly encourage you to check it out and experiment. The [tutorial](http://static.rust-lang.org/doc/tutorial.html) is a great introduction to the language. Some of the wording might be out of date, but the code samples are linked to the testsuite and so should compile. I would also encourage you to look at [Tim Chevalier's slides](http://catamorphism.org/Writing/Rust-Tutorial-tjc.pdf), because they seem to be targeted at systems programmers such as yourself. A video of the presentation is apparently on the way. The community is also excellent. Come over to irc.mozilla.com #rust and have chat, we don't bite! 
Awesome reply, very informative. Thank you for the time you put into writing it! You've convinced me to play around with Rust :)
Can you elaborate on the "performance not being very good" part? I understand that Rust uses LLVM optimizations - isn't that what Clang uses too? Surely it cannot be much slower than C/C++.
&gt; I understand that Rust uses LLVM optimizations - isn't that what Clang uses too? Surely it cannot be much slower than C/C++. LLVM works on an intermediate representation (LLVM IR) and the backend optimizations are applied to that. There's only so much this can do, and your assertions only work under the assumption that the frontends generate roughly the same IR. Currently, that is not even remotely the case, as various messages explain in the mailing list thread [code generation and rustc speed](https://mail.mozilla.org/pipermail/rust-dev/2013-June/004480.html) there are systemic issues in rustc which generate way subpar LLVM IR. And thus instead of needing to summarize a 50 pages essay, LLVM ends up trying to summarize a whole encyclopedia (mostly) full of pointless crap. It may be possible in theory, but in practice that's not really what it's been built for and things don't really end well.
There's a bug open for this: https://github.com/mozilla/rust/issues/5073
I've been playing with Rust SFML bindings lately, it's been great so far. https://github.com/JeremyLetang/rust-sfml
The issues with performance right now aren't an inherent property of the language, they merely represent a lack of time spent on optimization. I fully expect the Rust compiler to produce code that's near-identical to Clang after we're done turning the screws.
&gt; rather than problems with the Rust language and the code that rustc generates. In the thread I linked, it is repeatedly noted that the generated IR is fairly terrible. &gt; This has a positive takeaway though: the IR rustc creates can only improve, so Rust (and rustc) can only get faster. Oh absolutely, my comments were not intended to drive anybody away from rust, merely to lower expectations of the current implementation, especially compared to e.g. http://www.reddit.com/r/rust/comments/1gs93k/rust_for_game_development/canbbl8 which I find... over-eager.
&gt; Rust's data types map to machine memory predictably (except for enums, I think) Yes, the representation of enums is deliberately left unspecified so as to optimize the representation based on the shape of the enum. For example, `enum Foo { Bar, Qux(int) }` can be represented as merely a single nullable pointer to an int at runtime.
Yeah, I'm not disagreeing that the IR rustc passes to LLVM is terrible, but LLVM seems to clean it up very nicely in many cases, it just takes far, far longer than necessary, i.e. the bad IR is making LLVM do much more work than it needs to, but it gets a (slightly) decent result in the end.
Thanks for that. But don't you mean: `enum Foo { Bar, Qux(&amp;int) }`?
Great! If you have any issues let us know. :) Oh, and I'd also recommend keeping up to date with the master branch rather than using 0.6. The language has improved greatly since then.
That may have been a bad example, because I'm actually not sure if it does that optimization if your tag is something as small as an int. But `enum Foo { Bar, Qux(MyHugeStruct) }` should get that treatment (but you'd have to ask someone like strcat who knows for sure).
I believe it only happens for types isomorphic to `Option&lt;pointer&gt;` i.e. `Option&lt;&amp;T&gt;` if `T` is not `[]` or `str`, and `Option&lt;~T&gt;` and `Option&lt;@T&gt;` for any type `T` (including `[]` and `str`). This is because rustc needs to have a "spare" value that it knows will never occur, and the only such value (at the moment) is `NULL` for non-unsafe pointers.
As somebody that's spent quite a bit of time with codegen and optimization in rust, I can say that the poor IR has a much bigger impact on compilation time than the actual, optimized code at the end. Barring a few cases, most code in rust optimizes down to equivalent to C++. This is evidenced by the fact that most recent speed increases for the compiler have been from improving the IR output, not from code-level optimization. 
I have been thinking about this recently and about the structure of game development in rust. Here are my incoherent thoughts on multicore game engines in Rust. If you look at structure of games at the some big name engines such as [Valve's Source (PDF)](http://www.valvesoftware.com/publications/2007/GDC2007_SourceMulticore.pdf) and [ID's Doom3 BFG](http://www.fabiensanglard.net/doom3_bfg) there is elegant way of designing game engines that maps directly to idiomatic rust without the complex code. If you look structure, they divide the engine into two parts. The Front end, where rendering and input is. One of the main reasons rendering is done here is because ["on windows, OpenGL can only safely draw to a window that was created by the same thread" (John Carmack, 2012)](http://www.fabiensanglard.net/doom3/interviews.php). Back end, the logic centre. This where everything from level management to AI can be done here. What about the glue which binds the two ends together? The answer: pipes. Rather then a lock-free queue (Fober, et. al.) or using a [ring buffer](http://www.fabiensanglard.net/doom3_bfg/threading.php) you can use the [tasks and pipes](http://static.rust-lang.org/doc/tutorial-tasks.html). This architecture is not only limited to games but to all resource intensive programs like [Servo](https://github.com/mozilla/servo/blob/master/src/components/main/servo.rc#L111). So why Rust? C++ is pain to write concurrent code right. C++ threadpools are can pain to write. When you need to panellize code sending the information to the right job queue can take 100's lines of code that in reality only does 1 thing. There are times in c++ code where you write so much boilerplate when all you need is a simple function. With Rust there is a lot of stuff built in out of the box. You don't need to reinvent the wheel. :-) Now to actually answer your question directly. &gt; "Is Rust currently suitable for game development?...Should I wait for the language to become more mature, or should I dive into it?" Syntactically: yes. If you are not an AAA developer wanting to make it for every console that happens to trendy yes: Yes. Rust is currently in Beta and is not recommended for production but that is not bad. It means you can set a trend. You can be first. You can use Rust to design better algorithms that would be a nightmare in other languages. You should not be afraid to fail but be afraid to not try. To be honest most indie games now days don't need to "Amazing performance". This is era where we can be a few wasteful cpu instructions aren't going to slow the world down. You should choosing languages that are designed for the right domain rather than saying "WRITE ALL THINGS IN C++".
Yes, I'm dumb, I totally forgot to put the pointer in. :P
&gt; Rust is currently in Beta and is not recommended for production but that is not bad. OP be warned, we're not actually in beta. We're not even in alpha. If you use Rust for one day, you *will* end up longing for features that have yet to be implemented, you *will* wrestle with legacy features that we're in the process of excising, and you *will* crash the compiler a dozen times by doing things that usually work but sometimes don't. Furthermore, our libraries are largely inadequate and we make breaking changes weekly. So yes, Rust is awesome, but unless you have an infinite supply of patience for dealing with the quirks of a partially-baked project (some of us do!), I would have to recommend waiting a few months before embarking on any large endeavor.
And it's not so much optimization at this point as removing the useless, accumulated cruft.
Aha! Thanks - looks like I didn't dig deep at all enough! I've read up on the wiki some more, and picked an "easy" bug to get started (removing uv from libextra). The I/O stuff sounds interesting, and especially the concurrency. Will have a look once I've had some more experience. Thanks for all the info...
Cool, those would make some great small projects.
Yup. Especially if this included `bool`/`u8`/any type smaller than the target's word size. (Also cool would be collapsing `Option&lt;(float, ~int, Foo, Bar)&gt;` to make the internal pointer nullable, and use it as the discriminant.)
You mean use a single tag for multiple enums? Wouldn't that break if you were to take a reference to one of the inner enums?
Thanks for the useful summary, it's particularly great to know that the work on trans has started.
Thanks for the write up; it really helps the casual user (like me) keep up with Rust development. 
That's the goal! :D
It never really stopped, it's just more focused and deliberate right now.
I get `for x in expr { ... }` from python and lua, mostly. My biggest problem with the `||` syntax is that *it is not a closure* with the new semantics. I think it is confusing to use the same syntax as closures with non-closures.
How about `for foo.bar().baz() patt { ... }` just like `if expr {}` and `match patt {}`?
Or even just the past year.
Or even just the past few months. (Although most of the changes are less visible, since the syntax has mostly stabilised.)
Is that unambiguous?
I don't know. On second thought I can't see how that syntax would even work. It just looked good. Usually in `for foo.bar().baz |qux| { ... }` baz expect a closure as the last/only argument? So what would it mean in `for foo.bar().baz() patt { ... }`? Why is baz called with parentheses? I do agree with you that the proposed syntax is confusing in pretending that the code is the closure when it is not. 
Nice article, good encouragement to get involved.
Thanks. The first of many about `rustc`, mostly aimed at rust beginners but relatively experienced developers (ie, not new programmers) 
Some of these have forgotten what they are supposed to be testing. "for each": http://brson.github.io/archaea/foreach-nested.html
Great post. I put this link on the wiki in a few places. We should make a prominent page on the wiki about getting involved.
I moved 'picking something interesting to do' section to its own page and linked it from the wiki home page: https://github.com/mozilla/rust/wiki/Note-guide-for-new-contributors. Please add to it.
Sure, but it isn't like C/C++ where undefined behaviour is intentional, right?
I don't know rust all that well, but I'd say yes. Unsafe blocks for starters could contain code with undefined behaviour (invalid pointers etc).
Good, this is what I hoped. Programming in C/C++ is a nightmare due to undefined behaviour :/
You hoped that Rust had undefined behaviour?
I'm not sure that counts as undefined behaviour; at least, not in the same style as undefined behaviour in C/C++, e.g. there is no equivalent to sequence points (since there is no need), so, this is undefined in C/C++: i++ + i; but the equivalent in Rust (assuming a method `.plusplus` that increments and returns the old value): let mut i = 0; i.plusplus() + i; is *not* undefined, since there is no magic `++` operator. (This is true even in `unsafe`.)
There are many things in C++ that produce undefined behaviour other than sequence points. For example, race conditions in multi-threaded code, invalid pointer access and so on (search http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3690.pdf for "undefined behaviour"). As I said, I don't know Rust that well but my understanding is unsafe blocks are isolated bits of code that interact with C/C++ that could exhibit undefined behaviour. From what I've learned of Rust outside of unsafe blocks, it appears to provide less opportunities that C/C++ to write code that exhibits undefined behaviour.
I hoped it didn't :P
Is this basically automated smart fuzz testing? (Like http://codemonkey.org.uk/projects/trinity/, which is the only fuzzer I have real experience with)
Yep
It never sleeps.
Nice! Do you have any plans to implement shrinking? In my experience, shrinking has been incredibly useful for identifying and understanding bugs more quickly. I might even go so far as to say that shrinking is a necessary feature (planned or otherwise) before you can claim the QuickCheck name. But YMMV I guess.
The shifts sound like "unspecified results", which is not nearly as bad as the C concept of "undefined behavior".
Depends a bit on what you mean by "automated smart fuzz testing". It's smart like trinity, in the sense that it's not purely random: trinity is guided by some extra knowledge about how to make syscalls, and this mockup and quickcheck are guided by type information. However, IME the term "fuzz testing" is usually applied to tests that check for crash bugs. But quickcheck goes beyond crash testing to correctness testing. You supply "properties" that are supposed to hold for the results from the code that you're testing. Then quickcheck checks that the properties hold for random inputs to the code under test.
To be fair, `plusplus(i)+i` isn't undefined in C++ either, it's just unspecified whether it results in 0 or 1. Does rust specify that?
It should be possible, but I'm not sure if it can be as elegant as in Haskell.. We'll see It should be the next step, but I'm afraid it will require Clone on every value. So far I've worked with the API to get something that works without macros. `config.size(100).trials(20)` seems to be the good way to express configuration as a single expression.
Using a generic function is how this is done in rustc: see src/librustc/util/common.rs.
As dbaupp said, there's no `++` operator in Rust so there's nothing to specify with respect to that. If we actually construct the program: trait PlusPlus { fn plusplus(&amp;mut self) -&gt; Self; } impl PlusPlus for int { fn plusplus(&amp;mut self) -&gt; int { let oldval = *self; *self = oldval + 1; return oldval; } } fn main() { let mut i = 0; let x = i.plusplus() + i; println(fmt!("The value is %i", x)); // The value is 1 } ...we see that the end result is 1, and I would expect to be able to rely on this behavior.
Why would you expect to rely on this, given that rust already contains unspecified behavior when doing things like bit-shifting?
Waylaid is one way of putting it... I'm hoping I tangent off enough so I come back around full circle. 
[Link, for the interested](https://github.com/mozilla/rust/blob/master/src/librustc/util/common.rs#L21).
Maybe there's some low-level reason why I'm wrong, but the logic here seems very straightforward to me: 1. The `+` operator expands the expression to `i.plusplus().add(i)` . 2. The subexpression `i.plusplus()` evaluates to 0, with a side effect of assigning the value 1 to `i` . 3. The expression is now effectively reduced to `0.add(i)` , which is evaluated with the value 1 for `i` . I don't see how there's any room for interpretation without throwing determinism out the window. Do you happen to know why C++ leaves this unspecified?
[Rust specifies* operators evaluate left-to-right](http://static.rust-lang.org/doc/rust.html#operator-precedence), so, yes, it is specified that the result is 1. *As much as anything in Rust is specified.
True.
I would suggest clarifying the wording by adding something like "operands are evaluated from left to right". I read it as specifying grouping (i.e. in `a+b+c`, `a+b` is evaluated first), not evaluation order.
Hm, I possibly misread it.
C++ allows operands to be evaluated in any order. This is an optimization in expressions like `i+plusplus(i)+i`, where you can choose to load `i` after calling `plusplus` instead of both before and after (which would be required when going left-to-right). You can see [here](http://gcc.godbolt.org/#%7B%22version%22%3A3%2C%22filterAsm%22%3A%7B%22labels%22%3Atrue%2C%22directives%22%3Atrue%2C%22commentOnly%22%3Atrue%2C%22colouriseAsm%22%3Atrue%7D%2C%22compilers%22%3A%5B%7B%22source%22%3A%22int%20plusplus(int%20%26i\)%3B%5Cn%5Cnint%20unspecified(int%20i\)%5Cn%7B%5Cn%20%20return%20i%2Bplusplus(i\)%2Bi%3B%5Cn%7D%5Cn%5Cnint%20left_to_right(int%20i\)%5Cn%7B%5Cn%20%20int%20a%3Di%3B%5Cn%20%20int%20b%3Dplusplus(i\)%3B%5Cn%20%20int%20c%3Di%3B%5Cn%20%20return%20a%2Bb%2Bc%3B%5Cn%7D%22%2C%22compiler%22%3A%22%2Fusr%2Fbin%2Fg%2B%2B-4.8%22%2C%22options%22%3A%22-O2%22%7D%5D%7D) that the left-to-right assembly code is longer than the unspecified-order code, because it reorders the expression to `plusplus(i)+i+i`, which is then optimized to `plusplus(i)+2*i`, which not only removes a load but exploits the fact that on x86 there is a special instruction for calculating `a+2*b`. The ordered code can't do that since it has to load i twice, and each load might have a different value.
Brilliant. The generic function approach is perfect for what I need.
Ok shrink is done.. for simple types. Now.. can you help me express this in a more elegant way, so that it's easy to impl trait Arbitrary for user types? It can now reduce an invalid utf8 example like the bytes ~[122, 192, 179, 22, 24, 12] to the minimal example ~[192, 128]: Falsified with value '&amp;~[122, 192, 179, 22, 24, 12]' rust: task failed at 'QCheck falsified (130 trials) with value '~[192, 128]': '|v: ~[u8]| { if std::str::is_utf8(v) { v.iter().all(|&amp;c| c != 192 &amp;&amp; c != 193 &amp;&amp; (c &lt; 245)) } else { true } }, qc.rs:502'', qc.rs:52 Cover your eyes, [this implementation has *tons* of allocations](https://gist.github.com/anonymous/008e74e0adf396a348a9)
That example is a bit misleading, as it implies that `doesnt_change_i()` is somehow enforcing referential transparency (at least for `i`), which we don't have a mechanism for. To clarify, if `i` is immutable, then we *know for sure* that calling any (not-unsafe) function cannot mutate `i`, and that's what makes the above optimization possible.
Oh man, do I know the feeling :)
These two comments speak highly of Rust, moreso than you might think at first blink. The difference between the original, "Yah, I think I can add that" comment about a nontrivial feature, and "Should work now" minimum-viable-implementation was only 4 hours. I'll assume you worked on it for a reasonable percentage of that, but even still -- in it's still nascent state, Rust gave you enough tools to get that done in a highly efficient manner. Just a thought.
&gt; p: strcat and I have done a survey of different languages, and most of them use external iterators in some fashion. Ruby and ML are the exceptions. Haskell uses iteratees, don't know how they work Haskell uses "internal iterators" (normal HOFs) for the *vast* majority of cases (i.e. everything in-memory, i.e. nearly everything you'd use a `for` loop for in Rust). The most relevant prior art here is probably the [`Functor`][1]/[`Foldable`][2]/[`Traversable`][3] hierarchy (but see also [lens][4], which generalizes it further, if you feel like you have sufficient protective gear around your brain). [1]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Control-Monad.html#t:Functor [2]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Foldable.html#t:Foldable [3]: http://hackage.haskell.org/packages/archive/base/latest/doc/html/Data-Traversable.html#t:Traversable [4]: http://hackage.haskell.org/package/lens WRT iteratees, in the original formulation basically no one understands them. The use case they solve is doing streaming IO in a functional, referentially transparent, performant way while controlling allocation/deallocation of resources (like file handles). The problem in Haskell's case is that while you can write file I/O imperatively it's not "nice" (or more to the point: composable), and the "quick hack" method of lazily loading file contents as they're needed during the evaluation of pure code, besides being distasteful, isn't suitable for situations where you can't wait around for the GC to free your handles. Given that Rust is neither of functional, referentially transparent, or non-strictly evaluated, it doesn't really have this problem. There were several significantly less Oleggy iterations on the iteratee concept later on, of which the most approachable (but less suitable for serious use) was probably the original [pipes][5]. [Later versions][6] advanced the state of the art tremendously in terms of generality, expressiveness and actually solving problems, at the cost of *gahhhh too many type variables*. (No idea how this relates to what Clojure has.) [5]: http://hackage.haskell.org/packages/archive/pipes/1.0.2/doc/html/Control-Pipe.html [6]: http://hackage.haskell.org/packages/archive/pipes/3.3.0/doc/html/Control-Proxy-Tutorial.html
(FWIW feel free to use me as the go-to guy for "how does Haskell do things" if I can be useful in that way, and there isn't already someone else to fill the role.)
Indeed. http://en.wikipedia.org/wiki/Undefined_behavior E.g. random number generators should produce unpredictable results, but their behaviour is quite well defined (never crash, no side-effects etc.).
Undefined behaviour in C doesn't just mean unspecified behaviour. They use the term implementation defined for that. A program doing something qualified as undefined behaviour is not actually valid C, and the compiler is free to *assume* the undefined behaviour never happens. For example, consider the following `is_zero` function: bool is_zero(int x) { x + INT_MAX; return x == 0; } C qualifies signed integer overflow as undefined behaviour, so the compiler can assume *it never happens*. Therefore, `x` must always be zero if this is valid C, and the compiler can safely optimize this to always return 1.
So Rust will have `for` for external iterators, `while | loop` for plain old loops and `do` for closures? Three different loop protocols seems a bit much. Is `do` really necessary?
`do` isn't a loop protocol, it's just a way to make higher-order functions look more like C-style control structures. // This isn't a loop! do spawn { println("Hello from a different task!"); } // It's the same as this! spawn(|| { println("Hello from a different task!"); }); As someone who constantly forgets the `);` after my callbacks in Javascript, I absolutely adore the `do` construct. EDIT: Furthermore, this change to `for` actually makes us much less idiosyncratic. Whereas our old `for` loops were basically unprecedented and a constant source of confusion, our new `for` loops will be immediately intuitive to anyone with a Python background.
`do` isn't a loop, and `while`/`loop` aren't a protocol. Heck, the proposed `for` isn't even a protocol, it's just sugar. The only loop protocol I know of is the janky current one. Maybe we have differing concepts of "protocol" though. I see protocol as an API the language itself uses to implement features.
It can be "loopy", like `do [1, 2, 3].map |&amp;x| { x*x }`, in that it executes the passed closure more than once, though. But it's just syntax sugar for passing a closure, it doesn't have provisions for `break`/`continue`, so it's not really a loop protocol.
Actually, I think the upcoming `for` *is* a protocol. IIRC strcat's idea is to only have it work with anything that implements either `Iterable` (the `.iter()` method) or `Iterator` (the `.next()` method). This seems to be exactly how it works in Python (EDIT: though obviously we enforce the protocol statically, rather than via duck typing like Python). // works because vectors implement Iterable for [1,2,3] |x| { ... } // works because the return type of .zip() implements Iterator for [1,2,3].iter().zip([4,5,6]) |x| { ... }
&gt;p: can we switch to new scheduler? &gt;b: not in the tree. no one has reviewed it. &gt;b: bounced in and out for a month. anyone who wants to look at it is welcome. only about 6K lines of code. Is this true for anyone? If it is I'd like to take a look (purely because I am curious not because I want to actually review it, officially).
Yes. And 6k was an overestimate.
VB.NET and C# do something similar- you can foreach anything with a GetEnumerator method (even if that thing doesn't for whatever reason explicitly implement IEnumerable).
I think you're looking at two separate rustc bugs. Redefining add_child and set_parent like so works around the Some(self) issue: fn add_child(&amp;'self mut self,mut node: Node&lt;'self&gt;){ node.set_parent(&amp;self); self.child.push(node); } priv fn set_parent(&amp;mut self,p: &amp; &amp;'self const Node&lt;'self&gt; ){ self.parent = Some(*p); } As far as the borrow checker errors in main are concerned, I believe the borrow checker is still block-based and not flow-based -- I've ran into this issue before but forget what the workaround was. Can one of the Rust devs confirm?
I'm not sure why "internal" is not a good name. It spells out where the iteration happens relative to the iterator object: internal iteration happens within the iterator and is controlled by it, external iteration happens outside the iterator and is controlled by the caller. You can have external iterators using callbacks if that's how you roll: change `next` to take a closure instead of returning a value.
&gt; This seems to be exactly how it works in Python FWIW in Python an Iterator is an Iterable (it has a `.__iter__()` which essentially returns itself): http://docs.python.org/2/library/stdtypes.html#iterator-types
I agree. The argument against it on the mailing list is that it would be a breaking change, but i don't agree at all. It seems to me that the proposed syntax is the breaking change It seem to me possible to have the "for x in iterable" to live along the old syntax, so it would not be a breaking change and the switch would be easier.
It's not just block-based anymore, assuming you have an up-to-date compiler.
*There is no new syntax*, the *only* thing changing is semantics. There's no reason to have the current semantics, internal iterators are all but gone, the switch is done. Changing the syntax requires more work, and nobody agrees on what it should be yet. And 0.7 is due in 3 days. It'd be awful to still have the `advance` hack required ('specially since internal iterators compile so slowly).
In my case, "cursor" vs "callback" was clear without further explanation. As an outsider looking in, I needed someone to either write the couple of sentences you just wrote (thanks!) to initiate me or for a more transparent naming convention to be used.
Good point. In Haskell data isn't just data, it's either data or arbitrary code to produce the data. In a sense the data itself is the iterator, and HOFs are all iterator transformers.
Oh awesome, I must have missed that commit. When did that happen?
Don't worry, very soon you'll never again need to know the difference between internal and external iterators in the context of Rust. :)
"Imagine never hearing the phrases 'internal iterator' and 'external iterator' again" (INHTPITAETA)
Flow-sensitivity landed sometime in the first half of the 0.7 cycle, with the advent of the new borrow checker.
Still not convinced that `+` expanding to `lhs.add(rhs)` is preferable over `add(lhs, rhs)` :(
Do you mean that having the operator-overloaded syntactic sugar for `+`/`.add`/`add` isn't worth it, and you would rather write `add(x, y)` than `x + y`?
Oh, no, I worded that badly. I just feel like addition (and most binary operators really) should desugar into something symmetrical and not have one of the operands as the privileged method receiver/implementor of the addition trait and the other as a plain old argument/type parameter to the trait. I haven't really played with operator overloading a lot, but (vaguely) coming from Haskell it feels all weird. :)
Regarding the borrow check errors in main: Whenever you have `&amp;'a mut T&lt;'a&gt;` as a parameter, the caller can't know that the borrow doesn't escape into itself, so the borrow is indefinite. In this particular case, the borrow is escaping, but it's as an `&amp;const`, so it can coexist with other borrows. The caller doesn't know this, though, unless you explicitly make the valid mutable borrow available to the caller after the function returns. You can do this by returning it or using double indirection. (Note: I haven't been able to test this, but I think it would work). I'm not sure whether the compiler is intended to inspect how a lifetime is used (mut, const, immut). I suspect it isn't, in which case this is the expected behaviour. The only way I can see this being resolved is to introduce const lifetimes which can only be used in const contexts, or eliminating const entirely.
That should be the release criterium.
after fleshing out quickcheck for a bit, it's a bit more removed from Rand. I think I'll stay with this in library-land, because compilers are hard (preconcieved notion), and the crazy implementation of lazy lists is probably not going anywhere near anyone's real project unfortunately.
As far as I understood the new external iterators will be based on internal iterators rather than closures. The new syntax looks really nice, but will it be just as fast for numerical simulation code such as matrix multiplication? Can you use the new iterables to generate the same machine code?
Theoretically, internal iterators and external iterators would be equally fast (at least, for the subset of things that internal iterators can do). In practice, our new external iterators are actually *faster* than our old internal iterators, because of a few bugs related to closure inlining. Furthermore, external iterators are much simpler to optimize than internal iterators, so compilation times should be reduced. The fellow in charge of the external iterator initiative is very familiar with all of C++'s iterator libraries and has been checking our machine code to make sure that we're on par with Clang. Fast iterators are definitely a priority.
Rust is fantastic, and has great potential. But I don't know yet if the speed is close to C... On [0install benchmarks](http://roscidus.com/blog/blog/2013/06/09/choosing-a-python-replacement-for-0install/) it was kinda slow... Nevertheless, it isn't yet even in 1.0 - give it some time and I bet it will be really great to use in production!
As you say, it's not 1.0 yet. [The previous discussion about that article](http://www.reddit.com/r/rust/comments/1g1dw1/replacing_python_candidates/) explains why it's slow: &gt; The problem with Rust is, again, the stack switching. We really need Niko's FFI stuff to land. [[#6661](https://github.com/mozilla/rust/pull/6661)] &gt; He's also testing 0.6 [this is many months out-of-date, and literally about to be superseded] (Also, I could imagine that the JSON parser isn't particularly optimised, but I don't know this.)
Unless you want to live with both options forever, making a breaking change now would be better.
How can i play this game? i really want to play after watching some videos on it.
patience*, patients are for doctors. Unless, you consider yourself a doctor of code, and bugs your patients...
ups... haha nice catch.
&gt; Is Rust stable enough that I can write the game with relatively few breakages? It depends what you mean by "stable enough". If you mean, "programs compiling for more than a few days after they were written", then no. &gt; If I were to use Rust, should I use the .6-4 version shipped with my distro and get updates as the package manager sees fit, or use the nightly version? [...] And even further, how often should I update the nightly? Once a week? A day? The released versions aren't "feature releases", they're just whatever was in the tree on the date that the release was due (this is by choice), and Rust moves *very* fast, so using the releases leaves you a long way behind. Some people have had success building from master every week, which gives a more stable platform to develop against, but still gives you the latest and greatest features relatively soon after they are implemented. &gt; IsRustFast website seems to say that performance sometimes absolutely plummets on some commits [IRFY](http://huonw.github.io/isrustfastyet/) is essentially so noisy as to be unhelpful (for something to have a chance of being a legitimate performance regression, you need to see a "long term" regression, not just one or two commits that are slower than normal). &gt; What about Rust is broken at the moment? No, it works mostly fine. The 1300 issues are often library issues, or things that can be worked around. (As proof, rustc is written in Rust, and the compiler mostly works.) &gt; Anything to look out for regarding performance? Libraries are slow, FFI can be slower than necessary; both of which are (hopefully) going to be fixed. &gt; if number &lt; 64 { return str::from_byte(encodeTable[number]); } Were you compiling with `-O` etc? Also, Rust strings are UTF8, so there is an extra branch in there to check the byte is valid (assuming the C++ strings are not UTF8), and, finally, if you were using 0.6, then performance has improved by using LLVM better. &gt; Where should I look to stay up to date with all the changes that occur? There's the This Week In Rust blog and the manuals (that if I'm not misunderstood are constantly kept up to date?), anything else? TWiR is probably the best source. (The manuals aren't quite up-to-date/correct, especially with respect to best-practices idioms.) &gt; what's the overhead of calling these? Fairly large, but there's [an attempt](https://github.com/mozilla/rust/pull/6661) to remove much of it, and adding the `#[fast_ffi]` attribute to the extern functions (i.e. `extern { #[fast_ffi] fn foo(); }` ) helps too. &gt; How is Windows support? Specifically 64 bit. No [64-bit support](https://github.com/mozilla/rust/issues/1237) yet, and Rust has to use mingw (I don't know why though). &gt; Finally, what's up with the @ and GC? [...] I have no idea how prevalent the @ pointers really are. Well, you can literally avoid using `@` at all, except for `io` code (A bug. This is changing with the new runtime.). And, if/when GC moves to libraries, it will not be a bigger breakage than the ones that already happen semi-regularly. --- This is all a bit doom-and-gloom; but using Rust is fun, and it's really exciting to see the language blossom before your eyes.
By the way, inferring that you're using Arch Linux from the `0.6-4` version, there's a repository with daily builds of master: [thestinger] SigLevel = Optional Server = http://pkgbuild.com/~thestinger/repo/$arch I definitely recommend using it instead of the tagged releases since we don't backport any fixes to releases at this point.
(Could you format your comment nicer, thanks. (You probably need more blank lines between the `&gt;` quotes and your text.))
&gt; What do you mean by this? I don't quite understand, do you mean ALL libraries, even rust libraries are slow? Why would that be? Just unoptimized compilation? No, just much more effort has been put into implementing features than optimising the libraries (for the most part), so they are minimal working implementations, rather than fast working implementations. &gt; So are Go's, actually. That's why I thought the performance regression was so weird Yeah, I know, that's why I only compared to C++. Some aspects Rust's string and vector libraries are particularly bad from a performance point of view, but `from_byte` doesn't look horrible... so I don't really understand what's going on. &gt; Might I ask why that's not default? Compatibility or stability I assume? The latter, until the any/all the bugs have been ironed out, I think. &gt; The link makes me think that the compiler is 32-bit only? It is 32-bit only on windows; it works fine for 64-bit linux and mac. &gt; I'll make due note of where each instance is so I can update accordingly. The compiler will almost certainly complain at you when a breaking change is made, so there's probably not a need to do this (obviously you're allowed to if you want). (Also, as MaikKlein said, [join us on IRC](http://client00.chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust), there's almost always someone willing to help.)
Have you seen [rust-sdl](https://github.com/brson/rust-sdl)? (And the libraries that [Q\^3](https://github.com/Jeaye/q3) uses?)
This will become a lot more comprehensive + gain lots of examples (the container part is bland at the moment), but I wanted to get some documentation on iterators for 0.7. Suggestions are welcome!
Yes, I'd seen rust-sdl -- but I've read that SDL2 is much better for 3D graphics (which is what I'm interested in) than 1.x is. But thanks for pointing me towards Q^3 -- I hadn't seen that before!
Awesome! I'll make sure to talk to you (tomorrow).
where do i get a beta key at?
This subreddit is for the Rust programming language, not for Garry Newman's game :(
From what I can tell [on the Manjaro wiki for a different repo](http://wiki.manjaro.org/index.php/Catalyst_Repository), you should still be able to add a custom repository manually to ```/etc/pacman.conf```. Edit: After you add the repo, it's just: pacman -Sy rust-git If not, it's a shame that a derivative of Arch messes with something so fundamental.
No problem! I didn't want other Manjaro users to get the wrong idea if it was possible. :)
I'm happy that you're so enthusiastic about Rust, and we'd *love* to have more people contributing to our nascent game-making subculture, but I want to make sure that you know what you're getting into. If you write your game in Go, it will just work, forever. They're stable and backwards-compatible. With Rust, you'll have to wrestle with compiler bugs, incomplete features, and rapidly shifting libraries, idioms, and APIs. It's not for the faint of heart! If this is too much for you to put up with, it's always possible to start writing your game in Go and port it to Rust once we get more stable and backwards-compatible (hopefully by spring 2014 at the latest).
I... I think you're on the wrong subreddit.
You're right :V
I'm confused. Why the deletion of an element would re-order the elements ? Also, I don't really see what a sorted vector has to do with lookup tables (except in the specific case that you want to use the lookup table in both directions).
Consider a vector of 10 integers. If I wish to delete the fifth, I take the value from index 10 (the last) and place it at index 5 (and decrease the length of my vector). This means the vector has been reordered, but is a very cheap operation. There is nothing about lookup tables here, just binary search http://en.m.wikipedia.org/wiki/Binary_search_algorithm
Binary search (provided by std::vec::bsearch) is very very fast at searching for elements in an array, but it only works if the array is sorted. Inserting or removing an element requires the array to be re-sorted, which is a relatively slow operation. Therefore, vectors can be used as fast and efficient lookup tables *if* you construct them once, sorted, and then never change them.
Probably about the game rust http://www.reddit.com/r/Games/comments/1gfmhd/garry_newmans_from_gmod_fame_new_survival_game/
I know it's old, but I've come across it before and just stumbled across it now. I'm a little biased to be sure, and I admit that Rust isn't there *yet*, but I think that I would take that bet on Rust 1.0. The libraries are important to be sure, but they just need to be written. The language is slowly solidifying into something that allows you to have an incredible amount of control over. Which I consider more important than the libraries. The OCaml float example he gives is especially amusing as there is currently a suggestion to *force* you to choose the size of floats. Rust gives the programmer a level of control on par with C/C++, but ensures memory safety at compile time. With the unsafe escape hatch you can do things that might be unsafe, allowing for code that might not be possible in other languages. It's telling that it doesn't ever seem *necessary* to drop down into C. The asm! syntax extension also means that it's pretty uncommon to even need to drop down into assembly.
C allowed for high level abstractions without the need for dropping down a level. It left no room for something else lower. C++ is the same, pushing abstraction to even loftier heights whilst still allowing for 'zero cost abstractions'. Rust really does have this potential to push this even further, but it still lacks some of the tools to do so. I have confidence that this will come in the future though.
""You're more dependent on the decisions made by the language implementers than you think."" Yes! Sometimes this is good, sometimes bad.
&gt; I hate to be impatient Yes! So hard. The guys are doing the best they can!
Yeah, that level is probably the shakiest at the moment. The thing is, I consider it less important in the long run. To clarify, it's almost always much, much easier to fix libraries than it is to fix the language. I'd rather have a solid language with shaky libraries at the moment than a bad language with solid libraries. 
&gt; I'm a little biased to be sure, and I admit that Rust isn't there yet, but I think that I would take that bet on Rust 1.0. I wouldn't take the bet on Rust 1.0. There will always be problems, and until a language is used heavily by tens of thousands of programmers for many different tasks, there is no way to know what those problems are. Because of the Rust team, I'd be willing to take the bet on Rust 2.0. They'll make the necessary changes.
I concur with this. Check out the milestones on the issue tracker: https://github.com/mozilla/rust/issues/milestones For the purpose of this hypothetical wager, the most pertinent milestone there is "maturity #5 - production ready". However, that almost certainly *won't* be reached by the time we're at 1.0. Going by the **bare-minimum** criterion, 1.0 could be released as early as the "maturity #2 - backwards compatible" milestone. Though I personally hope we hold off until "maturity #3 - feature complete" for the 1.0 release, as a 1.0 release without a good stdlib is basically suicide in this day and age. 
I could not agree more, just have a look at C: lots of libraries but goodness so many "bizarre" things going on at language level :(
Super awesome project! A while back ago I made an example for the visitor pattern http://maikklein.github.io/2013/06/22/vistor-pattern-in-rust-with-dynamic-and-static-dispatch/ But it's not a good example, I should bring it into some context. But out of curiosity, why did you use @ in your example?
I much, much, much prefer a compiletime failure to a dynamic failure, even if it means I have to do more work. It shortens write-&gt;run-&gt;fail-&gt;debug-&gt;fix to write-&gt;compile-&gt;fail-&gt;fix (and debugging often takes the most time, "What object was I given, where did it come from, and why doesn't it have the things I expect?"). I have some crazy far-future proto-ideas for this, though. I might expand upon them later on.
Nice little example. However, there are a couple problems with the code: first, as others have mentioned, there's really no reason to be using `@` here -- `~` should probably be used. Also, from the [style guide for traits](https://github.com/mozilla/rust/wiki/Note-style-guide#trait-naming), "avoid words with suffixes (able, etc). try to use transitive verbs, nouns, and then adjectives in that order." Therefore, instead of `Burnable`, the trait should probably just be named `Burn`.
In Rust 0.7 there will be all of them: http://static.rust-lang.org/doc/std/num.html#trait-trigonometric
Hi, maybe this is a stupid question but what are 'a 'r I know that 'static is for static things, but those 2 above I see them everywhere except for the documentation.
Is this a standard function or do I have to import to use it? (Sorry if the question is somewhat naive. I'm just starting to learn rust).
&gt;There's some work in moving... That's a rewrite of library code and not a language feature thing right?
Git commit / tag?
Right now we only support 32-bit Windows (though we do support 64-bit Mac and Linux). IIRC this is a result of us making use of MinGW rather than MSVC. We have a long-term plan to change this (https://github.com/mozilla/rust/issues/1768), but I'm not familiar enough with Windows to know if that'll make it possible to build Rust in VS. A lot of our Windows-users build Rust in a Linux VM (or even go all the way and install a Linux partition). At this stage of the language's maturity, I'd suggest this route to minimize frustration.
Okay, thanks for the response! So the only option right now to try the language is to compile on Linux or Mac? I'm considering using Rust for games later on, so Windows is honestly the most important platform for me. Is 64-bit support scheduled for 1.0?
Please note that this is a collection of articles, which is WIP.
On 64-bit hardware you'll have to be running Linux or Mac, yes. I found the issue for 64-bit Windows: https://github.com/mozilla/rust/issues/1237 and I've left a comment in an attempt to get this done earlier. I *know* that it will come eventually, because Windows is Firefox's most important platform and not supporting 64-bit would be suicide for Servo. But I agree that it's very important to not launch 1.0 without 64-bit Windows support so as to not give a poor first impression. I urge you to send a message to the mailing list to spur the devs into action.
I've been successfully using rustc since 0.3 or so (up to 0.6, haven't tried 0.7 yet) on Windows 7 64bit. There are some missing DLLs that you can just download and drop into rustc.exe's folder, and it should work. Keep in mind that unless you have a proper, working MinGW installation in your %PATH%, you're only going to get .o object files from rustc.
Are you using 0.6? It's not in 0.6, but it should work in a master build from the last month or so (and, if you wait a day or two, in the soon-to-be-released 0.7).
Neat! I'll be interested to see how well OOP design patterns map on to Rust, or if "design patterns" from Haskell and functional programming in general are a more natural fit.
I think the transitive form would have to be `Burns`. But given that transitive verbs are hard and adjectives are easy, I'd say that `Burnable` is acceptable. :P They're also idiomatic for Java interfaces, which is probably the biggest audience for a resource on design patterns. But yes, the use of `@` should be discouraged.
Ah, ok, I see! I thought I had one, but I apparently didn't! I did get notices about one missing DLL, but after that I got no more notices, just a plain crash. 
I would send a message, but I'm not quite sure where/how. Mailing lists isn't something I'm used to, haha.
To send a message to the mailing list, just send an email to rust-dev@mozilla.org. Though if you want to ever see responses to it, you'll need to subscribe at https://mail.mozilla.org/listinfo/rust-dev (or I guess you can just check the archives every day...). EDIT: You might even need to be subscribed to send a message, I'm not sure. It's easy to unsubscribe though. :)
Although `Burns` sounds better to me, it doesn't follow the examples in the standard library -- `Copy`, `Add`, etc.
Well there's your problem, you think that the stdlib is idiomatic rather than the grotesque, frankensteinian monster that it is. :)
Cool! From a quick glance it looks like a lot of these apply to any language with typeclasses—so these could apply equally well to Haskell (well, purity notwithstanding).
I have a similar question in the Adapter example; why does the `SpaceXAdapter` hold a `SpaceXDragon` instead of a `SpaceXShip`? Shouldn't a single Adapter class work for any future ship SpaceX releases under that interface?
Perhaps I'm being naïve about it, but is the adapter for the `SpaceXDragon` really needed? The way I see it, you could implement `RocketShip` directly like this: impl RocketShip for SpaceXDragon { fn turn_on(&amp;self) { self.ignition(); self.on(); } fn turn_off(&amp;self) { self.off(); } fn blast_off(&amp;self) { self.launch(); } fn fly(&amp;self) { (self as &amp;SpaceXShip).fly(); // this line being the main difference } } And then in main, just call `pilot(&amp;dragon);` (on the dragon directly).
The following changes seems to work. I added a parameter, and turned SpaceXAdapter.ship in to a borrowed pointer with a life time struct SpaceXAdapter&lt;'self, S&gt; { ship: &amp;'self S } ..... impl&lt;'self, S : SpaceXShip &gt; RocketShip for SpaceXAdapter&lt;'self, S&gt; {....} .... let dragon_adapter = SpaceXAdapter { ship: &amp;dragon }; Perhaps it would be nice to avoid the struct named SpaceXAdapter completely and just implement the trait Rocketship directly for all classes implementing SpaceXship, but I don't know how to do that. 
I think the goal is to implement RocketShip for a large number of structs that already implement SpaceXShip. In other words he wants to add a new interface to an existing codebase. So I guess that he wants to write as few lines as possible for each struct.
I got it working using the instructions on the wiki! Thanks a bunch! There was something in the testsuite that failed, but I can't figure out what. My test programs have been compiling fine though. :)
Thanks! That certainly does add a bit of syntax. Pardon my ignorance, but it's sort of hard to search for, so what is with the `'self` in your sample? SpaceXAdapter looks like it would be a parametric type over S in C++, so what does self need to worry about? I seem to remember `self` being used as an important word without the quote; are the two different, or did the syntax change at some point?
No worries. I am learning too. I am looking forward to reading the rest of the series. The 'self is a lifetime. It ensures that dragon_adapter.ship does not end up as a dangling pointer while dragon_adapter is still in use. (In other words dragon must survive dragon_adapter). The rust compiler wishes you well, so it requires you to provide a life-time before accepting the code.
I see. So as a guess, the lifetime annotation is preferable to a shared pointer because we expect `dragon_adapter` to have a shorter lifetime than the source of the borrowed pointer (in this case, `dragon`) in the majority of use cases, so the overhead of a shared pointer for `dragon_adapter.ship` would be wasteful? So I'm guessing the compiler checks whether this assumption is true and just throws an error if not, right? It doesn't convert the borrowed pointer into anything else, right?
I think the compiler ensures that you can never compile code that creates a dangling pointer (or at least it tries very hard to do so). AFAIK "&amp;" is a "borrowed pointer", and it is stored in memory as a normal c-pointer, so you don't get overhead from reference counting or garbage collection, but I guess that you always have to pay a small price for using a pointer. I am not an expert on the internals but you could possibly gain a small speedup by implementing the RocketShip directly for SpaceXDragon as suggested by /u/jmgrosen but the code would perhaps no longer correspond to the adapter pattern.
Already released :D
But... [Are Design Patterns Missing Language Features](http://c2.com/cgi/wiki?AreDesignPatternsMissingLanguageFeatures)?
There is a *lot* of good stuff in this release. And from what I've been seeing in this subreddit, it looks like 0.8 is going to have a comparable amount of "good churn." Congrats, Rust dev team!
As the author, I really appreciate the advice! I'm really new to Rust and still have to get better at discerning between using @ and ~. If you have any useful resources, I'd love to read them to learn more.
Awesome, thanks a lot guys. I'll change it to `Burns` first thing in the morning as well as using the tilde.
Hey MaikKlein, honestly I used @ because I didn't actually understand the difference. It was purely ignorance on my part. I'm ashamed as I'm still learning Rust (and very bad at it). I got into Rust because I loved the idea of the algebraic types as well as the structs. I even really like the syntax, I just have yet to learn it's more delicate features. =]
Nice stuff. Here is another example of the pattern. As far as I understand it is "rustic" to let the visitor be a function. let element = ["a","b","c"]; fn visit(node: &amp;str){ println(node) }; for element.iter().advance |&amp;node| { visit(node) } for element.rev_iter().advance |&amp;node| { visit(node) } EDIT: The iterator syntax is currently being rewritten, so they look a little verbose right now.
But those checks all get done at compile time, or don't they?
(especially a small team with no windows devs!)
well, 1. you’ll only use `Option` if there’s a possibility that it is `None`, so the check is necessary. At positions where it isn’t, you’ll likely be in a match branch and have access to the reference to the object inside `Some()`. 2. ### [Method get_ref](http://static.rust-lang.org/doc/core/option.html#method-get_ref) fn get_ref&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a T Gets an immutable reference to the value inside an option. #### Failure Fails if the value equals None #### Safety note In general, because this function may fail, its use is discouraged (calling get on None is akin to dereferencing a null pointer). Instead, prefer to use pattern matching and handle the None case explicitly. 
Hello All, Congrats to rust-dev team on new release! I installed rust 0.7 on OS X 10.8. But when I run `rust` command on terminal I get this error. rust(96233,0x10d33a000) malloc: *** mmap(size=7522747388523884544) failed (error code=12) *** error: can't allocate region *** set a breakpoint in malloc_error_break to debug Abort trap: 6 Any idea what could have gone wrong with installation? Thanks
Clicking on the revision hashes in the upper graph adds that revision's complete memory profile to the lower graph. The x-axis for the lower graph is CPU seconds, and the dotted marker represents the point in time when peak memory is reached. All in all, very cool. It would be even cooler if, when you had only a single revision selected on the lower graph, it broke down the profile according to compiler phase (parsing, trans, llvm, etc.). EDIT: Also, a link to this page from the isrustfastyet home page would be useful.
I'm planning on replacing the IRFY homepage with this (or with a proper landing page). And yeah, that's exactly what I was going to do wrt LLVM passes, just I've not got the information yet.. */me pokes cmr* (Well, I was actually thinking of also showing it when there were just 2 in the detailed graph, for comparisons.) Also, the dotted marker is actually the time when the build without memory profiling finished (the memory profiling skews the timing slightly). I'll write some descriptions sometime.
Hm, the `rust` command is still half-baked and not widely used yet (all of its sub-tools basically need to be rewritten from scratch). Try using the `rustc` (note the `c` on the end) command on this hello world program: fn main() { println("hello, geodel!"); }
Thanks! It looks to me that somehow old and new version of rust are conflicting. Now I get this error: hello.rs:1:0: 1:0 error: multiple matching crates for `std` hello.rs:1 fn main() { ^ note: candidates: note: path: /usr/local/lib/rustc/x86_64-apple-darwin/lib/libstd-4782a756585a81-0.6.dylib note: meta: name = "std" note: meta: vers = "0.6" note: meta: uuid = "122bed0b-c19b-4b82-b0b7-7ae8aead7297" note: meta: url = "https://github.com/mozilla/rust/tree/master/src/libstd" note: path: /usr/local/lib/rustc/x86_64-apple-darwin/lib/libstd-6c65cf4b443341b1-0.7.dylib note: meta: name = "std" note: meta: vers = "0.7" note: meta: uuid = "c70c24a7-5551-4f73-8e37-380b11d80be8" note: meta: url = "https://github.com/mozilla/rust/tree/master/src/libstd" error: aborting due to previous error Is there a way clean up old version now that I have deleted root dir for 0.6 version? 
I believe you also use "length" in your Log constructor despite not being in the Log struct definition.
Ok, I was able to delete 0.6 libs and now rustc works. However rust command still does not works. For now it should be ok. Thanks!
It looks like your rust installation is a little messed up. Try to remove all rust 0.6 files (probably in the /usr/local/lib/rustc directory) and reinstall. Or, if you want to have it the "apple" way, install rust from MacPorts. Right now it's at version 0.6, but I'll manage it to update it today or tomorrow. EDIT: just updated it to 0.7. Enjoy!
Right. The region system and borrow check together ensure that all `&amp;` pointers never become dangling or point to a value of the wrong type at compile time by reporting errors if they detect that things might go wrong. At runtime `&amp;` pointers are as cheap as C pointers: the GC ignores them and there's no overhead associated with creating or dereferencing them beyond that of C.
Does Rust have the ability to give pre-made implementations to an interface for a struct (mixins?)? Does it have have the ability to override one or more of the methods in these implementations? If not, are there plans for mixins down the line?
Good eye, my friend. Changed =]
For comparison, D originally had an [`opApply` and `opApplyReverse`](http://dlang.org/statement.html#ForeachStatement) methods that are called by `foreach` statement, but it now has [`std.range`](http://dlang.org/phobos/std_range.html) module in the standard library and `opApply*` methods are now considered an internal mechanism for getting `std.range` work. This situation is surprisingly similar to Rust 0.7's iterator reform, though as far as I recall `std.range` was a result of tight relation between C++ feature proposal and D. Also note that D's `foreach` statements have the default behavior for built-in types like array and associative array.
No problem. Great article. :-) 
Off topic: This D-forums software is the best forum software I ever used anywhere. It's bloody instant, even on lousy Internet connection I'm using ATM. It feels like I'm browsing locally stored page, not retriving anything from over the ocean.
AFAIK, the forums and the D website are written in D. Perhaps having the same for Rust would not be a bad idea ;)
what i noticed browsing the E-easy issues that there are a lot of old issues with merged commits attached that are not closed. for someone new to the language like me, it's unclear weather these issues are already fully resolved and just not closed or if there is more work to be done.
You can use the [deriving attribute](http://static.rust-lang.org/doc/tutorial.html#deriving-implementations-for-traits) to let the compiler implement some of the traits for you. But you can't override any of the generated methods.
You write me a good pair of http libs and I'll write you forum software :)
Well, you should be able to just do this: impl&lt;T: SpaceXShip&gt; RocketShip for T { fn turn_on(&amp;self) { self.ignition(); self.on(); } fn turn_off(&amp;self) { self.off(); } fn blast_off(&amp;self) { self.launch(); } fn fly(&amp;self) { (self as &amp;SpaceXShip).fly(); } } and that would work for all types implementing `SpaceXShip`... but unfortunately it doesn't work right now due to [this bug dealing with method dispatching](https://github.com/mozilla/rust/issues/5898).
There do exist **default methods**, which allow a trait to specify some default behavior that users can either override or not: trait Foo { fn foo(self) { println("default behavior!"); } } impl Foo for int; impl Foo for uint { fn foo(self) { println("custom behavior!"); } } fn main() { 4i.foo(); // default behavior! 4u.foo(); // custom behavior! } Though note that they're pretty buggy at the moment, you have to compile with the flag `-A default-methods` to enable them.
But sometimes the fail-&gt;debug-&gt;fix is faster than compile-&gt;fail-&gt;fix.
I believe you're looking for /r/playrust.
This is probably answered elsewhere, but why not just spawn { println("Hello from a different task!"); }
I think there are difficulties making that unambiguous in cases where the closure actually takes arguments. For example, what does the following mean? foo |x| { y } Under your proposal this would be a closure (`|x| { y }`) being given as an argument to a function `foo`, but as far as the parser is concerned this could be a variable `foo` being bitwise-OR'd with a variable `x` being bitwise-OR'd with a block that evaluates to `y`. 
If you (or anyone else) sees an issue like this that you'd like to work on, feel free to comment on it. There are many experienced rusties watching the Rust news-feed on github, and hopefully one of us (and/or the issue filer) will notice it and answer appropriately. (Another, possibly more efficient, route is coming on to [IRC](http://chat.mibbit.com/?server=irc.mozilla.org&amp;channel=%23rust) and asking there.)
You should try asking bblum, I believe he was the original author of the ARC module.
Thanks for the update as always. I'm not familiar with rust's branching and release policies. Are the changes mentioned here included in 0.7, post-0.7, or a mix?
I don't think any of it is in 0.7. I just go through the list of pull request merges every week and pick out the ones that are important (which is usually most of them), like `git log --author=bors --since='1 week ago' --pretty=oneline`
Releases don't yet have any support or backporting. The 0.7 tag is just the state of master on the the scheduled date for releasing 0.7.
I know that it's not important right now, that there are bigger fish to fry at the moment, and that it might be implementable with a macro etc, but I had the idea and wanted to write it down somewhere :)
Hmm... couldn't we do the same thing without the `in`? I.e. allow destructuring on the LHS of assignments, period? fn foo() -&gt; (int, int, int); let mut x = 42; let mut z = 0; loop { (x, _, z) = foo(); // ... } // Do something with x, z ... 
I believe that this would make it impossible to be LL(small* number), since the pattern grammar and the expression grammar are different; and arbitrarily large look-ahead is required to distinguish them: (x1, x2, ..., x100000) // either an expression or pattern at this point = foo(); // ah, it's a pattern. (* I guess "small" = "finite")
I think this is probably a poor use case for the `in` keyword, as it's not especially intuitive and (imo) the circumstance described seems rather obscure. A macro would probably suffice here.
I guess you mean `x = *x` and `&amp;x = x`?
I can't imagine a way for that to be well-typed (with `y` on the rhs, OK)... and are you sure? Why/how is it different from `let`? (The parser ambiguity is enough to sink it, but I'm curious nonetheless.)
When I see `let` I know to expect a pattern. I suppose it's just a personal thing, as I feel like patterns are appropriate in the context of declaration but not necessarily in the context of assignment in general.
lol 
That looks really nice, but I wonder which version of fly will be called if you write let dragon = SpaceXDragon; dragon.fly()
Most contributors already know about this page, but I thought it'd be useful to share the knowledge. It's useful for knowing where your PR is in the queue. For those who like to watch pots boiling in greater detail, the [Buildbot Console](http://buildbot.rust-lang.org/) is also very handy.
Traits are good idea, suitable for high-level compression though not as suitable for stream compression. `~str` in Result probably not good idea, would parameterize it with `T:ToStr`, so enum of error codes could be returned for some decompressors while still having a string available (perhaps more thought needs to be put into unified error handling?), unsure.
I feel like error handling in Rust is somewhat undefined so far. I feel like we could use a unified Error struct, something like struct Error { code: uiint, msg: ~str } For your trait thing: Yeah, I was fooling around with exposing a sort of compression iterator, where compressed data was written to a fixed size buffer on each iteration, but it's unsafe with the iterator paradigm, as the values of any reference to that data would change on ever call to `iter.next()` perhaps there can be some sort of stream trait though. Or each individual compressor would handle it, and if you needed that, use the compressor directly. What do you think about something like this going into extra? Given extra::flate's existence, it seems that compression is seen as something that should be in the libraries somewhere.
The point is to *not* store and generate strings with your error results; you lazily get a string with to_str() if you need one, otherwise you don't pay the penalty re: iterator, not unsafe, the iterator just needs to hold a `&amp;mut`. Yes, it should definitely go into extra.
No I'm not. Do you care to explain?
&gt;re: iterator, not unsafe, the iterator just needs to hold a `&amp;mut`. I'm not so sure that's the right thing. Isn't an `&amp;mut` reference a guarantee that you're the only one who can change that data? The issue I have is that if you have say let mut it = compresser.compress_iter(in_buf,out_buf); let first_ref = match it.next() { Some(mut ref x) =&gt; x, _ =&gt; fail!("lol") }; let second_ref = match it.next() { Some(mut ref x) =&gt; x, _ =&gt; fail!("lol") }; `first_ref` will have changed under you. Isn't this something that `&amp;mut` says *will not* happen?
That interface doesn't really make sense. What would `next` return? Why are you giving it two bufs? Anyway, the borrow checker will ensure that `&amp;mut` is used correctly, if a given program compiles, ostensibly it's correct.
The two bufs thing is a mistake, but the basic problem is, that if you are decompressing the input into a single, statically allocated buffer, and giving a reference to that buffer through each iteration, since all of those references point to the same buffer, and the what's in the buffer changes as it is being replaced with newly decompressed data upon each iteration, then it is a fundamentally unsafe interface. [Look at this commit, where i sort of had something like this working:] (https://github.com/MarkJr94/bzip2rs/commit/087e50f7c6446da94b234af913020a95597e0763) The whole reason I was trying to have that sort of interface, was to avoid memory allocations. So if I'm decompressing into a fixed buffer, and I hand you references to it, you have to be aware that it's contents will be altered on the next call of `iter.next()`. Have I explained the issue any better? And the borrow checker can't handle this properly, as I have to use unsafe calls to `transmute` to turn a slice into a raw pointer an vice-versa. 
There is also some info here. I don't know how much has been settled * https://github.com/mozilla/rust/blob/master/src/libstd/rt/io/mod.rs * https://mail.mozilla.org/pipermail/rust-dev/2013-April/003746.html * http://thread.gmane.org/gmane.comp.lang.rust.devel/4234 
Yay! I must try it soon. Edit: Yes! Seems that everything is fixed. I'll definitely report any outstanding issues. :)
This fixes the `match` block indenting! Awesome.
Funnily enough, the thing I've liked best about this since I did it was that `=` now works well, such that I can use `gg=G` to get correct indentation on an existing file. If you find any problems with it, I'll try to fix them up quickly. See also https://github.com/mozilla/rust/pull/7676 where I fixed the remaining problem that I am aware of, for indentation of multi-line comments with no leader.
That interface idea of yours seems really nice! Maybe that's what should be in `extra` as maybe `extra::codecs` or something similar. I'll try and rewrite the compressor/decompressor to fit that.
If I followed the conversation correctly, it seems like the major standing options being considered are: * Allow developers to annotate functions to specify a stack size. * Provide small stacks and call morestack to grow them as needed. * Provide a very substantial stack (~2MB) to tasks that need one. * Provide a large initial stack and then fall back to stack growth only for the rare tasks that exceed it. What if we used an inverted approach to the fourth option? You could give tasks a very modest starting stack (I believe 64k was offered in the meeting as a goal), and then have stack growth done in much more aggressive blocks, say 512k. In this approach a program can spawn lightweight tasks all over the place without worrying about dramatically over-allocating. Then, in the case that something does actually require more stack space, morestack calls are comparatively rare since the growth is big. You could tune the defaults via annotation. I'm not at all qualified to evaluate these approaches, so I'd love it if someone more knowing could throw in their two cents.
I agree with your option. I was also wondering why nobody mentioned an exponential growth strategy (maybe it was discarded a long time ago ?). I also wonder if there is an issue specific to task-allocation boundary. Specifically, suppose that you decide to allocate memory in 64K blocks: - you are consuming 63,5K - you call a function that requires more stack: allocate 64K - you exit that function: free 64K - you call a function that requires more stack: allocate 64K - you exit that function: free 64K - ... So, not knowing the underlying mechanism, I wonder if they relinquish the latest stack block immediately or keep it around as a buffer for a while (for example, until you drop the block below it).
Exponential growth is the current strategy. We start with small stacks (~2k) and each time you hit the end you get progressively bigger stacks. The main issue here is that stack growth is expensive (where pretty much anything more than a function call is 'expensive', sadly), and this includes both growing the Rust stack and calling into C (they are effectively the same - both require switching stacks). So running on small stacks (anything less than the amount of stack required to run C code) penalizes all FFI and quickly runs into unpredictable 'stack thrashing' in Rust code where tight loops are crossing stack boundaries. Many people think having this behavior by default is untolerable.
My proposal was to use large stacks by default (to handle the general case), and allow passing an alternate initial segment size to the task spawning API. It would always do growth if you actually reached the end. If the initial stack segment size is more than a few pages, there's no gain from segmented stacks on anything but 32-bit because lazy allocation is fine-grained enough to win in memory consumption too. The stacks have to be larger than the size used to call foreign functions (2MiB), or there will always be a large performance hit. FFI calls need to run on the initial stack segment most of the time.
64K isn't nearly enough for safely calling many libraries, so we'll experience the same issues we do now. FFI calls are everywhere, so they can't be second-class. A `malloc` call that takes 15-30ns in C takes 60-100ns in Rust *without* thrashing. It gets much, much worse if it's not holding onto the segment.
Disclosure: I maintain the [glfw wrapper](http://github.com/bjz/glfw-rs). This just shows that it's in a usable state now. Here is an example of setting up a basic window: extern mod glfw; fn main() { // Initialize the library on the main platform thread do glfw::spawn { // Create a windowed mode window and its OpenGL context let window = glfw::Window::create(300, 300, "Hello this is window", glfw::Windowed).unwrap(); // Make the window's context current window.make_context_current(); // Loop until the user closes the window while !window.should_close() { // Swap front and back buffers window.swap_buffers(); // Poll for and process events glfw::poll_events(); } } } As a warning, the internal [callback implementation](https://github.com/bjz/glfw-rs/blob/master/src/private.rs) is pretty ugly, so any assistance/suggestions on that front is most welcome. The external api is quite clean though (in my biased opinion of course).
I removed the mention of Rust there, we just have a list of stack segments (1 child, not n-ary).
No worries, let me know if you have any issues or suggestions. I'm progressing further with my [maths library](https://github.com/bjz/lmath-rs) – I don't know whether it would be of use to you. I really need some folks battle-hardening it. :)
Ok, the meeting notes give the impression that the expected number of tasks is relatively small though (a few hundred rather than a few thousand).
Bad timing. Rust is in the middle of switching its dominant iteration idiom from internal iterators to external iterators. See also: https://github.com/mozilla/rust/wiki/Doc-detailed-release-notes#iterators http://journal.stuffwithstuff.com/2013/01/13/iteration-inside-and-out/ The `for` construct will be changing in the 0.8 cycle to reflect this change. Currently it looks like: for [1,2,3].each |i| { /* a closure */ } Where eventually it might look something like: for i in [1,2,3] { /* just a regular block */ } I'm actually surprised that the author praises the "freedom and flexibility" of our old `for` loops... the reason for making this switch at all is because we found our old semantics to be neither sufficiently composable nor sufficiently flexible! :) Hopefully the author will revisit this space once the new implementation is finalized, perhaps with a comparison to D this time. **TL;DR: Rust is switching from Ruby-style iteration to C#-style iteration because (for our purposes) it's more performant, easier to optimize, more composable, and more flexible.**
I've paraphrased his comments in this interview before, so I thought it would be a good idea to share it. Here are the main parts (edited for clarity): (2:37) &gt; C's big claim to fame and the reason Dennis Ritchie deserves the Turing Award was because he did something they said couldn't be done. He created a high level language that was portable, at high performance, leaving no room for a language lower than assembler. We can niggle about the C declaration syntax and other things that didn't work out so well, but it's really important to recognise that those are side experiments that were details in a much larger thing that he accomplished. &gt; Now, advance another decade. 1979 and early 1980s – Bjarne is working at Bell Labs on C++ and what he accomplished there was to take a systems programming language that was portable, with strong abstractive power especially with classes and templates. He proved that you could actually do a portable systems programming language with strong abstraction and strong typing. That was a major advance. &gt; I would love to see somebody tackle the challenge of demonstrating that you can create a portable systems language (meaning that it has performance equal to C, C++, Fortran) that is portable, type safe, and also memory safe. To add that on top of classes and templates and the portability of C, to add strong type safety (which requires a form of garbage collection) and not leave performance on the table – that would be something. That would be the first strong competition to C++ on its home turf. (6:00) &gt; One of the things Go does that I would love C++ to do is a complete left-to-right declaration syntax. That is a good thing because the left-to-right makes you end up in a place where you have no ambiguities, you can read your code in a more strait-forward way, which also makes it more toolable.
I copied the link from HN. This whole thread is a 100% ripoff.
And another! http://www.reddit.com/r/programming/comments/1i2t1f/philosophy_and_for_loops_more_from_go_and_rust/cb0ggq9
Does each even work anymore? 0.7 barfs whenever I try it. You have to use something even uglier like: for [1,2,3].iter().advance |i| { println(i) } Honestly the projects documentation could use improvement. It's a big jump from virtually every other language to have things laid out specifically by trait. For instance in the vec module documentation you can't directly tell that a vector iterator actually implements advance. No you have to go to the iterator module documentation and then figure out that all iterators implement the IteratorUtil trait and then realize that it exists at all. This is messy and inefficient and requires way more jumping back and forth to the toc than good documentation should. 
You're right to be angry! We suck at documentation. We're completely rewriting our documentation generator at the moment and it will address these concerns. The `iter().advance` bits of that example are temporary workarounds. I was sad that we had to release 0.7 with this shim in place, but the numbered releases aren't so much "hey here's a stable and nice thing that you can use" as much as they're "look, we've actually done things in the past three months, please don't forget about us".
I replied to @brson [here](http://www.reddit.com/r/rust/comments/1hy6l9/meetingweekly20130709_split_stacks_ffi/cb0j7m5), though I somehow doubt that nobody in your team already tried and discarded those ideas :/
Oh yeah, well I also wield the unimaginable power of CSS pseudo-selectors! Who's the copy *now*, wise guy?
Yeah I don't want to knock the effort. It's a lovely language overall just one that isn't very approachable at the moment. It's good to see work continuing on it though and continuing syntax improvements. Keep up the good work.
Can you describe what "actors" are and how they differ from Rust's default concurrency primitives? I'd also love to see a real working example of Monster Flower Finder. :)
He only said some form of garbage collection. For example the compiler can not enforce the lifetime safety of a shared resource in a multi-threaded environment, so you have to have some kind of garbage collection, even if that means something as basic as using an atomic reference count. But then while you do get type-safety with reference counting and it's a pretty simple form of garbage collection, it doesn't protect you against cycles, so you can either introduce weak pointers, which weaken type safety, or you implement a more general form of garbage collection like mark/sweep.
What's the rest of the overhead btw?
Switching stacks on FFI calls in any form is going to be a huge performance hit. Branch prediction and loss of data locality (caching) are the main things taking a constant hit from segmented stacks when thrashing isn't happening. Rust can't have *split* stacks and be competitive in performance with C, even if you ignore the allocation costs. The checks for stack size in every function wouldn't be needed if we just had protection from stack overflow, the 2-10% overhead from those can't be ignored either. All you need is a few guard pages + checks on allocations *larger* than the guard pages.
The C code will be running a new stack segment (whether or not it's newly allocated), so it breaks branch prediction and caching. The CPU cache is supposed to know about the stack used by the program and with Rust it doesn't. The MMU essentially provides you with cheap lazy allocation and we're missing out on that too.
Oh, my. Sorry to keep raining on your parade but we do these optimizations already. If you are curious to know all the gory details, I laid out exactly what the costs are for stack growth once here: https://github.com/mozilla/rust/issues/3565 This is a little out of date but probably more-or-less correct still.
afaik the only way to communicate from one task to another in Rust is via Chan/Port. And this involves listening for messages (it blocks). I hope that I am wrong because at the moment I see no way to implement event based actors. I think I found a way to do this via ARC. Thanks! 
Yay! Thank you for bestowing this honor on me, lord kibwen!
Sometimes I get confused between whether I'm in the Go or Rust subreddit. If you were asking about Go, I could give you lots of neat ideas, but I simply don't have any Rust experience or know enough idiomatic Rust to suggest anything remotely useful.
What do you mean by 'ownership patterns'?
E.g. one common pattern is unique ownership where data is owned by a single owner. Another pattern is shared ownership, where multiple things share ownership of some data (and it needs to be destroyed when all of them are gone). In Rust you'd be hard pressed to have complicated shared ownership of data without involving the GC.
Event based actors dont need to be in different tasks. That is what makes them light enough to have millions.
Well, it's also not Discourse with all the fancy scrolling, Ember.js, etc. it's just vomiting the contents of the DB with pagination
Tasks are event-based N:M threading though, although it will be possible to have 1:1 scheduled tasks alongside them. As far as I know, they're already supposed to be usable that way.
There's an iterator tutorial I wrote for 0.7: http://static.rust-lang.org/doc/tutorial-container.html#iterators It explains why things are the way they are right now (trailing underscores and `advance`). I also have a pull request open expanding it a bit: https://github.com/mozilla/rust/pull/7736
...or return an `Option`, no?
A short, simple, self-contained example. Nice!
Random thought: It might be useful to have a special trait describing an actor with a single port and an single channel. That would allow you to create a pipeline of actors. let pipeline = PipeLine([actor1, actor2, actor3]) See also http://gstreamer.freedesktop.org/data/doc/gstreamer/head/manual/html/section-elements-link.html EDIT: Perhaps this requires Port and Channel to be separate traits??
Hmm... yes :-)
You are probably well aware of it but there is a popular Actor library for Ruby: [Celluloid](https://github.com/celluloid/celluloid). I wonder how much of it could be implemented in Rust though. 
Yes I am currently looking at libcppa, akka and celluloid.
By "some kind of GC" I mean *any* kind of GC, not just the tracing kind. I.e. reference counting would count. Good point about weak pointers though.
Well, I am not too surprised, those are fairly simple ideas. Still, great to know they are already implemented; it's always annoying to have to choose between two alternative solutions without knowing really what either could really give, at least here the split-stack has been explored quite deeply it seems (though I note that there are still potential optimizations in the link you gave).
Well, first of all, like in Erlang, you can simply remove the idea of sharing data between concurrent threads of execution (of course, the runtime system of the language might need them, but it's an implementation detail). Shared resources are otherwise the bona fide counter example to deterministic lifetimes, however in practice I have found that there was *very few* cases where shared resources really were necessary.
Unfortunately, you seem to be considerably underestimating the issues that arise with distributed code. I recommend that you read [The making of Warcraft part 3][1] by Patrick Wyatt, and specifically the *Playing against a human* part where he describes the issues that arise when trying to keep 3 representations of the same world in sync. TL;DR: it's more complicated than just spawning a master and slave. [1]: http://www.codeofhonor.com/blog/the-making-of-warcraft-part-3
Tasks have a bit more overhead than event-based actors because they need their own stack. However, tasks have a lot of upside too because you can take advantage of all the work being put into the Rust scheduler. It is also easier to avoid callback spaghetti code with tasks. With 64 bit address space you can probably run millions of tasks, but with some work. For example, on Linux you will likely run into the VM regions limit which defaults to a number much lower than what you'd need for millions of task stacks.
Actually that is a really good idea. Thanks!
Would it be possible to pre allocate. an FFI stack? That way most FFI calls wouldn't need an allocation.
If you have a thread-local FFI stack, you're paying for a branch prediction miss and wiping out the cache with every call. It's very expensive.
Two great quotes from Robert O'Callaghan: &gt; A quick scan suggests that all 34 sec-critical bugs filed against Web Audio so far are either buffer overflows (array-access-out-of-bounds, basically) or use-after-free. In many cases the underlying bug is something quite different, sometimes integer overflows. In other words, 100% of these bugs would be eliminated if the code was written in Rust. and &gt; There are 4 sec-high bugs --- DOS with a null-pointer-deref, and a few bugs reading uninitialized memory. The latter would be prevented by Rust, and the former would be mitigated to the extent Servo uses the fine-grained isolation Rust offers. There's still a lot of performance work to do though.
This technique is actually currently used by the ast visitor (libsyntax/visit.rs), but, in many cases (not all, unfortunately), it can be replaced by default methods on a trait, which is a neater solution.
The main problem that I had is that traits can only have methods or functions. But if you need to build something that needs to have some members then you can't really do it with traits. For example if I want to build an actor, it needs to have at least a Port and a Chan. I can't do this with traits. In java or in c++ I could just do "class MyActor : BaseActor;" and then I could start to extend the functionality. In Rust I would implement the trait and then wire it up with my getters like I did here https://github.com/MaikKlein/RustActors/blob/master/actor.rs . I end up writing a lot of boilerplate code. But I just realized that I can write a macro that implements all this. Are macros intended to be used like that?
The original inspiration was that I always end up writing tons of boilerplate code over and over again. This method removes most of the boilerplate code, but I think I could have just used macros for this. But then I feel that inheritance would be a much better fit. :(
You could add getters and setters to the trait to avoid requiring direct access to members.
Presumably it is mostly the side effects of overflow that is dangerous, and these are (in theory) not exploitable in Rust.
There was a proposal earlier this year to have single inheritance of structs, with is-a-subtype-of being a kind of trait bound: http://www.reddit.com/r/rust/comments/19a70o/meeting_weekly_20130226/
Honestly the first time I saw your proposal I thought the syntax looks extremely weird for this. But after reading it a few times it looks kinda nice. I think you can do a lot of crazy things,like struct MyActor { base: BaseActor, base1: BaseActor, base2: BaseActor, some: int other: float, fields: ~str } It avoids the diamond problem by default.
...unless you're using unsafe code to avoid bound checks, because they are slow. But, then at least you know exactly where to look, rather than the problem space being the entire program.
Millions of tasks is still "only" a few dozen tebibytes of address space, assuming large (4MiB) stacks, whereas the address spaces of x64 is 256 tebibytes. I was able to allocate 128TiB and touch each 1GiB block using https://gist.github.com/cmr/5990624 (the actual allocations happens in 0.320 seconds, using jemalloc (2.819 with glibc)) with `vm.overcommit_memory=1`. 
This looks nice and elegant. My only worry is that you may need a lot of getters and setters to delegate access to the members of the wrapped struct. 
The existence of a very similar construct in [Kotlin](http://kotlin.jetbrains.org), another new programming language, namely [*Delegation*](http://confluence.jetbrains.com/display/Kotlin/Classes+and+Inheritance) (scroll a bit down) substantiates the usefulness and validity of your proposal. However, as far as I understand, Delegation in Kotlin is more flexible than your proposal since, in Kotlin, the particular "BaseActor" would be chosen when a new "MyActor" is instantiated. So I guess the Kotlin analog would look something like the following: struct MyActor { base: Actor, // &lt;-- some: int other: float, fields: ~str } #[delegate_to(base)] impl Actor for MyActor {...} .... let base_actor = BaseActor {...} // &lt;-- let my_actor = MyActor {base=base_actor,...} // &lt;-- Now that I have written it down, it seems to me the use cases are a bit different :). Kotlin's way of delegation primarily eases dependency injection whereas your proposal primarily aims to avoid boilerplate code. So yeah, I don't know how useful this post is ;).
I'd like to have a mode that traps overflow and fails at some point. As dbaupp said, I'm not sure how many security vulnerabilities this will actually prevent, but it seems worth exploring. There is a paper that I read a while back that shows that the performance impact of this can be less than 10% if done properly.
&gt; let monster = Monster::spawn(); and it would create a monster on the client and on the server Yeah, that's a cool idea. One question I have is how remote spawn would be done in an AOT-compiled language. Distribute the code ahead of time? Use JIT?
Also it probably makes sense to turn down that 4MB (can I/will I be able to?) with super-lightweight tasks.
IMHO, the remote part of it should be separate to the actor part of it (and ideally the Actor code completely independent) so I can choose if I want the Actor task abstractions or just remote tasks.
Regarding the double copies in transmute, yeah, LLVM does remove that, but I'm currently focused on improving the IR that we generate before LLVM gets to optimize it. There are basically three reasons for that. a) No-opt builds are way too slow, b) IR is too complicated / too hard to read, c) I can't seem to figure out what's wrong with the IR that makes our builds take so long and I hope that a simpler pre-opt IR helps me to understand what's wrong.
I would hope that: struct MyActor&lt;BaseActor&gt; { base: BaseActor, ... } #[delegate_to(base)] impl&lt;B: Actor&gt; Actor for MyActor&lt;B&gt; {} would work (I.e. have the base class parameterised), since this works in the non-sugared version of just writing `fn foo(&amp;self) { self.base.foo() }` over and over. 
You can do `struct Type { priv inner: T }` so that you're forced to go through `new`.
&gt; Exchange allocation headers (ie, the headers on ~T) were removed. This was a heroic effort by strcat and Luqman. Understatement of the week. Bravo to all involved for pushing this through.
It will be completely configurable in the future.
No, it's impossible to construct structs with private fields outside of their... module, I think. Might be crate.
Awesome, that's good to hear.
I didn't realize Rust task stacks have no guard page, in this case you won't run into the VM regions limit. However, not having a guard page seems like a step backwards. Does Rust have another way of preventing stack overflows from overwriting arbitrary memory?
It's module.
It should be noted that this is *only* on `~T` where `T` doesn't contain any managed types (i.e. `@`), and the headers have not been removed for `~[T]` yet.
Example: trait DoFooAndBar { fn foo(&amp;self); fn bar(&amp;self); } struct NetworkBaz { socket: Socket } impl DoFooAndBar for NetworkBaz { fn foo(&amp;self) {} fn bar(&amp;self) {} } struct LocalBaz { file: FileHandle } impl DoFooAndBar for LocalBaz { fn foo(&amp;self) {} fn bar(&amp;self) {} } struct BarLogger&lt;Baz&gt; { baz: Baz, message: ~str } #[delegate_to(baz)] impl&lt;Baz: DoFooAndBar&gt; DoFooAndBar for BarLogger&lt;Baz&gt; { fn bar(&amp;self) { println(self.message); self.baz.bar(); } } This would be even nicer if it could somehow hook into any recursive calls of the trait methods and make them a call on the "parent" self (i.e. calling the "sub-class"'s method), but that seems ridiculously infeasible.
Presumably the former: the client and server are written so that they understand each other.
What about things other than security? Diablo 3's economy was affected by an integer overflow: http://www.gamasutra.com/blogs/MaxWoolf/20130508/191959/ http://games.slashdot.org/story/13/05/08/2130203/integer-overflow-bug-leads-to-diablo-iii-gold-duping 
Nice ! I definitely think that Rust could be well suitable for doing numerics / scientific computing once it reaches maturation.
Any way to load it at runtime via name?
 use std::unstable::dynamic_lib::DynamicLibrary; fn main() { let name = Path("libsilly.so"); let libsilly = DynamicLibrary::open(Some(&amp;name)).unwrap(); unsafe { let silly = libsilly.symbol::&lt;extern "C" unsafe fn(i32)&gt;("silly_run").unwrap(); silly(1); } } Compile with `rustc silly.rs` and run with `LD_LIBRARY_PATH=. ./silly` (on Linux, at least).
Well, that makes it ~1500% faster, then.
I don't think my math is incorrect. `(before - after)/before` gives 0.93... Also, I thought `is_utf8` *was* correct. What is incorrect about it?
 after/before? 
I think you are using the wrong relation. https://github.com/mozilla/rust/issues/3787 This issue is pretty important, especially for the case of multiple possible encodings of "security-critical ascii chars" like directory separators, or I guess just about any character.
If you said "JSON parsing time reduced by 93%" it would be right. Not the same as "93% faster". Speed is "amount of data/time", which has increased by ~14x, what you're reporting is reduction in time.
Ah, I thought that was already fixed with https://github.com/thestinger/rust/commit/51eb1e14d4285f157e9820f5ee61bc150cf554ad, but that is something else.
Thanks!
Correct
http://www.reddit.com/r/playrust
I was wondering, since Rust usually expect ask isolation I would guess than a malloc implementation would necessarily imply the use of "unsafe" Rust ? In this case, would it be conceivable to "port" either tcmalloc or jemalloc straight from C since both have been heavily optimized to minimize contention and the number of instructions necessary when allocating.
Dang, all the awesome stuff is in America! :P Will there be videos of the conference? It looks quite interesting.
It's definitely possible; there are a few points that could be tricky (global mutexes without any allocations), but other than that, it'd be a tedious but neat project.
I found this to be one of the ugliest details about Rust's implementation, so I'm very happy to have killed it off :). When the garbage collector lands, the new header-free code paths can be extended to most managed types (`@mut` still needs to track freezing). Unique strings and vectors of non-managed types are now 32 bytes smaller (on 64-bit). They're just a pointer to a block of memory containing `[length|capacity|data]`. This is a follow-up to removing these headers from unique pointers, which are now represented as just a pointer to the data for non-managed types. It's exactly the same as calling `malloc` and `free`, with a check for out-of-memory. There's still some ugliness like having the headers on all closures (even stack closures!) but closures are likely going to be overhauled as a whole anyway.
Are you strcat on IRC?
\o/
The headers are 32 bytes of legacy data per allocation. They existed for garbage collection (a linked list is threaded through it with next and prev fields), and for setting a few status bits. They existed on owned allocations because those evolved from managed ones. EDIT: Whoops, 32 byte, not bit...
Yeah, it's 4 pointer-size fields. There are `prev` and `next` pointers implementing a doubly-linked list tracking managed and managed-unique allocations, to deal with cycles at the end of a task. This will be obsoleted by garbage collection, and isn't needed for a type like `extra::rc` with type bounds to stop cycles (`Rc` has a one-word header). There's `ref_count`, a `uint` used by managed pointers for reference counting. It also distinguishes managed-unique from managed at the end of a task, and used to distinguish unique vectors from managed-unique vectors at runtime before I added `contains_managed` for a static check. I think it's also the field used by `@mut` to handle freezing by borrowing some bits. Finally, there's a `tydesc`, which is basically a descriptor of type with enough information to destroy/copy/visit it. This is needed because the types aren't known while walking the linked list of pointers to free cycles. None of this *should* have been needed for unique pointers/vectors/strings, but it was heavily tied to the implementation and it took me a while to wean it off using them and then do the tricky removal.
Thanks for all answers :)
This is awesome. I've been hoping to use rust on embedded Linux targets like ARMv7A (Cortex-A9 and friends) when it becomes more stable. Running it on a a ARMv7M is more impressive yet!
 let x = Channelf64(-0.1); shouldn't it be let x = Channelf64(-1.0); ? 
Could you explain the advantage of rust malloc over something like jemalloc?
A vec slice is something like struct VecSlice&lt;T&gt; { ptr_directly_to_data: *T length_in_bytes: unit }
So the &amp;[T] is syntactic sugar that can be either a vector pointer or a slice structure?
(I'm not sure I understand your question exactly, but...) `&amp;[T]` is always a slice like the above, and is a separate type, not syntactic sugar; types like `~[T]` and `@[T]` auto-coerce to `&amp;[T]` which involves taking the pointer to the data and the length (at the other end of the pointer) and putting them into the struct, so it's not quite a no-op.
This should be helpful: http://static.rust-lang.org/doc/std/vec_raw.html#struct-slicerepr This struct is exactly the representation of a vector slice, byte-for-byte, and you can safely call `std::cast::transmute` to go between a `&amp;[T]` and a `SliceRepr`.
[**@metajack**](https://twitter.com/metajack): &gt;[2013-07-16 20:10:16](https://twitter.com/metajack/status/357230723100463104) &gt;Servo now supports floats. It's growing up so fast! [*pic.twitter.com*](http://pbs.twimg.com/media/BPUjczHCUAEPxrJ.png) [^[Imgur]](http://i.imgur.com/puPcdPR.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1igtdk%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) ^\(times ^are ^utc)
Quality stuff here
This is not programming?
No, it is a game with the same name, they accidentally come in here sometimes.
I like the `&lt;u16, .. 8&gt;` syntax: it makes obvious parallels with `[u16, .. 8]`, and allows for "auto-splatting" analogous to the fixed vector syntax: `&lt;0u16, .. 8&gt;`. Although it results in the possibly confusing `size_of::&lt;&lt;f32, .. 4&gt;&gt;()` (personally, I'm not bothered by this). --- Rust is turning out to be a really awesome numerical/scientific language: similar speed to C*, expressive, backed by LLVM (so relatively easy to port to target OpenCL directly, etc), and, a lot of smart people working on the numerical aspects (and the language in general)! */me hugs jensnockert, sanxiyn, bjz, ... everyone!* *Maybe even faster with the right tuning (for some tasks), since rustc has much more static information available to it straight from the types, no need to do complicated analysis to check for (e.g.) aliasing.
Sorry, but wasn't @ sigil planned for removal? Along with GC? I'm a bit confused. 
Yeah I assumed all `@ variable` will become `GC variable` or something. Thanks for clarification.
Making Rust awesome for numerical and DSP programming is - well - awesome! :) I would _love_ to see first-class SIMD and OpenCL support. My background is in image processing and graphics, so this is very interesting indeed. One consideration is that we would need some way of determining runtime support for features. OpenCL already has all this, but presumably we would need some sort of runtime feature detection support for SSE2 vs SSE3 vs AVX etc. I think not imposing arbitrary limits at this stage is important, as you point out. There are bound to be interesting hardware innovations in years to come, so being able to seamlessly support huge registers, a bazillion cores or something would be cool. 
I believe that SIMD types have more restrictions (and generally different/surprising semantics) than general vectors, e.g. SIMD types have different alignment requirements.
I need to start working with SIMD so I have an excuse to use the word 'swizzling'.
Not a dumb question at all, the types are very similar, but they don't map down to the same type in LLVM, which have some very different properties. [f32, ..3] is not padded to 16-bytes, &lt;f32, ..3&gt; would be for example. [RandomStruct, ..n] is a legal type, &lt;RandomStruct, ..n&gt; would not be. [~T, ..n] is a legal type, &lt;~T, ..n&gt; would probably not be. (LLVM allows vectors of pointers, so I guess it could work in theory) But in general, yes, the fixed-length vectors could probably be replaced by SIMD vectors without anyone noticing too much.
Yes, allowing someone to compile a crate multiple times and then pick the right one based on runtime checks would be cool.
That proposal would also be dedicated syntax, unfortunately. It could make developers a bit confused on why they can use an integer in making a generic type for SIMD, but not for any other types.
&gt; It could make developers a bit confused on why they can use an integer in making a generic type for SIMD, but not for any other types. That's actually one of the issues I find interesting, what problems prevent us from having this syntax (or even a broader support for [dependent types](https://en.wikipedia.org/wiki/Dependent_type) in general)? Is this just something that needs fixing in the current version (that may change in the future) or a general language design decision (why)? Perhaps at least a partial support for restricted notion of dependent types, like that in [Dependent ML](https://en.wikipedia.org/wiki/Dependent_ML) (natural numbers only) or [C++'s non-type template parameter](http://en.cppreference.com/w/cpp/language/class_template#Non-type_template_parameter) would be a feasible compromise?
I had forgotten the lack of dependent typing. Damned.
&gt; Perhaps at least a partial support for restricted notion of dependent types, like that in Dependent ML (natural numbers only) or C++'s non-type template parameter would be a feasible compromise? If someone would implement that, then I think it would be a good idea to utilize that facility. Ps. I don't think anyone has made a design decision to not allow it something like that in the future, but I could be wrong.
I talked to nmatsakis a while ago on IRC and he tentatively indicated that he saw no reason why constants couldn't be included in type parameter lists. This would also be super useful for other multi-dimensional generics, like general impls on fixed length vectors (ie. [T,..n]), which currently isn't possible. Also units of measure, perhaps.
I assume that is the simplest way to do it, but there could be linker magic that would allow a nicer way.
Either that or get a job as a bartender.
That is very cool - I hadn't seen that feature before. And now I have some ideas on how to use it at work...
Units of measure would be a cool addition. There's a Boost library that provides this for C++ now, which allows you to do things like specify a distance in metres, time in seconds, and evaluate the velocity in km/h. In the tutorial, it mentions how an `enum` could be used to implement something like this. Could ADTs actually enforce these sorts of constraints? Or would macros or something be required?
A plain enum wouldn't work (their variants are value-level, not type-level), but you can probably hack something together using traits and types (although expressing something like `m/s * m == m^2 / s` at the type level will probably be tricky). In any case, there are [a variety of Haskell libraries for units](http://www.haskell.org/haskellwiki/Physical_units), maybe one of them only uses features that Rust has.
(Note that `R` doesn't really need to be special cased, it is just the "1" unit, i.e. `m^0 s^0 ...`.)
Correct. Probably shoulda just said it was a scalar.
Ah, thanks indeed for the distinction.
Out of curiosity, a related question: what's the closest analog to [`std::integral_constant`](http://en.cppreference.com/w/cpp/types/integral_constant) in C++?
That's a very misleading image. It makes it look like Servo is passing Acid3, which it most certainly isn't.
It doesn't really exist (yet\*). So a direct encoding of natural numbers like https://gist.github.com/huonw/6034302 is as close as you can get. (There are various tricks you can do to define arithmetic on these [Peano-encoded naturals](http://www.haskell.org/haskellwiki/Peano_numbers) in Haskell, I don't know if Rust is powerful enough (yet*). There are also various tricks to use a more efficient encoding, e.g. binary rather than this unary encoding.) \* Pure speculation, I have no idea if Rust will ever have these.
Thanks for the reply, neat! Would be nice to have the general feature in the language, though :-)
Isn't that DST?
Update: Graydon is quite convinced that the best way to implement this is via generic tuple-like structs that are marked as lang_items. But such an implementation is blocked on generic tuple-like structs not working cross-crate.
Hi, I'm not too much in the loop about Rust development, but from what I've heard there is a chance `@` sigil will be overridable. There was idea that GC could be provided by an outside class/library. I'd definitely think the right option would be to have a trait that would override like rest of operators.
Dynamically sized types? No. DST is super cool: it lets you have [int] as a first-class type even though it doesn't have a known size. But it doesn't let you *prove*, at compile time, that the size of the result array (above) is the same as the int argument which is provided at runtime. (FWIW I'm not sure if/how you could prove this particular case in a dependently typed language: it's just for illustration. But it's something you could write. As it stands in Rust this is simply meaningless, you can't refer to a runtime function argument in a type, it doesn't reach the point where the compiler checks whether or not you've satisfactorally proved the type you're declaring.)
There's not anything inherently special about `~`, it's trivial to define in a library: [an example](https://github.com/huonw/rust-malloc/blob/master/malloc.rs#L219-L250). In the future, there will be some traits that you can implement that make these custom smart pointers "first-class", e.g. automatic borrowing to `&amp;T`. You *can* hook in your own definitions of malloc via "lang items", e.g. [the allocator used for `@`](https://github.com/mozilla/rust/blob/99b33f721954bc5290f9201c8f64003c294d0571/src/libstd/unstable/lang.rs#L63), and [the one used for `~`](https://github.com/mozilla/rust/blob/99b33f721954bc5290f9201c8f64003c294d0571/src/libstd/rt/global_heap.rs#L81). At the moment, overriding these loses you the whole stdlib because you essentially have to go the [`zero.rs` route](https://github.com/pcwalton/zero.rs). (I filed [#7901](https://github.com/mozilla/rust/issues/7901) about allowing overriding them, but I'm pretty sure it's unworkable with the current implementation of crates.) That said, this is probably an area that Rust is very weak at (at the moment), since there are a lot of other priorities.
Is your approach essentially an arena? That said, I'd be curious to know if Rust's borrow check gives people more confidence to allocate on the stack rather than the heap, which should reduce fragmentation in general.
Could you elaborate further on the differences? (Or pointing me to the relevant LLVM documentation is good too, if the Rust&lt;-&gt;LLVM mapping is clear.) (I'm going to use the `&lt;T, ..n&gt;` syntax to mean SIMD vectors because it's clear enough, but that shouldn't be taken as a vote for or against.) Is `&lt;T, ..n&gt;` just a set of restrictions on top of `[T, ..n]`, such as stricter alignment and which types are allowed as `T`? (And what else?) Is there anything that's looser for `&lt;T, ..n&gt;`, or just plain different? The benefits of conflating them look significant enough: you don't need to add a new type, anything you write generically for fixed-length arrays you also get for SIMD vectors "for free", you don't need to write separate `impl`s for the two, etc. Much less duplication and much better interoperability, basically. Having fixed-length vectors automatically use SIMD might also be a nice thing, though I'm not sure if it's always a win? Going by the differences you've cited so far, I could imagine a couple of possibilities: 1. There would only be `[T, ..n]`, and it would be a SIMD type whenever `&lt;T, ..n&gt;` would be legal (i.e. T is not `RandomStruct`), and if `&lt;T, ..n&gt;` would not be legal, then it would be a plain fixed-length array as today. This has the obvious drawback that the alignment requirements for some `[T, ..n]` would be gratuitously strict when you're not intending to use it for SIMD. So it's a tradeoff, with the previously mentioned benefits. But given that Rust puts a lot of emphasis on giving the programmer control over this kind of thing, it might be unappealing for that reason. (Maybe it could be finessed with `#[attributes]`, even though in some sense that's just shifting the complication around (because the alignment would still need to be part of the type to prevent bad things), but maybe it gets the defaults more-right?) 2. If the set of legal `&lt;T, ..n&gt;` is a strict subset of the set of legal `[T, ..n]` (i.e. it is indeed just a bunch of additional restrictions), `&lt;T, ..n&gt;` could be a subtype of `[T, ..n]`. That way in theory you still could get anything written generically for `[T, ..n]` for `&lt;T, ..n&gt;` for free, while being able to distinguish which one you want, though I'm not sure it would work out so easily in practice. Rust doesn't have this kind of subtype relationship at present I think, and it's probably for a reason. E.g. if you declare `impl Num for [T, ..n]`, even if you could use it with `&lt;T, ..n&gt;`, the return types would be `[T, ..n]`, so compositionality is lost. Just thinking out loud... this is one case where I can't look at how Haskell does it, because Haskell doesn't have fixed-length arrays (which is occasionally annoying) :-)
I think that depends on the use case. If you have a genuine ownership cycle, and are using weak pointers in one part of it with the hidden invariant that you need to be holding a strong reference to some other part of the cycle, then yes, the box being dead is a bug and there's nothing better than to fail. But if you genuinely don't want the weak pointer to keep the box alive (even GCed languages have weak pointers) then you probably have other plans.
The set &lt;T, ..n&gt; is a strict subset of [T, ..n]. But you shouldn't think of them like that, there are almost no operations that make sense on [T, ..n] that make sense on &lt;T, ..n&gt;. And the reverse, almost no operations on &lt;T, ..n&gt; do make sense on [T, ..n]. A [T, ..n] in Rust is placed in memory, but a &lt;T, ..n&gt; is placed in registers and the calling conventions are significantly different because of this.
You two knuckleheads got the wrong subreddit. This one is about the Rust programming language.
This is great feedback - I'll make sure he sees this. Maybe you should post your comment on his blog. This kind of critique is very constructive, and I don't think he would have a problem with it at all.
Not the author, but I can field some of these. &gt; it's best avoid using terms like "C/C++" I don't believe Niko was attempting to conflate the two languages, only to emphasize their similarities in that specific case. &gt; what are the preferred and recommended legal alternatives to illegal constructs shown on slide 17 and slide 24? It doesn't really make sense to "legalize" those examples, since they were constructed specifically to demonstrate forbidden behavior. In the case of slide 17, you would need to move the forbidden `table.insert()` to somewhere not in the scope of `value` (you could even move it just above where `value` is declared, within the inner scope delineated by braces). For slide 24, you could make it work by changing the `move_to` function to return an `~int`, and then change the `move_to(x);` line to `x = move_to(x);`. &gt; could you elaborate on "2. Memory disjoint tasks" on slide 16 Given that this in the context of GC, I'm not entirely sure what he intended there. However, perhaps the point was to emphasize the fortuitous side effects of our powerful ownership guarantees; given that owned pointers already need to have a single owner in order to be passed between tasks, why bother with GC-ing everything when the pointer, due to its unique nature, is trivial to free?
I appreciate the feedback. For what it's worth, I did point out in the soundtrack (which of course isn't "visible" in the slides) that this example was really more of a C example than C++. I intentionally avoided C++-specific smart pointers and the like to keep the example simple and accessible. That said, I agree it'd be nice to use the C++ idioms, if nothing else to demonstrate that the same dangers can occur even if you avoid C-like coding patterns (though most projects I've personally worked with still have a fair amount of "C-like" code in them). That was partly why I included a realistic example of using a C++ iterator. Perhaps the next time I give the talk, I'll experiment with updating the slides to use distinct syntax. Regarding the term C/C++, while I agree that C and C++ are distinct languages with distinct idioms and a distinct feel, there still share many important characteristics that are not present in most other widely used languages. So, in my opinion, it is not invalid to group them together; when it comes to type and memory safety, most any statement that applies to one, applies to the other. Regarding your questions: Slide 16: I elaborate on the meaning of my comment regarding "memory disjoint tasks" here: http://smallcultfollowing.com/babysteps/blog/2013/06/11/on-the-connection-between-memory-management-and-data-race-freedom/ Slide 42: Regarding constexpr, I'm not sure how it relates? My main point was that a C++ `const` reference (i.e., `const Foo&amp;`) is similar in purpose to Rust's `&amp;Foo`, but does not offer the same guarantees. PS, I don't normally read reddit, so I apologize if I don't write back promptly. :) I'll try to remember to check back though. Otherwise, feel free to comment on the blog itself.
it's a nice example of Rust's capabilities and I would keep it as an example in a Rust tutorial, but would not like to really have Mul implemented for strings in a standard library. Especially in a language that uses * also for dereferencing pointers. Just my 2 cents. 
Might be confusing with e.g. "2" * 2. "22", "4", 4? It's the same as the issue with JS' string/int concatenation: 2+'2'
The audience of this talk wasn't C++ programmers needing to be convinced that modern C++ isn't memory safe. It was instead a more academic audience looking for an in depth explanation of how Rust's memory model works. A talk along the lines of my comments on Reddit whenever somebody claims that C++ is memory safe would need to use more modern C++ idioms. That wasn't this talk though.
Excellent, thanks!
Please god no. This is one of those smart features that backfires in the majority of use cases. As someone who's been using python for ~a decade, this 'feature' is close to the top of my list of things to take out the back and shoot. I'm also not keen on overloading addition for string concatenation either. This feature could easily be satisfied with a method, like say "foo".repeat(4) without any down sides.
(`"foo".repeat(4)` already exists.)
&gt; ~a decade any reason why you put `~a` on the heap?
[**@metajack**](https://twitter.com/metajack): &gt;[2013-07-22 19:29:28](https://twitter.com/metajack/status/359394783610949632) &gt;Servo continues towards Acid1. [*pic.twitter.com*](http://pbs.twimg.com/media/BPzTpsvCUAAEk5I.png) [^[Imgur]](http://i.imgur.com/iF2PRFD.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1iv7ts%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) ^\(times ^are ^utc)
Good ol’ days…
For comparison see http://en.m.wikipedia.org/wiki/File:Acid1_reference.png
Just want to say thanks for posting these. I quite enjoying reading what's new in the world. 
&gt; The semantics of each of the three types of closures are slightly different. Only the latter two are true first-class values while the first one can only be passed to other functions, not stored or returned. Let's take a look at each one in turn. I would argue with this on the basis that a borrowed closure is no less usable than any other borrowed reference. I was surprised to see it could not be stored too, though that may be a limitation in Rust at the moment I would expect to be able to store it like any other borrowed reference: as long as I store it in a container which does not outlive the lifetime of the borrowed reference.
Hi! Sorry, I did not get you meaning: would you argue that borrowed closures are first-class? Or not? Here is an example of returning a borrowed pointer -- https://gist.github.com/alco/6064855. Can't build it atm, but assuming it's correct, it makes sense: by specifying the lifetime we promise that `c` will still be alive when `get_ptr` returns. With borrowed closures, you can't do this. Even though such a closure may be referencing a value that will outlive the current stack frame, the closure's environment itself (which is invisible to the programmer) lives on the same stack frame, and as far as I understand, there is no way to specify a lifetime for it.
You're creating a stack closure there, so the environment is on the local stack frame. Stack closures aren't represented in the type system so they just become `&amp;fn`, although `&amp;fn` can also refer to a heap closure or a stack closure further up the call stack. If you took an `&amp;fn` as a parameter to `foo` you would be able to return it. If stack closures *were* a first-class type, you would be able to pass them by-value via a move.
&gt; Had someone post a tutorial on Rust closures, but if we're deprecating them, we should get on with it Closures are being deprecated? How are thunks going to be worked into the mix?
Ok, that's an annoying restriction.
As far I know it's mostly because the environment has a dynamic size, so although the size is known at the point where you declare the stack closure, we don't really have a way to represent it in the type system. C++ handles this by giving each lambda a unique type and forcing you to pass them as generic parameters if you want to avoid boxing them as `std::function`. So you *can* have them contained by-value in struct, but the struct will also have a globally unique type.
[Discussion in r/programming](http://www.reddit.com/r/programming/comments/1ixnf6/benchmarking_roguelike_level_generation_go_rust/)
Thanks for emphasizing this, @strncat. I've edited the post to better explain the terminology I'm using -- https://github.com/alco/blog/commit/05af8a274544dfaf8e7e5f7e42fa6a815682e1aa.
The plan is still fuzzy, but IIRC only @fn and (and maybe ~fn?) closures are being deprecated. The full explanation is part of the overarching "dynamically-sized types" discussion, so it's not exactly easy to summarize... for the full story, see http://smallcultfollowing.com/babysteps/blog/categories/rust/ and read the articles between "Dynamically sized types, revisited" and "More on fns".
Also on [the D forums](http://forum.dlang.org/thread/mwgpybjozghhwhfgulis@forum.dlang.org).
Thanks, I'll get to readin'. I think I'd miss ~fn. I have a soft spot for returning runtime-generated functions. :)
I used Rust to solve the 70th problem from [Project Euler](http://projecteuler.net), and I learned a lot from it, and thought it might provide a nice ~150-line example of how to use Rust, for those looking for some examples a bit simpler than the standard libraries or Servo.
Thanks for doing this. I believe firmly that Rust's greatest weakness is the horrible documentation, and the complete lack of usage examples. The IRC channel is amazing to get help, though. I've always got answers to all of my stupid questions, usually within seconds, from the good people on there. All in all, Rust is fun to play with. With small example projects like yours, it will sure become much friendlier to newcomers.
There should be some flexible options left, to replace ~fn() and so on.
well... a decade is quite a heavy object to keep on the stack!
It looks pretty good, but let fullfac = do std::iter::product |f| { clist.iter().advance(f) }; should use [`product` from `MultiplicativeIterator`](http://static.rust-lang.org/doc/std/iterator.html#trait-multiplicativeiterator), or failing that, `fold`. use std::iterator::MultiplicativeIterator; let fullfac = clist.iter().product(); // otherwise let fullfac = clist.iter().fold(1, |old, &amp;new| old * new) (In general, the `iter` module should be avoided, `iterator` is much better.)
@true_droid: it may be worth clarifying that the case of an empty environment is a `&amp;'static fn`... or maybe it's `&amp;fn:Send`. I can't remember...
Personally, I quite like writing: do some_iterator.all |x| { a complicated thing } --- Also, removing `do` would mean that the following doesn't work do do do do do foo |x| { }.bar |y| { }.baz |z| { }.qux |å| { }.foo |ä| { }; which would be *almost* as bad as having less than 351 different pointer types. 
The article has been updated a few times (it currently (mostly) uses the same RNG, defined in each file); now it goes Clang, LDC, rustc, GCC, ...; also, in terms of [sloc](https://github.com/logicchains/levgen-benchmarks), Rust is only beaten by Haskell, Scala and the XorShift version of D, none of which define an RNG in the file. (Part of the problem(s) were Go using a very basic random number generator (which Rust also uses now), and using `uint` == `u64` rather than `u32` so `XorShift` had to do double the work, also fixed by the new code.)
Apparently I wasn't looking at the updated version... That's much more reasonable. I guess LDC codegen is more mature than Rust's... (Also great work rewriting that benchmark! I know that sort of benchmarking doesn't really measure anything, but it does affect perception of the language.) Edit: Though looking at the D version it appears that their version of [XorShift](http://dlang.org/phobos/std_random.html#.Xorshift) is using 128 bits... Edit^2: Ignore me! I see I've looked at the wrong D source code...
It's interesting comparing it to the idiomatic C++ somebody submitted (here: https://github.com/logicchains/levgen-benchmarks/blob/master/CPP.cpp), which has more than double the Rust version's SLOC (and yet is still slightly slower than Rust, if compiled with G++ rather than clang++). Also, I'm not sure how sloc is calculated, but could the fact that Haskell and Scala declare the struct fields all on one line rather than with one field per line affect their relative sloc scores? 
I believe the 128 bits refers to the amount of state (Rust's XorShift is the same: 4 `u32`'s). (Thanks :) doener and ChrisMorgan and a few others on IRC assisted too.)
I mention that closures with empty env have static lifetime, there is also an error message in the post that reads &gt; error: cannot capture variable of type `@fn:'static(int) -&gt; int`, which does not fulfill `Send`, in a bounded closure Couldn't have said it better myself :)
&gt; I believe the 128 bits refers to the amount of state (Rust's XorShift is the same: 4 u32's). That will teach me to speak about what I don't know... :)
the hype of mongo and rust? That's like going to the burger joint and when asked "pepperjack, american, swiss or blue cheese?" you answer with a resounding "yes" 
Is the video running around anywhere?
I'd keep my eye on this page: http://www.youtube.com/playlist?list=PL055Epbe6d5aclKNAa8msO1VvDOJ8sYlS
If only I was done with college... This sounds like my dream job T-T
Yes. No real limitations besides that you probably don't want to use the runtime, as you won't have the scheduler and stuff available (at least I don't think...). You declare a function that can be called from C like: pub extern "C" fn foo(...) -&gt; ... { stuff; } Make sure your arguments and return types are things C understands!
Yes, I was referring to Eric's research on Rust for the GPU.
Thanks for the information. I'm only just starting out with Rust so maybe this is a dumb question but, do I have to do anything special to disable the runtime? Or do I just need to avoid using certain features?
Just don't call anything that depends on it.
Basically don't use IO or `spawn` (anything that uses the scheduler). There might be a way to start the scheduler though, but I don't know it (I'd imagine it'd be whatever the Rust pre-entry-point setup code does). There's also zero.rs for completely runtimeless (including no libstd), a newer copy of it is at https://github.com/huonw/rust-malloc/blob/master/zero.rs. But if you want to be using Rust, you probably don't want to be missing out on libstd!
Here's an old post from Brian Anderson on writing a Rust lib that's callable from Ruby: http://brson.github.io/2013/03/10/embedding-rust-in-ruby/ The details may have changed in the months since then, but the general approach should be similar. Feel free to inform the mailing list or IRC channel (see sidebar) of your progress!
I believe the Rust team does take interns :) Don't hold it on me though. Edit: grammar
Does it log the errors 90% of the time? :)
The dollar sign actually goes *before* the amount, not after. (Note that some Frenchies in Canada do not follow the same convention as the rest of the world. If you are a French Canadian, please do not bother replying to this auto-generated message.)
Not a fan of this. `*T` is very explicit and matches what you would expect it to do from other languages. What does one gain from removing it and replacing it with some sort of other construct?
I'm ambivalent about this; I think it's worth considering only if we can keep the simplicity that `&amp;T` parameters don't alias (in LLVM terms, so the contents cannot be mutated by the current function, even through other parameters). Related to this: - bstrie was also proposing that we move to `Unsafe&lt;T&gt;` (mostly a syntactic change) - [erickt has a pull request that adds lifetimes to `*`](https://github.com/mozilla/rust/pull/7967) (this would make them safer, e.g. one can express that the pointer in `vec.as_imm_buf` lives as long as `vec`, rather than allowing it to be freely copied around).
`&amp;'unsafe T` isn't explicit? :) Advantages: - No need for a separate built-in type; `num_pointer_types -= 1` - `'unsafe` as the empty lifetime is elegant, and all of the desired semantics fall out logically - You can separately talk about "pointer which may be null, but is alive if not" and "pointer which is non-null, but is not guaranteed to be alive", whereas `*T` conflates the two - Nullable pointers are handled through the same `Option` interface as nullable values everywhere else. This is both safer and more consistent.
Considering many of my rust experiments currently involve bindgen I can tell you that going from `*T` to `&amp;'unsafe T` is just going to make everything more confusing. Aside from that is giving lifetimes special behavior confusing. &gt; You can separately talk about "pointer which may be null, but is alive if not" and "pointer which is non-null, but may not be alive", whereas *T conflates the two It's an unsafe pointer. You can't talk much about that because you don't even know what the thing is since it's externally managed memory. &gt; Nullable pointers are handled through the same Option interface as nullable values everywhere else. This is both safer and more consistent. I find that questionable given the nature of that pointer.
&gt; I think it's only worth considering iff we can keep the simplicity that &amp;T parameters don't alias (in LLVM terms, so the contents cannot be mutated by the current function, even through other parameters). I *think* that holds, though it might be a question of naming. Also I'm not sure whether or not dereferencing `&amp;'unsafe T` inside `unsafe { }` should be allowed directly, or if you should have to convert it to a non-empty lifetime first, though I lean towards the latter. But in any case: if you have an `&amp;'unsafe mut T` parameter you can't mutate *anything* through it, because it's not known to be alive. You need to use an `unsafe` block, and in the `unsafe` block it becomes the programmer's job to uphold the invariants. Really the only difference between `&amp;'unsafe T` and `*T` is that the former can't be null. So if something was true with `*T`, it should continue to be true. &gt; erickt has a pull request that adds lifetimes to * (this would make them safer, e.g. one can express that the pointer in vec.as_imm_buf lives as long as vec, rather than allowing it to be freely copied around). Yep, this was actually my jumping-off point. It seemed perverse to me to be adding lifetimes to `*` when the whole point of `*` is that it doesn't have any invariants. Once you do that what's the difference between `*T` and `&amp;T`? erickt seems to think (if I'm not misinterpreting his comments) that the point of `*T` is nullability, and that's indeed a difference, but to me that's what `Option` is for. After that the remaining difference is that `*T` requires an `unsafe { }` block to dereference or convert to `&amp;T`, which has meaning even if the pointer is known to be non-null, because you may not know anything else about it: represented by introducing the empty lifetime, `'unsafe`. I think that makes better sense too: what's the point of a lifetime on `*'a T` if you need `unsafe { }` to use it anyways? On `&amp;'a T`, at least a known non-empty lifetime has value.
&gt; Aside from that is giving lifetimes special behavior confusing. There's no special behaviour for lifetimes. Just as `'static` is defined as the infinite lifetime, `'unsafe` is defined as the empty lifetime. From there the existing lifetime rules apply with no change. &gt; It's an unsafe pointer. You can't talk much about that because you don't even know what the thing is since it's externally managed memory. Certainly you can say whether or not it's null: just test it! Which is why it makes sense to talk about that part of it separately from the properties of the pointed-to object, which as you say, you can't know anything about. &gt; Also one more thing: NULL is not the only special value a pointer can take. Tagged integers are incredible common, the same holds true for encoding more constants into low even numbers that are not multiples of wordsize. So why would NULL be specially represented through Option&lt;T&gt; but you still have a few million special values potentially sitting around? Because `Option&lt;T&gt;` exists and does what we want. The *default* assumption about pointers is that they may be null, but otherwise they're valid. You don't see people testing for tag bits every time they get passed a pointer as argument, they test for null (if they don't forget). Same thing here. If you also want to introduce the possibility of tag bits, you could: - Represent it as `&amp;'unsafe T`. It's guaranteed to be a valid pointer for the lifetime *&lt;&lt;empty&gt;&gt;*, i.e. it's not. So no problem. You won't be allowed to dereference it without going through `unsafe { }`. - Represent it as an `uint`. Again, requiring `unsafe { }` if you want to treat it as a pointer. - Perhaps best of all, introduce a new wrapper type, say `TaggedPointer&lt;'a, T&gt;`, which following the same philosophy as `Option&lt;T&gt;` encapsulates the unsafe code from above and presents a safe interface, letting you handle the tag bits and the pointer-without-tag-bits separately.
&gt; There's no special behaviour for lifetimes. Just as 'static is defined as the infinite lifetime, 'unsafe is defined as the empty lifetime. From there the existing lifetime rules apply with no change. Yes, there are special lifetimes like `'self` and `'static`. However those are still lifetimes. I see a lot of value behind giving a `*T` a specific lifetime. Just because it's unsafe memory does not mean that it does not have a lifetime. On the other hand `'unsafe` does not behave like a lifetime. I would not want to have everything in my rust be able to get an unsafe lifetime and them opt out of the safety in the language. &gt; Because `Option&lt;T&gt;` exists and does what we want. I don't see how it does what you want. `*T` says very clearly: there be dragons. You are dealing with something that looks like a pointer but we cannot guarantee that it is one or that it's valid. If you want to dereference it you need to be in an unsafe block. `&amp;'unsafe t` on the other hand now suddenly mixes and matches things. All the sudden a lifetime indicates that it's unsafe, that seems very much in conflict with what Rust is all about. This all seems to add a lot of complexity and confusion and removes functionality for no good reason. I like the idea a lot of giving a `*T` a lifetime because it does mean, that even if you are dealing with unsafe things, the compiler can still assist you. Sorry, I just can't see the advantage of your proposal. On the other hand just from playing around with your idea it makes every interaction with C libraries more complicated for no gain.
Aside: `'self` is not a sepcial lifetime. `'static` and (as proposed) `'unsafe` would require subtyping. Every lifetime is a sub-lifetime of `'static`, and no lifetime is a sub-lifetime of `'unsafe`. I don't really like this proposal either.
This isn't just a bad idea, it doesn't even make sense. As I keep telling people, lifetime annotations have absolutely no effect on the lifetime of data. Sure we have a "special" static lifetime, but that's because static variables are special. They are guaranteed to last for as long as the program is running. The proposed unsafe lifetime doesn't work with the borrower checker at all. You wouldn't be able to return it from a function without special casing. It also doesn't make sense as a lifetime. Static makes sense as a lifetime because it still represents the lifetime of some data. What lifetime does unsafe represent. Finally, there are more invalid pointers than just NULL. If I have a pointer to 0x04, that will very likely still segfault if I dereference it. There is more to memory safety than NULL pointers. 
One difference I forgot about is that `&amp;mut T` isn't copyable, while `*mut T` is.
Anyway, no one likes it so I retract the idea. Too bad you can't delete an email.
&gt; Finally, there are more invalid pointers than just NULL. If I have a pointer to 0x04, that will very likely still segfault if I dereference it. There is more to memory safety than NULL pointers. *Obviously*. But given that it now has the same representation, `Option` would be a great interface for checking for the ubiquitous sentinel value of `0x0`. If the pointer is not null that implies only that the pointer is not null, nothing more. I'm not sure why you would expect otherwise (or think that I would expect otherwise).
You shouldn't think that at all! It's still a really interesting idea and I'm glad you posted it. There may be something to your idea of a bottom region, even if this doesn't pan out. I spent all day thinking about your RFC and I don't think it was a waste of time at all, even though i ultimately concluded it wouldn't work.
+1 to erickt's encouragement. I think it was a super interesting idea, and we're always in a much better place when we've had several compelling alternatives to choose from. Regardless of whether it ends up being in the language, your idea at the very least is illuminating, and that's worth a lot!
This is a really good explanation, I am finally starting to understand rust pointer types just from the examples here. Understanding closures is a nice bonus too :)
Nononononono, it's great to have emails like this. The worst thing is when folks just sit around thinking in their own little bubbles and don't give their ideas a chance to 'mate' with others in order to create something incredible. Just because you may not have succeeded to day, doesn't mean your post won't have unforeseen knock-on effects in other people's minds.
&gt; erickt seems to think (if I'm not misinterpreting his comments) that the point of *T is nullability, and that's indeed a difference, but to me that's what Option is for. We already have `Option&lt;&amp;T&gt;` or `Option&lt;~T&gt;` represented as a single pointer, so I don't think nullability plays a part in this. I think reusing the same `&amp;` and `&amp;mut` syntax is going to be confusing simply because we'll now have to distinguish between pointers with an `'unsafe` and those without *whenever* we talk about any of the lifetime, aliasing, mutability or ownership rules. Raw pointers really have none of the invariants of borrowed pointers. They are not necessarily a non-owning reference, they do not have strict aliasing rules, they do not have freezing, there are no *strict* rules around mutability and finally the lifetime checking does not apply.
Raw pointers differ from borrowed pointers in far more ways than just the lifetime and nullability. A borrowed pointer is a *non-owning* reference to existing data, a raw pointer doesn't specify ownership as part of the type.
Option represents that either something is there *and it's valid*, or nothing is there.
`Option&lt;T&gt;` says that either a `T` is not there, or a `T` is there. That's it. Any other properties (the definition of "it's valid") are associated with the particular `T` type, and hold universally for that type, whether it's inside an `Option` or anywhere else. (So your sentence is literally true, but "and it's valid" is independent of `Option`.) `&amp;T` has all kinds of invariants. "It's valid" encompasses that it points to a T, it's alive, it's immutable, etc. `*T` has no invariants. "It's valid" holds trivially, any bit pattern is "valid". `SortedList&lt;T&gt;` is only inhabited by sorted lists, etc. All `Option&lt;T&gt;` does is that it adds an additional `None` case on top of whatever values `T` itself can have. Thought experiment: introduce `NonNullUnsafePointer&lt;T&gt;`. This is the exact same thing as `*T`, except we disallow it being `0x0` (with a smart constructor or whatever). Now the definition of "it's valid" is "it's not `0x0`". This holds whether we find it inside of a `Some` or anywhere else. Now we can take advantage of this fact and represent `None : Option&lt;NonNullUnsafePointer&lt;T&gt;&gt;` as `0x0`, because we know that's a value that NNUP itself can never have. And now we have that `Option&lt;NonNullUnsafePointer&lt;T&gt;&gt;` is both *isomorphic* to `*T` and also has the *same representation in memory*. Meaning that anywhere we expect one we can safely substitute the other, and it will be binary compatible. The only difference is the interface. With `*T` nullability is implicit, `0x0` is not distinguished from any other value, you can dereference it etc. With `Option&lt;NNUP&lt;T&gt;&gt;` it's explicit, `0x0` is distinguished and called `None`, and we are forced to pay attention to the possibility. The only reason this is beneficial is that it's common practice to distinguish `0x0` as a pointer that's "not pointing to anything". If we're not following that practice, then this has no value to us. But if we're distinguishing it in our heads, we should distinguish it in our code.
See also https://mail.mozilla.org/pipermail/rust-dev/2013-July/004975.html Rust is getting a crowded place. People working on similar stuff in parallel. :)
What a good pull request message.
It's probably overkill for the case of fmt!, but in my view an interesting and fairly well thought out templating library is StringTemplate by Terrence Parr: http://www.antlr.org/wiki/display/ST/Examples which as also been ported to Haskell http://www.haskell.org/haskellwiki/HStringTemplate
Looks like a nice position ;-)
A fan of everything except Oswald.
A search feature! This has been sorely missed.
From the article: &gt; There were impressively few breaking changes last week. Actually, I'm impressed when there *are* breaking changes --- those which continue to simplify the language, anyway. 
It looks like you're extracting doc comments. That's great. I'd *really* like to see them rendered as html (since those comments are generally markdown-formatted anyway). Also, I think the bold heading font (Oswald) looks good and goes well with the Rust logo. (Actually, I think the logo might look even better with just a slight gradient going from black toward that #8c6067 you're using ... maybe similar to the r/rust logo here.)
There are *always* breaking changes though :P. Usually library, though.
Yes, known bug that doc comments are poorly extracted in rustdoc_ng (https://github.com/cmr/rustdoc_ng/issues/4)
It looks like the markdown-formatted comments make it through to the static site, they just need to be further processed by a markdown-&gt;html program.
+1 :-)
Also noted by Graydon: https://mail.mozilla.org/pipermail/rust-dev/2013-July/005024.html
Original post: https://mail.mozilla.org/pipermail/rust-dev/2013-July/005026.html
I vote for sticking with the solid color. Simplicity is good, no need to complicate things.
Thanks, i like it. Also thanks to @cmrx64 for rustdoc_ng.
The syntax looks nice, but will it also be required for debug!() ? I am a little afraid that this could slow down my debugging.
I think it could be, e.g. `debug!("{} {?}", foo, bar)`; is this what you are referring to?
Though since it's a macro, it will be trivial to "bless" both the one- and two-argument forms so that they can be invoked as simply as: debug!(foo); debug!("foo: ", foo);
While refactoring the Iterators in the str module, I found it really nice how much you can reduce struct wrappers and Iterator implementations with clever use of the existing Iterator adapters and DoubleEndedIterators.
It definitely looks pretty neat. Since C++ allows to overload `operator-&gt;` it might be interesting to reach to the C++ community and check if there are known pitfalls. One particular example I have in mind is *double* dereferencing: `Foo` becomes `Bar` becomes `Gus`. Should it be automatic ? Or do you have a single level of auto-derefencement ? I also wonder how to handle ambiguities, in the previous example let us suppose that both `Foo` and `Gus` have a `x` field, so: - `foo: Foo`, `foo.x` is valid and refers to `Foo::x` - `bar: Bar`, `bar.x` is valid and refers to `Gus::x` (via auto-dereferencement) Right ? Okay, so now I take the expression `compute(bar.x)` and, since `Foo` can pass for `Bar`, I upgrade the parameter of my function and thus now use `compute(foo.x)`. No functionality change right ? Wrong... suddenly I am accessing a different `x` (namely `Gus::x` instead of `Foo::x`). And of course forbidding the case (compile-time detection of ambiguities) is not appealing, as the author of `Gus` who does not know what `Bar` and `Foo` are (oh, guys using my code) I don't want to receive a bug report saying: *"Hey you moron, you added a `x` field to your `Gus` class and it broke my code"*.
It's nice! Some remaining issues I'm thinking about: we can't clone the Map iterator at the moment, because we can't `.clone()` a closure (even if this one is 'static). But it's stuff that will be solved down the line I think. Since reversible iterators are so common, maybe the separate types and methods to create them can be removed.. the only problem is how the user will know which iterators are reversible and which are not.
I have to say, I caught myself writing `fn blah(blah): blah {}` a few times and thought "hey! why isn't that the syntax!", but inside a parameter list or other context where there's another colon nearby, it's immediately much less appealing.
Please note that `foreach` is just a *transitionary* keyword. For those new to Rust development, whenever we drastically change a core feature we have to briefly introduce alternative constructs like this to allow us to gradually eliminate the use of the old feature which we're seeking to replace. For example, nowadays we have the `fail!` macro, but `fail` actually used to be a keyword. But when making the switch we couldn't just define a macro named `fail!`, because keywords obviously can't be used as identifiers. So we did the following: 1. Introduced a macro named `die!`, with the same functionality as the `fail` keyword. 2. Did a massive find-and-replace of the old construct with the new macro throughout the compiler. 3. Removed the `fail` keyword from the parser. 4. Created a new stage0 compiler snapshot, so that our baseline compiler won't recognize `fail` as a keyword and thereby object to its use in identifiers. 5. Did a massive find-and-replace of `die!` to `fail!`. The joys of self-hosting compilers! :)
Please move the struct, function and such documentation back to the module page, with every one of them having a separate page it's much harder to read what a module does.
So basically we're using `foreach` temporarily until we can switch to `for`? That's good, `foreach` is too long.
I ported ~1200 old for loops to foreach and compile time has already dropped from 1832s to 1543s. :) http://huonw.github.io/isrustfastyet/buildbot/ There are still quite a few left to kill off though.
Yeah, it's entirely a temporary thing and should be gone in at most a week.
Was Dave Herman's question that should there be a mechanism for web developers to opt-in to have cross-origin iframe behavior(running in parallel, etc.) even for same-origin iframes? Tim Kuehn's answer seemed to be about how to do that automatically, which is a non-issue if authors explicitly annotate. Also, I thought HTML5's iframe sandbox attribute is such an opt-in mechanism. Am I right?
Yes, I think the question was somewhat lost in the IRC-to-real-life translation, and I think your answer is correct. :)
I asked dherman afterward, and yes I misunderstood the question. My fault! We had a small discussion regarding whether remote iframes basically enable this behavior already...and I think the short answer is they're not intended for this use case. 
What is a "remote iframe"? (And why aren't they "intended" for this use case?)
How does this work? I guess closures are harder to optimize/ harder to expand? 
Harder to optimize, and result in less-optimal code.
Thankfully Alex Crichton is rewriting `fmt!` right now :)
Isn't it possible that the random number generator is heavyweight in Rust? Indeed, Linux `random(3)` call uses a nonlinear additive feedback RNG, which is (I think) way simpler than Rust's ISAAC RNG.
I asked this on IRC too, but, as cmr pointed out, there is little time spent in the RNGs for either. (So it is a problem, but nowhere near as much of a problem as in the last language showdown.)
I only learned about them today from David Zbarsky's presentation, but the way he described them is that if an iframe is tagged as remote, then it is run in a separate process. I believe this is currently only available on Firefox OS and likely is only fully supported for cross-origin iframes since same-origin frames can modify each other's document properties.
It would be great to have both.
Looking at the annotated assembly in perf, a lot of time is spent in bounds checking in `int_to_str_bytes_common`. Unfortunately, trying to use `mut_iter()` to loop over the buffer made things even slower.
http://slid.es/timkuehn/architecting-servo-pipelines-and-parallelism
I was not under the impression that the remote attribute was honoured from web content - it's purely an implementation detail on the part of Firefox.
Why the randof macro? Afaict a function would do.
That site could drag a super computer to its knees at times.
Because it was already in the code and I'm lazy.
My sincere apologies. (this doesn't sound sincere via text, but I meant it!)
http://people.mozilla.org/~ptheriault/FirefoxOSSec/?full#remote I could still be mistaken, but it seems to me that it's intended to be an API?
(sorry if it came across harsh. grumpy today from fighting rustc)
Heh. Understood. I'd have the same reaction. Gate it when people get stuff wrong like that when its clear i was wrong. I'm happy to say sorry!
Use "&amp;amp;mut self". For the other issue ill defer to those more knowledgeable than myself but it seems to not like returning a reference to self. Not sure how to resolve that or if its a limitation/decision/oversight.
Answered at http://stackoverflow.com/a/18028901/497043.
We should have a better error message when you try to return `&amp;Vector` without giving a lifetime.
I transitioned all 1500 or so `for` loops in the codebase to `foreach` (and ~80 to `do`, due to missing iterators) so the `for` keyword has now been replaced with one using external iterators. The `foreach` keyword will go away after the next snapshot and I plan on doing that ASAP. http://static.rust-lang.org/doc/tutorial-container.html#iterators
Will there be an `Iterable` trait so that we can do `for i in ~[1, 2, 3] {}` instead of `for i in ~[1, 2, 3].iter() {}`? I had heard that there would be, but am not totally updated on the subject.
It's planned, but it's looking like it will be tricky to get to work properly; specifically: being able to implement it for `Iterator` itself.
Thanks! :)
I didn't expect this to make /r/rust :) Keep in mind that this requires a patch to rustc (the no-main patch) and a patched SDL (which I haven't yet submitted upstream because it depends on the Rust patch). So don't expect this to compile quite yet.
Yes. That's been possible for a while, actually (see the Embedding Rust in Ruby post, for example).
(The patch in question looks to be this one: https://github.com/mozilla/rust/pull/8279.)
I just ported the examples from the machine learning course as I stated in the readme. 
Interesting, never seen that ticket. (The Rust of two years ago has so many strange and unfamiliar things... the other day I stumbled on the old notion of object types.) I'm not sure what happened in the interim, but the Rust of today does not have higher-kinded generics. So there presently isn't a better way than what you did.
Your criticism is very welcomed, but it was too aggressive. Instead of "Your code seems to sucks" you should have said "Cool! Here some advises to improve your code:" and instead of "why is/isn't..." it would be nicer to say in a way such as "Instead of HouseData it would be better to just use a pair" 
Thanks. One thing that surprised me was that this also doesn't work: trait Functor&lt;A&gt; { fn fmap&lt;B,O&gt;(&amp;self, f: &amp;fn (a: &amp;A) -&gt; B) -&gt; O; } It seems naively as if something like that should be preferable, because otherwise (or, again, so it seems) functions accepting &amp;Functor arguments or that are themselves generic over functors would have to specify not only A, the type that paramaterizes the functor but also B and O, which must be extremely limiting. (e.g. I can't figure out how you'd define something like: Prelude GOA&gt; let f x = if True then fmap length . fmap show $ x else fmap (+1) x Prelude GOA&gt; :t f f :: Functor f =&gt; f Int -&gt; f Int where depending on which branch you take you pass fmap a function Int -&gt; String or a function Int -&gt; Int.) E(again)TA: the problem with parameterizing fmap seems to be a parse error. The compiler accepts the trait definition, but barfs on impl&lt;A&gt; Functor&lt;A&gt; for Option&lt;A&gt; { fn fmap&lt;B,Option&lt;B&gt;&gt;(&amp;self, f: &amp;fn(a: &amp;A) -&gt; B) -&gt; Option&lt;B&gt; { match &amp;*self { &amp;None =&gt; None, &amp;Some(ref s) =&gt; Some(f(s)) // cargo-culted out of option.rs } } } saying that it expects a , rather than a &lt; in the &lt;B,Option&lt;B&gt;&gt; part.
Giving encouraging feedback is a learned skill. Thanks for providing good feedback to the overly negative feedback above. 
&gt; Thanks. One thing that surprised me was that this also doesn't work: &gt; &gt; trait Functor&lt;A&gt; { &gt; fn fmap&lt;B,O&gt;(&amp;self, f: &amp;fn (a: &amp;A) -&gt; B) -&gt; O; &gt; } Translating that to Haskell syntax (sorry, I just find it easier to think about) we get: class Functor a self where fmap :: forall b o. (a -&gt; b) -&gt; self -&gt; o but there's no reasonable way to implement it: we have to be able to return an arbitrary `o` result type, not determined by the others in any way. &gt; the problem with parameterizing fmap seems to be a parse error. The compiler accepts the trait definition, but barfs on &gt; &gt; impl&lt;A&gt; Functor&lt;A&gt; for Option&lt;A&gt; { &gt; fn fmap&lt;B,Option&lt;B&gt;&gt;(&amp;self, f: &amp;fn(a: &amp;A) -&gt; B) -&gt; Option&lt;B&gt; { &gt; match &amp;*self { &gt; &amp;None =&gt; None, &gt; &amp;Some(ref s) =&gt; Some(f(s)) // cargo-culted out of option.rs &gt; } &gt; } &gt; } &gt; &gt; saying that it expects a , rather than a &lt; in the &lt;B,Option&lt;B&gt;&gt; part. This is the same as saying `forall a (Maybe b). ...` in Haskell, so the compiler is justified. &gt; It seems naively as if something like that should be preferable, because otherwise (or, again, so it seems) functions accepting &amp;Functor arguments or that are themselves generic over functors would have to specify not only A, the type that paramaterizes the functor but also B and O, which must be extremely limiting. Yeah, the "right" solution would be implement higher-kinded generics. :)
Just implement iter() in terms of clone()? As far as I can tell they are equivalent, and Iterable can be implemented for an Iterator if and only if Cloneable can. But they should use different syntax, since the iterator version has side effects (changing the iterator) and the other doesn't. Perhaps "for &lt;x&gt; mut &lt;iterator&gt; {}" and "for &lt;x&gt; in &lt;iterable&gt; {}" ? 
They aren't equivalent. An iterator is definitely iterable (little i), even if it isn't clonable.
Awesome post that I missed until it appeared in TWiR 9.
It is a combination: it uses LLVM to check that the condition is always true or always false (in a limited way, but more than just checking for `if true` and `if false`), but then doesn't trans* the branches that can be eliminated, so LLVM never sees them (as you say). \* translate from the Rust AST into LLVM IR.
Ah, thanks. The Haskell translations are helpful---I have to admit I haven't found the documentation incredibly clear about this stuff (and some other things, actually, but I don't want let loose w/ a flood of questions just yet).
Do great awesomeness and thou shall be punished be ending up on /r/rust
In part the documentation is poor because our features are unstable. For example, we do currently lack higher-kinded generics, but this is mostly because no developer has ever had a pressing need to implement them, rather than an intentional rejection of the concept. If someone were to take the time to implement them (hint hint) they may very well be accepted. For the time being, if you have a large volume of questions I encourage you to check out the IRC channel and the mailing list (see sidebar for links).
(Which has now landed.)
You should post them here/on the mailing list! :)
I'm sure this project is really neat, but it's not all that apparent, given the lack of an actual README. Could you either explain what this does, or add a useful README to the repository? Thanks! 
planet.rust-lang.org like planet.haskell.org (and p.python.o, p.mozilla.o, p.debian.o, etc, etc).
It does: http://michaelwoerister.github.io/feed.xml
Oh, thank you. (It's not linked, right? I'm not going blind?)
[**@metajack**](https://twitter.com/metajack): &gt;[2013-08-06 02:18:49 UTC](https://twitter.com/metajack/status/364571230331875331) &gt;All the acid1 boxes are there. ~3 bugs left to fix before we pass. All small. [*pic.twitter.com*](http://pbs.twimg.com/media/BQ83mjfCMAI2_lP.png) [^[Imgur]](http://i.imgur.com/40U5wvs.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1jsn9g%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
I added the RSS feed just yesterday. You can get the RSS icon back in FF: http://www.webmonkey.com/2011/01/firefox-4-ditches-the-rss-button-heres-how-to-get-it-back/
Very, very encouranging :)
Well, Gecko managed to scale from Acid1 to Acid3. :P But I think the more important thing is that Acid1 is a significantly simpler test than the subsequent ones, and the functionality required to render it properly is probably necessary for them as well. It's just an interesting milestone, is all. Soon we'll be able to properly render pages from 1998!
You kind of need to go in order; each acid test builds on stuff in the previous. But that said, we do a lot of things out of order, so that life won't be hard for us in the future. For example, you don't need JavaScript or incremental reflow to pass Acid1 or Acid2, whereas you do for Acid3, but Servo supports both to varying degrees. (In fact, it supported JavaScript before any of the stuff that Acid1 tests.)
Interesting stuff. I don't wanna bikeshed but @ for denoting lifetimes looks better than current.
If @ goes away as a pointer type, I'd tend to agree.
I dunno, `&amp;@a` looks gnarlier than `&amp;'a` to me.
&gt; Tragically, this stuff doesn't really exist yet in a usable language, nor has anyone proven it to work programming at large scales. Sigh... And after you've had the obvious reaction, note the date it was posted.
I meant the old `&lt;Type@Lifetime&gt;` syntax.
Tim back then suggested to use http://planet.mozilla.org/research/
The `'self` thing is a bug, and the restriction is actually *required* to be removed, because there are [use cases](https://github.com/mozilla/rust/issues/4846#issuecomment-20590745) for multiple lifetime parameters on a single type. (Use cases which have been "accepted", nmatsakis is working on it, I think.)
http://www.coyotos.org/pipermail/bitc-dev/2013-July/003574.html &gt; Unfortunately the performance is not where it should be its at about java level , stdlib , poor gc and segmented stack has been blamed and they are now working on those ( going back to a normal stack with the segmented an option) ) . Even if Rust was neither faster nor slower than Java for good, that would be a win to me already. Rust provides more memory safety (having no iterator invalidation nor NullPointerExceptions) and does not depend on an external runtime.
Interesting thoughts about a region system, but there are some points they missed with GC - it's not just performance its also accessibility (i.e. calling Rust or calling other code from Rust).
I've posted this as a discussion point - not really as a statement for/against.
I clearly don't understand many things here, or they are a product of a confusion in someone's head (possibly my own!): &gt; Owned pointers should be expected to have similar retention problems, though at least without the consequent collector overheads. Owned pointers essentially have the shortest lifetime possible: they get deallocated as soon as they go out of scope, i.e. as soon as they are unused. &gt; (b) Rust advocates the wrong default (owned pointer), and (c) for most purposes it's completely unnecessary I assume that Shapiro is saying the "correct" default is GC'd pointers. I'd argue that GC is completely unnecessary for most things; the only time I've used GC pointers in all the Rust code I've written is when I've been unfairly coerced to by the other code I've been interacting with (`std::io`, I'm looking at you). In any case, GC is a disease: as soon as a single piece of code that you call uses GC, then your app (or lib) needs the entire GC (and so does everything that uses that lib, if it is one). Owned pointers/stack allocations are the least impact/lowest common denominator pointers. I get the feeling that the "owned pointers" that are being discussed here are different to the implementation in Rust.
It seems as though Shapiro understands neither our unique pointers nor our "region system" (which he seems to believe has something to do with allocation, possibly due to mistaken conflation with arenas). Furthermore, his assertion that unique pointers were the wrong "default" pointer is baffling, as his argument is that these are less capable and flexible than managed pointers. He's missing the bigger picture, in which task isolation means that GC'd data cannot be shared, lack of clear ownership means that inherited mutability cannot work on GC'd data, and borrowed pointers with built-in notions of mutability mean that mutating GC'd data through a reference can lead to dynamic failure. In short, his assumptions about GC completely fall down in the face of Rust's particular combination of features. Maybe we can get pcwalton over there to set him straight. :)
To be fair, region systems usually *do* have to do with allocation. That's why Niko prefers the term "lifetime". But I do take issue with his notion that the region system is "half-assed"—in fact you can get classical Tofte/Talpin style regions out of Rust's lifetime system, with the arena module.
Hmm, what's with the sudden appearance of BitC mailing list here? Slow rust day?
ipc on #rust mentioned that they were discussing Rust. We can't have folks talking about us behind our backs!
That community has a lot of experience in this space and it's interesting to read their insights. I saw that Shaphiro had started this subthread on what it means to be a systems programming language after reading bjzaba's link, and thought other /r/rust readers might be interested or want to discuss.
Rust isn't quite a systems programming language yet. It is getting close though. What it needs is an explicit way to arrange things in memory. Structs that are packed/unpacked aren't quite enough. You also need the equivalent of C (untagged) unions so that overlapping fields can be described. Rust also seems to lack a well-defined ABI. This is understandable due to its age and rapid development. However, a true systems language needs to have a way of describing well-defined interfaces at the binary level. At that point, other languages can then think about interfacing with you, rather than vice-versa.
&gt; You also need the equivalent of C (untagged) unions so that overlapping fields can be described. It's possible-ish now, all it needs is a liberal application of syntax extensions to make it look nicer: use std::cast; struct IntOrFloat { priv data: [u8, .. 8] } impl IntOrFloat { fn i64&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut i64 { unsafe {cast::transmute(&amp;mut self.data)} } fn f64&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut f64 { unsafe {cast::transmute(&amp;mut self.data)} } } fn main() { let mut i_f = IntOrFloat { data: [0, .. 8] }; *i_f.f64() = 1.2345678e234; printfln!(*i_f.i64()); } &gt; ./untagged-union 8108970413479199786 
Well, you use references a lot, I assume? Shapiro is also objecting to Rust references and their lifetime restrictions, in favor of some system that treats everything semantically as a GC pointer and infers borrowed-ness as a compiler optimization (as Java/.NET/Go/etc do).
We've got plenty of languages with GC. Can't we have one more without it and see how far we can go?
I guess the rust object files still needs those two symbols to be defined.
I would ask though: *should Rust be a system programming language* ? I can understand the appeal of being able to write drivers/kernel modules in Rust, but it seems unnecessary to be a great language for writing high-performance applications.
The problem, as the OP acknowledges, is that "systems programming" has become a muddled phrase. One camp uses it to mean "anything that you'd use C++ for" (e.g. demanding, high-performance applications) while another uses it to mean "anything that you'd use C for" (e.g. kernels, drivers). The latter definition is probably closer to the original meaning, but given that Rust's target is C++ users--and that C++ users consider C++ to be a "systems language"--Rust must be able to make the same claim (at least with respect to the capabilities of C++), or it's dead in the water. Better to abandon the phrase "systems programming" where you can and simply say whether a language is intended for the high-performance application domain or the embedded domain. Rust is firmly in the former, though that doesn't mean people won't try to use it for the latter (they've ported Linux to Javascript, anything is possible!).
There are a *lot* of posts about Rust in there. It would be interesting to see a Rust dev answer some of the questions (there really do seem to be a quite a few misunderstandings going on).
I believe pcwalton has replied but is stuck awaiting moderation. :(
I'm not sure I understand, but in theory (assuming I haven't typoed), the `unsafe` in the above code doesn't break any of the rules about `&amp;mut` (aliasing, etc.), which has the automatic consequence that the mutable forms of the pointers don't alias. If there were `-&gt; &amp;'a i64` and `-&gt; &amp;'a f64` methods, there would be the possibility of aliased pointers with different types; however, the type system means all such pointers at any given time would be immutable, and so they still satisfy the LLVM definition of NoAlias. (If there was `-&gt; &amp;'a const T` methods, then this would be a different story, since `&amp;const` can alias with `&amp;mut`; however, `const` is deprecated/slowly being removed.)
Oh, you're completely right about `&amp;mut`, my bad. What C/C++ actually say is that it's illegal (with a [few exceptions][1]) (and as usual, unenforced) to read an object out of memory as a different type than it was written/allocated as. If you were to directly translate IntOrFloat into C then it's the standard example of what you're *not* allowed to do (basically this is what C has unions for). [1]: http://stackoverflow.com/a/7005988 The question is whether Rust does and will continue to avoid making similar assumptions... or I guess, more interestingly, do Rust's mutability and aliasing rules make it superfluous? I.e. maybe C's aliasing rules are not a subset of Rust's, but their intended effect is subsumed by them?
I tried with `#[fast_ffi]` and get a link time error without include zero.c: mylib.rc:(.text+0x4f): undefined reference to `upcall_call_shim_on_rust_stack'
&gt;Owned pointer is the wrong default because we teach general-purpose programming, and the semantics of an owned pointer isn't general purpose. Doesn't a huge performance win in general purpose GC implementations come from generational hypothesis - most objects have short lifespans - this maps to owned pointers really well - you have objects with a single owner that gets passed around and can be shared trough bound references - actually needing to use a GC when you have a nice system to express owned pointers becomes the edge case and also the mechanism you want to use is situational as they all have specific tradeoffs (reference counting, task local GC heap, shared GC heap, etc.) - so I don't see the need for special syntax just for a specific kind of box. Also if you can isolate the usage of the GC heap I suspect that you can use a trivial tracing collector implementation for task local GC heap because generational hypothesis likely doesn't hold and the heap size to scan should be small so no need for a incremental scanning. Since it's a task local heap stop-the-world only means stopping the task owning the heap being scanned - so the pauses are isolated. This trivial implementation means it's simpler to implement but also you don't have to deal with hidden overheads such as write barriers. Also strict type system should allow you to do a precise collector that can skip a lot of memory (type system is able to figure out if the type contains GC heap references).
Why not have one both with and without it. You should be able to disable functions that cant be used without GC and still write good code.
That's what Rust is looking like at the moment. (I think we don't currently support disabling the GC nicely, though.)
I think my biggest problem with the current crop of languages is that we need a new language designed to be able to write both an OS and the applications on top of it. We still have lots of Security flaws and segfaults coming from the OS due to the same problems seen in user land applications but that requires the ability to shut off high level features for segments of code. GC, Dynamic allocation, etc. That and personally as someone who deals with enterprise programming on a regular basis (sadly in java), I must say trait based single inheritance is something I just cant do without for a lot of things.
 A Systems programming language is a language that can be used to create a bootable system without any outside dependencies except hardware specific assembler. If it doesn't meet that requirement then its an applications language.
Has anyone noticed any performance changes with the new runtime (for the better or worse)? Can we now use Emscripten?
Maybe he's talking about LLVM JIT?
What about approaches like https://github.com/charliesome/rustboot ?
I believe this is an anomaly, but we'll find out when the next builds land.
Oh, indeed. It appears that `#[fast_ffi]` currently has no effect on foreign-abi functions written in Rust, only on declarations of actual external foreign functions.
I'm confused by that, because there were two builds of the newrt (both of which succeeded), i.e. there is two data points essentially on top of each other; so I have a feeling that the slowdown of the second one is due to the buildslaves getting overloaded, or something.
That is basically a blog post around the already posted example from http://www.reddit.com/r/rust/comments/1jvqvf/an_minimal_example_of_calling_into_a_rust_lib/
What were the first browser versions that passed Acid1? Like IE5 or something?
I really hate reading Haskell function definitions of (a -&gt; b) -&gt; a -&gt; b I don't even know where to start reading, and everything's curried anyway. Does anyone else have this experience or is it just from not being used to it?
Not sure why it'd be confusing, it has nothing to do with string/int concatenation and is simply a shortcut for the `repeat` method. And the type of the result is clearly defined, there's none of javascript's ambiguity. Though because it's just a shortcut for the `repeat` method, unless it's constant-folded I don't really see the point: in Python — which has this — the only place where I use it is doing printf-debugging to insert "separators" e.g. print '&lt;' * 60 print information_to_debug ... print '&gt;' * 60
We can't quite do that because we need at least the C wrappers (written in C++) for parts of LLVM that don't have adequate C bindings.
Which is useless, as a requirement, really. Because you could actually write a "simple" bootable runtime in assembly and then use Python or Javascript on top and claim that they obviously are system programming languages since they satisfy this requirement.
The only type of pipes you'd get are flat pipes (with serialization) which would translate into `postMessage`, so I imagine most tasking Rust code would have to be slightly ported. But yeah, you could use Web Workers. You'd need a new scheduler backend though. (Thankfully the new scheduler is designed to have pluggable backends.)
We might be able to upstream it all, yeah.
Golang said "systems language" about itself as well, meaning something like "Anything you can use Java for".
I am very curious about rust and follow rust news occassionally. Thanks for the code sample. I have a rather n00b question. I was very surprised by this code: impl HouseData { fn new(size: float, cost: float) -&gt; HouseData { HouseData{size: size, cost: cost} } } That looks like a very menial constructor. Is that boilerplate required or is there a simpler way in Rust?
Was just noise: http://huonw.github.io/isrustfastyet/buildbot/#9f11018ebba448b0802eadd52cb5fb863f3ffbd7
thank you for the update
I'm following master since bugs are fixed and niceties are added all the time. Rust is a very experimental language at this stage and it's better to keep up with it than let your code stagnate with timed release versions IMHO.
Two of the biggest problems with targeting the point releases are that no support is provided (nobody keeps old copies of the compiler installed), and bugfixes aren't backported. The first means that if you have a question that's 0.7-specific, nobody will be able to help you. The second means that if you hit a bug, you're SOL. I'd recommend that people track master whenever possible. But the even wiser solution might just be to not use Rust at all until things calm down. :)
If you're on Ubuntu, there's a [Rust PPA](https://launchpad.net/~hansjorg/+archive/rust) with point releases and nightly versions where you can easily switch between versions with update-alternatives.
The only required constructor is the part that looks like: HouseData{size: size, cost: cost} ...so if you had a struct like this: struct Foo { x: int, y: int } ...by default you'd construct it like this: let bar = Foo { x: 1, y: 2 }; The only point of the code in your post is to allow one to do the following: let bar = Foo::new(1, 2); It really only makes sense to do if you expect to have some sort of custom logic every time that you create an instance (or if you anticipate that you might one day have custom logic, and you don't want that to break the API).
This was very interesting presentation, thanks. Does anyone at rust team work on implementation of Chase-Lev Deque at the moment?
Linux 64-bit got faster.
It indeed is referring to &amp;self.lvl. In general, taking a mutable pointer when an immutable pointer is live is not safe, because mutation can invalidate an immutable pointer you took. In this case, since gm.lvl and gm.actors are disjoint, I think what you want is actually safe, but the compiler can't see that; that spawn_actor only takes an immutable pointer to gm.lvl, not gm itself, is not reflected in its signature, and the compiler doesn't look into the method beyond the signature. After all, the method can be arbitrarily complex.
This works: let actor = Actor { lvl: &amp;gm.lvl }; gm.actors.push(actor); You can put that in Actor::new as a method too, as long as it takes &amp;Level, not &amp;GameManager. The reason it can't be a method of GameManager is that then you are taking &amp;GameManager.
It's freezing the object while you have an immutable loan. If you were allowed to mutate the object, you'd be able to change gm.lvl, and that would change the contents of lvl in the Actors. Solution: Don't have `Actor` contain a reference into the `GameManager`, thus not freezing it.
I personally would use less `use name::*`. Instead of a wildcard, list names you use.
This is really cool! (Btw, [`#[Deriving(Hash)]`](https://github.com/peterdn/Galvanized/blob/master/basic_block.rs#L12) does nothing, it's being silently ignored by the compiler; to get an automatic hash implementation, it's `#[deriving(IterBytes)]` (`Hash` is implemented iff a type impls `IterBytes`) but I see that you've got a custom implementation of `IterBytes`.)
What could invalidate my pointer for example?
Aha, thanks, that explains why I got confused at that point! Is there any way to get the compiler to issue warnings for unrecognised attributes?
The big highlight is of course the new runtime, I am really looking forward to the implementation of the Chase-Lev deque and generally the optimizations left up... it seems promising already and it would be great if it could deliver really good performance. I've noticed that (surprisingly) a [number][1] of [people][2] were also investigating the Go runtime, and maybe it could be worth stealing some ideas from there. [1]: https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit [2]: http://www.cs.columbia.edu/~aho/cs6998/reports/12-12-11_DeshpandeSponslerWeiss_GO.pdf
See https://github.com/mozilla-servo/rust-png/issues/3 for the png issue. One easy workaround right now is to copy the relevant headers and library into /usr/local/{include,lib}; setting CXXFLAGS and LDFLAGS might also work.
Neat! That's pretty cool that reddit looks decent.
Or sad, because that means Reddit uses a lot of floats :(
Might even be nice just to have some way of specifying 'known attributes' -- even something as simple as a flag / file that gets read by the compiler and then spits out a warning when one of the attrs isn't on the list.
Yes, but it somehow has this 90ies touch. :) I have filed an issue for the PNG transparency propblem. It seems the alpha channel is ignored.
This is a problem with stb_image; the solution is to replace it (which we need to for other reasons).
Thanks for the explanation. My point was that the language should have sensible defaults to avoid boiler plate. 1. If I don't declare a constructor, it can assume a bare bones constructor is defined. 2. If I don't use new( ) but the class has a custom constructor defined, then use that rather than the bare bones one. ..etc.
Analyses: Rust: Bootstrap uses `[class*="span"]` selector to set `float: left`. Table. `white-space: pre` (#115). Reddit: Background covers sidebar (#677). Tabbed menu: background is not applied to inline boxes. Asymmetric border colors (#689). Vertical alignment (#226). "want to join?" needs `position: absolute`.
This looks interesting because it means people are considering looking at Rust as a possible replacement for C in the embedded programming arena. Right now when doing embedded development for microcontrollers (where there is no OS) your options are really only C. And the more I know and learn C, the less I enjoy it. The problem is C++ generally isn't an option and has its own problems. I'd love to use a modern language for embedded development.
Actually, seeing as Java is touted as being used in embedded programming (Java is used for your phone &amp; car, fear!) I would think that C++ *can* effectively be used in all those places. I suspect that the main attractiveness of C are: - really minimalistic runtime: for really small/low-power devices - existing (buggy) compilers for weird targets that nobody bothers generating code for I do agree though that getting Rust working on bare metal would be sweet.
This was posted in r/programming, however given the current work on the scheduler/IO-subsystem of Rust I believed it could be interesting to have a peek at the bottlenecks they faced to avoid making the same mistakes or draw inspiration from their design.
Interesting that they feel libuv is not sufficient due to lack of concurrency. I believe the current rust plan involves libuv? 
Java doesn't matter at all in the microcontroller-programming sphere. C++ does exist, for example there is a compiler for the new PIC32 architecture by Microchip, and probably for other micros. The problem is that it's not common-place. C has a huge head-start and C++ has a MASSIVE runtime, which makes it quite undesirable for embedded uses. We're talking systems that normally don't use malloc(), so dynamic allocation isn't normally a perk. The problem is that there isn't a worthwhile replacement for C at this time. D could work, but it's at least somewhat proprietary and has no big backing by anyone. Rust, however, has a small enough overhead with the optional garbage collection and minimal runtime now. It's open source with a reasonable license as well, so maybe it'll get picked up by people. I could actually see AVR going this route since they seem to be more user- and hobbyist-friendly. The only issue with Rust right now is that it's a massive language compared to C, but since it relies on LLVM (or at least I thought so), that shouldn't be a big issue as long as someone makes the back end.
&gt;The header font isn't very readable. Completely agree here. A standard font would likely be a lot better. (I'm on Windows and the font looks terrible due to poor anti-aliasing as well. It's a bit startling.) The search bar is a bit out of place here IMHO, it's just off on its own corner. It looks quite a bit better when it's placed in the sidebar nav (like Mediawiki does). Anyway, not to derail anything, great work on this!
I was not talking micro-controller for Java, but embedded at large. For example cellphones. I agree that C++ massive standard library is a non-starter for most micro-controllers.
FYI: FRU expands to Functional Record Update, and refers to `{key: value, ..base}` syntax. Commenting here because I had to lookup what FRU is.
And if you're really bad with acroyms: FYI expands to For Your Information.
Very nice. It would be cool though if there was a real latin1 in there as well. Yes, in HTML, latin1 is not latin1, but for HTTP for instance latin1 remains unchanged.
Slide page 12 claims @Trait could only contain Static values and &amp;Trait could contain any values. Isn't it the reverse? I thought &amp;Trait could only contain Static values and @Trait could contain any values.
Is it actually possible to build display list by top-down tree traversal? CSS2 defines [painting order](http://www.w3.org/TR/CSS2/zindex.html) that is not quite tree order. Specifically, when there is a float, order is non-float background, float background, float text, non-float text. I thought about it and couldn't figure it out. Reddit has light blue background on even-numbered items. Painting order is why this background is not painted over the sidebar. Since Servo currently paints in tree order, Servo gets it wrong. :(
There was a question about whether Google Maps uses quadtrees, but I think more relevant question is whether other web browsers do. How is this handled in, say, Gecko and WebKit?
I also agree that streaming support would be very worthwhile. Ideally supporting the iterator interface would be great of course, if this is not possible because of performance concern then a buffer approach could be used instead: pass a partial buffer to the encoding/decoding method and it need indicate how many bytes/chars could not be consumed at the end of the buffer because it's incomplete.
Thanks. I knew there must be a better way to parse the tab-separated lines :-)
It's worth pointing out that, by default, creating an RNG is relatively expensive because it needs to be seeded from urandom (or equivalent). This uses the task-local RNG. In 0.8pre there's also rand::weak_rng (currently XorShift) for a cheap, fast, non-CSPRNG.
It's also worth pointing out that one can get the best of both worlds (avoid creating a whole new rng and still use `gen_uint_range`), by using `let rng = std::rand::task_rng();` which fetches a pointer to the task-local rng explicitly.
This is extremely cool. That is all.
Chris just tweeted this: "I tried to do a List with length as a phantom type parameter in Rust but got stuck on list_zip. Code is here https://gist.github.com/doublec/6240315" https://twitter.com/doublec/status/367981156408373248
Yes. Very good talk, and it sounded like nobody was paying any attention to him. Must have been hard.
Thank you for pointing out that, I was completely unaware of it. Unfortunately I cannot test or review the design in detail at the moment. My design allows for the partial (stateful) decoding and encoding, just like eridius' design. The lower interface not visible to the README processes up to the first error or the end of the input whichever comes first, and is able to point out the error range and remaining portion of the input to be processed later. Compared to the iterator-based design (which I think neat without its own caveat), it allows for much more tight loop (look at UTF-8 decoder, its main loop is very compact) and has a drawback that the output needs to be allocated temporarily and it requires more code to achieve the optimal performance (for example, the partial prefix is handled in the different code from the main loop). Given that the former drawback can be controlled by splitting the input and the latter can be mitigrated by testing, I think it is reasonable tradeoff. I still hope some automated way to generate the encoder and decoder code though... The original reason of not using condition was that I had a limited knowledge and experience with it and I was unsure about the usability with conditions. There is also a tough design problem as well: the notion of the error range is hard to define, so WHATWG Encoding standard specifies that the error range is always one byte long but it didn't fit with a trivial partial decoding interface (for example, the remaining portion of the input cannot be a slice since bytes in the internal buffer may be added as well). The current partial encoding and decoding interface is able to handle both notions of the error range (see whatwg::TextDecoder etc. for details), but I think it is impossible or at least requires more complex interface with the condition.
Could someone explain to me how the first example, the one with the `TI(int)` constructor and the `TS(~str)` constructor, works? I understand the basic idea involved, but what isn't clear to me is what guarantees that the value constructed by `TI` is tagged with type `int` and that the value constructed by `TS` is tagged with type `str`. I mean, in Haskell we have a similar pattern called GADTs (generalized algebraic data types), but there the constructors explicitly indicate the what the type tag is, such as in the following: data Expression a where IntVal :: Int -&gt; Expression Int FloatVal :: Float -&gt; Expression Float and then you could define plus :: Expression Int -&gt; Expression Int -&gt; Expression Int plus (IntVal x) (IntVal y) = IntVal (x+y) By contrast, in the first example I don't see how the type tag is constrained by the constructor that was chosen. Edit: Revised my example of GADTs to better match the article.
It actually isn't. There isn't any reason why the example that broke wouldn't compile with it: let d1 = TI(1); let d2 = TS(~"Hello, "); let x = plus(d1, d2); display(&amp;x); (in this case, d1 and d2 are both inferred as T&lt;int&gt;, which is obviously not what is intended!). The only thing it is giving you is a guarantee that you can't use the same value as both a T&lt;int&gt; and T&lt;~str&gt;. And the output of one of the functions is constrained. I think a better example (which is more along the lines of the bless example they illustrated before) would be: fn mkstr(~str) -&gt; T&lt;~str&gt; { ... } fn mkint(int) -&gt; T&lt;int&gt; { ... } let d1 = mkint(1); let d2 = mkstr(~"Hello") let x = plus(d1, d2); // compile error display(&amp;x); But, of course, the only point of doing this at all (and not, for example, just using ints and ~strs) is that you have some functionality on all variants that you want to preserve. And even then, you don't really need phantom types at all, you could do this with plain old type parameters: enum T&lt;A&gt; { T(A) } let d1 = T(1); let d2 = T(~"Hello"); fn plus(lhs : T&lt;int&gt;, rhs : T&lt;int&gt;) { ... } 
The example in the article is (much!) weaker than Haskell's GADTs, it doesn't actually restrict the validity of the variants based on the specific type of `T`, e.g. writing: let d1 = TI(1); let d2 = TS(~"foo"); let x = plus(d1, d2); the `d2` gets inferred to `T&lt;int&gt;`, since there's no rules against it, and this causes a runtime failure. (One can possibly use something like [this](https://gist.github.com/sstewartgallus/5605948), but I haven't actually tested it, and it's old enough that it probably requires some adjustment to compile again.)
Awesome to see someone working on this, and based on the right specs ;) I just gladly deprecated rust-webencodings in favor of this.
It would have been even better if each_line() had implemented an external iterator. Could have read and parsed the entire file in one line of code then. As far as I've understood, external iterators are the way to go now, so I'm sure it will eventually be rewritten. I might even do it myself.
Alright, thanks, these three comments I got were exactly what I was hoping for. Cheers.
The title got me all excited that Rust is getting higher-kinded types. The presentation was nevertheless interesting, even though not about the feature I was hoping for.
Not to rain on your parade, but there is already a list implementation in rust [here](https://github.com/mozilla/rust/blob/master/src/libextra/list.rs). I tend to like the use of owned pointers, but you are doing a lot of copying. I think that those lists make a lot more sense to use the shared pointers like in libextra.
One could get around this with smart constructors, I assume, though? Don't expose `TI` and `TS` to the outside world, only constrained functions that will return correctly-phantomed types.
It's efficient to use `Rc` or `Arc` for a persistent data structure with low fanout (like a list or binary tree) so you can still reap the concurrency benefits.
It's only an internal iterator because no-one has converted it to be external yet. (*hint hint* ;) )
It'll be nice when `extra::fileinput` gets converted to an external iterator (along with the rest of `io`), so that the examples can be: for line in fileinput::input() { ... } rather than having to do the ugly `while !fi.eof() { .. }` ugliness.
Nice! I do wish they just buckled and introduced syntax for conditions (at least the handling bit). Something like: fn main() { try { let pairs = read_int_pairs(); for &amp;(a,b) in pairs.iter() { println(fmt!("%4.4d, %4.4d", a, b)); } } trap malformed_line(_) { UsePreviousLine } trap malformed_int(_) { -1 } } match raise malformed_line(line.clone()) { UsePair(a,b) =&gt; pairs.push((a,b)), IgnoreLine =&gt; (), UsePreviousLine =&gt; { let prev = pairs[pairs.len() - 1]; pairs.push(prev) } }
I believe they're moving in the opposite direction, @ is potentially losing it's special place. I have to say, I quite like the possibility of having the core language be independent of the runtime.
Well I agree with that. It would be unfortunate for the core language to require thread-local storage. But I still wish using conditions could be a little prettier. Especially since it looks to become a core error-handling strategy used all over the place.
&gt; # Verbose, requires matching results or calling Option::unwrap everywhere. Am I the only one finding very odd the apparent distate for all the combinators `Option` provides? You don't have to match or use `unwrap` any more than you do in Haskell, you have `map` (and `map_default`), `chain`, `or`, `get_or_default`, `get_or_zero`. If they apply to the situation, they work fine.
In “The Design and Evolution of C++” there is a very interesting chapter 16.6, about making choice in favor of termination semantics for C++ exceptions. I'd be very interested in hearing thoughts on the “termination vs. resumption” debate from people close to Rust.
I think you have to put it in the context of error handling. If you were to use Option as your primary mechanism of error handling (I don't know who would), it would be a lot more verbose than the condition system. I took it to mean 'it is really useful in its place but shouldn't try to do too much'. I do agree that the alternate ways of retrieving values from Option generally need more exposure.
There is still an unhandled error in the final code : when the 'UsePreviousLine' trap gets called for the very first input line. Fixing this error would make this simple example code more complex, though. 
Indeed, given dynamic binding and some other things (that rust already has), you can implement conditions yourself, and use whatever macro you want. There are several conditions libraries for clojure on this principle, [e.g. mine](https://github.com/bwo/conditions/). (For all I know conditions are already written as a library in rust.)
I'm a bit worried that installing a trap requires to open a new scope. This brings the same inconvenience than traditional try/catch syntaxes, where it is difficult to use the result of a protected expression outside of its 'try' block. Is there an alternative syntax that does not introduce a new scope ?
Does result have a method similar to get_or_default? Something like the following could perhaps be handy let reader = file_reader(path).get_or_handle( |e| fail!("FAIL")); 
They are a library; the only thing built-in to the compiler is the `condition!` macro which is syntactic sugar for declaring them.
Small nitpick: If I remove all the traps in the last example then I get a runtime-error that refers to a line number in the standard library. It would be nice to get a line number in my own code. task &lt;unnamed&gt; failed at 'Unhandled condition: malformed_int: ~"f"', /home/niels/local/src/rust/src/libstd/condition.rs:43 Would it be possible (and useful) to check compile-time that all conditions are handled? At some point in the future it would be super cool to expose a list of conditions in an API. EDIT: But so far it looks pretty cool. This will be fun to toy with :-)
It still uses a new scope, but you should be able to use this to get the result: let five = do malformed_line::cond.trap(|_| (-1,-1)).inside { 5 } assert_eq!(five, 5); 
PCAP BINDINGs?!?! NO. Seriously, how can you do this to me. Every time I pick up a new language I implement the same project. But it requires TCP/UDP and PCAP. It's a cool project, for me. It's a specialized VPN that will rewrite local packets from Xbox on local networks and bridge them. There is absolutely ZERO use for this project as it applies to exactly 6 hard core nerds that like Halo 1 Xbox online :P EXCEPT, now that there's pcap, I guess I know what I have to do for the next month. Anyway, cool, great work, can't wait to try it out!
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: http://www.reddit.com/r/Serendipity/comments/1kls6n/functional_list_in_rust_xpost_from_rrust/
I'm a little late to the party, but I want to point out that using the modulo operator to adjust the random number to fit the range [0, vocabulary.len()] results in an uneven probability distribution within the range. For a small range and a large max uint, the difference is negligible, but imagine what would happen if your vocabulary length is 2/3 of the length of the max uint, and the same trick were used. Any random numbers above (max uint)*2/3 would get remapped down to the first half of the range, so the first half of the vocabulary would be twice as likely to get chosen as the second half.
I'm not familiar with the autocomplete feature you refer to in LibreOffice (is it something like T9? I can see how that would get annoying). Perhaps we're talking somewhat past each other, but as it is right now, just pressing enter does nothing. So you wouldn't have to press backspace to use only the text you entered, since that wouldn't do anything. I would agree if enter had actually done something by default, however (e.g. if it had defaulted to a full text search, although in that case it would be better if that was actually the top item in the list and highlighted by default), Although perhaps it should only highlight the top match if it is an exact match.
Do phones and computers in cars still qualify as "embedded"? To me one part of that definition means specialized and restricted hardware etc. But today those computers run desktop apps and are more powerful than the PCs of some years ago.
Okay, that sounds better than what I feared. In LibreOffice Calc, if you have a cell with the text "Reddit", and then below try to enter Red&lt;enter&gt;Green&lt;enter&gt;Blue&lt;enter&gt;, the result is: * Reddit * Red**dit** * Green * Blue (Emphasis mine.) So if you don't pay close enough attention, your data is now corrupt (there is no 'Red'). This problem can be avoided by making autocompletion features require a down-arrow press before acceptance of the completion. In many shells, the alternative is to use &lt;tab&gt; instead of the down arrow, which also works nicely imo.
The overflow checking is nice. Is there a way to choose those when using the operators `+`, `*` etc?
Sorry, i misread the WSGI part: &gt; However, I quickly came to the conclusion that its design is optimised for a quite different use case; my goal in Rust is to provide the entire server, rather than to interface with another server.
From the README: &gt; I determined not to model WSGI at all. In the end, Go's net/http package has been my primary source of inspiration, but I am creating something which is quite definitely distinct: net/http is for inspiration only, then I do my own thing. You see, Go lacks all sorts of nice things Rust has, such as its enums and iteration on aught beyond built-in types.
Ah, yes, I recognize that kind of feature. That really is annoying. I actually studied artificial intelligence and the usability of intelligent systems, and this kind of thing is a pet peeve of mine as well. There's so many systems designed to be "smart" and "helpful" that in reality are just plain dumb and annoying.
ifmt! is really nice. The only thing missing to me is the ability to access fields of arguments in the format string, a la Python's ```"{foo.a} and {foo.b}".format(foo=foo)```
No, they're purely library traits at the moment; I'm not even sure if there is a plan to make them have syntactic sugar, although it'd be neat.
Since Rust tends to favor the safe alternatives, one should consider using those for the operators.
It was actually discussed on the mailing list a few months ago (iirc); this was well before we had the concrete `Checked*` traits though. (~~Unfortunately I can't find the thread in the archives right now.~~ [Here's the first email](https://mail.mozilla.org/pipermail/rust-dev/2013-April/003654.html), and the [whole thread](https://mail.mozilla.org/pipermail/rust-dev/2013-April/thread.html#3654).)
I meant fixing "could be a little prettier" with a macro that looks like try...trap syntax above and expands to nested `do $cond.trap($f).inside { ... }` blocks.
\o/
Awesome work! Does anyone know if this closes [#6194](https://github.com/mozilla/rust/issues/6194), or if this [awesomeness](https://github.com/bjz/gl-rs) works now?
It did not seem to help much on the execution time: http://huonw.github.io/isrustfastyet/buildbot/ So could someone explain what is the benefit of this PR ?
This seems like a very good step to make Windows a first-class platform for Rust development.
This is really great stuff. Does Chris have a BTC address?
This has been a *long* awaited fix (for me, almost a year) which will, amongst other things allow us to call external function pointers from rust code. This is of vital importance for supporting the full OpenGL API, for example.
Great work! HTTP library is one of the core packages of any language. Would love to play with it, need ... more ... time ... :)
So is the new ifmt! similar to .NET string formatting? If so, yay!
Rust's fixed-size integer types exist to expose the functionality of the hardware, and are entirely safe. If you choose not to use a big integer type, you're opting into a type with a bounded range and need to consider wrapping. The checked operations return an `Option` and allow you to handle overflow with a different strategy than wrapping. There's no way to provide the functionality through the normal operators beyond raising a condition, trapping the overflow, setting a boolean and branching on it. If we branch on every operation by default and `fail!()` if overflow occurs, every overflow bug will still be a bug resulting in a crash (unexpected failure) and Rust will be significantly slower/larger. Instead of a single operation, you will have a branch + the code bloat for formatting an assertion string. There will be no loop vectorization, and most other optimizations will no longer work except when it can prove an operation never overflows.
~~I don't at present. You seem to be suggesting I should get one promptly?~~ OK, I has me an Bitcoin address: 14NcYrmxFCN9wAHhxCHkyt4PU9bqnveBUj
It's taken me a while to get this in the approximate shape I think it should be, and I still haven't got anywhere near all that I want in there, but I think the intent of most things (especially the header scheme) is sufficiently clear that, should anyone wish to contribute, they can probably do so with meaning now. Enquire if interested. Frankly, I have been spending more time on it than I should—but it's just so addicting!
At the least, it will be quite some time before this is ready to go in the standard library, if indeed it ever does. I'm not convinced either way about whether it is best in the standard library or out of it.
Bad wording on my side. I didn't mean I expected it to go into the standard library, I just wanted to say that a HTTP library is a very important library to have for any language.
There is a bug about [a HTTP library](https://github.com/mozilla/rust/issues/6167), and I think there's a loose plan for one in `extra`/as a core-dev-maintained package (although the bug doesn't make that clear).
http://huonw.github.io/isrustfastyet/buildbot/ Does anyone know what's up with `linux-32-opt`? Its testsuite performance snapped back to the values when the C++ runtime was around.
Personally I think it's time to start disabling some subset of the tests; bors' queue is too long.
I mean http://huonw.github.io/isrustfastyet/buildbot/#6a88415 The testsuite time is back to before the RT switch for `linux-32-opt` (green line).
Ah, sweet, thanks!
Two little things from a quick glance over the code: - [`iswc(c)`](https://github.com/darkf/rust_sexpr/blob/ecb839cabca811874d0e1c54dac5c6f23fd5b3fd/sexpr.rs#L13) == `c.is_whitespace()` - [`v.slice(x, v.len())`](https://github.com/darkf/rust_sexpr/blob/ecb839cabca811874d0e1c54dac5c6f23fd5b3fd/lisplike.rs#L148) == `v.slice_from(x)` (correspondingly `v.slice(0, x)` == `v.slice_to(x)`).
This is very nice!
Thanks for the feedback!
Not for implementing the lisp, since I'm not generating Rust code, I'm just interpreting it. Scoping and closures aren't hard to implement, and if I ever get around to it I will, but it's not a really mandatory feature, I just wanted to get something up and going.
So like a check-fast for all builders, not just Windows? Are we capturing statistics of which tests fail most often on bors? It would be fascinating to see what percentage of the test suite is the most useful for finding regressions.
Thanks for pointing that out. I opened an [issue](https://github.com/mozilla/rust/issues/8644) about it.
There are several solutions, depending on the number of requests that are thrown out by bors: - speculative: if you have power available, schedule a test of 1, 1+2 and 1+2+3 in parallel - aggregated: pick up PRs N at a time At work we implement both solutions depending on the products to test. Speculative is pretty good if you can easily parallelize tests, and its worst case is just as slow as without it. Aggregated is pretty sweet for big tests suites, and if it fails you just go back to integrating those N PRs one at a time to check which caused the failure (you lost one test cycle). Note the choice of N really depends how the typical commits go: in theory you could probably deduce the best N depending on the frequency of failure... but starting off at 3 is not too bad.
The aggregated method is precisely how nurses tested for syphilis in (if I remember correctly) World War I - they mixed together blood samples, and if one batch gives a positive result, then they would binary search. The number of mixed samples should be chosen so that the chance of a positive result is 50%. (Of course, when dealing with physical samples, there are many more things to take into account - for example, false negatives due to a low concentration of what's being tested for, and limited quantities of each sample.)
Seems worth trying. It's pretty easy to make mistakes though. I've definitely made a lot of trivial commits that broke something unexpectedly. How many of those mistakes can we tolerate per day?
We could potentially have a sort of "r+ doc" where approved documentation-only commits (or the stuff in `src/etc/`) all get batched together into a single test cycle once a day. But I don't feel like we really get enough documentation-only commits to make this approach useful at the moment.
Just very cursorily eyeballing the pull queue, it seems reasonable to assume for discussion that there are 2 pulls per day that would get labelled as 'no-test'. If they all go well, right now they're taking 4 hours out of the day. With a no-test flag, that drops to 0. If these no-test candidate pulls have a 50% chance of failing (which seems really high), that's 1 per day that needs to be re-run. In the status quo, for that pull, that implies another 2 hour run (so up to 4 hours total for that one pull, or 3 hours if you want to say the failure cut the build's time in half). Then along comes a tested commit that passes, that's 5-6 hours total to land 2 pulls. In the no-test scenario, that failure pops up during the build for the next tested commit. I'd assume if it failed once, you probably don't want to run it no-test again, so you get 2 hours for retrying the originally untested commit, and 2 hours for retrying the originally tested commit. That's the same 5-6 hours for 2 pulls, but you still save the 2 hours for the other 'no-test' pull from the same day that passes w/o any problems. I don't know, maybe not worth the trouble, but I don't think it would hurt. As long as there are only 12 build slots in a day, even 1 or 2 more landings is an 8-16% improvement in the churn rate. 
[c.is_whitespace()](http://static.rust-lang.org/doc/std/char.html#function-is_whitespace) is not exactly the same. It’s based on Unicode categories and includes non-ASCII whitespace, which may or may not be what you want.
Could it be possible to just detect it automatically ? I am not too sharp on git's interface, however there should be a way to obtain the list of files affected by the patch and have bors automatically skip the tests if ever it sees that changes affect only non-testable files.
I proposed it yesterday in r/programming, basically there are two simple ways to speed things up: *speculative* and *aggregated* builds. What you are proposing is a speculative build, you run A, A+B and A+B+C in parallel betting on the fact that A won't be breaking things (most of the times), and thus in the time spent to integrate one PR you might manage to integrate 3. It's very interesting if you have more hardware and usually easier to setup that parallelizing the test suite itself. The aggregated build is even simpler. In a way. Just grab the N next PRs and do a single test with all of them. If it passes, you've really gained time. If it breaks, try them all one at a time. A variant is not to fix N and instead just pull all pending requests (rejecting those that do not merge), in case of failure though one at a time might be too slow so you might have to implement a binary search approach for the faulty PR. There is one subtle default with the aggregated build: you lose isolation. If a bug sneaks in in PR #5 and is fixed in PR #7, then the bug might sneak in in the tree if both #5, #6 and #7 are integrated at once. This is subtle, but it can break binary searching for another bug. And thus another variant is to have a "fast-integration" branch that validates PR one at a time to ensure the basics are covered (aka, I built a compiler and it compiled correctly a bunch of test cases). Then, this "fast-integration" branch is followed by the aggregated build check that will forward to the real "master" branch in case of success. Some bugs might still sneak in, but not in the important areas.
Any write up and opinion on rust is great but he starts out by saying c++ has all the same pitfalls of C memory management, which I would strongly disagree with.
I haven't gone over the code in the repo, but one thing I noticed from the article: let computed_key = match key.len() { _ if key.len() &gt; self.block_size =&gt; self.zero_pad(self.hash(key).digest), _ if key.len() &lt; self.block_size =&gt; self.zero_pad(key), _ =&gt; key }; This would probably be more clear as the following: let computed_key = match key.len() { len if len &gt; self.block_size =&gt; self.zero_pad(self.hash(key).digest), len if len &lt; self.block_size =&gt; self.zero_pad(key), _ =&gt; key }; ...and contains fewer method calls too (which is perhaps the only good excuse for not using an `if` here instead of a `match`). Otherwise, quite nice. Explorations always provide an interesting perspective.
That's a documentation change, but the code sample in it is tested under the stage2 compiler/libraries because those snippets are extracted from the documentation.
I'm operating under the assumption that merge conflicts in documentation are rare, and thus that doc-only pulls can sit in the queue for a good while without rotting. Code errors in documentation should also be trivial to track down and exclude from further batching, unlike the situation where we batch together non-doc PRs. But again, I wouldn't expect a big win from this approach unless we started a major doc-improvement initiative with many users submitting many doc PRs a day.
I still can't tell what either of those damn matches are doing, despite having used matches previously.
[kzrdude on HN](https://news.ycombinator.com/item?id=6253666) suggested: match key.len().cmp(&amp;self.block_size) { Less =&gt; self.zero_pad(key), Equal =&gt; key, Greater =&gt; self.zero_pad(self.hash(key).digest), } which is the nicest alternative I've seen.
Nobody is stopping an intern from using bare pointers.
C++ has very similar pitfalls to C memory management from the perspective that in both, you must be very careful to follow the rules and if you screw up you get memory corruption.
Rust isn't stopping you from using bare `T*` pointers either! It's just harder.
You have to declare things unsafe and it can't leak outside of the unsafe block. Besides, there's lots of things like iterator invalidation, concurrency + state issues, etc. that are solved problems in Rust and still suck in C++
Have you used Erlang pattern matching? It's not obviously different.
He is right, because C++ is unsafe by default and forces developers to put extra effort for safe memory access. Whereas safe systems languages like Rust, which goes back to Modula-2, force the developer to feel dirty when doing typical C pointer tricks by making them explicitly marked as unsafe code. 
Incidentally, these bare T* pointers are badly documented in rust. 
Many thanks for blogging about your forays in Rust. Indeed the pattern you have here is quite elegant, though the `enum` idea might be quite specific as it requires compatible function signatures (which not all libraries have).
 let computed_key = match key.len() { len if len &gt; self.block_size, len if len &lt; self.block_size, _ =&gt; key }; That, I could understand, I don't get what the "=&gt;" and afterward is doing
Yep! :) Just need to do a fix or two to get them to compile.
That's excellent news, I've been wanting to do something with OpenGL and Rust for quite some time now. Rust seems like it has a lot of potential as a language for doing game programming with.
I can't help but think it would be even nicer as match key.length().compare(&amp;self.block_size) { Less =&gt; ...
Heck, we could add a lint check. Then #[forbid(unsafe-blocks)] would stop an intern from dereferencing bare pointers. It does seem a little perverse to have lint checks for allocating memory and for unused unsafe blocks, but not for unsafe blocks period.
So, I was in fact getting confused by the "len" on the two pattern lines. They are in fact matching a pattern, both matching the same pattern of "whatever value key.len() is". let computed_key = match [key.len()] { [len] if [len] &gt; self.block_size, [len] if [len] &lt; self.block_size, _ =&gt; key }; Forgive my use of brackets, but for anyone else, the []'d items are all the "Same thing" IIRC now.
Yeah, give me a day or so and I should have it up and running. I'm usually on #rust at irc.mozilla.com, so give me a buzz if you need a hand.
Yeah, I support this. For some reason I thought it was actually done, but I guess not. If this was done, `rustc -F unsafe` would be guaranteed (modulo compiler/spec/unsafe library code bugs) to produce memory-safe code.
&gt; `length` :( (I'm fine with `compare` at a pinch, but you'll never take my `len`s.)
My rustc doesn't list it, but it's approx. 2 months old. I've been wanting to get my feet wet in the code since forever (and haven't because I'm really bad with the motivation and time management thing). Do you think this might be a good first project?
I've just updated the post reflecting some of the feedback I've received. The pattern matching section now reflects a nice suggestion made on HN which is much more idiomatic Rust, and hopefully explains it better.
It'd be awesome if `Port` was an iterator, [this block](https://github.com/logicchains/Levgen-Parallel-Benchmarks/blob/d1e3a45910b698881a6d7a830d81d94c34e55bcf/PR.rs#L28-L34) would become: let biggest_lev = port.iter().max_by(|lev| lev.rooms.len()).expect("no rooms!"); possibly even without the `iter`, if `Iterator` was implemented directly on `Port`. *e*: [Discussion](http://www.reddit.com/r/programming/comments/1kxt7w/parallel_roguelike_levgen_benchmarks_rust_go_d/cbtqsjt) on /r/programming (and IRC) have made me less enamoured of this: it's very easy to deadlock the iterator by accidentally keeping the corresponding `Chan` in scope (especially bad with `SharedChan`), since `try_recv` only returns `None` when the channel is actually closed.
I'd be interested to know what it is, but doubt there's any fundamental problem there. Probably just something that needs to be fixed.
[**@metajack**](https://twitter.com/metajack): &gt;[2013-08-23 22:50:03 UTC](https://twitter.com/metajack/status/371041675633647616) &gt;Servo passes acid1! (diff from reference is [*evernote.com*](https://www.evernote.com/shard/s2/sh/e8141281-6cf3-42a9-8aaf-d85ce954b8de/876ea5eeae0daf5039a49acefb653e96/deep/0/acid1-diff.png.png)) [*pic.twitter.com*](http://pbs.twimg.com/media/BSY0cClCYAAcoDF.png) [^[Imgur]](http://i.imgur.com/C9sHiUi.png) ---- [^[Mistake?]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Error%20Report&amp;message=http://reddit.com/1kz0ue%0A%0APlease leave above link unaltered.) [^[Suggestion]](http://www.reddit.com/message/compose/?to=TweetPoster&amp;subject=Suggestion) [^[FAQ]](http://www.reddit.com/r/TweetPoster/comments/13relk/) [^[Code]](https://github.com/buttscicles/TweetPoster) [^[Issues]](https://github.com/buttscicles/TweetPoster/issues) 
Because that would be weird; it'd stop iterating as soon as there was nothing in the channel, e.g. if the producer had to do a little more processing to produce the next element, and the loop caught up to it.
I think having `lib.rs` == `pub mod geohash` will be peculiar: it means that users will write `extern mod geohash; use geohash::geohash::whatever`; which is pretty strange. I'd replace the contents of `lib.rs` with `mod.rs` to get `use geohash::whatever`. You can write: pub fn encode(lat:f64, lon:f64, precision:uint) -&gt; ~str { let mut ilat = Interval{ lo:-MAX_LAT, hi:MAX_LAT }; let mut ilon = Interval{ lo:-MAX_LON, hi:MAX_LON }; let bit_count = 5 * precision; let bits = do range(0, bit_count).map |i| { let (interval, dimension) = if i.is_even() { (&amp;mut ilon, lon) else { (&amp;mut ilat, lat) }; let is_higher = dimension &gt;= interval.median(); interval.contract(if is_higher {UpperHalf} else {LowerHalf}); is_higher }.to_owned_vec(); do bits.chunk_iter(5).map |char_bits| { base32::encode_vec(char_bits).unwrap() }.collect&lt;~str&gt;(); } And, is [`a` missing on purpose](https://github.com/nullstyle/rust-geohash/blob/57cf39bc12b99199ce907710b5594e6242630759/geohash/base32.rs#L79-L80)? Other than that looks pretty good.
The differences are regarding to font rendering, which varies from OS to OS and browser to browser, which the text itself says are ok.
The text says "(except for font rasterization and form widgets)". Servo has differences in font rasterization and form widgets are nonexistent, but we consider that a pass per the text.
Ah, ok.
Yes, small segmented stacks will still be supported. It may or may not be the default kind of task though.
It would be neat if tasks started at 64 bytes or something and grew using segmented stacks until it hits 16k, at which point it allocates segments in 1GB chunks with lazy commit (so really 4KB at a time, but you let the HW/OS handle mapping new pages in).
Ah, you'll need to move the contents of the geohash folder up to the same directory as `lib.rs`; or use `#[path="geohash/encode.rs"] mod encode;` etc, because `mod &lt;name&gt;` only looks for `./&lt;name&gt;.rs` and `./&lt;name&gt;/mod.rs`. (I'm not really sure of the best directory structure for rust code yet, so I don't know if this is good advice: it seems ugly to have to put all the files in the same directory as `lib.rs` just to avoid the double-layered export problem.)
Oh - great! Thanks for the replies everyone.
I just can't see how anything you do could be faster than just letting the HW TLB and OS page tables handle the "segments" for you. If the page fault is too much of a hit, you could always just check yourself and commit pages as you need them without letting the HW trigger a page fault and involving the kernel.
The only platforms on which this is possible are 64-bit ones, and I think (at least, people have said) that we can just start off with huge stacks and delegate to the lazy-commit of the OS to not have runaway physical memory usage. On 32-bit platforms, there'll never be a chance to allocate a huge stack, as the address space is too small.
You might want to allocate your vectors memory from the beggining with std::vec::with_capacity as in encode.rs : let bit_count : uint = 5 * precision; let mut bits: ~[bool] = ~[]; let mut result: ~[char] = ~[]; becomes : let bit_count : uint = 5 * precision; let mut bits: ~[bool] = vec::with_capacity(bit_count); let mut result: ~[char] = vec::with_capacity(precision); I don't know if you looked at extra::Bitv. Using it instead of ~[bool] might be a perf win. Otherwise, it's easy to read and understand from my newbie point of view so I guess it's good Rust code, well done !
ML post: https://mail.mozilla.org/pipermail/rust-dev/2013-August/005362.html
Well, that's one step covered. I wonder if the focus will now be put on Acid2 or if there are other directions ?
There's some [nascent discussion on HN](https://news.ycombinator.com/item?id=6268291) too.
Seems like this is something that I need.
Awesome! Let me know if I can help with anything. Bear in mind there are also still some things to do (see the readme). PRs are welcome! :)
See pull request. Seem to be working for me now. I plan to switch from `rust-sdl` to `glfw-rs` + `gl-fw` for my `rustyhex` game. Edit: Any way to keep the ~GL in some global static variable and use it transparently from the gl:: scope? Is this something missing yet or what? Passing the `gl::~GL` pointer around does not seem convenient and no application will want to have multiple instances of this loaded, so singleton makes sense.
I've been playing with this since yesterday when I saw that foreign function pointers were finally in place! It's great work, I like that you stripped the 'gl' prefix from the function names and then namespaced everything into mod gl during generation, it looks a lot cleaner to me. Thanks for doing this and glfw-rs -- I've primarily been waiting for rust to become a viable alternative to C++ for gamedev, and it's definitely getting there. For those who are curious as to how much works, I've got several working demo programs targeting OpenGL 3.2 based on a 'thick' wrapper I wrote on top of GL to keep things idiomatically rusty. It includes things like procedural texture and geometry generation and it all works great. It even performs competitively to the C++ version it will hopefully replace someday.
Have you looked at the gl_ptr version? That might be what you're after. I'm actually wondering if I should keep the struct version - I'm not sure how useful it is to anyone.
&gt; Does the task associated with the entry point not run on the main application thread in rust by default? That is correct. brson would have more to say about this - he's the one behind our lovely new runtime.
Glad you like it! Do you have your wrapper in a repo online? I'm interested to see how you do things.
At least, in the README.md you should put `rustc gl_ptr.rs` as a way to compile by default... :D I was unaware of the difference between the two and got mislead by the README. :S
There are several directions; Acid2 is one of those (and work has already started on layout features needed to pass Acid2). But there are many other things: we need HTTP support in the new scheduler, we want to experiment with parallel layout, we need to improve performance, we need improved incremental reflow and invalidation, etc.
This looks awesome! Thanks for the great code. One question, though: is the only time I should use this over rust-opengles when I need functions that aren't in ES? Or are there other reasons for choosing gl-rs over rust-opengles? (I'm currently using rust-opengles along with glfw-rs for my simulator.)
Sure! Sorry, I had to anonymize it since I don't like my Reddit account being linked to my Github account for Reasons™. I'm currently rewriting it a bit so this is just the 2D portion right now. Repo [here](https://github.com/FluffySauce/fae).
That's the primary use case. OpenGL-ES is beginning to gain more traction because its a much more lightweight API. It is also very standardized, which is why portable bindings could be made very easily without resorting to a loader. Unfortunately it does not provide the full API, or the vendor extensions, so if that was all we offered, graphics devs looking for the extended functionality would be unimpressed. Personally most of my usage more lines up with the ES API, but I spent time on this because I really want Rust to gain more traction in the graphics community.
Cheers for that! I will peruse it tomorrow. I'm looking at messing around with a wrapper too, so it'll be most inspiring :)
BTW. Any recomendation for dealing with matrix operations? Open GL seem to remove internal matrices and I'm wondering what to use in Rust for these operations.
Change this: pub fn yell() -&gt; ~str{ ~"YELLING" } to this pub fn yell(&amp;self) -&gt; ~str { ~"YELLING" } Much like Python, Rust requires explicit self for method signatures. The yell function above is more like a ```static``` function(which is a bit of a misnomer since these aren't classes ala C++/Java). As you can see in your code, Yeller::yell() works, since really it's just a free function. Also, if you need to alter the values of the structure in a method, make sure you use ```&amp;mut self``` for the self parameter. 
You need a linear algebra library. This post might be of use: http://solarianprogrammer.com/2013/05/22/opengl-101-matrices-projection-view-model/ I am currently working on one, [lmath](https://github.com/bjz/lmath-rs), but it's bitrotted since I've been working on gl-rs. Time to get back to it! I plan to divide it up to and focus mainly on math - I currently have some collision stuff that shouldn't be in there. If you'd like to be of assistance, I'm usually on #rust (irc.mozilla.org).
Also, note that this isn't a "trait", it's just a freestanding impl on a type. A trait would be pub struct Yeller { priv num: uint } impl Yeller { pub fn new() -&gt; Yeller { Yeller { num: 0 } } } pub trait Speak { fn say_something(&amp;self) -&gt; ~str; } impl Speak for Yeller { fn say_something(&amp;self) -&gt; { ~"YELLING" } } // could also have struct Whisperer, and `impl Speak for Whisperer`. fn main() { println(Yeller::new().say_something()); // prints YELLING }
That's one way to think about it, but they have very different visibility properties. The methods on a freestanding implementation are "built-in" to the type they're on, and so don't require any special imports to use; but the trait needs to be in scope (i.e. imported unqualified) to call its methods.
Not yet.
&gt; The name is not standardized. new will probably be the most common choice, but people will be rolling their own methods named construct, create, etc. Standardization is a good thing. Constructors are just one instance of this. We have to enforce standardization on many things. Bringing constructors into the language to enforce naming conventions doesn't make sense to me because it's a slippery slope. By the same argument, you could argue that we should build a `print` keyword into the language so that people don't name their methods `write`. &gt; If you define a constructor that has some required initialization, there is no way to force the end user to use it. They can still construct the struct directly, which can lead to an inconsistent experience (e.g. maybe you want all Foos to be registered in some container class upon creation). Yeah, I've wanted a `priv new;` thing (maybe an attribute) to enforce this. Right now you can make a private field to require that the constructor be used. &gt; This is the most important one - If you later decide you need some initialization logic, you have to manually go through your program and replace all Struct() calls with Struct::new(). So your option is to either deal with this, or create a useless constructor upfront to make refactoring easier down the road. I don't think constructors really solve this, unless you want to get rid of the `Point { x: 10, y: 20 }` syntax. For usability, most modern languages distinguish between "POD-like" records, which don't have any special magic and are just fields, and objects, which are constructed in a certain way. In Rust this is done by defining a constructor. &gt; I am fairly certain if something is not done about this, people will either resort to using macros for this, or they will generate them by hand like in Java-land. I don't think either of those are a good solution. We decided to defer these to Rust 2.0. I don't think we want to use magic method invocation syntax like Scala has; D is getting into trouble going down a similar road. Rather we should just adopt C#-like properties if we want to do this. &gt; There is no way to have a struct inherit another struct's fields. Traits allow you to piece together methods, but not fields. The "solution" is to add a reference to the other struct. So an Employee struct has a field called person which is a Person struct. In an OO language this would be Employee extends Person. We've discussed this a lot, but it is not quite so easy in Rust. First of all, you get the slicing problem in C++ if you are not careful, because of unboxed value types. Secondly, this makes variance a lot more important than it is today; we need to have a solution for variance before we do this. Finally, when you have field sharing you're going to want to have single-inheritance-style virtual methods and shared fields, and it's not clear to me what the most elegant design for this is that doesn't wantonly duplicate functionality Rust already possesses with enums and object types. Because of this the consensus seems to be that we should defer this feature to Rust 2.0. Proposals are welcome, of course!
I kinda disagree about all three. 1. Standardization is not a good thing here. A class with ten different constructors is confusing, and sometimes there are multiple constructors with the same parameter list, but with different logic which usually leads to ugly hacks to get things to work. Being able to name constructors by how they construct the object, is much better, and leads to less confusion. Perhaps a convention that it starts by "new_" or something isn't bad, but I don't think a keyword or anything is warranted. (I think you can hide the normal construction too - so users only see the constructor). 1. Not sure what you mean, but I don't want expensive logic hidden behind something that looks cheap (i.e. reading a field shouldn't call a "getter"). And if you already have a method for the setter/getter replacing it is trivial. Macros can help (and even if you spell it out, Rust is concise enough that it's not big deal). 1. Field inheritance is the worst part of OOP. It's over-used, and mis-used to the extent where losing it is worth the cost (the number of genuine uses is much smaller than you'd think). Most of the times things really don't have "is-a" relationships. Composition is the right approach, and traits let you get the correct interfaces (why the hell would a method, which you expect to handle generic Employees, take a specific struct rather than a generic constrained to the trait?).
&gt; Standardization is not a good thing here I should have just left that part out - that is not the central point. That is just an extra benefit to having compiler-support constructors. The main point is it allows for additional abstraction. &gt; Not sure what you mean, but I don't want expensive logic hidden behind something that looks cheap (i.e. reading a field shouldn't call a "getter"). And if you already have a method for the setter/getter replacing it is trivial. Macros can help (and even if you spell it out, Rust is concise enough that it's not big deal). Replacing a field access with a getter is not trivial. Think about Rust succeeding in a huge enterprise environment where dozens of programmers are on the same codebase. Your only choice then is to resort to simple grepping, which is error prone and..icky. &gt; why the hell would a method, which you expect to handle generic Employees, take a specific struct rather than a generic constrained to the trait? If you follow that logic then we're back to discussing getters and setters. You can't define fields in a trait, so to access an Employee's fields you'd end up defining getter methods in the trait.
Replacing a field accesser with a getter shouldn't be trivial, because now code that used to rely on it being a simple accessor has transparently been changed to do some crazy new logic (that may even have side effects!). If you need to access fields through an interface, then yes declaring a getter/setter is appropriate, or just put all your trivial data like that into its own struct and have a single getter.
&gt; Replacing a field accesser with a getter shouldn't be trivial, because now code that used to rely on it being a simple accessor has transparently been changed to do some crazy new logic (that may even have side effects!). Maybe when we fetch a user's phone number and its blank we would like to return a default number. Maybe we want to pull phone number out into a `ContactInfo` struct owned by a `Person`, but we still want to be able to call `Person#phone_number`. There are countless valid use cases. Just because someone can come in and add code to send an email from a getter doesn't mean the feature should be dismissed completely. 
A lot of cool stuff (at least stuff that's cool to me as a Windows user) landed this week. I need to compile a more recent copy and start playing around with the changes to the FFI for some ideas I have for Windows.
btw, does Rust have polymorphic data types as Haskell does? List a = Nil | Cons a (List a)
 enum List&lt;A&gt; { Nil, Cons(A, ~List&lt;A&gt;) } (One can replace `~` with `@` for a single-task "functional" list, or `extra::arc::Arc` for a cross-task one.)
The summary says 'fprintf' was added but it was not, the names changed (yay). It's a common oversight that the PR text is not updated with the PR changes (why should it be =&gt; because it is in the merge commit log).
I think this is a matter of style. If you come from a OO/SIMULA background then you will miss these. For those coming from a more ML background, data has always been more open rather than ways to hide masses of state and behaviour. 
let me look that up.. hehe https://github.com/alexcrichton/rust/blob/a3e39b945402475dbe0eae91833981dad4622cb7/src/libstd/fmt/mod.rs * `ifmt!()` changed name to `format!()` and this is the name for the successor to `fmt!()` * the suggested `fprintf` changed name to just `write` instead, as `write!(&amp;mut Writer, "format string", arguments...)` new API surface should be the family of macros * `print!(fmt, args..)` * `println!(fmt, args..)` * `write!(writer, fmt, args..)` * `writeln!(writer, fmt, args..)` * `format!(fmt, args..)` also, acrichton is amazing for implementing the new ifmt/format.
Excuse my ignorance, but is "issue churn" resolved issues - new issues?
The libuv/process/pipes/`std::run` stuff hasn't landed yet (broken on the windows build) :/
The new functionality will be more efficient than allocating a new `~str` with `fmt!` too, so `print(format!())` will be an antipattern.
Would this feature be moot'd if/when the closure-&gt;thunk happens?
&gt;Field inheritance is the worst part of OOP. I disagree with this. Field inheritance can be done in a much semantically cleaner way than in OOP languages, such that the abuses you make reference to are non-issues. In fact, comparing the two is apples-and-oranges because structs are not objects and carry none of the baggage associated with them. It would be a simple language construct with no abstraction overhead at runtime. &gt;the number of genuine uses is much smaller than you'd think. Composition is the right approach... Again, I disagree with the first sentence completely, and field inheritance *is* a form of composition. &gt;why the hell would a method, which you expect to handle generic Employees, take a specific struct rather than a generic constrained to the trait? You could implement a trait on a "base" struct and not have to re-implement multiple times on deriving types. Composition entirely through traits means traits are littered with getters which is not DRY at all. 
Wow, this was quite a week for the runtime: - File I/O - Thread-safe I/O - Grab bag of optimizations - Callback optimizations ... I tried accessing the "is Rust fast yet" site but it would not display anything... apart from the ping pong bench, did another bench saw significant improvement ? 
I believe so, yes. Seems like it's negative every week, but the total issue count never seems to go down much...
As someone who likes to use very verbose method/function names (for self-documenting of code), I've always felt that constructors were an oddball thing. They were all required to be the same name and you couldn't really put in the name about what the constructor was doing. Many times you'll see several different constructors with several different parameters, but you don't know what those constructors do differently from all the other constructors. So I personally like the way rust lets you make constructors on your own. It allows you to name the functions such that you can document what they do and why they have different parameters. It would be nice to be able to disable the the Struct { ... } syntax without resorting to making a priv field on the struct, as pcwalton mentioned. This sounds like it could be a relatively easy change to implement as an attribute. I've been looking at a way to contribute to rust, maybe this could be something I can attempt doing.
`Eq` and `Ord` are sad and tricky traits since they expressly allow inconsistent orderings, and that is for example why both the `eq` and `ne` method exists. It's not really low hanging fruit for default methods (See [PR #8360](https://github.com/mozilla/rust/issues/8360))
It might be that Github's pulse page is inaccurate, which I wouldn't rule out.
Primarily, Rust may be the first ISWIM/ML-ish language to eschew forced garbage collection, though I could be mistaken here. In exchange, there's a rather novel smart pointer-esque memory management system that allows you to allocate precisely what will be managed by garbage collection and you may avoid using it outright. There's more differences lurking, but that's the big one and rather significant if you're interested in systems programming with an ML-ish language.
Still, it's reasonable to expect that a good refactoring tool will be developed which *would* make something like that easy. Static typing is good for that sort of thing.
A nice thing would be if you could solve trait implementation through composition automatically. Suppose we have a trait HasName for something that "has a name", i.e. implements get_firstName and get_lastName say. A struct Person has an impl for this trait. Now we want to declare the Employee struct which is a composition of a Person and some extra data. Now rather than writing an explicit impl for Employee to just delegate all the individial HasName methods to its Person field, it would be nice to shorthand that to its Person field like "impl Employee : HasName(self.Person) { }" or a similar shorthand. The reason developers too often choose inheritance over composition in languages where this is a choice, is because it is tedious to write the boilerplate delegation code, especially for a large interface. 
Minor details! :)
Good article, thank you
Constructors are an anti-pattern. C++ programmers had to invent the "factory" pattern just to deal with the fact that constructors are really inflexible. Any object should be able to return any other object. A constructor won't let you return a Square if sides are equal and a Rectangle if they're not. What if the requirement is not just that this struct is constructed, but also later initialized? Meaning, the constructor runs and sets the first few members of the struct. But then but then you need to put in actual data that comes from the user on input and only THEN you can do calculate() or whatever. Also, it's really silly to complain about refactoring. Your IDE will probably do perfect static analysis and refactor your Struct() calls with Struct::new() in the exactly correct places. What's really insane is doing work up front when you don't even know if you ever need to change it in the future AND when this change is purely mechanical and can be done with a sufficiently smart IDE. This isn't even like the mythical sufficiently smart compiler, these IDEs exist today and you can convert fields into methods automatically. I mean, does this even require a change at the language level?
It seems that if one really should have separate traits for pre-, partial and total orders. They can share the same syntax, and might even be empty code-wise, but essentially record what must actually hold. I hope the mess that's called Haskell numerics will drive Rust to have a proper mathematically consistent hierarchy where possible, rather than just trying to provide convenient syntax...
thank you. Is it legal to write this: pub trait Eq { fn eq(&amp;self, other: &amp;Self) -&gt; bool { !self.ne(other) } fn ne(&amp;self, other: &amp;Self) -&gt; bool { !self.eq(other) } }
There's `TotalOrd` and `TotalEq` for the total comparisons (not empty though). Re numerics: [behold #4819](https://github.com/mozilla/rust/issues/4819). (It use to be a complete mess, but now it's just a small mess, thanks to the hard work of bjz and jensnockert (among others).)
Yes; it's legal, same as Haskell; although as giselher says, it can cause stack overflow/infinite recursion if the user doesn't override at least one.
Yes, something like that. 
&gt; What if the requirement is not just that this struct is constructed, but also later initialized? Meaning, the constructor runs and sets the first few members of the struct. But then but then you need to put in actual data that comes from the user on input and only THEN you can do calculate() or whatever. How can constructors not do this? &gt; Also, it's really silly to complain about refactoring. Allowing boilerplate to exist in a language just because tools exist to remove the boilerplate is a poor argument. Not everyone uses those tools, and we don't even know when they will be available for Rust. Go ahead and try to apply that logic across the board and see what a language looks like after you remove all the niceties because they can be generated by a tool. Your other points are valid, though.
I do prefer Haskell-style type classes; not having overloading in OCaml does sometimes make the code tedious. I won't be surprised if Rust's type system is not as expressive as OCaml, but I don't know. Does Rust support "fancy" data types, like GADT?
 fn double(x: int) -&gt; int { x + x } struct Wrap&lt;'self&gt; { f: &amp;'self fn(int) -&gt; int } fn main() { let w = Wrap {f: double}; println((w.f)(10).to_str()); } Would be the solution. You need to tell your &amp;fn that it needs to be valid the entire lifetime of Wrap. Also you have to call it like (w.f)(10) because f is a field not a method.
Hi, A &amp; pointer stored into a structure must always outlive it otherwise that would be a dangling pointer, therefore unsafe. You have to explicitely tell the borrow checker that the pointer has a long enough lifetime by adding &lt;'self&gt;. It means basically that the pointer has the lifetime of the struct. Here's your code with 'self : fn double(x: int) -&gt; int { x + x } struct Wrap&lt;'self&gt; { f: &amp;'self fn(int) -&gt; int } fn main() { let w = Wrap {f: double}; println(w.f(10).to_str()); } I don't have my hand on a compiler right now (It still gets 2h to compile here) so I didn't check if the code above works fine.
Nope. There was discussion about adding [datasort refinements][1], but it almost certainly won't happen before 1.0. [1]: http://smallcultfollowing.com/babysteps/blog/2012/08/24/datasort-refinements/
The thing that harms the ISWIM/ML-ish style of programming in Rust is that function composition is difficult. You'll often have to open a lambda to deref/ref pointers to get things to fit together properly. There's also no pervasive currying. I love Rust, but it's important to note these things.
Thanks! A follow up question: How should I go about doing an `impl` for Wrap? impl Wrap { fn go(&amp;self, x: int) -&gt; int { (self.f)(x) } } throws that exact same error again: try2_v1.rs:7:5: 7:10 error: Illegal anonymous lifetime: anonymous lifetimes are not permitted here try2_v1.rs:7 impl Wrap { ^~~~~ error: aborting due to previous error Tried this, but it doesn't do anything... impl&lt;'self&gt; Wrap {
Thanks @vcque, that works! I have a follow up question I posted above (http://www.reddit.com/r/rust/comments/1l6nft/beginner_question_illegal_anonymous_lifetime/cbwagen) if you could take a look.
Try `impl&lt;'self&gt; Wrap&lt;'self&gt;`.
Not really a Rust user, but I can think of at least one alternate way of enforcing those invariants: encode them in the type system. Instead of having the `person.age` field be of type `int` or whatever, have it be of type `Age` and have that type fail when you try to initialise with a negative number. Any changes in requirements on the field can now be done in that type instead. Of course, this does not capture cases where you actually want side effects on get/set, which could be either a good or bad thing, depending on your views. It's also arguable that this solution is too inconvenient, depending on the boilerplate involved in creating a light-weight (but strongly typed) wrapper like `Age`. Anyway, just thought I'd give you an alternative, so there you go!