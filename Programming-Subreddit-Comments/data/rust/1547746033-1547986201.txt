Same here, no confirmation email
&gt; Published 29 Apr 2016 FYI
Rust is indeed much easier if you know (modern) C++
It was a very interesting read. I was amazed by the fact, that numpy performs so well. I like how python module creation in Rust is so easy and elegant (of course the gory details are hidden in macros, but that proves rust‚Äôs expressiveness and abstractive power), compared to C.
Just because we all wear robes doesn't mean we are a cult. Some of us just look good in robes. They are very comfortable.
I'm curious. Does anyone know if this has any implications on the try ? operator? Guess it might be context specific and therefore be okey, but I don't know
&lt;https://en.wikipedia.org/wiki/There_Is_No_Cabal&gt;
For a second I thought Huon was back. Too bad it was an old post :/
Is there anywhere we can try to actually play it? :D
i'd be happy with a caveat like "clang c++"
I totally agree. I had this problem when learning Rust (Hashmaps were tricky to use at first due to lifetimes), and there weren't any helpful results on Google. Particularly 'get or insert' patterns where some kind of action is required on the insert.
Yeah you're definitely right about WebRender. For now it seems to be a decent solution just because it's hella fast. GFX-RS is really cool, and yeah I agree something based on WebRender ideas w/ gfx-hal would be really cool. It's still going to be quite difficult though :( I guess that's the price, when you want a write-once, cross-platform system that supports multiple APIs, for GUI. Difficulty.
&gt;So to summarize: OP, use a C API between C++ and Rust? :-) sure thats what I do at the minute, just a question of how far could interoperability go. For an extreme idea, imagine compiling Rust to some C++ subset (via MIR?). &amp;#x200B;
At least they didn't start with a linked list implementation.
I would guess that rust is just very....... rusty
That is a huge improvement
Comparing opt 0 doesn't really make sense. The primary goal of opt 0 isn't to produce a fast binary, so why benchmark it? Rust relies on a lot of abstractions that C code won't. It all optimizes away reliably, but if you aren't optimizing it away, yeah you're gonna pay a cost.
Yeah, I am working with clippy all the time. Thanks!
Generic answer: you aren't doing anything wrong. Idiomatic Rust simply leans more heavily on the compiler to remove abstractions than idiomatic C. At lower optimization levels, those abstractions layers might not be removed, so that means more function calls and less opportunity for optimizing across function boundaries. For a more specific answer, you'll want to look at the assembly output of each program. Also, note that your post is unreadable for us reddit users still using the old layout. Code fences don't work. Use 4 space indents to create code blocks instead.
Thanks. 
Have you benchmarked it as `fn f(photons: &amp;dyn Iterator&lt;Item=(u64, bool)&gt;)` to make sure that isn't fast enough?
Rust compiler emits notoriously bad code for unoptimized builds. Measuring performance for them is basically pointless. Rust program converts every number to a string, which means allocating and freeing memory in every loop iteration. On the other hand C code prints the number into a buffer that's on the stack, so it completely avoids allocations. Rust's formatting machinery AFAIK internally uses virtual calls. I'm not sure how that compares to C - it has to parse the format string each time, but having only a fixed number of formattable types might give it some advantage. One way to speed up Rust program would be to do the check without any formatting - just divide the number by 10 while it is not 0, and at each step check if the last digit is 3. Alternatively, you could create a string outside the loop, clear it at the start of the iteration, and then `write!(&amp;mut string, i)` to format the number - you will be basically benchmarking Rust's formatting machinery, but you'll at least avoid excessive allocations. 
My untested guess is that in Rust you're creating a new String each time with i.to_string(). The C program is using a hardcoded buffer. 
Well, you are right, Rust is permitted to be slower than C. However, as the standard library of rust is precompiled, I don't understand the huge difference between the Rust optimization levels. Since I'd assume the impact of the optimization done in the main method is rather small, and I doubt the linker's optimizations play a huge role either (as seen with C), where does the huge difference in Rust vs C come from?
Never bench without optimizations. Especially across languages. It's completely useless. The difference is that in the C version, you're pre-allocating a string (`char s[15]`) on the stack before the loop. In the Rust version you're allocating a String on the _heap_ every iteration. Afaik there's no 1:1 equivalent of `snprintf` in Rust (writing formatted output to a preallocated buffer). I'm sure you can do it, it's probably just messy. 
I would encourage you to read some code from other implementations in the ecosystem. For example, the rustc parser is in a crate called `libsyntax`, and it's pretty approachable: https://github.com/rust-lang/rust/blob/master/src/libsyntax/ast.rs
Can you post the type of `naming_function`?
That's unrelated. They ? in macros is like the ? in regular expressions, it's not about the ? character or operator in code
Let's just say that I *do* have a sticker on my laptop that reads "Rust Evangelism Strike Force - Have you tried rewriting it in rust?"...
Rust is marketed as a general purpose **systems** language. Maybe he means even more general purpose than just systems work, which I would actually agree with. 
rust community != this subreddit 
Cool library. Your library has some feature overlap with [sled](https://github.com/spacejam/sled), which I've used for some small command line utilities that need persistence. From looking at your code, it looks like sled would scale better to larger stores, since pickledb-rs appears to dump the whole db at once rather that persist incrementally. But, this feature of sled is also a limitation that couples it more tightly to actually having a filesystem for storage. Your design would allow it to store to anything that the whole db could be serialized to. In this case, I'm thinking you may be able to find a niche with wasm and binding the Dump to LocalStorage instead of the filesystem. This would avoid the overhead of individual key/value writes over the wasm&lt;-&gt;js interface. It would only be suitable for smaller storage sizes, but the dump-a-whole-file concept is more suited to the smaller sizes anyway.
You can use `write!`.
To be clear I was talking about parsing ambiguity, but it think a similar argument still holds! :)
Rust is cool, but Cython and Numba at the very least deserve a mention. 
I can't really tell you exactly what is accounting for the slowness without inspecting assembly output. But at the very least you have a loop with a constant beginning and end that is probably not being taken advantage of. Inlining the `.iter()` method being called in the loop could explain some overhead. Lots of possible places for the abstractions to add up.
Do you mean CPython and Numpy?
Looking at [his GitHub](https://github.com/huonw) I think your first reaction isn't too far off :)
Sure. That's why we obsess over Rust instead of [getting](https://github.com/servo/servo) [stuff](https://aws.amazon.com/de/blogs/aws/firecracker-lightweight-virtualization-for-serverless-computing/) [done](https://mobile.twitter.com/andreapessino/status/1021532074153394176).
That wouldn't solve the issue, unfortunately. You'd have to box the iterator at each step to keep the `if` list flat.
I'm actually surprised that Rust is faster than C when optimized &gt; -O2. I'm not a Rust pro, but as far as I know, String contains an owned value, so each `to_string` call should do an allocation unless optimized (while the C version never does one). Allocations are fairly expensive. You could find that out by looking at the assembly. Also, if debug mode is enabled (which it seems isn't, but just so you know), each `+=` checks for an overflow in Rust, in C overflow would always be undefined behavior. I guess if you used the libc functions and a stack variable, you'd get almost identical results for different opt levels, assuming the same optimizations are enabled. But of course, that would pretty much defy any reason of using Rust.
And to be clear, this is aspirational based on what folks have said here and there ‚Äì software development timelines are *always* aspirational. üòè
Probably not
Much of the standard library is generic, which means there isn't as much that's pre-compiled as you might imagine. For example, your `to_string` call is actually a generic method provided by the `ToString` trait. That in turn [calls `String::from`](https://doc.rust-lang.org/src/alloc/string.rs.html#2161) (or perhaps, `to_owned`). [`to_owned` in turn calls `clone`](https://doc.rust-lang.org/src/alloc/borrow.rs.html#95). And `clone` probably delegates to `Vec`'s clone. And so on. This is an example of the layers of abstraction Rust's core libraries use. All of these functions are trivial and inlined when optimizations are enabled such that they have no cost. Without optimizations enabled, all bets are off and you might have several function calls to go through for a trivial implementation. Since all of this is generic, this doesn't get monomorphized until you compile *your* program, so any inlining that might happen in std's concrete non-generic code might not actually happen at all. This is a bit of a hand wavy explanation. I'm not 100% clear on the precise time at which each function gets inlined. For example, `str`'s impl on `ToString` is actually a specialization of `ToString`. Does that mean what I said above is wrong since a precompiled std could possibly monomorphize that? Maybe! But this is just one example and I'm not an expert on how rustc works at this level. :-)
Great article
I tried to have the C code as equivalent to Rust as possible. I can get it down to 6s by replacing strstr with pointer operations, however, a better Rust implementation might also be possible.
&gt; This is not really correct any more. Well, at the very least, as you yourself mention, you need to support two ABIs: one for MSVC on Windows and one for all others.
I'm not sure that that boxing would have that much overhead without benchmarking, though. I guess the other solution would be to create an iterator adapter that would conditionally apply an adapter given a condition? Sort of like: ``` iter.do_if_some(start_time, |i, st| i.skip_while(|x| x.0 &lt; st)) .do_if_some(end_time, |s, et| i.take_until(|x| x.0 &gt;= et)) .do_if_some(photon_count, |s, pc| i.take(pc)); ```
Cython is a CPython extension. It lets you add C annotations to your Python code. The Cython compiler will convert the code to C when it will have equivalent behaviour and run the rest as CPython.
No, Cython the compilable Python superset, not CPython the Python reference implementation.
That looks like python with types, which is really nice if you don't like python's dynamic nature and also allows for some more optimizations.
Numba translates numpy-using Python code directly into LLVM IR and compiled that lazily, a cheesy and very effective way to speed up your numeric code with a single decorator
I can see essentially two issues here: 1. The C++ ABI, specifically its object-inheritance part, is complex and differs based on the C++ compiler and flags used. 2. C++ templates are VERY difficult to interact with. --- ### C++ ABI You mentioned virtual tables, and that it is indeed what most people think of first: how difficult is it to gather a bunch of function pointers together? Well... there are various difficulties: - There are actually generally more function pointers than there are functions defined, as compilers will emit *trampoline* functions to bind together the adjustment to the `this` pointer and the function code itself. - There are multiple deleting destructors, as well. - And above all, there is the whole RTTI information embedded in the `virtual` table; this is where you find a description of the whole inheritance hierarchy in a format that `dynamic_cast` can interpret to perform cross-casts. And this is specific to a compiler and its flags. For example, `-fno-rtti` will remove RTTI information, as the name implies. There are two major sources of ABIs: Itanium for everyone, and MSVC playing solo. MSVC has different versions depending on their compiler version, as well. So... that's the easy part. ### C++ Templates The hard part? C++ templates. The simplest way to interact with C++ templates is... to compile to C++. Cue Nim. Anything else is a descent into madness, because C++ templates are the Wild Wild West of programming semantics: - Const generics. - Variadics. - SFINAE. - Partial &amp; Full Specialization. And the whole thing is tied to overload resolution (a nightmare). Your very example mentions `std::vector&lt;T&gt;`; well, `std::vector&lt;bool&gt;` is specialized and does not store `bool[]`, but instead acts as a bitset. Its iterators do not dereference to `bool&amp;`, but to a special proxy type. Okay, everyone knows that `std::vector&lt;bool&gt;` was a terrible idea. What of `std::hash` then? It must be specialized *by a user* for any type you wish to use as key in `std::unordered{_multi}{set|map}`. And the signature of its `operator()` is not even fixed, due to C++ implicit conversions rules the user can return any type that will convert into an integral... ### A restricted subset, maybe? Well... how do you insert a Rust `String` as a key of `std::unordered_map`? Its layout is not that of `std::string`, you need a copy there... or if you push a `String`, then C++ will not be able to interact with it. You cannot view a `std::string` as `String` or vice-versa; and if you create a Rust type for `cxx::String`, then you'll need a version for each C++ standard library out there, and you'll need to copy the content when converting to/from `String`. --- I understand the desire to interact with C++ code; but honestly, I don't see any "easy" way. It just seems like a massive undertaking, with lots of difficulties, and I can only wonder at the rewards...
Rust has small string optimization. When strings are shorter than a pointer type, they don't have to allocate. C++ does the same, I think. Some things are magic like that. 
And conversely, knowing Rust makes modern C++ easier. I was like "aha, unique_ptr, I know who you really are!" :D
It's Python with *C* types. And when you annotate you make some promises you have to be careful to uphold. Regular Python now have optional typing that cover the entirety what typing you may use in Python. However the types are discarded are runtime. They are there for the tooling so your CI or IDE can warn you about type errors. Different engines can give different errors because they can have smarter inferring engines though you should never have false positives. I personally only annotate my signatures (pretty much as I do in Rust), it's good documentation and helps people who want to call my code.
You could implement this as a recursive macro ([Playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=43468cdac2e8c17fb7dbe65afe5e8da0)): macro_rules! expand_patterns { // if we're out of patterns, call `f` with the iterator we've constructed (CONTINUE: ($iter:ident =&gt; $use_iter:ident)) =&gt; { break $use_iter($iter); }; ( // distinguishes this case from the entry case CONTINUE: // we have to define the identifier used for the iterator because of hygiene // defining the identifier of the function is just for reusability ($iter:ident =&gt; $use_iter:ident) // expressions cannot be followed by blocks in macros if let $pat:pat = ($expr:expr) $new_iter:block $($rest:tt)* ) =&gt; { if let $pat = $expr { let $iter = $new_iter; // we recurse with the remaining patterns in both cases expand_patterns!(CONTINUE: ($iter =&gt; $use_iter) $($rest)*); } else { expand_patterns!(CONTINUE: ($iter =&gt; $use_iter) $($rest)*); } }; // entry case (($iter:ident =&gt; $use_iter:ident) $($args:tt)*) =&gt; { // we expand inside a loop so we can use `break` as flow control loop { expand_patterns!(CONTINUE: ($iter =&gt; $use_iter) $($args)*); } } } fn use_iter(iter: impl Iterator&lt;Item = (u64, bool)&gt;) { for (t, v) in iter { println!("{}: {:?}", t, v); } } fn main() { let start_time = Some(0); let end_time = Some(100); let photon_count = Some(100); let iter = (0 .. 100).map(|t| (t, t &amp; 1 != 0)); let res = expand_patterns! { (iter =&gt; use_iter) if let Some(start_time) = (start_time) { iter.skip_while(|&amp;(t, _)| t &lt; start_time) } if let Some(end_time) = (end_time) { iter.take_while(|&amp;(t, _)| t &lt;= end_time) } if let Some(photon_count) = (photon_count) { iter.take(photon_count) } }; }
&gt;The server will need to be able to send videos too (before any initial release) and receive videos for analysis using machine learning (at a later time). I'm assuming this won't need to be in real-time? (ie you aren't receiving the video from a live source and then processing it in real time). Because if not, I think the easiest solution would then be to use S3 (or similar cloud storage solution) as an intermediary. You can provide a link to whoever is providing the video to upload to, then you can download the video from S3 (or other storage) to process it when you need to. Also gives you the ability to go back and look at the video and iterate on your machine learning algos. Otherwise if you receive the video by for example uploading over a TCP/WebSocket connection, you will need to store it somewhere anyways in case you wanted to iterate on your algorithms and re-run them. If there is a hard real-time requirement things are much different :)
Rust explicitly does *not* do SSO.
I came up with a recursive macro to generate the conditional tree to avoid having to implement any special adapter: https://old.reddit.com/r/rust/comments/aft2h3/hey_rustaceans_got_an_easy_question_ask_here_32019/eeaks2l/
You're correct. I saw a patch years ago but it was never accepted. 
I followed the readme was able to run the server but got these errors in the JS console: In Chrome ``` Error loading Rust wasm module 'rustmith\_frontend': CompileError: AsyncCompile: Compiling wasm function "wasm-function\[12663\]" failed: local count too large @+3346616 ``` In Firefox ``` CompileError: wasm validation error: at offset 3346617: too many locals Error loading Rust wasm module 'rustmith\_frontend': CompileError: "wasm validation error: at offset 3346617: too many locals" ```
That's exactly what I felt like! I was able to understand C++ way better after getting comfortable with Rust. 
there's a link at the top of the github project: https://rustmith.rocks/
It kind of has to be, but unless there is a fundamental change that has to be done for that (i.e. Rewrite most of the request-handling), I don't want to waste a lot of time on it. I might implement it a long time from now (my guess is at least 1 year, probably 2-3) IF there will be research in parallel that requires further analysis of the user of the app (medical research). If I get to that point, I'll have a whole team available. 
You could use a bounded `sync_channel` so that the producer blocks until a worker thread takes another task from the channel. I'd also recommend looking into the rayon crate, if your problem can be expressed with a `par_iter()` or `rayon::join()`, it can be very simple to use and very performant. 
But there is smallvec/smallstring if you'd want that behavior
I would very much like 1. to hear what the Rust team at Mozilla thinks. 2. to be a part of such an effort
&gt; Will Rust stabilize in 2019 ? It depends what you mean by stabilize. - Will development stop? No. - Will the rate at which new features are introduced slow down? Most likely. No programming language seem to ever stop evolving. Even dear old C saw a dramatic change in 2011 with the introduction of a standardized memory model for multi-threaded programs. &gt; Coming in Rust in 2019 I need to understand the edition thing (which IMHO is a very good way to fragment the projects and thus collaboration) Actually, the edition model is *specifically* about avoiding fragmentation. As counter-examples consider: 1. Python: the move from Python 2 to Python 3, with modules incompatible with each other, is still not finalized after so many years. 2. C++: the cruft accumulated since 1983 is still mostly there, even when obsolete in practice. The edition model is specifically an answer to those issues, assuming you can use a relatively new version of the compiler^1 then you can mix and match crates written for edition 2015 and for edition 2018 and deliver your own libraries and binaries targeting either edition. As a result, the only fragmentation in the ecosystem is due to part of it being unable to upgrade their compiler for reasons other than technical^2 . The most prominent example likely being embedded development where a toolchain need be certified, which is a costly endeavor. ^1 *A general requirement in other programming languages.* ^2 *A technical reason would be a bug or backward incompatibility in the Rust compiler; the former is inevitable, though bugs should be temporary, the latter is generally avoided, with crater runs being used to compile every (public) crate under the Sun to ensure backward-compatibility.* &gt; I need to understand that the code I'm reading which was written 4 months ago may not embeds the latest Rust features so there is a better/cleaner way to achieve the thing today. That is true for any major programming language: the introduction of streams Java 8 helped clean up vast amounts of code, the extension of `constexpr` facilities in C++14 likewise, and the upcoming modules and concepts in C++20 (or later) are total game changers. The key thing, however, is that the old approaches *still work*. You should never feel pressured to rewrite code that is clean and works well. &gt; I need to understand that in 6 or 12 weeks a new feature will be released in stable and thus the pattern I'm struggling to implement will be deprecated See above. Although, if you struggled because of the lack of said feature, it should be a relief to have it be simple now. No? &gt; I need to understand that some talented peoples works on projects targeting nightly only and thus we are not be able to collaborate / take advantage of their work in a stable project. There are two cases. You are willing to live on the bleeding edge, and suffer the inevitable blood loss that results from it: use nightly too, simply pick your version carefully. You are NOT willing to live on the bleeding edge. In this case, the work of those talented people is also too cutting edge for you. They are *explorers* working on *prototypes*; they are *trail blazers* discovering *patterns*. The feature changing over time is the least of your worries, the fact is that experience is just insufficient to efficiently use said feature: no pattern, no idiom, ... let the explorers explore, and stick to well-trodden features! *Note: There are always people exploring, even in stable languages. Exploration is fun. Prototyping is fun. Prototypes are not meant to be used in production code, however. And Proof Of Concept are not meant to be used at all.* &gt; I'm may miss other things that result in a fragmented community, but is there any plan to stop this ? I have followed Rust for... 8 years now? There is *one* schism in the ecosystem, and it is located over the `no_std` divide. Otherwise, I do not observe a fragmented community; I do observe groups: hobbyists vs professionals, explorers vs production users, ... but such exist in any community, and do not necessarily translate into fragmentation of the community as long as there is sharing of ideas (if not code) and people move from a group to another. &gt; I'm a little bit afraid to invest my time in a language which will become a monstrosity of stacked features in 2 years. This is indeed a risk. The only reassurance I can offer is that you are not the only one wishing for some conservatism in the feature-set, so hopefully the more conservative among us will manage to rein in the more progressive ones. This is no guarantee, of course.
I really wanna read this article but the website is RIP. :(
I think everyone could learn a thing or two from how numpy gets its speed. Struct of Array is really how modern computers want data to be, I think.
I know from previous work on this that not having the iterator be inlined is slow enough that I want to optimize it. We can call `.next()` 10^10 times a minute, and the logic surrounding the `.next` call benefits from being optimized with the logic inside it.
This article was an excellent read! I do have one question, though. In the article, the original rust code is fn count_doubles(_py: Python, val: &amp;str) -&gt; PyResult&lt;u64&gt; { let mut total = 0u64; // There is an improved version later on this post for (c1, c2) in val.chars().zip(val.chars().skip(1)) { if c1 == c2 { total += 1; } } Ok(total) } and near the end a reader sends in an improved version which only iterates over the string once fn count_doubles_once(_py: Python, val: &amp;str) -&gt; PyResult&lt;u64&gt; { let mut total = 0u64; let mut chars = val.chars(); if let Some(mut c1) = chars.next() { for c2 in chars { if c1 == c2 { total += 1; } c1 = c2; } } Ok(total) } The latter function is a little over twice as fast as the former function, which is a little surprising to me. I would have guessed that the original version is pretty close to the post's fairly-speedy C implementation, with some additional overhead for the unicode checks: uint64_t count_byte_doubles(char * str) { uint64_t count = 0; while (str[0] &amp;&amp; str[1]) { if (str[0] == str[1]) count++; str++; } return count; } Is this difference between `count_doubles()` and `count_doubles_once` primarily due to the overhead of having to repeat the unicode checks twice for each character? Or is there something else at play here?
You should almost always use cdylib in preference to dylib. cdylib only exports functions with the C ABI where dylib exports rust ones too. By making it a cdylib you can save some precious bytes in the resulting binary. Since the rust ABI isn't stable there really isn't any point in using dylib.
&gt; std::vector&lt;bool&gt; is specialized and does not store bool[], but instead acts as a bitset Does the spec require `vector&lt;bool&gt;` to be a bitset or is that just a path taken by the compilers? 
That's a cool macro! Thanks!
This is the final thing that I am trying to do: for word in input.split\_whitespace() { let str = foo(word) - foo is a function that returns a string if !hashmap.contains\_key(str) { hashmap.insert(str, value) } else { &amp;#x200B; } }
As an example of this, here's the same code with both optimization levels: https://godbolt.org/z/r7mHrU For just the loop part, the unoptimized assembly contains 28 lines, with two function calls (their line count not included in that 28, so even more there!) and a panic handler. In the optimized case, it's three lines.
I know that amethyst have found that using rayon causes a small percentage of the cpu to be in constant use.
Have you dealt with the issue that everyone is pointing out, that you're allocating a heap object on every iteration in Rust, but not in C? If you haven't dealt with that, there's no reason to expect anything close to similar performance.
Yes, I should have mentioned this, thanks.
Why is Rust trying to have as small of a runtime as possible? Having a larger/bigger runtime, doesn't mean it's slower, but just that more stuff comes with it, correct? So the only negative point I can think of is binary size. But that doesn't seem like such a huge problem. So what am I missing?
Agreed that the boxing itself wouldn't be an issue, that's a one time cost. The issue would just be the fact that `dyn` means the compiler can't inline the different `next` functions. I haven't (yet) benchmarked the impact of the options themselves, the proposal of basically just always doing a comparison for each option isn't a bad one, though I like the macro solution better.
The `dbg!` macro is pure awesomeness!
There is a generic array based string tho. It's still being optimized but it works and more eyes won't hurt. https://github.com/paulocsanz/arraystring
I've updated the rawloader benchmark up to 1.32: http://chimper.org/rawloader-rustc-benchmarks/ In total rust has gotten ~8% faster since 1.20, with specific cases getting 50-60% faster. The regressions in 1.25 are actually still present but `chunks_exact()` and `chunks_exact_mut()` solve those regressions (and then some) and their usage isn't too hard to make backwards compatible: https://github.com/pedrocr/rawloader/commit/da5ed8cf5b09ccaeeb8b63e0abb1d3c9289a6521 I can't recommend these APIs enough. They make tight loops have fewer bounds checks without doing a bunch of unsafe and ugly code. It's a great example of how rust abstractions make for really good tradeoffs between code quality and speed. These results don't even include all the gains that recent versions of rust allow from new features: - **u128**: some algorithms can take advantage of the wider integer types. I've done some tests but haven't yet used it - **const fn**: this can probably be a big gain for some things that can be calculated at compile time for common cases instead of always on demand (e.g., huffman tables) - **target_feature**: for auto-vectorization just being able to have several versions of functions compiled with support for extra CPU features can be quite valuable I agree that the focus for the next edition of rust should be stability, in no small part because we already have a bunch of goodies like these that not all the ecosystem is taking advantage of.
What do you mean by RIP?
Cool! The `dbg` is a nice addition!
https://en.cppreference.com/w/cpp/container/vector_bool In short, it seems optional... but done by everyone.
An absolutely staggering amount of effort has been put into Cython and Numba (27K commts between them). It is exceptionally tedious (and just plain difficult) to write code with Python's FFI that upholds all the expected memory invariants. Having written a fair bit of code at the here-be-dragons interface of CPython/Numpy/C, I can't help but think that an alternative version of Numpy could be written in Rust (with safe and simple Rust interop) by a small team in just a few years and would be absolutely revolutionary. Reason being that Rust is expressive to write bindings that would guarantee that if it compiles, the safety invariants are being upheld. You only have to look at `PyO3` to see what is possible. Writing the supposedly-gnarly low-level optimizations would no longer be the preserve only of CPython experts. To be honest I'm a bit saddened to think of all the effort that has been put into Python's C ecosystem. If something like Rust had existed 20 years ago, so much effort could have been saved.
I would like to read an instruction on how to integrate Rust into Python package and publish it on PyPI. I wonder if we will be able to achieve the same level of seamlessness as with npm.
What‚Äôs it doing?
Could you confirm that macro is not compiled in ‚Äîrelease mode?
Are the newer benchmarks using the default allocator? I'd like to know the practical differences in execution time between system and jemalloc, as well as other factors such as memory usage and binary size.
`dbg!(expr)` will print `$expr = debug(eval($expr))` to stderr.
I see that‚Äôs nice! Thx for explanation ;)
I made the macro a bit shorter/simpler, thanks again! macro_rules! expand_patterns { // if we're out of patterns, call `f` with the iterator we've constructed (($iter:ident =&gt; $use_iter:ident)) =&gt; { $use_iter($iter) }; ( // we have to define the identifier used for the iterator because of hygiene // defining the identifier of the function is just for reusability ($iter:ident =&gt; $use_iter:ident) // expressions cannot be followed by blocks in macros if let $pat:pat = ($expr:expr) $new_iter:block $($rest:tt)* ) =&gt; { if let $pat = $expr { let $iter = $new_iter; // we recurse with the remaining patterns in both cases expand_patterns!(($iter =&gt; $use_iter) $($rest)*) } else { expand_patterns!(($iter =&gt; $use_iter) $($rest)*) } }; }
It is compiled in at all times.
(you're probably being downvoted because it's explained at length in the post)
Visibility on free-floating bounties like this is limited...which is why I organized Not-Yet-Awesome Rust's issues to serve as anchors for bounties too: https://www.bountysource.com/teams/not-yet-awesome-rust/issues
Oh must‚Äôve missed that. Sry I‚Äôm on my pohne...
Website is not pulling up. Might be my work firewall though.
Everything uses defaults so I think the default allocator is being used for 1.32+. I'm not currently storing memory usage, so I'll have to rerun the benchmark to get that. For file size here's the situation: Version | Size --- | --- 1.20.0 | 4.8M 1.21.0 | 4.9M 1.22.1 | 4.9M 1.23.0 | 5.1M 1.24.1 | 6.2M 1.25.0 | 5.7M 1.26.2 | 6.4M 1.27.2 | 6.5M 1.28.0 | 5.0M 1.29.2 | 5.1M 1.30.1 | 5.0M 1.31.1 | 5.0M 1.32.0 | 3.4M beta | 3.4M nightly | 3.5M The difference seems quite large. Could jemalloc really be taking up 1.6MB?
Another nice feature is that it will return the same thing `expr` would have returned so you do not have to restructure your code when adding it.
Rust aims to be usable everywhere where you can use C. Having a runtime adds some restrictions - usually any bigger runtime features require an operating system to be present, so that would probably prevent Rust's use on embedded devices.
It's all good!
&gt;Having a larger/bigger runtime, doesn't mean it's slower, but just that more stuff comes with it, correct? Not quite; a runtime system isn't just the stuff that comes with a program, but the execution environment the program runs in. Rust borrows from C in having this be as minimal as possible - the runtime in Rust is really just setting up a panic handler, calling the main() function, and letting it do what it wants. This is really useful for having code be predictable because there's no surprises - if you write some lines of code in a Rust function, those lines will execute and nothing else, whereas in a language with a heavier runtime there may be arbitrary other code running (like insertion of GC pauses, or transfer of control between green threads.) Rust chose to have these kind of things be managed by the user; it has runtimes (like `tokio`,) but they are opt-in and work basically the same as any other Rust code.
OK. Take it [https://gist.github.com/kpp/617abd2656a21a319022f202a207b37c](https://gist.github.com/kpp/617abd2656a21a319022f202a207b37c)
That's really quite nice, i'd missed that bit!
That does sound a bit large; you could try adding the jemallocator crate and comparing.
Did you mean `impl&lt;T: Foo&gt; Handler&lt;Msg&gt; for T { ... }`?
To clarify, `dbg!` isn't a debug-compile version of `print!`, but instead invokes the `Debug` implementation to print out a value. This is useful because Debug has a `#derive(Debug)` macro, which makes it easy to implement, but implementing [`std::fmt::Display`](https://doc.rust-lang.org/std/fmt/trait.Display.html) is (usually) manual. Source: [https://doc.rust-lang.org/std/macro.dbg.html](https://doc.rust-lang.org/std/macro.dbg.html)
Also, modified so that `use_iter` is an arbitrary expression: macro_rules! expand_patterns { // if we're out of patterns, call `f` with the iterator we've constructed (($iter:ident =&gt; $use_iter:expr)) =&gt; { $use_iter }; ( // we have to define the identifier used for the iterator because of hygiene // defining the identifier of the function is just for reusability ($iter:ident =&gt; $use_iter:expr) // expressions cannot be followed by blocks in macros if let $pat:pat = ($expr:expr) $new_iter:block $($rest:tt)* ) =&gt; { if let $pat = $expr { let $iter = $new_iter; // we recurse with the remaining patterns in both cases expand_patterns!(($iter =&gt; $use_iter) $($rest)*) } else { expand_patterns!(($iter =&gt; $use_iter) $($rest)*) } }; } fn use_iter(channel: bool, iter: impl Iterator&lt;Item = (u64, bool)&gt;) -&gt; u64 { let mut ret = 0; for (t, v) in iter { println!("{}: {:?}", t, v == channel); ret += t; } ret } fn main() { let start_time = Some(10); let end_time = Some(50); let photon_count = Some(70); let iter = (0 .. 100).map(|t| (t, t &amp; 1 != 0)); let res = expand_patterns! { (iter =&gt; use_iter(false, iter)) if let Some(start_time) = (start_time) { iter.skip_while(|&amp;(t, _)| t &lt; start_time) } if let Some(end_time) = (end_time) { iter.take_while(|&amp;(t, _)| t &lt;= end_time) } if let Some(photon_count) = (photon_count) { iter.take(photon_count) } }; println!("{}", res); }
Possibly, it really is quite large, between the library and its debug symbols it's probably north of an MB.
It really is! Debugging support isn't great on OSX, so convenient printing of expression results is SO HELPFUL!
Switching back to jemalloc as described in the release notes makes the 3.4MB go up to a whopping 7.5MB. So it may very well be jemalloc and apparently as a crate it's even worse
The links on "[Other Installation Methods](https://forge.rust-lang.org/other-installation-methods.html#standalone)" haven't been updated since 1.31.0 (now two versions behind).
I tried this too, but then I get " type parameter \`T\` must be used as the type parameter for some local type" as an error.
I googled it and found that it‚Äôs a macro for temporary debug and not supposed to be retained in the code. [dbg! reference doc ](https://doc.rust-lang.org/beta/std/macro.dbg.html) It is a bit unfortunate that we cannot run a debugger mode with variable inspection for this case. I googled this for vs-code and felt to dumb/newbie to attempt this. 
Could you [file a bug](https://github.com/rust-lang/www.rust-lang.org/issues/new/choose) please?
The crate uses jemalloc 5, while rustc provided jemalloc 4 (or maybe 3?).
Good find!
After talking with alex, I think my understanding of jemalloc's size was actually smaller than it was, so this does seem inline. Another way you could test this would be to use 1.31 and use the system allocator there. Anyway, thanks for doing all of this!
Yeah I guess the `loop` wasn't necessary.
If you read the title &gt; Rust is a *systems programming language* focused on three goals: safety, speed, and concurrency. Most of peoples considers it be a "Better C++", but I look at it rather "Better C#/Java/...". I'm myself a senior C# developer, and worked with it quite enough to see that Rust is much more powerful than just C++ replacement. If you want concrete examples: you probably don't want to write a web server in C++. In older days you would pick Java sprint or C# MVC, but now you can bring Actix-web, for example. I've written an article about "Rust being system language" myths, but they were not designed for English readers so I probably should translate them.
I can see myself using the dbg macro a lot. Now I'm wondering how I can make sure that I never accidentally leave a dbg call somewhere. Do you guys think this is something that clippy should warn about? Or would that be too annoying?
[done](https://github.com/rust-lang/www.rust-lang.org/issues/688)
Is there a good overview somewhere of how modules work with the 1.32 changes? The link in this post just goes to the [github tracking issue](https://github.com/rust-lang/rust/pull/56759), which isn't a great introduction to the current module system.
C++ isn't really general purpose. For example, you *might* write a web server in C++, but you probably *won't*, because you can take Java/C#, or even Go (sorry about the latter). I mean I look at rust like a language that is as convinient as Java/C#, but with extra safety layer. In my day-to-day C# development I spend lots of time fighting NullReferenceExceptions (NPEs in Java). I strugle because of exception-based flow, where I just can't design `Either&lt;Result, Error&gt;`, and I have to write ugly wrappers to handle async exceptions... All this stuff is resolved in Rust, and it doesn't bring much overhead if you look at things pragmatically. I have written a messenger bot (basically a web server that handles web hooks) which do some magic with images, and I didn't specify a single lifetime, and I have written just one single `Arc&lt;Rc&lt;MyDb&gt;&gt;`, to share image database accross multiple web requests.
How competitive is `ralloc` from the Redox OS project by the way?
That seems odd to me. You would think that would get stripped from a release version. 
I like have they have the most important thing for last: Cargo registry now has usernames
Shame indeed. But I'm afraid a hella lot of the Numpy effort was about creating bindings to Fortran libs, not even C ones. Also, I don't think that Numba's effort counts towards C ecosystem as it targets, IIRC, either LLVM IR, or CUDA.
Firefox currently uses an MP4 decoder in Rust, by the way: https://github.com/mozilla/mp4parse-rust &gt; If someone is looking for something to do, writing GStreamer plugins around the already existing Rust implementations for various codecs we have would be very useful and I'd be happy to help. Oh, this is very cool! If those are filed on GStreamer bug tracker, we could get them included in This Week In Rust call for participation and/or highlight them on this subreddit.
I think you may be misunderstanding what that means; this is for HTTP auth, not some sort of namespacing feature.
NOOOOO
https://doc.rust-lang.org/edition-guide/rust-2018/module-system/path-clarity.html
Thanks!
&gt; clippy &gt; too annoying 
That‚Äôs because you can only do this `for T` trick if the trait is defined within the same crate. The way I see it, there are two options. 1. Create a macro the generate the handler and call it for each new actor. 2. Create a `struct FooHandler&lt;T: Foo&gt;(T)` that acts as wrapper and implement the handler as `impl &lt;T&gt; Handler&lt;Msg&gt; for FooHandler&lt;T&gt;`.
Ahhh ok. Very nice explanation. Makes perfect sense. Thanks so much for clearing this up!!
If you want this in Clippy, please open an issue. There's already [check for `unimplemented!`](https://rust-lang.github.io/rust-clippy/current/index.html#unimplemented) but this is disabled by default. For now you can always create a pre-commit hook with simple `grep`.
Are rust files considered objects to the compiler? I noticed while making some general helper functions for a small snake game, that you could reference functions within the file using the 'self' keyword, without a struct/impl in sight. For example: `pub fn to_coord() -&gt; f64 {`
Have you considered trying Rayon's parallel iterators?
I'd like to add that there are platforms where every byte counts, so having a minimal binary size is a plus.
Nice article! It‚Äôs also worth checking out the [PyO3](https://github.com/PyO3/pyo3) crate for developing python extensions in Rust. There is a breakdown of the difference with rust-cpython [here](https://github.com/PyO3/pyo3/issues/55) 
Whoops, thanks! Missed that.
Nice article! It‚Äôs also worth checking out the [PyO3](https://github.com/PyO3/pyo3) crate for developing python extensions in Rust. There is a breakdown of the difference with rust-cpython [here](https://github.com/PyO3/pyo3/issues/55).
Just did a small "Hello world" test to measure how much binary size has changed between 1.31 and 1.32 (which I assume would completely or almost entirely be due to the switch from jemalloc to the system allocator. A more rigorous test would've used the jemalloc crate, but I just did something quick without setting up a Cargo project). Source: fn main() { println!("Hello world"); } Compiled with -O flag. Binary size compiled with 1.31: 584,508 bytes (389,660 bytes with debug symbols stripped using `strip`) Binary size compiled with 1.32: 276,208 bytes (182,704 bytes with debug symbols stripped using `strip`) That's a 53% reduction in binary size (for both stripped and non-stripped versions), which is pretty impressive. Although for larger programs the impact would likely be a lot smaller, this is the starting point. By comparison, here's other languages for a similar "Hello world" program: **Go:** Source: package main import "fmt" func main() { fmt.Println("hello world") } Result: 2,003,480 bytes (1,585,688 bytes with debug info stripped using `-ldflags "-s -w"`) **C:** Source: #include &lt;stdio.h&gt; int main() { printf("Hello, World!"); return 0; } Result: (LLVM 8.1 compiled with -O2): 8,432 bytes (stripping with `strip` actually increases size by 8 bytes). Per [this article](https://lifthrasiir.github.io/rustlog/why-is-a-rust-executable-large.html), most of the remaining size is likely due to dynamic linking and use of libstd, changing which is a bigger effort/impact than just switching out the allocator. Just thought you may find this interesting.
wasted a good opportunity to call it rushb
No. It's a bit confusing, but `self` as a keyword can signify either an object or, in this case, a path. In paths, `self` refers to the current module's scope.
 macro_rules! debug_dbg { ($($x: tt)*) =&gt; {{ if cfg!(debug_assertions) { dbg!($($x)*) } else { $($x)* } }} }
&gt; Another way you could test this would be to use 1.31 and use the system allocator there. That's easy enough to test, how do I set the system one? &gt; Anyway, thanks for doing all of this! It's been a fun way to get to know rust performance a little bit better. And while there is still plenty to do I think it's already at a great level compared to C/C++.
I think it's meant to be even more temporary than that - you can always use `log::debug!()` and the `release_max_level_info` feature flag (or logger configuration) to have debug statements which don't show up in release builds. I would expect any log statements left in a codebase to use `log` anyways, not `println!()` or `eprintln!()` (or `dbg!`, which wraps `eprintln!()`). I imagine `dbg!()` is more of "add, compile, test, remove" thing. Having it in release mode as well is consistent, and useful for anyone who's codebase is too slow to effectively use or debug in debug mode. It shouldn't stay in a repository _in any case_, and it prints to stderr directly. I'm excited to use it in playpen examples, small test programs, and as a more convenient version of "println debugging". There are already facilities for more advanced and/or permanent debug logging. Though a `debug_dbg!()` macro to call `log::debug!()` with the semantics of `dbg!()` would be quite nice. 
Which Go version did you use?
This? [https://docs.rs/amethyst/0.9.0/amethyst/ecs/prelude/type.ReadStorage.html](https://docs.rs/amethyst/0.9.0/amethyst/ecs/prelude/type.ReadStorage.html) which leads to [https://docs.rs/amethyst/0.9.0/amethyst/ecs/prelude/struct.Storage.html#impl-Join](https://docs.rs/amethyst/0.9.0/amethyst/ecs/prelude/struct.Storage.html#impl-Join) and then to [https://docs.rs/amethyst/0.9.0/amethyst/ecs/join/trait.Join.html](https://docs.rs/amethyst/0.9.0/amethyst/ecs/join/trait.Join.html) which Amethyst implements for tuples of various sizes.
 use std::alloc::System; #[global_allocator] static GLOBAL: System = System;
1.11.4
Thanks. In 1.31.1 using the system allocator makes it go from 5.0 to 4.0MB. So it does seem like the jemalloc penalty was 1MB+ and apparently the new crate one is 4MB+ at least in rawloader. Odd.
I wonder if it really is the different versions, maybe jemalloc itself has gotten much larger.
You could still use a closure for that. Something like `let mut addbar = || { foo += bar; bar += 1; };`
&gt; Also, how do you define methods on Tuples? Is it a trait that must be implemented on every element in the tuple in order for the method to be called the entire tuple? I'm not familiar with Amythest in particular, but this is most likely what's happening. Any crate can declare a trait, and then implement it on other types (like tuples). The methods of the trait will be usable like inherent methods on the tuple, but only when the trait is imported. Here's a simpler example: trait MySum { fn my_sum(&amp;self) -&gt; i64; } impl MySum for i32 { fn my_sum(&amp;self) -&gt; i64 { *self as i64 } } impl MySum for i64 { fn my_sum(&amp;self) -&gt; i64 { *self } } // impl MySum for all two-element tuples made up of things which are also MySum. impl&lt;T, U&gt; MySum for (T, U) where T: MySum, U: MySum, { fn my_sum(&amp;self) -&gt; i64 { self.0.my_sum() + self.1.my_sum() } } // impl MySum for all three-element tuples made up of things which are also MySum. impl&lt;T, U, V&gt; MySum for (T, U, V) where T: MySum, U: MySum, V: MySum, { fn my_sum(&amp;self) -&gt; i64 { self.0.my_sum() + self.1.my_sum() + self.2.my_sum() } } // ... fn main() { println!("{}", (4i32, 5i64, -22i64).my_sum()); } If someone else wants to use `x.my_sum()`, they have to first `use my_crate::MySum;` to bring the trait into scope. This is a simple example, but it looks like Amythest goes even further and implements the trait for much larger tuples as well. I think I can see an implementation for a 17-long tuple? When making this many implementations, macros can be used to help reduce the boilerplate - making all this quite a bit simpler than 17 implementations written all the way out. But in the end it is just an `impl` block for each size of tuple, and then one for every concrete type the tuples can contain.
Is there a similar benchmark for compilation speed?
Quick web search found this: https://perf.rust-lang.org/dashboard.html
Found my previous worries on the subject on extending Python with Rust: https://www.reddit.com/r/rust/comments/9j17g9/write_your_next_c_extension_in_rust/e6o2nq0 TL;DR: number-crunching doesn't exactly make Rust features shine, CPU-heavy logic code requires taking a sizable chunk of your problem's domain across the language border, so why choose Rust then?
Perfect, thanks!
`Join` is a trait located in the `specs::join` module. It has a few methods, but the main one is `join`. Any type that implements `Join` has a `join` method. specs itself provides implementations of `Join` for tuples up to so some reasonable size. This is how `.join()` exists on tuples and why you have to import `Join` to use it, since you must import traits to use their methods. As for what it actually does: ultimately there's a `Join` implementation for all storages, the containers that store components, and an implementation for `EntitiesRes` which stores entities. Under the hood, each of these containers has a `BitSet` which is a hierarchical bitset that stores which entity IDs have components (or for `EntitiesRes` which ones exist). When you call `.join()` on a tuple, it returns an `Iterator` that finds entity IDs that appear in every `BitSet` and returns those components and entities.
D implemented something [pretty similar](https://dlang.org/spec/cpp_interface.html) to what you're suggesting. It has a lot of caveats (no RTTI, no exceptions, all used templates must be instantiated in the object file, etc.), but it's certainly still useful. Rust should be able to do the same thing, though it will likely be more difficult since Rust's type system is more different from C++ (no overloading!). I doubt we'll see it any time soon, though; nobody is going to donate a person-year of time for this. Mozilla would certainly benefit but they seem happy enough with bindgen and manual interface wrappers.
Thank you kind sir! I appreciate that.
There was a big discussion on whether the output should be stripped out in release mode or not; I thought it should be but the libs team decided to keep it in all modes.
This is cool! I've been learning Rust, and have been trying to do some reverse engineering and DLL injection as well. We've got some stuff in place to add password hashing and better user management to a private server for a game we play now :)
Seeking past the end of a `File` doesn't return an error either: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=42f8e267846dae79e270bdb4e718468a
In case #2, since a new struct is getting created, I'd have to essentially pass that around instead of the original Actor correct? And if I wanted to treat that new thing as an Actor I'd have to do some kind of casting magic to get at the inner Actor struct? 
Somewhat related: Florob gave a great talk on the Fn traits, see the video/slides linked [here](https://rust.cologne/2018/09/05/fun-traits.html).
The `dbg!` macro makes [ad hoc profiling](https://blog.mozilla.org/nnethercote/2018/07/24/ad-hoc-profiling/) a bit easier, which is nice.
Can confirm, came from C++, also started by writing windows dynamic libraries to load in other processes.
Hmm, debugging in CLion with IntelliJ-idea seems to work fine for me in OS X. 
Am I reading that right? Hello world in Go is 1.5 - 2Mb? 
Yes, hence the "a lot of work."
Once you start packaging a mandatory GC, it could easily get that big.
Go does static compilation by default, which is why the binaries have a larger ‚Äúminimum ‚Äú size. 
&gt;Go does static compilation by default So does Rust, no? That's why it's so much bigger than C - it statically links stdlib.
Ah I see. Thanks for clearing that up! :) 
Hah; interesting! -- when I proposed `dbg!` I never imagined it as a profiling aid; happy that it has more use cases. :)
 vagrant@vagrant-ubuntu-trusty-64:~$ ldd main-go not a dynamic executable vagrant@vagrant-ubuntu-trusty-64:~$ ldd main-rs linux-vdso.so.1 =&gt; (0x00007ffccb1e3000) libdl.so.2 =&gt; /lib/x86_64-linux-gnu/libdl.so.2 (0x00007efe9a7b5000) librt.so.1 =&gt; /lib/x86_64-linux-gnu/librt.so.1 (0x00007efe9a5ad000) libpthread.so.0 =&gt; /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007efe9a38f000) libgcc_s.so.1 =&gt; /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007efe9a179000) libc.so.6 =&gt; /lib/x86_64-linux-gnu/libc.so.6 (0x00007efe99db0000) /lib64/ld-linux-x86-64.so.2 (0x00007efe9abeb000) vagrant@vagrant-ubuntu-trusty-64:~$
It does static linking only for other Rust libraries. Other things are usually dynamically linked like C.
This is an API offered by the specs crate for handling entities in the world. It creates an iterator that filters entities which lack associations with those storages.
I didn't have time to trial-and-error my way to build sizes which exactly match the original poster's results, but re-testing with a statically linked libc is as simple as: rustup target add x86_64-unknown-linux-musl cargo build --release --target x86_64-unknown-linux-musl (With the `cargo build` line adapted to match whatever /u/GeneReddit123 was using before, of course.)
We need a more prominent place to put this.
Any ideas why stripping the C binary increases size?
Probably because it's already stripped. It's like trying to zip an already zipped file, it only increases the size slightly due to added metadata, but without being able to meaningfully do anything.
It's a closure that accepts a &amp;path::Path and returns an io: :Result&lt;&gt;. I ended up reworking the block and got everything to work. The issue appeared to be that I used the name entry_path twice. The second line referenced the first, meaning it was borrowing and couldn't pass the borrow to the naming function since the original entry_path was overwritten. I think.?
&gt;but nobody ships Rust's one (yet). I think the Debian team are making quite a lot of headway to make that possible.
Thanks for your suggestions. I guess one could share the receiving end of a `sync_channel` between consumers with some combination of `Arc` and `Mutex`. I'll look into rayon as well, though I'm not sure how I would express my problem there.
Thank you for this really elaborated response! According to your experience, If I want to create a backend in rust composed of multiple services, is it possible to have some targeting stable, and some other nightly or is too much headache ? (the services need to communicate)
Right, but I was pointing out that Go does this **by default** to explain why the binary is so large. Here is a comparison using your static build approach: vagrant@vagrant-ubuntu-trusty-64:~$ ls -la target/x86_64-unknown-linux-musl/release/main-rs -rwxrwxr-x 2 vagrant vagrant 2613977 Jan 18 00:32 target/x86_64-unknown-linux-musl/release/main-rs vagrant@vagrant-ubuntu-trusty-64:~$ ldd target/x86_64-unknown-linux-musl/release/main-rs not a dynamic executable vagrant@vagrant-ubuntu-trusty-64:~$ ls -la main-go -rwxrwxr-x 1 vagrant vagrant 1906945 Jan 17 23:00 main-go
Thank you!
Yep. For me `write!(s, "{}", i).unwrap()` runs in about half the time as the original with optimizations. Without optimizations, they take about the same time. https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=f5b535898a46fc7561d4458b08c7eed3
As of the time of this post, the [official standalone installer page](https://forge.rust-lang.org/other-installation-methods.html#standalone) incorrectly lists 1.30.0 as the latest stable release. For users who prefer or need standalone installers, please use the URL templates bellow or the following concrete links to download your packages until this issue has been resolved. The URL template for the normal rust installation is: * `https://static.rust-lang.org/dist/rust-1.32.0-{TARGET-TRIPPLE}.{EXT}` * `https://static.rust-lang.org/dist/rust-1.32.0-{TARGET-TRIPPLE}.{EXT}.asc` The URL template for additional compilation targets (`x86_64-unknown-linux-musl`, `wasm32-unknown-unknown`, ..etc) is: * `https://static.rust-lang.org/dist/rust-std-1.32.0-{TARGET-TRIPPLE}.{EXT}` * `https://static.rust-lang.org/dist/rust-std-1.32.0-{TARGET-TRIPPLE}.{EXT}.asc` ## Standalone Installers (Standard Toolchain + Host Target) [aarch64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-aarch64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-aarch64-unknown-linux-gnu.tar.gz.asc) [arm-unknown-linux-gnueabi.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabi.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabi.tar.gz.asc) [arm-unknown-linux-gnueabihf.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabihf.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabihf.tar.gz.asc) [i686-apple-darwin.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.tar.gz.asc) [i686-apple-darwin.pkg](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.pkg) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.pkg.asc) [i686-pc-windows-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.tar.gz.asc) [i686-pc-windows-gnu.msi](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.msi.asc) [i686-pc-windows-msvc.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.tar.gz.asc) [i686-pc-windows-msvc.msi](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.msi.asc) [i686-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-unknown-linux-gnu.tar.gz.asc) [mips-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mips-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mips-unknown-linux-gnu.tar.gz.asc) [mipsel-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mipsel-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mipsel-unknown-linux-gnu.tar.gz.asc) [mips64-unknown-linux-gnuabi64.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mips64-unknown-linux-gnuabi64.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mips64-unknown-linux-gnuabi64.tar.gz.asc) [powerpc-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc-unknown-linux-gnu.tar.gz.asc) [powerpc64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64-unknown-linux-gnu.tar.gz.asc) [powerpc64le-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64le-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64le-unknown-linux-gnu.tar.gz.asc) [s390x-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-s390x-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-s390x-unknown-linux-gnu.tar.gz.asc) [x86_64-apple-darwin.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.tar.gz.asc) [x86_64-apple-darwin.pkg](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.pkg) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.pkg.asc) [x86_64-pc-windows-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.tar.gz.asc) [x86_64-pc-windows-gnu.msi](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.msi.asc) [x86_64-pc-windows-msvc.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.tar.gz.asc) [x86_64-pc-windows-msvc.msi](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.msi.asc) [x86_64-unknown-freebsd.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-freebsd.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-freebsd.tar.gz.asc) [x86_64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-linux-gnu.tar.gz.asc) [x86_64-unknown-netbsd.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-netbsd.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-netbsd.tar.gz.asc)
As of the time of this post, the [official standalone installer page](https://forge.rust-lang.org/other-installation-methods.html#standalone) incorrectly lists 1.30.0 as the latest stable release. For users who prefer or need standalone installers, please use the URL templates bellow or the following concrete links to download your packages until this issue has been resolved. The URL template for the normal rust installation is: * `https://static.rust-lang.org/dist/rust-1.32.0-{TARGET-TRIPPLE}.{EXT}` * `https://static.rust-lang.org/dist/rust-1.32.0-{TARGET-TRIPPLE}.{EXT}.asc` The URL template for additional compilation targets (`x86_64-unknown-linux-musl`, `wasm32-unknown-unknown`, ..etc) is: * `https://static.rust-lang.org/dist/rust-std-1.32.0-{TARGET-TRIPPLE}.{EXT}` * `https://static.rust-lang.org/dist/rust-std-1.32.0-{TARGET-TRIPPLE}.{EXT}.asc` ## Standalone Installers (Standard Toolchain + Host Target) * [aarch64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-aarch64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-aarch64-unknown-linux-gnu.tar.gz.asc) * [arm-unknown-linux-gnueabi.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabi.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabi.tar.gz.asc) * [arm-unknown-linux-gnueabihf.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabihf.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-arm-unknown-linux-gnueabihf.tar.gz.asc) * [i686-apple-darwin.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.tar.gz.asc) * [i686-apple-darwin.pkg](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.pkg) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-apple-darwin.pkg.asc) * [i686-pc-windows-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.tar.gz.asc) * [i686-pc-windows-gnu.msi](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-gnu.msi.asc) * [i686-pc-windows-msvc.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.tar.gz.asc) * [i686-pc-windows-msvc.msi](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-pc-windows-msvc.msi.asc) * [i686-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-i686-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-i686-unknown-linux-gnu.tar.gz.asc) * [mips-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mips-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mips-unknown-linux-gnu.tar.gz.asc) * [mipsel-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mipsel-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mipsel-unknown-linux-gnu.tar.gz.asc) * [mips64-unknown-linux-gnuabi64.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-mips64-unknown-linux-gnuabi64.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-mips64-unknown-linux-gnuabi64.tar.gz.asc) * [powerpc-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc-unknown-linux-gnu.tar.gz.asc) * [powerpc64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64-unknown-linux-gnu.tar.gz.asc) * [powerpc64le-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64le-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-powerpc64le-unknown-linux-gnu.tar.gz.asc) * [s390x-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-s390x-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-s390x-unknown-linux-gnu.tar.gz.asc) * [x86_64-apple-darwin.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.tar.gz.asc) * [x86_64-apple-darwin.pkg](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.pkg) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-apple-darwin.pkg.asc) * [x86_64-pc-windows-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.tar.gz.asc) * [x86_64-pc-windows-gnu.msi](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-gnu.msi.asc) * [x86_64-pc-windows-msvc.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.tar.gz.asc) * [x86_64-pc-windows-msvc.msi](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.msi) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-pc-windows-msvc.msi.asc) * [x86_64-unknown-freebsd.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-freebsd.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-freebsd.tar.gz.asc) * [x86_64-unknown-linux-gnu.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-linux-gnu.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-linux-gnu.tar.gz.asc) * [x86_64-unknown-netbsd.tar.gz](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-netbsd.tar.gz) [asc](https://static.rust-lang.org/dist/rust-1.32.0-x86_64-unknown-netbsd.tar.gz.asc) (Continued ...)
&gt; Self can now be used as a constructor and pattern for unit and tuple structs. Well that's awesome.
Using [milksnake](https://github.com/getsentry/milksnake) you can easily build Python extensions written in Rust and publish them (as wheels) to a pypi server (either pypi.python.org or your own): ``` python setup.py bdist_wheel ``` I'm using [twine](https://github.com/pypa/twine/) for this: ``` twine upload --repository-url https://internal.localshop.pypi.example.com --username ${TWINE_USERNAME} --password ${TWINE_PASSWORD} dist/* ```
Whats the best way to not doctest comments that are obviously not rust code? For example, in writing my interpreter I wanted to document the spec in the comments for the checking functions so I know whats going on. So I write something like this. pub fn is_binop(&amp;self) -&gt; bool { //! checking if a binary operator //! //! '+' | '-' | '*' | '/' | '^' | '%' | '..' | //! '&lt;' | '&lt;=' | '&gt;' | '&gt;=' | '==' | '~=' | //! and | or ... } And it tells me the tests fail, because it tries to compile this as rust code. I thought it would only test things that start with `\`\`\`rust`. I know I can start the code block with `\`\`\`ignore` but its messy and it shows ignore in the test output, making me feel like I'm doing something bad. Any way to set it so that I only test blocks with `\`\`\`rust` or a nicer way to display that kind of stuff? Mostly this is for inside the code (not the docs) so I can make sure I understand what the spec is.
Why can't rust automatically generate thin wrappers? In go I was able to do `C.function_name` or `C.struct` and it was just available without declaring each function.
Thanks, fixed!
Thanks! Hygiene sort-of works, to be precise, if you don't do any string manipulation and only use macros in item position (which is the only stable way to use them), then hygiene should work.
&gt; It is a bit unfortunate that we cannot run a debugger mode with variable inspection for this case Some people need to debug their code in release mode, because the non-optimized version is so slow that it is almost unusable. Being able to use `dbg!` I'm such a scenarios is very important.
Is there a version of Haskell programmer but with inject/reduce pattern so you still use a functional iterator, but don't need to build an intermediate array to calculate the sum?
That's what I would aim for as well. Is it that hard to want a functional style but not want an intermediate array built each time? I never coded in Haskell, but I coded Ruby and would just do `iterator.inject {|sum, val| sum + val }`, or (because Ruby loves being magic), the equivalent `iterator.inject(:+)`. Boom, done. For error handling though, unless I *needed* to handle it somehow special (and yet not special enough to just crash the program with `.unwrap()`, I'd just replace invalid values with 0 and pipe them to the fold like nothing happened.
That's what I would aim for as well regarding using `fold`. Is it that hard to want a functional style but not want an intermediate array built each time? I never coded in Haskell, but I coded Ruby and would just do `iterator.inject {|sum, val| sum + val }`, or (because Ruby loves being magic), the equivalent `iterator.inject(:+)`. Boom, done. For error handling though, unless I *needed* to handle it somehow special (and yet not special enough to just crash the program with `.unwrap()`, I'd just replace invalid values with 0 and pipe them to the fold like nothing happened.
Rust is amazing because it's also one of the few languages where resource safety is encoded into the type system. Affine typing is magic.
What game?
I really like how this looks on the new website. Clean, clear, works well on mobile. It's more than a lot of sites can say, and I appreciate it. Well done, website designers.
Oh that debug macro is making my day!!!!
I was wondering why people was sharing code with ``` Then I checked out and the new layout of reddit renders it correctly while the old doesn't :(
I wrote a tutorial on how to do write a rust extension with py03 and distribute through pypi here: http://benfrederickson.com/writing-python-extensions-in-rust-using-pyo3/ . The post lists out how to get binary wheels uploaded automatically to pypi everytime you tag a release on github.
Does switching to system allocator make it link against it, or does it statically compile it as well (and still save space)? 
By default, rust dynamically links to libc. You can use MUSL if you want as well.
Hey. Thanks for the tip. But I don't get one thing: what if I need to change e.g. my var type from `const` to `let` (in js it's legal)? It seems impossible because I get just `Statement` enum back. ```rust enum VarKind { Var, Let, Const, } struct VarStatement { kind: VarKind } enum Statement { Var(Box&lt;VarStatement&gt;) } fn main() { let var = Statement::Var(Box::new(VarStatement { kind: VarKind::Const })); var.kind = VarKind::Let; // error[E0609]: no field `kind` on type `Statement` } ```
Wow, that's super helpful, thank you!
Blame reddit. That's intentional on their part.
This article is a nice little journey over lifetimes. There's more to proving them to be safe than I could ever imagine. Hopefully I'll get a better understanding of them soon, but for now I'm happy the compiler has those rules strictly applied.
Because when you index into a vector you're actually calling the vector's [index_mut](https://doc.rust-lang.org/std/ops/trait.IndexMut.html) method, which mutably borrows the whole vector.
The thing about enums is that you don't know what variant it is. So you'd want to do something like: let mut var = Statement::Var(Box::new(VarStatement { kind: VarKind::Const })); match var { Statement::Var(var) =&gt; { var.kind = VarKind::Let; } }
What is the difference between what `index_mut()` does and what my `foo()` does? To me, both borrow the container mutably and then mutably borrow its members (except `index_mut()` returns the mutable references while `foo()` doesn't), so the difference between how a vector is treated over a struct remains unexplained.
May be it could behave like debug_assert
For starters, you could post this to the correct subreddit. This is for the Rust programming language :) /r/playrust
People like you make me love Rust more! Thanks for this. 
I'm not sure if this quite answers your question, but in order for Rust to provide the guarantees that it does, it would have to prove that multiple mutable borrows to the same array are disjoint, that none of them ever point to the same element(s). Say that you index into an array based on a runtime-known value. How does the compiler prove that you're not doing mutable aliasing on the same array?
Your `foo()` is effectively equivalent to `split_mut()`. To borrow two things using `index_mut()`, you'd have to call it two times, which you can't do unless the first borrow expires before you make the second call. `foo()` and `split_mut()` produce two borrows from a single function call. Think of it like locking. All these functions take a lock on the Vec which is released when the returned item(s) go/goes out of scope. You can't `index_mut()` two things at once, because the lock held by the first one blocks the attempt to acquire a second one. `foo()` and `split_mut()` on the other hand, borrow multiple items using a single "acquisition of the lock". If you want to go lower level, it's because indexing is implemented via a function call, while struct member references are compiler-internal constructs. As such, the compiler can reason more strongly about the struct member references.
Did you strip the binary? I can easily get 3-4MiB in a Hello World in Rust *without* using musl-libc just because it embeds debugging symbols.
Hi, project developer here. The project is still in early stage and I'm simply exploring what is possible to achieve with Rust+WASM. Could you please let me know your browser version/platform, so I could debug the issue? &amp;#x200B; I added right click prevention to workaround a problem with long taps on tablet devices. So that you could record sustains when in editor mode.
Can this be parallelized using rayon? It would seem that a divide and counter that checks out the borders when joining results (subtracting from the 'total repeated char group results' of the partition if there is a partioned group at the borders, with a minimum of 0 (for the case where the whole partition is a repeated car) might work?
Could anyone suggest me some good books about Rust? I'm actually sick of Python right now...
I assume "mutable aliasing" means having multiple mutable references to the same thing. &gt;How does the compiler prove that you're not doing mutable aliasing on the same array? You're saying that a struct's members being bound to names assures they're unique while an array's members being bound to index numbers doesn't? But if you push a non-`Copy` thing to `v[0]`, pushing that same thing again to `v[1]` would be illegal because it would be trying to use a moved value (with the `Copy` trait, it would just be copied). So how would you even do mutable aliasing? It seems like having different array indices, as in `&amp;mut v[0]` and `&amp;mut v[1]` would be enough evidence that the members are different in the same way as different names are.
i swear to god
Just my 2c here. I recently was doing a similar survey of the options and landed on actix-web. I liked how I could take synchronous code like diesel and put it in an actor to make it async. My goal was to make the server async ‚Äúall the way down‚Äù. In general I found that it just works. 
&gt;indexing is implemented via a function call, while struct member references are compiler-internal constructs. As such, the compiler can reason more strongly about the struct member references. I think I understand. Will you permit me to interpret this as, "even though vectors are structs, their access to members doesn't have as much compiler magic as that of regular structs, making stricter borrowing rules necessary"?
Your interpretation is incorrect. Vectors are the same type of struct as your Colony. The difference is that when you index *you are making a function call*. Accessing a field of a struct is *not a function call*. That is the fundamental difference here.
I apologize. I edited my post (right before I saw yours) from "even though vectors are structs, their access to members doesn't have as much compiler magic as that of regular structs, making stricter borrowing rules necessary" to "even though vectors are structs, their access to members **via indexing** doesn't have as much compiler magic as that of regular structs, making stricter borrowing rules necessary" I assume you sign off on the edited version.
Not quite, because again, indexing is just a function call. You could substitute the code `[4]` for `.get_mut(4)` and literally nothing would change. Thats the important leap to make here, there's nothing special about indexing, there's nothing special about vecs vs other structs. The only thing that matters here is that the borrow checker does not look inside function calls, it only looks at their signatures. Because member access is not a function call, this limitation doesn't apply. That's all, there's no magic.
Too late, I already squat that noooo crate.
Are there any Rust engines for Code Climate I'm missing? I want to start using some automated code review engine in CI but I don't see any engine available for Rust.
&gt;indexing is just a function call. You could substitute the code `some_vec[4]` for `some_vec.get_mut(4)` &gt;The only thing that matters here is that the borrow checker does not look inside function calls, it only looks at their signatures. And so it can discriminate between named member borrows but not indexed member borrows. This deciphers the magic and makes complete sense. Thank you.
There is not.
Thanks for providing all this information. It seems you have tons of knowledge about storage systems! PickleDB is obviously not meant to be used at the scale you mentioned. In the small scale PickleDB is targeted for these complex scenarios are probably rare and handling them would be an overkill. But I do agree about the atomic move and I'll make that change. I'll also take a look at fsync. Thank you so much for your help!
Thanks for you suggestion. I'm not sure how the binding between wasm, js and pickledb would work. Could you please elaborate on that?
This seems like an [XY Problem](http://xyproblem.info/).
/r/amethyst
What if you have a situation like this? fn multi_mut(some_index: usize, another_index: usize) { let my_array = [0; 200]; let mb1 = &amp;mut my_array[some_index]; let mb2 = &amp;mut my_array[another_index]; // . . . } Since these indexes are only known at runtime, how would the compiler know that `some_index != another_index`?
You can specify a language, so it will not be tested, and will not appear with the ignore highlight: //! ```text //! stuff goes here //! ``` 
&gt; what makes even the C program compiled with -O2 use over 3 times more memory than the assembly example I assume because it's loading `libSystem`. Check with `otool -L a.out`. The variation in memory usage is probably due to some quirks in the dynamic loader. Also compile with `cc -m32` to make the comparison fair. (It's the same length on my system.) &gt; is calling printf more complex than making a direct system call to write? Yes, because it has to handle format strings. But in this case Clang is smart and specializes `printf("string without any percent signs")` into a call to `write(int fd, void *buf, size_t nbytes)`.
You're welcome! Standalone rust installers are currently poorly documented (especially for auxiliary/cross-compilation target installers). I'm merely doing my part to change that. If you found my post lacking in any way, please don't hesitate to point it out.
Note that the actual raw pointer types in Rust use `_.is_null ()` and `!_.is_null()`, so you're in good company for the explicit options. If you really want convert-to-truthy and nobody else has to read your code, the convert-to-truthy idiom in JS is `!!val` which works in this situation as well! (Please don't do this. Use `.is_null` for a nullable type if you must, and consider if your use case can instead use `Option` instead of an intrinsically nullable type.)
I think you can implement Deref to automatically cast to the inner Actor.
Maybe you can describe some abstract future which will be easier to implement (or even only-possible) with polonius?
In that case, the compiler could not know. :o But that isn't the same case as hard-coded numbers like with `&amp;mut v[0]` and `&amp;mut v[1]`. (But I've taken from the other subthread that index access entails a function call and that the borrow checker does not penetrate into functions even if it sees hard-coded numbers passed as arguments to those functions. That explains why the compiler ends up not knowing at all when it comes to indexing while knowing when it comes to normal, name-accessed struct members. I repeat this to reify it to myself and to offer it up for any possible corrections.)
That's just a black line for all Rust community. Thank you for everything you done.
[vector.bool]p3 says: &gt; There is no requirement that the data be stored as a contiguous allocation of bool values. A space-optimized representation of bits is recommended instead. That's just a recommendation. OTOH, p2 says: &gt; Unless described below, all operations have the same requirements and semantics as the primary vector template, except that operations dealing with the bool value type map to bit values in the container storage [...] So that implies that the container storage *has* to be bit-packed. Anyway, realistically you can't not bit-pack, because that's what the users expect.
You can also do stuff like if dbg!(value == 1) {} Which will work as a normal if, but also print the equality and the result
You're looking for r/playrust.
To illustrate, if you had: impl Colony { fn get_colonists(&amp;mut self) -&gt; &amp;mut Vec&lt;String&gt; { &amp;self.colonists } fn get_logs(&amp;mut self) -&gt; &amp;mut Vec&lt;String&gt; { &amp;self.logs } } this would also fail: let cols = plymouth.get_colonists(); let logs = plymouth.get_logs(); because they are function calls, instead of direct member access...
https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d58e0e8dab9b4f8c0fc11e1c29678f5e - just to get rid of the boilerplate, without `failure`. I think the io::Error here is kind of justified.
Congratulations on successfully blogging about a career change without feeling the need to air your grievances about your employer.
And damn, there is no crate named sulphur. Someone, grab it!
Coincidence or not, but I believe, situation could be completely different for both you and Steve, if Mozilla treated you differently. In my opinion, if an exceptional specialist, like you guys, leaves a company it almost always means that something in the company is not quite right, or at least, not as good as it could be. Otherwise it would not even be a question.
It depends on what Nick will be working on. It's okay to leave something to work on something else. About Steve: yeah I think so.
The tarballs page is not updated https://forge.rust-lang.org/other-installation-methods.html It currently still lists 1.31.0 builds, so didn't even get 1.31.1 (however the src link at the bottom is for 1.31.1)
&gt;It's okay to leave something to work on something else. Of course. Only Nick knows for sure, and he's the only one to decide. Everything else is just my speculation.
Same with LLDB integration in VSCode, it just works. 
Out of the Loop here. Which Steve?
dbg! Isn‚Äôt a macro that you should keep ‚Äúaround‚Äù. We probably should a check to the style check in the compiler to error if the code contains dbg!. Same to clippy.
[Steve Klabnik](https://words.steveklabnik.com/thank-u-next)
I think it‚Äôs reasonable to switch companies after 7 years. Sometimes you‚Äôre just looking for a fresh challenge in a new environment. 
The new crate can compile jemalloc in debug or release mode, with / without debug symbols, and many other options. So just saying ‚Äúthe new crate‚Äù isn‚Äôt enough to conclude anything. How are these users precisely using the new crate? 
Thanks /u/zzyzzyxx and /u/CyborgPurge. That's what I need :)
You could also do custom windowing of the source range and simply have the ranges overlap by one character. Custom windowing is probably already necessary if you want to split a string on UTF-8 boundaries.
I thought rust really disliked sending overlapped ranges to threads?
Not if they're read-only.
Thanks for all your great work Nick!
Damn, that sucks. Best of luck to him and hope Mozilla gets their shit together.
Also i think the range will have to be two shared characters at least. (because of the BA|AB case where neither side started counting a group so instead of subtracting from the total a if a group on the second substring starts on the first, you need to add).
You can implement `BitAnd` for your `Thing`, with `type output bool`.
Could it be that you're overcomplicating the problem statement? I don't understand what special case you mean here.
Watch out for some edge cases when using printf in eg benchmarking code: https://github.com/rust-lang/rust/issues/50519
Well, he wants to count the number of character repetitions. By dividing the string we can intersect a group in these ways: AA|AA - group started on string 1 (s1) and continued on s2, s2 must not count that group. BA|AB - s2 must realize that there is a group (s2 since each substring only looks at the predecessor) BA|AA - doesn't matter, s1 doesn't count a group and s2 does.
&gt; Well, he wants to count the number of character repetitions(groups). No, based on the implementations, he wants to count the number of adjacent identical characters. So "AAA" counts for two. 
Doh.
&gt; dbg! Isn‚Äôt a macro that you should keep ‚Äúaround‚Äù. We are agreed; the issue was around accidental around-keeping and the embarrassment that might cause users of `dbg!`.
Disabling optimizations is a Rust feature that allows you to step through the execution of your program "as you wrote it" in the debugger. This is not possible when optimizations are enabled because the compiler can re-write your program to something completely different from what you wrote, as long as it produces the same output, but that's completely useless for debugging. 
Indeed, /u/doublehyphen pointed out that `dbg!` returns the evaluated value.
A lot of workloads got significantly slower due to changing the default allocator away from jemalloc. Disk is cheap, this is a silly thing to optimize for in a default, as long as people who need to cut bloat are empowered to do so. Compute is not cheap. Datacenters are set to use 20% of the world's power by 2025 and we need to strive to keep usage down. This is a serious ethical issue, and I'm disappointed about the specific metrics that the project decided to optimize for in this decision. People will die in a non-theoretical way because of higher compute requirements. People don't expect to need to change their allocator in a modern language, they will just spin up twice the servers for allocation-heavy workloads. It's amazing how infrequently people who do capacity planning actually look at cost or energy usage, and this concern tends to be punted to a small number of engineers in large organizations who usually have their hands full with 10x the problems they can actually get to. &amp;#x200B; If rust is going to be a serious systems programming language, then it needs to consider serious metrics at scale. &amp;#x200B; This also reduces the funnel for rust, as a lot of the people who write articles about how surprised they are that their naive rust beats \_\_\_\_whatever\_other\_impl\_\_\_\_ are going to see far lower performance, reducing the "holy shit, wow!" moment. These people don't know about GlobalAlloc.
&gt;jemalloc Current sled gets 2-3x SLOWER without jemalloc, as will many allocation-heavy workloads. I think this is a totally bad and unethical decision.
People seem excited about that `dbg!` macro (and I don‚Äôt want people to think I‚Äôm whining: I‚Äôm not) but I don‚Äôt get why they‚Äôre so excited. The Rust ecosystem has been building for years and LLVM provides already pretty neat tools to debug (`lldb` and the `rust-lldb` wrapper, etc.). You also have `valgrind` and all of its tools, and there‚Äôs even `rr` that kicks poneys in salt water. I‚Äôm not blaming them for this macro (it actually seems to be doing its job), but I think it encourages people to do print-debugging. Print-debugging is fine when you don‚Äôt have a debugger. But we do. I remember a time when I thought *¬´ Print-debugging is okay in web development ¬ª*, but as you might all already know, that argument doesn‚Äôt hold anymore since pretty much all modern web browsers have an integrated debugger. The only place where such print-debugging might still be a thing is in scripting languages and DSL. What is missing the most to me (only talking about dev experience here) is a somewhat involvement into famous debuggers and editors to have a better experience. For instance, I would love rust-lang to officially provide or support a (neo)vim plugin to integrate `lldb` into (neo)vim. Or maybe a nice GUI backend to `lldb`. Have you tried `lldb` yet? Besides the very stern aspect of the user interface, it‚Äôs a really great debugger. Also, kudos for removing `jemalloc`! As a demoscener, I‚Äôm hyped about this. :) I‚Äôm also **very happy** to see that `literal` macro_rules matcher! I‚Äôve been wanting that for a while! Congrats on the new release and have a beer! \m/
It was a pleasure working with you on racer, however briefly before I had to move on to other things. Can't wait to see what you do next.
The guy who looks after Rust debugging is also leaving (Rust, I don't know if he works at Mozilla or not) https://internals.rust-lang.org/t/how-to-support-rust-debugging-post-tromey/9207
dbg! has nothing to do with debuggers. It's a replacement for `eprintln!`.
A match arm's return value will be the return value of its last line, with the one exception that if the last line ends with a semicolon, it'll return `()`. (I think of it as a terminal `;` being equivalent to `; ()`.) You can replace `info!("{:?}", i);` with this: info!("{:?}", i); i ...but you're likely to get an error about some branches of the function not returning the expected return type so you'll need to have every branch return *something* that is allowed by the function signature or reach a diverging function. (A diverging function is a where the function signature uses `-&gt; !` to indicate that it never returns, like the stuff `panic!` expands to.) (eg. If you put `Option&lt;i64&gt;` as the return type, you can return `Some(i)` or `None`.) If you want to return multiple different types, declare an `enum` and return that instead. (`Result` and `Option` are just enums that all the standard library bits are designed to use.)
MOC?
I agree, goodbye //TODO: delete this upon print statements for me :D
I really loathe the redesign, I don't want clickable bigass boxes for posts, it's stupid. Like, if I have the reddit window in the background and partially obscured by another window I have to click on the browser window in order to bring it to the forefront. Now what happens if I accidentally click on a post "box"? Well it opens it even though I don't want that to happen. Same thing for the whole damn modal window that opens up when you click on a post. Why the hell would I want a modal window that's narrower than the index view of the subreddit? Why would I want the modal window to close when I misclick so that I hit the background instead of the modal part? Annoying mobile only redesign... /rant
Other posts have explained that there is a difference between borrowing elements of a vector, fields of structs and calling methods that borrow the struct and then return references to it's elements. I feel like they didn't explain the actual reasons behind these seemingly diverse behaviors though: The rust compiler is conservative in it's assumptions - e.g. if it cannot be absolutely certain that a particular operation is safe, it errs on the side of caution and does not allow it. Since the compiler cannot prove that you are borrowing disjoint parts of a vector, or that a function only borrows certain parts of a struct and not the whole object, it forces you to borrow the whole vector/struct to be safe. In the case of vectors, one could easily prove disjoint borrows when using constant vector indexes. The only reason this is not done yet is that nobody has implemented it :) The case of functions borrowing the entire struct (self) instance instead of a set of fields is a bit more involved. Functions will suddenly become much more complex behind the scenes since they'll need to incorporate information about the fields they borrow. What if it's a method borrowing private fields? Those should be private implementation detail, yet the callers of this function will suddenly need to be able to deal with this information, since their ability to use this method now suddenly depends on information that is actually supposed to be hidden from them. There are multiple open questions on how to deal with the issue without unnecessary bloat in function metadata and leaking information/forcing callers to deal with private state that they should not be aware of. Directly borrowing fields of a struct is a cut and dry case though where the compiler can easily prove which fields are borrowed and when. Note that if you are willing to use unsafe code, nothing prevents you from taking a pointer to the vector's buffer (or fields of a struct) and materializing multiple mutable references to it's elements. The onus is then on \_you\_ to ensure that you'll never have multiple mutable references to the same item. There'll be people who are militantly against this (it does circumvent a lot of the safety guarantees that rust gives you), but it is a tool that one can use to work around the current limitations. There is a saying though that if you feel that the borrow checker is limiting you, there is a good chance that it is a sign that your data structures are not laid out optimally and you should spend a bit of time thinking about different approaches (struct of arrays vs. array of structs, using indices instead of references, etc...).
```rust fn main() { let var = Arc::new(Mutex::new(0isize)); secret(move || { let x = rand::random(); { let mut var = var.lock().unwrap(); *var = x; } sleep(Duration::from_millis(1)); { let var = var.lock().unwrap(); assert_eq!(x, *var); } }); } ``` The code gives up lock protection and wish the value stay the same afterwards, which I am inclined to see it as a program logic flaw versus unintended bug in Haskell... 
[Meta Object Compiler](http://doc.qt.io/qt-5/why-moc.html), a code generator that Qt uses. 
I'm also a bit suspicious of the "bug" that makes pages keep loading using the new layout until I refresh.
Hi. I'm trying to clear the console and draw back to it in a way that doesn't flicker. I've tried a couple of clear screen methods but they all flicker. Any suggestions for a flicker-free way to render console output?
its cool! thx a lot!
The code is not formatted properly on mobile - perhaps indent with 4 spaces? 
That was definitely my intention. That's what I was getting at by referring to this code as "blatant" and "explicit." The original Haskell bug is unintentional and harder to spot.
AFAICT it sounds like an emerging trend of devs leaving Mozilla and/or rust. If indeed that's the case I suspect (and I hope) that so much fantastic work has already taken place to bootstrap rust to where it is now. Momentum for this fantastic New language that will continue to propel it and attract talent. 
For those of us on old Reddit, your code block doesn't display properly because old Reddit doesn't support fenced code blocks. Please indent by four spaces instead.
I like the way you structured the learning. It is a good way to people know that they are making progress. Nice work! &amp;#x200B;
You could read this [issue](https://github.com/rust-lang/rust/issues/36963) to know more about the reason behind changing default allocator.
Do you still have the code? The link does not work anymore. I am also busy using tiberius with rocket and i think your code would help me solve some of my issues. Could you please share it again?
With a good debugger, anywhere you would put a `println` you should be able to put a breakpoint and see the variable value, with the bonus of not needing to recompile to change breakpoints or look at a different value. That said, i heavily use print-debugging. ^^^^^Mostly ^^^^^because ^^^^^i ^^^^^don't ^^^^^know ^^^^^how ^^^^^to ^^^^^use ^^^^^real ^^^^^ones ^^^^^very ^^^^^well.
Hi, my name is Daler, I‚Äôm the project manager of the website this article is published on. Could you please go into details and tell what happened? What do you mean saying ‚Äúwebsite is not pulling up‚Äù? We‚Äôll try to solve the problem so that such issues no longer arise.
[The official on](https://doc.rust-lang.org/book/) is pretty popular, rightfully so imho. And it's free! There's a good one from O'reilly, too, but I suggest you dip your toes first before shelving out money :)
&gt; but I don‚Äôt get why they‚Äôre so excited The reason why folks are excited is because many of us, myself include, are printf debuggers. For me in particular, I am an *unabashed* printf debugger. Therefore, when that experience gets a noticeable increase in quality of life, folks get excited. I know I'm certainly happy about it. Now maybe this is just a proxy for you not understanding why someone would use printf to debug a programmer when a suitable debugger exists. But that's a totally separate question, and I think it's pretty easy to chalk it up to some combination of "personal preference" and "problem domain." (For example, just because I am an unabashed printf debugger doesn't mean I never use a debugger.)
Here: fn main() { let var = Arc::new(Mutex::new(0isize)); secret(move || { let x = rand::random(); { let mut var = var.lock().unwrap(); *var = x; } sleep(Duration::from_millis(1)); { let var = var.lock().unwrap(); assert_eq!(x, *var); } }); }
It's also completely broken in old reddit. Looks like they added some sort of non-working version of fenced code blocks to it.
Isn't that exactly the point the article makes around this snippet? &gt; I don‚Äôt want to imply that it‚Äôs impossible to make a logic error in the Rust code. What I am saying is that Rust naturally pushed us towards the correct solution here. However, it‚Äôs possible for us to reduce the scope of the mutex lock and end up in the same situation as the Haskell code: &gt; [ snippet above] &gt; However, this code is more blatant about its lack of thread safety (technically atomicity, it's completely thread-safe).
What about writing or adapting a C++ library that is easier to mix with rust That might be an option in the specific collaboration I'm thinking about - one of the people uses his own vector instead of std::vector (something i've also done alot myself). I do understand it's very hard.. 2 complex languages , some similarities but a lot of differences too.
How about a clippy flag option to warn on dbg!? Maybe one to strip dbg! Too? That could be considered just bloating clippy I guess. Maybe one of the smart people here will make a stand-alone tool to warn and strip them or maybe better still turn dbg! bits into comments. i.e /** dbg!(something) **/ Hmmm that would make it possible to revert them back too. Newbie here, please tell me why this might not be a good idea.
&gt; sled [sled?](https://github.com/spacejam/sled/blob/master/README.md) Why is it alloc-heavy though? I'm far from an expert, but similar software (filesystems, database engines) have been living with primitive and slow allocation for a long time, no? What fraction of the *total* workload is sled in a typical application? 3x 0.1% isn't very much. Also more real time is not necessarily the same as more energy consumption, and loading more and more statically linked instances of jemalloc into memory has an energy cost too. Are you measuring energy?
&gt; Firefox currently uses an MP4 decoder in Rust, by the way: &gt; https://github.com/mozilla/mp4parse-rust Last time I looked at it, it looked rather minimal and only really suitable for web use-cases. MP4 for the web is relatively easy but if you need to handle it in general there are so many variants out there... &gt; Oh, this is very cool! If those are filed on GStreamer bug tracker, we could get them included in &gt; This Week In Rust call for participation and/or highlight them on this subreddit. If you point me &gt; to them I will get a call for participation out there. That sounds great, I'll reply to this message some time this weekend with links once I got to it.
See the release notes. I did it exactly like that. If debug symbols are the default for release mode builds then that may explain it. It's an odd choice though.
Why are you wasting time on double-posting to lobby Rust when glibc matters much more?
Steve Klabnik, however, [mentioned his reasons for leaving](https://words.steveklabnik.com/thank-u-next): &gt; What‚Äôs really important is this: I‚Äôm not proud to be a Mozillian anymore. A variety of incidents contributed to this, but at the end of the day, it‚Äôs what‚Äôs true. Furthermore, I don‚Äôt have any personal opportunity at Mozilla; I recently discovered I‚Äôm the lowest-paid person on my team, and Mozilla doesn‚Äôt pay particularly well in the first place. In order to have any kind of career growth, I would have to not do the work that aligns with my skills, and what I‚Äôm doing now is really how I can have the most impact on Rust moving forward. I don‚Äôt believe that has any chance of changing; when I‚Äôve tried to express my frustrations, I‚Äôve only gotten disciplined. Mozilla is not interested in hearing what I have to say.
As \u\RetraRoyale said, Clippy could suggest to replace `assert_eq` with your compile-time check using `transmute`.
It's in documentation [https://doc.rust-lang.org/std/io/trait.Seek.html#tymethod.seek](https://doc.rust-lang.org/std/io/trait.Seek.html#tymethod.seek). &gt;Seek to an offset, in bytes, in a stream. &gt; &gt;***A seek beyond the end of a stream is allowed, but behavior is defined by the implementation.*** &gt; &gt;If the seek operation completed successfully, this method returns the new position from the start of the stream. That position can be used later with [SeekFrom::Start](https://doc.rust-lang.org/std/io/enum.SeekFrom.html#variant.Start). And implementation [https://doc.rust-lang.org/src/std/io/cursor.rs.html#206-224](https://doc.rust-lang.org/src/std/io/cursor.rs.html#206-224) for Cursor (answer for OP). For File if I checked correctly it uses lseek() on Linux and SetFilePointerEx on Windows. And both allows you to seek beyond the end. &gt;**lseek**() allows the file offset to be set beyond the end of the file (but this does not change the size of the file). If data is later written at this point, subsequent reads of the data in the gap (a "hole") return null bytes ('\\0') until data is actually written into the gap. &amp;#x200B; &gt;Note that it is not an error to set the file pointer to a position beyond the end of the file. The size of the file does not increase until you call the [SetEndOfFile](https://msdn.microsoft.com/2a579609-144a-4b77-8605-87aecf1f0957), [WriteFile](https://msdn.microsoft.com/9d6fa723-fe3e-4052-b0b3-2686eee076a7), or [WriteFileEx](https://msdn.microsoft.com/6995c4ee-ba91-41d5-b72d-19dc2eb95945) function. A write operation increases the size of the file to the file pointer position plus the size of the buffer written, leaving the intervening bytes uninitialized. &amp;#x200B;
Some database engines solve this by having their own allocators, for example PostgreSQL uses their own arena allocator to reduce the number of malloc() and free() calls.
An old MMORPG called Monster &amp; Me
&gt; 0.2.8 should be a small release to switch to TinyTemplate for generating HTML and reduce the size of Criterion.rs' dependency tree in a few other ways. Glad to hear that! (I remember that one library rejected my port of their benchmarks to Criterion.rs due to the large dependency tree.)
At the end of the day though, it really, really helps to have people paid full-time to work on Rust the language.
Which basically boils down to ‚Äúuse C as a middle layer‚Äù.
We discussed the !! option on github, we decided not to and made fun of javascript for a bit :p
Thanks for all your work, and thanks for that awesome talk! https://www.youtube.com/watch?v=vqavdUGKeb4
The chief speedbumps with debuggers for me is that they don't play nicely with minimalist text editors or optimizing compilers. It would be really, really nice if there were a way annotate a breakpoint in a comment or - even better - to write a compiler barrier which the debugger also knows about and can set a breakpoint at *automatically*. Print, though, look: it's a limited compiler barrier which is easy to drop into the source code The best of both worlds would be something like dbg! that instead of gathering some data (source code location, value) and then printing it, gathers that same data and calls a function which, in debugging builds only, invokes a no-op side-effect. You can then set a breakpoint or conditional breakpoint there. In release builds, it's a pure no-op. You could use that macro within fully optimized debugging builds (which are a thing); it'll report the watch expression exactly as coded and in the same order. ....And now I know what my first published crate should be. Darn.
It‚Äôs only a default. The blog post explains how to opt into using jemalloc, and this will [soon](https://github.com/alexcrichton/jemallocator/tree/master/jemallocator-global#documentation--usage) be reduced to a single line in `Cargo.toml`.
&gt; Print-debugging is fine when you don‚Äôt have a debugger. But we do. Not always. For example I‚Äôve never managed to get the stars to align well enough to use remote gdb with an Android target. Debugger support absolutely should be improved as well. The existence of `dbg!` does not go against that.
[Sure](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2015&amp;gist=744e1be5a0f76746a1175e06ba66ffc9) you can do this, but you should be ashamed if you do.
 # This site can‚Äôt be reached **habr.com** took too long to respond. Looks like its being blocked by my companies firewall. Opens up just fine on my cell phone. Nothing you can do about it! Thanks for reaching out though. 
Does it still block a thread? If yes, what‚Äôs the point of turning sync code to async?
I've written of [pyo3-pack](https://github.com/pyo3/pyo3-pack) which tries to do exactly that, supporting pyo3, rust-cpython and cffi. For pyo3 it's as simple as `pyo3-pack publish`. Note that for publishing manylinux-compliant wheels for linux you need to build in a docker container (this is also true for milksnake). There's an prebuild image which you can use with `docker run --rm -v $(pwd):/io konstin2/pyo3-pack build`
&gt;And a catalogue of such pages: [https://wiki.mozilla.org/Areweyet](https://wiki.mozilla.org/Areweyet) It should have been called *Are we there yet*
Do you offer educational discounts? 
I have hehe. I like the control Rust gives me compared to Python that automates things that I'd rather do manually :). 
Print debugging and ide integrated are both just tools, useful in different situations. 
I couldn't agree more. I don't use Rust much (yet-working on it) but I follow it closely, as I work in a security-related field that is also performance-conscious. It's a natural fit. Having a full-time team and commercial backing is a *huge* factor in getting the management types to go along with a decision like switching/adopting a new language, as they must consider things like labor pool, hiring and retention, long-term maintenance and support, and not just the technical features. I also follow nim and zig a little bit, but with nim being mostly a community language (congrats to them for getting some backing recently) and zig being a passion project of a few people, there's much less chance for them. Of course they don't do what Rust does, I just bring that up for contrast.
be careful if you have #[macro_use] extern crate nom; in your file because nom has a macro named `dbg` as well! 
As far as I can tell, [the bounds checks aren't optimized away](https://godbolt.org/z/4lg_t9).
People *have* written web servers in C++. The fact that other languages exist has nothing to do with the capabilities of C++.
Check out [Writing an OS in Rust](https://os.phil-opp.com/) 
I was wondering if warp supports sending headers with it. Could you show me how if it supports it?
&gt; you should always prefer to use a debugger over print-debugging, no matter what ... no, thanks I love debuggers and I have used `rr` with `gdb` to debug some quite gnarly bugs. The kinds of tricky manipulations and advanced inspection of the state of the program you can do are incredibly valuable. However, for the vast majority of common bugs, it takes less time and effort to just add a few print lines and dump all the info I care about, which more often than not shows me what the problem is, than to launch a debugger, set breakpoints, step through code, inspect values, ... Again, for more advanced debugging tasks, a good debugger like `rr` is a godsend. I value my time and mental effort. Why should I reach for the complex tool when the quick and simple solution will do the job? Also, there are many kinds of software that are comparatively tricky to navigate in a debugger. Examples: complex asynchronous networking that runs in a runtime like `tokio`, performance-sensitive software that is so unusably slow when compiled without optimizations that optimized builds are necessary even for debugging, software that is sensitive to timing and latencies, such as real-time audio or video games (games also interact with the GPU, which makes things even trickier), software that is non-deterministic, etc... Coincidentally, the kinds of software I described above are precisely my areas of interest ... 
Good, thanks for the clarification!
Use `///` and markdown syntax. `//` to comment line, and `///` to document code.
The standard library follows this: https://github.com/rust-lang/rfcs/blob/master/text/1574-more-api-documentation-conventions.md#appendix-a-full-conventions-text Rustdoc via `cargo doc` is what you're looking for. 
Favorited, thanks. I will use that in my current project. 
I will read it. Thanks for the help. 
Oh, and for non-api documentation, mdbook is great too. It‚Äôs a separate tool though.
I will keep that in mind. 
Yes pls! Redox is fascinating to me!
Interesting how lately there's been a lot of interest into GUI from the Rust community. One question : &gt;The next big topics will be in semi-random order beside the ones already mentioned in the other sections: &gt; &gt;\* Rendering with OpenGL Why implement the rendering part using OpenGL, and not a more modern graphic API, or better yet, [gfx](https://github.com/gfx-rs/gfx) ? &amp;#x200B;
Part of the problem is us though. Part of the problem is me. I always "intend" to donate. I use so much of their stuff on a daily basis. But I haven't.
The slides look really interesting. I think it would help to add a short pitch to your post, though. 
Do you accept only germans ?
Ouch. Hard to believe Mozilla has so little appreciation for Steve's work. I hope they realize soon what a loss this is.
They make enough money from search engines and pocket ads. They give most of that money to executives though, not software devs
Or just throw !'s everywhere
This guy is incredibly entertaining to watch. Loved the presentation!
&gt; Why implement the rendering part using OpenGL, and not a more modern graphic API, or better yet, [gfx](https://github.com/gfx-rs/gfx) ? GL does the all the resource management for me, so I can focusing on figuring out how to actually build a render tree. wgpu would be an alternative but as far as I know it's not usable in the current state. So I'm sticking with [grr](https://github.com/msiglreith/grr) for now. 
Yes, couldn't agree more. This is one of the best Rust talk. Thanks for all your work. Wish you the best.
I haven't used it, but I can't wait to try, as I am hoping the development build/run cycle is faster. Can anybody speak to this? Of course, we would always `cargo build --release` as usual before releasing the project
As open source projects grow, it's good to cycle in new blood. Too often the projects become an ivory tower plus cults of personality that lean on the judgment and decisionmaking of a handful of lifers to break disputes in the community. Leaving when there's no clear problem with the language's continuing growth was a great way for both of them to ensure Rust has a life of its own.
To clarify: Are you asking why you can't do something like: ``` struct MyType(Vec&lt;u32&gt;); fn foo() { let x = MyType(vec![1, 2, 3]); x.len(); // Error: no method named `len` found for type `MyType` in the current scope } ``` ?
You'll want to use Actix's state management. Take a look at [this example](https://github.com/actix/examples/blob/master/state/src/main.rs). If you want to mutate the vector, you'll need to use a `Mutex&lt;RefCell&lt;Vec&gt;&gt;`, because Rust won't just let you share a vector between concurrent handlers willy-nilly.
(Please note that the paragraph separator here is important; and I've also patched it to say "I don't believe that Mozilla's direction has any chance of changing;")
No, but it's quite useful if you speak at least a little bit German ;-)
That really sucks considering how good the tech coming out of Mozilla is and their outward facing mission as a company. You will think they would have a better company culture considering what they stand for
What I mean is. When doing FFI with C, you must manually declare every C function inside of an \`extern "C" {\` block. And then you have to re declare each struct, union and variable. &amp;#x200B; While with go, point it to a header and all the C functions and types are just available through the C "namespace".
&gt; They give most of that money to executives though, not software devs I'm all for cutting the fat at the top of the company but from reading Steve's post, the real problem is that they don't consider Steve to be a "developer" but some kind of a technical writer and therefore he's not as highly paid. The company culture issue IMO is that they don't consider non-technical work to be as important as technical work.
One prime reason for `dbg!` is that in the playground, which many use from time to time to do something quickly, you don't have a debugger. Being able to use `dbg!(..)` in the playground is therefore a big ergonomics boost. Moreover, sometimes you don't have your primary machine available and so you don't have the debugging environment available; in those cases, print-debugging works well.
Awesome! I knew reading through the documentation that I needed `.with_state()`, but I didn't quite follow how it was laid out. I want to have a `.resource()` that accepts POST, and pushes the posted string into a vector (which is created in `main()`). From what I've gathered, I create a struct with two fields: the vector and the string. Do I just create a variable like `let new_vector = Vec::new();` and put that into `.with_state()`?
I personally think that Rust has quite a steep learning curve at the beginning, it was quite a while before I stopped fighting the borrow checker and embraced it. Rust has taught me a lot of things but I think the concepts of ownership and safe memory management may be a bit difficult to start with. In short: I think it would be great for you but a bit too much for her, try another language first and come back when she is ready to learn how memory management works. I don‚Äôt mean to put you off here as it is certainly possible to start with Rust, ‚Äúthe book‚Äù is an absolutely fantastic resource. Good luck with whatever it is you choose! Also, fellow rustaceans, feel free to disagree with me!
Compered to Python, php, JavaScript etc, it's really a complex language. It provides high level of control to the user, however under strict rules, combination of the two can prove to be overwhelming and thus counter productive.
Python, Lua, Javascript, you are not doing management of memory, at least you don't have to. Variable types aren't too big of a deal either. In Rust, types are strict! You sort of have to deal with memory but not in a "C" way, in that you are not manually allocating and releasing it. Taking what I know into consideration, I don't know if I'd recommend Rust as a beginner language. On one hand, once you get into the flow it's a really fun language. The Community is great and always willing to help. On the other hand, sometimes it feels like you can never do anything right and the borrow checker is always stopping you, or the crate you want to use has conflicts with a crate you're already using, or you forgot to implement one of the cases from a match case. If you choose to learn Rust first, you will understand some of the most important core theories of computer programming. It will be hard fought knowledge and it will serve you very well. I think "End goal" is something you also have to think about. Do you want a scripting language or do you want to be able to release a compiled product. 
I think Rust can teach a programmer _a lot_ about programming, but I don't think I'd use it to teach a non-programmer about programming. It forces coders to consider a lot of things more carefully and explicitly than in other languages - most notably lifetimes and borrowing. Starting someone off with Rust is like throwing someone who can't swim into the deep end of a pool: maybe they'll learn to swim, but a lot of people will sink. I'd recommend you try starting with something simpler and more flexible, something that won't bog a learner down with things they probably won't understand at all for a long time. In my opinion, Java, Python, and C# would both work well for beginners. Once your wife has become reasonably comfortable with programming in one of those, I think Rust (or even C) would then help make great strides in really understanding what happens under the hood and why we have programming languages (and so many).
New Reddit has markdown support, old one doesn't. Simple as that. ;)
There was a twitter thread about this yesterday, maybe [worth a look](https://twitter.com/fuzzybubble21/status/1085994563049848832). I struggle to recommend it as a first language. There is so much more to learn than Python, and the catch is that - unlike, say, C - you have to learn most of it before you can do anything useful. Having some understanding of the stack and the heap is essential. All the memory management stuff (borrowchecker) is pretty subtle and deep. There is also a lot of stuff that is done correctly rather than simply (e.g. floats don't implement Ord and therefore not being sortable, vectors can only be indexed by `usize`, not `u32`, error-handling code is in the type-signature and tends to infect everything) which a beginner will not want to worry about. I would be interested if anyone has in fact learnt it as their first language. It was my second, after Python, and even that was a big conceptual leap ("What's an allocation? How do I start the repl? Where is my IDE?" etc)
You guys look awemsome and tick all my boxes and more (Rust, IoT, industry 4.0, sustainability at heart)! Unfortunately I'm currently not looking for a job. I'll more than likely be moving near Stuttgart in some years, I'll keep you in mind till then.
IntelliJ IDEA is a really nice IDE so far as I can see.
And he's more than capable of contributing in a technical capacity, but perhaps their roles are a bit too well-defined for that. Idk, and it's sad that he's leaving. However, I thought Rust was going to suck after Graydon left, but it still keeps on trucking. I'll miss Steve's contributions, and I hope his departure won't limit Rust's growth too much. I'm looking forward to whatever project he ends up working on.
Here is one way to do it. It uses the nifty `FromResponder` trait to make `actix-web` just give the handler the bits you are interested in, namely the `State` and the body as a `String`. extern crate actix; extern crate actix_web; use std::sync::Arc; use std::sync::Mutex; use actix_web::{server, App, State, HttpResponse, http::Method}; type Words = Arc&lt;Mutex&lt;Vec&lt;String&gt;&gt;&gt;; fn index((words, word): (State&lt;Words&gt;, String)) -&gt; HttpResponse { let mut words = words.lock().unwrap(); words.push(word); println!("{:?}", &amp;*words); HttpResponse::Ok().finish() } fn main() { let sys = actix::System::new("words"); server::new(move || { App::with_state(Words::default()) .resource("/", |r| r.method(Method::POST).with(index)) }).bind("127.0.0.1:8080") .unwrap() .start(); let _ = sys.run(); } 
&gt; And he's more than capable of contributing in a technical capacity, but perhaps their roles are a bit too well-defined for that. Idk, and it's sad that he's leaving. Yes, I absolutely agree and I hope nobody reading thought I meant otherwise. I've been lucky enough to meet Steve in person a few times and have always been impressed by the qualities of him as a technical writer, as a developer, and as a person.
If you're not running them in parallel, you can just have your GPU return an `Option&lt;Interrupt&gt;`, and then feed that to the CPU's `tick` function.
I'm not sure, maybe because she never learned programming, she may just accept these Rust only concepts and find weird when learning another language later, like "why language x doesn't have that rust concept?". While he, as he has over 3 decades of programming skills, may fight more these new concepts that only Rust brings.
1. no, old reddit has [markdown](https://daringfireball.net/projects/markdown/) support, what it doesn't have is *commonmark* support, because it predates commonmark by‚Ä¶ nearly a decade? 2. and no, previously, fenced code blocks would just not render in old reddit, you'd get the triple backquotes and the text as regular text (with indents which would render as code blocks). Apparently this has been modified such that fenced code blocks become inline code snippets, which is not just no better but even worse as it's harder to copy/paste the code into a text editor in order to view the snippet correctly
I learnt it at back at v0.7 and made do just with Vim Rust-mode. We're spoiled these days, relatively speaking :)
How did it go?
\&gt; I'd just replace invalid values with 0 The problem I see with that approach is you've now injected policy into your summation function. Since your callers might or might not care about the errors, baking the policy in means someone will have to rewrite/mostly duplicate your function with another "almost identical" summation function every time a different policy is required. I admit the solutions provided have also baked in an "everything is valid or here's an error" policy, but what's nice about this policy is it can be readily refactored if a different policy was also required. (e.g. continue to Error if any invalid values detected, but the Error variant could contain a payload representing sum of all valid values). Anyway, just my 2¬¢.
1h08m47s: &gt; I honestly think Rust is going to be around forever, I really do. I think this is like, this is the formation of Ancient Greek.
There is no educational discount at this time.
Make tick() functions accept references explicitly? (Instead of storing them)
without seeing your actual code, it's hard to say what you're doing wrong. But here's a playground showing the various ways you can refer to things from other modules: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=3dc562552efbfd12f784391ada3718ef
Should.. should somebody tell him? He does know what happened to ancient Greek, right? 
In [day1.rs](https://day1.rs): use crate::utils; fn do_something() { utils::my_func(); }
Jesus
Ah, I get it. Well, it does mean that Rust doesn't have to bundle a C parser and preprocessor to let you use C libraries, and if you want to auto-gen that stuff the usual advice is to use the `bindgen` crate.
day1.rs: use crate::utils; pub fn day1 () { utils::hello() } utils.rs: pub fn hello() { println!("hello") } main.rs: mod day1; mod utils; fn main() { day1::day1() }
Coming from C++ and C# modules in rust are very confusing. 
Easiest solution might be for neither to own the memory that needs to be shared. The memory component can be passed into both tick functions.
It's not clear to me; do you mean single application/multiple crates? A single crate can only be compiled by one toolchain at a time; although using "features" you can toggle on/off certain portions of the code. A single application, linking to multiple crates, must also be compiled by one toolchain at a time, and can also use features to toggle on/off certain portions of the code. Based on this, with the understanding that you are creating a single application, then you need to pick a toolchain and stick with it. That being said, I'd argue that not having to juggle multiple toolchains is probably easier anyway! So my advice would be to pick stable if you can, nightly if you must, and compile everything with that choice^1 . ^1 *Don't forget that you can setup your CI to use multiple toolchains; so if you pick stable you may want to also enable beta in the CI to check your code against the next stable version.*
Absolutely brilliant talk.
Thank you very much for your answer. I knew for the Seek trait that a seek beyond the end of a stream was allowed (always RTFM ;) ) but didn't knew that it was the same behavior for Linux and Windows. It explains why this behavior was chosen for Rust.
I'll have to agree with you, but there's been a lot of changes to the module system the last releases and 1.32 wrapped that up, and now I feel there's not that much difference anymore. Huge improvements in my eyes.
Oh there was another meetup? I somehow missed this... 
To be honest, that sounds more realistic: be a heavy influence on the languages of the future.
Thanks for this great introduction for the workgroup. It is now very clear for goals and opportunities. I will be extremely excited to read more of your work 
&gt; all used templates must be instantiated in the object file I've heard about D's support, and the manual instantiation of templates does not exactly make me dreamy. I suppose that forgetting is at worst a link-time error so not too bad... but still :/ Still, I guess there's one potential possibility: reverse-FFI style. Instead of trying to interoperate with C++ code from Rust, go the other way around, and have rustc export C++ code wrapping the Rust types, and teaching C++ how to copy, move, etc... the types. It may be simpler, although there are gotchas: - C++ moves leave a destructible object, Rust makes no guarantee; so moves are best avoided (for one). - Rust doesn't have type-based aliasing (you can read/write to memory pretty freely), C++ does... so you'd need to compile with `-fno-strict-aliasing` just in case. - ... ? It does mean having to write a thin layer of C++ code, to manually pass the C++-ified Rust types to the actual C++ libraries, and back, but maybe that's not too bad. It would still limit the composition, though, as this is a one-way relationship: you would not be able to inject a C++ type into a `Vec`.
Sure, however if the "basically" is wrapped up in a pre-made library with idiomatic interfaces on either side, it certainly makes things *much* easier.
I'd love to use an OS written in Rust. Maybe by miracle, Linux gets rewritten in Rust, and my miracle, I mean miracle.
&gt; Is it time to rewrite X in Rust? yes
That's true. I think part of my confusion is `mod` vs `use` and why you have to put `mod` in lib.rs/main.rs in order for other files to `use` your module.
I'll have to agree with most comments here. For me, Rust required dedication to learn. I almost threw my mac out of the window getting an async-only db driver working with futures the first time. This was after I felt I understood all the basics for the 3. time. I would not say Rust is a good first language for learning casual programming. Assuming this project is for fun and not for work and you just want the computer to do stuff. However, I have never learned so much about CS, memory, terminology and how a computer works as I did when learning Rust, and never felt this kind of joy while learning (probably due to the feeling you get when you didn't throw the pc out and started to understand after some focused time learning how it all works). A great second language if you are interested to dig deeper, with a lot of well written documentation and books. 
Post your entire program.
My bugs are happening only in this block of code according to cargo check
`mod` declares or mounts a submodule as a child of the current module.
Yes, but we need more context to understand why they're happening. 
I learned it at school fairly recently, so it's still around.
`mod` defines where each module is located in the crate, so it defines the hierarchy that you access using `use`.
Switch to the 2018 edition: you then import macros selectively using `use nom::dbg;` instead of a blanket `#[macro_use]`.
What is std::borrow::Borow ? do you mean std::borrow::Borrow ?
Yes
It could be done on a per module basis using FFI. Would require a massive effort to finish though, and I'm not sure the Linux folks care much about rewriting in Rust.
Thanks. I‚Äôll get it anyway as soon as I can afford. Really love it
Good answer
I'm currently commuting, so I'd like to read the slides, but it wants me to create an account just to read the slides and it wants way too much information.
&gt; The variation in memory usage is probably due to some quirks in the dynamic loader. Notably, the dynamic loader may be setup for loading libraries at randomized addresses as a protection against hacking; AKA ASLR: Address Space Layout Randomization.
Go also has a much heavier run-time than Rust: support for GC and M:N threading come at a price.
Do remember, though, that the runtime of a language is a fixed-size cost: the 1 MB overhead of the Go runtime is the same for Hello World and for a TB-size program^1 . For server-size programs, 1 MB is relatively trivial, really. It does matter for small tools, or small devices, of course. ^1 *Of course, the GC is likely to have some amount of overhead proportional to the number of allocations made/reclaimed on top of the runtime overhead.*
Good work! I have similar crate for generating test cases: https://crates.io/crates/test-case-derive Your macro is reading files, in my case you must explicitly provide file name for each test. I think it is possible to merge those ideas just like in NUnit in C#, which has `TestCase` and `TestCaseSource`.
There you go: uwrtrhrl@10mail.org I've bullshitted the system so you don't have to.
so that fixes your problem?
Not all heroes wear capes
It is somewhat unfortunate that we can't have all the garuntees in a single language. (Preferably with ML syntax rather than C syntax. ;))
I respectfully disagree with print-debugging vs debugger. Just this afternoon, I was working on a test-case to track down an issue and put a break-point in a function... which ended up being called a good ~100 times by said test-case with the "failure" only happening after a good 50 calls. With print-debugging, this is not an issue: I just generate a huge trace, then step back through it until I find the one invocation among ~100 where things didn't go as planned! As such, I tend to mix print-debugging and debugger: - print-debugging to narrow down the issue, - debugger once I know which specific conditions cause the issue (so I can use a conditional break point). Now, if you have a trick to avoid pressing `continue` ~80 times when you have no idea what conditions cause the issue you are looking for... please do enlighten me! *Note: for some reason, `rr` and reverse-debugging never seem to work for me :(*
From his PS: &gt; P.S., it turns out that Steve is also leaving Mozilla - this is just a coincidence and there is no conspiracy or shared motive. We have different reasons for leaving, and neither of us knew the other was leaving until after we'd put in our notice. As far as I know, there is no bad blood between either of us and the Rust team. And I completely understand him, even when working with ~~teammates~~ **friends** I really enjoyed working with, running with, goofing with, at my past company, after 4.5 years and finally delivering the big project, I just needed a breath of fresh air and decided to move on. Honestly, the company could have doubled my pay and I would still have been looking to do something else; I was just getting bored :p
Who?
Is it? I find them less confusing than anything. I think only confusing part is that module can be in the same folder as parent.
 hashmap.insert(&amp;str.to_owned(), value); You create a `Hashmap&lt;&amp;String, _&gt;` here. Such a type isn't very useful. Use a fully owned `String` key and create a `Hashmap&lt;String, _&gt;`, by doing just `insert(str.to_owned(), ...)` (with no `&amp;`). Such a hashmap could be searched with both `&amp;str` and `&amp;String`. Also, check out [`entry`](https://static.rust-lang.org/doc/master/std/collections/struct.HashMap.html#method.entry) api.
But /u/Schnatsel does! 
I don't know her background, but I feel that one of the major requirements that a **first** programming language needs to fulfill is that it needs to produce tangible results quickly, so that the learner can move through high-level concepts with as much ease as possible. Learning programming patterns, data-structures, etc. while also being able to drop into developing an "app" with ease. I think that for even more experienced programmers, we still end up "fighting" Rust during the learning process, and it can be rather frustrating. I would recommend that, eventually, every programmer learn Rust if they are able to get around to it, but I would definitely not recommend it as a first language.
Sorry to see you go Nicholas. I appreciate all the mentoring you've done on rustfmt - it was a blast and I learned a lot. Also enjoyed meeting you in Berlin. Thank you for all efforts over the years and good luck in your future!
&gt; Most tasks shouldn‚Äôt require dangerous features such as `unsafe`. This includes FFI. I am genuinely curious what are your ideas to achieve this, seeing as a C function can do literally anything to the current memory. --- &gt; Many widely used libraries use unsafe code where it‚Äôs not strictly necessary. Typically this is done for performance reasons, i.e. there are currently no safe abstractions to achieve the goal safely and efficiently. I find this especially tricky. Sometimes the code is crafted so that it's *obviously* possible to elide bounds-check, for instance, yet the optimizer fails to do so. I think a huge step forward, in this domain, would be offering *guaranteed optimizations*. That is, either the code is optimized, or a compilation failure occurs. In general, this is a pipe-dream, quite literally: optimizations occur deep into the pipeline, after multiple transformations have already taken place (or not), and therefore preserving the attributes indicating the need to optimize would be difficult (requiring modifying all prior stages). For the particular case of bounds-check, and any *local* optimization available prior to inlining, it may very well be possible! At a high-level: - An annotation to indicate a desire to remove the bounds check: `#[opt(elide_bounds_check(vec, index))]` - A pass in `MIR` which either manages the optimization or explains which pre-condition is violated. In fact, this could more generally be organized through rewrite rules: #[rewrite_rule(elide_bounds_check, if(i &lt; self.len()), unsafe { vec.get_unchecked(i) })] fn get(&amp;self, i: usize) -&gt; &amp;T; And have the user specify `#[rewrite(elide_bounds_check(vec, index))]` on top of the expression containing `vec.get(index)`. Of course, the hidden subtlety here, is that one needs to teach pretty thorough analysis to MIR so it can realize that (1) at some point `i &lt; vec.len()` and (2) the length of `vec` didn't change since. The latter could potentially rely on a flow-sensitive analysis guaranteeing that `vec` was never modified; rough, but gets us going. The former, however, ... *fingers crossed*? 
So far it does exactly what I want!
Well the Linux kernel doesn‚Äôt exactly maintain a stable internal ABI, unless the ABI situation in rust changes I don‚Äôt see it happening because of the extreme dependency on loadable modules.
What about using `std::mpsc::{Sender,Receiver}` for sending data between the loops? 
AFAIK Polonius does not (directly) benefit Rust users, but Rust implementers. I believe the goal was to move away from the ad-hoc implementation of borrowing rules currently in use, and instead implement a logic engine (Prolog/Datalog), push rules and facts into the engine, and let the engine compute whether some relationships are satisfied, or not. The main advantages of the more formal approach are: - The engine itself can be validated in isolation. - The rules, expressed in a logic programming language (or close to), are easily amenable to audits/proofs. So, in the end, I believe that Polonius is first and foremost just a *refactoring*, and the primary goal is to *prove* that the borrowing rules are correct/safe. In the future, it may enable tweaking the rules more easily, however I don't think that's on the table yet. I, for one, would be quite satisfied with a mathematical proof of the current set of rules.
That's really cool, I need to play with this sometime! Bikeshed: If you have to explain what the `&lt;&lt;` is for it's a bad idea to use that syntax. :-P
I think it's because rust adds the additional concept of `mod` used as a keyword that C languages do not have.
Bryan Cantrill's presentation are always such a delight! In general I feel like both his and Joyent's contributions and to the community are vastly under appreciated outside their respective ecosystems (FreeBSD, NodeJS, PostgreSQL, SmartOS + ZFS, etc). Especially their foresight with regard to IaaS, PaaS, containerization (zones), and what everyone is calling "cloud native" these days.
You‚Äôre looking for r/PlayRust.
this is the wrong sub, is what it is
This quote was excellent for those who get the reference üòÇ &gt; The EPL people say, "Boy, did we save the day. We came up with this dialect of PL/I called EPL." Then Corbat√≥ is like, ‚ÄúPeople came up the dialect of PL/I, but we never used it, and it was completely insignificant." So it's like, "All right, I've got no idea what happened. It's like watching Rashomon for operating systems."
Linux will never get rewritten in Rust. It already takes 15 minutes to compile a distro kernel on a decent machine (128 CPUs, anybody?), and that's pure C. Why would you willingly wait hours for a recompilation?
If rust can survive a few centuries and keep being a model long time after it disappears, I would be quite happy.
Breakpoints aren't nearly convenient if you want to inspect multiple values over the flow of the program though...
I mean C is just bunch of files. Don't forget about inline modules. I mean everything is hard compared to C where it's just bunch .c and .h files. 
Oh boy, jQuery.rs, here I go.
Your crate is a tree of modules. The mod keyword doesn't import code, it just defines the layout of your module tree. So if you do this: main.rs: mod day1; mod utils; Then your crate tree looks like this: root / \ day1 utils If you want to use `utils` from `day1`, you need to specify the path to the utils module. `use` imports the names you want into your namespace. Either of the following will work. use super::utils; use crate::utils; Or you don't have to use `use` at all. You can put fully qualified paths directly where you want to call code. The following are equivalent. // Use a function directly use super::utils::some_function; some_function(); // Use a module, then call the function out of the module. use super::utils; utils::some_function(); // Skip use altogether, and call the function by its fully-qualified path. super::utils::some_function(); Code can only be in one module. It doesn't have to be in main, but you can't put it into your code tree in two places. If you want to put it in another module, the code's file will have to be in a subdirectory for that module.
&gt; For the particular case of bounds-check, and any local optimization available prior to inlining, it may very well be possible! Doesn't inlining make a lot of cases of bounds check elision possible in the first place? There was, however, talk of MIR-based inlining in conjunction with https://github.com/CraneStation/cranelift backend. But yeah, guaranteed optimizations are an interesting idea. It could be useful for auto-vectorization as well, although now that SIMD is stabilized a safe implementation with explicit SIMD and polyfills might be a better idea.
See here for an example you can play around with. Modules are defined inline instead of in external files, but the structure is the same. https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=0c79e409ba8c84824bc7f087824c2fd0
Please post at least the signature of `foo` and the type of your hash map. Also, the compiler's request for `Borrow&lt;String&gt;` for `&amp;str` is seriously weird - normally you'd borrow `&amp;str` from a `String`, not the other way around (in fact, I'm pretty sure it's impossible to have an `impl Borrow&lt;String&gt; for &amp;str`).
I think Cantrill has a great point here about how we're selling Rust. RIIR (Rewrite it in Rust) is a semi-official meme now ([source](https://llogiq.github.io/2017/09/13/resf.html)). But I think, by far, the most successful uses of Rust haven't been wholesale software implementations / rewrites in Rust. &gt;It's also a challenge with what is the advantage of rewriting things that are actually working code in Rust? &gt; &gt;... But this is working code that we want to replace. Why do we want to replace this working code, especially with respect to the kernel, which has multiply owned data structures everywhere? &gt; &gt;... I don't want to rewrite ZFS in Rust. That's what it boils down to. I wake up in a cold sweat when I think about rewriting ZFS in Rust. Instead, what are the biggest, most impactful use-cases of Rust so far? Firefox's WebRender and Stylo, GNOME's librsvg, Dropbox's brotli compression, possibly WASM now. They're replacing components of C++ software, not wholesale rewriting business-critical software, and Cantrill was advocating for this. Small things like kernel components and userland software, and then this: &gt;There is another operating system out there, an operating system that hasn't advanced technologically as much or at all, that's basically still running glorified DOS. But it's running all of humanity on top of it. And that is this absolute sewer of unobservable system critical, mission critical, software called firmware. I can't come up with a better slogan, but I think the point is that we shouldn't pull a [Netscape](https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/), and try to rewrite everything big and C++. Maybe instead go after the gears and sprockets that are rusting already.
This is the library I want to see happen
Better add namespaces to cargo then or we're gonna run out of names.
Sorry my bad
With Rust you wouldn't need to recompile the whole thing as often.
&gt; Doesn't inlining make a lot of cases of bounds check elision possible in the first place? It does, however it's not always necessary. For example: fn shuffle&lt;T: Copy&gt;(slice: &amp;mut [T], indices: &amp;[usize]) { assert!(slice.len() == indices.len()); assert!(*indices.iter().max().unwrap() &lt; slice.len()); for i in 0..indices.len() { let tmp = slice[indices[i]]; slice[i] = tmp; } } Here, I've proven that all indices are valid, so bounds-checking is not necessary! Hopefully, `indices[i]` is not bounds-checked (given the loop condition), though it'd be nice to be sure. Honestly, though, the LLVM IR is pretty complicated... nothing like the straightforward assembly [I get from C](https://godbolt.org/z/KOOH67). Also, rewriting can be applied incrementally: if you have a 3-levels deep function which could do with some optimization, you can always provide one rewriting rule per level. It composes!
The tone of the article is very encouraging. 
‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è
Thanks, this is the idea I like the most! This is probably the way I'll go for the GPU. It's not quite enough for the CPU - while it can return data it wants to write, it also has to _read_ data from the GPU, so will need a reference to it (or some part of it).
Looks very cool, happy to see fellow Stuttgarters here :D.
Thanks, for some reason this didn't occur to me! It definitely seems like the most "correct" way. The verbosity is bothering me though - the `CPU` code is pretty big - so I'd have to pass the reference through a _lot_ of methods.
Well on't laugh but JQuery rewritten in rust (guided by the existing tests), then made available in Chrome, Firefox , Safari runtimes as a binary, with JavaScript JQuery invocations routed thru to the rustc compiled functionally identical version would be astonishing.
I remembered a post about Flutter-like rendering in rust by Raph Levien, but can't seem to find it.
&gt; 15 minutes to compile a distro kernel on a decent machine (128 CPUs, anybody?) Woah, are distro kernels really *that* fat? Or is that not just the kernel‚Ä¶ I compiled Android kernels on a 4-core (non-HT) 2.7ghz Skylake in about 4 minutes IIRC. Another data point: FreeBSD clean kernel build is 3 minutes on an 8-core 3.9ghz Ryzen.
This is a wonderful talk, and it's very pragmatic, and probably correct. But I feel like it misses out on one aspect of the conversation. Excitement. People getting involved in Rust are excited to get to explore areas they've never worked in before. &amp;#x200B; I totally get the issues being brought up with binary compatibility, vfork and signal handling as compatibility issues, but I feel like this misses a little bit of why there are so many operating system experiments going on in the language. From my perspective there is a huge amount of excitement and it's a great time to try and capture that excitement and focus it on such a grand task like building a new OS. Personally I never became good enough with C that I ever felt like I was writing safe C. I always felt like the more I learned, the less confident I became in my code. With Rust, I can contribute to a kernel (if I had the time) and I won't feel like I have to be constantly looking over my shoulder. &amp;#x200B; Anyway, definitely worth watching, and a wonderful presentation.
Thanks for your effort. It is really interesting to see the path of people like you! Is it possible for you to elaborate more on the reasons why you moved on? As in do you have some new idea that can only be realized in a different context? Sorry for being general but I do not know your situation.
Yeah that can be true too, and i've seen stuff like async and multithreading mentioned too, and i don't do much of that. Though i don't see any reason a debugger *couldn't* inspect multiple values as easily as `println`. Maybe work to be done on the debugger front?
Rust isn't easy, and it also isn't difficult. Rather: it's the easiest way to learn system programming, which is itself a difficult topic. Think of it as a tour guide: it doesn't make the subject topic any easier, but it does make it more accessible.
It would tie you to the wasm/js stuff if you did it, so it might be better to have a core crate, and add crates to inject the storage backend. One could be the filesystem backend, but the other could do LocalStorage. &amp;#x200B; It basically gives you a big key/val interface similar to yours in pickledb, but you would just store your whole serialized db under one key (to avoid the overhead of the repeated calls) &amp;#x200B; The functions for getting and setting are in the Storage obejct: [https://rustwasm.github.io/wasm-bindgen/api/web\_sys/struct.Storage.html](https://rustwasm.github.io/wasm-bindgen/api/web_sys/struct.Storage.html) &amp;#x200B; Which you obtain via Window::local\_storage: [https://rustwasm.github.io/wasm-bindgen/api/web\_sys/struct.Window.html#method.local\_storage](https://rustwasm.github.io/wasm-bindgen/api/web_sys/struct.Window.html#method.local_storage) &amp;#x200B; An example of its use is here: [https://github.com/rustwasm/wasm-bindgen/blob/master/examples/todomvc/src/store.rs](https://github.com/rustwasm/wasm-bindgen/blob/master/examples/todomvc/src/store.rs) &amp;#x200B;
It's a little hard to find because it is under the xi-editor organization rather than his own github. https://github.com/xi-editor/druid
Just "coincidence"? Hmm, it may not have an explicit shared motive, BUT, who knows what's implicitly going on....
May I suggest to have a link to the playground in these? It isn't hard to find, but it would be nice to mention it to those who haven't heard about it yet, I think.
Oooh, saving this later to watch/read.
The great thing is that you don't need to handle it! You can just output that syn::Type as-is into your quote! macro anywhere you need to write the type of the field. Make sure you use a syntax that covers all of the valid type syntaxes is all, like `&lt;#fieldtype as thecrate::Trait&gt;::read_value()`
Android kernels tend to be not the same as a typical x86-64 distro kernel because of the amount of crap they (the x86-64 distro kernels) compile in -- ancient filesystems, network protocols that nobody uses, drivers for hardware that was never even mass produced, etc. It's the same with FreeBSD, it just doesn't have the same volume of drivers/code that a Linux "catch-all" build has. I can build a small Linux config on my laptop in ~20 seconds, but even that would probably be several minutes if it was the same amount of Rust code.
I'm learning Rust right now and also writing an OS, so I appreciate articles like this a lot. Unfortunately Rust doesn't target sh4 so I need to stick to C and can therefore only use Rust for utilities on my development machine :-/
This is very much an active project. I need to write some updates, it's hard for an observer to figure out where it stands. There are lots of modules, which can be confusing, but also represent a very modular approach to the design. Specifically, druid targets what in Flutter is called RenderObject. You can write UI in that (and this is what I plan to do for my synthesizer), but Flutter has this whole reactive-style Widget layer on top of that. I'd love to see people experiment with similar layers on top of druid, and feel like the "Paw" project could be at that level of the stack.
For those not familiar with Bryan Cantrill, he is known for his humerous talks. [Most famously for the most watched Usenix presentaton on youtube where they put a disclaimer above his head the entire talk](https://www.youtube.com/watch?v=-zRN7XLCRhc) [He also created a blog post with a list of his talks](http://dtrace.org/blogs/bmc/2018/02/03/talks/) I highly recommend his lightning talks from Surge, they're short and highly amusing. 
The `stuct a/impl a/struct b/impl b` vs `struct a/struct b/impl a/impl b` is something I've thought a little bit about before. I think most of my code is written as the former, although logically I think I'd prefer the latter. Given that inconsistency, my takeaway from the blog post is I should be considering moving things around within a file as part of the refactoring step. Thanks for sharing.
You mean \*Philipp Oppermann's blog OS\*, as it is now called?
There is also [this blog post](https://deterministic.space/machine-readable-inline-markdown-code-cocumentation.html), suggesting some conventions.
I strongly agree with this. Using operator overloading in a way that fundamentally changes the meaning of the operator is confusing. You could instead `impl add` and use the `+` operator, which would be a little better, but I think something like `p.register(...)` would be even nicer, and help reflect that the return value will be the widget ID. 
Why 11 year old? Why porn addict? Why 1971?
Indeed, but even knowing about it, it's pretty confusing as to *why* this is neccessary when pretty much every other language with modules declares/mounts modules implicitly when you use/import them.
TIL that Android kernels are actually reasonably sized
Doing God's work, son! 
Oh, what a fun topic! :D For me it's generally, this order: - free standing functions in order of generic helpers to more specific, or as this post calls it `fn main`-last - `struct a` - `impl a` - `struct b` - `impl b` The rough reasoning behind is, that it's supposed to be optimized for refactoring. So structs and their impls are easy to move around together. Generic helpers are also at the top, since that is where you would also import helpers from, so when they become generic enough, they slowly flow out of the module to the top and into their own modules (yes, we're getting a bit esoteric now). One thing I haven't found a satisfying solution for is how to order `impl a` vs. `impl trait X for a`, especially if the trait implementation is one you would normally derive. I generally want `impl a` to be the first thing after `struct a`, but it feels strange that e.g. for the few special cases where I manually implement `Display` or `Eq`, the implementations move from being very close to the struct (in the derive), to being potentially very far away. Involving generics is a whole other topic entirely...
I do the same thing. I like to put my implementations next to my structs when I'm building them, but after the data is pretty much set it's pretty annoying to scroll through a bunch of logic to look at the fields on another type down below. Not sure which way I'll end up with.
Busy, busy with tax season already... I just copied the project into playground, and starting commenting things out until I could get it to compile, because I was finally hoping to give example code. Then I double-check my comment and look for the Args struct... gone. But I do see a Commands enum and the match statement after the naming_function. Yay, you took my advice! :D Happy coding! (now to check out the recent commit history...)
&gt; Maybe instead go after the gears and sprockets that are **rust**ing already. Pun intended?
I think you'd want some kind of virtual grid implementation, like virtual DOM (i.e. only update characters that were changed.)
It got incorporated into every language that followed it?
I'd say you can't go wrong with Python. Rust's borrow checker can be very frustration at times, and that might be the last thing you want a learner to feel.
Thanks for this well-written and informative post. As a related note: it would be amazing if each WG wrote a similar post, outlining their vision for the next year. 
Something I have considered is implementing some form of \`if!\` macro which calls \`.into()\` on its arguments.
The better analogy is a C thing that acts like an \`Option\`. It is a common thing to ask 'is this empty?' and the extra verbosity is not buying much.
I pondered suggesting it for about 30 seconds. Totally agree. UGH!
No joke it would be awesome to see. It involves some really cool cross language interaction and would be interesting to see it on a scale similar to jQuery.
Very interesting! Also worth noting is that the 1.20 to 1.23 compilers compiled the same code to about half the binary size.
Love this post! I kind of wish this topic was one that was more covered in the community honestly. As someone newer to the Rust community I've spent a pretty good amount of time wondering if my code structure was correct or "idiomatic" but didn't find many great resources. &amp;#x200B; Personally I find myself doing the \`struct/a\` \`impl/a\` \`struct/b\` \`impl/b\` format. Mostly because my only other systems programming experience comes from C++ and that formatting reminds me of the separation around .h files vs .cpp files. I think I struggle most with where to put free standing functions. Currently the usually end up in some kind of utilities module as putting them in the main module file seems to clutter them up a little. It's definitely not something I've settled on yet though.
Thank you, I wasn't using the quote macro, because I thought it's mostly text substitution. But this helps a lot then.
If we could go back in time and change one decision in jquery, I wish it would error by default when you try to call a function on an empty set. Largest class of errors that I‚Äôm aware of. In the cases where you don‚Äôt care about empty sets (hide all/show all for example) add a quiet flag of some sort. But much of the time I was expecting an element and if it‚Äôs not found something very bad is happening. 
Holocaust is not funny. Nothing about it. 
If you're interested in low level Linux programmed: https://github.com/jbaublitz/neli It's a Rust API for a interacting with the Linux Netlink API, which allows for doing things like managing Linux network interfaces, firewalls, or auditing with pure Rust. The author (I am just a user) seems to be open to contributions.
That sucks to hear. I‚Äôve always wanted to work for Mozilla in its prime. 
Yeah, Python was going to be my fallback. It's just that I have experience programming in Python, and I wanted to close the experience gap a little.
You rock! Commuting home here atm. 
Ah, I see you're also trying to do Advent of Code. I struggled with this myself as well. 
The linked article is actually a sort of appendix to the main article, [An alias-based formulation of the borrow checker](http://smallcultfollowing.com/babysteps/blog/2018/04/27/an-alias-based-formulation-of-the-borrow-checker/). It says, head first, &gt; End-users don‚Äôt have to care. This proposal makes no difference from the point of view of an end-user of Rust.
Long story short, Rust will not be the right first language for her. I'd personally recommended Python or even better JavaScript because she'll be able to see the results more immediately and visualize the concepts more easily.
I personally like postfix `@`, which looks like line noise, but has a nice mnemonic: _@ for await_
If the structs have large impl blocks, I'd suggest moving them into separate files. That way this becomes a non-issue. If I want to read all of the stuff that relates to B, then I read b.rs. If I want to see all of the struct data definitions then I can read the top section of each file (which I can easily switch between with a single click).
Perhaps Lua, then. It's the only one of those three where neither of us would have any prior experience. I've heard good things about it, actually.
Good to know. She's smart. She actually has a good deal more formal education than I do, but it's in psychology, not CS. We've played around a little with Haskell together (which I told her at the outset was a *crazy* experiment) but I saw enough to be able to tell that she *could* learn to program if she really wanted to. She actually picked up on things pretty quickly that took me a while to catch on to. (e.g.: "Wait, isn't an operator just a function?") It's just that the fact that I have such a head start was intimidating for her.
Will the awaitables also be thenables?
POBOS. It's already got an acronym, you know it's gonna be great. The reason Plan 9 never went anywhere? No acronym.
grr looks awesome! *cries on macbook pro that only supports OpenGL 4.1*
Lua is real fun. Their tables are by far my favorite data container in any language.
Yeah JS unit tests in part being passed by a rust impl relies on a smooth foreign function interface and a programmatic call analysis of ‚Äúthe most depended on and least depending‚Äù function. If it were Kent Beck doing the migration work, that video would be a guide to programming at the top level.
Thanks so much. Now a bit of a deeper question: how would I use serde to work with a HashMap? I'll be inputting json as the content type and I want to store two strings into a HashMap which can then return json.
Thanks for the feedback. I wrangled with it a bit, and then realized that it wasn't practical (all my rust projects will have a cargo file). I did update it though to run a file in the examples directory using \`--example\` for convenience.
From what little I know of Haskell, I thought one of its biggest footguns was that it's easy to create a linear-size thunk out of relatively simple loops/folds without realizing it, and then running out of memory as a result. That's not a Memory Safety Issue in the sense of leading to undefined behavior, but I got the impression it would lead to subtle DOS vulnerabilities. Someone who knows Haskell better will have to tell me whether that really happens in practice. Rust has an issue that feels similar to me, where recursive types using Box can blow the stack in their default destructors, and you need to implement a custom iterative destructor to avoid the problem. This comes up in every tutorial about building a basic singly-linked list, but I'm not sure it ever comes up otherwise.
Great guess! Unfortunately I don't think it's right. $ cc -O2 hello.c -Wl,-no_pie $ time -l ./a.out 548864 maximum resident set size 143 page reclaims $ time -l ./a.out 557056 maximum resident set size 145 page reclaims Looks tentatively like memory usage is related to page reclaim count ‚Äî which makes some sense, I guess. My new theory is that there is unpredictable cache behaviour when mapping `libSystem` because such a small part of the library is actually used. But I'm going to step back and declare this an unsolved mystery. Working it out any further would almost certainly require a deep dive into Darwin [libc](https://opensource.apple.com/source/Libc/Libc-1272.200.26/) *and* [dyld](https://opensource.apple.com/source/dyld/dyld-635.2/) *and* [XNU](https://opensource.apple.com/source/xnu/xnu-4903.221.2/) and probably more debugging tools than I know how to use.
&gt; In the future, it may enable tweaking the rules more easily, however I don't think that's on the table yet. This... sort of, kind of, works today. [Polonius README](https://github.com/rust-lang-nursery/polonius) explains how. Everything there is correct, but it may be confusing. There is an internal toggle in rustc called "borrowck mode", which is one of `ast`, `mir`, `migrate`, `compare`. It defaults to `ast`. If edition is 2018, it defaults to `migrate`. If you enable `nll` feature, it is set to `mir`. `compare` is for debugging. On nightly, you can directly set this toggle with `-Z borrowck`. `-Z nll-facts` is ignored if borrowck mode is `ast`. Maybe there should be a warning, but there isn't now, `nll-facts` is very experimental. So if your crate is on edition 2015 and does not enable nll feature -- very common, it's the default after all -- `nll-facts` fails silently. TLDR: To use Polonius, use nightly. If `-Z nll-facts` doesn't seem working, add `-Z borrowck=mir`.
Use `enum` to solve it? Because the format of the query is fixed. Moreover, the main purpose is to transfer the value of the query to the web (using iron), I have not found a similar way of using...
I think we just have to `await future` to see how this issue resolves.
Well, Criterion.rs will probably never have a small dependency tree, for a lot of reasons. There's some stuff that I can replace pretty easily like `handlebars` or `itertools-num` (used for one call to `linspace`). Other things, like `rayon` or `serde-json` are not as easily replaced.
I like implicit, I think.
I'm hoping for postfix `await` or `@`. Using Rust, I've developed a pretty strong preference for simple left-to-right reading order on expressions. I also hope we don't end up with deceptive `.await`/`.await()`/`.await!()` syntax. It'd be yet another thing we have to endlessly explain to newcomers ("Yes, I know it looks like a function/field/macro, but it isn't. Yes, you're the millionth person to be confused by this. Yes, that's just *this week*.")
Thanks for everything and good luck!
I definitely agree on left-to-right reading order being nice. I don't think I'd like to have to match the opening and closing parenthesis on `await!(expr)` to understand what it means. To be honest, I think I'd actually prefer a magical and deceptive `.await()` just to avoid nests of delimiters.
Many people have been payed to work on Rust at Mozilla for many years now, far beyond the norm in software development. While I think this is a good thing, sometimes it's also healthy (both for the employee and the project) to have a change too, and give others the opportunity to help out.
&gt; await! is not something you can or will ever be able to write in Rust's surface language Why not? Wasn't it the case that the futures crate had an await! macro or something similar that was just plain rust? What's going to be special here, and why?
https://doc.rust-lang.org/rust-by-example/mod/split.html
&gt; To be honest, I think I'd actually prefer a magical and deceptive .await() just to avoid nests of delimiters. But you don't *need* to use method call syntax to avoid delimiters. `await` by itself would work; just think of it as like `?`, except using letters.
I'm assuming it's for consistency with the inline syntax that I like to use to constrain access to private members without creating a new file: mod foo { // Some stuff that shouldn't have its privave members // accessible by the rest of the file }
If you want your function to return data of more than one type, use `enum`. Based on your question, it looks like you intend that code to three different types of data, plus a `None` or error variant.
Have you considered that it might be easier to teach her something you are familiar with? I've been trying to teach my friend django (even though I've never used it before), and it's involved a lot of me suggesting that somethings works in x, y, or z way, only to discover that it doesn't, and having to backtrack. I reckon this must be a lot more confusing than if I could explain things and answer questions correctly the first time...
Great post! One thing I am annoyed with in current rustfmt is that it sorts mod declarations by default, and arranging *them* in ‚Äúsuggested reading order‚Äù is super helpful.
How many millions of lines of code are in the Linux kernel? I‚Äôm sure devs are itching to rewrite those
Another possibility is making a `RunningCpu` struct which just stores a mutable reference to the Cpu and a mutable reference to the memory. It would be created and destroyed each tick, and the purpose would only be to group `Cpu` methods which need memory also so they can use both from `self`. For extra niceness, you can implement `Deref` for `RunningCpu` with `Target = Cpu` to directly access other `Cpu` methods from RunningCpu. I've done this in the past where I want to temporarily (but frequently) group different data together which is operated on together but needs to be stored separately.
Why is this here?
One thing coming up in it is `.await!()`. I vaguely recall discussion of making postfix macro invocations a thing, that `x.foo!([‚Ä¶])` would be turned into `foo!(x[, ‚Ä¶])`, matching a macro pattern `($x:expr[, ‚Ä¶])`. Did that ever go anywhere?
It's not intended to be visible to end users that await! desugars to a generator yield, just as it isn't visible to users that `try` blocks create a breakable block scope and using ? inside them desugars to a labeled break. It's not part of the surface language-- it's just an implementation detail. In the future, we hope to have async generators which can be both awaited from and yielded from, which would not be possible with a user-visible desugaring of `await`.
C++ has the same issue if you naively build your linked list with `unique_ptr`.
Learn to Reddit. Stop posting here.
&gt; await! is not something you can or will ever be able to write in Rust's surface language &gt; &gt; I sure hope it won't be @. It's awkward to type on fin/swe keyboards since I have to hit alt-gr with my right thumb and then hit number 2 key at the same time.
I have not seen the video yet, but don‚Äôt we have an initiative for that already. Redox OS https://redox-os.org/ 
&gt; Non-lexical lifetimes that have landed in the 2018 edition of Rust made the borrow checker smarter, reducing the need for resorting to unsafe code. Were people actually using unsafe code to work around the lack of NLL? My impression was that in most cases it was rather easy to restructure the code to make the borrow checker happy.
that sounds terrifying, i don't see what benefit that would give? Just write `x` a bit to the right yourself? 
How would you delimit it? With a space? I think thats super confusing because keywords are almost always prefix. And the word should really be "awaited" in that case.
Yikes, that would be weird: `let f = foo() await?` I could probably learn to parse it eventually though.
@ _for await_ @ _for await_ @ _for await_ @ _for await_ @ _for await_ @ _for await_ 
By that token, why are methods a thing at all? Why do you need `foo.bar(‚Ä¶).baz(‚Ä¶)`? Why not just do `baz(bar(foo, ‚Ä¶), ‚Ä¶)`? Allowing the equivalent of `self` in macros would be consistent with methods, and would make for *much* more pleasant writing in cases where you get the common chaining style. There was a reason why the prefix `try!()` got replaced with a suffix `?`: suffixes are generally easier to read, because you‚Äôre just reading from left to right. If suffix macros were a thing, `try!(x)` could have become `x.try!()`, and I think there would have been greater resistance to essentially shortening it to `?`.
...and was updated - thank you! :-)
The problem is the lack of documentation. They should give examples with typical scenarios you would encounter if you wanted to write your first small program. The ridiculous example about the 3 nested mods at the start of chapter 7.2 of the book is terrible, no one will ever use this structure in one file when learning to write rust (if ever?). Start with something practical! Currently it's like trying to teach F=ma by showing how solving the Schrodinger equation leads to Newton's laws of motion. No wonder newbies are confused! Start with how to perfectly match a simple realistic directory structure, then introduce that modules don't necessarily need to match up with files and directories.
Unusual, but I don't see how it's any more confusing than `?` being postfix. `await` would just be another token that happens to be written with letters instead of symbols. Imagine it as `@` instead, if that makes you feel better. The only reason I slightly prefer the keyword is that I don't want Rust to get *too* sigil-y, given that we *already* see many people cowering in terror from the light dusting of symbols the language already has. (It's almost like some people have never written Perl or Brainfuck before and don't know what "too many symbols" actually looks like...) &gt; And the word should really be "awaited" in that case. I don't see why. You're instructing the compiler to await completion at that point in the chain. I basically view chains as a sequence of instructive sentences where the subject is carried on from the previous sentence. As in "Call `open`, then `connect`, then `await`." 
Isn‚Äôt there an RFC about postfix proc macros somewhere ? A benefit is that it allows to design postfix operators, e.g foo.try!() instead of having to write try!(foo).
I've added a [suggestion](https://github.com/rust-lang/rust/issues/57640#issuecomment-455763530) to use `fut@await` :&gt;
When will rust allow following code : `collection.set(key, collection.get(key) + foo)` ? Right now it raises an error with mutable and immutable on one line...
A big problem with a mature code base is inline documentation that will commonly go on for dozens of lines. If your text editor isn‚Äôt able to collapse it along with the rest, it‚Äôs a real drag. I would like someone to make something to provide information on folding on source code, probably based on LSP (or at least its principles), so that we can finally have halfway decent folding in Vim, where I never implemented anything beyond trivial folding on braces, which is really fairly terrible. Naturally this tool needs to be able to cope with incomplete source files. Ooh, while I think of it, I also want something to do true rustfmt-based automatic indentation.
For whatever reason[1], I'm in a seemingly constant state of needing postfix macros, and having them would solve *two* major problems that I've been having. I know that I could simply use regular macros, but they stick out in a *lot* in an otherwise macro free / free function free interface. [1] that reason is that I'm doing research on bindings systems, and I need a lot of complicated HRTBs, and I often find things that I need to express where the type of the thing I need can't currently be written. Either having postfix macros to paper over it or having GATs to really fix the problem would work.
Okay, let's rewrite it as following: Rust is a good choice when you don't care about service performance. Ownership, non-nullability and ADT make it as convinient as Java/C# are. So, you can write web-service in rust without problems, as you could do with MVC/Spring. But you can't write a C++ web-service without problems, and you probably won't if you don't have performance requirements (e.g. 1RPS).
&gt; Isn‚Äôt there an RFC about postfix proc macros somewhere ? There's one about postfix declarative macros [here](https://github.com/rust-lang/rfcs/pull/2442). It does not allow all macros to be called in postfix style like /u/chris-morgan proposes, instead it allows the macro itself to define its postfix syntax if it wants to. I like this idea because it mirrors how functions decide whether their first argument is `self` or not.
Not until Rust has a spec.
I'd say that's the problem of the editor, and not of the code :-) With all my respect to Vim and Emacs (both of which I use daily), using regular expressions to power code editor for a language with sane (no preprocessor) syntax is just wrong. It is exactly [parsing HTML with regexes](https://stackoverflow.com/a/1732454/1936422) problem. LSP already supports code folding. Rust analyzer already implements it, including [folding of comments](https://github.com/rust-analyzer/rust-analyzer/blob/275811667ce83bee76f916ec3b36026bd8d39565/crates/ra_ide_api_light/src/folding_ranges.rs#L125-L167).
Checked &amp; merged. Thank you.
Oh, that reminds me: I've been [meaning to](https://twitter.com/killercup/status/1085666787491041283) open an issue about parsing/highlighting markdown in comments and folding sections based on their headlines.
Interesting -- I hadn't thought about `mod` items at all. Tbh, they are pretty much noise to me, and I only read the (optional) `pub use` lines that follow them :) (damn, I should add something about `use` lines, too! Where to put them? At the top? Right where they are first used? Inline if possible?)
Try /r/playrust
Glad you liked it! I usually put free functions at the end of my files initially and then move them into modules (ideally named by domain but I've used "helpers" too) once I need them in &gt;1 file.
Thank you. I'm half way thru the book and dozens of blog posts. * Threads: Cool I was hoping for that :) * Arguments: What about [clap](https://github.com/brson/stdx#clap)? Seems more robust and the syntax is much more cleaner. * log: Cool. I will go for that.
Pro tip: put `mod`s [*before*](https://github.com/rust-analyzer/rust-analyzer/blob/7e84440e25e19529e4ff8a66e521d1b06349c6ec/crates/ra_ide_api/src/lib.rs#L12-L58) all of the uses. This * helps with navigation to child modules: you need to go to the top of the file and press goto defenition, you don't need to use tree view or fiddle with open buffers * solve "pub use after mods or after uses" problem by completely avoiding it * helps readability: you immediately see the module tree structure 
Glad you enjoyed the post! Moving things around is such a powerful tool to help with understanding. Sadly, it's almost annoying to read the diffs‚Ä¶
Is `collection` a `HashMap`? If it is, you can do `collection.entry(key).and_modify(|x| *x += foo)`. Or some other entry method, depending on what you want to do if there is no such key in the `collection`.
&gt; e.g foo.try!() instead of having to write try!(foo). Who does that benefit? We have `?` to replace `try!`, and for good reason. And even without that specific example, how is that a benefit? It's no less typing, in fact it's an extra character. You type the exact same characters, just in a different order, in a way that complicates macros and confuses them for functions?
&gt; Is there a way to do something like cargo fetch-dependencies and avoid having to try an initial build? You can use cargo vendor
Especially when it comes to OS development, I don't think there's any valid reason whatsoever to base any new system on anything but sel4. No, sel4 is not written in Rust, but the good ole mixture of C and assembly... Which has been formally proven not just to such pedestrian standards as memory safety, but functional correctness against the spec, which again has proofs as to behavioural correctness such as never causing dead-locks, not leaking a bit of information and much more. At some point, I think it's going to be implemented by directly extracting assembly from Coq, and they'll have proofs for correctness that encompass the instruction set itself, in the case of RISC-V it's realistic to prove the whole CPU+kernel combo correct, from syscall over verilog to the gates. Rust is just outclassed here, implementing *that* level of assurance is not what it's designed for. Performance is just fine, issues remaining are sel4 not being properly feature-complete (multicore support!) though such things are progressing quickly, and then, as always: Drivers. Less problematic in the case of sel4 though both because it's a microkernel, and pilfering other OSes for drivers is very much a commonly done thing. Systems programs, such like a new daemontools, distro programs, such as a package manager, configuration system, but also low-level things like an USB and network stack, that's stuff that would make sense to implement in rust. Heck, filesystems, and you don't even *have* to use `no_std` as it's all userspace.
Postfix await fails the "principle of least surprise" though - it's unlike anything else in other languages or Rust itself. 
&gt; Mostly remote, with a few stints in the Auckland office. As in New Zealand?
No, it is a custom graph, the code actually looks like this: `graph.link(node1, graph.newNode(...))`
Thanks for the help. the 'use crate' part was something new for me which was either added in the new addition of the book or i've missed it somehow
If you want to support older hardware you really need some sort of OpenGL support.
I think i get it now. Thank you for the help!
Re: #2, if you're using [RES](https://redditenhancementsuite.com/), you can hit the source toggle and copy the code from there to retain whitespace.
While this is true, as I understand it, one of gfx-hal's goal is to provide an unified API for every graphical back-end, effectively including OpenGL. Admittedly this goal isn't reached yet, my point here is that using a wrapper around OpenGL harms the forward-compatibilty of the project, while using gfx doesnt harm the backward-compatibility. So it seems like the logical option to me.
Rust all the things!
If `graph.newNode` returns something that borrows the graph, I don't think it's possible - at least, without some major `unsafe` trickery. If, however, it returns something that identifies the node without borrowing the graph, like an index into internal storage (`petgraph` uses this approach, see [here](https://docs.rs/petgraph/0.4.13/petgraph/graph/struct.Graph.html#method.add_node)), then you should be able to just move the `graph.newNode` call outside of the `graph.link` call.
You might also see: use ::utils; Or use super::utils;
Ordering funds from high to low level logic assumes that programmers review code linearly and comprehensively. They don't. Either they conduct a keyword search (find X) or manually search. Constructors go first. Everything that follows might as well be ordered alphabetically, to accommodate a manual search.
Anecdotally, it's often said that people learn Haskell a lot more easily when they've never done any other programming before.
Don't touch actix until you have a grasp of Rust fundamentals. You dove into the deep end of the wave pool without first learning to swim.
I've often heard that people learn Haskell a lot more easily when they've never done any other programming before. I don't think the same is true of Rust! But understanding Haskell type classes certainly helps with understanding Rust traits.
In the issue, withoutboats mentions that any operator which does not include the literal term ‚Äòawait‚Äò is highly unlikely. Anything that looks like a field access/method call is also unlikely. This leaves us with ‚Äòawait foo‚Äò and ‚Äòfoo await‚Äò. The first one is in line with nearly all other operators that change control flow (return, break, yield), but the second one allows for better chaining.
I think either `inflate` or `png` crate was doing that There's also `smallvec`using unsafe for (among other things) working around the lack of const generics
Programmers who don't read/review code in a linear fashion (per file, that is!) would not care about the order and my approach would not harm them. All the other programmers (who I know exist) would hopefully benefit from it, however.
I think the writer is rather looking for `cargo fetch`.
Yes, it is an index, so I have the index stored in temporary variable. That also means, that with smarter borrow checker, it should be possible to write that code with no extra temporary variables. Is there no RFC for such functionality?
That's true about not effecting the non-linear searchers. I looked through my code and realize that in many places I've ordered public before private methods rather than by alphabetical. So, I'm going to update my original comment with *retracted*. Mind changed.
What‚Äôs the benefit of private mod statements? Couldn‚Äôt you just use the module name?
That's a really neat idea! Thanks, I'll use this.
Ah yes, that‚Äôs what I had in mind. The particular self semantics I had forgotten.
To be fair, the C version should be compiled with \`-static\` as well to statically link all libraries. Furthermore, as far as I remember, there is a why to dynamically link everything in Rust with \`rustc -C prefer-dynamic\`.
You need mod to define the hierarchy. You can't use something that has not been "attached" anywhere in the current crate, so there's nothing to use. Of course, if you want to redesign the language, that's a different question.
[so close](http://www.tavi.co.uk/phobos/overview.html)
A good reason for `?` vs `try!` is that the suffixes look nicer. The postfix macro thing would make it nicer to chain them sometimes.
The \`!\` already clearly disambiguates a macro and a function. The point would be allow reading expressions in a left-to-right style instead of requiring nesting that needs more careful reading or indentation to interpret visually. For \`try!()\` we already have a postfix operator, but it's a useful comparison to show how that style of macro might have been useful in the past, and therefor might be useful again in the future (no pun intended).
I'm going to play a bit of devil's advocate here. While it's true that Python and other dynamic languages and even many statically typed languages are easier to learn than rust, they are so either because they allow you to write more error-prone code or because they present a simpler programming model at the cost of a more complex runtime. So, although it may be harder to learn initially, it might pay off in the future should she wish to learn other languages.
I equally feel that anything prefix is highly unlikely and that we should pick some postfix syntax that folks can live with. `await foo` being in line with `return`, `break`, and `yield` is misleading due to the suboptimal precedence `await f?` =&gt; `(await f)?` and because `return f: !` whereas `await f: T` where `f: impl Future&lt;Output = T&gt;`. The other operator that behaves in this way is `e?` and it is postfix.
hey, thats a nice idea, maybe we could give those struct-only files a special file ending, like maybe .hrs
Thank you, it's really useful for beginners like me. Could you recommend books about Rust essentials which are relevant for latest *Rust 2018* release? May be there is good choice for those who already has some background in other programming languages?
I'd rather not use @ for two reasons: - It's awkward to type on a number of keyboard layouts. - It's unsearchable; just try googling it... In general, I really favor full-blown keywords over sigils for those two reasons; being extra terse is not that useful, and does not compensate the weaknesses I mentioned above.
My personal recommendation is to read "The book" first 11-12 chapters https://doc.rust-lang.org/book/ as first book whether you are beginner or experienced developer, this book covers rust 2018! Programming Rust By Jim Blandy is also good book.
I tend to prefer types first and `impl` later as well, however this comes *after* the public/private divide: * Public: * Types. * Inherent Methods. * Trait Implementations. * Private: * Types. * Inherent Methods. * Trait Implementations. * Test: * `#[test]`. * Helper Types. * Helper Methods. * Helper Trait Implementations. One of the reason of grouping types first is that types are generally succinct, compared to methods/functions, and therefore provide a quick overview of what's there. Works pretty well for me :)
Is there a multiplication operation that doesn't lose data when it overflows? I'm thinking of something that implements `Fn(u64, u64) -&gt; u128` or `Fn(u64, u64) -&gt; (u64, u64)`. I looked at [the docs for u64](https://doc.rust-lang.org/std/primitive.u64.html), and the only thing I saw was overflowing_mul, but that returns `(u64, bool)`, so if it overflows, it still loses data.
Honestly a lot of jQuery isn't really needed anymore, with \`document.querySelectorAll\` and CSS animations becoming the better options for most things. For me it's become more trouble than it's worth.
You will probably get some good advice if you try adding typescript support to one of the existing Rust-based JavaScript parser projects. There is a list here that's pretty recent: https://gist.github.com/pitaj/b32e5d093698b311b793b29c7dab2b88 If you're just doing this as a learning project then it will still be valuable to look at the structure of what other people have done.
I think that includes compiling all the modules, and distros tend to compile all the modules they can of course.
If using the gfx-rs OpenGL backend is as fast as using OpenGL directly then I agree. My reply was more regarging your mentioning of modern graphics APIs in general. If you just use Vulkan then it will only work on hardware that supports Vulkan.
It certainly wont be as fast since there will always a small overhead due to the abstractions involved. I agree with you on Vulkan support, but hopefully things will change in the near future :)
&gt;I like implicit, I think. Implicit what? Or is it implicit? ;)
&gt;that sounds terrifying, i don't see what benefit that would give? Just write `x` a bit to the right yourself? How is it terrifying? The benefit is for chaining, just like you would chain methods. Some of the `await` proposals are awkward to compose or interact awkwardly with error handling. But, if the language supported postfix macros, special `await` syntax would be unnecessary and the macro would compose elegantly. Consider `await` syntax, similar to Javascript's implementation. Does this: await my_future? mean this: await (my_future?) or this?: (await my_future)? I wouldn't want to have to write either of those, if I could avoid it. With the postfix macro, you can clearly express your intent by writing either: my_future.await!()? or: my_future?.await!() or even: my_future?.await!()? with no ambiguity or clumsy parentheses.
It looks like the error you posted is from an old version of your code, before you changed f to be a vec of complex numbers from a vec of f64s. Anyway, here's the spot solution without trying to improve the rest of your code, just to get you in a compiling state (or rather, to get you to meet borrowck): let g = fft(&amp;(f[0..].into_iter().step_by(2).cloned().collect::&lt;Vec&lt;Complex&lt;f64&gt;&gt;&gt;())); `f` is already borrowed, so no need to re-borrow to slice `f[0..]`. Because it's borrowed, you end up with an iterator over borrowed items, `&amp;Complex&lt;f64&gt;`. But you want an iterator over owned items to construct a `Vec&lt;Complex&lt;f64&gt;&gt;`, so you can use `.cloned()` (which is cheap for copy types). Then once you have a Vec, you can borrow to get `&amp;Vec&lt;Complex&lt;f64&gt;&gt;` for a valid input to your function. However, idiomatically you should use a slice: `&amp;[Complex&lt;f64&gt;]` instead of a borrowed vec `&amp;Vec&lt;Complex&lt;f64&gt;&gt;`. There are a whole suite of other things you can change, but the first step is getting it to compile :)
Your problem is the `collect::&lt;&amp;Vec&lt;Complex&lt;f64&gt;&gt;&gt;()`. You're trying to collect into a reference, which isn't a thing that will work. Try removing the `&amp;`, so it can collect into a `Vec`, which you can than take a reference to.
&gt;Who does that benefit? We have `?` to replace `try!`, and for good reason. &gt; That's one macro made nicer. What about all the other macros that compose horribly? I suspect that there are useful macros not being popularised purely because they are hard to compose legibly.
I think the problem is that you try to collect into an reference to an Vec, instead I'd collect directly into an Vec&lt;Complex&lt;f64&gt; and if you need a reference later reference that object that you then own.... Just a guessüôÇ
If there is, I couldn't find it (but keep in mind, that's 20 pages worth of RFCs, so I may have missed it). Smarter borrow checker is always nice.
Sorry for being dumb. Are you saying it's intended to be built more into the language proper? Eg, `try!` was an user side implementation, eventually being used as `?` into the language proper. So `await!` will be turned into some unknown syntax in the language proper? This sounds nice, but TBH i'd kill for simply being able to use `await!` in my codebase without fear. Better syntax would be great, but i'm still not even using `await!` because i'm unsure where everything is at. It's a bit of a mess as a newcomer. I'm waiting.. rather impatiently, on the whole future usage. `await!` is plenty friendly for me to start using it, just like `try!` was - but i just don't know when that day is coming... or if it's already here. Confusing :s
Works like a charms, thanks :D &amp;#x200B; I didn't even think about declaring the whole function for slices. What I initially wanted was to use arrays since I guessed they were faster, but reading up on that it seems like I'd have to set a fixed length in my function for that and that's just no possibility.
Yep, the extensive answer from \_\_ah included that and fixed it, thanks :D
I don't understand `2018` - i use Nightly, does that mean i'm ahead of `2018`? I ask because i still have a bunch of `#[macro_use]` calls in my code - should i be importing them instead?
Yep, that's fixed it, thanks :D
Sorry my took my phone just noticed
When I do \`struct a/struct b/ impl a/ impl b\` style, I tend to agglomerate impls on private modules if they regard the same Trait or follow the same logistical need. Modules everywhere..
&gt;Arrange code to be in the suggested reading order It's slightly unfortunate that macros need to be defined before they are invoked. Sticking to this rule often means moving macros into their own module (which isn't necessarily a bad thing).
This is something I think about a lot and it‚Äôs nice to read someone else‚Äôs thoughts on the matter. One thing I struggle with is module granularity, although it‚Äôs probably more accurate to say file granularity since it‚Äôs not a 1:1 mapping. Writing java by day, I‚Äôm accustomed to very fine-grained ‚Äúmodules,‚Äù and tend to prefer it over huge thousand+ lines of code scattered with various types of definitions in no particular order (something I would mind a lot less if there were a tool that just did it for me). Taking into account the conceptual differences between rust and Java‚Äôs module systems, I‚Äôm still trying to find the sweet spot where I have reasonably focused source files while not micromanaging modules. 
Has no one suggested postfix `...` yet? It reads very intuitively, like the program text is trailing off waiting for the operation to complete. let response_body = client.get(url)...?.body.read_to_string()...?;
If you have a practical but succinct example, I‚Äôm all ears. We‚Äôve constantly solicited feedback on this stuff for years, and re-written it like five times.
Many people do want to get rid of mod for this reason, but many people really hate the idea, so it stays.
I can only hope you're kidding üòÖ
I think `await f?` === `await (f?)` is probably the best prefix option. Are there any major issues with that (other than chaining being more awkward than with postfix)
I've taken some time to drive down the number of clones and use more borrows. According to my benches it's also now \~50% faster. Not so bad. Thanks!
`await (f?)` is the consistent precedence but from what I hear, most of the time what you have a future of result (`-&gt; impl Future&lt;Output = Result&lt;?T, ?E&gt;&gt;`) as opposed to result of future (`-&gt; Result&lt;impl Future&lt;Output = ?T&gt;, ?E&gt;`). This means that you'll often have to write `(await f)?`. The inability to do ergonomic chaining is the deal-breaker for me wrt. any block or prefix syntax. We introduced `?` for a reason since `try!(..)` wasn't good enough; let's not repeat history.
Using nightly means that you can *opt into* 2018, however by default the code is still 2015. To switch, you need to edit your Cargo.toml: [package] edition = "2018"
Interesting, thank you!
Implicit await on all futures. It‚Äôs default strict semantics, you instead use a syntax construct to select which futures you *don‚Äôt* want awaited on immediately so you can move those to a concurrent join point or to block the event loop if you really really need the io next. The assumption of this is that most work done in futures is linearly depended upon by the immediately following lines and you just want to let the event loop take care of when the io is complete.
Isn‚Äôt there an implicit hierarchy by file name and presence of mod.ra files in directories?
There isn't
Is it more idiomatic to use getters/setters or just pub fields of structs? I know getters are used in the standard library, but I've read that pub fields can be preferable at times.
I don't think it's in the stdlib, but it's quite trivial to write yourself by casting to `u128` before multiplying.
FYI: This is part of a binary, and will not be exposed as a lib, and Playground does not cache the cpp crate
I have no stakes in unsafe code, but it looks to me like you're never changing `INSTANTIATED` to `true`. Maybe an oversight?
Check out the official book and then try to implement some solutions to Advent of Code. After, check others solutions on GitHub to get an idea of new techniques.
my understanding (which may be wrong) is that macros expand to code. The internals of how await works aren‚Äôt stable. This means that the macro could also not be stable. Syntax doesn‚Äôt have this problem.
What do you mean by that, specifically?
How often is await composed?
As someone new to Rust this is eye opening lol
truth
HashMaps need more love. Thank you for pointing this out :)
We have ? Because first we had try and one thing that delayed ? Is that we had no experience with using try in postfix position. Now we have await and are discussing another syntax extension similar to ? but have no experience with that. The reason we don‚Äôt have experience with await in postfix position is because we can‚Äôt use macros in postfix position yet. If we could do so, we wouldn‚Äôt need a new keyword or symbol for await, and we wouldn‚Äôt have needed a symbol for try. There are only so many symbols that we can use for things that could be solved with macros in postfix position.
I‚Äôd think so. They return futures which have a variety of ‚Äòthen‚Äô methods. 
I‚Äôve never had to, I suspect that if you have a nested future you need nested await, but you could also just flatten the futures and use await only once.
Yeah; this is an interesting question in this whole debate, I think.
The suggested `await? f` seems to be a good workaround for the `(await f)?` issues. I do agree that postfix would preferable. But I really don't like `f await`. That does read intuitively at all to me. I quite like the postfix function or postfix macro syntax's, and I wouldn't be adverse to `@` or `@await` though.
You're right, I did miss that. Thanks for the catch
Thought it was dying like a year ago lol
I have known horror... Should be easier on the eyes if you were to write a small C module to glue together C++ and Rust.
Rust is difficult to learn, but easy to master. There is a steep initial learning curve, due to the large number of concepts and types to learn beforehand. However, once you've learned and understand those concepts, then everything else will fall into place. One of the best benefits of Rust to beginners is the Cargo crate ecosystem. Many problem domains have already been solved and crated in the Crates repository. It's often a better idea to import a crate from someone who's already researched that problem, than to do the guesswork yourself. The concepts are really quite simple when you look back in hindsight. Most of the struggle is in learning how to write software that satisfies the borrow checker, rather than fighting it. The harder you struggle, the harder borrowck fights back. There are three basic rules: pass by value moves ownership, pass by immutable borrow can be shared, and pass by mutable borrow must be unique. In terms of restrictions, from least to most: `Self` &gt; `&amp;mut Self` &gt; `&amp;Self`. Restrictions for a value are based on the highest level of restriction active at that time. Immutable cancels mutable, cancels ownership rights. `Clone` can be used to make new, unique copy. As a general tip, `Rc` and `Arc` allow their inner values to have a static lifetime that can be shared in multiple locations, but they make that value immutable. `Cell` / `RefCell` / `Mutex` / `RwLock` ignores these rules, albeit with restrictions that make it possible to do so with some overhead. `Rc`, `Cell`, and `RefCell` disallow sending across thread boundaries. `Arc`, `Mutex`, `RwLock` do not.
Why?
It depends on what you're doing with the field. If your getters are just returning references and your setters are just updating the value directly, and you don't envision changing the way that field works without a breaking API change, a pub field is probably better. If you're doing any kind of transformation in them (like a getter that returns a `&amp;str` when you own a `String`,) I'd go with the functions. You can also do both.
I can't imagine Rust doing anything but improving its standing. Hopefully, the next few years will see it develop into a language used more and more in industry. It definitely has a lot of big selling points that are impossible for particularly innovative companies to ignore, if only from a financial perspective. I think it's inevitable that as Rust nears "feature completion", development slows somewhat and we see the core developer team shrink, with more emphasis placed on Rust's ability to interoperate with the rest of the software ecosystem. This isn't a bad thing: if anything, it's a sign that Rust is becoming mature.
[Project Euler](projecteuler.net) is a good set of problems to get started on.
&gt; and become a better option than c/c++? Questions like this make me uncomfortable because they all but call for zealotry. That said, here's how I see the future: In some projects, Rust will incrementally replace C or C++ but this will be a minority use case. This is basically what the language was built for; and it's a good C replacement but lacking a few features (specialization and const generics come to mind) to be a solid C++ replacement. The future I'm really excited for is the domains C and C++ were _supposed_ to cover but never actually did. My experience here is native code extensions for Python, where the C API is a supposed strength of the language but tragically underutilized because for a Python programmer, the safety/performance tradeoff isn't there. Rust changes this situation dramatically; I can write little Rust extensions to improve my quality of life in the Python world without living in fear of UB.
`INSTANTIATED` isn't atomic, or behind a lock; wouldn't it be possible in a multi-threaded scenario to create more than one `Stitcher`?
Extremely beautiful, yet comically inefficient.
Rust has been climbing the Github rankings for a while now (at least last I checked). As a professional JavaScript developer I'm mostly excited about the wasm target and for extending Node without having to dive into C/C++. I really like the high level abstractions available in Rust with (theoretically) no runtime overhead (pattern matching is a beautiful thing). I really want to understand Rust, and I want it to be my next language I could consider myself "mastering," but I still mostly feel like I'm fighting the borrow checker more often than I should be, and I feel like that hinders my productivity in Rust. I'd like to see more examples of common architecture patterns that are easy to grok for outsiders to systems programming.
/r/playrust?
It will have taken over the world and all other programing languages will be so inferior that the slightest mention of them will be heresy. No, my guess is that rust will have slow and steady growth, It will carve out a niche for itself in high performance web applications, video games, and other applications where the performance/efficiency gains outweigh the friction it puts on developers. It will slowly convert some c++ developers, but I'm not expecting a huge boom in popularity. I'm guessing it will pretty much have the same levels of growth. Rust doesn't feel like a gimmick to me, It brings real things to the system language table, I think it will have lasting power.
I have a module A, which defines a function F. I want to use F within a doctest/example illustrating the use of F. How on god's green earth do I import that function into the doctest?
People were tired of clans stomping, bad optimization, vast majority were unhappy with the updates they were rolling out with the game in general and what they were doing to guns..all I remember I stopped playing over a year ago with 1400hrs
Thats a great improvement. Thanks for following up. 
This sub isn't about the video game...
Rust 2.0
This is what you get for not checking where you're landing from /r/all :) This sub is about Rust the programming language, not the game.
Yes exactly... or node.js extensions
I hope to see Rust make grounds in HPC. I would really like to see a homebrewed replacement to MPI that somehow extends the notion of ownership across nodes. I think futures will be a large part of this story. On the GPU side, as SPIR-V targets improve I'd love to see a "better CUDA" come to fruition. Hopefully Rust also becomes the goto language to bind against Vulkan with. Numerics. Once `const generics` (my most awaited lang feature) rolls out, I really hope the numerics story rivals just about everything out there. Then maybe ranges (value-bounds) &amp; some sort of dependent type story to follow suit. Then hopefully a WASM revolution and some crazy Jinja inspired macro web-frameworks to really roll out a new era of performant minimalist websites. Web zero-cost abstractions? How absurd! I think it's the probable future given the state of the web. Will it get here in 3 years? Much of this I think will be far better, and I'm excited to see what progress is made.
maybe the game would be better if it was written in rust it is very slow with poor performance
I'd say it depends. It's very possible that some people have asked the same question before for any language, and now the result is just that every language has its own specialty. Rust is certainly better than C/C++ in some cases, like less vulnerable memory management, better efficiency, more ergonomic error handling and some handy functional programming features. The rust community has already proved the language's capability in lower level system programming/applications, by, for example, writing a full fledged operating system (Redox). What rust is not good at, and will probably not get _too much_ better, are the difficulty to learn and time of existence. The memory model is very good for the compiler to check for errors, but still a little counterintuitive for people coming from other languages. In addition, rust has only been around for a relative short time, and the libraries for doing anything is not so complete yet (especially the standard library). This will get improved as more and more people are involved, but would still be hard to be compared to the large amounts of libraries C/C++ has. With all that said though, I still think and wish rust can claim it's own place in the industries. I hope rust can continue to grow in the low level applications and replace a significant portion of C/C++. 
This is my constant struggle with "should I learn rust" On one hand I see all the benefits of rust and how a lot of what is opt-in best practice in c++ is default in rust. On the other hand C++ isn't just going to disappear. So should I continuing getting better at that? Of course a lot of concepts in software development are transferable. 
Oh lmao
&gt; `rustc main.rs -o main` Wait, isn't that the Debug binary? Shouldn't you use `rustc -C opt-level=2 -o main main.rs`?
This is really neat, especially the linked explanation [video](https://www.youtube.com/watch?v=HEfHFsfGXjs). I think you could remove the `(v1 &lt; 0.0)` check in the while loop, since there is a similar check inside the loop and `v1` is always initialized to zero. I'd be interested to know if that has any performance implication, or if rustc/llvm have already removed that check as unnecessary. 
&gt;\- Where do you see yourself in 5 years? &gt; &gt;\- I'm waiting in the checkout lane at a grocery store.. I'm wearing faded green shirt and jeans. I'm buying bread and some kind of cheese. People always overestimate progress speed (hello "Back to the future" ;)). 3 years is not that much. Hopefully Rust will smooth some rough edges by then, get GATs and existential types, etc. And we should see more Rust positions (aside of crypto). IMO it will take more like 20+ years to beat C\C++ and become a better overall option.
I don't know where the Rust will be in 3 years but I hope we will see Rust on the web on frontend in near future. I really hope wasm will replace Javascript hell we have now. Javascript is not bad. It could be certainly better but the language is not the issue. But what people do with that in past several years and what Node.js and 80% packages on NPM became.... jesus. Backend is fine. Python or even some simple PHP. Ruby... sure. Why not. I guess we are okay there. These thoughts come from my head. From fullstack developer head.
https://learning-rust.github.io might be helpful to check basics quickly.
If you wish to document the function `increment`, you should place the doc-comments (`///`) directly above the item itself. Then, `increment` should be available immediately. Otherwise, if you want to use `increment` somewhere else ‚Äî outside of `A` ‚Äî reference it with `A::increment` or hide the a `use`-statement inside the doctest: `# use A::increment`.
That's... a pretty wide net of a question. ### Language I would expect that in the next 3 years Rust should have gained a number of features. Compile-time computations: - const generics, generalized beyond built-in types. - const functions, generalized to any pure function. - const variables, generalized to any instance that can be obtained via pure functions (including `FxHashMap&lt;String, String&gt;`). Generators: - async/await. - coroutines. - generators. As well as Specialization and Associated Type Constructors. With the first two already being worked on and a Datalog engine being worked on which should unlock sound Specialization, I think this is a realistic schedule. And if we get all of that; then Rust will be a very solid language. ### Verification I would expect that the following items be ready by then: - Fully specified invariants for `unsafe` code; with MIRI detecting their violations. - Fully verified type system; including the use of Prolog/Datalog in the compiler for borrow-checking and trait implementation. - Formal specification integrated; such as [Prusti](http://www.pm.inf.ethz.ch/research/prusti.html). This would mean *full formal verification of Rust programs* for the masses^1 . This would also mean catching up to Ada/SPARK and FRAMA-C, the current state-of-the-art solutions. ^1 *In the absence of a certified compiler, such as CompCert, the toolchain itself may still introduce bugs. On the other hand, even in stringent domains, developers do not use formally verified toolchains and instead rely on certified toolchains with documented bugs. I do not expect a fully verified Rust front-end in a 3 years timescale, and even less a fully verified backend, although one day...* ### Tooling Hopefully, a lot of the tooling effort will bear fruit in the coming 3 years; and we'll be able to look forward to a fast edit-compile-test cycle as well as an IDE experience on par or better than C++. ### Reach With the aforementioned features, I'd expect to see development in areas where either of performance and reliability are primordial: - Embedded. - Low-latency services. - Low-memory utilities. - Number-crunching. And of course, who knows where WASM and the ease of integrating Rust with JavaScript/Python/Ruby will lead us. ### Portability Rust has already corroded a number of Linux distributions; the general trend for programming languages is to become more portable over time, so I expect Rust to be available and maintained on more and more platforms... sufficiently so that portability will cease to be a concern for a vast majority of developers, even in the embedded world. ### Conclusion To conclude, I think that in 3 years, there will be a number of domains where Rust will be the DEFAULT choice, and not using it will require serious justifications.
Nice, and a solid, reasonably quick read. Personally I value ergonomics more than you seem to, as I think Rust could use _a lot_ more sugar, and is also quite a few years and probably several editions away from being truly mature, but I suppose it depends on the direction you're coming from and the hopes you have for the language.
In this case ? Why ? Will anyone use that in production anywhere ?
Thanks for the feedback! I agree, \`register\` matches the semantic better, I will change it. Regarding the operator overloading I still need to reiterate a bit :D 
No worry, I will keep vulkan/gfx-hal in mind when building up the rendering parts. Adding another backend shouldn't be that tricky in a later stage. To be fair, GL has so many different versions and derivatives, gfx-hal won't (most likely) be able to cover them all. Maybe I will change over to raphlinus' piet, webrender, draw2d or whatever cross-platform 2D API will pop up. Currently, getting something on the screen is top-priority! 
Why not? What's the point of being inefficient? &gt; I found that 9 is about the limit a modern computer can calculate in a reasonable time (about 10 seconds on my 2018 Macbook Pro, and similar speeds on other computers). In Release mode, maybe you'll get it in 1 second, and be able to see 10 and 11!
Put it behind a feature gate 
I tried the following code and got the error below it. A couple of attempts to give it a use statement gave me errors about not being able to find the function in the module. ``` /// ``` /// let x : usize = 9; /// assert_eq!(10, increment(x)); /// ``` pub fn increment(x : usize) -&gt; usize { x + 1 } ``` Produces the error: ``` running 1 test test src/lib.rs - increment (line 4) ... FAILED failures: ---- src/lib.rs - increment (line 4) stdout ---- error[E0425]: cannot find function `increment` in module `self` --&gt; src/lib.rs:6:22 | 4 | assert_eq!(10, self::increment(x)); | ^^^^^^^^^ not found in `self` thread 'src/lib.rs - increment (line 4)' panicked at 'couldn't compile the test', src/librustdoc/test.rs:323:13 ```
Do you think we should exile holdouts or burn them at the stake? 
Or PHP!
I'd recommend learning Rust - the borrow checker enforces rules that you should be obeying in C++ anyway, and so the C++ you write once you've learnt to keep the borrow checker quiet is better C++.
Cpu performance gains are gettting smaller and smaller. People want performance and rust can deliver that in a safe way and with a nice language. My prediction is that it will grow. It will slowly gain traction in the gaming industry and after a while we will see a ketchup effect and it will be mass adoption. This is however over a timespan of years.. 5 to 10 
&gt; Mozilla using Rust in Firefox has left users of more obscure platforms and operating systems without a modern browser. Firefox used to be far more portable than chromium or other modern browsers, but that stopped being the case due to the Rust dependency. I have seen blog posts voicing such complaints, as well as projects that fork the last pre-Rust version of Firefox to backport fixes to it as much as possible and support it for users of platforms without good Rust support. To be honest, I was very surprised there. There were essentially two alternatives: 1. Fork Firefox. 2. Support Rust. Both require an initial investment and ongoing maintenance, though the orders of magnitude are not obvious to me. On the other hand, the consequences are *vastly* different: 1. Fork Firefox: well, that solves the issue of Firefox, but what of other uses of Rust? Keep forking? 2. Support Rust: problem solved. I find picking (1) to be a short-sighted move.
I'm learning Rust and program C++ at my job. I do use it for writing some high-speed tools (usually faster to get something working in Rust), and I think some of the things you learn from Rust transfer to better understanding of what's dangerous in C++ and why, or how you could do it in a more safe way in C++. On the other hand I have no hope that my industry will switch to Rust anytime soon. C++ is already "new".
&gt; The suggested await? f seems to be a good workaround for the (await f)? issues. At the cost of more complexity and special cases... I'll pass :) &gt; But I really don't like `f await`. It does look awkward on a single line `foo await.bar()` but it should mostly read fine when formatted on multiple lines. `rustfmt` can help here. Using `foo await` is not my favorite postfix syntax but I can live with it. &gt; I quite like the postfix function or postfix macro syntax's I fancy it too; for example, I'd love to write `foo.dbg!()` or `foo.unwrap_or!(continue);`. However, having `.await!()` would entail giving up `await` as a keyword and I don't think we should do that from the POV of future compatibility with language design. I also think `.await!()` as compiler built-in magic does not feel as first class as async/await deserves. &gt; and I wouldn't be adverse to @ or @await though. `@await` feels a bit weird, it looks like 2-consecutive `a`s. As for other postfix syntaxes, I can live with `foo#`, `foo@` and such. My preference would be for `foo.await` but folks think this looks to much like field access so you could write `foo#await`... IOW I don't feel too strongly about the exact syntax, but method chaining should be ergonomic, the precedence should be clear, and I'd like to keep `await` as a keyword.
I think usability and beginner friendly docs are really not there yet. I often struggle with some little things, even as a C/C++ programmer. I can't imagine somebody who never did manual memory management will have a fun time. On the other hand if you want to do WASM it's basically the only sane choice. C/C++ will kill you with bugs, and everything else is garbage collected (so from the benchmarks I've seen won't benefit at all from WASM).
Might just be a missing semicolon. Can you post the code and the error? &amp;#x200B;
The real problem is not people being disagreeable and arguing their positions. The real problem is that some programmers identify themselves as programmer of specific language and act defensively in regards to it. And personally, I don't think it's my responsibility to accommodate or play around those defensive reactions. Quite the opposite, catering to those feelings is prepetuating environment where decent healthy arguments a can't be had. And it's sad, because we should be able to learn from different experiences, yet this mentality completely shuts it off. Now back on the point at hand. I don't know. I do think it will make C++ to run for it's money. Rust seems to qualify as disruptive technology. And if C++ fails to improve, I think there are chances that it will be left behind much like cobol was. Maybe not in 3 years, there is a solid chance that will be the case. [https://www.youtube.com/watch?v=6qCH7Y2rc\_w](https://www.youtube.com/watch?v=6qCH7Y2rc_w) I loved this talk. It's about Go, Rust and Julia in a wider context of how programming languages change.
I posted the code in the link. I copy and pasted the basic example to see what it does
any performance impact due to the removal of \`jemalloc\` as a default allocator?
I would recommend also providing a playground link for next time: https://play.rust-lang.org
This sounds like it could be working. I have not seen any examples of that in the wild though?
I'm also guessing slow and steady growth. I hope the docs will improve to a point where it's much easier to see what a function does or how it's used. I find the standard doc format a bit bad from a UX perspective. It lists all the traits, instead of showing the functions first (which is what most people want when they first look at it). I find too many docs that just don't have any examples, and it would be easy to use if they had them. Spaces where Rust has a chance to make a dent: * High Performance Computing (especially if you need correctness as well as performance) * High performance CLI tools (especially text processing with borrow checking and COW is often magically fast) * Embedded (with the growing complexity in the embedded space and the same old memory and multithreading bugs in C/C++ I think Rust would really improve quality here and be cost-effective, however the upfront cost probably makes this a hard sell for anybody except a manager who really knows software.) Spaces where I don't see Rust yet * WASM is nice, but I think teaching the average JavaScript developer Rust will be difficult. However as WASM needs a no-GC language to shine, there aren't many contenders here either. Roadblocks: * Lack of library support for some basic things. * Some libraries don't have the same quality (although amazing for a 3 year old language). * Steep learning curve (C++ has this as well, but it's a steep learning curve my colleagues went through in the 90s and 00s, not now, C++ is way more complex). * Lack of easy ways to do some things that should be easy, but where there is a problem with Rust's memory model (implementing foreign traits for foreign crates, self-referential structs, returning mutability from a function) =&gt; I hope there are going to be some ergonomics changes here.
That makes two of us! Here, have a point. I'm just got used to seeing so many smart developers who work closer to metal around in this subreddit, that I'm just happy to see someone else coming from javascript world who got interested in WA.
There are 4 big features I think are truly necessary for having a well-rounded Rust language: - const functions/generics/variables, only limited by purity: if it can be pre-computed, it can be optimized on. - async/await, coroutines, generators: asynchronous + yield lead to very readable code. - specialization or rewrite rules: to heck out the last nanosecond. - associated type constructors: streams, collection traits, ... And the former 3 are already being worked on as we speak. Afterwards... well variadics are "missing", but a lightweight version based on manipulating tuples (splitting/joining) may very well be fully sufficient. And then I really can't think of anything big. I'm sure some polish will be necessary here and there: edge cases, quality of life improvements, papercuts, ... but I can't think of anything fundamental. Maybe I just lack imagination :) On the other hand, an avenue that has yet to be explored in Rust is a focus on reliability and performance: a combination of formal verification and rewriting rules, for example, could lead to both guaranteed correct and guaranteed optimized code. I'd imagine doing this with attributes, so that a compiler could just ignore them altogether, and so in this sense it wouldn't be strictly part of the language... yet yield interesting benefits when the compiler does support them!
Going off on the HPC-tangent a bit here, &amp;#x200B; \- MPI libraries for high-performance interconnects (e.g. Infiniband) resort to busy-polling, and thread-per-core affinity; when you have network latencies in the low single-digit microseconds you cannot affort to spend time on interrupts, context switching and all that stuff. Is the promise/async/await style programming amenable to this kind of world? Or maybe this is dumb way of asking, how are async/await implemented under the hood? Is each async function allocated its own stack, and then the reactor takes care of polling and progressing all the awaits until they are ready, at which point the stack can be deallocated, or what? And is this suitable for low-latency applications? &amp;#x200B; \- I think in general, the current Fortran/C/C++/python are ripe for replacement, but for general application code I think Julia has a somewhat better chance at succeeding than Rust. In my experience, most HPC code is written by grad students under time pressure to produce the next paper, and with very little knowledge about software engineering. I suspect the learning curve of Rust is too much for many of them. Note, I'm not blaming the grad students, they are doing what it takes to survive in today's cut-throat academic environment. It's more a problem with how we fund and measure academia in general (but that's another discussion entirely). Rust might have a place in some "moonshot" type problems, where you can afford to dedicate a team to a multi-year effort to write some massive code from scratch to run on the next generation of supercomputer, or whatnot. But that, I think, is a small part of the whole of HPC. To some extent I think that's a bit of a shame, since I personally prefer the kind of compile-time guarantees a strong and static type system gives you, but, oh well. Julia, as a language, is a big step forward from the matlab/R/python that people mostly use today, and they have managed to produce some pretty nice libraries too.
The compiler is sayng the error is happening on the line let counter\_opts = Opts::new("test\_counter", "test counter help"); because of the let statement. The specific error message is expected item under the let
The title says inelastic collisions, but the repo description uses elastic ones. Which one is the correct one? 
You wouldn't
You can parameterize `Foo` over a `?Sized + Fn()` parameter, which would accept both `dyn Fn()` and `dyn Fn() + Send`: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=797da586de83a398cd4e9701978eefec
If you want get better at C++, learn Rust. Seriously. Or rather: learn C++, until you know it pretty well. At that point, you run up against the law of diminishing returns. Further study of C++ won't improve your C++ very much. Athletes know the importance of cross-training, and so should we. Branch out; learn other languages. Eric S. Raymond once said that every programmer should learn Lisp. You'll probably never use it, he noted, but just learning such a radically different language will impact how you program in the languages you *do* use. The same is true of Rust.
I would, because PHP 7.0+ is excellent.
Nope, managed to get 9 digits in about 7s and 10 in 1m20s using: RUSTFLAGS="-C target-cpu=native" cargo build --release I'd guess rust is already pretty efficient with a simple program like this
Check out the [features section for your Cargo.toml](https://doc.rust-lang.org/cargo/reference/manifest.html)
I think most people wouldn't make their API smoother by adding huge warts
[url](https://doc.rust-lang.org/book/index.html)
Is this better or worse than `rustc -O main.rs -o main` ? 
Yeah I messed up in the title. These are perfectly elastic collisions. 
You‚Äôre trying to declare a let binding in global scope. That doesn‚Äôt work, it has to be inside a body of a function or something.
So is the basic example wrong? What would be the correct version of the basic example?
The basic example wouldn't work as is. It assumes (wrongfully) that these function calls and declarations aren't done in the global space. I can't run the example from my phone, but try wrapping everything under use prometheus... into a main function.
Most code samples in docs make the assumption that you‚Äôve wrapped the code in a main function, or some other function.
So should the basic example be in main?
Or wherever you‚Äôre using it.
I liked the `await?` version (where `await f?` means `await (f?)` but you have `await?` which is `(await f)?`) it isn't perfect but I do wonder how often chaining awaits is going to go deep. And more importantly how often that code won't be complicated but it's fundamental nature anyway. Especially when some examples brought up look like library functions that should be added (e.g. `await().join().await()` example, sure the prefix notation isn't ideal but `unwrap_join` or something with a reasonable name could turn that into a single `await`)
I would be interested in knowing which obscure platforms ran firefox before adding rust. Also supporting rust on those platforms would require that they be supported by LLVM first, in which case I don't think these "obscure" platforms have LLVM support.
I'm also intrigued by Go but I really want my next/secondary language to be non-garbage collected just to broaden my skillset. Also like another person commented here GC'd languages aren't good to target wasm.
Will do :D
It would help to know which library you use to draw. As u/uanirudhx says, it is likely that you are using either clear or refresh methods that force a full redraw of the screen. Comb the docs for your library, see if it provides a function that doesn't do this.
[The Book](https://doc.rust-lang.org/book/index.html) is hands-down the best method out there if you learn well by reading and doing exercises. If you prefer learning via some other method, could you describe what works best? I haven't done much exploration since "read this well-written book explaining concepts and follow along" works for me. But I know not everyone learns the same. See also this other thread: https://www.reddit.com/r/rust/comments/a7cmz3/looking_for_interesting_and_uptodate_tutorials/ 
Not being sarcastic (I only know some of the quirks of PHP, apart from that I don't know the language): What makes you choose PHP over, for example python or javascript? 
Using a feature gate won't work in some cases: Assuming a feature like `require-send` for the crate (let's say `lib`) providing `struct Foo { ... }`. Now imagine that a crate `a` depends on just `lib` and a crate `b` which depends on `lib` with `require-send` enabled. If you now have a fourth crate, e.g. `bin`, that depends on `a` and `b`, `lib` would be compiled with `require-send`, which would break crate `a`.
It will continue to be the language I use for the majority of my projects.
Try asking over at r/playrust, this sub is for the Rust programming language.
&gt; in which case I don't think these "obscure" platforms have LLVM support. If they had LLVM support, then adding Rust support should have been easy, so I'd guess they didn't and instead have GCC support. This leaves two possibilities: * Adding LLVM support for the platform, probably out of tree to start with. * Adding GCC as a backend for rustc; possibly through synthesizing C code. The former seems to have more benefits, though it may cost more in exchange. The latter is more restricted as it only paves the way for Rust, not other languages, but would benefit multiple platforms so the effort could be shared. Either route would be valid, really. And either would be less short-sighted than forking Firefox... so I think there were other factors in this decision that we are not privy to. *Note: the rustc compiler is already aiming at supporting an alternative backend, beyond LLVM, called cranelift. Once it supports a second backend, adding a third becomes even easier, and with the reach of GCC... it seems like a natural choice, license permitting.*
Oh shit...SORRY, thanks for the clarification
Yes, Rust has interopability with C, meaning you can call C functions from rust and Rust functions from C. This is how libsvg transferred from C to Rust, one function at a time until it was almost completely rewritten.
There are actually a number of C projects which have been or are being oxidized gradually; see for example: [Librsvg Oxidation](https://www.reddit.com/r/rust/comments/ae5xwd/librsvg_oxidation/). Since Rust has a C FFI, it can expose a C API, and act as C code for the rest of the world, so oxidizing C code is easy. It would be possible to tactically replace parts of a C++ too... but this requires going through C because C++ and Rust just have quite divergent rules. For an object-oriented C++ codebase this is relatively doable; for a template-heavy C++ codebase, it's a tad more complicated.
/r/playrust
Actually, it's Rust: you can even put the use clauses in main :)
/r/lostredditors
Convergence. Mathematical beauty. Enjoyment.
&gt;And what you know, perhaps Rust will motivate C++ to make quality improvements which will end up benefiting it. I'm not sure it will be willing to shed anything.
This is indeed a big selling point of rust, and has been since 1.0! Gradual migration of C/C++ projects is itself the reason a number of design decisions were made, and I'm sure it's a large part of why Mozilla sponsors rust. It's not 100% "drop in" since C and C++ projects have so many different build systems and different project organizations, but it's pretty close. I think the biggest current success stories are `librsvg` and the various parts of Firefox like WebRenderer and the css engine. There are also a number of C libraries written in rust, like html5ever.
I was using stdout. I was unable to find a good solution there. I'm not using EasyCurses (clean API) which is a nice wrapper around PanCurses (Unix+Win) which is a wrapper around ncurses (Unix) and pdcurses (Win). Sometimes the console kinda janks out. But I'm using this to capture video output for a blog post. So it's sufficient for my needs.
The only easy way I saw of having easy interop with C++ is nim's which transpiles to either C or C++ (or even Obj-C).
&gt; I can't think of anything fundamental. Maybe I just lack imagination :) Here's a start... 0. Finishing `type Foo = impl Trait;` and `impl Trait` in general. 1. GADTs, e.g. `enum Expr&lt;T&gt; { Plus(Box&lt;Self&gt;, Box&lt;Self) where T: Add, Minus(Box&lt;Self&gt;, Box&lt;Self) where T: Sub, }`. 2. Trait aliases, polymorphism over traits, associated traits. 3. Multi trait objects (`dyn Foo + Bar + Baz`). 4. Delegation. 5. Generic closures and `for&lt;T: Debug&gt; Foo(T)` bounds.
in order for it to be a "method", it needs to take a reference self as the first argument. Otherwise, you can't call it on an instance of the structure.
Well it can‚Äôt really, given its promise to maintain backwards compatibility. That‚Äôs why editions, while painful, are good for Rust, because they give it a chance to shed stuff over time. C/C++ is also backed into a corner because they support far more obscure targets than rust has ever had to (I think it‚Äôs good we stick to fairly mainstream stuff). Many of the weird, dangerous parts of C, like not being able to assume a signed integer is twos complement, is because that **couldnt** be assumed on some valid C targets.
Indeed :)
Obviously, the correct syntax should be `fut::&lt;await&gt;()` a.k.a. the "laterfish". ;)
Hum, what about: https://herbsutter.com/2018/09/20/lifetime-profile-v1-0-posted/
Another way of looking at it is that Cargo feature flags are *additive*: they should always extend the API, not restrict it. This is because Cargo resolves conflicting flags by unioning them together, such that downstream packages might end up with more features than they asked for. In the case of /u/diwic's library, such a feature flag will extend the API for *consumers* of `Foo`, since they can now `Send` the struct. But the API will be restricted for *producers* of `Foo`, since they now have to satisfy the `Send` bound. So the feature is not additive, and is not appropriate as a Cargo flag.
Someone commented an solution, but deleted it. I would have to add &amp;self to new
This is because `new` isn't a method but a function essentially under the `CachedFile` namespace. This is similar to static methods in other languages which means you can't refer to them via self since they're not part of an instance.
You have some examples of some ergonomic sugar? I cannot think of much, except maybe spme type of `throw‚Äò operator to replace ‚Äòreturn Err(...)‚Äò
Try ```rust Self::new([self.name](https://self.name), self.path, self.max\_age) ``` In Rust, there are two ways of defining and calling functions. Associated functions are called by prefixing the type name and using the path separator (as shown above) whereas methods are called by using the `.` syntax on a value as you attempted. The method syntax can only be used if the first argument of the function is declared with the name `self`. I'd recommend giving this page a read from the manual: [Methods](https://doc.rust-lang.org/rust-by-example/fn/methods.html). Also, in case this causes confusion, `self` and `Self` mean two different things. `Self` simply refers to the type that is being implemented whereas `self` indicates that a function can be called as a method.
&gt; Well it can‚Äôt really, given its promise to maintain backwards compatibility. That‚Äôs why editions, while painful, are good for Rust, because they give it a chance to shed stuff over time. Relatively speaking, they aren't very painful. Look at the Python 2/3 transition.
You need to add a `.into()` like `let test: TestFlagBits = TestFlags::flag1.into();`. I don't think there is a way to make it totally implicit.
Way more plug-and-play, better performance in some aspects, extremely good community and package ecosystem, etc. Really not much you can't do. Most of the detractors only know PHP from its older versions which had a lot of major issues, but things were smoothed out over time. It's easy to write safe code (not Rust levels of safe, but I digress), there's a decent type checking system now. https://phptherightway.com is a solid resource to show what PHP can do and how to avoid the trappings of legacy crap.
If `new` is supposed to act as a constructor, then that's probably not what you want. Instead, see my answer on how to use the associated function syntax.
Inertia, interoperability, and existing ecosystem. Same reasons as any "boring" tool, really. I'm glad php is continuing to improve because as long as there is web dev, it's going to be inescapable for someone; WordPress still powers a third of the web (2/3rds+ if we're talking just blogs). The improvements that happen to php affect servers in a way that essentially nothing else does, it's ridiculous. There's entire swaths of the internet where it's almost impossible to avoid php. Just like jQuery is still almost mandatory for certain things (data tables for example. Nothing comes close to the jQuery data table library and it's php integration)
Well, it's not the docs you consult as beginner, but "The book", main Rust learning source material. There is also Rust by example. And I was going through as far as I can say, it is a blast. I spent 1-2 hours with it every day. It's reasonably though when compared to something like javascript. But I find the learning process to be quite exciting. The language itself for me feels exciting to use. Dunno, did not have that feeling with Go for example. Can quite put my finger on it, but the language itself seems just very powerful, and though of it keeps me motivated.
Ah, ok. Sorry to bother you further over this, but I'm curious: I couldn't find a crate named `stdout` on crates.io, do you actually mean `stdout` as in `std::io::stdout`?
&gt; * Embedded (with the growing complexity in the embedded space and the same old memory and multithreading bugs in C/C++ I think Rust would really improve quality here and be cost-effective, however the upfront cost probably makes this a hard sell for anybody except a manager who really knows software.) Or those of us who have a lot of autonomy!
* LeetCode * Exercism Coding practices are good too. You learn to use every bit of standard libraries and data structures. 
The `try!` macro isn't intended for this, but that's what it does. If you want to alias it as `throw!`, it's just macro_rules! throw { ($exc:expr) =&gt; { try!(exc) } }
Even more similarities. I also tried to pick up Go first. And it was... quite easy to build simple crud application with it at the first day. However, I've found it to be directly competing to what I already use, rather than supplementing and extending my toolkit.
Thanks for sharing. I'm interested to read the rest of the series as you finish it up. I'm not very familiar with the implementation of a database, so I have a couple questions: - Do "real" databases use the linked list technique you describe for laying out data on disk? It seems like that would restrict the ability to index into the data. - Any reason you couldn't use a rust array instead of a vec and a size field? 
Better performance? example pls
Many databases use for index data B-trees. sqlite also organizes the data pages in a B-tree
When a type implements `Clone` and `Copy`, is `clone()` the same as copying it? As part of the implementation of some generic type I need to copy/clone it and currently I am having one single `impl` block which enforces `Clone` and calls `clone()`. But I guess that misses out on the opportunity to copy those types which also implement `Copy`, right?
While the best answer for conversion has been offered already, might I add another suggestion? Generally for newtypes, since the idea is encapsulating the wrapped type, it's best to simply implement the functionality you desire on the struct rather than getting the functionality by unwrapping the type. Here's an example where I implement Display, Default, and BitOr for your type: ```rust use core::ops::BitOr; use core::fmt::Display; use core::fmt::Formatter; use core::fmt; #[repr(u32)] enum TestFlags { Flag1 = 1, Flag2 = 2, } #[derive(Clone, Copy, Default)] struct TestFlagBits(u32); impl BitOr&lt;TestFlags&gt; for TestFlagBits { type Output = TestFlagBits; fn bitor(self, rhs: TestFlags) -&gt; Self { Self(self.0 | rhs as u32) } } impl Display for TestFlagBits { fn fmt(&amp;self, fmt: &amp;mut Formatter) -&gt; fmt::Result { write!(fmt, "{}", self.0) } } fn main() { let test = TestFlagBits::default() | TestFlags::Flag1 | TestFlags::Flag2; println!("{}", test); } ``` Also, do note there is a [bitflags](https://crates.io/crates/bitflags) crate. It might save you some time!
I lost it at &gt; This program is written in Rust, primarily for speed. Brilliant work
I mean std::io::stdout. There are some nice crates like [console](https://crates.io/crates/console) that work with stdout and have features like [clear\_screen](https://docs.rs/console/0.7.5/console/struct.Term.html#method.clear_screen). But that had flicker issues. Windows console in particular is somewhat notorious for having this issue.
&gt; If your text editor isn‚Äôt able to collapse it along with the rest, it‚Äôs a real drag. This. I really wish VSCode would do this, though i'm not sure whose responsible for it. VSCode or the RLS extension?
Right, rustdoc places the code examples inside the fenced code blocks into their own little crates, so you need to add use &lt;your_crate_name&gt;::increment; &lt;...&gt; before each example. This can be hidden via # use &lt;your_crate_name&gt;::increment; &lt;...&gt; I hope this helps.
Thanks for bringing this to my attention. I just added the link. 
Not the answer I was hoping for ... Thanks anyway ;)
Yea, I already have about 100 of those types and made a big macro with a heck of a lot of those Bit operation impl's. Thank you anyway ;)
rustc -O is opt-level 2, cargo release is opt-level 3
&gt;specialization and const generics come to mind Dumb question: what do you mean by specialization here? 
Thanks! Using it for a repl-like experience at the moment.
Then all I can say is I feel your pain :)
Not a dumb question. Suppose I have some function that is generic over a `T`. When I implement this function in Rust, for any `T` that fits the trait bounds, the same body code is run. But what if I have this super nifty optimization I can do if `T` is `usize`. Specialization would let me write a separate definition for this function that will be called instead of the more generic one.
Something equivalent to Python's comprehensions, for one. Iterators are fine, but they end up being _much_ longer and harder to understand than Python's syntax... they feel like what you end up with in Python if you handicap yourself and use only itertools and generators. And sure, just like throw you can do it with macros and there's some crates that make it available, but then you've got the added cost of boilerplate and / or an additional dependency for something that's just a good idea and a better approach to a given problem. Every time I end up assembling a HashMap with a bunch of chaining and a turbofish collect I want to pull my hair out, as I could do it in Python in at most a fifth the characters and be much more legible. Also anything whatsoever to reduce the boilerplate around generics. Basically just don't resist the things that are beautiful and pragmatic in other languages in favor of a false sense of stability, or an assumption that "we've already got a way to do it" doesn't mean there isn't a better way to do it.
Why does being sized or not matter for being Send?
&gt; I would be interested in knowing which obscure platforms ran firefox before adding rust. Unsupported ones, IIRC. They got lucky, then salty when they realized what "unsupported" means. They were always broken.
That's not about `Send` -- it has to allow unsized types for `dyn Trait` to work.
&gt; unsupported True. I was talking about things like PowerPC Macs running old OSX, OS/2, Windows XP, ReactOS, Haiku/BeOS, etc. Really niche use cases with few users, but with people passionate to support them. My point wasn't that Rust should make an effort to officially support such obscure platforms. I meant that I am hoping that Rust pays more attention to portability and makes it easier to support a variety of different platforms. The official focus should of course be on the various more-widely-used platforms, to address things that actually affect a lot of users, like Linux distros that support a wide variety of architectures. Hopefully, the better Rust's cross-platform support becomes, the fewer bugs it will have and the easier it will be to port it to even more platforms, which means that those people interested in supporting the really obscure platforms will be able to do it with less effort as well.
"the book" helped me a lot as a Rust beginner. It's a really nicely organized material I would say.
Thank you! Yeah, if I was really interested in speed, I would just print the value! Although, rust is fast in this case compared to Python. Regular python code took about 1 minute and 14 seconds while the compiled rust code took about 1.4 seconds. I decided to just drop the python version (you can see it in the commit history). I could have made an attempt to compile the Python code to C, but decided Rust worked good for this. 
I'm afraid not, but I appreciate the effort. I went and looked at the doc tests included in some big crates and they are indeed done exactly the way you're describing this this will not compile on my machine. My crate is definitely called 'lets_doc_test', but I'm still getting 'unresolved import' and 'cannot find function in this scope'. It doesn't work with the crate keyword or a glob import either. /// ```rust /// use lets_doc_test::increment; /// let x : usize = 9; /// assert_eq!(10, increment(x)); /// ``` pub fn increment(x : usize) -&gt; usize { x + 1 }
What does your program do? What did you expect it to do? Knowing little about X11 or FFI, I can only guess that if `XScreenOfDisplay` returns a pointer to a `Screen`, then the resulting `Screen` is managed by the library, and you shouldn't make your own copy of it with the dereference operator (`*`). Change: let screen = *XScreenOfDisplay(display, i); ...to: let screen = XScreenOfDisplay(display, i); ...and see what happens?
Yeah, I was not sure about that to be honest. That is why I put it in the second spot, so it can short-circuit (I don't even now if Rust does this) the OR when it finds the first value to be true. 
Well, I value consistency and simplicity of the language a lot more than syntax sugar. To me, a language is more ergonomic if it is consistent and predictable and simple enough that I can easily keep all the syntax and semantics in my head at all times, even if more verbose and requiring more typing/boilerplate. I would prefer this over a language that offers many different variations of syntax sugar to make it more concise. Such a language would be more complex in its design. Beginners would have to learn a lot more different syntax and experienced users would have to deal with a lot more complexity. For example, I am one of those people against `impl Trait` in argument position. I think that was an entirely unnecessary piece of syntax sugar. Now that it is stabilized, I use it myself, I am not the kind of person to resist using a feature out of spite, just because I don't like its design. It is already stabilized, so might as well, but I personally would have preferred that it wasn't. Another one of the new proposals that I am personally against is Ok-wrapping. Also, despite all its flaws, I really like the C programming language for the simplicity of its design. It is really not hard to be fully aware of all the syntax and features that it offers. Now ... using them correctly and safely is of course another matter. From your other comment: &gt; Basically just don't resist the things that are beautiful and pragmatic in other languages in favor of a false sense of stability, or an assumption that "we've already got a way to do it" doesn't mean there isn't a better way to do it. As I said in my blog post, I like things like the `?` operator which have a major impact on the ease of use of the language. &gt; Something equivalent to Python's comprehensions, for one. Same could be said here, if it can have a really big impact on the readability of the language and is designed well (which I trust the Rust language team is capable of). I am only against minor cosmetic improvements, such as `impl Trait` in argument position, Ok-wrapping, `throw`, and other such things. It is arguable how much these actually improve ergonomics (I personally think they don't), but they *do* add complexity to the language.
&gt;let screen = XScreenOfDisplay(display, i); &amp;#x200B; E0609: no field \`width\` on type \`\*mut x11::xlib::Screen\` help: \`screen\` is a raw pointer; try dereferencing it
The Program should output Total count screens: 1 Screen 1: 1920X1080 The program should find the current screen resolution. What happened was it failed to compile. If you want to play with the code that's all of it. Just throw [package] ... edition = "2018" [dependencies] libc = "0.2" x11 = "2.18.1" in your Cargo.toml
The Program should output Total count screens: 1 Screen 1: 1920X1080 The program should find the current screen resolution. What happened was it failed to compile. If you want to play with the code that's all of it. Just throw [package] ... edition = "2018" [dependencies] libc = "0.2" x11 = "2.18.1" in your Cargo.toml
I woke up wanting to suggest this and see that someone already has. I really like this syntax but it will face a lot of opposition, because if the awaited value is not a Result, you end up with four dots, like `client.get(url)....body` which is weird.
The best way to answer this question is to write a Rust program and benchmark it. If it's as fast as you hope, then you're done. If it isn't, then ask others if you're doing anything wrong and we can hopefully explain how to reach your performance goals. Ideally, it would be good to provide an additional benchmark that shows that your target is reachable. Otherwise, I don't think it's really possible to answer your question in any specific way. The generic answer is, "yes."
See ya later, allocator
I suppose that depends on your meaning and measure of complexity; Python is a more complex language than Rust, in both its implementation and its almost numberless ways of doing things -- including many _stupid_ things, and yet it has fewer keywords than Rust and remains _much_ easier to learn for a beginner. It's still quite manageable to keep in memory precisely because it's adopted so many constructs and idioms that have been worn smooth over the years by use. Those idioms supplanted older, less well-rounded ways, so yes the language becomes more complex, but at the same time the old idioms just stop being used and eventually stop being learned altogether... so the complexity is in a sense self-managing. If, for instance, Rust had always had a decent C++-style ternary operator and Python-style comprehensions, nobody would miss the old way or be currently arguing to add the denser, bulkier, and less ergonomic form into the language ... the greater complexity would have been worth the investment _all along_ and no one would question the value because they'd already have enjoyed it. As it is a nearly-perfect language lacks a lot of refinement that _works_ elsewhere and there's this apparent well of resistance to adding that refinement out of a fear of complexity somehow ballooning too far out of control. Personally I'm glad there's been the addition of the Edition mechanism and I look forward to a few more passes on making this language a lot more accessible and ergonomic (and therefore long-term sustainable) than it is, though I do understand not wanting lots of tiny little aesthetic shifts in areas that don't have a clear bang for the buck.
You've already posted in the correct subreddit for the game "Rust": r/playrust. Why would you crosspost it here, the subreddit for the programming language Rust? 
This seems like a problem that is very dependent on how the data you're sorting is organized, and each stdlib might pick a different implementation that may not necessarily be the best choice for your data. I'm pretty sure there must be a C++ sorting implementation that is just as fast as anything you can can find around. There's probably one in Rust and even in Java. I think the best way for you to test how much the languages, or really the compilers, would be to measure how fast the SAME (or as similar as possible) implementation is on each language.
The one that's been ongoing for over a decade of bitter tears and internet chatroom fights? I jest, but only partially :)
Hmm, okay. The `crate`-keyword does not work because it refers to the anonymous crate generated by rustdoc. Are you using Rust Edition 2018? And Rust 1.32? Is it really `lets_doc_test::increment` or is it `lets_doc_test::A::increment` (or similar)?
&gt;I can't think of anything fundamental. First things which come to mind: 1) [Anonymous enums/true unions](https://internals.rust-lang.org/t/pre-rfc-sum-enums/8782) which will make error handling a bit nicer and will allow to return several types as `impl Trait` without boxing. 2) [Target restriction contexts](https://internals.rust-lang.org/t/pre-pre-rfc-target-restriction-contexts/7163) or similar functionality which will make SIMD and handling targets in general more reliable and greater to write and use. 3) Inherent trait implementations. 4) Delegation and fields in traits. 5) Borrow regions/partial borrows. 6) Negative trait bounds. The need for them can be partially remedied with specialization, but in many cases it will be much nicer. 7) Self-referential types. (`Pin` is nice and all, but very limited)
Could you share your benchmark? 
Thank you for commenting :). &amp;#x200B; /u/snsmac is correct, most databases use b-trees throughout for almost everything, from indexes, to blocks, and I believe each table in sqlite is a giant b-tree. However, b-trees are fairly difficult to implement (at least, in comparison to the linked list structure I have here), so it's a tradeoff of spending a lot of time getting that working, or making something that works well enough and moving on to the next topic. If you think I've made a weird tradeoff in the interest of time with this instance, you should see what I did to store table rows :). Since I don't have much time to devote on this project, I'm trying to make solutions that are "good enough for now" and are also cleanly separated so that I can go back later and replace them with something better. &amp;#x200B; I used a vec because I wanted to make the block size configurable. That and stack-allocating the array seemed like it would lead to issues in the future. &amp;#x200B; No idea why the resume isn't rendering for you. It's "just" a pdf: if your browser can display them, it should work.
If a type implements both, then they will typically be the same. I believe they are identical in all std types. It is *possible* to have a `Copy` type not be `Clone`, or to have the `Clone` implementation be something different, but I can't really imagine a reason to do so. I'd say it's fine to just rely on clone like you said.
This is light but we still need to write header files, put a bunch of annotation on the rust code, and expose functions that take libc types. It would be nice to have that code generated from annotations. Is there a project that does that ?
IIRC, it was possible to compile the Linux kernel in 10 seconds using TCC on a Pentium 4.
After adding a `main()` to call your `resolution()`... fn main() { resolution(); } ...and working around what appears to be a bug in the `x11` crate by using `cargo rustc -- -lX11` as my build command, I got this output: Total count screens: 1 Screen 2 -864819360x21882 The fix for that was to move `i += 1;` to the end of the `while` block so you're not trying to call `XScreenOfDisplay` on a nonexistent screen and receiving garbage data in return. ssokolow@monolith rust_test [master] % ./target/debug/rust_test Total count screens: 1 Screen 1 4480x1080 Here's the working code with a few tweaks to make it more like I would have written: use x11::xlib::{XCloseDisplay, XOpenDisplay, XScreenCount, XScreenOfDisplay}; use std::ptr; pub fn resolution() { unsafe { // open a display let display = XOpenDisplay(ptr::null()); // return the number of available screens let count_screens = XScreenCount(display); println!("Number of screens: {}", count_screens); for i in 0..count_screens { let screen = *XScreenOfDisplay(display, i); println!("\tScreen {}: {}x{}", i + 1, screen.width, screen.height); } // close the display XCloseDisplay(display); } } fn main() { resolution(); } 
Also [Exercism](exercism.io), but it has been a while I‚Äôve last seen the Rust learning track over there.
&gt; People are constantly writing their examples with hardcoded paths that assume the binary is being run from the same folder the Crate's top-level cargo.toml is contained in, which is something that straight-up never happens because the example binaries always automatically get put in a specific subdirectory of the "target" directory by default. So? Running binaries from the directory that they're contained in is something that rarely ever happens. If it's a mistake to assume that the example is being run from the Cargo.toml directory, it's equally erroneous to assume that it's being run from target/&lt;configuration&gt;. Furthermore, where does `cargo run` run the binaries from? Without testing it, my money is on the Cargo.toml directory, not the target directory.
&gt;So? Running binaries from the directory that they're contained in is something that rarely ever happens. What? &gt;If it's a mistake to assume that the example is being run from the Cargo.toml directory, it's equally erroneous to assume that it's being run from target/&lt;configuration&gt;. Are you proposing some third option, then? &gt;Furthermore, where does cargo run run the binaries from? Without testing it, my money is on the Cargo.toml directory, not the target directory. I've never used `cargo run` even once. Literally ever. To me it's infinitely easier/more ergonomic/whatever you want to call it to just do something like `cargo build -j16 --all-targets`, wait a couple minutes, and then browse through the all-fully-built examples one after another at whatever pace I want. Even if it turns out I'm the only person in the world who does this (which I doubt) it still makes no sense to me how anyone would think a hardcoded path I*not* relative to the guaranteed-actual-location of the binary was the right way to go.
I guess I've been using Rust long enough because I could spot the problem from the very first code example. However, I think it's because 1) I knew where to look for it because of the title and the prelude, and 2) I've been bitten by similar mistakes several times before. It's unfortunate that Clippy doesn't have a lint for a case like this; on the surface it seems pretty straightforward: iterator producing `JoinHandle` going into a loop that waits on each handle in-turn. However I'm hesitant to recommend it as a lint for Clippy since they've got a massive backlog of issues and PRs already. It might be time to branch out and start working on a parallel project for more niche lints that could still save people's butts.
&gt; What? What part of my statement was confusing? There's a difference between the directory that a binary is contained in and the working directory at the time of execution. In the real world, it's rare for them to be the same. &gt; Are you proposing some third option, then? No, I'm proposing that we look to the official tooling's behavior for guidance, which is "examples and main get run from the crate root."
https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/php-node.html https://benchmarksgame-team.pages.debian.net/benchmarksgame/faster/php.html TLDR: Mostly slower than Node, but faster than Python.
I also do what you're describing. To me `cargo run` seems like something designed by a JavaScript developer who has not quite fully wrapped their head around how slow Rust build times unfortunately are. Coupled with the additional facts that in many cases people only have one example out of several that actually properly shows what their crate can do, or in worse cases only have one (or none sometimes!) that actually work properly at all, I've never seen it as anywhere close to worthwhile to `cargo run` them individually (which of course *also* requires you to open the TOML and arbitrarily choose one based on nothing other than the filename.)
Ugh. Language benchmarks are bullshit, they always compare apples and oranges. And whatever source I cite, someone will have some argument against it. It's better you pull your own conclusions. Example: https://www.techempower.com/benchmarks/#section=data-r17 swoole is #4 on this list. Swoole is a "framework" (PHP extension that adds an event-loop among other features) that makes the comparison between Node and PHP a bit closer in terms of how the benchmarked code looks. I'm sure someone will have objections to this, and that's okay, because you should take benchmarks with a grain of salt anyways.
What is the point of MPI, when there exist Spark, Flink, Heron?
The kind of bizarrely specific assumption you're making is the exact kind of assumption that keeps us all consistently out of arms reach of a Cargo that does not just happily download, build, and link duplicate copies of Crates simply because "that's what the SemVer syntax told it to do." You and I might be very familiar with the not-especially-logical specifics of semver, for example, but I think it is a huge mistake to assume that the same is true for anywhere *close* to the majority of Rust users. More broadly, as another example, personally I'm *very* convinced that the *majority* of Rustaceans think that crate dependencies specified like `SomeCrate = "1.9.7"` literally means "use specifically that crate version, and nothing else." This assumption is highly reasonable IMO: the *actual* syntax for that functionality is extremely arbitrary with no particularly good reason to have been thoroughly studied by anyone, and frankly I think it is insane to assume that everyone in the world is somehow magically aware of it as soon as they start using Rust.
Weird answer, but as someone who's been learning and using Rust recently for a high-performance server application (and really enjoying it), but with no C or C++ background, I can certainly see myself jumping ship as Swift matures over the next 3 years. Swift has stolen most of the things I like about Rust, the story for using it outside the ecosystem is growing, and it's substantially easier to use. It's difficult to see Rust getting easier to use before Swift gets better for the server.
&gt; To me cargo run seems like something designed by a JavaScript developer who has not quite fully wrapped their head around how slow Rust build times (currently) unfortunately are. I'm not sure how rust build times factor into this. The example that you're telling cargo to run won't get inexplicably rebuilt every time you `cargo run` it. You can even build all of the examples ahead of time with `cargo build --examples`. &gt; (which of course also requires you to open the TOML and arbitrarily choose one based on nothing other than the filename.) Why are you deciding which random program to run based solely on the filename? Shouldn't you read the example code first? If deciding which example to run is slow for you due to having to open the Cargo.toml rather than trying to understand how the example works, you might be doing it wrong.
It's pretty dumb that Cargo has any influence on the "current working directory" IMO. If the CWD was relative to the example binary there would likely never be an issue at all, because it would mean that even with an `images` folder in the crate root directory you could run an example through Cargo and it would still find them as long as it was looking for a relative path.
Wrong thread?
Even when I don't run examples using `cargo run`, I almost always run `./target/debug/examples/xxx` from the crate root. I might be being an idiot, but why would you ever `cd` into the target directory? I can get not using `cargo run`, but I don't get why you'd want the binaries to be dependent on being run from the target dir either. I can totally get behind using `walkdir` to find the assets directory but if you're hardcoding a path, why not hardcode the path 99% of users will be running the binaries from?
When you program large super-computers, being able to directly dictate cache-locality is a huge part of writing simulations. People professionally write these programs on behalf of physicists and scientists that hand of their formulas. I agree Julia's numerical computation story is more advanced, but not it's HPC story. Rust is also much closer to having a subset with which GPU computer-kernels can be written. If you are writing for SIMD, Cores, GPU-Compute Kernels, and further distributing across many nodes, then you are in a far better position to be writing actual HPC code. Yes man grad students write their own, but larger professional projects are handled differently. Especially on non-public* access super-computers. *public meaning open to students to freely play with and schedule non-critical computations on. I don't mean to suggest MPI is the write distributed architecture for today, it is however the standard for use with C/Fortran on most super-computers today.
Having the CWD be relative to the binary would be cargo exerting *more* influence on the CWD than it currently does. As it stands, cargo sets its own working directory to the crate root. Everything else that it does happens from there. Are all the people having trouble with this actually `cd`'ing to the `target/whatever/examples` directory so they can `./the_example` rather than just running `target/whatever/examples/the_example`? Or is this a Windows user problem where everyone's opening the target directory in Explorer so they can double-click the `.exe`?
The difference between static massively parallel computation distribution vs. elastic fault tolerant concurrent data stream distribution I guess? I'm not hugely familier with Spark, Flink, and Heron, but they seem to be different distribution architectures for solving entirely different problem domains on and entirely different technology stack.
I'm beginning to suspect that the users expecting the binaries to be run from the target directory are opening the target directory in their file explorer and clicking the binary to run, because you're right, `cd`'ing into the target directory doesn't make much sense.
&gt;To me cargo run seems like something designed by a JavaScript developer who has not quite fully wrapped their head around how slow Rust build times (currently) unfortunately are. What's the faster way to build and run a binary crate?
Answer with a more concrete example. You know the [`Iterator` trait?](https://doc.rust-lang.org/std/iter/trait.Iterator.html) What happens if you call `.next()` after the end of the iteration? Well-behaved iterators will continue to return `None`. But this isn't *required* behavior. It's considered correct to return anything (as long as it's safe). There's also the `.fuse()` method. This returns an iterator that keeps track of the end-of-iteration state. Once iteration ends, it always returns `None`. This method is implemented generically - it wraps the parent iterator along with a `bool` field that notices the end of iteration. Now, what if you know the iterator is well-behaved to start with? In Stable Rust you have to return a `Fuse` value. And the `Fuse` type must be parametric over its type parameter - it can't know anything about the parent iterator except that it implements `Iterator`. So there's no way to avoid adding the extra machinery of `Fuse`. At least, not in Stable. Enter specialization. This is what `Fuse&lt;T&gt;` looks like in the library today: #[stable(feature = "rust1", since = "1.0.0")] impl&lt;I&gt; Iterator for Fuse&lt;I&gt; where I: Iterator { type Item = &lt;I as Iterator&gt;::Item; #[inline] default fn next(&amp;mut self) -&gt; Option&lt;&lt;I as Iterator&gt;::Item&gt; { if self.done { None } else { let next = self.iter.next(); self.done = next.is_none(); next } } .... } and also #[stable(feature = "fused", since = "1.26.0")] impl&lt;I&gt; Iterator for Fuse&lt;I&gt; where I: FusedIterator { #[inline] fn next(&amp;mut self) -&gt; Option&lt;&lt;I as Iterator&gt;::Item&gt; { self.iter.next() } .... } If the parent iterator (type `I`) implements `FusedIterator`, then both implementations could apply. The compiler chooses the most specific. With the more specific implementation `Fuse&lt;I&gt;` still has a `.done` field but it's never accessed. And often the optimizer will be able to remove that field. (If the `Fuse&lt;I&gt;` is held in a local variable this is easy. If not, it will probably not be optimized out.) Specialization still needs a lot of work. "Most specific" turns out to be *quite difficult* to pin down exactly. It may be nice if data layout can be specialized (removing the unnecessary `.done` field in the front-end of the compiler, not hoping that the back-end can remove it). And there's the usual bike-shedding of course. In a nutshell: specialization is the set of rules necessary to make sense of multiple competing generic implementations, and possibly other situations where a generic type could benefit from knowing about its type parameters. The downside of specialization (and why it wasn't prioritized more) is that it opens up a can of bugs where a generic type acts differently depending on its type parameters. These bugs and non-obvious behaviors can in principle be as troublesome as inheritance was in earlier OO languages - and for basically the same reasons. 
I can't find what you are quoting. Where is it?
Yeah, I think this is mostly a case where I need to keep strengthening my mental models about the various pieces and how they fit together. Also how to tell a story without giving it away too quickly ;) I can‚Äôt dream up a good (i.e. not overly clever/tricky) reason to be lazily spawning threads in an iterator, so I could see a lint making sense.
Rayons par_sort should be pretty fast.
High performance sorting is equally possible in Rust and C++. I don't know Haskell, R, or Julia well enough to know if they'd be as fast but they're not going to be significantly faster. Any large differences between standard implementations in languages that can achieve very low abstraction are going to be because of differences in tradeoffs.
That depends on the sorting algorithm used. For example, Quicksort has a good avg performance compared to other comparison-based sorting algorithms. However, in the worst case, its performance is O(n¬≤). Mergesort needs more memory, but has a better worst-case performance. Bubblesort is usually slow (O(n¬≤)), but it's very fast if the data is already *almost* sorted. Radixsort or Bucketsort can be an order of magnitude faster than Mergesort or Quicksort under certain conditions. However, they aren't comparison-based. More infos here: [http://bigocheatsheet.com/](http://bigocheatsheet.com/) Another way to sort elements is to insert them into a hash table and retrieve them in the correct order. If the hash table can insert and access items in O(1), the sorting has linear runtime (however, it needs a lot of memory). This algorithm isn't comparison-based as well. Highly optimized sorting algorithms often combine various algorithms. For example, you can divide an array into slices, sort each slice individually, and then merge them with Mergesort. Or you can decide which algorithm to use depending on a heuristic, e.g. the size of the array. \------ Rust has a [sort\_unstable()](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.sort_unstable) method for \`Vec\`, which, according to the docs, is a modified version of Quicksort with a better worst-case performance. It also has a stable [sort()](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.sort) method, which is a modified Mergesort inspired by Timsort. stdlib only supports comparison-based algorithms because they are very easy to use with any type. If your data can be sorted faster by using a different algorithm, you probably have to implement it yourself.
It was in the README, but it has since been updated. You can still find it if you look back in the commit history though.
I don't think rust has an opt-level 3, and cargo release just passes `-O` to rustc.
Well, yes. Of course. I really thought that was obvious.
The amount of downvoting and implicit command-line elitism in this thread is so, so, so embarrassing. Now I understand why people seemed to legitimately *impressed* when Cargo added the little ASCII progress bar as though it was some kind of technical achievement. I don't mean to be rude but please try to recall what year it is, folks.
I'm a diehard lifelong Linux user of countless distros dude, and I definitely don't "CD" everywhere. Not even close. I click the friggin' folder icons because it's faster. You have no idea how crazy it sounds to me that you seem to be saying switching directories from the command line is *obviously* the "normal" way of doing things.
No, specifically the right one.
Sorting does not have to do with the "algorithm" part, because of how the CPU Hardware works ("caching"). You sort the quickest with the slowest algorithm, typically for n=20.
R and Julia work different. R is optimized for numeric stuff, C++ sorting is usually/often not optimized for speed. Julia is a JIT lang.
The fastest C++ algorithm isn't ported to other languages, seems in part because doing so is difficult. Porting a slow sort is meaningless.
Simple answer: running the example binaries with anything other than `cargo run` (which is run from the crate root) is a nonstandard use case. Cargo assumes that you use the tooling provided, and is configured for that majority case.
Here's an interesting question though: should iterator adaptors that advance internally silence the lint?
Cargo is an exclusively command-line tool. Most crates (and most examples across crates) are exclusively command-line tools. There is no elitism in assuming that the overwhelming majority of users will be operating Cargo (and related functionality) from the command-line, as it was designed to be used.
Really? You *suspect* it, like it's some kind of vague possibility? Responses like almost all the ones in this thread are what consistently tanks my long-term faith in Rust gaining widespread adoption. There's just *too many* people in the Rust community who are **way** too concerned with how awesome they think they are for intentionally doing things the hardest way possible because they think it makes people think they're cool.
Thanks, very interesting suggestion. I have found the following benchmark: [https://www.reddit.com/r/rust/comments/6jt4l9/rayon\_gets\_parallel\_sorts\_benchmark\_against\_java/](https://www.reddit.com/r/rust/comments/6jt4l9/rayon_gets_parallel_sorts_benchmark_against_java/) Super link. From those numbers Rust is slower than R or Julia for the single threaded case (my guesstimate), because of the comparison with std::sort (which r or Julia should beat by a margin). But interesting to look at the parallel case for the type of data that I sort.
I didn't say anything about C++. Write what you want as a Rust program without regard to performance as a start, so that folks can at least understand what semantics you want.
It has nothing to even DO with Cargo. Rust examples are literally just arbitrary programs that can do absolutely anything they want. They should behave like normal programs behave. Holy shit.
&gt; Especially if it's a particularly long/hard-to-remember Crate name or something that you're not really familiar with yet. tab-completition? 
Yes. And a majority of normal programs are either exclusively command-line or have no interface at all. That is the "default state" for *any* software; someone has to put non-trivial amounts of work in at some level to make it a GUI program.
This is a ridiculous conversation. I just build *fucking everything* and then look at the *working* examples, once, well, *built*, to see if I'm swayed by what the author chose to put forth as a demonstration. There's no academic / scientific aspect to the deliberation process.
[I just posted a summary comment.](https://github.com/rust-lang/rust/issues/57640#issuecomment-455836523)
No need to go on a tirade. I'm sure u/ComicBookGuy1991's didn't mean to come off as arrogant, rude or anything like that. 
If you're saying it will take 20 years to overtake C/C++ you might as well be saying it will never happen at all. 
Cargo examples are designed to be run via `cargo run`. That's a fact of Cargo, `cargo run` is how you run Cargo examples. Any other use case is non-standard. It definitely would be better if examples work when run in any directory, but this adds significant complexity to examples built for being read by potential consumers of the library, and probably isn't even possible if not being run through Cargo, as then you don't have environment variables to point you to the crate root. Also, most examples, if they output anything, will output it to the standard output `println!(..)`, so running it via the file explorer won't even do anything (other than succeed or fail).
Location of the binaries in the `target` directory is not guaranteed (though it's unlikely to change); it's an implementation detail of `cargo`. `cargo run --example` is the only official way of running Cargo examples.
I think this thread has gotten a bit derailed. In particular, I believe it may in part be caused by a lack of understanding of each other's workflows. I'd like to ask u/NotLadyIris2ISwear, u/ComicBookGuy1991 and u/TanhouseAle if you could kindly walk us through your workflows so that we may better understand one another. Since I'm the one asking this of you, I'll go first. &amp;#x200B; First I would open up my file browser and navigate to the place where I do all of my coding stuff and open a terminal there. Then, I would clone the repository. `&gt; git clone &lt;repo&gt;` Once that's done, I would open the local repository in VSCode and close the terminal. `&gt; code &lt;repo&gt;; exit` Since there's an integrated file explorer in VSCode, I'd use that to navigate to the examples directory and look at what available. Once I've picked an example that seems interesting, I'd open up the source code and compile the example with `cargo build --example &lt;example-name&gt;`. From there on out, I would usually just use `cargo run --example &lt;example-name&gt;` to run the examples one by one.
No, there's definitely a reason to go on one. Half the responses since my original comment in this thread are even worse: Cargo is **not** that big of a deal people, and it is **not** (or at least shouldn't be) relevant to the issue OP was addressing.
The behavior of Cargo to default to semver-compatible is not only shared by other build tools, but makes _more_ programs work. Imagine a world where one library specifies `serde:1.0.55` and one specifies `serde:1.0.56`, and they can't interop because they see incompatible `Serialize` traits. The idea of versioning is that it shouldn't matter if you get a higher versioned compatible version. If a nominally compatible version isn't, that's a bug. (Publishing via Cargo means respecting Cargo's expectations.) That said, how is that relevant to the cwd of programs? Checking my Windows desktop, _all_ shortcuts do _not_ set the cwd of programs they launch to the directory that the executable is in. The correct behavior for a relocatable binary is, in any case, to ask the OS where it should put stuff and look for stuff rather than just using cwd.
My interest was piqued by this question, so I started digging around. A summary of the what I found is that the structure of the data you are sorting matters a lot- some algorithms are great at random data but lousy with partially sorted data, or vice versa. Also, failed branch predictions apparently are a big thing and modern sort algorithms that use branchless comparisons can apparently beat C++ std::sort by 2x. You may find this thread interesting: https://news.ycombinator.com/item?id=14661659 It certainly appears that Rust is using state of the art sorting algorithms (pdqsort for unstable and modified timsort for stable). I‚Äôm unsure whether Rust takes advantage of branchless comparisons but comments I read indicated that it might. 
Here's a question: would you prefer the examples of a library primarily use the library in question, or primarily do a lot of hard work to be fully relocatable and not using even cargo environment variables to know where stuff is (since the way to run the Cargo example is apparently directly from your file explorer rather than through Cargo)? Ultimately, Cargo examples are primarily there to be _read_, and that means that they often make simplifying assumptions (like being run by `cargo run --example`) to avoid bogging down the reader. If running an example required packaging and installing a fully relocatable program, I'd use some other library. There's going to be a lot of useless environment management in that code that I don't care about; I only really care about how to use the library.
I'm sorry, but this is one of the most backwards things I've ever heard. It is simultaneously assuming that every Rust user is an utter moron while **also** catering to the *least* user-friendly and obvious use case there is. I really did **not** expect the weirdo rules-lawyering about Cargo when I made this thread. I really don't think Cargo is even *that* relevant to the issue to start with, nor do I think it is some kind of infallible measure of how to do things. It's just a pretty good package manager / project builder application, nothing else. Is there no one out there who is actually worried about how very much unlike the kind of atmosphere things like the new Rust website are advertising the real Rust community often turns out to be in practice?
I'm sorry, but you seem to be projecting your aggression onto others. Yes, most of the pro-Cargo voices in this thread have been arguing against your position, but I've not seen any aggression from them, just presentation of norms as they exist. Cargo packages are designed to be used through Cargo. That's sort of a fact of build systems. It's also a fact that most library crates aren't going to have graphical output. They're ways of processing data. What would you expect a [rand](https://crates.io/crates/rand) example to do, other than maybe print something random to the standard output? What about [syn](https://crates.io/crates/syn), [lazy_static](https://crates.io/crates/lazy_static), [log](https://crates.io/crates/log), [bitflags](https://crates.io/crates/bitflags), or any other of the [most used crates](https://crates.io/crates?sort=recent-downloads)? If you have an example of an ecosystem that does this "right", I'd love to hear it. All the developer ecosystems I've seen have their library examples built for stdout output and assume being run through the package manager you use to install the library (if they make any assumption about environment at all, which I will note is required if you are using any resource that is not compiled into the binary). Applications need to be relocatable. I honestly think that examples of library usage should be allowed to make some simplifying assumptions to make the reading of the example more about the library usage and less about the environment.
&gt;I'm sorry, but you seem to be projecting your aggression onto others. Yes, most of the pro-Cargo voices in this thread have been arguing against your position, but I've not seen any aggression from them, just presentation of norms as they exist. No, it's more like everyone is trying to have an argument that doesn't actually even address what I was talking about (as in, crates that have examples that are NOT command line apps) &gt;It's also a fact that most library crates aren't going to have graphical output. In what way do you think this matters as far as this discussion is concerned? &gt;What would you expect a rand example to do, other than maybe print something random to the standard output? Again, those kinds of crates are not the ones this thread is about. Did you even read the title?
[Here](https://doc.rust-lang.org/cargo/reference/manifest.html#examples)'s the part in the official cargo book that talks about examples. Specifically: &gt;You can run individual executable examples with the command `cargo run --example &lt;example-name&gt;.` It does not in any way mention any other way of running examples.
Even if you're seriously putting that forwards as proper documentation, do you think that snippet has in mind the OP's particular gripe about *graphical* examples that *load stuff from file?* I doubt it.
https://www.reddit.com/r/rustjerk/comments/ahuufw/my_reaction_to_await_future_vs_futureawait_debate/ :D
Even if you don't care about the other crates, it remains that they're the common case that Cargo is optimized for. And a graphical example failing under `cargo run --example` because it expects its cwd to be the binary directory would be just as surprising. On this we'll just have to agree to disagree. The common case is `cargo run`. I wish you luck in your search for better examples. Perhaps you could consider improving some along the way, and making the ecosystem better for everyone. ‚ô•Ô∏è
:D (Note my scare quotes around "real" method; there'd be a lot of magic around the `extern "rust-await"`.) (I made the reference to the "why not both" generalization in my summary a bit softer, I hope that represents your position a bit better.)
I don't expect huge changes in the language itself, but I do expect large changes in both the community and tooling. Rust has a much better foundation to build on than other languages in the same space. It just needs to mature and grow a bit more in tooling and ecosystem.
&gt; No, it's more like everyone Exactly. You're alone prefering your use case. So crate authors are going to keep writing for the 99%.
&gt; There is no elitism in assuming that the overwhelming majority of users will be operating Cargo (and related functionality) from the command-line, as it was designed to be used. Even if your IDE has a GUI for launching cargo commands, it will still lauch them from that same directory so it doesn't change anything.
It was a fairly recent change, the documentation might not be fully updated yet
Are you fucking kidding me?
`cargo run --example &lt;example-name&gt;` is the way the documentation tells you to run examples. When you do it that way, the current working directory becomes the crate root. It is therefore reasonable for example authors to assume that the working directory will be the crate root. This holds true regardless of whether it is a command-line or graphical examples, since both can be launched just fine by `cargo run --example &lt;example-name&gt;`. If there's something I've failed to address here, then I'd ask you to kindly tell me what it is (if you're still up for it).
Thanks! It is a really good post, BTW.
What platforms do you care about? - [`cargo-deb` for Debian](https://crates.io/crates/cargo-deb) - Actually I don't know any others `cargo install` exists, but that is not designed as a distribution method, but rather a convenience. It also doesn't do much. [ripgrep](https://github.com/BurntSushi/ripgrep) has the advantage of being pure-Rust and thus fully statically linked, but you could look at their scripts for inspiration.
The best way, that I know, is to use a second thread which does the actual `read`ing and a third thread with a `thread::sleep` inside it. In your main thread you'd wait for one of them to return something (e.g. through a tx/rx pair) and then stop the other. Pseudocode: main: Thread a: Thread b: loop { tx_a.send(read()) thread::sleep(Duration::from_secs(1)) if let Ok(text) = rx_a.try_recv() { tx_b.send(()) // stop thread b, break Some(text) } if rx_b.try_recv().is_ok() { // stop thread a, break None } } This works (if it works) because try_recv doesn't block, so this loop constantly checks which thread finishes first. You might wanna insert a `thread::sleep` with e.g. a millisecond, so that the loop doesn't use up 100% of a cpu core. This might not be the best way, it's just the first thing that I could think of. Let me know if this works, as I've never done anything like this.
You aren't helping your case. Try to stick to things you can back up with (hopefully objective) reasoning rather than ad hominem attacks.
If a crate example does not load anything from an external file (as in, it is a full-on command line app), then it **does not matter at all** what directory it is run from. My point literally cannot be applied to anything other than the kind of examples I described in the thread title. There is **no problem that needs solving** with other types of examples.
What hominem attack? The majority of people in this thread find the current behaviour more intuitive. It doesn't mean there's something wrong with OP but it's normal that crates are written the way a majority of people like better.
What I'm trying to say is, it's not a language issue (except very high-level or interpreted languages, like Haskell and Julia may be at a disadvantage). Sorting is almost entirely memory lookups and branches, both of which are slow operations that can't be optimized away. With a well-written implementation and any vaguely production quality compiler, choice of algorithm is going to make all the difference.
Your initial reply doesn't add anything to the discussion. (Though ad hominem was probably the wrong thing to call it.) Disparaging a real use case (examples of more complicated crates like Amethyst or GGEZ) isn't helpful. I definitely agree that examples should be able to assume being run through cargo to simplify the examples (they're meant to be read, after all), but pretending that complicated asset-dependent examples don't matter as much isn't the way to make progress in a thread about asset-dependent examples. "`cargo run --example` is the normal way of running examples, so it makes sense if developers optimize for that" is a much more constructive comment than "You're alone in running example executables directly". I hope you can see what I'm saying there.
How would I stop `Thread a`?
You should use cmake in this case. There is also `build.rs`, but that is less useful.
What is it you even think I'm talking about? What does "GUI" have to do with anything?
from [cargo docs](https://doc.rust-lang.org/cargo/reference/manifest.html): ``` # The release profile, used for `cargo build --release` (and the dependencies # for `cargo test --release`, including the local library or binary). [profile.release] opt-level = 3 debug = false rpath = false lto = false debug-assertions = false codegen-units = 16 panic = 'unwind' incremental = false overflow-checks = false ```
*He wasn't complaining about command-line crates. Command-line crates are not relevant to this thread at all.*
I didn't take that into account. There isn't a way of doing that in the stdlib. In that case the only thing I can think of is using `futures` or `tokio`. `rayon` might also be possible here, but I'm not sure about that one. I've never used either of them personally.
You've failed to address that "the way the documentation tells you to run examples" is not some kind of fucking gospel, for one thing. You need to realize that many people do NOT do everything in this strictly by-the-book way you appear to. You've also failed to address the fact that the problem of where *external files are located relative to a running applicaton has NOTHING to do with Cargo or documentation or anything official, and is a completely solvable problem directly by crate authors.
You've failed to address that "the way the documentation tells you to run examples" is not some kind of fucking gospel, for one thing. You need to realize that many people do NOT do everything in this strictly by-the-book way you appear to. You've also failed to address the fact that the problem of where *external files are located relative to a running applicaton has NOTHING to do with Cargo or documentation or anything official, and is a completely solvable problem directly by crate authors.
&gt;If a crate example does not load anything from an external file (as in, it is a full-on command line app), then it **does not matter at all** what directory it is run from. A full-on command line app can still load stuff from external files (git, cargo, etc). This goes for examples too. Just a little nit-picking.
Yep. Cargo currently lacks post-build tasks, so you'd either need CMake or [cargo-make](https://crates.io/crates/cargo-make).
Thanks, this is a helpful comment
Awesome!! Thank you so much for your help!! Do you know how I could build this in a project with multiple files in its src directory? &amp;#x200B;
Just to check my knowledge from the first code example, it's the lack of 'collect' at the end, right?
I think you mean to say that the fancier the algorithm, the higher the up-front cost, which hurts when sorting short collections. However, you say that without regards to actual measurements. As Kirk Pepperdine always says, "Measure, don't guess!"‚Ñ¢ (and yes, he has that trademark registered).
I use PHP 7 on some systems at work, it's still a horrible language. Yes, you can write perfectly good code with it. But there's too much shit allowed in the language s.t. you can totally shoot yourself in the foot. Not to mention, the "good" way of doing things isn't exactly the blessed way of doing things. There are so many languages out there which are just safer/more consistent by design. There's many many things wrong with PHP.
The thing is those are great learning resources, but they kind of don't help you if you want to explore other modules, or if you hit a specific problem where you know you can't do that. "The book" will leave you with a perfect understanding of why you can't do that, but not with how you can do what you want to do. That's always a bit case specific.
I also dream of that.
Yes. The range is lazy by default so it doesn't run any code until iteration starts, which is why the .collect() on map is done, it turns it from lazy into a fully-ran code segment.
I really like this both for its conciseness and symbolically the "..." as English language symbol represents a sort of pause for something else to occur (just like async behavior is!).
&gt; It is possible to have a Copy type not be Clone `Copy` requires `Clone`: pub trait Copy: Clone { }
This thread is exhausting. Anyway. Here's a snippet that would (hopefully) solve the issue [u/ComicBookGuy1991](https://www.reddit.com/user/ComicBookGuy1991) is having with example programs as a peace offering: fn force_example_cwd_to_crate_root() { let current_dir = std::env::current_dir().unwrap(); let current_exe = std::env::current_exe().unwrap(); let current_exe_dir = current_exe.parent().unwrap(); if current_dir == current_exe_dir { std::env::set_current_dir("../../..").unwrap(); } } fn main() { force_example_cwd_to_crate_root(); } This would make the example's working directory always be the crate root, unless you're doing something really funky. This means that OP is free to launch the executables from either `cargo run` or by navigating to them in a file browser and running them that way. &amp;#x200B; It could also probably be included in some custom `cargo` subcommand that temporarily adds the snippet above to examples just before they're compiled, invokes `cargo build --all-targets` and then removes it afterwards.
Does cbindgen match what you're imagining?
Definitely feasible, though with the gamedev story of Rust still being in its nascent stages you should be prepared for a lot of rough edges in the learning material and a slightly limited libraries pool. You‚Äôll want to lookmat ggez, Quicksilver and Tetra. All excellent 2D game frameworks!
Thanks! I will go with the current solution then.
&gt; Rust has already corroded Assuming you mean "Rust has become part of / deployable to," can we just use "oxidised?" "Corroded" has negative connotations.
People will only run one lint. Add the issue.
If the tooling(editors, debugging, code completion etc.) improves in the coming years, then it might get more traction. Currently its mostly early adopters and a few large players. I wouldn't be able to sell it to my collegues without the improved tooling.
you might give this one a try https://github.com/amethyst/amethyst but be careful, you might have to rewrite your entire codebase in later versions üòí
Oh, that's possible, but ugly. First, in order to maintain backwards compat I guess that means making `Foo` the non-send version, `SendFoo` the send version and then a `CustomFoo&lt;F&gt;`. But this makes documentation (rustdoc) more confusing, because the documentation is no longer on `Foo` but on `CustomFoo&lt;F&gt;`.
DAMIT that was my plan for the future after watching his video üòÜ
It's probably possible to improve your solution using the library crossbeam_channel and it select! macro. The code would be basically similar, but your would use the select! macro in your loop, then the macro can select on the channel to receive content, or have a default timeout. Have a look at the examples: https://docs.rs/crossbeam-channel/0.3.6/crossbeam_channel/macro.select.html. 
C++ has barely even overtaken itself in that timeframe. Some unfortunate souls still have to use C++98 or C++03. And it sure hasn't overtaken C, especially in embedded and libraries. C hasn't even overtaken itself either, AFAIK something like C90 is the most portable version of C to date. GCC supports newer since 4.5 but MSVC doesn't seem to, if portability is a concern, and some poor souls aren't even on GCC 4.5. Hell, [apparently C/C++ hasn't even overtaken COBOL](https://en.wikipedia.org/wiki/COBOL#Legacy) It takes a long time for programming language usage to change. Some places *never* bother changing and just pay premiums on people who still know stuff like COBOL. At this point i'm having trouble even defining "overtake", considering COBOLs apparent prominence.
You might want to look at [Are we game yet?](http://arewegameyet.com/) for some resources.
Takes too long to compile. I'd either make an engine with scripting support or use something completely
As an example of HPC done with Julia, there is [https://juliacomputing.com/case-studies/celeste.html](https://juliacomputing.com/case-studies/celeste.html) . Admittedly that's not a rethink of how to program large-scale parallel applications, AFAICT it uses MPI, the novel thing is that it's entirely written in a high-level language without having to write performance-critical parts in another language (e.g. a mix of python and C). Is there any comparable example in Rust? &amp;#x200B; As for GPU's, there's things like [https://nextjournal.com/sdanisch/julia-gpu-programming](https://nextjournal.com/sdanisch/julia-gpu-programming) , which looks like pretty idiomatic Julia code. It seems to leverage the LLVM ptx backend, so I guess something similar could be done for Rust as well. And of course, with Rust you wouldn't have to worry about the GC so maybe you'd get even closer to "idiomatic code".
I share the same concern. Please don't be the new Haskell (https://hackernoon.com/rust-2018-dont-be-the-new-haskell-a383dbd74481). Please favor consistency over new features. And more importantly, please focus on delivering real world projects instead of pursuing an hypothetical perfect language. Go or typescript are faaaaar from perfect, but their consistency have brought the simplicity necessary to ship some large real world projects.
Anybody come up with a parallelizable solution?
Thanks for the info, I'll definitely consider that! 
Conversions in Rust ought to be explicit by design.
&gt; Is the promise/async/await style programming amenable to this kind of world? Implementations of these in Rust don't have to allocate any memory, nor use many threads. There are libraries using Rust Futures as APIs for non-blocking MPI functions that do not add any overhead over C beyond the overhead of a function call via C FFI but as `-C cross-lang-lto` continues to gets easier to use, I'd expect these libraries to start using it more heavily and for that overhead to disappear.
Get the `x11` crate to add `-lX11` to the linker command line like it's supposed to and `cargo build` and `cargo run` will Just Work‚Ñ¢. I've never had to do that before, so I don't have a more detailed answer.
Unfortunately that‚Äôd break running examples if the target dir is elsewhere ‚Äî for example, I use the CARGO_TARGET_DIR environment variable to move the compiled bits elsewhere on my system. The reality is that cargo examples are developer oriented and expected to be run using cargo. So how about: try loading assets relative to the current working directory‚Äîif that fails, attempt to find the crate root by walking up the directory tree. Having some standard way to find the crate root would be pretty nice. This issue is, perhaps, relevant: https://github.com/rust-lang/cargo/issues/2841
Wow, waking up Sunday morning and seeing a random post on examples in crates I did not expect this much hostility. I'm pretty disappointed in that. I can see that you could read the original post as slightly rude (OP could also have asked "am I missing something or why don't examples work me in many cases?" and got "we tend to use cargo run" as answer), but come on, people. The main issue seems to be about how people run the example binaries. Some use `cargo run`, other don't. That's totally fine! If you rather press Ctrl+R to trigger a build in your IDE and execute the binaries from a file manager, so be it. Cargo is a CLI tool that has strong conventions *and* I lead the CLI working group but I'm *still* baffled by how you can discuss this with so much passion! There even is a simple, technical solution (aside from a section in the readme describing what the crate authors expect you to do to run the examples) and that is making the example code more flexibel. Add a nice `find_assets` helper that looks at the current working directory and figures out where the assets are. Then it works in both cases. Alternatively, embed (small) assets with `include_bytes`. Both are fine things to do and I would accept pull requests for that for my own projects' examples in a heartbeat to make it easier for people to get started using my library.
Cargo doesn‚Äôt change the CWD when running binaries. Compiled assets may be written elsewhere on the filesystem by setting the CARGO_TARGET_DIR env var ‚Äî if you assume assets are always relative to the example binary you‚Äôll break the documented way of running examples with cargo. So, first trying to load from the current directory then falling back to trying a heuristic to find the crate root sounds pretty nice. 
 &gt; I really thought that was obvious. Just FYI, I think this is where most of the miscommunication in this thread comes from. It's definitely not obvious to a lot of people. I'm not blaming you or anything, just wanted to let you know that figuring out which seemingly obvious things are not in fact obvious is one of the hardest things when commutating with other people, especially strangers, over text messages. From reading this thread it also looks like nobody *explained* why they use the command line for this, probably because it seemed obvious to them. Let me know if you want to know more on how some people use the terminal for all that and what makes them think that *this* is faster than using a file manager.
MPI is good at solving problems that are latency bound (e.g. physics simulation), give users control about the process topology, communication topology, portable direct access to interconnects, fast parallel File I/O, remote direct memory access, etc. 
Haven't seen a language which would have specific cases in it's documentations. My experience is that people either consult stack overflow for it or their senior developer in the team.
Why do you care ? Just let it continue in the background, and let it quit gracefully if `send()` fails (use `is_err()` instead of `unwrap()`). That way it'll exit if the receiver is dropped, and quitting the main thread will kill it anyway. Just don't try to `join` it.
It certainly didn't help the discussion that both the OP and all users who support their position continued being rude and pretend-outraged throughout the comments.
If I want to read more input after the game is lost (e.g. recording a name for the high score) then the user will experience a glitch where their first input key is captured by the supposedly dead thread.
This still doesn't solve the problem of having a thread blocking on stdin, though; it just puts it behind another channel.
Is specialization a controversial topic, considering composition has taken preference over inheritance? Adopting this feature seems counter-intuitive to me from a cultural perspective.
Unfortunately that's not as easy as others noted. * TypeScript is a strict superset of JavaScript, if I'm not mistaken. As a result, one may mix these two languages in single project. * Kotlin and Java are based on the same VM, infrastructure and bytecode. So they interop without much trouble. * Unlike previous two examples, Rust and C/C++ have few major differences. * Lifetimed references vs pointers. Rust uses pointers rarely, in a small set of places where they're needed. While C/C++ use pointers virtually everywhere. So for Rust the only proper way to communicate with C/C++ is to use FFI, which is treated as inherently unsafe, for good reasons. * C++ templates, with all their capabilities, don't have proper equivalent in Rust. Maybe proc macros are on par with templates. Rust's generics, if you name them, are really generics, and not a Turing-complete compile-time strange Lisp dialect. To sum up, gradual oxidization requires: 1. Split project into modules 2. Take one module for oxidization. Make C interface for it and use it in other parts of project only through that C interface 3. Rewrite module in Rust 4. If there are modules left, goto 2. 
Then don't make it exit and just pass the receiver to the active function. Timeouts aren't great.
I actually did something very similar! Happy to see other 3blue1brown fans here :) My code is [here](https://github.com/LukasKalbertodt/pi-collision) but it's quick'n'dirty code and does not really have any explanations -- so your repository is way better for that. However, in order to understand the problem, I also coded this little visualization (with WASM, huzay): https://lukaskalbertodt.github.io/pi-collision/ (note: potential spoilers if you still want to solve the puzzle yourself!). Specify the mass of the bigger object in the field at the top and the graphic should automatically generate. The points on the big circle represent one collision. The suffix `_b` stands for "big block", `_s` for "small block". I still haven't completely understood the puzzle, but I think I understood a fair bit. Certainly a lot of fun :)
Could you please clarify? IMHO, specialization has nothing to do with inheritance. Basically it's a way to provide special implementations for corner cases to boost performance. Think of Vec&lt;bool&gt; that would compact values in bytes in case if you don't need to access individual values by address.
Top post of all time on this subreddit is about chucklefish using rust to make their (2d) games. https://www.reddit.com/r/rust/comments/78bowa/hey_this_is_kyren_from_chucklefish_we_make_and/?st=JR4SG33F&amp;sh=ed5e2e9a
Brin it to iOS please!!!
Assuming you are using a channel to send data from the input thread back to the main thread, could you keep the input thread running and on the main thread pass the receiver from the `game-handler` to the `high-score-input-handler`? In general, there are some fundamental challenges building an interactive TUI program on top of the currently available libraries. Specifically, if you have an input thread watching `stdin` all the time, you cannot easily do things like get the cursor position or detect color support in the main thread, since those things also require reading from stdin. 
What is 'UB'?
Thanks for a great summary
// preqs for timeout use std::time::Duration; use tokio::prelude::\*; &amp;#x200B; // Usually end up using a future when dealing with timeouts could do something like this: let future = Command::new("expect") .arg("-c") .arg(&amp;cmd.join(";")) .output\_async() .timeout(Duration::from\_secs(3)) .map\_err(|e| println!("failed to collect output: {}", e)) .map(|output| println!("{:?}", output)); tokio::spawn(future); &amp;#x200B;
Nice work! The link to the technical reference paper you are citing is not working tough.
Side note: If you want to test out your FFT code to make sure it's giving the right answer, you can use the [rustfft library](https://crates.io/crates/rustfft).
Good catch! I didn't spot it myself reading your post. Stylistic note, I'd let rustc infer the generics: `collect::&lt;Vec&lt;_&gt;&gt;()`, specifying the whole type looks a little noisy to me (that is, unless you specified the whole type on purpose for the example)
Good like chaining your awaits this way ;)
Yea you got the point. I will do that in the future. Thank you.
Explicitness was definitely a goal in this case. However, I‚Äôd missed that you could use an underscore in types like that. Thanks for the tip!