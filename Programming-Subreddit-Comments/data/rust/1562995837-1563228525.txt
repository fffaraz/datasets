This sub is for the Rust programming language. You probably want r/playrust.
It's 3600 * 48 but otherwise looks good.
The speed of a rust program is not determined by the speed of the compiler. Rust is compiled into optimized assembly, that‚Äôs why it‚Äôs fast. Having a fast compiler just means that the compilation from Rust-&gt;assembly is fast. The compiler doesn‚Äôt actually run the code
Cute!
This makes sense. So, here is probably a really stupid question, so forgive me. (People always seem to get upset at these questions IDK why. I am just trying to learn and understand! So, apologies in advance.) Why isn't Python or JavaScript or whatever language written so that it compiles to assembly. Why aren't all languages this way? (I suspect the answer is simply, it would be a lot of work and very hard, but I wouldn't know, hence the question!)
I did consider `nom` for this. The language is simple enough though and I have plenty of experience handwriting parsers. I wasn‚Äôt sure it justified adding a crate dependency.
Azul and orbtk are both promising projects but still v0.x Docs aren't up to date, API had (and will probably have again) breaking changes. Right now only gtk-rs is a production ready GUI framework for Rust IMHO üòâ
Thanks, deleted.
Thanks, deleted.
Actually that‚Äôs a good question, and I‚Äôll give some of the reasons I can think of: 1, Portability. Assembly isn‚Äôt standardized across different hardware, so if you make your language compile to VM code like Java, or into C like Python, it is easier to make your language portable to all hardware 2. It‚Äôs easier! Creating assembly which represents a high-level language is difficult to get right 3. In the case of JavaScript, it was never meant to be fast. It was meant to run in a browser, so it would be silly to compile it to assembly, it just needs to compile to a format the browser understands
Please don't post links to downloads.
Thing about it like that. What you're asking is "My knife (rust compiler) was made with a hammer (ocaml compiler), how come it cuts (generates fast binaries) better than a hammer", the knife was made with a hammer, it's not made out of hammers. hmmm... might be confusing Ocaml compiler (in ocaml or whatever): Source -\[compiler algorithms\]-&gt; Binary Rust compiler (in ocaml or whatever): Source -\[compiler algorithms\]-&gt; Binary The binary depends on what the compiler algorithms do (what the dude does with his knife), it doesn't depend on how/with what language the compiler was built. The speed of the final binary depends only on the actual content of binary, bits and stuff, and not on how the compiler was made, the compiler could be made with excel scripts it wouldn't matter at all, as long as you can write the same binary file in the end. suppose you have source A and binary B A -\[compiler made in rust\]-&gt; B A -\[compiler made in ocaml\]-&gt; B B will have the exact same performance &amp;#x200B; ... i hope you could understand what i tried to say... i don't really know how to explain it well...
Computerphile has some really good videos on bootstrapping, check them out on YouTube. The TL;DR is that a program can generate another program that is faster than it. This is fundamental to how computing works. Think about it as the programmer as the input device instead of the program. We can look at generated output, find a better way to do it, and write a program that generates the better output running on the previous, slow implementation. Now our generated program is faster and smarter than the previous.
Also, to add to this: Rust isn't actually written in OCaml any more - the compiler was rewritten to Rust a long time ago, so Rust is quite literally written in itself.
Hey guys, I'm kinda new with rust. I have trouble mainly with strings. For example, imagine I've a string that I splitted and collected like so: ` let src = text.split_whitespace().collect::&lt;Vec&lt;_&gt;&gt;(); When I want to sort it using a custom function, coming from JS, I'll just chain it like so: ` let src = text ` .split_whitespace() ` .collect::&lt;Vec&lt;_&gt;&gt;() ` .sort_by_key(myFunc) ` .join(" "); and when I want to obtain the result, I can just print src. But in Rust, this will yield an error, and the result is only an empty tuple / unit type. I need to separate the chain into something like this: ` let src = text.split_whitespace().collect::&lt;Vec&lt;_&gt;&gt;(); ` src.sort_by_key(myFunc); ` src.join(" "); Any idea why?
Note that your first number (1) is followed by a comma, which breaks the numbering wikiformat. Otherwise you can follow prior lines with two spaces or two carriage returns to force new lines.
LOL
How do I take ownership?
Why not give it a dummy name on the rust side? Names don't affect ffi layout.
Check how bindgen does it, if i recall correctly they create a dummy named field for the union.
no problem, you have helped me a lot already. Thanks
One reason that Python and Javascript programs are not as fast as Rust or C is that they are dynamically typed. This makes them much harder to compile to efficient machine code since you sometimes do not know at compile time what type a value has and thus how to it will be represented in the machine. Another reason is that systems languages like Rust and C are a much closer representation of what actually happens in the CPU when running machine code. High level languages like Python, Haskell etc. are further from machine language and are thus harder to optimize. With that said, there has been a lot of research into how to produce good machine code for languages like Python, Ruby, Javascript etc. For example, look at [GraalVM](https://www.graalvm.org/) which can produce really fast executables for these languages.
Beyond what /u/00benallen said, it may also not be possible. If you look at Python compilers like [Shed Skin](https://shedskin.github.io/), [Cython](https://cython.org/), and the [RPython](https://rpython.readthedocs.io/en/latest/) transpiler, a recurring trade-off shows up: Either they have to only accept a restricted subset of Python or they don't fully compile things to machine code, because some of the language features have requirements that are ambiguous at compile time. (eg. If a variable can only take one type of data, that can be stored and operated on efficiently. If it holds multiple kinds of data over its lifetime, then the compiler has no choice but to either reject it or compile inefficient code that checks what it is and what to do every time it operates on it.)
Javascript runs in a browser, and it is sandboxed: it cannot be allowed to be given the full control it would have if it were compiled to assembly. However to make Javascript execution very fast, many tricks have been implemented: JIT compilers can compile verifiable, hot parts of a program directly to machine code. WASM has been developed as the lowest level language that javascript can compile to, and as you can see the name is kind of "web assembly", it's a sandboxable and safe binary code format similar to assembly.
On phone, but look over the older posts. Somebody asked the same thing last week.
No speed limit! :)
Saturday morning, still in bed, almost half awake, browsing reddit and found this. What an epic start of this weekend! Love it! Infinitely more engaging than reading the release notes :) Looking forward to the next episode.
have const pointer in a C struct look strange without context. Anyway smmalis37 is right just give it a name. C allow unnamed union as a increase of life quality... well in case of union I'm not sure it's good or even standard... well I don't want to verify in the standard right now :p just give it a name in Rust
You can also checkout sourcehut which is in Python.
Can you host them on your own server?
I think https://en.wikipedia.org/wiki/Trusted_computing_base is relevant here.
The `sort` functions sort *in place*; they reorder the items in the existing vector instead of copying/moving into a newly allocated vector. This is for performance reasons. But since you're `collect`ing anyway, check out the [`sorted` methods in the itertools crate](https://docs.rs/itertools/0.8.0/itertools/trait.Itertools.html#method.sorted)
It would be a pain to police ‚Äî lots of arbitrary decisions required. The big question should be: is it worth it? Squatting isn't currently a *big* problem, while forcing all crate maintainers to jump through hoops to get their crates published is sure to annoy a few folks (especially new publishers).
Majority of projects are binary project, where lock file should be checked in, right?
I'd be down to do an episode on Rust + Matrix (i.e. my project [Ruma](https://www.ruma.io/)) if anyone wants to chat with me about it. On the topic, what's a good podcast microphone? (I'm thinking sub-$100 since I'd use it rarely.)
Separate files for different functionalities seems reasonable. Typical structure [`main.rs`](https://main.rs) `|-foo.rs` `|-bar.rs` &amp;#x200B; Inside [main.rs](https://main.rs) `mod foo;` `mod bar;` &amp;#x200B; And inside [foo.rs](https://foo.rs) create your data structures, functions, traits, methods etc, but only add `pub fn` to the things you want to expose. Nice clean encapsulation by defaul
You are confusing compiler vs interpreter. The compiler outputs assembly. The amount of time this takes does not matter as the CPU will run the machine code assembled. Interpreters are different. You read the source code, interprete it and run it directly. So an interpreter written in language X is just another program written in language X.
Longtime lurker here, first post. While certainly on the opposite side of the spectrum language wise (primarily based on Ruby I believe)
Python and JavaScript are so dynamic that compilation to assembly is both non trivial and wouldn't gain much. For instance you couldn't inline functions so easily as function calls are often dispatched from some kind of dictionary and updates to said dictionary would invalidate all your compiled code.
 let my_cup = your_cup;
TBH it isn't just dynamic typing. It is stuff like being able to completely rewrite everything dynamically. All function calls in Python are rewritable function pointers. A dynamic language without this property (say Go) isn't that hard to optimise
so split, join and collect is like lazy-evaluated FP style operators, while sort is like the imperative command? thanks man!
[Bootstrapping with T-Diagrams](https://youtu.be/PjeE8Bc96HY) is a good one
In the case of Java, they still had to create the VM for each of the platforms.
Yeah but the VM was written in Java so they only had to compile it once.
At work we use Gogs, but are looking at moving to Gitea. Gogs lacks a lot of features Gitea has and is barely maintained in comparison. (Gitea is a fork of Gogs.) I can't help you with the Rust alternative thing, but I see no reason to prefer recommend Gogs over Gitea.
That's great to know! Thank you for your advice!
&gt;still have no idea when I have to use Mutex instead of RwLock The reason is because rwlock has a higher overhead than mutex and may suffer from starvation on some platforms
http://exercism.io has some nice problems to solve, if that's the kind of thing you're looking for.
There's another good reason why JavaScript is run in a VM: sandboxing. You need to be able to run JavaScript that you don't trust, and still rest assured that it can't access stuff unrelated to the website it came from.
Either you're joking, or I'm missing something here. The VM has to be compiled for each platform.
There's also the property that Mutex&lt;T&gt; is always Sync so long as T is Send, while RwLock&lt;T&gt; needs T itself to be Sync.
Right
The [HotSpot JVM is written in C++](https://en.wikipedia.org/wiki/HotSpot), not in Java. The advantage is that *your own* programs don't have to be compiled for different platforms.
I've always wondered why you can't sort iterators. Is it because the operation can't be lazy?
Thank you, didn't actually know you could assign a closure to a variable like that (new\_line func).
Rust is not interpreted by OCaml, or rewritten into OCaml code. Rust is turned into machine code, which runs directly on the CPU. Look up the difference between an interpreter and a compiler.
I'm definitely going to check that at some point. That's a cool idea.
The SIMD stuff is the actual poster child of RFC hanging there forever, it led to the RUSTC\_BOOTSTRAP debacle.
https://www.codingame.com/ has gamified challenges and supports rust
Thank you! #\[derive(Default)\] will save me a lot of time in the future. Don't know why I haven't tried it before tbh. 2nd and 4th points are good ideas too, will change this in the code. Also didn't know you could put literally anything in Result. I think I need to read more about error handling in rust. Thank you again :)
Why? QML is pretty great
If you like math you can check https://projecteuler.net/
Math checks out
Another reason why languages like Javascript and Python would still be much slower than, say, Rust, is that Objects in both languages are inherently dynamic. You can at any point remove or replace any property (which may be an instance method), or use methods that don't exist by are handled dynamically at runtime (things like `getattr`, `delattr`, `setattr`, overloading `_getattr` or handling `AttributeError` in Python all allow this and all are commonly used. In JavaScript it's even easier.) Additionally, dynamic typing of everything makes it really hard to predict the type of objects, so lots of runtime checks are required. All of this is possible with generated machine code or assembly, but they incur a non trivial overhead. Rust focuses on using zero-cost abstractions, meaning that language features and paradigms are chosen specifically to require no unnecessary runtime checks or overhead.
Were you thinking of `nannou`?
Although not git, pijul and its hosting service are written in Rust - but the latter's code isn't yet public unfortunately. If you're lucky they may share the code with you and you can port or extend it to support git. Or you know, make your own! https://nest.pijul.com/pijul_org/pijul https://nest.pijul.com/pijul_org/nest
rust-qt-bindings-generator is what i use. Disclaimer: I wrote it. I'm using it for a personal diary in production. That code is not clean enough to release at the moment. The most elaborate application that uses rust-qt-bindings-generator is the email reader demo. Applications that use rust-qt-bindings-generator use a mix of C++ or QML and Rust. This avoids the mismatch between the languages. https://archive.fosdem.org/2018/schedule/event/rust_qt_binding_generator/ https://www.vandenoever.info/blog/2018/09/16/browsing_your_mail_with_rust_and_qt.html
Funny enough the Crystal programming language was originally written in Ruby and is now almost as fast as C in many tasks. The original language doesn't count for much, otherwise languages would never get faster. The trick is that a lot of them use LLVM for machine code generation which allows them to be insanely fast.
I wouldn't make the decision depending on the language since you'll use the features, not the language the language they're implemented in. Also, I think that Go is a perfect fit for such a task and don't see the need for a Rust alternative here. &amp;#x200B; A friend of mine used Gogs on his server. Unfortunately, there is indeed a noticeable lack of features and the whole project doesn't seem to be continued as actively as it used to be. He therefore switched to Gitea and he is more satiesfied with that. &amp;#x200B; Nevertheless the top notch Git solutions IMO are GitLab, BitBucket and GitHub.
Exceptions are replaced by returning `Result&lt;T, E&gt;`. There‚Äôs also the `?` operator to make this as convenient as in other languages. The real complexity in Rust comes from handling shared mutable data and ownership, and that‚Äôs something you always have to be aware about, no matter how much is abstracted away from you. Frameworks can handle this for their own data and things they pass to your code, but not the data structures you‚Äôre using yourself.
 let open_source = true;
I am switching from Java to Rust by choice at work. Java was a good starting language for me but I wanted more control over my performance. Many people at my work are down at the C level and this was my attempt to meet them half way. I've written some applications in Rust but they're command line utilities. My next jump will be GUI with GTK, then I'll move to web through WASM. This transition is a safety thing for me, as each move takes my applications one step further from my use to more of the masses.
[Rosetta Code!](https://rosettacode.org) has many problems to solve and only a "small" subset have Rust implementations. It is an open wiki with "940 tasks + 236 draft tasks and 743 languages". Notably the [Category:Rust](https://rosettacode.org/wiki/Category:Rust) and [git mirror of all solutions](https://github.com/Hoverbear/rust-rosetta). The "challenges" are all tasks, where some are already solved - though you can improve on many of them or demonstrate different paradigms to solve the same problem.
I'm guessing he/she's looking for community or help.
I usually use PHP 7 / [Symfony](https://symfony.com/), but I tried Rust / [Rocket](https://rocket.rs/) at a "Hello word" level. I will try to list what I like or dislike in Rust and PHP. &amp;#x200B; # Pro PHP * That's an easy language that lets you progress at you pace. Once you understood the basic of HTTP you can use PHP and improve your skills, you don't need to be an experienced programmer to begin with PHP. * There are many, many resources about PHP so you are never alone. The [documentation](https://php.net/) is great. * The standard library is full of useful functions, classes, and almost everything you need already exists. * If someone doesn't exist in the standard library, it probably exist as a **stable** [Composer](https://getcomposer.org/) package. * There are great frameworks that are stable for years. # Cons PHP * Its typing system is poor and lead to many errors. It has been improved from PHP 7 by letting the developer enforce its method's return type, but even with that it's possible to create a variable as integer, then if a condition is met convert it to a DateTime, and outside the condition use it as a string. The behavior would be different and the code could crash only if we enter the condition for example. Quite hard to test. * PHP has no built-in package manager. Fortunately Composer came and is great to fill the gap. * When you push to the production server, you aren't confident with your code. A code may work well when you tested it, but once on production a user may use it in a way you didn't, enter in a condition you didn't enter, use a null variable, and there was no compilation step before sending it. Usually you use unit tests and others to be more confident with your code. &amp;#x200B; # Pro Rust * It's low-level with a high-level interface. It make the low-level development more fun. * Really performant. But on the Web performance issues are more due to database misconception that programming language execution time. * Documentation is great! Once we get used to search in the documentation we find everything we need to know. * Good standard library. For a low-level language the standard library is full of useful things that are easy to use and that would be distributed as external packages in other languages. * It makes you more confident. When you push your program on production you are more confident because your code has been compiled and the compiler tells you about everything. Also, the typing system lets you be sure about the types you are using. And there is no *null* value, that's cool! * There is a great toolchain. Cargo, rustup, that's easy to manage your Rust versions and packages. * There are lot of resources in addition to the documentation. The [Rust book](https://doc.rust-lang.org/stable/book/) is one of them. &amp;#x200B; # Cons Rust * The learning curve is hard. There are concepts like lifetime, borrow checker that are new in Rust (at least for me) and are quite hard to manage. The StackOverflow developer survey shown that even if Rust is the *most loved programming language* many developers don't feel productive with it after some months. * The amount of **stable** packages that we can install using Cargo are quite low. There are many packages but they are not stable, that's difficult for a company to use Rust (for Web or CLI apps) if the ecosystem seems not to be strong enough. The interesting thing is many packages in version 0.x are really stable in practice. * There are no stable equivalent to Symfony or [Laravel](https://laravel.com/) in PHP. Rocket is cool and does lot of work to let us work on our business logic, but it's in version 0.4 as of July 2019. &amp;#x200B; I really like Rust and I would like to use it in production, for Web and CLI apps, because it's safe, performant, that's important. On the Web many languages are interpreted and not statically typed, that causes many bugs and errors because when a developer is to *free* it does wrong. PHP is a good example. Rust with its type safety would be great on the Web according to me, it would avoid many bugs.
It doesn‚Äôt fit nicely in the ‚Äùpipeline‚Äù model of operation, since it requires O(n) auxiliary storage, and forces the evaluation of the whole input iterator the moment you want even a single element of output.
Let's start from a high-level overview. First of all, a *language* is not in itself faster than another language. Instead, what should be compared are *implementations* of languages. As a case in point, Python run by the CPython reference interpreter is in general an order of magnitude slower than Python run by the Pypy JIT compiler. As another case in point, Rust compiled by rustc in Debug mode is in general an order or two of magnitude slower than Rust compiled by rustc in Release mode. There are a wide variety of ways of implementing a language, roughly in order of speed: - AST Interpreter: the source code is parsed into a syntax tree, and the tree is walked up to execute it; not generally used for anything other than demonstration purposes. - Byte Code Interpreter: the source code is transformed into byte code, and the byte code is interpreted to execute it; this is for example how CPython works, the `.py` (source code) is compiled to a `.pyc` (byte code) which is then interpreted. - JIT Compilers: the source code (or byte code) is compiled into assembly in memory, and the assembly is executed. This is how Pypy works, or a browser's JavaScript engine. JIT stands for *Just In Time*. - AOT Compilers: the source code is compiled into assembly and turned into a library or an executable file, the executable is then executed. This is how rustc, clang and gcc work. AOT stands for *Ahead Of Time*. In general, Interpreters will not perform any optimizations and suffer from a significant overhead compared to native assembly: adding to integers is a single assembly instruction, whereas interpreting a `Add 1 2` byte code instruction requires dozens of assembly instructions. Today, when speed is wished for, Interpreters are often supplemented by a JIT compiler: if the same code is executed multiple times by the Interpreter, it will be compiled by the JIT compiler so that further executions are faster. The reason not everything is compiled being that compiling takes time, which would slow-down the start-up. Modern browsers such as Firefox even have multi-tiered JITs: the code goes from Interpreted, to compiled with a fast JIT (producing slow assembly), to finally compiled with a slow JIT (producing fast assembly). This is an attempt at balancing responsiveness vs execution speed. --- Going down! So, are all languages equal? No. The difference hinges on whether a language can be efficiently translated into assembly, or not. Some language features are harder to translate to efficiency, whereas others are straightforward. One big difference is how dynamic the language is: in Python, the developer can add/remove attributes and methods to any instance on the fly, can intercept requests for attributes and method calls in a generic manner, etc... this flexibility is extremely challenging to implementing efficiently in assembly, and thus Python interpreters and JIT compilers start with a greater challenge ahead of them than C compilers. Another difference is Garbage Collectors. As much as proponents of GCs like to claim their theoretical superiority, in practice implementations relying on a GC are slower than implementation without one. They are slower in average (throughput) but also tend to exhibit latency spikes, and there is an inherent throughput-latency tension which means that lowering the latency spikes comes at the cost of a lower throughput. Rust, however, has always been aiming at achieving optimal speed. As a result, its features have been carefully vetted to avoid any performance trap, and therefore its implementations can generate close-to optimal assembly.
Great links, thank you very much &lt;3
A VM enables more magic in a language, something that docker will never do.
I can kinda see your point but go isn't dynamically typed.
Performance thing is not true. Translations killed my PHP performance. As much as 1ms PER STRING. So every menu option, every button, etc. Added 1 ms per request. That's without DB access! I had to actually cache pages just to translate them! Before even any DB access. Like literally cache the front page. Same thing in Rust runs an order of magnitude faster, so 100 translations is more like 10 ms rather than 100 ms. It's acceptable even without an html caching layer
&gt; the whole concept of having an additional layer of a VM seems broken to me in the age of Docker I‚Äôm interested in that point. Would you elaborate this?
C# has also had the `unsafe` keyword for a long time, though Rust may be the first non-GC language with it.
Meaningless comment, translations are no native thing so it entirely depends on what you used here or if you implemented it yourself, how you implemented it.
I've recently built a small web application as an exercise which uses a Rust/actix back-end communicating with a Rust/wasm front-end (with a bit of React for the actual layout and UI event handling). As far as the back-end, Rust was pretty pleasant to work with. There was definitely a learning curve, but I didn't need to write nearly as many tests as I would with an Express server to get the same level of confidence that it's working as intended thanks to the strong type system. Honestly most of my frustrations were with Actix. This might be a matter of taste, but it's a little too "batteries included" for me. It felt a bit like working with Rails, where if you want to work with it as intended everything's fine, but when you need to something a bit more custom it feels like you have to fight against the library. For instance, it was a bit of a pain to figure out how to turn off caching on certain routes. Maybe there are less opinionated frameworks out there which I would like more. The front end experience was just awesome. Bindgen is an amazing tool, and it's so nice to be able to share types between the front and back end in the context of a language with a decent type system. There were a couple awkward bits involved in bridging between Rust and the JS/GC world, but it's already very good and I'm sure it's only going to get better.
I've used the same app in Rust as with Java. Rust needs a bit more code currently, but could be solved with libraries. Also the rust code needs much less memory, and a bit less CPU for the same thing. And the docker images can be a lot smaller. https://www.slideshare.net/GerardKlijs/rust-kafka52019unskip using Kafka and Avro it's also pretty easy to use both besides each other. Want to try using rust for GraphQL as soon as there is support for subscriptions.
The idea of a Docker VM is completely different from a Java VM. Docker isolates and standardizes the environment programs execute in. The JVM is a fancy Just In Time compiler to execute Java code. One's basically a mini computing environment, and the other's basically your program itself.
For all intends and purposes I think gitea is the best choice for a self host git service since go has a good security record and is reasonably fast. And also consumes very little resources when ideling, my gitea container only uses about 250m and almost zero cpu.
&gt; Define and register a qmlcontext on the c++ side, and forward calls from the qml side to rust and back via the qmlcontext rust_swig generates normal C++ classes, not Qt specific. I suppose you need inherit QObject, use Q_OBJECT inside class definition and mark methods as q_invokable to properly use C++ object inside qml. Plus you better to use Qt specific classes, like return QString when you need return string. It is not something complex, for my project I configure for example not only QFuture handling, but also returning QString if in Rust I return String, and use const QString &amp; if in Rust I use '&amp;str', but that you need configure, it is not out-of-box.
Larger projects/organizations might also want to try using sccache, which doesn't need to load all the cached content initially; it does require an S3 or equivalent setup, though. [Here](https://github.com/rustwasm/wasm-bindgen/blob/13b672aac0445e616dd516bbbbc26230166b7775/ci/azure-install-sccache.yml) is how wasm-bindgen does it.
The more dumb questions you ask, the smarter you become. Keep them comming :) Smart people ask dumb questions a lot.
Is there an advantage of that over iterating in reverse, `0..buf.len().reverse()`? It seems to me that deleting only invalidates the indices after.
I used an existing solution. Some C plugin. Even then, it was slow. It's single threaded, so all the calls go in order. You can't fix that without threading primitives.
Database access is not the only way to loose performance, there are many. Indeed in PHP we need to cache many things, generated HTML pages for example. But the database example was only an example, and even if performance is important on the Web, the execution time of the language is usually not the problem. Your problem might be the translation system implementation or usage, but when running PHP 7.x in production the execution time is good enough to not focus on this when choosing a language. Yes Rust execution is faster, but I think on the Web it would not make a significant difference. That's only my opinion.
I guess the final thing to add about the languages mentioned is that a lot of popular languages today are garbage collected. This also reduces runtime performance, and worse, makes the runtime performance non-deterministic.
I didn't look over much of the code, but I think you're making an extra allocation in https://github.com/projjal/indydb/blob/master/src/db.rs#L162 because of the Vec. You could probably use AsRef&lt;[u8]&gt; instead.
I don't really agree with some of your points: &gt;Java is also verbose, and it will only get worse the more features they will put in At least where I work, old java code was way more verbose than newer code in Java 8+. The whole functional shift, plus now `var` is making it way easier to write less verbose code. Can you point to new features that significantly increases verbosity? &amp;#x200B; &gt; The whole concept of having an additional layer of a vm seems broken to me in the age of Docker. They try to counter this with projects like [Graal](https://www.graalvm.org/) but it has it's own flaws ([only suppports JDK8](https://github.com/oracle/graal/issues/651), [bad peak performance](https://benchmarksgame-team.pages.debian.net/benchmarksgame/fastest/java-substratevm.html), loss of reflection, additional build steps,...) I don't really get what docker has to do with this? Does it provide just in time compilation, runtime reflection, ... all those other "magic" features that are enabled by the Java VM? &amp;#x200B; &gt; There is no unified way to write async code, just frameworks like [RxJava, RxJava2](https://github.com/ReactiveX/RxJava) and [Reactor](https://projectreactor.io/) \- although they offer more than just async-await - as far as I understand the later That's what project loom is for, as I understand. It provides the basis for how async code will work on the JVM going forward. Of course they don't brand continuations as async exactly, but async can easily be built on top of it.
reflection, like in self reflection :)
Most APIs in Go are
The code looks solid enough, my recommendation is moving on to the testing phase. It‚Äôs a really cool experience in rust to write unit, integration, and performance tests.
I can imagine following analogy: cars are made by slowly walking people, how can they be faster than humans?
That was the first one I had originally thought of actually. Then I noticed it's composed of 700,000 lines of ruby and I thought a compiled language would probably work better on lower end hardware(cheaper server).
Pijul looks really interesting thank you for introducing me to it!
Thanks, I see your point. Would giving it a name not interfere with FFI calls that pass in or return a `source_t`? For example, I have this C function definition: const source_t* find_sources(source_finder_t* p_instance, uint32_t* no_sources) If the C lib code is looking for those fields directly within the `source_t` struct and Rust is has those values inside of a named union field, could there be problems later down the road? Or does this not matter as much because Rust is more or less just sending pointers back and forth across the FFI boundary and Rust definitions don't need to mirror C definitions exactly?
I completely agree with making decision based on the feautres not the language. However I'm working on something that may eventually hold 10's of thousands of repos so I thought rust may allow me to make better utilization of lower end hardware and get more out of my dollar.
Wouldn't an event loop solve that?
That's good to know and thank you for that information. I am now heavily leaning towards gitea!
Thanks. You are right. I will change that.
The long and the short of it is that Python and JS offer a lot of stuff that isn't easy for a CPU to run. CPUs don't know what objects or methods are, and they don't know what garbage collection is either. CPUs only really understand 3 things: integers, floating point numbers, and memory. In order to run Python or JS on a real CPU, we have to tell the CPU how to know whether a bit of memory happens to be a float, an integer, or an object, and we have to tell it how to check whether a bit of memory is still used, or whether it's safe to reuse. Rust is different because the Rust compiler works out all this stuff ahead of time. When the program is running, the CPU doesn't have to check whether or not a bit of memory is a float or an int. It can trust that the compiler has worked it all out already. It's like the difference between driving your car to work and taking the train. It's fine to fall asleep on the train because the path of the train has been worked out in advance.
We are currently using java to develop our middleware for the ESA OPS-sat mission and part of the job does involve resuscitating our software engineer a few times a week. We are definitely looking forward to moving over to rust once we have our own standards for our tools!
Yes, that's the plan :)
I had to add a [build.rs](https://build.rs) file telling cargo to search the GTK\_LIB\_DIR path, and I'm still getting \&gt; cannot open input file '&lt;GTK\_LIB\_DIR&gt;\\gtk-3.lib But that path definitely exists. :(
With the advance of docker, Java and JVM have lost their most praised value: write once run anywhere. Sun promised, Oracle inherited, but they both failed the mission. Java is staying, simply because of the tools and libraries we've built around it. If we needed to build things from scratch, Java will very likely be out of the picture. Having that said, while I'm all for rust when we talk about command line tools and gui programs, for web development, at least for now, wasm is the only aspect where rust can shine. I'm not saying rust can't do what we are currently doing, but I just don't see the merits to have rust, let's say, replacing python or nodejs to do what we are doing on the web front. Maybe time will change it.
Certainly a valid concern as it pertains to self-hosting. I am currently using the hosted version so I can happily abstract that concern in peace :)
Yes, I was considering mentioning GC as an example of a non zero cost feature, but decided that GC doesn't have to mean bad performance. It may make it harder to get deterministic performance, but languages like Java, Go and C# are all fairly fast (certainly closer to Rust than to Python) despite using GC.
A good USB mic should do okay. I bought a used Samson Meteor mic for ~$25. It is a great mic for the price and a great starter mic for podcasting. You can find plenty of reviews of it on YouTube. Remember to buy a pop filter or windscreen.
Also: Instead of for f in foo.iter() { ... } where foo is a Vec, you can do for f in &amp;foo { ... } which is subjectively nicer. As for why it works: `for` takes something implementing `IntoIterator`. `foo.iter()` returns something implementing `Iterator` for which there exists an impl of `IntoIterator` for. But `&amp;Vec` also implements `IntoIterator` so you can use it directly.
&gt; At least where I work, old java code was way more verbose than newer code in Java 8+. The whole functional shift, plus now &gt; &gt;var &gt; &gt;is making it way easier to write less verbose code. Can you point to new features that significantly increases verbosity? Good point, lambdas really make life more easier and I really like omitting types with `var`. I meant the complexity of the language grows and there are some outdated but not deprecated things to do stuff, like using `Date`. A verbose example: There is a lot of boiler plate code to (de-) serialize plain old java objects. And even you would generate the code with [Lombok](https://projectlombok.org/), you still have the overhead of an `Object`, although all you want is just data: `public class Foo {` `private final Collection&lt;String&gt; bar;` `@JsonCreator` `public Foo(@JsonProperty(value = "bar") Collection&lt;String&gt; bar) {` [`this.bar`](https://this.bar) `= bar;` `}` `// lot's of code, but not even immutable :(` `public Collection&lt;String&gt; getBar() {` `return` [`this.bar`](https://this.bar)`;` `}` `// toString() ...` `// hashCode() ...` `// equals(...) ...` `}` Generics are a feature they didn't get right because of backwards compatibility. Type erasure often really gets nasty and Collection&lt;? extends Something&gt; is really not intuitive. Sometimes &gt; I don't really get what docker has to do with this? Does it provide just in time compilation, runtime reflection, ... all those other "magic" features that are enabled by the Java VM? I don't get what wrong with docker + jvm. I don't use it personally, but I know a lot of people that use Java microservices with docker and it seems to work just fine 1995 it was common to run multiple Java-Apps (.war-files) on a single Tomcat directly on the os/hardware. Nowdays it is common to abstract the operating system by using Docker. So you have at least *two abstraction levels*. Why use a virtual machine when you already know at build time that you are building say a x64 Docker alpine-linux image? &gt;That's what project loom is for, as I understand. It provides the basis for how async code will work on the JVM going forward. Of course they don't brand continuations as async exactly, but async can easily be built on top of it. Also the way continuations will work seamlessly works because they have the jvm layer. It's like value types, not going to land anytime soon :(.
&gt;down at the C level and this was my attempt to meet them half way. Just a nit; I wouldn't say Rust is halfway from Java to C. It's _all_ the way. Rust programs can be made to run as fast as C programs; and (with nightly) you can get access to _all_ the hardware features.
r/playrust
i feel like its.desrespectful to compare rust to php hahaha
Maybe this article on bootstrapping Rust from mrustc can help you [https://www.gnu.org/software/guix/blog/2018/bootstrapping-rust/](https://www.gnu.org/software/guix/blog/2018/bootstrapping-rust/) Alternatively have a look over at r/playrust
I highly doubt Crystal is anywhere close to the performance of C. It still has a GC so at best it may have Go-like performance.
Python is also thread safe, because of the GIL. :D
Edit: I looked into this just for a few minutes and the "yeti" mics from "Blue" seem to be very popular: https://www.bluedesigns.com/
None of the verbosity you mention is getting worst with newer java versions. At worst, it's the same if you use old apis instead of the new ones. &amp;#x200B; &gt; Why use a virtual machine when you already know at build time that you are building say a x64 Docker alpine-linux image? Because Docker by itself doesn't enable the language I'm using to have runtime reflection, seamless continuation integration, ... &amp;#x200B; &gt; It's like value types, not going to land anytime soon :(. I somewhat agree that they take a long time, however, retrofitting value types and adding things like continuations to Java are big tasks, so I don't mind waiting a bit. I mean, how long did Rust take to implement Async / Await? And it's not even as ambitious as Project loom / continuations. I'm also somewhat more hopeful now that they switched to a faster release cycle.
I've also found the Samson Q2U to be _excellent_!
Actually Python and JavaScript will be getting compiled, not by a standard compiler, but by a Just in Time compiler (JIT) which is built into the interpreters. It's probably not going to be as fast as a fully precompiled program though. But a lot of the performance differences will be other things rather than the raw code. Memory usage in those languages tends to be worse. The 'objects' in those languages have a lot of extra things in them (like a list of functions and their names). They use garbage collection and that adds overhead.
Are they? The vast majority of my projects are libraries üòÖ And for my few binary projects, I actually don't check in Cargo.lock as it can hold back updates
I agree with you haha, but the question was about the Web, and PHP is an important language in Web development. Too important to ignore it I think.
I'm looking forward to making my next backend in Rust: `Serde`'s an outstanding lib for serializing data, and being able to keep the same data structures and functions when paired with a WASM frontend is nice. Rust currently lacks the high-level tools I'm used to in Python/Django (high-level ORM etc), but I expect this to change.
RwLock is a great tool for the job, but it has considerably more complexity and runtime cost, compared to a simple Mutex. You should use Mutex when your lock-holding code paths are relatively short, or when you just don't have many threads using the lock at all. Again, RwLock is perfectly fine, especially when you don't really need every last cycle of throughput. But if performance matters, there are plenty of situations where Mutex is going to be a better choice.
IIRC, the `gtk` crate already has a `build.rs` file which searches `GTK_LIB_DIR`, so that's not something you have to do yourself. It looks to me like the problem is you've introduced some angle brackets somewhere.
I think you meant \`use crate::bar::\*;\`.
You can use Hyper if you want to go lower level and build something you need . Actix web is not so batteries included i would say more a minimal framework like Flask for Python
Out of curiosity what kind of resource usage do you see from that?
Is this because of it's structural typing, that allows you do things like this: https://play.golang.org/p/I4vKug4Ob5W
in my experience its.reversed too many use it just because its easy. the result alot of mediocare performance hell websites. php devs typically dont care about the performance detail. only a very few do and those who really do care are smart enough to move to another langauge
I'll say that generally, my bottleneck is generally _any_ network call, but a poorly managed database with millions of rows is generally gonna be the worst. Even with CPython (despite the love in my heart is quite slow compared to other runtimes) is able to provide quick responses even with lots of logic going on, even when using a full python stack (say gunicorn and flask) until you need to start making network hops to databases and other services. Once you start doing that, anything can happen. I've seen what should be a very fast index lookup timeout because someone runs a really shit query against a reader replica they shouldn't and suck up all the CPU on the box (as an aside `NOT IN` with a subquery should be a crime, not that I've ever written one of those üëÄ)
But as you said, there is no way to call it synchronously. It has to return a future because only with a future can execution ever be handed over to `g`. Rust might have ditched the `async` keyword and simply made it an error to use `await` inside a function that does no return a future but that would probably create ambiguities. Async functions and ordinary functions are very very different items within the compiler. Also, other languages that have similar patters (JS, C#, Scala, Typescript, etc) all use async on function definitions. Although the `expression.await` syntax is probably unique to rust.
&gt; I don't really get what docker has to do with this? Does it provide just in time compilation, runtime reflection, ... all those other "magic" features that are enabled by the Java VM? I don't get what wrong with docker + jvm. I don't use it personally, but I know a lot of people that use Java microservices with docker and it seems to work just fine Because cloud deployments can charge by time being used. The JVM ads start up time to every single container. If you start up thousands of containers when your application is being heavily used that JVM becomes very costly. The JVM was made to solve cross platform deployments. With docker that‚Äôs no longer an issue. So for many reasons JVM‚Äôs don‚Äôt make sense with container deployments.
I really like error handling in Rust. At first I was like WTF. Java , JavaScript, c++, c#, python all use try catch blocks. But Rust still propagates errors just fine without them.
They said they're using the hosted version, so I assume that means the instance at https://gitlab.com, so they wouldn't actually see the resource usage. Having worked with the self-hosted version personally though, it's a hog. I don't recall specific numbers, but it required a lot of tuning to get the memory usage under control. The gitaly process in particular seemed to have a tendency to just keep expanding its memory usage, likely due to how hard we were hitting it with all of our CI jobs.
I can't speak much to the cost model since I'm not exposed to that. However, I think it's false to claim that the only thing Java brings to the table is cross platform deployments. It enables a lot of other "magic" features that JVM languages use that aren't related to deployment.
I think he means you don't need JVM inside docker. JVM is used to run code on any architecture and OS but as long as you use Docker, you can run any code on any architecture and OS. So in a way, they both do the same thing, e.g. they provide a cross-runtime environment
Yeah I definitely missread that. Thank you for your insight, that is what I thought it would be like.
Arguably, error handling in Rust is more like Java than like Python, in that a function signature specifies any "exceptions" it returns, and they have to be handled by the caller, one way or another. As opposed to Python, where an exception can be thrown anywhere and caught anywhere.
That's not true. You can only run Windows containers on Windows, and Linux containers on Linux. Docker for Windows/Mac abstracts that by creating a Linux VM, but it's for development usage only. It also doesn't abstract the difference between architectures, so x86 containers online work on x86 hosts, not ARM or MIPS or RISC-V or anything else. What docker does is bundle all of the dependencies of your app in an isolated way, and allows each app to access the filesystem and network without directly affecting other apps. It doesn't make software more portable, it makes it more secure and easier to install.
I'm aware of Go's type system, I am referring to the fact that a lot of go code needs to resort to taking/returning empty interface and be cast by its caller, which is effectively dynamic typing
I've been playing around as well and even wrote an application (not too complicated, not too simple, but enough challenge for someone also learning the language). So far I'm in love with the language. Everytime the code compiles, I'm confident the code works fine (assuming I made the logic right). Most of the times this was true. The exceptions are because I was misusing the framework. And there comes the flaws: actix and many libraries I've tried are awesome, but lack a lot in docs. I had to dig a lot into docs.rs to figure out how things works and try a lot until I figured out the correct way to do things. And lastly, IDE support is still far from good. It is usable, but not good. The compiler is still your best friend. I've tried vscode and idea and both have their pros and cons. As also a engineer with Java background, I'd say I would switch to rust without any regrets. I think Rust will earn its space within web development as well, but I don't see it replacing those languages as they're much simpler (and have already shitloads of content to learn)
It is not data-race free though, which Rust considers unsafe.
Serde is good for serializing your data structures, not for parsing arbitrary formats. Here, your hack of splitting the document might be appropriate (YAML was specifically designed to support such line-based processing). In general, you would have to use a generic YAML parser like yaml-rust and assemble the data structure yourself.
For updates, Dependabot is your friend. For example, I update my deps once a month. Of course, that also would cause problems for caching `targets`.
Never types are pretty cool, but I think I speak for many when I say async/await is the number one nightly feature that I am waiting for. Luckily it will appear in 2 releases. I know others are still waiting for most const generics features as well.
I guess what I meant was that if you're fine just bumping dependencies anyway, then why check in Cargo lock?
`const_generics` I don't need anything too fancy about it, just the ability to write code that is parameterized by an integral value. *Of course, I do hope it gets better over time, but for now just being able to write `SmallVector&lt;X, 42&gt;` would really help using the stack more.*
Hey, thank for the info. I'm just learning docker about now and it's great to learn so much from a single post. I actually had to read your comment multiple times to get it. In the first part are you saying that if I build a golang app with binaries for linux [https://www.digitalocean.com/community/tutorials/how-to-build-go-executables-for-multiple-platforms-on-ubuntu-16-04], I will not be able to run a docker container with Apline that runs my app on Windows? Also, do you have any resources where I can read more about this? Thanks
Well, I would also like it to get vector and matrix operation chains that operate on a mix of dynamic and constant dimensions and automatically reject constant mismatches. Tons of other uses exist though. Just the reduced compile times for nalgebra and other typenum users would be super helpful.
I expected as much. :)
In Rust as well as C the names don't matter at all in the compiled code, they are purely a convenience for the programmer. Imagine a simple struct with two ints in a row. When my code passes this struct to your code, it doesn't matter that my code calls them `x` and `y` but yours calls them `a` and `b`. Once compiled, the resulting code refers to the fields not by name but by offset. The first field is 0 bytes past the start of the struct, and the second is 4 bytes in; the names have all been resolved to numeric offsets. In the struct you're working on, `p_name` is 0 bytes in, and both `p_url_address` and `p_ip_address` are 8 bytes in. Adding a name to the union won't change that.
`---` is specified in [section 2.2](https://yaml.org/spec/1.2/spec.html#id2760395) of the YAML 1.2 spec, and [yaml-rust](https://github.com/chyh1990/yaml-rust) (which serde_yaml relies on for YAML parsing) aims to be fully compatible with YAML 1.2, so talking about "arbitrary formats" may be premature.
Oh yes I agree. It is a higher level language. I think the biggest complaints are start up time and jvm size on hundreds to thousands of containers at once. GraalVM tried to address those issues. Not sure how good it is i use go and rust containers at work. Last time I was on a java project it was struts.
 try { ... } catch (Exception e) { throw new RuntimeException(e); } Java combines the worst of both worlds. Panics in Rust, at least, can't be caught.
There's [a lot about PHP you didn't mention](https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/). (Yes, that post is from 2012, but so much of that is either locked in by backwards-compatibility requirements or a symptom of fundamentally flawed development processes that it's still highly relevant.)
At least one reason is so that you can go back and recompile the exact same version of your binary as you did before. See https://doc.rust-lang.org/cargo/faq.html#why-do-binaries-have-cargolock-in-version-control-but-not-libraries
Two features that cut down on a bit of verbosity: [`associated_type_bounds`](https://rust-lang.github.io/rfcs/2289-associated-type-bounds.html) &gt; Introduce the bound form `MyTrait&lt;AssociatedType: Bounds&gt;`, permitted anywhere a bound of the form `MyTrait&lt;AssociatedType = T&gt;` would be allowed. The bound `T: Trait&lt;AssociatedType: Bounds&gt;` desugars to the bounds `T: Trait and &lt;T as Trait&gt;::AssociatedType`: Bounds ['trait_alias`](https://rust-lang.github.io/rfcs/1733-trait-alias.html) &gt; Traits can be aliased with the `trait TraitAlias = ‚Ä¶;` construct. Currently, the right hand side is a bound ‚Äì a single trait, a combination with + traits and lifetimes. Type parameters and lifetimes can be added to the trait alias if needed.
`specialization` - I don't use it super often, but when I do use it, it makes a huge difference.
Of course python is data race free: no two threads ever run in parallel.
I'm hearing that people are moving in droves to Rust. It's early days but even as a webdev who hates C and C++ I'm very attracted to Rust as a way to augment my web skills and potentially have a powerful new tool to build for the web. Rust + WebAssembly alone is a very powerful combination.
Actually, they can. Otherwise a panic in a web server thread could bring down the web server. Not sure where it is documented. It is greatly frowned upon though, and probably a little expensive.
&gt; Panics in Rust, at least, can't be caught. You can catch panics at thread boundaries, so it's still possible in Rust (and arguably very useful). Rust just doesn't make it the easy thing to do (like all the other anti-patterns).
I frequently find myself wanting to write something like let names = thingies.map(get_name).join(", "); only I can't because get_name isn't a method. It's a nullary function. So I have to write let names = thingies.map(|thingy| thingy.get_name()).join(", ") instead. This is kind of annoying. If I had a "method" macro to do the confersion, I could do let names = thingies.map(method!(get_name)).join(", ") which would be a little better. Does such a macro exist? Is it possible to write?
The... angle brackets aren‚Äôt the actual path... :( I just didn‚Äôt think it would help to paste my full actual path at the time. It was something like E:\Programming\Thirdparty\vspkg\installed\x64-windows\lib\gtk-3.lib
It is possible to have a single threaded data race, even in Python. This would involve two separate pieces of code having a reference to the same thing while operating in lockstep or just back to back. It is not possible to have a multithreaded data race, but single threaded is still possible.
If a JVM is written in Java, it still requires *another* JVM in order to run the JVM. Therefore, most JVMs are written in languages that compile to machine code (C, C++). Only the standard library (java.lang, java.awt, java.util, ...) is *mostly* written in Java.
* [associated\_type\_bounds](https://rust-lang.github.io/rfcs/2289-associated-type-bounds.html) * [trait\_alias](https://rust-lang.github.io/rfcs/1733-trait-alias.html) Might not be the most interesting, but they both reduce quite a bit of boilerplate.
There are many things that I wish that I think is cool. I like the declarative macros 2.0, try blocks, unicode identifiers, inline asm, box syntax
This podcast was recorded on a Snowball mic, also from Blue, along with a cheap $5 pop filter. Recording software was Audacity, which is free and OSS.
Along the same lines: `unsized_locals`, RFC 1909. `SmallVec` is a great crate, but the thought that a slightly-too-large number of inputs could cause a dynamic allocation in a hot loop makes me a bit nervous. Actual VLAs would be a big improvement. I believe VLAs would also enable neat tricks like... struct MyBitmap { width: usize, height: usize, pixels: [u8] } We can define structs like these in stable Rust, but we currently have no way to construct them without using arcane unsafe code.
Great, what framework did you use for frontend? For backend, I suggest looking into Rocket, it might be less opinionated, but sadly is still nightly-only
The interpreter was still single threaded, so each page will still have a bad render time. Again, doesn't matter what you do it's still one thread working on it. Not sure if they changed that in the newer versions
Honestly I'd just go a step further and pull up one of those fields from the union, then ditch the union. So: use std::os::raw::{c_char}; #[repr(C)] struct source_t { p_name: *const c_char, p_address: *const c_char } The union here is unnecessary and adds extra noise in my opinion.
`fn function() -&gt; ! {} // this means the function never returns and likely exits` Huh, I'm pretty sure I've used this on stable already to annotate a function that only ever calls `abort()`. Was I actually on nightly by accident?
Yep that already works in stable afaik.
That's a pity, I was really hoping for something cross platform. Have you tried the QT binding generator and, if so, is it as much of a pain as it looks? I shouldn't need too ask too much of it for the app I've got in mind: text input box, text display in scrolling window, links in text, clipboard get and set and a handful of buttons should be all I need.
With ‚Äúarbitrary formats‚Äù I meant something along the lines of ‚Äúmore complex data structures that YAML can represent but Serde wouldn't generate.‚Äù The ability to read arbitrary YAML documents is simply out of scope for Serde. While `---` is more of a syntax thing there are also many tricky semantics, e.g. pointers, tags, non-scalar keys in mappings ‚Äì most of which aren't even supported by yaml-rust at this time :(
Maintaining the macros is somewhat complex and unclear to people unfamiliar with their inner workings. This is a cherry-picked example but I definitely could not tell you the state machine of this macro without studying it for 5 minutes: https://github.com/retep998/winapi-rs/blob/77426a9776f4328d2842175038327c586d5111fd/src/macros.rs#L145-L279
Up until recently, the JVM still had huge issues running in containers, like not respecting cgroup memory limits. Fat JAR + JVM kinda replicates the ease of single binary deployments, but not really. We have to rebuild stable image tags all the time to update the JVM. JVM requires a lot of tuning to ensure memory use is consistent for sizing containers. Most of these are pain points with using Java in general, but it's definitely more friction than languages where you can just deploy a simple binary in a scratch container. Java's bread and butter has historically been big monolithic applications deployed into application servers. It's totally possible to write lightweight services in Java these days, but containerization is ideologically a shift away from pretty much everything Java has historically done.
Especially these days
&gt; this work is beyond my complexity budget and I don't understand it &gt; this doesn't work You can't really claim both.
I hear you, being able to actually manipulate the integrals at the type level would open up quite a few uses, and I certainly hope it gets there. I'd be happy with just the first step, though, as it is really helpful to avoid memory allocations.
This article is quite fun is still relevant. I encounter some stuffs said in the article even nowadays in 2019. I think it confirms my point about the confidence in the code.
I should have used another name... when I mentioned `SmallVec` I meant *fixed* capacity vector. Maximum N elements, any further insert results in a panic. Otherwise, VLAs are neat as a language feature, but I really don't want `alloca`. Dynamically-sized stack-frame sizes are fraught with peril, not only stack overflows, but also performance cliffs that are not that obvious. I think it should be possible to use a (parallel) stack, just for such dynamically-sized locals; according to SafeStack benchmarks, a second stack has negligible effect on performance, so it looks like a good fit. And then we'll be able to return `dyn Trait` objects.
"a lot of go code" I've used go at my last three jobs and I never saw this pattern. Sounds like poorly written go to me.
Ahh. Fair enough.
Can I borrow it?
Yes, but not mutable
In my experience using Haskell I've always found this to be a much more natural way of expressing the grammar of a language (likely because it's very similar to BNF notation). I'd be interested to hear how it works out for you!
I started learning Rust using last year's www.adventofcode.com The challenges unlock one per day during december, but you can go back and do previous year's problems at any time.
I love rust as much as the next guy, but the overhead of Java doesn't really matter much in the cloud. If you're already in the cloud, you can horizontally scale as much as you want.
I believe you can already have never types in stable already. You can create an empty enum, which can be coerced into anything. Rust even knows that you can't execute code after you have produced an empty enum. enum Void {} fn create_void () -&gt; Void { panic!("Can't instantiate void"); } fn coerce_void&lt;T&gt;(impossible: Void) -&gt; T { match impossible {} } fn main() { let bottom = create_void(); let anything: f64 = coerce_void(bottom); }
Yep but that's quite obtuse and I like the quality of code... :) At the moment FromStr impl for String uses this method. And I am willing to bet a few other places do this too.
Thanks, so I'll split the file before deserialization. I understand the rational behind not supporting pointers, tags, ... However the \`---\` syntax seems less exotic IMO and pretty well supported by ser/deser frameworks in other languages (e.g. Jackson/Java, Golang).
You're looking for r/playrust
&gt; Being able to build an rlib and add it to the Cargo.toml should be a thing. Can't you? You can just add the rlib to the link thing or add the crate with a specific path in your filesystem... Also in 1.36 landed an `--offline` thing that you mind find useful.
Ahh the second one is really nice. Didn't know it was an RFC.
Try Blocks? Not sure about that... :/
I'm no macro guru, but believe this is correct: macro_rules! method { ($m:ident) =&gt; (|x| x.$m()) } You may find that the closure has trouble with type inference at times. (This will happen whether or not you use the macro; the macro expansion pass is complete before type inference.) If so, it's cleaner to use universal function call syntax, and you might prefer it in general: let names = thingies.map(Thingy::get_name).join(", ")
You guys hiring?
It actually does get extremely close, and it's faster than go in most situations.
Sent a DM
GitLab's system requirements are not very light. The choice of language probably has some influence on this. But also, probably they aren't too focused on minimizing the system requirements, and more on scaling to the size of sites like gitlab.com. &gt; You need at least 8GB of addressable memory (RAM + swap) to install and use GitLab! The operating system and any other running applications will also be using memory so keep in mind that you need at least 4GB available before running GitLab. With less memory GitLab will give strange errors during the reconfigure run and 500 errors during usage. https://docs.gitlab.com/ee/install/requirements.html
There are two idiomatic ways to handle configuration odds and ends like that. One is to initialize a \`struct\` which contains that stuff and pass it around by \`&amp;\`-reference. This is equivalent to the Structured Programming paradigm: just don't use global variables. The other idiom is to have a \`lazy\_static!\` in a module somewhere which is accessed via getter functions. In fact \`std::env\` uses this pattern to express the idea that the process has only *one* set of command-line arguments and environment variables. This is Rust's interpretation of the Singleton pattern - note that Rust doesn't require functions to exist within classes, so an actual singleton object usually isn't required - it is required then it will be accessed or instantiated by a getter function such as \`std::io::stdout\`. &amp;#x200B; The singleton approach has all the usual singleton-pattern baggage - for testing you might need to mock-up the external resources. And the arguments-only approach puts an extra argument on all functions that represent major subsystems or subdivisions of the task. Likewise, singleton often yields cleaner code (or at least more implicit code) but argument-only makes it much easier to have your program invoke parts of itself without needing to fork a child process. Personally I like the singleton pattern for things which can logically only exist once per process or per thread, and for out-of-band concerns - the \`log\` crate would be much more cumbersome without \`log::set\_logger\`.
I don't believe there is a self-hosted git service written in Rust yet. I was throwing around building one eventually, but have decided not to because Gitea is good enough for my needs. That being said, if someone WERE to make a self-hosted git service in Rust, I think a really good name for it would be `grit`.
Great name
The most obvious reason not to use macros: if there's a nice solution without macros. If the type system makes it possible to define a function that does what you want, instead of a macro, it will probably give you clearer error messages, be easier to understand from reading generated documentation, etc. For example, [nom 5](http://unhandledexpression.com/general/2019/06/17/nom-5-is-here.html) provides functions that can replace the macros it previously used, since Rust didn't initially have the ability to implement the desired functionality as macros. Generally, macros are less clear (since functions and other parts of the language are capable of more clearly limited behaviors). So it depends how much of an improvement the macro provides over alternatives, how easy it is to understand and explain, etc.
If \`get\_name\` is a method on \`Thingy\`, why not \`thingies.map(Thingy::get\_name).join(", ")\`? Needs no macro and reads better in my humble opinion.
I have to admit, the benefit seems a little small considering that the desugar for `!` only gives `enum Void {}` (or `Infallible` in `std`), and the RFC states that both are strictly equivalent. Probably a more interesting tangent is to realise that the `-&gt;` arrow in function types is logical implication, and `!` is the bottom type. This means that the type `fn(P) -&gt; !` if populated with a nonterminating body is the same as ¬¨P (read: not P). We can use that to write logical proofs in Rust. For example the proof for the principle of explosion in Rust is as follows: fn explosion&lt;P&gt;(p_is_true: P, p_is_false: fn(P) -&gt; Void) -&gt; Void { p_is_false(p_is_true) }
Plus, when it comes to ease of introducing bloat in the compiled binary, macros definitely come out on top, followed by functions with monomorphized trait-bounded signatures and large bodies.
There's no reason to not be performance conscious, if only to save money.
Yeah, after I posted that I started wondering if ten years was a little excessive. I think I'm also a bit biased by the fact that my main experience with C++ is LLVM--they're using C++11, and I'm not sure when they'll ever update. &amp;#x200B; Not that I disagree with using C++11 for LLVM; I just want to have my cake and eat it.
This sounds very interesting. Would you DM me too, please?
Have you seen `diesel`?
Have you seen `diesel`?
I wasn't able to successfully download that PDF on my phone, but I did find \[another resource for implementing generativity\]([https://brson.github.io/rust-anthology/1/rust-reuse-and-recycle.html#generativity](https://brson.github.io/rust-anthology/1/rust-reuse-and-recycle.html#generativity)).
They're not really like exception handling, but rather a way to localized the scope of the `?` operator so that instead of early returning, it propagates the error up as an expression to some outer scope. For example: https://play.rust-lang.org/?version=nightly&amp;mode=debug&amp;edition=2018&amp;gist=ade231a58337edc02a28978628e5d630
IIRC gitlab was at one point just a front-end for gitolite. If you don't care about the many features of web platforms, gitolite+ssh is what I'd use. It's as lightweight as it gets.
That would really be nice.... :)
I don't really have any experience with large scale deployment of web applications. But greater efficiency should reduce hosting costs or allow handling greater loads at the same cost. Although at least things like VM startup time, which is quite annoying for CLI programs in Java, isn't such a concern. It also seems like the advantages of the JVM are reduced. Applications that are binary compatible across architectures and OSs are great for certain purposes. But if you're deploying your own application to your own server, it doesn't seem particularly useful. (I guess it could be helpful for making it easy for others to deploy a generic web application on whatever server they are using.)
Hmm, I was wondering about this. I'm coming from JS where making it async would improve it. Does that mean in Node if something is async it's actually using another thread?
Oh yes, and that certainly, if your macros are large. For quite small macros or functions, this isn't really a concern because the function likely would/should be inlined anyway.
If there is clear prior art on how those are handled, you should file a feature request for serde-yaml.
Better const support (like Vec::new) and libtest.
Oh that's nice.
&gt; dies Yep - Looks like the best bet, but not (yet) on the level of Django's ORM or SqlAlchemy.
Yeah you're absolutely right, there.
That's the behavior of the standard library, yes, but it's not part of the language. Maybe I'm writing too much `no_std` code :)
Yeah I agree with you about singletons. I mean, especially in the case of like a simple webserver implemented in a single file it's not necessary to complicate things. It's always possible to refactor if the project grows in scope.
When people talk about Java, they usually mean two different things: The language (syntax and semantics), and the JVM. You've mentioned Graal as an alternative for the JVM, but if the JVM itself isn't a complete show stopper for you, you should also look into alternative JVM languages like Scala and Kotlin: &gt;Java is also verbose, and it will only get worse the more features they will put in Both Kotlin and Scala are very concise, more so than Rust. &gt;They have been investigating potential performance tweaks like [value types](https://cr.openjdk.java.net/~briangoetz/amber/datum.html) for years now, but I doubt those features will land any time soon Kotlin supports [inline classes](https://kotlinlang.org/docs/reference/inline-classes.html), although they're currently experimental. &gt;There is no unified way to write async code, just frameworks like [RxJava, RxJava2](https://github.com/ReactiveX/RxJava) and [Reactor](https://projectreactor.io/) \- although they offer more than just async-await - as far as I understand the later Kotlin supports coroutines for async code. As a side note, I'm surprised you're so concerned about performance for web applications. My experience so far has been that web applications rarely have performance issues due to a slow VM or interpreter. Once you throw a database into the mix, backend performance is often determined by your database design and the queries you run against your database.
Thanks, I will check out Rocket. I didn't use a rust framework on the front-end (other than bindgen if that counts). I just did the model layer of the front-end in wasm, the rest is a standard React JS app. Usually I like to work without frameworks first (within reason) to understand what's going on a bit better.
The link for the demo ROM picture doesn't work for me
`!` has been a special function return type as long as I can remember; making it an actual type that can be used in other places is the part that's still being worked on.
Rustfix uses "deserialize iter" or something similarly named for newline-delimited json. Maybe that also works for yaml or could be a way to implement it.
Yeah, not sure why Reddit doesn't like my videos. I've linked directly to the [gif](https://github.com/RamiHg/RustyBoy/blob/master/docs/rustyboy.gif) instead.
No he is saying that to get around the issue of running linux containers on linux, both macOS and Windows create a virtual machine running linux and run the linux container on the VM.
Yes you can self host and it's open source
What are *your* primary uses for const generics?
The `-&gt; !` syntax for diverging functions has been stable completely separate from `!` as a proper type. Of course, work on the never type is generalizing the semantics here -- and it appears that it enabled you to use [some tricks](https://github.com/rust-lang/rust/issues/58733) to extract a `!` return type to a proper type.
&gt; many developers don't feel productive with it after some months. I think this is definitely a key and important aspect. I also think that the language is young enough that we'll start to see better idioms in Rust long term that reduce the mental overhead and lower the barrier for entry (probably from macros). However, my fear is that Rust's macros will eventually be too magical and we'll end up with a Rails style - it's magic until you hit an edge case - and then it falls flat. It's disappointing when I note that there are people that are against things that make adoption easier (e.g. named arguments/parameters, packing/unpacking arguments or arg expansion, a high number of mini-crates to handle compile times, etc.). I am looking into rust because of it's performance. So accepting some of these problems is just "part of the curve". But ergonomics thus far has been more of a "well safety comes first, and then performance.... and ergonomics is just nice to have". I'm not a big fan of Go, but it took the opposite approach and it has a significant adoption for that reason. Developer time (and mental overhead) is still quite critical - especially in an agile world.
What you're describing sounds like a race condition, not a data race.
[This Rust quiz](https://github.com/dtolnay/rust-quiz) could be pretty fun to do with the language reference at hand.
I recently wrote a cross-platform Rust program with a Qt front-end. I just compile the Rust code as a static library and call it from C++. github.com/spieglt/cloaker
As someone who has never dabbled with such things, could you give an example of where specialization really shines?
I'd be willing to be that someone already tried this, and maybe even published a crate. Regarding your question: indeed, the `TokenStream` is what it says, a mere sequence of tokens; it must adhere to some syntactic rules, but besides that is pretty free-form, and need not match Rust syntax. This allows macros like `json!` from `serde_json` to work, which accepts something that would not parse as valid Rust as input. If you want your macro to "understand" Rust syntax, you can use the `syn` crate to parse the `TokenStream` into various AST nodes. /u/jonhoo has done a [great video series on procedural macros](https://www.youtube.com/watch?v=geovSK3wMB8).
Simply put: no memory allocation. Many collections can be made with an internal array of N elements (`[MaybeUninit&lt;T&gt;; N]`): Vec-like, Map-like, etc... Unfortunately, there's no one size fits all; depending where the structure is used you may want only a handful of elements (8, 16) or you may need a larger amount (64, 128). This calls for being able to parameterize `N` where the collection is used, which requires const generics. Now, strictly speaking, it's possible today to work around the lack of such parameterization. However, it's a feature gap with C++, making it harder than necessary to pitch Rust for new projects instead of C++. So I'm hoping that once Rust has minimum support for integrals as generic parameters, I'll be able to get people excited about it. *Note: there are other gaps, of course, however, perhaps naively, I do not think the work I tend to rely as much on, say, specialization or variadics, as it relies on const generics.*
Thanks, I will take a look at Hyper.
I think there is no *easy choices* in this case. Rust lets us be confident in our code, it's robust and so on, because it is what it is. The design choices that have been made make it. The learning curve is quite hard (I needed lot of time to be a bit productive) even with the resources we have, but some companies are doing great things with it. I guess the *magic* part of macros is here to make our life easier, but this is great as a user. Once we try to dive deeper in the language we need to remove the magic part. But that's possible, and it lets us progress step by step I guess. As you said Rust is a a young language, some changes have been made, some big new features have been integrated in the standard library, it will evolve. These last months several companies wrote about Rust and its integration in their processes and the more Rust is known and used the more it will evolve to be approachable. We should let it some time. But the productivity is critical in a company, and the Rust team is aware of that. I believe Rust deserves its learning curve, the result is great. Once that's said, I understand why some companies prefer PHP when they launch a new project, because that's easier to recruit developers, mainly. There are pros and cons, of course.
Right now, advanced slice patterns. I've been working on a stack-based virtual machine, and when it comes to executing instructions (which are just an enum), I've found I can work on a slice of instructions and actually peek ahead at the next instructions using slice patterns. This allows me to match patterns of instructions which I can optimize. One of my favourite examples of this was finding the distance between two points: let mut instructions = &amp;self.instructions[..]; while !instructions.is_empty() { let next = match instructions { /*...*/ [Instruction::VectorSub, Instruction::VectorNormSquared, Instruction::FloatSqrt, ..] =&gt; { let (a, b) = stack.pop_two_vectors(); stack.push_float(Point3(a).distance(Point3(b))); 3 }, /*...*/ }; instructions = &amp;instructions[next..]; } In the real code it's even simpler than that. (It's split into a function that returns 1 unless otherwise specified, so regular instructions are super simple). This is effectively zero-overhead, and allows me to simply group together relatively expensive loop iterations. Managed to get my benchmark for one program from 400ns down to 182ns using stuff like this. Another benefit is that I can reduce my instruction set drastically, by emitting "dumb" verbose code such as for that distance between points thing. No need for a `Instruction::PointDistance` if there is zero overhead to using its individual instructions.
Proc macros in an expression position. When this feature ends up in stable we can have crates with much more powerful macros.
That‚Äôs great! Congrats
I'll take the rust we already have, just with less bugs in cargo.
What is the specific reason you need a git service written in Rust? I don‚Äôt get it.
I feel like you've never seen [catch_unwind](https://doc.rust-lang.org/std/panic/fn.catch_unwind.html). Thread boundaries are not needed to catch panics. You can literally catch any panic that is thrown from any code run inside `catch_unwind`, and you can nest `catch_unwind` blocks perfectly fine.
On first look, it seems like reverse iteration would be acceptable. That of course might make your life harder if the condition is working with mutable state - i.e you are counting how many things you are leaving in your vector and want to take the first X of each type rather than the last X. But otherwise it could work okay. However, I'm also all for using \`retain()\` over either method (forward or reverse) as it could potentially compile down to far more efficient code. For instance the Vector does not need to move the elements until all elements have been considered and perhaps deleted when using retain.
So trait aliasing means reclaiming our clean type signatures! That's a game changer!
Is this open source? I'd love to read the code!
Very interesting.... :)
Aah I remember a friend complaining to me about this... Hopefully this will be in soon.
This is wonderful. Going to check out the code later today.
You have to open the link in a new window (on Firefox at least).
If your panic_handler doesn't unwind that function does nothing, and it's possible to implement threading such that you don't have unwinding, but still can report panics back to the parent. But yes I didn't consider unwinding support, it's comparatively new and actually only meant for FFI interop.
I did a fair amount of googling yesterday but couldn't find an answer. I have a library I compiled that I initially was just linking directly from the application. Instead I wanted to compile the library to an rlib to make it easier to pack up and transport. I couldn't figure out how to set it up in the cargo file to call the rlib file
&gt; As a side note, I'm surprised you're so concerned about performance for web applications. My experience so far has been that web applications rarely have performance issues due to a slow VM or interpreter. Once you throw a database into the mix, backend performance is often determined by your database design and the queries you run against your database. It's not performance, it's the overhead that I don't like. * "Binary size": Let's say you have a microservice, 10mb Binaries Rust/Java each (let's assume their size is equal: boom, +200mb JVM size, that's 10 vs 210mb. * RAM usage: don't be surprised if Java used 10x the RAM. And it get's worse: you can / should fine tune your heap sizes and garbage collectors to your environment, especially for OpenShift * Peak performance: Somewhere between 30% - 30x slower, but I agree, for web apps it really doesn't matter because I/O is the bottleneck * on top of that: even small Java web apps need 30 seconds or even minutes to start up. Remember: in hosted environments you often get only 1 CPU assigned. Rust also plays in another league here
Javac is also written in Java I believe.
&gt; We couldn‚Äôt find any code matching 'unsafe' in projjal/indydb Well done.
Maybe I misunderstand, but isn't gtk cross-platform?
Big thing for `const` support is branching. Once that happens, `.unwrap()`/`.expect()` will be `const`, as will a ton of other functions in std.
Oh yeah, totally, `retain` is better than either manual approach. I was just wondering if there was some pitfall to the reverse approach that I wasn't aware of. `retain` does guarantee that it processes the Vec "visiting each element exactly once in the original order", and given that the closure is FnMut, that could be important. Thanks for clarifying!
Sadly, no. It's for a product I'm working on.
I'm not really interested by qt so I didn't try. I also try conrod because it's made by piston team... But also not prod ready and seems less active. If you like static type system and need cross platform OCaml + jsoo + electron is a more viable choice
There's a cool feature hidden behind your error message. :-) The name `bar` as expression is actually not of type `fn()` (hence, compilation fails). But that's a good thing. It's its own unique type that, when passed into something like this fn bar&lt;F: Fn()&gt;(fun: F) will make the compiler create a special version of bar that *directly* calls `baz` (subject to inlining in release mode) as opposed to an *indirect* invocation which would make inlining harder. So, it's a performance win.
That like a checked `Exception`. Some may think it is an anti pattern, but in e.g. Spring I like to throw a custom `RuntimeException` anywhere in the code, catch it inside a global error handling and let the error handling return the proper http response (status). Is this possible in `Actix-web` or impossible with Rust because it would be a misuse of `panic!`?
Recursion has equivalences with Iterators. But iterators can be stored, can be generated from ranges, can be used to fuse algorithms together without making your brain hurt. Iterators are superior. Iterators are love. Iterators are life.
As many have already pointed out, it is already gaining a lot of market share, even in areas which were once completely locked down by other languages. Looks like you just need to explore things in Rust a bit more (as you yourself pointed out in your post).
Awesome!
I think that panics don't clean up any memory, so every time you do that, you're going to get leaks. In my Rust code, nearly everything (including `new` functions) return a `Result`. It's not that hard, since you just have to add a `?` on such a call to pass the error down when you don't want to do anything special.
Extremely cool project! I like how small the CPU core is!
I think you can make a build.rs file to link your rlib. I'll try something and let you know if I can get cargo to link an rlib.
I mean, GTK works on MacOS and Windows, although it won't integrate with the system theme like with GNOME. Honestly, IMO GTK is a perfectly nice GUI system and I'd recommend it.
Node does create several threads, just the event loop is single threaded. In PHP you create a bunch of threads running the requests, but that only improves the throughput, not response times. Besides, performance in PHP is death by a thousand paper cuts. I hear it's better in version 7, but my company was stuck with 5, done the new version is not compatible
ah ok! still a cool idea though. I wrote an emulator in rust during the most recent Advent of Code. When you shortcut the next instruction, do you manually increase the instruction pointer by 2 then?
Oh, seems I had a misconception about the difference between the two. I guess I don't know what the terminology is for the class of race conditions that Rust protects against. I assumed that data races was a pure subset of race conditions, but after looking it up that seems wrong. In Rust, it seems you can prevent race conditions if you construct an API with the correct lifetime and borrowing semantics by grouping the things that cannot be mutated or read simultaneously into a struct. That is about as good as I can get to explaining it in technical terms.
5? That's rookie numbers, pump up these numbers!
Yep! You can see that in the above code, where it returns `3` to `next`, which increments the `instructions` slice on each loop. Although in my real code, the instruction execution is in another function away from that loop, where at the bottom it always returns `1`, and specialized instruction-combos that take more than one instruction at a time do an early return of however many they consumed. That way I can have regular single-instruction execution be very simple.
Fewer.
Generally at the point where you need to spin up more replicas with a JVM application (assume it's written half decently) the cost of spinning up a few more replicas is inconsequential. It's worth noting I help manage a few Kubernetes clusters at work. The benefit of using the JVM is the huge amounts of tooling and the familiarity with the language. With our Jenkins instance I can click a button and get a dump of what threads are doing, if they're blocked, etc. Just because you aren't deploying to multiple environments doesn't mean the benefit of the JVM is lost.
Because I didn't know that map worked on methods. I though the argument to map had to be a regular function from A to B, not a method of A that returns B. I assumed that the &amp;self argument was different somehow and wouldn't typecheck. I just tried it out in the playground, and found out that not only was my closure unnecessary, but you can even call methods in prefix position. Thingy::get_name(&amp;t) compiles without complaint and does the same thing as t.get_name(). Y'know, just when I think I'm getting the hang of this language...
I'll take the rust we already fewer, just with less bugs in cargo. ;P
You have a point there.
I didn't know about this feature, interesting! Just curious, where do you stand on the upstream-preferred vs downstream-preferred defaults? Downstream-preferred seems to be more similar to Objective C's swizzling, but doesn't seem as safe (and more difficult to implement)
For the first point - the recommended setup by Oracle nowadays is to use modules and jlink to make a "native" binary. While you can still use a Java image (and I'm sure that's what a lot of people do), you don't have to. [https://docs.oracle.com/javase/9/tools/jlink.htm#JSWOR-GUID-CECAC52B-CFEE-46CB-8166-F17A8E9280E9](https://docs.oracle.com/javase/9/tools/jlink.htm#JSWOR-GUID-CECAC52B-CFEE-46CB-8166-F17A8E9280E9)
Sometimes tracking protection will prevent embedded content from loading. (I'm no dev, but apparently services can include tracking capability when a simple image/etc gets opened.)
I feel like for a lot of use cases, the added dev time to make something more performant (especially if it involves switching changing toolchains / languages from something like Java to Rust) is rarely cheaper than just paying the extra cost in computation. Especially since Java is still fast, even if not as fast as Rust / C++ / ...
Thank you. This will be most helpful. On linux you can use the linuxdeployqt app to bundle the executable into a single AppImage bundle like on mac, even if you‚Äôre linking to Qt dynamically.
That is probably part of it but image hosting on Reddit works with tracking protection on other hosts, so I assumed it was a GitHub related "feature".
Interesting, thanks
The information you want is not available at macro expansion time. Macros are purely syntax-to-syntax transformers. They are expanded long before the compiler has started to think about a type system, so there is no way to look up methods. A macro that does this delegation might be possible, but it would have to encompass both the trait and impl declarations so it could parse out the information. &amp;#x200B; There have been some [RFCs](https://github.com/rust-lang/rfcs/pull/2393) to build this into the language, but AFAIK none have succeeded yet.
It's just jvms all the way down.
Both my [mutagen](https://github.com/llogiq/mutagen) and [overflower](https://github.com/llogiq/overflower) crate rely on specialization. Otherwise both would need full type inference for changing code. Specialization + proc macros is a very powerful mix. Also I'd love to have working macro expansion in proc macros. This has been a pain point for me for some time.
Yes, that's right. Javac is only the compiler, not the runtime, so this is no problem. According to [this stackoverflow answer](https://stackoverflow.com/a/9222031/3393058), javac was bootstrapped from C.
Does the library specify the reason the type can't be on the heap?
A
You're right; I forgot to mention the `syn` crate that converts it to an AST.
That's what I thought. I have access to the tokens in the `struct` declaration, but not to the tokens in the `trait` declaration that I would need to implement for the `struct`. I was kind of wondering why nobody had done this yet.
Does std::pin do what you're looking for? https://doc.rust-lang.org/std/pin/index.html
This has come up a few times, but I do not see it happning in the near future. Crate which does this using macros: https://crates.io/crates/delegate Some discussions: https://internals.rust-lang.org/t/new-rfc-for-delegation-anyone-interested-in-contributing/6644/59 https://internals.rust-lang.org/t/3-weeks-to-delegation-please-help/5742 https://github.com/rust-lang/rfcs/pull/2393 https://github.com/elahn/rfcs/blob/delegation2018/text/0000-delegation.md
I at least make 10 macros a day, 5 in the morning, after I wakeup, and 5 moree after dinner.
Yeah, methods are just syntax sugar for automatically filling in the self parameter (which is just a normal parameter with the type `Self`/`&amp;Self). This is known as the [UFCS](https://doc.rust-lang.org/1.1.0/book/ufcs.html).
As long as you don't have `panic = "abort"`.
Boilerplate begone! [implied_bounds](https://rust-lang.github.io/rfcs/2089-implied-bounds.html) will also help in that regard, although I am not aware of a feature attribute, unlike the previous two listed: `#![feature(trait_alias, associated_type_bounds)]` With the [new-style trait solver](https://rust-lang.github.io/rustc-guide/traits/index.html) (chalk), we will get implied bounds [for free](https://github.com/rust-lang/rust/pull/56384). Note: the new-style trait solver is a prerequisite for generic associated types ([GATs](https://rust-lang.github.io/rfcs/1598-generic_associated_types.html)) (which is a prerequisite for `async fn` in traits), and the primary user of chalk at this time (that I'm aware of) is [rust-analyzer](https://github.com/rust-analyzer/rust-analyzer).
A never type can easily be created like this: enum Never {} Since the enum has no variants, it can't be created, so it is equivalent to `!`. The experimental feature I find most useful is the `box` keyword. Imagine you have a enum like this: enum Term { Num(f64), Add(Box&lt;(Term, Term)&gt;), Sub(Box&lt;(Term, Term)&gt;), Mul(Box&lt;(Term, Term)&gt;), Div(Box&lt;(Term, Term)&gt;), } A `Box` can't be destructured, so you need the experimental `box` keyword for pattern matching: use Term::*; match term { Add(box (augend, addend)) =&gt; {} Div(box (Mul(box (multiplicand, multiplier)), divisor)) =&gt; {} _ =&gt; {} }
Also, Docker isn't a VM.
Specialisation allows you to have a generic "blanket" implementation of a trait to cover as many types as possible, while still being able to special-case behaviour for specific types. It's generally used for performance optimizations. For an exaggerated example, imagine an image-processing library which has: trait WritePixels { fn write_pixels(&amp;self, dst: &amp;mut [u8]); } You write a generic implementation for any image: impl&lt;T&gt; WritePixels for T where T: Image { fn write_pixels(&amp;self, dst: &amp;mut [u8]) { for x in 0 .. self.width() { for y in 0 .. self.height() { dst[x + y * self.width()] = self.pixel_at(x, y); } } } } You notice that this is much slower than it needs to be, since some of your images already store their pixels as a Vec&lt;u8&gt; with the appropriate byte-ordering. Therefore you want to add a more-specific implementation for better performance: impl WritePixels for BitmapImage { fn write_pixels(&amp;self, dst: &amp;mut [u8]) { dst.copy_from_slice(self.bitmap_pixels()) } } Right now, you can't do this. The typechecker will complain that there are multiple possible implementations of `WritePixels` for `BitmapImage`, and it doesn't know how to choose one. Specialisation will get rid of this error.
Thanks. Electron is cool and I really like javascript so I'd merrily use that if it didn't make such massive, slow to load binaries. I want to write a small, simple, fast graphical utility and I just can't justify the overhead of electron for that. I've had some positive results from conrod this evening though so that might be the way forward.
You might be looking for r/playrust.
Thank you my man
Data races *are* what Rust protects against. The Rustonomicon defines this as: &gt; - two or more threads concurrently accessing a location of memory &gt; - one of them is a write &gt; - one of them is unsynchronized It is also possible to construct an API that is free of race conditions in Rust, and the language does provide tools to help, but the Rust doesn't guarantee protection against race conditions like it does with data races.
This pattern is everywhere, including go's standard library. Check out e.g. h[ttps://golang.org/pkg/sync/#Map](https://golang.org/pkg/sync/#Map)
It is such a nice feature in C++. Being able to say that this is not a vector but a fixed sized array that has a real interface.
This is a very strange demand from the C library, we might be able to help you better if we understand WHY it must only be on the stack
Great and thanks a lot! Might it be possible to somehow compress the next episode more? This one comes in at 124MB. That might not matter in most developed countries, but hits hard if you are on a tiny German mobile data plan :P
I just wish I could use it in methods, `with (self) ...`
Yeah, linuxdeployqt was my backup plan if I couldn't get it the static binary working.
Try the raw github link you get when you open the image in a new tab: [https://raw.githubusercontent.com/RamiHg/RustyBoy/master/docs/rustyboy.gif](https://raw.githubusercontent.com/RamiHg/RustyBoy/master/docs/rustyboy.gif)
I don't like the `box` keyword, but I really want a way to destructure pointers other than references. `box` may be useful, but what about `Arc`, `Rc`, or `Cow`? Or heck, `String`?
I think I'd do let x = p .file_name() .map(|name| name.to_string_lossy().into_owned()) .unwrap_or("".into()); `to_string_lossy` replaces non-Unicode sequences with `U+FFFD REPLACEMENT CHARACTER` instead of giving `None` like `to_str` does. I guess you could also do `Path::display(name.as_ref()).to_string()`. The docs don't appear to say how `display` handles non-Unicode data.
The unsigned integer subset of \`const generics\`. \`const generics\` beyond unsigned integers is also cool and would also be nice to have. But, IMO, the lack of *specifically* unsigned integer \`const generics\` constitutes a *hole* in the language design rather than just a desired feature, as evidenced by all the weird impls in std for different array lengths. It would be *really* nice to fix that. I don't know if it's possible to do this, but if just integer \`const generics\` could be stabilized sooner as an MVP, without being blocked on figuring out all the fiddly stuff needed for the broader const generics feature, I would *really* like to see that happen.
That's a good point. So far, I didn't feel the need to destructure an `Arc` or `Cow`, whereas the `box` keyword has made my life easier. It isn't perfect, but maybe someone will come up with a better, more general solution :)
I was under the impression that GTKs cross platform support was tenuous at best but then I haven't actually looked at it for several years so it might well be better now. Their website does says it's cross platform so I'll look into it.
What does `&amp;[String]` represent when used as the type of a function parameter? I noticed it in the I/O project from the official book. It appears that this function will accept a `Vec&lt;String&gt;`; is it meant to represent a generic collection?
I don't know how anyone can sensibly claim new features have made Java more verbose. I can recall what it was like prior to annotations where every JEE module was XML soup.
Oh okay, I don't care about system theme integration so that's not a problem. I have used GTK in the past and got on pretty well with it but I was targeting linux desktop only. I was under the impression that GTK support on Windows and Mac was a bit sketchy but I guess things must have improved in the meantime. Thanks for the advice.
whats going on with EU facepunch 4?
That's one library which is limited by lack of generics. That never comes in for real world scenarios outside of writing data stores. To say the pattern is common isn't correct
Just for the exceptions, I'd like to say something. - Exceptions are "go-to" in another form. - You can return at anytime using "return". - Please consider *error as a part of successful processing result* rather than an abandon of processing. Correct program should not abandon execution at any stage. If you have no problem with exceptions already, I think you are already using exceptions as control flow rather than "abandon processing". In that case, maybe you are using exceptions to exit multiple call stacks at once. I think most instances of this structure can be converted into closures with classical control flows like nested `if` and `for`. And you still can use nested `Result` return at last resort.
It is. It's just called `PartialOrd` instead.
I assume it also replaces the unicode characters. Either way I would say it's fine if you're only doing visual stuff with the filenames. Which is does seem like OP's intention. But remember it " replaces non-Unicode sequences with U+FFFD REPLACEMENT CHARACTER" so if you then try to open that file, you're actually opening a different file.
Which C library are you interfacing with?
As a professional Java developer since it was released, and one that works on sizeable web applications using angular, spring and postgres, I think Rust is absolutely heading in the right direction. I've been developing my Rust knowledge by following in the footsteps of my day job and so far Rust is measuring up rather well in terms of complexity and effort (if you ignore the learning curve). Rust has weaknesses, mostly in the IDE tooling area but also in a general lack of guidance on current best practices for these types of applications - notable because the fairly mechanically designed web application backends are probably most of the server side development out there. Most of the rest being presentation and integration logic. Rust lends itself to some amazing systems programming and may slot well into that integration space quite well but that isn't where the bulk of developer effort is spent. Rust does have pretty damn good strengths to balance those weaknesses already and, where it's weak, things will continue to improve as they're either tool building or collecting a more easily discoverable corpus of knowledge for common problems. In terms of comparing frameworks, I've found Actix-web and diesel-rs to be pretty good in comparison to the comparable parts of Spring Boot. Actix does a good job of enabling efficient multithreaded development with no more ceremony than in java but with, anecdotally at least, more predictable performance and a much smaller memory footprint. With the ability to produce WASM I wonder if I'll find myself using TypeScript with angular far more for just the core UI orchestration than any significant logic. There is great stuff coming for Java, I follow the various projects quite closely, but the meat of them is probably several releases away. And with all of that change, VM, memory overhead and GC unpredictability will likely continue to be a leaden weight in this containerized world. Rust won't replace Java anytime soon but, just like it's eating into C and C++ spaces I think it will do more than nibble at the edges. Heck, if IDE tooling improves enough then it's batteries included build process might make it a practical replacement where people are writing perl and python (though Go will get there first I think).
Sent a message via chat
Macros don't compose. This has been the general problem with macros in every(?) language since forever.
Something that is worth watching before tackling such organizational problems is Brian Will's series on OOP - [Object-Oriented Programming is Bad](https://www.youtube.com/watch?v=QM1iUe6IofM) - [Object-Oriented Programming is Embarassing: 4 short examples](https://www.youtube.com/watch?v=IRTfhkiAqPw) - [Object-Oriented Programming is Garbage: 3800 SLOC example](https://www.youtube.com/watch?v=V6VP-2aIcSc) (This one is actually an NES emulator. Very similar problems with the original code to what you're facing right now.) His claims sound pretty overarching and the titles are definitely clickbaity, but it should be noted he's using a very specific definition of OOP that [he addresses in a later blog post](https://medium.com/@brianwill/how-to-program-without-oop-74a46e0e47a3). `class`es on their own obviously aren't evil, but jamming nearly everything into encapsulated objects calling each other almost always is. Sometimes you just need to reset your mental model of the code, and go back to mostly procedural code manipulating the content of structs from the outside.
That's surprising., because it doesn't work the other way 'round. For example, [here](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d480da87c57696eaca5efa57bc2a2759) the get_name function is &amp;self-&gt;String and other_get_name is &amp;Thingy-&gt;String. I can do t.get_name() or Thingy::get_name(&amp;t) or Thingy::other_get_name(&amp;t), but if I uncomment the line with t.other_get_name() it won't typecheck.
Instead of doing that, just use `std::convert::Infallible` -- it's the anointed version of this in the standard library, used by things like `u64: TryFrom&lt;u32&gt;`.
Generators. I'm building a library implementation of algebraic effects on top of them.
I'm looking forward to &lt;https://doc.rust-lang.org/nightly/unstable-book/language-features/marker-trait-attr.html&gt; -- being able to bypass coherence in certain situations would be really helpful, even with the restriction that one can't override anything from the trait.
Source for it being faster than go?
Specifically, it's a slice; along with a reference to some "array-like" memory, it contains its length.
I don't quite the understand the original question but mine is similar- is there a way to force the type system to never allocate objects of a certain type on the heap? My instant reaction would be no since Box doesn't seem to have any constraints on T but I'd love for a definitive confirmation either way.
No. Imagine a `Vec&lt;T&gt;`. You can often only know at runtime the size of the allocation. There are crates to help manage this, such as [smallvec](https://crates.io/crates/smallvec).
THanks for answering but my bad for not saying it clearly- I wanted to create the type on my own. I believe what you're answering is about forcing for arbitrary types in a program which I can see isn't possible. But what about my own type which implements Sized (by default). Thanks again and I will edit my question
I don't _think_ you can use type size as an impl constraint, but if you could, then it would be a pretty simple wrapper around Box. You could probably do something with a procedural macro and `std::men::size_of`.
I see, so there isn't a name for that kind of guarantee. Good to know that I can at least say that it is "tools to help" in preventing single threaded race conditions. I wish there was a word for having immutable and mutable references to the same data and having an unexpected ordering of reading and reading, so that I could better describe this effect, but it seems that race condition isn't specific to just data while data race implies multithreading or multiprocessing, so there isn't a word for the guarantee.
I ran across another rust gameboy emulator the other day with [corroboy](https://github.com/squidboylan/corroboy). How does your project compare ?
Macros impede IDE functionality, e.g. autocompletion. Because of this, I avoid libraries like nom (&lt;5) that heavily rely on macros. Nom 5 switched to a functional API. IntelliJ recently added some support for macro expansion. Haven't tried it out yet. Error messages from macros are often harder to understand than type system errors.
Out of curiosity, does anyone know why branching in const is tricky?
Can I not implement \`From\` for \`String\` for \`OsStr\*\` types or something similar to a Ruby refinement?
That's probably not a good idea since `Infallible` is not meant to be a generic "never" type. It's meant specifically where a function cannot fail, but returns a `Result`.
This is really cool. Thanks for sharing. How did you go about learning the OpenGL part?
That approach is super cool, but sadly rustc can not just use it as it operates under more constraints. As an example, consider the following case: ``` enum A { Foo, Bar, } struct B(A); enum C { A(A), B(B), } ``` Looking at the types algebraically, they should all fit within 1 byte. However, this can not happen in Rust. If `B` is to be the same size as A, 1 byte, then it must be `#repr(transparent)`, since we need to be able to turn a `&amp;B` into a `&amp;A`. Therefore, `A::Foo` and `B(A::Foo)` must be the same byte. Now, if `C` is to be 1 byte large as well, there is no place for a tag in its layout. Therefore, `C::A(A::Foo)` must be the same byte as `A::Foo` and `C::B(B(A::Foo))` must be the same byte as `B(A::Foo)`. Then, it follows all those would have to be the same byte, so there would be no way to tell `C::A(A::Foo)` and `C::B(B(A::Foo))` apart. This breaks everything, so it turns out the tag is needed here. Another example: Consider three enums `X`, `Y` and `Z`, each with 128 unit variants. Obviously, each of them fits into a byte. Now, consider ``` enum XY { X(X), Y(Y) } ``` Can this also fit into one byte? Yes it can, we just need to make sure to pick the 128 niches for the representation of Y that we didn't use for X. Now imagine somebody comes along and writes ``` enum XZ { X(X), Z(Z), } ``` This can still work, we pick the same niches for Z again. But now, a third person goes and writes ``` enum YZ { Y(Y), Z(Z), } ``` and now we've met a terrible fate. Although this type has exactly 256 inhabitants, we now must use 2 bytes to represent it. Worse, this decision is highly non-local and possibly dependent on which code we see first.
What does it mean to be stored on the stack? What if somebody allocates a bunch of memory using `malloc` and points the stack pointer into it? Is that the stack or the heap?
I thought we have some Rust libs for doing GPU compute? As well as one crate for targeting ptx? If you're ok with a wrapper and abstraction, ArrayFire has a rust crate, it's C++ based and does JIT compilation of kernels with optimizations.
No. You can only implement a trait X for a type Y if; X is located in the same crate as the impl, or Y is located in the same crate as the impl. The exception to this is if X has generic parameters, i.e you can implement X&lt;Y&gt; for Y.
It is but you must like 'gnomish' style widgets ü§∑‚Äç‚ôÇÔ∏è (which is totally fine to me)
Nice work! Did a quick test with [homebrew game](https://gitlab.com/BonsaiDen/vectroid.gb) and except for the sound - under ubuntu all I get is some crackling and a ton of \`ALSA lib pcm.c:7843:(snd\_pcm\_recover) underrun occurred\` logs - it works like a charm :) I may be looking into the sound issue later today and see if I can get it to work.
From a quick observation, RustyBoy is targeting sub-instruction accuracy (needed for 100% CPU accuracy, but much slower) - the original hardware has a number of edge cases where this can have consequences for accuracy. Corroboy is targeting instruction cycles (i.e doing everything involved in a instruction - memory reading, writing, ALU, etc in a single step). Much faster, but less accurate.
Make sure that GitTea offers some feature that you might want that Gogs doesn't offer, otherwise Gogs is still maintained in the sense that it has security patches, and with less code to maintain and audit, its most likely more secure as well.
I'd argue that some limitation off of full yaml compliance can be a good thing, as wandering out of the "safe subset" both limits the "human readable" part of yaml and definitely opens you up to some attacks. At least Rust doesn't have to worry about the "arbitrary class" thing Ruby yaml can do, but at least the reference deserialization bomb (exponential blow-up) works everywhere.
Generally I agree. So far I'm using (or playing with) Rust in little personal projects for exactly those reasons. From my experience at work (JVM + Kotlin) I can say that the margins are not that large. E.g. startup time for a medium Spring app is around 5 - 10 seconds, other Frameworks are generally faster, but still 3 - 5 seconds. Except the GraalVM stuff like Micronaut or Quarkus. Regarding RAM usage I don't have a direct comparison, but I think the 10 times is correct for idle state. Under load it depends what types are created in RAM but it's not 10 times. At most 2 times, which is by the way more than enough for Rust to score there. Peak performance: Depends heavily on the Java framework. Spring is generally slow. With vert.x you should not see significant differences. Having all that said in defense of the JVM: Rust is the supirior technology. Especially compile it and run it as fast and efiicient as it gets. No more to worry about or JVM to manage. The decision becomes hard to make when business comes into play. Saving money in the cloud vs. an old technology stack where you can easily get devs and libs and support ...
I need to get used to thinking about those. A slice can refer to the entire string as well, right?
It's not a slice of a string; that's called `&amp;str`. It's a borrowed slice of [an array of] owned strings.
No idea. Try asking /r/playrust
Pin almost does the opposite, requiring it be stored on the heap or statically.
I've not used Pin in anger before, but I don't think that its usage requires either of those; if you wanted to return the pinned pointer from a function then that would be true, but instead you can pin the pointer at the top of the scope where it's used so that there's no worry about its memory on the stack being invalidated. AFAIK the following code is free of undefined behavior (corrections welcome!): use std::{marker::PhantomPinned, pin::Pin}; // does not implement Copy, so has move semantics // also does not implement Drop, to sidestep a lot of the dark corners of Pin // contains no types that perform any heap allocation, so all on the stack struct Foo { f: i32, _p: PhantomPinned // opt out of the Pin auto trait } impl Foo { fn new(a: i32) -&gt; Foo { Foo {f: a, _p: PhantomPinned} } } fn main() { let a = &amp;Foo::new(0); // The invariant that we must uphold here is that we must take care to ensure // that it is not possible for the data backing the pointer `a` to be moved // before it is destroyed. Since the underlying data was constructed // anonymously through an rvalue and has no alias other than `a`, // I believe that never using `a` again in this program after this block // should suffice to ensure this result. let b = unsafe { Pin::new_unchecked(a) }; dbg!((*b).f); }
Yes! Thank you for finding that for me =)
That arbitrary class thing is a consequence of YAML tags, and is an extremely useful feature to describe dynamically typed data structures. See how this snippet of YAML corresponds closely to Rust syntax: --- - !Ok [ !MyStruct { x: 40, y: 2 } ] - !Err [ "something went wrong" ] .... which might be deserialized into a `Vec&lt;Result&lt;Box&lt;dyn SomeTrait&gt;, String&gt;&gt;` type. Such tags are not necessarily insecure, the issue is more that Ruby's approach is excessively implicit (no explicit registering of constructors that can be used for tags) and that it is enabled by default: instead of `load()` and `load_safe()` it should be called `load_unsafe()` and `load()`. While there is an argument to be made that a YAML writer should not use these features by default, a YAML reader would do well to support them (‚Üí Postel's law).
So... Alex is a 20x developer, but not terrible?
The library implements a conservative garbage collector, it checks if any references still exist to a value by scanning the stack for pointers to it -- this is "by the book" obviously very dodgy, but has worked fine in practice for 40 years so won't get changed.
That's on the heap. On the stack formally means taking the base stack pointer, and the current stack position (which you can get both of without too much trouble), and scanning that piece of memory.
GAP ( [www.gap-system.org](https://www.gap-system.org) ), a discrete mathematics system. It implements a conservative GC which scans the stack.
&gt;corroboy Yup, pretty much exactly. Cycle-level accuracy is probably not needed for 90% of games. But for the ones that do, it's the difference between the game being playable, and completely unplayable (or crashing at startup). The GPU is also equally as hard (if not harder) to get to run accurately. &amp;#x200B; To give you some context, running the same demoscene on corroboy runs for a couple of seconds with missing graphics, reaches [this](https://i.imgur.com/NBwGXDh.png) screen, and hangs. Not that it isn't an impressive project on its own! Just two different goals.
Ah! Cool project! I haven't tested sound on Linux yet. Maybe I should start up a quick Ubuntu VM and try to debug the issue. Let me know if you figure it out!
I was a graphics programmer in video games in my previous lifetime :).
&gt; I meant fixed capacity vector. Maximum N elements, any further insert results in a panic. That‚Äôs https://crates.io/crates/arrayvec
If you want to integrate with QML, you can use [https://github.com/woboq/qmetaobject-rs](https://github.com/woboq/qmetaobject-rs)
The vtable pointer points to a vtable, the first field of this vtable is an `usize` with its length in bytes. This is obviously not guaranteed by anything in the language, and can change the next version, so use at your own risk, and please don't publish any crate that relies on this for anything.
No. Is there a way to do that in C? Or how does C enforce that?
&gt; Also I'd love to have working macro expansion in proc macros. This has been a pain point for me for some time. You mean, you want syn to expand macros ? That has been a pain to me to. What I do, is I let users pass a macro identifier as a proc macro argument, and then just `identifier!` expand it inside the proc macro.
I like it how it works now.
There are some trait impls for "all types that implement some trait T". Often you can do better than those, e.g., for a specific types, or for types that implement some more traits, e.g., T,U,V. With specialization, you can just add those impls. Without it, those impls would conflict with the blanket one, because both would apply.
The only reason I can think of is that making \`Option\` and \`Result\` iterable causes some confusing error messages - e.g if forgetting that little \`?\`.
In the Youtube comments : &gt; Skip to 43:00 if you don't want to listen to a man and his imaginary social justice/diversity issues. Aww, Youtube comments, you always disappoint me
&gt; Just for the exceptions, I'd like to say something. - Exceptions are "go-to" in another form. - You can return at anytime using "return". - Please consider error as a part of successful processing result rather than an abandon of processing. Correct program should not abandon execution at any stage. Good point! I should look at sample applications, especially bigger ones, on how they handle it.
You could make the type private, stack-allocate it in a trusted function and then call the user code in an FnOnce callback which gets a reference to the value. Similar to how scoped thread guards work. Of course this is pretty restrictive and inconvenient.
learnopengl.com
The rules to do specialization could get pretty complicated. A lot of the template wizardry in C++ is based on template specialization. An important rule to select the right implementation is abbreviated as [SFINAE: Substitution Failure Is Not An Error](https://isocpp.org/wiki/faq/templates#templates-vs-overloading). Determining which implementation is used by the compiler can by really hard for someone reading the code.
My take is that in Java we have a very mature ecosystem with tools to catch many of our mistakes. In Rust, those mistakes usually don‚Äôt compile, and for writing correct code, that‚Äôs huge.
Recursion has not a direct equivalence with iterators. This is a simplistic vie on recursion.
Rust improves on the status quo with two features: * If a method in an implementation may be specialized, it's explicitly marked with `default fn`. This is a big clue to somebody reading the code or documentation that a particular blanket implementation isn't necessarily the end of the story. * The rule for which implementation is chosen is very intuitive: the "more specific" implementation wins. A concrete type is more specific than an `impl&lt;T&gt;`. An `impl&lt;T: Trait&gt;` is more specific than an `impl&lt;T&gt;`. When one trait inherits from another, `impl&lt;T: Child&gt;` is more specific than `impl&lt;T: Parent&gt;`. If one implementation *isn't* clearly more specific than another, it's an error.
It's all right, I've removed your post. Have a nice day.
Well, at this point you can hardly call yourself disappointed. What were you expecting?
Cool. Thanks for the clarification. I guess that is why he said "it's for development usage only."
I‚Äôm really looking forward to the try trait: https://doc.rust-lang.org/unstable-book/library-features/try-trait.html Would cut a huge amount of boilerplate from my core that‚Äôs wrapping C libraries and working with C-style error codes.
Need is a strong word. I definitely don't need one. Just looking to get the most value from the server.
&gt; up until now there hasn't been a major disagreement in the community Rember thee async/await syntax? Pepperdrige farms remebers.
Panics are altogether unsuitable for such a case: - The expectation for panics is unexpected failures. Every panic should be a programming error. - Panics are designed to only be caught at the thread boundary, when the thread that they come from exits. (This would render them a dubious choice because of the effect they‚Äôd have on any thread pool.) - Mutexes are designed to be poisoned if the thread currently borrowing them panics. (This makes things even worse.) - If a destructor panics while unwinding due to a panic (sometimes called panic-while-panic), the process will abort. While this shouldn‚Äôt be catastrophic (you should code in expectation of such failures, as they can always happen in any environment‚Äîpower failure, for example), it‚Äôs again troublesome. I would give a hard negative review on any code doing that.
The code unwinds and drops objects in an orderly fashion while panicking; you shouldn‚Äôt get any leakage.
I thought there was a big disagreement about unique/mut
I didn't join the rust ecosystem by that time, so I don't really know the scale!
Even without the \*empty\* interface pattern, Go interfaces are used everywhere and aren't zero-cost abstractions. Interface calls are indirect calls in a somewhat similar way as C++ vtables. This is expensive not just because of the indirection itself but because it prohibits inlining and associated optimizations. This makes Go inherently slower than Rust.
no, that is the problem with Rust. Someone will come tell you about the options but unfortunately Python is dominating scientific libraries and now the web backend sphere. There will something some day but it is way too young to have anything production level.
Yeah I love the rust section :/
Can you elaborate just a bit about your caching? There is no caching solution in actix-web.
Is this the equivalent of type\_traits in c++, where one can do things like is\_literal&lt;T&gt;, is\_same&lt;T,U&gt; etc? Because that is really useful for creating ergonomic APIs in C++ (like for serialisation) and I imagine would be similar in Rust
I found a problematic URL I'm trying to visit with the rust headless_chrome crate. Now, I'm trying to handle (try catch) the error so I can let the application continue without going to a panic and stop running. The error message: ```thread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: Error("unknown variant `closed`, expected one of `user-agent`, `open`, `close`", line: 0, column: 0)', src/libcore/result.rs:997:5 note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace``` Code: ```pub fn browse_link(url: &amp;str) -&gt; Result&lt;(), failure::Error&gt; { let mut path = PathBuf::new(); path.push(r"/bin/chromium-browser"); let options = LaunchOptionsBuilder::default() .path(Some(path)) .build().unwrap(); let browser = Browser::new(options)?; let tab = browser.wait_for_initial_tab()?; /// Navigate to `url` tab.navigate_to(&amp;url)?; tab.wait_for_element("a")?; //drop(&amp;tab); Ok(()) }``` I'm not sure what I can do to add more error handling to the code to prevent this from happening. Thanks for any input and help in this matter. :smiley:
The worst part of writing an web application is related to asyncio. I've worked with most of the parts of futures 0.1. I have achieved inception-level complexity using chained combinators within combinators within combinators, cloning Rc or Arc wherever applicable. Asyncio hasn't been well-documented. That may change but documentation is a hot potato that changes hands so often that I am skeptical whether anyone will pull it off. All of this changes in the near future but I chose not to defer my work to an indefinite time in the future based on open source collaboration and so made do with what has been available.
I mean, maybe it doesn't come up unless you want to use an object pool, or a concurrent hashmap, or a priority queue, or any other not-built-in data structure, or you want to serialize or parse JSON or XML. But all of these things are pretty common and I find &gt; I've used go at my last three jobs and I never saw this pattern. a bit hard to believe. And this: &gt; Sounds like poorly written go to me. is just wrong, unless you're saying that large parts of Go's standard library are poorly written.
Well, the ecosystem is not as well populated, but there are some components that may be useful. ndarray is useful in that space.
C doesn't enforce it at all, C programmers are usually more used to "don't do any of this long list of things, or horrible undefined behaviour will occur" :)
That's an interesting idea, I'll explore it. As you say, I might just have to say (like the original C API) "don't do that".
I don't think it's tricky, i think they're just being conservative about the way they're adding new cont fn features.
The `browse_link` function looks fine, you are propagating all errors apart from unwrapping the build result (which should never error). Are you unwrapping the return value of this function? If that's the case, you may want to consider matching on the result: if let Err(error) = browse_link(my_url) { // do what you want with the error }
Ah yeah so what I mean by caching is that I wanted to edit the cache control headers so the browser would not cache certain routes. I was looking for a way to just set a single header on the response, but since I was using a special response impl for serving files from the filesystem, it seemed like the only options were to go through actix-cors, or implement custom middleware to modify the headers outside of that request handler. It's possible there is a way to do what I was trying to do, but it wasn't obvious from the documentation. So that's an example of what I mean by opinionated. I guess in general what I like from a web framework *just* routing support, and a raw interface for handling requests and building responses. I don't want it to do anything that I don't explicitly tell it to. The impression I got from actix is that it's a more kitchen-sink approach, which is designed to make your code very terse and minimal, and to abstract away all the details. So like the canonical actix handler function would just return a string, or a serializable object, and the framework figures out all the headers etc. for you. It's just a different approach to composition than I personally prefer: it seemed like the problem solving process in Actix was combing thorough the library searching for the one purpose-built function or component designed for exactly that scenario, rather than having a small set of more general components which could be layered together to reach a result. Again, maybe that type of interface is also available within actix web, but it wasn't obvious to me from the documentation and from the trial and error on this one project.
Yes you can, sort of. To do this, you need a combination of \`unsafe\`, macros and \`std::pin\`. &amp;#x200B; Let's say your stack-only type is \`StackValue\`. Define a struct called \`Unmovable\` containing the following: &amp;#x200B; \`\`\`rust pub struct Unmovable { val: StackValue, \_phantom: ::std::marker::PhantomPin } \`\`\` &amp;#x200B; The struct's fields must be private so that client code cannot build an \`Unmovable\` directly. The \`PhantomPin\` marker is needed because we don't want users to move your stackvalue around from the pinned instance we'll be creating shortly. &amp;#x200B; As the sole constructor of your value, use an unsafe function. For simplification, let's say your \`StackValue\` can be constructed from an \`u64\`: &amp;#x200B; \`\`\`rust impl Unmovable { /// # Safety /// This function is safe to call only if the returned value /// is always kept on the stack. unsafe fn new(val: u64) -&gt; Self { Self { val, \_phantom : std::marker::PhantomPin } } } \`\`\` &amp;#x200B; This is enough, really, since you stated the requirement as the safety condition of the unsafe function. &amp;#x200B; But if you want this to be a little more ergonomic, you can also throw a macro that will: 1. hide the unsafe by creating the required conditions to ensure we are on the stack 2. return an pinned instance instead of directly the instance &amp;#x200B; \`\`\`rust macro\_rules! unmovable\_let { ($id: ident = $expr: expr) =&gt; { let mut $id = unsafe { $crate::Unmovable::new($expr) }; let $id: $crate::Pin&lt;&amp;mut Unmovable&gt; = unsafe { std::pin::Pin::new\_unchecked(&amp;mut $id) }; }; \`\`\` &amp;#x200B; This allows you to do the following: \`\`\`rust unmovable\_let!(my\_stack\_value = 42); // my\_stack\_value now contains a Pin&lt;Unmovable&gt; with a \`StackValue\` initialized from 42 // use \`my\_stack\_value\` from here... \`\`\` &amp;#x200B; I actually have an \[experimental library\]([https://github.com/dureuill/stackpin/tree/keep\_only\_stacklet](https://github.com/dureuill/stackpin/tree/keep_only_stacklet)) that tries to explore stack pinned values. Check it out if you like (keeping in mind it isn't released yet)!
&gt; Rust lacks hundreds of specialized libraries &lt;...&gt; file format libs for HDF5 https://github.com/aldanor/hdf5-rust (disclaimer, I'm the author) It's moving somewhat slowly, mainly due to the lack of contributors, but it's in a quite usable state now already (depending on what you're doing with HDF5). I would dare to say some parts of it like working with structured types are already more ergonomic than in the official C/C++ HDF5 API (e.g. due to Rust having #[derive] macros).
ndarray is practically rust's numpy, if u have experience with numpy, i reccommend u read the [ndarray for numpy](https://docs.rs/ndarray/0.12.1/ndarray/doc/ndarray_for_numpy_users/index.html) book. It will be pretty awesome when const generics land. Though it ain't at production level yet?
The reason string slices are qualitatively different from array slices is that strings contain UTF-8 which is a variable-length encoding. `&amp;str` is neither `&amp;[u8]` nor `&amp;[char]`.
Thanks for the help. Unfortunately doing this didn't change the outcome. There still was a panic. But I wasn't unwrapping the return value or anything though. I'll keep fiddling around though.
Interesting, I don't see any documented panics on the methods you've used in the `browse_link` function. Could you double check that the panic isn't coming from somewhere else in your code?
This is a really good governance talk. The speaker lightly slags "lazy consensus" and in the Rust context it's definitely fair, but I'd argue context matters. For example if you have a smaller community or a product where users aren't fundamentally impacted by changes I think just landing code in master and seeing if it sticks is an okay strategy. That being said it's a minor nit out of a talk that is just really astounding. This is the kind of overview that can provide insights into good governance far outside of just Rust or even technical domains in general.
Also, `int` vs. `usize` was a thing.
At some point people need to realize that there is an appropriate time and place for certain topics.
Out of curiousity I was waiting for this transition while I watched, checked the clock and I was at 60 minutes and didn't think there was anything worth skipping anywhere in the talk... It's like.. what is the commenter even whining about?
You can have a look at https://github.com/brendanzab/gl-rs and https://github.com/rust-windowing/glutin I'm currently trying to set up a small renderer with winit and gl-rs. It's not hard, mostly it's figuring out how interfacing with a C API works in Rust
Not that this is specific to Rust, but why do I see so many talks about language X at conferences for language Y?
If putting it on the heap causes it to be freed but still usable, you'd have to mark each function as unsafe. Since I think you're writing a library, doing both would be a good.
All the listed major disagreements here I would consider in the first place as bike-shedding discussions, that don‚Äôt at the end have a big impact on the usability of the language. I‚Äôm not a big fan of the postfix await syntax, but I can understand the pragmatic reasons for it, and with syntax highlighting it shouldn‚Äôt be that big of an issue for me, and after using it for some time it most likely will become a non issue. Just because something is a very vocal discussion at the end doesn‚Äôt mean it‚Äôs also a very important discussion. It‚Äôs a lot easier to have an opinion about the syntax and articulate it than about an advanced type level feature.
If you stabilize branching without looping, then people will write recursive const fns to loop. This is seen as un-ideal (and easy to blow the stack with), so we're waiting for stable const looping before stable const branching.
All those weird impls actually got replaced by const generics usage in the standard lib. They did add a hack to only allow values up to 32 as per the old impls so that they don't accidentally stabilize the actual feature. It helps tremendously with the size of the array page in the docs though.
Are you looking for something low level? If so, gfx-rs could be a good fit, otherwise rendy is a higher level library based on it
Thanks for the clarity of letting me double check where it came from. It seems to be coming from: tab.wait_for_element("a")?; I thought, ah, maybe the tab closes after it navigated to it, so I moved it above: tab.navigate_to(&amp;url)?; Seems to work wonders, thank you again!
It kinda looks like you're not reading past the name of the crates. What do you actually need? `glutin` is a windowing abstraction - it'll give you a window and an OpenGL context, but you have to supply your own OpenGL calls. `glfw` is the same thing, but wraps a C library rather than being pure Rust. `glium` is a safe OpenGL abstraction written by tomaka. `gl` is a set of raw, unsafe OpenGL Rust bindings. `gfx` is an attempt to provide a crossplatform rendering abstraction on top of Vulkan on supporting platforms, Metal on Apple devices and (presumably) OpenGL ES on mobile devices.
I am trying out the new `contains` method on `Option`. #![feature(option_result_contains)] fn main() { let opt = Some("a"); let _ = opt.contains("b"); // does not work let _ = match opt { Some(e) =&gt; e == "b", None =&gt; false, }; let _ = opt == Some("b"); } I understand the compiler errors, but it is really intended that this extremely simple usecase is not supported?
The dot operator only works with methods. Since it is so powerful, my guess is it only works on methods to limit the search when the compiler (or a programmer) has to find the right one. The documentation even help you by listing methods available from `Deref`. You can learn more about it in this [stackoverflow answer](https://stackoverflow.com/questions/28519997/what-are-rusts-exact-auto-dereferencing-rules/28552082#28552082).
Yeah there was zero content to do with social justice. Diversity was mentioned but it wasn't in the context of social diversity but rather diversity of use cases and individual hacker/enthusiast vs larger scale production users. I suspect the commenter just had a general bee in their bonnet regarding Rust's CoC (which want mentioned in the video).
It works if you write let _ = opt.contains(&amp;"b");
See Simon Sapin's [example code linked from this reply.](https://www.reddit.com/r/rust/comments/bea97e/flat_appendonly_buffer_of_dsts_for_a_deferredcall/el5zgzb/) If I understand his code correctly (and your requirements), he does it there by creating a fake reference with the correct vtable. Search for `fake_repr` in that source.
https://github.com/rust-lang/rfcs/pull/2580 proposes an official way of exposing this information. In the meantime, the closest thing available is `std::mem::size_of_val` and `std::mem::align_of_val`, but they take a `&amp;T` parameter that is expected to be valid, not just a vtable pointer. That said, what are you transmuting `std::raw::TraitObject` from? Is that a valid `&amp;dyn X`?
Wow, how did I not think of this myself? Thanks!
Yes, policing may you not be easy. I still think it beats the alternative of "no policing". I'm not sure about "lots of arbitrary decisions". Is it an empty crate or not? Does it's name match what it appears to do? Etc. Is it worth it? I think it is - because this problem grows, and at some point in the future is likely to become intolerable. Might as well start addressing it now. Yes, policing would annoy some people. I think the benefit is worth the cost here. Also, perhaps a boot-camp analog for "new publishers" isn't such a bag thing. If nothing else, I'd expect it to improve the quality off published crates. If it's at the cost of quantity - I can live with it.
A similar pattern is possible. The `?` operator will call \`Into::into(err)\` and then return early. A common pattern is to build an application wide error enum and include most errors that will bubble up rather than be handled at the call site. Libraries like [snafu](https://crates.io/crates/snafu) help you define the enum as well as the `Into` implementation with minimal boilerplate. Then you can have a global handler for these error types to convert to a error responses. You can even have your own custom types to define what the response code should be.
Not for social justice worriers. Only appropriate place is in a cave under the ocean far away from everyone else.
I really appreciate whenever this happens. There's so much programming sub-communities can learn from each other, and a keynote at a conference is a great way to inspire people to look over the rim of a programming language's tea cup :)
People complaining about "SJWs" are the most sensitive people on the planet üôÑ
My theory is that since multi-paradigm languages became really popular, everyone's kind of gotten used to the idea that cross-pollination of ideas is a good thing for language design and language communities.
The API might be a little rough in places (hence the need for const generics), but I would have no significant concern using it for production workloads.
First of all - thank you for reminding me these videos exist. When I wrote the OP I was trying to remember where I'd seen a video re: emulator structure and Brian's was the one I was thinking of. Whilst I appreciate a lot of what Brian says in these videos I think some of what he does in the emulator example in particular actually hinders readability. I appreciate some of this is likely so that he can take his approach to the extreme, but it would be nice to have a more balanced end product. That said, over the week I've been dialling down the OOP-ness of my design and things are moving along a lot smoother now.
&gt;What do you actually need? This part, if you are learning, go with the crate which has learning resources using it. Some of that may be within the crate itself, external resource or in the case of SDL/gl you can typically follow on with some C SDL tutorials, but you are making work by cross-referencing the C library to the Rust library.
You are looking for r/playrust
Oh ok
I'm surprised that nobody has mentioned generic associated types (GATs) yet. It is *by far* the limitation of current Rust that I bump up against most often. I'm surprised that others don't feel the same way, because you really don't need to do crazy advanced things to have it happen to you. For something as simple as a trait with a method that borrows `self` (like streaming iterators), you need GATs. I think GATs seem less important than they really are. When I was a C++ developer, I didn't think algebraic data types (Rust enums) were very important. Only after learning Rust and its enums, I really came to appreciate how important these ADTs are. I think it might be similar with GATs. If you're not used to them, they don't seem super important, but I think that that's mostly because we've gotten used to not having them, not because of an actual lack of usefulness. Here's another reason why I think GATs are more important than they seem. Think about how ubiquitous iterators are in Rust. They are everywhere, because they are a very useful, elegant and powerful abstraction. I think it's not too unreasonable to assume that if Rust had GATs from the start, we'd also be using a fair amount of *streaming* iterators. But we're currently using practically none. All of those places where we would've had a streaming iterator, instead there now has to be some workaround, be it interior mutability or hard-coding types that could be generic. Or the abstraction, library, or project might simply not exist altogether. And that's *just* for iterators. I really think that if we'd had GATs from the start, they'd be *everywhere*. The traits `Deref` and `DerefMut` for example, I think their associated type `Target` would've been generic over `'a`, rather than having references hardcoded (i.e. `Deref` would return `Self::Target&lt;'a&gt;` rather than `&amp;'a Self::Target`). You could say that most languages don't have GATs or any other form of higher-kinded types, and they seem to do just fine. But I think there's a crucial difference here between Rust and these other languages. In Rust, there's a very large emphasis on writing *safe* abstractions and *safe* libraries. The requirements that a function imposes on the caller are written in a form that can be checked by the compiler, whereas in other systems-level languages they're written merely as doc comments. That's what makes a strong type system more important in Rust than in these other languages. A pointer is a pointer, regardless of how many requirements you impose on it in the doc comments, but if you want to make the compiler able to check these requirements, you need to have a strong enough type system that is able to express the requirements. The implementation can use unsafe, the function/trait definition cannot. I think the longer Rust continues to have no GATs, the more we'll have libraries hardcoding workarounds for not having them into their abstractions (using interior mutabiliy, hardcoding references instead of generic types, etc).
What happens when the code panics while unwinding the stack due to a panic?
So much is configurable in actix-web, but that's not obvious at first glance. People need to continue contributing to the docs/tutorials/examples to help address that. The thing that modifies response headers *is* middleware. You might have used other frameworks that didn't make this distinction, but that doesn't change what technically ought to be responsible for header updates. The ``wrap(...)`` method is available at multiple levels within a web app. The one you're looking for is probably [resource-level](https://docs.rs/actix-web/1.0.3/actix_web/struct.Resource.html#method.wrap). I haven't personally worked on resource-level header middleware but when the need arises I will look at the [default headers middleware](https://github.com/actix/actix-web/blob/master/src/middleware/defaultheaders.rs) and modify it accordingly. Did you get this far?
Yes! Run `cargo build -vv`.
Weird question. I think it's pretty clear that the stack means the default stack. Is there any language other than assembly where you can arbitrarily manipulate the stack pointer?
No... I want it before the build inside the program itself.. what invocations it is gonna make..
We're just debating the term "dynamically typed" associated with go in the earlier comment
&gt; As mentioned previously that aho-corasick performs quite slow from my brief testing, I didn‚Äôt spend extra time to dig out why and I just accepted it. Do you have a reproducible benchmark that I could look at?
So you want to find out which commands Cargo *would* run, without having it run them? This sounds like https://github.com/rust-lang/cargo/issues/5579
probably referring to a major disagreement among stakeholders who actually decide
Oh, I see. If you are using (or willing to try) nightly Rust, you can use `cargo +nightly build -Z unstable-options --build-plan` which will give you a JSON containing (among other stuff) the rustc invocation. Does that help?
Thank you everyone organizing this. It‚Äôs nice to have a conference closer to Nebraska
I'll try and see... Thanks a lot
Excited to see you there!
You're welcome!
The process aborts, which will release all memory owned by the process. (See my other comment near here for more related remarks.)
I want to learn OpenGL but the tutorials are all in c++. Same with vulkan. It‚Äôs been a long time since I touched c++
[removed]
I swear I looked for something like this and did not find the `delegate` crate! My Google-fu has seriously failed me. Thanks for the excellent links.
Awesome project! Seems like the example image in the Readme isn't loading though?
Yikes. That's... *very* dodgy.
Yeah, inserted the question mark for that. Well, anyways according to me it is to rust's ML story as much as tokio is to rust's web. So good work there!!
You may not do this stuff yourself, but you surely use it indirectly. How do you think \`pthread\_create\` works? Or user-level threads/stackful coroutines? They allocate memory and then just use it as a stack. There is nothing fundamentally special about the "default" stack.
While not exactly the same, there is an OpenCV wrapper. Also ndarray like others mentioned. Also nalgebra which is mainly linear algebra focused but has some decomposition functions etc.
Thank you. I have fixed it now.
Fair
This looks great! Would you consider breaking it out into a lib? I‚Äôm using carbon for my project GistCard (https://github.com/mike-engel/gistcard) and this would be so much better/faster!
Yay I always like seeing cool new uses of syntect :)
Loads for me without issue. Nice work, op!
Reminds me of a colleague of mine, who used to explain all of his design decisions as "f***ing developers" :)
Cool project, I might use it for a few images. Is there any way to change the window shadow? In the demo image the shadow is a bit large, and it's not black.
You are looking for s/playrust
Thank you
Yes, you can use \`--shadow-blur-radius\` to change the size of shadow
I wish this trend of using images of code would stop. I've seen it appearing on twitter a lot and it's frustrating because I can't show it to my blind colleague. But well if it's pretty... Nice project anyway.
Awesome! Starred :)
That's a cool project. I will give a try( This should not be difficult, I think...
Thank you very much for your pioneering work. :D
You may replace rustc with your command, and invoke real rustc if you'd like, or refuse that invocation.
The borrow checker prevents data races and therefore solves the hurdle of multithreading.
This is because it depends on `cargo` itself. And it uses cargo just for [git routines](https://github.com/ashleygwilliams/cargo-generate/blob/2bc326ee4b37698fb2ec5627315f9929bbd83439/src/git.rs). In theory, you can make a pull request that replaces `cargo` with a bare `git`.
tbh this is a hard problem to solve at a systemic level and there aren't any good solutions(well not that I can think of though I don't have much knowledge on this) and I do wonder if solving this problem is worth the potential cost
Taking an image of code is stupid even for the sighted because it makes it impossible to copy and paste.
I don't get why you don't do the same with unimplemented
That would be neat, because it is the exact problem i tried to point out, it could be done via git, but it was easier to include cargo itself in the project. But don't get me wrong. I see why cargo-generate is useful. I have been using generators up and down, because they save a ton of time. I just don't know if people new to all of this should be able to skip that chapter and just take it for granted like people do with npm modules.
Where Y=Rust
This is the poor man's DRM.
Maybe try a solution written in C/C++ ?
Unfortunately, many places where people share code socially don't have syntax highlighting, so if you share it as text, you lose the highlighting. Do you have any particular suggestion?
How else would you show code on twitter though? It doesn't have syntax highlighting and it wrecks formatting. I agree it's not accessible, but usually it's accompanied by a link to a repo of some sort, which is a lot more than most Twitter content.
I'm the dev BTW, here's the repo:https://github.com/dessalines/lemmy And the test instance: https://dev.lemmy.ml/ Back end in rust, front end in inferno, comms in websockets so everything is live-updating. Federation isn't working yet, but is in development.
[nalgebra](https://crates.io/crates/nalgebra) provides matrix decompositions and other operations.
Use a site like jsfiddle or similar.
Thank god. I don't even know if this is it, but we absolutely need open source platforms for social media. Don't get me wrong, the numerous privacy violations and what not are awful. But like, YouTube is profit-driven, so obviously they're get to control the conversation to their benefit (like ban anti-capitalists). They're going to accept money for promotions They're going to sell data. They're going to slap ads on everything. They're going to take a significant cut of all contributions, because they have to make a maximal profit on everything, no matter what. I don't even need want libertarian, everything distributed, pipe dream. There can be an organization or hierarchy to make decisions (I'd prefer it). Just like, no ding dongs at the top making decisions for profit, since that's so clearly failing, especially when they can literally control conversations that affect their profit. The fact that everyone's just cool with the systems we have is absolutely buck wild to me.
&gt; twitter Never heard of.
There's like a bajillion of code sites that allow you to share code with colors, and even run snippets. For rust, repl.it would be an example.
Really awesome project! I'll play around with it tomorrow :)
This reminded me of my horrible experience with react-app-create, where the build commands reads the value of the homepage property in package.json to know where source files are placed. IMO if a system needs a project generator then it might be a good sign that things can be improved.
Really, one of the major strengths of Rust is that you *can* write unsafe code, as long as you mark it as such (and hopefully, are careful to use it correctly). Some people see "unsafe" as a bit of a misnomer, and have even suggested changing the keyword to something else. Unsafe code can be safe, but the compiler can't verify that, and the programmer thus must be careful.
I‚Äôd be happy if the image were svg :)
Right, sure. The problem is that such sites aren't *native* on a platform like Twitter. The investment in opening such a link isn't that large, but it's significantly more than just looking at the image provided. If such sites consistently had embeds which were comparable to an image, maybe it could work.
I understand and agree that Go isn't dynamically typed. I just thought this note about Go's dynamic dispatch is relevant to the discussion.
I responded to a [later comment](https://www.reddit.com/r/rust/comments/cd31vp/_/etrjuz6), but the gist is that links to such sites don't allow the viewer to read the code without performing an action (opening a link and loading who knows how much junk), which may take time, resulting in lower engagement. Embeds may help if implemented right.
This is a great post, thanks a lot for writing it! :) I still sometimes hear people say "but Rust has `unsafe` so all these safety claims are useless", next time I will point them to this post. :D
Very nice! I‚Äôll still get around to writing that Rust Game Boy emulator... Some day... üôÅ
Notice that cargo features are additive. You \*cannot\* use features to "take away" something, because if \*any\* crate in the dependency tree enables a feature, it will be enabled for \*all\* users of that crate.
You're totally right about that. The lack of generics drives me crazy. Some excellent points were presented. The part that rubbed me the wrong way was the dynamic comment. Rust is definitely the best choice when performance is critical, there is no doubt about that. Go is incredible when writing web apps which require high coding velocity (and maximum performance isn't necessary)
Gogs still has a very severe remote code execution bug: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-11229 Unkwon was informed at its discovery, was made aware of just how serious it was, shown the pr that solved it and even now months later it's still there.
How do you guys feel about prismo?
I‚Äôm more than happy to help as well!
Seems cool too, but it's written in ruby and also the main dev has had to step away for a while.
Check your math ;)
The code is neat. This looks like it was a lot of work to create. Thanks for sharing it! "Hack", the default font, is ‚Ä¶ not good. The left-justified closing parenthesis is painfully asymmetric, the shape of the equal sign and greater sign are quite strange, the x-height is weirdly large in general. The default settings should be no window controls and much smaller pad. The source code is the star here. Writing a PNG to stdout is a pretty harsh default: should probably have to ask for that. As another poster mentioned, SVG support would be cool.
Yeah I definitely wouldn't agree with strict adherence to anything in particular. After all, that's what got us into this mess in the first place. The NES emulator example does kinda go out of its way to avoid creating anything resembling classes. I would do something somewhat similar to his version, but tie some of the more obviously domain specific functions onto the types, and call some of this outward-facing API from the procedural-oriented bits of code that deal with the overall logic. Just to bring *some* of the logic that will never be seen outside of a component closer to the type that represents that component, because it isn't particularly harmful until encapsulation is abused.
Ooh. What kind of platform are you using?
I second that advice! Gitea is great!
Ah good point about threads.
Provide a link to your code.
That's mentioned in my comment. it's usually (not always) the case that pictures of code are accompanied by links to the code as well.
You just described why I never created a Twitter account.
[plotters](https://crates.io/crates/plotters) was posted here recently and should be a good alternative to matplotlib
Wasn't expecting to agree when I first loaded, but yes, 100%. I tried to use some wasm tooling about 6 months ago and found the rust specific tooling to be exactly as described.... I immediately had flashbacks to npm. At the time one of the tools was basically just a wrapper around a bunch of exec calls. The code was really poor, made tons of assumptions about dir structure and ultimately was more easily replaced with two fucking lines of bash. I hate that I'm writing this comment and hate that I feel this way but the rust wasm stuff definitely has a certain scent of mom that really annoys me.
Nice blog post! Learnt some of the stuff while trying to write an allocater, an exercise i recommend for all those who want to understand the nitty-gritty of memory management.
Fidonet is back!
The illustrative example is very informative! For a type where any bit pattern _is_ valid, is there _any_ reasonable way to tell the compiler: _"give me a value of this type, and don't waste time setting it to a value"_?
While it does sound dodgy, it's basically how almost every GC in a system based on C works -- while it feels unsafe, it's actually very hard to break as long as you follow a couple of simple rules. &amp;#x200B; As long as you make sure the compiler doesn't compile the GC with full program optimisation, when the GC function is called every pointer used by every function higher up the stack must have been put on the stack (where else would they get put), so just scanning up the stack finds all references to all objects. The GC also knows about objects it creates, it just doesn't know about objects which Rust creates.
Fantastic piece! Thanks much for sharing it. I don't understand why language specifications have gone down this road? It seems like requiring that a memory location be treated as having some specific unknown value even though you've never observed it is the "obvious" thing to do *(‚àÄa.‚àÉ!v.mem[a]=v)*. What optimizations or compiler simplifications does this "NaN" model of memory enable that are worth the cost of the kind of behavior you describe in your post?
Love talks of undefined behavior. There are plenty of cool examples, but here's one of my favorites: https://kristerw.blogspot.com/2017/09/why-undefined-behavior-may-call-never.html
I like the default dark theme. This is the first project I've seen to use the GNU Affero General Public License. Makes sense since it is mainly intended to run on a sever. Why federated over distributed? What advantage would running one's own server provide once federation is supported? How do you prevent it from becoming voat? Dose the fact that each lemmy server can set its own moderation policy mean that you'll see different content depending on which server you are connected to? What do you like/dislike about websockets? Actix? Does/will this use/take advantage of webassmbly?
Isn't federated a subset of distributed?
Nit pick. No byte can be 256. The range 0...256 shows off how easy we fall into the off by 1 trap.
Really cool project! I cloned it to play around with it and ran into some issues. You have test\_roms in the gitignore which is causing lots of tests to fail from missing roms. I see that they're available elsewhere online, but I'd personally just include them in the repo. Also, I'd consider those tests to be [integration tests, so they should be in the tests folder](https://doc.rust-lang.org/book/ch11-03-test-organization.html#integration-tests), not src/test. I also noticed that you're already using rustfmt (since I saw the rustfmt.toml), but when I run cargo fmt on the root of the project I get about 6 changed files and a couple errors noting whitespace in macros. I'd recommend also setting up [clippy](https://github.com/rust-lang/rust-clippy) (a code linter) which warns about a lot of common mistakes. (You can set it up fairly easily for VSCode to underline things that clippy points out). On a side note, IMO the gitignore in a repo should only be for things specific to that project, e.g. build files, logs, output, etc. For ignoring anything specific to your OS, IDE, shell, or anything else personal, you should set up your global gitignore. Not everyone agrees with this, but I think its the cleanest way to do it.
Which JVM was used in the Scala benchmark? GraalVM can sometimes be faster and use less memory than Hotspot for Scala applications, partly due to better escape analysis which reduces the number of heap allocated objects.
That's why I wrote `0..256`, which is a right-*exclusive* range. :)
Aaron's talk starts at 13:27
I use it mainly for the much nicer state management. Though, r/Lord_Zane's comments are valid.
Thanks, ya we'll have to add issues later for more theming, and possibly even community themes / styles. &gt; Why federated over distributed? The intention is to work with the activitypub spec. This is slightly different tho, because unlike mastodon / friendica, whose main federated activity is following people, this is about following communities across different servers. &gt; What advantage would running one's own server provide once federation is supported? The server mods (or admins) have full control over everything (removing communities, banning users, appointing other server mods), even over communities (even tho community mods can control everything within their communities.) There is a public modlog of course. But as a server mod, you have full control over all the content in your instance, which is nice. &gt; How do you prevent it from becoming voat? I built in a slur filter, but also IMO the best way to keep out the racist fucks is to have strong moderation abilities, which are there, but will probably need to add more as this develops. On the main instance, and this dev one, obvi I'm not going to racism, sexism, transphobia, etc. &gt; Dose the fact that each lemmy server can set its own moderation policy mean that you'll see different content depending on which server you are connected to? Basically your front page will be all the threads from you subbed communities, and federated communities. So the threads that are on external communities, you'll be able to see and interact with from your instance, but it'll actually be hosted and fully controlled elsewhere. &gt; What do you like/dislike about websockets? Actix? Websockets weren't that hard to learn, I'd used them in java a lot before and really liked them. IMO all chat-type apps should be using them. The fact that I have to constantly refresh this page to see new comments is one of the things I wanted to fix with a reddit alternative. Actix is great to work with, very performant, but does take some getting used to. &gt; Does/will this use/take advantage of webassmbly? The API is an open spec, [here](https://github.com/dessalines/lemmy/blob/master/docs/api.md), so anyone can build any client they'd like. I'm most familiar with react / inferno so that's what the front end is in. The next priorities for me would be a command line client like RTV, and an android client, before diving into webassembly, but anyone else is free to do so.
https://doc.rust-lang.org/std/mem/union.MaybeUninit.html
The same goes for wasm-pack btw. which is like web-pack for WA and is pushed by rustwasm.
Depends on who you ask, I guess. I'm using ‚Äúdistributed‚Äù to refer to "peer-to-peer" networks and ‚Äúfederated‚Äù for ‚Äúclient-to-server-to-server-to-client architectures‚Äù Both may be considered "decentralized".
What is this color theme? Is it available for VS Code?
Cross pollination of ideas is a very worthy idea to present.
That's a nice one indeed. :)
Not so. Modifying /u/ralfj's example to use `MaybeUninit` in the obvious way still causes the assertion to fail: https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=aad3bc2295100d0d3ab90043369f233e
You can disable type hints and use `ctrl+shift+p` to see the type of the selected expression. It is very convenient to use with `Extend selection` (`ctrl+w` on Linux) when you want to find out the type of the specific expression.
Nice i look forward to it. Yes valid trait objects. For now I've opted for adding a function to the trait itself. A bit redundant but hey.
It's just a C API, it works the same in C, C++, Java, Rust... What I'm doing is following learnopengl and building my renderer in Rust. I have no background in Rust but it's not as daunting as it seems. Follow glutin or winit tutorial to get a working OpenGL context, then you can grab the function calls from learnopengl and do the same. If you need a linear algebra library check out nalgebra and nalgebra-glm. There are other options out there.
&gt;Basically your front page will be all the threads from you subbed communities, and federated communities. So the threads that are on external communities, you'll be able to see and interact with from your instance, but it'll actually be hosted and fully controlled elsewhere. So if I don't want to run a server, but my buddy does. I have limited control to post, comment, and vote (unless I'm a community mod)? I can also create a new community on my buddy's server? My buddy essentially has admin authority over communities created on his server, but is limited to my level for communities hosted elsewhere? Am I getting this right?
The response to "carefully written C++ is just as safe as Rust" is that it is impossible to write C++ *that* carefully. See https://www.vice.com/en_us/article/a3mgxb/the-internet-has-a-huge-cc-problem-and-developers-dont-want-to-deal-with-it for the long version and https://twitter.com/LazyFishBarrel for a continuous stream of examples.
&gt;The fact that I have to constantly refresh this page to see new comments is one of the things I wanted to fix with a reddit alternative. Huge usability improvement. I agree.
&gt;The next priorities for me would be a command line client like RTV, and an android client Cool! Does the AGPL vs Apple policy limit the possibility of an iOS client?
Sorry that was not obvious to me. Only coding Rust for a few weeks now. Those ranges still confuse me even when writing Rust code. At university we learnt to use [0,256) for being explicit. But only people that learnt this mathematical notation would understand that.
I wasn‚Äôt aware of this, the devs of git tea made it relatively clear that the separation was because there was a lot of features they wanted but the gogs dev valued it as his project and Eva was the season of inactivity. I guess things has changed over time.
&gt;The trigger for this post is the [deprecation of mem::uninitialized()](https://blog.rust-lang.org/2019/07/04/Rust-1.36.0.html#maybeuninitt%3E-instead-of-mem::uninitialized) with Rust 1.36, but the post is just as relevant for C/C++ as it is for Rust.[1](https://www.ralfj.de/blog/2019/07/14/uninit.html#fn:deprecate) \`std::mem::uninitialized\` is being marked as deprecated in "1.38" according to the blog post: &gt;As [MaybeUninit&lt;T&gt;](https://doc.rust-lang.org/std/mem/union.MaybeUninit.html) is the safer alternative, starting with Rust 1.38, the function [mem::uninitialized](https://doc.rust-lang.org/std/mem/fn.uninitialized.html) will be deprecated.
FYI the right-inclusive range is written `0..=256`
This is awesome work by the way. I should have lead with that, but got excited and jumped right into details.
I agree but isn‚Äôt no downvote button good?
Reasoning around code is hard. Especially when doing so machinally. An important task in optimizing code is eliminating code that can statically be proven to never actually run. However the compiler has very little information for this. Therefore, they are forced to reason based on clues provided by the users code. These clues usually come in the form of undefined behaviour. User dereferences a pointer? This proves that this pointer was not NULL and had valid alignment. User dereferences memory? This memory must have been initialised previously. Memory location contains a bool? Said memory location can only contain a 0 or a 1. More importantly, the inverse of these proofs also holds! The compiler can then start reasoning from these clues. As these proofs must hold, any precondition that would probably lead to them being false must evidently also be false. This allows the compiler to elide unnecessary checks like double null pointer checks, unnecessary branches, and dead code. This is especially potent in combination with inlining and monomorphization as it allows the compiler to reason even across function calls. And that's basically it. The compiler has no specific reason why it wants to know that, it simply tries to have your code be as optimal as possible, based on the assumption that it is valid. And knowledge that uninitialized memory reads are invalid is simply another input into the optimizer. It allows it to eliminate any branch that would lead to uninitialized memory being accessed, which speeds up the program.
Im not sure. Also tho, the site is responsive and all features work on mobile.
Actually, Aaron didn't talk about the Rust language, he only talked about the Rust community and its governance. Of course this is also relevant for Racket, which is also an Open-Source programming language.
Or [0,256]. Which any CS student could understand.
Thanks! That's a great question. &gt; For a type where any bit pattern is valid First, to answer the question you did not ask, `MaybeUninit&lt;T&gt;` might be the only such type. Integers might be valid for any *initialized* bit pattern, but not for *any* bit pattern. See [this discussion](https://github.com/rust-lang/unsafe-code-guidelines/issues/71). &gt; is there any reasonable way to tell the compiler: "give me a value of this type, and don't waste time setting it to a value"? I think what you are asking for is an operation that is typically called "freezing". It is a non-deterministic operation (unlike allocation, which is deterministic as explained in the post) that takes in a bit string, and returns a bit string where every uninitialized gets replaced by *some* (non-deterministically chosen) *initialized* bit pattern. See https://github.com/rust-lang/rust/pull/58363 for some recent discussion and [this reddit comment](https://www.reddit.com/r/rust/comments/apreqi/ucgmiri_allhands_2019_recap/egarzl8/) for a whole bunch of concerns people raised over this operation. Frozen memory is certainly valid for any integer type, because it is initialized memory. One problem is that since C/C++ do not acknowledge that their memory model is what it is, we do not have crucial operations such as "freeze" officially supported in compilers. We would have to hack something to even implement "freeze", if we were to provide it as a language operation in Rust. At least for LLVM, [that might change though](http://www.cs.utah.edu/~regehr/papers/undef-pldi17.pdf).
Indeed, `mem::uninitialized()` and `MaybeUninit::uninit().assume_init()` are the same. That's why [the `assume_init` docs](https://doc.rust-lang.org/nightly/core/mem/union.MaybeUninit.html#method.assume_init) say you have to initialize before calling!
AFAIK, Rust entirely delegates the semantics of atomics &amp; memory orderings to LLVM, which itself picks the C and C++ memory model wholesale. As a result, I would expect to be able to apply the same reasoning in C11 (and later), C++11 (and later) and Rust.
Unfortunately that syntax would also not work well with the rest of Rust's grammar. It looks like an array access. And many non-students are also learning Rust. `[0, 256)`, while familiar to CS/math students, also strikes me as odd each time because the parentheses just feel unbalanced. I actually prefer `0..256`. But yes it's one of the more obscure pieces of Rust syntax, even though [it originates from Ruby](http://rubylearning.com/satishtalim/ruby_ranges.html) and Rust just copied it. But I figured the worst thing that happens if people do not know it is they think I am off-by-1, and I can live with that. ;)
Not just code, but any text. Even for those of us who can see, viewing ‚Äútext‚Äù that doesn‚Äôt scale or wrap/scroll properly is difficult to read.
Tbh, idk but some people are definitely bothered by it. I do think people on reddit abuse the downvote but you could argue that it's still worth it or maybe there could be a way to improve its usage.
...and using a half-open interval isn't just limited to languages beginning with R. Python's `[start:stop]` slice syntax works the same way. It makes it easy to take chunks of a sequence because the `stop` value of one iteration becomes the `start` value of the next.
My experience is more with formal methods than with compilers, so I do not know the concrete reasons. My best guess is what I wrote in the post: &gt; Ruling out operations such as comparison on uninitialized data is useful, because it means the compiler does not have to ‚Äúremember‚Äù which exact bit pattern an uninitialized variable has! [This older post of mine](https://www.ralfj.de/blog/2018/07/24/pointers-and-bytes.html#uninitialized-memory) provides some more details.
Agreed! However: &gt; User dereferences memory? This memory must have been initialised previously. Notice that a mere *read* of uninitialized memory is fine in both C and Rust. It is only when you "do something" with that memory that you get UB. Or in other words, you *can* `memcpy` (copy) uninitialized memory, but you *cannot* `memcmp` (compare) it.
From the documentation of [Ordering::Relaxed](https://doc.rust-lang.org/std/sync/atomic/enum.Ordering.html): &gt;No ordering constraints, only atomic operations. &gt; &gt;Corresponds to LLVM's [Monotonic](https://llvm.org/docs/Atomics.html#monotonic) ordering. From LLVM's documentation of Monotonic ordering: &gt;Monotonic is the weakest level of atomicity that can be used in synchronization primitives, although it does not provide any general synchronization. It essentially guarantees that if you take all the operations affecting a specific address, **a consistent ordering exists**. \[emphasis mine\] Since a consistent ordering exists, the code can still be reasoned about. Either thread A loads first, and gets a 0, or thread B loads first and gets a 0. In either case, the first thread doesn't store anything. So the second thread will also load a 0, and therefore not store anything. After joining, no one has stored anything, so your program will always print 0. The only way a thread could store 42 (Store A) is if it has loaded 42 (Load A), which can only happen if the other thread has stored 42 (Store B), which it will only do if it has loaded 42 (Load B), which it can only do if it the first thread has already stored 42 (Store A). So Store A must happen before Load A, which is before Store B, which is before Load B, which is before Store A. That's a circular ordering, which is **not a consistent ordering**. And LLVM guarantees that a consistent ordering exists. (Technically, Rust's memory model is not strictly defined yet, so nothing is guaranteed for future versions or implementations of Rust. But this particular example is unlikely to change.)
What code would break if the compiler were to assume that all integers are always initialized? My assumption is that if the integers are written to before they are read,the uninitialized memory would not be observable.
I've seen similar stuff in the elm world, where people use something called create-elm-app instead of the perfectly functional and simple basic tools. Then their project doesn't work because of some weird dependency. Having a rube goldberg dependency tree seems to be normal in JS though.
Would this be usable to spin up some sort of personal Reddit? I'd love to have something private for family and friends.
That's actually not how relaxed is supposed to work in C++, which LLVM is referencing (*This corresponds to the C++11/C11 memory_order_relaxed*); though I grant you that LLVM does not define **consistent ordering** here, which is confusing. But relaxed normally works on a partial order, and has to only admit the intra-thread order. So Load-A must be before Store-A, and Store-A must be before Load-B, but Store-A and Load-B are unordered, as well as as Store-B and Load-A (in relaxed memory orderings, a write-seen relationship does not impose an order relationship) . That's also why the C++ standard has to go out of its way to explicitly prohibit OotA-reads.
 src-&gt; silicon main.rs -o main.png thread 'main' panicked at 'assertion failed: !result.is_null()', /Users/jamie/.cargo/registry/src/github.com-1ecc6299db9ec823/core-graphics-0.17.3/src/context.rs:104:13 note: Run with `RUST_BACKTRACE=1` environment variable to display a backtrace.
I'd say both are distributed. Federated, IMO, means that we can have a far more usable system. Email is federated. Mastodon has had tremendous growth and other networks built on ActivityPub can interact with Mastodon very easily because of that. I think federation is a very pragmatic way of implementing a decentralized architecture and allowing a standard method of communication between them.
Yup. Install instructions here: https://github.com/dessalines/lemmy/blob/master/README.md#docker
You'd sign up on your friend's server, you can make any communities you want over there, and they could appoint you as an admin over everything, or you just mod the communities you make. The concepts are the same as reddit. If you sub to federated communities, you will see them on your front page, but the instance that hosts them has ultimate control over them.
&gt;But remember it " replaces non-Unicode sequences with U+FFFD REPLACEMENT CHARACTER" so if you then try to open that file, you're actually opening a different file. On a Unicode compliant system, I'd expect that U+FFFD is a blacklisted character in valid file names (similar to the forward slash "/" and null terminator "\\0" on Unix-like systems).
You could refrain from any kind of engagement, i.e. watch the "fight", but no participate.
On unix, filenames are just a series of bytes. The only invalid characters are null and the separator.
Why didn't you just do `0..=255`?
Ooo that sounds interesting to me. Do you have some good references on where to start?
I especially like the coreutils project: https://github.com/uutils/coreutils/tree/master/src If you‚Äôre familiar with gnu coreutils. It captures the unix philosophy where every application does one thing and does it well. I can‚Äôt comment on the code quality though, but it has been extremely helpful to me.
I had this same issue with one file, but couldn‚Äôt reproduce it in another.
Thank you. I‚Äôll check it out.
You're right that LLVM doesn't really define the words *consistent* or *monotonic* here, but I interpret it to mean that there is one particular, monotone order that will operations will occur at run-time, consistent with the ordering graph. I also interpret "monotonic" to mean that this graph is acyclic, so that at least one such ordering definitely exists. So while it doesn't place any extra edges on that operation's vertex in the graph, it still guarantees these properties of the graph as a whole. But I realize that it doesn't actually say "acyclic ordering graph" anywhere in the docs. It just says "consistent" and "monotonic", which is a bit fuzzy. And I also have no idea what `memory_order_relaxed` means in C++-land since I'm not a C++ programmer. So it could be that my understanding is incorrect here.
SVG would be nice, but I'm quickly demotivated when I see XML, as in f.x. [https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/SVG\_and\_CSS#Example](https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/SVG_and_CSS#Example) ... but the SVG image is really beautiful: [https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/SVG\_and\_CSS#Result](https://developer.mozilla.org/en-US/docs/Web/SVG/Tutorial/SVG_and_CSS#Result)
FB, twitter, insta, youtube all have either removed, or dont have downvotes. That's partially why the comments are so much worse.
&gt; My assumption is that if the integers are written to before they are read,the uninitialized memory would not be observable That's not how this would go. For some types, we need invariants that *always* hold any time the data is even just *copied* at that type. For example, a `fn foo(b: bool)` is compiled telling LLVM that `b` will be 0 or 1. This means calling `foo` with `3u8` transmuted to `bool` must be UB *even if `b` is never used*. The mere fact of doing a *typed copy* (for passing the arguments) is enough to cause UB should the invariant be violated. This is why `let b: bool = mem::uninitialized()` is insta-UB even if `b` is never used -- there was a typed "copy" going on when the function returned. For `bool` the invariant is "must be `0x00` or `0x01`", which is not satisfied by `0xUU` (my notation for an uninitialized byte). So, for integers, what is that invariant? We have two options: (a) anything goes, (b) the integer must be initialized ("anything except for `0xUU`", for the 1-byte-sized integer types). The pros and cons [are being discussed here](https://github.com/rust-lang/unsafe-code-guidelines/issues/71). If we go with (b), then the following code is UB: let x: [u8; 256] = mem::uninitialized(); some_fn_to_initialize(&amp;mut x); It is unclear if this code would actually *break* though as it is not clear how the compiler could exploit this UB. That is one of the arguments against making this UB.
There are clear downsides to federated servers over a p2p architecture. Just as p2p has it's disadvantages. I was curious why they chose the trade-off they did. Typically it comes down to ease of implementation but sometimes it a reflection of the intended function.
That doesn't address the issue of feigned engagement.
Huh, that's interesting, so if it is just a series of bytes, could a Unix file name theoretically be an illegal UTF-8 sequence like "\\x80\\x80"? Because in order to be Unicode compliant, these sequences must either be replaced with U+FFFD, be filtered out altogether, or signal an error that is appropriately handled by the caller. (see The Unicode Standard, Chapter 3, section 2)
I like your project and I say that it doesn't matter that some people don't find it useful to share code as an image. Can you imagine someone complaining to a farmer because the farmer grows peanuts, but the critic doesn't understand the point of nuts in general?
This is a good resource, but please keep in mind that this project was started long before Rust hit 1.0. The last time I looked, the code had evolved over the years, and is not necessarily idiomatic today. (The time leading up to Rust 1.0 was particularly difficult, where much had changed at a rapid pace.)
Yes that is possible and the entire reason Path/OsString can not just be converted to a string.
If the tool is only(\*) for Rust source code, I would change the GitHub repo title to include "Rust", i.e. &gt;Create beautiful image of your **Rust** code. Maybe change "code" to "source code"? : &gt;Create beautiful image of your **Rust source** code. &amp;#x200B; ad (\*) The README only mentions that your tool is implemented in Rust.
Or their friend is allergic? I mean, I totally get the frustration with sharing code (or text) in images. However, I don't see any better approaches in the current web. And yeah, OP's project looks pretty neat!
Can anyone send me a link to the Discord? The link here and on the Community page of the website ([https://discord.gg/rust-lang](https://discord.gg/rust-lang)) says it's expired when I try to join.
And it already found three bugs! This is great work. I wonder if we can somehow extend this to find miscompilations, which are usually much worse than crashes.
What are the downsides?
I haven't really followed the discussion about uninitialized memory and UB, but why aren't uninit memory simply modeled as data read from, say, `/dev/urandom`? That is, a byte is _a_ byte, but it's value is indeterminable?
I love it. Do you have any plans to implement comment collapsing?
That is addressed in [this](https://www.reddit.com/r/rust/comments/cd522f/what_the_hardware_does_is_not_what_your_program/etrxt3s/) comment.
Thanks for the heads-up.
Well, not really - freezing memory would change "nondeterministic" uninitialized memory (I would just call this arbitrary memory) to "deterministic _initialized_ memory". I'm asking why this isn't the semantics of uninitialized memory in the first place. I also looked through the links, but couldn't find anything except discussions about what this freeze operation could look like. (And really, GitHub issue threads is really a terrible way of reading a discussion :( )
IntelliJ-Rust has always been able to expand macros. But recently, experimental support for **autocompletion within macro calls** has been added. This is really hard to get right, because macros can contain complex domain-specific languages. Another point where IntelliJ-Rust still struggles is **name and type resolution** across macro boundaries, e.g. when types are defined or implemented in a macro.
Cool! I‚Äôve used [afl](https://github.com/rust-fuzz/afl.rs) in the past for fuzzing. This one is using [`libfuzzer`](https://github.com/rust-fuzz/libfuzzer-sys) by the looks of it? Is anyone familiar with how they differ in approach?
But that's my point. It's totally valid that their friend is allergic and has no use for the product. But this doesn't mean that no one should grow peanuts. There's plenty of other produce in the world. ;)
Our professor would do this for assignments so people would have to type out the code. Or use OCR...
Oh dang I'm impresaed
Thanks a lot for your advice and for the explaination. I think I'll use iter() though, I like it :)
I have to admit I really do not understand UB at all. let b: bool = unsafe { std::mem::uninitialized() }; I understand that this is UB, but what is the problem here if it is never used (or written to)? What can the compiler do here? It feels extremely sketchy that the compiler can use this UB to do something terrible. Can't it emit a warning? Reads on uninitialized memory that I can understand, but I really have a hard time seeing how this UB can cause something unexpected.
Right, I was adding to your point, not disagreeing.
Well it doesn't since input is passed as a &amp;str. When nom fails to parse it returns an error and the remainder but not the whole string
Apache arrow has a rust wrapper, I‚Äôve been exploring way to use rust + python together. High performance in python relies on native code, but as other posters have pointed out a lot of heavy lifting has been done already.
Will mom consume watermelon slices?
TWIR QotW nominee
At a high-level they're pretty similar. They both instrument binaries using llvm's instrumentation (if you compile using afl-clang), but libFuzzer uses its own mutator engine and is faster given it runs in-proc.
Very cool, spun it up locally yesterday. Once federation is in Ill likely have a small social network between my peer groups.
If the server hosting a community dies you don't have any replication for another server to take the lead ? I don't know, things like that
Mom consumes pizza slices.
I don't think the number of repo's is going to matter very much. That's just data on a disk. The number of concurrent users (both human and system) is what will have the largest effect on how it performs. If you are going to have a large number of users (say, more than a few hundred) you are going to need to be distributed - regardless of implementation language. Unless this will be a critical part of your solution, I would advise going with a hosted solution.
I have not actually tried to combine this parser with another one, maybe I should tinker with that and RTM some more - I'm a bit new to both Rust and Nom...
This is great work, but a grammar based fuzzer would be significantly more useful here. Using AFL/libfuzzer for this will likely only pop out super high level syntax bugs, like the ones it‚Äôs already found. It likely won‚Äôt find more complex issues which rely on interesting interactions between objects in the code, which could have much more serious implications, like memory corruption
Aww :( I clicked thinking this was something that would interpret my code as an image. Even though the image would look like something my 2 year old would draw if they got ahold of a box of sharpies, I still would of loved to see it.
It was joke. /s was missing.
Yeah, don't judge XML by SOAP. Some of the .Net based SOAP services I've seen would have encouraged me to forget programming altogether when I was starting.
 We invited Aaron because he and the Rust community have thought a lot about doing language design and community development in the open, and we thought there would be things to learn from him and from Rust's experience. Aaron is also a long-time colleague and friend of people in the Racket community, and has contributed to Racket in the past.
Not sure if this fits to your needs, but talking about stack-oriented listings, the crate [https://crates.io/crates/seq](https://crates.io/crates/seq) permits to lay out a container over nested stack-frames. Whereas smallvec has a fixed amount of elements managed on stack, the sequence seq may grow on stack dynamically with each lexical scopes and function call.
&gt; MaybeUninit&lt;T&gt; might be the only such type Or a similarly defined type: ```rust pub union MaybeUninit&lt;T&gt; { uninit: (), value: ManuallyDrop&lt;T&gt;, } ``` I haven't examined the implementation in detail, but as far as I'm aware `MaybeUninit` doesn't require any compiler magic to implement. I'm not sure there's much reason you'd define a union with a `()` variant, rather than just wrapping `MaybeUninit`, but *technically* it's not the "only" way for this to be safe.
Many security concerns due to the fact that upgraded standards are extremely difficult if not impossible to roll out. The replication of data across many unknown but trusted hosts. Federated systems like email are often largely captured by large organizations that provide convenient access to servers that users don't need to maintain.
I would also like to know more, please
&gt; the default destructor will be recursive too. That means a large instance can blow your stack and crash your program Fascinating, I had never thought of that!
This one was glorious, and looking back the core team did the Right Thing.
This is unexpectedly simple to setup, compared to what I'd expect to be necessary for a grammar based fuzzer which is a pro and con. The problem I see for finding miscompilation would be that the pure fuzzing approach doesn't have a baseline to compare against and no concept of semantic preserving mutations that would be necessary for that.
How much can the client trust the things hosted on other servers? As in, what stops a server admin to spooky links once something get popular, inject malicious code, manipulate votes or edit other user's comments?
How does it initialize the corpus? Since the compilation need not succeed but only some good initial coverage of syntax, I suppose it would be possible to simply download a few top Rust crates and put in their source files.
Awesome! I will check it out.
I will try it :)
Thanks for your advice. But in fact it uses syntect as a syntax highlighting backend, so it works for multiple languages.
 It's not, though. Microsoft themselves recommend Linux containers on Windows for production applications, and their flagship .Net Core example app is built as exactly that. [https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers](https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers) [https://github.com/dotnet-architecture/eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers). As long as you're not using Docker Toolbox, and are using regular docker with Hyper-VM, it's perfectly fine to run Docker on Windows with Linux containers.
Mastodon and other fediverse projects are dealing with this all right now. The short version is that if you've subbed to a community that ends up being spam, just unsub from that community. Also make it possible for one instance to blacklist others (Federated by default is probably preferred, as opposed to whitelisting).
Is `main.rs` the `silicon/src/main.rs`? Could you please create a issue?
Maybe worth clarifying, you get the same behavior in C++.
&gt;&gt; How do you prevent it from becoming voat? &gt; &gt; I built in a slur filter, but also IMO the best way to keep out the racist fucks is to have strong moderation abilities, which are there, but will probably need to add more as this develops. On the main instance, and this dev one, obvi I'm not going to racism, sexism, transphobia, etc. How do you prevent it from becoming Reddit? I'm only half joking, because I thought the reason people wanted an alternative to Reddit was to have less restrictions on what posts and opinions are allowed.
Ah, maybe my aesthetic is a bit strange XD Thanks for your advice. I will start to improve it.
https://www.arewelearningyet.com is a good resource for tracking the state of rust libraries in this domain.
What happens here is that the parser fragment fails (as expected). In order to give some context to the failure, it gives you the position by returning a slice of the remaining input when the error happened. In this case, it consumed both the `"` and the `t`, meaning the error occured at the end of the string, thus the error carrying the empty string. If you're calling the parser, you still have the original string slice you gave it (as nom parsers work on slices). Because of this, if you want to recover and try something else, you have the original string. And if you want to do something based on where the error occured (such as a lint?) you have that position in the string in the form of the slice carried by the error.
serde
At first I really planned to use svg as the output format, but I gave up, because I don't think I can calculate the size of text in svg in Rust. But it seems that I don't need to calculate it. I will give it a try.
Thank you for expressing your concern with this tool. I agree, we should take another look at the approach here. Considering that all of the work on wasm bindgen has been provided to us for free, I think we could take a more gentler way to criticise this. Imagine the author(s) of these tools reading criticism like in your post and in the top comment (‚Äúcode was really poor, easily replaced with two fucking lines of bash‚Äù). Any person would feel discouraged and become less likely to contribute in future.
Are you looking for /r/playrust?
Simply returning the bool from `mem::uninitialized` is already a "use": https://www.reddit.com/r/rust/comments/cd522f/what_the_hardware_does_is_not_what_your_program/ets5crl/
It's Dracula. Yes, it is availabe for nearly all editors.
Its been a long while, great to see this is still being worked on!
Yep! Definitely. In part it‚Äôs because we (the newer maintainers) haven‚Äôt really come to terms with a long term roadmap yet; if you have any thoughts, feel free to share them!
[std::ptr::eq](https://doc.rust-lang.org/std/ptr/fn.eq.html)
Emmm, it looks well on my computer, but it looks strange in Phone, and even more strange on Windows... You can custom it if it doesn't look well. like `--background '#abb8c3' --shadow-color '#555555'` (Maybe I should change the default theme...
Thanks much! That still seems wrong to me. I love your example there: I really don't *want* my compiler to optimize that hard-to-evaluate expression away, I think? In my fantasy world, there would be an analysis that would refuse that code with an error about "can't prove that x is initialized", but things are what they are. Again, great piece ‚Äî thanks for writing it.
Well, the problem with reddit is who it's run by. With reddit, if you don't like how it's being run, you can either lose everything, or suck it up. Most people choose to suck it up since there are tons of good resources on reddit Federated services by their nature are resistant to that kind of problem, due to the nature of federation. If you don't like the server you're on, host your own or join another you do like, simple as that! You can follow the communities you want no matter what server they are hosted on. This does, of course, leave the problem of how *communities*/"subreddits" are run. On reddit, if you don't like a subreddit, you just don't participate in it, or make your own, and you'd have to do the same on a federated service too. With the added challenge of if the mods don't like how the server is run, they'll need to migrate their community somewhere else somehow, which will probably be a challenge, but doable.
haha. Fancy, like it
So it seems like `always_returns_true` does not even appear in the assembly and the assertion just compiles into a panic. Presumably, the value of `x` is indeterminate at compile-time and a comparison with an indeterminate is always false. Is this correct? It would seem this has the advantage that the program has the same output. On the other hand, if `x` is an array, [it doesn't panic](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=da278adb50142d14909df74ea1e43069).
I was so fucking pissed off when a professor 'examples' of the consensus framework he gave us had hidden non-breaking spaces on the pdf. No shit it didn't work.
I was wondering about this. Now I know there's a website for ML and Rust. Thank you.
Ah yeah you're right. It's a tricky difference.
I've dabbled in Rust and think it's rather cool. But I am afraid having a package manager in the build tools is a mistake. The crate ecosystem scares me - it may destroy Rust for the reasons stated here. Adding a dependency should be a carefully considered decision, and should cause a bit of friction. It should not be like shopping in a candy store. Also, if your dependency has dependencies, that should be a red flag getting more blood red with each level of the tree.
Plenty of trolls are 'triggered' about mozilla. Makes me happy.
Thanks! What caveats are there? I updated the text of the question with a concrete example; would there be any problems using `std::ptr::eq` / `std::ptr:hash` for that purpose?
r/actix dominating TechEmpower benchmarks, amazing community, examples and ok docs üëå
i'll never get over the insistence on the sideways traffic light in those
The corpus is initialized by whatever you put in the `seeds` directory. I've had success using some files from [`rust/src/test/run-pass`](https://github.com/rust-lang/rust/tree/master/src/test). Using code from crates.io could make sense too. You should try it!
Yeah, I saw that! Really nice, I agree. But I'm pretty sure my point remains...? Not sure if you were intending to contradict me or just add some cool info.
The reason Actix has a high throughput is because they use an optimized setup in their benchmarks, designed specifically for that use case - they're not representative of real use cases. This has been pointed out in the past by others, so I won't go too much into it. Hyper and [Thruster](https://github.com/trezm/Thruster/blob/559fb8862bc07fdba6444ad7892333bf305c2245/thruster-server/src/server.rs#L36-L102) also changed their benchmarks to work in this way, which is why they're within margin of error and are "as good" as Actix. Gotham does not do this as it's fairly misleading, and we have no intention to do so. The numbers you see for Gotham are based on the numbers you're likely to see in your actual application when you ship it. But, just to point it out, Gotham would likely be comparable to Hyper if we did so (as Gotham is backed by Hyper). Either way, to say Actix "dominates" benchmarks is arguable at best, and to base your usage on those numbers is likely misguided. Apart from that, yes! Actix is another framework you can look at if you choose. More than one choice is never a bad thing :) That's the joy of building things!
Hmm, I don't see any issues. However, that does seem a bit inefficient, probably quadratic. I think you could just have a `Vec&lt;bool&gt;` of the same size and enable the bool at the same index whenever you use it. You could probably use `zip` to do it without having to index, too.
Something that is actually quite concerning about cargo and static linking is packaging software on distributions for security. A CVE in a library will need to be adopted by every application using it and can't be fixed by the distribution. It's the idea that application developers will fix their stuff and distribution developers shouldn't exist. On the other hand, I quite like cargo. It's a good thing for development and I'm not advocating going back to C because of this. But I feel that cargo needs some way of exporting and linking application packages that are better suited to this...
[`structopt`](https://crates.io/crates/structopt) is great for creating command-line programs.
I agree. Have you seen the grimoire paper? Source will be released supposedly within a month
Yessss, a perfect place to start! Thank you so much!
This looked nice, but when I tried to load it into my toml, it didnt work. :-/
If you open a ticket for it, I can get to it.
&gt; Convenience is the drug fed to lazy coders C'mon. If the "right way" to do things is inconvenient, the ecosystem will be full of wrong programs. This attitude isn't sustainable.
Yw! If it doesn't exist already, there should be a tracking page for these. "Are we X yet dot com"
According to [this](https://godbolt.org/z/0d_TNX), it doesn't look like it does currently. You do have to at least check for poisoning, so some of the locking would still have to happen regardless.
If you are fine with manually doing the optimization,you can use [Mutex::get\_mut](https://doc.rust-lang.org/std/sync/struct.Mutex.html#method.get_mut)
I don't know about optimizing away locking, but I do know of [`Mutex::get_mut()`](https://doc.rust-lang.org/stable/std/sync/struct.Mutex.html#method.get_mut) that allows getting an inner reference without locking if you already have mutable access.
If the JWT middleware works smoothly you should consider breaking it out as a separate lib/crate since almost all the other JWT crates are difficult at best. ([alcoholic\_jwt](https://docs.rs/alcoholic_jwt/1.0.0/alcoholic_jwt/) being the exception in my experience.)
Rust binaries run just fine on Raspberry Pi ‚Äî roughly the same as C/C++, because the compiled code is comparable in size and level of optimization. I'm happily running my Rust code on a Raspberry Pi 3B+, but I cross-compile everything on my laptop and then copy it over to the Pi. I didn't even bother installing the Rust compiler toolchain on the Pi itself because I strongly suspect that even if it didn't simply OOM, then the compile times would drive me insane. :)
https://doc.rust-lang.org/book/title-page.html
Its pretty decent on a PI. When it compiles it uses all the cores and keeps pretty constant memory usage, unlike haskell which can demand gigabytes of memory during compilation. My experience in developing haskell on the pi is what drove me to rust in the first place - back then cross compiling with haskell was less developed than now. I found that initial compile of my web server took about 20 minutes (compiling all the dependencies), with incremental compilation after that being fast enough for development.
Do you have anything to back up your claim of the TechEmpower benchmarks being lies? The actix benchmarks are doing whatever TechEmpower wants for every other thing, in whatever way is the right way. Awfully convenient for you to claim everyone else is wrong and somehow "cheating". Your only link being the existence of a convenient function optimized for small loads, not even related benchmarks. [TechEmpower uses normal crates.io actix](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/actix/Cargo.toml#L27) And according to their [intro](https://www.techempower.com/benchmarks/#section=intro) and [environments page](https://www.techempower.com/benchmarks/#section=environment), "Each framework is operating in a realistic production configuration" and everything is supposed to be set up "according to the best practices for production deployments gleaned from documentation and popular community opinion". If you think thats not the case then you should go fix the testcases, or at the very least open an issue, instead of complaining everyone else is wrong or somehow "misleading" and thats why your project doesnt do well on benchmarks. I suggest you check their [terminology section](https://www.techempower.com/benchmarks/#section=motivation) too, specifically this part: &gt; "What exactly causes a test implementation to be classified as Stripped?" It's not possible to paint an exact picture, but conceptually, a Stripped implementation is characterized by being configured or engineered expressly to the requirements of our benchmark tests. By comparison, a Realistic implementation will use a production-grade configuration of a general-purpose web application framework that meets our requirements. [...] I'll point out for you that Actix is not classified as "Stripped", that is, it is not tuned specifically to the benchmark tests. [They even have a semi-recent issue explaining Actix's speed on a particular test](https://github.com/TechEmpower/FrameworkBenchmarks/issues/4834) &gt; Either way, to say Actix "dominates" benchmarks is arguable at best, and to base your usage on those numbers is likely misguided. Only if you deny reality. Whether you consider the benchmarks themselves valid is another question, but one only needs to look at the chart to see what is on top.
I agree it works well. My approach : work on a PC, then git pull to the Pi when ready. Cross compiling creates more logistics headache and does not allow fast iterations on-Pi when convenient.
If you just need to call those two functions, could you do something like this? const T_FOR_SIZE: MaybeUnit&lt;T&gt;: MaybeUninit::uninit(); let size = std::mem::size_of_val(&amp;T_FOR_SIZE); let alignment = std::mem::align_of_val(&amp;T_FOR_SIZE); The docs say "`MaybeUninit&lt;T&gt;` is guaranteed to have the same size and alignment as `T`".
Thanks! Yes that's an important feature request. We'll try to have that for tantivy 0.11.0.
If you don't like it, you can hide it via \`--no-window-controls\` :)
We are not really shooting for feature parity with Lucene. \&gt; Having a stable index format and up to date docs could be huge for adoption? Strangely this is not a common request. Right now it is recommended to have a way to reindex everything. In the future, tantivy will most likely not have several codec shipped in like lucene does. Instead, there might be a utilitary to convert an index format into another. &amp;#x200B; \&gt; I say this because I tried [https://fulmicoton.com/tantivy-examples/simple\_search.html](https://fulmicoton.com/tantivy-examples/simple_search.html) ... thanks for reporting. This is not related with the index format at all though.
Thank you!
&gt; always_returns_true is a function that, clearly, will return true for any possible 8-bit unsigned integer ... Except it doesn't? It checks if the number is between 120 and 150. An 8-bit unsigned integer can be any number between 0 and 255. What am I missing?
Thanks everyone! :-) I think it's great that kids are learning Scratch and MakeCode on BBC micro:bit. So when they start working, they should naturally progress to Visual Rust! :-)
Awesome! I love using Gotham, and am excited to use the new async Diesel middleware. Has there been any discussion on the migration timeline towards `std::future::Future` once Hyper `0.13` is released [with this breaking change](https://github.com/hyperium/hyper/issues/1805)? I‚Äôd love to use `async`/`await` in my handlers. Thank you for all the great work!
The problem with Apple is they ban most social media platforms. Only those who ban a HUGE and unfair list of their competition, or people they don't like personally or politically are allowed to exist. &amp;#x200B; It's best to just support Progressive Web App format and let people send a link to their home screen.
I know of least one well-known algorithm their relies on an uninitialized memory to work, and I was curious if you have to have uninitialized memory keep the same value after the first time it is observed for it to work. I don't think it does though: https://eli.thegreenplace.net/2008/08/23/initializing-an-array-in-constant-time
@burntsushi: I got my old branch deleted so I created a branch with the changes based on the latest master to reproduce the result. You can have a look [here](https://gist.github.com/MnO2/85b8c7871edfe530a0fed34ec006d5e1), with the links to the corresponding commits.
Couldn't you store their indices instead, and then just check which indices weren't in the set of "used" ones? Then you wouldn't have to deal with reference equality.
I think I am gonna give compiling it directly on the pi a shot. The applications I will be writing will definitely be quite small for a while at least, so I am not too worried about compile times. This community is pretty awesome though. Appreciate you guys!
They weren't claiming that the library dependency was optimized for benchmarks, but that the *implementation* of the `actix` benchmark suite is. &gt; If you think thats not the case then you should go fix the testcases, or at the very least open an issue Whether a critic goes about cleaning up the issues they find is irrelevant to the veracity of their critiques.
Serde, byteorder, clap, log, lazy\_static (And/Or lazycell), nom, quote and syn, smallvec, bitflags, regex, typenum, parking\_lot, rand, failure (Or some other error handling crate), unicode-segmentation, rayon, crossbeam, num\_cpus, futures, walkdir, toml, structopt, itertools, cbindgen, url, either, backtrace. Then there's others depending on what you're doing (Like std-web if you're doing wasm through wasm-unknown-unknown or winit if you're doing graphics or rodio for audio). I was going to run cargo-tree on a repo of mine but then I ended up just skimming it's deps while it built :P
&gt; but that the implementation of the actix benchmark suite is. Thats literally the exact thing the rest of my comment goes into, but ok. You're focusing on one line out of the entire comment. &gt; Whether a critic goes about cleaning up the issues they find is irrelevant to the veracity of their critiques. Whether they bother to report them, however, is. Hence the second part of that sentence. If they think theres a problem with the implementation benchmarks, they can back it up and report it so it gets fixed.
There was an example of async/await put in PR a while ago, so we've definitely being thinking about it. We'll definitely be updating Hyper (we try to keep everything as near to latest as possible), so if that entails migrating `Future`, then I guess that's what we're doing :p
&gt; Do you have anything to back up your claim of the TechEmpower benchmarks being lies? Let me begin by saying that "lies" is definitely too strong; the numbers are not "wrong", they are simply not entirely representative of what you should expect. &gt; The actix benchmarks are doing whatever TechEmpower wants for every other thing, in whatever way is the right way. Awfully convenient for you to claim everyone else is wrong and somehow "cheating". This is very accusatory, and I'm only slightly offended :) I have zero interest in competing over such things. I also never used the word "cheating"; please do not put words in my mouth. If Gotham were slower (and it very well may be), then I am enough of a person to admit that and strive to improve it. To imply that I am petty enough to make things up around this is fairly unnecessary. &gt; Your only link being the existence of a convenient function optimized for small loads, not even related benchmarks. Sure! I didn't have the time to write earlier, but I'll expand a little. The link I provided is an implementation in Thruster designed to optimized for "small loads", where endpoints are "similar in their load". This essentially boils down to "the types of endpoints you'd expect in a benchmark", even though it doesn't say as much. I'd expect the author would agree with this, were we to ask them. This function is entirely related to benchmarks, as it is the function used in the TechEmpower benchmarks which can be seen [here](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/thruster/src/main.rs#L51). What makes this implementation different is that it spawns several threads, with a single-threaded Tokio Runtime on each thread. This means that any given runtime has none of the overhead that is usually attributed to dealing with concurrency and being in a multi-threaded environment. If you were to build a server using the framework, you would typically have a single, multi-threaded environment. This is quite a large difference. If you check, you'll see that the Hyper benchmarks [do the same](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/hyper/src/server.rs). Whilst Actix benchmarks used to employ this trick too, based on your links it appears that they have actually now made this a first-class thing in their library now, via: &gt; Actix is single threaded, it runs in multiple threads but each thread is independent, so no synchronization is needed Which is pretty cool, and makes sense why those three projects are all within margin of error. However, the Actix benchmarks are still doing things that I would consider "unusual", such as: * [Changing the default allocator](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/actix/src/main.rs#L1-L2) * [Using unsafe](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/actix/src/main_raw.rs#L85) * [Embedding header _values_](https://github.com/TechEmpower/FrameworkBenchmarks/blob/master/frameworks/Rust/actix/src/main_raw.rs#L26-L33) As I said before, this is not "wrong". However one must consider whether these numbers correctly represent the numbers you're going to see as a user. Are you comfortable with the benchmarks assuming that all users will change their default allocator? If not, then what do these benchmarks really mean for you? &gt; If you think thats not the case then you should go fix the testcases, or at the very least open an issue, instead of complaining everyone else is wrong or somehow "misleading" and thats why your project doesnt do well on benchmarks. There is little point as the authors would of course disagree (as is their right), and after long debates, nothing would change. It should also be noted that I was not "complaining everyone else is wrong", I was responding to someone using their benchmarks as their opening point when talking about Actix. I have zero interest in starting flaming threads, as I fear this will devolve into. &gt; Only if you deny reality. I guess that depends on your reality. If you actually do look at the benchmarks, you'll see that Hyper tops Actix whenever they're both present. Thruster is also up there too in many. To say that Actix "dominates" is definitely questionable, as "dominates" (at least to me) signifies a very large margin. Note that this is entirely disregarding the speed of Gotham either, before you attempt to claim bias again :) Your entire comment seems deliberately provocative and, if so, I'm not sure why you approached it that way. I have spent a fair while writing this reply and re-reading code to make sure I'm not misrepresenting anything; if I am, I welcome all (constructive) corrections.
&gt; Whether they bother to report them, however, is. Hence the second part of that sentence. If they think theres a problem with the implementation benchmarks, they can back it up and report it so it gets fixed. In general, I agree with you. In practice, your comments alone are evidence of why this is easier said than done. If a user gets this defensive/passionate about someone being (what is perceived as) critical of a project, imagine the response of the author! Good luck getting anything done at that point :) Everyone should at least be somewhat aware that benchmarks should be taken with a pinch of salt. If someone wishes to blindly select a project based on a single number, without prototyping their use cases and testing those, then that is their right and they can live with the outcome. There will always be benchmarks that are inaccurate, whether deliberate or not.
Declarative macros are simpler to use for simple tasks once you're familiar with the syntax, but less powerful. If you've ever written a web application, they're like HTML templates, but for Rust code. (They're Rust code with extra markers that say "insert argument here" or "repeat this part for each argument") Procedural macros are more powerful, but there's a certain amount of minimum work that you have to do, no matter how simple your task, because they're full-blown programs that run inside the compiler. (A procedural macro is a program which will receive Rust code and must modify it however it wants and then return it.)
Excited for `gotham v0.5`, then! Haha
Before reading your explanation of Rust as an abstract machine and distinction between frozen and uninitialized I thought it made perfect sense that a type with a disallowed bit pattern was a harder case for `uninitialized` than a type allowed to be in any bit pattern. But when embracing the idea that an uninitialized value is not (to the abstract machine) in *any* of those bit patterns, and is instead treated as uninitialized, I don't understand why the `bool` should be any harder than the `u8`. &gt; This is why let b: bool = mem::uninitialized() is insta-UB even if b is never used -- there was a typed "copy" going on when the function returned. For bool the invariant is "must be 0x00 or 0x01", which is not satisfied by 0xUU (my notation for an uninitialized byte). Why not loosen the invariant to "an [b]initialized[/b] bool must be 0x00 or 0x01"?
&gt;When should I use each kind of macro type if I want to make my own macros? For me it mostly comes down to complexity. Whilst you can do some fairly complex things with `macro_rules!` (aka [macros-by-example](https://doc.rust-lang.org/reference/macros-by-example.html)) such as [Incremental TT munchers](https://danielkeep.github.io/tlborm/book/pat-incremental-tt-munchers.html) things can get rather unwieldy and at a certain point you are better off switching to procedural macros. For simple pattern-matching of input fragments they work well enough. Procedural macros also allow for [custom derive](https://doc.rust-lang.org/reference/procedural-macros.html#derive-macros) &amp; [custom attribute](https://doc.rust-lang.org/reference/procedural-macros.html#attribute-macros) which are not possible with `macro_rules`. There is a steeper learning curve with procedural macros, including getting familiar with both the [syn](https://github.com/dtolnay/syn) and [quote](https://github.com/dtolnay/quote) crates (technically not needed by essential in practice). Personally I started out using only `macro_rules!` and then moved on to procedural macros and have since rewritten some of my early macros using the latter to improve the code readability.
Man I wish reddit could make your messagebox red without needing a refresh
You haven't heard? If it's not mimicking MacOS, it can't be beautiful *by definition*.
Just additional context.
Hey I appreciate the effort! I'm not sure exactly what I am going to build exactly, have to figure out what rust is really good at doing first. Definitely will get some inspiration looking through these crates though!
No worries. No orangered on RiF with dark theme.
I'll never understand apple or their customers.
thanks!
Could you open a ticket and list the stuff you would like to be able to do?
Distributed log indexing is a domain I am personally very interested in.
When I want to create an "uninitialized" \`T\` I usually wrap with \`Option\`. Isn't that the benefit of having that feature available? Am I missing something?
I blog a lot and have experimented with \*\*inline\*\* SVG for many years. Nowadays, I say fuck it and do so where it suits. I do this knowing that 1% of the world browsers can't render what I'm pasting in. That's reduce further over time. It is a better (leaner/quicker) delivery mechanism and allows for screen readers for those with vision challenges to have a go at reading the image.
Here is a good, curated list [of idiomatic rust crates.](https://github.com/rust-unofficial/awesome-rust)
Awesome! Thanks a ton!
https://github.com/rust-embedded/cross this is a lifesaver
Defining a specification that encapsulates a deterministic memory model is trivial and is left as an exercise to the languages‚Äô implementation.
https://github.com/rust-embedded/cross headache gone
Ahh, I always wondered how these fancy images were made, thank you!
&gt; So far whatever language we learned, they never care about it and just introduce the global variables proudly Weird, I learned programming in the late 90s and have *never*, in any language, seen globals introduced "proudly". It was always with a big fat "makes your program hard to understand, avoid using" warning.
&gt;Well it doesn't since input is passed as a &amp;str. When nom fails to parse it returns an error and the remainder but not the whole string. But in any case mom doesn't consume anything since it only operates on slices. Starving mom :(
&gt; In general, I agree with you. In practice, your comments alone are evidence of why this is easier said than done. The "WAAA i think you hurt my feelings so i won't do the right thing" response. Classic. Either you can back up your claims, or you can't. And it's pretty damn clear you can't. In the extremely unlikely event you *can*, thats just worse because it means you know a problem in the project and refuse to report it because you're, what? scared someone might be criticize you? I wish you the same "luck", bug reports-wise, in your own projects. &gt; Everyone should at least be somewhat aware that benchmarks should be taken with a pinch of salt. Thats true, but not at all what you were arguing. You weren't arguing the TechEmpower benchmarks, you were arguing specifically the actix benchmark as wrong and somehow "misleading", and arguing that despite actix clearly being on top of those benchmarks, it somehow isnt and that it's actually "dubious" to claim so. The classic "move the goalposts and cry about it" technique. Can't say i'm surprised. &gt; If someone wishes to blindly select a project based on a single number, Benchmarks aren't perfect, but they're a helluva lot more than just "a single number", and they sure as hell aren't meaningless. They might not perfectly model your specific workload, but a well designed benchmark can be a damn good indicator of the right tool for you, assuming of course you actually know what your needs are. &gt; If a user gets this defensive/passionate about someone being (what is perceived as) critical of a project, imagine the response of the author! This might come as a shock to you, but i'm not the author of TechEmpower or whatever. If you base your opinion for entire projects on what random people on reddit say, then boy do i have news for you.. Also, it's funny you should say that, considering your FUD-spewing behavior here. If theres one thing you've communicated clearly, its that your projects shouldn't be used under any circumstances. If the author gets this defensive/nonsensical about someone being (what is perceived as) critical of a project, imagine the quality of the project itself!
That‚Äôs a good to solve ‚ÄúI need (something close enough to) a `&amp;T` value but I don‚Äôt have a `T` value‚Äù. However `MaybeUninit&lt;T&gt;` requires `T: Sized`, so you can‚Äôt use it with `T = dyn X`. And if you know the concrete type that implements the trait, you can use `size_of::&lt;Foo&gt;()`.
That's really interesting. Thanks!
Thanks for this. I was concerned too when I first [had a look at wasm](https://dev.to/dystroy/an-exploration-or-rust-wasm-50gp) and all the first links I've found were boasting generate, node, npm and other nonsense. The useless nightmare of webpack and other tools managing at the same time to claim simplify things and making even the simplest ones painful, obscure and magical should absolutely be avoided in Rust. We don't want people to randomly copy recipes into configuration files until something seems to work.
I prefer [clap](https://crates.io/crates/clap), but structopt is great too
Are you working fulltime on it?
I'm having a quarter-life crisis and started learning Rust a week or so ago. I've decided to abandon a medium-size GRPC-heavy Go project and instead write it from scratch in Rust. I'm not sure how wise that is.Thanks for sharing your code.
That can't happen in safe Rust :p
Just don't go crazy with the code size.
I'm curious: How did you come up with that initial example? What kind of hardcore obscure voodoo magics do I need to read up on in order to know that, specifically, \`x &lt; 150 || x &gt; 120\` will trigger the \`assert\`?
How does this prevent issues like trying to compute 2^1000 at compile time and timing out? That's something I quickly ran into when fuzzing Python interpreters for example (because 2**1000 is already a valid program there).
I'm not sure the statement that indeterminate values are unclear in C is correct. The paper cited is a "proposal" that never got merged, while we have a merged Defect Report where the commit clarifies that the C standard should be read as: &gt; In the case of an indeterminate value all bit-patterns are valid representations and the actual bit-pattern may change without direct action of the program. That's crystal clear.
The gRPC part of the easy one, after all it just takes a request and forwards it to your systems, then takes back the response. If you use bi-directional streaming, you might not have as much flexibility as you'd have with other languages (at least in my experience), otherwise you'll be fine if your Go logic is translatable to Rust, and if you have crates doing what you need.
&gt; Your entire comment seems deliberately provocative and, if so, I'm not sure why you approached it that way. Because i did. Thats pretty much my default, I'm a poor debater, quickly assume bad faith, cynical, etc. I'd say sorry but i wouldn't mean it. &gt; This function is entirely related to benchmarks, as it is the function used in the TechEmpower benchmarks which can be seen here. That would've been good to highlight in your original post, which otherwise had no context whatsoever. &gt; This essentially boils down to "the types of endpoints you'd expect in a benchmark" That depends on whats considered a small/similar load. Not every website needs to 3 different servers to handle 100k connections per second, with 60k of them being on pages X, Y, and Z and the remaining 40k on pages A-W. It's more than general purpose/realistic enough to count IMO, though it can go either way. All depends on your projects needs. Probably better to bench *both*. &gt; If you were to build a server using the framework, you would typically have a single, multi-threaded environment. Typically, maybe. Which means that there are times when they don't. Are the TechEmpower benchmarks intended to exclude any project that serves a specific kind of server use-case? &gt; This is quite a large difference. If you check, you'll see that the Hyper benchmarks do the same. Guess the TechEmpower people think it's general-purpose/realistic enough to count, then. Not everyone is a big company needing lots of servers and concurrent connections. How many connections can the per-thread version actually handle? I suspect it's more than enough for plenty of use-cases. There *is* difference between optimizing for relatively small workloads and optimizing for the benchmark. &gt; However, the Actix benchmarks are still doing things that I would consider "unusual", such as: Luckily for IT and servers everywhere, nobody cares what you think is "unusual" &gt; Changing the default allocator ..would be one of the *first* things a realistic production web server does, would it not? Using an allocator that performs better for their needs? In what world is that unusual? &gt; Using unsafe I.. just.. what? *Huh*? I can't really respond further to this because i simply can't understand how anyone could possibly think thats "unusual". Are you discounting any C/C++ benchmarks for the same reason, since those entire languages are unsafe? &gt; Embedding header values ..again, what? Jesus you get worse the worse you go on. In what world is using constants for unchanging data a problem? [Other benchmarks do it too](https://github.com/TechEmpower/FrameworkBenchmarks/search?q=HTTP%2F1.1+200+OK%5C&amp;unscoped_q=HTTP%2F1.1+200+OK%5C). ---- If you still have a problem with any of those issues, then you need to be complaining about the TechEmpower benchmarks and their rules/setup/environment/classification/whatever, because they seem to consider it just fine. All well and good to criticize the benchmarks, but you're only criticizing the *actix* benchmarks for perceived "problems". Unless you can actually point to something where it runs afoul of the standards for all TechEmpower benchmarks, it's just your ill-founded FUD opinion that denies basic reality. In any case, no matter what you think of the benchmarks, actix is objectively at the top of them. &gt; Are you comfortable with the benchmarks assuming that all users will change their default allocator? Yes, thats the *entire* point of these benchmarks. To represent a realistic/production system. And a realistic/production system is sure as hell going to change the allocator. It sure sounds like your problem is that the TechEmpower benchmarks are trying to represent real workloads. *Why* you think thats an issue, no idea, and certainly no idea why anyone else agrees with you. &gt; If not, then what do these benchmarks really mean for you? That you're not using your framework properly or don't know what you're doing in general. &gt; There is little point as the authors would of course disagree (as is their right), and after long debates, nothing would change. And you're basing that on..? I mean, given your arguments, you're probably right that they'll disagree for the simple, objective reason that: ***You're wrong.*** But *you* probably don't think/admit that, so how do you know they'll disagree if you havnt opened an issue and started a discussion. If you're so confident in your points, why don't you think you can convince them? &gt; I guess that depends on your reality. No, it doesnt. Thats the wonderful thing about reality. If you look at the benchmarks, Actix is at top of them, at worst it's tied with other high-performance frameworks(they're all so close it wouldn't matter in practice, less than 1 percent differences). Nothing you say will change that. Whether you think the benchmarks are valid is a completely different question, but actix is certainly at the top of them. &gt; To say that Actix "dominates" is definitely questionable, as "dominates" (at least to me) signifies a very large margin By your definition Actix *does* dominate. The only benchmarks where it "doesn't" are the ones where it's still at/near the top, but everything up there is really close in performance. Specifically, only in the JSON Serialization, and Plaintext benchmarks does it not dominate. And in those ones, *none* of them dominate since they're all within 1% of each other. In the Single/Multi query, Fortunes, and Data Updates benchmarks it dominates anywhere from ~3% to ~40% faster than others. Far more practical performance differences to care about than the, for example, 0.1% difference between Hyper and Actix in the plaintext benchmark. And this doesn't even get into cloud hardware, which reverses some of this. For example, on the Plaintext and Json benchmarks, Hyper performs far worse on Cloud hardware, and Actix slightly worse, but far better than Hyper(~30% faster), and still near the top. For the other benchmarks Actix-core and Actix-pg remain in the top 3. &gt; Note that this is entirely disregarding the speed of Gotham either, before you attempt to claim bias again :) Too slow to practically measure doesn't count as disregarding :) &gt; if I am, I welcome all (constructive) corrections If only you were as welcome giving them, since the actix benchmarks apparently so sorely need your insight.
Having written the hyper benchmark, I agree that it feels gross. Throughout the benchmark files, I've left comments pointing out weird things that were copied from other benchmarks. The reality is far too many people just look at what's fastest and assume that means best. So I felt I "had" to do it.
macro_rules! will be deprecated to declarative_macros 2.0 where it would be possible to have full namespaced macros (macro_rules! need to specify `#[macro_export]` to use outside your crate and it will be always on your crate root namespace) Like this ``` mod mymod { macro mymacro { ... //Definition here. Internal syntax is the same as macro_rules! } } ``` And be loaded like this ``` use mycrate::mymod::mymacro ``` Also, declarative macros 2.0 will improve vastly the macro_rules! performance and hygiene Sadly, it isn't in the 2019 road map, so it will probably be completed 2020 or latter
&gt; What are some of the more essential crates I should install? None. For the crates mentioned here, you should read the summary of what they do and keep that in the back of your head. Don't just go wildly adding crates to your projects until you actually need them.
The compiler is not aware of what `Mutex` is, so no, it can't and won't.
&gt; One downside of procedural macros is that they must be defined in a separate create which &gt; only &gt; exports procedural macros (discussed &gt; here &gt; ). An annoying paper cut, hopefully this will be improved one day. I'm not sure, makes sense to me to be a outside crate, considering that proc_macros are pretty much a compiler plugin Also, sadly, procedural macros aren't hygienic as of today, but there is effort to improve that
Yup, I mean, it's really just your fairly typical web app, so nothing fancy and should be super easy to transfer over. I'm more concerned with the maturity of the libraries. I gather that prost and tower-grpc is the best way to go, but I don't know enough Rust yet to make any kind of meaningful decision about it. I'll probably build a simple endpoint service with prost/tower-grpc and see what's up. Is there any reason you chose two libs? I read on a thread here that tower-grpc is more idiomatic rust...
I guess install wasn't the right word. I didn't realize that crates were managed on a per project basis, I thought if you installed it once it was able to be used on a system wide basis, like pip with python.
Not resources exactly....,but i will tell u what i did. This first helped me a lot as a starting point-&gt; [allocators 101](https://www.jamesgolick.com/2013/5/15/memory-allocators-101.html). Then at the end of it there are links that i followed to understand in depth. Rest i just took inspiration from existing implementations when i got stuck (helps if u know c). Or u can google some memory allocators in rust. For the lazy-&gt; =[malloc(3) man page](http://man7.org/linux/man-pages/man3/malloc.3.html) = [http://www.cs.cmu.edu/afs/cs/academic/class/15213-f10/www/lectures/17-allocation-basic.pdf](http://www.cs.cmu.edu/afs/cs/academic/class/15213-f10/www/lectures/17-allocation-basic.pdf) = [tcmalloc docs](http://goog-perftools.sourceforge.net/doc/tcmalloc.html) and an advanced one, though comes close to compiler stuff =[http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/](http://dmitrysoshnikov.com/compilers/writing-a-memory-allocator/)
&gt; rube goldberg dependency tree Man this truly is the perfect name for it
I've experimented a bit with the \`async\` and \`await\` syntax in nightly. Is it supposed to replace \`and\_then\` chaining? I cannot figure out how to combine the two types of syntax, sometimes the \`and\_then\` type syntax is clearer even though the same is achievable with \`async/await\`.
What gRPC features do you use? I can give you an opinion on what I think about them, as tower-grpc isn't feature complete yet (in terms of what other gRPC libs support). prost is more idiomatic Rust, mostly because the structs that it generates as similar to the Rust structs you'd write by hand. protobuf (the Rust crate) create(s/d) a struct that has getters and setters. I can't remember what moved me to tower-grpc, but I've been using it for very long. I think I came across a streaming problem that I couldn't solve with the other library, and was able to with tower.
Well testing compile-time operations would require constant functions. And I'm pretty sure pow for u64 and f64 is not constant. Interesting idea though. I wonder if there is a constant functions which has a runtime of O(n\^2)...
`MaybeUninit` is for performance-critical applications, where even the small overhead of checking the `Option` is too much.
That's true. But there's [some tooling](https://docs.microsoft.com/en-us/visualstudio/code-quality/ca2000-dispose-objects-before-losing-scope?view=vs-2019) for it, and well-established patterns (`IDisposable`), which to me makes C# better at this than other (non-C++/Rust) languages.
That doesn't work in lots of cases, e.g. if you write web api and your controller returns a Stream. Of you just have a method that accepts Stream (or any other disposable resource). You will get false positives in these cases because you probably shouldn't dispose these objects.
Just dump a file with tons of constant functions in the fuzzer and see... ;-)
That's quite nifty, TIL `fexecve` and `execveat` :-).
&gt; Rule CA2000 does not fire for local objects of the following types even if the object is not disposed: `System.IO.Stream` [...] I think you meant false negatives. Indeed, it's not precise like RAII. But it's so much better than e.g. pre-Java 7.
It's just a hack. `Stream` is not better than any other `IDisposable` so having a special case for it is just a monkey patch for frequently used types.
The Rust implementation probably has a lot of algorithms with accidentally high complexity, e.g., how does compilation time scale with: number of types, functions, modules, traits, if chains in a function, match arms in a match statement, trait implementations, trait impl specializations, etc. Essentially trying to make compilation times explode by varying the input. There are a lot of tools to "hammer" C++ compilers, by using, e.g., absurdly large number of template specializations, variadic arguments, overloads, etc. I wish there were tools like this for Rust.
Obviously depends on the benchmark, but Crystal is pretty consistently faster https://github.com/kostya/benchmarks
Fair. However it seems like unions in general [might be more complicated than we had hoped](https://github.com/rust-lang/unsafe-code-guidelines/issues/156). :/
It is often much, much easier to exploit a certain kind of UB for an optimization than to have an analysis that warns about its presence. UB is exploited by out-right *assuming it does not happen*, so the question of "does this code have UB or not" just never comes up. Answering that question is a hard one!
Wouldn't that mean that you can't use `cargo` for building your Rust projects either?
I'd like to use language safety features whenever possible. Also, in this particular case I want to find both "unused conditions" (that did not match) and "unused rules" (that did not cause any changes, even if matched). So I'll need two levels of indices for that, and with that the code becomes complicated pretty fast.
It might be possible to define everything in a way that *this particular code* is fine. But already small variants of this code, like ``` fn id(b: bool) { b } // we tell LLVM that this is 0 or 1! let b = id(mem::uninitialized()); ``` violate what we tell our optimizer. Maybe there is a really complicated definition of "use" that can handle that, but I have not seen one and it would be so complicated that I am not sure we would want it for out notion of UB. One reason why "unused" data can cause UB is compilers introducing "extra uses", which is an important optimization around loops. Consider: ``` fn foo(x: usize, b: bool) { for i in 0..x { // Assume this is loop-independent code: `i` does not get used. let val = if b { /* */ } else { /* */ }; // Go on using `val`. } } ``` If you call `foo(0, transmute(3))`, your "bad" boolean will never get "used". However, since `val` does not depend on `i`, the compiler will try to move the computation of `val` out of the loop -- but suddenly `b` gets used! To permit moving code out of the loop even if we do not know for sure that the loop gets executed at least once, it is extremely helpful to assume that even "unused" data is "valid".
Yeah everything gets inlined in this example. And indeed you have to write it just right for the optimization to trigger. But if you have a sufficiently large codebase, somewhere in there the stars *will* align just right (likely only after a completely innocent change in a totally different file, but everything gets inlined together), and then you'll wonder what the heck happened. &gt; On the other hand, if x is an array, it doesn't panic. That just means you were lucky and the compiler did not exploit the UB in that code. Switching compiler versions or changing literally anything anywhere about your program can change that to panic again, or do whatever.
&gt; I don't understand why the bool should be any harder than the u8. Good observation! It is not any harder. However, it is *really hard* to explain why `u8` is a problem, so we always use `bool`. Now that I can point people to this blog post, it's slightly less hard, but the `bool` case is still more obvious. &gt; Why not loosen the invariant to "an initialized bool must be 0x00 or 0x01"? What is an "initialized" bool? The compiler considers `let b: bool = uninitialized()` initialized; after all you assigned it something.
The standard never says that observing the same indeterminate value can yield two *different* bit patterns. The statement that an indeterminate value is "some bit pattern" is either wrong (if you think of a bit pattern as a sequence of 0 or 1), or grossly misleading (if you have a notion of "uninitialized bit pattern" but never say so). I think that under every reasonable reading of the standard, [this is a miscompilation](https://godbolt.org/z/PvZGQB). There is no bit pattern for `x` such that `make_true(true)` can return `false`, and yet it does. However, my general impression is that the committee actually sees this behavior as being in accordance with the standard, from which I can only conclude that the standard is awfully unclear.
I've found [derive_more](https://crates.io/crates/derive_more) handy in the past.
I've worked in various languages without package managers. PHP before things like Composer resulted in a culture of copy-pasting snippets from the comments system attached to the PHP manual, often without scrolling down to read the comments that pointed out glaring flaws. That, or badly reinventing the wheel has been a common symptom of this problem in every ecosystem I've seen without good package management. Heck, in Python, I'd been guilty of doing it simply because *deploying* dependencies can be a hassle, while copy-pasting them into the project (or, for a simple script, into the file) simplifies deployment.
Have you looked at the main example in my post? The behavior of that program is *impossible* if uninit memory is "random data". That's the entire point of the post. Bytes are not `u8`. That's just a fact of modern-day C, C++ and Rust. (Also, randomness and non-determinism are not the same thing. Randomness implies a notion of a "random distribution" that says which value is how likely, non-determinism has nothing like that. But that is besides the point.)
I think the issue is more that there isn't enough effort to enable and encourage integration with the existing cargo install, rather than re-building its guts into other tools.
Good point! I know about https://research.swtch.com/sparse but that looks like the same thing. To make that code correct in C/C++/Rust, you would have to write `is-member` (from my link) as follows: ``` is-member(i): let x = freeze(sparse[i]) return x &lt; n &amp;&amp; dense[x] == i ``` The original code could lead to one value being used for the bounds check and another value being used for the actual access, which is no good. I actually tried for some time (a while ago) to cause miscompilations due to this, but I am not very good at stearing compilers into using exactly the optimizations I want them to use. So, the code-as-given has UB, but it *could* be fixed in a language that exposes "freeze".
It depends on what you're doing with it. The [rust-cli-boilerplate](https://github.com/ssokolow/rust-cli-boilerplate) that I use for quickly whipping up little command-line utilities has its own `apply.py`, not because it really needs it, but because `~/src/boilerplate/apply.py oneoff` is meant to replace `vim oneoff.py` followed by `&lt;Ins&gt;boiler\c` and some tabbing through snippet fields.
This is a crate allowing to use [monome](https://monome.org/) devices: controls and peripherals which are often used to do live music performances.
Defect reports are retroactively merged into previous versions of the standard.
I really just picked some random numbers. I was as surprised as you are that it worked on the first try. ;) The [C++ example](https://godbolt.org/z/PvZGQB) was harder to find. The value 120 does not matter, but removing 2 of the three extra comparisons directly in `make_true` makes the optimization go away. Don't ask me why.
I find `0..256` more symmetric. Powers-of-2 and all. I did add a clarification now as this seems to have confused several people.
What's the raspberry-pi target there ?
Got a source for that? Not refuting the claim, would just really like one more proof of apple‚Äôs abuse of power next time I get into this argument with some friends.
&gt; This particular defect report did not include any changes to the standard, since the consensus was that the standard is clear. The [defect report](http://www.open-std.org/jtc1/sc22/wg14/www/docs/dr_260.htm) explicitly suggests some standard wording changes: &gt; [#2] While an object holds an indeterminate value it is indeterminate. Successive reads from an object that is indeterminate might return different results. Storing a value in an object, other than an indeterminate value, means that the object is no longer indeterminate. To my knowledge, no change like this has been incorporated into C11, C14, or any later version of C, even though the defect report is from 2004. Are you saying to read C14 (or any C standard) I have to also dive through *all defect reports for all versions of C ever* and then use my own free judgement to determine if they still apply to the latest version? That seems plain ridiculous.
Guessing aarch64-unknown-linux-gnu/musl?
Cool. Thanks. I didn't know this. :-D
&gt; I'm asking why this isn't the semantics of uninitialized memory in the first place. See [here](https://www.reddit.com/r/rust/comments/cd522f/what_the_hardware_does_is_not_what_your_program/etrs779/), I think that is the same question.
&gt; I am not sure if it will ever gain a significant market share from Java, C#, Python and Go for web applications It may not and it doesn't have to. For most people other languages are easier to use and good enough.It may stay forever in the area of "1% of Java users switch to Rust for 1% of their projects", it doesn't make it any less useful for those who need it. &gt;with the absence of exceptions, how can I return at any point of the code and return an http error response to the caller? Is this even possible? Your only option is return a result from where you are. &gt; couldn't you abstract a lot of the complexity of Rust in a good framework? You certainly could. &gt;almost no day to day dev is able to write proper multithreaded code out of their head, it's really complicated and you just don't do it on a daily basis This is different from Rust usecase: it is not complicated here (at least a lot simpler) and many people reach for Rust because they want to do complicated things and they do them on a daily basis.
&gt; Are you saying to read C11 (or any C standard) I have to also dive through all defect reports for all versions of C ever and then use my own free judgement to determine if they still apply to the latest version? No, I'm not saying that - ISO does.
&gt; JVM still had huge issues running in containers, like not respecting cgroup memory limits What has JVM to do with that? Its an operating system feature and responsibility.
OTOH, as long as there is no `unsafe` we at least are sure there is no UB, which should make this easier than C-smith.
Its nowhere near Django ORM. It works, but... the ergonomics is terrible. You have to define your models several times. It doesn't create migrations. It doesn't support grouping. Documentation covers maybe 1% of what I do, so I had to resort to gitter almost every time I worked with it.
&gt; Their opinion is that the C standard text does not require anywhere the same bit-pattern to be observed, and therefore it is allowed for different bit-patterns to be observed; no changes necessary. Their opinion is that this is how the C standard is to be interpreted. *Of course* the default assumption is that memory holds some particular bit pattern, not some wibbly-wobbly thing that changes any time you observe it. Source: &gt;=99% of the C/C++ programmers I ever spoke with, online or offline. So I take it the committee does not just say that this is the right interpretation, they also say it is *the only possible* interpretation? The standard is supposed to be unambiguous after all. And there is clear evidence that there *are* other interpretations. Ignoring that is such an arrogant attitude, I better stop here.
I worry about dependency trees a lot too. I really like the \`cargo tree\` tool for keeping an eye on my dependency trees. There is quite a tree hiding behind a crate like \`reqwest\`. But compared to npm, I think most dependencies are worth bein in their own crate. You have to find a balance between putting essential stuf into std:: (if you ask me, stuff like tokio belongs there, but YMMV) and pushing stuff from being potentially in std:: to their own crate. \`serde\` is another thing. Could well be in std:: too to have Rust more on the "batteries included" side. &amp;#x200B; On the other hand, I still know CPAN from Perl. It was common there to add module dependencies too. And the core languge was quite small with regard to modules that came with it. It makes it easier to deploy stripped down applicaiton packages. Instead of stripping modules/packages from the distribution and hoping it doesn't break, it's easier to add only required modules. &amp;#x200B; I think it's more about the diligence of developers. I for my part reviewed the CPAN modules I added to my Perl projects. Seeing if the code is good, if the dependency tree checks out and if it solves the problem I have well enough. npm might have the problem that lots of inexperienced developers spam the system with incomplete and useless packages, and other developers not having a feeling for the cost of third party depenencies. &amp;#x200B; With C++ I carefully look at the libraries that I use, mostly because building and including them into my CMake projects on Windows is hard enough to look twice if you really need it.
&gt; Maybe there is a really complicated definition of "use" that can handle that You know, after reading each and every one of your blog posts, I now have a healthy appreciation for how much of the foundations underpinning computing that I once thought to be rigorous, are actually hardly more than a philosophical quagmire of unclear definitions and "definitely, maybe"s. Thanks, I guess?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rustjerk] [... and improving Cargo by incorporating best practices from npm.](https://www.reddit.com/r/rustjerk/comments/cdfwn5/and_improving_cargo_by_incorporating_best/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Not a crate but I would recommend [Clippy](https://github.com/rust-lang/rust-clippy), it can help you improve your code. You can install it using: rustup component add clippy and run it with: cargo clippy
Really dumb question, but‚Äîwhat is a "typed copy"? You've emphasized that three times in your comment and I'm not 100% sure what it means. Is it like a `memcpy` into a memory location that has a type, therefore expecting a valid bit pattern for that type?
`cargo` will take care of all the dependencies, you just need to choose which ones suit your project needs. https://crates.io / https://crates.rs to look them up.
&gt; So I take it the committee does not just say that this is the right interpretation, they also say it is the only possible interpretation? That's how I read their committee-speak. &gt; Of course the default assumption is that memory holds some particular bit pattern, not some wibbly-wobbly thing that changes any time you observe it. Source: &gt;=99% of the C/C++ programmers I ever spoke with, online or offline; and also the other comments here in this discussion. Then you are not speaking to compiler writers, which are often committee members, and the ones modifying the standard and writing defect reports, etc. Also, the larger saner majority in this discussion will tell you that the way to deal with this in C, is to not mess with uninitialized memory, indeterminate values, trap representations, etc. Just initialize the memory and call it a day. The minority that says "but uninitialized memory is super important", should know what the rules are. Many comments you are getting are of the form "this was informative, now I feel like I understand uninitialized memory", which to me hint that many people are using it without putting much effort in understanding it. This is understandable, since without blogpost like this, the effort required to understand it is large, partly because the C standard is, _by design_, a document that is not intended for users to read. This is probably a bad design, but that's what it is. Blog post like this help, but there are many, and they don't show up in search results, so even users that want to really understand what they are doing might end up reading different blog post arguing different models and end up confused. This is why it is important to be straight forward with what the standard precisely requires / guarantees. &gt; The standard is supposed to be unambiguous after all. The standard does have that as a goal, but nobody felt the need to make a PR making this bit clearer. They don't guarantee that you always get the same value.
I did read your post. And my point is that the expected behaviour of that snippet, namely that `always_returns_true` _should_ be upheld by the language. As far as I can tell, treating uninitialized data as an arbitrary but fixed bit pattern would do that. &gt; Bytes are not u8. That's just a fact of modern-day C, C++ and Rust. I assume this is in part what you're talking about in "Pointers Are Complicated, or: What's in a Byte?", and as far as I can tell, this is only a consequence of the way uninitialized memory is treated (I might be wrong here). Again, my point is that this treatment is bonkers, and it's reasoning for being so really seems to only be quality-of-life improvements for compiler writers and more interesting language semantics for people into formal analysis. This is why I'm wondering why the _obvious_ way of handling this isn't even being discussed.
There's a lot going on there, but after a quick glance, aho-corasick does not seem to be that far off. Have you tried, for example, [enabling the `dfa`](https://docs.rs/aho-corasick/0.7.4/aho_corasick/struct.AhoCorasickBuilder.html#method.dfa) option? It looks like your code doesn't enable that, so it's going to fall back to the NFA version, which is typically ~twice as slow. The NFA version is the default because it uses less memory and is much quicker to build.
Not the author, see his response in the thread.
Yeah, that seems to be what I'm trying to say (I got a bit confused by the notation there). From what I can tell it seems that the #1 selling point of _not_ doing it the way suggested by /u/po8 and me is quality-of-life improvements for compiler writers, which seems like an incredibly weak argument. On the other hand, I would think that this is the way most people think about uninitialized memory ("`malloc` returns memory containing garbage, or memory that was previously used by something else" type of things - nobody would expect any garbage bytes to magically change values!), and it's weird that this isn't a model that people seem to even consider! (Or maybe they have been, and it's been shown that it's actually really bad - I don't know:)
Good post, but I fear that the order of presentation is backwards and it ends to coming across to me as a bit of a lie-to-children. Leaving the discussion of the hardware, and the difference between that and the abstract machine, is left until later. But at the same time, you claim that the compiler can read the uninitialized value from any random register, which is a hardware concept. That's not actually what happens here anyway. Instead, of I'm not mistaken, the compiler just takes optimization shortcuts and does constant folding with whatever values are convenient for the comparisons, since they Can't Happen. This model of reading a different value every time really doesn't work for some things like dereferencing a null pointer.
On the amazing error handling. Does that mean most functions return `Result&lt;T, Box&lt;std::error::Error&gt;&gt;` to allow any errors to be propagated up the stack? Or is there a different approach?
I promise you, nothing will make my code look pretty
Cool crate! Small nitpick -- you're probably going to want to compress the image on this post (https://blog.paul.cx/monome.jpg), it is ~7.5MB and slows down the page load a bunch, and you might be about to get the reddit hug of death.
Ah, okay, thanks! :-)
I checked out the Aho-Corasick commit and tried to run the benchmarks, but your setup looks messed up? $ cargo bench Compiling jieba-rs v0.4.9 (/home/andrew/clones/jieba-rs) Finished release [optimized] target(s) in 4.36s Running target/release/deps/jieba_rs-ed2d79a1cf2bbc1a running 13 tests test hmm::tests::test_hmm_cut ... ignored test hmm::tests::test_viterbi ... ignored test tests::test_cut_all ... ignored test tests::test_cut_for_search ... ignored test tests::test_cut_no_hmm ... ignored test tests::test_cut_weicheng ... ignored test tests::test_cut_with_hmm ... ignored test tests::test_dag ... ignored test tests::test_init_with_default_dict ... ignored test tests::test_split_matches ... ignored test tests::test_split_matches_against_unicode_sip ... ignored test tests::test_tag ... ignored test tests::test_tokenize ... ignored test result: ok. 0 passed; 0 failed; 13 ignored; 0 measured; 0 filtered out All I wanted to do was see the impact of enabling the DFA on benchmarks.
I also share this concern, and I was thinking that some parts of the std lib would benefit from being expressed in GATs, but it would be a breaking change :/ E.g. iterators / generators, some container types, which others do you think?
That is only half the sentence and also a false dilemma. As i said, i love and use generators myself. I just know what they are generating. What i was trying to say is, teaching someone not to worry about adding dependencies, is actually opening an attack vector, which is already out in the open, not some distant fringe case.
What do you think of [Scala-like extractors](https://www.artima.com/pins1ed/extractors.html#24.2)? Wouldn't it make more sense to not only have `Box` as a privileged first-class citizen, but also allow user types to be destructured?
It's hard to say. It's possible that in some cases you want both. For example, even if you have streaming iterators, there's still a use for regular iterators, because while the former allows more implementations for the implementor, the latter is easier to use for the user (the values you get from the iterator don't borrow it). An obvious use case for GATs though would be for futures. The `Stream` variant in particular. I also think some fundamental abstractions are likely possible. I was experimenting with making some sort of "reentrant scope" abstraction that would allow you to open a scope (you'd call like `open_scope(f: impl for&lt;'a&gt; FnOnce(Scope&lt;'a&gt;) -&gt; Value&lt;'a&gt;) -&gt; StoredValue`), and then reenter that scope later like `reenter_scope(val: StoredValue, f: impl for&lt;'a&gt; FnOnce(Scope&lt;'a&gt;, &amp;'a mut Value&lt;'a&gt;)) -&gt; StoredValue`, or something like that. The idea is still very rough, and I didn't get it to work without GATs. With GATs it might still be unsound though, not sure. But an abstraction like this could be very useful in making code like [sound unchecked indexing](https://github.com/bluss/indexing) and [Shifgrethor](https://github.com/withoutboats/shifgrethor) not be limited to a single scope.
This is (still) super cool. Would you guys be up for a chat if I'm in Kyiv for the next year? :)
Did you mean the approach on this page? https://functional.works-hub.com/learn/functional-programming-jargon-in-rust-1b555 (Scroll down to "Higher Kinded Type (HKT)", the anchor link doesn't work.) pub trait HKT&lt;A, B&gt; { type URI; type Target; } // Lifted Option impl&lt;A, B&gt; HKT&lt;A, B&gt; for Option&lt;A&gt; { type URI = Self; type Target = Option&lt;B&gt;; } pub trait Functor&lt;A, B&gt;: HKT&lt;A, B&gt; { fn fmap&lt;F&gt;(self, f: F) -&gt; &lt;Self as HKT&lt;A, B&gt;&gt;::Target where F: FnOnce(A) -&gt; B; } impl&lt;A, B&gt; Functor&lt;A, B&gt; for Option&lt;A&gt; { fn fmap&lt;F&gt;(self, f: F) -&gt; Self::Target where F: FnOnce(A) -&gt; B { self.map(f) } }
small note: I am not the author of the crate
I'm surprised you didn't lose data because of mongo
You are asking if we couldn't *change* Rust (C/C++) to make uninitialized memory less weird? Indeed we could. Practically it would be hard for Rust as we'd have to change LLVM, but it would be possible. The exact costs of this in terms of lost optimizations are unknown. I'd love to have more data on this! See http://nondot.org/sabre/LLVMNotes/UndefinedValue.txt for some more motivation for why LLVM adapted a model of "unstable memory", and http://www.cs.utah.edu/~regehr/papers/undef-pldi17.pdf for more discussions of the optimizations this enables.
Which paper is this?
last I had heard the project was not moving forward - I take it some new people picked it up?
&gt; quality-of-life improvements for compiler writers, which seems like an incredibly weak argument. No I don't think that is accurate. I'd phrase it has, this enables tons of optimizations or makes them much easier to do, where "easier" here means "easier for the compiler to figure out if the optimization applies". Demanding stable memory would likely mean programs get slower, which would cause an outcry at best and people moving to a less sane compiler/language at worst. I'd really like to have more data on what exactly the performance cost would be, but I don't know of any analysis of that -- just that everybody thinks its "significant". &gt; On the other hand, I would think that this is the way most people think about uninitialized memory ("malloc returns memory containing garbage, or memory that was previously used by something else" type of things - nobody would expect any garbage bytes to magically change values!), and it's weird that this isn't a model that people seem to even consider! (Or maybe they have been, and it's been shown that it's actually really bad - I don't know:) Oh, I think everyone considers it, that's why I have to write blog posts to tell the world that this is *not* the real model. ;) But language/compiler authors went for the more complex version because programmers demand speed, speed, speed. There is a trade-off between a simpler language semantics on the one hand and faster resulting programs on the other hand. The way Rust partially escapes that trade-off is by shielding most programmers from the "weird" part of the semantics (as in, if you write safe Rust, you don't have to care about any of this). That's the best compromise that we currently know.
Wrong subreddit
I have added the term "abstract machine" in two places earlier in the post now because it was pointed out that the previous presentation as confusing, maybe that helps. &gt; But at the same time, you claim that the compiler can read the uninitialized value from any random register, which is a hardware concept. The paragraph that discusses the "why" has to talk about hardware, yes. Optimizations explain why the abstract machine is the way it is. &gt; That's not actually what happens here anyway. Instead, of I'm not mistaken, the compiler just takes optimization shortcuts and does constant folding with whatever values are convenient for the comparisons, since they Can't Happen. Constant folding is what happens for the concrete example, yes. The general reason for why "unstable memory" exists though is, to my knowledge, to be able to just use any value at run-time, including a potentially different value each time. I will try to clarify this further.
[https://github.com/fengjixuchui/FuzzingPaper/blob/master/Paper/USENIX19_GRIMOIRE.pdf](https://github.com/fengjixuchui/FuzzingPaper/blob/master/Paper/USENIX19_GRIMOIRE.pdf)
For Raspberry Pi‚Äôs running Raspian you want `arm-unknown-linux-gnueabihf`.
Cheers man will take a look at that .
Congrats on the successful project. I'd be interested in seeing a more in-depth case study. &gt; We started digging into cars and face recognition. It worked as helper for now but next year we want to control all workers by their face. Why is this necessary? Is there really that much of a bottleneck caused by people badging in?
&gt; smart pointers still don‚Äôt give you the same compile time guarantees Rust does Notably race freedom with multithreading.
Looks like they all have to be in the custom error enum?
&gt; Thanks, I guess? You are welcome! Now, do you want to help fixing that? :D
By "typed copy" I mean basically Rust's `=` operator as well as the copies occurring when passing arguments to a function and receiving the return value: they are like `memcpy`, but the copy occurs *with a type*, giving the compiler some extra power. For example, "typed copies" of a struct "forget" all the values stored in padding bytes. They all get reset to "uninitialized".
Does cross-compiling just work if I use C libraries ?
No bi-directional streaming, so basically just the Go server app that talks to the DB and streams back to the client. Thanks for explaining. I think at this moment, the best way to proceed would be to build a tower-grpc Rust client that can talk to my Go app and then decide if I want to rewrite the backend DB stuff too. I do need to read up about DB performance etc but raw SQL using the postgres driver should be decent right? I really should check out that arewedbyet site. Again, thanks.
Liquid is also adding a decent number of dependencies. (as the maintainer of Liquid)
I remember your post from last year. Gla dto see rust has been a success. I would also be interested in some more in depth posts about the problems you faced and how you handled those with Rust.
Personally, I've given up on using `cargo` as a crate. - Slows down your compilation a lot - Ties you to a specific version of Cargo.toml format Instead, I've been using - [clap-cargo](https://github.com/crate-ci/clap-cargo) for CLI flags and helpers - [escargot](https://github.com/crate-ci/escargot/) for `cargo build` - [cargo_metadata](https://github.com/oli-obk/cargo_metadata) for parsing `Cargo.toml` (disclaimer, I am the author of clap-cargo and escargot)
Wow monome devices look so cool! Maybe me in another universe would have loved to use these for music production. if it were in my budget I would probably get Grid and find some purpose for it.
I'm a bit confused. It sounds like your biggest concern is &gt; If you want to install cargo-generate to your cargo, it will download all of the following dependencies: Is it the build times? If so, there are pre-built binaries. Otherwise, I'm pretty much seeing a relatively small dependency list that pretty much makes sense: template language and supporting file parsers, network communication to get templates, UX helpers, and macro writing tools.
Your probably want r/playrust, this is for the programming language rust, not the game.
Actually that allocators 101 is definitely what I'm looking for. Thanks, comrade!
I have no problem with this being UB across function boundaries. Only for variables inside a function/block. If there is something let mut b: bool = unsafe { std::mem::uninitialized() }; b = true; As a user, I sort of expect the compiler not to do something weird behind the scenes because it should know that there is a write. Of course this isn't really much of a problem now that we have `MaybeUninit`. It is just something that I found very odd as a non compiler person.
Very cool. Any chance you'd share the source code?
`std::mem::uninitialized` is a function though. :) So the bad value has already crossed a function boundary. Also, making the function boundary special is IMO not a good idea. It means "outlining" becomes impossible (moving cold paths of a large function into their own function), which is something compilers do or at least has been proposed for them to do. It also means that inlining *loses* optimization potential because the magic function boundary goes away. &gt; It is just something that I found very odd as a non compiler person. I appreciate that. Unfortunately I do not know of a good way to avoid it -- there are always trade-offs.
Nice! I love the Monome, and it's been the inspiration for some of my Rust work as well: https://github.com/neobirth/neobirth
&gt; What is an "initialized" bool? The compiler considers let b: bool = uninitialized() initialized; after all you assigned it something. I thought the `always_returns_true` example was supposed to convince me otherwise. Also that the value is uninitialized xor it has a bit pattern, rather than viewing uninitialized as a bit pattern.
I'd strongly recommend the [Audio Technicha ATR-2100 USB](http://www.audio-technica.com/cms/wired_mics/b8dd84773f83092c/index.html) over the Blue Yeti or especially the Blue Snowball. Similar price range, *much* higher quality audio.
This is the first time I've even encountered the idea of rendering code to an image. At first I was, like, why? To hang pictures of your most beautiful code snippets on your walls? Twitter is truly an accessibility cancer.
Congratulations for the Team to handle such a big event! I am really interested in the database solution. I was using Mongo in a project where i thought it would be a good fit. Later, after some of the requirements changed (of course!) i had some big problems because i needed something similar to transactions. We could fix that in code and everything was running smoothly ‚Äì but i took the time to write a MVP with postgres and was amazed how fast it was. Even without the changed requirements ‚Äì just the plain start idea where i though Mongo would be a good fit ‚Äì i underestimated how fast postgres really is. On that note, did you made some comparisons/benchmarks for some of your use cases where you feared that a not-NoSQL database is to slow?
For example if you have an array of a thousand complex structs and you don't want to intialize them all to `None` just to set them to `Some(_)` eventually. It'd be better to use MaybeUninit to set the elements during a loop. You'd also remove the overhead of unwrapping the `Option`s after the fact as well if I understand correctly.
You may be interested in [this article](https://hoverbear.org/2016/10/12/rust-state-machine-pattern/) laying out some ways to do precisely that.
I have a fairly simple api and web server running on mine. The compiled code performs really well. Compiling the code takes a very long time though. Probably 30-45 minutes for a full release build from scratch. I'd just cross-compile and copy but I like being able to tweak it on my phone. Those little tweaks usually take 30-60 seconds to compile. It's definitely worth trying.
All privacy issues aside, face recognition is a great technology. Physical interactions have a relative low throughput and aren't convenient for the end-user.
Yea, it's a good article but I don't _think_ it illustrates the problem I have. Specifically, that I am trying to borrow and state transition at the same time. If I just had a single state per read type this wouldn't be a problem, and the method I outlined would work fine. However I need a transition at the same time as a reader is requested, to ensure only a single reader can ever be created. Hmm.
For CSV files, use xsv. https://github.com/tensorflow/rust
It's specifics of our festival. We have almost 800 food courts points, which are not related to our team, they "buy" place. Each of point has 3-15 workers. And often people use this to have additional "tickets" to sell them. Also security workers are just private company with more than 1000 people. We don't know them and some of them often sell rfid bracelet after first day. So staff entrance is the first place where we should take control. Managers send us photo of each worker and we compare face on entrance.
We also take photos of all visitors when they exchange their ticket to rfid. It's written in our rules when you buy ticket. We use this data in help center, for example, when someone steal bracelet or buy it from 3rd persons.
The most important part of our system is distributed permissions. We have very complex and powerful transactions management. Maybe I will write some articles in future about this.
New mongodb has transactions ;) Sure we did benchmarks, no-sql is winner in all places. Also our system if built from start with nosql in mind. So it's very fast. Almost all requests are processed in 10ms.
It's funny, but mongodb now is a very solid database. Also new version has transactions. I don't see any reason not to "believe" to mongodb. We did a lot of stress tests.
Sorry, unfortunately, no :(. It's developed for 3 years and is closed product of our company.
This response seems as though you didn‚Äôt read the other reply I added, backing everything up. I‚Äôm glad I took the time!
Not really. The (former) assumption on the JVM side, that it has full access to all resources of a machine is really backwards. It only makes sense assuming one application server running on one server. And yes the OS enforces its rules for process groups, so the JVM will be killed on initial heap allocation.
They've made some good progress recently and now have stronger guarantees. I still personally wouldn't use them though.
OP, do you mind giving a short explanation on how this post is relevant to Rust? As far as I can tell there are no direct references to it anywhere in this post.
Well, it's a problem when Java tries to use 1/4th of all available memory on a Kubernetes node and immediately gets OOM killed...
AFAICT, there's no relevance. Ctrl-F "Rust" yields nothing. :/
The only reason I'd say to not use tower-grpc is because of this issue https://github.com/tower-rs/tower-grpc/issues/2 But I don't know if the other rust libraries can do it, in which case if you need that kind of functionality it would be best to use Go or another implementation in another language.
Hi, author here. This is now fixed, thanks for taking the time to mention it! &amp;#x200B; For the record this didn't even move the needle on my monitoring dashboard, but I could see that it could be annoying for load times.
A few weeks ago this article was posted: http://cliffle.com/blog/rust-typestate/ I‚Äôm still trying to wrap my head around it but it seems like it intersects with what you‚Äôre trying to do.
This crate is great for demonstrating what rust does well. I've never seen such an easy way of creating high-quality CLIs in another language.
Unfortunately, you are normally not allowed to return both an owned value and a reference to it from the same function, as moving this value would invalidate this reference. I played around with the example and came up with [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=8ed1d66a16ca262da68b2e2d1a47e223). While this does enforce read order, it doesn't statically prevent another first-stage reader to be produced. This might be something that needs to be deferred to the runtime. Hope that helps.
r/lisp might like this. A frequent complaint I have been reading is how most advertised lisps are so basic, that people have a hard time considering them useful or even lisps at all.
Perhaps we should phrase it as "obtaining a value", not "using a value", just like how "obtaining a value of type `enum Void {}`" is immediately UB?
Nothing is displayed until scripts are enabled.
I was wondering if you‚Äôd end up here :p I‚Äôm glad you agree; I was worried it would seem as though I was being critical of the authors rather than showing the differences in throughput. It‚Äôs hard to get a good tone on Reddit sometimes! FWIW I think it makes sense that you would tweak things. As soon as any benchmark does so, the rest will be forced to follow. Otherwise people will just constantly use benchmarks to undermine your work (e.g. some of the comments in this post). I actually compared all of the benchmarks a while ago, when Gotham came in around 60-70% of the speed on Hyper/Thruster/Actix. It was helpful to see the tweaks made documented so well! It makes me wonder how much the little things affect it; like checking HTTP methods and so on. Actix benchmarks change the configured allocator as well, so I‚Äôm curious how those changes impact things too. I like benchmarks as much as anyone, but it‚Äôs always a little amusing when you think about how many people insist that 1.3M req/s is a must have when compared to 800K req/s, as if they‚Äôre going to have that traffic requirement (on a single machine!).
Yes, there‚Äôs currently a team of 3 moving it forward as best we can (and the community are chipping in frequently too, which is great). We haven‚Äôt really thought too long term yet, so feel free to join in!
üëçThat‚Äôs good to know!
That's perfectly understandable, but sad nonetheless. From your descriptions it sounds like great reference for someone learning Rust.
I have totally missed the DFA option even I skimmed through the doc for a few times to make sure I used it correctly. Thanks for pointing it out. :-) And it looks like it did help a lot on performance. I have updated the [gist](https://gist.github.com/MnO2/85b8c7871edfe530a0fed34ec006d5e1) to include the metrics after DFA enabling. Also pasting here. ``` Running target/release/deps/jieba_benchmark-b1dbba6c9adea591 Gnuplot not found, disabling plotting jieba cut/no hmm time: [8.0768 us 8.4702 us 9.0067 us] thrpt: [12.706 MiB/s 13.511 MiB/s 14.169 MiB/s] change: time: [-5.7439% +0.4358% +7.1091%] (p = 0.91 &gt; 0.05) thrpt: [-6.6372% -0.4339% +6.0939%] No change in performance detected. Found 12 outliers among 100 measurements (12.00%) 7 (7.00%) high mild 5 (5.00%) high severe jieba cut/with hmm time: [10.260 us 10.324 us 10.424 us] thrpt: [10.979 MiB/s 11.085 MiB/s 11.154 MiB/s] change: time: [-9.6201% -8.7676% -7.9562%] (p = 0.00 &lt; 0.05) thrpt: [+8.6439% +9.6102% +10.644%] Performance has improved. Found 9 outliers among 100 measurements (9.00%) 3 (3.00%) high mild 6 (6.00%) high severe jieba cut/cut_all time: [4.1104 us 4.1196 us 4.1305 us] thrpt: [27.706 MiB/s 27.779 MiB/s 27.842 MiB/s] change: time: [-9.2606% -7.7680% -6.2018%] (p = 0.00 &lt; 0.05) thrpt: [+6.6119% +8.4223% +10.206%] Performance has improved. Found 9 outliers among 100 measurements (9.00%) 2 (2.00%) low mild 3 (3.00%) high mild 4 (4.00%) high severe jieba cut/cut_for_search time: [11.844 us 11.958 us 12.126 us] thrpt: [9.4373 MiB/s 9.5705 MiB/s 9.6621 MiB/s] change: time: [-12.098% -9.8210% -7.4515%] (p = 0.00 &lt; 0.05) thrpt: [+8.0514% +10.891% +13.763%] Performance has improved. Found 9 outliers among 100 measurements (9.00%) 7 (7.00%) high mild 2 (2.00%) high severe ``` It is about 9-10% of improvement, which is a lot in my experience since although the part that relying on the multi-patterns string search is critical on performance, but it is not as critical as the change I made like by changing the other in-memory data structure to be more cache friendly. To run benchmark you have to specify it with `cargo bench --all-features` as specified in the [README](https://github.com/MnO2/jieba-rs/blob/master/README.md). I agree it is unintuitive but since the library is shipped with different parts guarded by `required-features = ["tfidf", "textrank"]`, so before myself and another author figuring about this best way to do it, it is the way we stick with.
Cool project! Thank you for sharing specific details on it. I found the comment thread here an insightful and interesting read as well.
Any reason you aren't using Result&lt;T, dyn Error&gt;?
Is TabNine still actively being developed? Do you find that it slows auto complete suggestions?
Doesn't C-smith rely on comparing the output of two compilers? I suppose you could try to compare the output of Debug vs Release.
Oh, I feel silly for not thinking about unsized types, and I see now that's the whole point of the `_val` variations. Thanks for the clear explanation!
The deadline has been extended! Get your proposals in before the end of July 17th (MST)!
Thanks. Yeah, it looks like you aren't really bottlenecked on search time. Aho-Corasick's DFA version should generally run at a few hundred MB per second. If I get some time I'll dig into your code a bit more to see if I can understand it better.
**TL;DR** The paper illustrates a technique to convert destructors of arbitrary data-structures so that they run in constant stack and heap space. It sounds nice, on paper; I am afraid I haven't understood how it was supposed to work, unfortunately.
Turns out you were right. Didn't realize cargo compiles into a separate folder.
I'm sure it passed the stress tests with flying colours. Its more the question of did it arrive to the disk "eventually".
Maybe, but MySQL and Postgres have JSON support and JSONB as well. And since I am an expert database performance engineer, I know how to tune it.
That makes sense, that way you can output an error message saying precisely where the error occurred. Like I said in another comment, I am pretty new to Rust in general and Nom in particular. Thanks for taking the time to explain!
Could you be more specific? I'm wondering what exact composability issues you refer to.
I'd be very interested to see how you handle permissions with actix-web. Could you give a brief description?
I‚Äôd probably reach for cockroachdb or TiDB, myself. But I‚Äôm also not nearly as experienced tuning MySQL or Postgres.
Here is a nice one-paragraph [summary](https://www.cs.rit.edu/~ark/lectures/gc/05_02_00.html) of pointer reversal. The idea is to temporarily use the pointers in the object you are deconstructing to hold the stack: they are "extra" space at this point. This is especially valid here, where you're not trying to GC but just to free stuff.
How does maxrect fair against Skyline?
Super cool! I would be really interested in also hearing about any issues you experienced‚Äïwere there things that were hard or inconvenient to do, was there something that was a source of bugs, etc. I think that would be valuable feedback from someone who's built and deployed such a full-featured production system written in Rust. Again, really inspirational to see this! Congrats!
Huh, didn't know this editor existed. Anyone have experience &amp; opinions with it? If it's deeply plug-able in rust _(eg I could write code to change how a modal selection of text would work)_ I'd be really interested in it!
Is it possible to use transactions with rust mongo driver? From what I can tell avocado is using [mongo-rust-driver-prototype](https://github.com/mongodb-labs/mongo-rust-driver-prototype/commits/master), and that one looks like it didn't get much updates in the last year. Readme specifies that it should not be used for versions other then 3.0.x/3.2.x. I have nothing against mongo, but for rust, mongo support seems lacking. I've heard a few complaints on this subreddit about performance of this driver, especially comparing to the C++ one, but also comparing it to the java/go drivers. You didn't have any problems with it?
Glad to see that more and more people switching to Rust. Hope I can go to festival next year. So, last year you considered to switch to Postgres, why don't you? Also I'm interested, is it monolith at one VM or you used some kind of orchestration?
Is this ideomatic rust? type MoneyAmount = i32; #[derive(Debug)] struct TaxBracket { income_cap: MoneyAmount, marginal_tax_rate: f32, } impl TaxBracket { fn get_small() -&gt; Vec&lt;TaxBracket&gt; { vec![ (10_000, 0.00), (30_000, 0.10), (100_000, 0.25), (std::i32::MAX, 0.40), ] .iter() .map(|(income_cap, marginal_tax_rate)| TaxBracket { income_cap: income_cap.clone(), marginal_tax_rate: marginal_tax_rate.clone(), }) .collect() } } The idea here is that writing a bunch of tuples looks neater than doing fn get() -&gt; Vec&lt;TaxBracket&gt; { vec![ TaxBracket { income_cap: 10_000, marginal_tax_rate: 0.00, }, TaxBracket { income_cap: 30_000, marginal_tax_rate: 0.10, }, TaxBracket { income_cap: 100_000, marginal_tax_rate: 0.25, }, TaxBracket { income_cap: std::i32::MAX, marginal_tax_rate: 0.40, }, ] } ...but both look truly horrible. And the first one is slower too. Is there a way to do something like the first thing without doing it during runtime?
You're in the wrong sub. You're looking for /r/playrust
It's really cool to see another systems language grow from rust's popularity! From my brief forays with D however, I'm unconvinced that it'll ever be a popular choice for a lot of people.
I've just got an email about update: [https://tabnine.com/blog/deep](https://tabnine.com/blog/deep) They added deep learning. No , it's now slow for me
I'd say the most idiomatic way would be to use a `new` associated function. impl TaxBracket { fn new(income_cap: MoneyAmount, marginal_tax_rate: f32) -&gt; Self { TaxBracket { income_cap, marginal_tax_rate, } } fn get() -&gt; Vec&lt;TaxBracket&gt; { vec![ TaxBracket::new(10_000, 0.0), TaxBracket::new(30_000, 0.10), TaxBracket::new(100_000, 0.25), TaxBracket::new(std::i32::MAX, 0.40), ] } } If you still want to use tuples, I'd go with: use std::convert::From; impl From&lt;(MoneyAmount, f32)&gt; for TaxBracket { fn from((income_cap, marginal_tax_rate): (MoneyAmount, f32)) -&gt; Self { TaxBracket { income_cap, marginal_tax_rate, } } } impl TaxBracket { fn get() -&gt; Vec&lt;TaxBracket&gt; { vec![ (10_000, 0.0).into(), (30_000, 0.10).into(), (100_000, 0.25).into(), (std::i32::MAX, 0.40).into(), ] } }
I still want D's metaprogramming in Rust. :D
You could define a macro: macro_rules! tax_brackets { ( $( ($ic:expr, $mtr:expr) ),* $(,)? ) =&gt; { vec![$( TaxBracket { income_cap: $ic, marginal_tax_rate: $mtr, } ),+] }; } // ... fn get() -&gt; Vec&lt;TaxBracket&gt; { tax_brackets![ (10_000, 0.00), (30_000, 0.10), (100_000, 0.25), (std::i32::MAX, 0.40), ] } I doubt the first one would actually be any slower, though, at least when compiling for `release`. Unless I had multiple methods where I needed to have `Vec`s of tax brackets (where I'd use the macro,) I'd use the first one, but use an array instead of a `Vec`, and have a `new` method for `TaxBracket` that took the `income_cap` and `marginal_tax_rate` and use that in the `map`: fn get() -&gt; Vec&lt;TaxBracket&gt; { [ (10_000, 0.00), (30_000, 0.10), (100_000, 0.25), (std::i32::MAX, 0.40), ] .iter() .map(|(ic, mtr)| TaxBracket::new(ic, mtr)) .collect() }
Fair enough, mixins were super intuitive
If you need multiple flags, either an enum (if there are three cases) or a struct of bools (if they're orthogonal) might work?
It just seems like so much boilerplate, but I guess that's rust. I skipped the `new` function since it seems a bit overkill for really simple structs.
I see... I also really appreciate the comment: &gt; Clever, but tricky Indeed :p
&gt; The only costs you pay are the memory costs of one `Cell&lt;usize&gt;` for preventing double frees, two empty `RefCell&lt;HashMap&lt;NonNull&lt;T&gt;, usize&gt;&gt;` for tracking adoptions, and an if statement to check if these structures are empty on drop. That's a pretty significant cost, memory-wise. A quick playground [check](https://play.rust-lang.org/?version=stable&amp;mode=release&amp;edition=2018&amp;gist=24c16c801a4110078a2efc8f5cdf8a2c): &gt; Cell(usize): 8 &gt; RefCell(HashMap(NonNull&lt;T&gt;, usize)): 64 So we are talking about a per `Rc` overhead of 136 bytes (8 + 2 x 64). I believe the cost could be substantially reduced: 1. Possibly investigate a simpler `HashBuilder` for the `HashMap`, with a zero-size foot-print. 2. Consolidate all extra state in a single `RefCell`. 3. Allocate said `RefCell` in a `Box`. It's a bit counter-intuitive, but that'll reduce the overhead to 8 bytes when not using `adopt`, which is as close to free as possible. Another possibility is to switch to thread-local storage for the links. `Rc` is not `Send`, so cannot migrate threads, therefore using thread-local storage works smoothly: - Advantage: it reduces the node overhead to just `Cell&lt;usize&gt;` with little algorithmic changes. - Advantage: it completely removes any extra allocation, assuming the TLS hash maps have reached a steady-state, instead of allocating anew each time a `Rc` adopts for the first time. - Issue: it may becomes more difficult to identify cycles when there are multiple `Rc` graphs sharing the same link maps.
Hi! I'm the author of the crate. According to the benchmarks provided in [this repo](https://github.com/juj/RectangleBinPack) and the paper container therein (which I used as a reference), MAXRECTS generally outperforms Skyline when it comes to offline packing - that is, packing all of your sprites at once and knowing all of their dimensions beforehand. Skyline can be faster than MAXRECTS in online packing, which is why I want to eventually implement it - it might provide better performance when the lib is implemented in the Amethyst asset pipeline. When it comes to pure packing though, MAXRECTS strictly outperforms Skyline IIRC.
Walter Bright is doing an AMA on r/programming. Apparently, he is aiming for the full-blown Borrowing experience, with lifetime annotations on arguments and return types. Next target: C++ :)
hopefully this will kill rust for good (and, as a side effect, D since nobody cares about it) and we can go back to focusing our efforts on C++
This isn't a Glade issue, but rather you're missing the GTK3 libraries. You can install them [from the project website](https://www.gtk.org/download/windows.php).
Are you planning to expand to other festivals? This seems like we're powerful software, how does it compare to what other festivals are currently using?
You can surely focus on C++ but I think this is a misdirected comment.
Thanks for the info; sounds great for an offline tool! Piston has a Skyline implementation for their packer that might serve as a good reference.
nah it's pretty apt. in fact iirc someone on the rust team once stated that the entire point of rust was to be obsoleted by some other language that pilfered its ideas. what's the point of sinking time and effort into a language that has a self-destruct mechanism?
I don't know of any, but would be excited to see one come along!
This is a project I've been working on for about 6 months now for applications on my team at Microsoft. Initially this started out as a side project, but I decided to work on it at my job to use in a fuzzer I was writing for a component of Hyper-V. One of the frameworks we use internally for writing structure-aware fuzzers required a lot of code to be written by hand which is both slow and can be error-prone. The idea for lain was to make something where I would only have to define my data structures and basics limits of their fields, then let the framework do the rest for me. One big downside to this framework is that since the layout and data types of the Rust structs are not the same as you'd find in the C side of things, you'd have to write your own logic for deserialization. I explain this more here: https://github.com/microsoft/lain/wiki/Deserializing-structs-over-an-FFI-boundary If anyone sees places that could be improved, by all means feel free to file an issue.
It would be interesting to know how difficult it would be to implement this in a C++ compiler.
I suspect that the assumption is closer to "The pointers on stack are an exhaustive list of root live pointers and every other live pointer is transitively pointed to by one of them" and the requirements can be satisfied by keeping a copy of every live root pointer on the stack.
Good to know, I'll keep it in mind =)
"..we had to scramble to figure out how to spread the load. So, what we did was turn *part* of the load Alex was carrying into two teams of people with [like] twenty people each!"
That sounds more like one developer's opinion of how the future will play out rather than a goal of the project itself.
Wow that blog was not up when I checked earlier. Perfect timing. I'm thinking about buying a license especially since it's valid forever.
It was Oracle JVM. I did the experiment about 1.5 years ago and GraalVM was too far from release at that time. Maybe one day I will benchmark each step in GraalVM but I will be forced to bench again Rust and JVM versions because the experiment was done just before revealing Meltdown and Ghost vulnerabilities.
I don‚Äôt quite remember... I think BC was giving me a hard time without the box.
Yeah. To me, it seems that Rust's taken all of C++'s good ideas, along with other modern programming ideas like async/await, ML types, Haskell typeclasses, etc., and in a way, is in the middle of obsoleting them. Just like how C killed many of the languages that came before it. And yadayada. I presume the original intent of the author of that quote is that eventually some other language will come along that's even better than Rust, which will hopefully obsolete it, because it's so much better. But that probably won't happen for quite some time.
My guess is that it would be too magical. It would just be too difficult to search library docs to see what's getting turned into what. In addition, what if A can be turned into B and C, which can both be turned into D. Which should the compiler pick? And what if one of those transformations are lossy in some way? At this point, it's much simpler to just specify each thing along each step of the way, especially since you don't often transform from one type to another like this with more than one into at a time. (This chaining reminds me a bit of Deref chaining, which people already think is too magical. And even then, what it can turn into is very straight forward; there's only one option for each type.)
Ah, understood - that makes a lot of sense. It's similar to the diamond inheritance problem.
Yeah I'm pretty sure you can't have an instance of `Result&lt;T, dyn Error&gt;`, unless I missed some very important feature announcements concerning unsized values.
Got a question for you /u/ralfj :-) So in the docs for `mem::uninitialized`, it states: &gt; The reason for deprecation is that the function **basically cannot be used correctly**: the Rust compiler assumes that values are properly initialized. And in particular, the `let foo: bool = mem::uninitialized()` example being insta-UB is something I think I understand, to a point. My question is, what are the words "basically cannot be used correctly" hiding? Do there exist instances of `mem::uninitialized()` in today's Rust that _are_ valid? If so, what are they? Or are they just limited to `let foo: u8 = mem::uninitialized()` (of which, whose status is being debated)?
Hi thanks for taking a look. I‚Äôll investigate trying some of the techniques you‚Äôve proposed. I wanted to clarify though that the memory cost is per `RcBox` and not per `Rc`. `Rc` stores a pointer to an `RcBox`. `Rc::clone` increments the strong count and copies a pointer. Since I wrote this post, I have eliminated the `Cell&lt;usize&gt;` tombstone.
I've never used Rust before but was always curious whether it provides any mechanisms that ease the process of using UNIX API's (or any other libs for that matter) by ensuring things like using only specified subset of functions inside signal handlers or doing other dangerous things for which the programmer is responsible to take care of and could easily be enforced by Rust compiler?
My first project with Rust was a terminal program that pulled a REST API and printed it in color. It taught me some common programming patterns and also how to use reqwest and more importantly serde.
When I looked for an idea to learn rust by practicing, I was installing Jellyfin only personal server. In this context I had a need for a program to help me to manually reorder and rename a whole disk of series and anime. That is what I did. So try to scratch your own itch, is feels always rewarding :D
Cool stuff! Also thanks for the shoutout ;) What do you use for the user facing aspects of it? JavaScript web UI?
I used it briefly a long time ago and it seemed like more or less a somewhat more sophisticated gedit
I will check out clap. I started playing with structopt, I love how much it seems to take care of the little things for you. Pretty intuitive.
Hi all, I wanted to learn a bit more about Rust with a small project, the output of which renders a simple gradient on screen. I'm having lots of issues with operator overloads for my matrix2d and vector3d classes, but the biggest problem I'm having so far is the inability to create a large enough static array. From what I've read online, my only option is to use a vector (std::vec), but that's a dynamic array. I don't want to pay the overhead of a dynamic array when I know the size at compile time. I also read about boxed slices, but they are also heap allocated. Is there a way to create a large static array?
That actually sounds interesting. I'll learn about those two libraries, thanks for your input!
;) Yep, web ui on vue framework. Which allowed us to run system on tablets, phones, laptops. And some "hardware" device use direct api to server.
The only real way right now is to use \`lazy\_static\`.
That's a great idea. I recently did it with Python, made a script to parse my MAL lists and give me useful output about them (e.g. sorting your entire Plan To Watch list by anime ratings). I'll look out for something of the sort, thanks!
As someone who is a big fan of Rust I must say that learning D is a lot easier than learning Rust. If you need to get an entire team of programmers on board then D might be the better choice. When it comes to idiomatic use, D follows many patterns that come from C or C++. Rust on the other hand is more heavily influenced by Caml-like languages (like Haskell or Scala) that many average developers are not familiar with. It's been a while since I last programmed in D but I enjoyed it a lot.
I've said something similar to what the parent is saying, maybe they're referring to me. &amp;#x200B; And yes, that's exactly the sentiment: time goes on. New languages come out, and they're (hopefully!) better than old ones. Rust is not the final programming language to ever exist. Stating this is mostly about keeping things humble, not about some day when we say "welp that was fun, time to delete the Rust repo".
Nice! Definitely will consider this next time I want to run serial experiments.
Interesting. May I ask whether you have not touched Linux at all or you are a vi/emacs guru who boots directly into the editor or things like this? Point is: Kate exists since 2002, i.e. for 17 years by now... :)
&gt; Next target: C++ :) Please no. C++ has enough shit bolted on to it at this point. It needs a full redesign, not more bolted on fixes that just further complicate an incredibly complicated language. Don't get me wrong, I love C++, but with every new version I'm convinced the standards body is full of really, really smart people who have no real world programming experience. Lots of cool stuff, but it's all kinda glued together.
Usually I recommend to write CLI-roguelike game for my students. It may be just a maze with doors and keys with different xolors. Then you can try to split it to client and server. Then use database to save progress between sessions. Then create a web client and modify your server to be REST. Then add mobs or multiplayer. Usually all of them have a lot of fun during that.
Too late: https://herbsutter.com/2018/09/20/lifetime-profile-v1-0-posted/ :P
The `dbg!` macro is a good replacement for `println!` https://doc.rust-lang.org/std/macro.dbg.html
It's usually a good idea to have some practice with rust at leetcode.com too.
Kate is a very slick editor that is developed by KDE. I use it alongside vim. It's had racer support for a while already.
(Preface: I skimmed this very quickly) I don't entirely dislike his approach, but I think it's not good enough. I'm glad he suggests using attributes instead of introducing new keywords/syntax as C++ is already swimming in keywords and syntax reuse. It looks like it's "optional" to implement, so no one is going to use it if it makes something that they could do without it harder to implement (safety be damned). Additionally, the standards body seems to have some sort of vendetta against attributes and refuses to make any sort of attributes standard and instead is convinced that anything important enough to standardize requires a keyword. Not to mention it would break a significant amount of existing code - sure it may be unsafe/potentially buggy, but the standards body is also hell bent on not breaking backwards compatibility. So I don't really see this going anywhere, even if it is no entirely unreasonable.
No the C libraries need to be cross compiled too. The cross images have some common ones already installed but you can also build a custom image. If the library you need is packaged for Debian then you can install the package for the arm target and avoid needing to build from source. Cross uses Docker so you will need that installed. I‚Äôve never used it on a Mac though. I would have thought it would work the same as Linux though.
Whether or not Rust ever reaches the popularity of Java or C++, I think it's likely that it will be enormously influential.
Uh, so you run a BFS for every deallocation of a potentially cyclic graph? If so, that will get very slow very quickly. Modern garbage collectors kick in only once in a while precisely so they would not have to do this every time - they defer the BFS until some further point and kill multiple orphaned cycles for the price of one BFS. Plus there are other tricks like generational GC to run BFS only on parts of the graph, and despite all that GCs still can be slow. I do not see any advantages to this approach compared to a full-blown GC.