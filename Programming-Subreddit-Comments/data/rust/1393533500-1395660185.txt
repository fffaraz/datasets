`try` is actually in ActiveSupport, not Ruby itself, but yes. What's really funny is the implementation.... https://github.com/rails/rails/blob/master/activesupport/lib/active_support/core_ext/object/try.rb#L45
I hope more are added, these are great!
I have been proposing this actually a few weeks ago: https://mail.mozilla.org/pipermail/rust-dev/2014-February/008607.html
Probably. When I was last using haskell I was terrible at functional programming. It was only with scala that I became (slightly) less terrible... But, in any case, my point is that I find code like the above to be much (much) *clearer* than having a special operator. I prefer code to be optimised for reading...
wait...i have to know rust to be here? :-(
Agreed, I prefer it too.
The simplicity of having the only kinds being `*`, `* -&gt; *`, `* -&gt; * -&gt; *`, etc, is very attractive though.
Haskell actually pattern matches the value on the left of the `&lt;-`. And also returns the "fail" of the type of g1 (which I simplified to None). Mobile reader has escaped my previous comment...
I was going to say "common Rails idiom", but I felt people would lunge at the opportunity to say Rails isn't a language (but yeah, it's used often outside of Rails too, when one just requires ActiveSupport). The implementation had me double take. Ruby sure is a neat language, but with that comes some... interesting code. 
It might be useful to look at the issues they have been experiencing. Also, he mentions Rust once in there!
Yeah, D does this very well. Seems odd to put an arbitrary restriction on the type.
The author is participating in the discussion over in /r/cpp: http://www.reddit.com/r/cpp/comments/1z1b1q/c17_i_see_a_monad_in_your_future/
Wrong subreddit (though I can see how you got confused).
You cannot do this with enums: one type cannot have two different sizes. Rust will never be able to do what you want in safe code. You'll be able to do various tricky things to achieve it, but it will always be an inherently unsafe type. Do you *really* need to save that memory, even when it causes that much pain and when it opens up such a large bug surface? Another option is owning `sub` (`~[T, ..2]`) rather than embedding it (`[T, ..2]`).
This sort of thing - a spatial datastructure - would be size/performance critical .. you'd want to apply all the tricks you can to optimize it (compact size -&gt; cache efficiency) one might end up using compressed pointers in arenas or whatever. obviously there's unsafe code for all that :) this example was illustrative: i think it could theoretically work if there were known restrictions.. i.e. the if the type could not just be mutated between variants. I also realise it might be very difficult to retrofit. Does look like a struct with optional extended data is a more reasonable safety/efficiency trade off in this example
You can actually get something interesting with DST: `struct OctreeNode(Content, [Option&lt;~OctreeNode&gt;]);` Now, you've lost the invariant that you can either have 0 or 8 elements in that array, but `OctreeNode` becomes a DST and only uses enough space to store the array elements it needs - e.g. you could create an `OctreeNode(content, [None, None, Some(sub_node)])`. **EDIT**: This makes me think - we could also allow marking an enum as a DST, with all the restrictions that come with it, but also the benefit of only allocating enough to store its data. Could also store the discriminator as the DST erased data (like the length in a slice).
 fn main() { spawn(proc() learn_rust()); spawn(proc() subscribe_to_r_rust()); spawn(main); }
Could you try something like: struct Leaf&lt;T&gt; { data: T, } type Nodes&lt;T&gt; = [[[Option&lt;~OctreeNode&lt;T&gt;&gt;,..2],..2],..2]; struct Branch&lt;T&gt; { data: T, nodes: Nodes&lt;T&gt;, } trait OctreeNode&lt;T&gt; { fn data&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a T; fn mut_data&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T; fn nodes&lt;'a&gt;(&amp;'a self) -&gt; Option&lt;&amp;'a Nodes&lt;T&gt;&gt;; fn mut_nodes&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;&amp;'a mut Nodes&lt;T&gt;&gt;; } impl&lt;T&gt; OctreeNode&lt;T&gt; for Leaf&lt;T&gt; { fn data&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a T { &amp;self.data } fn mut_data&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T { &amp;mut self.data } fn nodes&lt;'a&gt;(&amp;'a self) -&gt; Option&lt;&amp;'a Nodes&lt;T&gt;&gt; { None } fn mut_nodes&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;&amp;'a mut Nodes&lt;T&gt;&gt; { None } } impl&lt;T&gt; OctreeNode&lt;T&gt; for Branch&lt;T&gt; { fn data&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a T { &amp;self.data } fn mut_data&lt;'a&gt;(&amp;'a mut self) -&gt; &amp;'a mut T { &amp;mut self.data } fn nodes&lt;'a&gt;(&amp;'a self) -&gt; Option&lt;&amp;'a Nodes&lt;T&gt;&gt; { Some(&amp;self.nodes) } fn mut_nodes&lt;'a&gt;(&amp;'a mut self) -&gt; Option&lt;&amp;'a mut Nodes&lt;T&gt;&gt; { Some(&amp;mut self.nodes) } } Unfortunately it's abit of a round-about solution.
Writing pub mod bloom_filter { ... } and pub mod murmur { ... } etc. at the top level of the crates means that someone has to import the code with [repetition](https://github.com/brianmadden/rust-bloom-filter/blob/b46f63adca6b14eb4ec475ed1bb065b5a60d0648/bloom.rs#L16-L17). It would be better to just elide the modules, allowing `use bit_vec::BitVec; use murmur3::murmur3_32_seeded;` etc. Also, there's [actually a bit-vector in the `collections` crate](http://static.rust-lang.org/doc/master/collections/bitv/struct.Bitv.html) ([in `extra` in 0.9](http://static.rust-lang.org/doc/0.9/extra/bitv/struct.Bitv.html)).
There was already https://github.com/jedisct1/rust-bloom-filter Also, I am not sure that Murmurhash is still relevant today. If you need a fast but insecure hash function, xxhash (https://code.google.com/p/xxhash/) is likely to be faster.
I believe there's a case where you can perform an assignment at the wrong type and end up with an object that's got the base-class members from one object, and the derived-class members from a different object. That seems pretty close to a soundness issue. But alas I do not have the details handy.
Awesome! This is just the type of feedback I was hoping for, thank you! As for the bit vector in 0.9, I had seen the bit vector library in 0.8, looked for it in master and didn't see it there. I thought maybe it had been abandoned and decided writing my own would be another good exercise.
Thanks for the heads up on murmurhash. I looked in to what other libraries were using (Google's Guava bloom filter, and some others) and found them using murmur so I just followed suit. I'll look into the xxhash though, thanks!
BTW, there's already [a Rust implementation of xxHash](https://github.com/Jurily/rust-xxhash) (feel free to reimplement it though).
;P
Most common design patterns contain an ad hoc, informally-specified, bug-ridden, slow implementation of half of a monad. See: jQuery futures
Thanks for this contribution! I merged it this morning.
I'm pretty sure `int` in types would make Rust's type system dependent, which IIRC was a "don't do this" item.
It depends what operations are allowed on them. e.g. if you're only allowed to write either of fn foo&lt;N: uint&gt;(x: Foo&lt;N&gt;) fn foo(x: Foo&lt;3&gt;) (i.e. just being generic or specific, no arithmetic operations, and, in particular, no `if`s) then it seems unlikely that would be dependently typed... I could easily be wrong.
"Dependently typed" just means types can depend on terms (`int`s in this case). It's actually orthogonal to generics (terms depending on types) or type operators (types depending on types); see [the lambda cube](https://en.wikipedia.org/wiki/Lambda_cube). In this case, I think we'd get a *very* restricted version of first-order dependent types. Strictly speaking, [T,..N] is actually a dependent type, but it's not first class and you can't really do anything with it so you can't exploit its dependent-ness. I'm not opposed to adding a little dependence to the types, but it's easy for more to slip in than you expect. Proving soundness for dependent type systems is generally much harder. Throw in the part where the interaction of substructural types and dependent types is still an open question and you get a mess. 
My original comment was because I believe that the current plan for values-in-types would not allow writing fn foo(n: uint) -&gt; Foo&lt;n&gt; { ... } which is what I thought of as (an example of) the "core"/interesting part of dependent typing. (However, I'm completely inexperienced in this area, so what I'm saying is almost certainly incorrect. :) )
No problem, that's how I interpreted it :) I haven't worked through the details so I can't say whether adding just constant `int`s in types to the type system as is would work, but my thoughts were actually further down the road. I think it might be possible to use that + type operators from HKT to get type level Peano arithmetic. Once you're there you can start getting things like [this](http://stackoverflow.com/questions/17501777/implementing-a-zipper-for-length-indexed-lists). If you're really crazy you could use tricks like [Godel numbering](https://en.wikipedia.org/wiki/Godel_numbering) to encode data in the `int`s and do general type level computation (e.g. untyped lambda calculus). I think you're still guaranteed that everything happens during compile time since you can't promote runtime `int`s to types, but type checking may involve executing a nonterminating type level function (and thus never finish). I could be wrong. I only have a year or so of experience with dependent types and I don't usually deal with these maybe-a-little-dependent types.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Godel numbering**](http://en.wikipedia.org/wiki/Godel%20numbering): [](#sfw) --- &gt; &gt;In [mathematical logic](http://en.wikipedia.org/wiki/Mathematical_logic), a **Gödel numbering** is a [function](http://en.wikipedia.org/wiki/Function_(mathematics\)) that assigns to each symbol and [well-formed formula](http://en.wikipedia.org/wiki/Well-formed_formula) of some [formal language](http://en.wikipedia.org/wiki/Formal_language) a unique [natural number](http://en.wikipedia.org/wiki/Natural_number), called its **Gödel number**. The concept was famously used by [Kurt Gödel](http://en.wikipedia.org/wiki/Kurt_G%C3%B6del) for the proof of his [incompleteness theorems](http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems). (Gödel 1931) &gt;A Gödel numbering can be interpreted as an [encoding](http://en.wikipedia.org/wiki/Semantics_encoding) in which a number is assigned to each [symbol](http://en.wikipedia.org/wiki/Symbol) of a [mathematical notation](http://en.wikipedia.org/wiki/Mathematical_notation), after which a sequence of [natural numbers](http://en.wikipedia.org/wiki/Natural_number) can then represent a sequence of symbols. These sequences of natural numbers can again be represented by single natural numbers, facilitating their manipulation in formal theories of arithmetic. &gt;Since the publishing of Gödel's paper in 1931, the term "Gödel numbering" or "Gödel code" has been used to refer to more general assignments of natural numbers to mathematical objects. &gt; --- ^Interesting: [^Gödel ^numbering](http://en.wikipedia.org/wiki/G%C3%B6del_numbering) ^| [^Gödel ^numbering ^for ^sequences](http://en.wikipedia.org/wiki/G%C3%B6del_numbering_for_sequences) ^| [^Numbering ^\(computability ^theory)](http://en.wikipedia.org/wiki/Numbering_\(computability_theory\)) ^| [^Kurt ^Gödel](http://en.wikipedia.org/wiki/Kurt_G%C3%B6del) ^| [^Gödel's ^incompleteness ^theorems](http://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cfqqt9o) ^or[](#or) [^delete](http://www.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cfqqt9o)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 74004:%0Ahttp://www.reddit.com/r/rust/comments/1z3dnx/ints_in_generic_type_parameters/cfqqt4p)
A thought just came to me, **we already have the literals**. I've been using `[u8, ..N]` as a hack to achieve monomorphization of functions over integers. What we're missing is associated constants, to be able to do fn foo&lt;A: FixedVec&lt;()&gt;, B: FixedVec&lt;()&gt;, T: Default&gt;() -&gt; [T, ..(A::LEN + B::LEN)] The advantage of this method is arithmetic composability, where numeric literals would be stuck with user-provided constants. More complicated example: trait Factorial: FixedVec&lt;()&gt; {VAL: uint;} impl Factorial for [(), ..0] {VAL = 0;} impl&lt;X: Factorial&gt; Factorial for [(), X::LEN + 1] {VAL = X::VAL * (X::LEN + 1);} This might require hard (impossible?) to implement coherence rules, unless we want to include a SMT solver in rustc.
Types aren't depending on terms. We're merely defining `int`s to be legal at both the term level and the type level. Dependent types arise iff we allow type-level `int`s to depend on term-level `int`s. But we don't. It doesn't add much expressiveness over a unary encoding: struct Zero; struct Succ&lt;T&gt;; type Three = Succ&lt;Succ&lt;Succ&lt;Zero&gt;&gt;&gt;; if any at all, it's just more convenient, and perhaps more efficient, to work with. (I think it does add expressiveness when it comes to interacting with fixed-length arrays, which already *have* integer type parameters, we just can't abstract over them, and the primary motivation for this feature is that we *want* to, but this is just an artifact of prior design decisions in the Rust language, i.e. we could've chosen to encode the length of fixed-length arrays in unary as well, if we were crazy enough.) C++ has non-type template parameters (such as `int`). C++ is not not not a dependently typed language. I don't expect that Rust's system in this area, if it grows one, would be fundamentally different from C++'s.
Do you have any concrete advantages in mind that this simplicity would gain us?
I think this kind of thing might be possible *if* the allocation is statically known to be forever-immutable. I recall talk of this kind of optimization many months ago, but I don't think anything came of it. (In that case I believe it was for immutable `@` boxes, which are / will no longer be a language primitive.) This might also be an interesting optimization for values in immutable slots on the stack and/or immutable statics, though? (I'm assuming we don't already do it?)
&gt; also I took a look at the Rust compiler build process trying to add iOS support.. which I got nowhere with really A showstopper is currently https://github.com/mozilla/rust/issues/11312 For the time being https://github.com/shilgapira/ObjCrust could be a workaround.
&gt; And every PR to the main Rust codebase runs tests (and which have to pass) on Android too. See also https://mail.mozilla.org/pipermail/rust-dev/2014-January/008223.html
/r/playrust
Haha I was wondering when this mistake was going to be made =)
BTW, I have managed to get rustfind working again on the latest version of rust - see http://github.com/am0d/rust-find. I made quite a few other formatting and other minor changes as well while doing so, and it doesn't work 100% (some links are incorrect or not working), and since I didn't know if you were still working on this, I didn't send you a pull request. Can you let me know what if you would like me to send a PR for the work I've done, or stop work on it, or something else?
It happens a couple of times a day.
You must be kidding, this is a ridiculous proposal, although the point that unsafe types are "wrong by default" is a good point. However, doing this will make ALL OTHER TYPES wrong by default, since a type that fails to declare Freeze or Send but should is wrong. And it will result in a ton of repetitive boilerplate everywhere. Instead, reject this, and make Eq, Clone etc. also automatically implemented where possible. 
As someone in the mentioned set of those who haven't yet fully grokked when and how some of these magic traits kick in, this sounds really great.
&gt; However, doing this will make ALL OTHER TYPES wrong by default, since a type that fails to declare Freeze or Send but should is wrong. But it'd be wrong in a safe way: it doesn't allow badness to happen. &gt; Instead, reject this, and make Eq, Clone etc. also automatically implemented where possible. If you implement Clone by default then we'll have to have something like Google's `DISALLOW_EVIL_CONSTRUCTORS`. I think that was a mistake in C++.
Also, one could perhaps just add an "unsafe struct" syntax that disables all built-in kinds and derivings by default. 
&gt;I think you're right. You can't have the type of the l-value be anything other than compatible with all the subtypes in the chain of "?." expressions, in order for this to work. I interpreted it differently. The link says &gt; The “?.” operator is basically saying, if the object to the left is not null, then fetch what is to the right, otherwise return null and halt the access chain. So I think myFoo?.myBar?.myBaz will be either Baz or null; the type of myBar shouldn't affect this.
Ah, I see. So it won't return the intermediate non-null terms? That makes a lot more sense. 
I think it would work if types would have to be marked explicitly as unsafe.
&gt; However, doing this will make ALL OTHER TYPES wrong by default, since a type that fails to declare Freeze or Send but should is wrong. I think by "should" you really mean "could", and I do not agree. There are good reasons not to permit all operations that could possibly be legal. This is was the point of the sections about forward compatibility -- just because a type is freezable or sendable now, doesn't mean you plan for it to remain so. And just because a type is plain-old-data doesn't mean its instances should be copied freely. I don't think the situation is quite as black-and-white as you make it out. (Similarly, at least in my experience, only very few types really need to be compared with `==`, and so I am happy that types do not implement `Eq` by default.)
In general, to implement a trait in your crate, you have to have defined either the trait itself or the struct you're implementing it for (or both) in that crate. This is to prevent a sort of diamond-inheritance implementation conflict from different crates (i.e., if some struct Foo from crate A has impls for trait Bar in both crate B and C, and you try to use both crates from crate D, which impl of Bar for Foo should win? This avoids the problem). The Eq and Ord traits weren't defined in your crate, so you can only implement them for structs you defined in your crate.
I think this is a good and sensible idea. Not implementing it would be a clear (albeit minor) mistake for the sake of reducing boilerplate code somewhat. 
If we had `unsafe` fields this wouldn't be an issue. If the fields aren't marked `unsafe`, then safe code can already make a copy as deriving would.
Orphan instances lead to libraries that literally cannot be used with each other. This coherence was deemed to be more important than the few use cases for them.
&gt; I gather Go is implemented in C, which itself is stable, has that given them a more stable environment for writing supporting tools ? Well, they're moving it all to Go, now. I certainly do not want to be writing Rust tooling in C. Rust is a good language, even if it does at present change a fair bit. Using another language—*any* other language—for such things would be a step backwards.
Well, if you have a type that contains a bunch of public data fields, then that type should be: - Eq so you can create a set of it - Send so you can send it - Clone so you can copy it - Freeze - serializable - convertible to string Requiring all such types (which are theoretically frequent) to have 5 or more annotations is simply absurd. Now it's true that not requiring them might cause to provide more guarantees than intended, but that's probably not as bad. A possible middle ground is to always require some sort of annotation, but provide a way to implement everything with simple syntax. Or maybe types with all public fields could have it by default. But certainly having no traits implemented by default is a disaster, and will lead to endless frustration both by people who write structs who have to write the boilerplace and also users that will face library types with missing traits, and have to either fix the library or duplicate the library code. Imagine writing some software and then you suddenly realize that you can't send some data to another task because the library author forgot to declare Send, and you are now forced to either ship a fixed version of the library in your repository or copy&amp;paste the struct declaration from the library and write code to copy fields back and forth. You'd definitely get the urge to strangle the language designers in that situation. 
&gt; A possible middle ground is to always require some sort of annotation, but provide a way to implement everything with simple syntax. /u/nikomatsakis suggested this as `#[deriving(Data)]` in the OP.
Seconded. The above post would've been much better without the aggressive introduction.
I should still clarify what I meant—I do not mean that we should be a Rust-only ecosystem; interop with other languages is important. But any primary tooling and such is much better done in Rust than in another language. Rather than writing libraries in C++, I would prefer to write in Rust with a minimal API exposed with C++ bindings.
&gt; Well, if you have a type that contains a bunch of public data fields, then that type should be: I'd argue that it's far more important for the opposite: if a type has private fields then the creator needs to define all the traits that possibly make sense, since it's impossible to do outside the module where the struct is defined, even with a wrapper type. At least with public fields one can write a wrapper type that directly accesses those fields to implement the traits in a different crate.
Thanks. This is quite thought provoking. The premise alone (that people write abstractions to avoid match) is something I had not considered.
For the PR example given, I think a once-off macro is clearer; macro_rules! match_or_bug { ($m: expr, $x: pat =&gt; $e: expr, $msg: expr) =&gt; { match $m { $x =&gt; $e, _ =&gt; ecx.diag.handler().bug(format!("Expected {}", $msg)) } } } let path_id = match_or_bug!(t.node, ast::TyPath(_, _, i) =&gt; i, "TyPath"); let def_map = tcx.def_map.borrow(); let def_id = match_or_bug!(def_map.get().find(&amp;path_id), Some(&amp;DefStruct(i)) =&gt; i, "DefStruct"); let item = match_or_bug!(tcx.map.find(def_id.node), Some(ast_map::NodeItem(i)) =&gt; i, "NodeItem"); match_or_bug!(i.node, ast::ItemStruct(struct_def, _) =&gt; cur_struct = struct_def, "ItemStruct"); Of course this does require custom logic for each time you wish to use it, and isn't quite as nice as the example given.
All Rust lints already can be silenced with allow attributes.
The key feature of the `match!` block in this proposal is that the alternative branches aren't alternative bindings for the let statement--they return out from the whole `match!` block. Thus the type of the alternate branches has nothing to do with the binding vars. Instead, they have to match the type of the overall match block, which is simply the type of the last expression in the block (like any other block.) To illustrate: let b:Option&lt;int&gt; = whatever(); let what_i_got = match! { let Some(i) match b else _ =&gt; ~"I got nuthin"; // whatever is here has to be ~str, not int format!("I got {}", i) } Yes, that's weird. But it solves the nesting problem--the line after the let-match can be sure that the main pattern actually matched. The whole problem I'm trying to solve is that we shouldn't need to open another block just to get that. If I'm understanding your comment correctly, what you're describing with `let` statements is different. With yours, it'd be something like `let Some(i) = b else 0`, where the alternative branches have to match the type of the bound variable on the main pattern. And then the next statement would execute, whether or not the `Some(i)` pattern matched, with `i` set to `0` if `b` was `None`. But suppose you didn't have a suitable marker value--suppose 0 was a perfectly valid value for `b`, and so was every other possible integer? You could return out in the alternate branch - `let Some(i) = b else return WHATEVER`--or `break` or `continue` if you're in a loop. But you couldn't provide an alternate return value for the block, but not the whole function... ...but what if you could? It now occurs to me that this could be two different features, each of which is useful on their own. One is the refutable `let` with `else` as you described it, and the other is the quasi-return that jumps out of a block, but not the whole function. Suppose we had this: let result = LABEL: { let a = onething(); if somethingbad(a) { return "oops" for LABEL; // Or some other syntax...think of something better later } do_more_stuff(); "done" } // result is now "oops" if somethingbad(a) returned true, otherwise it's "done". do_more_stuff() only executed in the latter case. This, combined with the simpler let-else the way you described it, would fully reconstruct my let-match feature: let b:Option&lt;int&gt; = whatever(); let what_i_got = LABEL: { let Some(i) = b else return ~"I got nuthin" for LABEL; format!("I got {}", i) } I think that's too much of an alteration of basic syntax to be doable as an external syntax extension. It's certainly an interesting thought.
I would prefer this approach. I certainly struggle with not being able to tell the compiler explicitely what I want and only later find out that I accidently implemented a type that is not fit for use. Especially when starting out, learning the rules about which types gain which traits was a bit of a hassle. It moves all those errors away from the spot where types are used to the declaration. This also means that I don't have to write a test wheter a type implements Send if I just want to export it and never actually send it in a library. Declare and you have it.
...in Haskell you rarely see actual pattern matches, nor explicit recursion, because it's all been factored out into HOFs. One of the things that felt strange to me in Rust code was precisely that people seem to do this less: bare `match`es are a lot more prevalent, which are syntactically heavy in the way the OP describes. I think part of the "problem" here is that in Rust it's harder to do that, and the results are uglier. One reason is the lack of currying and partial application, another is our noisier function/method calls: `fmap (frobble_with x) foo` versus `foo.map(|y| y.frobble_with(x))`... I don't know if there's other reasons. (Edit: And of course there's the fact that Haskell lets you define the function itself by pattern matching, rather than an explicit `case.`)
This looks quite nice, a combination of pattern matching and monadic do syntax. It might be the better option for Rust. However, I think I still prefer Haskell's approach where you can pattern match in function definitions and have explicit syntax for things that are chainable (monads).
I think the fundamental feature that you're after is the ability to early-return from a `match` at any point, specifying the value that the `match` should evaluate to, isn't it? In other words to have something that does for `match` what `return` does for functions and `break` does for loops? If we combine that hypothetical feature with my [earlier idea][1] for refutable `let`s, would we fully recover the capabilities of the feature(s) you describe? [1]: https://mail.mozilla.org/pipermail/rust-dev/2013-December/007480.html (I'm not sure whether or how this might be best accomplished, particularly with regards to syntax, for now I'm just seeking to clarify.)
It has to be `#[feature(macro_rules)];` (note the trailing semicolon) as that attribute should be crate-global.
Wow thanks. Working now. That does seem kind of subtle though.
Here you would want the comment on a per-type level though, I thought it was at the crate/module level at the moment.
This seemed fairly simple and desirable so I had a go at it. The main issue I ran into was the necessity of providing a version that uses Option::as_ref() for access. https://github.com/monsieursquirrel/rust_experiments/blob/master/try_get.rs
You can silence a lint for a specific type. The attribute works on any item.
It's worth noting another gotcha is you need to use #[macro_escape]; to export macros that are in a module. eg. helpers.rs: #[macro_escape]; #[macro_export] macro_rules! trace( ($($arg:tt)*) =&gt; ( ::std::io::stdout().write_line(format_args!(::std::fmt::format, $($arg)*)) ); ) #[test] fn test_trace_macro() { trace!("Hello World {} {}", 1, 2); } Which you can then use via: use helpers; pub fn foor() { trace!("bar"); } Also note the use :: prefix to libraries so they're usable from modules without the required imports (that's an absolute path invokation).
This a long-maligned wart, and is being removed soon. :)
(The best way to optimize your request is probably to use the appropriated subreddit.)
Next time please read the subreddit description. This isn't about Rust **THE GAME**. It's about Rust, **the programming language.**
ohhh thank u!
So, there is an interesting theme to this and an [earlier](http://www.reddit.com/r/rust/comments/1z2gt6/c_is_getting_sometimes_called_the_safe_navigation/) discussion. As the parent here mentions, other languages with option types (haskell, scala, and ocaml if you use Jane St Core) tend to avoid pattern matching on them (scala docs state that pattern matching options is "less-idiomatic" than using HOFs). So, either rust programmers are not sufficiently familiar with map and and_then to reach for them automatically, or they don't like them for some reason. It would be interesting to know which of those is the case. If it is the former, then I guess "education" is the answer. If it is the latter, perhaps the community needs a discussion on what is currently missing, and how best to fix it.
As someone new to pattern matching concepts in rust and haskell could you point a layman to an article on this that doesn't make my brain bleed?
I have been privately thinking about this issue for some time. If there's anything that Rust is going to become infamous for in the future, the "pyramids of doom" are going to be high on the list. In Haskell and SML you can pattern match right in the function definitions: factorial :: Integer -&gt; Integer factorial 0 = 1 factorial n = n * factorial (n-1) To reduce a level of nesting, perhaps we could have some sugar: fn factorial(x: uint) -&gt; uint match { (0) =&gt; 1, (n) =&gt; n * factorial (n-1), }
 match fn factorial(0: uint) -&gt; uint match { 1 } match fn factorial(n: uint) -&gt; uint match { n * factorial (n-1) } :U
 match fn factorial(uint) -&gt; uint; match fn factorial(0) { 1 } match fn factorial(n) { n * factorial (n-1) } A few more iterations and we'll approximate haskell syntax yet!
&lt;3 separate type signatures
This actually looks pretty good and, as far as I can tell, not even so much out of place regarding the rest of the syntax.
Sure, you can pattern match on options, and in most code-bases you will sometimes. But, in many (or even most) cases, HOFs are cleaner and easier to read than nested pattern matching. And, to get back to the original point, people keep on coming up with new syntax *explicitly designed* to avoid pattern matching on options, which suggests that (even though you can use it) a lot of people don't want to.
I think it's the latter. From my read of the mailing list and /r/rust, most people who are interested in Rust know about the higher order functions associated with `Option` and `Result`. But closures are more limited in Rust than they are in Haskell, Scala and Ocaml, so Rust does not treat them as costless (or nearly-costless) abstractions that can be freely introduced in basic idioms. Scala, Ocaml and Haskell all use garbage collection and don't try to do the alias tracking that Rust does. Rust has to deal with borrowing and deterministic object lifetimes, and can't always do it perfectly around closures. The recent history of rust has seen a fairly consistent movement _away_ from requiring closures for common use cases. See in particular the move from internal iterators to external iterators. In order to get rid of match pyramids using HOFs instead of pattern matches, at minimum you'd need the following: 1. [unboxed closures](https://github.com/mozilla/rust/issues/8622). As it stands now, the borrow checker doesn't know that `map` or `and_then` is not going to call the closure multiple times, and therefore imposes unnecessary limitations on what you can do inside the closure with captured variables. Unboxed closures would get rid of most of the unneeded limits, if not all. 2. A syntax that hides the nesting, similar to Haskell's `do` or Scala's `for`. This in turn may or may not require [higher kinded types](https://github.com/mozilla/rust/issues/8922). Without this, all you can do is replace a pyramid of nested match statements with a pyramid of nested closures. At least it's only one level of nesting per HOF, instead of two per match statement, so you've cut the size of your pyramid in half, but it's a pyramid all the same. Haskell's `do` and Scala's `for` were created to solve exactly this problem. But pattern matching is not going anywhere. Also, look again at the example from the PR that I linked in the OP--half the match patterns are not `Result` or `Option`, so the common HOFs would not help. So even if the current problems with closures are alleviated (and I certainly hope happens), I still think it'd be useful to make pattern matches easier to deal with.
Yeah, sorry, I wasn't trying to derail your thread. But, as you yourself say, people seem to want an alternative to match statements, and I just wonder if it is worth having a discussion about what they do want.
It's a hack in C# (to add an operator) but it is a necessity due to 2 things: nullable by default and the lack of pattern matching. It makes Maybe really clumsy to work with and the risk of hitting an exception with parent.child.child.child.Value is huge. I agree strongly with the addition of this operator to C#, simply because pattern matching and nullable-and-mutable-by-default isn't going away. Any language being designed from scratch shouldn't fix the symptom like this, it should fix the cause. Rust doesn't need this.
&gt; might there be a need for the opposite 'corrective' keyword in rare cases, or would unsafe blocks stil need a 'restrict' (are unsafe pointers assumed to alias, or not? What would a "corrective" keyword do? (Unsafe pointers have the same semantics as plain non-`restrict` pointers in Clang/LLVM's C.)
I wonder if we could even drop the `match`.
(Edited to reflect this.)
Ok, i've seen you can make Rustc emit a static library - (i'm tried to modify a sample makefile to bring this in, but with no luck... i've cloned the sample and just copied the .a somewhere. i've seen various suggestions to add the .a to LOCAL_SRC_FILES or LOCAL_STATIC_LIBRARIES but it seems the android makefile is doing more behind the scenes (expecting that library is to be built itself) .. ) any suggestions /help/info would be welcome.. EDIT: I've managed to compile this manually ... linking a .a with rust code into the .so that the android APK works... verified i can actually call rust "extern"C" functions from their sample. So I feel a lot better now..:) seeing Rust code actually running on my phone removes a mental barrier to adopting it, I have more faith in this idea that LLVM makes it portable.
&gt; (Unsafe pointers have the same semantics as plain non-`restrict` pointers in Clang/LLVM's C.) They don't quite have these semantics since Rust doesn't use the C TBAA rules.
&gt; 'corrective' keyword in rare cases, or would unsafe blocks stil need a 'restrict' (are unsafe pointers assumed to alias, or not? &gt; &gt; What would a "corrective" keyword do? ok, so is it possible you'll still need something like 'restrict' for unsafe pointers to replicate unsafe C code with restrict exactly ... I realise there won't be as big a need for this since you have helpul assumptions for the majority of safe code
Why is this better than bjzaba's original suggestion? It's more verbose, loses the visual similarity to match expressions, and being broken up into multiple blocks suggests that the cases might be defined in multiple places. This doesn't look like Rust to me, it looks like Haskell envy.
That's the goal. Ideally it'd take a size and alignment, such as aligned_alloc/posix_memalign. Note that we [have arenas](http://static.rust-lang.org/doc/master/arena/index.html), and the `box` changes will make them more convenient to use.
I would use `try!` for writing, and for reading: ``` reader.read_line().ok().unwrap_or(~"nothing") ``` See [here](http://static.rust-lang.org/doc/master/std/io/trait.Buffer.html#method.read_line).
But if I do try!(stream.write_line(line)); I get the compiler error: mismatched types: expected `()` but found `std::result::Result&lt;&lt;generic #225&gt;,std::io::IoError&gt;` (expected () but found enum std::result::Result)
It's because you changed the function's type signature. Make it's return type `IoResult&lt;()&gt;`. Also, see my comment elsewhere in this thread.
adrientetar was saying to not use `try!` for `read_line`, just use it for `write_line`. We both pointed out ways to deal with the `IoResult` returned by `read_line`. Also, you don't have to use a separate function, it's just cleaner in general. If you want it all in `main`, you'll need to match on each call's return value in order to handle errors like your bulky example. Since you aren't doing any error handling anyways, it looks silly. If you're fine with failing on error, just `.unwrap()` each `IoResult`. Or any of the alternatives in http://static.rust-lang.org/doc/master/std/result/enum.Result.html
I know what adrientetar was saying. I'm just thinking forward, because I'm sure that I will be calling other functions that return Result, that won't return the same type as these std::io calls. I'm not really fine with failing, since that will kill the task. I really just want to break out of the loop if the result is Err
I've revised the proposal based on the discussion here. I've now made the early return from the `match!` block a separate feature from the refutable `let`, and linked back to the earlier mail thread discussion on refutable `let`.
I've now revised the proposal, and I've gone back to using `^` for this. It's in a unary position so it shouldn't be ambiguous vs bitwise xor. It's limited to only within `match!` blocks (and since I'm still proposing an external syntax extension that I'll attempt to write myself, rather than a change to the language itself, they must be so limited.) Labels are not used, though I note that as a possible addition. The reason I don't use `return` is that I think the `return EXPR for SCOPE` syntax would be entirely too long...too many longhand words that don't convey information and are only there for syntax disambiguation. 
 From the thread linked: " are virtual if invoked through a pointer-to-MyTrait ("object"). To clarify, using trait methods via the type itself or generics is still just static dispatch. It's only dynamic dispatch via a trait object." So things in impl blocks are called statically? Even when called in generics? Or did I misunderstand? 
I'll give that a shot. Thanks so much for all your help :)
One way is to define a macro that does what you want: macro_rules! return_on_err { ($e: expr) =&gt; { match $e { Err(_) =&gt; return, Ok(o) =&gt; o } } } // ... let line = match stream.read_line() { Ok(line) =&gt; line, Err(_) =&gt; break, }; return_on_err!(stream.write_line(line)) return_on_err!(stream.flush()) (I also pulled out the outer match to reduce nesting.) Other ways (with different semantics) are: - `let _ = stream.write_line(line);` (ignores errors entirely) - `stream.write_line(line).unwrap();` (calls `fail!()` on error)
This isn't possible in `main`.
Is this designed to handle build managment too or is that a seperate application?
My mistake, I was looking at an older revision with a separate function.
Not sure what that is, but anything it doesn't do now, it's unlikely to ever do.
Note that `x` is entirely missing from the body. The following syntax may get rid of such unused placeholders: fn factorial match(uint) -&gt; uint { (0) =&gt; 1, (n) =&gt; n * factorial(n-1), }
I did this write-up after experimenting a little bit. Comments on whether this is an adequate assessment would be very appreciated.
Is there a reason why you didn't consider the generics approach? That would typically be considered more idiomatic. The only change would be: -let kernel: ~ComputationalKernel = ~SumLinearTermsKernel{a: 13.41, b: 9.71, k: k}; +let kernel = SumLinearTermsKernel{a: 13.41, b: 9.71, k: k}; Performance should then be back to perfect.
What rustc version and what optimization level did you use for this? If your example doesn't devirtualize at `--opt-level=3`, it's technically a **bug** ;). **EDIT**: it looks like you used `--opt-level=3`, and I think I read the graphs wrong. It looks to be the fault of the original allocation. Could you also test with `&amp;value as &amp;Trait` to use stack allocation instead of the heap? **EDIT2**: you might want to consider giving `&amp;Trait` to [test::black_box](http://static.rust-lang.org/doc/master/test/fn.black_box.html) before running the loop, to emulate completely dynamic polymorphism.
Take this with a salt block, but the performance blip could be due to a break point in whether to unwind the loops or not. Not a contributor, just someone who likes to follow rust.:) 
My rustc comes from a fairly recent master checkout (one or two weeks ago). I am not at the same machine atm, so I can't give you the exact version. I probably should've mentioned that and the opt-level in the post ;) Thanks for the tips, I'll look into stack allocation and using black box.
Yes, I really wanted to see how run-time polymorphism affects performance. This would be required if I had used different computational kernels throughout the loop depending on some condition. I simply did not do so to be able to compare this to the less generic approaches by doing exactly the same calculation underneath.
The canonical way to perform benchmarks is using the [microbenchmarking support of the the unit test framework](http://static.rust-lang.org/doc/master/guide-testing.html#microbenchmarking). Although, it doesn't really provide an equivalent to vary `total` and `k` at runtime... you could probably use environmental variables if you had to, I guess. (One can even get a JSON file of the data with `--save-metrics &lt;filename&gt;`.)
&amp;gt; it's technically a bug ;). (To be clear, it would probably be a "bug" in LLVM, not rustc.)
I know that feature but the absence of parameters made me favour the bash script variant, since it was anyway only intended as a one-off. Could I maybe generate multiple benchmarks with a compiler macro?
Oh, I didn't even realise there was an older version.
This is an interesting post, but I think the title is inaccurate. There is in fact no actual polymorphism in this example. Moving something into a method with an impl doesn't make it polymorphic -- it still can't operate over multiple types, and the method call is still statically dispatched. It's interesting that there is any overhead at all, even if it is minor: I expect the overhead you're seeing is just coming from the access to `a` and `b` via `self` -- presumably LLVM isn't optimizing those accesses as well as it would local variable access, presumably because we aren't integrating with the alias analysis as full as we (eventually) will.
You can do better than that: fn sum&lt;T: ComputationalKernel&gt;(n: int, kernel: &amp;T) -&gt; f64 { range(0,n).map(|i|kernel.calc(i)).fold(0.0,|a,b|a+b) } Let's use those iterators! :)
I've noticed it's quite a bit slower on my Linux VM (Oracle VirtualBox). It's pretty much 2-3 times slower than advertised. But so is Firefox building (15min said, took me about hour or a two to fully build Fx).
Somebody mentioned this problem on #rust IRC channel. Posting it here in case other people need it.
its great to see that finally fixed.. I have been made to feel like i'm insane for wanting something outside of the standard C++ library implementaiton of vectors. its just 15 years late.
This bug has now been fixed: https://github.com/mozilla/rust/pull/12508
I think of it like this: Rust is a very new language (heck, it's not even 1.0 yet), and there will always be room for optimizations. Compilers for languages like C++ have existed for decades, and they are still getting faster, so the Rust compiler is bound to get faster the more work is done. I think, considering how new the language is, we're already approaching pretty decent compile times!
Quite so.
That means there was never a virtual call, just a heap allocation ;).
You can drive benchmarks dynamically if you want to, btw, but I'm not sure on the details atm.
Who would have expected compiling stuff is slower when virtualized...
I can? That's cool… I need to look into that.
Whilst I doubt I have enough experience to judge about this, I think you should definitely have a look at [SDL](http://www.libsdl.org/) and [its rusty wrapper](https://github.com/AngryLawyer/rust-sdl2). Also, I would definitely love to port an Android NDK sample to Rust.
The thing with the try! macro is that it is extremely limiting. By calling ```return``` it means that if there is an error then my function must be done. The macro unwrap_or_break (above) does what I need. But it is unfortunate that a one-off macro is needed where a typical try/catch block would do what I want.
Might be a dumb question, but... why do a hacky thing that's supposed to be temporary instead of something production-quality? (Or possibly better phrased as: why not make production-quality a goal?)
In C, it is unbuffered because `assert` (and such) will: 1. Print to `stderr` 2. Abort the program And it would be quite useless if the program aborted with the message still in its buffer. Given that Rust normally does not abort, the reason does not apply to it, so I wonder if other languages just followed in C's footsteps or had other reasons for this behavior.
am I right in thinking, theoretically context-free grammar should allow a future rust compiler to be smarter at caching and selectively reprocessing when you change - figuring out inter unit dependancies - compared to C++ where huge amounts of template code will always be brought into each translation unit. i think this is the problem, currently rust translation units are very large.. whole programs full of generics. Go is fast because the language is so simple. 
It has very little to do with generics and everything to do with optimization passes. Generic instantiations don't take much of the overall time.
AST deseralization doesn't usually take that long.
It depends on the ABI; with the Itanium ABI LLVM routinely avoids de-virtualizing method calls such as: for (auto&amp; object: list) { polymorph.call(object); } because the signature of call is in fact `call(Polymorph* p, Object*)` and therefore it cannot assume that the virtual pointer is not going to be altered :(
You can just not build with `-O0` and fastisel will be used (although currently we have some issues whereby fastisel usually gets disabled a lot, which the vector changes underway partially address).
Given the very small example you managed to produce, I would be interested in seeing the LLVM IR. If it is too big because of inlining, just the LLVM IR of the "main" function would be enough. Then we would know for sure whether there is a dynamic dispatch or not :)
While there are many possible improvements, most of which concern LLVM(FastISel, mergefunc, function-based parallel codegen), I have a sinking feeling people won't be satisfied short of incremental compilation. Already, it's not the case that compiling Rust code takes much longer than other languages, but that Rust's compilation unit is bigger and more code is recompiled when you make a change. This won't be easy to fix.
To join press f1 in game and type net.connect 148.251.44.210:28025 :)
Are there any optimization passes that can be disabled for dev builds? Or does llvm just do a crap ton of passes just as part of normal operation? I would accept slower code if it meant easier test driven development via faster compile times. 
With the caveat this is coming from someone who just started getting in to Rust, I think crates serve fine for incremental compilation. What I have written so far I've split in to crates. Each crate compiles very fast and it is probably good practice to split up disparate parts of your program in to crates anyway.
By default (without `-O`) no optimization is run, but sometimes we fall off fastisel. This is a series of bugs in rustc.
Isn't the core issue that you can't recompile parts of a crate and only the whole crate?
I would also be interested to see if there is a way to force non-threadsafe code to be queued on a single thread when run from a task. In Objective-C, I would dispatch this to a Grand Central Dispatch (libdispatch) queue running on the main thread to resolve this. I'm currently not aware of how to do this in Rust, but there may be a way that is off the beaten path a bit. For the task switch, I'm pretty sure I read in the Rust issue tracker that the green task scheduler will never switch inside a function at the moment and will probably never switch inside a block in the future if it becomes more fine grained. Take that with a grain of salt though since I'm not sure the exact issue this was referenced in. I get caught in a lot of edge cases like this too since there is little documentation on the runtime side. EDIT: See `&amp;'static dbaupp`'s answer for how task switching currently works. I would still love to see something in the runtime that helps with running non-threadsafe code safely in a task. I think the GCD method would be fine.
 plain_c_call(); if os::errno() { ... } will not switch threads between the two calls under libgreen: it's cooperatively scheduled and so only performs context switches when actually doing something that interacts with the runtime (like spawning a new task, or sending/receiving a message on a channel).
Call unwrap() on it. Or use try!, wrap it all in a function (or closure) returning IoResult&lt;T&gt; and then match on that result. 
Don't do this. Defining a new macro for such a simple task is a clear sign that you are doing it wrong or that the standard library is broken. Instead, use try!, wrap it all in a function returning IoResult&lt;T&gt; and then match on that. 
No, it's not a simple task. There are so many ways to respond to errors, and I would think just returning straight away without actually handling the error is pretty rare (i.e. ignoring it and quitting immediately). And macros in Rust can be regarded essentially just a different form of function, i.e. same as writing a helper function. `try!` is provided for instances where you don't wish to handle the error in the current function and just wish to bubble it up for the caller to handle as they wish. (The macro mainly exists to encourage this behaviour, rather than people just ignoring the errors.)
`try!` is has *one* specific goal: bubbling up errors for the caller to handle. It's not designed as a general error handling strategy. (Defining short helper macros isn't that bad, since they're far better than (e.g.) C's macros.)
Who would have guessed, that comment was useless... Yeah I expected the slowdown, but not as drastic. It goes from ~15min to 120min that means that virtual machine is running order of magnitude slower, which would be noticeable from the VM.
I would think it is possible to move the errno value when switching tasks between native threads. Shouldn't be much of overhead.
I assume eddyb is talking about [using libtest itself directly](http://static.rust-lang.org/doc/master/test/index.html). It looks like it might be as simple as [calling `test::bench::benchmark` with the closure](http://static.rust-lang.org/doc/master/test/bench/fn.benchmark.html).
The latter does work, and no, it should be as fast as the first explicit `while` (there have been some bugs in LLVM that main using `range` with small types like `u8` and `u16` didn't optimise as well as the `while` loop, but this is improving on their end). (Good catch to call that out, `for` + iterators is strongly recommended over the other loops, where possible.)
No, when you use optimization there shouldn't be a performance penalty; the range-iterator gets optimized away. I believe in some cases using an iterator is even faster than a loop, when iterating over a vector the bounds checks can be omitted?
I guess the problem I'm having is that I don't want to have to define a helper function or helper macro when doing something rather straightforward. In my code example, there are two calls that could fail: ```stream.write_line``` and ```stream.flush``` When handling errors here, I (for the most part) don't care which call failed. All I care is that they failed, and I should do something. In a typical C-style language, I would do something like try { stream.write_line(); stream.flush(); } catch (Exception e) { // do something } // some other stuff before the function returns With the proposed solutions in this thread (in particular the macro), I would now introduce a new macro every time I would need a "catch block". try_catch(stream.write_line()); try_catch(stream.flush()); // some other stuff before the function returns First off, I've repeated myself by typing try_catch twice. This leads to not very readable code. Second, if stream.write_line() fails and gets handled, I then don't want stream.flush() to get called. I don't fully understand the macro documentation, so perhaps there is a way to do something like try_catch!({ stream.write_line(); stream.flush(); }, { // do something }); // some other stuff before the function returns But I feel like if this was possible it would be a no-brainer part of the std library (please correct me if I'm wrong)
&gt; almost all of which was spend processing bytes for hashing This is a known issue, filed as [#10586](https://github.com/mozilla/rust/issues/10586), with a possible resolution being using FNV as implemented in [#12635](https://github.com/mozilla/rust/pull/12635). 
If that's so, [you should rephrase your issue](https://github.com/mozilla/rust/issues/12130) to be clear that it's about "Error handling strategies other than `try!` need to be easier to use &amp; better documented", rather than "can't use `try!` in `main`".
It's pretty much identical to stream fusion; most of the iterator methods return new iterator structs, i.e. they are reified/lazy computations, so `a.zip(b).map(f)` is a object that represents zipping `a` with `b` and then applying `f` to each element, but it only evaluates each element when you actually request it (via `.next` or one of the higher order functions).
A lot of people use the standard Go compilers when developing and then gccgo for their production binaries. Don't discount how attractive Go's fast edit-run-compile cycle is to a lot of developers. And I say that as someone who doesn't like Go. If there's one thing the Go team did well, it's how quickly you can go from code to a running binary. Slow C++ compile times are also _the_ primary reason why Go language development started.
But is there any libgreen call that would switch a task between threads without itself destroying errno?
&gt; when iterating over a vector the bounds checks can be omitted? A vector iterator is faster than using checked indexing, because you have a check against the end (like the iterator) *and* bounds checks. A loop using unchecked indexing via `unsafe` would be no slower.
That lets me check the errno value if I care about it myself. The question is how to present the information in errno to the callers of my library, some of whom might do similar numerical checking and some of which might want to stringify it to print the error? Creating my own error enum and strings would just be wrong. The stringify seems to not be supported at right now (needs to use strerror_r, which is not in std::libc even though it's posix, but note that there's an incompatible GNU version ... ABI fun)
It's a single loop without optimization. The optimization passes only have to do dead simple iterative inlining and eliminate some trivial redundancies for it to be as fast as the same code written by hand. Sadly, I don't think it will get vectorized without relaxing the floating point accuracy, which Rust doesn't expose.
Returning `Result&lt;T, int&gt;` would work. Or replace `int` with a wrapper type like use std::libc::c_int; pub struct Errno { priv num: c_int } impl Errno { pub fn number(&amp;self) -&gt; int { self.num } pub fn string(&amp;self) -&gt; ~str { // import strerror_r via FFI extern { fn strerror_r(x: c_int, buf: *mut u8, len: uint) -&gt; c_int; } // ... } } I guess this could be an abstraction in `std::os` too.
How much memory was the VM allocated? (Enough to avoid swapping?)
I thought this as well. Not at machine to verify, but I presume around 4Gigs, I don't remember hearing HD working. Update: No it was 2 GB, but I was kinda expecting it to be enough. I guess it's plausible the slowdown was due to swap.
Do you know which goals it failed to meet? So far, that seems to be some kind of secret... when Mozilla makes an important decision about Rust it should be able to explain it to the community.
I have zero experience with LLVM IR, so I have hard time telling. But nevertheless, here it is: https://gist.github.com/aepsil0n/9344651
If not using libgreen, then you're using libnative, i.e. exactly the same as C and `errno` works just like C (so the above code snippet works fine there too).
[follow-up post](http://blog.ebopp.de/blog/2014/03/04/follow-up-on-polymorphism-in-rust/)
I don't quite understand what you're saying. The idea behind a module system is that you organise it by modules. Not per object and not per function. It's up to the author to choose how granular the modules are. For me, the part of the module system I don't quite grasp is the need for lib.rs/mod.rs and `pub mod`.
While I have done virtually zero in rust, I have written a small n-body simulation in c (for a diffeq class, though irrellavent) Any way, I found it pleasing to make a vector struct and associated functions and throw them into a single vector.c file. So my vote is per struct, or really per module. (Of course in school we use java, and that may taint my opinion in such matters) A single file per function would lead to way too many source files in my opinion, and perhaps dicourages simplicity of functions?
This idea is perhaps the worst legacy of Java and C++. Its prominence makes the use of simple text editors next to impossible, forcing you to use IDEs to navigate your code. And it leaves a ton of leaky abstraction artifacts everywhere...often in some 'util' module...where you place your functions that dont seem to fit with one object more than another. And avoidance of the pesky util category often leads to entirely new objects created in order to fit the poorly categorized functions into the object/file model...increasing complexity without benefit. Sometimes verbs just do not need to be owned by a noun. 
I assume you are asking why msg: str rather than msg: ~str is used? Here's the [periodic table of rust types](http://cosmic.mearie.org/2014/01/periodic-table-of-rust-types/). What you're suggesting will be possible when dynamically sized types land, but it's not possible currently.
&gt; A lot of people use the standard Go compilers when developing and then gccgo for their production binaries. I don't think many people use gccgo in production. With gccgo you lose precise GC, for one. &gt; If there's one thing the Go team did well, it's how quickly you can go from code to a running binary. Slow C++ compile times are also the primary reason why Go language development started. I don't really want to write another translation backend, especially since I think it may not even help compilation time much. The biggest issue with compilation time is that our translation units are large, which is also a problem with Go.
C library functions don't set errno on success, at least not per spec: http://pubs.opengroup.org/onlinepubs/009695399/functions/read.html
Why can you ask for some arbitrary type without a box, but not a string? I understand that a string is not a type, but that alone does not necessarily answer my question, why are strings so speical? Also, isn't it better to use `&amp;` instead of `~` in this case then?
&gt; which Rust doesn't expose Will it? It seems like a desirable feature. 
Compiler optimization is something I've recently found facinating. I've never worked with a low-level language before I started experimenting with Rust, but I've worked with mathematical optimization techniques and AI techniques quite a bit. Is there any applicability of constraint programming, integer/linerar programming, logical inference, statistical inference, or rule engines to compiler optimization? 
Urf, I have some experience with C and C++ as IR, but this is quite different certainly :p Nonetheless, I think I found the body of the `while` loop: // For reminder, in Rust: while i &lt; n { s += kernel.calc(i); i += 1; } ; equivalent in LLVM IR @vtable3044 = internal constant { void (i8**)*, double (%struct.SumLinearTermsKernel*, i64)* } { void (i8**)* @"_ZN7_$UP$i89glue_drop17h36bbe25d1ef88d8aE", double (%struct.SumLinearTermsKernel*, i64)* @_ZN40SumLinearTermsKernel.ComputationalKernel4calc20h12bd1476fb1209ccFaa4v0.0E } while_body: ; preds = %normal-return16.while_body_crit_edge, %while_body.lr.ph %75 = phi [1 x i8**]* [ bitcast ({ void (i8**)*, double (%struct.SumLinearTermsKernel*, i64)* }* @vtable3044 to [1 x i8**]*), %while_body.lr.ph ], [ %.pre71, %normal-return16.while_body_crit_edge ] %76 = phi i8* [ %65, %while_body.lr.ph ], [ %.pre, %normal-return16.while_body_crit_edge ] %77 = phi double [ 0.000000e+00, %while_body.lr.ph ], [ %82, %normal-return16.while_body_crit_edge ] %i.070 = phi i64 [ 0, %while_body.lr.ph ], [ %83, %normal-return16.while_body_crit_edge ] %78 = getelementptr inbounds [1 x i8**]* %75, i64 0, i64 1 %79 = load i8*** %78, align 8 %80 = bitcast i8** %79 to double (i8*, i64)* %81 = invoke double %80(i8* %76, i64 %i.070) to label %normal-return16 unwind label %unwind_ast_242_17.loopexit So, as far as I can see the call is not devirtualized indeed (the `invoke double %80(i8* %76, i64 %i.070) [...]` is the actual call, and it uses a pointer to function derived (ultimately) from `%75` which is a pointer to `@vtable3044`.
I don't think LLVM does any kind of range metadata propagation, that would certainly have interesting results. There's also the modulo checks [I've given as examples here](https://github.com/mozilla/rust/issues/8106#issuecomment-30406678) that can't be used as information by LLVM now, which means my PNG decoder code isn't as idiomatic as it could be.
Wrapping C libraries that use TLS (thread local storage) is also different between runtimes. It would be nice if there was a way to use native tasks and green tasks in the same program.
I was under the impression that stderr is typically *line* buffered. I can't remember, but this may be true only if it's a tty (or ptty), otherwise fully buffered.
It all boils down to Rust being very low-level and simple with its basic types. For example, something like C++s `String` type would map more closely to something like `struct String { s: CowArc&lt;~str&gt; }` if translated to Rust directly, which in turn hides much more machinery in the definition of `CowArc` itself. But in Rust strings are just a special case of vectors/arrays (`&amp;str` is internally identical to `&amp;[u8]`), so I'll talk about them instead: A basic type in most programming languages is the array: A type encapsulating zero or more elements of type `T` arranged next to each other in memory. How exactly it is implemented depends on the language, but fundamentally there is one piece of information necessary to describe an array in addition to the `T` stored in it: The number of elements. However, because Rust allows unboxed values, the compiler needs to know the size of any type you want to store somewhere at compiletime. For example, to know how to lay out the variables on the stack, or which offsets the fields of a struct would have. Which means, if you want to store an unboxed array, you basically need a separate type for each possible size. Rust has these in the form of the types `[T, ..0]`, `[T, ..1]`, `[T, ..2]`, etc. But, in most cases you want your code to work with more dynamic array sizes, and not just with one singular fixed size. For example, if you load a file into memory there is no way of knowing the size of that file at compiletime. The simplest way to achieve this is to store a pointer to the first element instead, and store the original length separately as an actual integer value. In pseudocode: ``` [T, ..N] -&gt; *T, uint ``` The important thing here is that the end result (pointer to `T` and integer) is the same type with the same size no matter what size of array it points at - the integer describing the length simply has a different value. Which means you can write code that simply takes an array as pointer and integer, and have it work with any array length. (The term for such a pointer + size pair is "slice", btw) Now, interestingly this transformation does not change any ownership semantic - if you owned the array before, you still own it after turning the size into a value, and if you only had a reference to it, you still just have a reference and a integer. So semantically there is really no difference, which is why Rust is using the same sigil for it: A `&amp;T` is a reference to a `T` and represented as a pointer, and a `&amp;[T]` is a reference (or slice) to an array of `T` and represented as a pointer and a integer. However, because now part of the type is stored as a value next to the pointer, separating the `&amp;` and `[T]` in a `&amp;[T]` is impossible: They are a atomic unit in the type system. Which leads to the somewhat unexpected situation that you always need a pointer for arrays and strings, while other types can be stored unboxed. Other languages generally deal with this by defining seperate types for this, so a `&amp;[T]` would be a `Slice&lt;T&gt;` for example. We _could_ do that in Rust, but it has one problem: Composability with custom smart pointers. `~T` and `&amp;T` are build in, but we want custom library types like `Rc&lt;T&gt;`, `Gc&lt;T&gt;`, `Arc&lt;T&gt;`... to feel just as first class as them. Which means, if slices became their own types, you could only combine them by introducing double indirection. For example, `Rc&lt;Slice&lt;T&gt;&gt;` would consist of a pointer to the `Slice&lt;T&gt;`, which in turn consists of a pointer to he first element of the backing array. For this reason, we are currently moving in the opposite direction of making this special-ness properly part of the typesystem: Under the new scheme, a `[T]` _will_ be a proper type. You still can't store it unboxed, but `&amp;[T]` will no longer be special, and become just the composition of a `&amp;` with a `[T]`, which will allow stuff like `Rc&lt;[T]&gt;` or `Rc&lt;str&gt;`. (Note that this change is actually more motivated by trait objects like `~ToStr` or `&amp;MyTrait`, which have the same pointer+additional data requirement like slices) --- For you 2. question: The decision between `~str` and `&amp;str` depends on whether the `Msg` struct wants to own the string or just reference one. 
Well, the second one is not really supported in Rust (its possible with a syntax extension, but not part of the language semantic)
I think because in Rust a method with same name can't have different parameters. ~~Basically this is all to avoid creating a [vtable](http://en.wikipedia.org/wiki/Virtual_method_table) for different methods, which would slow things down considerably.~~ EDIT: Apparently I confused some issues on ML.
I prefer the default behavior in Rust, because the way the type system works. The example was to demonstrate it is possible to organize code similar to C#, Java and C++. I thought I expressed this clearly in the comments. *Edit: I updated the example with a disclaimer. More details in the comments.
By "this" idea, I presume you're talking about one object per file?
I updated the example with a disclaimer. Of course I am not writing libraries like this in Rust! *Shrugs*
I don't think that's accurate at all. [The issue is purely coherence being overly conservative.](https://github.com/mozilla/rust/issues/11166)
This example is to show what you can do with the module system, it is not recommended to use this approach. lib.rs/mod.rs is to make a module in a folder not dependent what it is called externally. 'pub mod' makes a module public, 'mod' makes the module reachable internally, but not externally.
I guess I would go with the default behavior in Rust. There are cases such when porting a legacy application to Rust one wants to keep the same layout. A single file per function is a pragmatic way of organizing large business logic functions. I am curious how large code bases will play out in Rust. Phantom types makes it possible to do much more with business logic in safe way by design.
Well, of course, you could do this: foo.rs: struct Foo { a: int } bar.rs: struct Bar { b: int } lib.rs: pub use foo::Foo; pub use bar::Bar; mod foo; mod bar; But I agree that it's not exactly practical.
I read it as 'not the way Smalltalk does it.'
&gt; I don't think many people use gccgo in production. With gccgo you lose precise GC, for one. I personally know several teams that use gccgo in production. &gt; I don't really want to write another translation backend, especially since I think it may not even help compilation time much. I'm not saying do or don't write another translation backend, all I'm saying is that compilation performance is both a) very highly valued by developers and b) apparently important enough to some to start designing *an entirely new language.* Just keep this in mind, that's all I'm trying to say. &gt; The biggest issue with compilation time is that our translation units are large, which is also a problem with Go. You mentioned in a different comment that by default (no `-O`) no optimization is performed in rustc. You also say that Go doesn't really do optimization, and I'll take your word for that (I trust you know better). So if both Go and Rust have the issue of large translation units, shouldn't that also mean that unoptimized Rust compilation should be comparable to Go's? 
/r/playrust
Thankfully... there is. [See the runtime guide](http://static.rust-lang.org/doc/master/guide-runtime.html).
The big problem with this is it makes working with strings extremely inefficient. Any sort of character index requires a linear search from the start of the string. And if you want to get byte access, now you need to round-trip through &amp;[u8], which not only is awkward but adds the overhead of re-checking the entire string to make sure it's valid utf-8 even though we knew that just a moment ago. In general, it turns out that most string manipulation does happen with byte indexes. This is a good thing, because of the aforementioned issue with character-based indexes being inefficient.
I'm not sure I understand your concerns. &gt; Any sort of character index requires a linear search from the start of the string. True, but true for the current situation as well, right? Either you want the ith character, or you want the ith byte - those are not the same thing, and its up to the user to make the choice of what they want. This proposal just makes that choice explicit. &gt; now you need to round-trip through &amp;[u8], which not only is awkward but adds the overhead of re-checking the entire string to make sure it's valid utf-8 even though we knew that just a moment ago. Yes, its a bit awkward - but so are Options and Results, with the awkwardness making safety explicit. I would argue that's the same here. As for the the re-checking: that's part of why I proposed the Bytes (or `UTF8`) object, which would have all indices / slices done on byte indices, but maintain the valid UTF8 invariance; then you don't need to re-check. Edit: I guess I did not make this clear, but the main purpose of this proposal is to separate indexing by codepoint and indexing by byte into two separate objects, to maintain safety. I mentioned making index-by-codepoint the 'default', but that is secondary and optional: which one is made default (or whether any is made 'default') is something that can be determined independently from the rest of this.
Hmm, some nitpicking: we already have taught ourselves to do any string operations only with iterators and specialized methods in Rust. For example, we don't use `s.slice_to(s.len() - 1)` to get the slice of the non-empty string except for the last codepoint [1]: the correct way to do that is `s.slice_to(s.char_range_at_reverse(s.len()).next)`. The better way to describe the situation is that there is no such "byte offset" in Rust: you only have two positions known to be valid (0 at the beginning of the string, s.len() at the end of the string s) and you get subsequent valid positions using `char_range_at` and so on. Subsequently, such positions are *handles* which conveniently coincide with byte offsets (and thus it doesn't incur the additional cost when using it). In this regard, and given the extreme inefficiency and ambiguity with the proper character-based operations, I firmly oppose to have another kind of position handles. It has simply no use. But that's not because we are already using byte offsets; it is rather because we already have working and efficient handles to the positions. [1] Yes, this is also a bad practice, but it is at least safe to do it. Unicode strings are built on codepoints and everything else (grapheme clusters and so on) is afterthought.
Thanks for your explanation of handles, that's more clear to me now. I see your point about not wanting more handles, but it seems to me that the current handles are well-defined in theory but not in practice: its far easier, for example, to do `s.slice_to(s.len() - 1)` than `s.slice_to(s.char_range_at_reverse(s.len()).next)`. Perhaps the handles should be their own type (perhaps simply a private alias of uint to maintain efficiency), so that one can't write `s.slice_to(s.len() - 1)` and have it compile?
&gt;In general, it turns out that most string manipulation does happen with byte indexes. Quite the opposite, what you said is only true if you're working with ascii. Doing byte indexing by default means that most people will use it without thinking much of it and then the code will seem right but will actually be wrong when more-than-one-byte characters appear. When people are manipulating strings they want code points, if they wanted bytes they would be manipulating bytes. Edit: Quoted my own words... That must have been confusing to read. 
A lot of Rust code is organized this way, one primary, public type per file, with a bunch of methods, though I suspect most Rust doesn't stick strictly to that formula. In the worst cases all your modules `foo` export nothing but `Foo`, and now your callers all stutter `foo::Foo` when they import your type. Using public reexports of private modules lets you present the API differently from the implementation. It may look kind of funny all alone here, but it is useful. This pattern is used in at least [std::io](https://github.com/mozilla/rust/blob/master/src/libstd/io/mod.rs#L211), though that is only one of the organizational techniques used there.
Two ideas: 1. Introduce bytes() and either cpts() or cpoints(), *or* glyphs() to make the difference abundantly clear, and deprecate all functions that do not make this abundantly clear (such as slice()). 2. For situations where only OEM extended ASCII (codepage 437) is needed, ensure there's an OEM extended ASCII data type where even glyphs() can be fast.
What if we had a `struct ByteIndex(uint)` that’s returned by `char_indices` and taken by `slice`?
The proper Unicode way would be to implement UAX#29 http://www.unicode.org/reports/tr29/
&gt; cpts() or cpoints(), or glyphs() Glyphs and codepoints are different, and having access to both is useful (just like having access to bytes is useful.) We currently just have `.chars()` and `.bytes()` since we don't have the glyph splitting logic yet.
&gt; True, but true for the current situation as well, right But encouraging the use of codepoint indices (not just for `[]` indexing) makes interacting with strings easy to make into O(n^2).
This is the solution I prefer: keep approximately the current scheme, but make the units clear. Maintains high-performance, but makes it harder to accidentally screw up. ([#10044](https://github.com/mozilla/rust/issues/10044#issuecomment-26982523) includes some exploration of the "return value with units" idea from a while ago.)
I opened [#12710](https://github.com/mozilla/rust/issues/12710) about removing `string[i]` entirely. &gt; When people are manipulating strings they want code points The most efficient way to manipulate codepoints is using the byte indices of those codepoints.
I know, that's why I listed them both. Granted, both could be implemented, but I put emphasis on the second or for a reason, which is missing in your quote. My point was simply that there should be distinction between component and glyph-based logic, and byte-basic logic. In that respect, bytes() is fine, but chars() and slice() can sound misleading. Since rust seems to aim to be a language that facilitates safe programming, I think it would be very worthwhile to evaluate if the current and future naming structure don't hamper that goal. After all, C and C++ could also be safe when used by an omniscient programmer, but for real world situations Rust should be more suitable, so too in this case.
Today I can't replicate this. No arguments i pass seem to make any difference, viewing what comes out with 'nm' always reveals something unexpected ... eg its only showing a *mangled* version of the rust unmangled C function :( .. any linkage to the rust function in the NDK sample means it fails to run, even if it doesn't reach that function
I'd personally be happy renaming our current `char` type to `codepoint` to make it clear that it's just a Unicode codepoint, then the iterator would be just `.codepoints()` and everything would be "solved".
Wouldn't the *really* correct thing to do here be having the indexing be based on graphemes/visible characters? The "human" unit of a string is the glyph, which [doesn't correspond 1:1 to codepoints (aka `char`) at all](http://www.eeemo.net/). &gt; an array of characters, and it should just work This is a leaky abstraction from the point of view of performance *and* semantics.
But slice() would also exist, right? I do think it leaves open the problem of slice misuse then, in which it is thought of as codepoints() while it's not, since it operates on bytes and not codepoints. To actually make secure programming easier, slice() would need to be renamed to (say) bytes() to make the expectations of non-omniscient programmers match reality.
Well rename them too: `.slice_bytes`, `.slice_codepoints`, `.slice_glyphs`. (Covered by [#10044](https://github.com/mozilla/rust/issues/10044).)
One Idea I had that is somewhat similar to this one is to move all the duplicated functionality (`slice`, `len`...) from the main string type into different "views" of it: ``` "foo".slice(1,2) -&gt; "foo".utf8().slice(1,2) ``` ``` "foo".slice_chars(1,2) -&gt; "foo".chars().slice(1,2) ``` Maybe we could even use the actual iterators for it.
The part of this proposal that asks for operations taking character indices is asinine, since it is not possible to efficiently access a string at arbitrary character positions. Inefficient access can and should be done using iterators, which is the correct pattern in use since the C++ STL was invented. Otherwise, you'll have novice programmers writing shitty code without realizing it because they think you can magically access the nth character of an UTF-8 string. Consider for instance the disastrous outcome of someone iterating on a string with a for loop on range(s.char_len()) instead of with a char iterator, leading to catastrophic O(n^2 ) runtime, caused by the absurd char_len() and char_at() methods being proposed. 
I am currently working on methods for iterating and thus counting grapheme clusters. Will publish a draft soonish.
'asinine' and 'absurd' come across very harshly here. I largely agree with your basic point but the question of providing convenience functions or not doesn't rise to the level of terribleness you're ascribing to it. Even if it did, there are more constructive ways to say so. 
That's a very good point...
Yes, in fact, the first half of it [already landed](https://github.com/mozilla/rust/pull/12491). Once [the rest](https://github.com/mozilla/rust/pull/12610) lands, it will just be `.borrow_mut()`. The idea being the RefCell shouldn't implement this sugar because it would create invisible runtime failure, which is a problem with `@T` that we want to avoid.
I think `ucs4` would be a nicer name.
&gt; When people are manipulating strings they want code points, if they wanted bytes they would be manipulating bytes. I don't think this is really true. If you're ever indexing or splitting by code point, your string handling is most likely incorrect. Code points don't map 1:1 to user-perceived characters.
These methods are rarely correct anyway. Concatenation, collation, formatting and comparisons are sensible operations to perform on Unicode strings. It's not really possible to do correct indexing, slicing and splitting on text.
Green tasks are cooperatively scheduled and automatic yield points are never going to be inserted. The only yield points are inside certain standard library functions.
Something else that is missing is a `Send`able version of a `RefCell`. I am aware of `RWArc`, but I wanted something that was non-blocking. It would be nice if `RWArc` had something like `try_read` and `try_write`. I ended up making [ArcRefCell](https://gist.github.com/rlepidi/9369457) for this. (Open to feedback!)
I've personally thought Go's choice of `rune` is quite neat for several reasons. It is not an user-perceived character nor a byte; it is a basic unit of Unicode string nevertheless and thus its name is concise enough; it has some notable precedents (BSD runes); it still relates to the text (runic scripts).
Go has smaller translation units than Rust does in general. Also, Rust even at -O0 has a lot more sophisticated code generation than Go does, in that it generates SSA-based IR. This is an entire step that Go doesn't perform. Changing this would probably require writing an entirely new backend.
`rune` is nice in that it avoids using "code point", which is not quite correct since we want to exclude surrogate code points. The set of code points excluding surrogates is called [Unicode scalar values](http://www.unicode.org/glossary/#unicode_scalar_value), but nobody uses that term.
Thank you for the explanation!
I look at it the other way: *Great minds think alike.*
I find `rune` very unfortunate actually. If I had to intuitively decide whether `rune` represents a `code point` or a `glyph`, I would *obviously* go with `glyph`... and fail.
Nice!
Oh nice... does this mean a 4th level I am unaware of or would the recommendation still be to manipulate grapheme clusters ?
&gt; Any sort of character index requires a linear search from the start of the string. Maybe, maybe not. Let me introduce you to [Fenwick Trees][1]: *Given a table of elements, it is sometimes desirable to calculate the running total of values up to each index according to some associative binary operation (addition on integers, for example).* This means that, as long as you are willing to build (and maintain) a parallel array, you can actually index (and change indices) in O(log N). Is this a panacea ? No, of course not, however when you present *alternative representations*, it could make sense to present an indexed representation using a Fenwick Tree behind the scenes. [1]: http://en.wikipedia.org/wiki/Fenwick_tree
If you're writing a console application with alignment and grid-based indexing, then using grapheme clusters is fine as long as you take zero-width and double-width code points into account. Strings should pretty much always be treated as black boxes otherwise. You can safely concatenate strings, make use of format strings or do collation. A search-and-replace is just a replacement of one black box by another black box. If you find yourself doing manual string indexing/manipulation, then I doubt the code is correct.
The `Deref` trait, which has half-landed, makes this a lot less verbose.
Yes, but *fools seldom differ*, and who am I to tell the difference?
Read the sidebar, this subreddit is for the programming language, no the game.
I really like the compact syntax of rust... its a big draw to the langauge, the fact you can design a compact syntax with the benefit of hindsight looking at other languages. Couldn't you keep @ as a user-defined shortcut. (~aswell, if you really do want to remove it :( ) Even though its just 4-&gt; 1 character, the fact it replaces a bracket of sorts is reducing nesting, that counts for a lot - and its supposed to be pervasive. you could just have a directive @=Rc; that stays in scope for a module.. making it clear its a shortcut. I would would have been happy with @ changed to default to Rc&lt;T&gt;. In the domain of games, many people are moving to a mix of C++ and C# (unity engine) They like having GC. But what if there was an option of staying in one language where the GC *was* very easy to use? 
/r/playrust
From my copy of stdio(3) on Ubuntu 12.04, stderr is always line buffered by default; stdout is line buffered if connected to a tty and fully buffered otherwise. 
LOL
You're pretty much spot on with everything. :) (And yeah, this is partly about DST) The literal form of a fixed sized arrays is indeed just something like `[1, 2, 3]`, which will infer to `[int, ..3]`. And there is also a form like `[5, ..3]`, as sugar for `[5, 5, 5]`.
I might have used the wrong terminology then. What I mean is that when I manipulate strings I usually want to do operations by character (or by word, but that's besides the point) never do I want to work by byte and risk splitting a character in half. 
Going by people confusion I think that I was using the wrong terms. The issue you opened seems spot on.
Doing operations on codepoints is similarly incorrect, e.g. calling `.replace('a', 'b')` on â̼̞̩̭̻͍ͦ̐̽ͯ̂͗ shouldn't convert the `a` codepoint into a `b`, the codepoints match but the visible characters don't.
This is pretty cool, nice job! A couple of tiny possible improvements I thought of when glancing at the code: * Implement MemUtil just as default trait methods on Mem instead of its own trait * instead of writing `[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]`, just write `[0, ..16]` -- it's a lot shorter :)
You can write `fail!("Invalid register/mode combo: Reg {} Mode {}", reg, modecode)` instead of `fail!(format!("Invalid register/mode combo: Reg {} Mode {}", reg, modecode))`. [This method](https://github.com/adwhit/uCTF-rs/blob/948304f199e2b125eb7e23ae90b785629bfea1e5/src/cpu.rs#L518) could return `&amp;'static str`, e.g. (NoArg,0b000) =&gt; "JNE", (NoArg,0b000) =&gt; "JEQ", Similarly, you can save an allocation [here](https://github.com/adwhit/uCTF-rs/blob/948304f199e2b125eb7e23ae90b785629bfea1e5/src/cpu.rs#L554) just by removing the `~`'s on the two literals.
OP, did you find a solution for this? I'm working on a project communicating with gpsd, and I want to set a timeout if I don't get data back from the daemon by the time I want to terminate the app. (If it can't get a GPS fix, it will just sit with the connection open without writing anything forever.) I'd love to see a select() function that could operate against both std::comm::Port and Stream instances, but I have no idea how complicated that would be.
&gt; I know, that's why I listed them both (BTW, I took your saying "bytes() and either cpts() or cpoints(), **or** glyphs()" (my emphasis) to mean only having 2 methods, either (`bytes` and `codepoints`) or (`bytes` and `glyphs`).)
Well yes, you have multi-byte UTF chars, but that doesn't mean you're indexing with character indexes. If you're consuming characters off the front of the string, you still use byte indexes, you just calculate the size of each character in bytes.
mkdocs looks like a wiki, without the benefit of being an actual wiki. Everything's in markdown, but you have to constantly rebuild it? And if sphinx was used my make the python docs..that's terrible too. I personally don't mind a github page if it's a small library with only a handful of methods. They can all be documented in the README.md. This of course wouldn't work for Rust. I'm personally a big fan of php.net's documentation, but PHP isn't very hierarchical like other languages (lots of classes that inherit from other classes or interfaces), so something a bit more like MSDN might work better. I like to see one or two sentences describing what the function does, then I like to see the function signature, then an explanation of each of the parameters and expected return value. And then a blurb that explains what the function does in more detail, possibly with some example code, and any exceptions or edge cases. Documentation should be easy to navigate, with a search feature that actually works, and each page should be as short as possible. I don't want to have to read an essay to get the information I need just to call the function.
Great work!
See also: http://stackoverflow.com/questions/22210164/image-reading-writing-libraries
See [#12730](https://github.com/mozilla/rust/issues/12730) for your bikeshedding needs.
It’s not a fork bomb, because it doesn’t fork (no new process) and the program fail with a stack overflow.
But this is the boring part! [Auto-deref going through Deref overloads (e.g. smart pointers)](https://github.com/mozilla/rust/pull/12610) is where it gets good.
I think it is very confusing for -g1 and -g 1 to do different things from a UI standpoint, so I think libgetopts is in the right here and your solution is the best that can be done while keeping the interface the same. That said, maybe it's best to change the interface to mimic the -O, --opt-level option: i.e. -g takes no arguments, and is equivalent to --debug-level 2 (or --debuginfo 2).
Are there also any thoughts to allow overloaded methods based on argument types, or is that explicitly not planned?
I'm working on it! (https://github.com/ksev/pix). Not much there at the moment. But the idea is to have the basic types for imaging and the codecs in that library and then have all the image manipulation routines in a not yet invented library.
It is my greatest nightmare that anyone start thinking that Rust is "C meets Haskell". Our ultimate all-encompassing goal here is to make low-level programming safer, not to make high-level programming more bracey. Maximal expressiveness is not a goal, and maximal concision is likewise not a goal.
I would be fine with either of those two combinations, although all three would certainly be better. The idea is to at least separate bytes and higher-level structures.
&gt; Keyword args would be nice and I guess they could happen at some point, since they'd be resolved at compile time. However, they're not very useful without also having default values for args and the implementation for those is not as straightforward. could a limited form be implemented early - eg defaults that are only constants Option&lt;T&gt;=None, or various other enum selectors. Defaults can also be considered educational, in that they show you a valid function call.. it can jog your memory. It was strange to start going back to having basically 2 variants of the same funciotn call... like CreateWindow and CreateWindowEx .. and you know one calls the other int he background with some params. Well, the absence of either isn't a showstopper for me - there's enough about the language right now to keep my interest - but it definintely felt like a step back initially and I can easily imagine many C++ programmers seeing that and concluding 'no, i'll stick with c++..' C++ of course has the variadic templates now which are very powerful
I had a look at the compiler source and had put default argument expressions into the AST, but I didn't know the remaining stages well enough to actually implement the feature, which was a shame.. - and I was discouraged from submitting that; I'd hoped that by making them available in the AST someone else with a similar interest could take the next step in implementing it... "continuous integration".. you break a change up into the smallest stages to help everyone stay in sync - but i've long since not maintained my branch :(
Well, fuck that rule, but all others are great.
I did not know that one :D
It's making progress on a pain-point though, so it's great news :)
Interesting. This is a bit over my head, but does return-value optimization (RVO) help at all here? Rust guarantees RVO, so if you have something like this: fn foo() { let x = bar(); } #[inline(never)] fn bar() -&gt; SomeStruct { return SomeStruct { y: 0, z: 1 }; } ...then `x` will be constructed directly in `foo`'s own stack frame, rather than being copied into it. The way that Rust guarantees this is that the signature of every Rust function contains an implicit parameter which is a pointer into which to write its return value, which sounds sort of like the "write-only" parameter that you're suggesting. But I have no idea what effect this has on the cache.
I've often thought a write-only reference would be useful, but /u/nikomatsakis has said that there are many problems with them. I don't entirely know all of them. One *huge* problem is that if the referenced type has a destructor, you can't avoid reading it... you need to call its destructor before you overwrite it.
well straight away is that a potentiall helpful compile time assertion: "ERROR can't assume type with a destructor is write-only, must be POD"
&gt; It is my greatest nightmare that anyone start thinking that Rust is "C meets Haskell". Why? &gt; Our ultimate all-encompassing goal here is to make low-level programming safer ... Maximal expressiveness is not a goal If we take expressiveness to mean "the range of things you can express in safe code" (i.e. without needing to resort to `unsafe`), then these two are the *same* thing. &gt; maximal concision is likewise not a goal I can agree with this bit. :)
Indeed. Could be useful, if limited.
Right, and ML is vehemently not Haskell. :)
Another couple of common patterns: - `-O` = `--opt-level=2`, `-O&lt;number&gt;` = `--opt-level=&lt;number&gt;` - `-v` = `--verbose` = `--verbose=1`, `-vv` = `--verbose=2`, `-vvv` = `--verbose=3`, &amp;c. Evidently being able to handle an optional number on an option (short or long) is desirable.
I, for one, am not in favor for this idea. I don't think the creator of the struct should care about the order in which the fields are placed (save a few limited cases), and reordering the fields in a struct in a library (say, to optimize padding) shouldn't break existing code that relies on that struct.
Ha! At the time, I didn't really know that ;)
 // Returns the top-left corner of the square in which the given point is pub fn get_corner(x: int, y: int) -&gt; (int, int) { match ((x, y)) { (a, b) if a &lt; 3 &amp;&amp; b &lt; 3 =&gt; { (0, 0) } (a, b) if 3 &lt;= a &amp;&amp; a &lt; 6 &amp;&amp; b &lt; 3 =&gt; { (3, 0) } (a, b) if 6 &lt;= a &amp;&amp; b &lt; 3 =&gt; { (6, 0) } (a, b) if a &lt; 3 &amp;&amp; 3 &lt;= b &amp;&amp; b &lt; 6 =&gt; { (0, 3) } ... Why not a simple if-else chain instead of a match with conditions?
RVO is of course generally helpful, - but this is about specifically overwriting existing buffers at existing physical adresses One thing that we've had in games programming is creating data at physical adresses with different caching hints- this is on consoles with unified memory. To create and update GPU resources with the cpu, you needed to write to memory pages that are flagged to always write to main memory. Otherwise the data could stay in the cache, and not be seen.
Match allows writing many conditions without having to have lots of `{}`'s and general noise. Although, I'd write the match chain as match () { _ =&gt; if x &lt; 3 &amp;&amp; y &lt; 3 =&gt; (0, 0), _ =&gt; if 3 &lt;= x &amp;&amp; x &lt; 6 &amp;&amp; y &lt; 3 =&gt; (3, 0) // ... } However, it seems the two coordinates are independent, meaning that `((x / 3) * 3, (y / 3) * 3)` would work as the return value (it rounds down to multiples of 3).
I definitely agree with the colons! I feel like colons are for types, and = is for variables/data. But as you say, it's a pretty small point.
/r/playrust
The hashmap type has [a `keys` iterator](http://static.rust-lang.org/doc/master/collections/hashmap/struct.HashMap.html#method.keys), so you can use that to get some keys. I don't think we have an email library yet, unfortunately. 
I think I only learned it on reddit, kind of blew my mind with how poignant it is!
I put together an Android.mk for the native-activity sample, which appears to work for librusty_android.a built with rustc 0.9. Hopefully it'll work for your version as well: LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := rust-prebuilt LOCAL_SRC_FILES := librusty_android.a include $(PREBUILT_STATIC_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := native-activity LOCAL_SRC_FILES := main.c LOCAL_LDLIBS := -llog -landroid -lEGL -lGLESv1_CM LOCAL_STATIC_LIBRARIES := android_native_app_glue rust-prebuilt include $(BUILD_SHARED_LIBRARY) $(call import-module,android/native_app_glue) To build: rustc --target=arm-linux-androideabi hello_android.rs --android-cross-path=/opt/ndk-standalone-arm/ --staticlib -o rusty_android # (same options as you had) $ANDROID_NDK_ROOT/ndk-build ndk-build V=1 output: rm -f /home/ME/workspace/native-activity/libs/armeabi/lib*.so /home/ME/workspace/native-activity/libs/armeabi-v7a/lib*.so /home/ME/workspace/native-activity/libs/mips/lib*.so /home/ME/workspace/native-activity/libs/x86/lib*.so rm -f /home/ME/workspace/native-activity/libs/armeabi/gdbserver /home/ME/workspace/native-activity/libs/armeabi-v7a/gdbserver /home/ME/workspace/native-activity/libs/mips/gdbserver /home/ME/workspace/native-activity/libs/x86/gdbserver rm -f /home/ME/workspace/native-activity/libs/armeabi/gdb.setup /home/ME/workspace/native-activity/libs/armeabi-v7a/gdb.setup /home/ME/workspace/native-activity/libs/mips/gdb.setup /home/ME/workspace/native-activity/libs/x86/gdb.setup [armeabi] Compile thumb : native-activity &lt;= main.c /opt/android-ndk/android-ndk-r9b/toolchains/arm-linux-androideabi-4.6/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -MMD -MP -MF /home/ME/workspace/native-activity/obj/local/armeabi/objs/native-activity/main.o.d -fpic -ffunction-sections -funwind-tables -fstack-protector -no-canonical-prefixes -march=armv5te -mtune=xscale -msoft-float -mthumb -Os -g -DNDEBUG -fomit-frame-pointer -fno-strict-aliasing -finline-limit=64 -I/opt/android-ndk/android-ndk-r9b/sources/android/native_app_glue -I/home/ME/workspace/native-activity/jni -DANDROID -Wa,--noexecstack -Wformat -Werror=format-security -I/opt/android-ndk/android-ndk-r9b/platforms/android-9/arch-arm/usr/include -c /home/ME/workspace/native-activity/jni/main.c -o /home/ME/workspace/native-activity/obj/local/armeabi/objs/native-activity/main.o [armeabi] Compile thumb : android_native_app_glue &lt;= android_native_app_glue.c /opt/android-ndk/android-ndk-r9b/toolchains/arm-linux-androideabi-4.6/prebuilt/linux-x86_64/bin/arm-linux-androideabi-gcc -MMD -MP -MF /home/ME/workspace/native-activity/obj/local/armeabi/objs/android_native_app_glue/android_native_app_glue.o.d -fpic -ffunction-sections -funwind-tables -fstack-protector -no-canonical-prefixes -march=armv5te -mtune=xscale -msoft-float -mthumb -Os -g -DNDEBUG -fomit-frame-pointer -fno-strict-aliasing -finline-limit=64 -I/opt/android-ndk/android-ndk-r9b/sources/android/native_app_glue -DANDROID -Wa,--noexecstack -Wformat -Werror=format-security -I/opt/android-ndk/android-ndk-r9b/platforms/android-9/arch-arm/usr/include -c /opt/android-ndk/android-ndk-r9b/sources/android/native_app_glue/android_native_app_glue.c -o /home/ME/workspace/native-activity/obj/local/armeabi/objs/android_native_app_glue/android_native_app_glue.o [armeabi] StaticLibrary : libandroid_native_app_glue.a rm -f /home/ME/workspace/native-activity/obj/local/armeabi/libandroid_native_app_glue.a /opt/android-ndk/android-ndk-r9b/toolchains/arm-linux-androideabi-4.6/prebuilt/linux-x86_64/bin/arm-linux-androideabi-ar crs /home/ME/workspace/native-activity/obj/local/armeabi/libandroid_native_app_glue.a /home/ME/workspace/native-activity/obj/local/armeabi/objs/android_native_app_glue/android_native_app_glue.o [armeabi] SharedLibrary : libnative-activity.so /opt/android-ndk/android-ndk-r9b/toolchains/arm-linux-androideabi-4.6/prebuilt/linux-x86_64/bin/arm-linux-androideabi-g++ -Wl,-soname,libnative-activity.so -shared --sysroot=/opt/android-ndk/android-ndk-r9b/platforms/android-9/arch-arm /home/ME/workspace/native-activity/obj/local/armeabi/objs/native-activity/main.o /home/ME/workspace/native-activity/obj/local/armeabi/libandroid_native_app_glue.a /home/ME/workspace/native-activity/jni/librusty_android.a -lgcc -no-canonical-prefixes -Wl,--no-undefined -Wl,-z,noexecstack -Wl,-z,relro -Wl,-z,now -L/opt/android-ndk/android-ndk-r9b/platforms/android-9/arch-arm/usr/lib -llog -landroid -lEGL -lGLESv1_CM -llog -lc -lm -o /home/ME/workspace/native-activity/obj/local/armeabi/libnative-activity.so [armeabi] Install : libnative-activity.so =&gt; libs/armeabi/libnative-activity.so install -p /home/ME/workspace/native-activity/obj/local/armeabi/libnative-activity.so /home/ME/workspace/native-activity/libs/armeabi/libnative-activity.so /opt/android-ndk/android-ndk-r9b/toolchains/arm-linux-androideabi-4.6/prebuilt/linux-x86_64/bin/arm-linux-androideabi-strip --strip-unneeded /home/ME/workspace/native-activity/libs/armeabi/libnative-activity.so adb logcat output: I/native-activity(17745): ****rust says 99****
Wow cool! Does this add much size to the final apk?
Obviously such a write-only pointer would point to uninitialized memory until it is destroyed by an assignment, so there is no such issue and no concept of "overwrite". If you a "write-only" pointer to an existing value, just wrap an &amp;mut in a struct that only provides an assignment operation. 
-----&gt; /r/playrust &lt;-----
Oh, a server? is it using rust-http or a new library?
The easiest approach is for the controller to own the views and model and tell the views to update (giving them a pointer to the model) every time the model changes (use an accessor that returns a RAII object that gives a borrowed pointer to the model and updates the views on destruction if an &amp;mut was requested). Anyway, with 3D graphics you need to redraw everything every frame anyway (assuming you can move the camera), so there is no need for anything beyond a draw loop. 
I'm not aware of anything being written down, just /u/nikomatsakis' thoughts on rewriting the type checking pass to transform the AST into a typed CFG (Control Flow Graph). The AST wouldn't have to change much, but the CFG could, in this case, have multiple instantiations of default argument values (which can't really be static values but rather expressions which may include previous arguments).
What I like about Rust is it's expressiveness compared to other languages. And I don't understand why Rust is for low level programming and not high level programming. As far as I know it just claims to be fast. Rust gives you the tools to be fast but lacks some high level concepts (like F#'s computation expressions). Of course most abstractions are slower than doing it "low level" but nobody would be forced to use them, as Rust is not Haskell. In my experience there are only a few critical paths in a software that need to be really fast. In all other parts I would like to use abstractions that give me even more safety. Having to use Rust for the fast parts and another language for the rest wouldn't be a step forward. BTW, the mixture of Rust's programming style makes me think, it is a general purpose language, as C++ is. 
That's certainly an easy approach to do it and probably works in simple cases. Although I fear it does not work any more, once you have multiple controllers for a model. (That is if I understood your suggestion correctly.) Say you have a pop-up menu and keyboard input, which can act on your given model (i.e. are controllers). Then the ownership relation between models and controllers is not so clear any more. Besides, dispatching all the model updates through one big controller object unnecessarily centralizes the whole communication logic.
Well, I'm experimenting with a video game and have used both approaches I described and they work for my simple example. So I'm not really looking for the solution to a concrete problem. I merely want to hear what approaches other people in similar situations have used and what kinds of problems one might encounter down the road.
Do you have any experience with that? How does it work out in practice?
I feel like colons are "reversed" in this instance. The label should be on the right if using colons, just like types are on the right. More importantly the value should be on the left for colon to make sense. You're saying "this value belongs to this /thing/". With equals I'd never get confused.
I don't think I understand what you mean.
So what happens if you write rustc -O lib.rs ? Won't lib.rs be wrongly interpreted as the argument to -O? (sorry, can't try it out myself right now. No rustc available)
You are right sir! I hadn't seen it...
tangentially i want to ask about the changes of struct inheritance going in; i've heard a lot of people dont want it because of OOP horrors. But from my POV, single-inheritance is something I could use easily in ASM, so IMO it makes sense to have it in a low level langauge. Makes code easier to navigate without an IDE (which is a big deal at the minute IMO)
one thing that haskell programming seems to do is mminimzie the number of named symbols required to do something; if you use a closure, you're naming a variable and referencing it. Maybe you could have some halfway house, i think scala has something where you can make a function call with '_' and that represents an argumment yet to come. add(a,_) .. |x|add(a,x) or how about rusts' .. add(a).. still - I realise what's there now is already good - better than having to write "[](auto x){return ...}" like in c++.
&gt; include $(CLEAR_VARS) LOCAL_MODULE := rust-prebuilt LOCAL_SRC_FILES := librusty_android.a Thanks! This works! Tthis might make sense because at one point, I'd messed with the makefile and had this line in there - but not the remaingin parts - later i thought it was useless and removed it, or it wasn't actually working right. Seems you know the android.mk better than me. so it turns 'librusty_android.a' into 'rust-prebuilt' and thats the step i was missing. I'm guessing that $include PREBUILT_STATIC LIBRARY instantiates a load of dependancies to do that, based on the previous vars i'd wondered if it was doing sometihng like signing or checksumming the code in that library..
The way C# handles this seemed relevant to the discussion. [This stackoverflow question](http://stackoverflow.com/questions/697055/c-sharp-and-utf-16-characters) seemed like a good starting point because it shows a corner case and the answers link to some solutions to iterating Unicode strings by character. I also think that we should give more attention to [this reply](http://www.reddit.com/r/rust/comments/1zlq21/should_rust_be_more_careful_with_unicode/cfuve12) by /u/the-fritz 
First, you may want to ditch MVC and go for a data-oriented component/entity system. It will give you better performance due to better cache usage, is more flexible, and helps decouple your code. Plus all the cool kids are doing it. To be honest, the classic MVC pattern is [not typical for games](http://stackoverflow.com/questions/2210026/is-the-mvc-design-pattern-used-in-commercial-computer-games). But if you insist on MVC, an approach you can take is to use a `RefCell` (single threaded) or a `RWArc` (able to send across tasks/threads) and have shared data stored in there. Ownership isn't as big of a question here, because the owner of the data is the `RefCell` or `RWArc`.
posix getopt() doesn't support optional arguments at all. One could think up a getopt interface similar to a parser, with some lookahead, where the user decides whether a 'token' is consumed as an argument or not. It would be an interesting experiment whether one could come up with something usable there.
What are the chances of this happening then?
As ryeguy said, you don't use Model View Controller in games, as much as in graphical application. Usually the MVC is too heavy for games, too many layers of indirection, without too many benefits. However if your game has several 'canonical' representations, like for example your data can be viewed by a 3D or a 2D (like for example if Natural selection had a first person view for the players and a 2D tactical map for commanders), it would be a whole different thing. http://stackoverflow.com/questions/2210026/is-the-mvc-design-pattern-used-in-commercial-computer-games#2210098 Also make sure you check out [this](http://gameprogrammingpatterns.com/) if you haven't already. 
I took the time to write new grammar rules to better support Rust in the Atom editor. There a still some small glitches in the syntax highlighting, but as a whole it looks usable. Feel free to improve it :)
... but would probably introduce some more.
I am sceptic about this editor. It has the overhead of node.js (struggles to process large files and loose of efficiency) yet they seem to be willing to sell it. I'd much rather stick with Sublime. Hopefully its dev comes out of silence.
I'm usually the first who dumps a slow editor (I strongly dislike all the Java-based IDEs which respond too slow for my taste). I'm also a happy Sublime customer. Still, Atom is very flexible (great concept) and surprisingly fast (at least for average code files)
I'm pretty sure he was referring to Sublime Text itself, not the Rust package. Dev activity on the editor has slowed from what I hear.
What ambiguities does `:` have and which ones would `=`? :) (I'd also be in favor of the latter, all else equal.)
Awesome, thanks! I was wondering how long it'd be before someone did it :)
`&amp;mut` fields are not implicitly copyable. What you need to do is re-borrow it with `&amp;mut *self.a`.
ah, but in real case I have io::Writer in a, I can't dereference that
Thanks for the link to Game Programming Patterns. That's certainly a helpful resource. Now I'm not sure whether I like this mixing of model data and graphical representation. After all, if you do multiplayer, you need to have a pure model for the server, don't you? Or have I misinterpreted the implication of MVC being too heavy?
You could return a `&amp;'a &amp;mut io::Writer`. Other than that, I'm not sure if there's any way to re-borrow a mutable trait object reference. It's probably worth [filing an issue](https://github.com/mozilla/rust/issues/new) about this, because it seems like there should be some way to do this.
I did some game programming, but admittedly nothing multi-player. But from what I've heard from the guys that do make games for living is this: Your only wish is for the game to feel smooth. For that to happen in multi-player for any reasonably complex game, the client only sends minimal amount of data - basically what actions the player took (e.g. pressed and hold key 'D' for 1 second, pressed attack button, etc.), because you want to minimize the amount of traffic your game makes. For that reason, your client has almost exact copy of server-side logic. I'm not sure what pure model of a server means? You probably have a mini server logic inside your client, that takes yours and opponents input and simulates what the server will reply, then compares the result with servers and does a rewind every time your own version of world differs from server's version of the world. MVC is too heavy for some GUI application's as well, because your workflow needs to pass through at least three layers. That's why some applications that work with performance sensitive information take Model - Delegate route. Model (on change)=&gt; Controller (updates)=&gt; View
It now appears to have been filed as [issue 12755](https://github.com/mozilla/rust/issues/12755).
yes, I jut did it :)
ty for help, don't really know how to get double borrow from self.b to compile
This is for Rust the programming language, you are looking for /r/playrust
I often type = and need to change it to :
Ah right, that isn't going to work. But if `self.a` is a `&amp;mut io::Writer`, what exactly is `self.b`? It can't be an `io:Writer` because that's a trait. However, `io::Writer` is actually implemented for `&amp;mut io::Writer`, so you should be able to leave the return type alone as `&amp;'a mut io::Writer` and just cast. Assuming `self.b` is a `io::MemWriter` for the moment (because that's a concrete implementation of `io::Writer`, this actually works): pub struct Foo&lt;'a&gt; { priv a: &amp;'a mut io::Writer, priv b: io::MemWriter, priv take_a: bool } impl&lt;'a&gt; Foo&lt;'a&gt; { fn take(&amp;'a mut self) -&gt; &amp;'a mut io::Writer { if self.take_a { &amp;mut self.a as &amp;'a mut io::Writer } else { &amp;mut self.b as &amp;'a mut io::Writer } } }
There have been some proposed syntaxes for various things that have been sunk by the `:` separator (can't remember a specific one off the top of my head), and I think `=` would at least mean that if foo { a = b } could parse as either if (foo { a = b }) ... or if (foo) { a = b }
Awsome, wouldn't even think of that. Not a general solution but better than not being able to continue :)
You should consider to learn Emacs, then. 
Actually, I'm a vim person. I use vim a lot in the terminal via ssh as well as locally. For larger projects I however prefer an editor with a local GUI. (Hard to explain why, maybe just my personal taste). I tried Emacs in the past, but never got used to it though.
I'm aware of existing attempts at QuickCheck in Rust: [dbp/rust-quickcheck](https://github.com/dbp/rust-quickcheck) and [blake2-ppc/qc.rs](https://github.com/blake2-ppc/qc.rs). But AFAIK, the former doesn't do shrinking and the latter approach won't work any more since heap closures have been removed (right? i think?). Also, manually pushing thunks [gets not-so-fun](https://github.com/blake2-ppc/qc.rs/blob/master/shrink.rs#L202).
It's mistaken to think that Rust traits were originally inspired by Haskell. The initial inspiration, all those years ago, was the original [traits paper](http://scg.unibe.ch/archive/papers/Scha03aTraits.pdf). That Rust's traits design has since tended towards resembling typeclasses is mostly a case of convergent evolution (though it didn't take long for the devs to realize the emerging similarities with Haskell).
There is `rustc --pretty` but it needs a lot of love.
Oh I didn't know that. So I obviously haven't read the paper yet. For what is worth, looking at trait properties def in pag 2 and 3 it does immediately make me think of Haskell classes though. Given the paper is from 2003, I'm surprised it doesn't mention Haskell at all. 
Indeed, me too. I'm very very skeptical. Particularly given that Haskell type classes [were introduced in 1989](http://www.haskell.org/haskellwiki/Research_papers/Type_systems#Type_classes). If this is indeed not the case, someone should probably update [Rust's Wikipedia page](http://en.wikipedia.org/wiki/Rust_\(programming_language\)#Description): &gt; The type system supports a mechanism similar to type classes, called 'traits', inspired directly by the Haskell language.
Vim does have an official GTK+ frontend. Although you don't get much for using it beyond a nicer cursor, support for more than 256 colours and optional sloppy focus.
&gt; `~Iterator` not being able to call `map()` seems exceedingly strange to me. The problem is apparently that trait methods that take self by-value cannot be called on trait objects. I can understand not being able to call these on `&amp;Trait` objects but it seems that it should work on `~Trait` objects. I consider this a bug, especially since the error message is misleading. Hmm. I guess that would make the world a bit saner if true. Although, I wonder if [#5087](https://github.com/mozilla/rust/issues/5087) is related? (But maybe not, since I can still, e.g., call `to_owned_vec` on an `~Iterator` object.) &gt; Of course, if that were fixed, `map()` still wouldn't work because the return type depends on `Self`, which isn't known. However, this issue has the potential workaround of defining your own trait implemented in terms of `Iterator` that provides a `map()` variant that returns `~Iterator&lt;T&gt;`. Ah, right. I tried messing around with this idea: fn hmm2&lt;'r, T: Iterator&lt;A&gt;, A, B&gt;(obj: T, f: 'r |A| -&gt; B) -&gt; ~Iterator:&lt;B&gt; { ~obj.map(f) as ~Iterator:&lt;B&gt; } But results in a compiler error (line 24): error: value may contain references; add `'static` bound to `A` error: value may contain references; add `'static` bound to `T` For reference, this compiles fine: fn hmm&lt;'r, T: Iterator&lt;A&gt;, A, B&gt;(obj: T, f: 'r |A| -&gt; B) -&gt; ~iter::Map&lt;'r, A, B, T&gt; { ~obj.map(f) } Which seems strange to me. Although, I must admit, the `'static` lifetime is still a bit mysterious to me outside the context of global identifiers... [#11612](https://github.com/mozilla/rust/issues/11612) looks relevant. &gt; For the record, `map()` and friends take self by-value because taking it by-reference would be problematic. For example, right now I can say `fn foo(v: ~[int]) -&gt; iter::Map&lt;int, int, vec::MoveItems&lt;int&gt;&gt; { v.move_iter().map(|x| x*2) }` but this only works because `map()` takes self by-value. Ah, OK, I think I get it. Thanks for the explanation!
For one Go is stable now. It's syntax won't lose or gain a keyword any times soon. Rust is still unstable. And while maintaining your rust program up to date, the rate of bitrot due to constant updates is large compared to any stable language. E.g. a program written 3-5 months ago that aren't update don't work at all. 
Your `hmm2()` function isn't going to work with trait objects, because you'd need a separate instance for each implementor of the trait and that can't be done at compile-time (when `hmm2()` is compiled, the implementors of the trait aren't known). Instead you'd need to define your own trait and implement it in terms of `Iterator`. But of course this can't work until the self-by-value bug is fixed. As for the error you did get, that's because the return value `~Iterator&lt;B&gt;` doesn't contain any lifetime parameters, and yet it needs to contain `f`, which does have a lifetime. The error is telling you that you need to instead say fn hmm2&lt;T: Iterator&lt;A&gt;, A, B&gt;(obj: T, f: 'static |A| -&gt; B) -&gt; ~Iterator&lt;B&gt; What `'static` means is that `f` is valid for the entire lifetime of the app, which means that `~Iterator&lt;B&gt;` doesn't need a lifetime parameter to capture it.
&gt; Your `hmm2()` function isn't going to work with trait objects, because you'd need a separate instance for each implementor of the trait and that can't be done at compile-time (when `hmm2()` is compiled, the implementors of the trait aren't known). So, my mental model of how the compiler works is that it monomorphizes functions based on their parameter types known at compile time. So for example, something like, `hmm2((~[1, 2, 3]).move_iter(), |x| x as f64)` should be instantiated for `A = int`, `B = f64` and `T = Map&lt;int, f64, MoveItems&lt;int&gt;&gt;`. (I think.) Where am I going wrong? &gt; What `'static` means is that `f` is valid for the entire lifetime of the app, which means that `~Iterator&lt;B&gt;` doesn't need a lifetime parameter to capture it. Ah. So does that limit `hmm2` to taking functions defined in the global data section? (So this would, e.g., not allow stack closures.) Or does the `'static` lifetime cause every closure passed to live forever? It seems to me like both guesses are wrong, so that leaves me confused.
It monomorphizes functions based on type parameters, yes. But in this case the type parameter is `~Iterator&lt;A&gt;`. So there's only one instance of the function for this type. And this isn't going to work because you can't call trait methods on trait objects if the trait method uses `Self` in its type signature. Your `hmm2((~[1, 2, 3]).move_iter(), |x| x * 2)` example will have `A = int`, `B = int`, and `T = ~Iterator&lt;int&gt;`. --- Regarding `'static`, it limits `hmm2()` to taking closures that don't close over any local values. Basically, closures that have no environment. You can give it a stack closure, it just can't capture anything.
Great, I get it now. Thank you so much for your patience and explaining everything. It's been a huge help!
Arbitrary region parameter should work as well, I went with `'static` because that way I didn't have to introduce a new lifetime parameter.
I think DST will probably fix this.
Yeah, eddyb suggested in IRC that it would. I can't wait until we actually have DST because it seems like it's going to solve every single problem in Rust :)
Ah, I see. Only the second one makes any sense of course. Probably gets more interesting if you write if foo { a = b } == { a = c }
Well, there could be a trailing plain scope `{ ... }` after the `if`, so that the first interpretation isn't ridiculous.
Yeah, but an `if` may only branch over a `bool` and a struct will never be one, so I don't think it would do much harm to disambiguate it to the second case? (As I said, that gets more interesting if you also consider things like #3.)
what is the ETA on this?
If you just want to start learning and tinkering, I'd just jump right in. Starting with the basics, learning the semantics. There are plenty of resources to keep track of the latest changes, such as the Rust weekly blog post that cmr does. 
I think rust almost stable now, except API.
Why not both? Afaik, current LLVM based compilers and GCC both do auto vectorization sometimes even for straight line code and adding intrinsics isn't too hard since clang has them all the infrastructure is there. GPGPU is a completely different beast, apart from currently pretty experimental things like OpenACC you'll have to write special code in CUDA/OpenCL, I'd guess it's already possible in Rust because you'll have to link against libraries anyway
So you don't think there will be many large syntax changes? Last time I tried to learn one of the loop constructs vanished on me, which was rather disorientating.
one benefit of rusts' syntax is grep works pretty well, which is a godsend in this early time without any sort of IDE eg grep -rn "impl.\*&lt;traitname&gt;.\*for.\*&lt;typename&gt;" . --include "*.rs" 
I'd start with the Rust tutorial. The biggest concepts to learn in Rust are the different pointers &amp; semantics, and lifetimes. So I'd just start reading up about simple normal pointers in C. Once you understand pointers, and understand how to use them, you can move onto the more advanced pointer types in Rust. In Rust, you don't want to use normal C-style "unsafe" pointers. Rust is all about safety. Thus, because normal pointers don't provide any safety on how many references are out there (or how many people are using the pointer), making sure the pointer doesn't contain a null value, etc.... After learning about pointers, you can start to learn about memory management. At it's core, it's pretty basic. You allocate a region of memory, use it, then free it. Again, the Rust tutorial is a great resource. You can also read the 30 minute Rust introduction written by Steve Klabnik (http://words.steveklabnik.com/a-30-minute-introduction-to-rust). However, it does skip over the basics of lower-level languages.
rustdoc now has the ability to tell you what traits a given type implements.
True enough. :) We could lexically require types - or at least user-defined types -- to begin with an uppercase character and terms to be begin lower-case to truly avoid the issue. I'm not sure why Rust doesn't do this kind of thing, rather than merely enforcing it by convention and not reaping the benefits.
From the sidebar: &gt; Anything whatsoever related to the Rust programming language: an open-source systems programming language from Mozilla, emphasizing safety, concurrency, and speed. No, this is not about Rust, the game ;)
I don't know about when the 1.0 release is scheduled, but I'm currently working on a personal project and using Rust 0.9 to write the embedded portion. 0.9 is stable and pretty well-documented so far, and I've been keeping up with the language features that were removed so that I can update to 1.0 once it's released. If you are ok with the idea that the language might change more in the future, 0.9 is stable and really fun to program with.
The trait you're thinking of is 'Show' and it has a default implementation for most types you'd use like this: #[deriving(Show)] struct EventTwo { x: int, y: int } ...but if you did want to customize it, you'd do it like this: struct StateLF; struct BlahLF { id: Uuid, state: StateLF } impl fmt::Show for BlahLF { fn fmt(&amp;self, f: &amp;mut fmt::Formatter) -&gt; fmt::Result { return write!(f.buf, "&lt;BlahLF:: {}&gt;", self.id); } } (this does seem messy compared to a simple to_string() method, but strings are actually rather tricky in rust, much closer to C style string handling than many other languages)
Have a look at http://people.mozilla.org/~lbergstrom/Korea2013/RustPatterns.pdf (Also maybe at the * pointer type in http://static.rust-lang.org/doc/0.9/rust.html#pointer-types)
This is what I wrote so far: http://adrientetar.github.io/rust-tuts/ Rust for Rubyists: http://www.rustforrubyists.com/index.html
There is an issue [here](https://github.com/mozilla/rust/pull/12524) that has recent discussion about some of the approaches to exposing simd.
I tried Rust for Rubyists, but the second code example they have doesn't seem to compile with rustc 0.9. Maybe I'll go through it again, and try and fix all the code. Could be a good exercise.
Thank you.
Ignoring history and considering the final product: if we think of Rust as "C meets Haskell" in the sense that it's a low-level Haskell (as low-level as C), what difference does that imply relative to Rust in the sense of "C meets ML" / a low-level ML? (Other than purity?)
Oh no! Let me check... Ah! I had gotten some bugfixes, but forgot to deploy them. :( Check it out again, please?
Hi! It's good to see that your new version looks a lot like mine. I used _ as the variable name, assuming that the rust compiler would discard the value. Is this not the case? :)
The reference implementation in ASM is available here : https://github.com/douglascrockford/DEC64
That was a very significant change and not undertaken lightly. The language is stabilising, features are still being added and removed, but code you write now should look pretty similar to code you'd write in 1.0
"There are 255 possible representations of zero. They are all considered to be equal." Seems a bit inefficient?
The `ToStr` trait is implemented on every type that implements `Show`, so you can use the `to_str()` method. http://static.rust-lang.org/doc/master/std/to_str/trait.ToStr.html
Not particularly. 0 is just `0 ^ e`, which is a simple shr and compare to zero. Certainly easier than making a single, unique zero.
Also see discussion on HN: https://news.ycombinator.com/item?id=7365812
The influences section of the Rust manual lists Haskell typeclasses as an influence. http://static.rust-lang.org/doc/0.9/rust.html#influences
Thank you :)
What is the motivation for a decimal floating point format in this day and age? The usual use case mentioned is monetary calculations, with the concern that rounding will produce the wrong results. But the problem there is not decimal *vs* binary, the problem is that you're using floating point! Anyone using floating point in that situation has already failed. If you're concerned about exact results, use an integer number of cents/mills (in other words, use a fixed point representation). The other historical motivation is efficiency of input and output, but that's really not a significant cost any more.
This is not the Rust you are looking for.
I don't like this argument they make that we should only have one number type. The ability to limit numbers (preferably in more ways than having floats + 4 int types) is very useful for writing safe software. You have to write more explicit checks for half values when you are dealing with something which would otherwise be an int, the possibility of bugs where you index by some infinitesimal amount off (example: your number printer renders 6 digits by default, your index is off by 0.0000001), etc. IMO, sized int types don't go far enough. Languages need explicitly ranged number types. Perhaps even completely custom number types like those which can be created using proofs in languages like Idris ("power of two" is an example which could be created trivially)
There are many tiers of SIMD use, and compilers are able to handle them to different degrees: * A single loop over a arrays that does an elementwise operation or a standard reduction (sum, min, max, etc.). LLVM can handle this today with the loop vectorizer, modulo aliasing concerns between sources and the destination and any limits on the amount of predication the compiler is willing to introduce. In theory, Rust's borrowing system is strong enough to provide aliasing information to the compiler that makes this predictable from the user's perspective. If you declare a function on arrays that takes immutable sources and a mutable destination, the destination can't alias any of the sources. * The previous kind of loop, but where the efficiency of the operation depends on available shuffles and multiple lane load/store instructions. These kinds of loops are very common in pixel math, and most SIMD instruction sets have specific instructions to deal with them. This is a trickier problem for the compiler, because even when it can recognize that a shuffle is necessary, it has to balance the cost of the shuffle against the benefits of vectorization. For 128-bit SIMD units LLVM just has huge tables mapping shuffles to instruction sequences, but this becomes intractable at higher vector widths. It doesn't help that some SIMD instruction sets have highly imbalanced costs between different shuffles. * Straight-line code that can use SIMD operations for efficiency. LLVM has this in the SLP vectorizer, and vectorizing straight-line code has some of the same difficulties with shuffles. I also think it currently doesn't support non-power-of-2 vectors by default, but this is just a bug that will be fixed. * Code written in a higher-level language with explicit vector types and SIMD intrinsics. A lot of people do this because they want to use vector types and specific instructions exposed by their ISA. This can be iffy with compilers in general, and LLVM in particular, since LLVM often does not always preserve the exact instructions or the assumed layout of the vector register file throughout the optimizer. The resulting generated code may not be what you intended when writing the intrinsics, especially if you are relying on interesting register file aliasing. There are extreme examples of this disparity in the NEON instruction set, which has instructions that are basically intended to only be used in specific algorithms, e.g. summation instruction for butterfly FFTs with even/odd register constraints that register allocators will generally handle poorly. Some instructions (e.g. the SSE 4.2 string instructions) are so specialized and specific to a given architecture that there is no point in even attempting to make a portable version of them. * Nested loop vectorization, like what is done by Fortran compilers, with an emphasis on restructuring nested loops for optimal memory access patterns. LLVM doesn't currently do any of this, and C compilers have generally not implemented this, due in different parts to poor alias information, longer compile times (less of an issue today), and the culture of C programming. In theory, Rust can give the compiler the same aliasing guarantees as Fortran, so these techniques could be implemented for Rust. I think it would be cool for Rust to be the first systems language with reliable autovectorization, while still having SIMD intrinsics for architecture-specific features. There are still some other tricky tradeoffs of SIMD use that are hard for a compiler to judge, e.g. is a 50% speedup worth it if it comes with greatly increased CPU power utilization?
This subreddit is about Rust, the programming language, not Rust, the game. Try /r/playrust
Yeah, I just dislike that, I mean no matter how you slice it 64 bits has 2^64 different possible meanings. Of course with wat you say, I guess every float spec using exponents has that weakness. Of course we could be using 0^e where e!=1 || e!=0 // (as 0^0 equals 1) as the nan or other things instead, heck the whole 1^e is a bit of a waste as well. of course making hardware use those wasted patterns may be painful 
The removal was a result from how 'proc' became the new owned closure. This changed the semantics and broke a lot of code without necessary changing the syntax. The decision was to remove 'do' to solve this problem. You can read more [here](https://github.com/mozilla/rust/issues/10815). The current direction seems to be to allow '{}' on macros, replacing 'do' with 'spawn!'. You can read more [here](https://github.com/mozilla/rust/issues/11892).
The discussion there is quite good, and they dissect it quite nicely.
Yes, and not just of zero [as Stefan Karpinski points out](https://news.ycombinator.com/item?id=7367021).
Because people don't understand and get upset by `0.2 + 0.1 != 0.3`. (Of course, as you point out, using decimal just papers over these problems, "weird" things still happen for `1.0/3.0` represented in binary, and it doesn't (&amp; can't) solve the round-off errors that are fundamental to floating point.)
Marvelous, thank you. The multi-threaded hello world, it's the second example on the first page. :)
When I think Haskell, I think laziness, purity, monads, and avoiding success at all costs. Monads we might have one day, but not before 1.0, and if we don't give them a better name than "monads" I'm forking the language. Instead of asking how Rust isn't like Haskell, I challenge you to tell me how Rust *is* like Haskell. Immutability, ADTs, and pattern matching are all lifted from ML, which far predates Haskell. In the end, Rust traits are just as much Java interfaces as they are Haskell typeclasses, and I'd be just as opposed to anyone who erroneously posited that Rust is "C meets Java".
ok that i can see, there's a big differentce between passing &amp;|| and ~|| (proc?) could you theoretically have 'do' being intended for ~|| and for sugar being intended for &amp; or something like that. the cases i'm thinking of are more like loops, so 'for' might seem nice, but then again "for from_fn(count) |x| { ...} doesn't read so well
Having a keyword just for sugar feels weird through. A macro seems more appropriate. BTW, `do` is still a reserved keyword for now in case it is decided to use it again later.
it *was* really neat though, and the nesting is helpful. If you're writing closure heavy code.. functional style.. its more significant. Some sugar out there has become very popular. foo(a,b) .. a.foo(b) .. the latter "sugar" for a function call is popular i beleive because of (i) reducing nesting and (ii). That's very much at the same level as "do" i think. It was very fresh. I wonder if you they have kept do and just made you have to write &amp; or ~.. do spawn ~{ blab blah blah} let myvec= do from_fn(40)&amp;|x|{....}.. or write 'proc', thats readable 'do spawn proc{.... }'
A proc can only be called once, while a stack closure can be called multiple times.
We also have 'loop', which is sugar for 'while true'.
I believe loop has some extra semantics though in regards to type checking. Can't remember though.
It is useful for embedded systems without floating point support. It can be useful for persistent storage particularly if you need to change the encoding for compatibility with memcmp based sorting anyway. The upcoming Sqlite 4 uses this approach. It can also potentially scale to a precision that are too large to fit in a machine word. Depending on the operations performed on it and the processor architecture, it may be more efficient then floating point. As dbaupp mentioned, there are fewer side effects.
I work on an application that handles and stores money values. We use integers for that but because we ship in different countries, and this shipping is progressive, it can be hard to define how many zeros we should save after the point. And we cannot save too much because one day we will have to ship to a country which the basic currency is expressed in billions of units. The worst being when the specs for a specific country changes. We still need to be able to read and write previously stored money values. A specific way to represent decimal values as proposed by the article would probably solve our problems.
Again, *if we're not interested in the history* as I explicitly stated, then immutability, ADTs, and pattern matching are things Rust has in common with *both* ML and Haskell, and after that Rust has type classes in common with Haskell rather than ML's module / functor system. On the other hand it's strict and impure etc., like ML. I think both are reasonable analogies. I also don't think it has much significance either way.
Ahhh yes. :)
I think it did with typestate but no longer. Not 100%
I've been learning and I don't see any reason not to jump in now. It is far enough along to write real programs if you don't mind keeping up with the changes to the language. I would probably wait until at least 1.0 before writing anything for production use. Being a metaprogramming friendly language, there is a lot to like coming from Ruby. Particularly if you have also used Erlang or Elixir. It is also a nice language for writing Ruby extensions. I would suggest going right to Rust 0.10-pre after you have the basics down instead of sticking with 0.9.
&gt;&gt; By the time LLVM can inline them, many optimization passes have &gt;&gt;already run, and they don't get very well optimized after that. That sounds very scary to me because with heavily templated/generic code you're relying on inlining/optimizing alot :( &gt;&gt; Once do was restricted like this, it was decided that it wasn't pulling its weight, and thus removed. heh. So it was reducing the demand for it then deciding there wasn't enough demand :) well I guess you'd have looked at all the implications exhastively - at a first guess i'd have thought just not making it co-erce would have prevented you from having hidden allocatoins (require writing ~||/proc() or &amp;|| explicitely.. do with an extra '&amp;' in my use cases would still have been great IMO )
You're on the wrong subreddit. This one is about Rust, the programming language, not Rust, the game.
Rust uses a region based system (which is why references have lifetimes), so it is pretty alien to most people including those with C++ experience. If you are just getting started, it is probably better to focus on other features first before really delving in to memory management since your programs will still run with pretty naive memory management. Memory management is easier to learn after you know the rest of the language well. Rust for Rubyists and the official tutorial are good introductions. You should also jump on the IRC and ask questions if you get stuck.
I'm doing the same with the back end to an Objective-C (Cocoa) personal project. I would have written this in C++, but Rust is much more fun. I don't expect to finish until at least 1.0 is out, so not a big deal to me too. No release date for 1.0, but the core developers have been saying they hope to have it out by the end of this year. 
Ditto! I'm building a data recorder for my car that will display live and/or recorded data through an iPad. The recorder runs on a Raspberry Pi, meaning that a low-overhead language and runtime (i.e. no garbage collection) that compiles well to ARM was critical for my project. I would have written it in C++, but I hate C++. Rust is wonderful.
There may be some big features added, but my impression is also that the existing features are pretty stable at this point. Most changes will be in the standard library. The standard library should be getting stable attributes on the parts of the API that are unlikely to change. Much of it may continue to be unstable when 1.0 is released.
Cool. Even though it sounds like you don't need it, I hope iOS support improves. I'm just integrating with Cocoa, so I'm fine. I think it currently only works on iOS (via a hack) without a runtime. I assume this is a medium priority since a major goal of Servo is to run on mobile devices. They already have some nice Core Foundation wrappers that make it easier to integrate with Cocoa.
Typestate was the original impetus but I believe it still has some semantic difference to `while true` from the perspective of the borrow checker. Something about being able to guarantee that the loop is entered at least once, with the alternative being to special-case the `while true` token. Whether or not that's grosser than having an extra keyword is a matter of taste.
I'm fuzzy on the details. You'd be better off asking on the mailing list.
Right. Even in the US you often want to use fractions of a penny. Using a decimal format you don't need to make assumptions like you would for a fixed point representation.
Yeah, but I think they meant just replace IEEE floats because otherwise why would they need integer conversion. I could see this DEC64 format being the only number format in an interpreted language or an embedded system with only integer hardware, but not in a compiled language. IEEE is balanced better for some uses (particularly for small floats) and is heavily entrenched.
I've got a rust giveaway too: http://static.rust-lang.org/dist/rust-0.9.tar.gz
Free? For me?? 
Especially for you
[Now with a taste of variadic generics.](https://gist.github.com/eddyb/9461271)
One of the use cases I had in mind was data-parallel iteration... do par_for(some_vector) &amp;|x| { // uses worker threads to process 'loop body' //..... }; it would be really nice to have that sort of thing look as natural as the inbuilt looping constructs 
You need to use backticks (&amp;#96;) around code segments, to stop reddit from italicising things like `*c_void' and another thing to make a char*`. (See the "formatting help" link for various hints about how to use Reddit's markdown.) Also, the `Any` bound on `T: Any` is pointless: it can just be `to_void_ptr&lt;T&gt;(&amp;T) -&gt; *c_void`. In any case, that declaration is incorrect to use with a `&amp;str`, in particular `to_void_ptr(&amp;(&amp;"foo"))` will not give a null terminated string... it won't even a pointer to the string's data. You *must* use [CString](http://static.rust-lang.org/doc/master/std/c_str/struct.CString.html) to get a null terminated C-string (the [with_c_str](http://static.rust-lang.org/doc/master/std/c_str/trait.ToCStr.html#method.with_c_str) method is helpful). In any case, do you realise that one can cast between `*`s via `as` (e.g. `x: *u32`, `x as *c_void`), and `&amp;T as *T` is valid too? See [#8144](https://github.com/mozilla/rust/issues/8144) for `offsetof`.
A hack in what way?
I believe it's mainly the (un)initialised variable checker, rather than the borrow checker (i.e. allowing the compiler to reason about how many times a certain block of code is executed); so the only difference is for regions of code before the first `break` in a `loop`, i.e. let mut a; loop { // guaranteed to be executed at least once // (i.e. a will always be initialised) a = foo(); if !condition { break } } is different to let mut a; while condition { a = foo(); } (even if `condition` is just `true` in both cases.) I believe Java special-cases `while (true) { ... }` for this analysis.
I guess you're on windows? Rust is completely free; here's the windows installer http://static.rust-lang.org/dist/rust-0.9-install.exe and the instructions for installing it https://github.com/mozilla/rust/wiki/Using-Rust-on-Windows. ^((you are looking for /r/playrust)^)
the void casts and string casts are 2 seperate issues, i was indeed using CString; i just made a helper funciton to do `.to_c_string().unwrap()` so i could write `c_str("foo")` . The code worked :) i was passing shader source and so on. r.e. *'s - yes i know - but there were times you seemed to have to write `(&amp;something..) as *T as *c_void ` 
Ah, that does make sense, thanks :)
It'd be nice if there were a way to control logging by compile-time attributes instead of always using environment variables. I know that the environment variable check is cheap, but it's not free. If I ship a production application and don't want any log statements to fire, why even generate the code to check it?
Best of luck!
You can use: if cfg!(foo) { /* logging stuff */ } The checks should be optimised away at compile time.
What would replace the string literal?
Not removing string literals entirely, just removing the sugar for boxing them on the heap.
Oh, I didn't even know you could compile Rust against iOS. I'm doing the iOS portion in Objective-C, since it's easiest and my day job. That's good to know, however.
Ooh, that looks much nicer :) Nice job.
Great, thanks! The source is all up here, if anyone reading this thread cares: http://github.com/steveklabnik/rust_for_rubyists
Thanks for the write-up. I for one enjoy reading through brain dumps like these :)
I'm looking forward to seeing more about "requirements on unsafe code". It's easy to introduce "hidden" unsafety like here: fn perfectly_unsafe() -&gt; &amp;int { unsafe { let x: *int = ptr::null(); return cast::transmute(x); } } Without knowing the implementation of this function, one can only assume that it's safe. When the programmer is responsible for not breaking any safety rules, it's all the more important to know exactly what the rules are.
See [#1906](https://github.com/mozilla/rust/issues/1906) (and related).
I speak in the theory rather than what is currently implemented. `-O` followed by anything *other* than a number would be equivalent to `--opt-level=2` and would not consume that next argument.
`_` is actually not a valid variable name—it's a special thing which means to ignore the value. (i.e. you can't get a value out of `_`, unlike in most languages where such a thing might be merely convention.)
I would argue that `loop` is the hack: the compiler is too dumb to understand things that mean a loop is always executed.
What matters is what you *can* do in Rust!
Hate to be that guy, but it's `'til`, an abbreviation of until. 
Go study your gerphics.
The `~[]` type will still exist as simply a boxed array, but will be different from the current `~[]` type in that it doesn't have a capacity. Right now `~[]` is it's own type, but after will be a composite type (a box containing an unsized array). The language is becoming more orthogonal, but loses the ability to express growable vectors, so that capability is moving into the library. Similar is happening to strings, but because the common case for strings is to just use them as literals, you'll probably continue to see `~Str` commonly (vs. `StrBuf`), whereas many use cases for `~[]` will become `Vec&lt;T&gt;`.
I can't wrap my head around that one. Could you please explain exactly what is going on? I know that variadic generics is a relatively recent addition to Rust, and presume that the `..Stack` in `&lt;T, ..Stack&gt;` is the variadic part, but I don't understand what it means or what the advantage is.
Well, if you are going to be "that guy", then you should be aware that till actually predates until, and so is not an abbreviation... Just, saying.
I would like more background regarding well-behaved iterators. I’ve skimmed the linked thread ([[rust-dev] RFC: Conventions for "well-behaved" iterators](https://mail.mozilla.org/pipermail/rust-dev/2014-February/008635.html)) as well as the earlier thread refererenced in it ([[rust-dev] Proposal for clarifying the iterator protocol](https://mail.mozilla.org/pipermail/rust-dev/2013-August/005113.html)), but I need some additional clarifications: - there is much talking about how much we want well-behaved iterators, but not so much the why - taking the need for well-behaved iterators at face value, why should it take the form of a shadow, mirror hierarchy rather than part of the trait requirements proper (aka the iteration protocol) — i.e. even if there were no actual `WellBehavedIterator&lt;T&gt;` trait, that would for all practical purposes be the net effect of such a proposal Personally I would bring up the case of `FlatMap` to justify well-behaved iterators. If I’m not mistaken the current implementation (in [`iter.rs`](http://static.rust-lang.org/doc/master/src/std/home/rustbuild/src/rust-buildbot/slave/doc/build/src/libstd/iter.rs.html#1718-1719)) relies on those, to save itself from storing additional state. (Consider a case where `next_back()` is called, populating `backiter`, and what should happen as `next()` exhausts `frontiter` and `iter` — although admittedly my understanding of `DoubleEndedIterator`s may be lacking.) To make the same point in a longer form, calling `next()` repeatedly after the first `None` corresponds to repeatedly querying if the iterator is (still) empty — an entirely pure operation. (Let us keep in mind that `fn next(&amp;mut self) -&gt; Option&lt;A&gt;;` corresponds to the fusing of the operations fn front(&amp;mut self) -&gt; A; fn empty(&amp;self) -&gt; bool; // don't need to mutate here fn next(&amp;mut self); // precondition: !empty() as it may in fact be spelled in other languages’ iteration protocols.) I would also like to know why `fuse()`/`Fuse` would be considered an acceptable alternative/workaround, as I’ve seen it mentioned. If you compose an iterator from several iterators, all of which use `fuse()`, then you get as many `bool`s. All of this to cache the pure query ‘does this iterator currently stand for an empty abstract sequence?’. OTOH if well-behaved iterators as a default might require some iterators to implement that same caching, that cost would be paid at most once (in a composition of iterators). I would also like to mention that I find it suspect that all the hand-wringing is centered around `next()`, and that `next_back()` is never mentioned.
For any who remain incredulous: http://www.quickanddirtytips.com/education/grammar/until-till-and-til Not only is "till" correct, but even if it wasn't comments that correct grammar only reduce our signal:noise ratio. adhochawk, please send a PM next time.
Oh, and I meant to say, good luck for your exams! You do an amazing job for the community, considering you have university to deal with as well.
Variadic generics haven't been added to Rust, and there's doubts they'll be in 1.0. The advantage is that you can work with variadic (variable arity) tuples instead of using tuples as a cons list.
Damnit, I was really looking forward to them. They help clean up a lot of stuff you'd have to do with trait objects.
I find it interesting that they define NaN as equal to itself when most programming languages, and even intuition, state that NaN is never equal to itself.
This sub is actually for Rust the programming language, which is completely unrelated to Rust the video game.
You are looking for /r/playrust. This is the subbreddit for Rust, the programming language.
sorry I wasn't paying much attention to it
You are looking for /r/playrustservers.
And the meta-RFC: https://github.com/rust-lang/rfcs/pull/2
This is *so* much easier to read. Nice!
what worried me was hearing that Trait Objects might be removed.. I really like the way they work now.. performance oriented code (from experience in games) avoids vtables, but with trait objects you could attach vables to existing objects and still get their versatility where needed. (eg .. world as flat arrays of specific entities, but then a general purpose entity iterator receives a trait object - the iterator can generate the type information from compile time info) The reasoning seemed to be that if a feature wasn't being used in servo , it wouldn't be justified - but might that be a premature conclusion too make. perhaps you could just add a little sugar to allow traits to assist 'manually rolled vtables' - one thing i've missed in C++ is the ability to change a vtable pointer (back in plain C, one could use a function pointer to represent some changeable state.. and extending that to a table would be useful) Another thing missing from C++ is the ablity to put data in the vtable.. "static virtual data", with a manually rolled table you could have a collection of all the objects' instances or whatever
Yes. You can force the dynamic linkage with `-Z prefer-dynamic` option. There are some other ways to reduce the binary size: * At the expense of compilation time (about 3x for my mid-sized project) you can use `-Z lto` option to enable the cross-crate optimization. This is, alas, currently broken on Windows though. ([#12471](https://github.com/mozilla/rust/issues/12471)) * Rust ships two primary runtimes (libgreen and libnative) to support the different task models, and the default runtime (libgreen) is known to be substantially larger than libnative since it contains the libuv library. If your program does not use the tasks at all (or your use of tasks is simple enough), you may [switch](http://static.rust-lang.org/doc/master/guide-runtime.html) to libnative to get rid of libuv and bindings. * You can also switch to the minimal runtime (yes, the runtime is pluggable) like [rust-core](https://github.com/thestinger/rust-core). This obviously means that you intentionally lose runtime features over the smaller binary, and your mileage may vary, but it might be a viable alternative when the binary size is prime concern.
A user on IRC requested someone post a related link to consider so I made an account. http://plan9.bell-labs.com/wiki/plan9/why_static/ Hope you find this info useful!
&gt; which blogs should i follow in order to stay tuned Almost all posts of interest get posted to /r/rust here.
Thanks for the link, however I find arguments against dynamic linking unconvincing. The posts you've pasted assume one scenario which doesn't work, but it seems to avoid going into examples which would show how SO's would be used with success ;).
Just what I needed, thanks.
&gt; what worried me was hearing that Trait Objects might be removed Where did you hear this?
From whom? Unless it was a Mozilla employee I would think that it was highly likely that it was a distorted/misinterpretated message, because I certainly haven't heard any word of trait objects disappearing...
i might have taken a comment wrongly. features do get removed though :) ok well thats reassuring , that its probably just a passing comment. What had seemed to happen with 'do' is a change was made that reduced the demand, then it was concluded 'the feature doesn't have demand' :) seems to me the design of DOM based code is constrained by external factors (scripts/plugins that already use it?) - so even if traits aren't the best way of implementing that, i hope the world doesn't conclude they're useless
A few tips from another Java/Scala dev that looked into it (briefly I admit): * learning a small measure of C will be needed, I couldn't even get regex working without it (and even then it was difficult) * lifetimes are a bit of an alien concept (too many years of "free" GC), and in the beginning you will end up just doing whatever it takes to make the code compile * as much as other members say it ain't so, the language is changing very fast, and code from yesterday may not compile today (vectors in the [] format being replaced, structs defaulting to private instead of public etc) * similarly, docs get out of date, so if you find an interesting article from a couple of months back, it's more often than not no longer applicable. The main docs mostly stay current. Edit: formatting
I'm also a Java developer and recently had a look at Rust. I think it would make sense to play with Rust for fun &amp; leisure and in order to learn new things. Just out of curiosity in case you like to answer: What's your interest in Rust as a Java developer? I was thinking of ways to combine it with Java, but I don't see much potential ...
I don't think trait objects will be removed, any more than interfaces could be removed in Java. Single inheritance is just too limiting...
Mostly i like the fact that it contains features expected for a modern program language, such as pattern matching, lambdas, etc. If i needed something like this for the jvm i would use scala, but rust also has manual memory management, which is better for game programming. But is hard to leave the java collections, they are so useful.
Thanks i'll have a look at these links.
&gt; Also i'm into game development, so i welcome resources related. - https://github.com/mozilla/rust/wiki/Computer-Graphics-and-Game-Development - https://github.com/rlane/rust-gamedev-kit – most of the libs are mine, so bug me on #rust-gamedev on irc.mozilla.org if you have issues (nick is bjz).
Interesting, it is handy to follow blogs featuring weekly language news.
I'm very timid, but i'll keep that in mind
Then you can count on me as a rustaceans (whatever it means)
Yeah. No real technical limitations since Objective-C also uses LLVM and there is no JIT or required dynamic linking. I have not tried it, but if you did want to run Rust code on iOS I think you would output LLVM bitcode and link that with your Objective-C code. I'm sure this will be fixed eventually since I think it will be a requirement for Servo when it gets off the ground a bit more. I also have no real issues using Objective-C for UI code. The safety guarantees are less applicable. Although I think it may be interesting to use Rust for canvas type UI in an OpenGL layer or any other very complex custom UI. It is pretty easy to forward events, gestures, accessibility data, etc. from Rust to a UIView or NSView. 
I'm not sure why this is so controversial here. "Guys" is considered a politically correct word for either gender in the United States. I think there may be more of a masculine meaning overseas based on a few things I found searching, so it may be best to try to use an alternate word in an international forum like this. However I hope nobody actually takes offense! It is an extremely common word for informal conversation in the US. Sounds like the international consensus is "folks" instead of "guys" ;-)
It's also worth mentioning that I personally have found it easier to work with HTML DOM and XML DOM using selectors and xpath, respectively, than raw node traversal. So having some run-time overhead for peer and parent traversals, as you suggest, would vanish if you take a query-based approach to acquire a node(s) from the DOM.
If the project has a childish name, it probably isn't a serious project. Not something to worry about in real life. Besides, if you need to struggle that hard to find a fault in an argument it isn't really a fault.
You realize this conversation is happening in the context of a discussion about efficiently *implementing* a DOM within a browser engine, and not from the perspective of a downstream user interacting with some external *model* of the DOM, right?
My mistake. Please disregard my comment.
&gt; rustaceans (whatever it means) It's a type of crustacean that likes sharp, bleeding languages.
Thanks for unfailing doing writing these updates up cmr! This is such a useful resource. Is Servo still going to get some love on your posts? I know you've been busy with exams - hope they went well btw! :)
Well, Rust has a great library for iterators. If you learn it, you'll probably not miss Java's collections.
That was my fault - I was traveling back from the Rust Workweek and didn't have a chance to get This Week in Servo to cmr. 
The restrict variants are (essentially) `&amp;` and `&amp;mut`.
ok thats interesting , and consistent i guess.. do you still get pointer-arithmetic on &amp;, i suppose you can always cast it back and forth
It's quite an ambitious project for a first, and you seem to already have it in good shape: congratz!
Thanks. I had been trying to champion Rust as a topic to talk about at our [functional programming group](http://fun.cs.tufts.edu), but was waiting for it to be more stable. I used the start of this project to introduce our group to Rust. :-) Being familiar with Haskell made learning Rust simple *except* for the linear type stuff. My mind is still kind of blown, actually. (In a good way.)
More precisely, the fundamental issue of dynamic linking is that instances of generic code and inline functions must of course be placed in the user crate. However, such generic code and inline functions can rely on internal details, which means that lots of changes to a library need to create a new library version since they would break the generic instantiations of library classes that were put in the program executable. At that point, it's often better to do static linking so you can do full LTO (i.e. global optimizations) and inline everything. Other languages either do not have generics nor inline functions (C, Go), have unsafe source-level generics and inline functions (C++) or have a JIT compiler or interpreter (Java, C#). 
&gt; You realize this conversation is happening in the context of a discussion about efficiently implementing a DOM within a browser engine, and not from the perspective of a downstream user interacting with some external model of the DOM, right? i'm suprised then that the pointer heavy version is required .. (but i've never written a browser)
Compile with `--test` and then run the binary with `--bench`. (The "correct" reference is the [testing guide](http://static.rust-lang.org/doc/master/guide-testing.html#microbenchmarking).)
The optimizations are important. Rust's safe type system though should ultimately enable all the optimizations C++ can do and more. This issue is about unsafe pointers only, where `*const` mostly just allows FFI callers to make some assumptions about what the callee will do. So my argument in the meeting was that this use case - calling the FFI with `*const` pointers - doesn't buy us much (`*restrict` apparently doesn't enable any caller-side optimizations so we determined Rust doesn't need to encode C restricted pointers). The end decision though looks like we're going to have both `*const` and `*`.
One thing I thought of just recently is having just `*const` and `*mut` and no `*` at all, so there's absolutely no ambiguity about Rust's `*` looking like `&amp;` but being mutable (under the new scheme), or, Rust's `*` looking like C's `*` but being immutable (under the old scheme).
Ty! It's working :D
&gt; The combination of preemptive multitasking and shared state generally leads to unmanageable complexity Seems to align with what we assume.
Dictionaries are descriptive, not prescriptive.
There arent amazing bindings for windowing libraries yet AFAICT. People want QT, but there are some problems for the differences between how they handle strings.. I agree though, I really wish I was smart enough to make / contribute to an editor made in rust.
Strings are the issue? I thought it would have been all those macros like `Q_OBJECT` and the signals and slots. I'm debating how hard it would be to make an interface with OpenGL. IDE's are pretty complex and stock widgets don't usually work well enough anyway.
[this thread](http://www.reddit.com/r/rust/comments/1w2o4l/qt_bindings_for_rust/) explains it well, Im not that smart about it thought.
&gt; What makes it next to impossible to bind is that Qt is not even C++. It compiles down to C++ with moc, and the result is still heavy on templates and macros. We can't even deal with pure C macros yet. Yep. That's about what I expected. That's a shame.
This might be slightly relevant if we wanted to build a Qt-like lib for Rust, what features would we need? http://woboq.com/blog/reflection-in-cpp-and-qt-moc.html [found here](http://www.reddit.com/r/programming/comments/20967a/can_qts_moc_be_replaced_by_c_reflection/)
It definitely removes any ambiguity and makes it harder to do the wrong thing.
I still don't get the argument. I think this is a difference in American English. I see evidence this word is less gender neutral oversees. 
&gt; Making learning curve less steep would be great, since it seems to be #1 marketing problem for Rust (together with "ugly syntax"), but I don't know how. I think a webpage with a series of (preferably interactive) worked examples with references/lifetimes, starting really basic and building up would be really helpful. Also, the newer [linked list section](http://static.rust-lang.org/doc/master/tutorial.html#implementing-a-linked-list) of the tutorial has scope for being extended to introduce lifetimes in a natural way (e.g. returning a reference to the first element of a linked list)... currently, they aren't introduced in the tutorial at all beyond: &gt; A reference can be borrowed to any object, and the compiler verifies that it cannot outlive the lifetime of the object. &gt; &gt; [...] &gt; &gt; For a more in-depth explanation of references and lifetimes, read the references and lifetimes guide. (And no mention of the syntax.)
My pessimistic opinion is that no amount of documentation will fix borrow checker usability marketing problem, because people don't expect documentation to be necessary. aka "You need that much documentation to explain such a basic concept?! Rust is too complex! I will use Nimrod." I really don't have the solution.
This dictionary definition came from wiktionary and was contributed by an Australian citizen. You wouldn't find this definition in a US dictionary. This does contribute to my theory that using "guys" (which is gender neutral and heavily used in the US) may be controversial in some parts of the world. 
Interesting article. Looking forward to read future posts.
This is pretty neat as I'd never heard of Robin Hood hashing before. I'm curious though, what is the benefit/reason for choosing an open addressed hash table rather than a closed hash table with collisions stored in a tree or something?
I just read through the post that this is in response to and I heavily disagree. I do not think that events are the end-all-be-all. This general attitude that if you're dealing with threads your program is unmaintainable is just wrong. I don't like the implied argument that we should only write single-threaded applications, either. If you have a language which allows shared memory, events will not save you if you use multiple worker threads. The problem expressed in the post is not a fundamental problem with threads, and the solution to that problem is certainly not events. The problem is shared memory. Rust deals with the problem very well by not allowing shared memory without unsafe {}. Rust doesn't have the combinatorial explosion the author spoke of, in my experience. Even when you're not using rust - it's very easy to opt to message passing as your default communication primitive across threads. I myself have written threaded code in C with no visible threading bugs when using message passing. It's more verbose than languages with stdlib message passing, and if you're shoving old code onto new threads it can mess up, but if you do a good job of having your code being written in a functional style (only operating on your inputs rather than global state, not side effecting) then it's not very hard to shuffle things around.
Using a tree effectively requires the keys to be orderable (which is a nonstarter), and a chained hashtable has other disadvantages e.g. [one comparison I found](http://en.wikibooks.org/wiki/Data_Structures/Hash_Tables#Open_addressing_versus_chaining) (note that the graph there is for linear probing, not robin hood hashing... and robin hood hashing isn't mentioned at all).
The article [Robin Hood Hashing should be your default Hash Table implementation](http://sebastiansylvan.com/2013/05/08/robin-hood-hashing-should-be-your-default-hash-table-implementation/) has a good argument.
That makes sense. Thanks. 
Did you read all 299 of them? :P */me idly wonders if that's the Rust issue/PR with the most comments*
Indeed, the message passing is strictly a library feature at this point. What Rust-the-language provides is a type system that makes the interface provided by the message passing library safe.
An entertaining read, even if one is not much into OS development.
Every investments has to yield some gain. We need to somehow reduce the investment and increase the gain, or make it more visible/attractive. I think Rust currently could benefit with more real-world cookbook-style examples. For example writing an iterator is simple in many languages. In Rust you have think about lifetimes and struggle with Send-trait error messages.
The old and new HashMaps have near identical protection against hashdos, they use the [cryptographically well-regarded SipHash](http://en.wikipedia.org/wiki/SipHash), and are (by default) seeded with random keys.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**SipHash**](http://en.wikipedia.org/wiki/SipHash): [](#sfw) --- &gt; &gt;__SipHash__ is a family of [pseudorandom functions](http://en.wikipedia.org/wiki/Pseudorandom_function) created by Jean-Philippe Aumasson and [Daniel J. Bernstein](http://en.wikipedia.org/wiki/Daniel_J._Bernstein) in 2012. &gt; --- ^Interesting: [^Block ^cipher](http://en.wikipedia.org/wiki/Block_cipher) ^| [^Jenkins ^hash ^function](http://en.wikipedia.org/wiki/Jenkins_hash_function) ^| [^List ^of ^hash ^functions](http://en.wikipedia.org/wiki/List_of_hash_functions) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg1han3) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg1han3)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
I recognise your name from something Go related. If so, any thoughts on Go vs Rust? (I know, I know; they're not competing etc.)
Stupid question but: I haven't found a way to pass those parameters with the rustpkg tool. I guess it's not possible then?
Just to be clear I'm neither for or against them. What are pros and cons of 'virtual fn'? What is their relation to proposals for some kind of object system (namely structs inheriting structs, traits inheriting traits and struct/trait inheriting)?
Anyone know if it is still the case that github will eventualy gc (delete) comments on outdated commits?
&gt; Send-trait error messages For implementing an iterator? I'm mildly confused by that.
There was a surprising error message that something was not sendable or did/did not imlpement the Send trait. Something about syntactic sugar and that one needs to add a dot somewhere. Don't remember right now.
&gt; Was it so hard? To get all lifetimes correct and make it compile was harder than in many other languages where you just hold GCed references to each others datastructures.
Oh, an `~Iterator&lt;..&gt;` trait object? In general one should rarely use an iterator like that: it acts more nicely if the type is not erased (i.e. not a trait object) and unboxed (i.e. no `~`). (Of course there are some instances where it is useful.)
That's what i'd love to see: a catalogue of more complex than tutorial-grade (real world) examples. When/how to implement an iterator this way, or the other way?
[Here's some of my thoughts](http://irclog.gr/#show/irc.mozilla.org/servo/76862) - I hope I'll get around to writing an RFC for that idea.
&gt; To get all lifetimes correct and make it compile was harder than in many other languages where you just hold GCed references to each others datastructures. Well implementing without lifetimes is significantly easy, just define the correct return value and implement the apropriate function (in my case `next`).
Except for a minor confusion it may cause (to which you proposed pretty good solutions) I find this quite pleasant. You have my support :D. (if that means anything)
What you described it does at the moment - "* is equivalent to const T* in C (i.e. mutable by others unsafely) - isn't bad IMO, if you're writing unsafe blocks you are having to think twice probably anyway. I've not had any problems just swapping absence of mut-&gt;const - wanting const default was a major draw to the language. [r.e. the optimizations just to clarify, i understand rust is going to optimize safe code better , which is great; its just you'll need the same control as c in unsafe blocks)
I know Mordy, he runs good servers. The RUST server is a lag festival. I was in his server when all the players started asking who they were hosting with and telling Mordy and his admins to switch to FPS something.
This is not the sub you are looking for 
Faster, more compact, fewer cache misses.
&gt; `&lt;T&gt;` same as `&lt;T: Sized&gt;` I think just the opposite. `&lt;T&gt;` should be for unsized vectors. How often do you know how big a vector will be at compile time? But maybe i'm not understanding the argument.
Hmm, Robin Hood hashes may actually avoid hashdos on its own, even if your hash function is not secure. It will spread out the collisions (and eventually hit the load factor limit). I guess insertions can get pretty slow eventually (since you have to keep walking the length of the table to "spread" something out), but you could always have a secondary criteria for growing the hash table (e.g. once you've walked more than 50 steps or something just force a resize - should really only happen on pathological data). This way you won't have to use a slow hash function as your default just to be robust to the extremely unlikely chance that someone will attack you. IMO the default hash table should use a fast hash (e.g. xxHash?). 
Your understanding point #1 is wrong. The type of `x` is the type of the element that the iterator `numbers.iter()` yields. `numbers.iter()` itself is the iterator. Your for loop there desugars into something that looks approximately like let it = &amp;mut numbers.iter(); loop { let x = match it.next() { None =&gt; break, Some(y) =&gt; y }; println!("{}" is a number!", x); } So in this case `numbers.iter()` is an iterator that conforms to `Iterator&lt;&amp;int&gt;` and `x` is therefore `&amp;int`. In the following code sample, where you use `&amp;x`, you're invoking a pattern. The variable binding in a for-loop is actually an irrefutable pattern, not just a variable name. This means it allows for the same sorts of things you can use in a `let`-binding (BTW parameter arguments are also irrefutable patterns). So in this case the `&amp;x` means that it should basically unwrap the pointer from the `&amp;int` that it's assigning here and the `x` binding itself will have the type `int`. This here only works for POD types (e.g. types that can be implicitly copied without calling `.clone()`). You'll often find this same trick used for tuples and other things. For example, if I want to iterate over your `numbers` array and keep an index of which value I'm at, I can use fn main() { let numbers = [4, 2, 7]; for (i, &amp;x) in numbers.iter().enumerate() { println!("{}: {}", i, x); } } This will print 0: 4 1: 2 2: 7 --- For #2, your guess is correct. The RHS of the assertion is an expression that has the type `&amp;int`, and it compares this `&amp;int` against the `&amp;int` that results from the `unwrap()` call. --- #3 is a bit special. The compiler has some interesting semantics regarding the lifetime of a temporary that has its expression taken. I *believe* that in the initial assignment, because the reference to the temporary is assigned to a let-binding, the temporary itself (the `X(3)`) is given a lifetime equal to the scope of the let-binding. But in the second assignment, there is no let-binding, and the temporary (the `X(5)`) is given a lifetime equal to the statement it appears in. But the variable it's being assigned to has a larger lifetime than the lifetime that this temporary was given, and therefore the assignment is illegal.
**Question 1** The type of `x` is `&amp;int` (a reference to an int). If you are ever wondering what the type of something is, try assigning it to a variable that is incorrectly typed, and observe the compiler warning. For example, add `let y: f64 = x;` to the top of the loop and notice it says `mismatched types: expected f64 but found &amp;&lt;generic integer #2&gt; (expected f64 but found &amp;-ptr)`. The reason `for &amp;x in numbers.iter()` also works is because you are now doing a pattern match on the type yielded from the iterator. It is pointless in this scenario, but it's valid. An example of more useful pattern matching is: fn main() { let numbers = [(0, 1), (1, 2), (2, 3)]; for &amp;(first, second) in numbers.iter() { println!("Got {} and {}!", first, second); } } **Question 2** I'm not entirely sure the implementation details of how this works (`0` may be put in a temporary variable behind the scenes), but it's taking a reference to `0` and comparing it against the the reference returned by `it.next().unwrap()`. If you look at the [source for that macro](http://static.rust-lang.org/doc/master/std/macros/macro.assert_eq.html), you'll see that it dereferences each reference (with `*`) and compares them, so it's really like doing `0 == 0`. **Question 3** This is a good question, I honestly don't know. 
I think you have the wrong subreddit - did you mean to go to /r/playrust? This is the subreddit for Mozilla's programming language called [rust](http://rust-lang.org).
~[T] (soon to be Vec&lt;T&gt;) would mean type T is sized, not the vector.
I like this proposal, especially because it allows to have a proper `Sized` kind bound without the annotation burden.
Not an expert, but I'm not sure that this actually prevents hashdos? If the goal of an attacker is to force hash lookups to occur in linear time rather than constant time, then what about robin hood hashing actually prevents that?
There's a significant difference between `~SpecificIterator` and `~Iterator`.
I kinda like this. (Also are the weekly meetings open to outsiders, or are you now at Mozilla, or how does this all work?)
The idea of using a `Vtable&lt;Trait&gt;` is interesting, I suspect it might be amenable to embedding *multiple* v-tables (for different traits) in the same object, which is kinda like multi-inheritance ala Java (multiple interfaces) without the issues of the dreaded diamond... as long as traits remain stateless. I wonder how traits could specify data-members at known offsets though; and even if they could, I suppose multi-inheritance for those would soon be nightmarish.
I know this is the Rust subreddit, but: &gt; and now I need to implement more operators for classes to preserve value semantics (don’t forget to implement the move assignment operator!) is just wrong. If you find yourself implementing copy/move/destruction operations on a regular basis, you are not using the language correctly. Specifically, you are probably violating the Single Responsibility Principle: - either manage a resource (which may lead to writing a clone of `unique_ptr` with some copying abilities) - or manage functionalities (copy, move and destruction are the default ones, and they work) I've played *a lot* with C++11, and writing copy/move/destructors is *rare*. Furthermore, the *Rule of Three* has been taken to heart with new rules in C++11 to avoid *forgetting* writing a special method: - declaring a copy-constructor, copy-assignment operator or destructor ? No move-constructor or move-assignment-operator are provided (your class is special) - declaring a move-constructor or move-assignment-operator ? No special method is provided (at all) So, at least, if you forget you'll be reminded at compile time. Now, of course, clearly C++ feels its age and the burden of backward compatibility; it's still incredibly verbose and much too unsafe for my taste. 
Or maybe the compiler should be improved ? Remember how Clang was so appraised when it introduced *human understandable* diagnostics, maybe we should strive to: 1. Explain things in plain English, trying to use the minimum amount of jargon possible (but using the right word help people searching on the Internet) 2. Point to the problem as precisely as possible: if you point at a symptom, and not a cause, people need to follow the analysis backward, and it's not always obvious 3. If possible, add a fix-it note (though it's not always possible) Encountering an error from the compiler is always a two steps process: - making head or tail of what the compiler is saying - thinking over your program to fix the issue and it's up to the compiler (writers) to make the first step as immediate as possible. Unfortunately, said writers might be too experimented to see the issue (because they have experienced those errors so many times they immediately understand what is said), so they will need help in pin-pointing the worst offenders.
The simplest solution is to use attributes instead of "real" syntax. You do raise interesting issues, which could be fixed on a case-by-case basis (since this is a specific layout optimization). And I did mention C++-like vtables somewhere in that conversation, maybe even interoperability with C++.
[#12238](https://github.com/mozilla/rust/pull/12238) is a good step towards 3.
Not sure about other people, but when I see Foo&lt;T&gt; I already say it aloud as "Foo for type T". Foo&lt;type T&gt; would become "Foo for type 'type T'" which feels a little awkward. Also, 'type' doesn't convey to me that this is expanding the set of acceptable Ts; a sized T is just as much a type as an unsized one. If "the point is that you would only use the type keyword when you wanted to explicitly say 'I am generalizing over all types...'", then why not 'all'? Then Foo&lt;all T&gt; translates to "Foo for all types T". Would kind of suck to lose 'all' to a keyword, but wasn't there a contextual keyword proposal floating around? 
Almost all of them!
Ah yes, my mistake.
100% Agree. Also, there's the unofficial "Rule of Zero" that it's exactly like you said: thanks to RAII you don't need to write any of the boilerplate five methods. Instead you use classes/structures which already do that for you. The only time you write those methods is when you want to do your own resource-management class AND do not want to abstract them using the built-in ones (like unique_ptr).
I'm not sure if I like this more than `unsized` or not. You are right that it makes sense since it means 'sized or unsized'. But given the context we're using it in, and that `sized` is implicit, it seems confusing not to mention anything about size.
So when you read [this blog post](http://subfictional.com/2012/07/02/language-matters-stop-using-guys-to-address-mix-gender-groups/) (from an American), what would you say? And what do you make of the fact that when surveyed, [women are a lot more likely to perceive 'guys' as gendered than men are](http://jvns.ca/blog/2013/12/27/guys-guys-guys/)? I'm generally unconvinced by someone who wants to engage in behavior that's hurtful to me and justifies hurting me by saying that the dictionary says it's okay.
I have to admit that this formulation is not obvious to me: I have no intuition for the distinction between `&lt;T&gt;` and `&lt;type T&gt;`, nor do I understand what `&lt;type T: Sized&gt;` means.
This is precisely what enums are in Rust. As an example, there is the [`Result&lt;T, E&gt;`](http://static.rust-lang.org/doc/master/std/result/enum.Result.html) type, which is either an OK value or an error: enum Result&lt;T, E&gt; { Ok(T), Err(E), } I/O error handling is now (in master) done explicitly, so that a file reader returns an `IoResult` ([`IoResult&lt;T&gt;`](http://static.rust-lang.org/doc/master/std/io/typedef.IoResult.html) is an alias for `Result&lt;T, IoError&gt;`, see also [`IoError`](http://static.rust-lang.org/doc/master/std/io/struct.IoError.html)).
I recommend upgrade to master if you want the code to run on the next 0.10 release. Rust does not have exceptions but it has Option and Result. These are enums with tuple variants. [Option](http://static.rust-lang.org/doc/master/std/option/enum.Option.html) [Result](http://static.rust-lang.org/doc/master/std/result/enum.Result.html) Edit: Fixed from 'newtypes' to 'variants'.
Those ain't called newtypes. The variants may be known as tuple variants.
I think long_void meant things like `struct Foo(int);`, but I agree that the wording was a bit confusing.
You can think of `struct Foo(int);` as just sugar for `enum Foo { Foo(int) }`. So newtypes are pretty much enums, but enums are not newtypes.
edit: nvm
Is the enum pretty much the same? I would think the enum would carry around the (useless) discriminant at runtime, whereas the "newtype struct" would only give information at compile time and would have the same size at runtime. Or perhaps the optimizer is smarter than I'm giving it credit for.
I believe it hasn't started yet (15 minutes from now).
thanks, haha, I must've misread the clock :)
&gt; I would think the enum would carry around the (useless) discriminant at runtime Nope. An enum with exactly one variant (which has one or more fields) is treated like a struct. Source: https://github.com/mozilla/rust/blob/98fa0f8/src/librustc/middle/trans/adt.rs#L185 Also, for \#ThrowbackThursday, the commit where that feature was added: https://github.com/mozilla/rust/commit/fb72be0
Julia's talk is funny!
I propose T: ?Sized or T:Sized? 
I'd write some macros or functions macro_rules! num { ($json: expr, $field: expr) =&gt; { $json.find(&amp;($field)).and_then(|thing| { match thing { Number(n) =&gt; Some(n), _ =&gt; None } }) } } let protocol_major = num!(root_obj, ~"proto_major").expect("invalid protocol response"); let protocol_minor = num!(root_obj, ~"proto_minor").expect("invalid protocol response"); // ... (At least until we get a library that manages completely optional fields automatically.)
Is there an archive link for those who missed it?
Cool, thanks! I had wondered if macros were the way to go, but I could not make heads or tails of the spec. With this example I see much more easily how it works. It's not all that different from the concept of a preprocessor.
I always love what I read in this blog, it's the real hands on approach for learning while doing. Really inspiring :)
Yeah, it was great. Good job Julia!
Thanks for detailed explanation. Regarding the assertion question, the fact that I need to dereference a `&amp;-ptr` makes me think that they are addresses of memory and thus it doesn't make sense to compare address of a literal `0` to another address that `unwrap()` returns.
Thanks for the trick to figure out the type of stuff. &gt; If you look at the source for that macro, you'll see that it dereferences each referenc Little bit confused here as the macro you linked first takes the reference of its arguments and then compares them. So basically it's doing `*(&amp;(&amp;0))`, right? 
Enums are highly optimised. In fact for any enum matching the form: enum A&lt;T&gt; { B(T), C } The following types would be optimised to be nullable pointers: type D&lt;T&gt; = A&lt;~T&gt;; type E&lt;'a, T&gt; = A&lt;&amp;'a T&gt;; Hence, whenever you see Option&lt;&amp;T&gt;, rest assured that it is pointer sized, not the size of a pointer + the size of the tag.
To be honest i find that very confusing.. I also didn't really like unsized (because it sounds like the struct has no size at all, that's confusing). But "type" doesn't say anything about the size to me and in the place where it is used it sounds like it is only repeating that there is some type T. How about "dynsize"? Or "dynamic"?
I am on master branch of rust and when I do a pretty print on the code you have, I get an internal compiler error (ICE): `rustc --pretty typed bench.rs` gives: error: internal compiler error: node_id_to_type: no type for node expr `1u (id=98)` Can you confirm the same result?
Here's a potential disadvantage to the "literals are associated constants" approach: associated types/constants are not injective in general. `Foo::AssocType` and `Bar::AssocType` could easily be the same type, therefore you can't conclude `Foo::AssocType == Bar::AssocType` =&gt; `Foo == Bar` (and likewise for constants). I'm not sure whether/how this might bite w.r.t. LAAC, just something to be aware of.
It's actually a bit of a trick. Rust has [automatic pointer dereferencing](http://static.rust-lang.org/doc/master/tutorial.html#dereferencing-pointers), which dereferences variables if you try to apply an operation on them (a method call, etc). This is why unlike C, you don't have 2 operators for accessing variables on a struct (the dot operator and `-&gt;`). So when you do `(&amp;(&amp;0))`, Rust is treating it as `(&amp;(*(&amp;0)))`. This effectively normalizes the code to always just deal with references for each argument. This allows you to use `assert_eq` for both reference comparison (as above), and value comparison (e.g. `assert_eq!(4, 1+3)`). 
Previous discussion here: http://www.reddit.com/r/rust/comments/1wvxcn/lazily_initialized_statics (I hope the repost is ok for this.)
(N.B.: This is not mine, but it looked pretty cool.)
&gt; So when you do `(&amp;(&amp;0))`, Rust is treating it as `(&amp;(*(&amp;0)))`. That's not correct. `&amp; &amp;0` is a distinct value (and type) than `&amp;0`. Automatic pointer dereferencing comes into play when calling methods on a value. However, the macro you linked explicitly dereferences the value when doing the comparison.
References are addresses under the hood, but that's not exposed (and I believe LLVM is free to implement `&amp;(&amp;x)` the same way as `&amp;x` if there's no way to tell the difference). If you need an address, you can coerce a `&amp;T` to `*T` with the `as` operator (as in `&amp;3 as *int`). That gives you a raw pointer, which is explicitly an address, and comparing raw pointers will compare their addresses. However, comparing references do not compare addresses. Instead, if the type being referenced implements `Eq`, then comparing two references will actually compare the referenced values. And if the type does not implement `Eq` then you can't compare the references. The reason for this is that `Eq` is implemented on `&amp;T` if `T: Eq`. You can see that at the very end of [the Eq docs](http://static.rust-lang.org/doc/master/std/cmp/trait.Eq.html) where it says `impl&lt;'a, T: Eq&gt; Eq for &amp;'a T`. The implementation looks like impl&lt;'a, T: Eq&gt; Eq for &amp;'a T { #[inline] fn eq(&amp;self, other: &amp; &amp;'a T) -&gt; bool { *(*self) == *(*other) } #[inline] fn ne(&amp;self, other: &amp; &amp;'a T) -&gt; bool { *(*self) != *(*other) } } So you can see it's dereferencing the reference. It has to doubly-dereference because the arguments to these methods are themselves references. In any case, the end result is that saying `&amp;x == &amp;y` is equivalent to `x == y`.
So at the point of comparison, is it doing `&amp;0 == &amp;0` or `0 == 0`?
I think [trait objects](http://static.rust-lang.org/doc/master/tutorial.html#trait-objects-and-dynamic-method-dispatch) are what you're after. If you really want to process a "vector of values with some arbitrary type satisfying some trait", then I suppose this would be what you'd use. But there are other forms of polymorphism that are statically dispatched that you should try to use first. (The link I gave you goes into more detail if you scroll back to the beginning of the section.)
Thanks, so it works sort of as follows I think? Generic containing only 1 type of struct, or I imagine not calling any impl method: Statically dispatched. Generic using impl methods on multiple kinds of structs: Use trait object that uses a v-table to dispatch dynamically. aka virtual functions. So if I made a hash table that could store any hashable thing it would need trait objects for dynamicness, but if I parmaterized it (like java hashMap&lt;foo,bar&gt;) it could be written to use static dispatch? 
&gt; Because C++ crashes when it runs into memory allocation issues, it weakens any browser that uses the language. That's actually the least of your worries: in case of memory allocation failure you (should) get an exception, `std::bad_alloc` (unlike C). The real issues with C++ are all the *undefined* bits of the specification, and in the context of memory *dangling pointers* and *pointer arithmetic* (out of range...) are the worst offenders as they might be used by hackers to do... whatever. I did not know that Reddit use had been so carefully thought out (and had not even noticed the absence of forum); interesting.
I agree, this is very instrumental in good IDE support. Love to see how this library improves!
To expand on burntsushi's answer, "virtual function" is a term specific to inheritance-based object oriented programming, and it's the main way to create a polymorphic function in an inheritance-based language. Specifically, it creates a function with "ad-hoc polymorphism", which means that the function has a completely different implementation for each type. In Rust, traits are used to create ad-hoc polymorphism. The other kind of polymorphism is "parametric polymorphism", and that is done in Rust with generics. In parametric polymorphism, a single function can be called with different types passed in, and the same function body is used no matter what type you pass in.
It will break as often as you pull and recompile - ie, you can choose how often you want it to break. A "stable" release is sort of a nebulous concept at this point in Rust's life (the point releases are mostly just snapshots), though hopefully not for that much longer.
Aren't conditions going to be removed from the language?
How much tends to break? Last time I tried to build rustc from master I couldn't even get it to compile. :/ UPDATE: I was able to compile Rust 0.10-pre with no problems and compile working code for my x86_64 Debian dev machine. I wasn't able to cross-compile for ARM, but I'm going to try grabbing one of Luqman's ARMhf snapshot binaries and try compiling directly on the target machine. I have to say that the improvements to IO error handling and JSON parsing make it worth slightly more instability and difficulty cross-compiling. I really love Rust.
Safe shared state via a concurrent data structure is no worse than a channel, and Rust allows both. Unlike Go, it does not permit data races. In Go, slices and maps have reference semantics so it's *very* easy to accidentally share mutable data.
Great, this explains everything that was confusing. Thanks.
So, is it better to say that Servo is Mozilla's test bed meant only for research, and that they hope to transfer some of their new techniques and technologies to Firefox when they could?
I felt like that article explained Rust's marketing strategy more than what's "under the hood" if you know what I mean... Well, it got me here, hello /r/rust!
It's a test bed for research in web parallelism in general, but that's not to preclude the possibility that Servo could become a released product some day. The Servo devs certainly treat the project with the gravity that a serious browser engine deserves.
EDIT: I'd just like to put a summary of what I've learned front and center for people too lazy to read the full comments below. Rust's GC will be task-local and only pause on GC allocation. This means that you only pay for a GC when doing a GC allocation. I think that's fair, and most of this comment becomes moot. However, I'm still not convinced as to the utility of a garbage collector in a systems language. It'd be easy enough to add, and it'd have few negative effects on performance sensitive code, but cyclical ownership is silly. Well-designed programs don't have it, and encouraging it by adding a GC just seems like asking for trouble. I'm personally very much against GC for rust. Rust is supposed to be a systems language. In applications I have in mind, I care about latency and "random" pauses in my code. There's essentially two tiers of code I end up writing for these. The first is what normal people write - basically anything goes as long as you don't leak memory or take some ridiculous amount of time doing stupid things. The second tier is in my fast path. This is code where latency is critical. I make sure this code fits in the L1 cache. I make sure there's no allocation. I make sure that branches are tagged likely/unlikely when I know better than the compiler. For this tier, my language is essentially a portable assembler with a typechecker on crack. I like that. Adding a GC would mean that I'd have to audit every single piece of code running in the same task as my fast-path to see if it ever allocates, even once. If it does, then I might hit a GC pause in my fast path and be very sad. That'd immediately kill my interest in rust. When I spend a lot of effort speeding up an inner loop, I'm not happy when a runtime decides to arbitrarily pause for an unbounded amount of time whenever it feels like. There's also an "I've been programming for a long time, and have never needed a GC" argument. Seriously. When the hell do you create a cycle of reference-counted pointers? I barely ever do that with normal pointers! That just sounds like a horridly designed application, and a proper garbage collector would just rm -rf your source. I can't think of any memory semantics that aren't covered by raw, owned, and refcounted (both with and without atomics) pointers. I've never needed a "weak" pointer. If you can't even keep track of the memory you use, you probably shouldn't be using a systems language. Okay I thought of even another argument against GC here, and that's philosophical. I use computers for two things: determinism, and speed. If it didn't have those two properties, I'd just do a task myself and computers would be useless. Garbage collection is less deterministic, and slower than managing memory myself. It's just philosophically "evil". That said, I'd be okay with GC as long as it was done in a GC monad or something. That way at least the bounds on this thing can be explicit and it'll be hard enough to use that most people won't bother.
To me, the most obvious use for GC is persistent data structures. Naively, you can't create cycles at all, so Rust's Arc is sufficient. However, many persistent data structure implementations have some feature for "suspending disbelief", where a graph would either be instantiated directly or build out of mutable nodes and then frozen. You pretty much need GC for this. Of course, I agree that Rust made the right tradeoffs, since it's possible to both have full control over memory and use GC-ed persistent data structures in the same program.
I do also use vim. ;D
&gt; Adding a GC would mean that I'd have to audit every single piece of code running in the same task as my fast-path to see if it ever allocates, even once. I'm not sure about this, but isn't there a way to mark a task as not performing gc, and rust can ensure that it never gc's? EDIT: I just looked and I couldn't find anything. I think I made this up, and it's not possible to exclude a task from garbage collection. EDIT EDIT: I was hoping that there was a way to statically ensure that a task never garbage collects. But I guess that would require an [effect](https://github.com/mozilla/rust/wiki/Proposal-for-effects) system.
You worthless piece of scum! You're even worse than I thought! ^^^^but ^^^^really, ^^^^vim ^^^^isn't ^^^^too ^^^^bad, ^^^^either.
Exactly. And in rust-land, you could just have those cyclical objects refer to each other with raw pointers and have everything owned by the request.
GC is going to be completely optional. As experience with Rust has shown, it is not required nearly as widely as was originally thought when the language was first designed. But that doesn't mean it isn't useful for some applications.
&gt; Adding a GC would mean that I'd have to audit every single piece of code running in the same task as my fast-path to see if it ever allocates, even once *Only* if it is a GC allocation, i.e. `Gc::new(1)` (or `box(Gc) 1`). Allocations like `~1` and `Rc::new(10)` are completely unaffected. (Of course, this doesn't prohibit libraries you're using in that fast path from using the GC... but then nothing's stopping libraries from using an O(n^2 ) algorithm where an O(n log n) one exists, or calling `sleep(random())` either.) Also, Rust's type system has the ability to have a safe task-local GC, i.e. GC in one task is completely independent of things that other tasks are doing; so you can have a lazy non-CPU-bound task that just uses GC where it feels like it, and the "real time" task running in a separate thread which is never stopped by any GC pauses. A canonical example of something that GC is useful for is graph data structures where there is no clear ownership. (This can be somewhat solved by storing nodes in a vector and then using indices into this vector rather than direct pointers, but the double indirection has obvious downsides.)
If you don't use GC, then it the collector won't run. As is the philosophy in C++, you won't have to pay for what you don't use. GC is going to be a library — not integral to the language implementation.
No, you are correct: any production ready GC that Rust ships will automatically be restricted to a task. i.e. GC in one task will not affect any other tasks.
1) Yes. I'd have to audit every piece of code called from that task, even if only used for initialization or "slow-path" code, for calls to a GC. And I'd have to audit all the code it calls. And the code it calls, etc. I refuse to do that, and will likely blanket ban libraries that use a GC at all. It's just too much work to look for, and the risk of random, unbounded latency at any point in time is too great. 2) For graph data structures, just have a "Graph" object own your verticies and edges. You don't need GC for this. I would rather have "unsafe" code than waste cycles doing things that are fundamentally useless. 3) I don't buy this "gc in one task won't affect the others, so it's ok" argument. In the low latency applications I have in mind, there's generally only one thread. One thread = one task, since I clearly won't be using lightweight threads here. Initialization is generally an "anything goes" land to bring the system up. Adding gc would make this impossible, and I'd have to do weird hacky things like initialize everything in one task, send the results to the real task, then kill the original task.
Never underestimate the power of marketing (not saying you do though). A language is never going be used in production if nobody knows about it.
Awesome, and welcome! Be sure to check out our IRC channels – they are very active. See the sidebar -&gt;
I'm attempting to improve the syntax highlighting btw - rewriting it from scratch. Agreed that it would be cool to have integration though.
&gt; 1) Yes. I'd have to audit every piece of code called from that task, even if only used for initialization, for calls to a GC. And I'd have to audit all the code it calls. And the code it calls, etc. I refuse to do that, and will likely blanket ban libraries that use a GC at all. It's just too much work to look for, and the risk of random, unbounded latency at any point in time is too great. If the library is designed to be used there, it won't be using `Gc`. Just like it would be dumb to call `sleep(random())` or do a network request, it would be dumb to use `Gc` in a library that's designed to be used in such contexts. Also, if you only use the GC on initialisation, and don't ever allocate again, that's fine: there will be no additional pauses. (Assuming that GC pauses are acceptable during initialisation.) &gt; 2) For graph data structures, just have a "Graph" object own your verticies and edges. You don't need GC for this. I would rather have "unsafe" code than waste cycles doing things that are fundamentally useless. Your use-case sounds like it's perfectly ok to audit all the unsafe code; most people will not want this, would prefer unconditional safety, and don't need the guarantees of no GC throughout their the whole application. &gt; 3) I don't buy this "gc in one task won't affect the others, so it's ok" argument. In the low latency applications I have in mind, there's generally only one thread. One thread = one task, since I clearly won't be using lightweight threads here. Initialization is generally an "anything goes" land to bring the system up. Adding gc would make this impossible, and I'd have to do weird hacky things like initialize everything in one task, send the results to the real task, then kill the original task. Well, if you don't use GC in that first thread, then it doesn't use GC at all and there's no GC pauses... The final design will almost certainly have a lazily initialised GC too, i.e. if you never allocate a GC pointer in a task then the GC object for that task won't even be created (including the very first task).
If a library I use during initialization calls sleep(random()), that's ok. That sleep will only happen before I send out a message to the world saying "Yo world! If you talk to me, you'll hear a response in less than 150 us or something went horribly wrong!" If it allocates ANYTHING on a GC'd heap that will be cleaned up.... whenever. When I do `i += 1` followed by `j += 1`, I expect that to be two instructions, without randomly blowing my cache in the middle. A gc pause introduced by objects thrown onto the heap during initialization will blow up those guarantees. I thought rust was supposed to be competing with C++ here! Zero-cost abstraction. If "safety" has a cost (GC), then it will be dropped by systems programmers. For us, safety only matters if it's zero-cost. I don't care how safe python is, I have a real deadline to meet.
The way I see it, there is a wide space between C++ and the next level up in abstraction and it creates a lot of debate when really two people want two different things. For your case I would use c++. GC is not philosophically 'evil' in other cases that still include native programming. A media player or a game with something like nimrod or D is still absolutely practical and a GC fully embedded into the language and all the standard libraries creates a huge gain in productivity through simplicity. The problem I see is that GC native languages and non GC native languages fall into two different camps. When people talk about a better C++ I see two diverging paths. One seems to be where rust and a niche language called clay ended up, one is where other productivity + native speed languages like D and nimrod went.
The sad thing is, I do normally use C++ for this stuff! I'd love to switch to rust, because it advertised itself as a low-level systems language. I guess that might change in the future...
&gt; If it allocates ANYTHING on a GC'd heap that will be cleaned up.... whenever. When I do i += 1 followed by j += 1, I expect that to be two instructions, without randomly blowing my cache in the middle. Rust is a systems language so it would be utterly ridiculous to insert garbage collection points everywhere. Garbage collections will not run randomly at any point in the execution: just like green tasks switches don't happen randomly. GCs will only occur for a GC allocation, and (presumably) when the user triggers one explicitly with a function like `std::gc::collect()`. i.e. `i += 1; j += 1` will be 2 instructions, and `for i in range(0, 1000) { foo(i) }` where `foo` never touches the GC will never be affected by a GC pause. &gt; A gc pause introduced by objects thrown onto the heap during initialization will blow up those guarantees. You still have guarantees, based on the size of your heap. i.e. if you always allocate only a little, you'll still have a small upper bound on the GC pause time. And, if we get an incremental collector, one could have even stronger guarantees. &gt; If "safety" has a cost (GC), then it will be dropped by systems programmers Note that Rust is safe without the GC for almost everything, except these persistent/cyclic/graph datastructures without clear ownership. And, for these, there are various safe abstractions that can be built (such as Rc with weak pointers, the vector idea I mentioned before, and GC).
&gt; that ran arbitrarily like in every other language. No. Most other languages only trigger a GC during an allocation, or on an explicit `collect()` trigger. e.g. it is a standard technique to write the inner loops of Java applications to avoid any allocations, and hence avoid GC pauses. Similarly, Microsoft's CLR [only GC's when memory is getting low](http://msdn.microsoft.com/en-us/library/ee787088.aspx#conditions_for_a_garbage_collection) (i.e. during allocation). And same for D, Go, Ruby afaict. That said, many of these languages need to do stop-the-world collection, because they cannot express thread-local GC like Rust can. This means they need to halt all other threads during collection no matter their point of execution. That is, there's two factors here: allocation triggers a GC collection, and the collection has to pause other threads (and hence it looks as if the GC just ran randomly in those other threads). In Rust, we can statically guarantee that no GC pointers leak into other threads, so the second stop-the-world step is completely unnecessary.
I guess the main part of your argument that I don't understand is this: how would a Rust library using std::gc be any different than a C program using some arbitrary GC library? For both, you could easily look to see if GC was used, and it's totally optional in both as well. What's the distinction?
Agreed, but there's a fun counterargument to this which I'm sure Rustees(?) can appreciate: In the Haskell community, there is a saying, "avoid success at all costs", which serves as a reminder that, had Haskell become popular early on, it would not be nearly as good as it is today -- it is much harder to make changes to a language after it becomes popular. (Go, although I do like the language, is learning the hard way: it locked itself into a contract guaranteeing no APIs will be broken and gained a decent amount of traction while the language still had warts.)
&gt; Generic containing only 1 type of struct, or I imagine not calling any impl method: Statically dispatched. I don't think this is right, but I could be misunderstanding. Firstly, structs themselves aren't particularly relevant. A struct is just one way to define a type in Rust. There are other ways, like with a type synonym, an enum, a tuple, and maybe a few others that I'm forgetting. Secondly, imagine your "generic" is just a `T`. It's an arbitrary type and *you don't know what it is*. What can you do with it? The answer is, not much. But if you know that `T` satisfies the `Clone` and `Rand` traits, then you know you can copy and randomly generate values with type `T`. This is done by *calling methods* on your `T`. This is all statically dispatched. Where I come from, `T` is a type variable and `Clone` and `Rand` are *contraints* (not types!). In Rust, I believe `Clone` and `Rand` are called bounds. In sum, `T: Clone + Rand` is a *bounded type*. &gt; Generic using impl methods on multiple kinds of structs: Use trait object that uses a v-table to dispatch dynamically. aka virtual functions. I'm more knowledgeable about functional programming than OOP, but I'd think that virtual functions are much more analogous to bounded types. A trait is your "interface" and there can be multiple implementations of this interface for various types. In this way, the same method is overridden depending on type. Trait objects are *existential types* (in contrast to traits, which are not types at all). An existential type is saying, "there exists some type that satisfies these bounds." The canonical example of an existential type is to use it to build a heterogeneous list or to use it for dynamic dispatch. Usually these use cases don't come up too often or there is a better design lurking with the use of bounded types. &gt; So if I made a hash table that could store any hashable thing it would need trait objects for dynamicness, but if I parmaterized it (like java hashMap&lt;foo,bar&gt;) it could be written to use static dispatch? I believe this is correct.
I guess I wasn't very clear about what I was thinking. I was hoping that there was a way to statically ensure that a task never performs garbage collection. But I guess that would take some kind of [effect](https://github.com/mozilla/rust/wiki/Proposal-for-effects) system.
Don't worry. Rust changes all the time to prevent wide use. Just kidding. That'll end with Rust 1.0.
No worries, it's a good question. I'll quote [Ted Mielczarek's reply](http://www.fastcolabs.com/3027664/under-the-hood-of-mozillas-new-multi-core-browser-and-the-open-source-language-that-powers-i#comment-5a56eef0-abd4-11e3-90aa-8dd0387b995d) to [a similar question](http://www.fastcolabs.com/3027664/under-the-hood-of-mozillas-new-multi-core-browser-and-the-open-source-language-that-powers-i#comment-bdbcbdd0-abcf-11e3-a798-65e16c3c4e91) that was posted on the article: &gt; Rust has a few features that make it more suitable than Go for this purpose--the ability to program without a garbage collector, for one (although the option is available) means managing memory usage more carefully is possible. Additionally, Rust's type system is designed without null values, rendering a large class of programming bugs impossible. Finally, Rust does not have shared state in its concurrency model, meaning that programmers aren't forced to deal with the same old concurrency bugs as they have in C. And here was [my follow-up reply](http://www.fastcolabs.com/3027664/under-the-hood-of-mozillas-new-multi-core-browser-and-the-open-source-language-that-powers-i#comment-49171f50-abd6-11e3-8e0a-77172a302ea7): &gt; In addition to Ted's comments, I would also add that Rust's concurrency features aren't baked in. The language semantics provide the building blocks for creating safe, statically checked concurrency features, like those in the standard library. However if those are not suited to the specific project they can be custom built. For example Rust's lightweight tasks are great for applications like web servers, but Servo has implemented its own highly optimized work queue to serve its particular performance demands. This kind of low level control combined with compile time safety simply does not exist in other languages like Go, Erlang, or C++. Languages like Go and Erlang that have concurrency baked in and pervasive garbage collection are good for some applications, but they are less ideal for the domain of systems programming.
Facebook doesn't exactly write code in an amazing diversity of application domains though. For example, how long is a "request"? Now write an application with ten thousand objects that can all reference each other throughout their life time, that has to run for hours. There's code where you genuinely need to have references all over the place and ownership is hard to be principled about. Without some kind of automatic memory management you're typically left with "preallocate everything up front, and then delete everything at thee end" type of approaches (very common for games, where game objects often have this kind of messy reference structure - it's safe if you never delete things until the end of the level). But of course this is intrinsically wasteful, especially if you want to be long-running and not have random interruptions where you transition from one level to the next.
Yeah, I agree that would be kinda nice. For example it would be nice to restrict IO in some instances, like /u/pcwalton's work queue that he described at the meetup.
cmr posts a quite comprehensive list of breaking changes every week: http://cmr.github.io/blog/categories/this-week-in-rust/ Yes, there tend to be a bunch every week, but most changes are more obscure and unlikely to break your code. Fixes for breaking changes are usually quite easy to do as well, whenever you're up for it.
&gt; No compiler support involved etc This is *really* unlikely to be literally true: precise will be much nicer with compiler support; although it will presumably be compiler hooks via lang items and intrinsics. (And even conservative GC benefits from a single intrinsic `contains_managed&lt;T&gt;() -&gt; bool` so that something like `~[u8]` doesn't need to be scanned by the GC.)
Please suggest /r/playrustservers instead, since /r/playrust does not allow for the server advertisements. I'm quite sure that users posting Rust server ads here won't read the subreddit rules at all.
I didn't watch the video, but this didn't look like a server advertisement to me.
Well, I didn't look closely :-)
this looks awesome! field/ methods are hard , but perhaps it could filter based on previous &amp; next character (eg anything after a dot, just show field/method names names, maybe put a "(" after the cursor to take a hint to show method/fn names) 
You do know that there exist GCs for C and C++ too right? Like Boehm GC. Do you audit every library you use to see if it uses Boehm or some other GC?
No null values and no such thing as shared states (I didn't even know that was possible) are pretty radical. Rust sounds interesting to say the least. I'd be really curious also to see how it stacks up against Goland performance-wise. Go is probably one of the fastest languages to come out in a while, I feel like that's really the only reason to be playing with multi-core features to begin with. Thanks for the thoughtful response though!
Yep. Implementation of a gc will also require unsafe code, doing the whole thing type-safe, ideally even without breaking abstraction barriers, is still a research topic, and won't be solved for quite some time.
[Reference counting is GC, too (pdf)](http://www.cs.virginia.edu/~cs415/reading/bacon-garbage.pdf)
Well first of all as mentioned before the GC only runs when an allocation occurs, so your inner loop would be unaffected. Secondly I imagine you could move the logging to a separate task and just send the data you want to log to that task. That way only your logging task has a chance of suffering GC pauses (assuming it allocates GC-objects after initialization), and your application might speed up because sending one piece of data to another task is presumably faster than performing IO. Thirdly we do not currently have a GC and yet a lot of useful code has been written in Rust, it is unlikely that most libraries will suddenly use the GC when it becomes available. Finally there are already many stupid things you can do in a library, and yet the only thing you seem to worry about is the GC. Even without the GC a library may have unpredictable performance, which may be introduced by performing IO, randomized algorithms, or even just bugs where something takes much longer than expected under certain specific circumstances. If you are going to use such a library in your performance critical code, you are going to have to audit it anyway, even if Rust did not have GC (and even then a library might include a self-written GC implementation that they didn't bother to tell you about. This could even happen in C/C++).
It was never a question that a GC implementation needs some measure of unsafe code (since it's essentially handling raw pointers). &gt; doing the whole thing type-safe What do you mean by "type-safe" in this context?
&gt; I'd be really curious also to see how it stacks up against Goland performance-wise. Go is probably one of the fastest languages to come out in a while, I feel like that's really the only reason to be playing with multi-core features to begin with. Rust is almost always faster. the Rust compiler uses a proper optimiser (LLVM), unlike the default Go compiler; and runtime reflection &amp; dynamic calls isn't a core part of the Rust standard library. Rust-the-language is as fast as C and C++; but the standard libraries haven't been extensively optimised yet (although most of them aren't slow, just not as fast as they could be).
Rust noob here - can't you just make an `Inited` outside the `init` function? 
Not using type coercions, that is, casts, or anything else that makes a type system logically unsound. Which, of course, raw pointers do all the time. There's also another issue, and I don't remember where I stumbled across it, and that's ensuring that things that ought to be dead are actually properly dead, not leaked, and related invariants, employing a region calculus with early deallocation. All that's horribly interesting but also quite far from usable in practice. Especially when you want your now absolutely verified gc to be fast.
Nope, that's what the dummy `priv` field is for: only the `example` module can see it, and hence construct `Inited`.
Is there a reason to use a macro over a function, besides guaranteed inlining? 
Specially when sharing between multiple threads. :)
Funny, I heard similar arguments from Assembly programmers about C back in the day.
The only reasons to use a macro is syntactical. If you can do the same thing with a function, don't use a macro.
I've never seen a C/C++ library use bohem gc. Can you name one?
C is still less powerful than assembly. Where the hell is my carry flag!? :)
`shared_ptr&lt;T&gt;` is equivalent to our `Arc&lt;T&gt;` type.
Even as a hacky interim solution, being able to make the task-local GC heap point to protected memory would be nice - i.e. if you accidentally call something that allocates GC memory, you end up with a crash.
Garbage collection has its uses: http://www.drdobbs.com/lock-free-data-structures/184401865
I was being ironic, because RC systems suffer from thread contention if the pointers are shared across systems. As they need to update counters from multiple threads. GC systems don't suffer from it.
Yeah, it is kind of reminiscent of Objective-C! It's funny, the linked article's proposed reference counting scheme ("ZCB") sounds exactly like what Objective-C and ARC already do.
My first (released) rust project. Most of the stuff is implemented, error handling is bad (Encodable, Decodable need a redesign). Streaming parser is also implemented. It is not tested in practice (I have no use for bencode right now), written just for learning experience. It also has optional struct fields, ToBencode/FromBencode (you can't really handle mapping [u8] encoding as "packed" bytestring without some really ugly code using current Encoder/Decoder api)
&gt; GC systems don't suffer from it. A global garbage collector certainly suffers from contention and scaling issues. That's why Rust is going to have task-local garbage collector, even though it *could* support global immutable garbage collected pointers.
Rust's garbage collection will be task-local so `Arc&lt;T&gt;` will still be the only choice for concurrent persistent data structures. The task-local reference counted equivalent to `Gc&lt;T&gt;` is `Rc&lt;T&gt;`.
They have contention when updating root nodes, however they don't suffer contention for each memory access across all threads using the same pointer, like RC has. Then there are pauseless ones like C4 in terms of scaling issues. But yeah, task local is a good compromise. 
hell no! This is the most absurd post I have ever read. Have you ever heard of APL? Thank the lord if you haven't. This has NOTHING to do with C++ and everything to do with code being read 1000 more times than it's written. The correct way to shorten code is by using abstractions, NOT by discarding vowels! 
Please try to avoid hyperbole like 'this is the most absurd post I have ever read'. The op appears to be in good faith and doesn't itself contain any inappropriate rhetoric. I'm sure you can frame your criticism more constructively.
&gt; for each memory access across all threads using the same pointer The only time a reference count occurs in Rust is during an explicit `clone` call to add a new owner. Move semantics and borrowing lightweight references mean that this is very rare. It would usually only happen when sending the data to another task, and will be dwarfed by the cost of spawning a task. The current Rust implementation of `Arc&lt;T&gt;` is not ideal as it's using sequential consistency for `clone` and `drop` when it's not necessary.
Some of the generic and lifetime syntax can be tedious to write, but perhaps macros are the way to simplify the problem, rather than new language constructs? Eg. let foo = hash!(A, B); 
I think you have the wrong rust. This is about the programming language, not the game.
http://www.hboehm.info/gc/ There's a list of current users there.
None of these are libraries for C/C++, except those implementing languages (compilers, runtimes, etc.) that are garbage collected themselves.
As a Python user, I partly agree on your point but want to make sure that it has also pros and cons. In Python lists and dicts, especially heterogeneous ones, are so widely used that the users do not look at the other designs which may be more efficient. Endorsing a particular data structure requires the optimization for every possible case: CPython/PyPy dict, for example, is particularly optimized for such use cases ("better" in the sense that it is more efficient) and actively discourages the use of other data structures unless seriously needed even though *they are already available in the Python standard library*. It might be acceptable for Python, but for Rust it would be a questionable choice. Not to mention that we can add such syntactic sugars after 1.0, in the backward-compatible way.
Thanks for the explanation. 
Well, I absolutely welcome better support for generalizedoverloading . and [] is certainly a step forward and that definitely offsets the syntax changes - but whats' suprised me about the "random symbols" comments is - it really takes no time to get used to them. minutes, hours at most. The module system on the other hand trips me up all the time :) (i'm trying to solve that with an error message patch). Language Syntax is easily composable - wheras defining your own wrappers adds navigation overhead to a reader of a sourcebase. (example - in the compiler source there's definitions of DefId, then DefIdMap&lt;T&gt; ... you might have learned a map syntax once, then whenever you see [DefId=&gt;...] you know what it is You could say C did the same thing for its domain: it compacted many common machine instructions (increment, offset loads/stores) into single symbols, hence a powerful, popular language that made it easy to express low level patterns: And to this very day I prefer writing for loops with ++, etc .. to using library symbols for iterators 'std::algorithms'. Its quicker to compose syntax than to remember what name does what, what order the parameters are in - **the syntax annotates meaning by place** am i just weird? I find it much easier to compose syntax than to learn the meaning of identiifiers? like learning maths vs learning vocabulary. Systems of elegant rules. &gt;&gt;"Blessing a specific datastructure just to save a few characters isn't very Rust-like. " They could still be shortcuts to library code - and a user could override them (recompile the stdlib? or something program wide like when you replace malloc/free in c) As time went on, there was a trend with C++ projects that worked on that thered' be someone enforcing consistency in the code base.. the boss used the exact word "Nazi". Now having someone there to ban me from using languaeg features to write my own datastructures is negative.. insulting even. Someone imposing standard symbols on me when I can define my own.. But having convinient syntax for common ones is posative : It took me literally many *years* before i was happy to use "std::vector" intead of rolling my own "DynamicArray/Array&lt;T&gt;/FixedArray&lt;T&gt;", wheras adopting ~[T] and being happy with it happened within *hours*. A *name* is something programmers can argue over... &gt;&gt;"Blessing a specific datastructure just to save a few characters isn't very Rust-like. " I see "programming in the large" as a stated rust goal - the justification for someone banning me from writing my own datastrutures was precisely 'team efficiency for a large project'. &gt;&gt; A big push in Rust is towards making expensive things look expensive. Using smart pointers like Rc should be done when you need it, not just because it's a bit easier. Do productive and efficient have to be oppostites -can't they be orthogonal properties of the same language? Division is an expensive operation, bitshift is 2 chars , that doesn't ever stop me using bitshifts over divides where needed. Something else that's happened in games is *mixing langauges*. Many people use a combination of C++ and Lua; and many of my ex-colleauges have moved onto C#, (with an underlying C++ engine provided by a 3rd party..) ; I would argue Rust had a unique ability to handle *both styles* in the same compile units, without needing complex interfacing. (its pointer safety that's doing it) Keeping it viable as one go-to language for simple tasks. ( I find myself using C/C++ for jobs that Python or whatever would be better for , simply because I've used them so much ... ) so you'll have another draw to the language.. perhaps more people in doubt would just use it and find the correct use later.. &gt;&gt; A big push in Rust is towards making expensive things look expensive. Using smart pointers like Rc should be done when you need it, not just because it's a bit easier. isn't that the job of profiling and architecture, not typing load .. In some cases, garbage collection can be a size:speed tradeoff, and some people claim it can create cache-efficiency. Consider the 80:20 rule. So today my ex-colleagues write their games in C#. And the optimizaiton work is done by a C++ libary vendor. Depending on your POV, they have specialzed *away* from lowlevel code, or deskilled themselves. But with Rust they could be just writing the same things with "@" everywhere, having the convinience they moved to C# for, and have kept the option of droping back to ~ &amp; where needed. The presence of @ means Rust is suitable for anything Go is currently doing. the reverse isn't true. currently anyone wondering 'what new language should i learn next..' would, if in doubt, be best served learning rust :) It's like for the sake of one character '@' .. you might potentially double or quadruple the potential language userbase. And you're doing something unique, that C++ can't. Also consider how the ease of interfacing with C made C++ ubiquitous - so by having the 'productive language' built in, you get an easy path to a high level audience for the low level work you do. I certainly understand your points about language complexity, and i'll be happier to see the fundementals solid ... but i hope this post has made sense.
already gone, in fact
but isn't most c++ code is just vector&lt;&gt;, map&lt;&gt;, array&lt;&gt;, unique_ptr&lt;&gt;, set&lt;&gt; shared_ptr ? Point taken about sugars post 1.0 you've got this thing in c++ of some peope (and i'll admit, certainly me!) 're-inventing the language whenever they start a project' and introducing stylistic differences between one coder and another. Language Syntax is a more compelling draw to one set of standards, IMO.
let me word it another way. learning and using new syntax is *fun* . learning and using new *vocabulary* isn't. and look at the power of Hoogle for haskell. "how do i do ... &lt;compose some syntax to make a query&gt; - but finding the write peice of vocabulary to do something is much harder. Searching vocabulary is easy ? not without namespace resolution which we dont have IDE's for yet
in most of the cases i'm talking about in this post , its about not removing what you already have - and keeping the users/knowledge you've already generated. 
Thanks to all the awesome people writing software in Rust!
Inventing a GC for Rust will no more change Rust's status as a low-level systems language than the invention of the Boehm GC changed C++'s status as a low-level systems language.
&gt; but whats' suprised me about the "random symbols" comments is - it really takes no time to get used to them. minutes, hours at most. They're nearly impossible to Google; and they don't necessarily have an obvious meaning, unlike words, so can be abused by being overloaded in unexpected ways. &gt; But with Rust they could be just writing the same things with "@" everywhere, having the convinience they moved to C# for, and have kept the option of droping back to ~ &amp; where needed. The presence of @ means Rust is suitable for anything Go is currently doing. the reverse isn't true. currently anyone wondering 'what new language should i learn next..' would, if in doubt, be best served learning rust :) `@`/shared-ownership data isn't convenient: ensuring memory safety means there's various contortions required for such things to be mutable. Just using `~` and `&amp;` is *so* much easier. &gt; The presence of @ means Rust is suitable for anything Go is currently doing Not really; Go is really *really* well suited to server middle-ware (it's essentially designed from the ground up to be optimal for this style of software; they even have their own calling convention). Rust is theoretically capable of having libraries that are equally good, but I don't foresee that happening in the near future.
ok so you're saying that where @ was used, you really do have to select something more specific? (objects lockable by threads, or a single-task version?) .. i do remember hearing a lot about @mut problems. Would an immutable @ have enough use cases. (graphics resources are usually setup and then immutable, but needing a destructor, that could be a refcounted case) I seem to remember last time i looked at the compiler, it has some unsafe workarounds too, but that was many months ago. perhaps @ can reappear later if one variation does appear alot (like refcounted immutable resources) 
&gt; if the langauge is good - like the transition from C to C++ - inbuilt feaures will replace macros for the most common cases; macros are there for things the language designers haven't figured out yet This is only the case for the crappy "macros" that C/C++ have. Real macros are perfectly good tools for simple transformations like `hashmap!(x =&gt; y, z =&gt; w)`. I don't understand why we need to move things into the language when its so much simpler, and more flexible, to use the other tools the language provides (e.g. macros).
well Map[A=&gt;B] isn't much benefit over Map&lt;A,B&gt; (a bit of infixing). i guess i was just thinking out loud with this speficic idea and on second thoughts anything using the "&gt;" character is going to look awkward inside other typeparams (&lt;&gt; balancing..) [A?B] might not be so obvious and ? might have better use (Option.. highly ubiquitous. a C programmer's *T becomes ?~T ?&amp;T ... ok maybe that does start to look silly) It's the `~[T] .. ~[~T]` notation that i've become really attached to. Vec&lt;T&gt; -&gt; ~[T] ... as a graphics coder i've got the word vector with various cases/abreviations/suffixes used for x,y,z,w , there's even been compilers with extentions that use that as a keyword for SIMD. &gt;&gt;" I wonder what Rust would do if collections crate was missing." given that we have the complete compiler I can't imagine its so hard to make a project wide substitution of the default collections, but i've not tried and practice is usually harder than one imagines
I'm saying that shared ownership is fundamentally more complicated to reason about (for humans and compilers), which results in its use being similarly more complicated. E.g. the requirement for `Cell` and `RefCell` (and the old `@mut`).
If the GC de-allocates that is, not all do. I think Java only started giving excess memory back to the OS in Java 7 (or Java 6?). Basically the JVM just allocates a buffer, expands it when it runs out of space, and collects garbage by marking which parts of the buffer can be overwritten by future allocations. Sometimes the GC can also defragment the buffer as well. This way, you don't allocate very often, and when you do it's in large chunks. You usually never de-allocate memory either (unless by giving back to the OS). The only penalty is scanning the memory. Luckily you only scan live memory, not the garbage.
Facebook is a web service though. Request and response, request and response. Web services are more or less stateless. After the response, very little remains of the interaction in the server. Anything you need to keep around is stored in a database and retrieved later. You are much more likely to run into cycles in other types of applications, like games or a mobile app or... a browser. The DOM for instance has a parent and child property on nodes. You don't need to use RC/GC for this in Rust as you could use a raw pointer and clean up with the RAII-pattern, but in other languages (say, PHP) every html-node would implicetly mean a cycle. Even in the case of Rust though, a GC would be nicer/simpler if a GC would be good enough.
Interesting. Idly, I'm using grunt from the node.js world to manage my rust code, because the the grunt-contrib-watch package lets me watch for file changes and compile, run tests and publish growl notifications as I code. Really, all this is, is a bunch of shell scripts shortcut'd into a make file right? I mean, that's neat, but why make? If it's just a set of shell scripts, why not use a set of shell scripts? Make's power is in mapping pattern blah.o from blah.c using rules; but you're not using any of the that functionality. Or, if you want more complex behaviour, use something that's actually a proper build system like scons, grunt, gulp or cmake; that (for example) runs in non-sane build environments like windows? I suppose I'm in the make-considered-harmful camp, because I have first hand experience with the hell of recursive make file hell (http://aegis.sourceforge.net/auug97.pdf &lt;--- Project I worked on ticked all the do not do boxes), but it seems these days there are other tools, and using make seems... odd, unless you're actually *using* the features of make that make it useful. 
The serialization format used in torrent-files. ;)
I tend to settle for make alot, being from C/C++; is it possible many people who might look at rust are going to be similar.
Well I figured that people would know what it is or google it (also added wikipedia link to README)
re "I've been programming for a long time, and have never needed a GC", Firefox has cycles, lots of them. Worse, it has cycles which go from JS objects to C++ objects and back again. We use reference counting and a dedicated cycle collector which takes up the the full time of several devs to maintain and improve. Similarly, Chrome/blink tries to avoid these cycles, but long term is (probably? I'm not 100% sure of the status) moving to a GC solution (oil pan). Whether or not GC is the right solution, there is definitely real-world motivation.
Make may not be the best tool in the shed, but it is everywhere and most people know it at least a little bit, or can make changes to an existing file (assuming you don't use too much make magic).
Sure, just know that clear submissions can get a higher vote total and thus become more visible. I tried to add the information to this thread, but those who voted caused wikibot to delete its comment. No idea why having wikibot delete it would help anyone. =/ Oh, also, the way the project was structured also caused me to jump around fruitlessly *first*, and only then did I Google, thus wasting more time. The Github pages I've seen usually incorporate such an information section on the landing page.
Built-in syntax that privileges a single type doesn't solve that.
I wanted something that worked out of the box. make is everywhere, most projects in Rust that I have seen uses it. I know somebody uses cmake for more complex things. I am not familiar with the other build systems you mentioned. Rustc optimizes on crate level, so you don't get the same benefit as in C with having a bunch of blah.o files (if I understood this correctly). Thanks for the feedback! I'll take a look at the pdf.
The goal is absolutely for GC to not interfere with the rest of the language and for non-GC code to never have to consider GC. The hope is for GC to be implemented as much as possible as a library and not a language feature. I don't think we know exactly how we will have to interact with the language to make it work, I guess we'll find out as we implement, but the goal is for as little as possible. I'm afraid I don't know more of the specifics.
The type `|&amp;mut Encoder&lt;'a&gt;|` means a closure accepting one argument, `&amp;mut Encoder&lt;'a&gt;`, and returning unit (`()`). `&amp;mut Encoder&lt;'a&gt;` means a mutable reference to an `Encoder&lt;'a&gt;` object. `Encoder&lt;'a&gt;` means `Encoder` with the explicit lifetime `'a` (i.e. the `Encoder` contains references with that lifetime).
Are ~[T] and &amp;[T} being phased out?
`~[T]` and `&amp;[T]` as atomic units in the type system are being phased out, and replaced with the composition of the regular `~T` and `&amp;T` types with the new type `[T]`. For `&amp;[T]` there will be no semantic change, but for `~[T]` there will be, so we are moving all uses of `~[T]` over to `Vec&lt;T&gt;` right now, which is the semantic equivalent.
OK that makes sense. What about syntax? Are we still going to be able to write [1, 2, 3] to get a vector? Also what about the ~[T ... Len] type? Is this similar to the ~str to Str change? It would be really helpful if there was some roadmap for changes so the hobbyists like me can try to follow along :)
In the case of things like the vector functions, the convention is to `use std::vec;` and then call `vec::with_capacity()` et al. In the new vector stuff, `use std::vec_ng::Vec;` and then `Vec::with_capacity()` et al.
For multiple-use you can abbreviate it to use std::io::{File, BufferedReader}; or, if you're only using `File` and `BufferedReader` once or twice, just `use std::io` and write `io::File::...` and `io::BufferedReader::new(...)` at the point-of-use.
there's an interesting idea from cmr to allow inheriting use. i've always got the hack of a "common.rs" glob import ... coming from C++ i'm trying to escape lots of manual dependancy maintainance. i got my perception of rust modules wrong originally, it seems 'use' is like a cross between "#include and using namespace". in C++ i was lazy with qualifications (yes i liked to just do 'using namespace std' instead of writing std::... everywhere like most people recomend) .. but in rust i've begun to get happier to qualify individual items .. because of the way modules/source-files *are* namespaces, it feels less bolted on than namespaces are in c++
&gt; ~[T ... Len] We will still have the `~X` and `[T, .. len]` types; and their composition `~[T, .. len]` will remainperfectly valid.
Until someone writes such an article, I found the Rust meetup presentation easier to follow then the series of 5 blog posts: https://air.mozilla.org/rust-meetup-january-2014/ These presentations should probably be somewhere more prominent on the Rust website. I wish I discovered them sooner. 
After clicking on /u/esummers78's link, I now know that we're talking about dynamically sized types. Which makes a lot more sense in this context than Daylight Saving Time.
* What are ~[T], &amp;[T], [T, ..N] and Vec&lt;T&gt; now? What will they be? They'll remain the same, except that `~[T]` will not support array growth (so methods like `.push()` and `.pop()` will not work). `~[T]` will continue to exist post-DST. * How do common patterns translate into a post-DST world? All patterns generally remain the same, except that array growth will not work. * Are we going to be stuck with Vec&lt;T&gt; and vec!(…) forever? Yes. But note that `~[T]` will still exist if you don't need pushing. * How do the str types fit into all this? I believe the plan was to convert them to a dynamically-sized `Str` type, so you'd have `~Str` and `&amp;Str`. The analogue to `Vec&lt;T&gt;` for strings will be `StrBuf`.
Neat! Looks like our constant is 0x1c, but according to the most recent standard (http://dwarfstd.org/doc/DWARF4.pdf) the highest assigned constant is 0x14 (Python). Have they added seven other languages since then?
I can't read with such line length. A typical recommendation is 90 chars per line. This page takes the width of my screen!
Will I be able to convert between `~[T]` and `Vec&lt;T&gt;` without reallocation?
Does it at last make sense to start officially referring to `[T]` as arrays rather than vectors? E.g. in http://static.rust-lang.org/doc/master/tutorial.html#vectors-and-strings . I feel like the current terminology is a relic of pre-0.1 Rust, when all arrays were growable and dynamically-allocated and fixed-length arrays weren't even possible.
Yup, that'll work just fine.
Free thyself from the tyranny of unreadable blogs: https://addons.mozilla.org/en-US/firefox/addon/clearly/
Some of it. The issue is that debuggers (GDB, LLDB) don't know that Rust exists and thus interpret machine code and debuginfo as if it were C/C++. So for now, we have to produce DWARF that could also be valid in those languages. For enums especially this means that we do something less elegant than would be possible. Having a DWARF language constant reserved for Rust won't help as long as debuggers don't provide specific Rust support. But it's a first step in that direction :) That being said, I'll be working on partial Rust support for GDB and LLDB which, via Python pretty printers, will solve at least the enum problem. Adding full support in one of these debuggers would be very interesting but also much more work.
I hope someone organizes a meetup group for Chicago. I've thought about stopping by the local C/C++ group to see if there were any rustaceans present.
If they had used a language with support for exceptions and pointers with ownership rules (like Rust and C++14) they would have a lot less problems =p. But now they first need to refactor it for better/safer memory management, and then they might seek better alternatives.
This is something that I've occasionally longed for, though I'm not certain if it doesn't just make the code more obfuscated.
-1 from me. This is a great example of making something unreadable just to save a few characters that you only have to type once.
What's the difference between `[T, ..N]` and `~[T]` if neither support growth? Aside from the unique ownership.
Yay! Joined. 
I think code is harder to read than write. And everything should be done to make code more readable than writeable. This proposal goes against that. But that's just, like, my opinion, man.
Then why is ~[T] being kept around? It doesn't seem very useful. (Or why isn't ~[T[ just changed to syntactic sugar for Vec?)
`~[T]` is going to be just `~U` with `U = [T]`. Wouldn't really make sense to specifically disallow it.
The former will have a statically-known size, will be treated as a value and can be directly embedded in structs etc., the latter is represented as a `(`pointer`,` size`)` pair and always has to be handled through that indirection.
The responses have been good, and I'm now more inclined to agree that this wouldn't really be a good change.
But isn't `Vec&lt;T&gt;` using `(pointer, size)` as well? Couldn't `~[]` be implemented in terms of `Vec&lt;T&gt;`?
Including indexing via vector[i]. I don't see an implementation of the std::ops::Index trait in vec_ng.rs currently.
I'm happy to write this article (I'm implementing DST, so it will be good to through the details for this to ensure my understanding). Also happy not to write it if someone else wants to or as already started. Please let me know if that is the case, otherwise I'll start writing in a few days.
http://www.reddit.com/r/rust/comments/1s8c0j/rust_with_emscripten/ was posted 3 months ago as well.
Well, I'm planning to do a workshop for teaching Rust inside the company. I may be the one to start making those articles!
Keeping the current ~[T] seems redundant and confusing. ~[T, ..n] should cover the case when I want to dynamically allocate an array with a statically known size and Vec&lt;T&gt; covers the case when the size is determined at run-time. What than would be the use case for something in between the two? Also, "Vec" is an abbreviation of a misnomer abused by C++. Why not call the thing by its name - an array?
Not only that, it'll make building tools that (like Eclipse) automagically insert/remove use statements based on what types you're using harder, even if not by much. No gain at all, in my opinion.
This has been the topic of many conversations on IRC already and *slice* seems to be the preferred term so far. Go, D, C and C++ all use *array* to refer to a fixed-size array. Go and D refer to garbage collected heap allocated arrays as *slices* so there's a precedent for using it for more than just views.
&gt; unique ownership Both `[T, ..n]` and `~[T]` have a single unique owner. The `~` single doesn't mean *owned*, it refers to a heap allocation that's *still* owned.
Going from `Vec&lt;T&gt;` -&gt; `~[T]` would require doing `shrink_to_fit` if we end up requiring allocators to pass a length to `free` for efficiency. So it won't *require* a reallocation but it would involve dropping the unused capacity from the allocator and it may or may not reallocate it.
The `Index` trait as it exists now will be removed. So I don't think it will be implemented for `Vec&lt;T&gt;` before the new one lands.
Go and D refer to unboxed fixed-size arrays as *arrays* and heap-allocated ones with a size determined at runtime as *slices*. There's a useful distinction to make between them. Rust will distinguish between a slice and a vector though.
Please, *please* open that up to us when you finish it. It would be awesome to have a a style and usage guide for idiomatic rust.
Thanks, I didn't know about this!
I agree that a distinction between an array and a view ("Slice") is good. The lack of such a distinction is a major design bug in D. So, [T] is a slice? I could live with that. I still think vec_ng::Vec must be renamed to "Array" or "DynamicArray" or even "ArrayBuffer". "vector" is conceptually wrong here and should be freed to be used by physics code. 
Well, I havent done anything beyond toy projects yet so you've got me (and probably a lot of other people) beat there.
I did not knew about this project! Thank you!
I have a small "Rust gravity hypothesis" of how to gravitate toward good design patterns. It is merely how to avoid bad decisions and not so much about "engineering". It goes like this: 1. Write a global function. 2. If the function should work on multiple types, add generics and look for traits in the standard library to use. 3. If no suitable traits are found, write your own. 4. If it still isn't flexible enough, split up the function into separate steps (put it into traits) or consider other types of abstraction. 5. If it still isn't good enough, ask in the #rust IRC channel. 6. If it still is bad, consider writing a small library for solving that particular problem on its own. Roughly put, the important thing is to start with something simple and if it does not work out, consider add types to support it. If you start with types, you are going to bump against the complexity and power of generic + traits, which might not be significant at all for the end result. Each step will open up for more flexibility, but it will also slow down the progress. Global functions are easy fixes or perhaps will work after being expanded with generics, but changes to types will break your code. Advices on code style: - The iterator pattern is very powerful and you solve seemingly hard problems with some maps and zips (be creative). - Early returns with multiple ifs reduces the indent level and is much more readable than one if with a complicated condition. It saves you from mentally context switching and it prevents you from writing multi purpose functions. - Option pyramids? Match tuples wherever possible! match (a, b, c, d) { (Some(a), Some(b), Some(c), Some(d)) =&gt; { ... }, _ =&gt; {} }
I am very interested in this! Please write something about the experience of the workshop itself too, that would be helpful for others when teaching Rust.
This is almost as good as the pyramids of doom.
But an Index-like trait will remain? I'm fond of var[i], but maybe that's just my Python background talking.
Definitely not a big fan of this either. 
Yes, there will be traits for indexing. It just won't what's currently available.
Of course.
(I'm not the organiser... and I don't know if they're on reddit, so contacting them directly might be more reliable.)
Yehuda Katz has replied to the announcement with some more details. https://mail.mozilla.org/pipermail/rust-dev/2014-March/009090.html
A tiny Rust club called *Rust Samurai* had had a few meetings held at Mozilla Japan office in Roppongi, Tokyo. Follow @rust_samurai at Twitter for future announcements.
They are very, very welcome. While cargo-lite has well intentions, it has some minor annoying problems (I'm sure /u/cmr acknowledges that, since cargo-lite was never meant to be high quality). Just today I was having some annoyances with it, and I thought about doing a package manager as a side project. So I went into the IRC channel and asked if I would be wasting my time, and the answer was... yes, I would. Because of cargo. Well, I'm glad I'll be able to focus on my own project instead of doing a package manager, so, thanks, guys!
Amazing! I imagine this will also go a long way to helping Ruby integrate with Rust too! ;-)
If you haven't seen Yehuda's talk from the first Bay Area Rust Meetup, you should: https://air.mozilla.org/sprocketnes-practical-systems-programming-in-rust/ I believe that as of last week, Tilde is now using a Rust extension to Ruby in production.
Nice! Thanks for that info that's right in my neighborhood! :)
I contacted them through meetup.com
In addition, it'll make changes harder to merge, since everything's smooshed into one line. I believe this style is discouraged in Python for the same reason.
I think there is another alternative. It would be nice if relative use declarations worked like this: use std::io::net; use net::tcp::TcpListener; use net::ip::{Ipv4Addr, SocketAddr}; This appears to work already: (just a bit more verbose) use std::io::net; use self::net::tcp::TcpListener; use self::net::ip::{Ipv4Addr, SocketAddr}; I wonder if it would be better to differentiate modules and other symbols. If Nihy's suggestion were used, I think this may be easier to read: use std::io::{ [Acceptor, Listener], net::tcp::[TcpListener], net::ip::[Ipv4Addr, SocketAddr] }; It would be nice if rebinding worked with glob braces: use std::io::{ MyAcceptor = Acceptor, MyListener = Listener }; Potentially modules with type parameters may be added one day, so any syntax used should be able to accept types. 
I'm not sure a central repository is necessary - Go, for example, works very nicely without one. Really, path-to-repo as identifier is a nice model, and better than the CPAN-ish model, IMHO. That said, I could see a central repository of *nicknames/symlinks.* So you do "cargo get hotsauce", cargo looks up "hotsauce" in the central repository and resolves it to "github.com/locotacos/hotsauce", and execs "cargo get github.com/locotacos/hotsauce".
It's not necessary, but it is nice. That's why wycats' post refers to the 'three ways': you can have it local, you can have it at arbitrary remotes, or you can have central repository.
With Bundler, non-registry packages are absolutely first-class, so I'd expect no less from Cargo.
Run your workshop by us on the mailing list first and we'll make sure you don't get anything wrong. :)
No problem! For anyone that's not a Rubyist, these four lines in a `Gemfile` do these four things: gem "rails" # install rails from rubygems gem "rails", github: "rails/rails" # install rails from GitHub gem "rails", git: "https://some.git.server.com/rails" #install rails from some server gem "rails", local: "~/src/rails" # install rails from the filesystem In all of these cases, the source code doesn't change. It's just about location. You can also, in the last case, use `bundle config` to over-ride the declaration to point at your local filesystem, so that your `Gemfile` still says `gem "rails"`, but it uses your local checkout of `rails`.
It could be supercool to aim for a general data format, so that future languages can choose to reimplement cargo instead of reinventing the wheel.
&gt; For a language that breaks all your code twice a week, why not emphasize full global rebuild? (which is also the big missing feature in Cabal) Because the language won't be breaking code forever; it would be silly to optimise for this experience when it will hopefully mostly disappear by the end of the year.
Go's model is broken for the corporation world. No support for: - proper versions - binary packages - makes use of source control references in package imports - static linking is also a problem with licensing
Without getting into the details for non-Rubyists, the 'RubyGems author' was a few people who were bored at RubyConf '05. There's a long history between RubyGems and Bundler. &gt; which does the heavy lifting/dependencies RubyGems has not done dependency management until very, very recently, and effectively nobody uses it. That's what Bundler is for.
The goal is to very rarely change/break things that aren't `#[experimental]` or `#[deprecated]` after 1.0 (modulo serious bugs). Furthermore, uses of these things are warned about by default, so programmers won't accidentally use them. &gt; It would be silly to optimize a package manager for the stability of a single package. I don't understand this at all. Choosing to do incremental rebuilds is optimising for "on average most packages won't need to be rebuilt", choosing to do global builds is optimising for "every single package is unstable". The latter is true at the moment, due to the language and libraries changing at the speed of light, but will become less and less true, as the language stabilises, and then as larger and large chunks of the libraries solidify in to a final form. Incremental rebuilds are strictly more flexible than global rebuilds. And, arguing "program *X* doesn't have feature *Y* therefore we need to emphasise *Y*" seems rather peculiar to me. --- Anyway: This is the very first public discussion; I would've thought the announcement is too short and the development too immature to form strong opinions. Maybe you could send an email to the mailing list *asking questions* to clarify your concerns (rather than just assuming that they're running off in "the completely wrong direction").
I only have read the tutorial, I'm not good at all with Rust, but I have understood that Rust does not have subclasses like C++ (confirmed by various blogs). You can only implement and "subclass" traits. At the end of section 17.7 of [the tutorial](http://static.rust-lang.org/doc/0.9/tutorial.html), there is a mysterious sentence that says: Note: Trait inheritance does not actually work with objects yet, which could mean that it may be added in the future. Edit: https://gist.github.com/anonymous/fe87c51acc652fa049eb but it's ugly...
AIUI the goal is to have a model reasonably similar to git, i.e. "cargo" is made up of a constellation of smaller tools that compose together (along with the `cargo` binary as a nice interface to them all), and then communication is performed in a machine readable format: JSON (I think).
(/u/cmrx64 actually)
This is really good news!
I'm in Chicago and can probably help out a bit :-)
I'll make sure I ask anything over IRC, at least. 
I'm just dumping a few of notes here -- sorry, no links, just titles. Languages w/ linear types: * Clean (Nijmegen), * Mezzo (INRIA) * Alias Java (Aldrich et al), * Joelle (Ostlund et al) * Gordon et al: "Uniqueness and reference immutability for safe parallelism" Ideas in type systems that need linear types: * typestate (enforcing the protocol of a class), * session types (describing the message sends/receives of distributed objects) Random papers that come to mind: * Wadler, "Linear types can change the world" * Kobayashi, "Quasi linear types" * Morrisett et al, on Alias Types * Boyland on 'Fractional Permissions' * Haller, Odersky: Capabilities for Uniqueness and Borrowing (read the related work section there) As linear types and encapsulation are quite related in 'what they express', you might also want to check out: * Almeida et al on Balloons * "Ownership Types for Flexible Alias Protection", Clarke et al * Connecting Encapsulation and uniqueness: "External Uniqueness is Unique Enough", Wrigstad et al A couple years down the road: * hopefully my dissertation ;-) Also, you might want to xpost this at /r/programminglanguages
If there were expanded globing options, I think you would still use multiple lines for unrelated modules partially for this reason. Globing is very debatable, but I think at least some globing is better then listing every symbol individually on a different line. I'm not exactly sure where the middle ground is, but I think it could probably use some minor changes. I think the '*' globing should go, but maybe there is room for other globing patterns that are not possible right now. I guess that globing could also just move to a macro. It doesn't necessarily need to be defined in the language. I'm pretty sure this could work with a macro: use_glob! { std::io::{ [Acceptor, Listener], net::tcp::[TcpListener], net::ip::[Ipv4Addr, SocketAddr] } }; Macros can also allow for some Ruby on Rails convention over configuration type stuff when including modules.
They're not source control references, they are file system paths that in many cases *also* happen to be things that you can figure out where to get the source from. Second, for package versioning, the official recommendation for users to vendor their dependencies. Static linking and binary packages are due to Go, *not* due to Go's approach to package management.
That tuple matching is a handy way of faking an Option monad; neat! I'll have to keep that in mind.
Oh, that's good to know. Should be back in Tokyo soon.
http://cyclone.thelanguage.org/wiki/Papers/ That's where we stole our region (now known as "lifetime") system.
Here's where the Rust devs stick any papers that lend them inspiration: https://github.com/mozilla/rust/wiki/Note-research
I second Mezzo. Maybe it's just me, but I found Mezzo articles helpful to understand Rust's system. http://protz.github.io/mezzo/
Ha, sorry.
I'm not a ruster at all, so Mezzo doesn't help me in that regard. I like Mezzo, it's applying some creative work in PL theory (adoption) and wraps it all up in something that seems quite practical. Also, it's a good example of a functional language that uses linear types: I find it insanely cool how linear types are very interesting to both 'imperative' and to 'functional' languages. Imperative languages want their safety and functional languages want their speed (to oversimplify shamelessly).
Thanks for answering, some light has been shed. :) 
Be careful though, I have read the tutorial only once and tried to write something that would compile. I'm installing the master branch of Rust right now and will try to learn once again.
&gt; Option pyramids? Match tuples wherever possible! match (a, b, c, d) { (Some(a), Some(b), Some(c), Some(d)) =&gt; { ... }, _ =&gt; {} } The issue is that `b`, `c`, or `d` have to be evaluated first, rather than short-circuiting.
Not only that, but as the developer migration/explosion from SourceForge, to GoogleCode, to GitHub has demonstrated, the software development web as a whole is constantly shifting. And those are just the big guys: there are untold numbers of private sites, repos and dev communities out there that may eventually support Rust. IMO, it's a better move to design around a GET-able URL for package releases, and move forward from there.
Please see the subreddit /r/playrust. /r/rust is for the Rust programming language.
somethings up with the font, as far as i know its trying to use 'Courier New' which is supposed to be a monospace font, `maintext {color:#f0f0f0; font-size:12px; font-family:\"Courier New\"}` it seems to be referting to something else.. i'm not an expert in html 
can you say use std::io; use io::net; use net::tcp::TcpListener; use net::ip::{Ipv4Addr, SocketAddr}; use io::{Acceptor, Listener}; then you've saved repitition bbut still have it multi-line
My example only works if they are evaluated before the match expression.
Nice. Hopefully the manifest file will also end up being so generic that a future language (TM) can also opt to reimplement everything from scratch without relying on prior rust code. The syntax looks simple and promising https://github.com/carlhuda/cargo/blob/master/DESIGN/MANIFEST.md
You should add `monospace` after Courier New, this will put the default monospace in use if Courier New is not present.
I know what the problem is - the `maintext` tag is being closed too early for some reason. I've been looking into it for a while, but can't work out why.
Still, does one need *one* central repository ? Even in corporations, once get past a certain size, you may want to organize stuff a little better than "throw it all in one place". As a result, maybe that a *list* of repositories would be more useful (searched in order, or with conflict resolution...)
[toml](https://github.com/mojombo/toml)
I'd like to enquire about 2 things i had to do , has anything changed in the months since i wrote this:- [1] I seemed to need to make a copy of the AST to make it easier to get from a *code location* to an node ident and hence a node - where one doesn't know what type the node will be; was there a better way? [2] What about the 'cross crate map' - my solution to links working between crates was: - when generating a crate, emit extra data (.rfx) containing node identifiers/types and source locations. Is this information held anywhere else in rust / in its crate metadata? is there a reason *not* too (like the way C/C++ programmers can distribute proprietary binaries with declarations but no assist for reverse-engineering) If its not there - would you consider adding it.
Dont assume you know what everyone else needs :) Before I was pressured by 'the herd' into using std::vector&lt;T&gt;, i used to distinguish these cases:- -dynamic size, but not growable (most common case) -compile-time fixed capacity, growable (clustering algorithms) -General purpose growable -vectors with relative pointers held in 'blobs' (useful for fast loading times, or on systems with DMA between local stores, where you could relocate chunks of object graph) in most cases size was defaulted to int but replaceable , since i was dealing with clustering algorithms in most contexts it was known size could be 16bits or something like that. non-dynamically resizing buffers were common I welcome the change of [] to some sort of raw slice - its my default from 15 years ago. Its a useful distinction for people who need to minimize allocations, minimize wasted bytes in headers (space taken in caches, alignment padding)
The repo does mention *exactly* C deps. The repo does lots of things, and their design has handled lots besides fetching.
If only it had a decent text editor... ^(sorry, couldn't help myself)
I'm curious how package management techniques used in a dynamic language like Ruby will be adapted for a compiled language. 
It does! evil mode is actually a pretty good vi emulation layer.
Personally I find JSON to be a bit of a hack, I think it gained the following it did because it's not XML which is just painful to work with. People talk about how it's valid JavaScript code, but you normally have to parse it anyway because of security reasons or because there are broken libraries around that do things like insert ' into strings which may explode depending on the parser's strictness. Or because your working in any language other than JavaScript. Crafting JSON by hand is a bit of a pain because you have all the ugly and unnecessary open close braces { }. And the names have to be inside useless quotes. Meanwhile people are stripping out the whitespaces to save a few bytes. Personally I like YAML. It has a much nicer syntax, whitespace termination so no unnecessary characters but is kept human readable by the specification. It also has heaps of extra features like embedding binary data and comments. Unfortunately it's losing traction. The main downside to YAML is it's much slower to parse. But if speed is an issue, people should be looking at binary formats anyway, like Cap'n'Proto, Ubjson, Google protocol buffers, Apache thrift, etc... The other problem is the YAML spec is quite large so making implementations is a pain. I'm not sure if the officially blessed C library has been updated to the new 1.2 spec yet, which was released in 2009. it seem the Python guys are making patches to support it though. There is a 1.2 C++ parser but of course being C++ it's harder for it to cross the language barrier since someone need to write C bindings. toml seems like it wouldn't be much use outside of the configuration file domain due to the syntax (but I haven't looked to closely and it might be my personal preference) and the whole point of these formats is so we don't make custom ones specific to what we are doing every time we need something (although it still better to have a documented, open standard). toml is basically the markup language for package manager manifests. Wouldn't mind seeing a restricted YAML format. The matching the feature set of JSON but the syntax of YAML (and a few of the features when speed isn't an issue). [JSON5](http://json5.org/) seems good too, but I'm not sure we will see any traction. Still requires close brackets on things like arrays though and if we add things to JSON then it will just become slow and large like YAML. EDIT: There is also work on [YAML 2.0](https://github.com/yaml/YAML2/wiki) which aims to be simpler.
hehe glad i'm not the only one! i used to use 'Array' as what ~[T] will be, 'DynamicArray' / 'Buffer'/ 'GrowableArray' whatever for the 'std::vector' case, and I completely agree, its much less confusing to talk about 'arrays of vectors' than vectors of vectors :) Thats one reason I really liked the [] syntax.. removal of a name that there is argument about..
 Does cargo do build management and dependency management like maven or ant/ivy or only download packages like cpan does? 
Possible Noob type comment warning, I probably missed the point but... "extra checks are definitely a measurable unaceptable performance penalty on some platforms" With regard to array bounds checking, isn't possible just to write an unsafe array? Of course then you need a function call per access which is non-ideal as well. although I can see why non-bounds checked arrays can lead to bad things I kind of want them anyway....
For Context: I am C++-guy and so my suggestion may not be very rust-like. The way I read OPs request is that the floating-point types of rust make use of NaN, which is a bad idea because it destroys many important semantics (for instance a&lt;b and b&lt;c no longer implies a&lt;c). If I had to design a language completely myself, I would just outlaw every such operation (either it would throw an exception, call terminate or be UB). Since it appears quite unlikely to me that rust will be that bold, the second best solution would IMHO be the introduction of an additional set of FP-types, that follows these rules.
See [Signalling NaN](http://en.wikipedia.org/wiki/NaN#Signaling_NaN)s, although I don't know if it's possible to get one in Rust without manually constructing it. I wonder if there would be any value in having two distinct types: f32 and, I dunno, r32 (r as in real) where r32 produces signalling NaNs for any invalid operations, and thus can implement TotalOrd since you can't have an unordered value *and* operate on it.
#####&amp;#009; ######&amp;#009; ####&amp;#009; Section 4. [**Signaling NaN**](http://en.wikipedia.org/wiki/NaN#Signaling_NaN) of article [**NaN**](http://en.wikipedia.org/wiki/NaN): [](#sfw) --- &gt; &gt;Signaling NaNs, or sNaNs, are special forms of a NaN that when consumed by most operations should raise an invalid exception and then, if appropriate, be "quieted" into a qNaN that may then propagate. They were introduced in [IEEE 754](http://en.wikipedia.org/wiki/IEEE_floating_point). There have been several ideas for how these might be used: &gt; &gt;* Filling uninitialized memory with signaling NaNs would produce an invalid exception if the data is used before it is initialized &gt;* Using an sNaN as a placeholder for a more complicated [object](http://en.wikipedia.org/wiki/Object_(computer_science\)), such as: &gt; &gt;* A representation of a number that has [underflowed](http://en.wikipedia.org/wiki/Arithmetic_underflow) &gt;* A representation of a number that has [overflowed](http://en.wikipedia.org/wiki/Arithmetic_overflow) &gt;* Number in a higher precision format &gt;* A [complex number](http://en.wikipedia.org/wiki/Complex_number) &gt;When encountered a trap handler could decode the sNaN and return an index to the computed result. In practice this approach is faced with many complications. The treatment of the [sign bit](http://en.wikipedia.org/wiki/Sign_bit) of NaNs for some simple operations (such as [absolute value](http://en.wikipedia.org/wiki/Absolute_value)) is different from that for arithmetic operations. Traps are not required by the standard. There are other approaches to this sort of problem that would be more portable. &gt; --- ^Interesting: [^NANS](http://en.wikipedia.org/wiki/NANS) ^| [^Nan ^Province](http://en.wikipedia.org/wiki/Nan_Province) ^| [^Nan ^River](http://en.wikipedia.org/wiki/Nan_River) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cg670qd) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cg670qd)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
This doesn't really fake an Option monad since, e.g., there is no short-circuiting and no plumbing of data dependencies - basically, no `bind`.
Problems with git? The solution is darcs. For darcs is the solution to every problem (but worst-case performance).
Yes, indeed you are correct. I haven't done any pattern matching outside of Haskell and forgot that Rust isn't lazy. The idea was that if a turned out to be `Nothing`, then it would short circuit the evaluation of Some(b,c,d). However, I guess those get evaluated anyway, so it's just handy syntax.
Sadly I just left DC. :( Best of luck!
I guess TOML is one of the working config file format with a complex data structure and can be replaced with other alternatives. (Not to mention that there is already [rust-toml](https://github.com/mneumann/rust-toml).) I personally dislike TOML due to its serious lack of consideration so much that I have written a [spec](https://github.com/lifthrasiir/cson/blob/master/README.md) of a JSON-compatible config file format (yet to be implemented :S) though.
Someone else did the correct thing and actually asked about C libraries, rather than just jumping to conclusion: https://mail.mozilla.org/pipermail/rust-dev/2014-March/009110.html (No answer yet.)
I dislike all file formats. The only one that I could work with is YAML since it's simple but clearly defined. JSON is like XML, and TOML tries to do everything at the same time (a bit like Rebol which... failed) and its specifications are confusing. Well, I'll have to read learn all those formats a bit more but I'm really not happy about it.
JSON isn't a very nice language for humans to read or write (it's certainly possible, and not *horrible*, but still, not particularly nice). The "internal" transfer representation of data is actually JSON (i.e. the various cargo tools pass information to each other via JSON, [example](https://github.com/carlhuda/cargo/blob/4e236c8558f88a8107f90ef068e6e1a90cedc538/src/bin/cargo-rustc.rs#L25-L27)). cargo is designed so that the input config is fairly generic, specifically, there's a binary to convert the manifest configuration files into the internal format, and of that [there's exactly 2 lines that are TOML specific](https://github.com/carlhuda/cargo/blob/4e236c8558f88a8107f90ef068e6e1a90cedc538/src/bin/cargo-read-manifest.rs#L51-L53), *and* the second of those is just [a wrapper](https://github.com/mneumann/rust-toml/blob/1389ceb42b2ae04dac40c8b2d4af8fe21823ecbc/src/toml/lib.rs#L1077-L1080) for the `Decodable` trait. That is, it will be very easy to swap in a new format (or support multiple formats). TOML is good enough for now, so that the "carlhuda" team can focus on getting the core of it working.
basic JSON is pretty horrible for a human-editable config format. No comments, trailing commas are syntax errors, somewhat verbose, etc.
Thanks for the detailed information. I'll learn a bit more about TOML since it seems to be required for the moment. I'm learning Rust and I'll see if I can use cargo to generate libraries (.so) usable in Android and iOS. Rustpkg had some problems with that and I hope that cargo will fix it.
Good cross compilation support is planned: &gt; - Cargo should support cross-compilation out of the box. As long as your &gt; version of Rust and its standard library are compiled for the expected &gt; targets, a single line of configuration should be enough to get builds for &gt; those targets https://mail.mozilla.org/pipermail/rust-dev/2014-March/009090.html
Wasn't all the metadata going to live in attributes in the `.rs` files instead of in a separate manifest?
Is there a particular question you have?
Oh, I forgot to put the real question. I will edit the post.
ah, there it is, thanks. I never even learned the details behind the jargon.. because we only dealt with one case.. but we even had slang in the office.. "QNAN SPACE" , that place you go when your engine breaks, where you can't see anything. qnans are the correct way for game engines - and code is simply designed never to produce them :) .. and we always worked without exception handling; a game on a disk isn't supposed to have any 'exceptional cases'. Its just supposed to *work* :) 
There's been good feedback from the Rust community on the previous posts from Philip Reames on this issue. I'll again plug [LLVM weekly](http://llvmweekly.org) and the twitter account [@llvmweekly](https://twitter.com/llvmweekly). There is also an /r/llvm subreddit which is fairly quiet other than my postings...
You need to import the trait to use its methods. use dummy::Dummy; // at the top of main.rs
You can write a function that handles vectors of different sizes with `~[T]`, whereas a function with `[T, ..N]` can only handle vectors of one size.
I would like having two kinds of floats! As you suggested: * *f32* without TotalOrd (or with slow standard-specific total ord), * *r32* with fast SNaNs or with no-NaN assumption and with TotalOrd matching Ord and Eq. I often create small structs with floats and ability to use "derive" would be great.
Pierce's "Advanced Topics in Types and Programming Languages" has chapters on substructural types and effects/regions, though as my copy hasn't arrived yet I can't tell you how relevant these are to Rust or whether you should go straight to some papers mentioned here ...
What problem are you having with derive?
Because a vector can dynamically resize, it is allocated on the heap. The `size_of` you use only gives you the size of the data on the stack. I believe this [relevant issue](https://github.com/mozilla/rust/issues/8981) might be of use to you.
Also note that vector representation will change with DST.
I also had another idle thought about this. From my understanding, a `~T` is, at the hardware level, just a regular pointer. The trick is that the compiler doesn't allow you to do anything that would make it null. If you use `Option&lt;~T&gt;` then, I believe, it's still really a pointer, except now it *can* be null (which is used as the sentinel value for `None`). It can do this because the compiler is hard-wired to understand that `0` is a value you can store in a `~T`, but it's not a value it's actually allowed to have, so it's free to use it to indicate `None`. So what if you could treat any type the same way? Say to the compiler "if you happen to wrap this `r32` thing in an Enum that looks like `Option&lt;r32&gt;`, you should totally use (canonical NaN value) for `None`. By the way, you should *test* for `None` using (function that tests for any NaN)." Then, even if you don't use signalling NaNs to kill the task, you could define all the operations on an `r32` with the assumption that they won't be NaN. Anything which *can* result in NaN (like division or sqrt) would return an `Option&lt;r32&gt;` which you get by just casting an `f32` to `Option&lt;r32&gt;` (if it's NaN, it magically turns into `None`); testing for `None` would turn into a call to `is_nan`. ... although I'm not sure how useful this would be in practice. It'd also make arithmetic with operations that can result in NaN a huge pain.
All Come Join really fun server best community 
wrong subreddit.. :(
Please use /r/playrustservers instead.
Why isn't Vec&lt;T&gt; just mut [T]?
JSON is nothing like XML. It has simple well-defined data types, and is generally much less verbose. It is appropriate as a message format, much more annoying as a configuration file format, and totally inappropriate to represent a complex document, which is what you really want XML for. Also, no XPATH and no validation. I didn't know TOML, but it looks like it's INI++, which is fine in my book. YAML would have been a good alternative as well, especially for a guy with a Ruby background.
I'm sorry that I am unclear, I guess I am just confused... I guess my problem is that if I look at the size of my FillsBytes struct, I get 16 bytes on my 64bit machine, even though it consists of a 8 + 1 byte. I therefore thought the result 16 included word alignment, and I therefore wondered if this alignment would be present in vector allocation.
I think there is something wrong with the post. `~[T]` and `~str` aren't unique array/string, they are * owned* array/string. To say they are unique is confusing, it makes me think of how set behaves (i.e. no duplicates). Also `[T]` seems to be valid object type. From what I remember you can write `Gc&lt;[T]&gt;`
Unique comes from the term unique pointer. The Rust compiler uses the term unique pointer/string/vector rather than *owned*, and there's no consistency in the documentation. It's also a bit confusing to say *owned*, because `int` and `~int` are both owned, as are `[1, 2, 3]` and `~[1, 2, 3]`.
I could have sworn I've seen a lot more people referring to `~` as owned pointer.
Yes, owned is possibly more common than unique, but they're synonyms in Rust-speak.
I don't know which term is more common among Rust users, just that the compiler uses `uniq` and that nearly everyone uses `Uniq` or `Unique` when referring to a hypothetical library version. I changed the tutorial to refer to it as an *owned box* in order to use a consistent box term. The pointer *itself* is owned whether it's a reference, `Rc`, `Weak`, or `~` - that's why you can mutate the pointer held in the local variable with `&amp;T`, but not what it points at.
I don't understand why ~[T] will continue to exist when it has supposedly been replaced by Vec&lt;T&gt;. Will we continue to see code that contains ~[T]?
It's a different `~[T]`. Currently `~[T]` looks like: struct Data&lt;T&gt; { length: uint, capacity: uint, data: ...T } type ~[T] = ~Data&lt;T&gt; where `...T` just means that the data is stored inline (i.e. `Data` is a variable length struct). The new `Vec` replaces this with struct Vec&lt;T&gt; { length: uint, capacity: uint, data: *T } where `data` is just a chunk of memory with space for `capacity` `T`'s. The DST `~[T]` fall out automatically by `~U` being a type, and `[T]` being a type. I.e. `~[T]` will just be `~U` with `U = [T]`. The representation of this will be struct DstTwiddleSlice&lt;T&gt; { data: *T, length: uint } It's the owned equivalent to our current borrowed `&amp;[T]` slices. That is, `~[T]` will not continue to exist, but a type with that syntax will appear in future (nearly by coincidence).
At the most basic level, the `Vec` change was for consistency. The change in representation *had* to happen ([see the benchmarks](https://github.com/mozilla/rust/issues/8981): the new vector is 7&amp;times; faster), and making the `~[T]` syntax have that representation would make it inconsistent with `&amp;[T]` (although the current `~[T]` is more inconsistent with `&amp;[]`... but that's not an argument for continuing to be inconsistent). DST for vectors gives us a lot more flexibility, e.g. one can have `Rc&lt;[T]&gt;` i.e. a reference counted slice. DST for traits also gives flexibility, e.g. `Rc&lt;Trait&gt;`.
&gt; Yes, owned is possibly more common than unique, but they're synonyms in Rust-speak. Sorry, while I understand that `~` has similarities with C++ `unique_ptr` I have no experience using `unique_ptr` in practice, or what it unique refers to. The terminology used by this post simply confused me.
Because ~ works in combination with any other type, why make a special case to reject a specific one?
Implementing AST-based macros from other languages requires a compiler of that language, and text substitution macros from other languages can only ever be implemented as a subset of what they can do in their native language. (The usefulness of the excluded functionality, however, is up for debate.) A simple example: #define subroutine(name, ret, ...) ret name(__VA_ARGS__) // alternate syntax for C function definitions: subroutine(printf, void, const char *fmt, ...) { /* contents */ } If you intend to only ever use the isolated case of macros - X macros, constants, etc. which rely on only features of the preprocessor and the syntax of language constants, then you could write a simple preprocessor statement parser and convert it to an equivalent of the target language. Macros which generate C code would require a C compiler to expand, and macros which rely on existing parser state (for example, a convienience macro which expands some simple parameters into the header of a `for` loop to iterate a linked list) at expansion time are more or less impossible.
` #define subroutine(name,ret, ...) .... ` yes, thats exactly the sort of thing I had in mind trying; any idea on the behaviour of include! within a macro invocation though, that seems to stop me at the minute i got the impression rust's parsing of macro *input* might be versatile enough to read some C syntax, in that you can swap identifiers and seperators around... around i've already made custom annotations for a struct declaration (attaching information to map a vertex layout to semantics.. very pleasing compared to the options available in C..) ... 
The problem with parsing expressions like that is that they rely on the previous parser state and modify the next parser state. In order to implement this for another language either requires writing C around the macro (at which point, you're looking at embedding C in Rust, rather than parsing macros), or a 1:1 mapping between every language feature (and in this case, every *token*) in C and Rust, which I do not think exists.
This is in principle possible using syntax extensions - but as mentioned you would need to implement a fairly fully-featured C preprocessor and parser for it to work. The exporting a C header file is probably a little easier, but you would need to wrap most of the file in a macro I think.
Possibly also worth noting that with DSTs `box` should work perfectly fine to create a `~[T]`. I.e. `box [1i, 2, 3, 4]` creates a `~[int, ..4]`, which coerces to `~[int]`.
I still don't quite understand why you'd want this. It only makes the Rust code more confusing.
Not sure if this complicates or simplifies the whole pointer semantic, specially for newcomers.
The first argument to the closure is the accumulator of the fold, and the type is fully determined by the caller of `fold`. So since we don't need indirection to sum, it's just a `0` to begin with and then the result of the addition in every step. The second argument is the element of the iterator being folded over. Since you want to be able to get iterators over vectors of elements that you can't copy freely and that you don't want to move out of the vector, the `.iter()` iterator is going to hand you references. If that asymmetry in the `fold` call offends you, you could also convert the iterator to one that yields copies of vector elements by value: let sum: int = v.iter().map(|&amp;x| x).fold(0, |a, b| a + b);
&gt; TOML tries to do everything at the same time (a bit like Rebol which... failed) and its specifications are confusing. I'm kind of baffled at this. TOML is basically a standardized `ini` format for specifying configurations. Rebol seems like it's solving a completely different problem. What part of the TOML specification do you find confusing?
And the native library concern was answered: https://mail.mozilla.org/pipermail/rust-dev/2014-March/009117.html
Only certain casts are allowed, so you need to do one at a time rather than all at once. I justify it as being less error prone (though the verbosity is annoying), but I don't know the reasoning. One shortcut is to just use transmute. `let ptr: *u8 = transmute(&amp;something);`
Using transmute where it's not necessary is getting fairly close to an unforgivable sin, I think.
The feature is being moved, not removed. Moved into libraries, that is. I.e. one can get essentially identical semantics with types defined as a normal structs with normal methods and normal destructors. For example, the [reference counted `std::rc::Rc`](http://static.rust-lang.org/doc/master/std/rc/struct.Rc.html) is a task local pointer like `@`. `@` is being removed because it's not actually necessary in the language: Rust is powerful enough to put most of the definition in a library (possibly with some compiler hooks to give it a little bit more flexibility, but nothing like the integration that @ requires). This keeps the language itself simpler and easier to standardize, as well as making the compiler smaller (leading to, theoretically, fewer bugs).
I think it makes sense to help keep things in sync. Although I don't know that I would do it with macro_rules since ideally you want to load in an actual file. I'd like to see the opposite: Generate a C header file from a Rust interface to make it easier to use Rust as a back end for another language. I think there is a plan to allow syntax extensions to work with attributes. That may be a better way to express generating files.
What about heap closures? To my knowledge, there are stack closures, plain ol' function pointers and closures-that-can-be-called-once (`proc`). Now that I think about it, removing heap closures is probably orthogonal to removing the `@` sigil. But still, I'm curious why it was done and if it's a temporary thing or not.
Couldn't you do the `Some` wrapping internally to the macro?
That would remove the possibility of using other enums, like Result. As it works now, you can use any non-exhaustive expression, including numbers. The goal is to make it behave the same as 'match' pattern matching. Edit: To clarify, you can send a number to the next arm, but then it needs a pattern match or guard statement to avoid the 'unreachable pattern' error. If you take the whole value it will trigger the error because the '_ =&gt; ...' is never executed.
Removing the `@fn` closures was necessary for removing `@`, but other than that, it's reasonably orthogonal. The removal of the old closures made way for the (slightly) nicer closure story we have now, but there's still some desirable work that will hopefully happen (not necessarily for 1.0, though), specifically, syntactic sugar for unboxed closures. This automatically gives a nice equivalent to `@fn`, `Rc&lt;|int| -&gt; int&gt;` (reusing the current stack closure syntax for unboxed closures), and significantly more flexibility besides. E.g. we don't have stack once closures, but unboxed closures give that automatically; it's impossible to return a stack closure with a capture now, but is possible with unboxed closures; it's impossible to have a sendable non-once closure =&gt; unboxed closures. (Basically, if you have a problem, it can be solved with unboxed closures. :P ) --- To be clear, we do currently have non-sugary unboxed closures: they can be made with the keywords `struct` &amp; `impl`. ;P E.g. the following are equivalent now (with the hypothetical `Fn1&lt;Arg, Ret&gt;` trait) let x = 1; let f = |y| x + y; let z = f(2); // 3 struct Captures&lt;'a&gt; { x: &amp;'a int } impl&lt;'a&gt; Fn1&lt;int, int&gt; for Captures&lt;'a&gt; { fn call(&amp;mut self, y: int) -&gt; int { *self.x + y } } let x = 1; let f = &amp;mut Captures { x: &amp;x } as &amp;mut Fn1&lt;int, int&gt;; let z = f.call(2); // 3 One gets benefits from unboxed closures by, for instance, removing the `as &amp;mut ...` trait cast (i.e. avoid erasing the type, to achieve static dispatch), or storing `f` directly, not behind a reference. (One gets once closures by making the `&amp;mut self` just `self` to consume the closure on call, and "aliasable" closures by making it `&amp;self`.) So much power! "Adding unboxed closures" is essentially just making writing the above sane, rather than having to define structs everywhere. (Anyway, I don't thing that was really answering your question; but it's (hopefully) something to look forward to and it's related to closures.)
Oh, I misinterpreted: you are piping non-monadic operations through the monad macro, i.e. maybe a better example would be avoiding monadising the "pure" operations and just chain them directly: Vec::new() { _ =&gt; File::open(&amp;Path::new("hello.txt")), Ok(file) =&gt; BufferedReader::new(file).lines().map(|line| line.unwrap()).next(), Some(first_line) =&gt; from_str::&lt;uint&gt;(first_line), Some(n) =&gt; range(0, n).map(|x| x + 1).collect() } Reads the first line as an integer `n` and creates the vector `1, 2, ..., n`. (The last line could/should be using `Vec::from_fn(n, |x| x + 1)` but I wanted to demonstrate some more chaining.)
I updated the example (added a .trim() in the str to uint conversion)
/u/pcwalton sent these to the #servo @ irc.mozilla.org IRC channel a few hours ago.
Awesome!
That first one: "Kill... me...." Great job /u/pcwalton! 
I have a few problems with TOML: * It's case-sensitive compared to regular INI or [Python INI](http://docs.python.org/3/library/configparser.html) which are not. There are so many programs out there that spit out random case all over the place that it would have been more generic to be case insensitive. * datetime is a special type but it's restricted to one form only * global values in a configuration file feels weird, Python has a DEFAULT section * the "spec" itself is written in an unprofessional way which should be "fixed" (this is from the same guy who wrote semver which is a good spec IMHO) * As long as a super-table hasn't been directly defined and hasn't defined a specific key, you may still write to it: WHY? So a.b then a is fine, but a then a.b is forbidden? * You cannot define any key or table more than once... unleeeeeesss it's an array of tables which is a magical type defined later in the specification and kind of breaks the first rule. * You can indent as you please. Tabs or spaces. TOML don't care: yes, but it's never mentioned if indentation is relevant. It seem to be but it's only in the example. * Also about the indentation: it breaks the rule that "As long as a super-table hasn't been directly defined" (unless you count the "and hasn't defined a specific key" but it hasn't defined any key at all which is confusing, what is this magical specific key? Is it any key at all? Or maybe the same key?)
Nice update. A suggestion: put these updates to the project repository (eg as Pages or Wiki) for more robust archival.
thanks for the explanations. my usecase was something i'd do in C like this (extracting data inside a blob) well, I guess rust macros could make some nifty custom syntax for doing it in a more presentable way (i'd have used macros in C usually anyway) ... its just another point of surprise you've added for C/C++ programmers coming across. my take would be "you've already signalled unsafe{}, thats enough" .. making it robust is done with tests and by making code more verbose we've just got less energy to put into that.. the nature of what you're doing is something that the compiler can't check, or that the syntax can't communicate. Here's my use case as i've currently done it, i was going to experiment with some macros aswell which would be interesting too impl BspHeader { fn get&lt;T&gt;(&amp;self, d:&amp;DEntry&lt;T&gt;, i:uint)-&gt;&amp;T { unsafe { &amp;* (intrinsics::offset::&lt;T&gt;( (intrinsics::offset::&lt;u8&gt;(self as *BspHeader as *u8, d.offset as int) as *T), i as int)) } } struct DEntry&lt;T&gt; { offset:u32, size:u32 } in C i'd probably have written a 1 line macro but had to pass the type to invoke it. I can see a Rust macro will be a lot clearer with custom syntax... get_rel!(header.field[index]) vs C GET_REL(header,field,index,ResultType) // whats what in the call when those are real arguments? of course the problem with writing lots of macros is you're introducing vocabulary, its' a point where one programmer can argue with another ("what should this be called" .."who gets to maintain this".."is this common enough to put it in the library, am I going to have to carry this around with me").... I favour powerful syntax in the first place because its' so much easier to compose .... its' hard to have a function in mind and find out what its' called, its' easier to just write it.. [I dream of a duplicate code detector as a means of searching.. you just write what you want and then the tool tells you what it was called..]
Your code can be written as the following, which will hopefully reduce your complains of verbacity. Using a temporary or two will make it clearer. &amp;*((self as *BspHeader as *u8).offset(d.offset as int) as *T).offset(i as int) which avoids: - specifying generic type parameters: the `::&lt;T&gt;` and `::&lt;u8&gt;` are very rarely required - wasting key presses on the `intrinsics::` junk And, with [a recent master](https://github.com/mozilla/rust/pull/12764) the `self as *BspHeader` can be written as `self as *_`, which infers the type for the first layer. &gt; my take would be "you've already signalled unsafe{}, thats enough" .. making it robust is done with tests and by making code more verbose we've just got less energy to put into that.. the nature of what you're doing is something that the compiler can't check, or that the syntax can't communicate. `unsafe` doesn't mean the compiler has to stop helping you: in fact, I'd argue that's when you want the compiler to be as helpful as it possibly can. &gt; [I dream of a duplicate code detector as a means of searching.. you just write what you want and then the tool tells you what it was called..] One example of this is Haskell's Hoogle, which searches based on type; with parametric polymorphism and smaller helper functions this is pretty close to a "duplicate code detector" for small composable functions.
This is a very cool use of monads! As a non-Haskell programmer I was a bit confused at first. I updated the gist with your example (with the equivalent of what I believe you want to do): let x = Some(1); let y = Some(2); let z = monad!( None { _ =&gt; x, Some(x) =&gt; y, Some(y) =&gt; Some(x+y) } ); assert_eq!(z, Some(3)); This works because in Rust you can shadow variable names. Each arm matches against the result of the previous arm. Because the pattern matching allows binding of variables to a name, you can 'unwrap' x and y before adding them together.
&gt;&gt; since ideally you want to load in an actual file Yes thats what i was trying to do ... `parse_c_header!(include!("foo.h"))` ... but i couldn't get the `include!` part working, so just tried to see if the parsing would even work. 'parsing C macros' was another possible angle, writing declarations in one set of macros designed such that both the C-preprocessor and Rust macros can understand
I was actually looking at them and thinking "looks like we got a bleeder here!"
You can take reference counted pointers (or any custom smart pointer) to trait objects and vectors, without an extra layer of indirection. This is very important for performance in many cases. It also allows you to write `impl Trait for [T]`, which is useful in some cases. Finally, it means that you can pass an `&amp;Trait` object to a `T:Trait` function, which is a major annoyance that will be fixed.
I had not yet seen the hammer.rs library, and that's a really nice library! Thanks!
You are absolutely right! Rust does not have first class functions on the level like Haskell, so constructing monads with a binary operators is not that "idiomatic Rust". The intention of this macro is to solve the problem of nested match expressions. What makes it a bit confusing is because it is modeled to be of similar syntax as 'match'. Each arm "a =&gt; b," follows the same logic as the arm of a match statement, except that the value matched against is the result of the previous arm. I use '_ =&gt;' to indicate the start. The macro takes advantage of this symmetry when expanding recursively. A composition of two functions (A -&gt; B) and (B -&gt; C) is another function (A -&gt; C). This means you can use any type in between! The pattern matching works as a "filter" and the value before the bracket is returned if stops before the last arm. I am considering calling it something else than "monad!" or inserting a "do" in front: do &lt;start&gt; { _ =&gt; &lt;start&gt;, &lt;a0&gt; =&gt; &lt;b0&gt;, &lt;a1&gt; =&gt; &lt;b1&gt;, ... =&gt; &lt;result&gt; } else &lt;err&gt; Edit: Updated syntax
Based on feedback in this thread, I changed the syntax to make the information flow clearer: let &lt;name&gt;: &lt;type&gt; = monad!( do &lt;start&gt; { &lt;pattern0&gt; =&gt; &lt;step1&gt;, &lt;pattern1&gt; =&gt; &lt;step2&gt;, &lt;pattern2&gt; =&gt; &lt;step3&gt;, ... =&gt; &lt;result&gt; } else &lt;err&gt; ); 
&gt; it's impossible to return a stack closure with a capture now, but is possible with unboxed closures; it's impossible to have a sendable non-once closure =&gt; unboxed closures Yup. I ran up against this while writing `quickcheck`. In particular, I wanted to be able to capture functions that fail a particular property *including* failures resulting from run-time errors like out-of-bounds errors. I dug into how `rustc` does this with `#[test]` and I came up with [running each test in its own task](https://github.com/BurntSushi/quickcheck/blob/master/src/lib.rs#L384). I couldn't figure out how to make this work with stack closures (and I suspect your comment has just confirmed that it's currently impossible), so I had to limit properties to `fn` types only. (Which isn't too much of a problem in practice.) On a related note, I don't think this handles stack overflows... That'd be nice to capture too. &gt; To be clear, we do currently have non-sugary unboxed closures: they can be made with the keywords struct &amp; impl. ;P Heh. I want the compiler to figure out free variables automatically, dammit! I think, reading your comment, the answer to my question is basically: "We're trying to make the language more consistent before 1.0. This process has resulted in the loss of sugary heap closures, but they should return in one form or another at some point." Is that fair? Anyway, thanks so much for your detailed response!
Any references to this effort?
This particular sugar is "monadic style", but yes, the name is a bit confusing. I considered naming it "do!" but that looks kind of ugly with the 'do' keyword inside the macro. Suggestions are welcome!
Thanks for the feedback! I'll settle for `chain!` for now. It was not my intention start a war with Haskell programmers ;-)
&gt; No problem, there's no point scaring people with "monad" if you don't have to! The OP could rename it to `warm_fuzzy_thing!` and all fear will melt away. :-)
That is interesting but it sounds very complex to implement -- possibly too complex for Rust. One thing to note is that the signature of Deref is actually trait Deref&lt;Result&gt; { fn deref&lt;'a&gt;(&amp;'a self) -&gt; &amp;'a Result; } So the type it returns is somewhat arbitrary. But doing complex calculations is beyond it (as you recognize).
&gt; I suspect your comment has just confirmed that it's currently impossible Yep; for the moment, stack closures are not able to be passed to another task (the new `Share` kind will possibly fix this somewhat, by allowing libraries to do some `unsafe` hackery to spread stack closures across tasks, but still expose a properly safe interface). &gt; On a related note, I don't think this handles stack overflows... That'd be nice to capture too. Yeah, currently we just do the (second) easiest thing on stack overflow: abort the whole process, so you'd have to fork/run a new process and monitor the output to catch that. &gt; I think, reading your comment, the answer to my question is basically: "We're trying to make the language more consistent before 1.0. This process has resulted in the loss of sugary heap closures, but they should return in one form or another at some point." Is that fair? I think that is reasonably fair, yes.
of course deref is actually a unary operator, this might require it to be binary .. not sure of all the details on how this would work. well back to the original problem i guess macros can clean that up , by rolling accessor methods (they seem able to scratch many itches, just have to escape the stigma the word 'macro' has from C)
&gt; of course deref is actually a unary operator, this might require it to be binary .. not sure of all the details on how this would work. How does that work? Also a binary `*` operator has a preexisting name and function.
&gt; urse deref is actually a unary operator, this might require it to be binary .. not sure of all the details on how this would work. perhaps what i'm describing here would have to be another operator, (or worse still , a hidden double call to unary and binary versions of "." where you see a "." . imagine a new operator .? object .? member sequence of events.. [1] calls '.' `let a=object.deref() ` [2] adresses the rhs, `let rhs= &amp;a.member ?` [3] calls '.?' `let c = binary_deref( object, rhs)` thinking out loud... could something be generalized to encompass the mechanism behind Trait Calls and potentially Fat Object calls /implementing C++ style classes . (hmm, that would be more complex still since the rhs could be something thats supposed to be in the vtable or something thats supposed to be in the struct) i know some people complain already that in C++ yada.foo() can mean different things (simple method call, vtable lookup, calling a function pointer, and i even discovered recently a use that escaped me for years, it can also call a static method of yada's type..) 
This is really cool. I could see this getting promoted from a macro to the language feature that uses the 'do' keyword Rust has been hoarding. I added an [implicit else](https://gist.github.com/jfager/9707714), where as long as all the arms have the same return type, the short-circuit return value can be the fail arm from somewhere inside the match chain: let res = chain!( do Ok(1) { Ok(x) =&gt; Ok(2), Ok(y) =&gt; Err('z'), Ok(z) =&gt; Ok(x+y+z) } ); assert_eq!(res, Err('z')); 
So basically you'd like to have offset.field where offset is already an offset do further offsetting without needing to call a suboffsetof!() macro? I'm not sure if that's worth the complexity, and anyway it looks like it would need high order types or associated types to express properly. 
its overkill for this specific usecase, but the question is broader, i speculated it might be possible to generalize several things related to object component acess.. kill multiple birds with one stone. eg what more could be done with vtables ... in C++ i'd always missed static virtual data (which could be done with manually rolled vtables). would people find a use for custom trait tables and acess. I know at the minute , for considerations on servo, they are discussing how to retrofit internal vtables 
Impressive, how long did it take for FF to take acid2? :)
This is a good idea! Updated the gist.
/u/jfager added implicit else in which case all the arms require same types. It makes it easier to retrieve Err(msg) from the individual arms when using Result.
There are times in C++ where you sometimes need to make chains of casts as well. When a particular chain makes reoccurring appearances, you can refactor it into a cast-like function template: template&lt;typename Dest, typename Source&gt; whatever&lt;Dest&gt; my_quasi_cast(Source source) { return static_cast&lt;Foo&gt;(reinterpret_cast&lt;Bar&gt;(static_cast&lt;Baz&gt;(source))); } // looks like a cast: auto b = my_quasi_cast&lt;Qux&gt;(a); It’s a mix of convention (with the name ending in `_cast` to look like `static_cast` etc.) and good refactoring practices, really. Do you feel strongly against writing (once) a function/macro that performs what you need and calling it from multiple places?
&gt;&gt;There are times in C++ where you sometimes need to make chains of casts as well. I never noticed because I drop back to C when it suits me :) &gt;&gt;"Do you feel strongly against writing (once) a function/macro that performs what you need and calling it from multiple places?" I can do this sure, i have done elsewhere, the issue becomes packaging all of those helpers, and how they start to make 'my' code diverge from anyone elses 'idiomatic' code. something else to argue over. submit some code to someone else and they say "i dont like your weird macros" .. its possible a few extra helpers in the stdlib might ease things alot.. ".to_void_ptr()" , ."offset_bytes(..)" perhaps. but it just seems crazy to have to create extra vocabulary for this in the first place. You've got 'as' in the syntax, and in C its helpful that void* doesn't need casting, thats its' explicit purpose Its great when you can build nice clean abstractions, but some parts of programming involve dealing with factors outside of your control (hardware, legacy libraries / binary formats, need for unexpected workarounds for badly designed hardware , old compilers / buggy compilers , ... )
&gt;&gt;"Do you feel strongly against writing (once) a function/macro " actually further to my last reply, in my last workplace individual programmers were explicitely banned from writing their own helper functions/macros. there was someone who's job descriptiion became "coding standards nazi", (yes that was the exact wording the boss used) to enforce it. 
LLVM doesn't support signalling NaN at all. Anything relying on signalling NaN, floating point rounding modes or floating point exceptions has undefined behaviour.
Let's clarify a few incorrect statements in the post above: 1. "the 'safety' comes from writing tests" - tests can only prove that the tested scenarios could not happen. They cannot prove correctness. Static analysis however does prove correctness of certain classes of errors. 2. "Asm -&gt; C was an unambiguous step forward" - when compilers were invented, ASM programmers objected to writing code in higher languages such as C due to their overhead. In general - new ideas are almost never a result of a person evolving their perspective but rather they come from new people. Even Einstein got stuck on trying to prove an incorrect theory for most of his life because he refused to accept newer ideas and theories (even though they were based on his own theories!). Yes, it's hard to learn any new language and in order to really be fluent it requires not only to learn the language syntax but also its idioms. For instance - clarifying your intentions explicitly is an important idiom for safety purposes, even in "unsafe" code. It allows other people including yourself in the future to know exactly what the code does. I can testify from the other side - as a young programmer at my previous job, I had to use some legacy FORTRAN function written by some veteran with decades of experience. The code was utterly unmaintainable: the function name was a random pick of 5 consonants that means absolutely nothing and the parameter list was a bunch of single letter integers. Even the god damn comments had no vowels in them! I'm sure that legacy hero was very proud of his brevity but for me it was an absolute nightmare that took several days to brute force into working. So no, brevity is NOT a good thing when it hurts readability and having a "code standards nazi" is a GOOD thing. more like "Code standards protector super-hero" to me. 
That's a pretty horrible workplace if you don't get to write some small abstractions to assist with code.
&gt;&gt;"the 'safety' comes from writing tests" - tests can only prove that the &gt;&gt;tested scenarios could not happen. They cannot prove correctness. &gt;&gt;Static analysis however does prove correctness of certain classes of errors. ... yes but Rust acknowledges that not everything can be predicted or handled by static analysis, hence unsafe{} When we got C, yes we were scared of the compiler, but we could and did interface directly to asm. Our commercial niche was getting things done *before* the tools were mature.. (which is why platform owners came to us to get showcase titles) It took a long time for compilers to be updated. you could wait like everyone else, or get things done *first* and stand out. &gt;&gt;So no, brevity is NOT a good thing when it hurts readability getting things working is more important IMO, surely. 
its the actual process of building those abstractions. And we dont have a choice. We have C libraries and strange datastructures imposed on us. Sony arent' going to be rewriting libgcm ,libgxm in rust. gl is going to stay in C. etc. Unsafe code can be written in small tests, and you see corruption very quickly. having to type 3x as much isn't going to save you from the obscure problems. Being able to write good visualizers is. 
I would rather wish rust would replace it's cast-syntax with the one C++ uses (static_cast, dynamic_cast, …) since casts really should be verbose in order to make them easy to find and ugly in order to make people think twice about using them.
https://gist.github.com/dobkeratops/9716302 here's a load of helpers I made to clean up what I was doing; these made it more decipherable (i.e. lets me actually see what i'm trying to do , instead of having a wall of casts and useless intermediates obfuscating things) It would be interesting to see what the minimum set of helpers would be that would do it. Streamlines 2 issues:- [1] the casting to &amp; from particular objects [2] the extra strict integer casts, (having to manually *up* cast is a bit much, i can well understand not wanting to throw away significant bits when downcasting of course) You all seem to know more about what i'm doing than me ? A very common use case for us was precompiled object graphs. a toolchain (preprocessors) manipulates dynamic datastructures, (which all needs to be safe, fine, abstracted pointers &amp; collections are perfect)... but the end result is just blobs, they're 'safe' and 'efficient' by virtue of eliminating memory operations, having been clustered, sorted, potentially using compact offset pointers , and being immutable. They allow fast loading times, and make the runtime *simple*. Consoles usually have very weak processors, relatively speaking. its bad to be dogmatic I think. one size does not fit all.
yes. What happened was, instead of having 10 copies of the same macro, you had 100 places where someone wrote it manually inplace (which then needed to be optimized individually; and this was a CPU with an in-order processor that practically ground to a halt with an IF out of place ,or information moving between SIMD/int or whatever). At least duplicate functions can be renamed and mapped to whichever is best :) I have very bitter memories of that episode. Sometimes its too hard to *find* the right function. Naming is hard, as is figuring out which existing name someone else wrote corresponds to the simple set of operations you have in mind. 
I think it's more that supporting it would greatly hurt the ability to optimize, so it will only be added if it's optional and modelled well for the sake of optimization when it's enabled.
but Rust Casts are already easy to find because it has a keyword: "as" ... and you got unsafe blocks, surely thats enough. I'm sold on Rusts' appeal for having a cleaned up syntax redesigned from the ground up. the easy grepping is great. People have no choice about what to use sometimes.. making painful processes even more painful just wastes time surely, time that could be going into refactoring to avoid the messy casts in the first place.
&gt; You all seem to know more about what i'm doing than me ? I think it's more that we're thinking about the more common cases of `unsafe`, rather than the edge case that you're wrapped up in. i.e. the consequences of relaxing more rules in `unsafe` are not just "make this subset of code easier to write", they're also "make incorrect code easier to write". I'm mainly concerned about the second category, correctness is *really* important. You can say "just write tests" all you like, but getting the compiler to catch it straight away is so much better. &gt; one size does not fit all. Yes, but making the `unsafe` portion of the language harder to get right is bad for almost everyone who uses unsafe; and doing it just because someone is doing raw pointer twiddling which has literally no replacement other that casting to `*u8` and back (i.e. a rare edge case)? That's seems silly.
&gt; again this post just makes me think i'm wasting my time :( If by this you mean "I'm not going to convince anyone that `unsafe` should be changed to make my specific usecase smoother at the cost of compiler assistance for most other uses of `unsafe`", then yes, you are probably correct. &gt; I can get things working in C++ significantly faster (2-4x). I'm ditching mature tooling, force of habit, 2 decades of experience, and moving away from a huge resource of existing code.. Is this surprising? Should Rust really be going against its goals of safety just to counteract the effects of C++ existing for so long and being used so much? That's not a very long-term view: what happens in 10 years, when there are people who have been using Rust for a decade, and are still hitting memory corruption bugs because we decided to loosen the rules just to help new C++ migratees write some code a little faster? &gt; But you've got both safe and unsafe and should consequently be able to handle the full spectrum of coding styles. Rust is designed for safety, `unsafe` code is an escape hatch that should be used in small patches to assist with building abstractions or calling other libraries (or, when nothing else works, for performance). --- Do note that `unsafe` code in Rust is not just "you can do whatever you like and it's ok". There's lots of guarantees that have to be upheld inside and outside the `unsafe` code; that is, it's easy to invoke undefined behaviour, e.g. if you use `unsafe` to create aliasing `&amp;mut` pointers, or modify the contents of a `&amp;` without using the `Unsafe` type. In general, writing `unsafe` code needs *real* care, and often detailed familiarity with the language.
the compiler doesn't know the structure of the data in question, and I also frequently have to experiment to figure it out. its might be documented in english (a lossy/imprecise format..), or the documentation might be someone elses working code. The type system doesn't cover it, so the compiler can't catch all the errors. if double casting helps some people , then great, I'm not saying *they* shouldn't do it :) Empirical verification is still needed. 
i *would* be interested in extra information in the type system to encode more of whats going on , see my other comment with ideas on 'Deref' . the feedback was, as I expected, ***its too complex.***
&gt; The type system doesn't cover it, so the compiler can't catch all the errors. My point is: in most cases, the type system does cover it (or almost does), and the `unsafe` code is just needed for one small thing. In your case, you have a peculiar input format and so need `unsafe` a lot more than usual.
&gt; it seams like I/o and everything in the library is incredibly verbose, I don't believe this is true. A few things *are* more verbose, but not incredibly so. &gt; changes every update We are iterating to find the library designs that work best, so stuff changes. If you're not wanting to put in the effort to keep up-to-date (which is perfectly understandable), then waiting for a while until things are really starting to get stable is probably the best plan. &gt; is it any different than just being careful? No, not particularly, but you'd have to be ridiculously superhuman to never ever make a mistake. The compiler never lets up its concentration and automatically catches pretty much everything that can lead to memory safety bugs (like use-after-free, dangling references, null pointers, data races). If you are capable of writing absolutely perfect C/C++, then you probably don't get any benefit from Rust. However, if you do say that you are capable of writing correct C/C++ *always*, well... I won't believe you. This only gets worse when you have multiple people of varying skill levels working on a project, where a single misunderstanding of how two components interact can lead to horrible horrible bugs in C/C++, but merely compile time errors in Rust.
When Carmack himself favours static analysis over carefulness alone (and has not been able to stop making mistakes himself), you know it's time to give up on the idea of just being careful and get some harder guarantees from your language + tooling. 
I'll ask you something that I asked so many times before: If you could completely remove whole categories of bugs from your code, then why wouldn't you?
I think this is something that will improve over time. People will write abstractions to reduce the amount of unsafe code you need to write manually. C libraries that are unlikely to be re-written in Rust can use inline wrappers to improve their interface. Strange data structures can often be treated as opaque in a wrapper. You can't simulate race conditions easily in small tests. Race conditions are always my biggest fear when I'm writing in Objective-C or C++. Security audits are also much more difficult relying on coding standards alone.
They don't make much sense with the Rust type system.
Yeah, that should really read "a *certain* category of bugs". ;)
Of course there are trade-offs. A safer programming language is less expressive, in which it tries to allow for correct programs, while prohibiting incorrect programs. But it is inevitable that it will prohibit some correct programs, too. The point is that, for most use-cases, having more safety is better, because most programs fit on the "correct programs" the compiler will allow. Fortunately, on the occasions that you DO need unsafe code (let's say, your own implementation of a doubly-linked list), Rust has "unsafe" blocks. So, if something goes wrong, it's easier to audit exactly where it went wrong, while not hindering your expressiveness.
I just don't like to proclaim which set of trade offs is the right one for people. Particularly with respect to general purpose programming languages. There are too many people on reddit who think they know which PL (or which set of trade offs) you should be using in most circumstances. I don't want to be one of them. To be clear, I'm not subtly attacking Rust or safe languages. I love Rust and have been having a lot of fun with it lately. :-)
Well, there are whole categories of bugs that are removed, but it's not *every* bug. Buffer overflows, dangling pointers, null pointer dereferencing, runtime type errors. But yeah, aiming at every bug ever is not reasonable.
&gt; If you don't mind about that (I think the "that" isn't referring to anything: maybe you meant to refer to something like "if you don't mind about getting very high-performance and/or close to the machine ..."?)
No worries! I can definitely agree with that.
I think the fundamental goal of a programming language is to give you as much confidence as possible that after you write a program, it compiles, and passes basic tests, that it also works properly in all cases. I believe Rust is perhaps the language that gives you that more than any other language, since it prevents both memory corruption and data races between thread. BTW, this is most useful for people who are "stupid", although in practice even intelligent people often fail to produce correct programs, as you can see from the endless stream of security holes everywhere, many of which preventable by using a safe language. 
Well if you are a sole developer on s small code base, I guess you can use C/C++ and just be careful. If you maintain a large code base and share with many other developers, things tend to get more complicated and confusing. Having the compiler point out the worst mistakes is a huge help. 
An exploit in Rust? If it's a memory safety exploit you should file a bug, Rust is meant to be free of them.
Am I missing something? This is an exploit in Rust the game, not the language?
&gt; If you are capable of writing absolutely perfect C/C++, then you probably don't get any benefit from Rust. Even if he can, his co-workers probably can't. Or the other people who will later on work on his code. This is why its great that the language is safer.
True. Being careful is pretty useful and can go a long way however for other areas like scripting and managed languages. Memory corruption due to faulty pointer arithmetic is a security hole, a IndexOutOfBoundsError usually not. This is why I'm so exited about rust: it cranks compile time checking to the max exactly where it counts: a system language.
the 'perculiar input format' is the default for allowing fast loading times, reducing allocations, compression, (synergy between compression and cache coherency) I was looking forward to the fact that Rusts excellent macro system would actually make rolling these kind of datastructures easier. declare a macro that generates both the 'dynamic' view of data with reallocatable components , and another version of the same thing compressed/concatenated into a single blob, and generates the boilerplate for translating between them. Rust would actually let me use these sort of things *more readily* :) another case i should mention about 'weird interfaces' is sony's libraries sometimes have opaque structures (forward declared) , you're just given the size as a constant :) so you create your buffer on the stack, and make calls with that. I haven't looked into the 'rustic way' of doing that, i suppose you'd make a newtype holding a [u8,..N]. 
Uh, or other people who code the system or applications he uses :)
I think it's safe (ha!) to say that for a systems language, where memory bugs lead to security holes for many people, pretty much any tradeoff in favour of memory safety is acceptable. I think we can agree on that: if the use case is known, there are empirical criteria for acceptable tradeoffs.
Even as sole developer on a small codebase (aka side-project), a single memory corruption can be very painful to track down :(
&gt;&gt;" but I dont see the benefit," **These are the benefits that drew me to rust:-** [1] **lack of header files.** The interaction of classes and header files in C++ annoys the hell out of me. I find it easier to reorganize code in rust. Context Free Grammar contributing to eleminating headers. [2] **'open world' idea of traits,** bolt new functions onto a type without having to derive a new type. inheritance sucks (generally, although single inheritance would be nice some places..). Closely related to [1]. I dont like the asymetry in C++ between methods and funcitons. sometimes i drop back to functions just to avoid having to update the header ('its not part of the external interface!'), then feel compelled to change them to methods because the asymetry makes code look a mess. bouncing back &amp; forth. [3] **"cleaned up C++"** better lambdas , more elegant expression based syntax, better type-inference, ~T vs unique_ptr&lt;T&gt; (if unique is supposed to be so common, it deserves syntax).. I enjoy this alot. (ok, C++14 has 'auto' in its lambdas now - which is where some doubts about the need to move begin to resurface). I've never liked C++ iterators, but I like the style of code where you pass lambdas in. I became interested in it from performance work, (which is parallelism on many levels, even in a single thread, and the mentality behind lambdas often suits that). C++ people say, 'lambda is just sugar for a templated functor' .. well its a pretty big deal for making a certain coding style useable. Rusts iterators work well, i think. [4] **Restricted globals**,(requiring unsafe{} ) I have always agreed with reducing global variables as a means of making code more modular / predictable to deal with. that was before multicore, and its way more important in the multicore era. I wish C++ had something like 'const' or 'pure' communicating that a function can't acess globals. I dont accept class / statics as the best way of dealing with it. [5] **pointer aliasing assumptions** - for performance C/C++ you sometimes need to use restrict to allow the compiler to cache temporaries in registers. this was a big deal in console development. This is a concrete, measurable performance benefit that's a posative side-effect of safety. It comes from the idea that immutability is making parallelism easier. In this case, ILP, not even concurrency/threading. lesser draws, [6] **tuples**, not having to create so many temporary helper structs. ok you can template them in c++, but you get the destructuring assignmnent and just throwing brackets around things seems more elegant. [7] **"a change is as good as a rest"**. I dont trust JIT/GC, so I have only had one serious choice in language path. asm-&gt;C-&gt;C++ forever. I've looked in envy at 'the rest of the world' having choice.. Java,Scala,C#, F#, . Its great to simply see an alternative appear. Even if the experiment fails its seems worth performing. **Then I discovered these features that make me want to persevere and stay with rust:-** [8] **Enums / match** .. switch on steriods.. tagged unions are useful, contrary to the false religion of OOP that classes should handle all polymorphism. There's a lot of simple cases thast these handle better than classes. I hadn't realised how good this was because i hadn't used the languages like ML where it supposedly originates. [9] the **Macro System**! .. this is **amazingly** good, one has a stigma toward macros as an anti-pattern from C/C++ but these macros with their repeats and abliity to parse datastructures make it easier to do data-oriented work .. things i've tried to fudge in C with the 'x-macros' pattern. the ability to declare datastructures with anotation, making associted metadata.. for example streamlining how shaders constants &amp; vertex formats interact. All can be done in a very natural way. From C/C++ I'd always looked in envy at Lisp macros.of course you could always write special code generator tools, but its so much nicer having it right there in the language. **Safety,** i'm in 2 minds about. Basically you can't keep track of everything yourself, or what other programmers do, so I'm sold on the idea of automating as many checks as possible through the compiler, and being able to communicate more about how your functions are supposed to be used. I've always liked the idea of const correctness and cared about that way more than "public/private"... const correctness doesn't even work properly in C++ due to aliasing. and see point 5. safety can help optimization sometimes. but there are many classes of bug and you need to write tests for other reasons.. visualizing whats going on with intermediate structures.. Simple memory errors usually show up quite quickly. If you get as far as creating correct inputs &amp; outputs to your code, memory probably wasn't an issue. so basically i'm not really finding rust reduces the amount of tests/debug code I need to write. I'd still want Rust if it had an --unsafe compiler flag :) Past couple of days i've been having some thoughts when writing unsafe code - the issue is , and real work I ever do is going to require interaction with C/C++ libraries as the main focus (infact i'll be lucky if I can even use rust in any real job). Whilst i'm taking tme out and looking at rust, the rest of the world is busy doing real work. All the low level code people need to build on exists in C++ and is being incrmenetally improved, and 'safe application code' seems to be moving to C# At the minute , even unsafe blocks still seem to carry some verbosity as you indicate , and this makes using C libraries more unpleasant than it needs to be. This could be eased with helper code, (I can write my own , but it negates the 'teamwork' benefits that others talkabout here), and I have some ideas on how things could be streamlined but they dont seem very popular :( (relaxing pointer coercions in unsafe, and ideally reintroducing C's relaxed void*, to make direct interfacing with C smoother... C++ is ubiquitous because it has C embedded, rust doesn't have that benefit.. you dont notice the extra casting in C++ because most of the time it freely interacts with C) It seems crazy to have to create so much vocabulary for basic operations when you've got cast syntax built into the language ("as") and you've already signalled what you're doing with unsafe {} and more information in the function prototype. **The things that would draw me be back to C++** [1] **"fear of isolation"** - anyone I know in reality is doing real work with C++, C# , objC. I doubt there's any jobs available to me where I'd have the freedom to choose Rust, and there's a huge body of opensource in C/C++. [2] **IDE's** as my main personal reason. Its very possible the rapid *navigation* of a foreign codebase is way more important. Rust has a theoretical benefit with context sensitive grammar. There might be a chicken/egg situation , without users there's no demand to develop an IDE, without an IDE it gets fewer users. I doubt the core team can prioritize it, they have to get the language fundementals solid. They know their own code so they can navigate. its as if the "dot completion" is actually the main reason class/method syntax exists. (write a parameter and instantly the IDE tells you the applicable methods, that is amazingly useful) [3] **familiarity**. I want to use rust, i'm a big fan, I'm still 2-4x faster at coding in C++ :( its possible 20 years of habit within myself is just impossible to fight. [4] **function Overloading**, I like straightforward overloading on multiple arguments - you *must* create/name additional entities (traits) to do it in Rust, (although I suspect it will be possible to make a macro to setup the 'multiple-dispatch' for many paramters.. and maybe even automatically create the trait for single functions.. ). I like trailing defaults too. Saves creating extraneous helpers.. do more in one place. [5] **near enough superset of C** / easy interfacing with plain C. I still like C, i dont look down on it at all. it was the right language to dramatically reduce the amount of asm the world needed. C++ is ubiquitous because it builds on C. C is a peak that lets you get a lot done without having to have agreed on the next layer of software organization principles that the world is still seemingly arguing about.. - I know people who will reject Rust because they've been heavily brainwashed into thinking in Classes, - and you've got the opposite pure FP crowd who are convinced any mutation is wrong, - and you've got people who think GC is the way to go).. C works close enough to the way the machine does, and whilst you're off writing vector maths abstractions with templates hoping the compiler can handle it, there were machines that had single instructions and dedicated registers for 3d maths .. dot product instruction.. C with intrinsics works well and you can reason about the asm it creates. Some people have responded to my posts with claims that 'bad things should be hard to write' .. well for performance and reducing allocations, C gets that right :) If I can't myself yet code as fast in Rust, it makes it less likely that I can establish my own self-driven 'real projects' in it. (whilst in the meantime I have ex-colleagues that have got their own new products out, established new businesses, in C++,C#)
&gt; the 'perculiar input format' is the default for allowing fast loading times, reducing allocations, compression, (synergy between compression and cache coherency) Yes, but *most* people aren't doing something that needs all of these for reading a pre-generated blob. &gt; declare a macro that generates both the 'dynamic' view of data with reallocatable components , and another version of the same thing compressed/concatenated into a single blob, and generates the boilerplate for translating between them. Can write an external syntax extension (i.e. a syntax extension written in Rust code that is dynamically loaded when expanding macros) to create all this. [An example](https://github.com/sfackler/syntax-ext-talk/blob/gh-pages/simple-ext/lib.rs) and [a talk explaining it](http://sfackler.github.io/syntax-ext-talk/#/). &gt; another case i should mention about 'weird interfaces' is sony's libraries sometimes have opaque structures (forward declared) , you're just given the size as a constant :) so you create your buffer on the stack, and make calls with that. I haven't looked into the 'rustic way' of doing that, i suppose you'd make a newtype holding a [u8,..N]. I don't think this is particularly unusual (although most libraries have it completely opaque, i.e. handling pointer only), but yes, that newtype is the correct thing to do. (Unless there are some alignment concerns too.)
&gt; The compiler [...] catches pretty much everything that can lead to memory safety bugs (like use-after-free, dangling references, null pointers, data races) I'm happy about Rust's memory safety guarantees, but I don't quite understand why it disallows more than one mutable borrow at a time. Obviously it makes code clearer and less buggy, but it's not actually for memory safety, or is it? Perhaps it helps against data races, but doesn't it also disallow multiple mutable references when there's no concurrency? The latter seems like a trade-off rather than a necessity.
&gt; it's not actually for memory safety, or is it? The one-mut-borrow-at-a-time rule is actually one of the most important parts of ensuring memory safety, e.g. it stops you from reallocating a vector and leaving other borrows dangling: let mut v: Vec&lt;int&gt; = vec!(1, 2, 3); // borrow the first element let borrow: &amp;mut int = v.get_mut(0); // push a whole pile of elements, surely causing v to reallocate for i in range(0, 100) { v.push(i + 100) } // whoops, `borrow` is dangling, we're writing to invalid memory. *borrow = -10; The compiler rightly complains about the `v.push`s. (`push` does a mutable borrow of `v` to be able to add elements at the end.)
I had one episode several years ago, where we needed to track down a memory corruption, with the customer bombarding the technical support every day, during a whole week. Not fun.
Stupid people will write stupid code no matter the language. We're protecting against common human failings, not making up for incompetence. :)
I have an answer that might seem unintuitive, but I think it's true. The restrictions imposed by a very strong static type system will *set you free*. If I use a language like C++ or Javascript, it is so easy to write a terrible, buggy program that I have to constantly worry and plan ahead. In a language like Rust, so long as I follow a few simple principles I can basically go nuts. This is true in other languages too, but in Rust I can even go nuts with threads! I can still create awful code with Rust, but it's so much harder that I don't have to spend anywhere near as much time worrying about it.
&gt; I can get things working in C++ significantly faster (2-4x). You can also shoot yourself in the foot 2-4x faster. It's your choice at the end of the day.
Well, we can see where the other direction leads you when looking at a language that was designed to chug on despite of whatever errors happen: PHP. But that's not entirely fair: other languages also have less guarantees than rust, and scripting languages only even have syntax errors at compile time!
I believe you're looking for Rust the game. This would be Rust the programming language, as you can see by "THE RUST PROGRAMMING LANGUAGE" along the top.
purity vs pragmatism, the age old debate.
You're probably right, but simultaneously, i'd wonder if you're underestimating the size of the Games industry, it's the sort of place where Rusts memory safety would be most useful, but not at the cost of arbitrary verbosity. But then again, thats why you can mix and match rust and c (libs) where you need to.
It is *possible* to do, and it's not that hard, except for a little bit of verbosity (and an `unsafe` annotation, since it's wildly `unsafe`: any corruption leaves you jumping around arbitrarily in memory). I don't think we should go back on the Rust's "make safe code easier to write" principles by relaxing rules that help people do the right thing. (And I especially don't think that `unsafe { ... }` is the right way to enable any of these relaxations; there's already a lot of invariants that one needs to think about when writing an `unsafe` block.)
&gt; underestimating the size of the Games industry I possibly am, but am I underestimating how often one needs to read a blob with internal pointers superfast (i.e. so fast that even a safe serialisation library like capn proto (which rarely needs to copy) isn't good enough)? &gt; cost of arbitrary verbosity I don't think this example is "arbitrary"; it's a single extra cast to move from the space of "Rust pointers" (`&amp;`) into the space of "machine pointers" (`*`), which seems quite reasonable.
Rust acknowledges that for practical purposes its beneficial to have an escape hatch. The term is *by definition* meant to be a tool of last resort, just like people are not wearing life jackets or oxygen masks in airplanes just for kicks. just as you were scared of C in the olden days of ASM, now you are scared of safety in Rust. Again, this in not a bug, this is an intended feature. It says so on the home page! No one's forcing everybody to use Rust and ditch their decades of experience and habits. The point is that for the long tern it is highly beneficial to improve upon language design and provide more safety guaranties. Rust *is* a practical language - it does provide the escape hatch when you really need it. It does try to provide reasonably familiar syntax. It is not as purist as most functional languages such as Haskel even though it draws on many of the theoretical benefits of such functional languages. "getting things working" is vague and clearly my definition differs a great deal from yours: Writing write-only code full of unsafe holes, no documentation and safety issues is NOT "working". it is the very definition of failure. I do programming for a living so I'm not talking about some academic purist values. I'm saying that it is important for a business to write sane code. Most successful companies do enforce this because it really is a matter of their bottom line. e.g. say I write feature A and than the customer request to add to it sub-feature B. If I now need to burn two weeks to understand what the fuck is going on in the code than I just cost my company a lot of money that it could have saved had I spent half a day to document better, add logging, use significant names, etc, etc. Rust is being developed by Mozilla for practical purposes - they are writing servo - a new browser engine in it. Rust is driven by that very practical purpose of "getting things working". It's just that their definition of "working" *rightly* includes a guaranty of memory safety. 
It prevents iterator invalidation. That's the easiest way to explain it.
Why not just reuse maven or something similar than, instead of reinventing the wheel in the first place?
Why is byte_ofs_ref not implemented with byte_ofs_ptr? If you `use std::libc::c_void` you save yourself a lot of typing. If you're going to write to those void pointers perhaps you want `*mut`?
&gt; but there are many classes of bug and you need to write tests for other reasons.. visualizing whats going on with intermediate structures.. Simple memory errors usually show up quite quickly. If you get as far as creating correct inputs &amp; outputs to your code, memory probably wasn't an issue. so basically i'm not really finding rust reduces the amount of tests/debug code I need to write. That's not our experience with Firefox, unfortunately. Memory errors have a remarkable tendency to show up in competitions like pwn2own, not in unit testing. :(
Right. Rust is really interested in the two classes of bugs that (IMHO) time has shown that dynamic instrumentation (Valgrind or race detectors) has been ineffective at preventing on real world codebases: (1) memory safety and (2) data races. Of those, (1) is the most important, because not having it often leads to exploitable security vulnerabilities, and Valgrind, as incredibly valuable of a tool as it is, has not kept them from showing up again and again (for example, in Pwn2Own).
Would you be able to say why Valgrind and other dynamic tools do not prove as effective on large code bases? Is it because they do not detect all that you need them to, or that they do not scale in terms of performance (or something else) ?
they'll need 'mut' versions aswell, my use case was immutable. yes byte_ofs_ref can be implemented using byte_ofs_ptr, i'm sure many improvements are possible. I can see safe version of these could be written that take an enclosing 'blob' , you could verify that the locations are within bounds.. fn unsafe byte_ofs_ref(..) `fn byte_ofs_ref_within&lt;..&gt;(blob:&amp;'a[u8], current_object:&amp;'a X, offset:int)-&gt;&amp;'aY /*this can assert that both 'current object' and the resulting offset location are within the given 'blob' */`` 