[@PLT's latest tweet](http://i.imgur.com/fuEogip.jpg) [@PLT on Twitter](https://twitter.com/PLT) - ^i ^am ^a ^bot ^| ^[feedback](https://www.reddit.com/message/compose/?to=twinkiac)
I'd use Rust in production in a heartbeat if I could. It sure beats spending weeks debugging segfaults in C++. It's funny how people ask whether Rust is ready for production, but nobody ever asks whether C/C++ are ready for production.
Just to give you an update, GITHUB_API_TOKEN is now supported
&gt; I think I might be THE most boring user of SDL2 though, because my entire use right now is keyboard / mouse / joystick input and.. an OpenGL context. That's exactly how I use it, too ;)
&gt; For Rust the story is more predicatable. You allocate an object, which is not necessarily cheap. And memory is immediately deallocated when released. This is immediately expensive, but the overhead scales near linearly with the workload. FWIW Rust's default allocator (jemalloc) should have buckets and freelists of small allocations. It's probably still more expensive than a good GC (I remember reading the JVM needing as little as 6 instructions for an allocation) it should not be as expensive as getting memory from the system and releasing to it immediately.
&gt; 0.x crates are not expected to handle security issues in a timely manner. No range of version numbers represents a guarantee of security or a timely response to security issues. That's just not what that number means. People use version numbers differently - usually to represent API stability or generally whether something is "good enough" to be useful. But I've never heard of it being used as an indicator of security or tied to a specific security policy. How do other communities work in this regard, in your experience? Rust is the same as any other - Java, Javascript, Perl, Python, Ruby, etc. They all have libraries contributed by the community - how do you go about determining when a library is "production-ready" in those ecosystems? The best you can hope for if you want to avoid doing the hard work of scrutinizing each library you use is to look for a collection of libraries that have been vetted by some reliable group. That's what we'll have at the end of the Libz Blitz. Yes it's limited in scope, but it's a good start. But even then - mistakes happen. You pull something in maintained by others, there's a chance it could malfunction, and there's a further chance that the maintainer(s) might drop the ball in dealing with it. As always. If you're using a third-party library without a support contract (or something of that nature), be prepared to fork it to fix bugs if necessary. That's an inescapable possibility for any open source library.
And is telling people to "grow up" the kind, empathetic tone we're looking for?
Nothing! It's pretty nice. I haven't programmed in my free time in at least a week, and it'll be the same this week. And if I don't program in my free time, I don't program in rust, basically. That said, while I'm not interested in writing code for the next week or two (nor do I have the time), I would love to get contributors for my rust actor library, derive_aktor. https://github.com/insanitybit/derive_aktor There are many open issues and I would be totally psyched to guide anyone through them.
&gt; Which, of course, is why you emphasized No, I emphasized it because if I *hadn't*, people would have said "they're just trolls, ignore them".
This is my third comment in this thread already, but I think this is my most important point of all. It's about these points: &gt; I've heard some alarming attitudes from prominent members of the Rust community: &gt; &gt; * 0.x crates are not expected to handle security issues in a timely manner. &gt; * You have no right to criticize the handling of a bug unless your own crates are bug-free. &gt; &gt; (from [here](https://www.reddit.com/r/rust/comments/6bqkmd/my_first_pr_for_a_rust_project/dhp7u4a/):) Don't the maintainers and regular contributors have some responsibility? Maintainers and contributors of open source projects have no responsibilities whatsoever to any of their users. Maybe legally that might not be true depending on the license (most open source licenses probably say that there is no implied warranty in all-caps for a reason). But I don't mean it in legal terms, I just mean that if someone creates something in their free time and then gives it away for free, they don't owe anybody anything. If they don't want to fix a certain bug, they deserve exactly 0 blame. So if the developers of a crate don't have the responsibility to fix security issues, then how can you rely on them in production code? The answer to this isn't Rust-specific: if you use any open-source project, you have to accept the inherent risk that there are possible bugs and security issues, and that those issues might take a long time to be fixed after being reported. As a project gets more and more popular, that risk gets smaller and smaller, but it's always there, because there's no boss to say "John go fix that bug now". If you want actual guarantees, with somebody to sue if it goes wrong, you'll have to pay money for that. I've avoided the term "production-ready" so far because it's such a vague term and I wish we would stop using it. A library that parses json might be production-ready for a Flappy Bird clone parsing highscore tables but not production ready for PayPal parsing API requests. So do you have the right to criticize the handling of a bug? Criticizing the developer(s) of a crate: no, most definitely not. They owe you nothing. Criticizing the crate itself ("crate X takes a long time to fix its security issues"): yes, of course, but even then you should be polite and constructive. This is more important than you might think, it's not just "superficial politeness stifling the discussion of real issues" like you called it in the other thread. Constant angry rants about their project makes people [simply quit open-source development](https://www.reddit.com/r/javascript/comments/40pag0/the_sad_state_of_entitled_web_developers/cyw11mk/). Especially in the JavaScript world, ["entitled" users are a particularly bad problem](https://medium.com/swlh/the-sad-state-of-entitled-web-developers-e4f314764dd). We should try very hard to avoid the same thing happening to Rust. 
You are misunderstanding kmc's point in that thread, like everyone else! His expectation was that known serious security bug be advertised in the readme or something. "do not use for auth requests". That is a reasonable expectation. Nobody else interpreted that comment that way. (It ultimately turned out to not be a serious bug, but that doesn't change the expectation)
Hmm. Perhaps. I think there were two expectations, with the second following from the first. Firstly, they expected reqwest to be "production-ready". This seems to have been based solely upon it's reputation, and that there was nothing in the Readme to indicate that it *wasn't* production ready. I don't think that's a fair assumption to make. Second, they expected reqwest to deal with security issues in a timely manner (assuming that it was "production-ready", that is). This could mean something as simple as putting a warning in the Readme, as you described - not necessarily "fixing" the issue. Which is... pretty reasonable I think. So I suppose I agree with you about that second one. What we consider "dealing with security issues in a timely manner" can surely differ from person to person - I had gotten the impression that kmc meant a lot more than a simple notice in the Readme. But perhaps that impression was wrong.
This isn't true everywhere. Where I work, my team is often defending R against people pushing us to use SAS. We have to take those people seriously, because they outrank us. A security vulnerability in R would be a black mark on open source at my company. So the original comment is appropriately dramatic :)
Ah, good to know. Thanks!
Request has been touted as _the_ library for easy http, like Python requests. That's where the expectations come from, AFAICT. The expectation is that it is a usable if incomplete lib, which is very fair. The "production ready" bit wasn't part of the expectations AFAICT it was a product of that discussion. Lack of cookie support is not the security issue at hand; lack of cookie support makes it more secure. The security issue was incomplete cross domain redirect handling. Which can be a major security issue if reqwest supported auth, but I don't think it does. Yes, more than a simple notice, also kmc meant that we should not advertise it as the go to library for easy http.
That's not "open source in general", that's a bake-off between 2 directly competing products. I'm not saying 'in every niche open source is better than a closed alternative', I'm saying that the concept of open source is not so fragile that a bug in one project has any bearing on the reputation of the concept of open source as a whole.
Not so. When Twitter was well-known both for using Ruby and for crashing all the time, the general reaction wasn't "Oh, Twitter are such nincompoops for complaining about Ruby." People questioned what was wrong with Ruby that made it cause Twitter such trouble.
What I meant was that there's no reason to do re-borrowing explicitly in cases where the compiler already does it automatically because it already does it automatically, because the question was about explicitly re-borrowing in cases where the compiler otherwise was automatically re-borrowing and now the phrase "re-borrowing" has lost all meaning to me. \**head begins smoking*\*
Wow, that looks great! Can I ask what libraries/tools you're using to build it?
I'm impressed, every commit message is a meme.
Ah yes, the Proof of Work idiom. That's indeed much better than my awkward work-around.
I started using Rust with Spacemacs a couple of months and it works pretty decent. I just wish I could use rls instead of racer with company. Edit: [This is how it looks](https://i.imgur.com/jlYqyUX.png)
Right, sorry about the part about cookie support - I had removed that from my comment in an edit. I had realized it was beside the point after considering it for a moment. But to your main point - sure, I can see that. I definitely missed that part- I have never seen it advertised as "the go to library for easy HTTP". I see the comparison with the Python "requests" library, but I had always taken it to be aspirational - it's much younger than Python's "requests" of course.
I expect to land [multiple cursors](https://github.com/google/xi-editor/issues/188) in xi, and hopefully start font metrics aware line breaking. Lots of other stuff happening in xi, momentum is continuing.
That's fair.
Some people did say that sort of thing, but a lot of those comments also generally had a undertone of "… where 'the job' in question is Ruby's primary use case." The largest groups seemed to be a) people who were disappointed in Ruby, and b) people who already disliked Ruby and considered Twitter's problems proof that they were right to dislike it. I guess what I'm saying is, I think it had more of a negative effect on people's perceptions of Ruby than it did on people's perception of Twitter. Most Rubyists were using Ruby for web app development at that time, and most non-Rubyists who were interested in the language also mainly considered it for that purpose, so they couldn't very well turn around and say, "Tut tut, what are you thinking making web apps with Ruby?"
The point is, "never" becomes "relatively quickly" when you're paying them.
Working on first Rust tiny project and I've recently added the `error-chain` crate. However, now every time I run `cargo run`, it has to compile, even if nothing had changed. &gt; cargo run Compiling cpp_demangle v0.2.1 Compiling backtrace v0.3.1 Compiling error-chain v0.10.0 None of my other dependencies are re-compiled. Is there an explanation?
[No, you don't have to implement it manually](http://play.integer32.com/?gist=9fb90fa79d8934398f6dc4f1372d8004&amp;version=undefined). Essentially, the default implementation for Drop is to recursively drop all of a struct's fields, and if that's all you need to do, you don't have to explicitly write anything. 
That's also true of open source developers. If you find a bug that you need fixed, and email the developer saying, "I'll give you $1000 to fix this," it'll probably happen quickly. 
I just flat-out don't recall anyone who I'd take even kind of seriously pointing at twitter and saying 'Ruby sucks'. I heard plenty of things like 'Rails isn't the right tool for sites at twitter's scale', which, frankly, it wasn't. I don't want to give the impression that I think open source is somehow exempt from valid technical criticism. My point is simply that people today understand how the process works, and a bug in a library isn't going to bring an ecosystem down.
Thanks! [Relevant part of the Cargo.toml](https://gist.github.com/Thinkofname/31ff8808c8d54165a227e2ba40e31db8) + luajit. I've kept the number of libraries pretty low to keep things easier whilst I was waiting for things to settle down and ended up writing my own wrappers/libs for things. At some point i'll go through and see what can be replaced with an existing library (e.g. my ecs or the audio mixer) and maybe open source anything that could be useful (although looking now I don't see anything). Tools wise its just rust, visual studio code, gimp and blender.
[rende](https://github.com/bbatha/rende) a virtual dom library targeting wasm/asm.js.
So last week I continued working on [tarpaulin](https://github.com/xd009642/tarpaulin) a cargo based code coverage tool. I've been trying to track down a bug which kills coverage collection part way through. Managed to track down what I believe the cause is but no idea how to fix it yet. I've also forked [rust-nix](https://github.com/nix-rust/nix) as it's missing somethings I need. Hoping to get my changes tied down and submitting my PR before the end of the week.
A bit of now near-ancient history, the standard library shipped with Visual Studio 6 had a number of major bugs. While I can't find the it now, there was a site from the vendor about how to manually apply the necessary fixes.
Hi! This in the community for the Rust programming language. I suspect you're looking for /r/playrust, the community for the Rest video game?
I seem to remember jemalloc being in the ~20 instructions ballpark for best-case allocation.
OK, thanks to this article (http://arthurtw.github.io/2014/11/30/rust-borrow-lifetimes.html) I finally understand how it all works. The borrow gets extended (as you said) not because of lifetime parameter `'a` in the first argument's type (`&amp;'a mut IntCapsule`), but because of the second argument (`&amp;'a mut RefHolder&lt;'a&gt;`), which allows mutable access to *something* that lives at least for `'a`. Since the compiler has to be conservative, it must assume the worst, which is that our re-borrowed reference extends its life by moving into the second argument. By the way, I made two more observations: * If we change the type of the second argument to a read-only reference (`&amp;'a RefHolder&lt;'a&gt;`), the code compiles again, because the compiler can be sure that the first argument cannot move into the second. * The compiler will assume a potential move even if the second argument's type would not even allow to contain a type of the first argument. So technically, the compiler assumes something that cannot happen, and it *should know* that (potential improvement?).
C and C++ compilers [are not bug free](https://www.reddit.com/r/cpp/comments/69eq8i/the_gcc_bug_affects_you/) though. Old, yes. Riddled with technical debt, yes. Widely used, yes. But they unfortunately still have bugs; on top of the surprising code transformations due to undefined behavior in the user's code.
Using OpenGL just loaded via sdl2. GUI is custom but I'm not a fan of it, [JSON doesn't make a good format for it](https://gist.github.com/Thinkofname/6898e297f97704073ad1d06eb8acbf96).
Awesome! I was looking for a good source/explanation for the references, I'm glad you found one. With the second thing, the reference being a different type, I think this not being inferred is a guard against breaking API chances. Similarly to how nothing is inferred past function boundaries, nothing _should_ be inferred from the private fields of a struct. By this I mean it should be a non-API-breaking change to add an additional private field to a struct, and if the compiler inferred all what types of references the struct could contain, adding this field could be an API breaking change to a completely unrelated method. The other case is that the comprise shouldn't infer stuff from fields of IntCapsule either - I mean the function could take a sub mutable borrow to a field of the IntCapsule and store it within a field of RefHolder - and thus adding a new private field to either struct could be an API breaking change. Glad that article explained it well though, think I'll have to bookmark it for future reference!
It's all good, it's a pretty common mistake :)
Ok, I ran the C# benchmark again for the Push - List only with the memory benchmarking, and got these results: Method | Mean | Error | StdDev | Gen 0 | Gen 1 | Gen 2 | Allocated | -------------------- |---------------:|--------------:|--------------:|-----------:|----------:|----------:|------------:| List100Int | 1.437 us | 0.0056 us | 0.0052 us | 1.1539 | - | - | 4.73 KB | List100_000Int | 3,273.670 us | 23.3333 us | 20.6843 us | 769.5313 | 382.8125 | - | 4687.59 KB | List1_000_000Int | 100,744.571 us | 1,991.6453 us | 1,862.9861 us | 9220.5882 | 5224.2647 | 1713.2353 | 46875.6 KB | List100Struct | 2.252 us | 0.0079 us | 0.0070 us | 1.5335 | - | - | 6.3 KB | List100_000Struct | 5,091.972 us | 18.5878 us | 16.4776 us | 1023.4375 | 507.8125 | - | 6250.15 KB | List1_000_000Struct | 132,603.393 us | 2,382.6986 us | 2,228.7775 us | 12133.3333 | 6908.3333 | 2141.6667 | 62500.87 KB | The units in the "Gen X" columns are in collections per 1000 operations according to the [blog](http://adamsitnik.com/the-new-Memory-Diagnoser/). Note that the units for time are now in microseconds for some reason, not nanoseconds like they were earlier (1 us = 1000ns). I'm not entirely certain how to get allocation reports for Rust, though.
mmap could be faster for this (as you would end up not reading the whole file) but it's yet another weird API that only works for a file (and not for something your read from the network) and adds a component of unsafety and lack of portability. Seems way too quirky API to optimize away an implementation detail that I may very well find a better way around in the future.
Wow that looks awesome. I'm envious of your small dependency list, and how well it looks considering the immaturity of Rust's gaming ecosystem. Nice Job!
Tabbed / Stacked layouts for Way Cooler is nearly done, last main bug I need to fix is where tabbed/stacked containers keep getting smaller when ever you do something to do them, which makes them basically unusable. After that I'll run the latest master on my system for a few days, fix any bugs I find, then release 0.6. For 0.7 I'll probably focus on improving the Lua / D-Bus side enough so that I can flesh out the documentation and then feel more confident about linking the fancy website I have set up for it.
What's that? 
Not quite as concise, but you can use a separate `fn`/lambda as a pattern guard. let p = |b| { b == b'1' || b == b'3' || b == b'5' }; let x = b'1'; match x { m if p(m) =&gt; println!("match!"), _ =&gt; println!("no match!"), }
I continue to port my web browser [titanium](https://github.com/antoyo/titanium) to [relm](https://github.com/antoyo/relm), which is an asynchronous GUI library based on GTK+ and futures. Most of this work is now done and the browser is somewhat usable: look at how [this new webkit web extension](https://github.com/antoyo/titanium/tree/master/titanium-web-extension) is now clean, safe and efficient thanks to using relm (actually `relm_state` which is a new crate containing the non-UI part of relm)! I'm very happy with this result: it proves that relm actually achieved the goal is was initially written for. I also worked on `relm`, last week, I: * added support for construct properties. * added the ability to connect to GTK+ events in relm widgets. * added IntoOption and IntoPair traits to simplify handling of GTK+ event return value. * added support for child events in relm widgets. * fixed to allow multiple values in message variants. * format the logged code (the code generated by the `#[widget]` attribute is logged (as a warning) to help debugging). * added a way to manage synchronous event in an asynchronous way. * splitted the `Widget` trait to make a new `Update` trait for use when no UI is needed (**breaking change**, sorry). * removed the `Clone` requirement for `Msg` (another **breaking change**, sorry again, remember that the API is still unstable). * created a new `relm-state` crate to be able to use `relm` features without a UI (remember the web extension in titanium?). * made various bug fixes. I'll have less time to work on that in the next weeks, but hopefully I'll finish to port `titanium` to `relm` soon.
/u/Manishearth pinged me (I think because I work on a Github API library) so here's what I think: - It's short simple and sweet. It does one thing well which is great. - You have a ton of nested match statements which aren't wrong but Consider using something like [and_then](https://doc.rust-lang.org/std/result/enum.Result.html#method.and_then) to cut the need for using nested matches for things like `Result` or `Option`. For instance [here](https://github.com/benwilber/gist/blob/master/src/main.rs#L72-L85) you could chain the `and_then`s twice and only need to deal with one error using something like [map_err](https://doc.rust-lang.org/std/result/enum.Result.html#method.map_err) and then panicking or however you want to handle it. - Consider using [GitHub Personal Access Tokens](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/) for authentication rather than basic auth. It limits the scope of what your application can do by giving that control to the user, if you ever decide to expand upon this. - Instead of panicking you could write to [stderr](https://doc.rust-lang.org/std/io/fn.stderr.html) and then [exit](https://doc.rust-lang.org/std/process/fn.exit.html) the process (just be wary of the possible different exit codes on windows and linux that the docs mention) Overall I really like it and it's a simple application. Welcome to the community! Consider also using [Clippy](https://github.com/Manishearth/rust-clippy) so you can get some extra hints from the compiler while you're learning!
&gt; I thought calling anyone out by name would distract from the point and get me in hot water, maybe even banned. I personally would not ban you, as long as you: - attempted to frame the discussion positively, - be sure not to misrepresent other people's opinions (exact quotes, *in context*). Communication over a forum, or a medium such as reddit, is *really* difficult: - the lack of interactivity means that people tend toward terseness, skipping context that seems obvious to them but may not be to anyone else, - the lack of eye contact means that a lot of "non-verbal" communication is missed, no catching the smirk or cheery tone which underlies a joke, or an attempt at levity, for example. As a result, *you*, the reader, have to assume the *best* of them. If you get riled up by a comment, *before* answering or crying wolf, you should doubt your own judgement. Re-read the comment, attempt to see other interpretations, attempt to see whether they may have mis-interpreted what they are replying to, ... You are encouraged to seek clarification if there is any doubt. Because, let's face it, it's really wasteful of everyone's time to get heated up about a misunderstanding. --- *Note: and personally, in this particular case, I'm annoyed that multiple discussions (production readiness, handling of security issues, people's attitude, ...) are mixed up in a single bucket where it's difficult to make head or tail of what's going on. Focused discussion is so much more productive!*
[There is](https://www.rust-lang.org/).
I think that the very issue the OP had is one of misunderstanding. I do not see the two following statements about reqwest as incompatible for example: - Reqwest is great! - Reqwest is riddled with security issues! In such a sparse ecosystem, the bar to be "great" is not high, it suffices to be better, or easier to use, than the few (if any) competitors. And I think things get even murkier as one introduces the term *production-ready*. Imagine I use reqwest in an ad-hoc script querying some internal service: it's in production! And it works great! In this scenario, I am not worried about security issues: all content consumed is strictly under control. And therefore, as far as I am concerned, it's production-ready. For *my* production case. Others may find that (1) it lacks a good number of features, (2) it doesn't handle a good number of edge cases and (3) there are serious compatibility/security issues. ¯\\_(ツ)_/¯
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/rustjerk] [Don't be a baffoon.](https://np.reddit.com/r/rustjerk/comments/6cphef/dont_be_a_baffoon/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
&gt; Instead of saying "the Rust community is friendly and welcoming" (danger of ossifying into groupthink), say "the Rust community has been friendly and welcoming towards me" or "in my experience the Rust community has shown itself to be friendly and welcoming" etc. QFE. And it's worth saying that, *in my experience*, this advice scales really well to all sorts of topics. The difference between declaring something as a *fact* and as something that is an *opinion that may not generalize* has been tremendous for me.
I would note that the OP unilaterally decided that the issue was a large security hole. /u/notriddle [explained it quite differently](https://www.reddit.com/r/rust/comments/6bqkmd/my_first_pr_for_a_rust_project/dhp1j6v/): &gt; Interestingly enough, there is a feature description at the top of the [reqwest docs](https://docs.rs/reqwest/0.6.1/reqwest/) that says that cookies don't work. And they don't; unless you write a cookie jar implementation yourself, reqwest will completely ignore cookies. No security hole in the default distribution. &gt; And this "fix" isn't really even the beginning of an actual fix, because it doesn't (and can't) handle the secure bit or the domain attribute that cookies carry (which allows mail.google.com to read cookies for *.google.com). Correctly implementing cookies on top of reqwest requires you to turn off redirect handling, so that you can use the cookie's settings to decide whether it will be sent or not. &gt; Reqwest is going to need to write some actual API surface to make this work. It's not a quick fix. So we have a "large security hole" in a feature that doesn't exist. It seems there was some misunderstanding on the OP part regarding the magnitude of the issue.
It should work with the company-capf backend, since it works with emacs' completion-at-point function. But you're right that it seems to have no direct company integration.
In this case it's only OAuth tokens which you can generate yourself and give it the gist scope and that's it. /u/benwilber can then read it from a file and add it as part of the [Authorization](https://elastic-rs.github.io/elastic/reqwest/header/struct.Authorization.html) header. Just an `Authorization("Token String".to_owned())` is fine. The bearer and basic types won't be the version needed.
so just fill a feature request somewhere ! :)
THe answers here are pretty good, I'm just bringing things together and adding a bit more to it. Rust always frees up memory of things when their lifetime ends. If an object doesn't implement `Drop` and none of its fields implements `Drop` then you can just free memory. Now the thing comes interesting when we have the `Drop` trait. Before we free memory we can do more elaborate drop of the object, the algorithm goes as follows: 1. Call `drop` method if the object`: Drop`. 2. For each field run the steps (even if the holding object doesn't implement `Drop`. Once the above has been done we can free up the memory. There's actually a bit more logic that happens here. Since you could have moved the object away you need to check if that hasn't happened, but I assume that it's not the case. As a last thing. When you declare multiple variables they are deleted in inverse order than they were declared (the last variable declared is the first drop/deleted and the first variable is the last one to go). Again this happens to be the "right thing" almost all the time, and the only cases were I've seen it work weirdly people have jumped through hoops to cause this.
This is really exciting! I'm going to try it out for one of my projects. Did you figure out a way to handle the use of [`setjmp`/`longjump` in lua's error handling](https://github.com/kballard/rust-lua/issues/1)?
To help with the Github access tokens, [release-party](https://github.com/matthewkmayer/release-party-BR) can be used as an example. The program takes in the github access token as an environment variable and uses that to talk to the Github API. [Here's an example of a POST with reqwest and the access token.](https://github.com/matthewkmayer/release-party-BR/blob/master/src/github/mod.rs#L181) release-party is a tool I made for work, where we make a PR from `master` to `release` then merge that for production deploys. When there's a couple dozen projects to release, it made sense to automate it. :)
you should also look at P#
I asked this in the last thread at the tail end, so reposting here for a bit more visibility: Is there a BTreeMap/Set equivalent of ArrayVec/SmallVec? 
Chucklefish Games?!
Hahahaha, I'm glad that you're excited!
Currently working on getting my [atom-rust](https://atom.io/packages/atom-rust) package, a package to support Rust development in Atom, beyond the barebones functionality it has right now. It's super early, but it currently works with RLS and it's installable through atom and apm.
Oh, very interesting! I'll find out if this is the case. I'm not sure though, why the reds would display correctly on native though because it uses SDL on native as well. 
To make this more findable in the future, I've sent in a [PR](https://github.com/rust-lang/rust/pull/42159) to get this information added to the Drop trait documentation. Other issues have been filed for the [new Rust book](https://github.com/rust-lang/book/issues/717) and the [reference](https://github.com/rust-lang-nursery/reference/issues/59).
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/lua] [New High-Level Lua Bindings for Rust \[x-post \/r\/rust\]](https://np.reddit.com/r/lua/comments/6cr5uh/new_highlevel_lua_bindings_for_rust_xpost_rrust/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
While poking around, I managed to find [this StackOverflow answer](https://stackoverflow.com/a/33787594) (apparently from [here](http://www.learnopencv.com/why-does-opencv-use-bgr-color-format/)) which claims that BGR is a historical curiosity which used to be popular with things like camera manufacturers, that got baked into things like Windows's `COLORREF` and OpenCV uses it for legacy reasons, while the higher-level APIs moved on. According to [this answer](https://stackoverflow.com/a/369955) and [this Skia design doc](https://dev.chromium.org/developers/design-documents/graphics-and-skia), the RGB vs. BGR thing is a leaky abstraction that traces its roots back to little-endian vs. big-endian handling of 32-bit unsigned integers. (ie. "xxRRGGBB" as a 32-bit integer is "BBGGRRxx" as a sequence of byte-wise reads on a little-endian system and that's apparently what Windows DIB format uses.) That'd make perfect sense, given that the `RGB`/`RGBA` approach comes from the libraries with the right combination of high-enough level and modern enough origins to justify properly abstracting away the machine's native endianness rather than needing to save every cycle possible, the way old graphics APIs did. (Hence, the hacks involving manipulating 32-bit ints rather than triples/4-tuples of bytes.) According to [this OpenGL forums thread from 2004](https://www.opengl.org/discussion_boards/showthread.php/144853-RGB-or-BGR-for-textures/page2?s=e0981db7df7f96bfe70b6363633a7bea), tests showed that nVidia cards accepted texture uploads in 8-bit-per-channel BGR much more quickly than 8-bit RGB at that time, because of hardware performance optimizations for translating BGR data supplied by little-endian (ie. Windows on x86) host machines. (With the only faster option being 5-bit-per-channel RGB, which the card could easily promote to its RGB565 internal format.)
Oooo, interesting. I'll have to evaluate this vs hlua for use in [Way Cooler](https://github.com/way-cooler/way-cooler). I've ran into limitations with hlua fairly often enough to want to give alternatives a try.
I also work on a large, fairly modern C++11 codebase, and I've spent the entire last week at work unsuccessfully trying to debug a crash. And I don't know what the cause of the bug is yet, but I'd give you 10:1 odds that it could have been prevented in Rust. Obviously, rewriting a giant codebase in Rust simply isn't going to happen, but that doesn't mean that it wouldn't be advantageous if it did magically happen somehow. 
Try using [SDL_ConvertSurface](https://wiki.libsdl.org/SDL_ConvertSurface) instead of "swapping some color values around". (Each `SDL_Surface` knows its own pixel format, so `SDL_ConvertSurface` takes a surface and an `SDL_PixelFormat` describing what you want to get out of it. It'll handle deciding whether to convert or just copy.) For best performance, you want to convert your inputs as early as possible, and then operate in the native output pixel format. If that doesn't meet your needs for some reason (eg. you're creating a surface with the right pixel format, then writing raw bytes of the wrong format into it), the "Related Functions" section of that page might have what you need. (or [SDL_CreateRGBSurfaceWithFormat](https://wiki.libsdl.org/SDL_CreateRGBSurfaceWithFormat) which is linked from one of those entries.)
As a developer accustomed to C/C++, traits and generics are two paradigms of the Rust language that I don't really understand. Usually, when I write code in Rust, I try to make associations to previous languages I learn. Can someone explain traits and generics in terms of someone with knowledge in C/C++? Are generics like a much more effective and explicit method of function overloading?
The primary reason why I wrote this is because I perceived that CSV parsing is something that beginning Rust programmers might want to try their hand at. Relatively speaking, it's a pretty low-risk kinda-nice-reward scenario where you might replace some (slow) Python data munging scripts with Rust. Certainly, I've done that quite a bit, and I've found that *lots and lots* of people use CSV data, *especially* folks in the data science crowd that might have less experience with programming and therefore might be turned off from a language like Rust. I hope more writing like this can help bring those sorts of folks into the fold, but I would also very much appreciate feedback on how this could be improved for that target demographic!
Generics are a much more effective templates. The key differences are that you can easily restrict what the template parameter is, and how the compiler generates generic code (we'll need someone more knowledgeable to comment on that). Currently there is a feature mis-parity, as Rust does not yet support integer template argument, but this is worked on by the compiler team. Traits are like an interface, or an abstract class. You use them to define a contract between an API author and an API user. In C++ you simply would have used a class with pure-virtual methods, but in Rust you can use a combination of Traits&amp;Generics to provide a better experience and avoid the virtual table.
I tried for longer than I care to admit to think of a clever name, and I completely didn't think of that :(
Reviving [genmesh](https://github.com/gfx-rs/genmesh)
I love you.
Hats off to writing an intro on CSV parsing for Rust. I'd just like to add that if someone is writing their CSV parsing using Python's `csv` module, which usually uses the C extension, the performance should be comparable to anything you can achieve in Rust. Furthermore, Python's `csv` implementation is considered the de facto third-party implementation of Excel-compatible CSV. Not to know you or anything, I'm thrilled you've done this (CSV parsing is one of my litmus tests for language maturity), but please do be careful not to give credence to the "python is slow" meme.
my previous reply didn't even mention this: http://blog.faraday.io/how-we-made-our-csv-processing-142x-faster/ it's a case in point of how real world Python CSV code was far from competing with Rust in terms of performance.
integer template parameters are restricted to constexprs, so isn't the problem better suited for macros? 
Sorry, I don't follow
&gt; In C++ you simply would have used a class with pure-virtual methods That would be an option, certainly, but not the only option; just as you can use traits with generics rather than trait objects in Rust, you can use templates and static-interface types rather than abstract classes in C++.
I think the main problem I have with people saying "python is slow" is that I hear it used in so many ways where it doesn't matter, or I just hear "don't use python, it's slow". People like to act like programming language performance is always a major thing to worry about. There's no point in worrying about some operation taking 10ms versus 100 μs if you're going to wrap it all in a 200ms HTTP request anyway. Most of the bottlenecks I hit are networking + database + disk, and I don't think I'm alone there. There's obvious areas where pure python ran with CPython is just too damn slow. I tried using it for genetic algorithm training of a neural net, and yeah, it's going to be taking minutes where C++ takes seconds. But rarely do I hear people say "Python is slow" in context like that. And a lot of the time the answer might be to write it in Rust/C/C++ and use the CPython API. Python is still sometimes a very convenient wrapper around code where performance matters.
This is definitely true, but I'm not sure it's a consideration for binary search. In particular, it seems rather hard to achieve a large speed-up by using SIMD for individual searches[1], and the effort might be better spent adapting a branchless form to run multiple searches at the same time (i.e. essentially have `target: [u32; N]` and return value `[usize; N]`) using scatter/gather instructions, although maybe the cost of those would be too much to be worth it. [1]: I did some searching, but didn't find much; my thoughts would be basically a B-tree with B=4 (or maybe 3, with some memory wastage) and some fancy (&amp; relatively expensive) horizontal reductions to jump to the right places
Well, I didn't argue with the "provide a better experience" part. ;-]
So, this is just a little crate I threw together. It isn't meant to be a "do every possible thing" kind of option parser. I just wanted something a little more convenient than `getopts`. There's an example of basic usage in [the docs](https://docs.rs/gumdrop/). Feedback is welcome.
You are mapping structured data from a file to structured data in the program, and back. A great way to introduce the power of parsing, the type system and also showing some useful language syntax. I have read similar tutorials in other languages before, e.g., parsing JSON in Haskell. A great opportunity for warming up in a new language, and you absolutely nailed it! Thank you!
A question about this example: //tutorial-read-serde-03.rs use std::collections::HashMap; // This introduces a type alias so that we can conveniently reference our // record type. type Record = HashMap&lt;String, String&gt;; fn run() -&gt; Result&lt;(), Box&lt;Error&gt;&gt; { let mut rdr = csv::Reader::from_reader(io::stdin()); for result in rdr.deserialize() { let record: Record = result?; println!("{:?}", record); } Ok(()) } Am I understanding this right, and for each row this copies each header field name anew into an owned string? If so that sounds like a significant downside, one that would be worth pointing out—it would likely bite people who naively stick a lot of those hashmaps into a big in-memory data structure.
How does paratext differ from this library? More aggressive caching and threading?
&gt; Python is objectively slow, it isn't a meme, at all It is a meme. Just because something is a meme or popular on Reddit doesn't make it wrong.
Woo! Thanks for the `Writer.into_inner` method. Now I can stop using a forked version of CSV :)
&gt; That's a ridiculous statement. C and C++ have rock-solid compilers and tooling, and there are battle-tested libraries for almost every possible thing you can think of. And many of those battles were lost. C/C++ has a lot of issues that make it not the best choice today, even not a good candidate in some cases. Being popular has value, but it's not a card that ultimately triumphs everything else. &gt; I'd pick Rust to write such a project from scratch now. And that's the conclusion we care about here. &gt; We all love Rust, but even comparing its ecosystem to the ecosystem of C and C++ is just wishful thinking. Few years ago, existence of Rust was wishfull thinking. I'd say we need more of such comparison and wishfull thinking, so that we can improve over what we have.
It's not a meme. I also have in the past migrated my Python (csv module,yep) code to in my case Closure because Python was not usable in our case. Thanks to this writeup I'm confident when the need of working with CSV arrives, I'll be trying Rust instead. 
That's an awesome writeup, congratulations. Not related to the CSV crate but the initial Error section is slightly intimidating for really newbies. Maybe an `error_chain` example? Even if as a different document linked from here. Also, what about Tokio for async or I'm going crazy here?
I don't know about this specific crate, but if some value is escaped/quoted, the return value can't be `&amp;str`, it may be `Cow` though?
Same. I love long posts like these 
Cool! Can it handle required arguments?
Does it support Excel's `sep=;` header? If you want to use CSV with Excel reliably this seems to be highly recommended.
&gt; Not related to the CSV crate but the initial Error section is slightly intimidating for really newbies. Ah this is exactly the kind of feedback I was hoping for! Thanks. Could you maybe say more about what put you off from it specifically? Basically, the reason why that section exists is because I didn't want to use `unwrap`/`expect` throughout all of my examples. So I wanted to give just enough of a taste of error handling to make the examples work reasonably well. I kind of feel like adding `error_chain` to the mix would significantly overcomplicate things with magics that are hard to explain. In the tutorial, we only ever use `Box&lt;Error&gt;`, so most of the "boiler plate" associated with error handling is skipped anyway. I don't feel particularly bad about that, given that ripgrep is actually still [using Box&lt;Error&gt;](https://github.com/BurntSushi/ripgrep/blob/master/src/main.rs#L55). :-) &gt; Also, what about Tokio for async or I'm going crazy here? I'm not sure I understand the use case? Could you explain more?
Yes, you're correct. I will try to think of a way to add a warning about that. But I think you kind of buried the lede there. The problem isn't really that we're duplicating keys for every record, but rather, that we're creating an entirely new hashmap for every record. The main point of using this technique is when you want to access fields by header name *and* you don't know the structure of your CSV data ahead of time. It's very very simple to do, and you never have to deal with lifetimes. And its performance is probably fairly reasonable, but will certainly be slower than all of the other examples. For example, here's a program that doesn't allocate a new `String` for every header value: extern crate csv; use std::collections::HashMap; use std::error::Error; use std::io; use std::process; type Record&lt;'a&gt; = HashMap&lt;&amp;'a str, &amp;'a str&gt;; fn run() -&gt; Result&lt;u64, Box&lt;Error&gt;&gt; { let mut rdr = csv::Reader::from_reader(io::stdin()); let headers = rdr.headers()?.clone(); let mut raw = csv::StringRecord::new(); let mut count = 0; while rdr.read_record(&amp;mut raw)? { let record: Record = raw.deserialize(Some(&amp;headers))?; if record["Country"] == "us" &amp;&amp; record["Region"] == "MA" { count += 1; } } Ok(count) } fn main() { match run() { Ok(count) =&gt; { println!("{}", count); } Err(err) =&gt; { println!("{}", err); process::exit(1); } } } This is somewhere around twice as fast as using a `HashMap&lt;String, String&gt;`, but it's still slower than everything else. And with this technique, your hash map can't live past the body of the loop. There is more design space here, for example, using a `HashMap&lt;String, String&gt;`, but reusing it from row to row. I'm not quite sure that's possible in Serde right now? See also: https://github.com/serde-rs/serde/issues/855 Anywho, yeah, I'll try and see if I can slip a short warning about this into the tutorial, but I didn't want overcomplicate it too much. (Especially since I kind of perceive using a `HashMap` for every record to be something you reach for when you specifically don't care too much about performance in the first place.)
I personally prefer your approach, leaving the it with the standard "native" errors, rather than something like `error_chain`. When I'm new to a language or library, the less magic the better, and the fewer intellectual dependencies I need to understand it the better. If I have think about something like errors, I'd rather it was language-native even if it makes the code a bit longer, because at least I don't need to go learn another language first.
Really good. Thanks a lot!
I'm vaguely aware of ParaText. From briefly looking at it again just now, it looks like something you'd build on top of the `csv` (or `csv-core`) crates. I'm somewhat skeptical of its ability to apply coarsely grained parallelism to CSV data in the general case, since you can't just seek to the middle of some CSV data and figure out where the next record starts/ends without reading the entire data. (That's why there is also a [`csv-index`](https://docs.rs/csv-index) crate that quickly indexes any CSV data which then allows very easy parallelism. `xsv` supports indexes in many of its commands so that, for example, slicing a CSV file only needs to parse the slice and not the entire file.) It would be interesting to benchmark against ParaText.
In the context of your 10ms vs. 100μs: that sort of thing *is* commonly significant: suppose we’re talking about a web server here; at heavy load, your 10ms handler might handle *n* requests per second, but the 100μs one can handle close to 100× as many. I recollect reading figures of NewsBlur a few years ago using fifty-something servers (the code being Python), where on calculating the actual CPU and network load from other figures given there I’m confident that *two* hardware-equivalent servers running sensibly-written Rust code would handle that load with ease—probably even one (though by my estimates it might have been getting a little tight).
I've never used those libraries you had referenced so I took a quick look. It appears they're doing statistics/number crunching on deep and wide datasets using Python, but not using Numpy/Pandas for any of the heavy lifting, as any practicing data scientist would (grandparent mentioned data scientists). I think there's great value in writing pure-Python libraries like the one you had been using because some people just want to run some analysis on an ARM chip that may not have access to Numpy/Pandas. There's a time and a place for using pure-Python implementations of libraries. If you were stuck Doug data analysis on an ARM chip, having a rust tool like the one you found would be very handy. In your use case, it sounded like you wanted to number crunch statistics on non-trivial data sets as quickly as possible. You didn't mention Numpy/Pandas, but I suspect that had your comparison used them, it would be much closer. Grandparent mentioned CSV parsing library had implications for data scientists due to Python's performance and I noted that for such purposes rushing to Rust and ignoring Python's C extensions was a mistake. Your example is also a case where C extensions (or in the case of Numpy, Fortran) could have helped you achieve your performance specifications without needing to reimplement your code in a systems programming language. (There's a burden of technical debt in rolling your own code for such things, which may or may not be acceptable to you.) There's nothing wrong with doing what you did. But please don't make unqualified statements about Python being slow without mentioning that for many applications, when C extensions are available, it's more than fast enough.
What is the actual benefit of masquerading return statements as awkward looking "dangling" expressions at the end of the function body? fn add(a: i32, b: i32) -&gt; i32 { a + b } I am very positive about Rust, but here it seems the designers of the language have forgotten that _explicit is better than implicit_ and make the learning curve of Rust even steeper for newcomers with one more non-obvious thing to remember. 
Like I replied above, I'm not familiar with these libraries and only took a quick look. To date, when I've needed to read and convert a CSV file, I've just leveraged Python's CSV and JSON libraries and gone on my way. If I've needed to do interactive slicing and dicing, I've use Numpy and Pandas. (The time it would take me to read the docs for xsv would also take me to just open a notebook and summon Pandas.) I guess if I had a need to write a shell script to do this kind of thing frequently, I'd reach for xsv in the future, but it's not a use case I seem to have. I'm glad to hear that you've achieved similar API's, I'll have to bookmark this for the next time I'm coding in Rust.
Sure that's fair! To further clarify, my entire endeavor into CSV parsing started when I was handed a 40GB file at work. That is prohibitively large even with the 32GB of memory I have on my workstation, but tools like `xsv` can handle it just fine, especially after running `xsv index foo.csv`. And it's not just about making more efficient use of the CPU, but being careful with how memory is used. For the most part, if `xsv` can do something in a streaming fashion, then it will.
 // A CSV writer maintains an internal buffer, so it's important // to flush the buffer when you're done. wtr.flush()?; Ok(()) } Is that just because it's a good habit in some *other* cases? As far as I can tell, Drop calls flush automatically (as I expected :-) so wouldn't that be enough?
Yes, but if the flush fails the error is swallowed, which is bad. I did it this way to be consistent with `std::io::BufWriter`, but I do think it's better to just call it explicitly in case it fails.
Mr.Sushi; can I recommend using an &lt;abbr&gt; tool-tip for some of the non-pronounceable text -- it'd help with new users who won't know what they're asking for if they don't know the terminology. For example, where this paragraph is _written_ as: &gt; This last step shows how we can use the ? to automatically forward errors to our caller without having to do explicit case analysis with match ourselves. We will use the ? heavily throughout this tutorial, and it’s important to note that it can only be used in functions that return Result. I would _read_ it as: &gt; This last step shows how we can use the &lt;abbr title="try operator"&gt;?&lt;/abbr&gt; to automatically forward errors to our caller without having to do explicit case analysis with match ourselves. We will use the &lt;abbr title="try operator"&gt;?&lt;/abbr&gt; heavily throughout this tutorial, and it’s important to note that it can only be used in functions that return Result. I've written up a small guide to semantic use of HTML5 abbr here: http://camendesign.com/abbr_redux
They seem quite similar indeed (here is the [readme](https://github.com/TeXitoi/structopt) in case anyone wants to know more)
Python pandas supports a 'sep' to explicitly state the delimiter.
Sure, pretty much every csv library has that. :-) But does it support the special Excel `sep=;` header that /u/1wd is referring to?
&gt; Slow is never better than fast, all other things being equal, and they are effectively equal for Python and Go Python is way more expressive than Go and in a lot of cases verbosity is a bigger concern than speed (i.e. as soon as your code is quick enough for your needs). See this [article](https://medium.com/@robertgrosse/my-experience-rewriting-enjarify-in-rust-723089b406ad) and [this one](https://medium.com/@robertgrosse/parallelizing-enjarify-in-go-and-rust-21055d64af7e), which compares Python, Go and Rust and discusses the trade-offs of each languages. Edit: From the first post &gt; Python, Rust, and Go had 2876, 3783, and 4971 lines of code respectively. The corresponding character counts are 83013, 108847, and 114180. That makes Go 70% more verbose than Python (in number of locs, 37.5% in term of character count).
structopt-derive was also discussed on this subreddit a while back: https://www.reddit.com/r/rust/comments/5u3sb5/structopt_when_clap_meet_custom_derive/ It seems a core difference is that structopt uses clap and gumdrop does its own thing from scratch. 
I have just finished a small procedural landscape generator (using `image`, `imageproc` and `rand`) https://rap2hpoutre.github.io/landscape-site/
Ahhh. I misunderstood. Never seen that lol
I'm not sure what you mean. If you mean that an option needs to have an argument when present, yes, that is the behavior for non-`bool` argument types. If you mean that a program will fail unless a particular argument was supplied, no, that kind of logic is left to the calling application.
Update your dependencies again with `cargo update`; this was a bug in cpp_demangle that has since been fixed. https://github.com/fitzgen/cpp_demangle/issues/79
Just use the [JSON](https://api.rocket.rs/rocket_contrib/struct.JSON.html) type from **rocket_contrib**. Your type should just be a vector of elements that contain those fields you need.
Further expanding my website written in Rust at the moment. It now features a content management system behind a login interface. I can edit, update, and publish posts, but I am working on being able to create and edit more than just posts. Last week, I was able to get it functioning with HTTPS.
burntsushi is a national treasure
&gt; It's not that easy, but it can be mastered, and good C/C++ programmers consistently write code that includes no UB. I've been programming in C++ for over 10 years, and at this point [I know I'm quite good at it](https://stackoverflow.com/tags/c%2b%2b/topusers). I *usually* write UB-free code, but I still makes mistakes from time to time. I could give excuses (aging code-bases, multi-threaded code, performance-sensitive code, ...), but while those are definitely contributing factors, the truth is that it just takes one moment of inattention, one careless mistake, or just bad luck, and **boom**. I've never met, or heard of, a single C++ programmer who could say with aplomb that he never triggered Undefined-Behavior any longer. I sometimes think that we could measure the expertise of a C++ programmer by the average number of lines of code they write between triggering UB. It is very much a problem, and my experience has been that the only kind of people who do *not* acknowledge that it's impossible to write UB-free C++ code are, ironically, those that know it less: - beginners hope to reach this horizon, - apprentice think they are getting close, - journeymen see themselves as successful, and blame the few times they do trigger UB on circumstances, - seniors accept the truth: it's called horizon because it's unattainable.
In the output for "tutorial-setup-01.rs" we see lines like "StringRecord(["Davidsons Landing", "AK", "", "65.2419444", "-165.2716667"])", but in the output for "tutorial-error-01.rs", the `StringRecord` output contains `position` and `fields` fields—what's the difference? They both use debug formatting.
Whoops. I changed the `Debug` impl half way through writing this and probably forgot to update some of the outputs. :-) Thanks for noticing!
And then I'm pretty sure I've seen middleware stuff to validate that given data... should be enough. Mind if I ask a few more questions later?
Ah, brilliant. That fixed it. Thanks for the advice!
Forums like this exist for questions to be asked, so sure.
Wow, that's really neat! I've been interested in Markov chains for a while now but have never really investigated them much. I had no idea there was a crate that made using them so simple. Cool application for it. Now I wish my friends used *text* on discord more often. I wonder if we have a voice to text crate...
I'm trying to switch [a project to `trust`](https://github.com/cobalt-org/liquid-rust/pull/100) but am getting a confusing compile error https://travis-ci.org/cobalt-org/liquid-rust/jobs/234404415
How do people recommend using clippy in CI? - Build failing for clippy issues makes it easy to review - Clippy's build sometimes fails because it is out of sync with nightly versions (I'm assuming) - I could make clippy build issues cause us to skip clippy but then I'd never know when for how long I'm not getting clippy coverage.
It's going much better now; don't let initial reception disappoint you ;)
Excellent article. I appreciate the technical depth. There are so many low level details that need to be taken care of in the embedded space. Articles like this give me confidence that the tooling and support libraries (and howtos) are getting to the point where I feel like most of my time will be spent being creative instead of doing low level reverse-engineering. 
&gt; But I think you kind of buried the lede there. The problem isn't really that we're duplicating keys for every record, but rather, that we're creating an entirely new hashmap for every record. I'll go back to one passage in my comment: &gt; &gt; If so that sounds like a significant downside, one that would be worth pointing out—it would likely bite people who naively stick a lot of those hashmaps into a big in-memory data structure. So the cases I have in mind are ones where you *have* to allocate individual in-memory containers for a large number of records you've read from a CSV, whose schema you don't know ahead of time. The problem here is how to avoid copies of the field names filling up a huge chunk of memory. So what I have in mind is: 1. Read all the header names at the beginning, and intern them into some sort of symbol table. 2. For each record, stick its field values into some newly allocated data structure that indexes them by symbol. One simplistic way to do this would be to just: 1. Build a map from `String` field names to their indexes in the CSV records; 2. Deserialize each record into a newly allocated `Vec&lt;String&gt;`; 3. To look up a record's field by name, look up the field name in the map to retrieve an index into the `Vec&lt;String&gt;`. A more sophisticated way would be to have some sort of data structure or library for in-memory tabular data whose schema is only known at runtime. A quick Google search yields [this example](https://www.suchin.co/2016/12/28/Introducing-Utah/).
I've been trying to figure out the idiomatic way to express errors while loading a configuration from environment variables. What I've done feels janky; I don't really like the way I've chained a different error onto `env::var()`. Additionally, should I create a custom ConfigError type and use that, instead of using a `&amp;'static str`? #[derive(Debug)] pub struct Secrets { pub token: String, } impl Secrets { pub fn from_env() -&gt; Result&lt;Secrets, &amp;'static str&gt; { Ok(Secrets { token: env::var("TOKEN").or(Err("The TOKEN environment variable must be provided!"))?, }) } } 
&gt;they are effectively equal for Python and Go. ~~If we're still talking about data science, the python ecosystem shits on Go's.~~ I can't read
Nice project. I will try to play it tonight. Btw, how much better this transpiler result compared to corrode? Its seem crust is able to parse basic C++ code. Any plan to split the C parser to separate crate? I imagine a good c/c++ parser crate in rust will be very useful to a lot of people. 
I think all the unused crates are all used in commands - it's a general practice to declare all the crates in main.rs that are used in the crate I think. After reading this, I realized I haven't looked at commands/ either, but I agree with everything you've posted.
np. it's a better name because you can explain it in terms of itself, rather than being like "it's called that because it's similar to this old thing that was deprecated even though it now does something different", hehe.
There's dozens of things this depends on. Basically: what are you trying to do? For local time series learning, RNN s and LSTM are the Google terms you want
I enjoyed the article, but I'd recommend that you reduce the self-deprecation a little. It's funny at first, but then becomes tiresome and detracts a little from the message of the piece.
Does this generate safe or unsafe rust code? Seems safe, but that doesn't make sense. Also, AFAICT the nanoparser thing is basically parser combinators.
How many memes-per-second? :)
You might want to note in your example or docs that the first item in `std::env::args()` is not always there, so skipping the first item is sometimes not correct. Otherwise very cool! I have always felt that `clap` was really heavy (esp. in terms of dependencies), and I didn't like that it built the options at runtime.
That seems reasonable, it mimics how panics happen when you try to go outside the bounds of an array. `Result`'s are more for things that you expect will fail every once in a while, and for errors that are recoverable. One thing you might try is finding analogous function in the std lib, or large popular libraries, and see what they do.
Inheritance in C++ is just flat embedding of structs, so: struct A { // ... }; struct B: A { // ... }; is effectively: struct A { // ... } struct B { A a; // ... }; except the members of `B::a` can be accessed as if they were members of `B`, and obviously the name `a` doesn't exist.
Thanks for the detailed reply with specific references to VM architecture. I know that tuples in Python are about as good as it gets, but like you said, Python coders generally don't do tricks like optimizing memory layout, or recycling allocated objects to minimize system resources. (I haven't dived into the implementation of the VM itself. I know VM's like the JVM usually have blocks of memory and allocate by bumping a pointer in a block, which achieves amortization. But I don't know whether the CPython VM does that, or whether it does it in C extensions.)
Thanks! I've been meaning to do something kind of like that, but not really known what to do about it. I wish there were some kind of pattern like `?`or `try!` that doesn't do an early return. Or maybe I should learn "The Rust Way" of checking `err` values.
Well, I would have predicted "rewrite it in Rust" memes, but those initial comments were... well... 🔥💩🔥💩🔥.
First of all, there is (actually was) a bug in the compiler, because the `'a` lifetime doesn't end at the end of the `next` function, it can extend long after the `next` is finished. The nightly version contains a better error message as it no longer makes it look like those lifetimes are the same. So back to your code. First, note that you don't actually need `ref` and `ref mut` in matches – the `a` variable is already a reference, no need to re-reference it. Now, we're getting "cannot move out from a borrowed context". Rust suggest adding `ref` now, but here's why it doesn't work: `ref` creates a temporary reference, that's valid only inside a match, (so it's shorter than your `'a`). Why it's "moving out of borrowed content" then? Basically, you're returning a (value based on) mutable reference inside `self`, and you want `self` not to be destroyed! That's against the "no multiple valid &amp;mut references" rule. So, basically, you can't do what you're trying to do with `Iterator` based only on `ArrayLike::at_mut` function. Iterator needs a proof that two calls to `next` won't point out to the same memory, which in your case is totally false – they will. So a possible solution for you now is either not to use the `Iterator` trait (try searching "rust streaming iterator" on crates on google), or you'll need to extend your `ArrayLikeMut` with an operation similar to `split_at_mut` or [`split_first_mut`](https://static.rust-lang.org/doc/master/std/primitive.slice.html#method.split_first_mut). There was [a thread here](https://www.reddit.com/r/rust/comments/6a9iil/unable_to_make_the_simplest_thing/) not so long ago that you might find helpful. 
This implication is true (this is the inherited mutability): "X is inside mutable variable or behind `&amp;mut`" → can mutate X This implication is false (it's usually true, but in general, not): "X is inside immutable variable or behind `&amp;`" → cannot mutate X Ie. types like `Cell` or `RefCell` or `Mutex` can be mutated through a shared (`&amp;`) reference. It's 100% safe. Although it's better design your code not to abuse these types. So I think it's a nomenclature fail. `&amp;` actually means shared (not *immutable*), `&amp;mut` means unique (not just *mutable*). There was a proposal before 1.0 to rename `&amp;mut` to `&amp;uniq` and never use *mutable*/*immutable* naming again. But the *mutable*/*immutable* is *usually* true, so we stick with it. Anyway, I think your parser's code would be more readable without the `Cell` and with `&amp;mut self` methods.
No problem! With the early return, I don't think there's _that much_ more one can do with errors. I mean, you can `match` on them, but that's pretty verbose, and you can use `.map` to do work on the Ok case while skipping the error case, but I think most often it's best to just skip to a higher level, and let the error propagate. There are the other ways of dealing with errors though - what would your ideal 'program flow' look like, if it isn't one of having errors return early?
Just that some of the errors I've been running into, aren't an indication of a critical failure, just a different case to handle. For example, parsing a number, and should it overflow, I still want it to continue execution, and default to 1 or maybe the maximum value instead. (which I do, `unwrap` or otherwise unencapsulate, check for `err` and assign a default value if there is and `err`, and continue executing)
This isn't the same. Yours is `HashMap&lt;TypeId, Box&lt;Any&gt;&gt;` where `Box&lt;Any&gt;` is `Vec&lt;T&gt;` However, the equivalent to the article would be `HashMap&lt;TypeId, Vec&lt;Box&lt;Any&gt;&gt;&gt;` where `Box&lt;Any&gt;` is `Box&lt;T&gt;`, so every element is a trait object.
Working my way through Rusoto PRs again, starting with fixing up an issue with [SQS requests](https://github.com/rusoto/rusoto/issues/586) the moving on to [correctly sending some S3 request parameters](https://github.com/rusoto/rusoto/pull/613). If I manage to work through that I can finally look into a PR that brings in streaming support which means I'll be diving into some bits of Rust I haven't had a chance to yet. 😄
I guess a more nuanced way to put it would be, "If you're looking to add a FFT library to your project, RustFFT is by far the best choice". RustFFT combines pure Rust (aka no external dependencies), with documentation that exists, with a stable API, with speed. RustFFT is the fastest pure-Rust implementation, but some of the bindings crates may be faster. (I haven't used clFFT but there's a good bet that it's faster, and FFTW's speed is legendary), but they both seem relatively immature, both lack documentation, and the fftw bindings at least are very unstable. For most applications, RustFFT will be fast enough. So just like any other optimization, I would recommend using RustFFT to start, and switch to one of those other libraries down the line when you know you need it.
Kate user here, so maybe not relevant to you, but I'd be really happy about improvements to the syntax highlighting file. I believe KDevelop and Kate share the file? It seems some old and outdated (deprecated before 1.0) types are still present and being highlighted, while the upcoming types i128 and u128 are not being highlighted. I'd especially love some highlighting of the `?` operator, black makes it hide away too much. It could be orange like `Ok/Some` and number literals. Another little bug with syntax highlighting I've discovered concerns numbers used in a range context: for i in 0..2 { println!("{}", i); } that code gets displayed as [this](https://pasteboard.co/9W2wSs4lM.png). The issue here is that the first dot gets printed like it were part of the int literal, which it is not. Generally, `let f: f32 = 4.;` is valid syntax, so generally it is okay to highlight a trailing dot this way, but in this case the dot doesnt create a float literal out of the int literal, but instead is part of the range. Would be cool to have some fixing here. Thanks in any case!
No yeah, that makes perfect sense! I guess at this point it's just learning the methods that are available, like Result::unwrap_or` seems like it would work well in that case (result.unwrap_or(1)`). Or maybe `result.unwrap_or_else(|e| {println!("parsing failed, default to 1: ", e); 1})` would work better - this executes a block of code on the err case, so this would print a warning then default to 1. http://blog.burntsushi.net/rust-error-handling/ is another resource which might be useful - not sure if it covers this particular case. Does [`unwrap_or`](https://doc.rust-lang.org/std/result/enum.Result.html#method.unwrap_or) seem like it'd fit that case well though?
Why add the extra boxes and downcasting? Wasn't the whole point to keep objects continuous in memory for better caching? You don’t need a box to create a trait object. impl AnyCollection { fn first&lt;T: Any&gt;(&amp;self) -&gt; Option&lt;&amp;Any&gt; { self.0.get(&amp;TypeId::of::&lt;T&gt;()) .and_then(|l| l.downcast_ref::&lt;Vec&lt;T&gt;&gt;().unwrap().first()) .map(|r| r as &amp;Any) } }
If the Input struct is exactly that it could easily be replaced by std::io::Cursor, which implements Read/Write/Seek for varying types, including &amp;[u8] (only Read/Seek)
/u/perplexinglyemma, be warned that Syntex is no longer maintained and you will not be able to parse any language features that have been added recently. See [serde-rs/syntex#114](https://github.com/serde-rs/syntex/issues/114). I would strongly discourage you from building anything new on it. Using [rust-lang/rust#41732](https://github.com/rust-lang/rust/issues/41732) may work but you will want to coordinate with the RLS, rustfmt, and racer folks. They live in #rust-dev-tools in IRC.
Yeah I guess that's a more elegant solution where the lifetime is not to the `Input` struct. So it's `Input&lt;'a&gt;` I guess. Do you have an example fo such correctness bugs? I did just update it like you suggested which indeed works. Edit: After some consideration I actually changed it back to &amp;self rather than &amp;mut self, my arguments are: 1. It makes the library more flexible as it can now be used in more contexts 2. The "mut" keyword is just a lie. People should not read it like "this will mutate the data structure" but "whatever this does cannot be done without an exclusive reference" 3. What it does _can_ be done without an exclusive reference so it's fine.
I'm deserializing a TOML file into structs and I've managed to get it working, but it feels inelegant because of the need to dance around the `Result` types. I attempted to use `map`, but I couldn't get it to work; I think this is due to my misunderstanding of the types. I wonder if I could please have some pointers on how I might improve my TOML loading code? fn read_toml() -&gt; Option&lt;Config&gt; { let mut contents = String::new(); let file = File::open("Assets.toml"); if let Ok(mut f) = file { if let Ok(_) = f.read_to_string(&amp;mut contents) { let config: Result&lt;Config, _&gt; = toml::from_str(&amp;contents); if let Ok(config) = config { return Some(config) } } } None } The repeated nesting of `if let` is a red flag to me. I feel like I'm doing something wrong :)
&gt; FFTW's speed is legendary Note that FFTW is written in OCaml and generates C code. Is generating a Rust FFT library by metaprogramming also a good idea? (perhaps with macros / procedural macros)
[Are we learning yet](http://www.arewelearningyet.com/neural-networks/) has some crates.
Why would we want a list of trait objects or boxed objects? Again, isn’t the whole point to keep similar objects continuous in memory. The type erasure should be provided at the collection API boundary. I don’t think the C++ version stores pointers to objects, that’s the vector solution they are trying to avoid.
I suppose that using the [Tensorflow bindings](https://crates.io/crates/tensorflow) is the most practical way to begin, since there's a lot of tutorials for Tensorflow, you would only need to translate them to Rust. [Like this](https://www.tensorflow.org/tutorials/recurrent#lstm) &gt; The core of the model consists of an LSTM cell that processes one word at a time and computes probabilities of the possible values for the next word in the sentence. (...) &gt; &lt;some python code&gt; But as I linked elsewhere, you may want to look into the neural network libraries at [are we learning yet](http://www.arewelearningyet.com/neural-networks/) (which have varying degrees of completeness).
I'll look into using those instead! Thanks for the reading material
Crust considers C as a subset of C++, so it doesn't differentiate between two of them. Crust was intended to translate subset of C++, but planning to improve it. Open to suggestions.
Hi, I'm one of the devs on this project. The reason for doing this project was to promote safer code. So, it aims to generate safe code from safe/unsafe code. 
How does it do that? The general problem here is intractable, C++ simply does not have enough static info to do this safely. What do you translate pointers into, for example? Or are you replacing everything with `Rc&lt;RefCell&lt;T&gt;&gt;`? That could work I suppose.
Has anyone mentioned any thoughts on leveraging the future `const T` syntax for variable sized arrays to also defining compile time trait instances? 
Welcome to Reddit. You have posted this to the wrong subreddit. You probably want /r/playrust. In future, I suggest you check what a subreddit is about *before* posting.
&gt; I would use **Rust** where I need an absolutely, perfectly uncrashable high-speed system under all circumstanses, for a large codebase. So, always? I mean, maybe not the last part. But I think there are rather few times you shouldn't worry about speed and crashes.
Just wanted to say thank you so much for your work on this! Looking forward to trying it out with my music project :)
They are orthogonal: `impl Trait` (in return position) is about avoiding naming specific types, that are "chosen" by the function they're in, whereas the current generics inside `&lt;&gt;`, both const and not, are things that the callers of the function choose. The [RFC covers this a bit](https://github.com/rust-lang/rfcs/pull/1951/files#diff-f1f0d3614f4fffe48ccff79fb26239f4R104). The orthogonality here is that `const` things can also (theoretically) be chosen by callers or choosen by callees, for instance, abusing/reusing the `impl` keyword, `fn foo() -&gt; [T; impl usize] {}` might represent a function that returns *some* fixed sized array, but callers of the function don't get to know its length statically. (This does however raise an interesting point: what if we do want to extend the unboxed existentials (in return values) and/or automatic generics (in arguments) to `const` parameters? The keyword `impl` is ... really bad for this.)
Well that's just a single byte; you don't even necessarily need `byteorder` for that. Just look at [the documentation for `byteorder`'s `WriteBytesExt`](https://docs.rs/byteorder/1.0.0/byteorder/trait.WriteBytesExt.html) and pick the appropriate method. **Edit**: To clarify a little; `byteorder` just adds some helper functions to otherwise basic IO. **Edit 2**: Actually, that specific example is literally just `&amp;[253u8]`, plus or minus questions of ownership.
hi Pointers are not handled completely yet. A pointer is converted to reference so far. RefCell or Box would help, Thank you :)
Ah, okay. You're going to have some trouble getting this to work well :) Rust references have the mutability issue, and there's no easy way of doing this program-global analysis. Additionally, you have to detect where ownership needs to be used.
and pack('&gt;I', 123) ? 
It's mentioned a little ways down the article: &gt; The cargo that will ship with Rust 1.18 supports a RUSTC_WRAPPER environment variable that makes using sccache with cargo simple: you just set RUSTC_WRAPPER=/path/to/sccache (or RUSTC_WRAPPER=sccache if it's already in your PATH).
Ah. I now I understand. I think that's unlikely, at least for now. It was largely designed as a tool for CI servers, and local development seems to be a bit of a work in progress. It's an easy enough install process that I'm not sure integration is worth the effort, unless it's just add a post-install script that calls "cargo install sccache; export RUSTC_WRAPPER=sccache;".
It says how in the documentation they linked, but here's how in case it's not obvious. The "&gt;" is big endian, and the I is 4 byte unsigned int (from the struct documentation [here](https://docs.python.org/2/library/struct.html)). So the equivalent in rust would be: use byteorder::{BigEndian, WriteBytesExt}; let mut wtr = vec![]; wtr.write_u32::&lt;BigEndian&gt;(123).unwrap();
Thanks for the update! I'm currently writing an audio feature extraction library using RustFFT, so this is great news!
Oh sounds interesting! Any chance that this code is public / open source? I'd probably be interested in contributing at some point as I'd like to experiment with some instrument auto-classification ideas.
I think you mean https://rust.godbolt.org/
sure, it's here https://github.com/meyda/meyda-rs It's very unfinished though, I'm in the process of writing tests and just found some functions aren't correct :P It would be amazing if you wanted to contribute!
Sounds like the same problem I had too. Check this out: https://github.com/rust-lang/cargo/issues/4056 Check if setting the binary inside the deps folder (the binary with the hash value in the name) helps.
That would introduce handling errors (which actually can't happen).
I think even for bigger match statements the readability is fine. In my implementation of the CHIP-8 I did use it for the opcodes (if you wanna have a look: https://github.com/demilich1/chip-8/blob/master/src/chip8.rs#L132 ) IMO the important thing with big match statements is that each individual match arm should be as short as possible; I just handled everything in its own function.
&gt; From the POV of concurrency if a task meets the conditions to access a resource then it has exclusive access to that resource as no other task can preempt it and access the same resource. So, from the POV of concurrency access returning a mutable reference is perfectly valid. However, access returning a mutable reference doesn’t sit well with Rust borrowing rules: mutable aliasing is a problem even within a single thread / context / task as it can lead to pointer invalidation ---- &gt; However this safety mechanism is also rather restrictive because you can’t mutably borrow two different resources within the same scope: ---- &gt; Resources have the following property: “once a task has accessed a resource then no other task that may access the same resource can start”, where “start” here means preempt the current task. You can flip that property into: “by the time a task accesses a resource no other outstanding borrows to the resource data can exist in other tasks”. Taking this to the RefCell context: “the first time a task accesses a RefCell resource the borrow count of the RefCell is zero”. Or conversely: “as long as Rust borrowing rules are not broken within a task the dynamic borrows of a RefCell can’t fail”. ---- Could you do a thing where tasks declare their `Local`s in the `tasks!` macro instead of within the `fn` body, and the `fn` signature of the task must correspondingly contain an `&amp;mut T` parameter for each `T` resource that was declared, and RTFM passes in the `&amp;mut` borrows when it starts the task? That seems to closely match the logic of "while a task is running it has exclusive access to all of its locals", and the borrow checker can handle all of the intra-task reborrowing and so on from here. (Alternately you could have a single struct containing all of the locals and pass an `&amp;mut` to that to keep signatures shorter.) I'm not sure whether (if the idea makes sense at all!) it could be applied to `Resource`s as well, which seems like it would have significantly greater value. 
I thought that it might be related to [this](https://github.com/rust-lang/rust/issues/39160) and tried with `codegen-units = 1`, but it still can't find the source.
I think you can use bincode with serde given some preconditions about the deserialization you're doing. If not, this would be a useful serde-based library to have.
[My first PR to Cargo](https://github.com/rust-lang/cargo/pull/3929) is there! :)
since Rust already has very good escape analysts and more metadata compared to for example C#/Java, can't we implement some sort of automatic arena allocator? (arena alocator is also pointer bump with deallcation being done in big chunks) its same thing Java does under covers if its escape analysts detects your variable is not "escaping" to another thread (works on inter-procedural level) also separate arena for "session" variables of some sort for server/webapps that are request/response based
When trying to figure these errors out, it is always a good idea to try and get rid of all the mentions of anonymous lifetimes and “autorefs.” The following change to `Bar`’s impl introduces only explicit lifetimes and unique lifetime names: impl&lt;'b&gt; Bar&lt;'b&gt; { fn do_foo&lt;'c, 'd, S: System&lt;'b&gt;&gt;(&amp;'c mut self, system: &amp;'d mut S) { Foo::foo(self.foo, system); } } (The lifetime elision rules say that each elided lifetime in a function’s arguments becomes its own lifetime, hence `'c` and `'d` for the two arguments.) Now the compiler has this to say: error[E0495]: cannot infer an appropriate lifetime for lifetime parameter `'a` due to conflicting requirements --&gt; &lt;anon&gt;:21:9 | 21 | Foo::foo(self.foo, system); | ^^^^^^^^ | note: first, the lifetime cannot outlive the lifetime 'c as defined on the body at 20:70... --&gt; &lt;anon&gt;:20:71 | 20 | fn do_foo&lt;'c, 'd, S: System&lt;'b&gt;&gt;(&amp;'c mut self, system: &amp;'d mut S) { | _______________________________________________________________________^ starting here... 21 | | Foo::foo(self.foo, system); 22 | | } | |_____^ ...ending here note: ...so that reference does not outlive borrowed content --&gt; &lt;anon&gt;:21:18 | 21 | Foo::foo(self.foo, system); | ^^^^^^^^ note: but, the lifetime must be valid for the lifetime 'b as defined on the body at 20:70... --&gt; &lt;anon&gt;:20:71 | 20 | fn do_foo&lt;'c, 'd, S: System&lt;'b&gt;&gt;(&amp;'c mut self, system: &amp;'d mut S) { | _______________________________________________________________________^ starting here... 21 | | Foo::foo(self.foo, system); 22 | | } | |_____^ ...ending here note: ...so that types are compatible (expected System&lt;'_&gt;, found System&lt;'b&gt;) --&gt; &lt;anon&gt;:21:9 | 21 | Foo::foo(self.foo, system); | ^^^^^^^^ So, it is failing to determine a concrete lifetime to use for the generic lifetime `'a` in `Foo::foo()` because there is a conflict. It needs to be at least as long as `'b` (which we now use for the lifetime of the `Foo` reference stored in `Bar` as well as the lifetime parameter of `System`), but it also must be at most as long as `'c` (which is the lifetime of the reference to `Bar` the method operates on). This can only be solved if `'b` and `'c` are the same lifetime, so `&amp;mut self` must be `&amp;'b mut self`: impl&lt;'b&gt; Bar&lt;'b&gt; { fn do_foo&lt;'d, S: System&lt;'b&gt;&gt;(&amp;'b mut self, system: &amp;'d mut S) { Foo::foo(self.foo, system); } } This conflict comes from the lifetime parameter of `System` and the fact that you tied the time `self` remains borrowed in `Foo::foo()` to that lifetime of `System`. So, because the `Foo` inside of `Bar` needs to remain borrowed for the lifetime of `System`, the `Bar` needs to remain borrowed for that lifetime, too. You are trying to get around that by sneaking a mutable reference into `Bar`, but the compiler won’t have any of that.
&gt; It would be interesting to benchmark against ParaText. [Done](https://bitbucket.org/ewanhiggs/csv-game)!
&gt; I would use C where you need high speed and don't care about security. Even if you don't care about security I think having stuff like an expressive type system and composable iterators makes Rust a compelling alternative to C in all circumstances. Even if you just turn off the borrow checker altogether it's a better language.
Exciting! I was getting really sick of having to choose between boxing returned iterator chains and writing out ugly type signatures for them. `impl Trait` in argument position looks pretty, but I'm not sure how necessary it actually is, given that we already have two ways of specifying generic argument bounds. `fn foo&lt;T: Trait&gt;(arg: T)` and `fn foo&lt;T&gt;(arg: T) where T: Trait` are so similar -- I wish we would have just picked one! (A very minor wart, all things considered; and `impl Trait` for arguments at least seems like actually useful syntactic sugar.)
You need to manually provide impls for `Box&lt;Hello&gt;` and `&amp;Hello`, [like so (playground)](http://play.integer32.com/?gist=332e6c04fe8654bdc5dca47f37f08495&amp;version=undefined). You can also provide impls for `&amp;T and Box&lt;T&gt; where T: Hello` instead – [see out this `impls.rs` from `std`](https://github.com/rust-lang/rust/blob/070fad1701fb36b112853b0a6a9787a7bb7ff34c/src/libstd/io/impls.rs). The only solution to be able to choose between static vs. dynamic without any custom impls, is to always take a trait behind reference: struct Test&lt;'a, T: 'a&gt; { value: &amp;'a T } impl&lt;T: Hello&gt; Hello for Test&lt;T&gt; { fn hello(&amp;self) { self.value.hello(); } } I think you should prefere manual impls for `Box` and `&amp;` though.
&gt; I wish we would have just picked one! You can only pick when the options are equivalent. In this case, the `where` syntax is a superset of the other and was added as an afterthought. In principle we could deprecate the older/less powerful syntax, but we have to keep it for backwards compatibility. 
(I'm not part of the RTFM project, so maybe I'm wrong. But here's my understanding.) No. A task can't be invoked during its invocation. The task's priority prevents any other task of equal (or lesser) priority from running, so Task1 can't be started while Task1 is active. Task2 can *request* another invocation of Task1, but that will enqueue it (by raising a flag in the NVIC), and it will run after the first invocation finishes.
Yeah, I posted a question about that particular syntax in this sub a while back. The backstory makes sense, and I'm definitely not suggesting that we break compatibility or anything. It's just sort of unfortunate (but, like I said, in a *very* minor way).
thanks for the response. this is true for non preempt scheduling, but if I understand the RTFM correctly it supports also preempt scheduling. Therefor it will not wait until the invocation is finished. (NVIC can interrupt other IRQs) like in this picture described: http://blog.japaric.io/fearless-concurrency/preemption.svg for the first scenario it is OK, for the second it can be still OK as far as the stacks are in the same order as the tasks priority, with the highest priority task having its stack at the bottom. So Rust needs to order the stacks at compile time? If this is the case, than I understand, how it can work. the 3rd scenario can be solved with the arguments from 2nd scenario
I'm trying to decide between Rust and C++ for my next project. The crucial feature of the project will be to parse the library source code and dump CFFI metadata which will then be used by some script to automatically generate API/bindings for various languages. In C++ I would do this using python and libclang as a separate build step and custom attributes behind a macro on functions/types I want to expose. How would I go about doing this in Rust, how much of a hassle would it be ?
Thanks for your reply! If I understand correctly, Is it because only the trait owner can make sure all trait method require only reference of the struct?
Huh, I wrote an unrelated static site generator with the same name about 6 months ago
Is it still faster than the equivalent branch? I'm not a big algorithms guy, does this have a hot and a cold branch that could make the branch faster than cmov?
Yes, if you cant represent it in type. Otherwise it's just an unnecessary noise. That is why I've created [`genio`](https://github.com/Kixunil/genio). I'd like to add `Cursor` too but I don't have enough time right now.
RTFM's preemption means that a higher priority task may preempt a lower priority task. A lower priority task can't preempt a higher; that's the meaning of "priority". And in RTFM, an equal priority task can't preempt either. &gt; So Rust needs to order the stacks at compile time? Yes, RTFM requires that an application declare all tasks and their priorities statically. That's what the `tasks!` macro does. And yes, that implicitly orders their stack segments to be in the same order as their priorities. But note that stack space is still allocated dynamically. It's just that task priorities restrict the order of invocation and therefore the order of allocation. japaric, does RTFM require a single `tasks!` macro per app, or can they be sprinkled through the modules?
I didn't test extensively, because it ended up not being useful in the end. From what I got if the argument is random (so random place in the array) (and the CPU is reasonably good with cmovg) then the cmov version is faster. If the position in the array is somewhat predicable (more toward the beginning of the array for instance) then the branch prediction version will be faster.
I have a couple dozen various boards. But I just moved, and they're still packed in boxes.
Probably overall best to use cmov then and take the hit on almost-sorted arrays. I wonder if it's possible to write "normal" Rust that consistently compiles to this or a couple of instructions off, for portability's sake. Even horrible unreadable Rust (which optimised Rust normally isn't, it's normally just more verbose) is at least cross-compilable.
&gt; Is *it* because only the trait owner can make sure... I'm not sure I understand your question. What behaviour do you mean by *it*? &gt; ...all trait method require only reference of the struct? Do you mean no `fn foo(self)` methods, that prohibit object safety?
...how can this language be so popular? Do they have something else do deal witha lack of parametric polymorphization?
&gt; the first item in std::env::args() is not always there, so skipping the first item is sometimes not correct. Huh, is that so? I didn't know that! I even happened to look at [the docs](https://doc.rust-lang.org/std/env/fn.args.html) recently. They say: &gt; The first element is traditionally the path of the executable, but it can be set to arbitrary text, and may not even exist. This means this property should not be relied upon for security purposes. At the time, I took "and may not even exist" to mean that "[the path of the executable] may not even exist [on disk as a file]". But yeah the simpler interpretation would be that the argument itself might not exist. If that's the case, how do you reliably know when/whether the first argument is the "executable", or when it's the first argument passed to the program?
oh snap
You can click "Asm" button on playground (or LLVM IR or MIR) to get particular type of output.
Yes. I forgot to change the name. It has now been fixed. (When testing I usually just name resources R1, R2, etc., but COUNTER seemed more descriptive in this example)
&gt; [GlobalStack][Task1Stack][Task2Stack][...FREESPACE....][HEAP] If your stack looks like that that means that Task2 priority is higher than Task1's. If a new Task1 event arrives the execution of a **new** Task1 will be delayed until after *both* Task2 and the original Task1 finish. In RTFM only higher priority tasks can preempt lower priority ones. The processor will never suspend a higher priority task to resume the execution of a lower priority one.
Yes, your Local in tasks! macro is a good idea. I haven't though about doing something like that for task local data before. &gt; Alternately you could have a single struct containing all of the locals and pass an &amp;mut to that to keep signatures shorter.) At that point it looks even more like a closure (struct + FnMut), isn't it? Where the captures are in static RAM rather than on some stack frame. I have been wondering if the upcoming [generator + yield] feature could be leveraged to make tasks more pleasant to write. Something like this: #[task] fn blinky(ref prio: P1, ref thr: T1) { loop { LED.on(); yield; LED.off(); yield; } } No explicit `Local`s. That would also remove the need to manually write state machines (where you store the state in a `Local`) when you coding tasks that have to handle something like an I2C session. &gt; I'm not sure whether (if the idea makes sense at all!) it could be applied to Resources as well One idea I had was to create a resource token per resource and make them required to access the token mutably (just like `Local.borrow_mut`). *All* the tokens would then need to be available to all tasks in their signatures. Something like this: tasks!(stm32f100xx, { t1: Task { interrupt: Exti0Irq, priority: P1, enabled: true, }, t2: Task { interrupt: Exti1Irq, priority: P2, enabled: true, }, resources: [ static R1: Resource&lt;bool, C1&gt; = Resource::new(false); static R2: Resource&lt;u8, C2&gt; = Resource::new(0); ] }); fn t1( ref task: Exti0Irq, ref prio: P1, ref thr: T1, mut r1t: R1Token, mut r2t: R2Token, ) { let r1: &amp;mut bool = R1.access_mut(&amp;mut r1t, prio, thr); // let r1: &amp;mut bool = R1.access_mut(&amp;mut r1t, prio, thr); //~^ error: cannot borrow `*r1t` as mutable more than once at a time thr.raise( &amp;R2, |thr| { let r2: &amp;mut u8 = R2.access_mut(&amp;mut r2t, prio, thr); // .. }); } fn t2( ref task: Exti1Irq, ref prio: P2, ref thr: T2, mut r1t: R1Token, mut r2t: R2Token, ) { let r2: &amp;mut u8 = R2.access_mut(&amp;mut r2t, prio, thr); // .. } Needless to say the signatures become unwieldy with this approach. Also it seems tricky to implement as the resource tokens are a property of the application but we want the `access_mut` to be defined in the RTFM crate and the resource tokens appear in the signature of `access_mut`. But maybe those two can be decoupled using some (unsafe) trait. EDIT: code format
The language is popular because google is backing it. Besides that, it does have good points, like: * It's faster than python while being as simple to understand. If you have codebase in python and want to speed it up, having employees learn go is a rational choice * It compiles faster than C * Asynchronous code is easy using goroutines and channels * _Easy to deploy._ It does static compilation by default (like rust) * External dependencies are easy to get, even if versioning is (or was?) a pain But I haven't used Go in a while, so take this list with a grain of salt. As for parametric polymorphization, well... You don't _need_ it. But if your codebase is larger than a weekend hobby project, it will become a noticiable pain. I believe the most common work around is generating code before the compilation step.
cool :D was testing this out, but doesn't look like the the dwt enable part is working for the bluepill I'm testing it out on. DWT: Peripheral { register_block: Dwt, ceiling: C1, }, }); // INITIALIZATION PHASE fn init(ref priority: P0, threshold: &amp;TMax) { let dwt = DWT.access(priority, threshold); dwt.enable_cycle_counter(); 60 let gpioc = GPIOC.access(priority, threshold); &gt;&gt;&gt; x 0xE000_1004 0xe0001004: 0x00000000 &gt;&gt;&gt; x 0xE000_1000 0xe0001000: 0x00000000 Dug around a little and it looks like the demcr register needs to be set to enable dwt access. [link](http://infocenter.arm.com/help/topic/com.arm.doc.faqs/ka16334.html) I'm using an st-link with st-util, rather than openocd. Maybe openocd enables it automatically? edit: tested with openocd and it works correctly. 
Right, but there is no more direct equivalent. Rust *can't* duplicate the Python interface exactly. `byteorder` is the simplest replacement.
In theory, this could be achieved via macros, isn't it?
The biggest thing that pushed me away from Helix to Ruru for a native Ruby gem was that Helix doesn't seem to allow me to call Ruby from Rust while Ruru does. For my use-case, I needed to be able to call Rust from Ruby and Ruby from Rust to marshal data back and forth from ActiveRecord. Either way, it's really cool to see libraries pop up to help abstract away FFI into more expressive interfaces.
This was a super useful video, thank you! 👍
Note that `sccache` is itself written in Rust! https://github.com/mozilla/sccache
Sure, understood, that's reasonable. But I'm still interested. :-)
Well yeah you obviously transfer the source to the deployment machine and compile it there to link to the libraries there. I don't get this new-age "binary distribution" fad, binary distribution is _horrible_ and a thing closed-source devs are burdened to figure out. The correct way is to distribute the source and compile locally which is far more flexible. Distributing binaries indeed sooner or later involves such horrible practices as static linking and/or entire docker images. Just distribute the source; it is the best way. POSIX does not define a binary interface and for good reason. &gt; Practically, interface{}, void* or Object (depending on your language). Is it safe? No. Does it work? Yes. I might as well be using dynamic typing then which I have nothing against on principle but having to resort to dynamic typing inside a statically typed language to escape the inexpressiveness of the type system just gives you the worst of both.
I've heard of this, but I don't know much about it. If it does what I think it does, I think this would be best as a separate crate that depends on RustFFT. I personally think RustFFT should be "as simple as possible, but no simpler". This way users can pick and choose what features they need by which crates they use
I compared it with the FFTW binding crate a few months ago, and at that time it was actually competitive -- but some caveats are that I only tested with FFTW_ESTIMATE and at the time that crate only supported f64. I haven't tested since then, but it would be reasonable to say FFTW is 10-50% faster depending on floating point size and what flags you're passing to FFTW.
Cool! That is really fast indeed! You might want to add fftw as an optional dependency for the benchmarks, and add some benchmarks that use fftw for the same operations as your library so that you can always have a good baseline :)
Hang on, it's faster? I would've thought FFTW was untouchable in terms of speed. Could you give details of the configurations used during the benchmarking? I'd love to see some graphs. By the way, this is amazing work you've done.
Despite the design intent, are there any major downsides to using this for basically all local development? I'm currently using it locally primarily to get comfortable with it before 1.18 with the aim of deploying it out in our (algorithmia) build service to improve build time of rust functions/APIs. But even in just a short time of using it locally, I find myself thinking I'll just stick with sccache locally since I find a lot of my projects end up rebuilding the same dependencies. It seems particularly valuable when jumping between projects after updating nightly rust.
You can use byte slices ([]byte, sort of Vec&lt;u8&gt; or &amp;[u8]) for a lot of use cases. In the past I missed the generics so much from Go, but today I learned to live without it.
We are working on it, it just didn't make the cut for the RailsConf release because we prioritized things like documentation and website :) I'll reply to this thread when we land the feature! (In case you didn't know about it – you can track those at https://usehelix.com/roadmap. I think I should probably make a Github issue per use case so people can subscribe to them.)
Sometimes it's better to just go fix a crash (or other bugs) after it happens than spend tons rather than spend a a lot of time proving that a crash won't happen. 
You should try it. You might be surprised how refreshing it is to just write the code. To just accept that your solution won't be beautiful and just start typing. I'm serious. It's nice.
I wasn't aware of the roadmap on Helix's website. When those features land, I'll definitely take a second look at how we wire our Rust up to our Ruby. Thanks for the info.
I'm pretty sure it would just be another algorithm in your crate, much like how you have specialized versions for different sizes this would be specialized for sparse signals.
Me too!
&gt; Despite the design intent, are there any major downsides to using this for basically all local development? Is there a performance penalty when the cache is empty?
The left-hand side of an assignment must either be a variable, or a let statement. Your left hand sides are tuple-values. Like, the tuple `(r0, rest)` as a value. You can use let-destructuring to destructure the tuples from the right-hand side into newly declared variables on the left, but that's a special syntax. You can't just generally assign things to a tuple. so, `let (mut r0, mut rest) = rest.split_at_mut(...)` would work, and you will not need the 10 lines of "let mut blah;" at the top either, which is a bit ugly.
it would, yes, but it would be [shadowing the previous declarations](http://rustbyexample.com/variable_bindings/scope.html). So, they would still exist, but they would no longer be accessible, which would have the same effect as the code you have up above, I think. The compiler might give you a lifetime error, so that's my big concern. If the compiler doesn't complain about lifetimes, then it shouldn't matter that you're shadowing variables. There are some alternatives we could look into if the lifetimes are a problem.
Thanks, that fixes it. The compiler does not give a lifetime error
Rust novice here, is there really not a better way to do things like [these](https://github.com/demilich1/chip-8/blob/master/src/screen.rs#L70)? Would macros be appropriate for this, or overkill? Would a lazy_static HashMap be better practice here? Would it result in better performance?
[`Vec::chunks()`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.chunks)?
The `itertools` crate provides this method.
Good find, thank you.
Ehh, yeah well. From what I read on Go Go is _a lot of typing_ which I deplore most about programming. Go seems to be a lot of work to do really basic things. I've now seen examples on the internet of someone complaining how in Go you needed to write 3 60 line functions to do what in most languages is a simple fold/reduce with a closure of two lines which is generic.
i like how he refers to australian text ʇxǝʇ uɐᴉןɐɹʇsnɐ oʇ sɹǝɟǝɹ ǝɥ ʍoɥ ǝʞᴉן ᴉ
&gt; What would a push method look like? First do a binary search for the correct `VTable`. If it doesn't exist, insert a `(VTable, Vec::new())` at the right position. Then copy the bytes of the value into the `Vec&lt;u8&gt;`. &gt; Would there be anyway to give the collection ownership over new values other than to pass it a `Box&lt;Trait&gt;`? Passing a `Box&lt;Trait&gt;` works, but you can copy stack values in using a template function. The template function can get the `VTable` by writing `&amp;value as &amp;Trait`, and can then copy the same way as usual. &gt; Also, how would you get the size of the objects from the VTable, so you could e.g. do pointer math into the Vec&lt;u8&gt;? The size and alignment are stored in the `VTable`.
How does its performance compare to FFTS (https://github.com/anthonix/ffts Fastest Fourier Transform in the South)? Apparently FFTS is faster than FFTW... I wrote a binding to FFTS and have been using that until now with no problems.
&gt; Couldn't you also generate a single struct containing all resources then, and pass it as an &amp;mut to every task? That's an interesting idea. There are some other constraints though. `access{,_mut}` must operate on `&amp;'static self`. Well the `'static` is not required for `access` to be sound but currently `access` returns a newtype, `static_ref::Ref&lt;'a&gt;`, over a reference (`&amp;'a`). That newtype asserts that the reference is pointing into static data (data that has a `'static` lifetime) and we require that invariant to ensure the memory safety of other API (that dealts with operations on the data that last longer than the span of a task): Basically we don't want `Resource`s created on the stack of a task to be used with that API. More to the point, that would require `resources` to have type `&amp;'static mut MyResources`. I heard someone else mention that `&amp;'static mut` causes UB per LLVM aliasing semantics. I'm not sure if that's true but the comment makes me wary of using something like that. &gt; So user code would look something like I'd love to make the API clean like that but there are some constraints that make it impossible to achieve something like that. Main problem is most things need to be stored in `static` variables. So your `Task::init` won't actually work because it would have to be a `const fn` and the compiler won't let us do that. Second, and most embarrasing, `Task::exec` must be called from a function with signature `fn()` that must be located in a precise location in memory and right now we are placing that function in the right place using ... ["duck typing"](https://github.com/japaric/cortex-m-rtfm/blob/c1465b211704a0c993ceb9df511ebf6d085d683c/src/lib.rs#L928). I can't think of a way to map `&lt;Self as Task&gt;::Interrupt` into a "store this function pointer in the field named `&lt;Self as Task&gt;::Interrupt` of this struct" or its equivalent "store this function pointer at index `&lt;Self as Task&gt;::Interrupt::nr()` of this array" (where Interrupt::nr() returns an usize). I appreciate your ideas but it seems hard to put them in practice. I admit we have pretty weird constraints that you don't normally see in normal Rust programs but we have them because we care a lot about performance and want to make everything statically allocated. &gt; Was there a reason for taking a reference to the resource instead of just specifying the threshold explicitly, besides convenience? Not really. Specifying the threshold like that would work as well. I'm just not too fond of the turbofish syntax; also `raise` has more type parameters so you actually have to type `threshold.raise::&lt;T2, _, _&gt;(..)`.
everyone of course has their own style, but it's more common (and i think more readable) to put the dot on the beginning of the next line. for example: let counts = bytes .group_by(|b| b) // returns something like { key: T, vals: Iter&lt;T&gt; } .map(|grp| (grp.key, grp.vals.count())) .collect(); 
The method you're probably looking for would be to mask those specific bits (which is what I am doing in my emulator project) and then match them to something like 0x5000. As for binding the specific bits, afaik that's not possible. I would just zero the bits that are the same every time to extract them and then right shift. Macros may prove useful here if you could do something like an `extract!` that would take the opcode and bit range as arguments and return a correctly shifted result. Just my two cents, good luck!
Great article. Why would you write this though? &gt; All the engineers had offices with doors that shut. And nice offices. With windows, hardwood desks and cabinets instead of grey plastic and fiber. That makes me so sad in my openspace...
I'm sure that not what you meant but for the possible reader : the array here are sorted. We binary search in them. The question is do we have prior information on where the target we are searching for is in the array. Somebody in the comments wrote the rust code that compiles to something similar to this.
Looking into this more, it seems using a specific nightly isn't supported natively by travis. I'd have to manually download it and run it all. https://github.com/travis-ci/travis-ci/issues/7167
Thank you for the pointer to `error-chain`. That looks like just the thing. 
I am planing to release a PR of `hex` as a drop-in replacement for `rustc-serialize`: https://github.com/KokaKiwi/rust-hex/pull/9 . I was too lazy to modify my code, so I just wanted a crate with same syntax.
&gt; Would an associated const work (or is this one of those cases the implementation can't handle yet)? It might since, today, initialization of Resources in RTFM is already constrained to struct initialization. I think associated consts have the same constraint. So if it compiles I don't see any downside. &gt; Otherwise I guess something like an unsafe trait saying it's OK to initialize the type with all zeroes? You can't use `mem::zeroed` in static / const context though. TockOS works around that problem using [a macro](https://github.com/helena-project/tock/blob/b16b8e847c297715f193aa73fa12640c63c86fda/kernel/src/common/utils.rs) that declares a static [u8; N] that has the same size as the desired type T and then transmuting a reference to that static to &amp;mut T. The downside is that the macro requires you to declare upfront the size of the type in bytes as `mem::size_of` doesn't work in const context.
That actually seems pretty cool, but I'm really surprised it is part of the standard library. The syntax looks a lot like rust too.
Event loops always have such overhead from all the work the operating system has to do to check all the IO, I shouldn't think it is worth wasting time making a more complicated system than that. If you want to talk micro-optimization, I'd say the fact that you've separated out the CPU execution into decode and execute stages is more significant, because it means you have to execute **2** unpredictable match statements instead of just 1.
This is great! thanks for building this
Thanks. It worked, although I had to make some adjustments to fit how I'm actually using this stuff. Assigning explicit lifetimes is a great idea. I'll make sure to try that in the future.
It's really impossible to say definitively which will perform better. It's really about tradeoffs. In general, I think it goes like this, from fastest to slowest: * Traits + generic code (monomorphized at compile time) These can ostensibly perform the best because it allows the optimizer to see through to the trait implementation and optimize across that boundary. The codepath is essentially static to LLVM. However, a large number of implementors for a trait can cause bloating binary sizes as each trait implementation produces its own unique codepath during monomorphization. This has really only become a problem with ubiquitous traits like `Debug` that are expected to be implemented by almost every public type. * Enums These involve dynamic dispatch but the potential range of values is quite limited and the optimizer can still introspect cases where it could only be one value. There is little inherent code bloat as all enum values use the same resulting code. * Trait objects (vtable) Trait objects are almost always a black box to the optimizer because they involve calling through a vtable/function pointer. There is still the bloat of the implementations themselves but like enums, there are no extra generated codepaths in usage. In general, though, you use enums where the set of values is limited and isn't expected to grow, or grow only when you add them. You use traits when there could be a variable number of different values, possibly even external implementations by users of your code.
Thanks for the feedback. I've corrected the malloc/calloc issue and pushed a fix to the repo. EDIT: Nevermind, also fixed the second issue.
I suppose this is for 1-dimension FFTs only? (Still, it looks good!)
The order in which things are dereferenced is incorrect. The caller is responsible for the memory location that `resp` points to. It's expecting you to put a pointer to a newly allocated response array of responses, which you do correctly with your `calloc` call. The problem is when you attempt to index the plain `resp` variable without dereferencing it first. That will give you an offset from wherever the caller allocated the slot for you to put your own pointer (and who knows what's actually there, probably dragons or something). By dereferencing `resp` first, you get the actual array that you allocated yourself, which you can then index.
I'm not sure if it's completely production ready, but it definitely is an awesome Rust reverse proxy server. I... don't know if it is available as a library crate, as IIRC it's primarily a server application, but I think some of the internals might be published as crates separately. One of the developers also did a talk (about sozu) at the last rustconf which was pretty cool: https://youtu.be/y4NdVW9sHtU
Thanks for the explanation. I actually just figured it out but your explanation makes it a lot clearer.
 fn ack(n: u64, m: u64) -&gt; u64 { if n == 0 { m + 1 } else if m == 0 { ack(n - 1, 1) } else { ack(n - 1, ack(n, m - 1)) } } try calling it with parameters like (4, 1) or (3, n). Beware of overflow :)
Glad someone picked up on that :)
That will totally happen after, but sounds like maybe a leak in the meantime.
I didn't like blowing the stack, but for some reason your function inspired me! fn ack(n: u64, m: u64) -&gt; u64 { let mut i = n; let mut j = m; let mut loops = 0u64; while i != j { j = (j + 13) % 1000000000000; i = (i + 2) % 1000000000000; loops = loops.overflowing_add(1).0; } loops } I didn't think too much about what numbers to increment i and j by.
First thing I thought of. Trivia: the time complexity is O(ack(n,m)).
It might be better to use add_overflowing so that if it's defined to overflow instead of doing the large modulus. On mobile right now but you should be able to search for it in the stdlib docs
Oh, that's a good idea. I want this to be as efficient as possible. Should be fixed now.
I kinda find it hilarious that in this Rust-C hybrid the piddly little c file is where all the bugs are. Not a comment on the authors capabilities -- C is hard and we've all been there -- just an observation of how hard it can be to get even 50 lines of C to be bug free.
"we want to support external entities" I can see why when I think about it, but I did a double take when I first saw that. XXE is a major source of security bugs. The library should be careful with it's defaults so that nobody accidentally causes an XXE by misconfiguring the parser. Servo's tendril library abstracts over encodings. I'm not sure if it fits the needs here. We use it in h5e which does similar stuff so probably.
So you know, the original commenter's use of the name `ack` is because it's the [Ackermann function](https://en.wikipedia.org/wiki/Ackermann_function).
&gt; I was planning to adapt it to emit Rust code. The parametric DFA for Levenshtein 1 and 2 have respectively 5 and 30 states. It is reasonable to write them by hand and hard code them.
+1... And your Levenshtein automaton interface is nice enough that it allows for using on a different datastructure too.
I think you hit the "send" button a too few many times, you have many duplicate comments.
Reading the [manpage for `pam_conv`](https://linux.die.net/man/3/pam_conv) shows that the caller is responsible for calling `free`, so you are fine here. However, when you return an error, you are responsible for freeing everything you've allocated before returning -- the strings and the array of structs. Right now you've got a memory leak in this case.
Nice, but one question. As you have already changed configuration format, then why have you, from all possibilities, choosen JSON, which is terrible for human to write?
 writeln!(&amp;mut io::stderr(), "You are not in the rudo.json file! This incident won't be reported.") .expect("Failed to write to stderr!"); process::exit(1); I _knew_ it was an empty threat all along!
I don't understand. Why was "impl Trait in argument position" merged even though it says it's in FCP? I find it somehow confusing that `impl Trait` in argument position has different semantics...
I chose json because it's really easy to express complex data structures 1:1, but as noted in the README I am looking for something a little less terrible. I'm open to any suggestions you might have. 
"whoops!" That brought back memories of when I saw the first DHH rails video.
How would you use UTF-16 in Rust? Write a custom string class that wraps Vec&lt;u16&gt;?
Just out of curiosity: does anyone know what's so hard about parsing XML? Is it the validation?
I guess doing it efficiently, with regards to speed and memory use... Parsing XML is indeed a simple task, but it probably does take effort and careful optimization to do it efficiently.
Brain fart, thought it was a sort instead of a search. Sorry. 
When to use array over vector? When to write plain function over method? 
A related (perhaps equivalent) question: Does Rust have concept of _text_ (or maybe it should be called _Unicode string_ or _character string_), _i.e._ a string that doesn't tie itself to any particular encoding?
It's a 'primitive' type, and they are lowercase (e.g. i32, u8, etc)
Non pedantic correction: TOML is not a superset of JSON.
Yes, and it's called `String`. I mean, the UTF-8-ness is close to the implementation detail (which does matter when you consider many Unicode strings in the wild are actually UTF-8). The only portion that is actively exposed is that the indices are for UTF-8, which can be easily abstracted over.
Pretty much baked in ;) Certainly you cannot create new ones.
You don't have a performance fee for creating your own types in Rust, either. You can e.g. create a complex number type, and make it behave pretty much like a primitive.
&gt; Yes, this is slower than just searching utf8 or building up search infrastructure to search utf16 directly. This was a stopgap to fulfill the requirements of Visual Studio Code, right?
No problem :) Brain farta happens to everybody from time to time. 
Please note that the java solution of "writing to stdin", even if would be possible in Rust, would be really error-prone, as tests are run in parallel. So I think that if you really want to simulate the actual input, then spawning a process, as u/MSleepyPanda suggests, is the only way. There are two ways to test the function while not really using stdin: 1. Make the function generic over `io::Read` or `io::BufRead`: fn read_line_from&lt;R: Read&gt;(reader: R) -&gt; Result&lt;String, Box&lt;Error&gt;&gt; Then you can pass `io::Cursor` as a mocked input. You can have your original `read_line` to be a wrapper that just calls `read_line_from(io::stdin())`. 2. `#[cfg(test)]`-away usage of stdin – **edit** [that got uglier than I expected (playground)](http://play.integer32.com/?gist=79d3ca2b343f413f6e4abf4fd991507d&amp;version=undefined).
This is ridiculously cool! I wonder how soon they are planning to integrate this to rustc.
Then what's the point of having primitives special syntax? 
&gt; I mean, the UTF-8-ness is close to the implementation detail It really isn't though, or you couldn't freely convert between `[u8]` and `str`.
&gt; I guess doing it efficiently, with regards to speed and memory use... Nah, that's an issue, but long before that XML stacks have been huge sources of vulnerabilities in the past.
It wouldn't test the the actual user input part, but you could split this into two functions: one that gets user input, and another that validates it. The latter wouldn't need anything special to test, and the former would be so trivial that it might not be worth testing.
Yeah, on the other hand IEnumerable.GroupBy can't be lazy, it has to traverse the entire sequence before it can yield the first group, and you can easily compose the other groupby with a sort to get IEnumerable.GroupBy, the reverse not being true.
Typo: "no matter what `Clone` is" -&gt; "no matter what `T` is"
Methods (usually) get called as `x.y(..)` instead of `y(x, ..)` – so it depends what reads better to you. With methods, you can simply move them to an extension trait, whereas you have to change all call sites if you try to refactor function → trait method.
Mind you, this is nightly only.
Try http://projecteuler.net ;)
This was the first thing I thought of when I saw the post just now.
Can you have a type like "number greater than 5", or even more complicated, and have it checked at compile time?
&gt; They were a huge pain to use and probably gave XML part of its reputation for being annoying For me, XML is annoying not because of parsers, but because of how data is represented, which means that it cannot be mapped nicely to a data structure. Since it cannot be directly mapped to a data structure, there are all sorts of ways that someone may want to interact with it, including: - SAX - DOM - xquery Supporting various ways of interacting with XML, as well as schema validators (which may not be valid XML, like DTDs) as well as other stuff like XSD, WSDL, etc that are related and users may expect some amount of support. So it's basically a scoping issue. The larger your scope, the more useful your library is, but the worse it is to implement and maintain.
I have no idea what you want to accomplish with this monster but here you go: struct Foo&lt;'a, A0 : 'a&gt; { data: *mut &amp;'a mut FnMut(A0), } Note the addition of `: 'a` to A0 which basically says "AO needs to have a lifetime which is a sublifetime of 'a where strangely a sublifetime means living as long or longer.
&gt; That would get into the territory of [dependent types](https://en.wikipedia.org/wiki/Dependent_type), which Rust doesn't support. I see. I was hoping that it would be possible without dependent types. Are dependent types something whose absence is fundamentally built into the language, or would it be possible to have them in a future version? Would it be considered desirable?
The problem is `A0` could live shorter. Consider this: (*foo.data)(&amp;a); drop(a); This code wouldn't compile because `a` lives shorter than `foo` but it should because `foo` isn't supposed to store the reference to `a`.
Nice project. Though I probably won't be trying it on my own system any time soon ;P. I noticed you used C to work with PAM. I'm actually about to look into using PAM for the lock-screen for Way Cooler. Are there no crates that can do that? I saw [this](https://github.com/1wilkens/pam-auth) and it looked sufficient for basic authentication. EDIT: ANNNDDD never mind, just tried that crate out and it has a very, very bad bug with authentication (and the code looks crazy unsafe). Frankly what you're doing is much better. There is `pam-sys` though, which looks fine to use if you're ok with using lots of `unsafe` code (it's a tossup if you're better of just using C here though honestly).
I'm not knowledgeable on the subject, so take my reply with a grain of salt, and I'd appreciate if anyone who is versed in this can correct me if I'm mistaken. Basically, the inclusion of dependent typing means that your compiler becomes, additionally, a theorem prover. This introduces a lot of complexity, but more importantly, the theorem prover is not omnipotent. For complex constraints, you'd need to provide it with hints as to how to go about proving that a given sequence of statements satisfies a particular constraint. And there are all kinds of interesting issues you run into. For example, a sort function would take an arbitrarily-sized list, and return a sorted list of the same size (where 'sorted', 'arbitrarily-sized' and 'of same size' would ideally be encoded in the type signature). So, inside the function, your 'unsorted' list will transition into a 'sorted' one at some point, but when? And would the compiler be able to infer that automatically, or would it need the help of additional annotation of invariants throughout the function? It might be quite a challenge to write this function so that the compiler accepts it. I think that, theoretically, you can augment the Rust language to have dependent types, but their usefulness would likely be very limited. In practice, adding such a big, cross-cutting feature would require *a lot* of work, and I'm not sure how it would blend in with current rustc internals. Besides, you'd need to annotate the standard library with proper constraints, which is also a huge effort by itself. I could envision it happening if there is an enormous push from both Rust contributors and developers using the language, but hardly in any other case. If you're interested in the subject, take a look at [this paper](http://www.cse.chalmers.se/~peterd/papers/DependentTypesAtWork.pdf) for an introduction to dependent types. For a lighter example on how formal verification of algorithms in general looks like in practice, I recommend reading about [the TimSort bug](http://www.envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/#sec2.1) which was found using KeY, a formal verification system of Java.
Have you tried /u/its_boom_Oclock's code? It solves that problem.
if you want something that does a good job of proving at compile time that a dependent type is satisified you can be dealing with *extremely* long compile times. Some experimental languages like Idris and F* do this. Another approach is the the compiler checks for obvious case, and runtime checks are then inserted. Ada and Nim do this, which is neat but that sort of hidden runtime cost might be antithetical to Rust philsophy? 
It should be 1. Faster 2. Less buggy 3. Easier to support new features. For an example of 3, Associated Type Constructors already work in Chalk.
That's pretty cool!
No, [it doesn't](https://play.rust-lang.org/?gist=ea8f491757ac9dc5c35b8a73be9193a8&amp;version=stable&amp;backtrace=0). :(
It maps just fine to data structures. Shit it maps easier than json in terms of types. Serde supports xml. The issue is more of it being a more complicated spec with lots of even basic validation rules.
We can and will make it work, that's how the skylight gem works and it's on the roadmap 🙂 https://usehelix.com/roadmap
There are far better ways, never do this ever ever ever 
Erlang has [Bit Syntax](http://erlang.org/doc/programming_examples/bit_syntax.html) that allows what you're talking about. I don't think anything like this is implemented in Rust right now, but I wonder if it's something that the community would be willing to get behind in a future version. I know that I would like something like this.
&gt; if you want something that does a good job of proving at compile time that a dependent type is satisified you can be dealing with *extremely* long compile times. I was thinking that the programmer might provide proofs for the compiler to just check. That should be faster, right?
Why?
Yep, that's exactly why. Most of the PAM crates I saw were either unmaintained, incomplete, or crazy unsafe. `pam-sys` was the exception but it linked against another PAM helper library that wasn't available on macOS so I just opted to write a small C wrapper instead.
How much of DTD is actually used in practice? I heard XUL uses DTD entities as a way to localize things. But that doesn't require more than simple references (i.e. `&amp;ref` never expands to `&amp;ref2;&amp;ref2;`), right? 
I've seen nested entities before in Firefox's source. Often you'll see an entity like `&lt;!ENTITY somelocalization.string "Updating &amp;brandshortname&gt;"`, so that the localization string can depend on another string. Billion laughs isn't even the attack I'm worried about, it's XXE, which is much worse.
Right, which is why I'm not worried about it. Besides, all the stuff in Firefox that uses this is privileged code anyway. You just want to turn it off for external XML files (language packs, manifests, RSS, etc)
Oh, would it at least be possible to have something like "non-negative float"? Would it be reasonable to define a struct for that, have it contain, say, an ordinary float as an internal implementation, and set up its construction and other operations such that it always is non-negative? Those operations would not be proven by the compiler to be non-negative, but a user of such a struct would at least know that it is always non-negative, assuming it has been written correctly.
FYI, this isn't my code. That's a really neat trick though. Is it also possible for enums you don't control? There isn't a way I know that allows you to map keycodes from `sdl2::keyboard::Keycode` as `u8`'s.
If your tests are operating on a single "test user" or something like that, could you instead have each test create its own test user? That might be slower, if creating a test user is slow, but you might get some of that back with the extra parallelism?
Jai has added a new feature; it seems pretty interesting. I wonder if this could be used to implement self-referencing structs...
Maybe I'm misremembering but I thought he was going to OS it at some point once he's happy with it. Or is he only going to release binaries and I'm completely off my rocker here?
&gt; he has chosen only to share video demos of Jai This is pretty weird. Any information you want about Jai, it's all videos. Any websites with text information are done by people who have an interest in Jai but aren't associated with its development.
I do not.
This answer does not match a hundred percent with your question, but depending on your use-case it may still help you. The common pattern in Rust for errors is to define an enum: enum MyError { XmlError(::xml::reader::Error), CustomError(String), } And then you implement `From` for it so that you can easily use it with `?`. For more complex errors, you might want to look into `error-chain`.
I had the impression that it's somewhere near top on the priority list. My impression is incorrect, right?
In the demo, he copies things via memcpy, and it still works. I am feeling a bit... fuzzy on this stuff today, but I don't see why it couldn't work. I think what you're saying is that this would be stored as a pointer, and the pointer would need to be updated on move. I think this is stored as the offset only, so when it's copied, nothing needs to be changed; it's still just an offset from its current place, the current place has changed. Think of it like this: struct Foo { x: i32, p: &amp;relative(i8) i32, } The value of `p` referring to `x` would be `-4`. This is always going to be pointing at `x`, even if it moves. At least, in my mind it would. I haven't thought *mega* hard about this...
Yeah, so long as the copy is ensuring keeping the packing/etc the same the relative pointer would point at the exact same relative address in the new block of memory (which appears to be the case in Jai)
Yeah; there's certainly a whole list of problems with doing some of this safely. Jai doesn't care about this, so it's not something that he has to deal with.
I got through a good portion of the survey then I realized it was not talking about the game Rust...
&gt; I was also looking at the second version (which is in beta) of the rust book and I found the material not so approachable. What struggles did you have? http://intorust.com/ has some stuff.
Shameless self plug - https://github.com/craftytrickster/mock_me . I'm not sure if it is considered good, but it worked for me the few times I used it. 
This is for consistency. The last expression in any block of code (anything surrounded by curly braces) is that block's return value. Functions are not special. Other blocks, like `match`, `if`, etc. also have return values determined by the last expression in the block. For example, you can do things like: let myvar = if some_condition() { let temp = something(); something2(&amp;temp); a + b + temp } else { something_else(); a - b } + 16; or with a `match` expression as well. This is kinda like the ternary (`c ? a : b`) operator in C/C++, but on steroids, because you can have arbitrarily-complex blocks of code. The last expression is the value of the block. If you use a semicolon at the end of an expression in Rust, then it turns into a statement. To understand it better, here is an example: if condition() { something(); a + b; } is equivalent to: if condition() { something(); a + b; () } Notice how, since, `a + b` is now a statement (being terminated by a semicolon), the last *expression* in the `if` block is actually an empty expression (aka `()`), so that means that the block returns nothing. Functions are consistent with that behavior. `return` is somewhat semantically different, in that it allows you to return from the current function from within an inner block. There is really no need for a return statement if the value is the result of the last expression in the function body anyway, because the value of the expression is the return value of the block, by the nature of how blocks in Rust work in general. If you ask me, the fact that Rust works like this (has that syntax that you consider ugly) is actually quite powerful and a good thing. It might be more familiar to people coming from a functional programming background. I don't consider it to be an unintuitive thing that steepens the learning curve for beginners. If anything, it is consistent with how the rest of the language works, so it would be unintuitive if an explicit `return` was required at the end of functions. I hope that my long post was educational. :) EDIT: minor correctness fix
That's really pedantic in a way that's unusual even for this subreddit. The software isn't yet at a point where there is anything to distribute to anyone. Free software isn't about getting other people to develop your software, it's a license that allows all users of your software to modify, redistribute and study the source code for any purpose (including commercial purposes). There are no users for Jai right now because it's not at a point where anyone can functionally use it. But that doesn't mean the software is proprietary or closed source. Jai is free software and once the product has been developed to some stage that makes using it or contributing to it viable, then it will be released under some free software license.
Mmmm. You might be right, it's listed as unavailable in a bunch of suppliers. I can only find it on eBay or in some obscure german shop. 
I really miss the functionality too :(
You'd probably have to provide your own test harness, which at the moment is super cumbersome and involves e.g. listing all the test functions explicitly in one place (via `test_main` from the `rustc_test` crate, for example). I think a better option is to add some automation scripts/tasks to the project and make that the canonical way of invoking Cargo. I don't know of any Rust-based task runners but personally I'm pretty happy with Python's [Invoke](http://www.pyinvoke.org/).
Yup! &gt; surely creating a closure struct and a constant allocation of an Option with every iteration and a constant call of next For example, this closure doesn't refer to anything in its environment, so the struct is empty, and never needs to be allocated. The Option is going to to be over a reference, which means that the tag can be elided, as the null pointer corresponds to the `None`. The call to `next()` can be inlined. etc etc etc.
&gt; Rust is full of helpful abstractions that make the code you write clean, easy to read, and less prone to mistakes, while doing a good job of not hurting performance. My advice is to use those abstractions. Yes, I subscribe to the idea of that as long as you can't quantify performance it is useless to persue it. I measure the difference when I try to make something more performant. But in this case I can't see how it is zero cost. This abstraction definitely has a cost in theory that the optimzier just finds a way to for to eliminate. If you purely reason about the code in a completely unoptimized environment surely the iterator does more every loop but rustc finds a way to optimize a lot of that away I guess.
&gt; I didn't like blowing the stack Hmm, I wonder if the Ackermann Function can be implemented without recursion (iteratively)? Or at least without storing values on the stack (use a growable vector on the heap as a stack to store values).
So it's a boxed box.
I am used to very simplistic introductions (both Python and c++) and the whole business about cargo and the .toml file kind of freaked me a bit. I will try it again and see if I can do better. But I think someone qualified should make video tutorials. They will be useful for the community. Thanks for suggesting introrust.com. 
The current state of this feature in Rust (constant values as parameters to generic types) is this RFC: https://github.com/rust-lang/rfcs/pull/2000 
**edit**: I'm talking about the performance of the debug (unptimized) builds here (the release ones are fully inlined and have the same (good) performance) &gt; The while loop version is generally going to have a bounds check on each time that you index the array to make sure you're not overflowing the vec bounds. Not only the `[]` operator performs a bound-checks, but also the `index` creates the 3 layers of function calls! First, the index on `Vec` is called, which calls `index` on `&amp;[T]` (that's one of the reasons you shouldn't use `&amp;Vec&lt;T&gt;`). Then, inside the `&amp;[T]::index`, the code is forwarded to `SliceIndex::index`, which perform actual indexing with bounds checking. I suspect that these 3 layers of function calls are causing a slowdown, as the actual bounds-checking should be well-predicted by CPU, and thus cheap. So in the `[]` version we have 3 fn calls per iteration. How about the iterator one? There's just a call to `slice::Iter::next` and the call to the closure. Just 2 of them! @u/its_boom_Oclock &gt; I felt that surely creating a closure struct... ~~The closure doesn't capture any environment, so its size is 0 :)~~ Oh, it captures the `&amp;needle`. But still, creation of the closure's struct should cost exactly the same as creating a `&amp;needle` variable (I suspect that a `move` closure (`move |x| x == needle`) would be slightly faster on a debug build). Anyway, the closure is created just once, not on every iteration! So the cost of creating it doesn't really matter. &gt; ...and a constant allocation of an `Option`. The `Option&lt;&amp;T&gt;` is returned as just one register (without optimizations its value is stored on the stack, as a variable, and then copied to a register, but it's the same as for any other value). The `Option&lt;i32&gt;` also fits in one register. Even `Option&lt;i64&gt;` is nothing more complicated than two registers. So, in optimized builds, there'll be literaly 0 allocation, and in unoptimized, just a few bytes on the stack (if you even count stack as allocations). Oh, and a one thing might be slowing you down in `[]`-version – the overflow check of the `i + 1` operation, not present in the iterator version :) Anyway, I was quite surprised to see that the iterator version outperforms the naïve one! Usually, when you talk about 0-cost abstractions in Rust, you mean the release builds (the memory usage shouldn't differ much between debug and release though). 
Rust was also closed source in a personal repo for a while. Though Graydon open sourced that history relatively recently. (In that case though it was I think a personal project with no public face, unlike Jai, which Blow talks about a lot)
If you are willing to _always_ take references, then this seems to work: https://is.gd/wpftD2. I can't see any reason why this should work, but a version that packs the reference into the A0 type wouldn't, so there's definitely something still missing here.
yes, but lets be consistent here struct ILove { language: Haskel , experiment: Elm , glorious_future: Rust }
/r/playrust
I believe because `FnMut(A0)` is just a sugar for `FnMut&lt;(A0,)&gt;`, which assumes that `A0` is stored inside that struct. I find such design unfortunate.
I wanted to make it fully generic even though I needed just `*mut A0` for now (raw pointer is due to FFI). Seems like I will not make it as generic as I wanted. That's fine, I just feel a bit uncomfortable due to my perfectionism...
HIR is generated before typecheck, and type checking operates on HIR, but LateContext lint passes have access to everything except translation info (HIR, types, etc) "HIR pass" is the term we use because the fundamental tree structure being visited is the HIR.
Welcome to systems programming? Seriously though, at least the Rust compiler is strict enough that most of the time, if you have idiomatic code, it will be optimised 
Oh fantastic, thank you!
Holy shit! You are the author of rust book. I didn't notice the username before. 
There's no exact point where a project is ready to go up on the internet, it's arbitrary. When you are writing an OSS library, do you make the Github repo before your first line of code? Or do you wait until there is at least something to show? Jon Blow just has a later arbitrary point than yours.
It's one of the design goals of rust to provide as powerful as possible abstractions while ensuring performance expected of a systems language. You will find it is very difficult to make benchmarking a useful exercise since performance characteristics in production will usually differ tremendously. This is due to competition over resources (cache, threads, etc.) and because benchmarks will often be optimized differently from the full source.
Big thanks to everyone who was kind enough to answer questions. This community is the greatest.
Yup! That's why I wanted to know how to make it better :)
He has indicated that he will open source it. It is alright that he has not opened it up yet, as he is doing it at his own pace. Your comments sound like you are not OK that he has not opened it up yet, even though he said he wanted to eventually. I was just saying that its fine, and he can take as long as he wants.
Well python has made a point of having a large, well designed standard lib as part of it's design philosophy. Rust is mainly focused on other things as of this stage in development. Also rust ships with Cargo mostly, which alleviates the need to put many things in `std`, because having them be separate crates is still easy to use. I think personally that having a bare-bones std lib is better for a language that aims to target low level and embedded systems. If you need something special, you get it from Cargo, but if you don't need it, you don't have it at all. Also, with rust being a young language, the focus for rust maintainers is more on usability, compile times, IDE's, ect. rather than on a comprehensive std lib.
Thanks, that was a really concise answer. I didn't know Rust was so young. It's probably not the language I'd want to use, personally, though.
Give it time! I didn't think I would like rust either, but now my new personal projects are in rust rather than my old standard C#. Anyway you might want to check back in a couple months and see what's happened at least.
In my experience the external JSON protocol did use the character offset assuming UTF-16 (yes, this is often the case). This makes the dependency to `widestring` very much necessary.
The type you create needs to be generic over a trait, which I don't believe Rust can support until it gets HKTs. And to realize the benefits of putting the values directly into a `Vec` you need ownership over the values, put there is no way to take a trait object by value (except as a `Box&lt;Trait&gt;`, but that would not be as efficient as the C++ version). You can get around these restrictions using the macro system, and e.g. generating a different `push` method for every concrete type.
Yeah I can agree. I would love to see the language progress and I'll give it a try again.
I can see that reasoning. But, there's something satisfying about having all the packages you need locally. (Thinking back to C/C++)
How do I trigger a minimal recompile of `rustc` after hacking on LLVM? I know there's a `build/**/llvm_is_built.stamp` or similar file, and removing that ensures that running `make` rebuilds LLVM, but it doesn't seem to rebuild and re-link `rustc`'s LLVM-using parts. Currently I get around this by removing all stage-1 and stage-2 build artifacts, effectively building `rustc` from scratch modulo the stage-0 bootstrapping phase; but surely there must be a more efficient way that recompiles only the relevant parts of `rustc`.
Ah yeah; I haven't done this in Rust yet; I just know that a lot of people in Rails world do it.
&gt; On the iterator, it's easy for the compiler to elide (optimize out) this check, and it's easy for it to perform loop unrolling. Actually, the check just isn't even in the source code, because we know the iterator is safe. The compiler doesn't have to do anything to get rid of it.
https://github.com/servo/rust-bindgen
(Minor point: now that the `features` key isn't needed, the version specification can be abbreviated to just `rustoto_core = "0.25"`.)
It's mainly because Rust has the philosophy of focusing a lot on anything before stabilizing it, and only committing to APIs that won't be a detriment down the road. When 1.0 shipped, the standard library was _tiny_. But that was alright, since the point of 1.0 was to declare that everything in the standard library is stable. As Rust has continued moving forward since 1.0, things definitely have been added to stdlib. But, every single addition has a community involved RFC, and a thorough discussion of if this feature is designed well. Cargo complements this strategy well: it's incredibly easy to depend on a library, and thus libraries implementing common features can get _tons of feedback_. Once a library is very confident in its design, it can even be added to the standard library. The fact is, Rust hasn't existed for long enough to amass as large of a standard library as Python - it's just around 2 years stable. The core teams have been working hard on adding and improving things, but it's just been 2 years, and wonk on the compiler itself is a large part of what's happening still. I'm sure in 4 or 5 years, after many of the more common functionalities have been fully stabilized as external crates, some of them will be added to std. But having them as external crates now lets them experiment with different APIs, and version differently than Rust itself, without the RFC process and without having to commit to stabilization. Anyways, that's what my observation has been, seeing what han happened and what people have talked about since 1.0. I believe the standard library will grow, but only when the features to be added have already been fully developed and discussed.
Yes, it's frustrating but it's kind of just how it is in gamedev afaict. When I wanted to learn Unity, I was hoping to get some informative blog posts or something. Nope- it was mostly "watch these hours of videos". 
Yeah I finished it and moved. :)
Interesting! Don't know if I've ever seen this particular series before. Do you know if/how it strives to be different form this-week-in-rust?
I don't see why you'd need to write a number generator. Just search Crates.io for what you want and it's probably already there. Rand for generation random data, for example.
Siummer, hopefully.
Is there a way to pass a closure that panics to a function? I have a closure of types `|msg: &amp;str| -&gt; !` that captures some variables from the environment and then panics with some error message + the `msg` given. Trying to use a type like `Fn(&amp;str) -&gt; !` doesn't work and mentions the `never_type` feature (https://github.com/rust-lang/rust/issues/35121). Any way around that? This is a proc-macro so I'd like to keep things simple and not bother with passing around Results when I just want to panic.
Yes I understand but my point was that just in order to create a random number I have to create a dependency. It's just silly.
There will eventually be a 'community prelude' that will help with this.
Part of the reason for this particular one is that there isn't yet a good "idiomatic" random number generator API in rust. If I understand correctly, `rand` was split out of `std` before 1.0 because of this: Ruts didn't want to stabilize an unideal API that then wouldn't be able to change in the future. Hopefully it'll be greatly improved in the lib blitz this year, and then once there's a consensus that it has improved enough, and it's stabilized, it can be added back into the standard library. I think random numbers are definitely something we should have, but there's so much we should have int he standard library that it isn't an instant process. I mean if you look at https://github.com/rust-lang/rust/blob/master/RELEASES.md, you'll see that the stabilized APIs in the standard library are growing at a rapid pace - `rand` just hasn't gotten there yet.
I don't really understand why it is easier. In quick-xml for instance, it uses `Cow` everywhere, it doesn't look complicated to use. Could you give a practical example? 
You want to post in /r/playrustservers
Note that option is stack allocated, which is basically free. Everything here gets inlined and optimized so it's the same as the for loop version, with some extra guarantees.
Is there any good IR to plug into for someone writing a compiler targeting the Rust ecosystem, a la [Malfunction and flambda for OCaml](https://github.com/stedolan/malfunction)? I tried compiling to Rust at one point, but I gave up because trying to satisfy the type checker was nearly insurmountable.
That'd be great!
The difference between a pull parser and a sax parser is who is in control. In a pull parser, the code that receives the events pull them from the parser one by one. This is similar to an iterator. A sax parser sends events to a handler. RustyXML is actually not a sax parser either, but a pull parser. Both RustyXML and quick-xml can be converted to be a sax parser by adding a parse function that loops over all events and sends them out via the sax handler. fn parse&lt;SH: SaxHandler&gt;(sax_handler: SH, reader: Reader) { loop { match reader.read_namespaced_event(&amp;mut buf) { Ok(Event::Start(ref e)) =&gt; sax_listener.element_start(e), Ok(Event::Text(e)) =&gt; sax_listener.text(e), Ok(Event::Eof) =&gt; break, .... Err(e) =&gt; sax_listener.error(e), } } } 
Yes! I was intending to extract exactly such a crate for a few months now. Good thing I don't have to anymore :)
Just wanted to chime in to clarify that Rust approach isn't really knew, it's just different from Python or C++. There is plenty of other languages that a follow similar split of lean stdlib and numerous dependencies: Ruby, Node.js, Haskell, etc.
I'm trying to create a file, but check that it exists first. If it does exist, I'm trying to throw an error, but I get an error with the return type (number of arguments). I've tried every combination I can think of, but can't work out what the solution is. My code: use std::path::Path; use std::fs::File; fn new_doc(name: &amp;str) -&gt; Result&lt;()&gt; { if Path::new(name).exists() { let mut file = try!(File::open(name)); try!(file.write_all("some text")) } else { Err("file exists") } } fn main() { println!("Hello, world!"); } And the error: error[E0243]: wrong number of type arguments: expected 2, found 1 --&gt; &lt;anon&gt;:4:27 | 4 | fn new_doc(name: &amp;str) -&gt; Result&lt;()&gt; { | ^^^^^^^^^^ expected 2 type arguments [Here's a playground link](https://play.rust-lang.org/?gist=c11830b7a0331170e9c3d76d35ee6657&amp;version=stable&amp;backtrace=0) - I've tried changing the Result type to `Result&lt;(), Err&gt;` but the compiler's not happy with that either.
Please don't mention Haskell, I'm still having nightmares about cabal
Stack is like the "blessed" approach for libraries, but for the whole ecosystem. I think it works well to fight "Cabal hell". I like that Rust disallows orphan instances, so it's not prone to having libraries being incompatible with each other just because they implement conflicting impls.
Well from a personal perspective it may be easier to write a small rust program than a shell / batch / powershell script, because you have more freedom in what you can do in regards to files, etc. Rust is very good at handling configuration files and serializing / deserializing via serde. But I don't think it's going to be particularly faster - plus other sysadmins may not know rust. If you have to work a lot with .csv files, for example, Rust may be a good choice, or setting up / reading configurations, etc or writing bots that can automate stuff / hooking into some foreign APIs. 
From memory, the Ackermann function became famous because it was the first function which could *only* be expressed using recursion. 
Rust has really good string and file support, plus it's strongly typed, so it's often what I'll use if I know the task I'm doing will be more than a couple lines long or be used more than once. [Clap] is excellent when you want to make a good quality command line tool with minimal effort. I find it's orders of magnitude better than python's `argparse` or go's `flag`! [Clap]: https://github.com/kbknapp/clap-rs
The error type of `File::open` and `write_all` is `std::io::Error`. This is confusing in the documention because `std::io::Result&lt;T&gt;` is an alias for `std::result::Result&lt;T, std::io::Error&gt;` but both are shown as simply `Result`. Also, `"file exists"` is not a `std::io::Error` so you will need to convert it somehow. Here is one way: [playground](https://is.gd/sFTifr). In a more complicated situation, you could use `error-chain` to make a custom error type containing `std::io::Error` and other errors.
Python also has [click](https://click.pocoo.org). 
It's kind of funny. Coming from a mostly C++ background, Rust feels amazingly like Python to me. Mostly because of the more "modern" feeling syntax elements like destructuring, match, and the amazing iterator chaining system.
Python/Ruby has chef/salt, tons of modules and examples for everything one can think of, including official SDK for nearly every existing service provider, interactive environment, lots of books, way more people are familiar with it, and learning it is easier. As a first language for devops people, Python/Ruby would be a better choice by large margin. Rust would make a very good second language though.
Thanks, I've been trying to work this out for ages!
I don't really know (obviously, as OP) but the PR also says it was originally introduced for one person's use. So I think calling it a "\"feature\"" is just illustrating that there was apparently never an intent to support it (in a truer sense of support than an interface merely existing in code) and that removing it is only superficially removing a feature; it didn't require as serious consideration as say a PR titled "Remove macros".
Welcome to Reddit. You have posted this to the wrong subreddit. You probably want /r/playrust. In future, I suggest you check what a subreddit is about before posting.
&gt; So in the [] version we have 3 fn calls per iteration. How about the iterator one? There's just a call to slice::Iter::next and the call to the closure. Just 2 of them! All of those things will almost assuredly get inlined. You can confirm it by looking at the assembly output on the playground.
FWIW, the last time the temperature of the libs team was taken, we seemed to be in favor of some small `std::rand` module (not necessarily the entirety of what's in the `rand` crate now). But we need somebody to champion that work.
At Faraday.io, we built our [core Docker-wrangling tools in Rust](http://cage.faraday.io/)! Rust is really good for highly-reliable, high-performance CLI tools. - The strong type system encourages you to think about and handle all the corner cases. - It's ridiculously easy to write CLI tools that work natively on Linux, OS X and Windows. There's often zero extra work except setting up AppVeyor builds for Windows. - Building against libmusl is usually trivial, thus allowing you to write 100% static Linux binaries that can be installed by `cp`ing a single file. Everybody in the company loves using our Rust CLI tools. They're fast and have extremely few bugs, and you don't need to install a huge dependency chain to get them running. So the question to ask yourself is: "Do my CLI tools need to be very fast, highly reliable and cross-platform? Or can I just get away with quick and dirty scripts?" That's a question that could go either way, depending on a lot of factors. If you need to automate third-party APIs, for example, check crates.io for decent libraries first.
cc u/rayvector I was talking about the performance of the debug version, which OP asked about. I think that neither of Rustc nor LLVM performs inlining at `opt-level=0`. But indeed, in the assembly there's only `Vec::index`. I suspect that indexing a slice by integer bypasses all the trait machinery and is built-in by the compiler, so it has to be inlined in the function's body. Anyway, I found some other crazy callchains in unoptimized asm that were making the `naive` version slower than `position`: * `Vec::index` → `Vec::Deref` * `RawVec::ptr`→ `Unique::deref` → `NonZero::deref` * `&lt;*mut T&gt;::null` 
Check out some image decoding code in a game engine I'm working on: [ChariotEngine/Slp](https://github.com/ChariotEngine/Slp/blob/cbd7a8f665cd7538d37ece20cb9568ed94277ed4/src/slp.rs#L245)
To your point...that's part of why I'm a regular Rust user now. :)
https://crates.io/crates/rusty-cheddar
&gt; Javascript uses UTF-16 Not quite. Technically, it's not supposed to be considered "UTF-16" unless it enforces the invariant that surrogates are paired. What you're likely to get out of a JavaScript "UTF-16" API is the same thing you get out of Windows APIs... something which may or may not be valid UTF-16 because it doesn't enforce that invariant. That's why `OsString` is [WTF-8](https://simonsapin.github.io/wtf-8/) on Windows. If you want to preserve the rule that every valid `String` is a valid `OsString` but also want `OsString` to be able to represent every possible string the OS may give you (eg. weird filenames), you need a relaxed UTF-8 that can round-trip un-paired surrogates. The proper way to describe what you get in systems like JavaScript and the Windows APIs is "[potentially ill-formed UTF-16](https://simonsapin.github.io/wtf-8/#ill-formed-utf-16)" or "WTF-16" for short and, generally, it comes about from retrofitting UTF-16 support into a system designed for UCS-2 back before Unicode outgrew a 16-bit codepoint space.
A lot of comments in this thread seem to have missed that this is all in debug mode, where most (all?) optimizations do not happen in the first place. There are a few things that happen regardless of optimization level, however: `.position` is monomorphized, so it will call the closure directly rather than through a pointer; `Option&lt;i32&gt;` is small enough that the ABI puts it in a register; the `.next` [method for slice iterators](https://doc.rust-lang.org/src/core/slice.rs.html#985-999) uses `unsafe` to omit bounds checks without relying on the optimizer. The first two of those mean that the only slowdowns of the debug-mode iterator version relative to the naive version are a bunch of direct function calls. These are trivially branch-predicted, and they pass and return their arguments in registers (though the program will do some redundant copies between the registers and the stack). The naive version, on the other hand, has a bunch of bounds checks in debug mode and relies on the optimizer to elide them in release mode. One caveat, though, is that it seems Rust does do *some* minimal amount of optimization in debug mode (based on other comments here referring to the output assembly). This could obviously change what I described, but it's still likely that the reason for the difference is that the iterator versions starts out reasonable and without bounds checks.
Embarrasing. Thank you :) 
&gt; The Option is going to to be over a reference, which means that the tag can be elided, as the null pointer corresponds to the None. I ran into that the other day, when I was examining the output of compiler for the `LinkedList` type, and discovered that the entirety of `Option&lt;Shared&lt;Node&lt;T&gt;&gt;&gt;` is compiled down to a raw pointer.
The HIR itself does not contain types. HIR passes operate on the HIR but have access to type info on the side which had been generated during type check. It's kinda B but not really. Basically HIR passes have access to more than the HIR.
I'm coming from the opposite direction, yes, but I can generate code, too (I have no difficulty making JavaScript or Lua) - I just have great difficulty generating code which can satisfy a type checker. The optimal place for me is the highest level IR available that isn't checked by a type checker.
Any recursive function can be expressed as a loop using a growable vector. In the case of Ackerman, you could also use memoization with a growing m * n matrix instead of returning values, along and a vector of `(m, n)` tuples to simulate the computational stack. The memoization isn't actually necessary, but it speeds up the computation and avoids any weird logic for returning values.
Open source programs don't have to still be in development. You're mixing a lot of different concepts. This is unreleased software, the notion of open source doesn't even apply.
I think you'll find we're the nicest community around. You can get your own free copy of Rust [here](https://rustup.rs/).
You probably mean /r/playrust, since Rust here is a free (as in freedom) programming language
Yeah, but if they aren't in development, it doesn't really matter if they're available specifically under a license that allows you to do development on it, right? It becomes important when the program inevitably turns out to be insufficient somehow and you feel the desire to develop it a bit.
eli rust noob why would somebody choose to expose these as integers directly instead of, say, enums wrapping those same ints (or something like `From`/`Into`)? If another exit code were to be added, the integer approach would Just Work whereas the enum wrapper would need to provide a special `Unknown(i32)` instance for that same flexibility -- is that a problem in practice?
That is an orthogonal problem. Python has the same issue, because libraries have since superseded the stdlib. That problem is solved through documentation and resources.
That is true. It's just that it's weird not having things like random integers inside `std`, and rather inside randomly strewn among crates. 
I'm not entirely sure what you mean. The project could be completely dead and untouched for decades but still be open source. If what you're saying is that an OS license only matters when "developing", then you are wrong. Personal modification doesn't require an OS license, and on the other side, you can have derivative works that don't modify the original. More to the point, Jai hasn't been released so the concept of closed vs open source doesn't apply to it.
&gt; you will pay for using Python more than you ever imagined, Yes. At some point. But getting there is much easier with Python. I do prefer more strict languages for things that are essential, but a) a lot of the things beginner will write will never need that and b) we are not talking about developers, but people who are just learning to program.
I don't disagree with any of this.
You always have to read the assembly if you really want to be sure that your code is optimized properly. Even something innocent like printing out an integer can break the optimizer down the line. https://medium.com/@robertgrosse/how-copying-an-int-made-my-code-11-times-faster-f76c66312e0f
Well, if that is an issue - I would go for Go - getting there with Go is just as easy. I personally hate it, but as a first language for devops it is already a much better choice than Python and Ruby, in my opinion. Sadly, I have a feeling it will eat up a whole universe before it's put to rest. 
You can vendor crates.io if you want to.
My apologies mlady
&gt; Now you have two libraries for the same thing in the stdlib. I really want Rust get to the point of such popularity that it has people writing and rewriting essential libraries as fast as possible. &gt; Back to square one. But now with thousands of developers using your language more than otherwise. &gt; Rusts regex API can have a complete overhaul without needing a Rust 2.0 release Having regex in stdlib is not stopping anyone from doing whatever they want outside of stdlib.
People already are rewriting essential libraries. Serde is an example. I have no issue with libraries getting rewritten, I'm saying that this situation of having to choose libraries is universal even in batteries-included languages and orthogonal to the discussion. You were talking about driving users to research solutions. I assert that it is equally a problem in mature batteries-included languages. I don't agree that the stdlib being battries-excluded in a language with a good package manager will end up with fewer people using it. Yes, it doesn't stop others from rewriting their own, but then you end up in the awkward situation where the stdlib has an API that nobody wants you to use (because the community prefers a newer one), but folks still end up using it because it's I'm the stdlib.
To be clear, the language has been stable for 2 years now. There are missing features, of course, but that's true of any language. Unless you actively miss those features while you're programming in Rust, I wouldn't consider it a reason to stay away. Rust's small standard library is not a symptom of its youth; it was intentionally designed that way. If you check back in a year or two to see what's changed on that front, you'll likely be disappointed. On the other hand, the Rust team cares deeply about having a healthy library ecosystem. They envision a crates.io with a stable crate for all of the common programming needs and beyond. It's not there yet, and that indeed is a symptom of Rust's youth! But I have confidence it will get there, especially with all the work that the library team has been doing in that direction.
Rust and Go don't really have that much of an overlap for it to cost us many users. And go's stdlib is highly geared towards a particular set of use cases. Most folks arguing Rust vs Go who use this reason are in a use case that Go is already very superior for for a multitude of reasons. The current crate evolution plan does not exclude moving things into the stdlib, in fact it's one possible outcome, but folks are conservative here. I think permashipped regex2/regex3 is a horrible situation to be in.
I'm still grumpy about `..=`, but I'll get over it. I much prefer `...`. I prefer having the feature to endless arguing about the detail, though.
Rust already supports being generic over traits. trait Simple {} impl Simple for i32 {} struct Generic&lt;'a, T: ?Sized + 'a&gt; { r: &amp;'a T, } fn main() { let i = 3i32; let gen = Generic::&lt;Simple&gt; { r: &amp;i }; } You can even use the [`Unsize`](https://doc.rust-lang.org/std/marker/trait.Unsize.html) trait in nightly as `T: Unsize&lt;U&gt;` to constrain `T` to a type that implements `U`.
Right there with you. `...` is still the right solution in my mind, but this has already been discussed to death.
No, not really. It's more fundamental than that. Again, I don't consider Go's stdlib to be rich, I consider it to be focused.
More precisely article says you should not store time in floats, i.e. use them as time instants. Using float for intervals and durations like frame time should be fine.
I'm not really its biggest fan of `..=` either, but the similarity of `...` with `..` is just too large, and too many programming bugs are off by one mistakes.
Interesting. Coming from C++, the compilers have warnings for `if (x = 1)`, and there is the so-called Yoda style (`if (1 == x)`) specifically because a single repeated character difference will *compile* BUT *have a completely different behavior*. There even was an allegedly backdoor attempt in the [Linux Kernel](https://freedom-to-tinker.com/2013/10/09/the-linux-backdoor-attempt-of-2003/) relying on this one repeated character difference being so small that humans' eyes will just glaze over: if ((options == (__WCLONE|__WALL)) &amp;&amp; (current-&gt;uid = 0)) retval = -EINVAL; *Note: for context, `0` is root UID, thus executing the following check would make the current process execute as root whenever the right hand side is evaluated... while still returning error so that it would only take effect for the next system call (sneaky, eh?).* I personally think that such a syntax is a design mistake, and I am therefore *very happy* that inclusive and exclusive ranges will differ in a more obvious way.
&gt; I don't do any of those things, but I believe those environments do provide at least one obvious and supported way of doing it (ASP.NET and Swing) You would be wrong! :) In the java case, there are 3 supported GUI api's (AWT, Swing, and JavaFx). Swing is currently in maintenance mode however, it has the most support. JavaFx was supposed to be the replacement for it, but oracle dropped interest in it. So now we are left in a nebulous state of which is the right one to choose. As for .Net. Well, that is just a mess in general. ASP.NET is the general overarching webapp tech. But under it are several competing techs in the std lib that you can choose. WebForms being the old style that is still in use and MVC, WPF, and a few others being in various states of competing usage. I'm not sure what the current hotness is, but the point is that it isn't clear cut at all. And that is somewhat my point. The problem of knowing which to go with isn't really any clearer than if it wasn't part of the standard library. You would still need to do some research to discover which way you should be doing things. &gt; I cannot say that about anything that comes when searching for "Rust webapp" or "Rust gui". It just does not exist yet. And for good reason. Making a webapp framework is hard to do and rust doesn't yet even have the full foundation down for making these sorts of things (like full windows api wrappers). There is experimental work going on in these areas and you can find a little about it (conro and rocket, for example) but none are production ready yet. Either way, what the community is doing and what the language maintainers are doing isn't the same thing. I can't say that the experience would be good if they brought in some of the earlier web frameworks into the standard lib (iron, nickel). &gt; Yes. Stdlib is for things that are good enough to be used, not necessarily for things that are best in class. If you want best tools that are actively developed, look somewhere else. The problem is, today's good enough is tomorrows horrible. Again, java is a prime example of this. They brought in so many techs into the std lib that, at the time, were good solid techs that nobody really complained about (Corba, XML handling, etc). However, today they are defunct and nobody really wants to use them. Java can't remove these old defunct techs and further they still have to support them when issues arise. That isn't where I would want to have the language team spending all of their dev time. I would much rather see them working on polishing the language and adding new features to it. Further, too expansive of a standard library can be detrimental to language changes. Again, java is the poster child for this. Their support for things like Serialization, reflection, and even the exposure of "unsafe" APIs which they couldn't hide due to the nature of the JDK have caused them all sorts of headaches in evolving the language and the API. (Serialization, for example, was one of the big holdups to keeping java from adopting lambdas) Ultimately, is is much easier to add new features than it is to take them away. &gt; Rich stdlib is primarily usable for beginners who don't know what's best (and even how to find it) and people concerned with long-term support. Supporting those needs would win Rust a lot of developers. Long term support isn't free though. Further, the larger the std lib, the more likely something is going to be unsupported. Again, java and many of the bugs around things like Swing. They don't fix some of the problems because applications have already evolved around the issues and the fix would break them, and they generally don't want to spend the time or resources maintaining these older portions of the standard library. &gt; Rich stdlib is primarily usable for beginners who don't know what's best (and even how to find it) and people concerned with long-term support. Supporting those needs would win Rust a lot of developers. I disagree. Primarily because the process of doing things through the standard lib and doing things through a package in cargo is mostly going to be the same. You google "How do I x in rust" and then you follow the links/blog posts/stack overflow statements on how to do X. Whether or not those include a std lib way to do things vs a non std lib doesn't really matter to the beginner, they have to do just as much research (especially if multiple ways to do x enter the std lib, which is more likely to happen with large std libs). &gt; Really, Java is an antipattern in terms of providing user-friendly APIs and developing stdlib, and Rust doesn't have any problems that java stdlib had, so I believe it would be easily much better. Really? java has literally done exactly what you are proposing rust do now. The only difference is we have the hindsight and time to see the mistakes that were made. When java put in their "unusable" apis, they all looked perfectly reasonable and usable at the time. They are somewhat learning from that mistake because they have by and large stopped expanding the std lib as aggressively, but somewhat too late. It is very hard to tell if a library is going to be good and usable without battle testing it first, and that is what the rust core team is doing before bringing stuff in. A great idea, because it has allowed them to see the evolution of things like mio and tokio. Things that would not have existed had they decided to just throw something in to get people over the hump. Rust doesn't have java's std lib problem because they have aggressively avoided expanding the std lib. But were they to follow the suggestions here of trying to be batteries included, it wouldn't take too many years before we look back and say the same thing "Man was that a mistake". &gt; Python and Go are far better examples of how things should be done. Like the current issues with python 3.x vs 2.x, primarily caused by having a standard library that wasn't well put together and making a breaking change? Or how about the fact that Go is now considering doing a 2.x version because of how terrible their http client is? &gt; With sane process of including things in stdlib, one of those libraries would just end there, making life easier for everyone. "Java got it wrong" is an argument for "we should do better", not for "we shouldn't do it at all". &gt; Yes, but a) Rust is in the early period of looking for new users and b) pros outweight the cons in my opinion. And I am not suggesting anything as big as .Net or Java. Rust does have a sane process, but it takes time. Rust's std library is anemic because core team is very particular about what can and can't end up there. Weakening that to bring in more stuff what other languages have done and it almost always leads to problems. The headache for a new user doing extra googling to figure out how to do something is outweighed IMO by the headaches of every future dev that has to deal with things being rushed into the standard library for new user ergonomics.
I was able to get a basic GUI running on the Raspberry Pi using the GTK crate [here](https://crates.io/crates/gtk) with minimal effort. I don't think you'd have any problems.
&gt; I prefer having the feature to endless arguing about the detail, though. I'm just here to watch the argument…
That's a nightmare to parse and ambiguous, though. Is `(0, 4)` a range or a two element tuple? Is `[0, 4]` an array or a range?
That would be unweildy. Exclusive ranges are super common and [0, 4) would be an extremely error-prone notation for it.
I can agree that Rust won't amass the size of the standard library python has - I just mean if that were to happen, there also hasn't been nearly enough time. I mean I think Rust could use at least a few of the most used utility creates integrated into it, and there really hasn't yet been enough time for everything to mature to that point.
Actually it's not affected by it. When you allocate a number of pages in userspace (either by calling `malloc()`, `mmap()`, `dlopen()`, etc), their addresses are randomly generated through ASLR, but they are not changed until you explicitly deallocate them.
I'm excited to see Rust being used by more and more companies in production! However, this project confuses me. The justification seems to be that compose did not satisfy all your requirements for complex projects. Why not contribute to compose itself? It just seems strange to throw another layer of abstraction over the tool. Now it's a Rust CLI on top of a Python CLI on top of a Go CLI.
Personally, I never agreed with considerations of "how the code looks like". The only "aesthetical" considerations that are important are about practicalities: is it easy to type, recognize, understand. On top of it - inclusive ranges are very, very rarely needed. I don't remember last time I needed one. So it's not like you'll have to look at them too often anyway. Because of that, it's even better if they stand out.
I think that `..=` is ugly, but the practical benefits over `...` far outweigh that.
This is awesome and I really like how `..=` is more informative about the usage intent. This clearly follows the rust path to promote explicitness on the user side.
'Lads' is not gender neutral, no.
I agree that they're exceptionally useful here, and a primary motivator for having a syntax for this at all. But other than that I find it quite rare to encounter a range that wants to be inclusive.
You say almost but that example looks like it works just fine. Are there any large downsides aside from the awkward ".clone_from" name?
If `Drop::drop` took `self` by value, then `self` would be dropped again when it goes out of scope, resulting in infinite recursion. Does `Clone::clone_from` do what you are talking about? It has the same signature. https://doc.rust-lang.org/std/clone/trait.Clone.html#method.clone_from
Because of how assignment works in Rust, I can only think of contrived scenarios where you'd ever find an op-assign inside of an expression, let alone as part of a for loop or pattern arm. Furthermore, all existing op-assigns are for single-character operators (i.e. we have no `&amp;&amp;=` or `||=`), so having `..=` stands apart. Finally, even if you have no familiarity with `..=`, it would only take a moment of consideration to understand that a hypothetical desugaring of `foo ..= bar` to `foo = foo..bar` makes no sense; since when would an op-assign ever change the type of the LHS?
We need to make a rust program that allows for searching in videos! Metainfo like subtitles but with embedded sourcecode ...etc. a rust compiler inside the Video that executes the example code! We're onto something!
I think it's just better to use nongendered terms (like hey everyone!) which is more inclusive which is what we want the community to be :)
Small point with `Drop`. A struct is _not_ immediately deallocated after `Drop::drop` returns: after `Drop` and before the struct is deallocated, all struct members with `Drop` implementations are dropped.
Ah, makes sense. I thought Drop was special enough that it might ne handled differently. `clone_from` mostly does (see my comment in the linked thread). The difference is you must allocate an object, allocate a second object, then call clone_from. You can't pass things around without implicitly invalidating your pointer. `Move` would be doing that implicitly whenever it moves, so it'd never be invalidated.
`a..(b+1)` doesn't work if you're, say, matching `u8` and want something like `128..=255`, because to write it as `128..(255+1)` you will overflow your `u8` (or if you write it as `128..256` will get a ~~compilation error~~ warning and unexpected wraparound). And of course `128..((255u16 + 1) as u8)` is even worse and still gives you wraparound. I guess you can instead always cast your `u8` that you're checking if it's in the range to `u16` instead, but then you need a `match v as u16 { ... }` everywhere you're trying to match on something in a range. I think `128..=255` is not so ugly once you go through all of the pain of trying to work around that.
Good points, though I'm curious where it'd cause breakage. For example, if it were implemented and I introduced my RelativePointer type, I would be surprised if it broke anywhere. I'm sure you can construct `move_from` in a way to break assumptions, but I wonder if that could be presented by a less naive design for it? Admittedly, all of these uses are highly specialized and can be replicated through careful use of the underlying types.
I'm glad we've moved the bikeshed here. I really like `..=`, it's just so much more obvious than `...`.
It can cause breakage because suddenly moves are able to call arbitrary code. Unsafe code dealing with callbacks has to be _very careful_ (especially in the presence of panics), and adding this trait is like adding a possible callback to every other line in generic unsafe code. Which is annoying anyway, but it also breaks existing unsafe code that has not been designed assuming that arbitrary callbacks are happening all throughout it. You'd need to introduce this as an extranormal class of types, e.g. `T: ?CpyMove`. Types which are `!CpyMove` are allowed to implement custom move hooks. This means that generic code can continue to assume that the types it contains are not going to do weird things on move. Generic code can opt into allowing types which do this via `T: ?CpyMove`. This is a pretty ugly API and there should be a high bar on adding something like this that will end up polluting generic code everywhere. See https://gankro.github.io/blah/linear-rust/ for another example of such an extranormal trait. 
I use this weird container with special memory management facilities that cannot be implemented in Rust without allocations because there is no Move trait. It provides O(1) insertion and O(1) handle-removal with minimal memory overhead (Minimally: 2 pointers + 1 offset per element). Most generally you have a vector of (T, Special) at a stable address. Special is a shared memory space containing an index and the stable address of the vector. Insertion inserts the element and returns a Witness of the insertion (a wrapped special implementing Drop). Removal is a swap-remove that updates the index in Special of the swapped element. Destruction of the Witness automatically removes the element, destruction of the Vec invalidates the Witness. (Because I like it like this, otherwise you can forget the Vec-pointer.) This is all possible in Rust. However both the Vec and the Special must be allocated to have a stable address. But C++ can drop the allocations: store a pointer to the location of the Witness in Special (which is now only a pointer and not allocated), store the (unstable) Vec-pointer and index in the Witness as POD. Every move of the Witness updates Special by following it's data, every move of the Vec updates all Witnesses. I use this data structure for subscribing a consumer without losing ownership or sharing: store the handle in the element itself. It also works really nice with binary heaps: you can truly use O(log(n)) sink and swim, which is normally dominated by linear search or HashMap. 
That has a meaning in mathematics, though. It makes for good programming and confusing semantics to those of us with a math background :)
**..** is a **classic inclusive range** in Pascal. type arr= array[0..5] of integer; I would make mistakes everywhere :'( If you follow the classic conventions, you could add an exclusive range with: &lt; and &gt; range_0_to_9 = 0..&lt;10 range_2_to_9 = 1&gt;..&lt;10 The ... could be used to define an unlimited range. r_10_or_more= 10... ; r_2_or_less= ...2; r_negative = ...&lt;0; r_unlimited= ....; No mistakes possible, since ... always has one side. The .... has no sides. --- alternative added: instead of 1&gt;..&lt;10 you can also use 1&lt;..&lt;10, which looks more like 1&lt;x &amp;&amp; x&lt;10 
As someone who always reaches for requests I've never heard of click
&gt; Isn't 'lads' mostly gender neutral nowadays anyways? Maybe I just haven't heard it used enough to have a feel for it. To me, as an American, it does not sound gender neutral (can't speak to UK, which is what I associate "lads" more with anyway). Now American "guys" on the other hand could be argued to be gender neutral. (It's how I express my second person plural, regardless of the makeup of the group I'm addressing.) But I don't perceive lads that way.
Arguably "guys" could be considered more gender neutral when referring to a group of individuals/friends. I know, there is still gender attached and it isn't ideal, but it is also usually fairly informal when it is used. Perhaps /u/daboross was thinking of that? Lads is definitely not USA centric so I couldn't say if it has started to take on similar mixed gender connotations. Time to make "ya'll" a thing everywhere :) .
I've always liked the point of view that array indices (or pointers, for that matter) don't refer to the items themselves, but to the boundaries between items. In other words, array index 1 refers to the start of the second item in the array and to the end of the first item. In that case the x..y notation naturally refers to the items between the two boundaries. In fact this model holds up really well when looking at the implementation. For example, if your array items are 20 bytes and you want index 4, you calculate the address as base + 20 * 4, i.e. you calculate a pointer to the start of the item. There's also a bunch of algorithms involving intervals where this way of thinking about array indexes is very natural, which makes it useful for algorithms and loops in general. 
(relm's author here) /u/Phrohdoh is right, you need to import `WindowExt`. If you don't, the compiler should give you an error like that: error: no method named `set_title` found for type `gtk::Window` in the current scope --&gt; examples/test.rs:25:1 | 25 | #[widget] | ^^^^^^^^^ | = help: items from traits can only be used if the trait is in scope; the following trait is implemented but not in scope, perhaps add a `use` for it: = help: candidate #1: `use gtk::WindowExt;` Hopefully, the errors generated by the compiler will get better soon (at least in nightly). The last line tells you exactly what to do. Also, there's currently no way to call a property method with multiple parameters. What you can do, however, is the set the properties individually. Here is a complete example: #![feature(proc_macro)] extern crate gtk; #[macro_use] extern crate relm; extern crate relm_attributes; #[macro_use] extern crate relm_derive; use gtk::{ Inhibit, WidgetExt, //WindowExt, }; use relm::Widget; use relm_attributes::widget; use self::Msg::*; #[derive(Msg)] pub enum Msg { Quit, } #[widget] impl Widget for Win { fn model() -&gt; () { } fn update(&amp;mut self, event: Msg) { match event { Quit =&gt; gtk::main_quit(), } } view! { gtk::Window { property_default_height: 650, property_default_width: 1000, title: "Window title", delete_event(_, _) =&gt; (Quit, Inhibit(false)), } } } fn main() { Win::run(()).unwrap(); } I used `property_default_height` and `property_default_width` to get the same effect as `default_size`.
Now to support down-stepping ranges in an idiomatic way...
There's more to ranges than array indexes though. In the end the a left-inclusive/right-exclusive range however more convenient to use in programming is symmetric as the set `{x : x &gt;= start &amp;&amp; end &gt; x}`; it just happens to be more convenient that's all; there isn't really some great mathematical logic behind it.
Airplanes have Wi-Fi on board, and airport terminals also have Wi-Fi. Most schools also have Wi-Fi access for students. That, and you don't always need an online connection just to use crates. You only need an online connection for the first build.
&gt; The additional thing this enables is intrusive datastructures like intrusive stack based linked lists. Could you elaborate on this a little? I've had stack based linked lists work just fine in current Rust, so I've clearly not understood something.
I'm still salty about ".." being the exclusive range operator in Rust, unlike basically all other languages out there (Ruby, F#, Haskell, Perl, Ada, Coffeescript, Kotlin, name it). Imagine if they chose "..." as the inclusive range operator, it would have been even more confusing coming from Ruby where the two are inversed...
Yes, obviously you can reason it out, but the point is that `..=` *looks like* and thus is likely to be *intuited as* similar in spirit to `&amp;&amp;=`, `||=`, and the rest. I for one an not a fan of this change, but I will grudgingly accept that all the options seem unfortunate and picking one and going with it is certainly the best way forward.
Computationally expensive in what way?
For managing infrastructure on various cloud providers, Rust is new to the game. Most of the tooling is still written in Go and Python. See: [Terraform](https://github.com/hashicorp/terraform), [Troposphere](https://github.com/cloudtools/troposphere). Higher level concerns such as running containers are dominated by [kubernetes](https://github.com/kubernetes/kubernetes), written in Go, and [mesosphere+marathon](https://github.com/mesosphere/marathon), written in Scala. The only Rust based cloud management system I know of, for loose interpretation of that term, is [kaws](https://github.com/InQuicker/kaws) which uses [Rusoto](https://github.com/rusoto/rusoto/) and creates and manages Kubernetes clusters on AWS. There's lower level crates for [Azure](https://crates.io/search?q=azure) and [AWS](https://crates.io/crates/rusoto_core) but they're all under heavy development. Chef has a tool called [Habitat](https://github.com/habitat-sh/habitat) that does management for "app automation" but I've not had a chance to play with it. There's a talk from Rustfest about [taking Rust to production](https://www.youtube.com/watch?v=zAXbPnfTJg4) for some more information how they use Rust for the project. Full disclosure: I'm a maintainer of Rusoto and my perspective is quite AWS focused. 🙂
I'd say that `&lt;=` and `&gt;=` are pretty universal.
*That* issue is a rustlang issue. I have a situation where I can specify that I have an `Error + 'static` and therefore the cause should be understandable as `&amp;(Error + 'static)` (otherwise the `Error` couldn't have it referenced as a cause), and I moved the `Error` into the function and don't let any of the references escape that function so it *should* work out... 
&gt;To my amusement on unptimized builds the latter is actually faster. Even without an opt level rustc clearly finds a way to optimize away all the next calls and does something intereting. The naïve one takes about 1.8 seconds on my machine but the one using the iterator does it in 1.3 consistently. Evidently the code with a closure *is* the optimized way. 
Just to add to this: functional languages have many years of optimizations, just like imperative languages. So take advantage of that, to the extent rust permits it. 
Slice it: let n = 10; let last_n = &amp;my_slice[my_slice.len() - n..]; additional conditionals are needed if the slice is not guaranteed to hold at least `n` elements. 
I'm still not quite seeing why this is necessary. You should be able to propagate the `'static` bound up the call chain as needed to and let the compiler make sure that it all works out. Falling back to this just opens you up to mistakes that are preventable by the language. Maybe I need to see a little more context?
To inspect a cause chain up to the originating cause to e.g. respond to specific error types based on their specific properties beyond what you can see just on the `Error` trait, you have to try downcasting. However, downcasting requires `&amp;(Error + 'static)` and `cause` will only ever give you `&amp;Error`, the `'static` does not propagate. See https://github.com/rust-lang/rust/issues/35943
In main: The match is redundant as you do not use its value, you can use an if let: if let Err(e) = create_archive() { } exit(0) is indeed redundant. "create_archive" does some things besides creating an archive. I would have taken the arg parsing out of the function. Because you've checked that the amount of arguments is greater than 3, you can just unwrap nth(1). (Also, the string error is not very informative). On line 45, when creating the zipfile, you hide the OS error, so the user cannot know what the error was. Does the directory not exist? Permission error? No space left on device? I'm not 100% sure but the extra scope in the loop is not needed. You can use the ? operator instead of try. No need to do an explicit return at the end of the function.
`...` is prettier, but `..=` is more obvious when reading. I already think that the `?` operator is too easy to skip over when reading code (although it's way better than using exceptions for control flow with the same ergonomics), so I'm happy that it's really clear what the difference is here.
&gt; Here ya go: https://play.integer32.com/?gist=12b70d98eb5e1579685e604f8427b4d4&amp;version=undefined Well, it looks like the extra scope was indeed unnecessary. Thanks so much for the feedback! I appreciate your time and advice.
Thank you! I had Window in use directive but not WindowExt (and tired as I was, I miss read WidgetExt for WindowExt… ^^')
Are there any examples of traits in the standard library with no implementors? I would love a `std::rand::Random` trait just so various crates can inter-operate nicely.
A Unix timestamp is a `i64`... You could use [Byteorder](https://docs.rs/byteorder/1.0.0/byteorder/trait.ByteOrder.html#method.read_i64) to read from a slice of `u8`.
Good point. Consider my previous message fully retracted.
You know it's a good compromise because all sides hate it equally. :D (I don't hate it, but I do kind of agree with the people who say the syntax should have made choices that avoided this problem in the first place. Either way, bikeshed painted, time to get on with life.)
Okay, thanks again, hopefully that'll be enough; I'll have a play around. :)
I originally thought the same about `?`. When it was introduced I was very skeptical about its usefulness and how easy it would be to miss while reading. But now I love how it simplifies my code, and with proper syntax highlighting it sticks out well enough to notice while reading. 
This is the comment I was looking for! I hadn't thought of why we needed it.
Well that is the most basic use case for slicing: &amp;my_slice[n..]
How?
I just wish it was more like Swift. In Swift `a..b` is `a..&lt;b` and `a..=b` is `a...b` which reads much nicer in loops.
I actually wrote a little library on top of ziprs that can zip directories (https://github.com/rookwood101/zip_path) - I never found anything in pure rust that could. How did you find the documentation, in terms of complexity, for ziprs?
&gt; depends on how it's encoded. a list of integers
A list of what size integers? Big endian or little endian?
Instead of `Question(Box&lt;Expression&gt;, Box&lt;Expression&gt;)`, prefer `Question(Box&lt;(Expression, Expression)&gt;)`
There's a `sha2` crate that [does what you want](http://brson.github.io/rust-sha2/sha2/sha2/index.html).
u64
You don't translate Java code to Rust line by line. Look at [the docs for the `Sha512` struct](http://brson.github.io/rust-sha2/sha2/sha2/struct.Sha512.html), look at what the available methods are, and use the ones that do the thing you want to do.
Looks like `input` and `result`.
If takes a bool expression. Assignment is a unit expression iirc, so the code will not even compile.
I was wondering how much of this applies to serde_json, as this is mostly about building a DOM of the JSON object (in contrast to using `#[derive(Deserialize)]` – which is most likely faster). /u/dtolnay already has an answer in the form of [this issue](https://github.com/serde-rs/json/issues/323). There is a good discussion about this [on HN](https://news.ycombinator.com/item?id=14421215), looking at using SIMD to make this even faster.
Ah you're right, when I found out assignment was an expression I just assumed it evaluates to the assigned value, I guess it's just convenient to have it be an expression, like for `match foo { BAR =&gt; x = y, ... }`
are you a teacher and am i your student?
You are asking a question and I am leading you to an answer. If you just want the answer, /u/eddyb has provided it already.
I'm not making an argument about half-open or closed intervals. Personally I prefer closed intervals, but as long as both are supported I don't mind. What I mind is using .. to denote an half-open range, which goes against existing convention in other languages. (Is there any language where .. is an half-open range ?)
And if you want to use the new-ish `conservative_impl_trait` feature where you can return a trait as the return type of a function, and return an `Iterator&lt;Item = &amp;u32&gt;` instead of a `&amp;[u32]`, you can do [this](https://is.gd/Sr9mUR).
So is each `i64` in your vector a separate timestamp? Or do they all represent one timestamp somehow?
all
and without "chrono"?
No, you are not. /u/eddyb gave you the answer.
how can I get a lenght of a result so that I can allocate a buffer for it? in java it's `dgst.getDigestSize()`
&gt; Having one interface - or one blessed implementation - would make it easier. Then you'd be in the same sorry state as Java's Servlet API, which constantly lags the not too fast evolving HTTP standard and client-server needs and paradigms. &gt; I'd gain more from having polished database drivers, webframeworks and template engines than I would gain from having HKT etc., however useful would that be. Rust core devs are probably pretty smart, but maybe totally unfamiliar with DB drivers, so they should do their things, and provide HKTs, and let the community provide the drivers. After all, if there were HKTs, it'd be easier to write things (such as drivers). &gt; I am proposing standardizing things people do agree on after using them first independently. That's why some people from the Rust team plan to bless a few crates and provide a "Rust Standard". For starters there's the [cookbook](https://brson.github.io/rust-cookbook/), which will be a semi-official collection of HowTo-s, of course with package recommendations. &gt; Java proposed entirely new APIs without anyone using them before, ... Well, yes, but now there's the whole JCP/JSR thing, and there is always a reference implementation, and so on. Not that does them a big wonder. &gt; in rush, with large set of entirely new technologies added to the mix, with the goal of satisfying every possible need. It makes a big difference. And you want DB drivers. Someone else wants better concurrency primitives, more language interop, more this or that, all takes time. And of course language designers and implementors are still probably not the ideal people to write these things.
I'm positive this individual is a troll. There are two questions they've posted, and in both it looks like they've taken no effort whatsoever in reading the documentation or interacting with those trying to help them.
I am holding out hope that the issue is the language barrier here.
For some quick, infrequent conversions something like `floating-duration` is perfectly fine, but the convenience and performance benefit of having a dedicated type for situations where the rest of the program uses floating-point values or you are doing arithmetic is significant in my opinion. Doing an operation such as "make a duration 50% longer than another" with std::time::Duration is a bit tricky and converting one to a float requires two casts, an addition and a multiplication. I've tried to make conversion to/from std::time::Duration and chrono::Duration as seamless as possible, so it *should* be almost as easy to use as `floating-duration`. Ultimately, I think there are some use cases just better suited with floating point values for time, just as their are some better suited with integer values, and hopefully this library can fill the former niche :). 
It's an interesting article and while I think doubles are probably "good enough" for 99% of cases, it's probably worth making the "total game time" in my game an std::time::Duration that can be converted to a FloatDuration if needed.
Sweet! Is it written in Rust? 😁
Oh, wow, I really like your library. Maybe I'll integrate it at some point. This whole thing is a learning project, so learning to use other people's libraries is kind of the point. Are you referring to [this](http://mvdnes.github.io/rust-docs/zip-rs/zip/) documentation? I've been using Rust for less than a week, so maybe I'm not the best to judge the documentation, but I didn't find it very useful. I had much better luck looking at the provided [examples](https://github.com/mvdnes/zip-rs/tree/master/examples), and reading the library's source code.
I'm currently trying to work with the [image-crate](https://crates.io/crates/image): I'm loading an image with [the `open`-function](https://docs.rs/image/0.13.0/image/fn.open.html) and want to copy it directly into a texture for rendering (with sdl2, if that matters). I found this [`raw_pixels`](https://docs.rs/image/0.13.0/image/enum.DynamicImage.html#method.raw_pixels), but it allocates a new vector. I could just match on the types and then use [this `into_raw`](https://docs.rs/image/0.13.0/image/struct.ImageBuffer.html#method.into_raw), but I would have to do that for all variants, which seems weird. I wanted to ask if there is another way? I also found [`to_rgba`](https://docs.rs/image/0.13.0/image/enum.DynamicImage.html#method.to_rgba), which looks like the straight-forward and future-proof way, but it seems, [that this incurs copying the image too](https://github.com/PistonDevelopers/image/issues/620).
Since you seem to want examples, here is the exact translation of your code to rust: extern crate sha2; use sha2::Digest; // byte[] payload = getPayload(); let payload = [1,2,3,4]; // SHA512Digest dgst = new SHA512Digest(); let mut digest = sha2::Sha512::default(); // dgst.update(payload, 0, payload.length); digest.input(&amp;payload[0..payload.len()]); // byte[] res = new byte[dgst.getDigestSize()]; // dgst.doFinal(res, 0); let result = digest.result(); The `sha2` crate digests are stack-allocated rather than heap-allocated, so there's no need to preallocate a buffer. Documentation is at https://docs.rs/sha2/0.5.2/sha2/index.html.
Several years ago, I used Exercism.io to become a lot better at JavaScript. I'm hoping for a similar experience with Rust!
Haha I didn't actually realise there were examples, that would make things a lot easier. Cheers very much :)
I think I managed to do it without an extra copy now, if the image is already RGBA: let texture = tc.create_texture(PixelFormatEnum::ABGR8888, TextureAccess::Static, image.width(), image.height()).unwrap(); if let Some(image) = image.as_rgba8() { texture.update(None, image, image.width() as usize * 4) } else { let image = image.to_rgba(); texture.update(None, &amp;image, image.width() as usize * 4) }.unwrap(); The data layout of the image is not RGBA but ABGR though, very confusing (don't know if it is an SDL issue or just the function naming). `Cow` may be useful here.
TIL that lad isn't the same thing as lady (non-native speaker)
There's not an official Twitter feature for this, but third party tools can do it for you: http://twitrss.me/twitter_user_to_rss/?user=RustStatus&amp;replies=on
What does each integer mean? Is one integer an year, another a month, another a day, and so on?
I wrote the rotational-cipher exercise. You're welcome. 
Ha, not yet. Maybe when it gets bigger than 100 lines I'll think about it!
Tweets are "via [Rust IRC bridge](https://aidanhs.com)" (twitter.com doesn't seem to show the client used for posting anymore, tweetbot and tweetdeck still do)
Ah, gotcha. I was unaware of that issue with the `Error` trait. Maybe you could use something [like this](https://play.rust-lang.org/?gist=609a5225e83a682dab1e340b536be2bf&amp;version=stable&amp;backtrace=1) to tack it onto existing `Error + 'static` objects in a safe/ergonomic way?
Why all the iterator adapters functions have their dedicated structs? E.G. map and Map, skip and Skip etc.
You've linked docs for an old version of the crate. Correct link is: https://docs.rs/sha2 `sha2` crate now is part of the [RustCrypto](https://github.com/RustCrypto/hashes) project. In the readme one can find additional examples.
They are iterator *adapters*, and work on iterators. They are currently implemented in the Iterator trait. Vec&lt;T&gt; doesn't have them because it is not **Iterator**, so you have to create an iterator out of it first. I guess technically it is possible to add a proxy impl to Vec&lt;T&gt;, but there's no one way to create an iterator out of a vector, there are three. (iter(), iter_mut(), into_iter())
no no
There doesn't seem to be much material - but there is a little bit in [PistonTutorials](https://github.com/PistonDevelopers/Piston-Tutorials).
Isn't it possible to replace all those structures with one enum? 
Wrong subreddit. you want /r/playrust
I don't quite follow. GTK is the basis for gimp and gnome, I think there are language bindings for Java, but it's not directly related to Java in any particular way. 
https://www.youtube.com/watch?v=IQO9N0Y8tcM&amp;t=9m45s
It is great to hear TiKV making great improving,this is the first well-known chinese rust project i heard.
Yes, but expanding the enum is impossible without modifying the standard libs, and breaking compatibility. With the current setup, anyone can write new iterator adapters.
I just implemented the changes based on your feedback. Thanks.
[removed]
I think it is because being downvoted because despite being provided guidance OP has shown no interest trying to obtain knowledge on their own (eg RTFM). They seemingly want to be spoon-fed the answer. Initially I wasn't quite sure about whether I liked the fact it was being downvoted, but the more I think about it, it is what the downvotes are meant for. OP isn't being downvoted because their question is simple or goes against opinion, they are being downvoted because the way they are acting doesn't contribute to the community. 
For line continuation, I mean code like let settings = Settings::from_file(CONFIG_PATH) .expect("Unable to read configuration file! Run --genconfig."); Usual style is to indent subsequent lines (how exactly varies) that belong to the same statement/expression. It is definitely proper style to have code in match arms. Keep in mind that `match` is also an expression that can yield a value - and if one match arm returns early, the other can yield anything. Often though, you don't even need that `match` and can use the many combinators on `Result` and `Option`, most helpful IMO are `unwrap_or`, `unwrap_or_else`, `map`, and `and_then`. For the specific examples here, I'd write let user = settings.get_user(&amp;username).unwrap_or_else(|_| { writeln!(&amp;mut io::stderr(), "You are not in the rudo.json file! This incident won't be reported.") .expect("Failed to write to stderr!"); process::exit(1); }); As for type annotations, don't worry - they certainly don't hurt :) It just can be a pain to write down more complex types, which is why Rust has inference in the first place.
Sure, but then you have to dispatch on each call *and* add a discriminant to your iterator data, also the size of your iterator enum will be the size of the largest variant (even setting aside that this enum isn't extensible). This is an unacceptable performance cost, even if the dispatch would be 100% correctly branch-predicted (hint: it will, but only in simple cases).
[removed]
In docs I often see patterns like impl&lt;T&gt; Eq for [T; 5] where T: Eq impl&lt;T&gt; Eq for [T; 6] where T: Eq impl&lt;T&gt; Eq for [T; 7] where T: Eq impl&lt;T&gt; Eq for [T; 8] where T: Eq impl&lt;T&gt; Eq for [T; 9] where T: Eq or impl&lt;Ret&gt; Eq for fn() -&gt; Ret impl&lt;Ret, A, B&gt; Eq for fn(A, B) -&gt; Ret impl&lt;Ret, A, B, C&gt; Eq for fn(A, B, C) -&gt; Ret * Is it not possible to abstract over this with macro? * Will it ever be possible to avoid this repeating pattern? 
When π types (integer generic parameters) land, the first will case will be solved. Something like (not 100% about syntax) impl&lt;T: Eq, const N: usize&gt; Eq for [T; N] The second case is more complex, and requires some kind of variadic generics, IDK about the timeline. For the meantime you can create a macro that does it for you.
To add to /u/i_r_witty's comment... OP was being rude to the people who are helping them, and not bothering to read the most basic of documentation linked before asking follow-up questions that are already answered in said documentation. That behaviour quickly changes "is anyone willing to help me?" into "someone do my job for me". I suspect that sort of demand doesn't sit well with a lot of people.
57 pts only 1 comment, this sub is full of bots
Why? I upvoted because this was very interesting.
And now I can't give it another try because it's no longer there.
What will happen to the existing [@cratesiostatus](https://twitter.com/cratesiostatus) account? Will that still get updated?
Great! Thanks a ton for thinking of this. :-)
[removed]
Well I'm coming from a land of well-founded induction, so I don't think this is an issue.
Me too. What's the point commenting when you have nothing to say? ;)
Your post
There's no JIT in Rust, to be clear.
I'm honestly not sure whether you're asking for clarification or if you just thought that was a funny thing to say.
Since your goal is similar to the [Libz Blitz](https://internals.rust-lang.org/t/rust-libz-blitz/5184) goal of bringing popular crates to 1.0 - you can look there for self-evaluation, especially the [API guidelines](https://github.com/brson/rust-api-guidelines).
[Swift syntax](https://www.reddit.com/r/rust/comments/6dhk59/after_nearly_2_years_inclusive_ranges_are_finally/di3vkym/) seems better? (`..` and `..=` becomes `..&lt;` and `...`)
Is it possible to pattern match on vector? Example : match vec![1,2,3] { [a, as...] =&gt; as } returns vector slice which contains [2, 3]
&gt; After a long debate, it has been decided to keep hoedown testing/rendering by default in rustdoc. However, you can test pulldown by running rustdoc with -Z unstable-options enable-commonmark. Eventually, we still will switch over to CommonMark, so please give this a test!
I'm doing a course on software foundations, and this just made my mind go into imagining implementing the whole lambda cube, then continuing towards automated proof search or something like that. Of course I don't actually have time for any of that, but it was still a fun little fantasy \^\^ For untyped lambda calculus your crate seems to cover the basics pretty well. There's always more you can add, but that can come after the 1.0 release. It all depends on what you expect people to use your crate for, I guess. 
To moan about bots?
&gt; As with Swift, I haven’t been able to find conclusive evidence nor credit given to suggest that there was any influence from Scala on Rust … Scala has at least shaped Rust indirectly. Aaron Turon (/u/aturon , Mozilla's Rust project lead?) used Scala in his PhD ([`ChemistrySet`](https://github.com/aturon/ChemistrySet) ) and his [`crossbeam`](https://github.com/crossbeam-rs/crossbeam) Rust library can be considered a follow up on that. What he learned during his PhD certainly has influenced his Rust's designs. I cannot point out to anything more concrete than that but maybe Aaron can (I just recall Scala being mentioned in some RFCs/talks).
And the memory savings! Web scale it!
If the author of the article readsd this, please please please for all the fucks, don't put URL text in alt attributes to images, especially when the URL is full of base64-like nonsense. Having the screen reader say a bunch of random letters and numbers isn't fun. If you're not going to put some meaningful text on the alt, leave it empty. 
Another way to look at it is that the relationship between `&amp;` and `&amp;mut` is different to the relationship between `let` and `let mut`. The difference between the pointers (aliasable, only special types are mutable, vs. non-aliasable, all types are mutable) is fundamental to safety, while `let mut` is just a "lint": Rust where all variables were implicitly `mut` would be as memory safe as the current Rust. (A bit before 1.0, there was the so called "mutpocalypse", which was started by a proposal to remove let mut. The later https://www.reddit.com/r/rust/comments/25ma77/an_alternative_take_on_memory_safety/ has a nice summary at the start.)
[removed]
[](/cheerwhat) Good writing, but it... doesn't really have anything to do with Rust.
Imagine the range defined on the reals, `[0, 5)`, corresponding to the second row in the image. Each square represents the subrange from a number to its successor, eg. 0-1, 1-2, 2-3, 3-4 and 4-5. The `[` on the left fully contains the range 0-1, so is inclusive. The `)` on the right mostly contains the value, but cuts a little off the end, so is exclusive.
I've been doing embedded stuff and I've actually had to implement this for a project. I'm wondering if the `rand` stuff can be generalized and implemented on embedded systems and then the timing stuff in this crate could be abstracted such that this could be used ergonomically in `no_std`.
There needs to be an equivalent of `ref mut`, but `*` is used to dereference both `&amp;` and `&amp;mut`.
I used 'mach build', like I have in the past and just to double check, I look at the firefox documentation. Ah. looks like an upstream issue.
Yes, those are good guidelines. 
Using the latest Rust, stable or nightly, should be enough.
You're conflating cargo and rustc version numbers. rustc is well above version 1.0.0, and it has been for the past two years, well before Firefox started using it, so Firefox definitely does not depend on ancient Rust. rustup has no idea what you're asking for with a version that low. cargo is currently pre-1.0, and the numbers you're using make sense for cargo. but the version you use shouldn't matter, as long as it is newer than what they used. is there a bug upstream that mentions requiring cargo 0.17.0?
Looks like the upstream bug is indeed related to cargo 0.18.0 https://reviewboard.mozilla.org/r/111682/#review113446 Even though the bug looks like it was fix back in march, it's not landing until 54 per https://bugzilla.mozilla.org/show_bug.cgi?id=1338655 Which means .. I don't know. wait until 54?
Probably try `rustup install stable` and then `rustup default stable`
Ah.. good point.
That's because its an associated type constructor (ATC), a feature which hasn't shipped yet.
I thought it was the flickr embed code (I still think this is the _old_ way of embedding), but the current one looks like this: &lt;a data-flickr-embed="true" href="removed for brevity" title="Staircase to hell... lomoed!"&gt;&lt;img src="removed for brevity" width="1024" height="1024" alt="Staircase to hell... lomoed!"&gt;&lt;/a&gt;&lt;script async src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"&gt;&lt;/script&gt; 
Hi there. Sorry about that. Was trying out a way of giving the original photographer some credit by using a new plugin to add a caption. Sadly I hadn't checked it thoroughly. It's hopefully fixed now.
Thanks for the explanation, I think you've convinced me. I suppose I was thinking that it would simply be consistent with the fact that you don't need to call methods that take (`&amp;mut self`) any differently. struct Hello { name: String, } impl Hello { pub fn set_name(&amp;mut self, new_name: String) { self.name = new_name; } } fn main() { let mut h = Hello { name: "bwahaha".to_string() }; h.set_name("lol".to_string()); println!("name: {}", h.name) } In case I screwed up the formatting here on Reddit, here is what I mean on [Playground](https://is.gd/1xcyr4)
Good catch! Thanks! I've fixed it.
I didn't intend to suggest otherwise. Those just seemed like rather glaring oversights from my perspective. ...but then safety is the main reason I've been migrating from Python to Rust, so I *would* focus more on things like locks you can't forget to release and type system tricks that help to ensure proper handling of potentially weird filenames at compile time.
Ok I see. That sounds very reasonable, and is similar to how certain *third party libraries* in Scala make use of the type system to bring extra safety to, say, working with Paths. What you said actually reminds me of a very good point that I forgot to mention in my post, which is the importance of a *well-designed standard library* that truly takes advantage of the language. Subsequently, I feel that Rust's std lib is fairly well thought out and implemented. That said, Scala's standard lib has its fans and critics, and comparing the std libs of the 2 languages is very challenging due to, among other things idiomatic differences. Maybe I might talk about this fairly important topic in a separate post...if I ever have the courage to stir that hornet nest.
Ah, so no way to express what I want now?
Not that I know of. :(
:( Thank you for all your replies, anyway!
But if you'll write function which takes `h: &amp;Hello` you will not be able to call `h.set_name(foo)` inside of it, so tracking mutability is very important. You can think that for convenience reasons Rust makes the following transformations for you behind the scenes: 1) owned `Hello` -&gt; `&amp;Hello`, `&amp;mut Hello` 2) `&amp;mut Hello` -&gt; `&amp;Hello` I think there was RFC which extends this behavior to passing values as arguments to functions, so you will be able to write `foo(bar)` if you don't mind `bar` to be consumed, instead of `foo(&amp;mut bar)`.
You're at least the fourth person to call this behaviour a bug on the issue tracker. I myself called it a bug. I'd say the only surefire of "disabling" this feature is have blocks' final expression always just be a declared variable. So, instead of `{stuff; stuff; stuff; expr}`, you'd have `{stuff, stuff, stuff, let ret = expr; ret}`.
I was going to ask about that last example, but in writing out my question I figured out what was going on. Ordinarily, variables created as part of a `match` or `if let` go away at the end of that expression, but (because of the behaviour we're talking about) temporaries in the last statement of a block are kept alive until the end of the containing block—that means the reference returned by `.try_borrow()` is kept alive until after all the local variables are dropped, and (in particular) the reference outlives the `RefCell` it refers to, hence the borrowck error.
TLDR; You can't properly abstract over `Rc` and `Arc`, but it could be made easier to do with macros. Would anyone be interested in a crate which does this? If so, are there anyone interested in creating such a crate? ;)
I find myself writing `&amp;mut` in places where I think it should be obvious I'm passing a `mut` pointer. Maybe removing the need for `&amp;mut` would be a nice usability gain. Not removing `let mut` altogether, but just not having to repeat yourself.
I actually started implementing a similar sort of crate. A couple of features that I was going to add: * A function that is called after the failed attempt but before the delay * Number of attempts to make before a permanent failure 
Hi! This is the community for rust-lang, the Rust programming language. It seems you are probably looking for /r/playrust, the community for the Rust game.
I believe mostly to make it clear what is happening (they all iterate), as well as to allow use of all of them on both borrowed and owned iterators. (vec.iter() / vec.into_iter()). Having them on Iterators means you can also combined any combination you want freely, and all you need for a custom list-like collection is an IntoIterator implementation and you get all of these for free.
&gt; Because `Rc` is waay faster than `Arc` Really? I thought the only difference is CPU bus being locked in case of `Arc` cloning or dropping. Is there something else I don't know about?
I have been working on a CoW B-tree library, tentatively named [infotree](https://github.com/johncf/infotree). There are a bunch of benchmarks written for it, which you could try running them yourselves separately for those two ref counting types by changing [this line of code](https://github.com/johncf/infotree/blob/master/src/lib.rs#L27). On my laptop, the improvement in performance by changing that one line to use `Rc` is between 15-25% for various benchmarks. It's probably because of the fencing instructions which makes it slower even though it's singlethreaded?
The first is implemented here: https://github.com/ihrwein/backoff/blob/master/src/retry.rs#L82 The second point is missing though. If you don't want to create your own library, perhaps you can contribute it to here :)
No, because the borrow checker is actually correct here. Non-lexical lifetimes allow code patterns that are fundamentally correct but currently fail the borrow checker's limited rules. But in this case, the drop order has the reference dropping after the referent. The borrow checker doesn't actually change semantics, and making it change semantics here would be weird.
15-25% is still more than I thought intuitively but I probably wouldn't call it "waay faster".
I don't think I get it given the example shown. container is in the outer block, the temporary borrow happens at the if expression block, but because of the rule, the temporary lives for the duration of the outer block, right? Then, it should be reasonable that temporary can be dropped before container because they both drop in the same block? I guess I'm misunderstanding something.
The FCP tag is never cleared after merging or closing, sometimes leading to some confusion.
It's the style system used in Servo. They are bringing it over to Firefox together with Webrender.
https://wiki.mozilla.org/Quantum/Stylo
&gt; isn't true, they're scoped to crates and require explicit imports My understanding is that this is for bringing traits (and their methods) in scope. Given that you've imported a trait, can you use different implementations of a trait (edit: for the same type) in different parts of your crate? &gt; is a feature that is massively overhyped by the Scala community. It just really isn't that valuable, especially in a language without implicits I can't say that it is or is not massively overhyped by the Scala community: most of the people I know just accept that we have something like that and don't really talk about it, much less rave about it massively. That said, it does have its uses in Scala, for example, when you are writing different Tagless Final interpreters for the same functor and don't want all of them overlapping. Outside of HKTs ... ordering for a given type might be defined as a typeclass, but you might want different ordering in different places for the same type w/o having to write newtype wrappers/unwrappers. &gt; is there any value at all without implicits? Value can mean different things depending on the context, but I think it might be useful.
No worries at all. It looks like you are passionate about accessibility, which is something I respect and want to get better at. Thanks for pointing out the problem.
I have no doubt that stylo will be reused everywhere, like SVG content creators.
&gt; Isn't heap allocated memory, such as the Vec&lt;u8&gt; that stores the contents of the file, freed when the variable goes out of scope? Would that memory be freed on each iteration of the loop without the extra scope? Yes, each iteration of the loop is its own scope, so variables local to the iteration will be freed at the end of it.
I agree.
It will be behind a preference that’s off by default, at first.
I think that in some sense the functions in Rust can be thought of having two layers of blocks, so the container example desugars to: fn does_not_compile() { // outer { // inner let container = RefCell::new(0); if let Ok(_) = container.try_borrow() { } } // RefCell dropped here } // Ref dropped here Here, `try_borrow()` creates a temporary `Ref`. The whole `if let` is the last expression in the `inner` block, so due to the problematic rule, it's treated as if it was created in the `outer` block. That result in the drop order above.
Traits can be implemented differently on `&amp;Foo` and `&amp;mut Foo`. If you were calling a function which takes a generic argument, you'd be unable to pick which one you want to give it.
http://rustjobs.rs/ &lt;- check this out
More accurately, Stylo is the name of the project that takes servos existing style system and makes it possible for gecko to use too. It's not really a component thats moving from servo to gecko, it's being shared.
&gt; Also, this is my debut as a blogger so I have a lot to learn :) OK, I'll just say this briefly. In my experience, these types of short blog posts that are somewhat inflammatory almost never lead to good discussion. I use the word "inflammatory" here because you're implying that Ada has *failed* (asking the question suggests a reasonable answer might be "yes"), which is just patently not true. It has been used successfully, and implying that it has failed is more likely to elicit a strong negative reaction from anyone who has actually worked on Ada or invested their career into it. I'm hoping you don't intend for that to happen, but you're basically setting a very negative tone with your language. If you're interested in seeing how Rust might replace Ada, then I think it would be better to aggressively focus on concrete examples. Where is Ada is being used today, and what are the specific barriers Rust faces to being adopted in those environments? Or perhaps take a different tact: what new languages features could Ada add that would bring some of the benefits of Rust? Personally, I do think there is too little material on comparisons between Ada and Rust. It would be nice for someone that is an expert in both languages to do a detailed comparison between them.
To be clear, that snippet was taken from the part where I was pointing out differences that Scala devs might notice about Rust's handling of typeclasses. In the context of typeclasses, Scala devs understand "no scoping of implementations for a given type" to mean the ability to have and choose different typeclass instances for a given type via scoping. Rather than a claim that stands on its own against multiple interpretations, that snippet was a forewarning, like "hey Scala devs, you *might* miss this functionality in Rust". I suppose I can see how it might be interpreted differently though. Regarding the hype: I can't speak for every Scala person or community, but I haven't found "typeclass instance scoping" to be one of the things we overhype or really talk much about; I might have gotten lucky. Every language has its share of [magpies](https://blog.codinghorror.com/the-magpie-developer/)/[expert beginners](http://www.daedtech.com/how-developers-stop-learning-rise-of-the-expert-beginner/), but I assure you there are good Scala communities out there filled with nice, smart people who can adequately explain the good parts of Scala's type system to anyone with an open mind.
Why would they waste time trying to adapt it for gecko? It sounds like a better use of time to just work on getting servo itself into Firefox sooner and then you'll end up with it anyways.
I know this thread is somewhat old already, but could I still try to convince you to change your mind? It's just that I've seen a pattern repeated plenty of times in the past with many languages and IDE's: somebody puts a lot of effort to make a plugin for an IDE to handle a certain language, but over time the burden of maintaining it turns out to be too much and the quality of the plugin degrades as the language changes. And that's why I think RLS is so great: it makes it much easier to maintain an IDE plugin as the language changes: one person can write a patch for RLS instead of all IDE plugin authors having to do it themselves. But that's not the only benefit of RLS. Say person A writes a Rust plugin for IDE 1, and person B writes a Rust plugin for IDE 2. Person A cares mostly about Rust features X and Y, while person B cares mostly about Rust features Y and Z. If they each write their own plugin, then IDE 1 will probably have poor support for Rust feature Z, while IDE 2 will probably support Rust feature X poorly. But if both authors instead use RLS, and person A makes contributes to RLS to make sure it supports feature X while person B contributes patches to RLS to support feature Z, then both IDE's should support features X, Y, and Z very well. At least in theory. So why not use RLS and contribute to RLS the features that you'd like to use yourself? I know it would be more work to contribute to RLS than it would be to write it from scratch yourself, because you'd be working with an existing codebase rather than one that you've written all yourself. But on the other hand, if you do everything yourself, you may end up "reinventing the wheel" a bunch of times. Also, as someone who's written some patches for the Rust compiler I can tell you that doing semantic analysis on an AST is a lot more complex than it seems. Something as simple as code completion would require a lot of effort. If I write `x.` and you want to show a code completion popup, you'll have to figure out what the type of `x` is, which requires type inference, then if you find out that the type of `x` is a generic type `T`, you'll have to figure out what all the trait bounds of `T` are (and some of these, like supertraits, can be implicit), etc. I think if you want to provide accurate code completion, you might have to reimplement like half the compiler. Finally (I hope you're not getting tired of this way-too-long comment, but I do want to try to be as convincing as possible), you write &gt; I'm not convinced integrating the protocol into an IDE like KDevelop makes a lot of sense as it would effectively bypass almost everything in the IDE, save for the text editor. I think there's still plenty of stuff that the IDE can do that's out of scope for RLS, such as: debugging, cargo.toml editing, run configurations (debug/release mode, compiler arguments, ...), compiler output parsing (so you can provide clickable links in the compiler output), unit testing, profiling, and of course graphical interfaces for all the functionality RLS provides. These are more than enough to fill one summer of code. Don't take this the wrong way. Whatever you end up doing, it's awesome that you're going to make KDevelop support Rust, and whether you use RLS or write semantic analysis from scratch is in the end only an implementation detail. But I just think that while providing Rust support for KDevelop is *good* (great even), providing Rust support for KDevelop while also improving all other IDE's that use RLS is even better. Wow, this comment got way too long.
In the short term? Sure. But would you want to spend your life doing something that can be performed by computer? I imagine it would feel rather meaningless.
Could use * and *mut but that looks too close to raw pointers. I feel like that's the original wart: Putting * in the type name of a pointer. Makes no sense.
Not every component is ready yet, and this approach manages the risk of transitioning to rust. The piecemeal approach is also good PR for other teams who might want to try rust but would otherwise be unwilling to if it was an all-or-nothing decision.
Mozilla explains the idea and workflow of the [Quantum project](https://wiki.mozilla.org/Quantum) on their wiki. TLDR: Servo is a research project. Mozilla is focusing their resources on porting components from Servo into Firefox. This makes Firefox better incrementally.
Yep it works here: https://twitter.com/RustStatus/status/869203230152171520
That was the best explanation ever!! Thanks for taking your time to answer =)
Since Arc uses atomic instructions, it can be WAY slower, much slower than the mentioned 15-25% in multi threaded scenarios. 
Cool! Glad you liked it! :)
I believe the main reason is to retain behavior with all other Vec accessors: in general, out-of-bounds errors are panic by default. Not sure if there is a reason beyond that: then it'd be just the same reason that indexing or slicing out-of-bounds is a panic: there are many more cases where having the wrong index is a bug than cases when you want to remove an element at a specific position but recover the error case. With your specific example: I'd probably say this should be using VecDeque w/ pop_front(). There aren't convenience operators on Vec for this because removing an element from the front is an expensive operation by nature - it has to shift all other elements back one every time you remove one.
Sounds cool, I'll write up an example multiplexing the two and see if it's ergonomic feeling.
[Apparently not](https://www.thebeijinger.com/forum/2010/03/09/lady-female-version-lad-or-other-way-around-come-there-must-be-connection), they look alike but have a different etymology. In the thread someone suggests 'ladette' as the feminine of lad.
This is exactly the path for "Servo in Firefox". The web is huge and complicated and servo isn't going to support everything any time soon. Replacing gecko components piecemeal with relatively-finished servo ones is a good path forward.
&gt; Stylo is very much tied to Gecko. I had the impression that Stylo were used both by Servo and Gecko, abstracting between them. Is that inaccurate?
Well yes, but electron isn't suitable for games, really. Yes, it's "half a browser", but without the Javascript engine, only Rust, which makes it more safe to use. RAM and CPU usage of electron is way too high for games or realtime applications. Servo uses glium, IIRC. So more or less my goal would be a browser without JS. I can do my 3D graphics in glium or pison-gfx and then render Servos UI on top and directly get events from servo without any JS middleware. I know that embedding is not a target for Mozilla right now, maybe in a few years.
Yes, but Stylo isn't suitable for basically the same reasons? You want a decent rendering API. Stylo handles CSS. You don't want to design your games in CSS. You still need the DOM to do anything interesting with CSS. You're looking for a higher level Rust rendering API (Webrender maybe?). Stylo is not one. Iirc servo doesn't use glium.
Yes! Using a parser like this will require higher-kinded lifetimes (see `takes_parser`), but it's totally possible: pub trait Parser&lt;'a, T: 'a&gt; { fn parse(&amp;self, FilePos&lt;'a&gt;) -&gt; MyResult&lt;(FilePos&lt;'a&gt;, T)&gt;; } impl&lt;'a&gt; Parser&lt;'a, Token&lt;'a&gt;&gt; for WordToken { fn parse(&amp;self, file_pos: FilePos&lt;'a&gt;) -&gt; MyResult&lt;(FilePos&lt;'a&gt;, Token&lt;'a&gt;)&gt; { unimplemented!(); } } fn takes_parser&lt;T, P&gt;(parser: P) where P: for&lt;'a&gt; Parser&lt;'a, T&gt; { } (compiles on playpen: http://play.integer32.com/?gist=187c89629376e7d61f731e3741922d2d&amp;version=undefined) The `for&lt;'a&gt; Parser&lt;'a, T&gt;` syntax has `takes_parser` accept a parser that's valid for any lifetime - which I believe is what you want. It reads "for any lifetime 'a, this struct implements Parser&lt;'a, T&gt;".
I would recommend spawning a thread for the synchronous driver and communicating through asynchronous channels (available in `futures`) with the rest of the code. That said a REST API is more along the lines of HTTP. I think hyper's master branch currently uses tokio, so hopefully that will be out soon.
Right, so you also need a layout engine in there, and that's where things can get slow. Or you do fixed layout, but then you basically don't need CSS. Basically, getting rid of JS will not suddenly make it a much better solution than electron. When folks say "JS is slow" this is often about DOM manipulations slowing down layout and styling, which will be a problem here as well.
Sounds good. Thanks for the response.
Congratulations. The new async server looks great. BTW: It could be great to add an example that doesn't follow the request-response pattern. This is useful for chatservers and webgames, where the server sometimes sends a message that isn't direct response to a request. One idea is to add ~~a metronome that sends a tick every second.~~ something like this https://pastebin.com/H9McWLrH Source: https://users.rust-lang.org/t/solved-webchat-using-rust-websocket/4840/2
What's `rehash` in the instructions?
Hi! I think the issue here is that you have an `Arc&lt;Mutex&lt;World&gt;&gt;` and you're trying to store a `Mutex&lt;World&gt;`. Arc is a reference-counted pointer, and works only through the `Arc` struct. When you have an `Arc` struct though, you're only allowed to own at as an `Arc` (turning an `Arc&lt;Mutex&lt;World&gt;&gt;` into a `Mutex&lt;World&gt;` would remove all the information that it needs to be dropped at runtime). Technically the `*` here you're using on `Arc` is calling https://doc.rust-lang.org/std/ops/trait.Deref.html#tymethod.deref. This trait lets you use Arc as if it were the underlying type, but it can only operate on borrows (for Arc it's defined `fn deref(&amp;self) -&gt; &amp;T`). This is what's giving the cryptic error message (if it didn't have a borrow error, it would then give you a type error since you can't deref without &amp; like `&amp;*c`). To solve this, `Arc&lt;Mutex&lt;World&gt;&gt;` could just be stored in `EchoService` instead of `Mutex&lt;World&gt;` (just `pub world: Arc&lt;Mutex&lt;World&gt;&gt;`) Now, that gives us one more error: expected a closure that implements the `Fn` trait, but this closure only implements `FnOnce` This one basically means you are totally consuming something in the closure, while `server.serve` requires that it can be called any number of times. Replacing `world: c` with `world: c.clone()` will let the function just borrow `c`, and should work correctly.
Shells keep a cache mapping program names to the executable's path; `rehash` (or for Bash, `hash -r`) tells it to flush the cache and traverse `$PATH` again to find newly-installed programs.
Sorry for being late...had a bit of a connectivity problem today. I'm swamped with $work, so I probably won't get anything done apart from TWiR stuff.
Thank you /u/burntsushi for your valuable feedback! Let me explain myself: English is not my native language so I've a lot to learn to express my thoughts better. Also, my Internet "marketing" skills could be improved (title could be perceived as a form of clickbait). No, my intention was not to start a flamewar but rather to simply describe, IMHO, main pain points of Ada - issues that stop the mainstream adoption. Well, in my opinion Ada is one of the best languages out there and it COULD be one of the most popular general-purpose languages (instead of the one that appeared in 1995 - which is also a great language / platform). I'm not an expert in either Ada or in Rust but I will try to come up with more detailed comparison. In the end - languages don't matter and I think we both want our communities (Ada and Rust) to grow!
I [blogged](https://llogiq.github.io/2015/07/10/cow-redux.html) a humorous take on this some time ago.
I transitively used this crate via slack-rs when writing a toy chatbot. It worked just fine for my needs so far, though I do recall an issue about [not warning of a dead websocket](https://github.com/slack-rs/slack-rs/pull/69). My only complaint was that development had stagnated and was transitively keeping me on an old SSL version for a long time, resulting in forks like websocket-vi specifically due to that inactivity. But then the slack lib moved to [tungstenite](https://github.com/snapview/tungstenite-rs) (in part because of that stagnation) and I forgot all about it. So, I guess moving forward, why would I use this crate with tokio over tungstenite, which also supports async uses? I haven't directly worked with either crate, so is there a significant difference in API/performance/whatever? Does it come down to whether or not I already have/want/need to integrate with tokio?
object safety is harder, but you can prove any local type does not implement any trait using the negative reason trick: https://is.gd/91eM7y it only works for Send and Sync if you have an explicit negative impl, which is interesting..
Well, it's not very easy to provide a choice between `Arc` and `Rc` to an end user who is going to be indirectly affected by internal design choices of a library. That was the primary reason behind making this post -- to see whether people have a better solution than what I currently have. _UPDATE:_ Looks like someone do have a [better solution](https://www.reddit.com/r/rust/comments/6dz0xh/_/di6wvk9/).
This is great! Thanks a lot for this example! I'll update the readme to point to this.
I haven't used tungstenite myself and I'm not looking to put down someone else's work, but I do think they are significantly different, for better or worse. I would be interested to hear more about your issue, I haven't come across it yet.
Yes, it is quite cumbersome to try to make a "thread safe" version of some data structure and then a "normal" version. This is sort of why, for example, the linked list in rust's std lib is not the best (it uses Shared rather than Arc for speed, but this limits functionality a lot). 
I created my first rust crate, [query_param_group](https://crates.io/crates/query_param_group)! It lets you separate query params into separate subunits in rocket routes, e.g. `QueryParamGroup&lt;(LimitOffsetParam, RouteSpecificParams)&gt;`. Still needs tests, though.
Sorry for the very abstract question, but I'm not sure where else I can really find a good and up-to-date answer for this. Tokio is obviously a very popular and quickly developing project within the Rust ecosystem. My understanding is that as it stabilizes are becomes more widespread, we can probably expect most libraries that have APIs that block on synchronous IO to be rebuilt to use it. Take for example a Hyper, Diesel, or Rusoto. I want to write more Rust code, but I don't really want to write a program that's going to be a jumble of code relying on legacy APIs before I even know it, and with Rust I always have the feeling that I'm building on unstable ground. So my question is, how do you think about building software based on libraries that may have pretty significantly rebuilt their APIs by this time a year from now? Am I making some bad assumptions here, like are existing non-Tokio APIs expected to be supported in near perpetuity already? Or is the conversion to Tokio expected to be trivial? Thanks! 
Aside from the other replies, which are excellent, I'd also add that this gives us real world battle testing for Servo's style system. This has already paid off: for example, Bobby Holley found and fixed [a big problem](https://github.com/servo/servo/pull/16971) that was killing our parallelism for documents that didn't have large DOM node fanout. Without the real world testing that Gecko integration provided, we might not have noticed this for a long time.
I'm planing to expand SucreDB test suite to include more cluster tests. SucreDB is the distributed KV database I've been working on for a while, https://github.com/arthurprs/sucredb
&gt; Yes, it's "half a browser", but without the Javascript engine, only Rust You can’t cut Gecko or Servo in half like that and still get something that runs. For example Servo’s DOM (the data structures for the content tree) are tied to SpiderMonkey (the JavaScript VM): https://research.mozilla.org/2014/08/26/javascript-servos-only-garbage-collector/
I tried it and it's much, much faster than python. Thank you very much, it'll be handy to me!
I continued to port my web browser [titanium](https://github.com/antoyo/titanium) to [relm](https://github.com/antoyo/relm), an asynchronous GUI library based on GTK+ and futures. I faced some issue with the RefCell, so I decided to removed the ones that are not necessary (they were added for convenience). In doing so, I had some challenges with the glib main loop. I'm still not finished with this issue, but hopefully I'll find a solution soon. Next week, I'll continue to fix the design of relm, related to the aforementioned issue.
Thanks, it was a really interesting read. I heard about this experiment once, but didn't know about the blog. I mentioned jitter in the README in the meantime.
I believe it's possible, but maybe it doesn't worth the tradeoffs. I use std::time::Duration and Instant, but they aren't available in libcore. And there is ::std::thread::sleep, too.
Congratulations on your crate, but could you elaborate how it is more correct than core::fmt?
Actually not rust but Python. Im working on a discord bot to make events, keep track of them, and to take attendance of who will be going to the event. It's not overly complicated. I got the event making part done, now i need to finish the attendance part.
I'm clearing out some long-standing bug fix PRs in [Rusoto](https://github.com/rusoto/rusoto). After that I'll be able to take a look at the [PR for adding streaming support](https://github.com/rusoto/rusoto/pull/627) which is important for big files from S3. Also on deck: respond to feedback from my RustConf talk submission and write a blog post on how Rusoto generates Rust code from the JSON botocore AWS API service definitions.
Thanks for reporting! I've pushed the fix to repo but I'm unsure if it's correct. Can you look at it please? I'll publish new version if it is.
Ah, neat website, thanks!
maybe they could hire out of their 60 million dollar marketing budget instead of destaffing firefox
OMG yes! I can finally put together my X11 web app without having to use two separate servers/ports!
Indeed it is right there. My apologies, I must have missed it. I guess I only looked at the documentation and didn't spot it there. 
&gt; I must have missed it You are not the only one; I missed it too. When dealing with libraries I usually expect the whole documentation to be available in the rustdoc output / docs.rs. Makes me wish there weren't two places one has to look at.
And the ruby community likes jap names, because the creator of ruby is japanese, maybe?
You don't need another extension to enable breakpoints in non-C/C++ files. Just open your config and add: "debug.allowBreakpointsEverywhere": true
I tested your crate on a microcontroller and your crate is faster than core::fmt but it seems like it's slower than doing the formatting yourself using [numtoa](https://crates.io/crates/numtoa). This is the operation I tested: `println!("{}, {}", -32768i16, 32767i16)`. Here are the numbers: - No formatting: `Port.write_str("-32768, 32767\n")` - 137 clock cycles - numtoa: `Port.write_str(&amp;numtoa_formatted_buffer)` - 319 cycles - fast_fmt: `fwrite!(&amp;mut Port, -32768i16, ", ", 32767i16, "\n")` - 574 cycles - core::fmt: `writeln!(&amp;mut Port, "{}, {}", -32768i16, 32767i16)` - 797 cycles As `i16` had no `fast_fmt::Write` implementation I implemented it myself using numtoa. So both the "fast_fmt" and the "numtoa" measurement above used to numtoa to do the formatting but they performed the operations in different order. "fast_fmt" looked like this: format -32768i16 into a buffer, write that buffer to Port, write ", " to Port, format 32767i16 into a buffer, write that buffer to Port, write "\n" to Port. where "numoa" looked like this: format -32768i16 into a big buffer, write ", " to the big buffer, format 32767i16 into the big buffer, write "\n" to the big buffer, serialize the big buffer through Port. It's important to note that Port.write_str has a fast path when the input is bigger than 7 bytes. In that case it can serialize the input in *words* (32-bit chunks). OTOH, if the input is small then it serializes the input in bytes. In the case of the "fast_fmt" measurement the fast path was never hit; whereas in the case of the "numtoa" measurement the single Port.write_str operation was done using the fast path. To do more fair comparison between these approaches I think a `DevNull` writer should be used. That writer implementation would do a "no-op" in its write_str and write_char implementation. That way you would directly be measuring the formatting overhead (zero I/O overhead).
It can't be; you can't transmute between generic types like that because the compiler has to be able to prove whether the code works or not *without* knowing what `A` and `B` are. That trick would work in C++, but not in Rust.
Ok, then make it a macro... :( What we need is an auto trait `A : SameSize&lt;B&gt;`
OK, just wanted to make sure :) I should make sure that the macro interface does not require std though...
Whoa!
May I ask what kind of product you built with Rust?
Ah good point. I think I got confused between `Arc`'s use of the heap and `Mutex`'s `Box&lt;sys::Mutex&gt;` field. `Vec` is definitely a better example regardless. Thanks for the heads up! Fixed.
Variance is another tricky one. It's possible to check it for a single definition https://is.gd/BFyh74 , but I don't know how one would define a nice reusable way of doing so, even with a macro. Should these be in the standard library / language ? Something like #[assert(Sync, Send, covariant(X), contravariant(Y))] struct Foo&lt;X, Y&gt; { _phantomX: PhantomData&lt;X&gt;, _phantomY: PhantomData&lt;fn(Y)&gt;, } EDIT: these macros seem to work for types with a single type argument : https://is.gd/uk6W0w. Error messages are terrible though
I don't think so, but it is definitely not easy to implement in the compiler
What would you like to have seen instead? Curious
FWIW, the transmute in the parent is also safe.
Putting `mut` breaks the construction/deconstruction symmetry, just like `ref` and `ref mut`. (Maybe having only one case of breakage is significantly better than two, but IMO it's unlikely to be worth deprecating syntax/having two ways to write it, especially when it is replacing moderately Google-able keywords with symbols.)
Does it produce comprehensible error messages?
This sounds awesome! To ease installation (which for this project is rather complex), this should really use Flatpak or something similar.
OK? I've published fixed version. I agree this whole `no_std` story is imperfect. I totally forgot that `void` uses `std` by default and I didn't realize that `--no-default-features` doesn't turn off default features of dependencies. I consider myself "no_std developer" too although I didn't develop anything `no_std` for a while but I certainly plan to do it again in the future.
Do you think it'd be practical to add support for "operating system provided" websockets to that library? In particular, I'm thinking about Rust code running in WebAssembly, where the lowlevel primitives to implement WebSockets are absent (no raw TCP, no servers yet), but the high-level JavaScript libraries (and possible future native WebAssembly versions thereof) could be wrapped into an implementation that provides the same traits as a Rust websocket. An example (but providing a bespoke unpolished API rather than a standardizable trait) is at https://github.com/rust-webplatform/rust-webplatform/issues/23 .
Ported my dotfile manager, [dotter](http://www.github.com/SuperCuber/dotter) from Python to Rust. FeelsGoodMan, please do PR if you see any ugly code, I'm still learning!
Exams phase (my last one ever if I don't fail anything) so I guess you'll all understand that I won't have much time to work on [imag](https://imag-pim.org), sadly. Also my other crates won't get much attention in this time. I hope I can do more after the exams phase.
&gt; How often do you use get in order to know whether the indices you feed it are in bounds? Always. Every time I need to know whether the indices I feed in are in bounds, I use `get` instead of indexing via brackets, that's what it's for.
Note, this is 83% written in C.
Quick check since you're a new rustacean: Are you compiling in ''--release'' (optimized) mode?
Yes, I'm compiling in release :)
It would be interesting to compare it against https://github.com/housleyjk/ws-rs which is directly on top of mio.
I can't find the visualizers in the Rust toolchain folder. Specifically the `\etc` folder does not exist. Is this because I'm using stable msvc 64bit and the visualizer is only available in nightly? It's great that this stuff is written down in an easily comprehensible blog post, I hope it gets more visibility.
image::open actually reads the whole image to memory, so that might not be the fastest thing in the world.
You might like /r/playrust
OTOH reading the source of gdk-pixbuf, assuming OP is using `gdk_pixbuf_new_from_file` or something like that it also looks to be loading the image data eagerly.
Now that [rocket](https://github.com/SergioBenitez/Rocket) is a thing in Rust, I got really confused by the title here ...
Could this engine be used to create reproducible TeX environments? This would be a killer feature. To clarify, some way to express that *this* document requires such and such versions of such and such packages. I have too many old (ish) LaTeX documents that don't compile with current TeXLive. Edit: formatting
I know. It just happened to me that I needed some simple HTTP server. python's SimpleHTTPServer is exactly what I need except it's awfully slow. I don't need it to be very fast. I need it to not be too slow.
If you don't mind writing your own checks, here's how Python tests whether a file is an image. https://hg.python.org/cpython/file/3.6/Lib/imghdr.py
Loot whores considered `unsafe`
Finally pushed `cbrt`/`recip`/`sqrt` implementations for [`uom`](https://github.com/iliekturtles/uom) (type-safe zero-cost dimensional analysis). While simplifying one of the internal macros found an ugly bug that would cause multiply/divide operations to have the wrong value the values involved had different base units and dimensions! I've also been working on making tests run for both `f32` and `f64` instead of just `f64`. This week I'll likely release an update on `crates.io`. I'm also looking for any crates that use physical quantities as raw values. I want to experiment converting to uom to ensure that what I'm building has a easy-to-use API.
How does your library compare with other statistics libraries?
Thanks for the writeup! Is there a way to enable stepping into std? I have the rustup component rust-src installed but the debugger presumably tries to find the sources in the directory they were in on the builder. That leads to errors like: Unable to open 'str.rs': File not found (c:\projects\rust\src\libcollections\str.rs).
Your solution fails to work when I try to implement `Clone` for `Hello` (which is needed for my use-case). See https://play.rust-lang.org/?gist=ad6ccd39c23e023fc4732709c1895e49 Do you think there a workaround for this too?
You're likely running into this RLS issue: https://github.com/rust-lang-nursery/rls/issues/227 EDIT: And if so, please leave a comment on the issue saying you want it to work, as currently they're planning on disabling goto-def'ing on stdlib source altogether &amp;ndash; I **can't** be the only one who finds this arbitrary and limiting... I _do actually_ want to goto-def on stdlib source; why is this controversial in the least??
Wouldn't `filter` make more sense? OP is just printing the input file path if the file is an image and at least 16x16. And maybe something like a `fold` to serialise the result and print it to a pre-locked stdout? Collecting to a new vec seems like a waste as it means no way to produce output on the fly during the run.
It doesn't need to be difficult to get tiresome. Is it that hard to read `fn`? ;-]
I am exactly of this philosophy so Tectonic is very careful NOT to require ANY network access for builds if all of the needed resource files have been cached locally. And because the I/O backends are pluggable, you can download the "bundle" file containing the install tree and point Tectonic to use it for fully network-free operation.
It's unnecessarily verbose. Also see: const, mod, impl, pun, enum, struct, …
Impressive. This is your third reddit submission, and the first two were *also* about the Rust game and *also* posted in the wrong subreddit.
I wrote an [RFC](https://github.com/rust-lang/rfcs/pull/1990) you might be interested in that allows for external docs to be included at compile time. This would allow things like README files being in the crate root and only having to be written in one place.
The hope is to move towards full Rust, but it will take a while — there are ~120,000 lines of very messily interconnected C/C++ code. Just to clarify, though, I wouldn't call Tectonic a "driver" for XeTeX — it's a fork. The hope is to track changes made upstream as closely as possible, although the projects will necessarily diverge. There's a separate "staging" repo that tracks and can update the underlying C code that was used as a base, so changes in XeTeX can be monitored and imported.
It has to go to the definition to let you see the source in order to step through it. I assume goto-def is the same for debugging as for editing, but I could be wrong.
looks like you want to use [mspc](https://doc.rust-lang.org/std/sync/mpsc/) 
* Pretty much identical for now; hope is to improve the most common ones * No * Caches them; once they are downloaded, recompiles do not touch the network at all * Not planning on it; I tend to think that sort of functionality is beyond the scope of the core engine.
I would love to gather more data about what kind of installation methods to promote and/or packages to build. Comments on [the relevant GitHub issue](https://github.com/tectonic-typesetting/tectonic/issues/4) are more than welcome!
Should a lib crate panic? For example, `put_pixel` in `image` crate panics if pixel is out of bounds: http://www.piston.rs/image/image/struct.ImageBuffer.html#method.put_pixel It seems weird to me, cause I though I can avoid many runtime errors with Rust. But I can't avoid runtime errors with a lib that could panic (I have to check by myself in my code, but maybe I'm not aware some functions I use may panic). So I thought that a lib must not panic. And always return things like Results (and the user of the lib can choose to unwrap (risky) or not). But maybe I missed something. Side note: I really appreciate `image` crate ❤️
I'm pretty much in the same boat :(.
&gt; it might just be best if there was a way to access to mutex's contents without locking Uh, that's exactly the wrong solution. The mutex is there to protect your data, so you need to use it the correct way to get the memory safety guarantees. Instead of spawning a thread, can you just put the event checking and the user input checking on the same event loop? I assume `discord-rs` uses `tokio`, so is there some way to use that to get user input?
It is not unusual in situations like this one (bound checks). Say, `Vec` itself panics on an out-of-bounds indexing operation. Though I'd say it's always the best to provide an alternative, `Option`/`Result` returning method (think [`Vec::get`](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.get)).
Another idea: inside the for_each write to a [mpsc queue](https://doc.rust-lang.org/std/sync/mpsc/), and have another thread read from this queue and print to stdout.
~~No `tokio` in their Cargo.toml.~~ They are indirectly using tokio, yep Anyways, how is `std::io::Stdin::read_line` going to work concurrently without threads?
Yeah the API is still developing. I've been more focused on feature implementation but hopefully soon I'll have the time to do a review of all the interfaces and fix/improve some of them including adding support for other return types and maybe a `checked_` interface as well
Your keyboard doesn't have a `fn` key on it? Mine totally does. oO
wrong subreddit bro 
Early in the development of rust, there was a belief that keywords needed to be short, concise, and no more than 4 characters. This was re-decided (see `trait`), but a lot of it just 'stuck'. In the end, it doesn't really make much difference, as long as it's all consistent.
You'd have to find some way to use stdin with `tokio`. [Here's the issue on mio about it](https://github.com/carllerche/mio/issues/321), and the first reply linked an example using blocking I/O, and since tokio is built on `mio` you can probably do something similar.
Another advantage of these shortcuts is that `function` is still available as a variable name. I sometimes wish `type` were `ty` instead.
Comments, PRs, issues, etc are all welcome! It definitely needs some more work, I'm noticing little (and not so little) things constantly. Like I'm pretty sure the trait is overkill and redundant with `io::Write`, but traits are how I usually start new code and I haven't cleaned that up yet. Before publishing, I rewrote it to use buffers instead of just outputting strings directly, which greatly improved performance. Any other tips for optimizing memory and CPU would be great.
&gt; Should a lib crate panic? In general, if the error is unrecoverable, then you should panic. If it's recoverable, then you should not. The bar is a bit higher for libraries; you may not know if an error is unrecoverable or not, and panicing removes choice from your caller. &gt; For example, put_pixel in image crate panics if pixel is out of bounds: This is a pretty classic case of where a panic is appropriate; I second /u/sjustinas below that a second method would be good as well.
It's great that you added support for categorical distributions. I was using statrs already but had to write my own methods for generating random numbers based on a given pdf. I find it a little peculiar that when generating samples for categorical distributions, you're using unsafe and get_unchecked() to bypass the bounds checks on array access, so you clearly care a lot about efficiency, but then you also use linear search on the cdf instead of binary search. ;)
Could someone please explain the "layers" of a modern TeX setup, and how they work? My current understanding is: 1. At the base is Knuth's original Pascal program (or, well, WEB, due to his [literate programming](https://en.wikipedia.org/wiki/Literate_programming) schtick). 2. This is then translated into C using `web2c`. 3. Modern engines (pdfTeX, LuaTeX, XeTeX, and now Tectonic) are wired up to the resulting C code base. I'm quite confused how that last step works, though, since many engines seem to make some significant changes "under the hood" of TeX to add Unicode support, modern font support, font expansion and other `microtype` features, etc. And what does it mean to say that Tectonic is "powered by XeTeX"? Is it a fourth layer on top of the previous three? 
Well, I answered this very question right above: https://www.reddit.com/r/rust/comments/6e151u/why_doesnt_vecremove_return_an_option/di6xk0e/. TL;DR: because for most methods the failure would be silent, because the result would never be checked.
The way XeTeX is built is: 1. There's the original WEB source of TeX. 2. This is merged with "change files" using a custom patching system. 3. The resulting patched WEB file is then translated to Pascal, with custom fixes. 4. The resulting Pascal code is then translated to C, with custom fixes. 5. This is all then compiled in conjunction with extra C/C++ into a single binary. You can think of these are "layers" but what goes into the final executable is simply a lot of C code; the previous steps are all preprocessing. As such, if you add a new function written in C, you can "call" it in WEB because all of the prior steps are just text manipulations. (More or less.) Tectonic is based on a snapshot of the C code that you get from steps 1-4. I say "powered by XeTeX" to try to give credit to the very large amount of work done by the XeTeX authors to build the system; but a more to-the-point description would be that it's a fork of XeTeX.
This is pretty cool! The code is nice and short. let mut c = 10; Protip: let mut c = b'\n'; :-)
Yeah but you have _both_ in this case. One that panics and one that returns an Option. So why not have both in all the other cases? I think it's silly, like I said I scarcely write or encounter code that needs to perform a bounds check unless it's user input. The index is typically generated by some algorithm and it's already in bounds; if it's not in bounds the algorithm is wrong. Same reason you rarely when accessing str slices check and branch on whether the indexes lie on utf8 boundaries. Whatever algorithm generated your bounds should ensure it.
Not OP, but I'll be building a game server with UDP, TCP and WebSockets (have UDP and TCP working now) where a message from one client could be repeated across all sockets. I think an example here would be awesome. I was considering kludging something with HTTP/2, but this now makes WebSockets an option.
There is a Rust program called `watchexec` that makes it super easy to set up file watching that runs a build command on any changes, including the ability to interrupt running builds and restart.
How to write a code example? Is there guidelines for this? I wrote some example in various crates (mentored) and the rules seems to be slightly different for each. It's just a feeling, sorry if I'm vague. In some examples​, I created a simple main function, in other I used a `run` method returning `Ok(())` as a result. I think my examples could be better if I could read something about the rules (I'm not speaking about content but style)
That isn't mentioned anywhere on [the project's installation page.](https://tectonic-typesetting.github.io/en-US/install.html) The installation method the page does mention is far more onerous.
Why not the newer and more extensible LuaLaTeX?
Yep, LuaTeX *is* the clean, modern version. It should have been the starting point :( Lua is completely optional for the user, it's just the saner scripting language for some kinds of macros than TeX itself
You can already do code hot reloading in rust. https://github.com/Lokathor/hotload-win32-rs is a Win32 example, and the readme links to mac and linux versions. I expect that a cross platform version would not be immensely difficult to abstract out if one was determined.
Yes there are plans to integrate [cretonne](https://github.com/stoklund/cretonne) once its ready, which is a JIT written in Rust.
Well today my libc PR ([#599](https://github.com/rust-lang/libc/pull/599)) got merged. So I'll be removing the definitions of the structs I used from tarpaulin. I'll also likely submit a PR for rust-nix as I'm having to use unsafe blocks with ptrace to handle certain events. Work continues on [tarpaulin](https://github.com/xd009642/tarpaulin). Made good progress last week in tracking down an issue and nearly resolving it, just about to tie up the last bit (high program counts in DWARF may be a greater value than the actual highest PC in a function). After that it's testing to track down more bugs, and hopefully onwards to decision/condition coverage!
I've never done this but it looks cool. I'll add it to the wishlist for now until I look into it more (I'm at my work orientation rn). Next step is add compression, which is coming along.
For Linux and macOS versions, have a look here: https://www.reddit.com/r/rust_gamedev/comments/6bbhep/code_hot_reloading/
To be fair, LuaTeX has some of its own warts. Apparently some of the font handling is done using code borrowed from FontForge, whereas XeTeX uses Fontconfig. The latter seems to handle font naming much better, and generally plays nicer with one's system (since on *nix, it's what your whole system is using for font configuration, lookup, and substitutions). For example, say I'm using Adobe Garamond Pro and want to use its _semibold_ weight for bold type in my document. In XeTeX that's as simple as: \setmainfont[Ligatures=TeX, ..., BoldFont={* Semibold}, BoldItalicFont={* Semibold Italic}] {Adobe Garamond Pro} But in LuaTeX, the font loading and caching relies more heavily on font files' actual names, so I'm forced to do something like \setmainfont[Ligatures=TeX, ..., BoldFont=AGaramond Pro Semibold, BoldItalicFont=AGaramond Pro Semibold Italic] {Adobe Garamond Pro} or rename the font files on my system just to make LuaTeX happy.
Just say "Japanese names". "jap" is a slur. (ETA: at least in American English.)
Sorry. In my defense, non native speaker of English.
Best of luck! :D I just passed the other week and am also done!
This means: 1. `?` will be able to be used with `Option&lt;T&gt;` as well as `Result&lt;T, E&gt;`. 2. A new trait, `Try`, will allow you to write your own types that work with `?`.
Mio is based on events that appear on a file descriptor (FD) ( in linux terms) a network socket is a FD , stdin is a FD , stdout , stderr , and opening a file will give you a FD. So if you want mio to react to your stdin you can probably register its FD. I tried something similar with child processes ( which can expose 3 new fd's with stdio::piped ) but it took me quite a while to figure out. 
From the book: 1. first edition:https://doc.rust-lang.org/stable/book/trait-objects.html#object-safety 2. second edition: https://doc.rust-lang.org/nightly/book/second-edition/ch17-02-trait-objects.html
`?` still won't work for a result in an iterator, right? (`Err(x)` -&gt; `Some(Err(x))`)
TIL, Thanks! I will updated the post.
Aha! Still wonder if it would support Cursive, but... this is probably the best solution for pure text interfaces. Cheers!
You might also be interested in https://github.com/aepsil0n/carboxyl it allows functional reactive programming with rust
The goal of integrated HTML output is very attractive to me. Trying to output HTML from XeLaTeX has been a nightmare for me. This is a large part of why I end up doing things in Markdown, but for a final PDF output I end up having to export tex files and refine those by hand. If I could have a single system to do seamless, high-quality PDF and HTML output I'd be very happy.
Not exactly sure how that could help me either
This is great news! Being able to use `?` on `Option`s is something I've wanted for awhile.
I've had a real world case for this sort of thing before, not for testing. Generating fake data on a network.
It looks like you support performing the cropping on multiple files at once. You could probably trivially implement this with rayon and get parallelization, since nothing's really shared data there. I wonder why saving is slower in rust - possibly not using buffers properly somewhere? I feel like this is not the first time I've seen rust IO slower than Python IO.
Ha, I was just thinking yesterday that I was sad that I was so preoccupied with the three-day weekend that I forgot to submit a talk. :P Let's see if I can remember to do so now...
This seems strange to me. Why does this use case warrant the creation of an entirely new method? Isn't the most conventional solution just to do a bounds check before indexing, `if (index &gt;= a.length) {...}`? 
Can you explain what the larger problem you're trying to solve is? Using a hard-coded address to a function is a really bad idea on many levels, and it probably isn't the solution you actually want, so understanding the real problem being solved means we can be more helpful.
This somewhat messy profile that I did locally suggests that you might be bottlenecking on image decoding: https://perfht.ml/2qxtwU5 You shouldn't need to decode the whole image just to get the size of it. If image doesn't support this functionality it probably should (especially because servo's using it).
why were you generating fake data though?
For the record, this is the way to convert a function to an integer and back: fn do_something(x: i32) -&gt; i32 { println!("squaring {}", x); x * x } fn main() { let f = do_something as fn(i32) -&gt; i32 as usize; println!("{:#x}", f); unsafe { let f = std::mem::transmute::&lt;usize, fn(i32) -&gt; i32&gt;(f); f(42); } } but as coder543 points out, this is unlikely to be what you want - it is dangerous and unsafe as illustrated by the `unsafe` block.
I'm all for it if you're willing. We can probably run some basic benchmarks and see but my hunch is binary search will be faster than linear regardless of bounds checking as long as the number of categories is sufficiently large
To trick attackers.
Finally. I've been watching the repo hoping this would get merged at some point.
I think this is not quite right: there is no step of translating WEB code into Pascal (as `tangle` would do) and then the Pascal into C; instead `web2c` goes directly from the WEB source to C. As I understand it (from having built TeX locally to experiment with): 1. There is Knuth's original program `tex.web`, written in WEB (roughly: a literate and better Pascal). This is translated into C using `web2c` to produce the `tex` binary. 2. For other extensions like e-TeX, pdfTeX and XeTeX, there are either change files (`.ch` -- they're like patch or diff files) or directly patched `.web` files: for example `pdftex.web` and `xetex.web`. These are translated into C using `web2c` to produce binaries like `pdftex` and `xetex`. 3. LuaTeX is independent: it's written in C, starting with a hand-translated version of Knuth's TeX program. (See [this article](https://www.tug.org/TUGboat/tb30-3/tb96hoekwater-pascal.pdf).) It's simply compiled into binaries like `luatex` and `lualatex`. There's an additional step of dumping each commonly used macro package (like plain TeX and LaTeX) into a "format" file that is preloaded by the *tex binary.
That's the dream! I have been working toward this goal for a while and have become convinced that it will be very difficult to achieve. But it will be worth the work. It's ridiculous to me that scientific papers still look better in their PDF incarnations than in HTML. If we can make it relatively easy to author and render interactive math- and reference- heavy documents on the fly with high-quality typography, I think that could really change the nature of 21st century scientific communication. Not to get too ambitious, you know.
Under the hood, `web2c` goes through a Pascal intermediate step; see the code [here](https://github.com/tectonic-typesetting/tectonic-staging) for a reproduction of the process used.
I hope syntax highlighters make those questions mark bright and bold. It's a useful operator, but it's small and easy to miss. I really like being able to spot the exit points of my functions at glance.
You can `collect()` into a `Result&lt;Vec&lt;T&gt;, E&gt;` which would have a vector of the results or an error. This could be added implicitly into the `?` operator but it would not work as you expect it too. It short circuits (stops at the first error and returns that). There's also the thing that iterators are lazy, that is you want to do error checking at the very end, while `?` (or `try!` for that matter) is eager, and you want to do checking as soon as possible. If you put a `?` between two iterator steps you'd actually be preventing effective collection. What makes sense is to simply push foward the `Result` until the very end, when you do the `collect` it'll be smart enough to short-circuit (that is stop early in case of any result). If there are multiple steps you want to do on members of an iterator, then you can use `?` on the functions mapped on the iterator to skip all remaining steps in case of a failure.
I was talking about creating an iterator, not consuming one, but your points are also valid.
&gt; LuaTeX is independent: it's written in C, starting with a hand-translated version of Knuth's TeX program. Holy crap. That must have been an insane amount of work!
This is exciting. Anything that brings user defined types into a realm of near-primitives by extending first-class features is a huge win for the language. 
That's the plan. ;)
&gt; I don't want to use a `move` closure, because commands needs to be accessed later to perform a lookup of the vec whenever a command is invoked. Well, if you've decided for `CommandFn` to be `+ 'static`, then you have to use the `move` closure. `clone` looks quite ugly, but it's rather idiomatic to do it. If you don't want to clone the whole `Vec`, you can use the `Rc` pointer: let commands = Rc::new(commands); ... .on_run({ let commands = commands.clone(); move |discord, message| { ... } }) You can also put the `clone` call somewhere outside the `.on_run()` call, but then you'd need to invent some new variable name. &gt; I used to have lifetimes implemented so that an `on_run` closure only lived as long as the command that contained it, but I removed them in favor of the `'static` lifetime while debugging, for the commands themselves live throughout the entire program, so a reference to their `on_run` behavior should, too. The question is, what does "for the whole program" mean here. If your program ends while the `commands` variable is still alive, you can savely put the lifetimes back (although I'm not sure if it's worth it).
Macros need to expand to some AST node and it seems that one branch of the `match` is not an AST node (or maybe it is, but it's not macro-enabled). The rationale for this is that it makes the parser simpler and also makes the macro simpler to analyse for humans. Anyway, in your case it's not a big issue, as the macro could be rewritten to cover the whole match statement, so the usage would look like: match8!{x, cpu, rotate_left_carry, 0; rotate_right_carry, 8; .... }
Try it on nightly, every time I find a bug on stable it's already fixed here.
Sounds like you're blowing the stack with some absurdly huge type (like an array). IIRC, Rust doesn't have stack probes on linux, so it'll just segfault if you go too far.
Yes, though it requires / provides slightly different methods
&gt; But rust macros aren't really true macros, more like semi-functions Assembly and C provide text-substitution macros while Rust has a more advanced "syntactic" macro system. Syntactic macros have been been around for decades in languages like Prolog and the Lisp family and nobody credible disputes that they're really macros. As long as it's a "mapping from a convenient input to a less convenient output", it's a macro. There's no such thing as a "true" macro. Heck, if you talk to enough computer scientists, I'm sure some will hold an opinion opposite to yours and tell you that C-style macros don't qualify as "true" macros for reasons similar to an assembler not qualifying as a compiler. (too primitive/simplistic.)
`extern fn` defaults to `extern "C" fn` :)
I have improved the numbers. Fist I tweaked my write_str implementation to make it faster but I actually worsened most benchmarks. Here are the numbers after that modification: - no_fmt: 47 cycles - numtoa: 319 cycles - fast_fmt: 651 cycles - core::fmt: 829 cycles Then I tweaked the Fmt implementation of i16. I replaced some internal buffers that were initialized to zeros with mem::uninitialized and that made a big difference and the fast_fmt version now only takes 361 cycles, which is close to the numtoa version. I'll send you a PR with the Fmt implementations for integers. &gt; Could you try also serializing the number directly using write_char I didn't test this but I expect it to be rather slow. The thing is that my `Port` thing is fastest when working with `[u8]` buffers that are *4 byte aligned* because then it can serialize the buffer in 32-bit chunks. This actually makes numtoa a bad fit for formatting through this `Port` because numtoa formats "to the right": for example if you pass it a `[u8; 16]` buffer the number may end at `&amp;buffer[11..]` so it doesn't matter if your initial buffer was 4 byte aligned because there's no compile time guarantee that the formatted number is in a 4 byte aligned slice. &gt; Edit: you should probably use LTO (in case you forgot). I had LTO enabled. It's a must when working with microcontrollers :-). EDIT: Formatting
I've had this exact issue. Allocating frame buffers on the stack is not feasible - that's the lesson I took away from my experience.
I speak as a primary user of LaTeX. I would rather have dead simple explanations like "you forgot X" or "you are occluding Y". LaTeX knows all of these things but it isn't putting them together. Instead it's just throwing numbers, brackets, and Tex style lingo at me. I know that getting to know my Knuth better is the best solution but I just don't have the time to invest. I guess I'm just speaking from a user centered perspective. 
&gt; I would probably say that a segfault from std is also an issue Very much so.
Does `time::get_time()` return the seconds since 1/1/70 UTC in local time or in UTC? The source code uses `sys::get_time()`, so I'm guessing it's local time? What's an easy way to check this sort of thing in the playground?
I’m planning on contributing to get Windows support up and going, and HTML and/or SVG output is another thing that I would *really* like for my project. You’re just ticking so many of the boxes for what I’m wanting that I think I’m going to *have* to become a major contributor! Thanks!
Iterator follows monadic patterns, so does Result. It's well understood that making two different monads work together is kind of clunky, it makes sense that it would also be the case here.
What would the behaviour of `for` loops be in that case? Would the for expression evaluate to `Result&lt;(), Error&gt;`? Then you would probably have to add a ? operator after the curly brace. I actually do like having iterators with Results as the Item, and I think it would be better to just have some syntax that helps make that cleaner in `for`. ? patterns would probably do it: `for line? in lines.iter()` rather than `for line_r in lines.iter() { let line = line_r?;`. For most other iterator operations you can just use `collect()?`. And for /u/barsoap, if you want something that can do retry and stuff, I think it's better to just make a Stream type that can be converted into an Iterator if the user doesn't care. I've seen some libraries take this approach.
Why not make the whole match expression a macro? https://is.gd/fyyem5
I'd rather do that than starve, sure.
Is it possible/a good idea to encode the notion of a range in rust's type system? That is, if I wanted to type to represent real numbers between 0 and 1, such that it could be checked at compile time, how could I/should I do that?
Kind of reasonable to me, and likely easy to implement.
They're true macros. C macros are just pre processor directives with no ast. If a macro could emit arbitrary text instead of ast it would be hard to type check.
They both follow monadic patterns but there isn't general support for monads, so trying to get two separate monadic behaviours to play nicely is going to be clunky.
&gt; I feel like this is not the first time I've seen rust IO slower than Python IO. Because unbuffered IO is a surprising default to beginners.
oh, so this is why people call their 2MB js-filled websites "light".
I made a PR to use Rayon as well as the buffered output mentioned below: https://github.com/Ritiek/auto-image-cropper/pull/1 . 🙂 Went from 8.8 to 9.5 seconds from `master` to about 6.5 seconds on my Macbook, running `time cargo run --release -- --input benchmarking/ --output blob` once to compile and a couple more times to roughly benchmark runtime.
Is he still involved in the development? I never saw him on the Rust repos, but why not?
You can significantly speed up your code: 1) You should open image only once 2) Optimal coordinates could be found in one pass like this for x in 0..(im.dimensions().0) { for y in 0..(im.dimensions().1) { let col = im.get_pixel(x, y); if col[0] != 255 &amp;&amp; col[1] != 255 &amp;&amp; col[2] != 255 { x_max = cmp::max(x, x_max); x_min = cmp::min(x, x_min); y_max = cmp::max(y, y_max); y_min = cmp::min(y, y_min); } } } 
Ok! BTW, you and /u/dtolney helped me writing examples! Thanks again!
Note that there is a more elegant way to do what you want, without macros. You can take x modulo 8 as an index into a static array containing your registers. This way 0-7 get mapped to B through A, the same for 8-15, etc. The compiler is probably even smart enough to remove the bounds check on the array in this case. Then the only thing that needs to be determined is which operation to perform. For this, you can use the range syntax inside the pattern matching arms of the match expression. Something like: let reg = &amp;registers[x % 8]; match x { 0 ... 7 =&gt; cpu.rotate_left_carry(reg), 8 ... 15 =&gt; cpu.rotate_right_carry(reg), _ =&gt; unimplemented!(), }- Furthermore, you could also store all the different operations as closures (i.e. `move |cpu, reg| cpu.rotate_left_carry(reg)`) in an array, and then use x / 8 (which uses integer division) as an index into this array to retrieve the proper operation. That would save you from having to manually write the range bounds for each of the arms of the match. Since the closures don't capture anything, I think it should be possible to store them in an array the contents of which can be determined at compile time. Perhaps you could even store the functions directly without using a closure, like `&amp;Cpu::rotate_left_carry`. I don't know if this is possible in the Rust language. Finally, from the code fragment you've posted it seems that it is perhaps possible for the Registers enum to implement `#derive(Copy)`. That way you would not have to pass it by reference to the rotate_left_carry and other functions. Edit: it is probably necessary to store the closures/functions by taking a reference to them, because each closure has a different type and thus they have to be stored as trait objects so that all of the elements in the array of the same type.
Oh, didn't know that. Well, I had less problems with LuaTeX, abd LuaTeX supports microtypography
Goodness, that document is interesting! I was confused about the use of the word *tectonic* and *tectonics*. Fortunately the author explains: &gt; I use the expression tectonics based on the definition below. It’s nerd slang, for describing &gt; the code building process. Using a lookup from the dict:// protocol bank of open servers: &gt; &gt; "Tectonics" gcide "The Collaborative International Dictionary of English v.0.48" &gt; Tectonics \Tec*ton"ics\, n. &gt; 1. The science, or the art, by which implements, vessels, &gt; dwellings, or other edifices, are constructed, both &gt; agreeably to the end for which they are designed, and in &gt; conformity with artistic sentiments and ideas. &gt; [1913 Webster] &gt; &gt; Trying to infer that building with GnuCOBOL is rock solid and artistically pleasing. Ok fine, I mean wicked cool!. Has anyone else heard the term that way? It's new to me, but I definitely like it.
I too was speaking from a user-centered perspective: that of a plain-TeX user rather than a LaTeX user. :-) Yes, LaTeX's error handling is not great, but this is not a flaw in TeX itself. Macro languages are not fun to program in, and LaTeX already pushed the limits of both TeX and the computers at the time, so they didn't have much room left over for error handling. But TeX itself does a reasonably good job IMO (explained in Chapters 6 and 27 of the TeXbook). Maybe if you have specific examples of poor error handling we can discuss this more concretely.
I want to inject my rust code into an windows application and call an function inside it, possibly even try to hook it. For example the app TestApp.exe has an function void log(const char * message). Now I want to inject my rust library into the application and call the log function. But since no symbol is exposed, I can't use libloading for example.
(I'm a newbie too) You could run it on your computer and on the playground and compare, since you probably have different timezones
I appreciate your dedication to the cause and I'll try to remember this thread for the next conference paper I write haha
Thank you! I'm concerned that there might exist implementation of `Write` which isn't optimised for `write_str`, so serializing to intermediate buffer would slow it down actually. Wouldn't it be better to put `Port` behind some buffer? That way intermediate buffers would be optional.
Oh I'm sorry. should have answered this more in detail. This is more of a meta answer not really an answer to your question: When programming with event systems (specially with GUIs) I experienced that much of the complexity of getting data from one point of the code to the other end is reduced by using FRP (functional reactive programming). In fact you end up just sending events between the threads and (nearly) not sharing any locking object is needed. The downside is, that you need to follow this FRP model rules that might be overwhelming and complex looking. carboxyl is one implementation of FRP in rust. It is basically the "same" thing as mspc but with more possibilities.
This will make you pay the cost of bounds checking twice (once in the `if` check, once in the internals of the indexing, unless you employ unsafe code with`get_unchecked`). Also, I think `if let Some(value) = vec.get(index) {` is more elegant and conveys intent more clearly.
A consideration could be whether or not the country .rs belongs to could be considered politically stable. (See former .ly troubles) .rs belongs to [Serbia](https://en.wikipedia.org/wiki/Serbia), which apparently "ranks high on the Human Development Index (66th), Social Progress Index (47th) as well as the Global Peace Index (48th)." My superficial research indicates that there is no immediate problem with relying on .rs :)
That is indeed! Thanks.
Note: This is 4th time a Rust-game video is posted from this account here, and it's the only activity on this account.
Useful read: https://gist.github.com/jFransham/369a86eff00e5f280ed25121454acec1#assert-conditions-beforehand 
Maybe it's useful as an alias for docs.rs?
How would a `Monad` trait make an iterator of Results easier to manipulate?
Am from Serbia, can confirm. [RNIDS](https://www.rnids.rs/en), our registrar, is a very responsible and professional organization.
That makes sense, I guess it's just bias from my background. However I find the rust macros far less expressive than Lisp ones. To the point where you can barely compare them. But I think that's forgivable.
Isn't that segfault supposed to be fixed by [this](https://github.com/rust-lang/rust/pull/31333)?
I think you should allow to pass the rng as parameter. Also I don't know If I get this wrong but look at this: for exp in exps.iter() { exp.generate(buffer).expect("Fail"); } I think you drop the max limit you passed earlier to the function I think every child expression should have the same limit. Also expect is kinda bad because it would cause a panic. However I really like this crate. I created something similar in C# and It was looking much worse.
&gt; If we use this, our program will have to iterate over every pixel in the image and even if we try to "optimize" our program to return the optimal coordinates when it finds a white pixel after finding a non-white pixel, there is a high chance that an image will be cropped incorrectly because there is always a chance of having a white pixel in the actual image itself. This makes sense when result image size will be more than a half of source image. But you can optimize it in another way: When you've found `min_x` with `get_top_left` - you can use it in `get_top_right` and `get_lower_right` etc. Now: http://imgur.com/94WkyzM After: http://imgur.com/RGxlPlz So you can remain `get_top_left`, `get_lower_left`, `get_top_right`, `get_lower_right` and just pass image and new limits as an arguments into it.
That's fair. "Less expressive than X, which is also syntactic" is a much more reasonable criticism.
Why do you need to write `do_something as fn(i32) -&gt; i32 as usize` instead of `do_something as usize`? isn't `fn(i32) -&gt; i32` already the type of `do_something`? I mean, you don't need `as u32` here: let x : u32 = 2; let y : i32 = x as u32 as i32;
&gt; However I find the rust macros far less expressive than Lisp ones. Macro by example are less expressive (Maybe? Not sure if they are Turing complete). But [procedural macros](https://doc.rust-lang.org/book/procedural-macros.html) (that is, a function that receives token trees and returns token trees) are as expressive as Scheme macros. But not as much expressive as Common Lisp macros though, because they must be hygienic (can't capture a variable that you don't pass as a parameter)
We could explore that then :)
I would prefer that you transfer it to me than let it expire; message me please!
There is (was?) a way to use an identifier with function-like procedural macros (the older ones, not the new ones): struct_def_with_reflection! Something { x: int, y:float } Perhaps we need a way to do that with the new `proc_macro` system?
This has been suggested a few times (though not *formally*, I don't think). The major sticking point has always been that the syntax *looks* like it's doing type-dependent lookup for the macro, when that would be more or less impossible. Personally, I'd love to see it. Actually, I'd love to see more "suffix position" stuff in general, given how Rust code tends to compose.
You could put it behind a flag so that it's by default single threaded but users can opt to add threads.
Another great resource is the manual for the memoir documentclass (an excellent package in itself). It has an appendix called "The Terrors of Errors" which explains *all* of the errors TeX can give, and I think all of the core LaTeX errors. Usually it will include suggestions for common causes/remedies as well. It'll be in most installations as memman.pdf.
According to the documentation, it is in UTC. The fact is hidden in the ‘Z’ at the end of `since 1970-01-01T00:00:00Z` which gives the time zone as ‘zulu time’ aka UTC. 
Or perhaps it should link to docs.rs, the Rust Book, Rust by Example, and other documentation resources, for the benefit of people landing from Google searches. In a fashion like "are we __ yet?" sites
Maybe. Note that sometimes things that are undefined behavior appear to work on most cases. And IIRC unsafe code sometimes needs double casts (but at the moment I'm unable to find any reference about it).
We use Rust pretty heavily and are excited to make a (very small) contribution to the ecosystem. We use this library to make sure that our code, which is all written in Rust, applies exactly the same consensus logic as Bitcoin Core. We have more (&amp; more substantial) open-source releases planned!
I'd like to believe that.
My earlier comment wasn't clear: I saw consistent speedups using Rayon. Here's the speed differences I saw on my Macbook: master | PR branch ------- | ----------- 8.8s | 6.3s 9.5s | 6.6s :)
&gt; branching operations as expressions Are you talking about match on pattern?
Hey! I happen to work at a hosted cloud company, and am working on the [sozu reverse proxy](https://github.com/sozu-proxy/sozu). The websockets proxying is something I had to work on, so I'd be happy to chat if you need it :)
Who's "we", your company?
This is a terrible idea as Tectonic as a software product already exists: https://tectonic.com It is a registered "computer software" trademark as well: http://tmsearch.uspto.gov/bin/showfield?f=doc&amp;state=4805:nppxvg.2.11
I'll take a look and see what might be done.
what?
Come on. A lot of secretaries learned and used TeX, back in the day. If you use it simply as a typesetter, it is not that hard to use. You just type some text into your document and end it with `\bye`, and you're done. Everything is documented clearly in the TeXbook. Yes you need to read a book before you can start using it. (BTW, the book _A Beginner's Book of TeX_ by Seroul &amp; Levy is excellent.) That's a few hours and not a big deal. It's only LaTeX that adds all the confusion and unpredictability, by hiding away everything in layers of macros in an attempt to make things simpler for the user. For a programmer to understand the program itself, I don't think 40 hours is unrealistic: I spent about 10 hours on the syntactic routines (sections 20 to 28 of [`texdoc tex`](http://texdoc.net/texmf-dist/doc/generic/knuth/tex/tex.pdf)) and the main loop, and got a fairly good idea of what's going on. Edit: Another story: apparently Mike Spivak had never used a computer, let alone done any programming, when he attended Knuth's lectures on TeX — and then he went on to wrote _The Joy of TeX_ (an excellent book) and amstex (an excellent macro package for TeX).
No, but it's hard to answer with out the code or the error message.
The RFC mentions an anticipated use for `from_ok()`. What is it?
What's the status on those stack-probes btw?
&gt; he does not consider the fact that it is completely impossible for even most computer-savvy people to figure out how to use TeX is not considered a bug. Feature? ;) By the way, I appreciate your work. Several years ago I built and maintained a bootstrapped from source-only distribution of TeXLive (don't ask) and the amount of effort getting something workable was way more than I had appreciated.
I can feel your pain. I tried to use nom for a project but i was failing very hard. I just gave up after ~2h and delayed that part to later and wrote it by hand for the time being. I just felt very stupid and never talked about that :D I have never used a parser combinator before in my life and hoped to get my feet wet with nom. Not the love story i would tell to friends ...
Maybe you're joking, but I'm morbidly curious.
No, not joking. Been waiting for proper WebSockets support in Tokio forever. I tried to figure it out on my own a while back but I just didn't have the time to devote to it (also, the libraries were rapidly changing breaking my stuff).
I mean, why are you building an "X11 web app"? What is it for? I have a very difficult time imagining why someone would need websockets and display (let alone X11 in particular) on the same port.
extern crate image; use image::ColorType; use image::png::PNGEncoder; use std::fs::File; use std::io::{Result, Write}; fn write_bitmap(filename: &amp;str, pixels: &amp;[u8], bounds: (usize, usize)) -&gt; Result&lt;()&gt; { let output = try!(File::create(filename)); let encoder = PNGEncoder::new(output); try!(encoder.encode(&amp;pixels[..], bounds.0 as u32, bounds.1 as u32, ColorType::Gray(8))); Ok(()) } use std::io::Write; fn main() { let args: Vec&lt;String&gt; = std::env::args().collect(); if args.len() != 5 { writeln!(std::io::stderr(), "Usage: mandelbrot FILE PIXELS UPPERLEFT LOWERRIGHT") .unwrap(); writeln!(std::io::stderr(), "Example: {} mandel.png 1000x750 -1.20,0.35 -1,0.20", args[0]) .unwrap(); std::process::exit(1); } let bounds = parse_pair(&amp;args[2], 'x') .expect("error parsing image dimensions"); let upper_left = parse_pair(&amp;args[3], ',') .expect("error parsing upper left corner point"); let lower_right = parse_pair(&amp;args[4], ',') .expect("error parsing lower right corner point"); let mut pixels = vec![0; bounds.0 * bounds.1]; let threads = 8; let band_rows = bounds.1 / threads + 1; { let bands: Vec&lt;_&gt; = pixels.chunks_mut(band_rows * bounds.0).collect(); crossbeam::scope(|scope| { for (i, band) in bands.into_iter().enumerate() { let top = band_rows * i; let height = band.len() / bounds.0; let band_bounds = (bounds.0, height); let band_upper_left = pixel_to_point(bounds, (0, top), upper_left, lower_right); let band_lower_right = pixel_to_point(bounds, (bounds.0, top + height), upper_left, lower_right); scope.spawn(move || { render(band, band_bounds, band_upper_left, band_lower_right); }); } }); } write_bitmap(&amp;args[1], &amp;pixels[..], bounds) .expect("error writing PNG file"); }
He doesn't claim there are no bugs, only that fewer bugs are reported, so he doesn't plan to check the bug reports next until 2020. In fact, his version numbering scheme implies an infinite number of bugs, as the number converges to pi. From his webpage: &gt; I still take full responsibility for the master sources of TeX, METAFONT, and Computer Modern. Therefore I periodically take a few days off from my current projects and look at all of the accumulated bug reports. This happened most recently in 1992, 1993, 1995, 1998, 2002, 2007, and 2013; following this pattern, I intend to check on purported bugs again in the years 2020, 2028, 2037, etc. The intervals between such maintenance periods are increasing, because the systems have been converging to an error-free state. The latest and best TeX is currently version 3.14159265 (and plain.tex is version 3.141592653); METAFONT is currently version 2.7182818 (and plain.mf is version 2.71). My last will and testament for TeX and METAFONT is that their version numbers ultimately become $\pi$ and $e$, respectively. At that point they will be completely error-free by definition.
hey! I'm sorry, the documentation is still not in the state I'd like, mainly because I do not have a lot of time to work on nom. To address a few of your remarks: &gt; How should I have known that &gt;&gt; stands for "append next parser" or that you can bind fields to a parser? How can I make more obvious that it's just the syntax used by [do_parse](https://docs.rs/nom/3.0.0/nom/macro.do_parse.html)? It is something that's completely arbitrary, there is no special meaning, I needed a separator. &gt; You can also pass a ~ character in the place of the &gt;&gt;, but I am not sure what it does because there's no documentation about it The ~ came from an old combinator that was removed in nom 3.0, `chain` (you can still look at the old [documentation](https://docs.rs/nom/2.0.0/nom/macro.chain.html) ). Unfortunately, it still appears in older blogposts. I can probably replace that example with another one, like a [basic JSON parser](https://github.com/Geal/nom/blob/master/tests/json.rs), but the balance is hard between an example that is too simple (what I had before the arithmetic expression parser) and another that shows too much. What do you think? Why do you think nobody on the IRC channel knows how to parse a file header? There are some pretty knowledgeable people there, maybe you asked at a time when they were not available? If no one's there, you can still open an issue on nom's github repository. I can't guarantee a fast answer, but I'll help. As examples of simple file parsers, you can take a look at the [FLV format](https://github.com/Geal/flavors/blob/master/src/parser.rs). For the endianness, it depends on what you want. There are some integer parser with fixed endianness (be_u32, le_u32, etc), and there are combinators where it can be set according to a parameter like the [u64](https://docs.rs/nom/3.0.0/nom/macro.u64.html) one.
Okay, I've assembled a tiny recipe to get started: "recipe": { "location": "dpwiz/eclipse_che_rust", "type": "dockerimage" } It uses my image on dockerhub ^[source](https://bitbucket.org/dpwiz/eclipse_che_rust/src/master/Dockerfile) that extends default CPP/GCC image with rustup and a couple of bashrc tweaks. There are no highliting or racer support as someone need to write a Java wrapper to pull the language server strings.
error[E0252]: a trait named `Write` has already been imported in this module. I changed it after your first reply, so now my only question is would there be any instance where you would have to import both? 
Thanks for your answer, I'm sorry for my rude tone, I was a bit frustrated. I do like nom, and I completely understand that the documentation is lacking if you don't have time to work on this. Some of my ideas to improve: - The examples on github link directly to the projects, so you have to sift throught unrelated code in order to get to nom-related code. There's no way to find the FLV parser if you don't know where it is. - The IRC guys told me to wait for this video. I tried posting my example, but got stuck with [this](http://play.integer32.com/?gist=9fa93907f022dee9a9d95f418a6c5283&amp;version=stable) and couldn't figure out what's going on. - Do you mind a PR on github for the readme and documentation? It's a bit hard to explain what I expect from an introduction documentation. Basically, incremental difficulty. Don't throw 50 macros at me right away to show what your library can do, just the bare basics and gradually get more difficult until I can read and understand the documentation without looking at examples. The [blender file format](https://wiki.blender.org/index.php/Dev:Source/Architecture/File_Format) has a byte that is 'v' or 'V' for the endianness. So I wanted to somehow read the file header and then decide what endianness the coming file blocks are formatted. The u64 macro will probably do in this case.
&gt; The major sticking point has always been that the syntax looks like it's doing type-dependent lookup for the macro, when that would be more or less impossible. &gt; sure, for me a.foo(b) is as much about syntactic convenience as type-dependant lookup.. because from C++ i'm already used to the fact that foo(a,b) can *also* depend on types . i wonder if there are any other parsing subtleties, i.e. macros can take expressions or idents .. would the parsing work ok (treated like any other macro parameter) , or would there be restrictions on how it behaves.. if the latter perhaps that would be a displeasing asymmetry 
Ah, but the `T: Clone` bound isn't on the `PtrMarker` trait, it's on the `PtrMarker::clone` function! You can have `PtrMarker&lt;T&gt;` without `T: Clone`, you just won't be able to clone the associated `Ty`. I'm sidestepping the recursive relationship by assuming in the trait definition that `T: Clone` implies `Self::Ty: Clone`, which seems like a reasonable assumption.
I've got some C# code I am porting and am looking for tips on making it efficient both in terms of runtime, and also not being a mess to look at in code. We have an array of unsigned bytes: short[] perm = {151,160,137,91,90,15,... } // C# let perm : [u8;512] = [151,160,137,91,90,15, ... ] // Rust The table is bytes to help with fitting it all in cache but is not strictly necessary, it may perform better overall with int32s but for now let's say bytes. That's fine, now we want to do the following var foo = grad2(perm[ix0 + perm[iy0]], fx0, fy0); // c# In Rust this ends up being a bit messy and I'm wondering if there is a better approach: let foo = grad2(perm[(ix0 + perm[iy0 as usize] as i32) as usize] as i32,fx0,fy0); Maybe more generally, how do you twiddle bits without making a mess? =) 
This looks really useful, thanks! I'm writing a [FAT32 library](https://gitlab.com/susurrus/fat-rs) and I think it'll be useful in testing. I'm not certain if this will work with my library which is `no_std`, but I think it should. I'll definitely try to integrate it when I get to that level of testing.
You've never wished you could remotely connect to your desktop via a web browser before? It would be super handy! Not to mention the fact that my implementation works with graphics acceleration so if you have enough bandwidth (and the latency is low enough) you could literally play the latest graphics-intensive game over the network with any (modern) web browser. That's just one example. Another thing you can do is launch a single application and just share that window. So you can make any X11 application available over the web!
I also tried to use nom and got stuck on the documentation. I also wasn't sure if there was a way to parse from a reader directly. Pointing at a bunch of examples is only somewhat helpful. From what I recall, what's really needed is a good up front step-by-step example of how to parse a binary file, and a complete organized reference of the tags and operators, ideally also with a very brief example of how to use each one. I took a look at the examples but there was simply too much going on with file formats I'm unfamiliar with to fully understand each example without spending a lot of time and effort on each one. I suspected what I wanted to do was tens of lines at most, but I ultimately commented out the nom attempt and did it manually with byteorder and custom serde deserialization.
&gt; because from C++ i'm already used to the fact that foo(a,b) can also depend on types . With specialization you can have a generic foo(a, b) and then specialize it, to dispatch on the types. Not as powerful as C++ generics though (and it looks less powerful than Rust's method dispatch too).
So `macro_rules!` is implemented by a compiler plugin?
Can quotes inside single quotes be escaped? Can you perhaps do something like `"gpg --a 'a b'\''c d'"` to get `["gpg", "--a", "'a b'c d"]`? (like shell scripting). Should spaces in the beginning and end be ignored? Anyway, either way, you may try to solve this writing a regex to extract either a word without quotes or a quoted phrase, and search for it many times. But this may get unreadable fast if your rules are too complicated. But I'd write a parser in [combine](https://github.com/Marwes/combine). Take a look at the examples [here](https://docs.rs/combine/2.3.2/combine/#examples). Other interesting libraries are [rust-peg](https://github.com/kevinmehall/rust-peg) and [nom](https://github.com/Geal/nom).
Why does any of that need both protocols to run over a single port?
1) No. 2) What? I don't think so. 3) The spaces at the ends should be striped before any of this happens. 
If you want shell-like behavior, there is a [shlex](https://crates.io/crates/shlex) crate, that mimics the Python [module](https://docs.python.org/3/library/shlex.html) with the same name. Another option is to use a CSV parser, there are multiple on [crates.io](https://crates.io/search?q=csv), and despite the C standing for 'comma', CSV implementations usually support a different separator (in your case, space). A custom parser is surely overkill, you want to use an existing library here. 
Could one just write: `Some(x? + y?)`
I have a `Vec&lt;Vec&lt;Value&gt;&gt;` (where `Value` is an enum that just wraps some primitive types). All `Vec&lt;Value&gt;` are the same shape. I want to get a projection of that data -- a `Vec&lt;Value&gt;` for one logical "column" of that data. This has been very difficult. I feel like my first problem might be that I shouldn't be using `Vec&lt;Vec&lt;_&gt;&gt;` for this, but I don't really know where to go from there. wat do
Because browsers don't have a mechanism to accept the SSL certificate for the Websocket connection separate from the page that served the content. You used to be able to get away with just using the same certificate for both ports but browsers stopped letting you get away with that (hack) not too long ago. It can all work OK if you have a certificate that validates against your browser's configured authorities but it's wishful thinking to assume end users will know how to get that sort of thing setup. Lastly, having your Websocket on a completely different port should be entirely unnecessary. That's also another port that needs to be open on the firewall, allowed via proxy servers, etc.
Because explicitly specifying the calling convention is better because it prevents misconceptions like this: &gt; while `extern fn` means use the use some undefined rust calling conversion.
Oh, you meant you're running websockets *and HTTP* on the same port. That's of course perfectly normal. You're not running websockets and X11 on the same port.
No, that would be silly :) X11 will only be listening on a Unix socket for this purpose. It won't be exposed to the Internet. That would be insane!
Great! Please let me know if you have any issues with the new release.
Hey! Looks like you're the author of `genio`? I'd be happy to accept a PR, as long as it's behind an optional feature like `tokio`, and doesn't complicate any existing interfaces. In particular, [`PartialOp`](https://facebookincubator.github.io/rust-partial-io/partial_io/enum.PartialOp.html) is currently hardcoded to `io::ErrorKind` and I'd prefer not to change that.
You're right, it being `std` shouldn't be a problem. Sweet!
Not exactly rust, but how does ripgrep decide between reading stdin (from a pipe actually, is that the trick?) and looking at local files: `rg foo` searches the current directory, but `echo foo | rg foo` reads stding. The command `echo foo | grep foo` works as expected but just `grep foo` hangs until \^D/\^C (like `rg foo -`).
Yes. I'll put this into my todo list. Of course I'd put it behind feature flag! Error types are completely generic (that's the point of it! :)), so it should work (but I haven't seen the code yet).
Nope, iterating over bytes works much better, and is faster. If you iterate by characters, you will only end up butchering strings that contain multi-byte characters, as one character does not equal one byte. Iterating by bytes thereby allows you to keep track of the proper index values, as when you slice a string in Rust, you are slicing by byte positions. Source: implemented the Ion shell.
Huh, thanks. That's very informative. I'll edit my comment.
Basically, iterating by characters incurs UTF-8 checks. Since our search space is restricted to ASCII characters, we have no use for UTF-8 validation. There will also never be a collision with code points when checking for ASCII characters because these points will always have a value that's invalid for the ASCII space (reserved values).
If you wanted to (for some reason) match non-ascii characters, would decoding UTF-8 bytes be necessary? Of course, you might just try matching multi-byte sequences for the code point of the character you want to match instead of decoding the entire string.
Exciting!
What if there is no error?
What means to have an unsized struct with `#[repr(C)]`? Does `data` gets translated to the same as a VLA?
Yeah it just checks if stdin is attached to a TTY (otherwise it's a pipe): https://github.com/BurntSushi/ripgrep/blob/master/src/args.rs#L394
Dunno; I have a vague recollection of it being blocked on appropriate support in LLVM or somesuch.
... yes and or no? I mean, from what I remember, it's hooked into the macro machinery like compiler plugins are, but it's implemented in the compiler itself.
&gt; ... or would there be restrictions on how it behaves ... Well, there'd *have* to be. The compiler has to parse the subject of the "call" before gets to the `!` and realises it's dealing with a macro. Even if it didn't, it has to know where the subject begins and ends. Method position macros would almost certainly be restricted to expressions as the subject.
Yes. It's still nightly only.
I'm trying to initialize an Option&lt;Box&lt;i32&gt;&gt; array. Something like this : let my_array: [Option&lt;Box&lt;i32&gt;&gt;; 10] = [None; 10]; Except the compiler complains that Box is not copyable... How can I initialize my array with Nones?
But an `ident` *is* an expression, so there isn't really any meaningful distinction between the two, assuming the macros are changed to allow for re-tokenisation of AST nodes, which it currently can't do. And given that the parser needs to have already parsed the subject, it's *going* to be an expression node whether you like it or not.
The compiler is correct. Box is a pointer. Option&lt;T&gt; is copyable if T is copyable. In this case, it wants to copy the Box from the first to the rest of the elements. This would lead to multiple (possibly mutable) data acesses to the same i32, which is why Box isn't copyable (I think, I may be wrong). You must use a vector for this. let my_vec: Vec&lt;Option&lt;Box&lt;i32&gt;&gt;&gt; = vec![None; 10]; The vec! macro is basically a mini-loop which CLONES the element for each element. However, it is questionable to use a Box for an i32 - way too much overhead for a simple integer.
I would take this feature even if it was limited to $self:expr (e.g. for the chaining cases). $self:ident would just open up a few more desirable opportunities like the more pleasing enhanced-struct decls etc. 
&gt; I would love to see one based on Alpine, but I think it's not possible now due to a bug. https://github.com/frewsxcv/docker-rust-alpine
Actually I was just doing it to ensure fields don't get reordered and as a lint against using something that C doesn't support inside it.
This is one of the papercuts of the language/libraries that hasn't really been addressed yet. There is the [`array-init`](https://crates.io/crates/array-init) crate which provides some convenience functions. You could try `array_init()`: extern crate array_init; // The closure is passed each index to initialize, this doesn't matter to us let my_array: [Option&lt;Box&lt;i32&gt;&gt;; 10] = array_init::array_init(|_| None); Addendum: I just now realized you can do this without needing an external crate, using `Default`: let my_array: [Option&lt;Box&lt;i32&gt;&gt;; 10] = Default::default(); This works because of the following impls: impl&lt;T: Default&gt; Default for [T; 10] {} // Implemented for arrays up to size 32 impl&lt;T&gt; Default for Option&lt;T&gt; {} // just returns `None` I'm kind of ashamed that I didn't think of this approach first, to be honest.
You are already allocating on the heap by using a Box. A box is a pointer to a heap-allocated piece of memory. And an array of boxes = a vector (more or less). You can do: let my_vec: Vec&lt;Option&lt;i32&gt;&gt; = vec![None; 10]; // uses heap allocation or: let my_arr: [Option&lt;i32&gt;; 10] = [None; 10]; // uses no heap allocation But this isn't possible with boxes. Not sure why you would want to use a Box. May I ask what the use case of this is, some surrounding code?
Something something Occam's Razor.
I'll have to take a look into these, but there ought to be a way to combine them into a single solution. I'll probably play with this once I get more serious about my current game projects.
As a former FB intern returning for full time, I'm super excited to hear that Rust has traction at FB. I'd love to know what team you're on, if you're allowed to say (but of course don't risk a leak if you're not sure)
&gt; the purpose of doing this is to be able to handle errors. My intent of `next() -&gt; Result&lt;T, E&gt;` is to allow iterators have an arbitrary termination result `E`. The `Result` is *solely* used to signal whether the iteration is should stop or not, and nothing else. I don't think it's a good idea to bake success/fail into `E` directly, because not all loops can fail. If it can, then all it has to do is to set `E = Result&lt;(), Error&gt;`. Yes, this means `next()` would return the ugly `Result&lt;T, Result&lt;(), Error&gt;&gt;`, but it shouldn't matter since few people will be calling `next()` directly. Instead the user can easily do for line in stdin.lock().lines() { println!("{}", line); }? to handle errors as you suggested.
I think The Register had the most amusing take on it: https://www.theregister.co.uk/2016/10/18/facebook_mercurial_devs_forget_git/
nice, i'm just googling, but can you point at a 1 page minimal example that demonstrates exactly how this works and what it can do? 
&gt; In that case it should really not be Result, since that implies either Ok or Err and the latter is misleading. It's an "error" in an abstract sense. Much like how `Err` causes `?` to break out of the function, `Err` causes `for` to break out of the loop. I don't mind having a dedicated enum but Result has a lot of nice convenience functions that IteratorState don't unless you duplicate them all. &gt; it does make fold-type methods like count() and collect() a bit ugly then They can just ignore the `D` if it gets in the way. Chances are that if you're using `count` or `collect` you probably won't care about `D` and they're all convenience functions anyway.
&gt; It's an "error" in an abstract sense. I don't mind having a dedicated enum but Result has a lot of nice convenience functions that IteratorState don't unless you duplicate them all. Like you said though, someone's only ever going to see it if they're using `next()` directly which is somewhat rare. And anyway, it really isn't the same as a `Result`. Convenience methods could be created as needed but it probably wouldn't need the same kind of stuff a `Result` would have. &gt; They can just ignore the D if it gets in the way. Chances are that if you're using count or collect you probably won't care about D and they're all convenience functions anyway. I don't think this is true. If you're using `collect()` you want to be able to handle an error in the middle of the stream, rather than just getting a collection that has the partial results and not knowing about the error. There would also have to be some kind of lint that warns you if you use `collect()` etc. where the termination value is a `must_use` value like Result, since the current lint wouldn't catch it. FYI, `collect()` does currently correctly handle errors; there's a FromIterator for `Result` that folds the results together
I'm sorry, but I have to admit this doesn't make sense to me &amp; it seems like cargo culting. &gt; without being able to talk about monad's generally, the usefulness of such a trait is limited. What does this mean? Such a trait *would be* how to talk about monads generally. &gt; Rust needs something equivalent to higher kinded types to really open the doors for functor/applicative/monad/etc Yes, because the trait can't be expressed without higher kinded polymorphism. --- None of this answers my question - how does this make an iterator of results easier to manipulate?
UPDATE: Instead of trying to send a closure, I sent an enum and acted appropirately. Now it complains about possibly uninitialized variables, like I predicted. UGH
From Serbia, although things aren't great here by the standards of the Western Europe, it's getting better. I would say that there shouldn't be any problems with RNIDS registrar in foreseeable future.