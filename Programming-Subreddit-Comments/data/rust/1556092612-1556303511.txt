Excellent work, keep it up. Can't wait for dynamically allocated types.
No it still the same.... Maybe a reinstall might help?
Basically now rust Hashmap are provided by the crate : [https://github.com/Amanieu/hashbrown](https://github.com/Amanieu/hashbrown) In a nutshell, it is a new hashmap structure and algorithm that uses SIMD instruction, allowing to save allocated memory and instructions at the same time. It was inspired by a google project swisstables : [https://abseil.io/blog/20180927-swisstables](https://abseil.io/blog/20180927-swisstables)
Exciting release!
&gt; supposed to be a stripped down version of std library It’s not, if you’re thinking of code size. The point of `alloc` (as opposed to `std`) is that it does not depend on functionality provided by the environment / operating system, other than an allocator (which can also be provided by a crate). Currently there is no way to disable `fmt`-related code inside `alloc` or `core`. Maybe in the future with https://github.com/rust-lang/rfcs/pull/2663, standard library crates will have an `fmt` default feature that you could opt out of. That said, the linker should already eliminate from your binary functions and other symbols that are not used. Have you measured code side of your linked binary and found actual bloat that can be attributed to `fmt`? I’ve used https://github.com/google/bloaty before. There’s also https://github.com/rustwasm/twiggy which is Rust-oriented. If `fmt`-related code is present in your binary, it likely means it’s actually called by some functions in your call graph. This means two things: * If you simply removed `fmt`-related code from `core` and `alloc`, your unchanged code might fail to compile or link. * You may have semi-hidden usage that could be eliminated, for example replacing `.expect("…")` with `.unwrap()` to make the panic message `fmt`-less. At that point the linker might achieve your desired outcome, without any change to `core` or `alloc`.
Have you tied either C# with Roslyn or Kotlin/Java with IntelliJ? They basically just work in my experience.
Const generics is something that must come soon, IMO. The arbitrary limitation of 32 for the std arrays implementation, and the fact that the users can implement only a subset of the arrays through macros, are bad.
&gt; Empty hash maps do not allocate any memory. https://github.com/Amanieu/hashbrown
[Orzo](https://en.wikipedia.org/wiki/Orzo) could describe this antipattern though.
It seems you both are humble and kind individuals then ;-) Congrats on the cool crate!
Is there any article or tutorial that explains futures 0.1 all the way to the abstractions async/await? I am kind of lost in the complexities
You probably don’t know a lot of the things IntelliJ can do for Java :-) Do you use smart completion (ctrl+shift+space) in Java, for example? I am pretty sure I don’t know a significant number of useful tricks IJ is capable of :)
&lt;3
Thanks. Really great answer.
The std implementation will not allocate for empty maps. Changing `new` to do that would be a breaking change.
How does the new hashmap handle the case where you insert and remove a lot of elements so it is full of tombstones. If it doesn't count them that means every lookup must search the entire container. Does it reallocate if there are lots of tombstones?
Did you consider using e.g. SQLite as a backend ([1](https://www.sqlite.org/fasterthanfs.html), [2](https://www.sqlite.org/intern-v-extern-blob.html)), rather than the filesystem ? Reading this comment from your [docs](https://pijul.org/model/): &gt; This is actually the optimal complexity, since the call to fopen required to read a patch (a commit) is also in complexity O(log h) on good file systems. and what the SQLite page I linked first says: &gt; SQLite reads and writes small blobs (for example, thumbnail images) 35% faster than the same blobs can be read from or written to individual files on disk using fread() or fwrite(). &gt; (...) &gt; The performance difference arises (we believe) because when working from an SQLite database, the open() and close() system calls are invoked only once, whereas open() and close() are invoked once for each blob when using blobs stored in individual files. It appears that the overhead of calling open() and close() is greater than the overhead of using the database. leaves me under the impression that it might be beneficial. That's what the [Fossil](https://www.fossil-scm.org) SCM does, btw.
I thought the old implementation of `HashMap::new()` allocated, though? Good to know that the new one won't in either case! Thanks.
1) It IS easier to use a scripting language than Rust. This is the very reason why they are so popular and present everywhere. It is a fact proven by decades of experiments and studies. You cannot deny that. &amp;#x200B; 2) A scripting language is for scripting, not for coding the core of your application. Unreal and Unity make huge uses of scripting as well. It allows you to have a very efficient core in C/C++/Rust with complex logic, but have very flexible components on top through scripting languages. For instance you can have all the rendering/layers/pathfinding in Rust, and implement mission scenario through scripts. You don't need Rust or C++ to code the logic behind a mission scenario, or even the stuff behind most game features. Low level languages are mostly needed for the most demanding parts. &amp;#x200B; 3) Scripting allows mods, and having many language possibilities for that is very valuable. &amp;#x200B; 4) In game dev, you have several teams. The engine team can be proficient in Rust while others might be in JS or Lua. The UI for instance would benefit from scripting as many UI devs already use plenty of JS/Lua. &amp;#x200B; 5) Amethyst's aim is not being a pure Rust demo for Rust hardcore fans only. Integrating scripting does not reduce its appeal, it increases it by an order of magnitude.
I think `std` (actually `core`) futures are basically the same as `futures 0.3`. The latter has some extra stuff like combinators, but the `Future` trait is just a re-export of the one in `core`.
Due to you leaving Mozilla, or are there other factors?
No, the old implementation [does not allocate](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.new)
I suspect that a lot of people don’t use IDE precisely because IDE support is pretty bad on average, across the whole industry. Only Java and C# have moved beyond the basic stufff when it comes to in the fly language analysis, but these are niche ecosystems.
Huh, TIL. Double thanks!
I haven't seen one. The major user-facing change is the change from `Future&lt;Item = X, Error = E&gt;` to `Future&lt;Output = X&gt;`. So representing an error would be `Future&lt;Output = Result&lt;X, E&gt;&gt;`. The big changes are more internal and only relevant if you implement Future for your own types and are in the [Future::poll](https://doc.rust-lang.org/std/future/trait.Future.html#tymethod.poll) function: * now takes `Pin&lt;&amp;mut self&gt;`, which you can read about in a few `Pin` related blog posts * the explicit `Waker` argument rather than an implicit context
Yes, there have been promises that the current implementation requiring thread-local storage is an implementation issue that will be fixed (although probably not before the initial async/await stabilization). There are a few ways to implement it, but the easiest by far (well, easiest for the async to generator transform at least) requires extending the underlying generator feature to support "resume arguments" to pass the `task::Context` in. You can follow https://github.com/rust-lang/rust/issues/56974 for any updates.
The interesting thing is, supporting arbitrary project configuration, at its core, is pretty easy *if* you don’t specifically hard-code a single build system. Rust analyzer *already* supports non-cargo based builds: https://github.com/rust-analyzer/rust-analyzer/pull/939
I haven't seen one. The major user-facing difference is the change from `Future&lt;Item = X, Error = E&gt;` to `Future&lt;Output = X&gt;`. So representing an error would be `Future&lt;Output = Result&lt;X, E&gt;&gt;`. The big changes are more internal, only relevant if you implement Future for your own types, and are in the [Future::poll](https://doc.rust-lang.org/std/future/trait.Future.html#tymethod.poll) function: * now takes `Pin&lt;&amp;mut self&gt;` instead of `&amp;mut self`, which you can read about in a few `Pin` related blog posts * the explicit `Waker` argument rather than an implicit context
As WellMakeItSomehow said, futures `0.3` provides additional functionality on top of `std::future::Future`. The std Future is the bare minimum and does not have any combinators, etc. This is to stabilize as quickly as possible, I guess as time goes on some things from futures could be moved into std. As for the differences between futures `0.1` and `0.3`, I haven't seen one. The major user-facing difference is the change from `Future&lt;Item = X, Error = E&gt;` to `Future&lt;Output = X&gt;`. So representing an error would be `Future&lt;Output = Result&lt;X, E&gt;&gt;`. The big changes are more internal, only relevant if you implement Future for your own types, and are in the [Future::poll](https://doc.rust-lang.org/std/future/trait.Future.html#tymethod.poll) function: * now takes `Pin&lt;&amp;mut self&gt;` instead of `&amp;mut self`, which you can read about in a few `Pin` related blog posts * the explicit `Waker` argument rather than an implicit context
http://rust-lang.github.io/rfcs/2592-futures.html#historical-context is the best resource I know of currently, if you follow the link to the "futures RFCs" those explain a lot of the motivation behind the larger API changes (other than the later change to support pinning).
Thanks so much for your answer and the links. This is quite astonishing. I don't have a formal CS background so I just assumed that programming basics like hashmaps are already fully understood and optimized for. Apparently that's not the case. Nice. Also: Wow, how much faster is this hashbrown implementation. How does it achieve this? Is it simply due to the SIMD optimizations? And one last question: The GitHub issues title is: "Replace HashMap implementation with SwissTable (as an external crate)". So what does that mean? Is hashbrown still an external crate? But if it's an external crate then how was the old HashMap implementation "replaced"? It's still there, but I can use the external hashbrown crate if I want faster HashMaps? I'm confused. :-)
Is it just me or do a lot of people just assume that everyone knows what their library is about? A lot of times there's not even any information about it at the actual repository.
Have started learning Rust from 'The Book'. I don't have any formal computer science degree(Civil Engineering Graduate.), so getting first time into system programming language. I have used Javascript(extensively) and Go before(occasionally) I am working on creating a very simple rust api to strengthen what i have been learning. Planning to add more feature as i go along. Though it's very simple any feedback on the approach would be appreciated. https://github.com/sachinmaharana/rust-app
That's why I mentioned RLS 2.0 :-). I don't think it's too feasible to support this in the current RLS. Anyway, you can probably get most things working, but there will be problematic stuff like build scripts.
What does `into_bytes` have to do with this? That just gives you a representation of your string as UTF-8. You probably want to break it up into components (rows/cells?) as strings first, and then convert those smaller strings into whatever you need.
I never considered using SQLite, because that would add a lot of extra complexity (such as the SQL layer) for no benefit (including on performance, because Sanakirja is faster than SQLite anyway). Another issue is that we wanted some form of branches, and SQLite doesn't support that. By the way, despite what you may think, Pijul branches are **absolutely not** like Git branches. Do not use branches before you become really confident with Pijul. Personally, my only use of branches is on the Nest, to allow people to download older versions in a single command. I was aware of Fossil, but I do not really understand why someone other than the developers of SQLite and Fossil would choose to use it over Git. It has the same algorithms as Git, bringing most of the drawbacks of Git (bad merges, lack of commutativity), plus some extra problems of its own (speed, disk usage…).
`rustup -Vv` output please?
With hashbrown, the new implementation of std::collections::HashMap merged, compile times will likely improve. The long-term goal is to make debug builds in [cranelift](https://github.com/CraneStation/cranelift/blob/master/rustc.md) instead of LLVM. But that requires a bit more work.
Indeed, we are not yet benchmarking on such giant codebases because this was still out of reach before 0.12. Disk space is still an issue, but as soon as we mitigate it (probably in the next version), we'll start taking the appropriate steps to fix this (such as using file modification times instead of diffs). My guess is that for a small diff, the time is completely dominated by the comparison between the old and new version for each file. Also, if the diff is in a small enough portion of the file, a linear diff will not go much faster than a quadratic one on small input sizes. The reason I'm so excited about this release is that this is the kind of problem we've been putting aside for a long time, but that we'll start to tackle now that we're probably done with the core theory.
You probably want to create a closure or just a separate function that creates and returns your `impl Future` with the arguments added to the state. This is also how `async fn` work I guess.
Thank you for your answer. Exactly, i want to break it in rows, where each line is a vector. So , first I split the string into several strings and then convert these strings to vectors?
Yes.
Thanks for your comment ! Sorry for mentioning Fossil, which I realize brought confusion and wasn't my point. Please allow me to rephrase my question as follows. Here is an example of a Pijul repository taken from [your documentation](https://pijul.org/manual/format.html): ``` .pijul ├── changes.wUAdAoNq ├── hooks ├── id ├── local │ └── ignore ├── meta.toml ├── patches │ ├── AKm4JUd3Z5LRLpg1jezgmhLWGyzR27Qv9ATDFSb7QquqvtMwqEoNiiuVP1u2ho7V8qbprRCRPsmGY6bumYjoH8JZ.gz │ └── AKm4JUd3Z5LRLpg1jezgmhLWGyzR27Qv9ATDFSb7QquqvtMwqEoNiiuVP1u2ho7V8qbprRCRPsmGY6bumYjoH8JZ.sig ├── pristine │ ├── db │ └── db.lock └── version ``` the `patches` directory is where the patches lies, currently stored as files (one patch = one file). Did you consider using another backend than the filesystem for storing the patches (SQLite being one candidate, and surely not the most appropriate), in order to minimize the overhead of multiple calls to `fopen()` and `fclose()` ?
You shouldn't need to convert the lines into vectors. [`str::lines`](https://doc.rust-lang.org/std/primitive.str.html#method.lines) returns an iterator where each item is one line of text as an `&amp;str`. You can then call other `str` methods (e.g. [`split_whitespace`](https://doc.rust-lang.org/std/primitive.str.html#method.split_whitespace)) to further break up each row into its elements.
It would be nice to have the error trait 'fixed', see for example https://github.com/rust-lang/rust/issues/53487. At the moment dealing with errors in Rust feels a bit clunky, when you are combining things from different crates. I've been following the BurntSushi method https://blog.burntsushi.net/rust-error-handling/. It works but is a lot of boilerplate. Others recommend using `failure` or `error-chain`. It's confusing for beginners, the web is littered with articles about the best way to define error types, all seemingly different and some of them probably obsolete (I have seen `failure` described an an 'experiment').
TL;DR: `poll()` is indeed called in a loop, but that polling is paused while waiting for IO, via Waker/Reactor machinery.
So, a simple almost C enum, with no intention to use it as a sum type, just a tri-state boolean: enum Foo { A, B, Neither } so that this `let x = Foo::A` works, but `if x == Foo:B` **does not**? That can't be right, using `std::mem::discriminant(&amp;Foo::A) != std::mem::discriminant(&amp;x)` would be downright silly. So, how do I actually use it in a simple comparison?
Ok, so we already partially do that, in the sense that most of the useful information from the patches is in pristine.db already. We sometimes need to open a patch, but that's either to apply for the first time, or to unapply it, so it doesn't sound like the calls to `fopen` and `fclose` would be the main cost in that case, but who knows? We haven't benchmarked it too much yet (we were waiting on the algorithms to be ready, and this is the first release where they really are!).
Is sonr open source?
That's essentially what my 'solution' with the `extract_bool!` macro does, but that requires 8 complex loads + 1 move to the SIMD register, instead of 1 single gather operation. It 'works', but the solution using gather _should_ probably work too, but I can't get it to work. But thanks for thinking along. :)
I'm not sure what your question is, but I think you want to derive PartialEq: #[derive(PartialEq)] enum Foo { A, B, Neither } fn main() { let x = Foo::A; if x == Foo::B { println!("Not this!"); } else { println!("Hello, world!"); } }
Thanks. Indeed, i used: "let strings: Vec&lt;&amp;str&gt; = "s".split(",").collect();"
Hi. I needed the same thing a few days ago, here is what I came up with. pub fn from_csv(csv: String) -&gt; DOK&lt;i64&gt; { let nrow = csv.lines().count(); let ncol = csv.lines().next().unwrap().split(',').count(); // That's my matrix structure, ignore let mut res = DOK { ncol, nrow, data: HashMap::new(), }; let mut lines = csv.lines(); for i in 0..nrow { let mut numbers = lines.next().unwrap().split(','); for j in 0..ncol { if let Some(c) = numbers.next() { res.set(i, j, c.parse::&lt;i64&gt;().unwrap()); // replace with your method to overwrite element at i,j } } } res }
&gt;This is the very reason why they are so popular and present everywhere. https://www.logicallyfallacious.com/tools/lp/Bo/LogicalFallacies/40/Appeal-to-Popularity &gt;It is a fact proven by decades of experiments and studies. You cannot deny that. ​I deny it. What definition even separates scripting language from non scripting languages? Dynamic typing? The set of studies on dynamic vs static typing are pretty inconclusive, with some studies showing dynamic languages are less productive and harder to maintain. A good summary is: https://danluu.com/empirical-pl/ &gt;Scripting allows mods, and having many language possibilities for that is very valuable. Some of the most modded games in history were moddable without scripting languages. Minecraft for example, or Quake3 I personally think the appeal of scripting is mostly misguided. I'm not the only one in the gamedev universe who thinks so.
&gt; Also: Wow, how much faster is this hashbrown implementation. How does it achieve this? Is it simply due to the SIMD optimizations? I think this is answered in the link given above. &gt; The GitHub issues title is: "Replace HashMap implementation with SwissTable (as an external crate)". So what does that mean? Is hashbrown still an external crate? But if it's an external crate then how was the old HashMap implementation "replaced"? It's still there, but I can use the external hashbrown crate if I want faster HashMaps? I'm confused. :-) As far as I understand, the old implementation is deleted, and `std` now depends on the `hashbrown` crate. You don't have to do anything to get up to 2 times faster hash maps, besides using Rust 1.36. You might want to consider to use a different hashing algorithm for further speed ups.
I think it's that sometimes they don't want to put the time into polishing things and making their software attractive until it's truly ready for that next step.
Can you say more about why the boilerplate bothers you? Do you spend a lot of time writing out error types? Fixing the `Error` trait isn't going to do anything to alleviate the boiler plate. The only thing left there is to provide a convenient way to get backtraces. But beyond that, the standard approach to error handling should continue to be what I laid out in my blog post, which gives you a choice between using a trait object and defining custom error types. I kind of suspect that once the `Error` trait is fixed, fixing the boiler plate will mostly be a matter of a better version of `derive(Fail)`. There are a few crates out there that purport to do this. For example, [`snafu`](https://docs.rs/snafu/0.2.2/snafu/). Although, `snafu` has a bit more than that.
Note that by default, Rust does not automatically implement any traits (besides `Sync`, `Send` and `Sized`). In particular, `PartialEq` is not implemented on your enum, so comparison fails. You can fix this by annotating your enum with `#[derive(PartialEq)]`. In practise, you probably want to derive `Clone`, `Copy`, `Debug` and `Eq` as well.
Perhaps it helps to think of "poll" in this context as "do some work". When a future is asked to "do some work", it proceeds with some computation. At some point, the future would block waiting for a network request, DB query, computation, or some other "slow" event. So the future has made some progress, but still has more to do, but cannot make further progress yet because it needs to wait on the above-mentioned "slow" event. So instead of blocking, it returns a status of "Pending" to its caller. The Future's caller understands what "Pending" means, because it's basically a mini-scheduler--Rust calls this an Executor. The Executor realizes there's no point in asking this Future to "do some work" again because the Future just reported it couldn't make any more progress. If it *were* to ask the Future to do more work right now, this would be *polling* in the wasteful sense that we want to avoid. So instead, the Executor hands the future off to another piece of code whose job it is to determine when it *would* be appropriate to ask the future to "do more work" again. This other piece of code is called the Reactor in Rust. The Reactor doesn't every ask the Future to "do more work", it just determines when it would be appropriate to ask, and returns the Future to the Executor under those conditions. With this design, the Executor can only ask Futures which *can* make progress to "do more work". It doesn't even have to skip "blocked" Futures as it works--every Future it holds is ready to "do more work". The fact that "do more work" is called `.poll()` can be misleading, but I think it is still appropriate, if one can think of a poll as a way to make progress by getting to the next checkpoint. The Executor does not know how many "checkpoints" a given Future has before it returns `Ready` with a result (ie. it does not know how many times a Future will return `Pending` before getting the final result)`, so one can think of this as a 'poll' of the Future to attempt to get a result. It's just not pegging the CPU in order to do it--it's only calling when it is appropriate to do so.
It's released under the MIT license. You can find the source here: [http://github.com/hagsteel/sonr](http://github.com/hagsteel/sonr)
The other answers are already great, but personally visualization help me understand such concepts. /u/jonhoo made a great video on the topic which goes very in depth. https://youtu.be/9_3krAQtD2k
&gt; What are the most popular frameworks/crates/general resources to get started with this in Rust? https://www.arewewebyet.org/ &gt; Does anyone have resources to help me get started or could tell me the common frameworks/tools used? You could look at the Parity ecosystem: https://github.com/paritytech
I don't see stabilising SIMD in this roadmap.
Thanks for this question. I often went with the `enum` approach but was never happy with it. Looking forward for good answers!
https://tokio.rs/docs/futures/getting_asynchronous/ https://tokio.rs/docs/futures/streams/
Unused `Debug` impls should never end up in the binary. My BLE stack has `#[derive(Debug)]` on pretty much every single type and fits in &lt;20 KiB. What Cargo profile are you using, and how are you testing this?
If the process path is determined by someone hitting a URL, it's a server, so you should use a server framework like Rocket. If it's something else, like if you want "debug mode" or have the program be for something totally different, I'd suggest Structop or Clap. You can use both together.
Ahh OK. So does that mean that this new implementation isn't even in `std`? That seems odd. Are there currently other functions in `std` that are actually external crates and not "really" a part of `std`?
I am building a static lib, but it could be that I am having some problems with linker gc. I will check out my call graphs and see if I actually have any hard dependencies on Debug.
I didn't know about gather/scatter, you probably don't but I learn a ton of things, thanks =) Anyhow, what do you think of this: let offsets_simd = isizex8::from_slice_unaligned(offsets); let bool_simd = cptrx8::splat(some_bools.as_ptr() as *const u8); unsafe {bool_simd.offset(offsets_simd)}; let bool_simd: m32x8 = unsafe {bool_simd.read(m8x8::splat(true), u8x8::splat(0))}.ne(u8x8::splat(0)).into(); It's basically your last example but it works for me.
I'm defining a lot of different structs, but there are a set of common fields I want to add to them all. Instead of adding them directly, it would be nicer if I could abstract them out somehow. I don't think the `..` operator allows for combining fields from struct definitions, so I've been thinking my best bet is using an attribute. I haven't really touched attributes yet though and don't know where to begin. Is there an existing crate which does this? Is it easy to create one myself? Is this the right way for me to add this? Basically I want to add `#[include(common_fields)]` on my structs.
Me so confused
It's also on Youtube if it's easier: [https://www.youtube.com/watch?v=2nhF62S5P3A](https://www.youtube.com/watch?v=2nhF62S5P3A) :D enjoy!
Hi there. Why in ```rust fn main() { use std::rc::Rc; trait A {} struct N; impl A for N {} let x = Rc::new(N); let temp = Rc::clone(&amp;x); let y : Rc&lt;A&gt; = temp; let z : Rc&lt;A&gt; = Rc::clone(&amp;x); } ``` creation of `y` works, but creation of `z` causes an error: "expected trait main::A, found struct `main::N`" (E0308). Also, why ```rust let z : Rc&lt;A&gt; = Rc::clone(&amp;x) as Rc&lt;A&gt;; ``` fixes the problem?
But I often find repos with great explanations of how to get the project running and so on, but they don't have a single sentence explaining what it is about/for.
Yes, Microsoft opening PDB format was a breakthrough moment. It was mostly for Clang, but everyone got benefited. It was a dump of header files, not a specification people expected, but it turned out that even Microsoft themselves does not have a specification!
`enum`s can be made extensible by marking them as [`#[non_exhaustive]`](https://github.com/rust-lang/rust/issues/44109) or adding a `#[doc(hidden)]` variant
Keep in mind that for embedded applications it's often important to minimize size, so `Box&lt;ErrorTrait&gt;` will be a bad fit not only because it requires `alloc`, but also because it always pulls quite heavy formatting machinery. Personally I think that the error code approach is a good one, especially for low-level interfaces. But yes, there is a problem that understanding a bare [error code](https://xkcd.com/1024/) can be quite hard? especially if error is bubbled or unwrapped. Ideally I would love to see some way to construct a global array `[(NonZerou32, &amp;'static str)]` elements in which can be "registered" in other crates. Using linear search we will be able to find error message associated with the given error code, plus if we don't use these messages anythere dead code elimination will be able to remove such array altogether. But unfortunately I don't think it can be done without language and compilation process changes. There is another `errno`-inspired approach (mentioned by you in one the previous discussion), i.e. we define (thread-local) global variable in which we store additional data regarding error, one advantage here is that it can be used not only for static error messages, but also for additional data which even can be dynamic if needed (i.e. you use one constant for messages and another one for pointer to associated dynamic data). But it feels pretty awkward and unidiomatic, plus I think dead code elimination will not be able to remove this functionality if you don't use it.
Ah, of course, same as for a struct. Thank you two!
By extensibility here is implied an ability to define custom errors in third-party crates. In the context of `rand` we want to define a trait and error type, which will be used by RNGs defined in other crates and those RNGs may want to define their own custom errors.
&gt; compile times will likely improve. They do, the compiler has gotten 1% faster, but the new hash table implementation is "heavier" to compile, so if your crate uses hash tables a lot it might actually take longer to compile it, even though the compiler got faster.
You can take a look at [the `Cargo.toml` for `std`](https://github.com/rust-lang/rust/blob/master/src/libstd/Cargo.toml) (it's (almost) just another crate). There's a few crates being pulled in by version number like `libc` and the backtrace support crates.
It's already stable ?
Why was the `Error` being removed? Is it because `!` is still not stable?
&gt; Tail recursion would be nice. It isn't *too* big a deal due to having imperative loops, but would still be useful for making tail calls to other functions. It's not just tail recursion but general tail calls that I'm most interested in. Things like recursive descent parser would be much easier to optimize with `become`.
Ahh I didn't know that `std`is just a "normal" Rust project. This is really cool. Thanks for the link!
It did not work well with async fn. You can imagine always having the error type be `!` except when the async fn returns `Result`, but that was weird special casing and I think there was an edge case which made it quite gross but I can't remember what it is now.
Oh, why is it always that simple/silly in the end? Indeed, the alignment was off. At first sight, using `from_slice_unaligned` solves it. I don't think I was reading "junk memory", though, according to the documentation of `from_slice_aligned` it checks at run time whether the alignment is okay, and panics otherwise. Probably the `4`, `5`, etc. values in the failed assertions are the number of elements by which the slice is out of alignment. That still leaves the weird "invalid monomorphization" error for the more straightforward solution, but at least this way I can continue. Thanks!
That's great news, I was wondering about it for a long time. Thanks for the update.
Thanks. There is one thing that I don't understand. Why do you used a file DOC, if you want to convert a file csv to a Matrix?
Maybe I'm misunderstanding this, but why does this have to be done with macros? Why can't you just do this with a normal function that accepts a closure?
Cool plan! Any idea what is still missing from custom allocators?
The code references `DOK`, which is, as /u/n-brr said, a custom matrix structure they're using. It doesn't work with DOC files.
Maybe you were thinking of `BTreeMap::new`, which used to allocate until https://github.com/rust-lang/rust/pull/50352.
\&gt; try to compress it to a minimal list of core facts and arguments that are strictly true &amp;#x200B; Agreed. It was simply tl;dr, and hasn't offered any interesting proposition, theorem, model, problem, question up-front :/ &amp;#x200B; \&gt; This is an example of empathizing with what I believe a common mental model of subtype relationships and pointing out that it's totally wrong for the task at hand. &amp;#x200B; Could you expand a bit on why it's wrong?
Thanks for the explanation, this makes a lot of sense.
I actually wrote an entire crate to do this in a fairly efficient manner called [data_reader](https://crates.io/crates/data_reader). You can think of it fairly similar to numpy's loadtxt function but just a whole lot faster. Let me know if it might be of any use to you for your needs.
I restarted and tried rustup -V and got 1.18 so I guess it must have updated event though the unclear prints
Sorta kinda both. I have less time, but I also really need rust-analyzer to mature before such a thing makes sense.
I don't know if this counts as a "big gain", but it's pretty low-hanging fruit: https://github.com/rust-lang/rust/issues/58465 TLDR: compile crate graphs faster by starting downstream crates' compilation before upstream has finished running codegen.
I actually wrote an entire crate to do this in a fairly efficient manner called [data_reader](https://crates.io/crates/data_reader). You can think of it fairly similar to numpy's loadtxt function but just a whole lot faster. Now if you want to see how to parse a file as raw bytes while taking into account the different fields you could take a look at the [parse_txt function](https://github.com/rcarson3/rust_data_reader/blob/master/src/reader/parser.rs#L18-L552). I know it might be a bit long but hopefully the comments in there should make it a bit easier to parse. If anything might be unclear let me know or file an issue on the repo because I want to make sure if anyone wants to modify it they can do so easily. Now once you have the raw bytes of everything you want I use a [macro to convert it over to any type.](https://github.com/rcarson3/rust_data_reader/blob/master/src/reader/macro_src.rs#L113-L161) The macro itself doesn't do anything too fancy, so it should be pretty readable as just regular text. I've been meaning to show how to use this with the [ndarray crate](https://docs.rs/ndarray/0.12.1/ndarray/), so I can put an example of how to do that on the repo by tomorrow if there's interest.
While this is interesting, this is `syn` s AST, which is very different from the one in rustc.
No need to be choosy, any anti virus snake oil will do.
I've used RustEnhanced for SublimeText, and I've had to shut almost every feature off; the auto-completion was too slow, incomplete, disorienting; in-line errors were obnoxious; the syntax highlighting is ugly and inconsistent; I don't need integrated build because "cargo build" is not a challenge to type. And none of these features are adding value for me. The point being, even if these things are just matters of taste, a single taste is not something that 'just works' for everybody.
Wise words
I am using `mpsc` channels for a single producer single consumer need. I basically just want a queue, but I am only accepting messages from a single producer. Should I be using something else?
That's interesting to me that `Future` is in `core`. Do you know if async/await will work in `no_std` once everything is in stable? Or maybe it already is, I haven't played with this stuff at all yet.
I agree with the analysis newpavlov, though there are a couple of other possibilities: - `struct Error(&amp;'static str)` avoids indecipherable error messages, but isn't ideal for matching on errors - placement new alongside fixed-size static or thread-local memory would allow the equivalent of `Box&lt;ErrorTrait&gt;` without a general-purpose allocator Usage of `Display` or `Debug` to "stringify" error details requires formatting machinery, however a trait doesn't require this (e.g. `Error::description` — I don't understand why this was deprecated).
Found it: ..."specifically planning to develop a plan of attack for custom allocators associated with instances of collections (e.g. Vec&lt;T, A: Alloc&gt;)."
I know everybody is sick of lifetime questions, but a simple PathBuf -&gt; &amp;Path plus a trait does not work and the compiler error message seems to be essentially *"don't do X, instead do X"*. The same with with just `impl structname` works. A trait `trait GetPath&lt;'a&gt; { fn path(&amp;'a self) -&gt; &amp;'a Path; }`, and a `struct Foo { file : PathBuf }` implementing it work, but as soon as I `actually_use_it(p : &amp;GetPath) { let _ = p.path(); }` the lifetime trouble starts. The contradictory errors are: expected &amp;dyn GetPath&lt;'_&gt; found &amp;dyn GetPath&lt;'_&gt; and earlier these identical ranges: note: first, the lifetime cannot outlive the anonymous lifetime #1 defined on the function body at 17:1... note: but, the lifetime must be valid for the anonymous lifetime #2 defined on the function body at 17:1... See [the rust playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4755a1267631783aa939b735b1dd26cc), the `let _ = p.path();` line is the culprit. I suspect a more complex lifetime annotation is required.
As a follow-up, this has been addressed now that 1.34 is stable.
It should, more or less. See https://github.com/Nemo157/embrio-rs for an example.
Also, the rust book and rustonomicon lifetime explanations don't go into that much depth, is there something more complex to understand lifetimes once and for all?
formatted: fn main() { use std::rc::Rc; trait A {} struct N; impl A for N {} let x = Rc::new(N); let temp = Rc::clone(&amp;x); let y : Rc&lt;A&gt; = temp; let z : Rc&lt;A&gt; = Rc::clone(&amp;x); } error: 9 | let z : Rc&lt;A&gt; = Rc::clone(&amp;x); | ^^ expected trait main::A, found struct `main::N` | = note: expected type `&amp;std::rc::Rc&lt;dyn main::A&gt;` found type `&amp;std::rc::Rc&lt;main::N&gt;`
[Lotus](https://github.com/NerdyPepper/lotus) \- a simple currency and number formatting library. &amp;#x200B; This is my first rust lib, written to get a feel for writing tests, docs, internal organization of modules etc. The library functionality itself is fairly basic, perhaps beginners like me could learn bit about writing libraries in rust! `Lotus` is a port of the popular JS lib, [accounting.js](http://openexchangerates.github.io/accounting.js/). &amp;#x200B; Feel free to ask questions here, drop an issue if you find one! &amp;#x200B; I will take on something larger for my next project, until then, its back to reading TRPL. &amp;#x200B; &lt;3
Just saw this on crates.io before seeing it on Reddit. Looks great!
For those of us who don't follow very closely, does this mean that async/await will be stable in 1.36 as well?
It's amazing!
`&amp;'static str` also has a bigger size than `NonZeroU32`: two words vs 4 bytes. IIUC `Box&lt;dyn Error&gt;` always pulls formatting machinery, because `Error` trait is an extension of `Debug + Display`. This is why this [code](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=09ab1e28bf75fdc68ebbdade94a1d1e8) works.
Servo is a browser engine, and on Wikipedia, [https://en.wikipedia.org/wiki/Browser\_engine](https://en.wikipedia.org/wiki/Browser_engine) one can read about browser engine The primary job of a browser engine is to transform HTML documents and other resources of a web page into an interactive visual representation on a user's device. and Besides "browser engine", two other terms are in common use regarding related concepts: "layout engine" and "rendering engine". In theory, layout and rendering (or "painting") could be handled by separate engines. In practice, however, they are tightly coupled and rarely considered separately. In addition to layout and rendering, a browser engine enforces the security policy between documents and implements the Document Object Model (DOM) data structure exposed to page scripts. It also handles hyperlinks and web forms. Executing JavaScript (JS) code is a separate matter, however, as every major web browser uses a dedicated engine for this In a Rust GUI program the JavaScript engine are replaced (hurrah!) by Rust ( I DON'T like JavaScript). In addiction there are also Stylo, which is CSS engine written in Rust. I would prefer something else than HTML and CSS, because it might be a very bad idea to make DOM and CSS a foundation for a GUI API.
Sorry but no. [https://areweasyncyet.rs/](https://areweasyncyet.rs/)
You're totally right, I thought the error was from the assert\_eq!() but it's the alignment difference. For the last error, I think it comes from [here](https://github.com/rust-lang/rust/blob/bfb443eb1de484fde141fa9090a9f4291cbe60a5/src/librustc_codegen_llvm/intrinsic.rs#L1468). But it should work or at least give a better error message. I opened an [issue](https://github.com/rust-lang-nursery/packed_simd/issues/237), we'll see what comes out of it.
Nice! Looking for ward to a std futures :)
This is the subreddit for the Rust programming language. You're looking for /r/playrustservers
the Future is here.
I did the same yesterday too, Tim. I *think* I like the format? :) I do enjoy seeing people's eyes light up, or that smile when the moment of comprehension dawns. This is the first time I've done "async" mentoring (I figured that term is apropos). :) Thanks for putting the word out there--mentoring here or anywhere else is a nice way to give back to the community.
Well the streams poll() method is called in a loop, but not in an *endless* loop. In the example, `Display10::poll()` does call `Fibonacci::poll(`) in a loop, but note that if `Fibonacci::poll()` would return `Async::NotReady`, `Display10::poll()` would break out of the loop and return `Async::NotReady` itself! That is not immediately clear from the code, due to the `try_ready!` macro that takes care of that. It's a handy helper but it obscures what is happening to the uninitiated. Now `Fibonacci::poll()` does not ever return `Async::NotReady` because there is *always* a next number in the Fibonacci sequence available in this implementation. However, if, say, it had to read data from a TCP socket to get the next sequence number, that data might not be available yet. In that case, `Fibonacci::poll()` would return `Async::NotReady`, and so would `Display10::poll()`. As Tokio is the entity that called `Display10::poll()`, it will not immediately call `Display10::poll()` again when the poll method returns `NotReady`. It will just do other things, or sleep, until it is informed that the TCP stream that is used by the Fibonacci stream has data available again - and the whole thing starts over again. So the "magic" thing is what happens behind the Tokio curtains to make that work. To find out more about that, read up on what an "executor" and a "reactor" is and how they work together - Tokio supplies both.
Tracking issue for `async fn` in `no_std`: https://github.com/rust-lang/rust/issues/56974
vscode with rust-analyzer is pretty good.
I think that's a good question. First, I wouldn't say it has to be done with macros. I did do it with macros, but this crate is certainly not intended to be the last word on the topic. It's just a small crate I extracted from one of my projects, as it seemed more broadly useful. Second, as for a specific reason to use a macro over a function with closures, you can't return from your outer function or break/continue your loop, if you're in a closure. However: - One of the macros, `repeat_timeout!` explicitly requires closures, so this advantage isn't realized there. - None of my usage examples make use of this capability anyway. So why did I use macros? Short answer, I don't know. I assume that there is either a non-obvious reason that will become apparent when attempting to refactor this, or (perhaps more likely) that a good reason to use macros once existed in my code base, but doesn't anymore. (For context, I wrote this some months ago, but only recently got around to polishing/documenting it, therefore my limited memory of my own reasoning. I definitely think that yours would have been a productive question to ask myself during the polishing/documentation process :-) ).
I recommend creating a new struct (component) for each set of common fields and find an appropriate name for it. You then add all the necessary components to your actual struct. This pattern is called composition. Let's assume you currently have defined two structs `Human` and `Parrot` with each some distinct and some common fields. Let's look at the hypothetical common fields `pos_x`, `pos_y`, `pos_z`, `vel_x`, `vel_y` and finally `vel_z`. From this, you can extract the two component types `Position` and `Velocity` both with the fields `x`, `y`, `z` (in real code, you'd use a dedicated `Vector3` type possibly wrapped inside a newtype). The refactoring replaces the old common fields with `pos: Position` and `vel: Velocity` (and that, you might also want to abstract into the component `Physical` or whatever). The memory representation stays the same after using composition because the components lie flat without pointer indirection. Composition is an established pattern and (imo) should be prefered to copy/paste-inheritance (what you are asking for). Nonetheless, I claim it's rather easy to write a procedural macro (especially if you are familiar with the declarative ones already). Start with [this docpage](https://doc.rust-lang.org/reference/procedural-macros.html), include [`proc-macro2`](http://crates.io/crates/proc-macro2), [`syn`](http://crates.io/crates/syn) (probably with feature `full`) and [`quote`](http://crates.io/crates/quote) and read their documentation (`docs.rs/&lt;crate&gt;`). Then, to become acquainted with writing macros, read any crate that exports proc-macros (search for `derive` or `attribute`) like [`derive_more`](https://crates.io/crates/derive_more).
Comparing equality on strings is just a memcmp, it's really fast.
Cool! Is that screenshot your actual terminal or just mockup?
There's no rule that you have to use multiple producers, so if you only need one then only use one. If performance vs a specialized spsc channel is the concern, then it's worth noting that the `std` channel [already has](https://github.com/rust-lang/rust/blob/master/src/libstd/sync/mpsc/mod.rs#L119-L148) optimizations for the spsc case.
I'm more interested about your riced up tmux(?) configuration. What do the hearts mean?
Thank you. This is the best answer for me. There's just one last thing that's not clear to me. Why would you ever need to poll more than once? The api that makes more sense to me would be future.run() to start the computation and something like future.wait() to block until the future comes back.
He has only two lives left.
https://docs.rs/linkme/0.1.1/linkme/ uses linker shenanigans to give you a `&amp;'static [T]` with elements from multiple crates, so that might be worth checking out.
Yep it is tmux, you can find my full config [here](https://github.com/NerdyPepper/dotfiles/blob/master/tmux/.tmux.conf). The hearts represent the current battery level :p. You can find that script [here](https://github.com/NerdyPepper/dotfiles/blob/master/scripts/scripts/bat).
Seriously! How do you get the screenshot to look like that?
It is my actual terminal :D &amp;#x200B; A tmux window having two horizontally aligned panes, with a slightly modified statusline. &amp;#x200B; Here's my [full tmux config](https://github.com/NerdyPepper/dotfiles/blob/master/tmux/.tmux.conf), you can find the scripts I used to produce that statusline [here](https://github.com/NerdyPepper/dotfiles/tree/master/scripts/scripts).
Does usage of two words here matter? Why would a mention of the `Display` trait automatically include all formatting machinery when this isn't used? If it does pull in unused code, that's an optimisation error.
Sweet! I'm glad you like it! Criticism is welcome, I'm looking to learn!
\^\_\^
Hi, is there a way to simplify this to remove the need to repeat `T: PartialEq` in the definition of `Bar`? trait Foo&lt;T: PartialEq&gt; {} trait Bar&lt;T&gt; where T: Foo&lt;T&gt;, T: PartialEq {} Since `Foo` already declares that `T: PartialEq`.
Looks great, thanks!
r/unixporn Cool stuff!
I guess what I'm unclear on is if the code inside the loop will return the entire function every time, then what is the point of the loop?
I appreciate the people that make this. It is simply amazing.
You have to look through the examples..
By using the same lifetime in the declaration of `GetPath`, you've essentially declared that the lifetime of the borrow and the return value must be as long as the the value itself. However `actually_use_it` involves two unrelated lifetimes due to elision; the signature is essentially `fn now_actually_use_it&lt;'a, 'b&gt;(p : &amp;'a GetPath&lt;'b&gt;)`. Those two lifetimes have no relationship to each other so the constraint from `GetPath` that the lifetimes be the same cannot be satisfied. You have three potential solutions - explicitly specify that the lifetimes are the same fn now_actually_use_it&lt;'a&gt;(p : &amp;'a GetPath&lt;'a&gt;) - use two different lifetimes but ensure that one of them lasts long enough relative to the other fn now_actually_use_it&lt;'a, 'b: 'a&gt;(p : &amp;'b GetPath&lt;'a&gt;) - do not use explicit lifetimes for this case - removing all explicit lifetimes from `GetPath` and the `impl` will work as-is
&gt;Does usage of two words here matter? It was discussed in one of `failure` issues, IIRC to optimize error size they want as far as to use double boxing. Either way having a smaller error (especially which fits into one register) is obviously better than a bigger error, but degree is of course discussable. &gt;Why would a mention of the Display trait automatically include all formatting machinery when this isn't used? Because in my understanding `Display` methods are included into vtable of `Box&lt;dyn Error&gt;` and AFAIK Rust currently can't remove unused methods from vtables (it will have to analyze all crates in a project to do that).
Interesting! Though the limited platform support is a big issue.
Use `let`: enum Foo { A, B, Neither } fn main() { let x = Foo::A; if let Foo::B = x { println!("Not this!"); } else { println!("Hello, world!"); } }
Only for short strings.
Yeah, but you don't often see extremely long strings as hashmap keys. Even if it is a pathological case, I feel like the tradeoff on smaller keys is justified..
It's still `O(n)`. Presumably there are use cases where`n` is large enough that it's not really fast, like storing chapters of a book.
Yes, it's possible and recommended. Cargo can be integrated into the build system, and the generated library can be linked by the build system.
I misread the title as "concurrency and number formatter library" and got really confused
That's it? It probably won't help all that much so let's not do it? From your example it might be smart if only to force English speakers to be agnostic of keyword meaning. But y'all are missing the big picture here. Clearly a non-native speaker is asking for it to be changed because maybe they don't know how to do it themselves, and you're telling them it will be better for them if they just use the default English keywords. What kind of attitude is that? I can see this developing into a huge issue one day.
This is just stabilizing the futures API in the standard library. Now that that's stabilized, everyone is going to be focused on stabilizing async/await.
Futures and Hash brown not mentioned in the updates to Rust Core? Those were 2 of the biggest changes to Rust in recent history! I guess they were late last night, so they might have just not made to cut to be included, in which case we should see them next week.
How do you get this cool window to just show code in?
oh no!! your beautiful tmux setup has distracted us all!
btw, what terminal emulator is that??
I think there are many things: - Accessibility: at work, I use CLion for my C++ code because the company pays for it so it's freely accessible; at home, I don't bother. - Project size: at work, I work on large projects, so IDE support is essential to navigate; at home, I play with small prototypes, I easily remember everything. - Number of contributors: at work, I work on a shared codebase, so once again IDE support is essential to navigate; at home, I play mostly solo, so once again I just know the code, or can easily dive right back into it. Of course, if I had a good quality, easy-to-setup IDE available I'd probably use it at home too; but since I don't, and since I'm used to do without with C++, I don't particularly miss it.
&gt; Too bad there's no variadic template like feature in flight to be stabilized. Do you really use them often in C++? It seems to me the most common usage of variadics in C++ are: - tuples, - variants, - perfect forwarding. In Rust, tuples and variants are first-class concepts and perfect forwarding is unnecessary in the absence of user-defined copy/move constructor. I have only done a bit of programming in Rust, to be honest; still, I've never really felt the need for variadics, beyond implementing traits for tuples which can be worked around. I do not doubt that there are usecases, and getting rid of macros to implement traits for tuples certainly qualifies, however I just feel there's much less pressure than there was in C++. Heck, I remember emulating variadics in C++03 using cons-lists for a variety of reasons; I could do so in Rust if needed, I just never needed to so far.
&gt; Agreed. It was simply tl;dr, and hasn't offered any interesting proposition, theorem, model, problem, question up-front :/ TL;DR should be so short that it goes so far as to omit information &gt; Could you expand a bit on why it's wrong? You can fined the quote in context here: https://medium.com/@orbitalK/rust-lifetime-subtype-variance-b58434fe36ed For normal subtypes, even with diamond inheritance, you can pretty much draw all the is-a relationships in a venn diagram using the visual overlap to indicate what is a subtype of what. In that system, the outermost types are parent types and subtypes occupy a smaller area. In lifetimes, the longer lifetimes are the subtypes of the shorter lifetimes. You can't use the overlap of lifetimes to guide intuition about the direction of subtypes. You have to focus on which can be substituted for which to interpret the correct subtype relationship.
My reaction exactly! Talk about a *fallow year*, those 4 features are HUGE! I'd be overjoyed getting half of them!
This echoes what mgatozzi said on internals: our tooling is optimized for hobbyists: https://internals.rust-lang.org/t/tools-team-tell-us-your-sorrows/9657/47?u=matklad Note that you can use IntelliJ for free, it doesn’t include debugger, but everything is the same.
Perhaps it's an existing codebase where the rust object files are just linked into the greater whole. rustc is good enough for just that.
I read the manual but it's still unclear to me how “feature branches” are handled. The documentation tells me they are not needed but I still don't understand what the actual workflow is. I think it will be a stumbling block for many people coming from git.
Sweet. Plugin updates are, by far, the most impactful change to my daily rust usage.
&gt; our tooling is optimized for hobbyists That's exactly my feeling yes. Not too surprising, either, seeing as: - The community is largely made of hobbyists. - The contributors are largely hobbyists. - The community and contributors are the ones deciding the roadmap. I expect that as the language starts getting used more and more by companies, the community (and mayhaps part of the contributors) will shift over time, and this in turn will shift the goals on the roadmap. I certainly would not mind a more conscious decision to go after companies more aggressively; supporting custom registries in Cargo was a major step in this direction, for example.
Not to dismiss your experience, but Sublime Text even with all the plugins you can imagine is still not an IDE. VS Code with Language Server based plugins feels better generally in my experience. But neither of these come close to the reliability, features, and cohesion that I get at work with Java in IntelliJ. It really is a different world.
One way that they are useful is that it is very easy and efficient to chain calls together, such as `xs.filter(...).map(...).filter(...).collect()`. At no point, except for when it is collected, does it actually iterate over all of its values, so it isn't creating an unnecessary collection at every intermediate step, but it still lets you separate out different parts of your logic into discrete blocks of code, if that makes sense.
Futures are mentioned in the Tracking Issues section.
Correct me if I'm wrong here, but there seems to be an increase in instruction count in the opt modes for many of those programs. Is that related to this change?
Will async/await rely on standard futures as a base or it will be a separate feature?
That's a good question, feature branches are just patches. You don't need to think as much about organising your work as you do in Git. Feature branches are just a way to simulate patch commutation, or in other words to make a "clean and tidy" branch with just that feature. In Pijul, this is just called a patch. My advice is to not use Pijul branches until you become very comfortable with Pijul. I don't use them myself on my local repositories (I do use them on the Nest to make my repositories easier to browse).
You seem to be under the impression that Rust has a runtime underlying futures; if so, that's not correct.
One of the point of feature branches is to be able to easily jump between a half-done and broken new feature and the working master. Or to jump on the half baked work in progress of a colleague. It's not the tidy part I'm having trouble with, it's the switching.
Just record the patch for the half-broken feature (the command is interactive), keep working on the other feature, record that, and then amend the first patch. Patches producible in parallel always commute, so you can edit two patches on the same branch completely independently (without changing the identity of either).
Not so much a runtime, but I'm surprised the feature won't require allocation in particular, which would disqualify it from no_std
If having a compiled async function somewhere in your binary required allocation, that would be a runtime. Remember, whether it's possible to actually write any Future or actually execute any async code in `no_std` is not the question. (It is possible, of course, but not the question.)
No, stabilizing async/await would be another issue similar to how Pin was stabilized before Future. Stabilizing async/await is dependent on stabilizing Future, but not sufficient to stabilize futures.
Both scenes look awesome to my innocent eye!
&gt;futures\_api Is there any good docs/examples for it yet?
&gt;stable Futures Where can I find examples for using Futures? The docs are just specs right now.
I'm not sure I follow, unless you are counting OS runtime. But by that logic, Vec would also require a runtime, and I've never seen it stated that way. I do know that rust itself has no runtime. But as I said, I haven't used async/await at all (at least, knowingly). I also don't know anything of the underlying process that allows the thing to work. And actually my question was intended to be about whether async code can run in no_std, not whether or not you can define a Future in no_std (obviously you must be able to since it is in core)
I just ported a pet (~200 lines) projection, it's pretty nice. &gt; do I use it with or as an alternative to crates like failure? It's an alternative, but it should more or less interoperate with errors created by `failure`. I prefer it to the latter. &gt; what are the upsides? - better compatibility with other errors (errors implement `std::error::Error`, no `.compat()` needed) - easier to get started with, mostly; I found the `failure` user guide a bit opaque - I'm not sure if that's how you're supposed to use it, but you can attach multiple context values to an error - better maintained &gt; what are the downsides? - the source field (inner error) is a bit quirky to use - not sure whether it supports tuple error variants - fewer users
Is the name a reference to [Lotus 1-2-3](https://en.wikipedia.org/wiki/Lotus_1-2-3)?
AFAIK, something like 7 bits of the hash is still stored. The final equality comparison (when you found the key you are looking for) is probably going to occur more often than random hash collisions anyway.
&gt; do not use explicit lifetimes Indeed, first I thought I oversimplified my code too much, but... it *is* properly elided - as I expected, actually, but somehow I started to add lifetimes and made everything worse. &gt; explicitly specify `&amp;'a GetPath&lt;'b&gt;` - that totally does not make sense to me. I thought lifetimes in `&lt;'...&gt;` can only be declared once. The first lifetime `&amp;'a GetPath` means the reference itself, and `GetPath&lt;'a&gt;` the value it points to? Should have learned that when elision was much worse to non-existent.
I don't like the something\_**str**, I know it's an str you want. instead perhaps thousand(), or thousand\_indicator(), or thousand\_separator() (technically it's called a separator), or even thousandth(). I know, a nit pick, but it's one of those annoying things in api's that bug me. Usually their is some type of name for the thing that the person writing the api just doesn't know about. Also, is there a way to indicate arbitrary separator placement? I know that in some India they use a different comma spacing convention 10,00,00,000 for example (ten crore).
What would indeed work for a one-time comparison, but the non-assignment despite the `=` is... weird.
Tombstones are most often than not elided. Consider the case of a 16 element group that has an empty slot (`F` is full, `E` is empty and `T` is tombstone): ``` F F F F F F F F F F F F F F F E ``` If you are to remove the 3rd F from the group normally the group would look like this: ``` F F T F F F F F F F F F F F F E ``` Instead you can elide the tombstone and result in: ``` F F E F F F F F F F F F F F F E ``` This is possible because this group already had an empty slot which means it always terminates probes coming through it. Elision in the face of floating groups is a tad more complicated but still possible (see [raw_hash_set::erase_meta_only()](https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h#L1417-L1436) for details)
https://rust-lang-nursery.github.io/futures-api-docs/0.3.0-alpha.14/futures/index.html is probably the best that I'm aware of.
It probably is. One tricky thing is that say, Cargo, also uses HashMap a lot, and new HashMap is faster to run but slower to compile. So new rustc compiling Cargo-with-old-HashMap is faster than old rustc doing the same, but new rustc compiling Cargo-with-new-HashMap is different matter... Does that make sense?
I've been working on the overall design of a program I'm writing with Tokio that uses both Futures and Streams. I have a lot of structs that act as config settings, some of which implement Stream because they are the source of data based on their own config settings. However, I have others which don't produce data: they just take a value and output a value, so it makes sense that those are Futures. Here's my problem: the only way I've seen to allow those Future structs to operate on some kind of data is if that data is part of the struct (i.e. a field value). If I do that, though, that means my program will have to create a new instance of that entire struct for every single value I want to process, which is a lot. Is there a better way I can do this where I just create the struct once and then operate on a value from another location outside of its own fields?
Will it print five crore and ten lack rupees? ;)
Have: `Path/PathBuf{ "file.abc" }`. Want: `PathBuf{ "file.abc.OUT" }`. Current ugly solution which only works with utf-8 filenames: let want = { let mut p = String::from(have.to_str().unwrap()); p.push_str(".OUT"); PathBuf::from(&amp;p) }; `set_extension()` won't help, `append()` adds whole path components. I'd also be happy with some `map()`, `filter()`, `collect()` etc. chains.
I think I managed to fix it. I had to do a 2 stage linking to get --gc-sections to work on my static lib. The final link would not do the --gc-sections properly, but if I first did a partial link (ld -r ) of the rust static library, and then added that to the object of the final link, I got a much more reasonable executable size. Thanks for nudging me in the right direction. /u/jshievink
It don't follow? No "function" is returned, it's just poll() method calls calling poll() method calls. Note that `Fibonacci` is a Stream, and `Display10` is a Future that drives the stream.
&gt; you can attach multiple context values to an error Interesting, how does this work?
What I mean is `try_ready!()` implements early returns, right? So the only time a value would be bound to a variable `let my_var = try_ready!(lkhjsdkf);` would be if `lkhjsdkf` were ready and returned a value, otherwise your entire `poll()` function will be returned early by `try_ready!()` right? If that's true, then I don't understand the loop since it wouldn't reach the end of the loop anyway, right? I am new at Futures (in case you couldn't tell) so this is probably just a dumb question.
You'd need to poll more than once in the case of a future composed of several other futures. Eg think of a case where you query a db, await the result, and then make a network request based on the result of the query. The composed future would contain within it both the future for db query and for the network requests. On first poll, the db request is sent, and `Pending` is returned. When the db request is completed and the executor is woken again, it polls the top-level future, which continues to do some work, namely making the network request. Since there's more waiting to be done now, the top-level future returns `Pending` again. Then when the network request is done and wakes the executor again, this time there's a `Ready&lt;T&gt;` instead of `Pending`, and polling is complete for that future.
Yes, and it would return to the calling code in the Tokio library. However, later on, when the Fibonacci stream is able to return at least one more item, Tokio will call Display10::poll() again, which will _continue where it left off_. Note that the counter value is stored in the Display10 struct, it is the "state" of that Future.
You probably want /r/playrust.
Nested errors, I assume. See at the bottom of the example here: https://users.rust-lang.org/t/snafu-0-2-1-released/27269.
Thanks. Does this seem like a good use? ping u/bea-b if you wana see let source = "mat.txt"; let params = ReaderParams { comments: b'%', delimiter: Delimiter::Any(b','), skip_header: None, skip_footer: None, usecols: None, max_rows: None, }; let input = load_txt_i64(&amp;source, &amp;params).unwrap(); let (n, m) = (input.num_lines, input.num_fields); let mut res = DOK::new(n, m); for i in 0..input.num_lines { for j in 0..input.num_fields { res.set(i, j, input.results[m * i + j]); } } After this brief try, I have two main suggestions: - provide a method on the result to access a piece of data with `(line, field)` coordinates, since you consider the input as a 2d array it feels relevant; - provide functions to directly read a `String`. And a couple of more futile ones: - this `comment` field could be an `Option` too; - provide some function to build common `ReaderParams` instances (e.g. specify a delimiter and consider the rest to be `None`). --- A few remarks/issues after experimenting a bit more after I began writing this: - it seems that the `usecols` field consider column numbers start at `1`, not `0`, this should probably be documented; - when the `max_rows` param is set, the `num_lines` is scaled accordingly in the result, but that's not the case with `usecols`/`num_fields`; - no check is made on whether the specified `usecols` numbers do exist, in relation with the precedent point it we can have non zero `num_*` while the `result` vec is empty. Would you like me to report this on the github repo for traceability?
I just did a second review of it today, and I'm thinking about trying it out in ripgrep core as an experiment to see how it works. I'm still pretty uneasy about the macro creating new types for you in a way that's pretty hidden, _but_ it does solve a few nice problems. e.g., It benefits from `From` impls without exposing those impls in your public API. Its transparent handling of backtraces in concrete error types also looks really nice.
Looks like the code on the left is just part of a normal source file but with enough empty lines above and below so we don't see other parts of the code. And the rest of the look is tmux running in a floating terminal window, using a window manager like i3wm with disabled borders.
Chapters of a book should be the value, not the key. That kind of pathological case exists, but in that case you should be designing a different custom structure yourself anyway.
And four numbers between commas in Japanese IIRC.
Thanks for the tip! I feel like the auto-complete is better when cargo check is enabled too
Thank you
If you use `OsString` instead of `String` then you at least gain the ability to work with non UTF-8 paths: https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=4d0147f683bdd0e9f255e50ae8b824c7
You can [push](https://doc.rust-lang.org/std/path/struct.PathBuf.html#method.push) to a `PathBuf`. Using `to_path_buf` on a `Path` should solve it for both.
It just feels a little ugly. Not very DRYish, having to implement a `description` and `fmt` function that both do very similar things, for example. I went and looked at the `errors.rs` module in my current project which follows your approach, and, OK, it is short - 55 lines. So it's not exactly time consuming. But I'm still unsure as to whether it is best practice when writing a library - aren't we supposed to 'hide' the types we are using, so is re-exporting them in my own error enum 'wrong'? I'm currently exporting errors from std::io, git2, walkdir and csv, for example. Also it's hard to recommend `failure` when you go to crates.io and literally the first thing you see is 'Experimental error handling abstraction' - and that's it. No explanation as to how to use it. I'm not even sure what it gives me. Perhaps there is a good blog post somewhere. Maybe the whole thing could be cleared up with some 'this is the best practice' guidelines. I tried to explain the error approach to some non-Rust using colleagues and they commented it 'feels odd'. Bottom line is this is basic stuff that programmers will hit in a day or two after starting with Rust, and it should not be hard or confusing or in a state of flux. I don't think it sends a good message to potential Rust programmers that the recommended crate is labelled 'experimental'. It would be nice to fix that in what is designated a 'year of stabilization'.
Yes, Korean and Japanese use 4 digits instead of 3 for groupings. The other rust crate I checked for this also had the same issue.
You can also disable swap buffering to control rendering yourself, or you can even write your own event loop if the standard one doesn't suit your needs. Piston is designed to be extremely modular and flexible, and the design encourages workarounds instead of providing a single abstraction that works for every situation. The default settings are design for First-Person-Shooters, which is usually wrong to use for GUI applications.
Thanks for the input :) A lot of these I hadn't really thought about, so it's nice to get input from other people. I'll see if I can't find some time this weekend to address most of these comments and push out a new point release. Yeah if you don't mind reporting this on the GitHub repo that'd be great, since it'll definitely help me keep track of things that need to be improved/fixed. Also, feel free to make an issue for any more ideas or potential bugs you can think of or come across.
Are you looking for `if let` (https://doc.rust-lang.org/book/ch06-03-if-let.html)? let m = MType.Bool(true); if let MType.Int(x) = m { // do something } else { // raise error }
Still using Rust 2015?
&gt; having to implement a description and fmt function that both do very similar things, for example. Note: `description` is deprecated, and it's totally fine to just omit it these days. &gt; But I'm still unsure as to whether it is best practice when writing a library - aren't we supposed to 'hide' the types we are using, so is re-exporting them in my own error enum 'wrong'? Definitely not. Take a look at the core libraries in the ecosystem, and std itself. All of them export their errors, where it makes sense to. &gt; I'm currently exporting errors from std::io, git2, walkdir and csv, for example. Ah yeah, I generally try to avoid this. But it's not really anything to do with error handling per se, and more just about keeping your public dependencies under control. If you don't _need_ to export those errors, then don't. This might indeed require a bit more work on the error type itself if you're publishing a library. For example, the regex crate specifically does not expose `regex_syntax::Error` in its error type, and instead converts it to a string. But using `Box&lt;Error&gt;` there would be okay too. &gt; Also it's hard to recommend `failure` Indeed, I don't. :-) At least, not yet anyway. With that said, there's certainly docs for how to use it. See this [guide](https://rust-lang-nursery.github.io/failure/) for example. The guide could use some work, but `failure` is in a bit of limbo state at the moment. So many people jumped to depend on it---even in its experimental state---that releasing semver breaking releases of it will cause serious ecosystem pain. So I think there's a "wait and see" mentality until backtrace support lands in std. &gt; I tried to explain the error approach to some non-Rust using colleagues and they commented it 'feels odd'. Bottom line is this is basic stuff that programmers will hit in a day or two after starting with Rust, and it should not be hard or confusing or in a state of flux. I don't think it sends a good message to potential Rust programmers that the recommended crate is labelled 'experimental' (edit: and it worries me that people need to talk about 'fixing' the Error trait). It would be nice to fix that in what is designated a 'year of stabilization'. The fixes to the error trait are exceptionally small. I guess, to me, error handling in Rust is just a matter of writing out error types and using them like you would any other value. But yeah, I agree, there is some complexity here. Mostly, my question was meant to probe at why the bare bones approach to error handling bothers you so much. The boilerplate-ness is something a lot of people mention, but it's hard for me to really appreciate that because, to me, error definitions take so little time and so little code compared to everything else. But sure, we still have work to do with error handling in Rust, and a big piece of it is coming to a consensus. That's hard work.
The results are in! Mostly very nice perf gains. Strangely a few but not all optimized builds actually regressed (especially cargo-opt) do you have any idea why this migth be?
If I may troll a bit, is it ok for the Nest to be closed source as it most probably use \`pijul\`?
Thank you. beside "features" is it possible to set such variables for conditional compilation? For example, can I define "dev" and "release" and use them accordingly, making each one compile in the correct context.
Nope. There’s just no hook to the profile available.
The best way is to match it with a `match` or `if let`. I changed your code with `match`, but it can be similarly done with `if let`: ``` pub enum MTypeError { NotAnInt, NotAnBool } impl Error for MTypeError { ... } impl Display for MTypeError { ... } pub enum MType { Int(i32), Bool(bool), } impl MType { fn try_as_int(self) -&gt; Result&lt;i32, MTypeError&gt; { match self { MType::Int(x) =&gt; Ok(x), _ =&gt; Err(MTypeError::NotAnInt), } } fn try_as_bool(self) -&gt; Result&lt;bool, MTypeError&gt; { match self { MType::Bool(x) =&gt; Ok(x), _ =&gt; Err(MTypeError::NotAnBool), } } } ``` So first I created an error type for your functions, then I changed the return type of those functions to be a `Result` which is either the specified value or an `MTypeError`. Then I just changed the match blocks to match the thing wanted and return it as `Ok(...)`. Otherwise (the `_` case) will return an error `Err(...)`. The important thing is to always return a `Result`, so it's either `Result::Ok(something)` or `Result::Err(an_error)`. Since the `match` only has two cases you can also apply: ``` if let MType::Bool(x) = self { Ok(x) } else { Err(MTypeError::NotAnBool) } ``` I omitted the implementation for the error type. This is usually a lot of work. I tend to use the failure crate to construct error types.
Am I the only one struggling to read a Rust Backtrace. I don't seem to understand the backtrace generated. Is there a good article/video/resource in decrypting a backtrace?
Your question doesn't make it 100% clear what you're looking for. Are you looking for a migration guide from futures 0.1 to std::future::Future?
How do you feel about an already existing federated Reddit alternative called [Prismo](https://gitlab.com/prismosuite/prismo)?
Nice! One day I hope to put the whole of rust-analyzer on the web: https://github.com/rust-analyzer/rust-analyzer/issues/1007 :-)
It *looks* like suckless.org st? His dotfiles have an alacritty config in them though, maybe it looks like that under 2bwm.
Looks good, but im not a huge fan of the interface, nor the fact that it doesnt have subs or communities.
Looks great. What's the underscore for in ``10_000`` and ``15_000``?
I'm struggling with the borrow checker and unable to understand what it is going on here. Here is the code: fn string_issues(&amp;x: &amp;String) -&gt; bool { return true; } This code will not compile. I'm not sure why. My goal is to borrow "x", a String. I'll do some immutable operations on "x" (comparing it to another string). So what it is exactly wrong here?
This is a desirable feature, but it's impossible to implement without hurting either performance in the general case or interoperability with C ABI, and those items took priority.
I've never gotten beyond `gnome-terminal`. What am I missing from `rxvt`/`alacritty`/et al?
I use it as a main Rust IDE now, occasionally highlights errors incorrectly here and there (especially for Future combinators) but in general works great albeit somewhat slow. A big missing feature for me is a support for cross development.
Every developer in Smalltalk and TurboPascal/Delphi has always used IDEs (well, Smalltalk invented the IDE concept). So, IDE support depends strongly on the language.
That's a [digit separator](https://doc.rust-lang.org/reference/tokens.html#numbers).
Humm, I actually forgot about `if let` but I still don't think it's the solution I'm looking for. if let (MType::Int(v1), MType::Int(v2)) = (v1, v2) { Ok(MType::Int(v1+v2)) } else { Err(Error::EvalError) } In the approach I presented I enjoyed the fact that I could make use of the `?` operator to avoid the `else` clause. let v1 = v1.cast_to_int()?; let v2 = v2.cast_to_int()?; Ok(MType::Int(v1+v2)) I guess it's a matter of getting used to it.
I currently have a somewhat finished Rubiks Cube implementation in C++ however to practice and learn Rust I wanted to port it over. There are 48 ways you can turn a Rubiks Cube which can be read here [https://ruwix.com/the-rubiks-cube/notation/advanced/](https://ruwix.com/the-rubiks-cube/notation/advanced/). Which one of these two methods would be better for a Rust approach? I use the first for C++. \`\`\` impl Cube { fn U(&amp;mut self) { // Rotate Pieces U } fn D(&amp;mut self) { // Rotate Pieces D } fn L(&amp;mut self) { // Rotate Pieces L } fn R(&amp;mut self) { // Rotate Pieces R } fn F(&amp;mut self) { // Rotate Pieces F } fn B(&amp;mut self) { // Rotate Pieces B } } \`\`\` \`\`\` impl Cube { fn Rotate(&amp;mut self, dir: &amp;str) { match dir { "U" =&gt; (),// Rotate Pieces U "D" =&gt; (),// Rotate Pieces D "L" =&gt; (),// Rotate Pieces L "R" =&gt; (),// Rotate Pieces R "F" =&gt; (),// Rotate Pieces F "B" =&gt; (),// Rotate Pieces B \_ =&gt; (), } } } \`\`\`
Nothing. They actually have "less" features. If you like simple or minimal things you might like st/rxvt/etc. Alacritty is GPU accelerated... for whatever reason.
&gt; I thought lifetimes in `&lt;'...&gt;` can only be declared once That's true. Any lifetime name can only be declared once. But you may declare multiple lifetimes and they can each be used any number of times as well as refer to each other for bounds (like `&lt;'a, 'b: 'a&gt;`). &gt; The first lifetime `&amp;'a GetPath` means the reference itself Correct. &gt; and `GetPath&lt;'a&gt;` the value it points to? Not so much. Your `GetPath` trait has a lifetime parameter which must be filled in at any usage site. Usually the compiler can fill this in for you, but not always. Under the lifetime elision rules, each reference and lifetime "hole" for inputs gets its own unique lifetime unrelated to any others. So `&amp;GetPath` is the same as `&amp;'a GetPath&lt;'b&gt;` to the compiler. Normally this is fine, and is why everything works out when you elide all the lifetimes. But your trait declares `fn path(&amp;'a self) -&gt; &amp;'a Path`, where the `self` reference and the output reference lifetimes must match that of the trait's lifetime parameter (they're all the same `'a`). So you cannot call `path` through an `&amp;'a GetPath&lt;'b&gt;` because the `self` reference must be at least as long as `'b` (due to the declaration in the trait) yet it cannot be any longer than `'a` (since that's the input lifetime and you cannot extend it). If it were known that `'a` outlives `'b` (`'a: 'b`) then things would be fine, but this is never done by elision. In other words, the lifetimes created by the elision rules are incompatible with your trait declaration because the inferred lifetimes are unrelated to each other while your trait requires they be the same. Specifying the same lifetime with `&amp;'a GetPath&lt;'a&gt;` matches the trait's requirements and succeeds. I should note that you probably want is to elide all the lifetimes (or at least remove them from `path`) because `fn path(&amp;'a self) -&gt; &amp;'a Path` is liable to extend borrows longer than you want.
Sorry for the late reply. I read that blog a couple of times, it actually helped me writing my linker script. I will follow the code in that blog when I switch from arm to x86\_64
Thank you!
You giving it a github star was what what brought me to look at it. Please share your experiences somewhere after trying it.
I'm using a fork of [`hexo-theme-cactus`](https://github.com/probberechts/hexo-theme-cactus). You can see the forked version [here](https://github.com/onatm/hexo-theme-cactus).
Small tip, use something other than characters that look very similar when grouped next time, the examples are hard to read.
Haha oh boy. Now I'm going to be all self conscious about starring stuff... :P
I currently have a somewhat finished Rubiks Cube implementation in C++ however to practice and learn Rust I wanted to port it over. There are 48 ways you can turn a Rubiks Cube which can be read here [https://ruwix.com/the-rubiks-cube/notation/advanced/](https://ruwix.com/the-rubiks-cube/notation/advanced/). Which one of these two methods would be better for a Rust approach? I use the first for C++. first for C++. this? impl Cube { fn U(&amp;mut self) { // Rotate Pieces U } fn D(&amp;mut self) { // Rotate Pieces D } fn L(&amp;mut self) { // Rotate Pieces L } fn R(&amp;mut self) { // Rotate Pieces R } fn F(&amp;mut self) { // Rotate Pieces F } fn B(&amp;mut self) { // Rotate Pieces B } } or this? impl Cube { fn Rotate(&amp;mut self, dir: &amp;str) { match dir { "U" =&gt; (),// Rotate Pieces U "D" =&gt; (),// Rotate Pieces D "L" =&gt; (),// Rotate Pieces L "R" =&gt; (),// Rotate Pieces R "F" =&gt; (),// Rotate Pieces F "B" =&gt; (),// Rotate Pieces B _ =&gt; (), } } }
Thanks for the link. Bookmarked for future ref. But in the meantime, I think I am just going to keep it 'bare bones'. The approach itself doesn't bother me too much, not knowing whether I am spending time learning something that might become obsolete or is not the best way to do it is what bothers me.
I don't think &amp;x means anything because x is the argument name and not an actual variable defined outside the function. Although I am still new to Rust and could be wrong.
I think that the modern term IDE has slightly different meaning than the one from Smalltalk era. It seems like now, when people talk about Roslyn or IntelliJ, they emphasize static on the fly code analysis, rather than integration. So, it’s more like Intelligent Development Environment. I’ve tried at once to switch to a more correct term, code analyzer, but it just doesn't stick with me (and, more importantly, others). Disclaimer: I actually don’t know Smalltalk, so I have no real idea how Smalltalk IDE looks like, but I trust [this](https://martinfowler.com/bliki/PostIntelliJ.html) (old) post by Fowler which says that IntelliJ and Smalltalk represent different stages of IDEs
This and match are the canonical way to test the value of an enum. Generating a PartialEq function for the test seems wasteful.
Mostly unrelated, but what is the best way to represent “arbitrary” error? Context: I write a lot of one-off applications, and, for these cases, I really would love the error model from swift/midori, where there’s a single error type. I can use Box&lt;dyn Error&gt;, but that’s too many irrelevant details (specific memory management and dynamic dispatch), not Send, is two words and doesn’t have backtraces. I can use (and actually use) failure::Error, but I don’t want/need other parts of failure, like Fail trait and derive macro.
&gt;I could do so in Rust if needed, I just never needed to so far Rust lacks specialization, so while you're not entirely wrong, there's not much you could do with them if you tried.
The github README doesn't have a lot of content. Do you have some information about the project?
I'd vote for just "separator()" - it's only for thousands once, and then only when it's a three-digit language.
Smart. Built-in `Stream` had 3 or 4 major revisions in Node.js and it has been a huge source of pain being coupled to the Node.js version.
&gt; impossible Really? From the postponement discussion, I was under the impression that it's impossible to have LLVM do the optimization at that level for all platforms, but I thought there was still hope at the MIR level. It seems we could do it with a combination of inlining and eager drops.
&gt;The question now, what does &amp;x means? Is it actually legal to do that? It's indeed legal to do that. The reason why it's legal is because [function parameters are patterns](https://doc.rust-lang.org/book/ch18-01-all-the-places-for-patterns.html#function-parameters). What that actually *means* is kinda hard to explain though. But in short, you wrote the pattern `&amp;x` on the left-hand side and then supplied `&amp;String` on the right-hand side. The pattern `&amp;x` matches against `&amp;String` and then binds `x` as a `String` (and *not* a `&amp;String`. And then since Rust doesn't like you trying to steal a `String` out of a borrow, it throws an error in your face.
Perhaps this: #[derive(Copy, Clone, Debug)] enum Dir { U, D, L, R, F, B } impl Cube { fn rotate(&amp;mut self, dir: Dir) { match dir { Dir::U =&gt; (),// Rotate Pieces U Dir::D =&gt; (),// Rotate Pieces D Dir::L =&gt; (),// Rotate Pieces L Dir::R =&gt; (),// Rotate Pieces R Dir::F =&gt; (),// Rotate Pieces F Dir::B =&gt; (),// Rotate Pieces B } } }
Hmm that looks good, but with 48 different options would it look better to have 48 different functions or 48 match cases?
This is called destructuring, which can be done with references, tuples, structs, etc. You can destructure a reference to a `Copy` type (like `i32`) and it does the same as a dereference. Its the same as when you do it in a let binding. let x = 7; let x_ref = &amp;x; // The following are equivalent let &amp;y = x_ref; let z = *x_ref; // y and z are both i32
I hope your request will get fulfilled at least partially. I think it would be disrespectful to hate on dentists because I believe they actually help so much and bad things such as gum disease are a bad risk that is important to be aware of. Hey, you know I heard that Australia is not real. I wanna say right here that I would absolutely love to be a mod on this sub; wouldn’t that be cool? It must feel so great when the power is in your hands, and getting a great deck of sports players is a cool example of this. Good luck about offers for you, I mean I hope that they benefit you for sure. I must say, providing highlights of things is nice for other people! Having fun and attending a roller derby could be a great way to ease one’s mind as they work things out. It’s very interesting how other hidden things and perspectives can change your opinion on something and this thinking could totally help you in this regard. Predicting the future is something that you can use to fascinate others to add to any success you get from the requests you made, so better start predicting now! Remembering simple things such as the difference between spring and summer; like how May and June are in the spring, can keep you from falling apart. Some advice to help might be that looks can be deceiving so don’t always trust someone to be smart just because they have glasses. Interruption causes corruption. It’s healthy to think of things as staying together and complete for your mentality, even if the thing is as simple as a bike you ride to work. Thinking about cute animals may calm you more. Payments should always be fair, let’s be real. Your post seems to show some meaning or desire. I wish you luck and, on this final note, hope any problems you may have with another individual get sorted out.
I'd suggest using type (and trait aliases - these require nightly). type TError = std::error::Error + Send + &lt;Some backtrace trait&gt;; type Error = Box&lt;dyn TError&gt;; fn a() -&gt; impl TError fn b() -&gt; Error
Oh, cool, it implements timers and uart for nrf51 using futures. Not sure if async/await works with no_std yet, but this is a big step in the direction of being able to write super readable async code on that specific microprocessor.
Fair point. But the root problem still remains: is the increased number of equality calls negligible on perf? Are there structures that make sense to be used as keys where equality would be the bottleneck?
I'd consider spliting up the options into parts: #[derive(Copy, Clone, Debug)] enum Move { Turn(Face, Amount, Modifier), Rotate(Rotation), } #[derive(Copy, Clone, Debug)] enum Face { U, D, L, R, F, B } #[derive(Copy, Clone, Debug)] enum Amount { CW, CCW, Twice } #[derive(Copy, Clone, Debug)] enum Modifier { Single, Middle, Double, } #[derive(Copy, Clone, Debug)] enum Rotation { X, Y, Z } impl Cube { fn move(&amp;mut self, move: Move) { match move { Turn(f, a, m) =&gt; {} Rotate(r) =&gt; {} } } } Especially since you can probably factor out some common code based on these parts, so you aren't copy-paste-tweak developing your functions :)
Even if you were willing to use nightly for this (and you shouldn't be, unless you're already committed to nightly for other reasons), this doesn't really address how the backtrace gets created in the first place.
I just use `Box&lt;Error&gt;` (or `Box&lt;Error + Send + Sync&gt;` ifneedbe) for one-off applications. The irrelevant details (memory management, dynamic dispatch) and the size (two words), aren't even close to things that I'd worry about in most cases. The key thing you're giving up is backtraces, and that's kind of where `failure` helps you. You really need some way to get the backtrace into your error when it gets created, and `failure` will do that for you (and so will `snafu`). I haven't tested it, but I wonder if this works using snafu: pub enum Error { #[snafu(display("{}", source))] Any { backtrace: Backtrace, source: Box&lt;std::error::Error + Send + Sync&gt; }, } impl&lt;E: std::error::Error&gt; From&lt;E&gt; for Error { fn from(err: E) -&gt; Error { snafu::Context { error: Box::new(err), context: Any } } } Then you can just treat `Error` as "any error," and it should come with a backtrace.
Oh of course, I wasn't suggesting switching to nightly *just* for this. I don't write production code in rust (unfortunately) so I mostly use nightly for the sweet new features. As for backtraces, yeah there needs to be a concrete implementation somewhere obviously. A quick look on crates.io finds [backtrace](https://crates.io/crates/backtrace). All you'd have to do is add it to your error type in a constructor and potentially manually implement `Debug`. Come to think of it there's probably no need for a backtrace trait in that case.
&gt; All you'd have to do is add it to your error type in a constructor and potentially manually implement Sure, but that's a huge pain to do. Go look at every place you use `?`. You'd need to add it manually there or insert `From` impls that do it for you. That's part of the pain that `failure` and `snafu` are trying to alleviate.
Console logging performance does matter to me as I regularly have thousands and thousands of lines printing a second regularly for hours if not days. It doesn't matter enough for me to actually check if alacrity is faster, but I do have some bad associations with the performance of gnome-terminal. I just associate it with gnome, though, whereas I prefer tiling WMs.
PHP dev here. It says on the homepage that it's for IDEA and related IDEs, does that mean that I can use this with PHPStorm? My employer is kind enough to let me use it at home so I can use their license.
Wrong subreddit, read the description
Practicing amethyst ahead of Ludum Dare this weekend
I think so. Look up the rusg plugin and give it a go. I use Intellij. The debugger only works in CLion though.
Fast, Correct (\* for base 10), Flexible, pick 2 * Floating point is fast and flexible * Fixed point is fast and correct * Decimal is correct and flexible
`Box&lt;str&gt;` looks weird. Why can't you use `String`?
I read the introduction on crate.io but I don't understand the significance of pull parsing? What does that mean? Does it work differently that other markdown parsers?
It basically means that it provides the document as a sequence of begin/end and text events, through an iterator. Most parsers build an abstract syntax tree, which is generally slower because the nodes of the AST need to be allocated. If you're just generating HTML, then that's extra unnecessary work.
&gt; I just ported a pet (~200 lines) project, it's pretty nice. If you ported from another library, I'd love to have your feedback on the appropriate issue ([from failure](https://github.com/shepmaster/snafu/issues/50); [from error-chain](https://github.com/shepmaster/snafu/issues/57); feel free to open a new one if from somewhere else)! In fact, if you have any feedback, I'd gladly accept it ;-) To chime in on a few of your points: &gt; better compatibility I'm also striving for wide Rust version support (back to 1.18, at the moment) *and* backporting the newer `std::error::Error` features for those versions (see [`ErrorCompat`](https://docs.rs/snafu/0.2.3/snafu/trait.ErrorCompat.html)). &gt; I'm not sure if that's how you're supposed to use it, but you can attach multiple context values to an error Absolutely. I'm a big fan of adding as much context as you can to an error to help the user. There's two types of context, I feel: how the error came about and details that went into it. For example, "opening a file" has no context in most applications. It's better to say "opening the config file" or "opening the log file". Then there's the details of the file. In SNAFU, that would look something like: ``` #[derive(Debug, Snafu)] enum Error { #[snafu(display("Could not open the config file at {}: {}", path.display(), source))] OpenConfig { source: io::Error, path: PathBuf }, // ^^^^^^^^^^^^^^^ The context // ^^^^^^^^^^^^^ context details #[snafu(display("Could not open the log file at {}: {}", path.display(), source))] OpenLog { source: io::Error, path: PathBuf }, } // ... fn example(path: &amp;Path) { fs::open(path).context(OpenConfig { path })?; } ``` &gt; can format `PathBuf`s You can call any arbitrary functions / methods in your `Display` implementation; being able to call `Path::display` is one common one. &gt; the source field (inner error) is a bit quirky to use Would you mind sharing the quirkiness? [I've an issue](https://github.com/shepmaster/snafu/issues/10) detailing some ways to make this more flexible; would those help? &gt; not sure whether it supports tuple error variants It does not. This is a deliberate decision, but I'd be open to well-reasoned counter arguments. Without a name, using the context information in e.g. `Display` is hard, as is constructing the error in the calls to `context`.
I don't, but the wall times uniformly improved even though instruction counts regressed on those ones. So it's all good :)
Or `HashSet::new`, which also allocated until https://github.com/rust-lang/rust/pull/36734.
In that case you probably want this: match (v1, v2) { (Int(v1), Int(v2)) =&gt; Ok(Int(v1 + v2)), _ =&gt; Err(...) }
We actually have exactly one sentence describing what glutin is for in our repos. It's the vaguest thing I've ever seen: &gt; Alternative to GLFW in pure Rust. Yeah, we could do better :/ Anyways, I didn't bother explaining what glutin is in the post because my reasoning was that it's a migration guide, and I sort've assumed that you're already using glutin and are looking for a quick-ish guide for upgrading. I'll be sure to include an explanation with what glutin's for, and update the readme with something more helpful before the next release.
&gt; trying it out in ripgrep core as an experiment to see how it works kirby-heart-eyes.gif Please let me know all the good and bad things that you discover! I've been trying to introduce it to more and more places to suss out how it works. The Playground [moved a while ago](https://github.com/integer32llc/rust-playground/commit/35d1e493c75fd3692f632bb0fc26c92f12aa0fd7), and I'm taking a peek at replacing error-chain in a reasonable sized code base to see if there are any pieces missing. &gt; the macro creating new types for you in a way that's pretty hidden I do think this will continue to be a big sticking point for certain people. In fact, it's been one of the main issues brought up whenever I've discussed it with people one-on-one. I do try to [make it clear in the user's guide](https://docs.rs/snafu/0.2.3/snafu/guide/the_macro/index.html#context-selectors) that types are created for you; I don't want these to be *hidden*.
`match` is the canonical way to do this. If you need to combine multiple enums, you can match them all at once, rather than nesting: match (v1, v2) { (Int(v1), Int(v2)) =&gt; Ok(Int(v1 + v2)), _ =&gt; Err(...) }
An error type is just a plain Rust enum, so context is provided by the enum variant name and context details are provided by fields in that variant: ```rust mod login { #[derive(Debug, Snafu)] enum Error { #[snafu(display("This account does not exist"))] NoSuchAccount, #[snafu(display("This account is locked until {}", until))] AccountLocked { until: chrono::DateTime } } } #[derive(Debug, Snafu)] enum Error { #[snafu(display("Could not log in as {}: {}", username, source))] Login { source: login::Error, username: String, password: String }, } ```
Thanks for that
The Rust plugin works with the free community edition of IntelliJ IDEA too, if that doesn't work.
Yeah, I am trying my best to look past it. One thing that might help is being *super* clear in the docs what exactly is generated. I (stupidly, in retrospect) kept trying to call `fail` on a context selector that had a `source` field. After thinking about it and more careful reading, I realized that was a nonsensical operation. The generics on the context selector also threw me off, but once I embraced it, I realized it was a nice ergonomic improvement that saved me from having to write some convenience constructors. All this info is in your guide, but it took a few careful read throughs to grok it all. I haven't finished converting ripgrep yet, so I'll probably have more feedback once it's done, but I've hit two things so far: 1. It seems tricky or impossible to define a recursive error type. If you basically try `source: Box&lt;MyError&gt;`, then you get an error saying it doesn't impl Borrow. You can add that impl, but context selectors don't really seem to work. (I'm on mobile now, so I can't elaborate on my use case. There are usually alternative designs that don't require a directly recursive error type, but they usually entail making more types.) 2. Sometimes I just want to manually construct an error variant with a source error (e.g., collecting a `Vec&lt;MyError&gt;`, and doing that is a little weird today since you have to do `Context { .. }.into()`. Might be nice to have a constructor (like fail) that accepts the source error (unlike fail) on the context selector itself.
&gt; Probably doesn't work due to the blanket impl. Indeed. (For those unfamiliar: [How is there a conflicting implementation of `From` when using a generic type?](https://stackoverflow.com/q/37347311/155423)). The closest I see immediately is: ``` #[derive(Debug, Snafu)] pub enum Error { #[snafu(display("{}", source))] Any { backtrace: Backtrace, source: Box&lt;std::error::Error&gt;, }, } impl Error { fn absorb&lt;E&gt;(err: E) -&gt; Self where E: std::error::Error + 'static, { Error::Any { backtrace: Default::default(), source: Box::new(err), } } } fn example(a: &amp;str) -&gt; Result&lt;i32, Error&gt; { a.parse().map_err(Error::absorb) } ``` You could create an extension trait on `Result` to do that for you though, then you could have `a.parse().absorb()`...
For sure. (It's why I put "less" in quotes, but I probably should have explained that.) Being able to Control+C a command with a ton of output instantly, or just the experience of lower latency/smoother experience in general is *very* nice. I'd consider it a feature.
The "rope science" link is dead: https://github.com/raphlinus/pulldown-cmark/blob/master/third_party/xi-editor/rope_science_00.md
Since I'm new to this whole directory business, could you explain how I would check whether I did cargo run in the right directory.
What do you think about just adding some simple test organisation features as a library? I spent a day writing some code that lets me declare groups of tests as a tree. It's inspired by the Haskell library [Tasty](http://hackage.haskell.org/package/tasty) : https://gist.github.com/dbousamra/615199173ccc384b86e64386c1186b69 Gives some output like this: https://imgur.com/a/C5vMzBz I didn't particularly have a goal in mind (other than learning how to write a macro), but perhaps it's worth me pursuing further.
This has probably been asked but I'm a newbie so here we go anyway. &amp;#x200B; Besides straight up telling me to read The Book and watch the first few videos that I see with the title "Rust Programming Tutorial" or something like that, could you tell me some \*effective\* strategies in not only just reading and listening, but also being able to process the information and get creative with it? &amp;#x200B; What I've tried is to read from The Book and kind of do like a 'personal documentation' kind of thing where I rephrase what The Book is saying, but first of all, that seems way too tedious, second of all, I might write in inaccuracies, and third of all I just want a better option. &amp;#x200B; For some reason I just have this ideal where I read it, and then it just all makes sense right then and there. For some other topics, you learn just from reading, but I heard you can also learn by \*doing\*. I want to learn how you can get to the doing part. &amp;#x200B; Yes, as a reply, you can suggest me some nice resources as well. Thanks.
As others have said, `if let` or matching on a tuple of enums is the idiomatic approach - but I have taken the `cast_to_int` approach in a lisp interpreter where it's used in functions in which it would be a runtime error to pass an incorrect type. It can make your code look cleaner if it's something you're doing a lot. I imagine you could also use TryFrom or TryInto - I haven't used this traits yet though.
Numerical literals in rust allow using `_` as a visual separator. It is ignored by the compiler. ``` 15_000 10___3000 _30 ``` are all valid numerical literals.
what you are looking at is tmux with two horizontal panes, the left one running vim (with the statusline and line numbers hidden) and the pane on the right running the program in quiet mode.
Write! The best way to learn is to write a lot. There are lots of collections of small challenges out there. Rustlings is a neat one, because it can verify your solutions, and it can monitor the solution file you are editing. [https://github.com/rust-lang/rustlings](https://github.com/rust-lang/rustlings)
 I chose lotus because it is the symbol of Goddess Lakshmi, the Hindu goddess of wealth ;) Never heard of Lotus 1-2-3, looks ancient!
Thanks, but as a Rust newbie, it would be a lot easier to grok if the docs had like an example or two... hopefully this will happen before release.
Does anyone know if std wil get a runtime for executing the futures?
If was pretty popular in it heyday, but Microsoft Excel eventually eroded Lotus 1-2-3's market share. I though of it because spreadsheet applications usually have some sort of currency formatting.
My point is that I don't need these features. So an IDE having more features is not offering me anything I want. What I want is for my programming workflow to be seamless using only a text editor and a terminal. *That's* the feature that is important to me. And IDE's remove that feature by definition.
Some way to organize test would probably an useful feature, and "groups" is kind of standard way to do. I hover find it insufficient. The problem is that often tests fit into many groups, or figuring out the right way to group test is challenging and best grouping keeps changing as the code grows and evolves. That's why I advocate for tagging as more flexible approach.
404?
Wow, you got unlucky... sorry about that. Should work now. Funny enough, I'm replacing a Docker Swarm cluster with a Kubernetes cluster right now (future article...), and when I rolled over this site 90 seconds ago, it used an old Docker image. Should be fixed now. ;)
As I understand, "portable" SIMD is not yet stable.
Congratulations!
`Box&lt;str&gt;` is just a string that doesn't need to grow, so it saves one word of size (the capacity). It's probably not worth the hassle over not using `String` itself. But for serious use one might want some form of [string interning](https://docs.rs/releases/search?query=string+intern).
Yeah, I've turned it off and used the `self.window.wait_event_timeout(Duration::from_millis(50))` to get the desired effect I need for my timer widgets. The problem I'm running into now is, inside that event loop, I now see the following error when trying to perform a draw: error[E0277]: the trait bound `&amp;&amp;std::option::Option&lt;input::Input&gt;: input::generic_event::GenericEvent` is not satisfied --&gt; src/core/main.rs:147:25 | 147 | self.window.draw_2d(&amp;event, |ctx, gl| { | ^^^^^^^ the trait `input::generic_event::GenericEvent` is not implemented for `&amp;&amp;std::option::Option&lt;input::Input&gt;` So, I'm not sure how to fix that just yet.
I found that when using failure, I had a hard time using a type that derived failure as a serde error type. Might try out snafu instead.
Is crossbeam-channel also replacing mpsc?
Haven't seen one yet, but [color-backtrace](https://docs.rs/color-backtrace/0.1.3/color_backtrace/) might help
what in tarnation. A kubernetes cluster for blog? I'd definitely would love to read the rationale behind that.
Hey, what's the story between glutin vs winit? Will some day everything move to winit?
What does your `Cargo.toml` look like? My first suspicion is that you used `crate-type = ["dylib"]` rather than `crate-type = ["cdylib"]`, so the Rust dependencies never got linked into the `.so` file.
I used to use hackerrank, but I recall something about rust support beinglacking/incomplete so I signed up to leetcode. Leetcode has been really good for rusting. The only nitpicks I have is that they're on 1.31 but in practice that doesn't mean much. I have also noticed their tree problems might have [a tad more complex type signature than warranted](https://www.reddit.com/r/rust/comments/acme4g/how_to_deal_with_a_tree_node_with/), but I'm not in a position to judge.
No, not really. Check out your sibling comment and the source, though.
["staticlib"] . If I dont use println! It works fine with just returning a pointer to a u8. But now I am having a similar issue with creating a vec! inside of the function I am calling. I might just ditch this project and just write it in c so I dont have to deal with the headache.
I have around 20 apps and services in the cluster with varying amounts of redundancy. My blog is just one of them. :)
The quick start page (https://intellij-rust.github.io/docs/quick-start.html) says: &gt;You can use any JetBrains IDE to develop Rust. If you are not sure which one to choose, download IntelliJ IDEA community edition, it’s free.
Missing dependency
Ok when using a dll it works. Just not a .lib
&gt; I'm also striving for wide Rust version support (back to 1.18, at the moment) and backporting the newer std::error::Error features for those versions That's nice, I suppose, although I think most people don't really care about supporting these old versions, and it's sometimes [detrimental](https://github.com/rayon-rs/rayon/issues/586). &gt; I'm a big fan of adding as much context as you can to an error to help the user. There's two types of context, I feel: how the error came about and details that went into it. Sorry, I didn't mean having two fields or the inner error inside the variant. I was referring to this snippet: let username = stdin .next() .context(UsernameMissing)? .context(UsernameMalformed)?; , although I later realized it's just nesting two errors, so not really specific to `snafu`. &gt; Would you mind sharing the quirkiness? I was missing the `source` field, and got compile errors, which you might argue is a good thing. I feel like the inner error should be optional. &gt; It does not. This is a deliberate decision, but I'd be open to well-reasoned counter arguments. Without a name, using the context information in e.g. Display is hard, as is constructing the error in the calls to context. That's totally fine, just that I was using those in my old version and I had to look over the docs for anything similar. You should mention that they are not supported. `failure` uses `_0`, `_1`, and so on.
Getting the cluster set up sucks, but having a distributed system automatically manage a bunch of services is *chef kiss*
Just say no to builder objects with their long chained calls and bloated interfaces that adds nothing except double the size of your code base and can be confusing af I'd they do anyone anything more than just set a value (but then why use them at all). I hated Java's decent into builder madness.
An alternative way of doing the build method is [this](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=6e07fdc738c4b3a64776e4967630bf46): trait TypeIdentity{ type Type:?Sized; } impl&lt;This:?Sized&gt; TypeIdentity for This{ type Type=Self; } impl&lt;A,B,C&gt; StateBuilder&lt;A,B,C&gt; { fn build(self) -&gt; State where Self:TypeIdentity&lt;Type=StateBuilder&lt;Initialized,Initialized,Initialized&gt;&gt; { State { x: self.x.unwrap(), y: self.y.unwrap(), z: self.z.unwrap(), } } } Which produces this error message Compiling playground v0.0.1 (/playground) error[E0271]: type mismatch resolving `&lt;StateBuilder&lt;Initialized, Initialized, Uninitialized&gt; as TypeIdentity&gt;::Type == StateBuilder&lt;Initialized, Initialized, Initialized&gt;` --&gt; src/main.rs:120:44 | 120 | let sb = StateBuilder::new().x(1).y(2).build(); | ^^^^^ expected struct `Uninitialized`, found struct `Initialized` | = note: expected type `StateBuilder&lt;_, _, Uninitialized&gt;` found type `StateBuilder&lt;_, _, Initialized&gt;` &amp;#x200B; &amp;#x200B; About being able to change the type parameter when doing struct updates,here is [some discussion](https://github.com/rust-lang/rfcs/pull/2528) about it. &amp;#x200B; &amp;#x200B; &amp;#x200B; Once const generics are a thing and fully implemented, the problem of proliferation of type parameters can be solved by using a struct const-parameter. This is pretty speculative, since some of this may not work in the first implementation of const generics. I imagine something like this: struct Rectangle{ x:u32, y:u32, w:u32, h:u32, } struct RectangleInit{ x:InitState, y:InitState, w:InitState, h:InitState, } impl RectangleInit{ const FULLY_UNINITIALIZED:RectangleInit=RectangleInit{ x:InitState::Unitialized, y:InitState::Unitialized, w:InitState::Unitialized, h:InitState::Unitialized, }; const FULLY_INITIALIZED:RectangleInit=RectangleInit{ x:InitState::Initialized, y:InitState::Initialized, w:InitState::Initialized, h:InitState::Initialized, }; pub fn init_x(mut self)-&gt;Self{ self.x=InitState::Initialized; self } pub fn init_y(mut self)-&gt;Self{ self.y=InitState::Initialized; self } pub fn init_w(mut self)-&gt;Self{ self.w=InitState::Initialized; self } pub fn init_h(mut self)-&gt;Self{ self.h=InitState::Initialized; self } } enum InitState{ Initialized, Uninitialized, } trait IsInitialized&lt;const STATE:InitState&gt;{} impl IsInitialized&lt;InitState::Initialized&gt; for () {} trait IsUnitialized&lt;const STATE:InitState&gt;{} impl IsUninitialized&lt;InitState::Uninitialized&gt; for () {} struct RectangleBuilder&lt;const INIT:RectangleInit&gt;{ inner:RectangleBuilderInner, } #[derive(Default)] struct RectangleBuilderInner{ x:Option&lt;u32&gt;, y:Option&lt;u32&gt;, w:Option&lt;u32&gt;, h:Option&lt;u32&gt;, } impl&lt;const INIT:RectangleInit&gt; RectangleBuilder&lt;INIT&gt; { pub fn x(mut self,x:u32)-&gt;RectangleInit&lt;{INIT.init_x()}&gt; where ():IsUninitialized&lt;{INIT.x}&gt; { self.inner.x=Some(x); RectangleBuilder{init:self.inner} } pub fn y(mut self,y:u32)-&gt;RectangleInit&lt;{INIT.init_y()}&gt; where ():IsUninitialized&lt;{INIT.y}&gt; { self.inner.y=Some(y); RectangleBuilder{init:self.inner} } pub fn w(mut self,w:u32)-&gt;RectangleInit&lt;{INIT.init_w()}&gt; where ():IsUninitialized&lt;{INIT.w}&gt; { self.inner.w=Some(w); RectangleBuilder{init:self.inner} } pub fn h(mut self,h:u32)-&gt;RectangleInit&lt;{INIT.init_h()}&gt; where ():IsUninitialized&lt;{INIT.h}&gt; { self.inner.h=Some(h); RectangleBuilder{init:self.inner} } } impl RectangleBuilder&lt;RectangleInit::FULLY_UNINITIALIZED&gt;{ pub fn new()-&gt;Self{ Self{ inner:Default::default() } } } impl RectangleBuilder&lt;RectangleInit::FULLY_INITIALIZED&gt;{ pub fn finalize()-&gt;Rectangle{ Rectangle{ x:self.inner.x.unwrap(), y:self.inner.y.unwrap(), w:self.inner.w.unwrap(), h:self.inner.h.unwrap(), } } }
What happens when the data layout of a struct changes between versions? Does the compiler somehow know about that?
Thanks for sharing that. That was extremely helpful in understanding the concepts.
I have no direct experience with `staticlib` and the docs I search up seem to indicate that it should work, so I can't be much help for the specific problem, but I'll see what I can do with my more general knowledge. &gt; I might just ditch this project and just write it in c so I dont have to deal with the headache. Hopefully not. I really want to know what little "nobody thought to document it because it was obvious to them" thing you are tripping over. When you say `std::io::write` wasn't working in the title, how exactly are you making use of it and did you test using without any calls to `println!`? I ask because both `println!` and `vec!` are Rust macros and I'm wondering whether that's significant in some way.
Ahh, you're on Windows. I'll do what I can, but I only ever wrote Python for Windows in any serious fashion and it was back in the Windows XP era and before.
The library will get recompiled anyways, so as long as the public API is the same, things should continue to work. If they don't, then you make sure you're using the right version with `"=x.y.z"` instead of `"x.y.z"`. In fact, the reason why using Rust functions and types for dynamic/static libraries is discouraged in favour of `extern "C"` and `#[repr(C)]` is specifically because the Rust ABI is unstable and likely to change between versions.
Without macros it compiled and ran. It was not until I used macros that it messed up. Its a simple interop right now. Just a function that returns a cstring to C so I can output it. Its just the beginnings of a bigger project that rust was going to make easier. The dll does work though and fixes the issue, I just wanted static linkage so I didnt have to include the dll.
I think you might be talking about a slightly different scenario. If I have a dependency on crate `foo` at version `0.1` and a transitive dependency through crate `bar` on crate `foo` at version `0.2`, then a function in `bar` returns me a struct defined in `foo`, and the struct has had its layout changed between `0.1` and `0.2`, what happens when I try to use that struct with `0.1`? Will the compiler not let me?
What would be ideal is a `next()` call that implements a timeout, so that `None` can be treated as a timer. So far, I see _no way_ of setting a timer in the `next()` call that Piston provides. `next()` provides an `Option&lt;Event&gt;` where `wait_event_timeout` provides me an `Option&lt;Input&gt;` which is _not_ the same thing. I need to be able to wrap a draw around the event, so the `Option&lt;Input&gt;` is entirely useless. I'm kind of stuck.
Ah, wasn't trying to ridicule or anything. I honestly have been meaning to find an excuse to use it for personal stuff too. I just always thought it was an overkill for personal blog etc. but yea, I'd like to read how others are using it.
Yes, the compiler forbids that - even if the struct is the same.
I think you must have some misconception about how systems programming works. When in a high-level language — even Rust itself — functions close over "value[s] from another location" they are actually creating structs containing, in one of several senses, all of the data they're closing over. Moreover futures are like nullary functions (i.e., they don't take any parameters) so *all* of the data they operate on must be closed over. Without async function support you have to write that closure manually as a struct, but it's the exact same thing that's happening behind the scenes in Rust closures and other languages' closures. Just because you can now see it instead of it being hidden doesn't actually change anything about the code. What you *can* do is put the repetitive config information behind `Rc` or `Arc` or even plain references (if possible, depending on your control flow and how you're handing the future off to an executor). This will probably be its own config struct. You'll still be creating a `Future`-implementing struct for each piece of data you're operating on, though — that's just how it works.
Any plans to target WebAssembly? Having a compliant (CommonMark or GFM) markdown renderer in wasm would be outstanding.
That makes a lot of sense. I guess that's one area where dependency hell is not "solved".
I doubt anyone can help you (besides your not-exactly-a-fix of switching to `cdylib`) without seeing a lot more code, and the linker command you're using. That said, what it looks like to me is that `Write` is the first/only thing you're pulling in from `std`, and that you're not linking all of the libraries required by `std`, and that resolution happens before linker gc (as it presumably must). It's not impossible to link a rust staticlib without using cargo itself to invoke the linker, but it's going to be unnecessarily difficult if you're pulling in `std`. You may want to do it the other way around instead — use cargo to link your C components, which rust has direct support for using (iirc) the `#[link()]` attribute on each extern "C" name. You could kind of cheat and get the benefits of both approaches by using the `--verbose` flag to build a trivial rust binary linking to your crate and seeing what its linker commands look like. But that could be subject to change with future Rust versions.
How are you linking your C program? Looks like you need to link against some Windows DLLs.
No, it won't, but part of the new API is the task module that includes the primitives that are used to build a futures runtime.
But perf.rust-lang.org doesn't compile `rustc`, `hashbrown` or `std`. But I often notice regressions there, so it might be something else. Some of them do actually go down, but I would have expected bigger impact, especially as `rustc` is supposed to use a lot of hashmaps. And of course, an increase in instruction counts might actually [improve](https://perf.rust-lang.org/?start=&amp;end=&amp;absolute=true&amp;stat=task-clock) the running time.
Nope. I just had to link against a rust dll. Not a static lib.
I fixed it due to another comment , i will put an edit. Had to link to a cdylib not a staticlib. Wanted static linking.
Nope, you should troll people casually starring horrible projects and watching people go crazy debating why it's a good one.
Compile Error. Even if they had the same layout, it would still be a compile error. 1 is a `foo_01_Baz`, the other is a `foo_02_Baz`.
While unrelated, keep in mind that STDOUT in Rust is buffered, which means you may want to flush it if you are mixing C and Rust I/O. use std::io::{stdout, Write}; #[no_mangle] pub extern "C" fn do_stuff() -&gt; i32 { let stdout = stdout(); let mut stdout = stdout.lock(); writeln!(stdout, "Hello, world!") .and_then(|()| stdout.flush()) .map(|_| 0) .unwrap_or_else(|e| e.raw_os_error().expect("Operating system error")) }
I haven't really played with it (other than just doing a cargo build on the wasm32 target), but it should Just Work.
I am using it when building the actual parsers, for example when building the simple xml parser from the article fn attributes() -&gt; impl for&lt;'a&gt; Parser&lt;&amp;'a str, Output = ..., Error = ...&gt; { ... }
IMO [projeteuler.net](https://projeteuler.net) and [adventofcode.com](https://adventofcode.com) are really good. They are language agnostic, and less focused on things like ranks and points.
We should show derived metrics like IPC on there.
Having a personal kube cluster for a bunch of services is pretty convenient on eg Google Cloud. Highly recommended.
There they say: “Since we last featured it on the Radar in January 2015, we've seen steadily increasing interest in Rust. Some of our clients are now using Rust, mostly in the context of infrastructure tooling but also in high-powered embedded devices. Interest was fuelled by a growing ecosystem as well as improvements to the language itself. The latter included straightforward performance improvements but also changes that make Rust more intuitive, for example the change to non-lexical scoping. Most of the significant changes are included in the Rust 2018 standard released last December.” Disclaimer: I am a ThoughtWorker, and act as Rust advocate internally.
Object file you mean rlib and such? But then what is the link with RLS? I am definitely missing something but using cargo to generate these files to be linked later on looks simpler than using rust directly. The reasons I can think of are - they use some forked rustc with some special flags? (Embedded etc?) - they don't want to rely on yet another tool - legacy: they have a build system using rustc directly and upgrading it to use cargo is not worth the effort - some obscure parameters are just not available via cargo (this is the real reason why I'm asking)
Multiple versions of a dependency is often surprisingly painless. Where it becomes a problem is when the duplicated library is in the public api of the crate you are using, and you are using types/traits in your own code. This can cause obscure compiler errors and incompatibility issues, especially around traits.
no, it's not. the point of using a language is that it makes it easier to write programs. if you just drop your language's common features to use the obscure ones that let you write low-level code, then you're better off just writing in ASM or C. this claim "wow hello world is just as small in rust as in C" can only be done if Rust itself is smart enough to compile println! down to that stuff. If it isn't, then it's not more compact and there's more work to be done on the compiler. No point using Rust like C, use Rust like Rust.
One more reason not to use lazy_static in libraries i guess. Cargo lint warning?
I think [ipgeolocation.io](https://ipgeolocation.io) will be a better option to use as ip geolocation api. As it has avast database which is updated weekly on regular basis. [IPGeolocation](https://ipgeolocation.io/) provides three APIs: * [IP Geolocation API](https://ipgeolocation.io/documentation/ip-geolocation-api-201812061140) IP Geolocation API can be used to get real-time and accurate geolocation for any IP address. You can geolocate your online visitors and provide them customized experience accordingly. * [Timezone API](https://ipgeolocation.io/documentation/timezone-api-201812061213) The Timezone API can be used to get accurate, real-time date, time and timezone information. * [Astronomy API](https://ipgeolocation.io/documentation/astronomy-api-201812061214) The Astronomy API can used to get the location-based rise and set times for the Sun and the Moon, including current position, angle, and distance of the Sun and the Moon for a date. It is quite easy to use and having upto 99% accuracy at the country level and 75% at city level. [ip geolocation](https://ipgeolocation.io/) has very low latency and pricing schedule. **Because of these features, we can say**[**ip geolocation api**](https://ipgeolocation.io/) **is the best service among all.**
You should use `print=native-static-libs`, see https://users.rust-lang.org/t/c-to-rust-ffi-library-linking-in-windows/27385/5
&gt;* Why does Rust need macros for even the simplest things? I'm looking at you `println!()` and `format!()` and `vec!()`. &gt; This is nothing to do with syntax. Using macros makes all of those variadic. Notice that (for example) `println!` checks _at compile time_ that the number is expected arguments in the format string matches the actual number of arguments that you provide. This is amazing, but couldn't be done with _only_ a different choice of syntax for functions.
He addresses this specifically in the article. It's worth noting that both versions of the library can coexist in your final binary, they just can't interoperate with each other, which may not be a problem.
The compiler won't let you because it treats the struct from `0.1` as a different type than the one from `0.2`, even though they have the same name. This is basically a natural result that comes from the chosen symbol name algorithm.
Yeah, it's not totally painless, but it sure is better than the alternatives.
FWIW, I'm using DigitalOcean managed Kubernetes. I don't get paid enough to use kubeadm. ;)
Oh, hmm. Did I miss that, or did OP edit the article?
Which Rust DLL did you link against?
Very similar to what NPM does as far as I see? Would be nice with a comparison to this as I am sure there's more people familiar with that than Java/Composer
No, just because sometimes you have to go really low level it doesn't mean you shouldn't benefit from other features.
As far as I understand, async! and await! are basically “only” macros that expand to code very similar to futures version 0.1. So yes, kind of migration guide and futures backstory.
Sorry new to Reddit and was looking to chat about Rust. I'm sorry if I posted in the wrong area.
You should probably use Rayon; I remember someone (/u/carllerche?) saying Rayon's scheduler is optimized for running the whole thing as fast as possible, while futures-pool is a fair scheduler (which probably has some overhead)
Nope, unedited so far. :)
My 0.02: It probably would not be fine if the getters would not have `as_` prefix (and therefore would conflict with the constructors). But considering very popular libraries already use this pattern ([another one, toml](https://docs.rs/toml/0.5.0/toml/value/enum.Value.html)), it is fine. It may cause some confusion though if the value gains more single-word methods.
thanks, this seems to answer my question! a few more minutes of googling would have shown that Tokio seems geared primarily toward network/IO.
Other than avoiding global state in our libraries, are there any guidelines for how to write libraries that play nicely with this? I can easily imagine the hypothetical log crate writing to a default log file, and the two versions attempting to write to the same file causing problems.
I think that should be solved by the compiler being smart enough to figure out that two types would look identical to the user and adding a hint about possibly different library versions.
This is about a hashmap. All we are doing is fast here, it is very hard to rule out something without benchmarking "because it is very fast".
True, but also a memcmp on an average key-sized string is like, literally single digit cycles.
&gt; * |a, b, c| { a + b + c } for closures. What. Were. Y'all. Thinking? Instead of something that actually looks remotely like a function, like (a, b, c) =&gt; { a + b + c }. And now you can't tell the difference between a function with 3 arguments and function with one argument of trple tuple type.
this is not a case where you should have to go low level. there should be a compiler flag to optimize for size, and it should take care of this for you. printing output is not such a complex scenario where the user is required to babysit the language.
It's nice when it *just works**^(TM)* like this in Rust, when you'd have to deal with difficult issues in other languages instead
Ah thanks! Yes at least the top 7 bits are stored. At least, that divides the need to check for equality when the key are not equal by 128.
Do ThoughtWorks provide Rust support to companies?
[warmy](https://crates.io/crates/warmy) is a crate that provides scarce resource loading and hot-reloading. The default behavior is based on a synchronous polling that must occur in some kind of manual event loop. Shared references observe changes on disk on logical (computed) resources. The crate includes a system of resource dependencies, allowing to reload specific resources if another is reloaded (hence, yielding resource dependency graphs). Since version **0.11.2**, it’s possible to switch the internal representation from ref-counted references to atomic-ref-counted references, allowing to share resources between threads. The current async-IO situation is a bit blurry as I would like to polish warmy to make it use an API Future-like (but I’m still ensure how to). Future enhancement: - Have an IO loop thread that distribute (schedule?) the IO work so that less context switches for IO-heavy operations (that should provide a stream-like interface). - Green / light threading. - Possibility to change sources of resources. Currently, the default (and sole) IO resource source is the filesystem (with a very simple yet working VFS implementation). I would like to provide people a way to plug “resource sources” like network, archive, etc. - Possibly everything you might in mind? :) A big thangs to /u/icefoxen for his tests, ideas and help on warmy, especially with [ggez](https://crates.io/crates/ggez). Go check that crate out, it’s worth it! Bisous !
I thought there was an unnecessary amount of snark against Java here. Apache Maven has been in widespread use for well over a decade. This has been accompanied by a community who adopted Sun's principles of cautious change to APIs, with interface abstractions and compatibility being highly valued. In practice this provides a robust and predictable experience for resolving dependencies. As a real example, consider Apache Commons, which despite being a grab-bag of useful functionality, decided it was better to adopt a [new namespace](http://commons.apache.org/proper/commons-lang/javadocs/api-3.7/overview-summary.html) than cause breaking changes. A counter-example would be Google, who are not a very good citizen with their libraries. Occasionally (usually when writing plugins for products) you _do_ run into dependency clashes. The Maven Shade plugin, which has also been available for more than a decade, can handle this for you with name-mangling - much like outlined here. It's a relatively rare event when you have to bring this gun out, especially with today's focus on smaller services.
Yeah, their effort is awesome. Especially because RLS is available as well, but for IntelliJ they're building a whole different system.
Even if the strings vary in size (and pairs are equal in size)? If that's true I wonder how it could get rid of branch misprediction.
Yes, they were late I believe. But you're right, two awesome additions!
Rust strings aren't null-terminated, they store the length in the header (same with string slices, too!), so comparing strings of unequal lengths won't even _get_ to a memcmp - it'll just compare the lengths and bail immediately.
ThoughtWorks' focus is on helping companies build great digital products using modern technologies (and Rust is obviously one of them). Providing support for specific technologies isn't something we normally do. That said, send me a message if you want to explore further.
&gt; It was discussed in one of failure issues, IIRC to optimize error size they went as far as to use double boxing. Interesting. Can you link? Because my testing showed no benefit to size optimisation.
Not what I meant at all. If I run a bench in which I compare two strings that do have equal length pairwise, but varies during the iteration (so what would happen in a real life bench, precisely because rust stores the length on the string and won't call memcmp if they don't match). I don't see how memcmp could remain 1 digit in average with that setting... but maybe they have a solution?
The [link /u/davemilter posted](https://users.rust-lang.org/t/c-to-rust-ffi-library-linking-in-windows/27385/5) reminded me of the knock-on effects of [rust-lang/rust #56568: Remove dependency on shell32.dll](https://github.com/rust-lang/rust/pull/56568). I think the macros might be a red herring and, rather, that they just happen to add dependencies beyond the ones that are handled by default, which `staticlib` requires you to manually ask for in a build script.
What if you have a mixed workload, or if you're reading from disk? In .NET, the thread pool is very smart about varying the number of threads while checking whether the throughput increases or not. So it works very well regardless what you're throwing at it. It would be nice if `rayon` supported that, but unfortunately it's not planned.
I don't see any magic in this implementation at least. [https://github.com/intel/intel-ipsec-mb/blob/master/include/memcpy.asm](https://github.com/intel/intel-ipsec-mb/blob/master/include/memcpy.asm)
Oh yeah, I see what you mean now. I'm not counting the branch prediction penalty here, just the comparison itself. It's really fast, especially on modern CPUs with stuff like AVX.
There is an established way to combat the problem of multiple semver-incompatible versions of a library called [the semver trick](https://github.com/dtolnay/semver-trick/blob/master/README.md). I haven't been able to use it in a library of my own yet, but it seems tantalizingly clever.
Yeah but AVX only kicks in for rather large strings. &amp;#x200B; Back to the magic discussion, I wonder if it could be beneficial for strings that one know are smaller than let's say 16 bytes, to check if ptr + 16 - 1 is on a different page (assuming a uniform distribution, that only happens with a proba of 1/256), and if there are no risk of segfault run the memcmp over the entire 16 bytes and then mask according to the string lengths to remove the branch mispredictions.
Thanks. They are just test scenes to match the Rust vs. C++ output, but some of them look great.
Could an editor be built upon `pulldown-cmark` ? How could it operate without an AST ?
You can mix rayon-futures (or futures-pool) and Tokio on the same program, and mix their `Future`s freely, so a Rayon computation may depend on some Tokio I/O for example. Each future will run on its corresponding runtime when polled. So it at least runs. Now, I'm not sure if Rayon (or futures-pool) would be too eager and starve Tokio or anything like that. Perhaps open a an issue on them? It would be very cool to have multiple runtimes cooperating to not starve each other of resources. Perhaps we should have some kind of "hypervisor" that could manage this (this resembles Erlang's supervisor trees to me). Another thing it could do is to ask the OS if it's getting out of memory and instruct the runtimes to drop some caches or something.
How can you mask according to string lengths without branching though?
Types can't protect against changed functionality between versions. Consider `subtract(a, b)` vs `subtract(b, a)`
perf.rust-lang.org definitely does compile the new HashMap, because it is a generic type that is instantiated.
Tokio and friends give you _concurrency_. What you seem to be looking for is _parallelism_.
Something like that. https://godbolt.org/z/MxBJAv Just compare the entire 16bytes and then mask using `(1&lt;&lt;strlen) -1`. (It might be better to do it with SSE2 __m128i but I am lazy)
In such a case, you wouldn't see an obscure compiler error though, right? If it won't compile, and the error would output a type name which has the possibility of being non-unique; i feel like the compiler could do \*something\* to make that more clear. &amp;#x200B; I suppose there's nothing really to be done about the sort of problem in your example. For all anyone knows, that's the behavior you wanted.
`push()` results in "file.abc/.OUT", sorry, I meant that, not `append()`
Oh. I thought you were trying to avoid the branch that compares the lengths and somehow mask it to two different lengths. For a case like this, glibc basically already does what you're suggesting.
So, in the terminal, before the cursor (where your typed text appears). There’s a little string showing which directory you’re currently in. Ensure that this matches up with the file you have open in your editor
Asynchronous tasks rely on scheduling and task switches. They are useful when they wait for stuff the majority of the time, so that you don't waste time by waiting and can do something useful in the meantime. If your task is CPU-intensive, it'll try to occupy as much CPU as possible constantly. It won't be waiting. Using concurrency (that's what this asynchronous thing is about) will make these tasks interrupt each other, but normally, only one task will be run at a given moment. Which doesn't make your CPU-intensive tasks run any faster, of course. What you're looking for is Calle parallelism, that is, execution of many pieces of code _at the same time_ (not in the interleaved manner as it's done with concurrency).
Ahhh I missed that. But it's an `OsString` internally anyways, so you can do `PathBuf::from(have.into_os_string().push(".OUT"))` without any losses.
Thanks!
This is similar to the way NPM handles dependencies, as I understand it, and yet Node gets all kinds of flak for huge numbers of dependencies while Cargo is hailed as having "solved dependency hell." What's the difference? The first idea that comes to mind is that each crate-version only exists on disk in one place, ~/.cargo/registry`, rather than having a tree of `node_modules` directories. It seems like there should be more to it than that, though, given how the responses are polar opposites.
I think that's pretty much it, you can't see the modules source in your project. Also rust doesn't tend to have tons of tiny modules like node does.
Could npm end up having the same dependency duplicated several times over?
Post your code and the exact error you're getting.
Thanks for the bisous
why use `mem::replace` instead of the original code? Are there any advantages?
That's mostly it. Npm doesn't even try to reduce the number of different versions of a library used, so it's a very inefficient solution, even though the approach is basically the same concept.
&gt; As far as I understand, async! and await! are basically “only” macros No, `async` is far more complicated than that. But async/await aren't stabilized yet anyway, just the Future trait and related things.
"quite" as in "quite a bit" sounds like a typo on "quiet" as in "shhh, quiet down". This does look quite useful, and we shouldn't be quiet about it! Syntax suggestion #[option(-s, --format &lt;format&gt;, "format output")] #[option(-r, --recursive, "recursively")] #[command(rmdir &lt;dir&gt; [otherDirs...], "remove files and directories")] vs. #[command(option(-s, --format &lt;format&gt;, "format output"), option(-r, --recursive, "recursively"), rmdir &lt;dir&gt; [otherDirs...], "remove files and directories")] to avoid the issue regarding the loss of span information.
Ah, gotcha. Yeah, I do that too – the consumer of such a parser library should always be able to use the impl trait syntax, ideally. I don't really mind having to make `Map` and such separate types since that's more of a one time thing.
Glibc really does that? Do you have something I can read about it?
/r/playrust. Also, cheating is bad.
It doesn't exactly do the "read until the end of page" trick, but it does use machine wide loads to speed up memcmp and I believe sometimes uses masking for the last block.
The code I saw for memcmp had a switch case for the last len%4 bytes. That's the branch I am talking about.
They actually do the opposite thing, judging by [this](https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/x86_64/multiarch/memcmp-avx2-movbe.S;h=d779639a6137b967104ee17b5dedaf4cee4d3fd2;hb=HEAD) - they make the last load overlap the _previous load_, so they don't even need to do masking.
Yesterday it occured to me when discussing in a group that it might be a good idea to have a cheatsheet to list just the types of parameters and return value of some functions. Meaning of many functions is obvious enough with just the name and such type information. So I made this cheatsheet. It's more comprehensive than I initially planned. Hope it may be useful to others.
&gt;A big missing feature for me is a support for cross development Could you point what exactly do you need? At least, cross-debugging between C/C++ and Rust works out-of-the-box in CLion.
I recognize your answer to my crate announcement. Thank you for sharing your thoughs, that's inspiring!
Do I read correctly that this version only contains clippy fixes? If so, why does this need a new Rust release? Couldn't the updated clippy be rebuilt against 1.34.0?
Not to discourage you, but I suppose you are aware of [`structopt`](https://github.com/TeXitoi/structopt#example)? It's more or less similar, but it won't hurt to have more than one choice. &gt; Since Rust is a static language, the compiler needs to know all the details at compile time. It conflicts with the dynamics of the CLI. I'm not sure what it means. &gt; I developed homepage for it. See ./homepage for more details. It's developed bashed on React. I like it. I didn't see that coming :-).
Because clippy is distributed along a rust release. If you install clippy from rustup, you cannot update clippy without updating the rest of the toolchain. So the compiler seems to be the same, Cargo too, and so on, it just the clippy component that got modified, but it needs a full release to make it available.
[removed]
I have opened an issue about it some time ago (don't remember its number). In short it would be nice to be able to specify a triple target for the compiler and/or source parser. So for example I can stay on Linux environment and do a Windows app. Currently IDE assumes host environment as a target and so the sources are not correctly processed and errors highlighted. Also relevant for embedded targets with conditional compilations or for example different wchar sizes (16 vs 32).
Oh right, that s so much simpler and better ! Thanks.
Glad the answer was helpful! Let's make a simple, contrived example to help illustrate why `.poll()` is called more than once: let's say you are servicing a web request containing a read request and an auth token. When you receive the request, you'll pass received auth token to an external service. If it comes back valid, you'll then read the requested data from your local DB. Checkpoint 1: receive the web request, and send the auth token to the 3rd party service. The round trip takes several hundred milliseconds to come back and your webservice needs to scale, so you don't block. So instead, you return `Pending` at this point. Checkpoint 2: Once your responce from the 3rd party auth service have arrived, you want to read it, and assuming it is valid, fetch data from your DB for the user. Again, you don't want to wait, so you again return `Pending` at this point. Checkpoint 3: The DB has retrieved your data, and you are now able to return it to the user. Because this is the 'final' answer in our contrived example, you return `Ready&lt;SomeType&gt;` to at this point to the caller, indicating the `Future` has resolved. Now looking at this from the Executor's perspective, let's say the Executor called `.process_web_request()` (please forgive the oversimplification of this example) and got back a `Future`. The Executor has no way of knowing how many times this `Future` will have to be `.poll()`ed before it resolves (indeed, it may already be resolved) so `.poll()` will need to be called from 1 - n times to get the `Ready&lt;T&gt;`. In this example, I created three checkpoints (meaning `.poll()` would need to be called three times), but in the event the auth check failed, the DB read step would be skipped, and perhaps an auth failure error would be returned. This illustrates that not only are the number of `.polls()` required unknown, the number may change dynamically depending on the flow you design (this is the state machine Boats refers to in their talk). So in practice, Executors will keep calling (only when they can make progress) until `Ready&lt;T&gt;` is returned, and as a result, you are free to design your flow to have as many checkpoints as you need to achieve whatever you are trying to accomplish.
Simply having little dependencies can help a lot. Adding features to decide which dependencies you actually need is great too.
JavaScript has such a small stdlib that we've gotten basic language features implemented 4000 different ways.
What would you replace the OP's code with, then ?
Rust definitely tends towards tiny crates. Perhaps not as tiny as in the js ecosystem, but way smaller than what most other programming communities are used to. It's not uncommon to have 100-200 transitive dependencies in a Rust project, even in smaller ones.
This is awesome.
Nice! Could you put up a single column version as well?
I have to say I haven't yet done too much with rusts testrunner, but I would like to add some points from my experience from other languages. &amp;#x200B; The test framework I most like are [doctest](https://github.com/onqtam/doctest) and [Catch2](https://github.com/catchorg/Catch2) (C++). They take the no dsl to an extreme: Their asserts are simply int x = 1; int y = 2; CHECK( x &lt; y); The check macro parses its argument to determine the expressions that are actually compared. I believe this would be very easy to implement with rust macros. &amp;#x200B; But what I really like is the tree like structure of their tests: TEST_CASE("root"){ common_setup(); SUBCASE("sub1"){ sub1() ; }; further_setup(); SUBCASE("sub2"){ sub2(); }; } Runs: * Test1: * common\_setup() * sub1() * Test2 * common\_setup() * further\_setup() * sub2() &amp;#x200B; Which is really genius for statefull objects. The one test per method of JUnit and similar approaches seems to only work well for functions which are strictly data-in data-out. &amp;#x200B; I am not sure what exactly you mean with naming test cases, but I would agree that finding a unique function name for every test case is uneccessary (and mashes badly with generative or table based testing). However I do believe, that every test needs a description describing what is actually tested and purely tagging is not enough. &amp;#x200B; I like having a defined execution order (or at least reporting order). I normally set my tests up like an onion, the first tests test the smallest components, then the next tests test composites of these components ... Then the first test which fails is the test I have to look at. &amp;#x200B; To your last point I agree that setUp hooks are a bad idea they are just too inflexible. (the main reason why I like doctest and Catch so much is because they handle setup code so well.). But I disagree that a complicated setup indicates bad API or test design.
Roses are red, violets are blue, yorick is een mietje, that much is true.
Sorry, I meant, that is was too long for me, so I haven't read it. (Just scrolled down to skim the headings.) And it did not offer anything up front to keep me interested/motivated to read it to the end. Thanks for the lifetime subtype details.
Sorry and thanks. My mother tongue is not English. So some contents in my mother tongue is easy to understand, but It's not easy for me to translate them into English and keep their meaning. And homepage is under development, due to my negligence, this paragraph was incorrectly added. Thanks for those all.
I think it's a good habit to use it instead since it is more powerful in cases where the type isn't `Copy`. Moreover, I think the reduced number of `let` bindings is helpful.
Greate idea. And thank you for pointing out the mistakes. I think you have understood the issue. :)
I don't follow this. Where am I wrong? For all that matters the special version of a library that my dependency is using can be swapped by a fork of it made at that version that got 'renamed'. Then what's the problem? Is it that renaming the library is too hard or not properly defined?
I want an A2 poster of this to put on the wall at my office!
`push()` has no return value (it really should have), this chaining won't work :). Also, `into_os_string()` requires a `PathBuf`, not a `Path`. let want = { let mut p = PathBuf::from(have).into_os_string(); p.push(".OUT"); PathBuf::from(&amp;p) }; really seems to be the shortest solution as u/FenrirW0lf pointed out.
I love that all of the functions are links to their documentation! I’ve never seen that with cheatsheets
I think it was this [issue](https://github.com/rust-lang-nursery/failure/issues/9), but looks I've remembered it somewhat incorrectly (i.e. it was discussed, but not decided).
if you want this kind of thing just use normal tcp and not async api I guess, or maybe even wrap the normal tcp inside a future with your custom logic or something
Can you prepend "fn " to them so I can ctrl+f for them without ambiguity, or is there an alternative?
Oh, don't worry about the language. I suppose a lot of us aren't native speakers. My comment about the dynamic nature of the CLI was prompted by the fact that the commands an application supports don't really tend to change at run-time (except the git/cargo plugin thing, but that doesn't count). But my background is mostly in statically-typed languages, so someone more used to JavaScript could very well think otherwise. It's all right to have a homepage, but a little unusual. I appreciate that you excluded it from the package crate (`exclude` in `Cargo.toml`).
Rust hasn't solved dependency hell. It has carefully designed around certain parts of it. The underlying issue -- dependency management is Complicated -- is still there.
Os error 2 is usually "No such file or directory" (ENOENT), so I'm guessing the path that was supplied to rustc doesn't point to an existing entry on your filesystem. But in any case, we need more information. What was the exact command you ran in the command prompt, and what was the exact output?
Yeah, smaller than C++ or java (or even python and ruby), but still not as tiny as js, with it's single line modules. For comparison, I've got a medium sized react app with 2686 transitive dependencies
I wanted it like 2 days ago. You rock!
You can do this yourself with $('main').style.display = "block" $('main').style.transform = "none" Or you can highlight and drag this text into your browser bookmarks as a bookmarklet and click it: javascript:void(($('main').style.display = "block") &amp;&amp; ($('main').style.transform = "none"))
A naive solution would be to consider different versions of a library to be different libraries, as if they had entirely different names, and have as many as necessary of those running simultaneously. When does that approach fail?
In C++ in particular, I think this is 100% percent due to the difficulty of using dependencies. Even just *building* a project with around 10 different dependencies will usually take am afternoon or two of troubleshooting. Adding a dependency to your own project is much harder and can take many days in the worst case (the worst case being that the dependency also has dependencies and is using a different build system than you are). If most C++ projectd used, say, Conan+Cmake, I think the community would soon gravitate towards having more and smaller dependencies in their projects.
'$' is jQuery's doing
Not sure how the compiler would be able to do this without brittle heuristics. I think it is only given crates, without knowing that they are two different versions of the same library.
It's addressed toward the end of the article, in the "All Together Now" section: &gt; Since different versions produce different unique identifiers, we can't pass objects around between different versions of a library. For example, we can't create a LogLevel with log 0.5.0 and pass it into my-project to use, because it expects a LogLevel from log 0.4.4, and they have to be treated as separate types.
$ is commonly jquery, but vanilla javascript also has `$` which is just `document.querySelector`
But it's solved. `foo::0.2::Bar` and `foo::0.1::Bar` are different types, so you get a type error. If you want to interface between those, you have to convert them to one another, or to some other type. The compiler tells you "these types are different", and then its up to you to do whatever you want. Many libraries offer compatibility layers, that allow you to convert a `foo::0.1::Bar` to a `foo::0.2::Bar` and vice-versa.
Nice :). If you don' t know it yet, you can cache your dependencies in your Dockerfile, and build on musl to run on alpine (it creates really small builds), see this gist: [https://gist.github.com/terry90/cbb5337335d3252146ef92b62969f83d](https://gist.github.com/terry90/cbb5337335d3252146ef92b62969f83d)
This has been a repeatedly reported issue with Rand: [1](https://github.com/rust-random/rand/issues/766) [2](https://github.com/rust-random/rand/issues/754) [3](https://github.com/rust-random/rand/issues/732) [4](https://github.com/rust-random/rand/issues/645) [5](https://github.com/rust-random/rand/issues/695)
in chrome $() is equivilent to document.getElementById(), not document.querySelector() That means you can't grab elements based off of class or their relation to other elements.
$ is not even aliased to anything by default. It's just a plain variable name that can be used for anything.
[https://developers.google.com/web/tools/chrome-devtools/console/utilities](https://developers.google.com/web/tools/chrome-devtools/console/utilities) it's a part of the command line API
gotta get those clicks from r/rust
Try opening a new browser tab and typing `$` - I get `function ()` in Chrome and Firefox.
Inconvenient, it is. I wish rustup have incremental update feature.
Chrome and Firefox. Though it's odd that Chrome implies that it's `document.getElementById(id);` because it definitely works like `document.querySelector`. Try it - I'm also pretty sure the linked page does not include jQuery: https://builtwith.com/?https://upsuper.github.io/rust-cheatsheet/
jQuery isn't included on the page
Read my edit. Some pages, such as when you open a new tab in chrome, alias it as document.getElementById(). It's annoying, but it very much works exactly like document.getElementById(). Opening about:blank or some other page would preserve it as somewhat equivalent to document.querySelector(). It has an extra parameter, which is why I same somewhat.
TIL that I can stop writing `console.table` and just use `table`. Thanks for the link!
This is amazing! As somebody that can't understand why Rustdoc outputs things the way it does, this is incredibly readable.
Yeah, same here, no problems writing it in the lib for exactly the same reason.
Thank you for this.
`flat_map` is not a method of a collection, it's a method of the generic `Iterator` trait that you can return from anywhere. You could add a `flat_map` method to, say, `Vec`, that just returns `self.iter().flat_map()`, I guess, but you'd still need to `.collect()` it, because iterators are lazy, so nothing will be evaluated until you tell it to. Alternatively, you could implement it eagerly, but then you'd lose all the flexibility that comes with iterators.
Awesome article! I'm really excited about AV1. [Also, rayon is just amazing](https://github.com/xiph/rav1e/commit/156cc72edf03b5605844b4ecae84dee647fda221). Shame that I've never found an excuse to mess with it so far.
To be able to do this, it would have to figure out the `flat_map` is on `Iterator`, and that you wanted `Iterator::flat_map` instead of say, `Future::flat_map`. I know `String` doesn't implement `Future`, but from the eyes of the compiler it doesn't know that std will *never* implement `Future` for `String`. This argument applies to *any* trait that has a `flat_map` function on it, for example a hypothetical `StreamingIterator` could have a `flat_map` on it, what if `String` where to then give `str::stream_iter`, then you would have some ambiguity about which `flat_map` you wanted. tldr, it isn't possible to do so and also maintain Rust's stability guarantees.
Oh wow I love this! It would make a great poster
Well, I was thinking about the possibility that collections, instead of relying on iter() to create the iterator, the would implement "kind of" the trait "Iterator", thus it would have direct access to all the method. About collect() I was thinking about the possibility of implementing in the trait Iterator the automatic conversion to the different collections, instead of making it explicit.
The collection can't really implement `Iterator` because that would mean you could have multiple things (potentially) iterating over the collection at once, or iterator invalidation, or lots of other fun things. As for implementing `.collect()` implicitly - that's really tricky given Rust's type system, and has weird side effects, which Rust generally tries to avoid.
what's your monthly costs for your cluster and cluster size, if you don't mind me asking?
BTW there is [RFC 1977-public-private-dependencies](https://github.com/rust-lang/rfcs/blob/master/text/1977-public-private-dependencies.md) that will make Cargo better about the remaining problems when it is implemented.
This looks really helpful but I don't like the way the ampersand is rendered in the chosen font. I find it distracting instead b/c it's styled differently than the one you'd see in code.
Yes it works in the console, but if you create a script tag with `console.log($)`, it'll say it's not defined on Firefox. Chrome is a bit weirder in that if you create and add the element via the console it'll work, but if you just open a page work it, it'll be not defined just like on Firefox.
Here's what I have set up at the moment: - Swarm cluster: $30/month - 6 tiny (1GB) nodes - K8s cluster: $40/month - 3 small (2GB) nodes + 1 load balancer Load balancer just improves network traffic handling, but you could save $10/month without it. Not including block storage (which is cents on the GB).
I think there are a couple of reasons why it is so: 1. Iterators are specifically external iterators, so they need to hold iteration state in addition to referring to the collection data. 2. As the OP itself notices, there are at least two ways to iterate a collection: `.iter()` iterates by reference, while `.into_iter()` consumes the collection and iterates by value, with potentially better efficiency due to being able to move the retrieved values rather than cloning them.
I didn't mean to be particularly snarky; rather, I felt like I pointed out some flaws in Java that I felt were fair to point out as a comparison. I use Java every day at work, and I am very experienced with it. My point wasn't necessarily that Java is in a bad state, but contrasting how Java certainly offers a footgun should you choose to use it, whereas Rust avoids the situation almost entirely. That being said, we do semi-regularly run into dependency wonkiness at work. We're using Gradle. Not sure what is at fault though.
Neat! I previously printed out this SO answer specific to string/byte vecs: [https://stackoverflow.com/questions/41034635/idiomatic-transformations-for-string-str-vecu8-and-u8](https://stackoverflow.com/questions/41034635/idiomatic-transformations-for-string-str-vecu8-and-u8) but this is a good companion sheet :)
This seems like a fair question, and I'm not sure how to respond other than my initial feelings: - When I look at a long list of crate dependencies, I usually think: "Sigh, yeah I guess that dependency makes sense." When I look at a long list of NPM package dependencies, 50% seem to be useless sub-1000 line packages. To be fair, this is primarily an emotional reaction and not a logical one. - I mostly don't care how big my binary size is for a desktop or server application. I care a _ton_ how big my code is for JavaScript frontend. - In _general_, I find the average quality of a library on Cargo to be higher than the average quality of a library on NPM. Thus, I am more likely to assume a dependency is trustworthy in the former case. I think this is in part that the barrier of entry for Rust is higher.
Gotta know how to write titles to get people to click! Sad, but sadly true too.
I want that on a blanket.
On the flip side, when you have a lot of deps and those deps have a lot of deps you can't help but look at your cargo.lock file and all the duplicated libs and wonder how much unecessary junk is compiled into the exe. It would be nice to have a switch that force-tries libs to build with a specific version of a crate, e.g. if I have a dep on 0.4.20 of a crate and something else depends on 0.4.15 then try to force it to use the later one.
I don't use C++ now that I've switched roles, but the shop where I work has a large C++ code base that heavily uses TMP (not available in Rust, I know) and IRC we used variadic templates in the definition of type safe variadic functions and in deserialization, along with non type template parameters. &amp;#x200B; So, yes, they're frequently used in an existing C++ codebase in a largish company, but you make a good point that some significant use cases don't have the same impact in Rust, especially given that Rust will lean on macros where C++ and D would lean on template metaprogramming. I'll be happy when the other features come to Rust, and I'll withhold judgement on variadics.
Thank you.
I think you might be looking r/playrust
I actually really like this, and I may consider my overall design of the cube because of it. The only reason that this currently wouldn't work with my setup is because I have each piece's position represented as a vector and to perform a rotation I use matrix multiplication. So each modifier itself wouldn't necessarily make sense. I am going to continue working around and seeing what I like but if you are interested in seeing my C++ code (Which I can't say is good, but should be fairly readable) it is [here.](https://github.com/alanbaumgartner/RubiksCube) &amp;#x200B; thanks again :)
As someone just learning rust... wow. So much available.
That makes the APIs generic instead of rewriting the same stuff for each collection.
That would lead to compatibility issues down the road. I should be able to add new private fields to my struct without a major version bump. But if that potentially breaks someone's build (because they were relying on the compiler's willingness to equate "identical" types across lib versions), then I have a problem.
TBH I'm posting it everywhere so people are aware of it so more people can use it.
This is a subreddit for the Rust programming language. I've removed your post to spare you the embarrassment. Also please note that posting a topic in too many subreddits is considered spamming and may result in a site-wide ban.
Very cool! Just a small nit, `rev()` is *reversing*, not *reverting* an iterator.
&gt; I was thinking about if it would be possible and what would be the drawbacks if we don't need to write .iter() (or .into_iter()) to be able to access all the available methods on a collection. As well as if we can infer that we are assigning the return value to use .collect() I would encourage you to think about writing this idea as a "rule". For instance, in your example, the compiler would see that `words` is an `[&amp;str]`and that there are no traits in scope which `[&amp;str]` implements which provide a method named `flat_map`. How would the compiler know to call `.iter()`?
Um... Tokio gives you parallelism?
Uh...seems like you're referring to these lines: let username = stdin .next() .context(UsernameMissing)? .context(UsernameMalformed)?; let password = stdin .next() .context(PasswordMissing)? .context(PasswordMalformed)?; ...and that's not nesting, because `?` returns before `context` gets called again. Am I missing something?
I already understood what you're talking about here -- but what is the meaning of "nesting"? I think /u/WellMakeItSomehow is talking about something different -- see my reply [here](https://www.reddit.com/r/rust/comments/bgy80y/any_experiences_with_snafu/elr7ooy/). Also, protip: fenced code blocks (triple backticks) don't work for several mobile Reddit apps, and it makes yer stuff look UGLY. :) I definitely prefer them to indented code blocks, but...the indented ones work for everyone viewing your stuff on Reddit.
It does? I thought the default executor was single threaded...
My first attempt was to use musl, but I didn't know if it was possible to get mysql support to work since it's a dynamic library and I didn't know if it was possible to staticly compile that in. I'm curious though where the dep caching is happening in that dockerfile. Do you just mount an external volume, or is it something with the `COPY . .`?
`collect()` means something. It does something. It takes a lazily-evaluated potential sequence of items, and then eagerly evaluates them and stores them in memory. This is a profound change. Making it implicit or automatic would be a big mistake, and it would cause or encourage many bugs. For example, the iterator returned from `repeat()` will return an *infinite* sequence of items. Do you really want to risk accidentally collecting this?
I was so confused at that.
Indeed. And it makes sense if you think of “Rust” as a collection of tools that together make up what we refer to as “Rust v1.xx”. It just so happens that that set of tools first only consisted of Rustc and Cargo. Now it also includes Rustdoc, Rustfmt, Rustfix, RLS, and Clippy. In the future, more tools might be added to this collection. And whenever you download a specific version of “Rust” you get the versions of all those tools that have been validated to correctly work together.
 pub struct Raw(Vec&lt;String&gt;); This should be `OsString`, so you can correctly handle paths, usernames, etc.
Do you think it would be better if all the trait functions were listed along with the other functions? Because I think that would improve discovery of features
That's nice! What kind of game are you working on ? Do you also plan to add more features like collision detection, sprite sheets / atlases, Tiled file support, etc ?
To be honest, Java deserves some prejudice. I have seen version conflicts cause horrible runtime issues (once in prod). The compile time issues can get out of control too (maven dependency with dozens of transitive dependency exclusions...) The root cause is that libraries can (and often do) define mutable static class variables, and store all kinds of things (thread pools, cache, etc) in them. You don't tend to see people writing static Arc&lt;HashMap&lt;...&gt;&gt; in Rust.
Actually the causality is a bit opposite, they wanted to test new infra for point releases and they asked if anyone had things to backport, and clippy had minor, useful but inessential backports that we made.
What Cargo has done is optimize for the common case, instead of optimizing for the rare case. I have run into one dependency issue over many years of Rust development on small to medium size projects. It only takes a few heavy dependencies in Maven to run into problems. \`mvn dependency:tree\` is a shell alias for me...
But the type not being `Copy` shouldn't be a good reason not to Copy it?
Looking at the API's detailed here like this, it's easy to see how this API is a major achievement of the Rust team. It's quite impressive how beautiful it is considering the range of functionality it supports.
Ha. I'd just like to point out that it is just the ideas about testing I disagree, not criticizing your crate specifically. Your code is probably good, crate well-rounded etc. and implementing well what have been done in many existing testing frameworks. If I was a bit too harsh it is mostly because I had little time to type it in, and just wanted to voice my point.
Yeah, you'll only run into issues if you do something like this: 1) One of your dependencies returns a type of the common dependency. let myPoint = some\_util\_package::calculate\_point() 2) You try to use that value with a direct dependency of a different version. point::add(myPoint, 2.0) You can resolve this by making your point dependency range compatible with the version required by some\_util\_package.
Have you seen [Frunk's HLists](https://github.com/lloydmeta/frunk)? Those are cons-lists in the type system, which give you all the power of variadics... at the cost of long compile-times and unwieldy syntax. Just like in C++03, really.
Why are there spaces before the parentheses?
I would think that /u/flying-sheep was only talking about tuning the diagnostic message to make it less confusing; not changing the behavior...
Not only made it into "trial", but is right on the border of "adopt" too!
Rust: where releases are so painless that we're just releasing for fun :)
Until `#[no_std]` and `cfg` are fully supported by `cargo` this post is relatively evergreen. [issue if you are interested in learning more](https://github.com/rust-lang/cargo/issues/4361)
You can make any method consume `self` to enforce that property.
Supposedly, it gives you a threadpool executor by default...haven't poked into the internals much: &gt; the CurrentThread executor will block the current thread and loop through all spawned tasks, calling poll on them. ThreadPool schedules tasks across a thread pool. This is also the default executor used by the runtime. From [Runtime Model](https://tokio.rs/docs/going-deeper/runtime-model/)
Suggestion#2. Instead of using `run!()`, allow each command/entry to specify its subcommands, to enable deeper than 1 trees. Do the macros work well when subcommands are defined in their own modules?
It's already been mentioned that `.collect()` is needed because iterators are lazy and `.collect()` forces the iterator to run to completion, *but why is that?* * Infinite iterators. Sometimes it's nice to be able to write an iterator that can generate theoretically an infinitely long sequence like an `Iterator` of prime numbers. Since iterators are lazy you can write code to generate an infinite sequence, and rely on the user of the `Iterator` to only take as many elements as they want. * Iterator combinators. Being able to take two iterators and chain them (IE turn two iterators into one iterator that seamlessly iterates over all of one iterator, then switches and uses the next iterator to completion as well) can be nice. Similarly there are operations like zipping iterators so you get one value from each two different iterates at the same time. * Both of the above are how you get something like the `.enumerate()` method for iterators. * Saving memory. `.collect()` necessarily has to create a new `Vec` to hold all the elements collected, and that means using more memory and allocations. If you have an array of numbers, want to filter out only even numbers, and use `.collect()` to create a new `Vec` of numbers, then you do something with that `Vec` you've used more memory and made an allocation you could have avoid if instead you made an iterator over the array of numbers, added a `.filter()` to ignore odd numbers, then did a `for num in the_iter` you wouldn't need to make a new Vec to contain all the even numbers first.
Thanks! &gt; What kind of game are you working on ? I am not entirely sure yet, I am currently prototyping a bunch of ideas! What _I do know_ is that I want to make something with procedural generation, base building and resource management. &gt; Do you also plan to add more features like collision detection, sprite sheets / atlases, Tiled file support, etc ? First, I want to focus on stabilizing the current features (and [many basic ones that are still missing](https://github.com/hecrj/coffee/issues)). I feel that the features that you mentioned could be offered as separate crates built on top of the engine, but I am not really sure about that yet! Though, an interesting feature that I want to try to tackle is UI integration.
You shouldn't be able to call drop as a method on an object: https://doc.rust-lang.org/std/ops/trait.Drop.html#tymethod.drop &gt; This method is called implicitly when the value goes out of scope, and cannot be called explicitly (this is compiler error E0040). However, the std::mem::drop function in the prelude can be used to call the argument's Drop implementation. And std::mem::drop takes ownership of the object, so it shouldn't be possible to call drop twice on the same object.
Should `words.flat_map()` borrow `words`, borrow it mutably, or consume it? Well, `Iterator::flat_map` consumes, so if `words` were a `Vec` you wouldn't be able to do anything else with it. Okay, suppose we change the signature of `flat_map` so that it borrows. Then we have the severe problem of it simply not working. `flat_map` returns a struct of type `FlatMap` which has to implement the function `next`. But how can you implement `next` without calling the `next` method of the underlying iterator? You can't: `next` requires `&amp;mut self`. Alright, so `flat_map` will have to borrow mutably at the very least. But that doesn't make any sense for `Vec`. It would be nice to iterate using just an immutable borrow. The solution for this can of worms is to not even open it. Rust makes you write `.iter()` or `.iter_mut()` or `.drain(..)` so that it's explicit how iteration interacts with ownership.
This is really useful! I was reading through and learned about a bunch of methods that I had overlooked when skimming through docs.
Ahh yeah. Seems a tad painfull, but at least you get to accept non-utf8 filenames as well.
(Disclaimer: I've installed and used node-based programs, but never written one.) Across projects, for sure. Since dependencies are installed in the project directory, I don't see how sharing dependencies would work. Within one project, I don't know. It seems reasonable that if both A and B depend on C, you could install C in A's dependencies and then symlink B to A's copy, but I don't know if NPM does this.
I was pretty surprised to read the release notes here. Two false positives in clippy don't seem like a very good reason to put out a point release. Arguably the rustup problem seems more serious, but I am unclear on how a Rust release helps there, since I thought rustup's release cycle is separate from Rust's.
Would be useful to mention that in the release notes, since otherwise it's very unclear why these fixes would merit a point release.
&gt; Npm doesn't even try to reduce the number of different versions of a library used If A depends on C v0.4.\* and B depends on C v0.4.4, you're saying A and B will each get different versions of C? That's surprising given that the OP cites NPM as another dependency manager that uses semver ranges: &gt; Like NPM and Composer, Cargo allows you to specify a *range* of dependency versions that your project is compatible with based on the compatibility rules of [Semantic Versioning](https://semver.org/). This allows you to describe one or *more* versions that are (or might be) compatible with your code.
I quite often avoid `collect` by returning `impl Iterator&lt;Item = ...&gt;`. I'm also intrigued by an idea to have some `trait ImplicitIter` that would allow `words.map(...)`.
I think we could default to iterating by reference. Iterating by value would just require `into_iter`.
Oh you're totally right. I don't know what I thought I read.
AFAIK even if two packages depend on the exact same version of another package there will be two copies of it, at least as far as npm is concerned (bundlers and minifiers may deduplicate this later).
&gt; Is there an easy way to get my local copy of the docs `rustup doc --std`
I think there's a few main issues for me: 1. Unintuitive order for many docs. For example, on the [Iterator](https://doc.rust-lang.org/std/iter/trait.Iterator.html) docs, the first method I see is `next()` which makes sense because it's the core method for the trait. However, the next method I see is `size_hint()`, a method I have literally never called in 4 years of Rust programming. The docs for `size_hint()` fill my entire 2K screen and I have to scroll one entire screen to see the next method `count()` which I rarely use. I have to scroll ~5 screens worth to find the `map()` methods, one of my most used iterator methods. - Idea: Alphabetize the method list like it is done in the sidebar. I know I can scan the sidebar and I'm trying to learn to do that, but I don't usually think of doing that until I've already scanned a few screens worth of docs. 2. Kind of related to #1 but a different case. For types that implement a trait unconditionally (no where clauses), showing methods grouped by trait doesn't help me when I'm looking at that type. Again, I'm usually looking for something and I might have a good idea of what it's called but I don't care what trait it belongs to until I actually find it. That's the point I want to see what the trait is so I can know what I need to import. [String](https://doc.rust-lang.org/std/string/struct.String.html) is a good example of this. I'd prefer that the methods from `Deref&lt;Target=str&gt;` be interspersed with the inherent methods on `String`. - Idea: Don't group methods by trait unless the trait is conditionally implemented. Show them in alphabetical order instead. 3. I often know the shape of what I'm looking for and I just need to find the right method regardless of what it's called. At this point, I'd like to be able to quickly scan the names &amp; types of methods in a type without getting bogged down by a bunch of prose. - Idea: Collapse examples so they aren't shown unless the user clicks to view the example. Hide all other prose for each method until the method is expanded. Just give me the type signature and a one-line summary of the method by default.
The default executor is a multi-threaded work-stealing scheduler.
Thanks. Hmm...it still takes a while to load but not as bad as the web version.
I also came across this container cheat sheet a while back: [open in Google Drive](https://docs.google.com/presentation/d/1q-c7UAyrUlM-eZyTo1pd8SZ0qwA_wYxmPZVOQkoDmH4/edit#slide=id.p)
&gt; What kind of attitude is that? In this case, an evidence-based one. In general: people from all over the world have learned to program in FORTRAN, JavaScript, BASIC, COBOL, and what-have-you. A programming language's keywords mean almost nothing: a programming language is not a natural language. Point in case: in rust, functions are labeled `fn`; ref, mut, impl, etc. are all simple syntactic markers, only slightly more pronouncable than &amp;, * and :. Even `for` doesn't really mean "for". If you ever want to read code written by another, it's a lot easier if you can make out the basic structure. Can you easily make out what this says? pr lees_inhoud(pad: ∇rks) :: Uitkomst[Reeks] { laat wzg uitkomst ∞ Reeks//maak(). laat wzg bestand ∞ Bestand//open(pad)✓. bestand.lees_tot_einde(∇wzg uitkomst)✓. Goed(uitkomst) }
Calling drop implicitly is functionally the same as calling drop explicitly. I'm asking if it's a good idea to hijack Drop for purposes beyond its its intended use?
Yeah the only way to deal with the problem in my example, as far as I know, is to pester the library developer and ask them to please follow semantic versioning or some other *social protocol*.
Maybe see this comment in the pre-release thread: [https://www.reddit.com/r/rust/comments/bh6v6c/rust\_1341\_prerelease\_testing/elraesu?utm\_source=share&amp;utm\_medium=web2x](https://www.reddit.com/r/rust/comments/bh6v6c/rust_1341_prerelease_testing/elraesu?utm_source=share&amp;utm_medium=web2x)
Agreed! Rustdoc is incomprehensible. I usually end up using the search to find stuff (which does work quite well thankfully) because finding modules, structs and traits by browsing is just impossible.
Another reason why `Map` is better a separate type is that you can conditionally make it `Copy`/`Clone` :) Having copyable parsers is such an ergonomic win.
Very interesting and well written! Regarding the low 8-tile speedup: are these really 8 cores or are these 4 full cores with hyperthreading?
Sure this is useful but remember that the standard docs are awesome aswell and has more detail.
This is generally not what you want, because `drop` is not guaranteed to be called; we have `mem::forget` to avoid calling `drop` on something. Any trickery based around `drop` can be defeated by `forget`. So the question is whether you want something to be used *exactly* once, or *at least* once.
As Manish said in the pre-release thread we had to try a few new things infra-wise in the release process, and we didn't want to do that in a full release in case things broke. The backports we included were not that critical, but they're still useful (I ran into the closures lint a lot myself) and we needed some of them to actually test the release infra.
Yeah I definitely see this too. The page for iterator takes like 30 seconds.
In this case I am looking for exactly once and ASAP. It's for releasing a wayland WlBuffer resource to a waiting client.
Not easily, no. So what is really important is whether or not the docs are multilingual?
Wow just great job!
Looks like a known issue: https://github.com/rust-lang/rust/issues/55900
We had the const fn lint on deny until it spat out hundreds of suggestions for our trait implementations that simply return a constant value. Finally we can use it again project wide. Woop! 🎉 I wonder how such an obvious error made it into the previous release...
Sometimes, you have a CPU-intensive problem benefits a lot from a divide and conquer approach. Finding a needle in a very large haystack is one such problem: if you break the haystack up into N parts and have each of N CPU cores work on a part, you'll maximize resource utilization, and find your needle faster. Other times, dividing the work, or re-combining the separate results from the workers, actually costs so much that it's better to just do it all in one thread. For a sufficiently small haystack, coordination will take longer than just searching the whole thing. And for some problems, it's not obvious how to divide the work, so you end up with a lot of locking or message overhead to implement coordination, and single-threaded turns out to be better. Often though, if you think about it hard enough, you can find a way to re-structure the work to fit into the first category. This usually requires genuine cleverness. For the first class, Rayon and similar parallel computing frameworks will help with dividing, coordinating, and re-combining, leaving you to just do the conquering.
Rust certainly didn't "solve" dependency hell. But npm and https://www.npmjs.com are two sides of the same coin and a good number of npm's historical failings are really in the latter. crates.io avoided some of https://www.npmjs.com's grievous mistakes.
I’m on mobile.
My favorite java feature is that if two classes have the same name and package, it just picks one implementation apparently at random at class load time.
Do you mean when clicking on identifiers on this cheat sheet?
\&gt; and a new version of rustup, 1.18.1. How do one updates rustup?
Oh, I see what you mean. Yeah, unifying versions doesn't help much if it still installs the same version twice. Thanks for the clarification!
I think you're right. So... what does it do, then?
It's automatically updated when you update anything with it. If you want the explicit command (even though it shouldn't be needed): ``` $ rustup self update ```
Certainly for learning, documents must be accessible. For many people, English isn't easy to read, so if you want wide acceptance for a complex product (and rust certainly is complex), you'll need to translate, perhaps even using less "literate" ways to communicate (drawings or animations). Having documents with different levels of abstraction might also help overcome a language barrier. There's of course a commercial opportunity here, but only for larger languages; smaller languages will need financial support. Building a global community is more difficult: that would almost certainly be limited to a common language.
Never open that page on your phone or a weak system. That page comes close to being a stress test. So maybe it was not loading for a long time in terms of network but CPU activity?
Huh. I thought it was just an event loop.
This is gorgeous, I love the presentation! I'm sure I'll refer to it constantly.
As mentioned by TimNN, this is a known issue: the page is just **massive**. This is essentially a combination of: - Traits documentation pages mentioning all known implementations. - Known implementation of a trait mentioning all trait methods. And the fact that there are many implementations of `Iterator` and `Iterator` has many methods, giving us a beautiful MxN complexity, resulting as @samcday [mentioned](https://github.com/rust-lang/rust/issues/55900#issuecomment-480574471) in: &gt; 172k elements! That's one big blob of HTML...
The only npmjs.com issue I'm aware of is the left-pad incident, where an author removed all of their projects from the registry and caused new builds to break. I'm not sure if crates.io solves this; yanking a version won't break anything, but what about an entire crates? Would you mind elaborating on what other issues npmjs.com has had?
Hi. Coming from a Python and Java background I'm trying to break out of my very OOP mindset, but I'm struggling a bit with what would be the 'procedural' way of avoiding repetition with structures that have lots of shared properties. For example in a game, you might have lots of structures with attributes of 'health' and 'movement\_speed'. To me these would obviously be part of some superclass, but in Rust, I always end up with something like: struct Player { health: u32, movement_speed: u32, level: u32, } struct Enemy { health: u32, movement_speed: u32, gold_dropped: u32, } What would the Rust way of avoiding this kind of repetition be? My first thought is some kind of 'stats' struct which each actor would have as one of its fields. By the way, I know about ECS, but that seems more specific to games rather than non-object-oriented programming in general.
Depends on what purposes you're talking about. `Drop` isn't just about releasing memory, but rather any kind of resource. `File`s and `TcpStream`s and graphics windows and whatever else can be managed by `Drop` impls.
See here is the thing... I don't really mind. I love the single-page style of this. If I'm on the iterator page and I'm looking for something I simply CTRL+F it and boom. Also I think it's very funny. That's the page I show to new people to see their reactions. ¯\\\_(ツ)\_/¯ Though I sometimes wish there was a man-page alternative to the web-based docs for super fast lookup. I'm pretty reliant on CLion helping me out most of the time. Imagine calling \`cargo doc Vec::new()\` or some other keyword and getting a beautiful coloured (maybe even with the same colours as the dark theme webpage) manual page with the same examples and everything.
I think this would be a legitimately awesome addition to rustdoc. I would love a cheat sheet for every crate!
Npm does some deduping. As I understand it, it can hoist one version of each dependency to the top of node_modules and refer other dependencies to use that top level instead of duplicating it. (I'm not sure if this is top level only, or happens some deeper in the file tree as well.) Other versions of that dependency end up getting duped. E.g. maybe you dedup the four inclusions of `foo 1.0` but duplicate `foo 2.0` three times.
If I understand you correctly, that's what cargo does - log 0.4.0 and log 0.5.0 are considered different crates and will both be included in the final binary if they're both depended upon. That breaks down when dependency A produces a type from log-0.4.0 and B consumes a type from log-0.5.0; because they are considered different crates, the types are not compatible. For example, consider: // `common` - common dependency pub trait Foo { // ... } // `crate_a` - depends on `common` 0.1 pub struct MyFoo { // ... } impl common::Foo for MyFoo { // ... } // `crate_b` - depends on `common` 0.2 pub fn use_foo&lt;F: common::Foo&gt;(foo: F) { // ... } Because `crate_a::MyFoo` implements common 0.1::Foo, not common 0.2::Foo, it is a compile error to pass a `crate_a::MyFoo` to `crate_b::use_foo`.
Is there a crate that has a function that works like .NET's String.CompareTo, ie localized string compare?
Why can't clippy simply suggest use of things like `.as_ref()` when needed to make a point-free style conversion correct?
Hi, hopefully, this is a simple question! I'm using StructOpt to parse command-line arguments and have been using `required_unless` to allow for things like only passing `--version` to the application. #[structopt(short = "s", required_unless = "version")] status: bool &amp;nbsp; I've since added a second option like version and wanted to use Clap's `requied_unless_one` modifier but I can't for the life of me figure out what the correct syntax is to do this in the derive line: // This doesn't compile, but I can't figure out the syntax to // pass a slice in structopt derive line. #[structopt(short = "b", required_unless_one = ["foo", "bar"])] bar: bool If I only pass a string to `required_unless_one` the error would seem to indicate it does expect a slice there: note: expected type `&amp;[&amp;str]` found type `&amp;'static str` Hopefully, that makes sense. Thanks!
It makes sense, it would just have been nice to mention this in the blog post, since it now it kinda seems out of left field (or potentially even of the undisclosed vulnerability fix type).
Nice article. There's another piece of the puzzle worth mentioning, that contributes to solve the dependency hell problem. It's the orphan rule. It guarantees that two libraries cannot be incompatible one another. It means that adding a dependency will never break your project.
&gt; what about an entire crates? I don't know if you can remove entire crates. If you can, yanking seems less useful. Ownership can be transferred, though, and that has potential to be worse. &gt; Would you mind elaborating on what other issues npmjs.com has had? Quickly off the top of my head: - Left-pad. - "Left-pad" again just a few months after left-pad. - [Teapots](https://github.com/npm/npm/issues/20791) - Can't sign packages. - Model encourages the JS micro-package distribution, irrespective of what anyone feels about many dependencies in general. - Name squatting (Rust got that one wrong, too), although npm finally added support for namespaces about 4 years ago.
`stdin` in this context is going to be `Fuse&lt;Lines&lt;StdinLock&gt;&gt;`, which implements `Iterator&lt;Item=Result&lt;String, std::io::Error&gt;&gt;`. So, the expression `stdin.next()` is of the type `Option&lt;Result&lt;String, std::io::Error&gt;&gt;&gt;`. Let's go through this step-by-step: let username = stdin .next() // expression of type `Option&lt;Result&lt;String, std::io::Error&gt;&gt;` .context(UsernameMissing)? // -&gt; returns `Err(UsernameMissing { ... })` if `None`, otherwise yields `Result&lt;String, std::io::Error&gt;` .context(UsernameMalformed)?; // -&gt; returns `Err(UsernameMissing { ... })` if `Err(...)`, otherwise yields `String`
you can do this yourself by adding some custom css to what's already there: li::before { content: "=&gt; "; // change this to fn if you want. margin-left: -2em; }
Thanks
Thanks
It could, but not if the versions are compatible (usually). You can type "npm list" and it will show you a tree of dependencies. It's common to see lots of "(deduped)" in there.
Even in OOP circles, people often advise you to "prefer composition to inheritance". I would do exactly what you say: break out a struct that contains both \`health\` and \`movement\_speed\`. If you need to later treat "things with stats" generically, you can create a trait that gets the stats out of a thing, and implement it on both \`Player\` and \`Enemy\`.
I had the same question. Also if it really has 8 cores, it probably has a higher boost clock with 1-4 cores then with 8 cores. For that particular Laptop it doesn't matter, 3.6x is the speedup he can get. But if you want to know how well your encoder *can* scale with threads, disable Boost and state the actual CPU Model.
Sounds like the perfect use case to me!
Sorry I didn't describe the use case I was looking at. It's about signaling across IPC that this program(a wayland server/ competitor) is done with a shared memory buffer. The client would be patiently waiting for this message.
This is a really interesting representation, I'd also like to port this into Rust as a learning experience.
Damn. If only unicode supported keywords, that would *basically* make a difference.
My laptop have a `Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz`. ``` $ lscpu ... CPU(s): 8 On-line CPU(s) list: 0-7 Thread(s) per core: 2 Core(s) per socket: 4 ... Model name: Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz ``` and `cat /proc/cpuinfo` shows 8 processors.
Ah, that's my miss. Fixed, thanks!
The "threads per core: 2" is the key: your cpu has hyperthreading, and the performance boost by hyperthreaded "CPUs" is not the same as for a full core. I'm on the phone now, so can't provide helpful links, but I suggest you read up on hyperthreading - i suspect it is the reason for the "missing" performance for your 8 threads case.
Sounds like a good use case for Drop to me. In fact I'd even say that *is* the intended purpose of the Drop trait. As long as the object being dropped is what provides an API into the shared memory buffer (as in, once that object is dropped, you have no way to get to the buffer beyond creating a new instance of the object), then this is perfectly reasonable. To give an example from the standard library, when you lock a `Mutex&lt;T&gt;`, you get a `MutexGuard&lt;T&gt;` which acts nearly identical to a `&amp;mut T`, except that it implements drop so that when `MutexGuard&lt;T&gt;` is dropped, the original `Mutex&lt;T&gt;` becomes unlocked automatically.
It's just a massive amount of html being sent to the browser. I trimmed down the weight of the html by a good chunk [a few months ago](https://github.com/rust-lang/rust/pull/56874), but that still left a gigantic page. I don't think there's a ton of room left to optimize with tactical changes. The page either needs to lose features, or rustdoc needs to lean in to js more for progressive enhancement (e.g. fetching trait data when the user scrolls, or maybe switching from plain text to markup for traits based on visibility). &amp;#x200B; The JS route is rough because noscript users are important to the project, and because there's basically no js build tooling available. You'd need to write JS without tool support that ran on very tough to target platforms like UC Browser for Android. It's not impossible, just tricky and painful to get right, and very hard to keep bug free during maintenance. I think that means dropping or taming the "all trait info on a single page" feature is the most promising route. &amp;#x200B; What would be super interesting is reviving the json output of rustdoc. Then you could have external doc viewers that could make different tradeoffs than rustdoc in core.
Great article! About the columns function: there is no unsafe block in there. Does that mean one could write an (intentionally) unsound version of that produces an iterator that includes some iterator more than once without the use of unsafe? If so, then something is not right.
Thank you for your post!
Thank you, I [updated](https://blog.rom1v.com/2019/04/implementing-tile-encoding-in-rav1e/#limits) the article :)
My code was very short, it was fn main() { println!(“Hello Image_test”); extern crate image; use image::{GenericImage, ImageBuffer}; } The command I used was rustc Image_test.rs
In the '80s, an IDE was a single application containing an editor, a compiler or interpreter, and a debugger, as features of that application, not distinct programs. In the '90s, modular IDEs appeared, consisting in one application invoking an external compiler or an external debugger, and communicating with them. In any case, the essential features of any IDE remain: a text editor, a compiler or interpreter, and a debugger. Any other features, like interactive code analyzers, and refactoring commands, are optional; the more powerful IDEs have them, the simpler IDEs do not.
I haven't been able to search things in the docs anymore. It never loads. It sucks, but I've only been hitting this fairly recently (with the same pages).
Re 3, does toggling the +- button right at the top do what you want?
Hi, I'm getting a trait bound error that I don't understand. I have this function, which is a copy from the Rust Cookbook fn sha256_digest(path: &amp;str) -&gt; Result&lt;String, Error&gt; { let input = match File::open(path) { Ok(file) =&gt; file, Err(error) =&gt; return Err(error) }; let mut reader = BufReader::new(input); let mut context = Context::new(&amp;SHA256); let mut buffer = [0; 1024]; loop { let count = reader.read(&amp;mut buffer)?; if count == 0 { break; } context.update(&amp;buffer[..count]); } let digest = context.finish(); let digest = HEXUPPER.encode(digest.as_ref()); Ok(digest) } The calling function converts the `Result` immediately into an `Option` so I thought why not just return an `Option` instead. fn sha256_digest(path: &amp;str) -&gt; Option&lt;String&gt; { let input = match File::open(path) { Ok(file) =&gt; file, Err(error) =&gt; return None }; let mut reader = BufReader::new(input); let mut context = Context::new(&amp;SHA256); let mut buffer = [0; 1024]; loop { let count = reader.read(&amp;mut buffer)?; if count == 0 { break; } context.update(&amp;buffer[..count]); } let digest = context.finish(); let digest = HEXUPPER.encode(digest.as_ref()); Some(digest) } This doesn’t compile error[E0277]: the trait bound `std::option::NoneError: std::convert::From&lt;std::io::Error&gt;` is not satisfied —&gt; src/image_file.rs:61:25 | 61 | let count = reader.read(&amp;mut buffer)?; | ^^^^^^^^^^^^^^^^^^^^^^^^^ the trait `std::convert::From&lt;std::io::Error&gt;` is not implemented for `std::option::NoneError` | = note: required by `std::convert::From::from` I don’t understand why not. The `input` type hasn’t changed and if `File.open` errors we never get to line 61, so I don’t get what the problem is.
It's Firs Code, which is very common.
I've been working on an [Othello minimax solver w/o alpha-beta pruning](https://github.com/loganintech/miniothello) and [a tool called gitpub](https://github.com/loganintech/gitpub) that lets you create remote git repos for a few services. Happy to talk about either project :)
&gt; Does that mean one could write an (intentionally) unsound version of that produces an iterator that includes some iterator more than once without the use of unsafe? The `columns()` function is part of the "trusted" implementation (it can access private members of `ColumnsMut`). From another file, you could not instantiate `ColumnsMut` that way. In rav1e, a public function [`new()`](https://github.com/xiph/rav1e/blob/c83f7cc7a122a3ff601c1945f632d946b1854b9d/src/tiling/plane_region.rs#L138) is provided to guarantee soundness. See [The scope of unsafe](https://www.ralfj.de/blog/2016/01/09/the-scope-of-unsafe.html).
&gt; I wonder how such an obvious error made it into the previous release... This is very easy to miss during review, and AFAIK Clippy isn't regularly tested against the entirety of crates.io to look for suspicious amounts of warnings. We have that infrastructure for rustc, but at least currently it isn't used for Clippy. Maybe that'll change in the future though, now that Clippy is a pretty important part of the official toolchain.
I just keep internally laughing because it reminds me of the Arrested Development part where they celebrate being classified as Don’t Buy” by Cramer[0] [0]: https://youtu.be/7pbRYljkDFg
What if it wasn't my intention? Let me explain: let words = ["alpha", "beta", "gama"] let mapped_words: Vec&lt;_&gt; = words.flat_map(|s| s.chars()); let merged: String = mapped_words; Lets go into the list of issues that are not obvious: * Calculating `mapped_words` both creates an `Iterator`, and then merges it. The thing is that it might have been more efficient to join both lines, but to a inexperienced programmer this wasn't obvious. Why didn't they just make the type `Iterator&lt;_&gt;`? Well because they didn't realize this and the compiler struggled to guess their intent when it gave a well meant advice. If it always recommended Iterator I could give you an example of when it would have been the wrong thing. * So the compiler here fails because it's not always easy to guess *intent.* * Do you see how a `String` took a `Vec&lt;char&gt;`. This seems very magical for starters. Second of all it might be a mistake. Third of all there may be better ways of doing that conversion than `vec.to_iter.collect()` but sometimes that's exactly what we want, it's hard to know which one applies. * Again intent here is the big issue. Lets add methods here that show the intent. let words = ["alpha", "beta", "gama"] let mapped_words: Iter&lt;_&gt; = words.flat_map(|s| s.chars()); // Obviously this is half-step. let merged: String = mapped_words.collect(); // And this is when I'm done or maybe let words = ["alpha", "beta", "gama"] let mapped_words: Vec&lt;_&gt; = words.flat_map(|s| s.chars()).collect(); // I do want to collect values here. let merged: String = mapped_words.iter().collect(); // I do want to be that inefficient And that's the reasons for the methods. Less magic means that: * It's more intuitive for early devs to know what they are doing. * The compiler understands what you are trying to do because you told it. * No unexpected operations exist. If I add a `FromIterator` I don't expect it to add an implicit cast without at least implementing a from `From&lt;Type&gt;` as well.
"Come program rust, we have cheat sheets."
E.g. I'm very happy using Vim for Rust. But I miss very much a good &amp; rusty GUI lib though.
Good to know (I know very little about CSS). I think this will probably replace the rust stdlib docs for me in most cases. Pretty neat.
couldn't you just do an 'Into' trait implementation?
I love this. I'd print an A4/letter formatted version. Also I'm thinking it may be possible to generate this automatically for any crate by extending rust-doc...
Err, this is the kind of user error (sabotage) that it doesn't make much sense to guard against. Splitting hairs about 'your users could be &lt;some insult&gt; and call forget and then where is your drop implementation naive person' is not a real argument to not use drop for it's intended purpose imo.
I'm struggling with an `mpsc` channel because I have a single producer and multiple receivers (not mpsc). Since `Receiver` is not `Clone`, I tried adding a `&amp;'static Receiver` to some of my structs, but have been battling lifetime issues since. The current issue I'm getting is it can't infer the lifetime of it and it may not live long enough. Am I doing something wrong? I rewrote my code a bunch of times to try to deal with this and can't figure out the best solution.
You're on the wrong sub, go to /r/playrust
Thanks! I'm sorry just a little frantic!
You're really good at explaining this. Thanks!
I would just test on linux, and use a windows VM or CI platform for making a final release. That's what I did, worked pretty well.
Billy maintains `billy_goat_selector`, a crate which allows the user to select a random billy goat from his collection. I'm bad at coming up with examples, sorry... He uses `rand` v0.1 as a dependency. This is pretty outdated, but Billy's a pretty outdated kinda guy. He also recognises that some people want to select billy goats using an arbitrary random distribution, so he exports the function `billy_goat_with_distribution&lt;D: rand::Distribution&gt;(dist: D) -&gt; BillyGoat`. Now the user can also depend on rand, choose a random distribution of their choice, and pass it into Billy's library. Therefore, `rand` v0.1 is part of the public API of Billy's library. Johnny also maintains his own library, `johnny_depp_rs`, which allows the user to select a random image of Johnny Depp. Johnny also uses `rand`, but he's much more up-to-date, so he uses `rand` v3.6. He creates the function `depp_with_distribution&lt;D: rand::Distribution&gt;(dist: D) -&gt; JohnnyDeppImage`. In this case, `rand` v3.6 is part of his public API. Now I come along, and want to create pictures of Johnny Depp sitting on billy goats. I can depend on both `billy_goat_selector` and `johnny_depp_rs`, and each will draw in their own dependency on rand. However, if I want to use the `*_with_distribution` functions that both provide, I'll run into a problem: which version of `rand` should I depend on, to create the `rand::Distribution` instance that I need, to pass to each of the functions? To make things worse, I find out that the `Distribution` trait has had some backwards incompatible changes made to it between v0.1 and v3.6 - this means that I can't just force Cargo to inject a different version of `rand` into one of the crates, because then that crate will start calling methods that don't exist any more. What I believe happens (I haven't checked this in Rust, I've just experienced similar issues in other languages) is that I can install a version of `rand` that is compatible with one of these two crates, but the compiler will prevent me from using it with the other crate. This means that I have no real way of calling the other `*_with_distribution` function. I hope this makes some sense. This is not a solved problem at all, and it crops up in other languages that do similar things that Rust does. JavaScript introduced the concept of "peer dependencies", which are essentially dependencies where the external library declares that they need access to a dependency, but they want the version of that dependency that the rest of the application is using, not their own unique one. This mostly works, but it causes other problems, and dependencies can still become mismatched, meaning that there's just no coherent way to use two incompatible libraries, despite them both theoretically depending on the same code. Also, `rand` here was used as an arbitrary example because I'd seen someone else mention it and so it was on my mind. I vaguely remember that it does have traits for distributions, but I don't know how useful they'd be in the context of selecting a random element from what is presumably a list. Or why anyone would create any of the other projects described in this comment...
How does this compare against [Interledger](https://interledger.org)? Interledger uses [Hashed-Timelock-Agreements](https://interledger.org/rfcs/0022-hashed-timelock-agreements/) for atomic transfers. HTLA requires only a hash commitment and bidirectional channels between each pair of directly connected nodes. No asymmetric cryptography like signatures are necessary, unlike Offst. The failure case of HTLA is that both the sender and receiver get the funds, but the connectors (relays) in the middle are responsible for mediating their losses and mitigating risk through managing whom they trust directly. The ILP (Interledger Protocol) includes automating the routing to reduce fees for the end users and the user may further constrain these paths to, for instance, never leave their country, for regulatory and similar reasons. Better, the routing may transparently traverse markets and exchanges to enable users to make their purchases using any currency supported by the network. Interledger is backed by the W3C and has been formally proven. \--- I haven't seen any mention of "mutual credit" within Interledger's documentation. It is a payment system; not a currency. It is the responsibility of the connectors to manage their own dept and set reasonable bounds on losses they're willing to risk. AFAICT this matches the definition of "mutual credit".
Trying to use an `mpsc` queue as if it were `spmc` sounds like jamming a square peg down a round hole. I'll bet https://crates.io/crates/crossbeam-channel will suit your purposes better than the standard library channels
page it?
I think a lot of JS apps have much larger development dependency installs than they do production dependency installs. Webpack and similar bundling and building tools are much more likely to pull in only partially-necessary dependencies because (a) they do a very complicated job (Webpack is essentially a small, single-purpose JS compiler, plus TS/Babel, plus minification tools, etc), and (b) they will only be run on developer machines, so their size is not a huge problem. On the other hand, most big frameworks, and most utilities that I've seen written aimed predominantly at solving frontend problems, will be significantly more concerned with bundle size, and will generally not pull in further dependencies. The Rust ecosystem generally doesn't have this problem, because the Rust compiler covers most of the work done by webpack/parcel/babel/etc, and is therefore a required tool. From a JS perspective, it would be as if Node came with a bundler built into it.
The problem is that `reader.read(&amp;mut buffer)?` returns `io::Error` if the function fails, but you've changed the function's return type to `Option` instead.
The node dependency logic tends to be a bit convoluted, but generally it "flattens" modules, so that if A and B depend on C, C will get hoisted such that A and B can both depend on the same C, assuming that both A and B have set compatible version ranges when declaring their dependency on C.
Great write up! I really enjoy reading about the development of `rav1e`, especially articles like this one which don't assume deep video coding knowledge.
I've been using them for async stuff since it's in the `futures` crate (`futures::sync::mpsc`). Does `crossbeam` work with Tokio?
&gt; The page either needs to lose features, I wouldnt say repeating method signatures for every trait implementation is a "feature", at least a useful one. They're all the same, we know what trait documentation we're reading? Seems like remove that, page size goes down quite a bit.
&gt; Model encourages the JS micro-package distribution, irrespective of what anyone feels about many dependencies in general. &gt; The same can be said about the crates.io model - anyone can host packages, and people are somewhat encouraged to create smaller packages as this tends to make compilation faster (iirc). The big differences, I think, are that JS has a much lower barrier to entry, and that Rust has a much bigger and more powerful stdlib, which means that there's much less call for most micro-packages. IIRC, the NPM registry itself signs packages, and they're planning on allowing self-signing in the future. I don't believe Cargo does any signing of packages at all, although I could probably be corrected on that one.
That's a good suggestion, we'll keep that in mind for future releases if the need of a "technical" point release arises again!
I don't believe so, but maybe this will? https://crates.io/crates/futures-mpmc
I think if it's possible to write unsound code using this method, then it should be marked `unsafe`, even if it's not a part of public API and guarantees are enforced by constructor.
I just added a simple "single" mode which you can enable via adding "?single" to the url like: https://upsuper.github.io/rust-cheatsheet/?single I may add a link for that in the footer later.
Now that we have dependency renaming, is it possible to install two versions of a dependency under different names? Seems like that could nicely solve the issue of using separate versions.
I'll take a look at the code. Kind of hesitant to use it looking at the download activity.
Yeah, it looks like it got one commit ever and then the author hasn't done anything with it since. It might be worth digging around crates.io to see if there's anything better.
My code was very short, it was fn main() { println!(“Hello Image_test”); extern crate image; use image::{GenericImage, ImageBuffer}; } The command I used was rustc Image_test.rs The exact error says couldn’t read “filename”: the system cannot find the file specified (os error 2)
okay thanks. do you think anyone here would be open to doing a code review of my project? it might be a simple issue i'm not aware of, plus it's an interesting project that i would love feedback on from a architecture / design point of view.
Paging breaks control-f, so that'd be hurting some users. I'm not arguing against it, just pointing out it's not a clear win.
yeah i was thing paging with an option to show all
Maybe possible with some extra annotations (or AI maybe?). There are some information which may not be easily derivable from the existing documentation, e.g. grouping functions with similiar / related functionality together, replacing certain types with trait they implement.
Keep those RPDNs coming. :-) Very interesting and appreciated. Especially the explanation and context/links!
Of course. It’s really obvious when you pointed it out. Thanks.
Thanks for posting! I have tried Amethyst and ggez before, looking forward to trying out Coffee. One question right off the bat: Why `gfx` and `wgpu`? I thought that the benefit of gfx was that you got support for all of the backends for free? I'm really terrible with graphics though, and don't follow gfx closely, so I'm likely missing something. Actually, a second related question: Why control the graphics backend by cargo features? I can see why you wouldn't want to bring in webgpu if you only need opengl (and vice-versa for gfx), but likely in a production build I would want to define a preferred order of backends I support and let the user override it. Is that possible now? Again, thanks for posting. Busy finishing up my degree right now but I hope to dive into this in a couple of weeks :)
You can load the docs locally.
&gt; most people don't really care about supporting these old versions, and it's sometimes detrimental. Well, supporting old versions by itself isn't detrimental. When supporting old versions prevents also supporting new versions, then I'll have to revisit. We do [test in CI] on Rust 1.18, 1.30, 1.34 (each of which enables new features or syntax), stable, and nightly, and we also make use of `-Zminimal-versions` to be widely compatible. I want the *technical* reasons to not use SNAFU to be as close to zero as possible. &gt; I feel like the inner error should be optional. The inner error *is* optional: use snafu::{ensure, ResultExt, Snafu}; use std::{fs, io, path::PathBuf}; #[derive(Debug, Snafu)] enum Error { #[snafu(display("Could not read the file {}: {}", file.display(), source))] HasAnUnderlyingCause { source: io::Error, file: PathBuf }, #[snafu(display("The file {} contained a bad word", file.display()))] IsALeafError { file: PathBuf }, } fn main() -&gt; Result&lt;(), Error&gt; { let file = "/etc/passwd"; let data = fs::read_to_string(file).context(HasAnUnderlyingCause { file })?; ensure!(!data.contains("password"), IsALeafError { file }); Ok(()) } &gt; You should mention that they are not supported. Absolutely, great point! I hope to add a "migrating from other library X" section, and this would fit right in. I've also [opened an issue](https://github.com/shepmaster/snafu/issues/61) to discuss if tuple variants are worth the cost, if you are interested.
[Huh, good point](https://github.com/MrJohz/test-rust-versioning/tree/master). That's actually a pretty powerful tool. It doesn't entirely solve all cases - for example, if for some reason the same instance of an object needs to be passed to both APIs, you'd need to be able to convert from one version to another (I believe `rand` actually includes a certain amount of ability to do this between different versions). But in general, that's a nice feature - one I didn't expect to work, but one that was extremely obvious to use .
Minor nits: 1. `.context(UsernameMissing)` returns `Err(Error::UsernameMissing { ... })` (the error's variant, not the context selector) 1. You have a copy-paste-typo on the second `context` example.
I think some people *would* call `.context(PasswordMalformed)` as "nesting" because the underlying `io::Error` is bundled along with the newly-created `Error` enum, but I agree that the two back-to-back calls to `context` with `?` between are not themselves causing any nesting.
The K type here is the key of the hashmap itself, it’s not used in the type signature of the function but it is user in the type signature of the HashMap&lt;K, V&gt;. The reason they introduce a separate key type is so that you can use &amp;str in your lookups for a HashMap&lt;String, V&gt; (or other similarly related types) The reason they return &amp;V isn’t because cloning would be too expensive, it’s because not every type has a clone method, and they don’t want to force any extra requirements on the values of your HashMap that aren’t necessary. Option&lt;&amp;T&gt; has a .cloned() method that returns Option&lt;T&gt; anyway.
If you can create a small example of what you mean, I'd be glad to help make sure it works. I don't see any reason it shouldn't, but I'm also not sure why a failure error type would cause any problems, either...
The docs you send was about HashMap collection, not about Vec. I personally don't remember a get method for Vec
IMO, the problem here is the page is just full of junk at the macro level. No amount of DOM improvements will fix the fact that there's just too much on the page. Once a trait reaches a certain number of implementers, it should really just be a list of said implementors, i.e. "Implemented by: FooIter, IterThing, BestIterator, ..." with links to those structs/traits. As is, it's just absolutely overwhelming to the point of being more noise than signal. I haven't contributed to Rust before, but I'm happy to start here if it'd be helpful. I love Rust, but I find the docs to be really inscrutable for popular types for this reason.
1. Gah, they're overloaded. That's a good distinction. 2. Derp, thanks!
Could the combinations be generated clientside? It seems like if the server sends a list of all known implementations and a list of known methods, the client could generate the product of those lists automatically. This would result in much less bandwidth but the same total number of generated elements. On the other hand, maybe it's not worth the complexity if most pages are small enough. (I'm assuming it's considered beneficial to actually include so much repetition in the docs.)
It is large but the html file could be gzipped which would help a lot.
Well, alright then: https://vigorous-lichterman-548fbb.netlify.com/
So I wasn't able to cd to &gt;"Rust Programming", but was able to cd to &gt;rust_program then &gt;src but wasn't able to cd to &gt;main.rs Is there something going on here?
You can’t cd to a file, it has to be a directory. Also, back out of the src directory by using cd ..\ then try build and run
Thanks for suggesting Rustlings. I've heard of it but don't know how to get started. Also I have pretty much never used github for anything so yeah that might be a hurdle for me. I tried to do the first step for Windows in VS Code terminal, which is the git clone https://github.com/rust-lang/rustlings but it gives an error. DO I have to do the steps on the command line? How could I use RUstlings with VS Code?
Think of GFX as the low end API, and wgpu as the higher end API based on GFX under the hood. You also want to control the graphics backend by feature flags because not all graphics backends run on every OS, and a given OS may be able to run multiple backends.
This is incorrect. npm does deduping.
That would improve page download time, but the browser would still need to parse, apply styles and layout all 12 megs of HTML. That will always be slow. The page just needs to have less content.
Spontaneous idea: it would be cool for local docs (generated using cargo doc) to have a feature to search for concrete generics, e.g. Vec&lt;u8&gt; and get a list of all methods, that are available on this type after resolving all traits.
Mm, okay. Thanks semantic sense to me. I don't think that's what the original nesting comment was about, but I definitely never would have thought of that.
Hold up, I messed up with the "Rust Programming" directory, it should've had a underscore instead of a space in between. Is there any way to change the name of a directory? By the way I was able to solve the issue of the program not updating by right clicking on the file and pressing Open in Terminal.
Rename the folder in file explorer does the same thing
Winit only provides the window. Glutin provides the gl context on that window. Other crates provide vulkan contexts on the window, ect, ect. There is currently no plan to merge the two crates, nor any ETA for 1.0.
&gt; Why gfx and wgpu? I thought that the benefit of gfx was that you got support for all of the backends for free? I'm really terrible with graphics though, and don't follow gfx closely, so I'm likely missing something. For OpenGL, I am using `gfx` pre-ll (pre low-level), not to be confused with the new `gfx-hal` implementation. `wgpu` builds upon `gfx-hal` to provide support for Vulkan, Metal, D3D11, and D3D12. As far as I know, [the OpenGL backend for `gfx-hal` is still rough on the edges](https://github.com/gfx-rs/wgpu/issues/115). Thus, for OpenGL support I use the now deprecated `gfx` pre-ll crate. I went with this approach instead of using something like `glium` because I was already familiar with `ggez`, which also uses `gfx` pre-ll. Fortunately, the API between graphics backend (`gfx` or `wgpu`) and the `graphics` module in Coffee is quite clear. This should allow us to try other approaches (like `glium` or `rendy`) easily. &gt; Actually, a second related question: Why control the graphics backend by cargo features? `wgpu` and `rendy` do it this way. I copied it :3 &gt; but likely in a production build I would want to define a preferred order of backends I support and let the user override it. Is that possible now? I think this is not possible right now, [`wgpu` defines the backend at compilation time](https://github.com/gfx-rs/wgpu/blob/master/wgpu-native/src/lib.rs#L4). I am not sure if this is a `gfx-hal` limitation or not.
I am trying to use a type that derived Failure as an error type for Sede deserializer. Serde requires that the error types implement [`serde::de::Error`](https://docs.serde.rs/serde/de/trait.Error.html) which requires `std::error::Error`. I cannot implement `std::error::Error` on the Failure type: ``` error[E0119]: conflicting implementations of trait `failure::Fail` for type `serde::de::error::Error`: --&gt; lib/src/serde/de.rs:28:21 | 28 | #[derive(Debug, Fail)] | ^^^^ | = note: conflicting implementation in crate `failure`: - impl&lt;E&gt; failure::Fail for E where E : 'static, E: std::error::Error, E: std::marker::Send, E: std::marker::Sync; ``` and I cannot implement `serde::de::Error` for `failure::Compat&lt;Error&gt;`. So I have to use a newtype struct which wraps around `failure::Compat`. See [the code[(https://github.com/lawliet89/ferrous-chloride/blob/835ad9155b70caa8b4794070d73fec9892864ad8/lib/src/serde/de.rs#L90).
Yeah, locks my whole browser up sometimes when I look at rusoto docs.
&gt;I don't think bandwidth is the issue. My CPU is spiking like crazy on some doc pages. Adding more CPU burden to the client seems like a bad move. &gt; &gt;Perhaps rust should be adopting wasm?
Thanks,. I'm going to have a look about that.
I'm not a frontend dev, so idk if this is even reasonable, but isn't something like parsing a good fit for wasm? Seems like a nice, pure compute issue.
The difference between Firefox and Chrome here is \*astounding\*. Firefox handles this page in &lt; 1 second, Chrome took so long I gave up (tab was starting to freeze up). I have webrender enabled as well. I was also able to recreate the result with the dynamo PutItem page that always hangs for me. I'll have to reconsider my default browser.
I didn't read through all the reponses but one big thing I think people are missing is running rust in server less environments IE aws lambda, gcp cloud functions, azure functions blah blah. It's a hayday in today's world where choosing a tool you like and one that is lucrative don't always align. I think if you're older this is harder to transition into unless you are doing you're a free lancer doing what you want as long as the job is done.
Since you don't mention location, can I assume this is remote?
Could various pieces be loaded via iframes?
My pleasure! :)
No. First, `A { a: C }` is not a valid expression, because `C` is not a valid expression. `C` is a type, not a value. Even if you initialized `a` to some valid value, the compiler must determine some type for that value. That type is fixed at compile time, and never varies, even if you don't see an explicit type in the `let mut` binding. What problem are you actually trying to solve?
FWIW, I get the slow load times on FF. I haven't used Chrome (outside of work where it is sometimes required due to Google websites) in a few years and I think it's a worthwhile switch in many ways.
I believe the "apply styles and render" part is the real bottleneck. Parsing wont get you much.
`A { a: C }` is a valid expression here because `C` is a unit struct.
Oh, wow! That's pretty close. Is there a way to make it remember my choice across pages?
Ahh, fair enough. Still, the type cannot vary.
Interesting. It's very consistently the case for me. I even disabled extensions in Chrome to make it fair, and enabled extensions in Firefox. Try enabling webrender?
Ah ok. Thanks - this is well outside my wheelhouse.
I tried it after you mentioned it but I didn't notice a difference. I didn't restart FF. Maybe that matters?
You do have to restart Firefox for that to take effect, yes.
/r/playrust
Ah, I see. I do think that SNAFU should be a workable solution for your case. Please let me know if you run into any issues!
&gt;I'm changing type of a and not just the value. Rust has limited capability of type introspection - if you aren't using trait objects information about types isn't remembered during execution. There's no opportunity to change types in this case. Imagine running `a.a = D;` in if statement with nonconstant condition expression, right after that if statement the compiler wouldn't know if `a` is of type `A&lt;C&gt;` or of type `A&lt;D&gt;` and couldn't store the information about the type anywhere so it can't even compile program such that it would figure it out at runtime.
I think one of the positions is. "Sr. Software Engineer, Systems" is listed as "US / REMOTE" but "Sr. Software Engineer, Tools and Performance" is shown as "SAN FRANCISCO".
I was reading through the documentation and I found one thing to note: the Sobol sampler introduces artifacts that are more pronounced partially because it’s a global sampler. If you do it per pixel there are noticeably less artifacts.
The parsing being referred to is the browser’s native HTML parser. This is already highly tuned C++; WASM won’t help here.
The majority of the time is being spent on parsing and rendering. I wouldn’t expect removing the download time by rendering locally would help much.
[Should the ETAs be so specific?](https://imgur.com/0mXIJfS)
I know that some package (rand) used within library A (Billy's) is older than the one I'm using, why would I want to mix it up all those definitions? My code should break until I explicitly prepare structures for the old dependency just to be able to use library A, I'd rather code explicitly against some older API (Binomial_rand1) and make the glue by myself than code against a fake "single" API (Binomial) and get a broken build or really weird issues. Its clear that ideally the dependency should be updated, but that shouldn't make it impossible to use the library. Also, if the dependency doesn't leak, there's "nothing to do" to make things work.
That’s interesting. What if the various implementers started collapsed with no actual DOM content. When you click to expand it could fetch an HTML fragment from the server and render it inline. You might even be able to reuse the documentation page for the implementer by using a selector to “snip out” the element you need. If Javascript is disabled then clicking expand could take you do the implementers documentation page.
Cool, that makes sense. I thought it might have been javascript parsing some data from an API or something.
If you disable webrender, is it slow?
No, however, (and I edited my post), I believe the major cause was in fact an extension. Perhaps I didn't properly disable it when testing. DarkReader hangs both Chrome and Firefox. Firefox is still faster for the page, but Chrome doesn't hang when DarkReader is disabled.
For someone who wants to go a bit deep this is the PR [link](https://github.com/rust-lang/rust/pull/60260)
In fact, this is exactly how `drop` is implemented: it’s just `fn drop&lt;T&gt;(_x: T) { }`!
Hey all. I just wanted to chime in and say that Buoyant is great! It is where /u/seanmonstar and I work. Buoyant is also investing a ton in Rust OSS (Tokio, Tower, Hyper, ...). The linkerd2 proxy is all Rust backed by the above stack :)
The implementation is literally UB, super weird. pub unsafe fn _mm256_store_ps(mem_addr: *const f32, a: __m256) { *(mem_addr as *mut __m256) = a; } https://doc.rust-lang.org/src/core/up/stdsimd/crates/core_arch/src/x86/avx.rs.html#1669-1671
You can't do this because type `A&lt;C&gt;` is fundamentally a different type from `A&lt;D&gt;`. When you make an `A&lt;D&gt;`, the type of the field `a` is `C`, and it will only ever accept a `C`. You might be able to achieve what you want with enums (depending on your use case). ``` enum X { C, D, } ``` where C and D can be just as complex as types, if I'm not mistaken. Then your field could be of enum type `X` and you can set it to either value. This sounds right in my head, someone should let me know if I'm wrong.
good luck with your ban :3
Copy/paste error from `_mm256_load_ps` methinks. Submit a PR!
&gt; _mm256_store_ps Looks like this also applies to: `_mm256_store_pd`, for both the `x86` and `x86_64` variants.
Congrats!
Can you point me towards where it is specified that casting a `*const T` to as `*mut T` is undefined behavior? As far as I am aware,only casting `&amp;T` to `&amp;mut T` (even indirectly) is undefined.So, as long as the `*const T` doesn't come from a `&amp;T` this should be fine. If you run [this playground](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=72470ea474acb6025b6166f08c7c3e8f) using the "miri" tool,you'll see what I mean.
https://doc.rust-lang.org/reference/behavior-considered-undefined.html &gt;Mutating non-mutable data — that is, data reached through a shared reference or data owned by a let binding), unless that data is contained within an UnsafeCell&lt;U&gt;. Of course in that specific situation the compiler devs may know what will happen, but it's literally a lie to the compiler and the compiler may fuck you up for that. But it also may not.
&gt; literally UB [Check out the source of `core::ptr::Unique`](https://doc.rust-lang.org/1.26.2/src/core/ptr.rs.html#2514-2522). At the language level the only tricky rule imposed by `*const T` is covariance. *Mutation is allowed.* If `U` can be cast to `T` automatically then the compiler will also cast `*const U` to `*const T` automatically. If you then mutate through the resulting `*const T` pointer it may have the effect of implicitly casting `T` back to `U`, which may be UB, especially when `T` and `U` are the same thing with different lifetimes. In that situation `T` -&gt; `U` will cause use-even-earlier-before-free (utterly safe) and `U` -&gt; `T` causes use-after-free (exploitable UB). The signature of `_mm256_store_ps` is still wrong because it will cause the compiler to generate spurious errors and require users to use `as`-casting, probably without understanding why that casting is necessary.
https://doc.rust-lang.org/reference/behavior-considered-undefined.html &gt;Mutating non-mutable data — that is, data reached through a shared reference or data owned by a let binding), unless that data is contained within an UnsafeCell&lt;U&gt;. That doesn't explicitly say you can't do what simstd did, but the pointer was probably obtained from a shared reference. So when that shared reference is read it may be a cached value so the write will be ignored. Of course in that specific situation the compiler devs may know what will happen, but it's literally a lie to the compiler and the compiler may fuck you up for that. But it also may not. I'm talking about writing to a const pointer (that probably originated from a shared ref).
Maybe you could use `sqlparser-rs` for sql query parse, I think. BTW are there any benchmarks?
The problem is when the user gets that `*const` from a shared reference...
Isn't it a breaking change? Although one to solve soundness holes? I know breaking changes are allowed, but is a PR enough in this case? Or just to start the boureocratic chain?
Please actually try the code before asking questions. And no, it won’t work. You need to have update take in a mutable reference to self since the Child element being mutated is still part of the parent struct.
Sure. thank you. But in that case, how do I achieve pattern like this?
Currently no. But soon I will add some benchmarks.
Love it. &amp;str contains() -&gt; bool) -&gt; bool) typo
Change `pub fn update(&amp;self, data)` to `pub fn update(&amp;mut self, data: u32)`
This is a general problem with working with pointers,you have to know their provenance to not cause memory safety related UB.Which is why this `unsafe fn` documents the requirements for the caller.
A3. Office bathroom gentlemanly harassment for the "fullstack" people
It is not possible to go from `&amp;T` to `&amp;mut T` which is what you'd need to "achieve the pattern". Doing such a conversion would have undefined behavior (meaning the compiler can do whatever it pleases with your code). Can you please explain the higher level construct you are trying to create? With more context we could perhaps help you out more.
Thank you! Please let me know what you think - it’ll help me derive what should be the next focus areas.
Or from ptr::null()...
The only documented requirements are the alignment. It doesn't say anything about getting the `*const` from a shared reference. Which it should if it was a intentional part of the Api, but it doens't seem so.
"Writes `a` into memory",`mem_addr` is obviously the address of the memory it writes to given the documentation as written.
Will do! Do you plan to support other database libraries, such as diesel?
Sure, that's generally how writing works, but there is a reason we have explicit type safety. Why not have everything as a *const anyway? If you want to avoid UB you can even use volatile. Beautiful world.
Great question! I’d need to look into what that involves (assuming it’s driver before abstraction) but would be more than happy to pursue it given demand.
Oh, that's what I get for posting before bed. My comments (now struck) apply better to the reverse case, calling something `*mut` when it shouldn't be.
 fn handle_conn(&amp;mut self, key: usize, event: Event) -&gt; Result&lt;(), io::Error&gt; { if let Some(client) = self.conns.get_mut(key) { match client.execute( &amp;self.poll, event, &amp;mut self.context) { Ok(()) =&gt; Ok(()), Err(e) =&gt; { self.conns.remove(key); Ok(()) } } } else { Ok(()) } } Here in this case, when I execute the client, I am passing one reference from \`self.poll\` and other reference \`mut self.context\`. This won't work i guess, but then what is the alternate solution?
This title is misleading, to me it suggested that someone is recreating the UWP stack in rust. 'Rust support for UWP API in progress' or something similar would be more accurate.
This function would certainly benefit from changing it to `*mut T.`
I don't see any problem with `&amp;str`'s `contains()`. It's &gt; `contains (char | &amp;str | &amp;[char] | (char) -&gt; bool) -&gt; bool` Are you seeing something different?
I'd still wait to see if anyone proposes an alternative design that's more idiomatic but, if nobody does and you can modify the definition of the immutable struct, you could use [interior mutability](https://ricardomartins.cc/2016/06/08/interior-mutability).
Sure, RAII is very useful, but my point was based on the interpretation of the OP's question as "this type can only be instantiated exactly once in the program lifetime" (a singleton pattern), not - as now seems more likely - "this type can be used exactly once at a time in the program lifetime" (which is what `Mutex&lt;T&gt;` is for). Enforcing single creation through using the destructor sounds counterintuitive because it can be defeated. I would argue designing around `mem::forget` is critical, however, to avoid UB in Safe Rust. Calling out to Unsafe Rust is an explicit waiver of the guarantee of not causing UB, and the allocator calling `panic!` due to not being able to allocate memory is not UB. If you look through the RAII types of the Rust standard library, they're all safe even when you call `forget` on them (heap types will leak, and leaked memory does not cause aliasing violations; mutexes will never unlock, and deadlocks do not cause race conditions). It's generally pretty easy to guarantee defined behaviour on desktop and mobile, but it gets much trickier in the embedded environment when you consider Direct Memory Access (essentially hardware `memcpy`) and ensuring that Rust does not `drop` your data buffer while the hardware is still using it (and no, designing a `Mutex`-like won't save you here, because you can `forget` the guard and start a new DMA transfer while the existing one is in progress).
Are there plans for an msvc target? Mingw32 is a nonstarter for some of us.
wow, this makes a lot of sense, thank you.
What is the overhead of using \`RefCell\` inside \`Rc\`?
I know this is nitpicking, but I think it's better to blame the signature for being so easy to misuse rather than calling the body UB. It's the silent coercion from `&amp;f32` that's really nasty, IMO. Anyone using it should know that modifying an `f32` while it's borrowed behind `&amp;f32` is UB. I don't think it's really even a documentation issue. It's that the type system won't catch a moment of knowing better without thinking better.
Shipping Rust in UWP apps is actually possible already (we have a windows store app with a rust lib). You just have to be careful with winapi-rs features of your deps. A relatively popular crate term will be incompatible. Using mio will work.
Yeah, I took this parse combinators thing (possibly) a bit too far, and made three parser traits, `ParserOnce`, `ParserMut`, `Parser`. So naming types like this allowed me to reuse large portions of codes. Macros are amazing at handling this sort of boiler-plate and I used them extensively, this would not have been possible if I had to use `impl ...` and anonomize the types. But by making three different, but related `Parser*` traits I was able to expose a more flexible api, for example the `and_then` combinator could take a closure that returned a `ParserOnce`, which means that I could move stuff around inside the closure without having to clone stuff, which was a big win for me. I also made some other changes, like allowing you to collect directly into `String`, or `HashMap` when using the repeating combinators, like `zero_or_more` or `one_or_more`, which I found to be very interesting.
Can I also use \`Box\` and then \`DerefMut\` and then mutate internal data?
You'll have to wait for someone else to answer that. I haven't needed to use it yet, so I never looked into the specifics.
`DerefMut::deref_mut` takes `&amp;mut self`, so it can only be called if you've already solved the problem you're having.
Did you use precompiled binaries for gtk? How did you get it to work? Compiling on Windows doesn't work for me
Can i do something like `let mut value = *x`? Where x is boxed pointer, i will `deref` it and then i can mutate it?
The `*` operator is just syntactic sugar for `Deref::deref` or `DerefMut::deref_mut`... both of which enforce by their function signatures that you can't produce a `mut` output from a non-`mut` input. From the compiler's perspective, you're trying to find a way to store "pineapple" in an integer variable.
First of all, make sure you’re not confusing “run time” with “runtime”. Some context also might help discern the meaning of the phrase in the situation you are in. My best guess goes with the comparison you mentioned: comparing the run times of different programs or languages would mean executing the same programs in different languages and comparing the speed of execution.
Eh, my very first link is to the same place...
How about |a, b, c| =&gt; {a + b + c }? Still reads like a function
Oh.. run time and runtime are different? What is rust runtime means?
Didn't see that and had to search for it. 👍
It sounds useful and interesting. I will consider it in v2.x.x
The language “runtime” is essentially code that gets included in every program in that language, regardless of the contents of the source code. https://en.m.wikipedia.org/wiki/Runtime_system So talking about the language runtime and comparing it to another language’s runtime is often a comparison of the fundamental overhead associated with each language.
Okay well i’m not a rust expert, i just like the language, but from my experience, dust does not have a runtime as it produces native binaries that are executed directly as machine code. Runtime can mean various things though. The run time phase of a program (in contrast to compile time) is the stage of a program’s life cycle in which it gets executed. This is often referred to in the context of execution of certain actions happening at runtime or compile time; for instance, you might hear the phrase “this would incur some run time cost”, meaning that adding something to a program would impact its performance during run time. Rust’s const fn’s are evaluated at compile time, so they (theoretically?) don’t incur any run time cost. A runtime can also mean a program or library meant to support the execution of another program. A good example of this would be the Java virtual machine (JVM). Java, along with other JVM languages, compile down to JVM bytecode, which is executed in the Java runtime (or JVM runtime). Under this definition, Rust does not have a runtime; I say that, but I may be wrong, as Rust generally has environment setup and panic handling functions and a main function separate from the user defined one (lang_start, rust_eh_personality, etc) and this could likely be considered a runtime. If Rust has a runtime at all, it is minimal, as Rust generally strives to include as much as it can in the standard library rather than building things in to the language itself. I’m sorry if this was a bit too much information. i’m not sure how experienced you are with Rust, but I tried my best to provide a complete and (hopefully) correct explanation. I do not have access to a computer right now but I can add some articles related to this topic later if you want. Good luck with whatever you’re doing!
How can we measure rust runtime? How can we measure the included code runtime if that is the runtime? Can use the eg code command: $ time ./hello in linux
OTOH, a "runtime" is the code that the language bakes into your program, just to run it. Stuff like basic error handling minimally, but memory management too, and often math, threading, error handling, string conversion, etc. A minimum hello world in a language will include its runtime, and not much else, but that runtime needs to be able to write to stdout, probably do string formatting, memory management (though hello world isn't demanding in that respect), error handling, etc. For Python, the whole interpreter is the runtime. For Java, it's the JVM and the system libraries. For a realistic C program, it's libc. Similar code is baked into Rust and Go programs (the latter also needs its garbage collector). Check out [this](https://github.com/rust-lang/rust/blob/master/src/libstd/rt.rs). It's Rust's "real" main. The thing that calls main. It sets up some basic panic handling, and calls main. It (with all its dependencies) is the smallest thing you could call Rust's runtime. Generally though, I would think of all of libstd and libcore as the Rust runtime. And most of the time, that includes some sort of libc as well.
let me double check this for y’all. i believe a third, non-rust, role is open to remote for sure. in my experience, it’s usually a good idea to figure that out after a chat or two since the right person is *the right person* :) also buoyant does have a few remote devs so it’s not out of the question!
Typically a program’s _run time_ is the amount of time that elapses as the program does its work, whatever that is. Contrast that with a program’s _runtime_, which is the local environment in which the program runs as and when it executes its work. Contrast that with a _programming language runtime_, which is all the machinery provided by the language itself and which is injected into and supports the execution environment mentioned above. This can include things as small as some minimal routines for laying out and freeing memory, as in C, to entire virtual machines, interpreters, and standard libraries, as in Python or Ruby. Think of it as the minimal machinery that is both part of the language and _must_ be present — either in the executable itself or installed on the computer — for _any_ given program written in that language to run. Rust strives to have a very fast _run time_, it does this in part by compiling to an executable and injecting only a _very_ minimal _language runtime_, which does not, for instance, provide a memory manager / garbage collector that operates during the executable’s _runtime_.
Cc /u/nasa42 we should link this in next TWiR. /u/sanxiyn do you perhaps want to join us?
`Vec` does have a `get` method through its `Deref` to `[T]`: pub fn get&lt;I&gt;(&amp;self, index: I) -&gt; Option&lt;&amp;&lt;I as SliceIndex&lt;[T]&gt;&gt;::Output&gt; where I: SliceIndex&lt;[T]&gt;, https://doc.rust-lang.org/std/vec/struct.Vec.html#method.get
Not really, no. UWP is an API, so the context of “port” is clear.
But UWP isn’t really a successful platform with Windows Mobile never catching on and most Windows users preferring regular desktop applications.
Since you are relying on external dependencies (image), shouldn't you be using cargo instead of rustc? The rustc error itself seems to imply you aren't giving it a valid path to a source file. Is Image\_test.rs in the same directory you are running the command from? If not, you should either switch to that directory, or supply the full path to the source file. If you are not familiar with how the command prompt works, I advise learning more about it before trying to use command line tools.
UWP should die in fire
But it’s the only avenue for normal people to ship to Xbox One.
You can collect the streamed events to build a document tree (AST/DOM) yourself.
Its hard to answer those questions in the abstract. What are you actually interested in knowing?
This is also a bit more low level than one might exepect. For example a markdown header`# Header` will be parsed as * found the start of a header * found a text "Header" * found the end of a header instead of simply * found the header "Header"
PM sent.
If you are interested in measuring how long your program takes to execute from a human perspective, ‘time’ is a good tool to use. If you wang to know specifically how much tine it takes to execute the Rust runtime setup code... thats kind of sophisticated measurement to make. If you are interested in measuring how much code overhead is included by the language the ‘time’ command really cannot help you. If you want to measure the size of the rust runtime, then as someone else mentioned compile a program with no code in the main function and look at the size of the resulting binary (don’t forget to compile in release mode though ‘cargo build —release’). Its not really as simple as that though... and its not clear to me that this information is particularly useful. Do you have a specific task you are trying to complete that involves measuring something or are you just looking to expand your understanding of the usage of the phrase “runtime/run time”? If so, I would suggest making another post that directly states your actual goal/issue you are trying to solve. Cheers :)
It seems to me Mozilla's main interest is HoloLens.
Perhaps, but that does not change the fact that there is demand for UWP port. I have my opinions (POSIX should die in fire, x86 should die in fire, etc.), but I understand constraints and compromises Rust project must work with.
To test out /u/raphlinus's claim of "it should Just Work", check this out: [repo](https://github.com/Walther/pulldown-cmark-wasm) and [live demo](https://walther.guru/temp/pulldown-cmark-wasm/) \- it took me about 20min to do. Massive applauds to all of their great work! (And to the people behind \`wasm-bindgen\` and \`wasm-pack\` too!)
This is amazing. When someone else pointed out that there's links to the actual documentation my mind was blown. Bookmarked.
It sounds like you're running up against the fact that the OS's task scheduler has the final say on when something gets woken. I'd suggest reading through [this OpenSUSE page](https://doc.opensuse.org/documentation/leap/tuning/html/book.sle.tuning/cha.tuning.taskscheduler.html) on interacting with and tuning the Linux kernel's process scheduler. Have you tried
&gt; This won't work i guess, Why not? I don't see anything obviously wrong with this code, except perhaps a conflict between holding the `client` reference while trying to remove from the hash map. (Not sure if NLL is smart enough.)
Since when are we banning people for playing with Rust? ;D https://play.rust-lang.org/
I second this motion
That was a bug in \`rustup 1.17.0\` and \*should\* have been fixed in \`rustup 1.18.0\` -- are you still experiencing this?
 Theory: your **X** program consumes CPU very often, so the CPU never goes to some deep sleep mode, that takes time waking up from. The scale of the Y axis is nanoseconds, right? If you use chrt to put your program into SCHED_FIFO, then you might be able to use the wakeup_rt tracer to figure out if the kernel does other things before waking up your program.
Maybe you can help?
&gt; if it's possible to write unsound code using this method But this is not the case. You can't use two iterators returned by `columns()` simultaneously: ```rust let it1 = columns(&amp;mut data, 4); let it2 = columns(&amp;mut data, 4); it1.for_each(|mut col| col[2] = col[0] + col[1]); it2.for_each(|mut col| col[0] = col[1] + col[2]); ``` ``` error[E0499]: cannot borrow `data` as mutable more than once at a time --&gt; test.rs:43:28 | 42 | let it1 = columns(&amp;mut data, 4); | ---- first mutable borrow occurs here 43 | let it2 = columns(&amp;mut data, 4); | ^^^^ second mutable borrow occurs here ... 47 | } | - first borrow ends here ```
Has Microsoft publicized any stats or you are doing the sampling fallacy?
There is an issue opened here: [https://github.com/rust-lang/rustup.rs/issues/1798](https://github.com/rust-lang/rustup.rs/issues/1798) about that. Any useful input appreciated :D
Remote or US remote?
Python is a dynamic language, Rust is a static language, with a rather advanced type system. Because of this, you will have to accept large differences between them. I'm happy with using Diesel, and while it is still being developed, it is plenty feature complete for my use. And what's already there is stable. Perhaps you will have the same experience?
Awesome, thanks! I didn't realize it was already packaged on npm. 178k wasm uncompressed for a full renderer is amazing. 58k after a single pass of brotli.
I wonder if that version on npm is a) official b) actively maintained / updated 🤔
Doesn't look it, but it shouldn't be terrible to update it.
The thing is if you need to mutate something inside of an immutable struct it's a good sign there's a bad pattern occurring. The goal is to make things as immutable as is reasonable. However, there are some cases where this kind of behavior is acceptable or even required. In my recent Othello game I created a player type that stores a pre-defined list of moves. I was using it to test a specific game State with the algorithm. I was able to keep this list mutable [in this example](https://github.com/loganintech/miniothello/blob/master/src/player/specific.rs) by using a [RefCell](https://doc.rust-lang.org/std/cell/struct.RefCell.html) which may be what you need. If you need it to be multithreaded use a [mutex](https://doc.rust-lang.org/std/sync/struct.Mutex.html)
Thank you!
Imagine that your program either stands alone or is a "save" for another program to run. In Python or C# the relevant "runtime" starts up and then loads your code. Your code is not the only program running - arguably your code is not a program at all. It's data + someone elses "runtime" In Rust (or C, C++), the compiler creates a program with your code that stands alone. The OS (Linux, Windows, Mac) runs through a startup process then your program stands alone.
I use VSCode (Visual Studio Code) with the extensions C/C++, CodeLLDB, Native Debug, Rust (rls) (and some others). This will let you debug rust code (with some complex setup) and give you a lot of hints when looking at the code. I suggest you follow the rust book, putting examples on your disk and playing with them. As soon as possible start trying to code your own ideas and struggle with that. It's the best way to learn.
that would move x, so it'll probably say 'can't move out of borrowed context', or if x is Copy then it'd copy it to value, so you could mutate value but not the original x
Yeah, Mozilla already yak shaved an entire programming language to build a better browser, it might be reasonable to stop before they end up designing their own ISA too
Yeah, that was what i thought too. I wonder if there could be an official distribution (by the pulldown-cmark team) that would be (automatically?) updated within the release process of the crate itself
Oh, thanks. I was typing mostly from my phone, and I didn't check the types.
Diesel is technically not a "real" ORM and there is a reason why. Looking at the history of ORMs it has become clear the ORMs are not really holding up to its promises and i agree. I have worked with Hibernate for a long time and i think that kind of abstraction is not worth it. Both for the user and the developer of the ORM. Diesel strikes a good middle way between convenience (you want from an ORM) and the flexibility you have with pure SQL. I worked a bit with Diesel and and first it feels not finished but the design is deliberately chosen and it shows. So no Diesel is not like SQLAlchemy – for a reason – and i don't think there is a goal to achieve that. I don't know a library that is more like the old ORM way but at this point i don't feel like i want such a thing. Diesel works pretty well. There are some things that could improve but the overall design is pretty much finished, at least that is how i feel. &amp;#x200B; In the past i ran into serious problems with Hibernate that are due to the very nature of ORM. So you write your "hello world" example and it feels great but in production systems you tend to circumvent all the "niceties" of the ORM design anyway and rewrite the hole thing. And in the End it looks pretty much the same as the code you write in Diesel from the beginning.
This looks like a wonderful idea. Rust is a great place to get started with this sort of system. Since people choose rust because they want to work slightly harder for safety, control and trust in the code it stands to reason they'd want to vet their dependencies. Sorry to hear about your health issues. I hope you continue to recover.
Oh. My bad. This `(char) -&gt; bool) -&gt; bool` is some odd nightly `Pattern` thing. [https://doc.rust-lang.org/std/str/pattern/trait.Pattern.html](https://doc.rust-lang.org/std/str/pattern/trait.Pattern.html) I wonder if nightly stuff could be visually flagged?
I usually use VSCode for all the other languages (but golang) but I have heard that with CLion its easier to debug and since I have a free student license I went with that.
Related issue: https://github.com/gfx-rs/gfx/issues/56 Rather than fighting this, I'd suggest for now to instead ship one binary per backend and then build a launcher that either automatically detects the best backend or gives the user a choice.
I'm currently reading through The Book, and i don't quite get the point of `Mutex` by itself. It allows you to have shared mutable state between threads, but it needs to be wrapped in `Arc` to actually work across threads. So what's the point of `Mutex` by itself?
Would RISC-V suffice?
&gt; Cargo is hailed as having "solved dependency hell." What's the difference? There are few: 1. Rust has saner stdlib which is also easier to extend, so there is less need to replace and reinvent parts of it. 2. There is no pressure to save every byte. If you want some functionality, you can do it in a generic way that can be used in many situations, there is no need for creating custom modules handling exactly one specific usecase. 3. Rust is more specialized and complex and less popular, so as a result you will have higher quality of developers choosing it.
I can't reproduce dtolnay's results (possibly because the optimiser is now too smart?), but our use-cases may well be similar to his example: zero-sized `Ok` and typically a huge portion of calls being successes. I guess I should accept this as "soft evidence" that error type size can matter. This still doesn't enforce too many constraints on our design however; e.g. we could double-box a custom trait or use `ThinBox`. I'll have another think about the PR.
You don't *need* to wrap it in an Arc to make it work across threads. The problem is that if the reference to the Mutex has anything shorter than a `'static` lifetime, there is no guarantee per se that the initially created Mutex will still be alive when other threads are trying to access it. The `Arc` (Atomically reference counted pointer) transfers the Mutex to a shared heap location and will make sure that the Mutex is not freed until all references have been dropped. Two cases where an `Arc` is not needed: 1) You use `lazy_static!` to create a `Mutex` reference with a `'static` lifetime. This reference can be sent safely across threads because it lives for the entire runtime of the program. 2) You use `crossbeam`'s `scoped_thread`, which ensures that the child thread has finished before the main thread is allowed to continue and potentially drop the Mutex. Find a more detailed explanation with code samples in [this stackoverflow thread](https://stackoverflow.com/questions/33116317/when-would-you-use-a-mutex-without-an-arc).
Just a note, a runtime isn't necessarily something that lies outside of the compiled binary, it may actually be included, like in the case of Go.
I see, thank you
No something else is going on. I've wondered if I was going crazy, with docs that just never finish loading. It doesn't always happen to me hence why I thought it could have been an issue on my end. I'm glad someone posted their experience with rust docs going crazy :D
I think Diesel is appropriately called a query builder. I second your recommendation of eschewing ORMs and using a query builder instead.
Hi james, thanks for showing interest. I never heard about Interledger before, so I'm really glad that you have sent this. &amp;#x200B; \&gt; How does this compare against [Interledger](https://interledger.org/)? I just read a bit on Interledger website. I think that the main similar point is that Interledger also uses some form of "mutual credit": Two users that have funds on the same ledger can open some kind of a channel, and from what I understand payments can be pushed along routes of those channels, going through many ledgers. I am familiar with the lightning network and this seems to be a similar idea. On the other hand, I think that Offst and Interledger are attempting to do different things. If I understand correctly, Interledger attempts to allow users to liquidate funds between multiple different ledgers. while Offst does not require any existing ledgers for its operation. It is a full economy on its own. In other words, you don't need a previous bitcoin or ethereum account to use Offst. Another difference I see is that Offst is more decentralized. Interledger uses "Connectors" that mediate transactions. Those are not usual participants Interledger seems like a more federated solution: There are various Ledgers and Connectors set by few people, and there are many end users that can use the services of the Ledgers and the Connectors to pass funds. The way I see it, Offst is more decentralized: The only economic entity is an Offst node, and it fully belongs to the end user. Offst uses relays to transfer TCP communication, but Offst relays have no idea what is going inside the communication, and they have no relation at all to funds transfer. They are just dumb TCP proxies. Offst also uses index servers to find routes between nodes, but index servers also don't have any relation to funds transfer. The only reason we have those relays is because of real world connectivity problem, and the reason we have the relays is that at this time there is no known decentralized solution for finding routes (At least to me). &amp;#x200B; \&gt; No asymmetric cryptography like signatures are necessary, unlike Offst. I was really curious about how this is possible. The answer is that Interledger uses TLS (HTTPS), which means that it definitely uses asymmetric cryptography. In fact, if I understand things correctly, if you ever want to host your own Connector in Interledger you have to get a domain name and register a certificate. In addition, Interledger relies on the existence of ledgers, which are mostly blockchain based, which in turn rely on asymmetric cryptography. &amp;#x200B; \&gt; The Timelock doesn't require global consensus on time either, only the reasonable assumption that clocks step at approximately the same rate I have to read more about how it works to comment about this. It sounds interesting. I can say though, that if you use Interledger with your bitcoin or any other blockchain backed assets, you are actually relying on some kind of time synchronization. All blockchains I know of have some underlying assumptions about time synchronization. Regarding Offst: Offst transactions has no time in them at all. As far as I know, even if your computer clock decides to go twice as fast, or even starts going backwards, your Offst credit will still be safe. This is by design. Time is so difficult to do right in decentralized settings (especially in security sensitive ones) that the best choice for us was to avoid it altogether. &amp;#x200B; \&gt; Interledger is backed by the W3C I wish they would have backed Offst too! But I believe there is a flip side to this. I just googled how to host your own Connector in Interledger, and I found this: [https://medium.com/interledger-blog/running-your-own-ilp-connector-c296a6dcf39a](https://medium.com/interledger-blog/running-your-own-ilp-connector-c296a6dcf39a) On my screen it looks like 8+ pages of configurations where you have to get domains, register certificates and configure your nginx. As a comparison, this is what you have to do in Offst if you want to run your own Relay: stmgr gen-ident --output relay.ident strelay --idfile relay.ident --laddr 0.0.0.0:8000 &amp; Two commands and you are good to go. &amp;#x200B; \&gt; and has been formally proven I think that this one very important. If things go as planned I hope to get a serious formal review of the Offst protocol, and also have the core code that handles the transactions formally verified.
Phpstorm + Rust plugin
It's been the case for months now, nothing new here.
👍
https://mobile.twitter.com/andreapessino/status/1042120425415700480 is an excellent guide 🦀
Discourse works around that by hooking ctrl-f with a custom search implementation. Pressing ctrl-f twice gives you the regular search. Feels slightly clunky but in some cases (like Iterator) it's probably much faster than waiting for the entire page to load. Maybe it could be conditionally hooked depending on the size of the page/whether it's completely loaded.
Looks like I'm one of today's 'happy' 10000 (probably needs a scaling factor for Rust's community size).
Implementations can have documentation.
So the short answer is no. But for the fun let's talk about why: Rust wants to provide extremely efficient abstraction. So let's step through compiling some code: ``` struct S; impl S { fn do(&amp;self){} } struct G&lt;T&gt; { g: T } fn main() { let x = G { g: S }; x.g.do(); } ``` When we try to compile `main`... **the first thing we need to do is allocate space for x:** which should just be the size of `G&lt;S&gt;` which is just the size of its members which is just the size of `S` which is 1 byte This is really cool! It means we don't pay any cost for using `G&lt;S&gt;` compared to `S`. **then we generate the function call:** We know that `x` is of type `G&lt;S&gt;` so `x.g` will be of type `S` so `x.g.do` must be `S::do` so we can use *static dispatch* *static dispatch (n):* when we know at compile-time which function will run and do a faster call (a direct `jmp` in assembly). Wow! Despite all the types and structures this program has the same performance as: ``` struct S; fn S\_do(&amp;S){} fn main() { let x = S; S\_do(&amp;x); } ``` But doing this optimization means we have to give up some flexibility. Let's look at compiling your code now **the first thing we need to do is allocate space for x:** which should just be the size of `A&lt;C&gt;` which is just the size of its members which is just the size of `C` which is 1 byte **then we handle the assignment:** Now, because `a.a` is only 1 byte in size we know the right side of the assignment has to also be a 1 byte value. In your case, `D` is also 1 byte in size, but you should be able to see how we can't allow just *any* value. Like we couldn't allow `a.a = 256i32` because `256i32` is an `i32` which takes 4 bytes. It simply wouldn't fit. Right away, from a language design perspective it would be weird to allow this assignment with some types but not all. But hey, let's suppose that we think we should allow this. Let's consider a slightly modified version of your code: ``` struct A&lt;T&gt; where T: B { pub a: T } trait B { fn do(&amp;self); } struct C; impl B for C { fn do(&amp;self){ 1; } } struct D; impl B for D { fn do(&amp;self){ 2; } } fn main() { let mut a = A { a: C }; if(random()) { a.a = D; } a.a.do() } ``` Let's skip straight to thinking about how to compile the line `a.a.do()`. `a` is of type... wait, what type is `a`? We can't know what the type of `a` would be until runtime when we can actually run `random()`. So we can't do *static dispatch* anymore. Instead, we'll have to figure out which `do` to call at runtime. So how can we tell that our 1 byte value in `a` is of type `C` or `D`? We can't! In an effort to do zero-cost abstraction we got rid of any and all extra data. What we would need to do is expand `a.a` to be a tuple of `(value, type)`. So when we allocate `a` we allocate 9 bytes: 1 for the value and 8 for the pointer to the type (on a 64-bit machine). Then when/if we do `a.a = D`, we update both the value and type. So then when we compile `a.a.do()`, We say "go find `do` inside `a.a.1`. This is called dynamic dispatch. *dynamic dispatch (n)*: A slower way of calling a function where we lookup the function using a runtime value. This *would* work. Unfortunately `A&lt;T&gt;` is now larger than `T` which is a bummer, and also this whole scheme only works for `T`'s that are the same size. Instead of breaking the zero-cost goal and in the name of consistency, rust only does dynamic dispatch on references. Here's why: What's the size of: ``` struct S { s: &amp;dyn Foo; } ``` It's the size of its members which is the size of `&amp;dyn Foo` which is a tuple of pointers `(data_addr, impl_addr)` which 2 * 8 bytes which is 16 bytes. Because all references are the same size, we can safely replace `s` with pointer to `MyStruct` or whatever we want. If someone writes: `my_S.s.do()` then we know to look under `impl_addr` (which is just a vtable if you're familiar with the concept), find `do` and pass the `data_addr` to it. So to answer your question you need to use some kind of reference (`&amp;dyn Foo`, `&amp;mut dyn Foo`, `Box&lt;dyn Foo&gt;`, `Rc&lt;dyn Foo&gt;`, etc.) or use an `enum` instead of a trait. Sorry if that was too much, but I was up late and wanted to write. Happy to answer questions!
\`Pattern\` is nightly stuff, which means you cannot implement it for your type in stable, but you can still use types which std already implements for you on these methods. That's why I think the concrete types are more useful than a generic \`T: Pattern\`, and thus list them here rather than just copying what the doc says.
Nice work!
Btw, the first line can be shortened to `let input = File::open(path).ok()?;`. And line 61 to `let count = reader.read(&amp;mut buffer).ok()?;`
In this case [Cell](https://doc.rust-lang.org/std/cell/struct.Cell.html) could be used instead since `u32` is `Copy`
I definitely have 1.18.1. I'll have to check next update to see if it still does it, or if I was just doing it wrong.
Haha, sounds like we're just writing the same parser library... I have `ParserOnce`/`ParserMut`/`Parser`, too. I'm not yet using macros to write them though, but it's a pretty obvious next step based on the level of duplication in my code. One of my favorite things about this design is that I can create a `Tokens` parser that succeeds when the first elements of the stream are the same as the items produced by an iterator, without having to make the iterator cloneable. It's fantastic. I actually spent a large amount of time getting those "iterator parsers" right, because I was quite disappointed with how limited `combine`'s support for them is. I've called them `many` and `many1` so far because of precedence but I honestly like your names more. My `many` takes a `Fn(&amp;mut many::Iter&lt;Self, &amp;mut Input&gt;) -&gt; Option&lt;O&gt;`, so that closures gives you an iterator that you can do anything with, like collecting it into a data structure or ignoring it (and returning `None` indicates that parsing failed). Those two turned out to be so common that I added `collect_many` and `skip_many`, too, and the same for `many1` and `sep_by`. I also added `iter_many` that also takes an input stream to turn the parser into an iterator, which is sometimes useful, but then you obviously can't parse anything after that anymore. I'd be very interested to know what you think of this design!
I have been inside the directory, but now that I think about it, I probably shouldn’t be using rustc to run a cargo program. What line should I use to run the code?
I've updated it and added a "fn " prefix to every item. They are kept hidden for simplicity, but you can now search e.g. "fn map" for functions starting with map.
It's the other way around: `Foo` does not inform that `T` impl's `PartialEq`, it *requires* its argument `T` to implement it. It's a transitive requirement. Syntactically, you can shortened it to `… where T: Foo&lt;T&gt; + PartialEq`. The problem is known and might be fixed in the future with the feature `implied_bounds` ([tracking issue](https://github.com/rust-lang/rust/issues/44491)). There are already some implied bounds in the language: * some lifetime bounds * super trait bounds * bounds on projections (associated types) [Source: Rendered RFC](https://rust-lang.github.io/rfcs/2089-implied-bounds.html). So in today's Rust, if you are able to switch `T` and `Self` in your trait definitions, you will take advantage of implied supertrait bounds. That's the best it can get as of right now.
\&gt; why the bare bones approach to error handling bothers you so much. The boilerplate-ness is something a lot of people mention, but it's hard for me to really appreciate that because, to me, error definitions take so little time and so little code compared to everything else My problem with vanilla error handling is that you have to make a choice, early on in the code's life, about what your error handling strategy will be, and you could make the wrong choice. Exposing custom Error types scales but has a high initial investment, and exposing Box&lt;&amp;dyn Error&gt; doesn't scale well but gets you started faster. It's hard to your mind change later and decide you like option 1, because changing likely has many touch points (\~every function in the code) AND because errors are part of the public API. This is the conundrum that error-chain was trying to break: [https://brson.github.io/2016/11/30/starting-with-error-chain](https://brson.github.io/2016/11/30/starting-with-error-chain) &gt;If you follow these simple instructions right from the start you will have all your error scaffolding set up in a way that will scale as your project grows across many crates, no thinking required. &amp;#x200B; {start small, scale well}: right now i can pick any one of those, but I want to pick any two. &amp;#x200B; Option 2 is never going to scale well because scaling well means doing actual API design on the exposed errors, and option 2 doesn't allow that by definition. Therefore, the only way to achieve both goals is to make Option 1 start small, and I think that is why so much effort is being paid to reducing the amount of code it takes to start with custom error types.
Maybe you should get in touch with RustSec to get official crev integration from them
How does it compare with cargo audit? We use audit in our CI pipeline right now.
I don't understand the meaning of "in a particular scope". I'm not aware of the context of that line, but I think something like "You can have either one mutable reference or any number of immutable references to a particular piece of data at any one moment." describes the borrow checker more clearly.
Microsoft themselves don't use it. E.g. they published Office on Mac App Store but not on Windows Store.
So, I just looked into several crates and it seems to me that the current form of this cheatsheet is probably quite specific to std API, especially to the subset here. It's not a one-size-fits-all style. To actually make it useful for any crate, we may need to try creating cheatsheets for different crates manually first, and gain more experience about what kind of information is important for different kinds of crates. Only then it is possible to generalize this concept and make it automatic, I think.
Why isn't there x86_64-pc-uwp-msvc?
I'd also like to play the game at around 60fps on pretty good settings.
This is a huge contribution to the community. There are many applications that cannot function without this
On windows I've run into this issue and its basically global timer resolution shenanigans, aka timebeginperoid (on me phone so I can't check the exact name) Linux probably has something similar
&gt;What line should I use to run the code? `cargo run`
I think I've asked some version of this question before, but a key thing that demotivates me from using a tool like this is that it looks like you're _supposed_ to review every version of a crate. In particular, I'm looking at these docs: https://github.com/dpc/crev/wiki/cargo-trust:-Concept So basically, imagine that I'm totally sold on the idea. I really want to be able to run `cargo trust verify` and have my full set of transitive dependencies trusted. Let's say I'm so motivated that I even go through and do all the code review necessary to make that happen. It'd be a lot of work, sure, but it would be a valuable thing to do. And others could benefit from it. Now let's say I go do a `cargo update` on my application. If my understanding is correct, then `cargo trust verify` will now show potentially untrusted dependencies, either because new ones were introduced or simply because a patch release was made. I now need to go track down the diffs between what I was using and what I've updated to. Speaking as someone who _does_ actually do this from time to time (for example, when someone [deep in my dependency graph brings in two very heavyweight crates](https://github.com/dguo/strsim-rs/commit/d6717dbac880e9f311b9931525285b5b279663ee#r33186236)), I can say that this is a ton of work. For most version bumps (e.g., a patch version bump in serde), I don't look at diffs. But if something strange happens---like the addition of dependencies---then I might go check it out. But I don't necessarily do a thorough review. So basically what I'm trying to say is that in order to maintain that dopamine hit every time you look at the output of `cargo trust verify`, you need to do a potentially crap ton of work every time you run `cargo update`. It's so much work that I can easily imagine myself skipping it, and letting the "trusted" status of my dependencies slowly fade away, thus seemingly negating the use of `crev` in the first place. Do you have a solution to this? To me, it feels like it really cuts deeply into the way the tool is supposed to work in the first place. But I haven't actually used it yet, so maybe I'm misunderstanding something. It would be great for a more detailed account of a typical `crev` user's flow in all aspects of dependency management to be included in the docs. --- Separate from that, I really think that alternative solutions to _trust_ should be explored. A significant component for me, in evaluating the trust of another crate, is simply: _who_ else is using it? For example, the other day, at work, we were talking about potentially switching to [json-iterator for fast JSON deserialization in Go](https://github.com/json-iterator/go). _But_, it uses a truly [gratuitous amount of `unsafe`](https://github.com/json-iterator/go/search?q=unsafe&amp;unscoped_q=unsafe) to seemingly achieve performance. Now, there's no way I'm going to go through and verify all that code myself thoroughly. Not just because I don't want to spend the time to do it, but because I am probably incapable of doing so (being human sucks sometimes). Given the fact that this `unsafe` code is pervasive and sits at the boundaries of our system, this would normally put my trust level in such a dependency to near zero, and thus not want to use it. *But!* It turns out that [Kubernetes uses it]https://github.com/kubernetes/kubernetes/search?q=json-iterator&amp;unscoped_q=json-iterator), and Kubernetes is _ridiculously_ popular. If they use it, my trust level in `json-iterator` goes up substantially, because I generally don't have a problem hitching my wagon to Kubernetes. Is it enough to actually use it? So far, no, but it's now in the "maybe" pile instead of "the no way" pile. I apply a similar calculus in the Rust ecosystem to dependencies I use. It's much more economical than something like `crev`. There are of course some really fundamentally philosophical differences here. Does trust come from code review, or does it come from use? My opinion is that the real answer is probably somewhere in the middle (which is why I do occasionally code review my dependencies, especially when they aren't widely used), and it doesn't seem like `crev` really accounts for that. It makes it a tough sell.
Alternative video link: [https://vimeo.com/332649771](https://vimeo.com/332649771)
how does rust-decimal compare to decimal (decNumber) and rug (MPFR)?
This is why this thread grabbed my attention.
charogne.
You would think that the [`Auto-hide item methods' documentation setting`](https://doc.rust-lang.org/settings.html) would do the trick, but I can't get it to work :/
Thank you for the thorough explanation, so my `trait GetPath&lt;'a&gt;` told the compiler that I'd like to forego elision there, but then I did not provide my own correct lifetime when I used it, then the compiler *did* elide something, but that did not match. I'll have some reading to do, maybe the rustlings exercises have something about lifetime. Being able to completely disable lifetime elision of rustc for learning purposes would be helpful, but unfortunately that does not seem possible.
Ah yes you are very correct. Rust’s runtime also lies within the compiled binary, right?
One of the biggest problems, in my opinion, is the huge amount of duplication on the traits list. Every implementation of a trait has all the associated types and functions displayed. If the docs were changed to only list those types and functions once, it would cut down quite a bit of data. I had a concept for how to address this a while back, but I never had the time to properly finish it and have since lost the implementation. [This](https://i.imgur.com/LQVP7DU.png) is what the standard docs looked like at the time, and [this](https://i.imgur.com/gNc3Qht.png) is the concept I had.
One of the creators expressed this very well in Twitter. Unfortunately I'm unable to find the tweet right now so I'll paraphrase: &amp;#x200B; The major difference between Diesel and ORMs is that ORMs map form **tables** to **objects** that are *associated* with them, whereas Diesel maps from **queries** to **structs** that are *compatible* with them. This means that you can have an arbitrary query and still be able to map from it, and you can have multiple different structs that you are able to map into. So it's a query builder and mapper, but it doesn't bother with rigid object associations which makes it more flexible.
Trying to understand moving and borrowing a bit better. I thought I understood it, but I guess I don't. Say I have the following struct and function: struct BigContainer { first_vec: Vec&lt;i32&gt;, second_vec: Vec&lt;i32&gt;, third_vec: Vec&lt;i32&gt;, } fn change_two_vecs(v0: &amp;mut Vec&lt;i32&gt;, v1: &amp;mut Vec&lt;i32&gt;) { ... } Then say I have a `let mut bc = BigContainer { ... }` that I'm working with, and now I want to use the function. I can write `change_two_vecs(&amp;mut bc.second_vec, &amp;mut bc.third_vec)`. This works. Why!? I would've thought I'd get an error along the lines of "cannot mutably borrow bc for the 2nd argument because you mutably borrowed bc for the 1st already". I'm actually trying to duplicate a toy scenario that I ran into trying to make a game, but it's on a different computer, and I can't exactly remember the situation that is breaking. It's clearly not this, haha, but I'd like to understand this anyway/as well. Thanks!
Just before that: `COPY Cargo.toml .` `COPY Cargo.lock .` `RUN mkdir src` `RUN echo "fn main() {}" &gt; src/main.rs` `[add build dependencies...]` `RUN cargo build --release` Docker won't do these steps unless Cargo.toml or Cargo.lock has changed, which is cool. I never tried to compile with mysql so I don't know sorry :/ but it works well with postgres.
I love this! Thank you.
Rust is smart enough to understand that you are borrowing non-overlapping parts of a struct, so it permits both.
not that much 3d games - any idea why?
How to specify different RUSTFLAGS for debug and release builds in Cargo manifest or config?
Absolute Madlad
What about setting `max_fps` to a lower number and disable `lazy`?
Interledger is a mutual credit system between connected pairs of nodes, and the routes that may be constructed via these connections. It does not require fulfilling dept, though to minimize risk and enable exceeding the predetermined dept limit, it should be fulfilled. Otherwise if you hit the limit and don't fulfill the dept, you've blocked the unidirectional path. I don't see Offst solving this issue, it just assumes an unblocked path exists. Interledger "connectors" are Offst "relays". Neither is more centralized than the other per the protocol. This is a question only of the deployment of the network itself. You say offst uses the social graph, whereas Interledger would be obviously deployed between the big entities (such as banks), but that doesn't forbid any smaller entities (like yourself) from running your own. Regarding asymmetric cryptography, I meant in the protocol itself. The usage of TLS isn't strict. What is strict is that the nodes trust whatever communication channel they use. This may be TLS, or Host Identity Protocol (HIPv2) or a tor onion service, for all that it matters. I agree, TLS using DNS and CAs is poor, but Interledger doesn't require it. IMHO both Offst and Interledger should use SPAKE2+Argon2i (or similar protocols) to securely authenticate the first handshake between the two parties, using a secret passphrase that does not need to be used ever again after pairing and may become public information afterwards. A random 6-word phrase using https://eff.org/dice would be sufficient. Asymmetric cryptography may be used at these higher levels to authenticate the endpoints WHEN you need to use an untrusted network. The following is off the top of my head.. don't use it directly, use HTLA's specification. A -&gt; B (1 credit) -&gt; C (1 credit) -&gt; D: 50 credits D: let preimage = secret chosen by D (may be known to A) D: let commitment = H(preimage) A&lt;-D: commitment A-&gt;B: commitment, ab=commit(A-&gt;B: 52 credits, expire 4 minutes), (D via C) B-&gt;C: commitment, bc=commit(B-&gt;C: 51 credits, expire 2 minutes), (D) C-&gt;D: commitment, cd=commit(C-&gt;D: 50 credits, expire 1 minute) C&lt;-D: preimage =&gt; cd.apply(preimage) B&lt;-C: preimage =&gt; bc.apply(preimage) A&lt;-B: preimage =&gt; ab.apply(preimage) A&lt;-B: ; Alice receives a message from Bob. A-&gt;B: ; Alice sends Bob a message. x, y, z ; messages sent. x =&gt; y ; x implies y The protocol boils down to 3 independent agreements: 1. Alice commits to pay Bob 52 credits, if he can produce the preimage within 4 minutes. 2. Bob commits to pay Carol 51 credits, if she can produce the preimage within 2 minutes. 3. Carol commits to pay Dave 50 credits, if he can produce the preimage within 1 minute. Carol may misbehave by forwarding the preimage without acknowledging to Dave, that she received the preimage on time; then later claim Dave was too slow. Bob does not care if Carol and Dave are having relationship issues, actually he can't even know. Bob only cares that if Carol can produce the preimage on time, that he must pay Carol. In this event, Dave was "paid" as far as Alice and Bob are concerned. It is up to Dave to hold Carol honest, to take her to court, or to just terminate his relationship with Carol at the loss of 50 credits (to prevent future losses). If we move the failure from Carol, to Bob. Then Dave is always paid, but the risk moves to Carol. Finally, if we move the failure to Alice, the risk moves to Bob. For each pair of nodes, the receiver trusts the sender up to the predetermined max dept. This max dept should start low while two parties establish trust and may grow to high throughput only if the receiver is willing to absorb the risk of the sender failing to fulfill their obligations. The receiver may trust a third party (just another hop along the chain), that is more willing and capable of absorbing the risk. If the third party uses a consensus protocol such as HoneyBadgerBFT, which hides the transactions until it has already reached consensus, through threshold decryption; the sender cannot learn the preimage until they've successfully paid the receiver. However, this isn't perfect either because the timeout is relative to crystal clocks, not consensus protocol rounds; it is possible for the receiver to be paid after the expiration should have cancelled the transaction. I.e. a BTC block should take 10 minutes on average, but it might take 15 or 20 minutes.
Yep, Clion is better, plus now with perf and other profiling tools
You can try practise questions [link](https://github.com/sn99/rust-practise-questions)
It might be that that line predates NLL?
 ...- . .-. -.-- -.-. --- --- .-.. -.-.--
Why all the popping? let len = input.trim_end_matches(&amp;['\r', '\n'][..]).len(); input.truncate(len);
Translated text: verycool! --- ^(I am a bot created by /u/zero-nothing. Please PM him if I'm doing anything stupid! Reply to a comment with '/u/morse-bot' to call me and I will translate the comment you replied to from morse-to-text or vice versa!)
My personal preference is to trim the string (assuming that whitespace on either side is irrelevant). https://doc.rust-lang.org/std/string/struct.String.html#method.trim There are some swings and round abouts here though, the big one being it doesn't return an owned String, nor modify the original. Instead you get an &amp;str back. This might be ok depending on scoping and what it's being used for. If you must have an owned String you can create a new one from it. Obviously this has a teeny tiny cost in speed and memory but I assume that it being based on user entry from the terminal performance doesn't need to be blistering. 😉
Thanks for this one! Just added this variant to the article.
I believe it means within any set of curly braces. Since that would be the bounds of a function or code block.
Where is nom's `&gt;&gt;` operator documented ?
To my experience Rust plugin works better than rls + vs code. So not sure what triggered your reaction.
 Really waiting for the rendy push. [https://github.com/amethyst/amethyst/tree/rendy](https://github.com/amethyst/amethyst/tree/rendy) supposed to be done in the next few days, and then plans to add a full 3d game example over summer so it's known how to use it.
that's great news! i have problems with currebt flat 3d renderer so i hope rendy may solve some problems &lt;3
`&gt;&gt;` is just the bitshift operator, but at least with nom 4 the `&gt;&gt;` operator was only available in macros (`do_parse!`), so it's not necessarily a real operator.
Would love to see an interface (or variant) which didn't convert to float internally - something where you could pass a number in cents and have it formatted as dollars. Dealing with currency in floating point is a recipe for problems
Newbie here. Is there any rust equivalent to the Scala project [cats](https://typelevel.org/cats/)?
That bee game looks fun! Great job everyone :)
Thanks for sharing. I like the launcher idea personally, it keeps things simple. I think changing the graphics backend at runtime does not bring enough value to the end-user to justify the complexity.
Thanks. I knew about the shift and couldn't make sense of it. Didn't occure to me it could be macro specific.
Thanks for sending the explanation about HTLA! It looks like very similar to what I remember. I learned a lot from your last two messages. &gt;Quick note on Offst's signatures. If Carol commits to send Dave the credits only if he signs the receipt, then you've got exactly a signature version of the hash commitment I agree on this one. A friend once mentioned that he has seen a protocol that uses a hash commitment instead of a signature there. I thought for a while about how to do this, but I couldn't find a reasonable way. What do you think? I will be happy to get rid of one more signature here. \&gt; Interledger "connectors" are Offst "relays" I think I do not agree with you here. On a LAN setting, Offst can work without relays at all. Relays don't really add anything to the underlying transactions protocol. From what I understand in Interledger Connectors do play a key in the transactions. I even think that they make money from forwarding the transaction. &gt;Otherwise if you hit the limit and don't fulfill the dept, you've blocked the unidirectional path. I don't see Offst solving this issue This was made by deliberate design. When you hit the limit it is equivalent to having no more money. You shouldn't be able to send funds if you don't have money. &amp;#x200B; &gt;Bob commits to pay Carol 51 credits, if she can produce the preimage within 2 minutes I tried to design some kind of timeout like you mention for a while, but I was never happy with the results. You can see some of it in the legacy notes (They are not on the main documentation website): [https://github.com/freedomlayer/offst/blob/15262d24cc8b686e7062a8e1576b90c91e2fa73b/doc/docs/legacy\_ideas.md#the-idea-of-time-limiting-requests-abandoned](https://github.com/freedomlayer/offst/blob/15262d24cc8b686e7062a8e1576b90c91e2fa73b/doc/docs/legacy_ideas.md#the-idea-of-time-limiting-requests-abandoned) &amp;#x200B; &gt;whereas Interledger would be obviously deployed between the big entities (such as banks) I can't see a reason why you won't be able to do something like this with Offst. Banks can add each other as friends with certain credit limits. &amp;#x200B; I have another question: Would you use a system like Offst? If not, what do you think is currently missing (Besides a nice interface, which is obviously missing). You have some really cool ideas, would you like to help out?
(I edited my message while you were writing your response)
Ah thanks, this makes sense! I guess in my game there must be some overlapping I need to address.
A couple of suggestions: 1. The docs state that −2^(96) ≤ *m* ≤ 2^(96), but that should be −2^(96) &lt; *m* &lt; 2^(96) because a 96-bit unsigned number cannot actually hold 2^(96). 2. After a *very brief* look at the code for the constructors, I saw that in `Decimal::new`, when *num* is a negative 64-bit number, you are using `num.abs() as u64`. This would panic if *num* = −2^(63), as −2^(63) ≤ `i64` &lt; 2^(63); specifically `i64` can be −2^(63) but cannot be +2^(63). Better would be `num.wrapping_abs() as u64`. Or even better, since you already know that *num* is negative: `num.wrapping_neg() as u64`.
Is `cargo bench` just for libraries? I want to benchmark my binary.
&gt;Offst differs from Interledger at least in two ways I think now I understand that the most important difference is that Offst does not require any ledgers or banking systems in place to work, while Interledger does. &gt;Offst doesn't have timeouts to distinguish temporary network failures and nodes going offline forever This was a deliberate design choice. The end users in Offst are not always connected. For example, a mobile phone can have a very intermittent internet connection. Therefore having a timeout can cause many strange phenomenons, or may allow participants to cheat the system. I wrote about it here: [https://github.com/freedomlayer/offst/blob/15262d24cc8b686e7062a8e1576b90c91e2fa73b/doc/docs/legacy\_ideas.md#the-idea-of-time-limiting-requests-abandoned](https://github.com/freedomlayer/offst/blob/15262d24cc8b686e7062a8e1576b90c91e2fa73b/doc/docs/legacy_ideas.md#the-idea-of-time-limiting-requests-abandoned) Do you know how these kinds of problems solved in Interledger? &gt;AFAICT you should just adopt ILP Nooo, it will be the boring thing to do!
Origin of runtime is "compile-time" system versus "run-time" system, i.e. code/libraries/etc needed to compile the source, versus code/libraries/etc needed to run the binary.
\&gt; It also feels weird defining a struct for querying and another for inserting. I think this makes a lot of sense, and is something I already do in my C# projects. Depending on your table, there could be multiple columns which get default values, or populated from triggers, etc... So the data you're inserting can be quite a bit different than the fully formed row you query later on.
I would like to use a [divisible E-cash](https://eprint.iacr.org/2015/972) backed bank, with additively homomorphic blinded balances of all accounts, to enable all users to verify the bank has not produced any phantom money or stolen from any users. The bank knows the income of each of their subscribers and all taxes may be perfectly automated. The bank only sees you deposit `($5, $7, $10)` and withdraw `($500, $500, $100)`, they do not see with whom you transact; which you may do completely offline. If you withdraw `$500`, you could divide it, offline, into smaller wallets of say `10x $50` and carry around different quantities or backup the data. Alternatively, you may leave the money in the bank account for safe keeping. Each merchant MUST deposit their income before they can re-spend it. If a consumer attempts to spend the same credits twice, they effectively write their name on the money. When the merchant deposits the double-spent money, they receive a proof from the bank that YOU owe the merchant money. The merchant may use this information to forbid future transactions, or to require you pay your dept before continuing with future transactions, or may take you to court to recover the funds. E-cash can scale, at a bank-level and may be online, to enable merchants to immediately reject doubly-spent cash. In order to reach Internet scales, these banks would use Interledger to make transfers between the banks. Using the same customer-merchant anonymity, the user doesn't need to reveal who they are to the bank to transact with the ILP network (unless they try to spend more than they have).
There's a list of frameworks here: https://www.arewewebyet.org/topics/frameworks/ As far as *active* frameworks, the ones I tend to hear the most about are * rocket.rs * actix-web * tide * tower-web * warp rocket.rs is probably the farthest along, features-wise, but currently only works on nightly and therefore is subject to periodic breakages. it is also _not_ async. actix-web is based on the actix actor framework and uses async i/o. as far as I know it's a close second to rocket.rs in terms of features, and it works on stable afaik tide is (iirc) the framework that is being developed by the net-wg, though I'm not sure how active it is tower-web and warp are being developed by authors close to the futures/tokio ecosystem, and last time I knew they had plans on merging the two frameworks, though I don't know if that's still the case, and if so, how far along in the process that is.
Again, no, Interledger doesn't rely on any ledgers existing on top of it. Extra ledgers only enable fulfilling the depts, usually in batches (days, weeks, months, years) to reset the dept back to a net of zero, to unblock further transactions. Not required, but certainly helpful and entirely up to the pair of nodes to manage. &amp;#x200B; That time problem is not resolved by ignoring it. It is resolved by allowing it to happen and absorbing the risk up to the predetermined max dept between the two parties. The fees should be low enough to be accepted by the users, but high enough to account for the potential risks involved due to network partitions and unexpected delays in the network. If you want to send $500 over an unreliable channel, you may decide to send small deposits of say 10x $50. Each transaction risking at most $50. &amp;#x200B; If your configured timeouts result in frequent losses, you should increase the timeouts or prune the unreliable peers.
Feel free to post your actual code and error messages, maybe we can help.
It does, but it’s still accurate. NLL still involves scopes.
actix-web works fine on stable and is actively developed, I'd say its best option
I'm confused - isn't that code safe from even a panic? You're the one panicking on an `Option` outcome, no? Ie, if you handle the `Option`, just like you would with anything else, it's just as safe as everything else no? Also, how is it not following borrow rules - it will only ever have one owner, which is the point of borrow rules no? If there are more than one reference, you'll get `None` and it will not be mutable. I'm new to this method on `Rc`, so pardon me if I'm wrong. It just seems quite meaningfully different than something like `RefCell`.
To add to the answer by u/Dispersia, the current 3D rendering story in Amethyst is pretty basic, and doesn't work really well with AAA kinds of assets as much as we would like it to. Fortunately the new renderer coming in a couple days will unblock many great new features and improvements on 3D.
Wrong sub, you are looking for /r/playrust/ This one is for the [Rust programming language.](https://www.rust-lang.org/)
I should take another look at it -- the last time I did was ~6 months ago, and it was great until I needed a feature that it didn't have.
How do I race two futures to completion?
We (people who make computer things a whole) really need to address the words problem. It's hard enough to explain the above to someone who is a native english speaker, let alone someone who is still learning english. Expecting anyone who's not already proficient to parse the meaning out of context... well that's an even bigger problem. I say this here, because the above comment is a *very good* explanation, and I still can see how confusing it must be to outsiders.
Completely agree. I’m actually trying to work up a series of posts on the etymology of Python to try and chip away at just the tiniest bit of this, as it’s a good entry point language. The more people I try to help learn who aren’t native English speakers the more I see just how much of a wall of seeming nonsense we put in front of them, almost as matter of programming language design.
Yeah, but I can't have a &amp;mut, and then enter a new scope where I create a new &amp;mut (can I?)
This can often be simplified with \`impl Trait\`. However, this feature is relatively new, so it isn't used much in the standard library. With \`impl Trait\`, the above function would have the signature pub fn create(path: impl AsRef&lt;Path&gt;) -&gt; io::Result&lt;File&gt;
Cool thanks, what a great community. It's on a different computer, so I'll revisit when I get home. If what I've learned so far doesn't give me enough guidance already, I definitely will.
Oh cool. I know a few folks who teach python to non-traditional programmers (that is people who use programming as a tool, not who do programming as a living, think librarians and data scientists for examples). If you'd like input from them, dm me and I'll see if they are interested as well!
Using it with phpstrom :) (I know it works well with any jetbrains IDE but it still is better on Clion due to debugger and profiler support)
The rust plugin is the same for all intellij products, and CLion has debugger integration, so there is no reason to take Phpstorm over Clion or Idea community
Well, employer buys me the license. And since Rust is my offtime undertaking, I havent considered buying one for clion myself.
But in general they have not, if there *is* documentation for a specific instance then you can still indicate that and allow it to be viewed.
Not if the new scope is within the current scope. On the other hand, you can easily have an impl with all the methods taking/returning &amp;mut self.
I don't think it related, clear your cache.
Does this [example](https://play.rust-lang.org/?version=stable&amp;mode=debug&amp;edition=2018&amp;gist=d319c9ac2c23c22d39be3e2042efb9ee) cover off your question?
That's a fair point.
You can "reborrow" from one `&amp;mut` to create another, but the first one becomes inaccessible while the second one is live. Reborrowing is not really covered in the main documentation yet, but the [stacked borrows](https://www.ralfj.de/blog/2018/08/07/stacked-borrows.html) model describes it in formal terms, and I tried to give a brief informal explanation in [Rust: A Unique Perspective](https://limpet.net/mbrubeck/2019/02/07/rust-a-unique-perspective.html).
You still need a `Box` if you want an owned value -- but yes, one can simply use references as you demonstrate. :)
...okay?
I'm assuming you are talking about the game. This is not that subreddit.
actix-web is close is working through beta right now for a 1.0 release as well so I think that is a point in its favor.
Yes go to /r/playrust.
I think OP's concern is slightly different. Here, they're fundamentally frustrated about the naming of the parameter in `Path::new` ([source](https://doc.rust-lang.org/src/std/path.rs.html#1798-1800)), which is just `s` instead of `String`. As a new learner, I can understand, but I think it will get better over time, as [the docs are excellent](https://doc.rust-lang.org/std/path/struct.Path.html#method.new).
What you're experiencing is ambiguity, and it's a common cost of writing &amp; using generic algorithms. It's like the author is asking you to pick a number between 1 and 10, anticipating that someone *else* might reply "1 over 8π + 2.1", and the author has to respond "technically, that's allowed" even though it is not what you would expect (an integer.) *Ideally*, you wouldn't see things like `P` and `S` here, because they would be given descriptive names instead. But descriptive names are conventionally given to concrete types and traits, and doing the same for parameters would be very confusing without some software to analyze and describe them as such. With that said, with a little experience, you shouldn't have much trouble understanding the std library, because it is well documented and these parameters tend to be the obvious thing. The real problem is when you start using some rando library and they don't bother to document these traits &amp; parameters. (But you probably shouldn't be using a library that immature anyway...) &gt;Does it cause any friction in your daily basis? Yes, but not in and of itself; Any code that you use which is unfamiliar to you will cause friction. It is unfair to blame others for your own lack of familiarity. You can maybe blame them for making it difficult to get familiar (but I don't think that's something true of `std`.) &gt;Does it mean that for Rust, library makers have to make an additional effort to provide examples? It means that library makers have to document their libraries and teach them. Though this would be true regardless of whether generics are involved, because very little software is actually neat, clean, and obvious.
&gt; Do you have a solution to this? It is really hard to even get to 100% trust coverage, unless you really keep your dependencies to the bare minimum. For any larger scope it is pretty much impossible to actually do all the reviews. To simplify: if my crate depends on 100 other crates, that means (very roughly) that when I added N lines of code, other developers are potentially adding 100N lines in total in my dependencies. That is why the web of trust part is so important. The hope is that when you do `cargo update` some people that you've already added to your WoT, have did their reviews of new versions, and you might be left with just a few that you have to redo yourself. It also make me think, that I could add some form of `cargo crev diff`, which would print a diff between last trusted version of a crate (or reviewed yourself) and current version. &gt; Does trust come from code review, or does it come from use? [as recent events show](https://snyk.io/blog/malicious-code-found-in-npm-package-event-stream/) one can have millions of downloads, and yet noone will look to see what are they execution. &gt; which is why I do occasionally code review my dependencies, especially when they aren't widely used This exactly what I am hoping for! I don't want you to review everything and aim at "100% green". I want you to first: have an easy way to identify what are you using (`cargo crev verify deps`): see how much lines, how much unsafe, how many downloads etc. they have and pick up things that seem most risky. And then if you do already review crates, it doesn't cost you much to just `cargo review somecrate &amp;&amp; cargo publish` to record that fact. Since you're so popular and reputable member of the community, a lot of people would easily trust your judgment, add you to their WoT, and soon see that they can themselves focus on reviewing some other "weakest link" dependency. Since you're already doing that work, `crev` will just magnify the benefits by sharing them with the community.
Judging by README https://github.com/RustSec/cargo-audit just checks `Cargo.toml` against a db of known security vuln. The newest `cargo crev` release includes that functionality, and then contains much more stuff.
OP, my favorite two languages at the moment are Kotlin and Rust, which is where I'm coming from in this answer. Have you ever written C or C++? If not, then as a heads up, the model you have in your head from Java/Kotlin likely will frustrate you in Rust for a while. What's surprisingly worse is that Rust and Java/Kotlin do have quite a few similarities, which actually makes things feel that much harder to deal with when they are different. However, stick with it and you will overcome the obstacles. With Java/Kotlin I often think in interfaces, and everything is a reference; with Rust, I juggle thinking about traits and reified types, and how both approaches affect API considerations and performance. At this point, I have no issues with functions behaving differently depending on type. I am equally fine with `let result: Vec&lt;T&gt; = list.(transformations).collect()` and `val result = list.(transformations).toList()`
Wrong subreddit, /r/playrust
This is the subreddit for a programming language called Rust, you want https://www.reddit.com/r/playrust/.
Also, if you're not using the [Quick Doc](https://www.jetbrains.com/help/idea/documentation-tool-window.html) feature in IntelliJ, start! It's amazing.
Wouldn't you say it's a bit ambiguous now, since a variable could still be considered "in scope" when using it would cause a lifetime error?
Slightly offtopic, but is `crev` pronounced "see rev~iew~", or like the Polish word "krew"?
Thanks. Do you know of any existing code (not necessarily related to Markdown nor pulldown-cmark) that applies this strategy ?
Lmao im such an idiot
Need help
There kind of cannot be an equivalent yet. Cats relies heavily on higher-kinded types and Rust does not have those. There is some work being done in the area in the form of "generic associated types" ([tracking issue](https://github.com/rust-lang/rust/issues/44265)). It seems to be waiting on the Chalk project ([tracking issue](https://github.com/rust-lang/rust/issues/48049). GAT is on the roadmap for the year but I don't think that's necessarily a commitment to ship the feature. I had loose plans to make such a Rust+Cats crate whenever Rust got GAT. I was going to call it "rats" but somebody already took that name for their own FP library. I'd be pretty amused if it turns into the same thing I was thinking.
Full Office for Windows is a desktop app has always been shipped as a key-activated desktop app. I'd imagine it'd not only be a bunch of work to port it over (it hooks into the system so the UWP-&gt;Win32 sandbox-wrapper is probably a no-go) but to also get people to understand it's in the Windows Store. Then there's also the Windows key-activation system &amp; business with their volume/kms/active domain licensing for Office keys, I'm not sure that'd work in UWP. Anyway, they do actually have Office in the Windows Store, but they're really close to the mobile apps (basically native Office Online) instead of being full versions.
Since it's been mentioned a couple of days ago, I thought maybe he's talking about `Iterator`'s doc page.
Good bot!
&gt; my `trait GetPath&lt;'a&gt;` told the compiler that I'd like to forego elision there, Not quite. `trait GetPath&lt;'a&gt;` declares a lifetime parameter for the trait. That parameter may still be elided at any call site. The parameter must be resolved at any call site but "elision" just means "resolved by the compiler" instead of "explicitly stated". If the compiler can resolve the lifetimes you do not state, everything is fine regardless of the signature. The trouble is that `fn path(&amp;'a self) -&gt; &amp;'a Path` uses the same lifetime as in the trait declaration. If you leave out the lifetimes here with `fn path(&amp;self) -&gt; &amp;Path`, then things will resolve again just fine and the lifetimes can be elided at the usage site. This is because the new signature is the same as `fn path&lt;'b&gt;(&amp;'b self) -&gt; &amp;'b Path`, using a different lifetime than the trait parameter. With the first definition, elision fails because `&amp;GetPath` uses two different lifetimes (as described before) while they must be the same. With the second definition, elision succeeds because there are two different lifetimes in play and two different ones are allowed.
That’s lexical scope, which, since it uses the adjective lexical, is not the only kind of scope :)
What was that last one? The hex-tile one?
What are your thoughts on [IEEE 754-2008](https://en.wikipedia.org/wiki/Decimal_floating_point#IEEE_754-2008_encoding)? I don't know why, but somehow I have weird love for this particular standard, and I've been looking for pure Rust crate implementing it, only to find none (there is one crate, I think [decimal](https://crates.io/crates/decimal), that wraps IBM library, but I also have a very strong RIIR attitude :D, after dealing with C FFI and fallouts caused by libraries wrapped in haste).
Again. I get phpstorm for free :D
I found myself using a postgres library and liking it, it's good for now. I've found both SQLAlchemy and Django "good enough", like any abstraction it comes at a cost, but the benefits of quicker onboarding and development are significant. So some day I would love to see an SQLAlchemy like project for Rust.
Diesel seems to add complexity rather than take it away - and it comes at a performance cost. If I'm going to lose performance, I want the complexity of the code to decrease. So for now I'm staying away.
Thinking about things in this way could be useful, but personally I prefer being able to define all of that in one place. It's something that both SQLAlchemy and Django's ORM do very well. It reduces the amount of boilerplate you need to write.
Yea I dunno, I applied to Buoyant a while ago and didn't even hear anything back, not even a 'no thanks'.
I don't know. I tend to use "crew", though I think "krew" is probably more correct. "see-rev" also makes sense. I hope someone tells me which one is best. :D
Thanks for another grate post! I sincerely hope you continue to blog about the progress! &amp;#x200B; I will need to reread this, I think we can some of the error reporting strategies to improve the messages that Cargos resolver makes. When the it is on [crates.io](https://crates.io) I definitely want to try using it as an oracle for the tests!
If there's any process still on the run queue then the CPU can't enter lower power states. If your process is the last one running and then it goes to sleep, the CPU will likely enter a quiescent state. There's usually some latency to exiting these states. Depending on how deep it went down it may take differing amounts of recovery time. Your question doesn't include even rough orders of magnitude of the latency so it's tough to be sure whether this is the issue or something else. Paging is another big potential source of latency. `latencytop` is designed to help find issues like this IIRC.
Thanks for your reply, it was insightful and productive.
except Mac App store is just an easy market place and Windows store has a whole different API and in the case of Office, a whole different language (since office is written in C++). You're dumb to base your opinion on this.
remote. so far we have a few folks in emea and latam
apologies for that. definitely no excuse. but we have a recruiter now (me :) so shouldn’t happen again! shoot me a dm if you’d like me to check on your application.
I’ll decline, thanks.
You can have one struct for both. You just have to know the primary key and every field you want entered into the database. Alternatively you can define only one struct and insert a tuple of values that matches a defined subset of fields. Crates.io uses this method. Diesel literally (using the schema.rs mapping it generates) looks at the fields on a struct and writes the equivalent of: INSERT INTO self.table (self.field1, self.field2) VALUES ($1, $2); How would the code work if you only had field2?
Does it come at a performance cost? Can you elaborate a bit on that?
Seems odd a common font would have the wrong rendering for an ampersand. It doesn't loop at the top.