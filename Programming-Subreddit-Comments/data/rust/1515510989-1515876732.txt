As a C++ dev, these are my top two "I can't use Rust because of X": * long compile times (especially incremental compilation and parallelization are much faster with C++. And C++ is already bad) * immature tooling (libclang already enables [compiler backed code completion](https://github.com/rust-lang-nursery/rls/issues/6) for C++)
What do you mean when you say full-featured? I'm looking for specific use cases here, otherwise it will be hard to communicate what exactly is missing!
I only discovered it today from [this thread](https://users.rust-lang.org/t/embedding-a-web-view/14860), want to raise awareness.. Did you ever want to write a standalone Desktop app in Rust, with the frontend in Elm or PureScript?
&gt; long compile times (especially incremental compilation and parallelization are much faster with C++. And C++ is already bad) I hear this a lot but in my experience Rust compiles much much faster than C++. How many TUs are you using in your C++ code vs how many crates are you using in your Rust code? In particular, I work a lot on header-only projects (modifying them so pch are not an option) like Boost. For example, range-v3 and Boost.Range v2 are size-wise and functionality-wise comparable to the iterators in std. Compiling `liballoc` and its tests (which do not only contain the iterators but also the collections) takes a fraction of the time it takes to compile range-v3 and all its tests. In particular, modifying range-v3 and recompiling takes as much time as an initial compilation, while for `liballoc` it does not when incremental compilation is enabled. Sure if you are comparing a single monolithic rust crate vs a spliced C++ project with lots of TUs, for the same number of lines of code, C++ will compile much faster. But that's an apples to orange comparison. To make it fair you would need the same number of crates as of TUs, which you could achieve either by splicing the rust crate into many sub crates, or by making the C++ project a header only project with a single TU. Still, this doesn't mean that we shouldn't improve compile-times further (those coming from dynamic languages are used to zero-second compiles).
&gt; monads, result sum-types This ought to be a redundancy.
Do you have a badge that people can put at the top of their GitHub readme's? Like one of those TravisCI badges but it links to audits. 
On the feature side, I'm thinking about DTD and XPath especially. Also, being fast would be nice. I've been using xml-rs last year, and it was *sloooooooooooooooooow*. My dream XML library for Rust would be something akin to Diesel, which gives you a statically-typed view of *your* XML Document, inferred from the DTD with some kind of `infer_schema!` macro. But I don't even know if it's possible (it should be possible since Rust type system is Turing Complete, but I don't know if it's doable in a convenient way in practice).
Each .cpp file is a TU. &gt; To make it fair you would need the same number of crates as of TUs, which you could achieve either by splicing the rust crate into many sub crates, I wouldn't say that this is fair, since "creating a new cpp file" is A LOT easier than creating (and using) a subcrate. &gt; or by making the C++ project a header only project with a single TU. I'd also say that this isn't fair, as C++ projects with only one TU are very rare while many Rust projects consist of only one crate. I could rephrase my point: Too hard and cumbersome to create a new TU compared to C++.
Thanks! I guess I was considering the lifetime parameter to mean "bounded by the lifetime 'a", where it is really just "this explicitly is the lifetime 'a." Makes sense.
What is the macro story in unsafe Rust? Is is it the same as Rust? Could one implement many of these ergonomic updates using Macros?
Thanks! And yes, NLL will make things like that implicit and probably allow more than what the code really means. Maybe that can be linted for, or even just highlighted with lifetime markings in IDEs to help programmers catch errors like that. Or newbies like me can just learn how to properly code rust.
&gt; What is the macro story in unsafe Rust? Is is it the same as Rust? Yes. `unsafe` lets you do things you can't do in safe rust (deref raw pointers, call unsafe functions, access mutable statics and implement unsafe traits), AFAIK it does not alter the langage, "safe rust" should behave the same in or out of an unsafe block. 
Hmm, you know I haven't thought about doing that actually... I've used macros that generate unsafe code (see the [ones here](https://github.com/swaywm/wlroots-rs/blob/master/src/macros.rs). The main ones are making it easy to do literal C-style strings (`c_str!("some string")`), making an equivilent to the wayland `container_of` macro (super unsafe) and then a really complicated one [here](https://github.com/swaywm/wlroots-rs/blob/master/src/macros.rs#L127) that generates a lot of boiler plate). The main things I really want though are unsafe specifications and diagnostics (which aren't do-able with macros). If it's possible to ergonomically have a dereference operator with macros 2.0 though, someone please tell me because that would make it at least more readable.
thx
So that was really instructive, thanks again! A quick summary for anyone who stumbles across this thread (feel free to correct me if I'm wrong!): 1. On the Rust side, every time an error occurs, information about it is stored in [thread-local RefCells LAST_ERROR and LAST_BACKTRACE](https://github.com/getsentry/symbolic/blob/10d3f310570c7fed2d18010cbb406b179526319f/cabi/src/utils.rs#L10). 2. This information is accessible from Python via FFI functions like [symbolic_err_get_last_code()](https://github.com/getsentry/symbolic/blob/10d3f310570c7fed2d18010cbb406b179526319f/cabi/src/core.rs#L154) or [symbolic_err_get_backtrace()](https://github.com/getsentry/symbolic/blob/10d3f310570c7fed2d18010cbb406b179526319f/cabi/src/core.rs#L189). 3. On the Python side, a wrapper function [rustcall()](https://github.com/getsentry/symbolic/blob/10d3f310570c7fed2d18010cbb406b179526319f/py/symbolic/utils.py#L74) makes sure calling FFI functions and handling their potential errors is consistent. It starts by clearing any previous error, then calls the function, then checks whether an error has been set. If not, it just returns the result. Otherwise, it builds a Python exception using information about the original Rust error extracted via the FFI and raises it. 
I currently develop web apps in Python. I long for the day when something like Rocket+Diesel reaches a level of maturity where it can supplant Django for my uses. It's not (primarily) about performance... it's about having a powerful type system and monadic error handling to catch as many mistakes as possible at compile time so I don't have to reinvent Rust's type system (badly) in my test suite.
Got it working on fedora. Here's the Dockerfile: FROM fedora:latest RUN dnf install -y mingw64-gcc mingw64-freetype mingw64-cairo mingw64-harfbuzz mingw64-pango mingw64-poppler mingw64-gtk3 mingw64-winpthreads-static gcc RUN curl https://sh.rustup.rs -sSf | sh -s -- -y RUN echo -e '[target.x86_64-pc-windows-gnu]\nlinker = "x86_64-w64-mingw32-gcc"\nar = "x86_64-w64-mingw32-gcc-ar"' &gt; /root/.cargo/config &amp;&amp; \ echo export PKG_CONFIG_PATH=/usr/x86_64-w64-mingw32/sys-root/mingw/lib/pkgconfig &gt;&gt; /root/.cargo/env &amp;&amp; \ echo export PKG_CONFIG_ALLOW_CROSS=1 &gt;&gt; /root/.cargo/env &amp;&amp; \ . /root/.cargo/env &amp;&amp; \ rustup install nightly &amp;&amp; \ rustup default nightly &amp;&amp; \ rustup target add x86_64-pc-windows-gnu &amp;&amp; \ echo ". /root/.cargo/env" &gt;&gt; /root/.bashrc then run: docker build -t cross . docker run -v/Path/To/Project:/code -i -t cross /bin/bash cd /code cargo build --target=x86_64-pc-windows-gnu --release It's using nightly by default, you can change it if you want. edit: I'm pushing the image now, it's called rust-crosscompile
There are plenty of other languages with more mature type systems and libraries besides Rust, why is Rust the one you want? AFAICT Rusts biggest advantage is the concept of ownership, which is irrelevant if you have a garbage collector.
&gt; I wouldn't say that this is fair, since "creating a new cpp file" is A LOT easier than creating (and using) a subcrate. Creating a sub-crate is as easy as adding a TU to a C++ project (or at least not harder): - to add a sub-crate one just adds a line to Cargo.toml (dependency with path) and an `extern crate ...` to use it. If you have many subcrates you might want to add `[workspace]` to your Cargo.toml to make this even easier but you only have to do this once. - to add a TU to a C++ project one has to add a couple of lines to a CMake file or Makefile and to use it you need to `#include` it or `import` it. If you want to `import` it you also need to modify the modules description file if you want to make the new TU a module. You might be using an IDE that does all of this for you in C++, but there is no reason that a Rust IDE couldn't do this. &gt; I'd also say that this isn't fair, as C++ projects with only one TU are very rare while many Rust projects consist of only one crate. Indeed, but projects with zero TUs are extremely popular: most Boost libraries have zero TUs, the Standard Template Library has zero TUs, Eigen3 has zero TUs, ...; releasing libraries as single headers or header-only libraries is popular. Which is again my point, whether Rust compiles slowly with respect to C++ depends on how you are structuring the Rust and C++ projects that you are using to make the comparison. A monolithic project with one TU is going to compile slower than if the same project was properly decomposes into independent translation units, but this is the case in both Rust and C++. The worst case is C++'s range-v3 test suite: the range-v3 library has zero TUs, but every test in the test suite is a different binary, so the test suite ends up recompiling the whole range-v3 library 200 times, once for each test. There are ways to improve this slightly, but this is still extremely slow. At least a single monolithic Rust crate does not have this issue. Anyways, I agree that Rust compile-times should get better, but I doubt they can be made as fast as if every Rust module would be a TU, at least not for the first compile.
Cretonne is certainly on the table as a future improvement for translation and code generation time.
wow, I didn't even realize they had pricing models. Don't know how I managed to do that.. Using the library I was wondering why I couldn't find anything on the web about how to use sciter (outside of their official website), and why there weren't more people using it for gui's, specially with rust's lack of support in that area.. Guess now I know why. Thank you for the quick answer btw.
&gt; Is this as incremental as it is going to get? No, there are still lots of improvements planned for incremental compilation. We'll enable those as they become available.
So the `-&gt;` deref chain asked for in the article should be implementable as a macro ?
&gt; It seems to me that rustc still spend a lot of time doing llvm stuff even in incremental mode. If a change affects anything in an object file, then the whole object file has to be re-compiled. That means that lots of functions might be re-compiled even though only one function has actually changed. We are exploring ways of making the compiler handle this more intelligently.
&gt; I would wish for one thing: some flag to have CARGO_INCREMENTAL=1 only apply for debug builds Incremental compilation is the default for debug builds right now, so if you don't set the CARGO_INCREMENTAL flag, you should get incremental builds anyway. 
I was thinking more of SQL Server. :-P /u/shadow31 says it's cross-platform now though (and gives another possible explanation of the whole thing)
Let me know if there's still something missing. More than happy to document it. :)
Thanks for the clarification! I haven't used Go, and based that comment on observations from /u/burntsushi and others saying it has a nice interface (e.g it's easy to bind with libs easily). 
From my mostly Python background, I didn't have "TU" in my vernacular. For anyone reading to learn new things and also not immediately understanding "TU": &gt; According to standard C++: &gt; &gt;A translation unit is the basic unit of compilation in C++. It consists of the contents of a single source file, plus the contents of any header files directly or indirectly included by it, minus those lines that were ignored using conditional preprocessing statements. &gt; &gt; A single translation unit can be compiled into an object file, library, or executable program. &gt; &gt; The notion of a translation unit is most often mentioned in the contexts of the One Definition Rule, and templates. From [Wayback Machine Link](http://web.archive.org/web/20090427201018/http://efnetcpp.org/wiki/Translation_unit)
Thanks! I've added a ninja EDIT to the original post that spells TU out, sorry for the confusion it created.
Yes, that example was probably a bit contrived, and practically you might not want to do that exact thing. The point is though that there are signatures like this that can look fishy but are actually very sane. Being warned and forced to provide justification for these fishy things would be helpful to the ecosystem, IMHO. &gt; Personally, using unsafe code means that I'm probably doing something extremely wrong, or that I should use the byteorder crate for transmutations. And that's a good attitude to take! I try to use as little unsafe code as I can too. Unfortunately, there's still cases when you need to use it though (especially in "systems programming") and it's still a (pretty integral) part of the language that hasn't really been touched since 1.0. &gt; For the case of your Wayland graph, I once implemented Graph&lt;V, E&gt; as Map&lt;usize, (V, Map&lt;usize, E&gt;)&gt;. Using a FnvHashMap or even a VecMap reduced the performance penalty to an absolute minimum, while still providing all the functionality of a graph. Additionally you could clone the graph by taking a deep copy, something that is impossible if you let yourself in with unsafe code. I assume you're referring to the tree structure I used in Way Cooler? That unsafe tree I made was back when I didn't know any better, and today I use something like that via petgraph. That was the correct solution for sure. But using a graph library to be a tree is a hack, because not only do you have to write a compatibility layer but there's also performance issues with using a graph as a tree. A library that implements efficient trees (by probably using unsafe) would be much better. My main use case when I'm thinking about this unsafe code is libraries that implement fast data structures or APIs to other libraries. &gt; IMHO one line of unsafe code in a crate that doesn't interface directly with C code is one line too much. If that line can be justified, I don't think it's too much at all. It depends on the use case. Handling e.g SIGTERM in Rust apps still requires unsafe code. &gt; Why and where exactly did you need unsafe code in your cooler? [sic] There's not much unsafe code in Way Cooler. The majority of it is pushed to the framework binding libraries I'm writing. My wrapper for wlroots is what I'm working on currently, and so issues with unsafety is something I'm thinking a lot about. If you want justification for why I want these unsafe features, [just take a look at the wlroots-rs repo](https://github.com/swaywm/wlroots-rs). There's a very complicated memory ownership model going on that I'm having to make safe to use in Rust, and it feels unpleasant because all it takes is one thing for me to forget and I might have to redesign the entire library (in fact the constant churn of wlroots has made this very hard--they changed how a lot of things work and suddenly my design is bad and I have to start over).
I definitely know very little about this. What would you use these for?
You'd have to wrap the entire thing in a macro e.g. `deref!(a-&gt;b-&gt;c)` at which point I don't know how much you gain. Though maybe you could have a module-level attribute and a procedural macro letting you do that for an entire source file? Or maybe an `unsafe!` procedural macro which adds a deref operator? I don't know how flexible procedural macros are.
I'm not on the rust team, but I hear tale that they are really friendly with commercial entities. In the past I believe they've signed NDAs so they could debug compiler issues on private repos. Perhaps there is room for that here? /u/steveklabnik1 ?
Can that be achieved with e.g macros 2.0? I've only used the old macros, so maybe that can be improved. It would certainly make me happy to learn I can fix at least the ergonomics issues myself :)
We've talked about such a thing, but never actually done it.
&gt; For 2018 I propose that there should be a team that should seek out defining, very clearly, at least some of these rules so that we understand what is and isn’t permitted in Unsafe Rust code. Good news! This already exists. Check out RFC 1643 and this section of the rust internals forum: https://internals.rust-lang.org/c/language-design/unsafe-code-guidelines I hope to see a lot more movement on this in 2018. I think RalfJung is carrying the torch on this.
Note: incr. comp is also the reason target/ first are so huge recently, especially if you have many test binaries.
For me, the strange part is the engine, not syntax. It's slow (sad KDE user), resource-heavy and has no native GUI support (even for text rendering).
Yeah, that's fair. Entity doesn't tell you anything about what you can do with it, even though it's the RFC's name for the key abstraction. Maybe I will go with http-servecontent. Thanks for suggesting it. Any other good names out there before I commit?
I didn't do anything special, just followed the instructions.
`Error` is a fancy version of `Box&lt;Fail&gt;` which you can't use in `no_std`. The crate fully works with non-boxed errors.
Making unsafe code easier is a great goal. :D It's currently quite tricky to figure out when you're doing something invalid or not. One thing that I think crates.io *could* do is just show the fraction of unsafe code in a crate, just as a rough percentage. It wouldn't be *precise* or super informative, but it could give a user a general idea of what sort of crate they're looking at.
Does `CARGO_INCREMENTAL=1 cargo +nightly bench` use ThinLTO? Runtime performance for my sudoku library is halved.
&gt; - to add a sub-crate one just adds a line to Cargo.toml (dependency with path) and an `extern crate ...` to use it. If you have many subcrates you might want to add `[workspace]` to your Cargo.toml to make this even easier but you only have to do this once. Wouldn't I also need to create the correct folder structure? And a Cargo.toml for the subcrate? Also what to do when subcrate A depends on subcrate B and vice-versa? What about macros? &gt; - to add a TU to a C++ project one has to add a couple of lines to a CMake file or Makefile and to use it you need to `#include` it or `import` it. Why "a couple of lines"? At most it's 1 line, 0 lines if you're using glob. &gt; If you want to `import` it you also need to modify the modules description file if you want to make the new TU a module. Are you talking about the unfinished modules TS? &gt; releasing libraries as single headers or header-only libraries is popular. You're talking about libraries here, but continue to talk about "projects". I would assume that most C++ projects are programs (afterall someone has to use those libraries). So my point still stands: For the majority of projects C++ is faster.
&gt; does anybody have a clue on how to package the engine with the executable Like any other software which comes with its libraries: tar/zip archive, installer, etc. In this case it is just one library and it doesn't have to be _installed_ system-wide. Personally I don't have any objections to use dynamic library over the static one (Qt, for example, had (has?) similar licensing model for ages), but it's up to you, of course. 
&gt; undefined behavior detection: 100% reliable detection of undefined behavior at run-time Have you seen [this talk by Chandler Carruth](https://www.youtube.com/watch?v=yG1OZ69H_-o)? He makes some very interesting points about the nature of undefined behavior and the limitations of detecting it. I mention this because he thinks that detecting 100% of undefined behavior is absurd/impossible.
To be honest, I *don't* want a full featured XML library. At least not for parsing anything that is potentially user-submitted. I'd rather have locked-down XML library that I can throw at any content without fearing network connections or DOS.
Even though this would not immediately help me in any way, I completely agree that this is the direction Rust should take in 2018. It is a systems programming language, and it should become a first class systems programming language.
You'd include it with whatever way you package your program. Rust doesn't have a specific, official way to package programs, as it depends on the platform specifics.
Actually, it is for me. It's literally the single blocker left. And I know that some usages are heavily dependent on SIMD. This reminds me of a fictional anecdote about Word Processors: &gt; A journalist receives a new Word Processor to review, which claims to have shaved off the fat by only implementing features that were used by at least 80% of the people. To try out, they type their article and are indeed impressed with the functionalities. Ready to submit the article, they try and check how many words/characters are there. They can't. It's not implemented because "nobody" uses it. Frustration, and a scathing review, ensue. The problem with "I only use X% of the features", is that not everyone use the same X% :(
Nice post! Re: &gt; I *really* don’t think we should use ., because a raw pointer is very different from a reference This is from a beginner POV, but I'm not sure why you feel that way. Is there any reason that allowing auto-dereference (for field access) for raw pointers wouldn't work? To me, auto-dereference would make raw pointers fit more closely into the rest of the language. But I'd be okay with "-&gt;" as well. 
Has there been any investigation into compiling dependencies in release mode but compiling the final binary in debug mode? For some projects (games), you pretty much *have* to compile in release mode because it's unusable otherwise, though most of the CPU usage comes from dependencies, so there could be some significant gains in compile times. I don't know what this would look like, but it would be interesting to instruct `cargo` to selectively always compile certain dependencies with release-mode optimizations (perhaps it's already a thing?). That way you could take advantage of incremental compilation while having near-release execution speeds.
If you already have --help output for your program you can use help2man
That's why I put it last. I think we're doing a pretty good job so far, but we are a comparatively small community for now. I also mod /r/java and know the amount of spam and trolls larger communities need to deal with. So if you want to make sure this stays a friendly place, support your mods! Report &amp; downvote violations, try to mediate wherever you find quarrels or even better: Become a mod!
Better support for cross-compiling is a must. [Xargo](https://github.com/japaric/xargo) is amazing, however it would be even better if it were integrated with Cargo. Building a bare-metal program with Clang is as simple as `clang -ffreestanding -target x86_64-elf file.c -o file.o` and then run the output through a linker. With Rust, I need to set up Xargo, [create a target spec file](https://github.com/japaric/rust-cross#target-specification-files), output a static library, link it with some `.c` / `.asm` glue code and then I can link the final file. I think [integrating `lld` into `rustc`](https://github.com/rust-lang/rust/issues/39915) would also be amazing, it would allow Rust to cross-compile without even needing an external linker. Nevertheless, Rust is amazing: you can write low-level code and still use iterators or other high-level features while maintaining a consistent performance.
&gt; Crates.io seems to copy the very controversial NPM security model. Could you expand on this? Also, have you heard about the rust-lang organization on github and its nursery? It's a way to have "official" crates without putting them in `std`.
I wonder if I should add "incremental-recompilation" as flair for you Michael, what do you think?
For sparse matrices, there is [sprs](https://github.com/vbarrielle/sprs). It does have some missing functionality (as far as I can see, there is no vector-of-vectors matrix that would allow efficient insertion), but with some community effort and good `ndarray` interoperability it could be the solution. I haven't personally found the ergonomics to be bad, but stable SIMD would be great.
**SOAP** SOAP (originally Simple Object Access Protocol) is a protocol specification for exchanging structured information in the implementation of web services in computer networks. Its purpose is to induce extensibility, neutrality and independence. It uses XML Information Set for its message format, and relies on application layer protocols, most often Hypertext Transfer Protocol (HTTP) or Simple Mail Transfer Protocol (SMTP), for message negotiation and transmission. SOAP allows processes running on disparate operating systems (such as Windows and Linux) to communicate using Extensible Markup Language (XML). *** **ONVIF** ONVIF (Open Network Video Interface Forum) is a global and open industry forum with the goal of facilitating the development and use of a global open standard for the interface of physical IP-based security products – or, in other words, to create a standard for how IP products within video surveillance and other physical security areas can communicate with each other. ONVIF is an organization started in 2008 by Axis Communications, Bosch Security Systems and Sony. It was officially incorporated as a non-profit, 501(c)6 Delaware corporation on November 25, 2008. ONVIF membership is open to manufacturers, software developers, consultants, system integrators, end users and other interest groups that wish to participate in the activities of ONVIF. The ONVIF specification aims to achieve interoperability between network video products regardless of manufacturer. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Linux and OS-X use LF, Windows use CR LF, so just counting LF works on all platforms for correctly formatted text. *Trivia: old MACs used to use only CR, then it was hell!*
Better ergonomics. Better generics for primitive integers, better generics for primitive floats. Gotta start with the basics. I *wish* I could spend my full time on such a thing.
&gt; Wouldn't I also need to create the correct folder structure? Nope, you just need a new folder for the new crate, but you don't need to add a `src/` dir to it or anything, check out how servo does this, it has a [`components/`](https://github.com/servo/servo/tree/master/components) directory with many sub-crates, most of them are only one `lib.rs` file. &gt; And a Cargo.toml for the subcrate? Yes, this you must do. &gt; Also what to do when subcrate A depends on subcrate B and vice-versa? Rust crates can't have circular dependencies, but Rust modules can. So if you need a circular dependency either refactor your way around it (e.g. adding a third crate) or use modules. C++ modules can't have circular dependencies either, but while C++ TUs do support circular dependencies, their support is limited at best. For example, you can't access the layout of the classes involved in a circular dependency (which might mean that you can't use them in template functions because they must be written in the header files), the program including macros are still parsed serially, etc. &gt; What about macros? You just export them. C++ modules do not support this yet but clang modules do. &gt; Why "a couple of lines"? At most it's 1 line, 0 lines if you're using glob. Big C++ projects aggregate TUs into modules (not C++ modules, but just larger object files, think of it as larger TUs). These modules get linked into larger modules that get linked into larger modules and at the end you just link a few modules into your final binary. Otherwise, your compile times are limited by link time and your build does not scale. This typically requires editing some files stating which TUs belong to which module. C++ modules (Modules TS) requires you to do this as well, a single C++ module file is composed of many TUs, and you need to specify these relationships in the module definition files. &gt; Are you talking about the unfinished modules TS? Yes, I've been using C++ modules (clang modules) for about 6 years now. Is it still not finished? I read somewhere it was already in PDTS. &gt; You're talking about libraries here, but continue to talk about "projects". A test file in a library is just a binary that uses the library. For those who develop the library, the library itself is a project. Even if the library is header only, it still needs to be compiled when working on it, for testing for example, does that make sense? &gt; I would assume that most C++ projects are programs (afterall someone has to use those libraries). So my point still stands: For the majority of projects C++ is faster Nobody really knows what the majority of the C++ projects actually do (not my words). All the projects I work on use header-only libraries extensively (Boost, Eigen, PointCloud, CGAL, range-v3, the STL, etc.). And even using modules and pch's Rust compiles orders of magnitude faster than C++. The following two code snippets summarizes it all. This C++ and Rust programs do the exact same thing using a similar library/API (the C++ program uses a C++11 compatible implementation of the Ranges TS): // C++: #include &lt;iostream&gt; #include &lt;range/v3/all.hpp&gt; int main() { using namespace ranges; auto v = accumulate(view::ints(0, 10) | view::filter([](auto&amp;&amp; i) { return i % 2 == 0; }), 0); std::cout &lt;&lt; v &lt;&lt; std::endl; // prints 20 return 0; } // Rust: fn main() { let v: u32 = (0..10).filter(|x| x % 2 == 0).sum(); println!("{}", v); // prints 20 } These are their compilation times on my machine: * C++ debug (clang-6.0 trunk): 5.6 seconds * C++ release (clang-6.0 trunk, no LTO): 6.2 seconds * cargo build (debug): 1.1 seconds * cargo build (release): 0.8 seconds * cargo build (release-thick-LTO): 3.1 seconds So in this case the compile part of the edit-compile-debug cycle is about 5-6x faster in Rust (I use debug builds here). This correlates with my experience with larger projects (in both Rust and C++, I would have said that Rust compiles ~10x faster, but 5-6x seems feasible). Obviously, not all C++ code looks like this, and if you are writing C with classes then this results won't be representative for your projects. But as mentioned all the C++ projects I work on are build on top of collections of header only libraries (some open source, some internal ones). To develop these projects I evolve the header-only libraries we internally use, which get reused in many other projects. To test those I need to compile binaries that use these libraries and test them. C++ compilation times are slow, very very slow. The Modules TS gave us a best case 20% speed-up but compile-times are still too slow. So in my experience anybody who uses templates extensively (or libraries that do) will find that Rust compiles blazing fast compared to what they are used to. If you are not using templates or header-only libraries extensively in C++ then I don't know how fast Rust compiler is compared to C++ ones. If I remove the range-v3 dependency: #include &lt;iostream&gt; int main() { int v = 0; for (int i = 0; i &lt; 10; ++i) if (i % 2 == 0) { v += i; } std::cout &lt;&lt; v &lt;&lt; std::endl; return 0; } then clang compiles in 1.4 seconds, which is way better than with range-v3. So if you are not using header-only libraries extensively, you are probably not hitting the same problems that we are hitting with C++ build times.
It's more of a way to visually signify that this is different. Eg the could segfault because it can be null. I don't feel too strongly about that though, using `.` would still be an improvement. I'm ok with a new sigil here though because it advertises something different than usual dereference. Others might feel differently though, and I admit I'm not strongly committed to not using `.`
No problem at all. Learning new things is why I always try to read just a little above my level. ;)
Looks like we had the same idea: https://github.com/LeoTindall/rust-mingw64-gtk-docker Thanks!
Agree on the other points, but: &gt; but I still think it's a bad idea to have the compiler version be determined by the project. It would just result in an endless churn of "use new compiler" updates to the project files Wouldn't the version constraint normally be a minimum version, so you don't have to change it unless you use a new compiler feature? An intelligent cargo could use whatever installed compiler best matches the constraint. Also, "use new compiler" releases already exist, they are just mentioned in the readme or release notes since there isn't a place in Cargo.toml (Look at clap-rs releases for one I noticed recently). If you try to compile with the wrong version you get an obscure error. If cargo knew about compiler versions it could just say `ERROR: Toolchain 1.12.0 does not match constraint "&gt;1.17"` or something. 
ohhhh.. Ok, I think I misunderstood it at first. So we are allowed to use sciter's engine, it just means we have to load it at runtime. Yeah, that's fine. The way you phrased your answer makes me think you have used it before, if so, could you tell me how you dealt with making it standalone (or how you would)? If I'm not mistaken, to get sciter to work, I had to add it to my path, would that be something that the installer would need to do?
&gt; language-wise: memory model (C and C++), virtual enums/thin pointers (C++), inline assembly (C and C++), const generics (C++), type-safe variadics (C++), ATCs (C++), specialization (C++), macros 2.0 (C and C++), async/await (C++), existential types (C++), custom DSTs (C), alloca (C), VLAs (C) That's a very long list! I definitely agree with: - ATCs (C++): at least over lifetimes, - const generics (C++): and I would add Pi Types (aka, non-type generic parameters), - inline assembly (C and C++), - memory model (C and C++), I am not sure the following are necessary in 2018, an abridged/simplified version would be awesome enough: - async/await (C++): aren't macros enough? - ATCs (C++): unsure if the full power is necessary right now, - specialization (C++): sweet, but I haven't felt much need for it yet personally, - type-safe variadics (C++): with built-in tuple and `Fn` support, I've found my need for variadics much reduced; maybe stabilizing `call_once` (and co) and providing some help with popping/pushing items in tuples to manipulate them generically would be kinda sufficient? (I liked the "pack"/"unpack" operators proposal) I am not even sure the following are necessary *at all* (from my POV): - alloca (C)/VLA (C): what I've used in C++ is thinks like "SmallVector&lt;T, N&gt;" which switch to heap-allocations above N elements and it has completely obviated the need for VLAs for me, especially as it doesn't suffer from the stack overflow drawbacks... - custom DSTs (C): isn't it redundant with macros? - macros 2.0 (C and C++): I know many people clamor for it, I just don't it need in any project I've worked on. - thin pointers (C++): I had an old proposal about handling those in libraries (mostly), if Rust had a stable interface to split/join trait objects and exposed alignment/size dynamically, it may completely obviate the need for language support of thin pointers. I have no idea what those are about: - existential types (C++) ? - virtual enums (C++): virtual enums and C++ don't mix, so... ? --- I think you've missed an item for your list: SIMD! On the language side, it requires being able to specify multiple implementations of functions based on available CPU features, and then at load time/run time pick the right implementation based on the current CPU.
I believe rustup can't be merged with cargo for the fundamental reason that cargo is tightly coupled with rustc and rustup, for obvious reasons, can't be. For example, if you look at the new modules RFC, IIRC, some of the logic is/will be being implemented in cargo and some in rustc and the versions of both tools have to match exactly to make it work. There's a lot of advantages to that tight coupling, so a discussion on merging rustup and cargo is probably a non-starter. The other point about the merging of the two tools is that rustup already does that, in a way, by wrapping cargo to do toolchain resolution. When you invoke cargo, you're actually invoking rustup's cargo wrapper which invokes the correct version of cargo. So if there's a need to silently download the correct version instead of failing, that could easily be implemented in rustup alone without any changes to cargo. The hook to add the requested functionality is already there without merging the two tools. Also, on the topic of enterprise-friendly rustup, I'd expect that enterprises would want to bless versions of the Rust toolchain before developers could start using them and would prefer to host those blessed versions on some local server to minimize bandwidth usage and time spent waiting to download. Some way to override where rustup pulls its index and releases from would probably be helpful. This would be somewhat of a corollary of cargo's support for [alternate registries](https://github.com/rust-lang/rfcs/pull/2141).
&gt; ergonomics (writting `if *x &lt; 0` is just annoying, same for `x += &amp;(a + b)`) I must admit that I find really annoying to have to care about borrowing for `Copy` types. Each time I have to remember that large types could be `Copy`, but it's still annoying :(
I have an RFC for "raw identifiers": https://github.com/rust-lang/rfcs/pull/2151
For Stack Overflow, there's a relatively simple way of dealing with this; questions and answers are editable, so if the syntax in the question, or in the answer, is out of date, you can edit it (or submit a proposed edit, depending on you reputation on Stack Overflow). If you don't know the right way to edit it, you can leave a comment on the answer asking about how to do this in modern Rust; most of the people who have submitted accepted answers are still active in the community and would likely be willing to update their answers. It might be a good idea to have a central place to track these issues and get high-rep Stack Overflow users to edit things that are wrong that other people don't notice. Do you have any in mind that you know of that could use updating? Can you check your browser history and find any you've run across recently? I have plenty of rep on Stack Overflow so I'm happy to make edits to correct things, I just need to know which ones are actually causing problems.
&gt; Also, "use new compiler" releases already exist, they are just mentioned in the readme or release notes Or in the [rust-toolchain](https://github.com/rust-lang-nursery/rustup.rs#the-toolchain-file) file. That this file exists demonstrates that specifying the compiler version in the project is possible, and desirable to some people. I absolutely think this should be merged into Cargo. This would actually be better than what Gradle does. Gradle (via the wrapper) puts the version of Gradle itself into the project, but doesn't lock down a Java version - you get whatever happens to be installed on the system. You can specify coarse versions to accept for the language and use for the class files (eg 1.8, 1.7), but not particular JVM releases. 
Contributed some stuff in a PR :) I would be tempted to work on some of these myself, but due to non-competing things with my day job that's not really feasible. Other things I've had enough of for a lifetime!
One other thing about the `#[derive(Debug)]` that would be nice to change is this: #[derive(Debug)] struct Foobar { some_data: Vec&lt;u8&gt; } If you print this out it will happily dump the whole Vec for you, even if, for example, that Vec contains several megabytes of data. In the majority of cases this is not helpful. It's especially annoying when you have a huge structure with something like dozens of fields, and one of those happens to be a `Vec` with a bunch of data, so you have to implement the whole `Debug` yourself just to special-case that one since `Vec`.
Please no more web browsers. When we say *native UI* we mean native, as in the widgets provided by the platform. [The "cross-platform" bit already exists](https://wxwidgets.org/), it just needs Rust bindings.
I've been pretty happy with SWT in Java. It gets a bad rap from its association with Eclipse, but most of that is Eclipse's plugin architecture which adds a lot of bloat. Using SWT directly ends up being pretty low-level and also pretty lean. And unlike Qt, its use of OS widgets means there's no uncanny valley to cross whereas you can usually tell when an GUI is coded in Qt. I really like the approach of finding the LCD of OS widgets across the 3 major platforms and then implementing it for each using the existing low-level, OS-specific crates (cocoa-rs, gtk-rs and the one for windows which I'm blanking on at the moment). I'd probably have gone down that path myself by now except that it's just way more of a time commitment than I can manage.
Not at all. A monad is a type with some operations that follow certain rules; `Result` is merely one instance of the concept, which isn't even expressed at the source level in most languages that include it.
See this comment https://www.reddit.com/r/rust/comments/7p1s2l/notyetawesome_rust_tell_us_the_use_cases_you/dse4fjp We've got some stuff set up on github already and we're starting to identify which questions to go after first. 
&gt; ATCs (C++): unsure if the full power is necessary right now, This is required for streaming iterators, making the `Index` trait work with temporaries (e.g. a row view of a matrix), etc. So I think this is definitely necessary. &gt; async/await (C++): aren't macros enough? Too much work has gone into this to not finish it. &gt; type-safe variadics (C++): with built-in tuple and Fn support, I've found my need for variadics much reduced; maybe stabilizing call_once (and co) and providing some help with popping/pushing items in tuples to manipulate them generically would be kinda sufficient? (I liked the "pack"/"unpack" operators proposal) The function call traits cover most of my use cases as well. I wouldn't definitely say that type-safe variadics are high-priority either. C++ needs them more than Rust because `&lt;tuple&gt;` is not part of the language. The only thing were variadics are necessary is interfacing with C functions that use `varargs...`. &gt; alloca (C)/VLA (C) ... custom DSTs Briefly, [Custom DSTs](https://github.com/rust-lang/rfcs/pull/1524) let you implement your own slice types (e.g. if you want to implement a slice type on a matrix with a stride) and the [RFC for alloca](https://github.com/rust-lang/rfcs/pull/1808) was closed in favor of unsized rvalues. I feel that there is an overarching theme over both that hasn't been explored yet. Anyways `alloca` is useful to put dynamic arrays in the stack when you only know their length at run-time. `SmallVec` doesn't help you here because the capacity that can fit on the stack must be known at compile time. Also, because Rust lacks copy/move constructors/assignment (which is a good thing), when you `memcpy` a `SmallVec` to move it, you `memcpy` the whole variant, even if the elements are on the heap. That is, if you have a `SmallVec&lt;[f64; 256]&gt;` every move is a `memcpy` of 2048 bytes even if the vector has a million elements allocated on the heap (in C++ `small_vector` just copies 3 pointers in this case). &gt; macros 2.0 (C and C++): I know many people clamor for it, I just don't it need in any project I've worked on. I was referring here to finally having "good" macros by example in Rust (not procedural macros, those are pretty good already and I am sure work there with continue independently of what I write about here). For example, one currently cannot easily re-export macros in stable Rust. If you have two crates that define a macro with the same name and want to use both macros in a third crate you are out of luck. Writing reusable/customizable macros is hard, hygiene is hard, ... macros 2.0 fix most of the issues I currently have with macros which are not many, but are still there. C++ and C have macros, and while Rust macros are way better, &gt; thin pointers / virtual enums Those are the same thing. [virtual enums](http://smallcultfollowing.com/babysteps/blog/2015/05/05/where-rusts-enum-shines/) was one of the many ideas that the core team floated around about how to implement thin pointers in Rust. &gt; existential types (C++) ? C++ concepts gives you `auto foo() -&gt; Concept` which means that `foo` returns a single type that implements the concept `Concept`. That's just `impl Trait` (i've edited the OP to add `impl Trait` there). &gt; I think you've missed an item for your list: SIMD! Is in the library wish list ;)
I have the following loop: while ges.contains(&amp;line) || parse_line(&amp;line) == Some(Comment) { .... } I'm facing a little problem: `parse_line` is expensive, as well as `ges.contains`, and I'm doing this quite a lot. Short-circuiting of `||` does help nicely here, and the first one is the more common case for sure. BUT I need knowledge of the result of the conditions in the loop body. I don't want to reparse, and I don't really want to save it in a loop-external variable because then I'd have to `parse_line` every line. So something like this would do it: while ges.contains(&amp;line) || (is_comment = (parse_line(&amp;line) == Some(Comment))) { .... } but then, assignment in conditionals is not supported in Rust. Is there an elegant way around this? Thanks for any pointers :)
&gt; I mention this because he thinks that detecting 100% of undefined behavior is absurd/impossible. I have seen the talk, and Chandler is correct that doing so in C and C++ is extremely hard at least (we won't see such a tool in practice for the foreseeable future). The things that Chandler mentions do not really apply to Rust. The main objective for Rust's memory model is to be implementable as a checker, that is, to actually allow such a tool to be written.
&gt; I thought streaming iterators only needed lifetimes support in ATC, not any type/value generic parameters, did I miss something? No, you did not miss anything, this statement is correct AFAIK.
Ah! Then I agree with you. I actually mentioned ATC (with a mention of "at least over lifetimes") in the "I agree" section of my original answer specifically for this usecase.
I'm seconding this. I haven't done too much serious stuff in Rust yet, but I'm pretty much in the same boat as you when it comes to C++. I make heavy use of templated code, and it feels like C++ compilers are just not designed for that kind of code (especially when it comes to GCC 4.8.5 which I have to support). Add in the cost of all those `#include`s, which are necessary because you can't have circular dependencies in C++ and so you have to split up everything into declaration, definition and implementation (for classes). A lot of C++ compiles quickly because it does polymorphism during run-time, but that's rarely needed and wasteful, in my opinion. Heck, just look at the widespread use of PIMPL as a tool to push down compile times at a run time cost. If you only used trait objects and no templates (except for lifetime parameters, can't avoid those I guess), I'm sure Rust would compile faster, but who would actually do that?
That's not really my point. Result sum-types and monads are orthogonal concepts, you can have one without the other in *both* directions.
Don't do that, it's evil!
"Can", yes. "Is a good idea", questionable at best.
Meh. I haven't done previous surveys, but I thought I'd tackle this one. The advertising questions were fine, but there was way too much focus on the endless questions clearly intended to help SO optimize their job portal. I ended up just skipping through almost half the survey. (At least you can just skip a page)
SPRS is usable, but this issue https://github.com/vbarrielle/sprs/issues/125 means, that you had to reimplement quite a lot of stuff by yourself. 
&gt; + GPU support: gfx-rs recently added compute shaders support, and rlsl is an active project trying to create a subset of Rust which runs on the GPU. Can you point me to the compute shader support of `gfx-rs` and/or some examples? EDIT: Found in this crate: https://github.com/msiglreith/gfx_ocean
Agree it is almost there. But I would really love to have all of this stuff in stable, instead of 10 more random features in nightly ;) And as for GPU, I should probably wrote it as CUDA support. 
I'd like better support for being able to install a specific version of rust into a depot and build with that specific version of rust + cargo vendored crates without necessarily having a rust install elsewhere (eg. a build machine). I've been able to hook this up somewhat, but it's a bit flaky as I think it still sometimes uses the system rustc even if there's another one earlier in the path. Anyway, being able to use rustup to create a local system install in an arbitrary folder would be cool, along with a shell or batch script to enable that build environment.
[typed-arena](https://docs.rs/typed-arena/1.3.0/typed_arena/struct.Arena.html) is a useful example using this signature. (Though I have my doubts if this was a wise choice. I think I would have gone with the slightly uglier mutable API)
HKT isn't *that* uncontroversial, especially in Rust.
&gt; Eg the could segfault because it can be null. Understood. Yeah, I thought about that too. I guess I haven't worked on enough code in a fully auto-deref language where I was mixing pointers and "normal" types, that I could get them confused with each other wrt maybe being null. But I admit it's a possibility. Maybe that's why C didn't implement auto-deref for pointers. 
For my GGEZ game [rowdy](https://github.com/leotindall/rowdy) (~=2.5 KLOC), I observe: * Stable Debug Build (from scratch): 2:25.38 * Stable Debug Build (string change): 0:05.122 * Beta Debug Build from Scratch: 3:01.68 * Beta Debug Build (string change, incremental): 0:03.86 * Beta Debug Build (string change, non-incremental): 0:03.96 This... seems like a regression. In terms of release builds, I observe: * Stable FPS with ~= 750 particles: 55 * Beta Release Build with ~= 750 particles: 56 
Why specialization is needed: If you want to use the standard vocabulary of Traits for things like making your struct both convertable to and from a type, you now need specialization because `From&lt;T&gt; for T` is already implemented. I've seen this impact `assert_cli` and `failure` (this is why `Error` doesn't implement the `Fail` trait for example).
I think at least const generics are a must have. Otherwise we have to resort to unpleasant hacks like typenum and generic-array (which are great libraries, but wouldn't exist if we had const generics). From personal experience, type level numerics would be very, very helpful for cryptographic libraries, and I presume for a number of other applications too.
Indeed there's still some functionality lacking, I'll try to improve the library as soon as I get some spare time. I hope what's already there is useful.
Nice write up, liked all of it.
UB != UB. That is, the C spec has ~200 undefined behaviors. (This depends on which version you're talking about; I did a rough count of C99's in this case.) We currently [have ten](https://doc.rust-lang.org/reference/behavior-considered-undefined.html). Now, that isn't to say that this is a great metric; for example, the C spec is, well, much more of a spec! As we actually nail down a memory model, this will grow. But there's also a lot of stuff that's just straight-up not relevant to Rust; like (chosen at random) * Two identifiers differ only in nonsignificant characters (6.4.2.1). * For a call to a function without a function prototype in scope, the number of arguments does not equal the number of parameters (6.5.2.2). But, in general, I expect our "UB surface", if you will, to be much smaller than C and C++'s. A *lot* of it is due to legacy that we just simply don't have. Like that second bullet point there. As the OP mentions, since we're still defining our own rules, we can, to some degree, make them what we want, which means we can lean more towards things that are checkable. That's the hope, anyway.
&gt; As for the first reason, it's only an issue if you need to call into C... (RIIR :D?) Unfortunately for Wayland, it's all in C and will probably never be re-written in Rust (at least, I don't have high hopes for that). And that is the case for a large portion of software. This is the main use case I have for Unsafe Rust.
There's no way that I know of, currently, to do this in a loop condition. And it makes sense because how will code inside the loop body know when the loop is run because `ges.contains(&amp;line)` returned `true` compared to when `parse_line(&amp;line)` returned `Some(comment)`? What happens if it's the former but the code tries to access `comment`? My solution, though mundane, is to rewrite the loop body: loop { if ges.contains(&amp;line) { // ... } else if let Some(comment) = parse_line(&amp;line) { // ... } else { break; } } However, if `ges` contains the same type as returned by `parse_line`, you could chain `Option`s together; it just depends on what you see as most elegant: while let Some(comment) = ges.get(&amp;line).or_else(|| parse_line(&amp;line)) { // ... } Though I'm guessing you're going to want to insert `comment` into `ges` before doing work on it. If `ges` is a `HashMap` you can sort-of use the `entry()` interface but it needs a little extra because entries don't currently support optional insertions, and it requires an obligatory copy of `line` each time which you may not prefer: // guessing at the types here fn get_or_parse&lt;'a&gt;(ges: &amp;'a mut HashMap&lt;String, Comment&gt;, line: &amp;str) -&gt; Option&lt;&amp;'a Comment&gt; { use std::collections::hash_map::Entry; // copy of `line` here, unavoidable match ges.entry(line.into()) { Entry::Occupied(occupied) =&gt; Some(occupied.into_mut()), // to get a reference with the right lifetime Entry::Vacant(vacant) =&gt; parse_line(line).map(|comment| &amp;*vacant.insert(comment)) } } while let Some(comment) = get_or_parse(&amp;mut ges, &amp;line) { // .. }
Thanks! I enjoyed yours as well.
I have a Cargo project, and I want to call into a dynamic library. At one point, I was able to include the dylib with a build.rs, but at some point, I wasn't able to do it anymore on Mac. I haven't figured out how to work around it.
Awesome! Glad I'm not alone :) I'm not sure how optimization factors in with incremental compilation (especially since a lot of optimization happens at link time), but it would be pretty nice see if we can get reasonably good performance while still getting fast debug build compilation.
Some way to make collections in particular just give their type and size would be good, but that also seems like we'd need a whole RFC on how the compiler flag would work or something like that. Or have it be some feature on the std crate.
Because the monomorphization of the function that takes `&amp;[T]` still needs to know what type `T` is for codegen, even if you call it with an empty slice which doesn't give the compiler any way to infer `T`.
i would love to see a blog post comparing and constrasting these implementations!
future me is very excited how far `mrustc` has come :)
I'm interested in contributing, and possibly helping maintain in the future. I'm relatively new to rust, but I'd be glad to review PRs, look at issues, and test the library. I am starting a statistics-heavy project in Rust, so I was planning to use statrs anyway!
I think there needs to be a place to request help with fuzzing or some sort of fuzzing strike team that can be called in, because sometimes it can be difficult to conceive test cases for fuzzing and produce interfaces for fuzzing that can accept a single byte slice as input. For example, my crate `multipart` has zero unsafe code (with the exception of a single block in the fuzzing setup itself), and fuzzing is still immensely useful to find (primarily) bugs with parsing boundaries in requests, but that process inherently takes two inputs: the request body, and the boundary. I ended up hardcoding the boundary in the fuzzer and adding it to the fuzzing dictionary, but this doesn't seem entirely right. It did help to catch a number of bugs anyway, but some still slip through.
All important points. But at least for our application (comp. geom.) all must-haves are already there to replace C++. For us it is already good enough! Well some are moaning about the missing IDE experience...
&gt; Become a mod! How do you go about that? I'm not asking, btw, just wondering for the sake of discussion. I understand we want to make sure mods are high quality.
I guess the issue is if you have something like `foo\nbar\r\nbaz\n` on Windows, then there's technically only one "new line" character, but ripgrep will count 3 of them. But I suppose that probably doesn't happen too often, or at least, doesn't happen often _intentionally_?
Somewhat related, if you're looking for faster Rust builds for cheap: I've been using [sccache](https://github.com/mozilla/sccache) for a few months now. It makes my cargo (re)builds much faster and it hasn't played tricks on me, even jumping around toolchains and dependencies. cargo install sccache It's _especially_ good for CI environments where you always start "from scratch" (so incremental builds wouldn't help) but you actually know the result of compiling the dependencies is most probably going to be the same because they rarely change. 
Great post! Fun fact: the target is in beta now, which means it'll be in stable on the next release. Given that it's still a pretty new target, I'm not sure how much we're going to advertise it yet...
Yeah, I don't know how hard optimizations could be, so I'll avoid bikeshedding on this, but perhaps something where the compiler could track use of generics/traits in application code and keep partially compiled artifacts of relevant dependencies around to speed up recompile times. I don't know what the right choice here is, though I *am* happy that members of the core team are concerned and are making it a priority.
I assume short bits of data. Taken from the README: &gt; Note that for very short slices, the data parallelism will likely not win much performance gains. In those cases, a naive count with a 32-bit counter may be a superior solution, unless counting really large byte slices.
Yep, found it and fixed the first off the list that that had a very outdated accepted answer.
Thanks! Yes I'm very much looking forward to having the new target in the stable branch. I'm going to build another version of the module using wasm32-unknown-emscripten when I get time to see if there are any performance differences in this example. Any ideas if wasm-gc will be part of the build process for wasm32-unknown-unknown when it reaches stable?
Only the bootstrapped version is bit-identical, where the mrustc-compiled-rustc compiles rustc itself. So you are comparing the output of rustc with the output of rustc. The fact that mrustc can bootstrap is its achievement, but the fact that rustc is bit-identical is more of an achievement of rustc itself than of mrustc. Still very impressive though.
I don't believe it is, currently.
Sounds like what is needed is a "PrimitiveCopy" trait that extends "Copy" trait that could be applied to primitives. Then the compiler could possibly support better ergonomics for things that implement "PrimitiveCopy" and possibly automagically make primitive values "PrimitiveCopy". I'm not sure I yet know enough about Rust yet to say if this is feasible/useful though.
&gt; First, this is one of only two changes that relies on the epoch. When we announced our plans for epochs, many people were skeptical that we would do as we said, and only included small, parser-level changes here. But we did, and will continue to do so. catch and dyn Trait are the only two changes here. I know there are probably a lot of "me toos" from people wanting to get into the epoch action so sorry and please don't dismiss this out of hand. One area of concern I have is with maintaining the list of warnings for a project. You can deny all but that will break your CI as new warnings are added, so that leaves you with [explicitly listing them all](https://github.com/cobalt-org/cobalt.rs/blob/master/src/lib.rs) in every top-level file you care about (`lib.rs`, `main.rs`, each test file). I'd like to see a warning group `epoch_all` (name is a placeholder) and `epoch_next_all`. `epoch_all`s warnings do not change and you can `deny` them safely. You could warn on `epoch_next_all` just for informative purposes to try to minimize them but it won't break your CI. Additional notes - I'm unsure if it should be `all` or something more specific (some warnings are just warnings and can't be fully removed in proper code) - Clippy should have a similar facility This is a bit of a preview of my `#rust2018` post that is CI focused inspired by my frustrations in maintaining projects. I've been planning to write it up since RustBeltRust and am grateful for this initiative to force me to actually do it ... eventually.
I tried to use `impl Trait` with lifetimes but could not manage; am I missing something? #![feature(conservative_impl_trait, universal_impl_trait)] struct S {} struct Ref&lt;'a&gt; { s: &amp;'a S, } trait Trait { fn foo(&amp;self); } impl&lt;'a&gt; Trait for Ref&lt;'a&gt; { fn foo(&amp;self) { println!("{:p}", self.s); } } impl S { fn works_1(&amp;self) -&gt; Ref { Ref { s: self } } // fn fails_1(&amp;self) -&gt; impl Trait { Ref { s: self } } fn works_2&lt;'a&gt;(&amp;'a self) -&gt; Ref&lt;'a&gt; { Ref::&lt;'a&gt; { s: self } } // fn fails_2&lt;'a&gt;(&amp;'a self) -&gt; impl Trait { Ref::&lt;'a&gt; { s: self } } } fn main() { let s = S {}; s.works_1().foo(); // s.fails_1().foo(); s.works_2().foo(); // s.fails_2().foo(); } 
&gt; There's no way to find a sink in a graph or use regular expressions in Rust? Yes, but doing so efficiently does not need to rely on undefined behavior. 
To be clear, I don't think we've settled on the exact names and groupings of any warnings, as I said, this post is speculation! I think these questions are very real.
Excellent! I didn't know this existed. I hope this makes more progress too! I'll try to keep a closer eye on this
if libraries you're attempting to dynamically load are in the same directory as the executable, that works on most platforms. then you won't have to mess with the path. you just zip up the executable and the libraries in a single folder.
`impl Trait` doesn't capture lifetimes by default-- just types. To include lifetimes like `'a`, use `impl Trait + 'a`.
Ok. I knew the post was speculative but wasn't sure how much of epoch's has already been decided and that reflected in your post.
No. Really no. The world does not need another Electron; we didn't even need Electron in the first place.
I'm in the newsletter!
Also, apparently something similar to cretonne is what Blow is doing with Jai, which touts fast compile times.
This is a super weird and niche thing, but why does TWiR use images for emoji? Unlike real emoji, they can't be copy/pasted with the rest of the text, and they show up weirdly in RSS readers (that don't apply the required CSS to shrink them back down to text-size).
Have you had a chance to evaluate [sxd-document](https://github.com/shepmaster/sxd-document) / [sxd-xpath](https://github.com/shepmaster/sxd-xpath)? As you can guess by the title of the second, there is a large chunk of XPath 1.0 supported. There is currently no DTD support, but that's mostly because I've never needed it personally. It is slower than I want it to be. Reading ~100MB of XML into a DOM and printing it back out takes ~6 seconds, xmllint does the same in ~4. If you are willing to contribute, I think adding DTD support is totally feasible. 
&gt; I don't want a full featured XML library Well, you'd be OK with a full-featured library that has the knobs to turn off insecure things (or to turn them on if off by default), I hope? &gt; without fearing network connections or DOS. I know about recursive entity expansion, which is what I assume you mean by DOS, but what do you mean by network connections? Do you mean external DTD entities? I think any Rust library is going to allow injecting a network provider instead of baking one in, so you'd always have the ability to say "nope, no URL here"
&gt; which gives you a statically-typed view of your XML Document, inferred from the DTD with some kind of infer_schema! macro. I'd expect that to come with serde integration — a build script reads the XML file, generates structs that implement `Deserialize` and then you can deserialize at run time.
To emphasize this, if you look at `unsafe` in regex, you get * https://github.com/rust-lang/regex/blob/52fdae7169ec619530985a019184319ac4bbee5a/src/pattern.rs#L25 * https://github.com/rust-lang/regex/blob/af2c105114d1e55a173ac8e24506959bfa83325f/src/dfa.rs#L638 * https://github.com/rust-lang/regex/blob/5233b14da79ef9f9fcb223fd70adca7106bacc77/src/simd_accel/teddy128.rs#L459 and that's it, as far as I can tell. All other results are saying "we could use unsafe to skip this check but benchmarks don't bear that out", using it a few times in the benchmark stuff for `unsafe impl Send`, and the c API it exposes which uses unsafe for hopefully obvious reasons.
I was initially surprised by that too, but as mentioned it's new and the initial implementations are probably far from optimised. But it'll get there soon I am sure of it. Unless I have made a coding error of course!
Excellent article. I think your view is very realistic and I'd be truely happy if that goes like that. One question i had for you is how you feel about the rfc process and the growing community. For me I already struggle to keep up with new RFC without even trying to read in details. I am really curious to understand how you guys doing. Can you keep up with all of them? Do you focus on a smallish area? Aren't you worried about the pace at which is going?
If you feel passionately about this, you should consider writing an RFC.
Wasm isn't only about the front-end, nor is it only about the web.
Yes please! Your post is exactly what I would want from Rust in 2018: * Finish up and stabilize accepted features like NLL, the new module conventions, macros 2.0, impl trait, (possibliy ATC and generators) * get miri up and running * clean up the feature gate backlog by stabilizing or dropping them * clear up the issue backlog * polish up rustfmt, clippy, rls * don't accept any major new language features (!) * work on improving Rust as a systems programming language ( like other posts have mentioned) I'm most sceptical about the epoch approach. An epoch that only introduces two very small breaking changes seems wasted to me. I'd much rather see a forward looking epoch that also adds new keywords like async + await and immediately introduces the breakage ( no preparation period), but with but with the expected commitment of not having a new epoch for a few years. This seems very much like the current ECMAScript approach, which i personally find horrible. Even if rustc can handle multiple epochs just fine, it still brings churn.
It also does seem to speak to the effectiveness of modern JavaScript engines. For pure numerical code like this, I believe the hot loop is getting JITted into _very_ efficient assembler. I've set up a little benchmark locally, and the code seems to run natively at a comparable speed to what I'm seeing in the WASM, so I'm not convinced that WASM is being slow here. For pure numerical code like this, the JavaScript engine should be able to remove all kinds of type checking and guards that it would normally have to worry about. Once it gets into the `mandelbrot` function, it's probably just one big block of efficient assembly. Normally, JIT engines have to keep checking to make sure everything is okay.
&gt; While the majority (roughly 90% I’d say) of Rust written is safe Rust, that last 10% is just as, if not more, important. Where does these numbers come from? I've written Rust for about three years now, and the only time I've written anything unsafe was when I wrote a concurrent garbage collector. I would say that if a Rust project contains 10% unsafe code it's probably very low level, or a wrapper to a C API. &gt; The standard library is almost entirely unsafe Rust. This is not my impression *at all* (although I have trouble finding exact numbers here for the entire std). For instance, `vec.rs` in `liballoc` is 2745 lines long. By removing comments, `#[asd]` annotations, and empty lines, we get about 1120 code lines. If we remove all `unsafe` functions as well as all `unsafe` blocks, we are left with 812 lines in the file, which means that 27% of the file was unsafe code. Now, 27% is a lot, but seeing as this is `Vec`, it is kind of expected, as it's a prime candidate for using `unsafe`. Saying that the STD is almost entirely unsafe seems very misleading. 
Would it, really? Maybe I'm off base here, but I think part of the reason rust makes sense to many of us is because we understand the problems it's trying to solve around memory safety. A brief introduction to c would help a lot understanding why rust is the way to go for systems programming in general.
Yes, that's a good point. The WASM implementations could be extremely well optimised already, but the JIT-compiled JS could be equally well optimised too, meaning that the performance equality in this case is due to the fact that this example code cannot get any much faster in any form. I'll experiment with different sources for the WebAssembly module and post my findings. I am certainly looking forward to threading support landing in WASM, as this example could be split into 4 threads, each rendering a quadrant concurrently. As long as the WASM implementation provides actual threads for this, we should see a significant improvement over the plain JS implementation for multi-core systems. If anyone has ideas for another algorithm that might tax the JS engines a bit more, let me know.
I think that the built-in `Debug` should have at least some limited capability for displaying the type name as a placeholder instead of simply rejecting to derive. This way, if someone wanted more "detail" they could still "upgrade" to a more powerful macro, but at least every type could be made "Debuggable" by in some way. But yes, I also do view this as a success of the macro system. Plus, it was a good exercise for testing out the then new `proc_macro_derive` functionality :P
What version of the windows tool chain is appveyor using? Some google fu seems to suggest that error is from the linker running out of memory. Maybe decrease your debug information?
&gt; If anyone has ideas for another algorithm that might tax the JS engines a bit more, let me know. Generally with a JIT, it can only see a certain distance, because it can't spend forever thinking about how to optimize the code. With small benchmarks, JITs can comfortably see everything all at once and make great decisions. Where I believe they struggle is with medium-size or larger applications that have lots of functions. The "code horizon" becomes too small relative to the size of the code, and they have to make decisions based on limited information, with guards in the assembly to double-check that their invariants remain valid. Unfortunately, this means I believe making a realistic benchmark of JS vs WASM is really hard. Something that might be easier is benchmarking WASM vs native. [Rustface](https://www.reddit.com/r/rust/comments/7oz7r5/rustface_port_of_a_c_face_detection_library_to/) recently got posted. This is an example of a process that someone might want to use Rust for instead of JavaScript, if they were hoping to get better performance out of WASM. It would be interesting to see how much slower the WASM version is than the native version. It would be *amazing* to see a port of `rustface` to JavaScript itself for another performance comparison, but that would be a lot of work. A middle ground there is that the pure-JS `emscripten` backend for Rust could be used as a third point of comparison. I'm not sure how hard it would be to compile Rustface, it's just an example. There might be other projects out there with fewer dependencies that could be easier to test with. Benchmarking: - native performance - WASM performance - emscripten-JS performance emscripten is not like normal JS, so I wouldn't consider it fair to what JS might normally achieve, but it would still be interesting. Regardless, what you do is up to you! Thanks for posting this one benchmark! More would always be fun to see, of course, but I'm merely responding to your request for other ideas, not trying to pressure you into it.
&gt; code coverage in cargo Yes please! Including doc-tests of course would be amazing!
I have to agree. The biggest blogger is libraries.
I'm interested in the library, and have been following for a while without an immediate use-case. I'll see if I can knock out an issue or two to get to know the internals.
That makes sense. Thanks for pointing that out.
One thing that I have been wondering about in pandas interoperability, perhaps via parquet or arrow. Or alternatively, is it possible to have zero copy access to a numpy array/pandas dataframe?
This was really fun to read and it looks like good work! Personally, I wish the syntactic different between static and dynamic dispatch were more... self-documenting? ;) It's too easy for me to forget which syntax has which behavior.
Sorry, these numbers are ballpark (and I probably should have made that clearer!). In reality it's probably lower for unsafe code. But what I meant by these percentages is of _all_ Rust code, not code in a repo. (e.g, 90% of all Rust code is probably Safe Rust). So using that rough metric, yes the STD is almost entirely written in Safe Rust, but all of the things it's implemented (and the most critical stuff, I'd say, and the stuff most easily to get wrong) is written in unsafe. I should have written that differently though, as I realize I came across like I meant that there's more unsafe lines than safe lines in the STD. As you are probably aware, unsafe code pollutes the entirety of a module in Rust, so even though only 27% of the `Vec` implementation is unsafe it means you have to think about it for 100% of the `Vec` implementation code (i.e you always have to make sure the capacity is correct). So my argument more is that if we can improve unsafe Rust so that that 27% code has better ergonomics, better diagnostics, and better defined semantics it would make all of the code (even the "safe" lines in an unsafe module) easier to write because you'd have more confidence. I have to write modules that are like Vec, in that I'm providing a safe API over C structures that have a certain memory model that I need to adhere too. Having these things would make it more obvious something is e.g wrong in my safe code is writing safe code gives me a diagnostic error.
Nice!
&gt;First, I would like to point out, that I cannot imagine Rust being competition to Python in field of ML. Python is leaps and bounds ahead of Rust as a general-purpose ML environment, but I'd love to see Rust come to use in individual algorithms and interop with python. Lots of python ML libraries are just wrappers around native libraries, and rust could totally come into those areas. Generally speaking, I think that's Rust's entryway into a lot of areas.
Not to argue against any of the points you make for better defined unsafe semantics, but this specifically seems wrong to me: &gt; unsafe code pollutes the entirety of a module in Rust, so even though only 27% of the Vec implementation is unsafe it means you have to think about it for 100% of the Vec implementation code If you find you're getting UB, or a segfault or a data race you don't need to check all the code in Vec, you only need to check the code blocks that have the ability to put your running program into a state that could result in this type of error. Or am I mistaken?
Why is a TU a copy file in c++ but a crate in rust? Wouldn't a module be the equivalent?
What I’ve found when I was assisting teaching programming was that students actually benefit from having a stronger type system. The question for me is what you want to teach in an introduction class. At my old university they managed to screw it up entirely by letting different chairs teach that class every other year (how hard can it be to teach some coding, even if your focus is say HCI). I believe in one year the focus was on C, another year it was Python or Smalltalk or Java or whatever the current lecturer knew best. What I’m trying to say is that teaching how to program can be taught in a lot of ways, and every approach has its own advantages and disadvantages. So if we want to focus on lower level things Rust could be a good language to start with. Surely there might be some noise (why does print have an ‘!’) but that didn’t prevent other people teaching certain languages (looking at you Java). The main thing which I believe we are cautious about is the ownership system. But maybe students actually don’t care, since unlike us they don’t have the burden to relearn something. I guess the only way to know is if someone just tries anf shares the result. 
Thanks! A small number of tiny changes spread out over time is better than immediate, huge change, IMO.
Thanks for the post. Transitively this will have a big impact!
I suggest taking a look [at this article](http://smallcultfollowing.com/babysteps/blog/2016/05/27/the-tootsie-pop-model-for-unsafe-code/) to see why you might need to actually check all the code in `Vec` to see where a safety bug could be.
The single-line or -word code snippets are almost unreadable due to the low contrast in the color scheme compared to the white background. All-dark color scheme or bust.
Quite interesting how, despite being faster-to-execute on the hot/happy paths, static-dispatch functions have signatures that are noticeably slower for humans to read because of the decoupling from declaring the type-variables and using them; whereas with dynamic dispatch the trait bounds appear directly in the type.
Haha you are the second person to point that handle to me. :P But yeah, like /u/DataPath said, my nick has got me some cachet. Lots of it.
I *don't* think it's particularly interesting, because those are quite disconnected things, but `impl Trait` is [coming to nullify your entire statement](https://internals.rust-lang.org/t/help-test-impl-trait/6516) before long, so it won't be this way forever.
[quote]which is irrelevant if you have a garbage collector.[quote] I don't know if I'd agree with that. GC doesn't solve concurrency issues to the degree that the Rust ownership model does. Also, the use of "immutable by default" in Rust makes programs much easier to reason about.
It's not mentioned because this is his "What's Next After Rust." article. It's other horizons for PL-theory.
A Rust that gives the end user agency is the Rust I want. Maybe /u/dtolnay could chime in?
Ooh, it applies to argument types as well as return types! Awesome! (Also awesome that it's reached this stage; seems like it's been in-the-works for an eternity.)
After reading the rest of your responses in this thread, I would like to join your cult.
use it only for routing? it's not worth it.
ok, but how can I bundle a path, the name of a route **and my handler** together? something like: router.add("/articles/:article_id", "show_article", show_article_callback); where "show_article_callback" is a function where I can process a request
That's what I meant. I also think this politic is good way of supporting new project but my worry is the lack of differentiation between well-known libraries and brand new libraries. In NPM, that lead to [typo-squatting](https://www.theregister.co.uk/2017/08/02/typosquatting_npm/) or potentially [getting hacked through transitive libraries](https://hackernoon.com/im-harvesting-credit-card-numbers-and-passwords-from-your-site-here-s-how-9a8cb347c5b5). And nowadays it's almost impossible in Node.js to not end up with plenty of tiny 0.0.1 transitive libraries. I think there should be a clear separation and some clear rules which make sure you don't get hacked through transitive libraries. 
I was also quite shocked that `clap` takes much the majority of space in `tokei` even more than `std`. $ cargo bloat --release --crates 31.2% 487.2KiB clap 18.2% 284.8KiB std 12.9% 201.0KiB [Unknown] 10.9% 170.5KiB regex 7.2% 113.0KiB regex_syntax 7.1% 110.3KiB tokei 4.5% 69.8KiB ignore 2.5% 38.9KiB globset 0.9% 14.3KiB rayon 0.9% 13.3KiB env_logger 0.6% 9.1KiB rayon_core 0.6% 9.0KiB aho_corasick 0.5% 7.3KiB thread_local 0.5% 7.2KiB encoding 0.3% 5.4KiB coco 0.3% 5.0KiB crossbeam 0.2% 3.2KiB ansi_term 0.2% 2.5KiB rand 0.1% 2.1KiB memchr 0.1% 1.9KiB strsim 100.0% 1.5MiB Total
Disclaimer: RFCs are neither written law nor a spec nor anything. Often a team decides "let's do this aspect in some other way".
I agree... except that last "All-dark color scheme or bust." Light-on-dark strains my eyes more than dark-on-light for some reason.
alright- if that's what you want.
That's what the (accepted) [`dyn Trait` syntax](https://github.com/rust-lang/rfcs/blob/master/text/2113-dyn-trait-syntax.md) RFC is about! I think that adding a deprecation warning/error for the accidental-dynamic current syntax is one of the (few) things being considered for the next epoch.
just an interesting post from a few minutes ago: https://www.reddit.com/r/rust/comments/7pcy6m/wasm_vs_js_realtime_pitch_detection/
&gt; “Systems programming” I think one of the cornerstones of Rust is the commitment to low runtime overhead. Many people in the Rust community work hard to create the fastest solutions for a problem around (rustface, ripgrep, pathfinder, ...). This is also partially enabled by the language not standing in the way, but making it very easy to write performant or resource constrained programs. The term "systems programming" is not well defined, I agree. But I think that's okay, as it is rather a rough category, more of a commitment than a precise description of the language. &gt; Or maybe you’ve seen this: someone on a thread, maybe on reddit, posts that maybe Rust isn’t doing so well in a benchmark or in a particular scenario. Minutes later there are half a dozen comments dissecting what happened. I think this is awesome, because it means that people care about performance or those scenarios. E.g. think how cargo-bloat got released and a few days later the clap author makes a blog post about having made clap smaller using that tool. I find that such a self-critical attitude towards constant improvement is a virtue. Don't you think? &gt; “Non-lexical lifetimes” and more NLL is a [pretty shitty term](https://github.com/nikomatsakis/nll-rfc/issues/43) either way because it rather describes what it is not and not what it is. In general I don't have the impression that Rust is too elitary. In fact quite the opposite. I think many people who feel intimidated by the other systems programming languages like C or C++ choose Rust because it allows them to write programs with a lower cognitive overhead. Also, learning Rust in the first place is I think easier than learning C or C++.
I totally agree that this kind of post is helpful! Thanks for writing it! The next step beyond expanding the macros fewer times is actually suiting up to defeat the borrow checker (sorry, /u/jntrnr1) and use functions instead, right? What are the obstacles here?
If we're giving feedback anyway, I'd love to see the silly smoothscroll done away with. I have it disabled in my browser, no need to override my settings. It's really annoying that some WordPress themes have taken it upon themselves to try to override browser behavior.
I've changed the formatting of the code blocks, especially the inline ones. I hope this helps!
Of the `.text` section yes. But not the entire binary size, which was really my only gripe with `cargo-bloat`'s reporting. As a side note, I think people actually use less of `std` than you'd think; a few primitives, maybe some collections, and a few traits. It's not as if all of `std` gets included when compiled. So when people see that `std` is quite small in the binary, but then think of *the entire std lib* it inflates the mental picture all those other crates listed in the output.
Fixed.
&lt;3 &gt; What are the obstacles here? Primarily it's just how clap was built internally. For example, if some struct contains several fields you need to mutate, normally no big deal. But when those mutations become non-trivial and need to cross function boundaries you have to borrow `self` mutably, and `borrowck` then disallows mutating any fields of `self` while the borrow is "live" and it all spins into a giant mess. This is compounded when you are trying to reference something from inside the struct itself. Since `borrowck` can't peek into functions to see that nothing bad is happening, it just disallows it entirely. Non-lexical lifetimes *might* help here...but then they might also be somewhat limited in the initial implementations. The correct way would be to break up `self` into smaller structs, but this would be a *major* refactor. So I'd like to slowly work towards this goal. 
&gt; I think one of the cornerstones of Rust is the commitment to low runtime overhead. We're in agreement here. What I focus on in my blog post isn't about Rust's direction but rather the words we use. There are ways the words we use confuse other people, and it needn't be that way. &gt; The term "systems programming" is not well defined, I agree. But I think that's okay, as it is rather a rough category, more of a commitment than a precise description of the language. The commitment still stands if we use different wording. These aren't bound together. As you say, "systems programming" is not well defined, and as such we can work towards terms that are less confusing for everyone. &gt; I think this is awesome It can be quite exciting to be surrounded by people trying to help, but others have had negative reactions to it. What I'm trying to call out here are examples of where our enthusiasm gets the best of us and we aren't doing what's appropriate for the situation. That kind of response isn't one-size-fits-all. &gt; In general I don't have the impression that Rust is too elitary. In fact quite the opposite. And we'll work to improve it in time. Just like polishing the tools and the language itself, we can polish how we talk about Rust. You may feel that it's not elite now, and that may come from your background. I point out a few areas we can work to be less elite for folks from other backgrounds. &gt; I find that such a self-critical attitude towards constant improvement is a virtue. Don't you think? Absolutely. It's why I wrote the post, and why others are writing #rust2018 posts. There are ways we can improve, and having a keen eye to where those areas are and going after them is a virtue. 
Love it, thanks! I'll definitely be using this in the future 
 Next step: burnt sushi pizza 
This was a really cool and informative write-up. It's great to see a fully worked example of optimizing-in-practice!
I took the liberty to repost Niko Matsakis's #Rust2018 here. I share the same interest in making progress on "const generics, procedural macros, and generic associated types" in the post.
Except those multiplication issues, it is quite useful. And mainly when writing ML library I do not have to reinvent the wheel and can something, which is out there and maybe somebody else uses (so it is a big plus).
Sometimes Python wrappers are nontrivial ones over C/C++ API. Big examples of this are Tensorflow and CNTK, where C++ API is really low level one, at Python gives you also some convenient nice highlevel abstraction on top of low level primitives. But I generally agree, that this might be the way.
&gt; In general I don't have the impression that Rust is too elitary. I don't either, but I do share the concern about vocabulary. I didn't read about NLLs for months until they hit nightly because I did actually think the inaccessible name meant it was some corner of the language I'd never touch (I got used to this from C++).
A lot of people who don't use Rust have gained the impression that it is a very hard language to use. A way this is often expressed is "Rust is great when you need it, but not worth it for most things."
&gt; Any ideas if wasm-gc will be part of the build process for wasm32-unknown-unknown when it reaches stable? (warning: shameless plug incoming! sorry!) FYI, my [cargo-web](https://github.com/koute/cargo-web) will automatically run wasm-gc for you. It supports Rust's all three Web targets (asmjs-emscripten, webasm-emscripten, native webasm), so you can easily switch between them by passing the appropriate flag, and it will automatically set up Emscripten for you if you use Linux.
Because of possible static methods? A function can still call `T::default()` if it has no instance of `T`. As for why it is required for all types, and not just those with static methods available, I would say consistency, and forward compatibility. I mean, adding a static method to a trait shouldn't be a breaking change if it has a default implementation. If T was inferred to be () whenever no static methods were available, and required explicit annotations when static methods were available, it would be a breaking change to add a static method to a trait.
There is a wishlist issue for [partial borrowing](https://github.com/rust-lang/rfcs/issues/1215), which would help solve this. This is a huge pain point for me as well, and I too used macros just to work around this issue. I would personally like if Rust did indeed peek into functions, and see what parts of types are borrowed inside the function. There are two major problems with this. The 1st one is that changing implementation details of functions could break users of that code. The 2nd is that it might be too expensive, and slow down compilation. I think the 1st one could be solved by only doing this intra-crate. This would solve 90% of the pain, and you don't really need to provide API guarantees inside the same crate. For the 2nd one, I don't actually know how much this would slow compilation down.
What is the difference between: fn running_count(iter: impl Iterator&lt;Item = u32&gt;) -&gt; impl Iterator&lt;Item = u32&gt; and fn running_count&lt;I: Iterator&lt;Item = u32&gt;&gt;(iter: I) -&gt; I and is the following possible? fn running_count&lt;I: impl Iterator&lt;Item = u32&gt;&gt;(iter: I) -&gt; I
I can't find a point this blog post proposes where I disagree. Especially liking the part about making Rust more appealing to CTOs. The only point I don't understand fully is connecting the geographically diverse community of Rust. In which regard should the needs of Rust users in Brazil differ from the needs of Rust users in India? I feel that instead of country borders, the community is split on the lines of use cases: embedded, web servers, gamedev, etc. The only thing that comes to my mind is translating documentation or localizing the compiler.
Did you mean "Bruschetta"? I think even in this case the bread is toasted or baked which gives an opputrtunity to burn it.
&gt; How can Unsafe Rust be better? I keep saying this... there's a middle ground in C++ which rust loses. you have safe code which is more verbose for setup, and unsafe code which is hideously more verbose because the language wants to remind you not to write it In C++ the behaviour of the reference type is a great middle ground, safer and more communicative than raw-pointers, but not as verbose as safe code; and similarly the use of raw pointers from C is also better .. I wonder if there's a way of replicating that in a backward compatible way in Rust... I do generally like how Rust's syntactic space is used, I'd be surprised if it wasn't possible to drastically improve
The first function accepts a value of any type that implements iterator and returns some type that implements iterator. The second function accepts a value of any type that implements iterator and returns a value of that same type. The third function isn't possible: `impl Trait` is (sort of) a type, so it doesn't make sense to use it as a bound.
wow that does look interesting, if I'm reading right is that basically the idea of what I called an 'autoborrow' type? ("it's default binding is to borrow..")
&gt; It’d be great if cargo were able to perform this stub generation for you, using stubs that were distributes through a registry. Imagine just having to write something like cargo new rocket/app my-website, for example. I would rather have that out of Cargo and have something like [cookiecutter](https://github.com/audreyr/cookiecutter) on crates.io. Being able to ask questions during the setup to customise it is key, otherwise it's just a `git clone` that is harder to do.
What does ATC stand for?
One thing that comes to mind is that cargo still requires a somewhat stable internet connection which may present some difficulties to people in certain regions (or lifestyle settings, e.g. I sometimes stop coding on a train because I cannot `cargo test`).
Thanks for the references, Andrew! I'll definitely read through it.
Two small typos : &gt; Notice the removal of the generic type A and ~~it’s~~**its** ~~triat~**trait** bound. 
Re 'wrestling with the compiler', I've read the phrase 'sparring with the compiler' as a substitute, and I think that fits much better. It sure may feel as if you're fighting every now and then, but the compiler does it to make sure your code is fit to get out into this scary world. Re R.E.S.F., I always thought of us as a cheering squad for those writing Rust, and distinct from that stupid RiiR meme. So let's keep it that way. By the way, the latter meme should be eradicated where possible. And I fully agree, Rust isn't intimidating. If anything, it's enormously empowering, letting us deal with stuff that would be too intimidating in other languages with relative ease.
In C++ a TU is not necessarily a new file away (in many build systems this isn't the case). You need to specify somewhere in your build system how to compile that TU, how to compile its dependencies, and how to link it with the rest of your project. These are exactly the things you need to put in the Cargo.toml file of a crate. You can automate this significantly for small projects using globs and linking everything directly into the final binary (external libraries and all TUs), but this doesn't scale which is why no big C++ project does this (and why we have tools as powerful as CMake).
I watched that talk a long time ago but IIRC /u/chandlerc mentioned the following. First, the findsink function is unsafe (all C++ is unsafe), and it has a precondition "don't call me in graphs with cycles". Violating this pre-condition, however, does not invoke undefined behavior. Undefined behavior has a very precise meaning in C++, and some pre-condition stated in a comment of a library function isn't it. What findsink might do, is assume that the precondition is met, derive some other conditions from it, and do an operation that does invoke undefined behavior if those oder conditions are not met. So calling findsink, per se, does not invoke undefined behavior. However, findsink might invoke undefined behavior if, when the precondition is not met, it ends up dereferencing a nullptr for example. That's a very big difference. In Rust, `unsafe` code is still 99% statically checked. There are very few operations in unsafe code that are not checked (dereferencing a pointer, accessing a member of an untagged union, ...). These operations can be checked. So while in Rust you can still write an `unsafe findsink` function with a precondition, and call it when that pre-condition is violated, if `findsink` at some point tries to dereference a null-ptr, with instrumentation enabled you will get a `panic` with a nice error message and a backtrace (this happens already when running rust code over the MIR interpreter).
It is so unfortunate not having this in trait items. :(
Interesting article, thanks. I was pointed towards this by my own post (https://www.reddit.com/r/rust/comments/7pa85t/performance_analysis_rust_webassembly_plain/) where I observed no noticeable difference between WebAssembly and plain JS, so it's good to see another algorithm implemented in Rust that does indeed show an improvement in performance. Very interesting!
Thanks, it seems that performance currently indeed does vary wildly between different algorithms. It's very interesting to me; perhaps I'll add additional algorithms to my demonstration page in order to try to determine exactly where the JS bottlenecks start to appear. 
If you want to test whether a crate can work with core just add: #![no_std] use core as std; If the library uses only parts of std that are also available in core everything will compile just fine. Otherwise you will know where the problems are. 
Yes the current code coverage tools don't report code coverage information for doc-tests, that's a huge shame.
That's a point, agreed. And I'm in fact working on a solution for this since a few weeks :).
I understand why the borrowck complains, I've had that exact issue in my code as well. However, what I don't understand is how macros help in this case. Though, I must say, I barely understand macros anyways, and have never written one myself. 
As one piece of evidence, every official review for my book so far has mentioned that Rust does not have a reputation for being ready to learn. 
Smells like debug mode? Especially the last one, where Rust would be 7.5x slower than C. Since the article makes speed claims, it'd be _really_ helpful if the testing method would've been described. Successfully nerdsniped, will try to reproduce later on.
Dynamic types really need some polishing. During my refactoring of the Parity code, one thing that I really missed was the subtyping and coercion of references to a trait objects. For example, if I have a trait object `&amp;Foo` and `Foo: Bar + Baz`, I want to be able to pass `&amp;Foo` to a method where `&amp;Bar`, `&amp;Baz` or even `&amp;(Bar + Baz)` is accepted. Unfortunately, modern type system does not allow that. It works only statically. But once type was erased, it could not be casted and/or coerced anymore. And actually this is really tough problem, when you start some major refactoring. Basically it means, that once you've started using trait objects, you're not able to decouple the code anymore, and it tends to mutate into God object antipattern. You're forced to accept the whole trait object in a method that only needs one tiny trait, and so on. Of course, there is a solution to migrate to a static dispatch (and that's what I've done), but it may not be an easy task on a substantial codebase.
I agree that Systems Programming is a technical term, [but wikipedia is pretty spot on](https://en.wikipedia.org/wiki/System_programming): &gt; The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines and AAA video games, industrial automation, and software as a service applications).
**System programming** System programming (or systems programming) is the activity of programming computer system software. The primary distinguishing characteristic of systems programming when compared to application programming is that application programming aims to produce software which provides services to the user directly (e.g. word processor), whereas systems programming aims to produce software and software platforms which provide services to other software, are performance constrained, or both (e.g. operating systems, computational science applications, game engines and AAA video games, industrial automation, and software as a service applications). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt;If you try to compile with the wrong version you get an obscure error. Of course, this will always be an issue as long as breaking changes are made, and certainly having a compiler version constraint might sort of solve this, but the problem is that you can't know beforehand whether a new compiler version will break compilation of your particular crate. Even if there are breaking changes, chances are you may not actually be doing anything that would fail to compile. In order to ensure that it's always using a working compiler version, you'd basically have to just set a restriction to the current minor version (as is often done with crates) and then only update it after actually checking to make sure it compiles with a newer version. This would mean that you'd probably end up with a lot of crates just getting stuck on old compiler versions when they most likely could be using newer and better versions, just because the author hasn't gotten around to updating their Cargo.toml. (or made a new release after they did) Also, there's the issue of &gt;you're going to have output from different version compilers being linked into a single executable which would require 100% output compatibility between all the compiler versions involved. Overall, I just feel that the compiler is something that should be determined by the build system and that the same one should always be used for everything in a particular build.
Some of it ia fine, some of it (e.g ad related questions) seem to be quite random, but at least they are skipable. 
That's great to hear.
So what does profiling show? My guess is that most time is spent in the "bignum" libraries.
For default/optional parameters there is still a Builder pattern, which is sort of reasonable. 
I feel people like OP and you should really take a look at OCaml and/or Reason. They don't have affine types, but they would have many of the type system niceties you'd find in Rust, and should offer reasonably good performances.
&gt; Compared to which language is Rust hard to use? In which context/use case? Many people say things like "if you can't afford a GC, then Rust is worth it, but if you can, then there's no reason to use it."
I appreciate the work you put into MAY. For one thing , it can be good competition for the Tokio project. But the comparison between the mini http projects is not fair if you look at the code. 
The problem with function calls is that the borrow checker can't see what the function is doing. It only checks the function signature. If you use macros, you expand to regular code that the borrow checker actually analyzes. Here is a contrived example ([playpen](https://play.rust-lang.org/?gist=9570d7e1195249d19960ba900525c003&amp;version=nightly)): #[derive(Default)] struct Game { levels: Vec&lt;Level&gt;, level_index: usize, player: Player, } fn current_level(game: &amp;mut Game) -&gt; &amp;mut Level { &amp;mut game.levels[game.level_index] } macro_rules! current_level { ($game:expr) =&gt; { &amp;mut $game.levels[$game.level_index] } } #[derive(Default)] struct Player { hp: u32, } struct Level; impl Level { fn kill_enemy(&amp;mut self) {} } fn main() { let mut game = Game::default(); //let lvl = current_level(&amp;mut game); let lvl = current_level!(game); game.player.hp += 1; lvl.kill_enemy(); } If you replace the macro invocation with the function call, the borrow checker will complain about aliasing borrows.
We are using 2015 toolchain, I've tried removing "--verbose" with no luck. I'll try 2017 toolchain this evening. Thank you!
I've been thinking a little bit about a facet of this, because there is something I have had a hard time understanding: Rust is widely understood to be _hard_, but it is demonstrably _easier_ than C++. I wonder if it would be more accurate to describe Rust as _advanced_ instead, and if we could do a better job of advertising Rust as easy (caveats apply). Let me elaborate... First, Rust is demonstrably easier than C++: From the blogs describing Stylo and other Servo-initiatives, we know that Mozilla has struggled to improve the C++ code base of Firefox to such a degree that the easiest way forward was to invent a new language and write the improvements in that language. Okay, so some things are indeed easier to implement using Rust than C++. I have had the same experience: The guarantees of Rust (that the compiler mercilessly enforce) dramatically reduce the cognitive load of working with a code base, especially as it grows in size. (I could draw comparisons to other languages, but I have relevant experience with C++, and one example is enough to illustrate the point) So why the reputation of being hard? It seems to me that this comes from Rust using advanced and unfamiliar concepts, most notably lifetimes and possibly move-by-default. Traits are certainly unfamiliar to most programmers, but I don't think traits are the main cause of frustration for new users. How is Rust easy? Well, I think I don't need to convince you. You don't have to mentally track lifetimes when passing pointers around. It is convenient to move values (while possible in C++, it is tedious to do correctly in all appropriate scenarios). There's safety against data races. Could we do a better job of advertising Rust as easy, but advanced, rather than hard? (But let's try to steer clear of describing other languages as primitive or less advanced, so as not to create unnecessary conflict)
I mentioned in [another thread](https://www.reddit.com/r/rust/comments/7pdmgh/rust_in_2018_a_people_perspective_aaron_turon/) &gt; “You need to use a lifetime variable, they’re just like generics…” I find this an enormously confusing way to think about lifetimes unless you've already internalised how they work (in which case you don't _need_ an explanation).
That makes sense, thank you. So fn running_count(iter: impl Iterator&lt;Item = u32&gt;) -&gt; impl Iterator&lt;Item = u32&gt; is that same as fn running_count&lt;I: Iterator&lt;Item = u32&gt;, O: Iterator&lt;Item = u32&gt;&gt;(iter: I) -&gt; O
The post links to a repo that has the second two details: https://github.com/vmchale/ats-benchmarks
OCaml is quite nice, but I think Rust is nicer. Some reasons: * I think type-dispatched type classes are a better polymorphism system than ML modules or subtyping. * I think imperative programs are easier to read than functional ones ... as long as there's not a ton of shared mutation. * cargo * Better docs I suspect that if I used OCaml more, I would have more complaints. Whenever I dive into other language ecosystems I'm always shocked at how much Rust has gotten right that I take for granted.
Modify the binary offers so many opportunities. Instrumental tools are used in tons of fields. In my case (HPC), for micro-architecture simulations, C++ is THE language (for performance and classes-reusage reasons). Moreover, Rust could be a perfect alternative. In the academic research there is no need to reuse always old code, and every year new research projects are started, using Pin/DynamoRio/Others libraries (C++)... and guest which language is used?
&gt; I think type-dispatched type classes are a better polymorphism system than ML modules or subtyping. Fair enough. &gt; I think imperative programs are easier to read than functional ones ... as long as there's not a ton of shared mutation. OCaml supports imperative programming (I would expect Reason also does), it has (some) mutable collections, you can declare mutable fields on records, and it has mutable bindings of a sort in the form of ref cells. It also has (somewhat limited) `for` and `while` loops. &gt; cargo &amp; Better docs […] Whenever I dive into other language ecosystems I'm always shocked at how much Rust has gotten right that I take for granted. Also fair, but who better to fix the ecosystem than people for whom the language would be absolutely perfect if only the ecosystem were slightly better? ;)
I wouldn't say i feel passionate about it. I would be very pleased if it happened, but i am content with the current situation. I imagine the people working on the build tools have much more important things to do.
&gt; code coverage in cargo And not only line coverage, but ideally something like MC/DC (which is required for aviation &amp; automotive purposes). &gt; support compiling down to C This could be great for embedded devices - GCC is great and it supports more platforms than LLVM, but there are so many platforms that are only supported by dedicated Embedded compilers.
The spec is defined completely separately from the “host environment”. Non-web host environments exist in various capacities, for example, in Rust! The parity-wasm crate gives you a full interpreter you can embed in your Rust programs. Parity has done this because there’s been talk of replacing Solidity with wasm. The spec authors did it this way on purpose; for example, some imagine embedded devices, like consumer electronics, to be able to use WASM. My TV, for example, has a Roku inside; there’s no reason that they couldn’t use WASM as their application API, and get sandboxing, etc. I’m not sure how much of that is dreams vs reality today, though. But it’s possible. Super pie in the sky: I’ve been suggesting (and some have been writing) a Rust-based OS where userspace only runs wasm programs. Those are also super early...
&gt; And not only line coverage, but ideally something like MC/DC (which is required for aviation &amp; automotive purposes). I actually had in mind LLVM's code coverage instrumentation. It does not only detects which lines of codes are taken, but also which expressions are executed and their values. For example, if a boolean expression is executed in a branch, it tell you the number of times it evaluates to true or false, which is used to determine how often one branch was taken. This should be supported for branches / loops as well (and conditional control flow in general). Support for this already exists in LLVM and I think it would already be great if rustc/cargo would support this as well (generating code coverage reports from test should be as easy as `cargo doc`). LLVM supports more advanced use cases though (which integer values an expression evaluated to, and how often), to for example, drive fuzzers. 
It seems to me all you've done is show that a memoized `factorial` is faster than a non-memoized one. When you have silly results, find out why.
&gt; Ar modules (in Rust) compiled in parallel though? (No AFAIK) Nope. They are compiled serially but if you have a strong circular dependency there isn't really a way around this (at least not in C++, Rust, and most languages that need to know type layouts accurately to be able to generate any code).
Not quite. The later needs to get `O` since it can't infer it from the arguments. The former can get the concrete type from inside the function body with the power of `conservative_impl_trait`. `impl Trait` in arguments is just (very nice) sugar. `impl Trait` in the return types actually opens new possibilities.
OCaml was actually the first language I learned during my studies (I'm French, that might explain why ^^)
How will you debug a buffer overflow you don't even know is there?
Thin pointers, alloca, and inline assembly.
I have seen some occurrences where a more idiomatic Rust approach is available. For example, here: https://github.com/atomashpolskiy/rustface/blob/54005a03a54cc2382ef31eefc905a31c22c5a909/src/model/mod.rs#L117-L124 Using an (0..n).map(...).collect() is more desirable and short. Using such syntax we can utilize more of the underlying optimization without manually writing long code. In this case, the size_hint feature of the iterators provide the same capability as Vec preallocation.
&gt;Release mode probably makes the code a little bigger, due to inlining and if LTO is turned on or not. Not really. Debug build is much larger. Optimizations usually make code smaller.
I see, thanks for the reply!
&gt; Nope. They are compiled serially. But if you have a strong circular dependency, I don't know of any low-level language that is able to solve it in parallel. What do you mean by "strong"? Maybe the fundamental difference for our experiences is that your code is 99% templated, mine is about 1%. I've should said "non-templated C++" in the beginning ;) &gt; not even in the same translation unit ``` struct A; // forward decl struct B; // forward decl struct A { int a; void foo(); // template: must be in header file template &lt;typename T&gt; void bar(B&amp; b); }; struct B { int b; template&lt;typename T&gt; void foo(A&amp; a) { a.a = 2; } }; template &lt;typename T&gt; void A::bar(B&amp; b) { b.b = 1; } ```
thx
Thin pointers can be made safe (which is what the virtual enums proposal does, but the other proposals are safe as well). The proposed version of alloca was safe as well (lifetimes are enough to make it safe). Inline assembly cannot be made safe in practice, but such is life.
&gt; Well what about the cases where forward declaring does work (which are most of the cases in my experience)? Sure in some cases you can do this, I am just saying that you can't do this in general in C++ either (and with modules you can't do this at all). Rust crates just force you to design without "strong" circular dependencies from the very beginning. &gt; I find that your examples are a little handpicked. Sure, I built this particular example to exploit two-phase look up in C++ to make it fail (a way to make the two-TU solution above compile is to exploit two-phase look up a little bit more). &gt; If you look at real world C++ programs you'll find that most classes aren't templates I said it before but it seems that I must say it again. We have billions of lines of C++ code in the world. When you say many real world programs don't use templates, I agree. But that doesn't mean that when I say that many do use them I am wrong. You say Inkscape? I say Illustrator and Photoshop, check out the [Adobe Source Libraries](https://github.com/stlab/adobe_source_libraries), they are the masters of using templates everywhere. My point is not that you are wrong, because you aren't: many real C++ programs don't contain any single template. But the moment one of these programs uses the standard library for anything, they are using templates. And the standard library is mostly templates, and examples of circular dependencies like the one I picked above do happen inside the standard library, which is one of the reasons why it is hard to modularize it. And probably one of the reasons why the Modules TS takes so long (if the standard library developers can't use modules properly, what are the chances that anybody else can?). 
I figured it out; the link is the same color as the background text on my machine, so my brain skipped right over it. I even looked so hard :(
Great. Thanks!
&gt; of all Rust code, not code in a repo. (e.g, 90% of all Rust code is probably Safe Rust). Still, this doesn't sound right to me. I'd image most code bases doesn't even use `unsafe`, and those that do, with the exception of FFI wrappers, may use about 10-20%. I also imagine that the former category is way larger than the latter, so it evens out to maybe 99% safe rust. Again, this is just my general intuition (which, I guess, is similar to your numbers). Of course, you are right in that unsafe code isn't just locally unsafe. So if we count the entire `vec.rs` as unsafe since bad stuff happens if we mess up the `capacity` we'd get very different numbers. But now we're venturing into "unsafe vs unsafe" land, which /u/manishearth [wrote about](https://manishearth.github.io/blog/2017/12/24/undefined-vs-unsafe-in-rust/) a few weeks ago. It makes me wonder if it'd be helpful to have unsafe fields in structs that require unsafe to write to them, but not read. 
&gt; reasonably good performances Here's an off-topic question, sorry ;) I simply have to ask. English is not my native language so I simply don't know the answer to this but I find "good performances" to sound strange? I'm asking because I've seen the same expression all over this subredit. Performances...Performance?
Perhaps wie should focus our (planned) trait-objects/generics clippy lint to trigger in generics in error-bearing code (e.g. when a `Result::Err(_)` is in scope), and trait object dispatch otherwise? CC /u/Manishearth
The Rust factorial looks equivalent to a memoized one to me? There's no recursion there.
They are both memoized locally, but Haskell's is memoized globally. `factorials` is filled lazily and is allowed to stay live indefinitely.
Even knowing the link was there, I finally had to look at the HTML source for the page and search for "ithub" to find it. Unmarked links are just a bad idea, unless you're hiding Easter Eggs.
This could be nice as `cargo-cookiecutter`, to make it feel like part of the native workflow.
This looks great! Is the Hogwild implementation actually safe, though? ``` impl&lt;'value&gt; DataInput&lt;&amp;'value Arr&gt; for Variable&lt;ParameterNode&gt; { fn set_value(&amp;self, value: &amp;Arr) { let param_value = unsafe { &amp;mut *(&amp;self.node.value.deref().value as *const Arr as *mut Arr) }; param_value.assign(value) } ``` Looks like total UB; modifying a `non-UnsafeCell` type through a shared ref. Furthermore, by doing `unsafe impl Sync for HogwildParameter` there's even more UB from unsynchronised access from multiple threads to the same memory location. I have previously implemented Hogwild using `&amp;[AtomicU64]` and `Ordering::Relaxed` i.e. 1. `load` two consecutive `AtomicU64`-s at a time (using `Ordering::Relaxed`) 2. Cast them to four `f32`-s using `f32::from_bits`. 3. Put the four `f32`-s into a `f32x4`. 4. Perform the SIMD operation (i.e. add the gradient). 5. Convert the `f32x4` back to two `u64`. 6. `store` the two `u64`-s back (using `Ordering::Relaxed`) This solution has no UB and is still pretty fast (the relaxed loads and stores don't really impose any overhead). Though, strictly speaking, Hogwild assumes `x += y` is atomic, right? So I also tried doing the steps in a CAS-loop, with the final store in step 6 being replaced with a `compare_exchange`. However, I didn't notice any improvement (in a word embedding model), while at the same time making the thing about 2x slower.
I think this only holds true if you can assume that the vast majority of the ecosystem will move to the new epoch relatively quickly. Otherwise you end up with people having to learn the rules for N different epochs, and know them all if they want to understand 3rd party code that they are interacting with. And even if you know all the rules, then you need to know which epoch which library is in. That's fine if there's say 2 or 3 epochs over a 10 year period, and to a large extent it's fine regardless of how big the changes are. But it's not really fine if there are loads and loads of epochs.
Interesting. I see two problems with non-atomic writes: 1. If the write is not a single instruction (as would be the case, I believe, for any types larger than 4 bytes), other threads may read nonsense values (when one half of the value has been written but not the other). 2. If the writes are not atomic, threads will have stale reads. If (1) is the case, Hogwild will immediately diverge, and the results will be nonsense. I believe that while the original paper mentions atomic writes, (2) is in actuality not a large problem for the algorithm. This is definitely true for sparse updates (because there will rarely be concurrent reads and writes of the same parameters). I suspect that this is also true for dense parameter updates, but I haven't verified this empirically. Do you have your source somewhere so that I could use it for inspiration?
The `factorial` implementation has two issues. The first is that there is an off-by-one error, and outputs F0 = 1, F1 = 1, F2 = 2, ..., when in fact it should be F0 = 0, F1 = 1, F2 = 1, ... The second is that `std::mem::replace` should not be required, and it can be made a little faster with a small change: extern crate ramp; use ramp::Int; pub fn fibonacci_ramp(mut i: u64) -&gt; Int { match i { 0 =&gt; Int::from(0), 1 =&gt; Int::from(1), _ =&gt; { let mut n0 = Int::from(0); let mut n1 = Int::from(1); while i != 0 { n1 += &amp;n0; if i == 1 { return n1; } n0 += &amp;n1; i = i - 2; } n0 } } } 
An excellent point. Ramp implements AddAssign, which reuses storage; I wonder how using += would change the benchmark?
My vision is that currently dynamic dispatch is suitable for, well, dynamic covariant things like small handlers, heterogeneous collections and so on. If you keep such objects under control all would be fine, hopefully. But we all know, how legacy code may bloat after a series of last minute changes and some "temporary" hacks here and there. But you need to think twice, if you decide to make major part of the architecture dynamic.
Yes, I more or less know the reasoning behind current implementation and its limitations. I followed RFCs some time ago. Just wanted to ask, what's the current status of fat pointer idea? The way I see it, aforementioned issue would be completely solved if `&amp;Foo` would be a fat pointer that links to both vtables. As far as I understand, the problem is exactly what vtables pass where and how should we decide what we may omit. Coherence rules and transitive dependencies may be a pain too.
I liked someone else's reaction to similar view: "Why everything has to suck at something?" Currently I see only three cases in which Rust doesn't work well: * One liners or few liners (if the script is being too complicated, it becomes difficult to maintain it without types) * Existing code * Quick prototyping (usually for task that needs to be performed only once)
&gt; The first is that there is an off-by-one error, and outputs F0 = 1, F1 = 1, F2 = 2, ..., when in fact it should be F0 = 0, F1 = 1, F2 = 1, ... What do you mean ? `0! = 1` by convention, and `2! = 2` obviously (`2x1 = 2`). 
&gt; should the needs of Rust users in Brazil differ from the needs of Rust users in India? This. Borders are lines drawn on map by politicians. We need to not get distracted by this.
AFAIK create is a compilation unit, while TUs in rust are `.rs` file.
Sounds reasonable! Honestly I do not know enough on LLVMs coverage feature to comment (the documentation seems sparse). I just wanted to mention other kinds of coverage because it seems that lots of tools do not go beyond statement/line coverage and for many purposes this is not nearly enough :)
&gt; AFAIK create is a compilation unit, while TUs in rust are .rs file. This is incorrect :/
&gt; The issue underlying this is deep to how Rust’s ownership system interacts with async code, and the current implementation of generators doesn’t resolve it. In the current, unstable generator system, users cannot use references across yield points. This results in the same sort of Arc and clone everything mess that futures sees today. Before we can consider generators and async/await shipped, I think it is imperative that we solve this problem. Users must be able to program in generators/async functions using the same patterns they use in regular functions. **thumbs up** You are the first to point this out of the #rust2018 blogposts. This is spot on and especially the last sentence. Asysc/sync programming should compose seamlessly - unlike the current situation with tokio/futures/generators. Are you aware of alternative approaches to async programming for rust? Is anyone exploring the space? I have been looking into async programming across different languages and Kotlin seems to have a really good solution to [coroutines](https://github.com/Kotlin/kotlin-coroutines/blob/master/kotlin-coroutines-informal.md). They provide a lower level primitive (suspendible function) which can interface with futures, async/await, generators, etc without committing to specific library or framework. If tailcalls become a thing in Rust, do you think that such an implementation would be viable?
There is also [quasar](https://github.com/anowell/quasar)
You are quite right, thank you for the explanation. Would I be correct in thinking that wrapping the parameters in a `RefCell` and later unsafe pointer dereference (from `*mut Arr` to `&amp;mut Arr`) will head off the UB concern?
&gt; Modules aren't part of C++ yet. I checked and the Modules TS is in PDTS, so it will be part of C++ in March this year. Also, not being part of the standard doesn't mean that you can't use it. Its implemented in one way or another in all three major compilers (GCC, Clang, and MSVC). Like I said, I've been using modules every day since 2012, in 6 years, very little has changed (which I guess is a good thing). &gt; When your class uses templates No, this applies to template functions and variable templates as well. It is not restricted to classes. &gt; you can still split it up in hpp and cpp. You can put some function templates specializations in a cpp file to speed up compilation times, but you still have to put all the code in the header files if you want it to know with types unknown to the current translation unit. &gt; That's exactly the attitude not bringing Rust forward in 2018. No, that's the attitude one should have when forced to repeat the same argument in 100 different ways to people spreading FUD. FUD should not remain unanswered. I have showed you Rust code and C++ code doing the same thing, with the same number of lines of code, working at the same level of abstraction, producing the same results, and which Rust compiles an order of magnitude faster than C++. You keep saying without providing any proof that Rust compiles slower than C++ for you. Yet when I told you the "big secret" behind fast Rust compile-times (doing what you do in C++) your answer can be summarized as "I am too lazy to do that _in Rust_" while admitting that you are doing it in C++. So yes, - if you know how to speed up Rust compile-times, - are willing to do that work in C++ but not in Rust even though it takes roughly the same amount of work, - use those bad compile times that you are getting on purpose to extrapolate to all Rust code, - and then use those fictitious extrapolations to spread FUDs about Rust compile-times to fellow C++ developers then I must answer bluntly and I stand by my words: your compile-time issues are your own fault. Please stop spreading FUD.
Oh, I see. You're concerned that the Haskell microbenchmark framework might not be implemented properly and is just getting the cached result after the first time it calls the microbenchmark. Good point.
Wouldn't it be more rustish to make the collection mutable with a cheap clone function? (and then optionally add a thin wrapper offering a functional interface which first clones, mutates the clone and returns the new version)
`insert` takes `T` because `T` could be `&amp;U` and it should be the developer who decides whether a `HashSet` is responsible for deallocating its contents when it goes out of scope. If `insert` took `&amp;T`, then there would be no way for a `HashMap` to hold responsibility for deallocating the values it holds. If `contains` took `T`, then it would *have* to take over ownership of whatever you call it on, which would mean that you'd have to explicitly `.clone()` values which don't do implicit copies (types which don't implement the `Copy` marker trait) and wouldn't be able to call `contains` on un-cloneable values without the `contains` taking ownership and deallocating them.
There are multiple conventions out there for indexing the Fibonacci numbers. I don't really see this as a bug.
Yes, but that includes building GGEZ, which has a heck of a lot of transitive dependencies and is fairly large itself.
Maybe, but it is a "less persistent" style of using the data structures, which is my preference.
Yep, I have been clarified by some people. I didn't know that rust is building the whole crate as a single file because my intuition from others languages is wrong here, e.g. `A C# program consists of one or more compilation units, each contained in a separate source file.` Anyway, we don't have to compare Rust and C++ code in `fair competition`, we have to compare idiomatic Rust and C++ code instead. If C++ idiomatic code is splitted on multiple TUs and is compiled fast while Rust code is often a single crate and it compiles slow, than we can't say "but it works for splitted subcrates". Probably, but it's not a realistic example
Is there anyway to effectively use the new `?` syntax inside tests? Specifically, I ran into a case of mod tests { #[test] fn test_something() { let val: Value = serde_json::from_str(r#"{ "foo": "bar", "baz": ["qux"] }"#)?; // what is this, perl? (/s) assert_eq!(my_api_fn(val)?, "bar"); } } Yet apparently this doesn't work, even though I believe you can use `?` directly inside of `main`. Essentially, what I'd like to happen is if the `?` fails then the test fails. Pretty much something that wraps the test in a check like fn assert_result&lt;T, E: Display&gt;(result: Result&lt;T, E&gt;) -&gt; () { match result { Error(err) =&gt; assert!(False, "Try failed with Error({})", err)), _ =&gt; (), } } This just seems like something the compiler could handle pretty easily, but with all the bells and whistles around the more generic `?` that I don't fully understand yet. Any thoughts from the community of how to do this more easily than a nested function? #[test] fn test_something() { fn work() -&gt; serde_json::Result&lt;String&gt; { let val: Value = serde_json::from_str(...)?; my_api_fn(val)? } assert_eq!(work(), Ok("bar")); } This gets annoying fast.
My problem with trait objects are that they're not fun to work with in collections.
&gt; Why everything has to suck at something? That's a matter for philosophers. Observationally though, it does seem to be the case at least almost all of the time, so people are rightfully suspicious of claims to be a special case.
Juice and coaster crates are continuation of leaf efforts, but not super active.
Nice!
&gt; If you are someone who would consider using Rust in production, or advocating for your workplace to use Rust in production, I’d like to know how we could help. Are there specific features or workflows you need? We started using Rust 3 months ago for a bespoke on demand video transcoding server. It's great! I'm totally in love with the borrow checker and compile time safety checks of concurrency. I'll recommend Rust to anyone that want to write performant code that is close to system. Here are my biggest issues: * Async is not core. Sure there are core Rust devs involved in Tokio, but I'd like to see async (not async IO i.e. Tokio specifically), embraced and integrated much more into Rust's roadmap. I want futures, syntactic sugar, docs, examples, best practices etc etc. We opted for a synchronous thread based solution in the end, partly because we couldn't figure out futures (without IO, i.e. not Tokio) and partly because it didn't seem to be ready enough. * Rust compilations are not Docker friendly. Whether I like it or not, I must run our services in a docker container. To get performant docker builds, we really need to use docker's layer caching, and cargo isn't very helpful here (sure there are workarounds, but it's not nice). There may be an argument for considering this as a parameter when working on incremental compilations. Another starting point is here https://github.com/rust-lang/cargo/issues/2644 * RLS breaks down when the complexity of the project grows. I've just recently tried to do some contributions to RLS to see if I could help out, and I know it's early days for RLS, though I suspect this is a road blocker for big companies trying out Rust. * Build times. Enough said... Regardless. I'm thrilled about Rust. Rust core devs have brought fun back into coding and are my heroes &lt;3
Can you provide any example? Let's say "here is Rust code and we don't know if it's UB. Here is C code that violates some rule and I have following transformation ruleы to make it correct. We apply this and this and viola, this code is safe because of X, Y and Z"
Glad to know it wasn't just me wondering this. How is fn f(x: impl X) -&gt; impl X different from fn f&lt;T: X, U: X&gt;(x: T) -&gt; U 
Maybe feature rich rather than advanced? This way it makes it harder to do language bashing. Being feature rich means you can do more with less but it doesn't say you can't do it at all in another language.
Fixed, thanks!
Yes, my experience has shown that these are correlated. When an upgrade is easy, people do it quite often. When it's difficult, they lag behind forever. That is, of course, only my experience, but I've seen it in a number of languages.
Some version will for sure be available eventually! It's a really big design space with a lot of potential interactions. Once [RFC 2071](https://github.com/rust-lang/rfcs/pull/2071) is implemented you'll be able to return existentials from trait functions. We want to get experience seeing how people use existentials inside traits before we introduce `-&gt; impl Trait` in trait functions-- it helps to inform a lot of the design decisions. For a sketch of what full-blown "`impl Trait` in traits" might look like, see my [doc here](https://github.com/cramertj/impl-trait-goals/blob/impl-trait-in-traits/0000-impl-trait-in-traits.md).
This is one thing I felt was missing from Rust standard, coming from Scala and JS. I was trained to always write as immutably as possible. Vec doesn't supply any way to manipulate data without keeping the original Vec intact and returning a new Vec, even to append an element or concat an array. I suppose it doesn't align idiomatically with the way Rust is commonly done (references and pointers). It's probably just as well it's not in std lib, instead able to be provided externally.
Some random comments: &gt; I believe it should be possible to set some expiration date on each RFC and then have it remind itself (by a bot), and make a decision to YES! &gt; An incomplete idea, but would it make sense to ask the companies in the „Friends of rust“, if they could each provide one and collect them? Something to show the other companies why they want to invest in Rust ‒ something more than 2 sentences or story „We did this great thing… and by the way, we used Rust on the way“. Something to highlight why it was Rust, how it helped, but also where the pain was. We are actually working on this, expect to hear more soon.
You can however, study the source code to get an understanding how it works which might help you in rolling your own :)
I've obviously been wrestling with these questions. The fundamental issue on macOS is that the platform effectively provides only Objective-C and Swift bindings. So there are two paths forward - writing code using the native platform methods but in a different language, which means the pain of cross-language bindings, or dealing with the "uncanny valley" of using a cross-platform GUI toolkit. It's interesting that the story is very different on Windows, the native API is C with a thin layer of object dispatch (COM), which Rust wraps quite nicely. I'm doing xi-win (currently on hold for complex reasons) in Rust. I _do_ think there is an opportunity to build a good cross-platform in Rust, but it's an enormous undertaking. Drawing pixels is the easy part, the slog is integrating deeply with preferences and configuration, making accessibility work, and all those tiny polish details. One reason to be encouraged, though, is that the "native" GUI toolkits on both mac and Windows are very underperformant compared with what's possible.
I always thought something like this should exist. Are you considering adding finger trees?
&gt; Are you aware of alternative approaches to async programming for rust? Is anyone exploring the space? In the article, boats mentions that they "plan to write more about how this could happen over the rest of January." There's a lot of work being done on this right now, and I'm super excited about some of the ideas.
How does one mark an (otherwise empty) struct as non-`Send`? I tried: impl !Send for UI {} But got an error telling me that negative impls aren't finished and that I should use "marker types". The only marker type I know of is `PhantomData`, and I'm not sure how that's applicable here.
Using these as part of an enum. Wondering how one can `#[derive(Hash)]` on that enum when one of the possible values are the HashTrieMap. Maybe I'm just misunderstanding this... It gives me ``` error[E0277]: the trait bound `rpds::HashTrieMap&lt;MyEnum, MyEnum&gt;: std::hash::Hash` is not satisfied ``` Not sure how one can access [this function](https://github.com/orium/rpds/blob/master/src/map/hash_trie_map/mod.rs#L170) by just calling it 🤔
Yes, that's `-Ztime-passes` but it's output can be quite misleading these days because the compiler computes so many things only on demand, making those things add time to "passes" that don't have anything to do with them. E.g. part of MIR opimization actually happen during "metadata export".
I'll take it `:)`
Enjoy :)
It's really ugly, and I would not recommend it, but just for fun: you _could_ use a block as an expression to do an assignment in a conditional. https://play.rust-lang.org/?gist=f21a5c56a1c73b4469350adbcd250eb8&amp;version=undefined
Oh I do agree that a good story in unsafe in necessary. I think there are two points here: - unsafe operations should be easy to use correctly, and possibly harder to use incorrectly, - unsafe operations should be linted &amp; validated (see the work on MIRI &amp; unsafe guidelines). That is, on the one hand make it easy to do the right thing, and the other double-check that the right think was done as much as possible.
To explain why `insert` takes a `T` instead of an `&amp;T`: Imagine a `HashMap&lt;&amp;str, &amp;str&gt;`. That map works great for holding static strings. But it gets tricky as soon when we start using references to local strings: let mystr: String = read_some_input(); mymap.insert(&amp;mystr); The compiler's ok with that so far (`&amp;String` coerces to `&amp;str`). But Rust knows that `mymap` can no longer safely outlive `mystr`, because it's holding a reference to it. And because of that lifetime restriction, it's impossible to return `mymap` from the current function. Unless the map your using is very temporary, these lifetime restrictions usually mean you don't want to put references in them. Most long-lived maps of strings are `HashMap&lt;String, String&gt;` for that reason; they own their contents, and so they don't have to worry about lifetime issues. The downside is that inserting a `&amp;str` into such a map requires calling `to_string`, which makes a copy of it. And so, if `insert` took a `&amp;T` in general, that would mean that either 1) all insertions would have to worry about lifetime issues, or 2) the map would have to implicitly clone everything you inserted into it. Not all types can be cloned, and it's important to let the caller control these things anyway, so `insert` just takes a `T`. To explain why `get` takes an `&amp;T`: Unlike `insert` above, the argument to `get` doesn't stick around in the map. If I have my `HashMap&lt;String, String&gt;`, I'd like to be able to call `mymap.get("foo")`. Now `"foo"` in this case is a `&amp;'static str`, but it could also be a reference to a local string. The question is, is there any reason to make a copy of `"foo"`? No! The map isn't going to store it. The argument to `get` doesn't have to last any longer than that one function call, so using references there doesn't impose any restrictions on what happens after. Making a copy would be wasteful, so `get` takes a reference. (And as we noted above, not all types can be cop This gets at the core difference between `insert` and `get`. With `insert`, the argument you give it is going to _stick around_ inside the map. It's not safe for the map to outlive something that's been inserted into it. But with `get`, the argument doesn't stick around, so any lifetime is fine. It's a little tricky to see how these rules are baked into the signatures of [`HashMap`'s methods](https://doc.rust-lang.org/std/collections/struct.HashMap.html#method.get); you might want to read up on the rules for [lifetime elision](https://doc.rust-lang.org/book/second-edition/ch10-03-lifetime-syntax.html#lifetime-elision). Note that in the specific case of `&amp;str` vs `&amp;String`, the `Borrow` trait helps perform that conversion. It might've been nicer to pick an example type that avoided that complicating detail, but `String` is really by far the most common reason you'll have to worry about `HashMap` lifetimes, so I stuck with it.
I'm finishing the API and documenting [libui-rs](https://github.com/LeoTindall/libui-rs/), a cross-platform wrapper that calls into native GUI APIs. I'm hoping, once the base API is done, to take some inspiration from /u/antoyo 's awesome work on [relm](https://github.com/antoyo/relm) and make a functional API. Since there aren't a lot of fancy widgets available in libui - it's the least common denominator of Window, Mac, and GTK+ widgets - making an (almost) fully functional interface to each of them should be not too difficult.
Woot, crate of the week. Thanks guys! Note: the beta release (install through `cargo install artifact-app`) is _super_ rough. I'm in the process of doing a complete rewrite from the ground up (and nearly done with the ground!) and the beta release was my MVP to make sure the new features I'm adding were actually useful (they are!). I started this project when I was *first learning rust*, so there are a bunch of lessons learned that needed to be applied to create better software -- such as libraries I should have been leveraging and fuzz testing.
"including a book with your project" is a frikkan AWESOME idea. It's basically what I already to with [artifact](https://github.com/vitiral/artifact) and I would LOVE to be able to manage the book (which serves as basically a tutorial for the project) with the project itself. Awesome writeup!
I think this could be done in multiple steps: 1. Code-signing, 2. Review-signing. That is, first I'd like to know that whatever I get is *really* released by the original author: - not by a 3rd party (I hate that NPM allows re-publishing deleted packages), - not mangled during transfer, - not hi-jacked by the repository (source-forge injecting malware in the binaries they hosted...). Note that if each author of a crate signs their crate, you can build a white-list of authorized author, which would trigger a warning should anyone *else* publish the crate. Secondly, I'd like if *other* people could chime in: "I've reviewed the crate, LGTM, here's my signature of it". I don't see a problem in allowing anyone to review, but I'd present the signatures in order of "popularity". That is, an author of an oft downloaded crate is already trusted by many people, so their review that the code is trustworthy carries more weight (ie, if I had to compute a review score, I'd sum the downloads of the reviewers' packages over the month prior to their review). Both rely on public/private key schemes, which require managing key rings and publicizing them. As always, that's the tricky part...
I think you can put a PhantomData&lt;* const ()&gt; to remove Send, but I'm not 100% sure
One could say you're rather persistent on keeping it this way.
I would prefer off-by-default, since it's the most secure setting. I am fine with allowing to turn them on, but the documentation should be clear about the risks of the particular feature (me or my clients? DOS, leak or hijack? ...). There are multiple I/O issues, if I remember correctly: - network download (schemas, DTD, ...): allow for a vector of content injection which may trigger a parser vulnerability, signal your presence, ... - file system access (not every URI is a network access!): allow for including local files' content into a document potentially presented to the public (`~/home/.ssh/id_rsa` please...). I may be forgetting some, of course.
This will be enabled by [RFC 1937](https://github.com/rust-lang/rfcs/blob/master/text/1937-ques-in-main.md) which is currently being implemented. For now you can use a macro, probably? Something like this: macro_rules! test_result { ($body:expr, $assert:expr) =&gt; { #[test] fn test_something() { fn work() -&gt; serde_json::Result&lt;String&gt; { $body } let result = work(); assert_eq!(result, $assert); } } } test_result!({ let val: Value = serde_json::from_str("{}")?; my_api_fn(val)? }, Ok("bar"));
I've made some Python bindings to this: https://github.com/torchbox/rustface-py
I don't know enough about them, but they seem to have some nice properties such as efficient catenation, so I will definitely look into them.
For example, is this implementation of split at mut legal? impl [T] { pub fn split_at_mut(&amp;mut self, mid: usize) -&gt; (&amp;mut [T], &amp;mut [T]) { let copy: &amp;mut [T] = unsafe { &amp;mut *(self as *mut _) }; let left = &amp;mut self[0..mid]; let right = &amp;mut copy[mid..]; (left, right) } } both `copy` and `self` are `&amp;mut [T]` slices to the same memory at the same time. Is this code legal? Niko believes it should be legal, but right now we don't really know: having two unique references (&amp;mut must be unique) to the same data makes them not unique anymore, and if we tell LLVM when generating code that they are unique, LLVM might break your program while optimizing. Check out niko's blog posts for discussion about this: http://smallcultfollowing.com/babysteps/ in reverse order (read from bottom to top): - Observational equivalence and unsafe cod - Thoughts on trusting types and unsafe code - 'Tootsie Pop' Followup - The 'Tootsie Pop' model for unsafe code - Unsafe abstractions If you want concrete code examples, the memory order repo contains many in the issues taged with K-CodeExample: https://github.com/nikomatsakis/rust-memory-model/issues?q=is%3Aissue+is%3Aopen+label%3AK-Code-Example
Though, in this case I'd probably do this instead: #[test] fn test_something() { let val = serde_json::from_str(r#"{ "foo": "bar", "baz": ["qux"] }"#).unwrap(); // Test fails if this panics assert_eq!(my_api_fn(val), Ok("bar")); }
Why would you not recommend it? "Just" because it's ugly? Because I think I'll use this, thanks ;)
It would be nice, honestly, if there was some sort of dashboard/indicator/whatever that lists out the state of all currently accepted unstable RFCs. Ideally, with links to any issues which are preventing the RFCs from stabilizing. I feel like I read the thousand accepted RFCs and just have no clue what the current status of them is. Even for really popular ones like NLL.
Thanks, that loop might be feasible, I'll think about that. `ges` does not contain anything I can use here, it's just a C-style enum member that carries mainly this method.
Please tell me then how I can do TU 1: class B; class A { B* b; }; TU 2: class A; class B { A* a; }; in Rust? You assume a lot of stuff about me, which I find very rude.
We have the unstable book, which is organized by feature gate, and links to each issue tracking stabilization of that gate. Close, though not exactly what you're asking for.
&gt; since you already need a functioning toolchain for that platform in c before you can use it with rust. Well, if it's pure Rust, all you need is a linker, not a full toolchain. That said, I do think that this is an area we should eventually push on; I'm not sure if it's this year or later, exactly.
Did not know about this. Definitely nice to be able to see things in the pipeline. I guess I'm not really a rust team member though, so I don't know if ya'll would find it useful. However, it is something that seems like it would be important to have. With so many in flight RFCs, it seems like it would be useful to just be able to say "This isn't out because it needs xyz done". Even if xyz is "Sit in nightly for 6 more weeks".
&gt; Do I have to explicitly add this field to each variant or is their an abstraction to apply it to every variant. The former. &gt; it wasn't apparent to me how to pattern match a grouping of structs Yeah, you basically can't, unelss of course, your structs are variants of an enum, heh.
&gt; It's interesting that the story is very different on Windows, the native API is C with a thin layer of object dispatch (COM), which Rust wraps quite nicely Objective C and Swift are harder to wrap than C, but I always assumed that it would be possible to provide Rust wrappers around MacOS native libraries, although the effort might not be worth it.
Oh, that's because I think there are probably more concise and/or idiomatic ways to write such logic; I would personally prefer something like what /u/droidlogician suggested. But it all kind of depends on the context and what you're trying to do (see: [XY Problem](http://xyproblem.info/)). And hey, it's your code, do whatever you want. :p
Thanks, I'll take a look at that! I'm still pretty new to Rust, so tips and pointers are very helpful.
Yes. I agree that the best way is to show everything by default and disable by flags.
&gt; This is one thing I felt was missing from Rust standard, coming from Scala and JS. I was trained to always write as immutably as possible This is sort of a philosophical difference. It's often phrased like this: &gt; We all agree that shared mutable state is the root of all evil. But you need both parts: shared state that's immutable is okay, mutable state that's not shared is okay. Most languages take the former path, but Rust takes the latter path.
As I see it, the goals aren't about stopping the design of the language, but rather slowing down for a year. Imagine it like a racecar taking a pit stop. Sure, it's not moving during that time in there, but the changes that happen during it help it move faster after it gets out.
Great job on the docs! I love seeing tables like this: https://docs.rs/rpds/0.3.0/rpds/map/red_black_tree_map/struct.RedBlackTreeMap.html
Of course the needs differ. There are language barriers. Community composition. Preferred methods of learning (as someone who has studied in both the US and India, the approaches to education are very different). These communities will likely grow their own corpuses of resources and we should help them. There currently is a growing Chinese community that is almost completely decoupled from the "main" community because of the language barrier, for example.
Who said anything about borders? Borders may be drawn by politicians but regions are regions. Languages differ. Culture differs. Lots of things are different in different regions.
At the same time, as long as the compiler has helpful notes every time you stray, it's easy to correct the issues :)
What is the difference between fn do_something&lt;I: IntoIterator&lt;Item=u32&gt;&gt;(xs: I) -&gt; () and fn do_something(xs: impl IntoIterator&lt;Item=u32&gt;) -&gt; () ?
Thanks for the help, I was expecting this to be the case but I just wanted to be absolutely sure before I refactor ha!
Thanks, this definitely looks much cleaner! Though, in my case it probably won't work, because the map() function would contain a call to read_i32(), which depends on the order of iteration
Nice! Hope you don't mind if I put a link to your repo in the readme
I love the scrappiness of our community and how all the different crates and projects feed back into one another. I really hope that's something we can retain as the community grows larger and core libraries start to stabilize.
Well, that RFC is exactly what I want. Too bad it hasn't landed yet. I guess I'll keep doing it the harder way until it lands, and do my best to follow the progress of that issue. Thanks! 
When I think of all those data-races and iterator invalidation those people suffer :(
Yep. The author should try benchmarking something like this and see how that fares: factorial :: Int -&gt; Integer factorial n = last factorials where factorials = 1 : 1 : zipWith (*) [2..fromIntegral n] (tail factorials)
Link only redirects me to "Oops! That page can’t be found".
If you still want to go down the enum/struct road, pattern (matching) might help you out with common fields. If you'd want to make it more ergnomic, you could implement `Into&lt;Baz&gt;` from std. This way, your methods operating on commong fields could accept some `T: Into&lt;Baz&gt;` Some pattern matching code: https://play.rust-lang.org/?gist=7724c7c3a3512533a8440321881f2929&amp;version=stable Judging if its good design still depends on your specific design problem. You should also evaluate if accessor methods in a trait might work out for you.
Nice. I've used [im-rs](https://github.com/bodil/im-rs) before, but this one is not GPL licensed. MPL is much better :)
A related idea would be to add mutable versions of the various methods that mutate in-place with a mutable reference (which is safe because a mutable reference implies you're the sole owner). Raph Levien does something similar with his rope implementation as described in [his talk on the Xi editor](https://www.youtube.com/watch?v=SKtQgFBRUvQ).
I would appreciate work on dynamic types. There are a few things I'd like to be able to code, but seem currently impossible in Rust (even using `unsafe`). Most of it boils down to "it's `!Sized`, all bets are off", coupled with the inability to separate the data from the v-ptr in a trait (and reassemble them!). So, in increasing order of difficulty: 1. Rust supports using an Unsized type as the last member of a `struct`. It'd be great if this was extended so that *multiple* Unsized members could be supported. 2. It would be great if Unsized types could be regular values: - allocated on the stack, - passed as arguments, - returned by functions. 3. It would be great if it was possible to query the size and alignment of an Unsized value, to prepare a memory allocation for it. 4. It would be great if it was possible to tease apart Unsized values (like decouple the virtual pointer(s) from the data, decouple the size of the slice from the slice itself, ...) and of course be able to reassemble them. This would be `unsafe`, and that's just fine. In the end, I'd like to be able to implement a `DynHashMap&lt;KeyTrait, ValueTrait&gt;` which uses a *single* memory allocation: 1. A sparse array of N HashResidue in u8 (see Google's fast_hash), 2. A sparse array of N (Hash, Offset) in usize, 3. A dense array of size M &lt; N pairs of (KeyTrait, ValueTrait) laid out one after the other, with potential holes in the middle. *Note: to the question but how do you update it, the answer is that you only update "in place" if there is place, otherwise you push the dynamically sized element to the end, leaving a hole. On growing the allocation, you re-compact.* The following can be implemented in C++, in Rust I have failed so far. Point (4) allows potential savings in two cases: - the data-type is smaller than a pointer (eg, `u32`), in which case storing v-ptrs and data separately shave off padding at the end of the structure. - the data-type is aligned more strictly than a pointer (eg, 16-bytes because of a SIMD vector or 64-bytes because it's cache-line aligned), in which case storing v-ptrs and data separately shave off padding between v-ptr and data. A separate data-structure for splits is better, as this has an impact on the number of cache-lines to touch to access an element.
Sure, thanks!
Depends on an amount of inlining. 
The second function is essential syntax sugar for the first one. See my response in the IRLO thread for more detail, or check out the relevant section of [RFC 1951](https://github.com/rust-lang/rfcs/blob/master/text/1951-expand-impl-trait.md#motivation-for-expanding-to-argument-position).
Thanks!
This is the solution I'm familiar with. It does remove Send and Sync: playground link: https://play.rust-lang.org/?gist=7063b302cc516bf8e1a4d041ea190ffe&amp;version=stable
100% agree that we shouldn't shut down new things! I'm all for both getting some things stabilized, and still not closing off new features and new ideas.
&gt; If you are someone who would consider using Rust in production, or advocating for your workplace to use Rust in production, I’d like to know how we could help. Are there specific features or workflows you need? For me, the absence of value generic parameters is a blocker for my professional needs. In order to avoid memory allocations, we use many structs parameterized by one (or several) sizes which generally end up as an array size (but sometimes end up as an alignment...). An ability to more easily manipulated dynamic types, for type-erasure enabled collections, would also be welcome, but is less keenly felt.
It is worth pointing out, though, that doing that sort of global memoization in Rust is nontrivial.
I've seen that phrasing before, but I think there are lots of nice points to using immutable data structures even apart from the safety, especially in an expression-oriented language. It opens up idioms that are clunkier with mutation.
There was /u/japaric's `steed` crate which aimed at re-implementing the `std` library in pure Rust.
Sometimes, yes! Sometimes, though, it turns out you're implementing a type checker that frequently needs to remember old copies of all of its data structures for later use, and at times like that I would've really appreciated a crate like this :) Bookmarked.
I certainly don't think that they're useless, but they are a little strange. Sometimes strange is necessary!
Great post! I'll be pointing people to this in `#rust-beginners` in the future for sure.
[I think I'm stuck in a loop](https://i.imgur.com/Z2vqt2b.png).
Thanks for the detailed explanation. The idea of moving pieces of data from one data structure to another makes the idea of ownership easier to understand for me, rather than thinking about it in terms of passing values in plain function calls.
You can still find the post if you browse to all posts made in [ january.](https://unhandledexpression.com/2018/01/)
I fixed this by navigating to https://unhandledexpression.com/2018/01/10/. Does that work for you?
The Objective C wrappers seem to be in reasonably good shape (they're used by the [cocoa](https://crates.io/crates/cocoa) crate) but I wouldn't call it a good experience compared with developing in Xcode - you still have to translate APIs into more awkward syntax. Also, that's Objective C and not Swift, and the long term evolution of the platform may be to the latter.
I'm sorry about that, I don't know how to fix it on my end, it's an issue of wordpress.com :(
Wow, awesome post, Kevin! Thanks a lot! I will see whether I can do some measurements with imag (I'm a heavy user of clap) and report what my results are!
Good point, `steed` is about taking libc/musl out of the equation. As for Linux, it's a temporary state of affairs. Japaric's interests lie firmly in exotic architectures (he is the author of xargo). I'll let him speak for himself though: &gt; Non-Goals &gt; Supporting other unix like systems, like the *BSD ones. Mainly because we have no (easy) way to test them inside Travis and also to reduce the amount of work required. **After we are done with the C free port of std for Linux then we can start thinking about supporting other OSes.** Here you go :) Linux is only a first step. I expect to avoid dispersal of efforts and ensure orderly progress.
After submitting my own blog post and now reading all your articles I get the impression that I'm without inspiration... Because my ideas for 2018 are rather humble ... :-/
The hard part is that Rust doesn't let you just hand out references to big integers without cloning them; if you ignore that it's not all that hard. It's also probably orders of magnitude faster since you don't have to walk a 50-long thunk chain every call. lazy_static! { static ref FACTORIALS: Mutex&lt;Vec&lt;usize&gt;&gt; = Mutex::new(vec![1, 1]); } pub fn factorial(n: usize) -&gt; usize { let mut factorials = FACTORIALS.lock().unwrap(); while factorials.len() &lt;= n { let new = factorials[factorials.len() - 1] * factorials.len(); factorials.push(new); } return factorials[n]; } More akin to Haskell would be something like a `Mutex&lt;Vec&lt;Arc&lt;BigInt&gt;&gt;&gt;`.
Yeah I think(?) the ownership machinery originally came about as a way to help different threads pass mutable things back and forth safely (with GC taking care of most of the lifetime issues), and then it wound up later being a way to provide memory safety without GC. Rust did a lot of [weird stuff](https://youtu.be/79PSagCD_AY) at different points in its early history :)
Overall: I agree. --- *On elegance* I'm fine with temporary hacks to get going, as long as they remain unstable, but I certainly prefer an elegant solution (and possibly more fundamental/barebone). One thing I like to remember is that Rust's safety is not just baked in. Instead: - ownership and borrowing define two concepts which, when put together, allow for a memory safe and type safe Rust in single-threaded environments, - the `Send` and `Sync` *library* traits are layered on top to guarantee data-race freedom. Ownership can also be used to implement state-machines in session types. Borrowing can be (ab)used, with some weird constructs, to get the equivalent of "brands". --- *On saying no* I agree with the principle. I have different opinions on which feature are important and which are not, I am sure, which I guess is the heart of the issue. In my experience, each feature added incurs *debt*: - it eats at the "complexity budget" of the language, - it stands in the way of other features, both at language and implementation level. It's not a matter of who does the work, or how. Once it's in, it's a maintenance burden on so many levels! This is why I think Rust evolution should focus on *enabler* features. Features which, by themselves, unlock large swaths of possibilities. So, with regard to your list: - impl Trait: insufficient, first because it only allows one return type and second because it's specific to traits. I'd prefer Unsized types as first-class citizens instead. - NLL: I don't consider this a "feature" so much as a "bug-fix" (not saying it's easy...). - specialization: performance benefits are clear, semantic/usability implications are not. I've suffered a lot from C++ specializations (such a nightmare to debug!); I don't see how to do better. - GAT: clear enabler. The lack of "streaming" iterators is keenly felt, the lack of "collections" traits is also annoying. - procedural macros: I know everyone likes them, I still haven't found a use for them myself :P As far as I am concerned, I'd add as enablers value generic parameters (at least integrals) and compile-time compilation (at least on integrals). I am not sure about `catch`. I am not sure it's worth its keyword, and wonder how much a generic function could do instead `catch({ ... })`. *Note: for a counterpoint, Stroustrup's strong opinion that what can be implemented as a library should be implemented as a library has led C++ to have the worst possible ergonomics for `tuple` and `variant`. Something to keep in mind.* --- &gt; I didn't really know what to answer: even if the Rust code written today would compile 10 years from now, it might very well be that the ecosystem around it has changed so much, that even the tiniest maintenance will lead to massive amounts of code having to be rewritten just to update a few dependencies It is not clear why you would update a few dependencies, and how their APIs having changed over a 10 years course (which seems likely) is connected to the language having changed.
Interesting! That makes me feel a little better then. (There is still the case that `$` doesn't match `\r\n` but rather `\n`. GNU grep solves this by patching the text being searched on Windows IIRC, but it's a little gnarly!)
&gt; impl Trait - is a no. Because the same functionality is available today using newtypes. Newtypes don't let you return closures.
I think that misses the mark. It is not the amount of features, but some of the specific features (such as lifetimes and borrowing rules), that gives Rust its reputation as hard.
As i read several of the "rust 2018" posts, and as the docs team started putting together its goals for the year, this problem of "intermediate docs" came up multiple times. Feeling a little clueless, i asked Twitter what they thought that meant, and these are the ideas that came out. Hopefully i/we can put together some of these during the year, and help people build more and more things with Rust.
Good observation. To test this, I've taken the liberty of executing both Haskell's and Rust's fibonacci functions with 200000 as argument, and measure the time. Turns out Rust is actually 6.7x faster than Haskell, with Haskell taking 0m2.491s, and Rust taking only 0m0.367s I had to extract the Haskell code from the library, though, so I have no idea whether it is actually optimal or whether the Haskell code is even compiled in release mode. fib :: [Integer] fib = 1:1:zipWith (+) fib (tail fib) main :: IO () main = print $ fib !! 200000 
Well put, I should probably link to this post instead of the sword tweet in my 2018 post… nah, the sword tweet is great, imma gonna keep it! :)
Does `impl Trait` allow returning an object that can be converted to type `Dst` using `From`? Using `Into` it is straightforward: fn foo() -&gt; impl Into&lt;Dst&gt; { /* ... */ } However while `From` implies `Into`, `Into` does not imply `From`. So is there a simple way to return `ret` such that `From::from(ret) -&gt; Dst`?
I now link to the sword tweet later in the post. I'm pretty proud of that one.
I did find a workaround, but I don't know if there is a more idiomatic way: trait IntoNew&lt;Dst&gt; { fn into_new(self) -&gt; Dst; } impl&lt;Src: IntoNew&lt;T&gt;&gt; From&lt;Src&gt; for T { fn from(src: Src) -&gt; T { src.into_new() } } fn foo() -&gt; impl IntoNew&lt;T&gt; { /* ... */ } Edit: This workaround breaks down if instead of a crate-defined `T`, the destination is for example `Option&lt;T&gt;`.
Yeah, make a scripting language that can be (semi) automatically translated to Rust. That'd be definitely interesting.
It's a great tweet! (It's pretty late and I had some drinks, otherwise I'd come up with a great intermedieval pun!)
Sure!
Ok, it's a sample for "here is Rust code and we don't know if it's UB." Good points. What's about solving same problem in C? Rust is not ideal, but we are compating it to its competitors, and C has a solution for this problem (C11 model). Please, provide some samples for this guy too. I mean the rest of my question: "Here is C code that violates some rule and I have following transformation ruleы to make it correct. We apply this and this and viola, this code is safe because of X, Y and Z"
That is a good point if it is the case of memoization. I'm then glad that atm haskel is faster because of. Now would be nice to have a memoization. D was awesome because they had it baked in.
I don't think Rust should be aiming to move fast indefinitely. Personally I feel we're at the point where larger language changes really should be winding down. There are of course a bunch of features that Rust could really do with, but at this point they're specific and finite enough that the queue of things we're already figuring our way through feels enough. Obviously steady progress and incremental improvements are positive, but there's enough language available that libraries can take up the task of doing new things.
&gt; It’s also about figuring out barriers in Rust, polishing mental models, improving docs/diagnostics, and in general figuring out how to best present Rust’s features. *cough cough* error messages involving traits *cough cough* ;)
&gt; There is no reason why so many popular crates should live in user-repos instead of in community-managed organizations (speaking in Github terms). It'd be nice to know some best practices on this and guidelines on naming. One example is what is appropriate use of Rust "trademarks", not as much for legal care abouts but to avoid confusion on "official"ness The post I'll eventually write is going to propose a `rust-ci` org to centralize important CI related projects - To help ensure we follow consistent best practices (like having pre-built binaries to speed up people's CIs). - To provide a standard landing page for documentation (the number of out-of-date blog posts I've read as I've evolved my projects' CI...)
What is the difference between Rayon and Crossbeam? I'm unclear about what these libraries do, and from the READMEs, they look pretty similar.
From a noobs perspective (mine), this would seem counterproductive performance wise. Reading the comments, this is clearly something awesome, I would like to understand why. Can someone give an example of where this type of data structures shine? Or at least point me to some good article that explains it? Thanks!
I'd love to that, thanks!
&gt; Newtypes don't let you return closures. Exactly! Manually implementing the `Fn` traits should be stabilized, then it would be possible to return a struct that implements `Fn`, and you could just not use closures, but a struct that implements `Fn` and holds the relevant data (just like a closure). If we then also add the feature of being able to name the type of a closure somehow, then we don't need `impl Trait` at all.
I always wanted to be able to reuse rayon’s fold buffer, for example, by passing it my own vector that it should use for storage. Is this possible?
English is rather annoying here since technically "good performances" is absolutely valid. (Are you referring to the "performance" of the language, or its performance characteristics in various use cases e.g. "performances"). I'm not sure if there's any real rule that can quantify why every native speaker will choose singular over plural for this one.
A lot of projects I've seen, Rust included, have a [CONTRIBUTING.md](https://github.com/rust-lang/rust/blob/master/CONTRIBUTING.md) document in their repository.
If you ever hold two mutable references to it at the same time then no. Also without using some form of synchronisation (eg Atomic), you'll still get UB just from the unsychronised aspect of things (there's a reason RefCell isn't Sync) 
Some people do. Rust doesn't right now, but there was some talk about writing a book form "guide to the Rust compiler internals" sort of thing a few days ago.
True. I'm just especially aggravated that I can't explain *why* one form is "correct" here.
I'm not sure what you're calling a fold buffer. For rayon's generic `fold`, it's entirely up to you what to use for that accumulator, and how to then `reduce` it. If you're collecting a `Vec`, you can use [`collect_into`](https://docs.rs/rayon/0.9.0/rayon/iter/trait.IndexedParallelIterator.html#method.collect_into) with your own storage, at least for indexed iterators. Non-indexed cases (like after a `filter`) are more involved to collect, but you can still manually `fold`/`reduce` it in your own way, if you like.
Rayon's core is basically a work-stealing thread pool, and then there's a nice `ParallelIterator` API on top. Crossbeam is more about concurrency primitives, I would say. I think the only real overlap is the scoped thread vs. rayon's `scope` API into its thread pool. The rest is complementary.
Sure! *Persistent immutable data structures* allow you have immutable data structures (like Lists, Tree, Stacks...) with the speed and performance you'd expect from mutable data structures. Typically, you'd expect an immutable data structure to be slow and inefficient, as inserting (for instance...) a node to a million or billion-node structure would require copying over the millions or billions of *unchanged* nodes to the new structure. That's wasteful! In contrast, persistent immutable data structures use something called *structural sharing*. Unlike in the previous example where the all of the old structure's nodes are copied over to the new one, persistent immutable data structures diff between the old and the new structure, and only apply the difference. If you'd like to read more, take a look at the [Wikipedia page](https://en.wikipedia.org/wiki/Persistent_data_structure). There are also some great articles about persistent data structures in Clojure, JavaScript, and Scala, which I recommend taking a glance at.
wow, that does sound very good! Thanks for this well constructed explanation!
Asking out of interest - what do you mean by "covariant" here? Covariance introduced by subtyping? How is that relevant?
Urgh this comment is a good reminder to read the post, not just the comments, before commenting.
Check out the [Postmodern immutable data structures](https://youtu.be/sPhpelUfu8Q?t=26m40s) talk (at 26:40). The speaker demonstrates how you can easily implement a text editor on top of a persistent vector. Such a data structure gives you trivial undo/redo. It's also very fast: e.g. copying a huge chunk of text into a random position in the file is no problem. To me, the coolest feature is probably checking whether the file is modified in real time. For example, you can insert some text into the file, then manually delete it (I don't mean undoing), and the editor detects that the current contents of the file are the same as the original! Doesn't matter if the file is gigabytes of text - it's always instantaneous! Works like magic. :)
Supporting BSDs or windows would be complicated though. They have no stability guarantees on their syscall ABI. Instead, the libc is the stable layer.
Does Haskell automatically memoize all functions? or how do we know they're memoized?
Of relevance to this subreddit: * They're open to using ripgrep/fd in a similar manner to how VSCode uses ripgrep * They plan on rewriting their [child process spawn server](https://github.com/nathansobo/spawn-server) in Rust * They're thinking about porting other functionality from JS to Rust. Not sure whether they're going to use Neon or WASM.
For cross-platform terminal stuff, try easycurses.
You can pattern match a single struct with the common field and the rest as an enum. This might be close enough to what you want: match node { Struct { common, variant: Enum::Variant1(a, b, c) } =&gt; ... Struct { common, variant: Enum::Variant2(a, b, c) } =&gt; ... }
Why you want language changes to wind down? What's the harm?
You will likely have a better chance of finding it at /r/playrust =P
Thank you!
Possibly the worst problem with fast change is that it pushes the ecosystem towards rapid deprecation. Language stability isn't just important for enterprisey businesses, it's the very foundation of a stable and mature ecosystem. There are more specific costs to language changes, but I suspect my personal opinion mostly comes down to my experience of C++ and how that is very much not the route I'd like to c Rust take.
Happy to help! Be sure to check /u/stjepang's explanation; their explanation nails why these data structures are so cool in practice. An aside: React's Virtual DOM does a similar thing to what I described, but in a context that we typically don't consider to be data-structure oriented.
ahh interesting approach! This might work, thank you!
Glad that they are taking Atom's dog slow performance seriously. My experience with Atom over the last year is every few months hearing that performance has been 'fixed'. Trying it out and realising that it is still awful, and going back to vscode.
Weird, it worked from here.
You want... more pattern matching?
Fold reduces into a sequence of accumulators, not a single one. I want to reuse the storage of this sequence across calls to fold.
Totally agree! Documents that are easy to point to that highlight how to design larger Rust programs would be really helpful! I definitely struggled with the design of larger programs in Rust (and still do!) because a lot of common mappings don't work in Rust A large part of that information is very tribal knowledge-y w/ also chicken and egg mixed in as well, in that to understand how to write large programs in Rust...you have to write large programs in Rust (and probably badly a few times until you get it right!)
It’s great understanding the CLI to lldb, though personally I like to step through code visually. The VSCode integration with LLDB is wonderful.
&gt; Still, this doesn't sound right to me. Again, it was ballpark. I think I should stop using such rough numbers in my technical writing. They are most likely wrong, and very much coloured by my experiences (which are fairly specific). &gt; Of course, you are right in that unsafe code isn't just locally unsafe. So if we count the entire vec.rs as unsafe since bad stuff happens if we mess up the capacity we'd get very different numbers. But now we're venturing into "unsafe vs unsafe" land, which /u/manishearth wrote about a few weeks ago. True. Regardless of the percentages, you should consider all of the `Vec `code to be "unsafe" since there's so many more constraints on that code to keep up the invariants. I only addressed literal `unsafe` code in my blog post, but hopefully good documentation can help others learn how to write unsafe modules correctly. &gt; It makes me wonder if it'd be helpful to have unsafe fields in structs that require unsafe to write to them, but not read. That would be interesting. I'm not sure how much that would help in practice, but it could be easy to do that today with a zero-cost wrapper (e.g `unsafe` `set`, but a safe `get`). Would be interesting to see an analysis of this to see if this adds any value, but it sounds like it could just add unnecessary noise to me. I could be wrong though, I'd have to see it in practice.
&gt; Anything that will make wasm nicer will be awesome, but honestly, I'm thrilled with what we've got. It feels absolutely insane that I can just compile this language that's basically the opposite of JavaScript and it's running in the browser. This looks like QotW material to be! :)
Totally agree, you succinctly summarized the points I was trying to make in my article better than I could have :). Speaking as someone with experience with C, the first case is handled in the language by using common idioms to not mess up (which is more common in C++). In Rust there's not yet a really good guide or consensus on how to that yet. I'm hoping that both the community and the unsafe team can come out with good documentation later because right now apart from the nomicon it's pretty scarce. The second case is handled by the plethora of third-party tools (e.g valgrind) that do static and dynamic tests at runtime. Whenever I try to use `valgrind` on Rust I never feel like I can trust the results (and it's especially annoying because I need to use the system allocator, AFAIK, which adds another barrier to entry). Improving Rust's ability to work with these tools (or writing new tools, a good project for the community at large to take on!) would certainly help a lot. As it is now, the only way I feel I can test my unsafe code is by either thinking _really_ hard (and calling on others to think with me, [as I've done in the past](https://www.reddit.com/r/rust/comments/6hjfuy/is_my_set_timeout_function_safe/)) or by using a fuzzer.
Whoops. For some reason I thought pattern matching was tied to TR. I haven't actually done anything with the language, just following it on the side 😅
I gave two different types in my example, so I'm not restricted to returning the type T I got, just whatever type U is supposed to be. I guess I get it, though: -&gt; impl X guarantees getting an anonymous trait object back. OK. Are closure types and combinators unnameable? I thought there was the whole FnOnce() FnMut() thing?
cc u/Breaking-Away: &gt; Definitely would love to read the RFC one.
Speaking of nom, thanks for switching to standard Result in 4.0; it's made my life a lot easier (once I fixed all the type errors now that Done doesn't exist) Being able to short circuit with ? is a godsend since I apparently don't know how to make do_parse operate
Unstablity is great for horizontal innovation, but it is horrible as a foundation for building things atop. I personally hope that we see rust settles down so that it can be used by more people in practice.
thanks for the explanation! the intuitive leap I was missing is that I accidentally interpreted the signature for `factorials` as saying that it took an argument, as opposed to just defining a constraint on the type of `a`.
Pattern matching function calls would be great actually!
What do you expect to have happen if the data doesn't match the pattern?
i.e. struct vs enum
Well, probably that was incorrect use of term in Rust context (with C++ in mind). That time I thought about usual pattern in C++, where you typically have a container of pointers to a base class and you may call methods without knowing what type is actually there. Such pointers in C++ are covariant. In some sense it's similar to a vector of trait objects in Rust where you may call trait method for any type implementing the trait.
And they're bringing back features from Smalltalk where you can load/store an already existing VM instance. It's like computer science necromancy.
Even rust [has been branded](http://venge.net/graydon/talks/intro-talk-2.pdf) as "technology from the past come to save the future from itself"! That being said, I admit I have no idea what Atom is supposed to be saving us from.
Yeah I don't think necromancy is inevitably bad, it just feels like overkill / solving the issue the wrong way in this case.
&gt;That being said, I admit I have no idea what Atom is supposed to be saving us from. Emacs induced RSI I guess :) C-h , C-M-a , &lt;RET&gt;, &lt;ESC&gt;&lt;META&gt; a
Have you heard of JavaScript?
Sorry, what does that key sequence do? :wq
Tyty
What would "unsized types as first-class citizens" mean, exactly? Are you suggesting that local variables of unsized types be allowed? How would that work?
what can I expect? exception
Exceptions are not for error handling in Rust. The closest equivalent, panics, can't^^* be caught. You are strongly encouraged to not make a program that crashes when given slightly invalid data, especially if that potential crash isn't apparent in the code.
You can do `if let serde_json::Number(n) = ...) { ...} else { ... }`. Panics (exceptions) are not a good way to handle an error in Rust, so this kind of code makes you have to handle the failure case
I’m looking forward to do a day I can use servo. It would be awesome paired with next browser.
Rust from this presentation is completely different language than today's rust. Although the main goals are mostly (exactly?) the same.
i'm not saying rust should compromise on safety, i'm saying 'less unsafe' stuff within unsafe should be as ergonomic as C++ 
Good to hear this feature is planned! &gt; We want to get experience seeing how people use existentials inside traits before we introduce `-&gt; impl Trait` in trait functions It is unclear to me, how these two differs?
&gt; the Send and Sync library traits are layered on top to guarantee data-race freedom. Oh, I wanted to write about the problems with those as well. Basically, take a random type, e g `std::ffi::CString`. Is it Send/Sync or not? Currently it is. But it's not documented anywhere. Is it guaranteed to stay that way in the next release of Rust...? &gt; I am not sure about catch. I am not sure it's worth its keyword, and wonder how much a generic function could do instead catch({ ... }). We already have closures if you like to use `?` without exiting a regular function, like this: `(|| x()?.y()?.z())()`. It can be used in combination with macros like [inner](https://crates.io/crates/inner). 
&gt; I envisioned that you'd use a single compiler for the entire build. Ah, we agree on this then. I mentioned this, because it seems to be what tibodelor was suggesting. A simple version constraint could certainly work. That way it could be used as an additional criteria for dependency resolution. And give an error if it's unable to find a usable crate version that works with the compiler. It still seems to me like it would be of pretty marginal usefulness though. Even more so if the stable compiler never has code-breaking changes.
Using a vim emulation plug-in with Atom (or VSCode) is even worse! If the editor is even a little bit slow by default, a large plugin like that really brings it out.
&gt; Oh, and my never-gonna-happen breakage whishlist: make all of the Iterator fn return impl Iterator instead of concrete types. impl Trait all of the things. How is it a breakage ? If I understood correctly, returning ` impl Something` instead of returning a concrete type doesn't change the return type in any way, it just simplify the signature. Or maybe I understood it wrong ?
Ah, a good old [`unexec`](https://lwn.net/Articles/673724/) from Emacs? :) 
Oh yes, and paint yourself into a corner several times. To pass on the results of that experience is both necessary and hard. The larger a program gets, the more unique it becomes, but patterns of design are transferable 
re-read my question
Thanks, I missed that part. I added a small notice that it would be better to find some way to name the type of the closure, rather than tip-toeing around the problem with `impl Trait`.
Nice article! I'm also a student learning rust and have been amazed by the community, definitely one of the best out there :D Currently writing a console text editor in Rust, I just might blog about it :) 
Call as_i64() or as_f64() on the value, then unwrap() it. Literally the same you did with toml.
What's wrong with: let a = make_a().ok_or_else(|| "Failed to make A")?; let b = make_b(a).ok_or_else(|| "Failed to make B")?; make_c(c).ok_or_else(|| "Failed to make C") 
That sounds like an argument *against* having changes wind down... For a while there, JavaScript was this super old language, and someone had to invent CoffeeScript to make it tolerable to read and write
I know, I just thought that if this sequence of accumulators is stored by fold into a Vec i would like to be able to pass it my own that already has some capacity reserved.
Well put. While the *focus* should be getting the in-flight features into stable proper, we should not stop looking for moonshots. Interestingly, I see a lot of overlap with what currently happens with Java, albeit at a slower pace. They got a lot of stuff rock-solid and are now back to innovating the language.
you didn't understand my question. it's about syntax, not achieving the result
Your assumption is incorrect. `impl X` is an anonymous but *statically* bound type (that implements X), not a trait object. Fn/FnOnce/FnMut are traits, not types. Closures implement them but themselves have unnameable types.
That's not a general solution, though. I actually wrote half of a hash consing solution in Rust and it's really quite difficult to get right, especially if you want garbage collection to happen (you have to deal with weak pointers, too).
And in my opinion that's not as easy as it is in C++. &gt; I felt like you were accusing me of lying when I said that for me Rust compiles way faster than C++. Sorry if it seemed that way ;) I definitely believe you that Rust compiles faster than C++ for you. I hope you can also believe me, that my non-template-heavy C++ code compiles faster than Rust for me.
I would like to know the *why* too. The subject that is claimed to have reasonably good performance is plural in this case right? So why not performances? &gt; They ... should offer reasonably good performances. One wouldn't say "They ... should offer reasonably good characteristic. So why performance? Give me the why? :D 
&gt; that doing that sort of global memoization in Rust is nontrivial. I mean, you would need to write a run time that is able to execute some code, and execute the code that you want to get memoized inside that run-time. OTOH, if you just want to memoize a single function like `factorial` using a global or function local static `HashMap` works just fine. It shouldn't be hard to write a `#[memoized]` procedural macro that you can attach to any function to generate the boiler-plate for you. The only requirement is that the tuple of arguments `(args...)` of the function must implement `Hash`. 
&gt; D was awesome because they had it baked in. How does D have backed memoization in?
&gt; I hope you can also believe me, that my non-template-heavy C++ code compiles faster than Rust for me. Yes I believe that. Probably if we were arguing about this in person none of this misunderstandings would have happened. &gt; And in my opinion that's not as easy as it is in C++. Yeah I think maybe my C++ experience is just different than yours. C++ has way too many build systems, so I do believe that some handle this way better than others. Do you think like a `cargo extract-to-crate module.rs` file that generates the crate directory, adds dummy `Cargo.toml` files, and so on, would make it easier? Writing such a sub command shouldn't be very hard.
&gt; What's about solving same problem in C? In C it is not undefined behavior because it doesn't have unique references, so you would need to write it with pointers, and pointers allow this non-unique mutable aliasing.
If so many people serm to misunderstand your question, you may want to rephrase it instead of asking people to re-read it every time. The way you do that appears unfriendly, entitled and arrogant. Please give it a little more effort and save your and our time.
Let's go step by step, try and fix the errors the compiler throws at us. let json = r#"{ "some_key": 24 }"#; let json_data: Value = serde_json::from_str(json).unwrap(); let serde_json::Number(n) = json_data["some_key"]; error: expected tuple struct/variant, found struct `serde_json::Number` --&gt; src/main.rs:8:9 | 8 | let serde_json::Number(n) = json_data["some_key"]; | ^^^^^^^^^^^^^^^^^^ did you mean `serde_json::Number { /* fields */ }` This error complains that the syntax you are deconstructing with (tuple struct / variant) doesn't match the syntax of the type of `serde_json::Number`, which is a record struct (`struct Number { n: N }`). Let's fix that. let serde_json::Number{n} = json_data["some_key"]; error: mismatched types --&gt; src/main.rs:8:9 | 8 | let serde_json::Number{n} = json_data["some_key"]; | ^^^^^^^^^^^^^^^^^^^^^ expected enum `serde_json::Value`, found struct `serde_json::Number` This is a simple type mismatch error. The type of `json_data["some_key"]` is actually `serde_json::Value`, not `serde_json::Number`. Let's fix that. We want to extract the number from the value, so let's deconstruct the number. let serde_json::Value::Number(n) = json_data["some_key"]; error: refutable pattern in local binding: `Null` not covered --&gt; src/main.rs:8:9 | 8 | let serde_json::Value::Number(n) = json_data["some_key"]; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ pattern `Null` not covered Uh oh, `json_data["some_key"]` can actually be something else other than a number. We can't just blindly assume that it is a number. How about we only do something with `n` if the value is a number? if let serde_json::Value::Number(n) = json_data["some_key"] { } One last error, it seems: cannot move out of indexed content --&gt; src/main.rs:8:43 | 8 | if let serde_json::Value::Number(n) = json_data["some_key"] { } | - ^^^^^^^^^^^^^^^^^^^^^ cannot move out of indexed content | | | hint: to prevent move, use `ref n` or `ref mut n` Let's just do what the compiler says. if let serde_json::Value::Number(ref n) = json_data["some_key"] { } Huzzah, it compiled!
You should stick to writing questions or constructive comments. This subreddit aspires to a higher level of discourse than most of reddit, to follow Rust's code of conduct. Please read and follow the rules. Thank you.
sorry this is random but speaking of `:wq` you have no idea how many times I've closed windows such as emails or forms where I've written a block of text and naturally want to enter command mode again... except no vim... *sigh* 
great example of gtk use!
It's a popular commit message: https://github.com/search?q=%3Awq&amp;ref=simplesearch&amp;type=Commits&amp;utf8=%E2%9C%93
An alternative would be to let the scope of the patterns matched by a successful if-let last until the end of the block containing the if-let (or until scoped out). This shouldn't break existing code, since it introduces new bindings that weren't there before, and it wouldn't require any new syntax.
&gt; C++ has way too many build systems, so I do believe that some handle this way better than others. Definitely! For example you mentioned grouping up objects into larger TUs to scale compilation speed: This is done automatically by build systems such as Waf or Meson. &gt; Do you think like a cargo extract-to-crate module.rs file that generates the crate directory, adds dummy Cargo.toml files, and so on, would make it easier? Hm ... the main problem for me is still the dedicated directory, file location and additional `Cargo.toml`file. If I want to refactor the structure I don't want to move files around (messes up Git history for example).
Cool, thanks!
You can always use a dependently typed language without actually using the dependent types, and then they're really straightforward. Gallina programs (in Coq) have 17 different expression types internally, and that's *all*. You only really need 8 to do normal programming (apply, lambda, local variable, inductive, constructor, match, fixpoint, product), though two others (let in, constant) are needed in practice. Technically sort is also essential but you virtually never have to write it in normal programming; the others are either only situationally useful (projection, cast, existential variable), or barely ever to never needed (named variable, metavariable, cofixpoint). I originally had a longer description here, but for the most part the things people actually use have "non-dependent" versions that are just as easy or easier to use than Rust's version, with the exception of fixpoints (which have extra restrictions to keep the language total) and the fact that order in the file matters for referencing definitions. I'm mostly bringing this up because I find that when I'm using Coq, for the most part I don't *program* in dependent types--I write programs in the sublanguage I mentioned above while avoiding dependent types, then I prove things about the program separately (using tactics, which automatically construct the dependent types). For cases where I do need dependent types in my program, I usually try to just treat them as "enforced assertions" (that you would normally put in comments); if I actually need to call such a function within another function, I try to arrange things so that I can prove the obligations separately from writing the program, using the tactic language, instead of trying to construct a dependent term. Doing things that way feels a *lot* easier, at least in Coq. It's a functional language with crazily precise types available, if you happen to want them; but you can quite happily ignore them if you don't need them. As long as you don't mix your proofs and your programs, and don't happen to need any side effects whatsoever, I don't think it's much harder than any similar language (ML, Haskell, whatever).
The [UB reference](https://doc.rust-lang.org/beta/reference/behavior-considered-undefined.html) you posted above seems to suggest that the concern is that the compiler will issue `noalias`directives unless a value is wrapped in an `UnsafeCell`. If I wrap my arrays in a `RefCell`, they are also wrapped (internally) in an `UnsafeCell`, so I should be fine from that angle. As for atomics: as long as my reads and writes are single-instruction, I don't particularly care about memory ordering (at least, until I see evidence to the contrary). Maybe we could continue the discussion on the Github repo?
I worked with javascript in the 90s, so I know where you're coming from. But javascript never had a batteries included standard library, which means that it is consistently being rewritten. Each year, there is a new set of libraries to learn and the churn appears to be accelerating. That stratifies the community and a shifting foundation: It's a bad idea to build a house on sand. My pointed question definitely was a dig at Javascript and its current ecosystem and an example of what _not_ to do with a language. Winding down to allow the community to grow and evolve is emphatically a good idea. That said, polished developer experience is what's going to move Rust forward, and Rust's ergonomics still have some rough edges.
thanks, that's it
Is it only me or is the meaning of `if !let PAT = EXPR else BLOCK` reversed ? I would have removed the `else` or replaced it with `then`.
Yeah, but the values being read should be stored in the same exact order as in the file
Cool presentation! Glad to see Rust is used in the non-English-speaking areas too. Just a minor nitpick on slide [134](https://presentations.bltavares.com/nunca-ouvi-falar-de-rust/index.en.html#/18/22): I think by "operation*al*" systems you meant "operating" systems.
In my mind intermediate docs should teach how to actually write good Rust. What design patterns work well? What does a well written IO heavy program with lots of error handling look like? How do I write a good library? And so on.
Unless your text editor is for your own learning, I would consider looking to contribute to xi editor. It will hopefully become the next gen text editing engine
I've seen it yeah, will look into it when I have more time. Mine is for a course project :) So yeah, education purposes. 
&gt; especially if you want garbage collection to happen I didn't think it could happen in this case. Or are you talking about other cases?
You and I have different definitions of good
Try it and it exactly works as you describe.
Slide 3: &gt; We are not “rewriting the browser”. That's impossible. Put down the gun. Well, it doesn't _that_ impossible nowadays
Thanks for that. I'll have a look into how difficult that is to implement; I may have to re-jig some parts of the library (because I won't be able to use `ndarray` for my parameters any more).
No problem! We do use ndarray with wrappers around AtomicU32 sometimes. 
&gt; That being said, I admit I have no idea what Atom is supposed to be saving us from. [Paying seventy dollars to register Sublime Text.](https://eev.ee/blog/2015/05/31/text-editor-rundown/#atom)
&gt; You can place a file called .cargo-remote.toml in the same directory as your Cargo.toml. There you can define a default remote build host and user. It can be overridden by the -r flag. Can this be read from `.cargo/config`(or `.cargo/remote-config` if `cargo` doesn't like unfamiliar keys) instead? This file can be per-crate, but also can be used to apply to a folder and its subfolders, or as a per user config in `~/.cargo/config`
Worse is when you're helping a coworker and you ask for the keyboard, write some code and then struggle to save the damn thing and make a mess of it all because it's not vim and there's no bindings installed you just typed :wq KD dd u uuuuuuu I never feel so dumb as that
&gt; Is it guaranteed to stay that way in the next release of Rust...? Of course it is. Removing an impl is a breaking change, period.
As an Emacs user I have a similar issue when I try to cut/copy in a webform and my browser tab closes.
I like the *idea* of Atom and I've tried VSCode (and found it not too bad) but I am keeping an eye on https://github.com/google/xi-editor as a replacement for Emacs for me.
Have you tried CUA mode ?
Seems like a good use case for a maybe/either monad. F# solves this using computation expressions (a.k.a. syntactic sugar for monads). https://fsharpforfunandprofit.com/posts/computation-expressions-intro/#safe-division I've often thought about trying to implement a computation expression building in Rust using macros.
In many cases, you already couldn't do this, as you'd need to include the closure's type in the name.
This. XI text processing core is built as a library. Atom should use that to get a significant speed bump. Editing performance, large files, memory footprint -- it checks all the boxes. Even the API is in JSON!
I was on the same track until got to the point of suggesting newtype pattern instead of `impl Trait` which has made it pretty obvious that author understanding of elegance is quite different from mine :( It can be made to work in some of cases when `impl Trait` is desired but it still is a hack that leaks implementation details, and trying to reuse existing features no matter what is exactly the opposite from elegance in my opinion.
You're right, I was thinking of `slice::Iterator` which i have used, but it seems like one size does not fit all. 
To further elaborate, some may say, "well what about Go?" Go does not use the system libraries, instead writing the assembly to call into the OS by hand, even when this isn't supported by the OS. This can [lead to problems](https://marcan.st/2017/12/debugging-an-evil-go-runtime-bug/).
Regardless of whether or not you asked about best practices, I think it'd be pretty irresponsible of us not to tell you what they are. They're best practices for a reason, and they'll help you in the future.
The possibilities aren't actually endless if you want it to semantically behave as a "generic memoizer", though (one that doesn't require you to, for instance, carry around the lifetime in everything memoized from it). With a standard global HashMap, you have to wrap it in a mutex, which would mean *all* of your attempts to look up the memoized thing would be serialized through the global lock. That can be very slow! In Rust you'd also most likely need some sort of dynamic typing, so you'd probably have to box all the values in it, and you'd need to likely use Arcs at a minimum so that you could cheaply copy items out of the map (you wouldn't want to keep the global lock for long). You can speed it up using a concurrent hash map, but then garbage collection becomes something close to a requirement, and resizing the map can be tricky too (it can be done, but it's again not trivial). Having a shared, global, mutable data structure where you have no bounds on lifetimes or access is a *really* bad fit for what Rust is trying to do (unsurprisingly). That's not to say that you can't memoize really efficiently in any one case though.
Yes, that's a good idea. As soon as I refactor config reading and merging I will be able to implement that. For now I will have to write my own implementation, but hopefully clap will get the ("defaults from config file")[https://github.com/kbknapp/clap-rs/issues/748] feature.
I'd be interested in seeing that :) What design do you have in mind?
yup, it simplifies a lot of code, even internally in nom, so the code gets smaller too
For anyone curious about the discussion about the SIMD API, it can be found [here](https://github.com/rust-lang-nursery/stdsimd/issues/159). Having followed that thread, I can understand the author's frustration.
&gt; Are closure types and combinators unnameable? I thought there was the whole FnOnce() FnMut() thing? When you write this: let a = 10; let add = |b| { a + b }; assert!(add(20) == 30); It becomes something like this: struct Add(i32); impl FnOnce&lt;(i32,)&gt; for Add { type Output = i32; extern "rust-call" fn call_once(self, num: (i32,)) -&gt; i32 { let Add(a) = self; let (b,) = num; a + b } } let add = Add(10); assert!(add(20) == 30); The type of `add` is not `FnOnce(i32) -&gt; i32` - it's `Add`. The variable `add` is actually the data(the number `10`) and the code (adding it to the argument) is part of the type, not the variable. The same thing happens with the closure version - `add` is just the data(`10`), and the actual code is part of an anonymous type created for the closure. 
&gt; we don't need impl Trait at all. In some sense. I'd much rather have `impl Trait` than be forced to do all that.
I am still a rust beginner but wouldn't it be better to use an enum instead of your "imperialOrMetric: &amp;str" ? Apart of that, great job! I know nothing about homebrew but I am sure it would be really useful for people doing it. 
Oops, that's a big important typo. Let that be a note to myself not to finish and publish blog posts at midnight, I guess. It's fixed now!
I'd much rather by able to name my return types, and see Rust go into that direction. But it seems that many people in the community would rather go in the opposite direction, where, although the language is statically typed, it's often impossible to name types, and only the compiler knows what's really going on.
good thing we have a stable library to parallelize 
It might be a good idea to use [XDG](https://crates.io/search?q=xdg) crate so you can respect config folder locations automatically in many environments. 
I feel like a lot of the Henri's specific requests for stability with lower-level hardware access were justified in his mind because Rust can simply reuse LLVM technologies in order to implement them. However, Rust shouldn't be tied to LLVM, in my mind! To whoever knows more than I do about this: Wouldn't reusing LLVM concepts and terminologies only tie Rust further to LLVM? Are my worries even valid?
That disassembly tool sounds awesome.
That was presented very humbly and factually, which is a lot better than their early shtick. They would have to _far_ surpass VSCode to win me back, so just getting fast isn't going to be enough on its own.
I mean it's not much of a design yet, it amounts to where nom has a macro, using a proper (`const` where possible) function returning `impl Fn`. The rest was let to fall out from that decision. I've since learned to appreciate the benefits of using the macros more, but the original reason was for better IDE support (since it's using real functions). That's a horrible mess of spaghetti logic that I _would_ clarify with an example, but, uh.... GitHub is down. :panic:
Essentially anything that is LLVM-specific would have to be reimplemented for a compiler backed by something else. This is fine for things like `unreachable` or SIMD intrinsics, which tend to have a 1-to-1 mapping (or can be relatively-simply stubbed out), but can cause problems with the inline ASM syntax since we'd have to write a full parser that then converts the AST into whatever the other compiler uses, and make sure that it's perfectly compatible with LLVM's implementation. Not all LLVM concepts/terminologies are created equal. Your worries are valid, but I don't think they apply here.
Thanks! It really is. It's so nice to have all the calculators in one spot for a brew session. Also, thanks for the suggestion. I took a look into using an enum and it appears that it would be more code. Are you saying it'd be better because it's more "rusty" or are you seeing something more efficient that I'm not? I'd love to know more.
Well yes, for this case you can do `let c = make_c(b).ok_or_else(|| "Failed to make C")?;` and it's great. But a proper solution requires allowing this for destructuring complicated arbitrary enumerations, and not just standard library types. I suppose you could think of it as `?` with a manually controlled early return block. The standard example of refutable let in Swift is: guard let x = x else { throw "x must not be nil" } // ... rest In the syntax used in the post, this would be the equivalent: let Some(x) = x else { Err("x must not be nil")? }; // ... rest The maybe computation expression used in F#, as I understand it, just is F# `let! BINDING = EXPR` -&gt; Rust `let BINDING = EXPR?`, as `?` will soon be usable in a `Option` catch context. I want to be able to write the following: enum E { A(AData), B(BData), // ... maybe other variants } fn uses_an_e_b(e: E) { if !let B(b_data) = e { panic!("uses_an_e_b only takes E::B") } // some computation }
That would change the drop order of values, which can break code in silent ways.
Use `while let` there. More complex loop logic tends to be clearer if you just use `loop` and `break`. At least in my opinion, but this is an old argument dating back to Pascal rejecting `do ... while`.
I agree, definitely don't shut down experimentation and revision! At the moment it seems to be the norm for projects to target the current Rust release, plus maybe two older releases. This is usually great for development but I'm sure a massive pain for packaging in slower-moving things like Linux distros. I'd really like to see long-term stable targets, and epochs should be great for this.
Both are wrappers for `write!` which imply certain targets. `write!` can push its output to anything that can accept bytes (`io::Write`) or plain text (`fmt::Write`). `print!` writes to standard out, `eprint` writes to standard error, `format!` writes to a new buffer in heap memory (`String `).
&gt;&gt; I don’t understand why people complain about the syntax, possibly because the code is generally more heavily buried in angle brackets ```::``` quotes and ampersands (there's more information to specify); look at writing operator overloads in C++ compared to rust. I think just a little more inference e.g. omit the types in impl's when the trait specifies them unambiguously (like Haskell allows) would tip the balance. This is more important than it sounds because of the placement of the type: whatever is better analytically, I have deeply burned in 'intuition/habit' when reading/writing code from &gt;2 decades of exclusively writing C++ and I most often still put the type in the wrong place then have to go back and swap it.. adapting to writing *no* type (dabling with Haskell, JS ) is actually more comfortable.. even with haskell's more extreme syntax differences 'module type parameters' could also close the gap (I miss the ability to share type parameters in nested classes in C++, but parameterised modules would be superior) I do know people who react badly even to writing "fn" .. personally that side of it is fine by me, especially when it comes to using ```grep```.
If you really just want to "inject" a .so, you may could use LD_PRELOAD . But to answer your question: I don't know, I don't think so. Hint: are you trying to solve an XY problem? 😉
I feel your pain, i'm about as useless in my coworkers' "normal" editors as they are with vim :) 
&gt; I think it’s a problem to treat it as the first step of what is going to appear on non-nightly Rust I don't. People want vendor intrinsics. The vendor intrinsics are the keys that unlock the full potential of your CPU. &gt; However, doing the lower-level part first risks a lot of wasted effort ecosystem-wide as people will try to piece-wise re-create abstractions that LLVM already has and that the simd crate exposes or, alternatively, publish less portable code than the availability of the simd crate’s functionality would naturally lead to. Sounds like healthy experimentation to me. Hopefully we will get a champion to push towards a cross platform abstraction that we can expose from `std`. &gt; I think it’s important that the feature set of the simd crate appears on non-nightly Rust before or at the same time as vendor intrinsics. I don't.
That thread was frustrating for me as well.
Interestingly, coming from dynamic languages (mainly JavaScript), I find writing `name : type` a lot more intuitive than the C++/Java style of `Type name`. I think because in my mind, the name that I give the variable is a lot more important than the type. I think "I would like a variable to hold Foo's Email Address, oh and support it should be a String" not "I would like a String to hold Foo's Email Address" (this is especially the case with more complex types)
We've gotten some great contributions to the Reference recently! It's getting a lot of love to revitalize it and make it more complete. (And you should check the [Reference repo](https://github.com/rust-lang-nursery/reference) to report any incompleteness you find! Incomplete docs is totally a bug!) As for RBE, there was a recent paper talking about how people learn new languages, and it turned out that it was way more important than we thought. So we would definitely like to pull it into the official doc distribution and polish it up a lot more in the coming months. But RBE is more "beginner" documentation, and still highlights the "official versus unofficial docs" thing you mentioned. There's a wealth of documentation out there that isn't officially maintained. Things like [The Little Book of Rust Macros](https://danielkeep.github.io/tlborm/book/index.html), the [Rust FFI Omnibus](http://jakegoulding.com/rust-ffi-omnibus/), the [Rust Cookbook](https://rust-lang-nursery.github.io/rust-cookbook/), or the [rust-unofficial/patterns](https://github.com/rust-unofficial/patterns) repo (probably more that i'm forgetting or don't know about) are great resources that tend to go under my radar because they're not part of the official doc release. Sadly, i don't know the best way to highlight these pages without subsuming them all under the doc team's umbrella. (That would be a lot of maintenance burden for us! There's not a lot of people on the docs team, hint hint `&gt;_&gt;`) So yeah, more ways to transcribe what is effectively tribal/cultural knowledge right now would be ideal! I feel like there's a wealth of knowledge that's just oral tradition, and we should work to write that down.
[*^(please don't do this)*](https://github.com/neon-bindings/neon/blob/2277e943a619579c144c1da543874f4a7ec39879/src/lib.rs#L40-L44)
That's fine. :) We're both entitled to our own opinions. That said, don't expect this community to stay silent if you deliberately offend or dismiss somebody trying to help you in good faith.
Yeah that's what I'm gonna do I think
Honestly I may have to
This sounds like a use-case for dependent types/const generics, actually. I've definitely wished that it were possible to write code such that a dependent "settings" object could be passed in, for cases where there are many more configuration options than would "actually" be wanted, or when the options have more complex structure than just a bunch of bool flags. **EDIT**: Yeah, now I got to the last paragraph :P
At the end I point out the const generics rfc: https://github.com/rust-lang/rfcs/blob/master/text/2000-const-generics.md And give an example of how the code might be simpler with that feature...
How would you use `while let` here? E.g, take while { is_comment = parse_line(line) == Some(Token::Comment); is_comment } { println!({:?}", is comment); } I tried to do this with `while let`, but did not find a way.
Thanks for the shout-out. I wasn't going to brigade any of the threads, but I am hopeful that xi-editor will soon be a viable alternative to Atom for real work. After the recent performance work, I'm seeing smooth scrolling and low latency on a 165 Hz monitor, which I think puts it in a class of its own. Also, I'm happy for other editors to use bits of technology and code from xi. For example, the syntax highlighter in xi (syntect-based) is quite a bit faster than Atom's, so that's something they might well want to take a look at.
Wait, so you had `fn cmp_with_case(&amp;self, other: &amp;Self, ignore_case: bool) -&gt; Ordering` and now you have `fn cmp_case(&amp;self, other: &amp;Self) -&gt; Ordering` and `fn cmp(&amp;self, other: &amp;Self) -&gt; Ordering`. No branches on the inside, but the caller has to decide between `cmp_case` and `cmp`. If that decision is dynamic, that's still a branch… so is it fair to call that branchless?
&gt; The possibilities aren't actually endless if you want it to semantically behave as a "generic memoizer", though (one that doesn't require you to, for instance, carry around the lifetime of the map in everything memoized from it). With a standard global HashMap, you have to wrap it in a mutex, which would mean all of your attempts to look up the memoized thing would be serialized through the global lock. That can be very slow! In Rust you'd also need to likely use Arcs at a minimum so that you could cheaply copy items out of the map (you wouldn't want to keep the global lock for long). You can speed it up using a concurrent hash map, but then garbage collection becomes something close to a requirement, and resizing the map can be tricky too (it can be done, but it's again not trivial). There are just so many ways to make this blazing fast: two concurrent writers are going to write the same value, and if one thread is reading while another one is writing the reading one doesn't have to wait: it can just fail with value not found and the worst that will happen is that this value would be recomputed twice. So this would be a perfect case for a concurrent hash map that sacrifices consistency for speed. Also, you can have many `#[memoize]` procedural macros: one that allows memoization across threads, one that only allows memoization in serial programs, one that uses thread-local memoization only, etc. But anyways, probably none of this really matters. Memoization as an optimization works if it saves you from recomputing values often, that is, if most of the time you are only reading from the `HashMap` instead of trying to write new values to it. In this particular case, if a `Mutex` turns out to be slow, a `RwLock` (which allows concurrent reads) will speed things up a lot. However, if you have concurrent writes all of the time, then `RwLock` will perform poorly as well, but so will memoization in general and for this situation you are better-off performance wise disabling memoization altogether (it is just taking memory and cycles for no wins). All of these different designs directions combined is what in my opinion makes the possibilities for optimization pretty much endless. Does this make sense? --- Anyways, in Rust, C, and C++ memoization isn't really particularly trendy. Probably because if you expect to be recomputing some values often enough, there are many things that you can do that perform better than memoization, like filling in a look up table, and search it using binary search. Compared to a cache friendly sorted vector (like a level-ordered one), even the fastest serial hashmap performs extremely poorly if the only thing you are doing is lookups.
[`bencher`](https://docs.rs/bencher/) is almost a drop-in replacement for the unstable benching.
I don't point it out, but these two use cases are 100% independent code paths, not reliant on runtime differences. Basically during a lookup in the authority, and storing names in the authority, we only use lowercase names, so the method has no dynamism at the call site: https://github.com/bluejekyll/trust-dns/blob/5e9d820860d25aa5dff2c59dfeaff631bd9f5c52/client/src/rr/lower_name.rs#L225 (this hasn't been merged into master yet, but will be soon) Edit: you are correct, that if a caller needed to perform a check prior to deciding which method to use, it would require some form of a branch. Nowhere in the trust-dns code is that true though, which allows me to call this branchless. 
I'm not sure whether this will be a performance boost. It duplicates the amount of code the machine has to keep in RAM whereas the other code needs to have branch prediction to have no performance impact.
If there are a fixed set of options, an enum will definitely be more rusty, idiomatic and efficient. In addition, when you `match` an enum, you are required to get all options, and there's no need for a 'catch all' case because _only valid data can be represented by an enum_. As for "more code", yes, it will be slightly. But I would say it's worth it for the increased safety (I mean the fact that you won't ever be able to store invalid data in imperialOrMetric variables with an enum). #[derive(Copy, Clone, PartialEq, Eq)] enum Measurement { Imperial, Metric } If you don't do any displaying of it, that's all you need.
Do you mean in the case of the meltdown/spectre bugs? In terms of this specific usecase in the examples used, it's not about performance as much as correctness. You are correct about there not being a performance difference, I benchmarked both and there is no difference.
Zero cost abstractions - you don't pay for what you don't use. If you have to pay the price of branching then you pay it, but if you don't - this technique allows you to avoid them.
How does it contribute to correctness? I ment that you're duplicating the space used by this function in the executable, which is generally not a good idea. (No, I'm not referring to Meltdown/Spectre bugs.)
before with the boolean, I felt that it was too easy to accidentally pass the wrong value when performing case sensitive comparisons. With this, I feel that difference goes down significantly. this conversation is making me realize that I can still hide that boolean behind the two methods I introduced `cmp_case` and the standard `cmp`, which it self would handle the correctness issue... you make a good argument :) edit: though, I still think my general point about branchless programming is valid in the context of these CPU issues.
That code looks ok to me. I don't see why it would be slow. You are building with `--release`? When I have needed this functionality I have used the [walkdir](https://crates.io/crates/walkdir) crate. Hopefully an expert will come along to show us why it is slow. :-)
As far as I know, mere branches are going to continue to be predicted under Meltdown/Spectre mitigations.
Consider using a library for this: https://github.com/BurntSushi/walkdir.
Try using `Path::metadata()` instead of `is_file` and `is_dir` - the performance is probably dwarfed by file-system access and those methods will result in two accesses rather than one.
I don't know the mitigations in detail. My understanding is that before safety critical code caches will be flushed, which will be a performance hit. If we can remove branches in the code around these sections, then it seems that we shouldn't need to flush the cache. (which is partly why I'm interested in this) Independently of branches, there is also the PCID fix which seems like the proper fix for Spectre...
&gt; As for RBE, there was a recent paper talking about how people learn new languages, and it turned out that it was way more important than we thought. So we would definitely like to pull it into the official doc distribution and polish it up a lot more in the coming months. But RBE is more "beginner" documentation, and still highlights the "official versus unofficial docs" thing you mentioned. When I was initially using RBE, I didn't even realize it was meant for beginners. I thought it was a reference of examples of different parts of Rust's syntax and common practices. I found out it was for beginners when discussing rewriting the error chapter. Luckily I was able to find a way for it to serve both audiences. To clarify, the problem with it being a tutorial is that it would establish foundational information using non-idiomatic code and build people up to idiomatic code. This works great if someone is reading it front to back but can misdirect people when someone is jumping in the middle. So my goal with the error chapter is slowly introduce concepts while being idiomatic as much as possible. &gt; (That would be a lot of maintenance burden for us! There's not a lot of people on the docs team, hint hint &gt;_&gt;) Too many areas to improve. I'm going to be making crate CI my focus. Still need to write my #rust2018 about it...
&gt; Independently of branches, there is also the PCID fix which seems like the proper fix for Spectre... I don't think so. The KPTI/KAISER patches are the proper fix for Meltdown, and PCID is important because without it the effect on performance is terrible. Spectre is harder to fix, on the other hand. Avoiding branches as described in the WebKit blog post is part of the solution.
The following program does not typecheck: fn f&lt;F: Fn(&amp;i32)&gt;(_c: F) { } fn main() { let c = |i| {}; f(c); } expected signature of `for&lt;'r&gt; fn(&amp;'r i32) -&gt; _` found signature of `fn(_) -&gt; _` There are two ways to get rid of this error: * `f(|i| {});` * `let c = |i: &amp;_| {};` Why is the type inference harder for the original program than for these two variants? What's this `for&lt;&gt;`, what's it called and where to read about it?
I quite like this. Thanks for the post! One little thing, though: &gt; Oh, and my never-gonna-happen breakage whishlist: make all of the `Iterator` fn return `impl Iterator` instead of concrete types. `impl Trait` all of the things. This is actually a loss of information in many cases. Several of the Iterator wrappers also conditionally implement `DoubleEndedIterator`, `ExactSizeIterator`, `FusedIterator`, etc, based on their contained type. As far as i know, there's no way to represent that in `impl Trait` syntax.
I think the Sword Tweet should be eligible for QOTW material!
&gt; Interestingly, coming from dynamic languages (mainly JavaScript), I find writing name : type a lot more intuitive than the C++/Java style of Type name. I can see that, e.g. you could retrofit that over the dynamic language syntax (.. and doesn't "typescript" do exactly that?). perhaps I should just code in haskell a lot more to adapt to the 'no type' case.. I'm certainly aware of where C/C++'s type signatures get insane (the native arrays and function-pointer syntax), but templated types (like ```array&lt;T,N&gt;```) close *that* gap.
I have often thought something was missing, but couldn't put my finger on it. Now that you've done the work to articulate it, it just seems *obvious*!
The meta-staging seems like a wizard's tool... what is your experience debugging it?
Also, "Ion" is a thing (Shell implemented in Rust with a lot of Rust-isms).
That is a super neat idea. I’m keen on a variant of this using google container build or AWS code build as the remote code building service. Particularly for cross compiling environments that need linuxy/dockery things I don’t want to keep on my laptop. Cool crate!
I quite like the idea of *guards*, indeed. What about: let &lt;PAT&gt; = &lt;EXPR&gt; or BLOCK; With `or` being a contextual keyword? I can't think of any ambiguous parse off-hand, since as far as I know you cannot juxtapose an expression and an identifier.
I agree. I see no reason why `std` should commit very early to a SIMD design, when it can simply stabilize vendor intrinsics and let healthy competition in the ecosystem try out multiple designs in parallel. I wonder if the author would offer the same opinion if the intrinsics had been available from day one; that is I wonder if the frustration is not a case of "X months to wait for the intrinsics and we still won't have a good abstraction then".
Not to be dismissive (maybe I'm missing something), but typically what I've seen people do in these cases (across many languages) is move the whole project folder, and then you can symlink that from home if you need to. That seems simpler, and I can't see any major downsides.
How is this superior to passing a struct implementing some comparison trait (like c# `IComparer&lt;T&gt;`)?
Does setting `CARGO_TARGET_DIR` solve your problems? https://doc.rust-lang.org/stable/cargo/reference/environment-variables.html#environment-variables-cargo-reads
Thanks for sharing! 
That works for the SSD scenario, but not as well for the NFS scenario; disk is semi-regularly deleted (and I use occasionally use other machines, but that's less of a concern), so I'd only want to use it for caching.
OK, well `fold` doesn't store that "sequence" at all. Each `fold` result is just passed directly to the next step in the iterator chain.
&gt; Some of the libraries I use (mostly the graphics and windowing ones) sometimes use pretty hairy types where I can't really figure out which type to write out when I have to. E.g. I start writing something like this in main: &gt; let mut resource = some_crate::generate_resource(); &gt;and then use it. But resource does not have a simple type like Vec&lt;u8&gt; but something involving a ton of wrapped structs, type parameters and lifetimes. Which is not a problem at all because of type inference. As I'm trying to learn how to program in Rust, the exact type isn't always immediately obvious, even if I know I'm getting close. An easy way to get type information would go a very long way towards making my life easier.
The verbose splats kinda stink though. I hope that someone takes this and Rayon and makes a super-hyper parallel iterator library at some point.
Regarding your second question: this `for&lt;&gt;` thing is called ["higher rank trait bound"](https://doc.rust-lang.org/beta/nomicon/hrtb.html).
Probably some variation on `while let l = parse(lines) if l.is_comment()`
Kind of. There are some traits (and functions, and types) that the compiler needs to know about. These are marked as "lang items" with a special attribute: `#[lang_item="whatever"]`. If you're not using `std`, you merely need to define and mark these things yourself.
One simple improvement would be for your http_req function to take a Method directly as the verb argument, rather than doing a string comparison. A more complicated improvement would be to use less string concatenation to build urls.
I didn't think about that. The information could potentially be reintroduced, but the easiest way is just to use the concrete types.
I don't see why `impl Trait` fields should be prohibited unless those are part of exported crate API. Have always considered it as a temporary limitation - is actual intention different? &gt; I can't see how a newtype would leak more implementation details In my opinion it leaks it exactly because it provides a specific return type that can be named, which means library author can't freely change how it implements required trait without affecting downstream.
It is valid. https://play.rust-lang.org/?gist=488efb2ad0b498acd632733648094b55&amp;version=stable let x = if false { () }; println!("{}", x); EDIT: writing code on a phone is impossible 
If however, you, kind reader, are looking for a discord server about the Rust programming language, http://discord.me/rust-lang might be a good link to click.
 &gt; I don't see why impl Trait fields should be prohibited unless those are part of exported crate API. Have always considered it as a temporary limitation - is actual intention different? Since there seems to be no roadmap of how and when to remove that limitation, it does not look that temporary to me, in practice. &gt; In my opinion it leaks it exactly because it provides a specific return type that can be named, which means library author can't freely change how it implements required trait without affecting downstream. Eh? A newtype does allow the library author to change how the trait is implemented. I e, suppose you have a newtype `Yeast`: pub struct Yeast&lt;'a&gt;(Iter&lt;'a&gt;); impl&lt;'a&gt; Iterator for Yeast&lt;'a&gt; { /* ... */ } Now, you need to make `Yeast` contain something else, then just do that: pub struct Yeast&lt;'a&gt;(Take&lt;Iter&lt;'a&gt;&gt;); impl&lt;'a&gt; Iterator for Yeast&lt;'a&gt; { /* ... */ } ...and the API remains unaffected, since `Yeast` still implements the same traits.
I *am* aware of Ion, I just don't see where it would fit into my uses: 1. For trivial stuff, I script my `zsh`. 2. For intermediate stuff, I use Python because it's batteries-included and I have about 15 years of experience with it. 3. For stuff that's going to last or stuff where I want no external dependencies, I use Rust because of its compile-time guarantees.
Thanks :) I'm not sure how these services work exactly (I'm not a friend of cloud computing), but if they support ssh-ing into them it should work. If not I'm happy about issues/pull requests to support back ends. I also thought about using docker for easy and stateless switching of tool chains. In an early draft I wanted to read out the local rust version and make the same one the default one on the build host, but the global state would make concurrent builds using the same user impossible.
&gt; If you're not using std, you merely need to define and mark these things yourself. ... or use `libcore`, which defines all but one of them, and hopefully soon that one will be optional.
I missed the annotations. thanks! Overall I think It's nice to have traits define both features of libraries as well as features of the language. Keeps it clean
Is running a full LTO build still recommended for releases for distribution? With ThinLTO now on the scene, my understanding is that it's much faster but still not quite as effective as running the LTO optimization pass over the whole build.
I dunno... it's got one of them *hipster domains*. Could be a trap. Better safe than sorry.
&gt; http://discard.me/then Better?
http://www.discord.com/php/discord.php?channel=rust-lang There, *much* better.
Totally didn't think of that (taking a Method directly), thanks for the tip! I'll dig around on the concatenation stuff.
Because it's only right that PHP be quarantined to it's own little corner of shame. Otherwise, it might start fraternising with the other files and *we can't have that*, now can we? Actually, come to think of it, I *should* have used `/cgi-bin/`. Oh well.
This crate is one of the big reasons I'd really like to see stdsimd stabilized. It provides a friendly approach to simd that could really help benefit the majority of numerical codes in Rust. The stabilization of stdsimd and inline asm should really help Rust become a more usable option in the HPC community.
I would have said `println!` too, but it depends on how this program is being called. It might make a difference if the OP is passing thousands of dirs, but otherwise, the `println!` only executes once per top-level directory traversal. /u/throw19396 please provide more data. It's only proper that you share what you're comparing your Rust program too. :-)
What is the state of `futures` on `no_std`? Are there any examples out there of `futures` being used on embedded platforms, in particular?
Thank you so so much! I just reworked everything and I got rid of pointless functions, so the codebase actually went down by a ton (you were totally correct). And I do love having the type verified by the compiler. Much more clever. Also, I changed two images from svg to png so it now loads instantly (was another thing that was bothering me). Any other thoughts that you (and everyone else) have are more than welcome. Thanks again!
I actually only ended up needing PartialEq. Great suggestion. :)
You're welcome! Thanks for checking it out. :)
I agree that in this case it may not even be significant (i.e., invoke the command with only a single arg!).
Is it, though? Mandatory braces certainly help but this wouldn't change that, and the types also help- it's extremely rare to use `()` as the type of an `if` expression, even rarer to simultaneously use a diverging `else` branch, and crucially if you really meant to use `let..else` you'd presumably be using a refutable pattern which would then fail to compile.
In Emacs? No... CTRL-W is kind of hardwired now. 
There is a Electron front end for xi as well https://github.com/acheronfail/xi-electron (though I know nothing about it).
Rule: When you call a function, you can expect that the function will either drop each argument when done or save it somewhere if necessary. `insert` is a perfect example. It either inserts the item into the set, or it drops the item you gave it. `contains` doesn't have dropping responsibility. It compares the query to the items in the set and returns a `bool`. However, the rule still holds: all arguments are dropped, including the `query: &amp;Q`. Dropping a reference doesn't do anything, so this means that the `contains` method will look at the query without touching it. Q and T are either the same type or there exists some way to look inside T and find Q. This "looking inside" operation is called `Borrow` and it turns `&amp;T` into `&amp;Q`. For example you can take `&amp;String` (a pointer to a pointer to a memory box partially full of bytes which are valid UTF-8) and turn it into simply a pointer to some bytes, `&amp;[u8]`, or a pointer to bytes guaranteed to be valid UTF-8, `&amp;str`. Typically your program just wants to compare *data*, and Rust tries to take care of the pointer wrangling needed to get to it. Thus you can call `contains` with literal strings or byte strings or compare string buffers to byte buffers (`String` and `Vec&lt;u8&gt;`) relatively easily.
Persistent datastructures allow one to trivially go back in time. It isn't *just* about safety.
(I am sorry, this is kind of hijacking the post a bit) Architectually, It's how I would have done things (though as I've followed the development of xi I see how much I don't know about modern text editor internals :) ) though I prefer Go. I am also tracking the discussion around behaviour and key bindings (eg Vi bindings) since I am an Emacs user, I hope that that will be something that's taken into account - I feel (though now on the fence) that keybindings were something that the front ends should handle. I wanted to comment on the github issue, but didn't really think I had much to contribute. One idea I had was to do a new front end in Go (TUI probably - yes I know about Kod) using the keybindings/statemachine from https://github.com/nsf/godit but I am waiting for the issue to be resolved. Happy to do that as plugin for an Emacs like 'personality' if that's how keybindings will be done.
It's not really about what c# does, exactly. But in c# many bits of the standard library support passing objects that compare other objects on equality or ordering. E.g. `System.Collections.Generic.HashSet&lt;T&gt;` can be passed an `IEqualityComparer&lt;T&gt;` at construction. This interface provides two methods: `bool Equals(T, T)`, and `int GetHashCode(T)`, which do what you might expect. I'm a bit disappointed that Rust doesn't use a similar scheme in its standard library, because it would remove the need to introduce newtypes whenever you want to override a type's `Ord` or `Eq` or `Hash`... regardless, you could define a trait `Comparer&lt;T&gt;` with a single method `cmp(&amp;self, T, T) -&gt; Ordering`, then implement it for a `struct Ordinal;` and `struct OrdinalIgnoreCase;`, and define your `cmp_with_f` as `cmp_with_comparer&lt;C: Comparer&gt;(&amp;self, other: &amp;Self, comparer: C) -&gt; Ordering { ... }`. The result would be exactly the same as your version, except that: 1. In theory you could support a comparer that holds some state, rather than having access only to the two elements being compared (you probably don't need this in your code, but in general sorting strings is culture dependent) 2. You don't need the turbofish to call the compare function. 3. You can choose to use a trait object with dynamic dispatch. A simplified example can be found [here](https://play.rust-lang.org/?gist=96634f90a530a2f2a886975eaa1feb90&amp;version=stable).
That thread was very frustrating to read. The `stdsimd` crate exposes low-level compiler intrinsics. I have a hard time understanding the author, but to me it sounds like it is arguing that Rust should not expose low-level compiler intrinsics but a higher-level SIMD solution instead. Doesn't one need the low-level intrinsics anyways to implement a higher-level solution? 
I *think* the syscall crate has been abandoned some time ago. I recently used [sc](https://crates.io/crates/sc) (japaric's fork) instead.
Right, I'm not suggesting the types would influence the parse, only that they would mitigate the possibility of an unintended parse leading to silently wrong behavior at runtime. ...which is why I don't think the ambiguity is an important part of Rust's appeal, and why I don't think it's a good reason to avoid that syntax.
Thanks, haven't seen that before; I'll give it a try tonight.
s/program too/program to/ FTFY. ;)
The added boilerplate is a bit annoying though.
&gt; People want vendor intrinsics. Ah, but are they perhaps asking for faster horses when they really need a car? :p &gt; The vendor intrinsics are the keys that unlock the full potential of your CPU. Well [apparently](https://github.com/rust-lang-nursery/stdsimd/issues/27) it's okay for them to not even map to the instruction they claim to map to, so the state of these intrinsics is already that they are more like suggestions than a guarantee? &gt; Sounds like healthy experimentation to me. Forcing a lot of duplicated effort to reinvent something that already exists but is not provided deliberately doesn't sound very healthy to me.
You code get desugared and the result uses those traits explicitly. `for a in b` becomes `let mut b = b.into_iter(); while let Some(a) = b.next()`.
&gt; Ah, but are they perhaps asking for faster horses when they really need a car? :p No, I don't think so. &gt; Well apparently it's okay for them to not even map to the instruction they claim to map to, so the state of these intrinsics is already that they are more like suggestions than a guarantee? We do the best we can with the tools we have. &gt; Forcing a lot of duplicated effort to reinvent something that already exists but is not provided deliberately doesn't sound very healthy to me. I also said this: &gt; Hopefully we will get a champion to push towards a cross platform abstraction that we can expose from std. --- I will say that I found your comment to be unhelpful. If your tone doesn't change, then I'm not going to engage you any further because I don't have the patience to deal with it.
But wouldn't constant propagation also prevent branching? If I pass a `true` in `cmp_with_case` isn't the compiler allowed to create a special inlined version of that function without the branch?
I believe `(impl Iterator&lt;Item=T&gt; + DoubleEndedIterator + ExactSizeIterator + FusedIterator)` will be part of the supported syntax. Still, it'd be much more verbose to define, and to use for anyone who needs to store a complex iterator inside a struct.
There is libloading? or have I misunderstood your use case. https://github.com/nagisa/rust_libloading
Indices don't have a VAO while vertices do. They just have a u32, u16 or u8. glium uses [glDrawElements](https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glDrawElements.xhtml). See how the indices are a void* + an description of `GL_UNSIGNED_BYTE`, `GL_UNSIGNED_SHORT`, or `GL_UNSIGNED_INT`. That's why you can only construct an `IndexBuffer` from `u32`, `u16` or `u8`, respectively. A VAO (Vertex Array Object) describes a struct layout, i.e. if you have thousands of vertices, what are the struct offsets for each datatype into the struct (i.e. mystruct.position starts at 0 bytes and is 8 bytes long). This is calculated using the `implement_vertex!()` macro (in glium). All that glium does here is add some type safety wrappers around these functions (void* is not very type safe). Vertices and indices have nothing to do with each other. You should not be able to pass a `VertexBuffer` as an `IndexBuffer`. You can use empty indices, if you don't need them (in which case the graphics card will draw them according to the primitive type (TriangleStrip, TriangleList, TriangleFan, etc.)). But sharing VAOs between vertices and indices makes no sense, because indices don't have VAOs. No, this doesn't have any performance impact, you'd have to do the same in C or C++, just without the added type safety.
&gt; No, I don't think so. I guess it largely depends on whether they want to provide a very strong suggestion to LLVM to use SIMD in cases where autovectorization fails, or if they really need something like PHMINPOSUW. If a cross platform abstraction exists in LLVM right now for the commonly needed operations, it would be a shame if people have to use platform specific instructions instead. On the other hand SIMD support in Rust has been dragging on for a really long time now and I understand it must be frustrating for everyone involved, especially for those who need the more specialized instructions that wouldn't be available in a cross platform abstraction anyway. &gt; We do the best we can with the tools we have. I don't doubt that, it just seems that there is some disagreement about what "the best" should look like. If it is acceptable for LLVM to compile SIMD intrinsics to a different instruction than the intrinsic would suggest (because it thinks the other instruction is better, for whatever reason), then it doesn't seem like such a stretch to introduce even more abstraction at this level, especially since it wouldn't be feasible (like you say in another comment in this thread) to reintroduce such abstraction in a library. I'm actually not opposed to the whole platform specific intrinsics being stabilized. Perfect is the enemy of good, and a stalemate doesn't help either side. I agree with OP that it would be great to have a nice cross platform SIMD api, and that it would be a shame to have (partial) support for this in LLVM without making use of it. And I do find their argument that introducing platform specific intrinsics *before* a more platform independent SIMD approach will lead to code being written that is needlessly bound to a specific architecture. On the other hand, most programmers will never write a line of SIMD code in their life, and those that do will probably be happy to try out the cross platform options when they become available (where possible), if only to make their lives easier. So the amount of code that is (ultimately) unnecessarily platform dependent is likely to be small, and I agree this concern shouldn't block stabilization. &gt; I will say that I found your comment to be unhelpful. If your tone doesn't change, then I'm not going to engage you any further because I don't have the patience to deal with it. Fair enough. I suppose I reacted too strongly to your (as it appeared to me) curt dismissal of OPs arguments, and on a second read I must admit it fails to meet the standards of a constructive reply. Although the SIMD API discussion seems to have been frustrating for everyone involved, as an outsider I found reading it and seeing arguments for both sides enlightening. It also puts your original post (that I unhelpfully overreacted to) into a different perspective. I hope you've found my tone more pleasant this time. :-)
That's not the problem here. Consider [`std::iter::Map`](https://doc.rust-lang.org/std/iter/struct.Map.html): we have `impl DoubleEndedIterator for Map&lt;I, F&gt; where I: DoubleEndedIterator`. It is currently impossible to express such dependent typing using `impl Trait` and I don't think it ever will or really should pick up that functionality.
I have now updated the OP with this, as you changed my opinion. I didn't think about this, and really, the concrete types work here. Honestly, different discussions in this thread have made me less enthused to use `impl Trait` for trait-generic return values everywhere. I think the path for me will probably be fn returning `impl Trait` when they're made to a trait interface, but moving to a newtype wrapper for "release" quality.
I haven't had a chance to look in depth (I'm overloaded at the moment and dealing with other things), but from a quick look this is cool. I can imagine xi-rope having other uses, but haven't done the work of cleaning up the documentation and releasing recent versions. Happy to answer questions have about that.
C#'s specified behavior is that it uses monomorphization / polyinstantiation. The Java runtime and the CLR / CLI have many things in common, but their type systems are fundamentally very different. It's worth knowing the differences. I'm not trying to be a dick -- I just mean that it's worth the time to understand the CLR on its own terms, rather than looking at it as another Java.
I did not use the rand crate in a long time, but I think this is one of the cleanest ways to do way you want: http://play.integer32.com/?gist=c8461491de8cf9cd588c34932a3da3d5&amp;version=stable
In your `sub_exists` function, you can use the `find` function on the iterator instead of writing a for loop. something like ``` records.iter().find(|element| element.name == subdomain).is_some() ```
Please don't make JSON by string concatenation! the json! macro to create JSON objects instead of string concatenation. It prevents you from accidently creating invalid JSON https://docs.serde.rs/serde_json/macro.json.html or you can create structs that contain the data and convert them to strings https://github.com/serde-rs/json#creating-json-by-serializing-data-structures
&gt; but can cause problems with the inline ASM syntax I strongly disagree here. C and C++ make few guarantees about inline-ASM, and for good reason! Assembly is specific not just to an architectural family, but to the individual processor in question! **There is no way to stabilize ASM.** It will always be semi-stable. It *must* be. When a new processor comes out, it will have new instructions. Those instructions *will not* work on older processors. They definitely won't work on other architectures. Creating an entirely new, virtual instruction set just to ensure stability is absurd, and it would never support the instructions that are the *reason* people are reaching for assembly.
Is there an advantage to creating a custom iterator rather than using `(0..100).map(...)` if you're always collecting to a Vec?
I'd do it using an iterator, specifically take a 100 length iterator, map to random numbers, then collect: let mut rng = rand::thread_rng(); let numbers: Vec&lt;i64&gt; = (0..100).map(|_| { rng.gen_range(1, 21) // [1, 21) == [1, 20] }).collect(); playground: https://play.rust-lang.org/?gist=a0c2fe525aa1bd3f0267b38f05ca0d7e&amp;version=stable
I don't think there is an advantage, I just thought this code was clean overall. `(0..100).map(|| random_number_generation_here)` will indeed produce the same result but is not intuitive for a beginner IMO
(Author of faster here) I totally agree, and I'm working on it. As a matter of fact, I'd like some ideas for a less-verbose API. I was thinking of something like C++'s suffixes (which I don't think rust supports), but any other ideas would be welcome.
Oh yeah. Thanks.
Alright, that's reasonable. Doing a range iterator then mapping is what I reach for for simple code, but it is fairly unintuitive for a beginner. I guess I still want to expose this usage of iterators to beginners because it's very useful in practice, but you're right, it isn't the simplest.
I feel like a cross-platform API would be better served by something on crates.io, though. SIMD is a moving target, and having to go through the RFC bikeshedding every time Intel or ARM adds something which we'd like to expose could paralyze the project. Furthermore, API accretion could cause it to become crufty over time, and my understanding of Rust's stability promise leads me to believe we would be stuck with the API if we decide we don't like the design 5 years down the line. SIMD is especially vulnerable to this because vendors love pushing new paradigms with it (see Intel's mask registers, non-temporal hinting, etc). A library which has the option to release a 2.0 would be a much better option. Development is quieter, reacting to outside changes is less of an ordeal, design is only touched by those with a concentration in the area, and unilateral decisions are possible if the bikeshedding gets too wild.
I feel like you’ve measured intuitivity backwards here 
The Rust programming language.
&gt; Is the problem that the exe needs to still interact with the OS and these SDKs provide that? Yes, it is mainly for interaction with the OS or libraries present on the OS. Memory allocation would be one example. This is not the same allocation that you are using as a program, but OS level one which works in the level of pages. There is a layer in between that manages allocated pages. Another example would be opening windows and getting opengl contexts, or interacting with the GUI library of the OS. 
&gt; The library would require some modification Usually no modification is required as sciter-static.lib contains the same SciterAPI function but with internal linkage.
Wow, this is really awesome! I didn't know that this is existing. Do you think it is possible to extend this to handle Mac builds and possibly Linux Flatpacks? I mean this would instantly sky rocket my preferred GUI toolchain from C++/Qt to Rust/Gtk by saving a huge amount of manual release crafting for the former. 
Nice post! Found a typo: I believe you meant [clippy](https://github.com/rust-lang-nursery/rust-clippy), not lippy.
You can make your code easier to read if you extract the code out of main into functions such as `create_subdomain`, `update_subdomain`, `get_domain_records` etc. You look for a given subdomain twice: once in `sub_exists` and then, again, in `get_sub_id`. It would be better to just call `get_sub_id` once, but instead, make it return an `Option&lt;u32&gt;` or a known invalid id, if such a thing exists. You also trim the sub_id after returning it as a String, which is unnecessary.
&gt; Is the rust compiler treating the traits from the std library as kind of special? I'm seeing some of the language features disguised as traits from the std library. They're not disguised, traits are how you conform to a protocol, and the language can then have special support for protocols (iteration, errors, …) which you can take advantage of by supporting the relevant protocol, which you do by implementing the relevant traits.
I am preparing the release of `SliceDeque`, a double-ended queue that dereferences into a slice without using any extra memory: https://github.com/gnzlbg/slice_deque It currently works in Linux and MacOS X, but I plan to add Windows support before the 0.1 release. Also, it currently supports most of the unstable API of `VecDeque` as well, but for this reason it requires nightly Rust. I also plan to use a `build.rs` to detect nightly Rust and enable the unstable API automatically, and otherwise disable it and have it just work on stable Rust.
coreutils has a chown util that [uses](https://github.com/uutils/coreutils/blob/72b4629916abe0852ad27286f4e307fbca546b6e/src/chown/chown.rs#L266-L281) `libc::chown`
`String` is owned and can be mutated by its owner. A literal in immutable and can be shared, so it's `&amp;'static str`. Creating a `String` from a `str` involves copying and memory allocation, so it's explicit. In general you mostly use string slices as inputs, so you don't have to care about ownership, only switching to `String` when you really need to.
Rust has two types of strings. str(1), which gets embedded into the executable and is immutable and String, which is allocated on the heap and can be mutated. When you write "string" you create a str, so if you need to mutate it you have to transform it to a String with String::from.
`String` is heap allocated string, while `"string"` creates `&amp;'static str` which is essentially pointer to [data segment](https://en.wikipedia.org/wiki/Data_segment) of the compiled program, thus it can be mutated. `String::from("string")` and ` "string".to_string()` create heap allocated string and copy data into it from static string. While I think it's nicely helps to understand that happens under the hood, but on other hand it's understandable that it's perceived as a painful papercut especially for new users, so there is discussions ([1](https://internals.rust-lang.org/t/pre-rfc-allowing-string-literals-to-be-either-static-str-or-string-similar-to-numeric-literals/5029), [2](https://internals.rust-lang.org/t/pre-rfc-string-literals-through-prefixes/2928)) on how to change this behavior.
**Data segment** In computing, a data segment (often denoted .data) is a portion of an object file or the corresponding virtual address space of a program that contains initialized static variables, that is, global variables and static local variables. The size of this segment is determined by the size of the values in the program's source code, and does not change at run time. The data segment is read-write, since the values of variables can be altered at run time. This is in contrast to the read-only data segment (rodata segment or .rodata), which contains static constants rather than variables; it also contrasts to the code segment, also known as the text segment, which is read-only on many architectures. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/rust/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Not very helpful when it's a "hello world" style program to get familiar with the language
Maybe macros can help with the aesthetics, but I don't think there's any way to remove the explicitness of the splat here.
Literal strings are indeed embedded in the executable, but the `str` type represent a sequence of valid utf8 codepoints that can be anywhere (executable, heap or stack).
&gt; thus it can be mutated You mean it cannot be mutated. Which is one of the main reasons you want to convert it to String, to that you can own it and mutate it. 
Yes, I fixed the typo.
And I stand corrected. Thanks.
Can someone make a tl;dr of the state of the discussion about how to change it?
I agree on the Futures part. I have a situation where I use Futures without Tokio and the documentation for that is not clear. For future reference. If you want to use the Futures crate on its own you probably want to use the `futures::sync` primitives `oneshot` and `mpsc` and then `wait()` on them from the receiving thread. For composability you can use `join()` and friends and `futures::stream::futures_(un)ordered` to combine them into one big future to wait on. For me it turned out to be a great match in the end..
Something that I'm not sure is why `WebView.self.window` needed to be wrapped in a Box. When implementing `ffi.rs` I could pass `&amp;Window` to any C function and it worked but when I tried to pass `&amp;self.window` it gave me some boundary access errors.. I'm such a noob.
There are such examples [here](https://github.com/gtk-rs/examples).
I think everybody here said enough what those types are but ... Its interesting to note that you have this distinction in many programming languages. In the beginning of my Java career i had a hard time understanding the difference between `String` and `StringBuilder` and i ran into a performance problem before i get to know the difference. You have basically the same in C# with its own `StringBuilder` Rust was the first experience where i have to face this difference up front and i think its a better way to educate about this problem. Unfortunately Rust has this quirky way of naming these two different things. `String` and `StringBuff` (or something else) would be better but i think we just have to live with that. At a certain point you need to face this difference in many programming languages but you need to face this upfront in Rust. 
"lippy" sounds like a version of "clippy" with more attitude. 
The TLDR is that it will have a minor impact on some functions like `push` and mutable access. Folks are experimenting on it at https://github.com/rust-lang/rust/pull/46993
What is the difference between `ash` and `vulkano`/`vk-sys`? What are reasons to use ash?
&gt; Rust has this quirky way of naming these two different things. String and StringBuff (or something else) would be better but i think we just have to live with that. It's not really correct though, in Java `String` is an owned immutable string and `StringBuilder` is a way to build string, but in Rust `&amp;str` is not an *owned* string, it's a reference to a string owned by somebody else. Could be static memory, could be a bytes buffer, could be an actual `String`. There is overlap between `&amp;str` and Java's `String`, but there's also overlap between `String` and Java's `String`.
Thanks. This response was more helpful. :-) &gt; I guess it largely depends on whether they want to provide a very strong suggestion to LLVM to use SIMD in cases where autovectorization fails, or if they really need something like PHMINPOSUW. Well, I mean, yeah. In my domain (string search) for example, autovectorization doesn't cut it. I mean, this the whole point of vendor intrinsics: when you need explicit vectorization or explicit use of various CPU features (like, say, generating a CRC checksum). Autovectorization and vendor intrinsics are entirely orthogonal affairs. &gt; If a cross platform abstraction exists in LLVM right now for the commonly needed operations, it would be a shame if people have to use platform specific instructions instead. On the other hand SIMD support in Rust has been dragging on for a really long time now and I understand it must be frustrating for everyone involved, especially for those who need the more specialized instructions that wouldn't be available in a cross platform abstraction anyway. Keep in mind that there is no cross platform abstraction that covers everything. Platform specific instructions will always be needed. For example, in string search, cross platform APIs don't help me one bit other than to make certain things a little nicer while writing the code. I will still always need access to vendor intrinsics directly. &gt; If it is acceptable for LLVM to compile SIMD intrinsics to a different instruction than the intrinsic would suggest (because it thinks the other instruction is better, for whatever reason), then it doesn't seem like such a stretch to introduce even more abstraction at this level, especially since it wouldn't be feasible (like you say in another comment in this thread) to reintroduce such abstraction in a library. I don't really buy this line of reasoning. It is too abstract for my taste. &gt; and I agree this concern shouldn't block stabilization Great! As far as I can tell, this is a fundamental disagreement with the OP. The OP isn't arguing against platform intrinsics either. They are specifically arguing in favor of a stabilization process that couples platform intrinsics with a cross platform API. You disagreeing with that is disagreeing with the very crux of the OP's point. :-) &gt; curt dismissal of OPs arguments Yes, I was cranky because I just can't stand re-litigating things, and all of this was hashed out on the issue tracker.
Author here. I currently implemented it via a procedural macro, generating a new procedural macro emitting the code. Unfortunately, errors currently are completely opaque and rustc outputs that there is an error but not where. Scala LMS has similar issues when it comes to debugging afaik. My library is currently in a very early state, so I hope this can be improved!
I think the most interesting bit is that things like `deref_mut` would allocate, which I think means you can't make assumptions like "`String`'s pointer is stable" any more. (I don't necessarily have a problem with this, but it is a very interesting ramification.)
clippy already has enough 'tude. More would certainly be too much.
Why would you want to use dynamic dispatch for a problem that can be solved by static dispatch? As someone coming from a low-level C/C++ background (although I like Rust a lot more than C++), I've always seen dynamic dispatch as a necessary evil. Something you avoid unless you have to use it (because types are only known at runtime). It is an indirection with a large runtime performance overhead and it inhibits compiler optimisations. If you can do static dispatch, then dynamic dispatch is just slower and uglier. Both you and the parent comment you replied to seem to suggest that there are situations where both of you had chosen to do dynamic dispatch, even though you could have used static dispatch. Why?
I think I'm going to try to create a new stream adapter that combines ```Stream::then``` and ```Stream::filter_map```. I'll follow up to let folks know how it goes.
Well, #![no_std] // some shit about features, eh_personality etc. fn main() { let a = "Hello world!"; } should compile, while #![no_std] // some shit about features, eh_personality etc. fn main() { let a = String::from("Hello world!"); } should not. However, unless you're in embedded systems, it is indeed weird to use `String::from`. Therefore, if you are a library writer, I recommend you to use fn do_stuff&lt;S : Into&lt;String&gt;&gt;(name : S) { let name = name.into(); println!("Hello, {}!", name); } or fn do_stuff&lt;S : AsRef&lt;str&gt; + ?Sized&gt;(name : &amp;S) { let name = name.as_ref(); println!("Hello, {}!", name); } Which usually doesn't involve a memory allocation (as opposed to `Into`), but doesn't take ownership of the string either. 
Also, in Rust you **can** have mutable `&amp;mut str`, allowing you to change single characters in the string – but you cannot reallocate anything (the reference and its length must remain valid), and on the other hand you can have totally immutable owned `String`. In Java `String` is always immutable, while `StringBuilder` is always a mutable object (as similarly also is a thread-safe `StringBuffer`).
&gt; but in Rust &amp;str is not an owned string A more close equivalent would be `Box&lt;str&gt;`. Owned, but of fixed size. 
There are two completely different ways to approach this: The simple one is just to convert the type of "foo" from &amp;'static str to literal_string (which is exactly like &amp;'static str under the hood), and create implicit conversions from literal_string to both &amp;'static str and String, with automatic conversion to &amp;'static str whenever the type system cannot prove it needs a String. This is very close to how the numeric literals work today, and would change exactly nothing about how any current code works, except that usage of "foo" in a situation that requires a String changes from a compilation failure to be the same as replacing it with "foo".into(). The more complex way is to allow String to point to 'static data and allocate on modification and on deref_mut. This changes where code allocates -- it never allocates more than the existing situation (and in fact, real code will usually end up allocating less and thus being faster), but the function the allocation happens in moves from the constructor to any function that modifies it. I prefer the second way, but either approach is vastly better than the current situation. This is imho the biggest papercut left in the language after NLL gets turned on. And it's not just a beginner problem -- any code that does a lot of string wrangling that involves literals is just *really ugly* as of now.
Intelli-J is really coming along with its Rust support, just a month or so back they got initial support for inserting use statements for a type you're trying to use, and it already has lots of nice features like displaying inline the inferred types in every place where they aren't specified
I don't recall having to pass indices separately in C++ to my draw call. I could bind GL_ELEMENT_ARRAY_BUFFER containing index data to my VAO, then when I draw something I would simply bind my VAO, then call glDrawElements with 0 as indices argument. So it would llok like that: //Vertex buffer glBindBuffer(GL_ARRAY_BUFFER, target.vboID); glBufferData(GL_ARRAY_BUFFER, mesh.vertices_cnt * mesh.getVertexSize(), mesh.v_data, GL_STATIC_DRAW); //Index buffer glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, target.eboID); glBufferData(GL_ELEMENT_ARRAY_BUFFER, mesh.indices_cnt * sizeof(unsigned long), mesh.i_data, GL_STATIC_DRAW); //... Binding vertex attributes here ... //Cleanup glBindVertexArray(0); glBindBuffer(GL_ARRAY_BUFFER, 0); glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, 0); //... //When rendering: glBindVertexArray(_tq_vao); //... binding GLSL programs here, setting up uniforms etc ... //Rendering call: glDrawElements(GL_TRIANGLES, mesh.i_count, GL_UNSIGNED_INT, 0) 
&gt; Unfortunately Rust has this quirky way of naming these two different things. String and StringBuff (or something else) would be better but i think we just have to live with that. `Path` and `PathBuf` are better named, but unfortunately that ship has sailed for strings.
`"string".to_owned()` doesn't sound silly to me. 
I would love to see the kind of tooling discussed in this post.
yeah, you passed a null pointer. glium can do the same: /// No indices const NO_INDICES_BUFFER: glium::index::NoIndices = glium::index::NoIndices(glium::index::PrimitiveType::TrianglesList); // when you draw: target.draw(&amp;vbuf, &amp;NO_INDICES_BUFFER, my_shader, &amp;my_uniforms, &amp;draw_parameters,)?;
I'm talking about the Rust library, not the actual sciter lib. I linked to some meaningful Rust code..
I say the most idiomatic way is to restructure your types and come up with a nice nested hierarchy that composes well. Use a mixture of structs and enums. enum NodeChild { VariantA (prop1, prop2), VariantB (prop3) } struct Node { lineno: usize, // other common stuff child: NodeChild }
But it seems to just perform an unindexed draw, while my example is in fact indexed, it uses element array buffer, which is bound to my VAO. The "null pointer" is the offset of my index data, it doesn't mean no indexing is being used, it wouldn't even make sense, as there are other functions for unindexed drawing. I assume glium's VertexBuffer is just a wrapper for VBO, so no element array buffer is associated with it, but it could be with IndexBuffer, however these two are being passed separately, so they don't seem to be bound to the same VAO.
I couldn't get it to run at my work. It works for Python, but not for a C++ project - maybe the latter is too complicated?
But that would introduce undesired dynamic dispatch into all these places, which is what we want to avoid with `impl Trait`
Seems like I am always waiting on const generics for something. Physics, tensor processing, accelerators, and maybe even implicit bounds checking. Very exciting work. Along those lines, I am also interested in type refinement so that operations that work on a subset of variants can be guaranteed to get what they want. I see const generics as being a precursor because it may enable us to accept refined integers (within a certain range) which can elid some other checks. It will probably be longer till we have refinement on enums though.
I have this overwhelming urge to submit a PR so that my commit message can be, "one giant leap for mankind."
Rename to Text and TextBuf, export pub type str = Text; pub type String = TextBuf; slap a deprecation warning on them for a year or two String is a stupid word imo
The book has a whole chapter that covers this question in depth: https://doc.rust-lang.org/book/second-edition/ch04-01-what-is-ownership.html
Correct. It was build using `cargo build --release` I made one before with *walkdir* as well and this was a miniscule faster for my hardware and file structure. As a learning project I'd rather rely on as little external libraries as possible.
Haha indeed, thanks!
I think that data structure is missing a drain method. Drain is the Rust'ish way of pulling multiple stuff out a datastructure without consuming it. Of course additional utility methods could be added, like pop_front and pop_back.
i'm by no means a language expert, but i find that many of my java techniques apply to rust. 
glium doesn't map 1:1 to OpenGL concepts. And yes, there is a slight performance hit when using glium over raw OpenGL, however, in most cases it's negligible and as you add more and more draw calls, the percentage of the overhead shrinks. VAOs only describe the **layout of your struct** (and glium supports this). They are generated in the `implement_vertex!()` macro. glium chooses the method to draw (glDrawArrays or glDrawElements). Yes, glium binds all these buffers before drawing (instead of once when the VAO is created). However, this isn't a large performance hit. To change this, you'd have to redesign the API. glium does some tricks with `lazy_static!()`, so I am not sure if it caches the VAOs. You were correct about the offset. However, I don't know why glium does this or if there is a workaround - if you want more detailed informations, try to post about this as an issue on the glium repo or message /u/tomaka17 (the creator of glium). For games, I'd use [vulkano](http://vulkano.rs/) and the Vulkan API if it is available to you - it will be more performant than any OpenGL stuff.
Personally, I always use `"string".to_owned()` since I think it better captures the semantics of what I'm doing. I don't own `"string"`, but I want to. On most platforms string literals go into the `.rodata` segment, which is loaded as immutable by the program loader. Data in `.data` can be mutated at runtime, so it's essentially only `static mut` globals (with non-zero initializers) that go here. They're both data segments though, so you're not wrong. 
Fun, I regularly say that Rust can learn a lot from Java, too ;).
&gt; I’ll give an example: which language do you think of when I say “duck-typing”? Probably Ruby. Python, as Fredrik Lundh noted back in 2004: &gt; Python has always used duck typing (what's important is what you can do with x, not what type(x) happens to be). -- &gt; That wasn’t always the case. Indeed, Ruby was already quite an old language when it got introduced as a strategy: in Ruby 1.8.0. That was almost 10 years in! Nonsense, you don't need special API support for "duck typing", `responds_to?` is only an LBYL affordance, it doesn't make (or break) duck typing. As long as you can invoke a method dynamically and can somehow handle method missing/call failing, you have duck typing. Hell, back in 2001 [Matz stated](http://blade.nagaokaut.ac.jp/cgi-bin/scat.rb/ruby/ruby-talk/26814) &gt; "type" in Ruby is just an illusion. It doesn't really exist, but in our mind. We emulate it sometimes using a class of an instance, **sometimes using set of methods that an instance can respond**. (emphasis mine). In fact, significant/regular use of `responds_to?` [is sometimes called chicken typing](https://github.com/myronmarston/ruby-conf-2011-notes/blob/master/writing_solid_code.md).
Easier to do it this way :) Thank you all for the feedback! All of it has been very enlightening and brought to my attention even more areas to look into and learn about. I'm going to keep iterating on this small code base and then look at doing something "greenfield" with rust next. Coming from Go it has been quite the learning experience but it's been fun for sure.
&gt; Nonsense, you don't need special API support for "duck typing", responds_to? is only an LBYL affordance, it doesn't make (or break) duck typing. As long as you can invoke a method dynamically and can somehow handle method missing/call failing, you have duck typing. Read the link I put there, you'd find this: &gt; If there is a primary thrust to Ruby 1.8.0 (and the releases building up to it), it is duck typing. We differentiate our objects by the methods that they possess. You should see more and more respond_to? in Ruby code rather than kind_of?. This concept is a core mantra of Ruby, right alongside the principle of least surprise (POLS). The entire point is that it needs no API or language support. "duck typing" is a _strategy_ that Ruby started adopting rather late.
&gt; String is a stupid word imo Everybody uses it though. Is it really worth the churn to change it?
No, this specific sentence is the core of this: &gt; If there is a primary thrust to Ruby 1.8.0 (and the releases building up to it), it is duck typing. 
I should have been clearer. When I wrote "highly unoptimized" I meant that the implementation was not optimized for only counting files. For example it wastes a lot of time creating `std::string` and just throws them away. Rust binary was built with `cargo build --release` and the C++ binary was built using `-s -O2`.
That is not evidence for duck typing being introduced as a strategy anywhere near that point, only for the point at which the Ruby developer team decided to create additional specific affordances for the aforementioned strategy.
In a similar vein, but much simpler, I wrote [this tool](https://github.com/coder543/cryptoticker) to sit in the `tmux` statusbar and show prices in an always-visible location. I also (unfortunately) believe that `crypto-rs` is not the best name. "crypto" is not "cryptocurrency", as /r/crypto would be more than happy to point out.
I'd really be in favor of being able to just put all the build target stuff below `~/.cargo-target/$project` or something. Mainly so I don't have to remember to `cargo clean` all the projects before doing backups. For example a one-off try-out project that only has a couple of files: phaylon@anubis:~/git/brimstone$ du -sh * 20K Cargo.lock 4,0K Cargo.toml 32K src 14G target 
Thank you that actually did test a little better. Hoping I used it correctly. I've updated the original post with Edit #2
Thank you for you interest. I've updated the initial post with a little more information including the relevant C++ source lines. &gt;pre-allocating That did actually seem to help a little. My stack size became 10338 deep so now I preallocate to the next power of 2 greater than that. &gt;lock and flush the stdout buffer Thank you for the example code. I used it in mine as well but I don't think it made great difference in this case where there are only 12 arguments to iterate over. Still it's very useful information for future reference.
At present it's being only invoked with 12 arguments. &gt;It's only proper that you share what you're comparing your Rust program too. :-) Agreed. I've updated the original post with Edit #2.
I almost got it but passed because I thought it looked too basic/lightweight. I could be wrong, and at $10.99, probably worth a shot.
That seems like a pretty big oversight.. any reason why this doesn't already exist in the standard library? EDIT: Looks like an issue already exists, just waiting for someone to write the pull request. Still strange that the original author didn't include this feature from the start. https://github.com/rust-lang/rust/issues/42849
&gt; &gt; I’d like us to stabilise imperfect things &gt; That's a nice sentiment ... I got the impression that OP was talking about libs not rust-lang itself.
That makes sense!
A good interface to go from non-futures code to futures is the channel. There are some caveats, and I would like to get a second opinion here, but the futures-friendly [mpsc channel](http://alexcrichton.com/futures-rs/futures/sync/mpsc/index.html) It is poll-able and works as a stream and a sync, for futures. The unbounded channel won't block the synchronous code, the send method, `unbounded_send` returns a Result&lt;()&gt;. So you don't need any special machinations. However, to send from futures code to non-futures, you'd probably want a regular queue/channel that is unbounded, so that the futures code can send without blocking, and that the receiver can use in the traditional Rust way. Afaik, there isn't a queue that can be used interchangeably as sender and receiver for both futures and non-futures code. (maybe someone can correct me here) Another option: If your non-futures code is easily encapsulated, the [futures_cpupool](https://tokio.rs/docs/going-deeper-futures/tasks/#futures-cpupool) are another great way to run synchronous code within the context of a futures application. For your specific application.. I would recommend reading from stdin in a (blocking) loop in its own thread, then pushing the data into a futures::sync::mpsc unbounded queue. Then let the main thread run the reactor core and select over its events as well as events inbound from the queue. 
Hmm, I could use this to update a conky widget!
I'm not sure I understand this thread of discussion, or why – for the purposes of the article's thesis – the historical minutiae of the precise moment that duck typing was embraced needs to be nailed down. Isn't the salient point here that there exists *some moment* in the history of the Ruby ecosystem where duck typing was embraced, and that there's a possible counterfactual universe in which it was not – wherein the Ruby ecosystem looks entirely different even though the language is exactly the same? 
The first version seems less complex and I doubt that the reduced allocation of the second version really matters, because after all most programs will just have a few short literal strings.
Do you guys have any open-source libraries? I didn’t see a link to a GitHub page or anything like that. The reason I’m interested is that I’m a physicist and I imagine there’s a lot of overlap between writing code for robotics and writing code to control a bunch of equipment in an experiment.
Hey, I’m one of the authors of this library (it takes many to write a good HTTP/2.0 library, thanks Oliver, Sean, and Eliza!). Even though this is an 0.1 release, the implementation is already pretty advanced. The full HTTP/2.0 specification is supported and it passes h2spec. I wouldn’t say that it is quite ready for production yet, but that is where you all come in :) Now is the time to test it out and find bugs. H2 was built for Conduit (http://conduit.io/), so we will be continuing to invest in the lib. I’ll be around if there are any questions! Also, thanks to Buoyant for devoting so many resources to the project to make it happen.
Technically, yes. Consider the following example: #[derive(PartialEq)] struct Complex (f64, f64); impl Add for Complex { type Output=Complex; fn add(self, other : Complex) -&gt; Complex { Complex(self.0 + other.0, self.1 + other.1) } } Now, `Add` is a special trait, since it allows for _operator overloading_: assert_eq!(Complex(3.0, 1.0) + Complex(8.0, 2.0), Complex(11.0, 3.0)); However, we can also see the `+` operator as syntactic sugar, since `a + b` desugars to `a.add(b)`. The same applies to `Mul`, `AddAssign` and even `Fn(u32) -&gt; u32`. It also applies to `IntoIterator` and `for`-loops. It's just a convention for sugar to make the lives of software developers a bit easier. So yes, they're special, in a boring kind of way.
&gt; all but one of them Wait... Which one not?
`panic_fmt`
You're looking for /r/playrust
I made an issue [here](https://github.com/ErichDonGubler/not-yet-awesome-rust/issues/17)!
:confetti: I'm sure some are wondering, will hyper use this soon? Yes! There's a stability question here regarding minimum Rust version. The h2 crate uses the http crate, which requires associated constants. The minimum Rust version of hyper 0.11 is 1.18, which doesn't yet have associated constants. I believe it's probably wrong for a patch version to require a large leap in Rust versions. So, while having h2 internally is a for sure thing for 0.12, 0.12 will also include changes from the tokio reform. But what if you want h2 support earlier? I think it's fair to add h2 internally in hyper 0.11, behind a cargo feature. That way, existing apps don't break, and you have to opt in to the newer Rust version. It's a shame it can't be on by default... (Actually, a build script can detect the Rust version and enable by default then, but it probably would be very weird that your app was talking HTTP2 for you, but a colleague with and older compiler who was able to compile just fine, sees not HTTP2...)
It's great. Helps to understand the language
nursery crates have a "previous two rust releases" policy. i look forward to us all together figuring these things out
Diesel's current policy is "don't require a new Rust version unless you have a good reason" -- So basically we're fine with bumping if it enables new features, but not if it just makes implementing something more convenient. 
All crates I maintain have had a "patch releases should not break compilation" with a caveat of sometimes dependencies outside my control jump a bunch of Rust versions :cry:.
Whoops!
It looks to be like many classes; the material is all online, but released as the class goes along. This page has a bunch of material already.
Noted, thanks!
&gt; Inheritance, or some inheritance-like construct Made an issue [here](https://github.com/ErichDonGubler/not-yet-awesome-rust/issues/18). :)
No TLS?
The really problematic one isn't even deref_mut-- it's as_ptr. If I had a String and called as_ptr, I could reasonably expect to be able to mutate the string from behind that pointer (after, say, calling mem::forget on the String itself). There's not an easy way to make this work, since as_ptr takes an immutable reference. You'd need interior mutability with an atomic pointer inside String or something in order to perform the allocation.
Funny enough, one of my toy projects is a Minecraft server clone, and I was having similar thoughts as the OP. On the server side, recompiling with mods included is a much smaller issue than it is on a client, so I'm still interested in an answer to this question. Of course, if someone made a dynamic plugin system with no significant performance drawbacks then I would be all ears to that! I just can't justify the complexity of building such a system while I could justify building a compile time plugin system.
Oh yes. It is not a *replacement* for `impl Trait`. It is more flexible since it allows returning different types implementing the `Trait`, but this comes at a double cost: - the size is unknown, requiring `alloca` on the caller side, - the type is unknown, requiring dynamic dispatch. Still, this also has advantages over the current state of the art: - in return position, it would allow avoiding a `Box&lt;Trait&gt;`, - in argument position, it would allow avoiding monomorphization (code bloat, compilation times, ...). So it's a *complement* to what currently exist, *not a replacement*.
Congratulations on the launch! Thanks to everyone involved with this project. This is easily one of my personal most anticipated crate releases ever - a modern, async HTTP/2 lib powered by Tokio. OneSignal will be integrating this into our core Rust project ASAP. The h2 crate has a good client example, but it's missing asynchronous DNS resolution, SSL, and connect timeouts. I've written and documented [an example](https://github.com/jwilm/h2-get-demo) which includes those components as part of my own learning; hopefully it will be helpful to others.
It works fine with TLS :) See the akamai example: https://github.com/carllerche/h2/blob/master/examples/akamai.rs The library is designed to be flexible at the expense of requiring a bit more setup. The assumption is that Hyper will use h2 to provide HTTP/2.0 support at a higher level.
tokio-tls is notably missing ALPN support which is commonly used for protocol negotiation in lieu of HTTP upgrades. There is a tokio-openssl crate which supports it, but this has the downside of requiring openssl. Edit: There's [an issue](https://github.com/sfackler/rust-native-tls/issues/49) on `rust-native-tls` to add ALPN support.
This is really cool! How difficult was it integrating with the Redis module API? Have you written about this elsewhere?
More of what-not to-do than otherwise, I hope.
Coming from Python... Is there a library similar to NumPy in Python to do matrix operations in Rust?? I saw many of them but implementation wise and community support wise which one to choose for using and contributing
Is there any consensus around the production-readiness of _rustls_?
Have you tried to contact [mahkoh](https://crates.io/users/mahkoh) regarding possible ownership transfer of `http2` crate? IIRC he stated that he is willing to do it for appropriate projects.
Apparently the akamai.rs example linked in this thread uses rustls.
Well, I picked `or` because the English version is: &gt; My way **or** the highway. :)
The demo error message doesn't have an Oxford Comma, literally unusable. But very cool.
Looks like a non-trivial piece of code. I might take a stab at it this weekend.
I would be happy to write something soon. In the meantime you can find the API docs for Redis modules [here](https://redis.io/topics/modules-api-ref) if you're interested. They could be improved a little bit but for the most part I was able to get by.
Pretty sure the "simple" code here, from a beginner's perspective, involves a for loop. :p
https://play.rust-lang.org/?gist=b31d96e16938d7aa9c8f8c2009ab92b8&amp;version=stable
Ah, that's true. that's... much more obvious.
Politics should not be part of this sub no matter how reprehensible those words are. reported. 
Basically you answered for me. That's exactly what I'm thinking of: server-side mods.
Parsing within procedural macros is never a noticeable factor in compile times so I don't think it would be worthwhile to benchmark.
This reminds me of this project to port a rust version of wpilib for FRC.: https://github.com/robotrs/rust-wpilib Sadly it looks like it hasn't been worked on in months.
You are totally right, I changed the repo's name!
This example looks more complex than [Solicit](https://github.com/mlalic/solicit#simple-client) which I used for connecting to Nginx over HTTP/2.0. I do understand H2 crate provides granular control but simple wrapper/easy to use interface is preferred for easy integration.
Which is why I don't like the fact that `"string".to_string()` is the [officially recommended way](https://doc.rust-lang.org/nightly/book/second-edition/ch08-02-strings.html#creating-a-new-string) to convert `str` to `String`.
awesome! it's a cool utility!
I think the argument being expressed here is that there was no such moment in history unless you are counting the point at which the ruby's type system and method calling (message passing, ala smalltalk) was designed the way it is, ie before it was implemented. Of course it's indisputable that if the language didn't have duck typing built in, it would have created a very different ecosystem. For one, who knows if rails would exist. So the core point should stand, that duck typing influences the ecosystem, that's just a given I'd say.
When I first started scraping I used BeautifulSoup (https://github.com/adamhammes/WormToEpub/blob/master/ChapterCreator.py) Two reasons I switched: 1. This might have changed (or I might even be misremembering), but the return value of BeautifulSoup's various search methods can either be (a) `None`, (b) a single object or (c) a list of objects. That inconistency bothered me quite a bit. In contrast, I *always* get back a list of objects from `cssselect`. 2. Lxml has nice tools for generating HTML as well. Often I'm creating epubs, so it's nice to use the same module for both the scraping and creation aspects.
Depending on your needs, https://doc.rust-lang.org/std/collections/struct.BinaryHeap.html might be the tool to use.
I only recently got the feeling that i was able to write decent tokio stuff. It is not easy. There are a lot of little things i learned over the years that make things 'click' faster, for which i have no tutorial to point you to. It is (currently) very bare bone and you have to think about which future is polling other future's in what sequence , in what combination. People have built entire programming languages trying to figure out how to do async right ( erlang &amp; go off the top of my head ). &gt;Reading from STDIN while still catching events. This is what tokio is for. Last time i tried this i did it on unix with registering file-descriptor, but i believe i have seen a crate for it. It might be included with tokio at this point. Not sure TBH. &gt; To be honest, I just feel like rewriting the entire client to use tokio from the ground up would be easier than this. Yes. Maybe you should. I tried to add tokio to a project once, and it wasn't pretty and it didn't work. Starting with tokio and importing code from the old project turned out better. The most annoying 'failures' are the one's where you forgot to combine/'link in' a future and the system just doesn't continue. My advice would be , start really small. You will always run a single future on a single core. Then you start extending this future little by little, and you keep making sure that all parts of your code are actually reached. 
I actually wanted something similar to that, except without specifically caring about the C users, but I haven't found anything automatic so far. What I've been watching for is a Rust→C→Rust binding generator analogous to what `rust-cpython` does for Rust→Python and Python→Rust. That way, I could treat the C ABI as an internal implementation detail for a not-in-rustc/stdlib stable Rust ABI, similar to how I don't have to think about the CPython C API when I'm using `rust-cpython`. ...the end goal being writing a `dlopen`-based plugin API without having to manually convert to and from C-compatible syntactic constructs. (I'd try writing such a thing myself for the subset of functionality I need, but I barely trust myself around `unsafe` enough to call a single function from the `libc` crate.)
The goal is for hyper to provide the easy to use HTTP/2.0 interface. Hyper will build on top of h2.
A feature flag sounds like a reasonable way to get going on using h2 with hyper to me. Looking forward to being able to experiment with the combination.
Is there any consensus around the production-readiness of openssl? :D More seriously, I don't know. I would probably try using it, but ¯\_(ツ)_/¯.
I just released [actix web](https://github.com/actix/actix-web) framework, it uses `h2` crate for http/2 support. I can say, `h2` crate is very easy to use!
This may not be the right place to put it, but I was wondering how I could help out with Rust on GitHub as a beginner with the language
thanks! This was really helpful!
Check [ros-rust](https://github.com/adnanademovic/rosrust). We are using it happily, although haven’t updated to the latest release yet.
Nothing particularly *unique* to Java, however. And the flaws sometimes lead to excessive abstraction and decoupling (`SomethingSomethingContextFactoryAdaptorBean`) which is something Rust could definitely do without.
I just tried the code from Edit2 on macos and I got this numbers: C++ real 0m6.315s user 0m0.491s sys 0m3.478s Rust real 0m30.691s user 0m1.089s sys 0m25.568s As you can see that the main issue here is system time which is 7x higher in rust version. System time is the time spent in the kernel, so it's obviously not affected by a compiler optimizations, and other small things like print locks. It looks like rust version does a lot more syscalls or is using slower syscalls for the same task. Need to use `strace` to see what is happening.
If that's the case, you could have a build script that injects the plugin files at compile time from some folder. It might limit the structure of these plugin files, but you should be able to construct them so that they are just glue that compiled the real plugin (which may not be able to fit nicely in one file) 
There are several others in different stage of completeness. I think it is only one that is not based on hyper and one that supports both http and websockets at the same time
It'cool, make a restful SPA with Actix-web.
If you use break or continue, how are you ever going to use "x" in its unassigned state? you can't, so it's not invalid. Rust analyzed the situation and correctly determined it was safe.
As a library feature. Not like Haskell I believe https://dlang.org/library/std/functional/memoize.html
&gt; However, we can also see the `+` operator as syntactic sugar, since `a + b` desugars to `a.add(b)` Although keep in mind that this is not strictly true... (: // This compiles. fn foo() -&gt; u32 { 1 + 2 + 3 } // This doesn't. fn bar() -&gt; u32 { // error[E0599]: no method named `add` found for type `{integer}` in the current scope 1.add(2).add(3) } I've reimplemented Rust's typechecker in one of my projects and the type inference magic of the integer types was a real pain to support.
I neither argued for uniqueness nor have I said that Java doesn't have its flaws. I really don't see your point, sorry. I see that you don't like Java and will go great length to ensure people don't consider looking at it for inspiration.
You could perhaps use [sqlite precompiled to JavaScript](https://github.com/kripken/sql.js/) and then either modify Diesel's existing sqlite backend to use it, or write a new one? You could also try to get `libsqlite3` compiled as-is with Emscripten and link that with Rust somehow, and since Emscripten emulates a virtual filesystem AFAIK it should somehow work?
My point is that I don't think Java's implementations of any of these concepts are particularly worth emulating.
I believe `break` and `continue` "return" `!`, which allows them to be used in contexts such as this. I'm not 100% sure. `!` signifies that code past this point will not execute.
Please try this code, it should work faster than your implementation. http://play.integer32.com/?gist=6537b9ee937866af6d479e012255b3cc&amp;version=stable 
I really hope so, that would make my development so much more enjoyable.
In rust, the result of last statement without semicolon will be returned. And extra whitespace characters generally speaking do not matter. So when you write a b; Rust sees it as a B; Which is a single invalid statement (because there’s no separator in between) and does not return anything. That’s why it says it expected a semicolon.
From what I've seen, people tend to go with the later. Depending on how everything is modeled, you can also do: struct Foo&lt;'ctx&gt; { context: &amp;'ctx Context }
Almost! That’s the *type* that they return, which means “this never returns.” Hence it’s name, the “never type”. ! unifies with any type for this reason.
Hey, tooling is important, man! Props for speaking up. :-) We got to fight for that good development experience.
I've considered just implementing `Add&lt;f32&gt; for f32x4` et al, but I feel like we're approaching Javascript levels of wacky type coercion if we do that. I've settled on a name for both faster and the underlying vector library, but I haven't squatted it yet. The name will be changed some time between now and the 1.0 - whenever I feel like doing the clerical work.
It feels like most of the LLVM-related intrinsics are in stdsimd already. I was planning on forking stdsimd immediately before we make the switch to `__m*` types and rip out the LLVM intrinsics and doing some machine-aided transformations to make it wrap the new stdsimd instead. I'd also argue that a crate would have a much better time of juggling unstable and stable features, as stabilization is less of an ordeal and unstable features can be locked behind a crate-level feature gate with unlimited granularity, rather than a compiler-level `#![feature]` gate (which would likely have more limited granularity). 
This is almost but not quite what I wanted. I needed to pull the minimal element out of a BTreeSet each iteration, then on some iterations union another set into it. BinaryHeap can do heap.extend(set.clone().into_iter()), but that leaves duplicate elements in the heap. I wound up just using a variant of my good-enough-for-now take_min() code above. 
Unfortunately `Foo` and `Bar` actually represent references to memory owned by a C library, so doing that is somewhat complex. I'll definitely look into it, though, as it would be more convenient.
Respectfully, I don't think you're hearing me. It's not about the LLVM intrinsics. It's about not having the benefit of the code generator that resides in LLVM that knows a lot about how to implement even simple operations like multiplication. [Quoting from stoklund](https://internals.rust-lang.org/t/getting-explicit-simd-on-stable-rust/4380/209): &gt; There is a lot of details to work out. Suppose you’re writing impl Mul for i32x4. It would go something like this: &gt; &gt; * If MIPS MSA is available, use __msa_mulv_w(). &gt; * If ARM NEON is available, use vmul_i32(). &gt; * If SSE 4.1 is available, use _mm_mullo_epi32(). &gt; * If SSE2 is available, try to cobble something together out of 16-bit multiplications using _mm_mulhi_epi16() and _mm_mullo_epi16(), or maybe _mm_mul_epu32() combined with some shuffling. &gt; * Otherwise, expand into a lane-wise scalar multiplication. &gt; &gt; In particular, trying to construct an i32x4 multiplication out of existing SSE2 intrinsics requires some work and knowledge. Work and knowledge that has already been put into LLVM. &gt; &gt; This is just one operation for one type. There’s about 150 of those to go through. Then you would need to write individual unit tests for all of them since you’re guaranteed to have picked the wrong intrinsic by mistake at least once. Then find a MIPS machine to run your unit tests on. No, not that one. One with SIMD instructions available. When using LLVM with the appropriate cfg_target_features enabled, then all of this is decided *at compile time*. &gt; I'd also argue that a crate would have a much better time of juggling unstable and stable features, as stabilization is less of an ordeal and unstable features can be locked behind a crate-level feature gate with unlimited granularity, rather than a compiler-level `#![feature]` gate (which would likely have more limited granularity). Yes. I understand the general principles behind why putting things on crates.io is better. :-) (I am basically arguing Henri's point here about why the crates.io ecosystem is not necessarily an appropriate tool to solve this problem. Where Henri and I differ though is on path dependencies of stabilization; I (think) we agree on the details.)
That's why this is in /r/rust :) Invest in raspi3 if you like to experiment. Look at rust if you want beauty.
As someone who saw this talk in person, I highly recommend it. The most entertaining talk of the entire conference, IMO.
*Shh!*
To me it sounds sort of like a sneeze, but that's my only complaint.
&gt; Respectfully, I don't think you're hearing me. It's not about the LLVM intrinsics. It's about not having the benefit of the code generator that resides in LLVM that knows a lot about how to implement even simple operations like multiplication. Perhaps I'm not - are we both speaking about cross-platform LLVM intrinsic functions like `simd_mul`, `simd_shuffle_n`, etc? &gt; [snip] When using LLVM with the appropriate cfg_target_features enabled, then all of this is decided at compile time. I agree that not having LLVM's code generator makes any pure-Rust solution suboptimal. We simply don't have the manpower, nor the ability to get LLVM to commit to their API for life. That said, getting 80% of the result with 20% of the effort seems quite possible. Ignoring the SSE2 case, the quoted example could be machine-generated (along with `+ - / &gt;&gt; &lt;&lt; ^ &amp; |`) with just the names of each arch's instruction/intrinsic. Do we really care about machines which require special workarounds? HPC with a program compiled for generic `x86_64`, or on a chip with only SSE (or only AVX, for that matter) seems like a case of "doing it wrong". Even AVX512 looks like it'll only be incomplete for one generation.
Here's basically what I wrote before, when answering someone else's questions about that: * `ash` is very very lightweight, so it's easy to see the one-to-one correspondence between it and Vulkan, especially when following tutorials made with C-ish in mind (and there are at least two really good Vulkan tutorials written with C-ish in mind) * `ash` is used underneath by gfx-rs to interface with Vulkan, and eventually I want to move on to making gfx-hal/gfx-rs tutorials---the fact that `ash` is already being used by a prominent growing project such as `gfx-rs`makes me feel good vibes regarding open source co-operation/cross-contribution * vulkano has the cool idea of building a safe Rust wrapper around Vulkan, but in the process it's much harder to actually understand what's going on under the hood for a newcomer like myself: I found the guide to not be illuminating (not a fault of the project), and I found some of the design docs on github to be way above my level---**I hope to come back to it one day, when I understand Vulkan design and safety issues better, because I think having a safe wrapper is a worthy goal** Here's a new bullet point that I would add: * When I am browsing through the vulkano examples now (after a week and a bit more of fiddling around with ash), I find them making a *lot* more sense, so it seems that there is some pedagogical value to the whole "ash is more lightweight, so there is a pretty tight one-to-one correspondence with C-ish Vulkan, making it easier to follow non-rust Vulkan tutorials". vulkano has more convenience functions than ash, obviously, so its examples are a lot more succinct, but there it hides a lot of what is going on under the hood, for a new-comer...and I think that there is some value, for me at least, to know about what's going on under the hood. Plus, this way, one can hope to contribute more meaningfully to vulkano in the future, because you've got to know what's going on under the hood to figure out how to implement a safe wrapper Vulkan. **tl;dr:** I think (and I might be wrong!) that ash is lightweight, vulkano is more heavy-duty, given its aim of making a safe and rich wrapper around vulkan. ash is easier to use for understanding vulkan, and making tutorials, and learning ash will help you contribute to the gfx-rs project naturally (since gfx-rs uses ash). However, learning Vulkan details through ash will also help one contribute to vulkano, given that vulkano aims to build a safe wrapper around Vulkan, so you'd need to have some idea of what you are building a safe wrapper around.
I think this is just how the implicit return works. It returns the value of the last *item* of a block iff it is an expression. The last *item* of your block is `struct S;`, which isn't an expression, therefore nothing is returned. But this is just a guess, I couldn't find an official definition of implicit return.
Yes, you got my point now. You've made several blanket claims though, not all of which everyone agrees with. :-) Ignoring things will certainly make many tasks easier!
I don't think there is any perf difference. It's mainly for readability. I see you made the change in your code, but you still have an if statement. `is_some()` returns a boolean, so you can just return that directly instead of having the if statement and explicitly returning true/false
A `match` block can act as a statement if it returns the same thing as a statement: `()`. The compiler probably concluded this was meant to be a statement because of the following `struct S` and therefore has to return `()`, but because it doesn't, it throws the type error.
Hey, roborustaceans! Check my site for robotics rust libraries. http://robotics.rs
struct Point { x : f32, y : f32, } struct Edge { pointa: Point, pointb: Point } struct MyGraph{ edges : LinkedList&lt;Edge&gt;, } impl MyGraph{ fn AddEdgeIfNotAdded(&amp;self, Edge newedge) let foundedge = self.edges.iter().find(|e| e.pointa == newedge.pointa &amp;&amp; e.pointb == newedge.pointb); if let Some(currentedge) = foundedge{ self.edges.push_back(currentedge); } else { println!("edge already added"); } } The problem I have with the above code currently is that find returns a reference type and the push_back expects to use "value of currenttype", could someone help me to resolve this issue. Looked around could'nt find any way with the usage of find 
Need Help!!! struct Point { x : f32, y : f32, } struct Edge { pointa: Point, pointb: Point } struct MyGraph{ edges : LinkedList&lt;Edge&gt;, } impl MyGraph{ fn AddEdgeIfNotAdded(&amp;self, Edge newedge) let foundedge = self.edges.iter().find(|e| e.pointa == newedge.pointa &amp;&amp; e.pointb == newedge.pointb); if let Some(currentedge) = foundedge{ self.edges.push_back(currentedge); } else { println!("edge already added"); } } The problem I have with the above code currently is that find returns a reference type and the push_back expects to use "value of currenttype", could someone help me to resolve this issue. Looked around could'nt find any way with the usage of find
&gt; You can’t write a “real” OS in stable Rust yet, but you can write toy ones, and real OSes should be usable by next year. I'm curious what Rust is lacking to write a "real" OS?
Just sharing my experience - I've been doing some benchmarking work at my day job comparing common computation patterns across Fortran, C++, and Rust. I originally was using libtest for Rust, then switched to a stable crate version of the bench APIs. However I ultimately ended up going with [Google Benchmark](https://github.com/google/benchmark) . I created bindings for Fortran and Rust. The C ABI seems to impose ~5ns/iteration overhead (on my MacBook) which is not nothing for tight benchmarks, but it works well for most benchmarking scenarios.
Doesn’t support websockets. Speed is questionable.
I'll need some evidence for the performance claim
&gt; Java has a huge amount of practice around [...] generics programming. The impression that I had with Java generics back when I wrote java stuff was that using them is always a pain and that they are not really useful. When I then learned C++ it was far more fun to use generics. I was surprised: finally generics that work! Lately I had to do some java related stuff again and was very much shocked at how limited the generics support for Java was.... Java erases the generic type variables during compilation. The reason for this was ABI compatibility with non generic code but it has multiple real world disadvantages and ultimately it means that its generics are barely useful, for me at least. Kotlin has ABI compat to Java so it also has these erased generics. However, it does have a second system for reified generics. That system is limited to functions only though. So not much better either, although better (I want proper generics on classes!). Don't get me wrong. Java is the second programming language I've learned (more than 10 years ago now). I have positive memories with it. It is easy to hate on it with the experience with other languages I've gained but I think its big bonus is that its core language isn't very hard to comprehend while it isn't as artificially restricted as go.
Hope to see you there!
We've talked about using Rust instead of C for the equivalent course at my University. The blocker so far is that students want to get experience in an "industry-relevant" language. We're now counting the months until industry adoption gets to the point that we can say with a straight face that "Rust will be fine for that."
You probably want [`naive_utc()`](https://docs.rs/chrono/0.4.0/chrono/struct.DateTime.html#method.naive_utc). [Example](https://play.rust-lang.org/?gist=98aefdc0dd17127cd9a5efc4b65b61e5&amp;version=stable). The idea behind the name of the method is that the `NaiveDateTime` is inherently UTC, because that's how posix defines the `timestamp`, and it's sort of the only sane definition of "Naive". I want to more clearly deliniate the `from_*` and `to_*` methods in chrono, because they're all poorly named and lots of people run into confusion around construction and creation.
It doesn't as far as I know: fn main() { match () { () =&gt; () }//; () } This is a valid rust program that does nothing; you can uncomment the semicolon if you want to. This is a general thing with anything do with braces: fn main() { { let () = (); }//; () } Same thing.
Is it possible to build a [server-sent-events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events) library on top of Actix?
It is easy. Here is [sockjs server](https://github.com/actix/sockjs). It is abstraction over different type of http transport implementations. It supports xhr streaming, event source, websockets, etc I am still upgrading it to actix-web 0.3, but that should be done in couple days.
Thanks a lot! Do you have any tips on how to repeat stuff? On the server I did it using recursion, because I couldn't find any other way. Also, one thing that never made any sense to me: which callback should be in core.run over handle.spawn? The one that lives the longest?
Back to my computer. If a match block has value (), it's fine, because a () value is expected there. If the match has other values, then it will be a type mismatch. ``` fn foo() -&gt; Result&lt;(), ()&gt; { match 1 { 0 =&gt; Ok(()), _ =&gt; Err(()), } struct S; } ``` This one has two type mismatches. One is that the match block has type Result&lt;(), ()&gt;. Adding a ; will fix this one. The other is the return type is () because the last statement isn't an expression. You don't need a ; after } if the type is already (), like ``` fn main() { let mut a; {a = 3} {a = 4} } ``` But not ``` fn main() { let a = 6; {a + 3} {a - 4} } ```
Ada is also viable and arguably has more to speak for in terms of safety, though I can't say the development experience is as good.
&gt;Instead of only rotating 13 places, ROT26 rotates twice as many characters in the alphabet and is therefore twice as secure. Haha that's actually true, but only because `2 * 0 = 0`
I am a big fan of rust. That's why I lurk in this sub. That said, it's not perfect and some competition would be good.
thx
You need to add a way to copy or clone your strict (usually derive from the corresponding trait) and `push_back(*currentedge)`. You could also derive `Eq` to make your find callback simpler. Also I might be wrong, but your snippet does not seem to match the function name, you're re-adding the edge you found in the list, rather than add the new edge you did not find. Finally unless you're on nightly and have nll enabled I thing you're going to run into the borrow checker. 
Thanks Masklin for the solution.. I did manage to resolve the issue. I have derived the copy and clone for the structs
Congrats!!! What I'm wondering though: Isn't 0.1 super-conservative as a signal when being that feature-rich already?!?
if i understand your question correctly . to repeat stuff you have to fold. So my code tends to look like struct Server { &amp;handle, users , etc } impl Server { fn fold(self, msg) -&gt; Future&lt;Server&gt; } .... let fut = stream_of_msg.fold(server , Server::fold); core.run(fut); For the core.run / handle.spawn question. You can only run one future at a time through core.run. This future should be the 'big' future which is usually a combination of all the incoming IO. handle.spawn ( and 'futures_unordered' IIRC ) create new entry points (tasks) for the core to wake up / jump into. So make sure when you dynamically create some new IO future , you spawn it with the handle. P.S. If you test your code and somewhere it just stops and you don't know why. See if the struct (usually mpsc channels ) has a flush function. 
Someone on the HN thread noted that one of the instructors is Rocket author Sergio Benitez. I really enjoyed his RustConf talk and the overall design of Rocket also seems to be really good so the couse will probably be equally awesome.
You might like to follow the development of [Carp](https://github.com/carp-lang/Carp)
Ok, I just stumbled across https://docs.rs/futures/0.1.17/futures/future/fn.loop_fn.html. I think this is what I want.
You should include that you are using diesel! You can set the id field to be nullable, and make it an Option in your model *but I would not recommend that*. Diesel isn't generally meant to be used with one-struct-per-table. I have seen a third-party crate that introduces `#[derive(Model)]` to extend/generate Diesel code for this purpose, though. (Can't find the link on mobile, sorry)
I'm trying to like Intellij IDEA with the Rust plugin, but it's irritating. Little ergonomic things like the light themes being too bright and Darcula being undersaturated and back-arrow navigation being hit and miss. But mostly because it falls into a kind of 'uncanny valley' where you expect more and get disappointed. With the Eclipse CDT, I rarely break C++ programs because it really has got that good at identifying bad code. So I fall back to Geany, which doesn't try to offer a rich experience, leaving me with the code without the distractions of an IDE. Navigation is just right, provided that method names are fairly unique, and the bookmarking easy. However, I will say that for _beginners_ IDEA provides a marvelous thing; explicit type annotations. This is important because learning Rust means learning to infer types like `rustc`. (It also underlines mutable variables which is a useful piece of feedback)
we mentioned here before that you can write a proc macro that does exactly that, I think it would be a cool way of learning proc macros
&gt; It was originally written by @japaric, @japaric is actually a secret army of robots and everything was originally written by them. For some reason they seem to like us humans and to give us some purpose they sometimes decide not to release some pieces of code so that there is something left for us to do.
How do I do that?
If lippy finds more than lets say 10 thing to warn about it instead prints one of the "code quality" xkcd quotes. src/main.rs:8:5: 11:6 warning: Your code looks like you read Turing's 1936 paper on computing and a page of JavaScript example code and guessed at everything in between.
Your intuition is not correct for C# either, due to partial classes, where one compilation unit stretches across multiple files.
**I want to use a crate for each bloody simple operation**. Then what? I'll need other crate that fixes something what that 2nd doing wrong?
You are of course free to write something that fits your needs better yourself.
You have to use `std::str::Chars`. When getting the `nth` element, there might be no value, therefor the returned value is wrapped in an `Optional` - [code example](https://play.rust-lang.org/?gist=96510882e53b3484ff307d87883c38d2&amp;version=stable)
Thanks for Hyper and the work on getting it up to supporting HTTP/2. Cant wait for the day when Rocket + Hyper + H2 + Tokio provides a feature rich stack by default with all the bells and whistles. HTTP/2, Async, fast, and easy to use. 
&gt; h2 I read it as deuterium this whole time. . . :D
It would be easier if you posted a URL to your crate. Rust's explicitness makes reading other's code easier, but it's not THAT good that we can do without the code altogether. You may for example try to reproduce the issue on the [Rust Playground](https://play.rust-lang.org/).
You might be in the wrong subreddit - try /r/playrust instead
Haha, you are talking about the game? This is the subreddit for a programming language called "Rust". You're looking for /r/playrust. :-)
```rust let s = "abcdef"; let c = s.chars().nth(2).unwrap(); println!("{}", c); ```
MEDIC! OVER HERE! THIS MAN LOST AN ARM!
IMO their worst server and update to this day. 2018 and we are still using this caveman http/toyokio+toyoiekiari servers? Like real people? wake me up when we actually have the capacity to see the potential in shit
I’m confused as to why you don’t just release 0.12 with h2 and then 0.13 with the tokio reform. You’re not running out of version numbers or anything.
I must admit to being surprised seeing yet another article clamoring for a pause in RFCs. I would like to understand the reason: - Bandwidth issue: there are big features in the pipeline, they require manpower, let's focus on those. - Impact issue: the upcoming big features will impact Rust, experience is necessary before thinking of building upon them. - Polish issue: it's not enough to get a big feature out of the door, it should also be polished before jumping to the next one; less exciting, but necessary. - Stability issue: new features every other month make Rust look unfinished and/or make Rust difficult to learn/teach as it keeps shifting and good practices keep evolving. Or maybe another reason (or a combination). --- I find this interesting, because users in the C++/Java communities, for example, generally complain about the glacial evolution of the languages. I mean, modules in C++ were talking about **8 years ago** and are scheduled for 3 years hence, if everything goes well, for example. I personally think that there are still *holes* in the language that have not even started being addressed: - unsized types are very restricted, - variadics have not been touched upon (macros are not the end-all be all, especially since it's not possible to implement a trait for tuples outside its crate). This means that both unsized types and tuples are second-class citizens today. And a number of the next generation features are only getting started: - const integers are one thing, but `const LOOKUP_TABLE: HashMap&lt;&amp;'static str, Actor&gt; = ...;` is not there yet, - specialization is coming, but negative reasoning is still being thought upon. I don't see the need to stop thinking about those.
Oh haha yeah indeed. Im kind of new to reddit so xD
The hardest part would not be learning but motivation. Without having experienced segfaults, heisenbugs and race conditions one wouldn't understand why rust is great, like it is.
What are you trying to do that makes you need to do this? 
Those are not very expensive. You could always use a c-compiler with proven semantic and a proof assistant like Coq to prove your chosen invariants. The result is easily as safe as rust but you might have to learn a bit more...
https://docs.rs/tokio-io/0.1.4/tokio_io/io/fn.read_exact.html + https://docs.rs/futures/0.1.17/futures/stream/fn.unfold.html 
&gt; Clone is not trait object compatible, so a MyTrait that requires it, is not either. This is where I don't get it. But I think your post together with the other ones explains it. Because `Box` will coerce `Deref` then `let x = myTrait.clone();` ends up on the stack, and then x size must be known. Thanks!
Yes! This is it! The automatic unwrapping is what makes this impossible. Great!
I don't agree with that exactly. The `Clone` trait contains a method `fn clone(&amp;self) -&gt; Self` and this is simply unsupported by trait objects, to have a method that returns `Self`. The motivation for the rule is something along those lines; to return it by value, it must be `Sized` (the size of the value must be known from the type of the value).
I've not called for a pause in RFCs but for me it's a mix of bandwidth and polish. If you look at the goals for 2017, so many of them are going to be released in "early 2018". For example I still wouldn't call the IDE experience stable, like the 2017 blog post did - I haven't had great luck with completions. Compilation times still aren't great, even with incremental compilation. I feel these things should really be tackled, even if it's at the expense of new features. If they can be done at the same time, great, but I think they should be the focus
The key is "stable Rust". There are a number of features that are needed that aren't on stable yet. Using nightly works just fine.
What happens with numbers?
https://doc.rust-lang.org/book/second-edition/ch08-02-strings.html#indexing-into-strings
The day before my birthday!
Yes thats the same suggest I received in the other forum too.. Will read through and refactor it with HashSet. Thanks for the suggestion.
But is it zero-cost and thread safe?
Wow I'm thankful for your review. I'm starting to learn low level programming and I thought this project would help me to practice but it looks like I have bitten more than I can chew. The reason I thought I could omit some fields is bc I had seen that the `webview` function implementation in `webview.h` wasn't instantiating all members of the `webview` struct. Now with your explanation I understand why: it's bc that struct if fully declared in the C side so the program knows it's size and where to put things. `external_invoke_cb` and `userdata` are easy to implement, my problem is with the `priv` member since it contains platform specific data. I know Rust already have bindings for WinAPI, Cocoa and GTK but I decided left it for later when I obtained more knowledge. Do you think that I should instantiate the `webview` struct in the C side and just pass a reference back to Rust? So when I need to do an operation I just pass the reference back to C without the need to implement the `Window` struct. Is that doable? I'm also not sure how to do that. Another hypothesis is to check with C the size needed for `webview-&gt;priv` then set `Window.priv` to `[u8; priv_size]` in Rust before calling `webview_init`. Does that makes sense?
That's a good suggestion! Thank you! Does that means that I'll need to have default values for `title` and `html` also? I'm saying that bc in theory one would be able to `WebView::with_size(200, 200).join()`, right?
If those robots ever combine forces with the Alex Crichton AI we'll be in for a wild ride. I, for one, welcome our new Rusty overlords.
If you're doing that, consider treating your data as byte arrays, not UTF-8 strings.
If you're building the schema yourself you can use UUID keys and just generate a new one in rust when you create your struct. If you want sequential keys you have to create multiple structs. I do have a (unreleased, because I'm not sure if it's a good idea or has a good API) crate that can generate multiple intermediate structs (search GitHub for my username and "diesel-derive-intermediate"). It's been good enough for me for several months so maybe I should just publish it, but I'm not sure what level of production-readinesd you're looking for.
&gt; I feel these things should really be tackled, even if it's at the expense of new features. If they can be done at the same time, great, but I think they should be the focus I hear you. At the same time, though, I am not sure that someone with the skills (and persistence) to formalize the semantics of Rust would be best employed in helping polishing the IDE experience. Skillset, experience, motivation, etc... If we have only a handful of people capable of designing sound and practical semantics for Unsized Types or Variadics, I'd rather they work on *that* compared to working on improving the IDE experience or the incremental compilation which need more *manpower* and not necessarily more *brainpower*. Personally, I would not trust myself in trying to divine those semantics. I have a lack of formal type theory and understand of the Rust type-system/semantics that would lead me to overlook critically important details.
So I know other programming langugaes(Java, Python, and JavaScript) and I would like to take this course to learn Rust and more about OSes. Should I learn C first?
There's something wonderfully perverse about this. Bravo.
that's what I'm asking about 
&gt;If you're building the schema yourself you can use UUID keys and just generate a new one in rust when you create your struct. that's ginious.
I think Ada as a language is more or less secure than Rust depending on the area. For example Ada has refinement types and Rust doesn’t (yet), but Ada doesn’t provide a memory management solution built-in (for most of what I used for when I had a job using it, memory allocation wasn’t allowed, and I think that’s typical). It’s more the absolutely massive amount of effort that’s been put into the language ecosystem and tooling over the past decades that make it as safe as it is. Rust isn’t nearly mature enough to compete in the safety critical arenas Ada dominates. See [SPARK](https://en.m.wikipedia.org/wiki/SPARK_(programming_language)) as an example.
no
This kind of reads like that [joke](https://infinitetasks.wordpress.com/2009/08/02/sir-osis-of-thuliver/)... "What the heck is water?" "What the heck is segfault?"
Thanks, this looks like a very featureful framework! As for feedback: The user documentation has a lot of misspellings, I recommend running it through a spell checker.
In most languages with which I am familiar, `[2]` is effectively an offset from an initial pointer. Indexing like that is very fast and occurs in constant time irrelevant of how large the index is. As someone new to Rust, if I saw a `[2]` in Rust I'd initially assume the same thing until I had reason to believe otherwise. A method such as `.nth(2)` makes no implied performance guarantee. Thus I'd know to look up relevant details if I'm concerned about performance. Getting a given character in a unicode string is complicated and does not have the performance characteristics I would expect with `[2]`. Thus, `.nth(2)` is more fitting. 
Hey thanks so much for this! To truly unlock fearless encryption we need an asynchronous rot26. Do you plan on implementing this on top of tokio, or would you be open to mentoring that task? :)
Hi I just wanted to post a little cleaner code, if you were interested: fn human_readable(byte: u8) -&gt; char { if byte &gt;= ' ' as u8 &amp;&amp; byte &lt;= '~' as u8 { byte as char } else { '.' } } could become fn human_readable(byte: u8) -&gt; char { if byte &gt;= b' ' &amp;&amp; byte &lt;= b'~' { byte as char } else { '.' } } I am glad that this example exists, a lot of people say that Rust makes things like this too hard compared to C, but this looks really good IMO.
Does anyone have a working direct2d example on windows using only the new winapi 0.3 crate?
This is not specific to Rust/Diesel at all, but you can use this: CREATE TABLE foo ( id SERIAL PRIMARY KEY, -- more fields... ); The `SERIAL` type will auto-generate values on insertions. 
Brilliant idea! I'll look into this once I fix something else. But I'm always up to mentoring, if you want to implement this :)
You could shell out to 'cargo build' during the build script and then copy it. ^^ Do not do this
All right, thank you! Chalk is already somewhere at the edge of my reading list.
Dear /u/hatessw, We were born ready. Thanks for staying up-to-date with the latest encryption standards! Would you like to be featured on our "friends" page? Regards, The Rust ROT26 Team
A fast way to do it is this: my_str.as_bytes()[2] as char but it assumes you have only ASCII characters. 
Dear /u/HeWhoIsTheChosen, Incompatible characters are simply not encrypted. Regards, The Rust ROT26 Team
Dear /u/jannicn, We placed the bug there to provide a sense a pride and accomplishment for whomever found it. Regards, The Rust ROT26 Team
I'm under the impression that breaking changes actually do have a cost in terms of community happiness, and too many means people give up. So I try to group them together. I could be being too conservative here, that's true.
I wonder how would this branchlessness also relate to general resistance to timing side-channels. First time I got into touch with this concept was when I heard about DJB and NaCl. 
Good point and I agree. But, I made mistake. I'm meant "some text"[2], as OP asked, not someString[2]. That would be O(1) as it's stored as sequence of chars which represent unicode code point. And would have same return value as chars().nth(2). 
summary: WASM was 8x faster in chrome, 23x faster in Firefox
I was thinking something like that, and then heapify. Also, merging without duplicates shouldn't be annoying, if the keys are equal you discard one of them. 
[cargo-cook](https://github.com/vityafx/cargo-cook) or [cargo-make](https://github.com/sagiegurari/cargo-make) perhaps. [This page](https://github.com/rust-lang/cargo/wiki/Third-party-cargo-subcommands) has a list of third party cargo tools. Might be something else on there.
would super recommend it, with java python &amp; javascript you think in terms of objects and/or functions - with C and rust you think a lot more in terms of memory. If you're not used to this you're gonna get some really weird compilation errors in rust that don't make sense - it's not clear WHY the rust compiler is so great until you know all the things that can go wrong at runtime in C.
I learned Rust without knowing C just fine. Then learned C in a few days due to knowledge of Rust. Easier to go from Rust to C than C to Rust.
Ada (or rather: SPARK) can prove some things that Rust can't: - array index out of range - division by zero - numerical overflow In Rust, these are caught at runtime, in SPARK you can use GNATprove to catch them at compile time, so that they can never occur. 
Yeah, with ; it will return (). The code is valid. The type error comes from the last statement of the function being different.
здоров!
There is no named Rust string type in the standard library that is represented in memory by a sequence of Unicode codepoints.
I wrote an OS for RasPi once. It could blink the device's main LED. That's all it could do. Its most advanced feature was interrupt handling. The code is here in case anyone is interested. https://github.com/pczarn/rustboot
That's the thing, it's not code points; the string is UTF8 encoded which is byte based, but I think the individual bytes are worthless unless you are working with a subset like ASCII. See these two examples of two-codepoint strings: fn main() { println!("{:?}", "👍🏽".as_bytes()); } &gt; [240, 159, 145, 141, 240, 159, 143, 189] fn main() { println!("{:?}", "ab".as_bytes()); } &gt; [97, 98] I think I'd rather have indexing syntax even though it has to be O(n).
small nit: this technique will work in slightly more cases than ASCII-only. In particular, if you have latin1 encoded text, then this will be a valid interpretation.
For that to work, `"some text"` and `"some text 👍🏽"` would be different types, which would be really confusing and generally unwieldy. Consider, for example, a `HashMap&lt;?,?&gt;` which is used to look up a given (constant) message in different languages: use std::collections::HashMap; let mut greeting = HashMap::new(); greeting.insert("English", "hello"); greeting.insert("Chinese", "你好"); This would result in a very confusing compile time error, because "hello" is a different type from "你好". Rust wouldn't be able to figure out what `greeting`'s type is. What Rust chose to do is to make string literals the most broad applicable type by default. If you want something else - such as a fast index-able sequence of one-byte characters (which, in Rust terminology, is `&amp;[u8]`) - you have to explicitly specify it. 
Amethyst seems to be coming together nicely. Great job! I think the main improvement for Amethyst would be to improve on documentation (api, examples, guide) =).
That's a terrible solution. Decimal digits should be encrypted using rot10 to prevent brute forcing.
I thought str is array of u32. And String is array of u8. Guess I'm wrong? (just started learning rust). 
argh, I should have seen that, changes pushed!
Ah, the Oracle model. You guys really are enterprise ready!
&gt; Doesn't #[bench] do this as well? That's possible. Maybe I need to look more closely. &gt; cargo bench-cmp does this too, have you checked it out? One of my requirements is to be able to do comparisons across the programming languages. I could do bindings for cargo bench instead, but since it's not stable and doesn't have as many features, it doesn't seem like the best choice for the multi-language environment. One of the best things about Google Benchmarks is that it supports parameterized tests, which cargo bench does not yet.
Agreed, actually. The point of 'learning C' is not the syntax (that should be quite trivial anyway), but the 'abstract machine/programming model' that C works with, which is actually quite close to what the *concrete* architecture of your computer exposes. Data types, bytes, words, addresses, alignment, pointers, structs, stack frames, memory allocation etc. etc. etc. The guarantees that Rust provides can come soon afterwards. (Basically in an ideal CS curriculum, you shouldn't expect - much less be expected - to write anything but trivial CS101-level programs in C itself!)
Well, I assume so, but in truth I just got started with rust like a week ago so I can't really say for sure.. Still trying to figure out the kinks myself :)
Are Akamai and co. not already using unikernels or something? It seems like kind of an obvious thing to do these days if they're really spending that much time switching in and out of the kernel. Just curious. I can definitely see how it would help a lot of people without the resources to do it themselves. On a broader, strategic level, I'm not sure Rust needs a "killer app". We have the attention of the developer community, we just need to not mess it up too badly. I'm still holding my breath for the first major CVE in a serious Rusty system...
"Character" is a super meaningless thing to say in Unicode. Code point? Grapheme cluster? Something else? (There are language-dependent "something else"s which may be more reasonable than code points or grapheme clusters.) That's the entire reason this conversation exists. If you're parsing a defined format (e.g. json), you have a few options: 1. Decode ahead-of-time to UTF-32 (or another fixed-width encoding) and enjoy constant-time indexing at significant memory cost 2. Operate in UTF-8 (or another variable-width encoding, but I'd strongly recommend sticking to UTF-8) and save memory and possibly a decoding step, but suffer linear-cost indexing or memory overhead to build a code point index over your string 3. Operate over bytes and verify UTF-8 correctness after, if at all. If you need to consider multibyte code points, this obviously either won't work or quickly becomes intractable, but most machine-readable formats define their syntax in terms of single-byte characters, and you will save memory overhead, indexing time, and even initial UTF-8-correctness check time. If you're parsing natural language strings, well, I'm sorry. =/ That's a *hard* problem in general, as evidenced by just how complex Unicode is. There are certainly no easy tricks that will work in every language.
But this let s = "àb".as_bytes(); println!("{},{},{}.", s[0] as char, s[1] as char, s[2] as char) prints: `Ã, ,b.` `à` is a latin1 character UTF8-coded by two bytes.
is there a production level unikernel?
This is annoying, I confess. Is there any way to clone a trait object and get a `Box&lt;Trait&gt;` out of it?
of course it's created this way. but if "id" provided, it won't be autogenerated
If by killer app you mean 'project written in Rust', then maybe. But I'm personally convinced what Rust lacks the most is a plethora of learning resources that hold people's hands while learning Rust. It'll come - but the sooner we reach that point, the more widely used Rust projects there'll be for any given point in time. (And I guess this could be my minimal contribution to the recent request for feedback.)
I've seen Carp in the past. It might be time to try it out. One of the reasons I've been avoiding it is that the memory management doesn't feel as explicit as Rust.
It's all related. People will make resources if a lot of people are willing to learn. A lot of people will be willing to learn if there are uses cases that other languages don't serve.
Modules isn’t a full list either, only the submodule of the module you’re looking at.
As with anything, try it and see. It may be better, it may not. If it was always better it would be on by default.
If you have a UTF-8 encoded string, then my comment about latin1 doesn't apply. What I mean is that if you have a latin1 encoded string, then `latin_byte as char` will turn that into the correct Unicode codepoint. (The first 256 Unicode codepoints are latin1. The first half of that block is ASCII, where each codepoint is encoded as a single byte. The latter half is the rest of latin1, but UTF-8 requires two bytes for each such codepoint, where as in latin1, each such character is encoded using a single byte.) I suppose in retrospect my comment is strange, since `my_str` is a `&amp;str`, not a `&amp;[u8]`. So I didn't properly qualify my nit. :)
&gt; S3 or Akamai could probably reduce their server costs to a fraction of the current costs. Nope. It might save a few percent, which would definitely be important to them, but it's not going to make a huge impact. HTTP is already being handled pretty efficiently by the kernel-userspace combination. It's part of why everybody uses Linux instead of Windows for this. Although…context switches are going to be a lot more expensive now because that new attack on Intel CPUs. What were they calling it again? "Shithole", I think? So maybe it's time to bury a bunch more of this stuff in the kernel. Although Linus and the crew will never take the patches — they are busy trying to move stuff *out* of the kernel. So probably not.
The encryption crate we needed — and the encryption crate we *deserve.*
I think it's got plenty of guides etc. more significant IMO is tooling i.e RLS quality/IDE integration 
Yes, forgot about that
You are going to completely circumvent userspace switch. How could it be only few percent. &gt; It's part of why everybody uses Linux instead of Windows for this. I don’t think so. Windows actually has features for shit like this. They are moving shit out of the kernel because they are writing C.
The Rust learning resources seem to fall into the reference guide category most often, and I'm not familiar with any that gradually attempt to introduce you to Rust from 'hello world' to 'write your own microkernel' (so to speak). It felt to me as if there was very little meaningful diversity, at least a little while ago. Even better would be the same thing, but as an online course, because that's a great motivator. Admittedly, I haven't evaluated the O'Reilly book yet, which I think has been released since.
I don't see how this would help rust in any way. Things that help languages are things which help the language's ecosystem or make it more suitable for a use-case. For example, rails helped ruby because it created an ecosystem of projects and made ruby more suited for large-scale web development. On the reverse side of it, single projects (such as kernel modules or user-usable binaries) do very little for language's adoption in general. Apache's mod_php helped php adoption, but it was written in C and did nothing for C's adoption. Furthermore, I don't see how an http kernel module could possibly be a "killer app". Thee's a reason people use userspace http servers (be it nginx or rocket or such), and that reason is rarely because of performance after a certain point. Most http servers (e.g. nginx) are fast enough that your application will bottleneck elsewhere, e.g. by running out of database connections. Sure, processing http in the kernel could be a tiny bit quicker, but it won't be quicker *meaningfully* because it won't change the average user response time (still on the order of 100ms-500ms due to network latency and application latency), and it won't make it scale noticibly better in terms of resource utilization (database connections will still limit it to xxxxx active users). This would be a cool project to write as a "look, you can do this", but I can't see anyone really using it. &gt; I can imagine this being a very successful open source project because say S3 or Akamai could probably reduce their server costs to a fraction of the current costs. They could reduce server costs by almost none, and in exchange they'd lose almost all the monitoring, expertise, and software they've written now. It would probably cost more to rewrite even a tiny bit of their stack to use a new immature difficult to deploy (due to being an out of tree kernel module) http processing library than they would save in the next 500 years in cpu time. &gt; Ingo Molnár... The idea never took off because there wasn't a safe language to write this in (I'm guessing) I don't think you should speculate about why it didn't succeed. I doubt there's any evidence that if it were written in a safe language it would have succeeded... in fact, I think not being written in C is basically instantly disqualifies a kernel module from existing currently, and probably for the next 10 years at least. &gt; and also it was a different time. We're in a different time now. We have nginx and apache, we have haproxy, we have service proxies like finagle. In this climate, it seems even more unlikely that something which competes with those, but lacks most of their features and usability, could have a hope to win. 
To increase performance in the rot26 case simply return the original string. I can't believe this thing is that inefficient!
Does it web scale?
&gt; of course it's created this way "Of course"? You explain *very little* in your question, so that was not obvious to me. If you have `id SERIAL PRIMARY KEY` in the database, then I don't see what the problem is. Everything is described very well in the [Diesel tutorial](http://diesel.rs/guides/getting-started/). Just don't include `id` in the `Insertable` struct, and an ID will be auto-generated for you. &gt; I don't want to create a second model You don't *want* to? Is that the only problem here? What's wrong with doing it the way the the designers want you to? &gt; with the only difference being that 2nd one will be without "id" That wouldn't be the only difference. `Queryable` structs need to allocate new memory to store the data in, and therefore look like this: #[derive(Queryable)] struct User { id: i32, name: String, // owned data } `Insertable` structs, on the other hand, only need to view data that you have been given from somewhere else. #[derive(Insertable)] struct NewUser&lt;'a&gt; { name: &amp;'a str, // borrowed data }
you could also write your own script in Rust. Rust is more verbose than `make` or `sh`, but it generally seems to provide cross-platform support with minimal effort.
&gt; You are going to completely circumvent userspace switch. How could it be only few percent. They're probably already batching up the operations which require context-switching so that it doesn't happen very often on a well-configured server.
AFAICT the performance hit comes from slowing down syscalls, which has nothing to do with whether a language is interpreted or compiled, slow or fast.
Measuring response time for "hello world" is not a useful benchmark, and frameworks that optimize for that benchmark often do so at the cost of performance in real world scenarios
Yeah, i wish there was a much bigger focus on C, just the knowledge of pointers, whether memory is contiguous, and stack/heap allocation totally changed the way i look at most programming language (barring super high level scripting stuff like python)
You are also flushing the cache which puts a dent into your perf. Like by a lot. You go from like 100% to 20% very quickly until the cache fills up again. Re windows: they probably weren’t making a comparison based only on speed tho. No this optimization has very noticeable impact. You don’t need the kernel to accept this? I mean what do you mean they don’t use kernel modules not shipped with tree? Has there ever been a need for that?
The design of the tokio API certainly requires brainpower, but it's a different kind of brainpower than the design of a language feature. That's why I spoke first of *skillset* and *experience*. We have extremely talented people on the various Rust teams and in the community, but they are *not* interchangeable. Each have domains of expertise and preference. --- I *do agree* that a solid IDE experience and a snappy compilation would be extremely appreciated by all. I just don't think that a "all hands on deck" approach would be the best way to get them. Adding more people to a project yields diminishing returns, both from communication overhead and from the inability of having everyone working exactly on where they could contribute the most. And of course, treading on each others' toes (aka: merge conflict). The higher the number of people involved, the greater skill it takes to plan the tasks and distribute them across the team. And this is only made worse with contributors having different experience and time to spend on various tasks. I doubt that doubling/tripling the number of contributors overnight would go smoothly, it takes time to grow smoothly. --- Honestly, I am *surprised* at how well supported Rust in IDEs today for such a young language. Then again, I come from C++, where even the best IDEs are nigh useless as soon as templates kick in (the latest CLion does not detect the usage of template arguments method calls in template bodies, does not detect the usage of constructor when using `make_unique`, etc...) and compilation has always been a nightmare. So maybe I've just got Stockholm syndrome :x
The Rust Book is a great introduction starting with Hello World, and there are more advanced materials like [Writing an OS in Rust.](https://os.phil-opp.com/) I don't often find myself in agreement with /u/dobkeratops, but I do here.
I want to measure framework overhead. Hello world seems good candidate for this purpose. I also added actix to [TechEmpower Framework Benchmarks](https://github.com/TechEmpower/FrameworkBenchmarks/tree/master/frameworks/Rust/actix). You can do the same for gotham. Then we can see independent results.
It would be cool to have a group to go through the assignments as they're released. 
But then you have to compile it for all the target build platforms. Which seems unnecessary :P Personally I would just use a shell script, but I just focus on Linux and don't care for such compatibility concerns.
I figured the script could be a `bin` target in the main repo, so the first step to begin development on a new machine would be to install that binary. git clone my-wasm-repo.git cd my-wasm-repo cargo install --bin helper helper --whatever But, I dunno.
Is there an example of how to use DB without ORM (nothing against diesel or ORM, just sometimes it is not necessary). It would especially be useful to see recommended setup for db connection pooling withing actix framework (sqlite example will be good enough).
&gt; I don't see how this would help rust in any way. A popular project will increase languages adoption. &gt; Things that help languages are things which help the language's ecosystem or make it more suitable for a use-case. Right, like say fast HTTP processing. &gt; For example, rails helped ruby because it created an ecosystem of projects and made ruby more suited for large-scale web development. Kinda like a fast HTTP middleware? &gt; On the reverse side of it, single projects (such as kernel modules or user-usable binaries) do very little for language's adoption in general. Apache's mod_php helped php adoption, but it was written in C and did nothing for C's adoption. C was already extremely popular. If C was an unknown language, you dont think that it would bolster C's adoption? &gt; Furthermore, I don't see how an http kernel module could possibly be a "killer app". Because everyone needs to be serving static files. &gt; Thee's a reason people use userspace http servers (be it nginx or rocket or such), and that reason is rarely because of performance after a certain point. There are many reasons. It's just that security beats speed. &gt; Most http servers (e.g. nginx) are fast enough that your application will bottleneck elsewhere, e.g. by running out of database connections. Right. So there's no point in optimizing what you are saying? &gt; Sure, processing http in the kernel could be a tiny bit quicker, but it won't be quicker meaningfully because it won't change the average user response time (still on the order of 100ms-500ms due to network latency and application latency), and it won't make it scale noticibly better in terms of resource utilization (database connections will still limit it to xxxxx active users). No, but it could reduce your number of static servers drastically. You are underestimating the power of 0. &gt; I can imagine this being a very successful open source project because say S3 or Akamai could probably reduce their server costs to a fraction of the current costs. Me too, that's why I mentioned it in the post. &gt; They could reduce server costs by almost none, and in exchange they'd lose almost all the monitoring, expertise, and software they've written now. Bold claim. Like they are deploying services using new technologies right? &gt; It would probably cost more to rewrite even a tiny bit of their stack to use a new immature difficult to deploy (due to being an out of tree kernel module) Depends on how much money it could save. &gt; I don't think you should speculate about why it didn't succeed. I doubt there's any evidence that if it were written in a safe language it would have succeeded... Dude your whole comment is just speculations. &gt; in fact, I think not being written in C is basically instantly disqualifies a kernel module from existing currently, and probably for the next 10 years at least. Again, why? 
&gt; I don't see how this would help rust in any way. A popular project will increase languages adoption. &gt; Things that help languages are things which help the language's ecosystem or make it more suitable for a use-case. Right, like say fast HTTP processing. &gt; For example, rails helped ruby because it created an ecosystem of projects and made ruby more suited for large-scale web development. Kinda like a fast HTTP middleware? &gt; On the reverse side of it, single projects (such as kernel modules or user-usable binaries) do very little for language's adoption in general. Apache's mod_php helped php adoption, but it was written in C and did nothing for C's adoption. C was already extremely popular. If C was an unknown language, you dont think that it would bolster C's adoption? &gt; Furthermore, I don't see how an http kernel module could possibly be a "killer app". Because everyone needs to be serving static files. &gt; Thee's a reason people use userspace http servers (be it nginx or rocket or such), and that reason is rarely because of performance after a certain point. There are many reasons. It's just that security beats speed. &gt; Most http servers (e.g. nginx) are fast enough that your application will bottleneck elsewhere, e.g. by running out of database connections. Right. So there's no point in optimizing what you are saying? &gt; Sure, processing http in the kernel could be a tiny bit quicker, but it won't be quicker meaningfully because it won't change the average user response time (still on the order of 100ms-500ms due to network latency and application latency), and it won't make it scale noticibly better in terms of resource utilization (database connections will still limit it to xxxxx active users). No, but it could reduce your number of static servers drastically. You are underestimating the power of 0. &gt; I can imagine this being a very successful open source project because say S3 or Akamai could probably reduce their server costs to a fraction of the current costs. Me too, that's why I mentioned it in the post. &gt; They could reduce server costs by almost none, and in exchange they'd lose almost all the monitoring, expertise, and software they've written now. Bold claim. Like they are deploying services using new technologies right? &gt; It would probably cost more to rewrite even a tiny bit of their stack to use a new immature difficult to deploy (due to being an out of tree kernel module) Depends on how much money it could save. &gt; I don't think you should speculate about why it didn't succeed. I doubt there's any evidence that if it were written in a safe language it would have succeeded... Dude your whole comment is just speculations. &gt; in fact, I think not being written in C is basically instantly disqualifies a kernel module from existing currently, and probably for the next 10 years at least. Again, why? 
There is no such example at the moment, I will add it. But In general it would be very similar to diesel example. You can run sync code in sync actor and then send messages from async handlers.
There are is a simple way to avoid the overhead of kernel code when doing network I/O: use a user-space network stack. You need a dedicated interface if you still want the machine to be generally accessible, but it's common for servers to have another interface for administration anyway.
Thanks for writing this great, constructive answer. If someone in the future comes across this: This is how it works in Diesel 1.0! The 3rd party crate I mentioned above is this one, by the way: &lt;https://github.com/behos/models-derive&gt;. I also recently found &lt;https://github.com/shssoichiro/diesel-derives-extra&gt; which does some similar stuff.
&gt; You are also flushing the cache which puts a dent into your perf. Like by a lot. You go from like 100% to 20% very quickly until the cache fills up again. Do you have any sources at all on these sort of claims, or are you just pulling numbers out of your arse?
Software development and systems management are all about trade-offs. Even assuming that it's worth the time and effort to optmize for reduced context switching to that degree, and that it doesn't impose an undue burden when it comes to maintainability, running something in the kernel still removes at least one layer of defense in depth and that hazard needs to be factored in when deciding whether the performance gain is worth it.