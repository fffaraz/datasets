No, something you iterate over is a distinctly different concept (and tuples can have elements of different types)
Yeah, something like that. There are tradeoffs to be made and Ducks and embed_lang opt for different ones. I don't think it even makes sense to think of there being a "best" or "right" approach, it depends on goals and priorities. Of course, as you say, the other end of a tradeoff with the more powerful type system is much harder to implement, which is why I'm not actually working on that at the moment either :). The language I'm designing now, called "Baseline", is conceptualized as something like "the lowest common denominator of modern languages". So it's type-safe, has first-class functions, algebraic datatypes, no nulls, basic generics, type inference, immutable data by default, expression-oriented syntax, pattern matching, and so on. But no fancy or familiarity-challenged type system features on top of that (though I'm currently having a *very* hard time hewing to it given how problematic opaque `function`s are for tasks with isolated heaps), and it's strict, impure, still fundamentally imperative, and with C/Java/JavaScript-family syntax that's not really intended to be lightweight. Boring and buttoned-up, in other words. Surprisingly (though by this point it shouldn't be) there are still **a lot** of interesting choices and design decisions down in the weeds. Good luck with your project! :)
so it would be something like this? fn concat_u16(i:&amp;[u8]) -&gt; IResult&lt;&amp;[u8], u16&gt;{ if i.len() &lt; 2 { IResult::Incomplete(Needed::Size(2)) } else { IResult::Done(&amp;i[2..],i[0] as u16 | (i[1] as u16) &lt;&lt; 8) } } not sure for what I'm doing if that would be any better than the trait implementation I posted above. Looks like it would be handy for more arbitrary binary data than packets though. I like the syntax of combine a little better just from glancing at it.
There have been multiple threads on this over the past few days. Here's my reply, from one of them: https://www.reddit.com/r/rust/comments/4bm3rk/how_would_cratesio_react_in_a_case_similar_to_the/d1bqxc0
&gt; Even though you know [...] Maybe I don't know that Also, how am I supposed to know to search for `banana-aging-routster`? Keywords? When I search for `router` and `banana-aging-routster` is burried down in the results but `router` is #1 on the list I'm going to initially invest my time investigating and using `router` as I believe most people would too. My point is that names matter.
Here's a version using the fnv crate instead of your hasher: https://gist.github.com/steveklabnik/c5cc4a5676778aac4075 runs in the same amount of time, roughly. `perf` says virtually all of the time is spent in `contains()`, so that's the only thing that matters here, performance-wise. I don't have time to dig in further, maybe you should make a top-level post to ask some other people. I'm not a performance master.
&gt; There was a byteorder crate, but that used unsafe code. So does `std` and a lot of other things. The important bit about `unsafe` is that it can be encapsulated behind a *safe* interface. This is what `byteorder` purports to achieve for reading and writing numbers of a particular endianness. If that's what you need to do, just use `byteorder`.
I take it your pages are hosted statically?
I didn't find any information about thread safety so I came here to ask a question. Can I create one Mruby instance per Rust thread so I can run multiple VMs in the same time? I don't want to share any data between these VMs, I just want to run multiple VMs within their own threads.
I've just found this paper: http://slide.rabbit-shocker.org/authors/pmq20/embedding_mruby_into_c_and_an_actual_example/embedding_mruby_into_c_and_an_actual_example.pdf Quote: "N VMs per process (could be one VM per thread, no need to lock), then call VMs from C"
There's no winner for "The most idiomatic way of reading files" :) It all depends on what you want to accomplish: parse a file into some data structure, search a file for some needle, database-like operations, etc. Each will have its own "optimal" pattern. If you are not going to handle very large files that don't (or barely) fit into memory, [read_to_end](https://doc.rust-lang.org/std/io/trait.Read.html#method.read_to_end) will suffice. Also see [bytes::buf module](http://carllerche.github.io/bytes/bytes/buf/index.html)...
The template is converted into rust code at compile time but that doesn't mean there is no runtime cost. The produced rust code still has to execute (and may reference dynamic content only known at runtime). However, it's still pretty fast because (1) horrorshow doesn't need to do any parsing/interpreting at runtime and (2) the template is optimized by LLVM.
Well, the good news is that they have tools to check if it stopped compiling, and they the code that stops compiling is runtime broken anyways. 
If you don't like it, don't use it. You can disable this by disabling the `ops` feature. However, for some background, this feature used to be quite a bit more useful because Horrorshow didn't support inline control flow. That is, to write a for loop, you had to write: ```rust html! { html { body { |tmpl| { for i in 0..10 { tmpl &lt;&lt; html! { li : format_args!("Item {}", i+1); }; } } } } ``` Writing `.append(html! {})` everywhere got a bit tedious.
Which, again, falls under the legal policies mentioned elsewhere in this thread.
It is standard to use asserts or panics for contract violations. e.g., `xs[i]` will panic if `i &gt;= xs.len()`.
read_to_string() as well as any other File Readers force you to do buffer management. This means that you have to allocate an appropriately sized buffer for the file reader to read into. Consider the case when you have to read a big file that can't all fit into memory: You have to use a buffer to read chunks of a certain size. I was trying to use fill_buf() because it appeared that the compiler would allocate that buffer size depending on the OS (maybe this assumption is wrong). 
I guess was aiming for a solution that would cover reading arbitrarily large files and I was hoping that fill_buf would allocate an appropriately sized buffer that would take the OS into consideration and minimize IO calls.
Thanks, I'll just use byteorder then
What you are trying to do is basically fn foo() -&gt; &amp;i32 { let x = 10; &amp;x // x stops existing right here } Return `Option&lt;Json&gt;` rather than `Option&lt;&amp;'a BTreeMap&lt;String, Json&gt;&gt;`.
Yes that is correct. I've already identified that my approach is incorrect. The only difference in your example is that you're choosing to return &amp;x. In mine I get back Option&lt;&amp;Object&gt; from the as_json method and I don't know how to make it live long enough. My question is more "what's the right way to define this kind of function so that it works" because my approach seems impossible. 
In Ruby (&amp; friends) your code would be acceptable, because Ruby would have allocated the `BTreeMap` on the heap and returned a reference to it. However, Rust allocates things on the stack by default, which means that **we often have to explicitly consider ownership in terms of scopes.** My first thought, when seeing the `Option&lt;&amp;'a BTreeMap..` is *who owns the BTreeMap*? That's to say, you're generating this `BTreeMap` within your function using `json_message.as_object()`, but then attempting to return a *reference* to it rather than the variable itself. When `parse_payload` finishes its execution, the *local* `BTreeMap` gets dropped—and Rust won't let you return a reference to something that's been dropped! You have two choices here: 1. Return the `Json` object (`json_message`) and call `as_object()` from within the calling scope (the `match` function) to get your values. Change the function return type to `Json`. 2. Call `json_message.as_object().clone()` and change the return type to `Option&lt;BTreeMap&lt;String, Json&gt;&gt;`. Option #1 is by and large preferable, because it doesn't allocate any extra resources but simply passes the parsed `Json` object down the stack. I only included #2 so that you might have an idea of how to reason about this with something pretty similar to your existing function signature. Hope that helps, and happy coding.
If I remember correctly from when I used it, mruby doesn't keep any global state, so I don't see any reason you couldn't even have multiple VMs on the same thread!
Given we no longer see those docs showing up, it may very well have been the final resolution; I do not remember how it ended however :(
Thanks for the answer. If I use the DEFAULT_BUF_SIZE by new(), I still have declare an appropriately sized buffer for the BufReader to read into: pub fn scan() -&gt; Result&lt;(), io::Error&gt; { let f = try!(File::open("foo.txt")); let mut b = BufReader::new(f); let mut buffer = [0; 10]; while try!(b.read(&amp;mut buffer)) &gt; 0 { // do something } Ok(()) } If I use a string to read into as some have suggested: ... let mut buffer = String::new(); ... with read_to_string(), it will read until it reaches EOF, so the String can be arbitrarily large even though the BufReader is set to DEFAULT_BUF_SIZE. I think at this point it's best not to use the BufReader and have something like this: const DEFAULT_BUF_SIZE: usize = 64 * 1024; pub fn scan() -&gt; Result&lt;(), io::Error&gt; { let mut f = try!(File::open("foo.txt")); let mut buffer = [0; DEFAULT_BUF_SIZE]; while try!(f.read(&amp;mut buffer)) &gt; 0 { // do work } Ok(()) } 
Yup. I'm giving my best to make sure they are and, as far as I'm concerned, they actually are. Value, for example, doesn't implement Send or Sync, so you won't have any problem there. There might be something that I'm missing, since no software is perfect. But, the same way no software is perfect, no software is unrepairable either.
I think there are circumstances, although rare, where a crate could be removed or broken (such as legal action, etc.). But even with `yank` working the way it does, I'm pretty sure (although I could be wrong) that if someone were to `yank` all versions of their lib, and upload a dummy lib with a compatible semver it would effectively be the similar to deleting a lib. AFAIK, `yank` only doesn't affect you when you have a current Cargo.lock, and pinned the version in your Cargo.toml with `"=ver"` which isn't the default, or dare I say even super common.
Right, but what about when someone hasn't pinned the version in their Cargo.toml (because pinning isn't the default), and a dummy lib has been uploaded with a semver compatible version, yet yanked everything prior? Doesnt cargo try to update to a semver compatible verison (the new dummy lib)? Then the build fails, now you have a lockfile without a working lib and cant downgrade to a yanked version. Sure, you could edit your lockfile manually...but if you're in that boat thats not a good solution at all. Edit: I'm actually asking the question. Not trying to be feceactious :)
Yes, it is most definitely wrong because the types in question do not match and calling this "a classic" based on dynamically types languages whereas Rust is statically typed is misleading at best. ```step_by``` is an iterator adapter and should apply to any iterator and I don't even know what exactly "float range" even means in this context. let arr = [1.2, 2.3, 3.8, 4.2, 1.5]; for arr.iter().step_by(2) { ...} the above should iterate over the values "1.2", "3.8" and "1.5" AFAIU. What you want is a separate type that defines a mathematical continuous interval and an inherent method that returns points on it. The Interval type must *not* be an ```Iterator``` because an Iterator defines a ```next()``` method in its interface which really doesn't make any sense for a mathematical continuous interval - real numbers are uncountable. 
If you'd like help with this, I suggest a minimal working snippet of the code you attempted.
What is the correct way to create and return an interpolated string as an str? I keep getting lifetime/borrow errors from the below: struct Session { user: String } impl Session { fn user&lt;'a&gt;(&amp; mut self, arguments: Vec&lt;&amp;str&gt;) -&gt; &amp;'a str { return format!("TEST {}", self.user).as_str(); }
I've added a link to a github repo with a minimal example
[removed]
The guy himself unpublished left-pad in response to his package Kik being yanked by npm because the company Kik wanted it and was claiming trademark infringement. Which they can ask for and is definitely legal. As is the guy choosing to yank his own package. Unlike Kik however his package left-pad was everywhere causing the breakage. The whole thing was legal it just caused a debacle. tl;dr if someone claims copyright infringement or dmca related and it's valid, then it has to be complied with. What happens after is anyone's guess.
Though we could also argue that unpublish is ultimately futile, because npm took exactly the code that was unpublished and put it exactly back at the location it was at previously, only with a new owner (who I believe is an employee of npm itself), removing any agency that the code's original author had in the process. The same could happen any time that an unpublished module causes a break in the dependency chain, though doing it automatically would entirely obviate "unpublish" as a feature, and doing it manually would be no less labor-intensive than the doomsday scenario laid out above (while still leaving your users open to broken builds via suddenly-evaporating dependencies).
Hey there! This was brought up in our gitter room recently. (gitter.im/sgrif/diesel) You can check the conversation there for more context Short version: It's possible right now, but requires some boilerplate. Improving this is on my radar for the near future. Here's a minimal example. https://gist.github.com/sgrif/ccde18cb872df9ef50aa Given that this has come up multiple times, I'll see if I can write a crate to handle this for you this afternoon. EDIT: It's later than I thought... Won't get to it today. I've pinged the core team to see if anybody is interested in working on it. I'm thinking this will live as a separate crate, and you'll just annotate the enum to generate the code from that gist. That gist would become #[pg_enum] enum Status { Draft, Pending, Published, } 
There's also [Maud](https://github.com/lfairy/maud), which is implemented as a compiler plugin instead. It has a cleaner syntax and slightly better performance, but requires a nightly compiler. (Full disclosure: I wrote Maud)
Thanks, I think I've got that working for my case currently. It would definitely be wonderful to see that boilerplate get generated. The caveat with the current solution seems to be that I have to manually use the table! macro, instead of using infer_schema!, as infer_schema! doesn't seem to know about the type even if I declare it. Also, am I right in assuming that the oid and array_oid values come from the pg_type table as follows? oid: from pg_type select oid where typname = 'foo'; array_oid: from pg_type select typarray where typname = 'foo';
&gt; I have to manually use the table! macro, instead of using infer_schema!, as infer_schema! doesn't seem to know about the type even if I declare it. Yeah, I need to make the type mapping pluggable. It's tricky to accomplish, as I want to support third party crates as well. Right now it's implemented really in a really dumb way, where it's just "pg type name = ::diesel::types::CaptializedTypeName" &gt; Also, am I right in assuming that the oid and array_oid values come from the pg_type table as follows? Yes. You can also just put 0 there. PG will try to infer the type of the bind parameter based on the query in that case, and default to string if it can't figure it out. Since enums are transmitted as strings anyway, this should work pretty much 100% of the time.
Hey all, I've put together a short blog post explaining Rust's different string types. This is not my first time writing about Rust (I worked a lot on the Rust FAQ), but it is my first time blogging about Rust. Feedback is appreciated.
Just to make it clear again: it's literally *impossible* to do compile time bound checking for all cases. You can reduce the halting problem to out-of-bound-checking. Thus it's undecidable for the all Turing machines on a Turing machine. Of course, it is possible for *some* (maybe *many*) cases, still. 
Well then, I will update it as soon as I'm no longer on a plane with a dead laptop!
Glad you enjoy the problem! It's one of my favorites. I totally relate to the urge to optimize this to silly levels. I thought using a HashMap to create a search index of the dictionary was pretty smart... but using a static array for the char conversions is even more awesome. It's also much faster on my machine (I have an i5 so it's longer than 20ms, but still consistently under 100ms!). Kudos, and I'm happy to find someone who also enjoys hacking at this. A lot of people ask me what the point of this program is. I always say, "pure fun."
In your case, I'd just return the `String` `format!` gives you. In general, if you want to return a borrow, its lifetime must somehow be tied to an argument's (including self) lifetime, otherwise Rust cannot know when to reclaim the value.
It helps having a concrete example. I appreciate it
That makes sense, thanks
Apart from what /u/CryZe92 wrote, `Cow` in Rust is actually *Clone* on write – it works with all `Clone` types, not just `Copy` types. Also the runtime overhead from checking is usually negligible compared to cloning a String. In my (not too recent) benchmarks, the latter took some 34ns, whereas the former took 1ns (comparing returning `Cow&lt;'static, str&gt;` and `String` against returning `&amp;'static str`).
/r/playrust
Also, I just added the pruning of words containing `e, q, w, or z` back into this version in my [`array_with_pruning` branch](https://github.com/amw-zero/qwerty_dvorak.rs/commit/6a7266290a4f3bee053f4a2fa77058bb0528be63). I know it seems like an unnecessary step but it leads to a performance improvement. Filtering these letters removes over 70% of the words in this dictionary, mainly because of `e` which is extremely frequent in English.
Good luck!
...or even: fn get_string&lt;'a&gt;() -&gt; &amp;'a str { "This is a string!" } 
Oh, forgot about that!
Kik wasn't really yanked, it was deleted from npm and its name was reserved. Cargo's yank doesn't delete the package, it merely marks a specific version of the package as deprecated. The `left-pad` wouldn't happen if all the author could do was to yank the published versions.
&gt; I still have declare an appropriately sized buffer for the BufReader to read into: ... The intended use-case of `BufReader` is to read until you hit a delimiter: [`read_until`](https://doc.rust-lang.org/nightly/std/io/trait.BufRead.html#method.read_until), or similar ones involving a delimiter. `read_until` returns a `String` and you don't have to do any buffer management yourself.
Thanks for the feedback. Yes, this is probably the greatest appeal of this style of logging for me :-)
Will do! Thanks :-)
I'm not trying to be harsh, but does it do something yet? For me it looks like the entirety of the engine is only about 600 lines long, but maybe I'm missing something. 
A lot of what has been going on is discussion and plans for how it will all work. You will probably be seeing a lot more actual code in the next few weeks as we get the ECS and such worked out.
I think I get what you're on about. Going off the second example (doing away with KeyType altogether, I think I've managed to do everything generically. This will probably explain what I am aiming for: http://is.gd/6vTyLA
&gt; `step_by` is an iterator adapter Well, given that my point since the start is that it is *not* an iterator adapter, but rather a fonction to *convert* an arbitrary range (be it of integer, floats, or whatever strange type you customized it for) *into* an iterator, that you seem particularly opaque to this view, even though what you ask for alrealy exists in the crate `itertools`, my view is already used in other langages (and I don't see how my two examples being dynamic is relevant here), and my view is the one chosen by the rust team (well, there's an issue with positive feedback to add the support to step_by `Duration` on a date/time range, so they must be *at least* as misleaded as me)... I guess there's nothing more I can tell you anyway. I'm happy enough to have a language with some mathematical consistency anyway. :)
Good question! The short answer is because I like them. The long answer is that when I saw [this interesting site](http://www.html5rocks.com/en/), it reminded me of [this funny scene](https://youtu.be/XZPeXKVjAzs?t=15s). Putting two and two together led me to where we are today.
Just further information, [there is a XHTML serialization of HTML5 data](https://en.wikipedia.org/wiki/HTML5#XHTML5_.28XML-serialized_HTML5.29) that conforms to both HTML5 and XHTML. I'm not sure this is actually the best practice.
Hmm, it appears I indeed forgot how that works. If that option still does not allow you to avoid writing the document declaration, could you please open an issue in the [issue tracker](https://github.com/netvl/xml-rs/issues)? I'm willing to support this behavior if I overlooked it before.
That said, I indeed intend `xml-rs` to follow the XML spec as closely as possible, and the spec says that XML documents SHOULD (in RFC sense) begin with a document declaration: https://www.w3.org/TR/xml/#sec-prolog-dtd I guess I can make it configurable, if it isn't already.
Sounds like PVS Studio and [their Blog](http://cpphints.com/). I really loved those and would be looking forward to seeing something comparable for Rust.
Just wanna say thanks for writing up a blog post like this. The different string types is exactly in that unglamorous valley between "let me show you how cool this Rust thing is, you should try it!" and "trait specialization, orphan impls, and global coherence." Things like the difference between String and &amp;str are confusing at first and then not very deep later, so people don't blog about that.
&gt; It's never too late True, but once an API is public, you'll have to wait for 1.8 stable to `#[deprecate]` it. :-P
So you're a brony? I won't judge ;)
Bringing up kik misses the point. The article and discussion is not about kik.
Is there a way to 'bundle' trait bound requirements up into a single trait? I've got a requirement for the following to be implemented, and it's annoying having to type them all out for a bunch of functions. T::Err: fmt::Debug, T: Clone + FromStr + fmt::Display + fmt::Debug + cmp::PartialOrd
If you are willing to rely on an external crate, I made [something](https://crates.io/crates/filebuffer) that does just this by mapping the file into memory: extern crate filebuffer; let fbuffer = filebuffer::FileBuffer::open("file.txt").expect("failed to open file"); // fbuffer can be used as &amp;[u8]. Advantages: * Simple. * Minimal disk reads: you can do random access and only the parts you touch will be read, and it will be read in large page-sized chunks. The OS might decide to read ahead if your access pattern is regular. * Minimal memory pressure: only the parts that you read need to be backed by physical memory, and if there is memory pressure the OS is free to evict pages from physical memory, because they can be read from disk again when needed. This is not the case if you read a file into a `Vec&lt;u8&gt;` using methods from the standard library, because the OS does not know that you are storing a copy of the file in the vector. * No need to do any memory management or allocation, this is all handled by the OS as efficiently as possible. There are no unnecessary copies of the file in memory. * Fast. It is only one or two system calls, and if your access pattern is regular then the OS can read ahead. If the data was in the disk cache, it is basically free. On the other hand, `BufReader&lt;File&gt;` is almost as fast, and if the disk is the bottleneck then it doesn’t matter anyway. Disadvantages: * It is dangerous on Linux: if the file you opened is modified by an other process, then you can observe those changes. An other process could even truncate the file. Accessing the truncated data will be undefined behaviour. * Does not support reading files larger than the virtual address space. (It is possible in principle, I just didn’t implement it.) There are also [memmap](https://crates.io/crates/memmap) and [mmap](https://crates.io/crates/mmap) which are less simple but more flexible.
Bingo! This seems to do the job pub trait MegaTrait : Clone + FromStr + fmt::Display + PartialOrd + Ord {}
Something like this? http://is.gd/IAYFo5 Edit: If you don't want to be cloning then you're going to need to specify lifetimes around the place to prove that the parent won't get dropped before the children. Here is a version without cloning with lifetimes introduced (I removed the boxes, but you can just chuck them back in): http://is.gd/5oAMki
The title: "SFML Pong". :D
The `read_to_*` functions don't require explicit buffer management: they extend/resize their argument as necessary, and in fact the ability to resize is why they take `&amp;mut Vec&lt;u8&gt;` instead of `&amp;mut [u8]`, and (somewhat) similarly for `String`. However, this resizing makes them inappropriate for arbitrarily large files... But, to be clear, it isn't because the programmer is forced to worry about vector sizes.
If you mean as a return type, you currently don't have much choice (`-&gt; impl Trait` will sooner or later alleviate that, once it's landed) If you mean as part of your types, well, you can either make it public or write accessors. The choice very much depends on your use case. If you mean as function arguments, you can mostly use pointers for that and reference the args before calling the function. Or if you like, you can even make your function generic over the `AsRef` (or `AsMut`) trait, depending on how the function is normally used.
Would it be correct to say "use String if it needs to be mutable, otherwise &amp;str"? Say I have a function that returns Hashmap&lt;string, f32&gt;. If the key is not going to change, would a &amp;'static str be preferred to String for the key? Edit: Thanks for the answers!
Thanks, make sense!
&gt;because the documentation wasn't the best in many places Did you read [the book](https://doc.rust-lang.org/book/README.html)? It's pretty good in my opinion. There is also [Rust by example](http://rustbyexample.com/) for a more "visual" approach. Here are some thoughts on your code: - You can use [`const`](https://doc.rust-lang.org/book/const-and-static.html) for constants - It is considered more idiomatic to not write `return` for the last expression, because the final expression in the function will be used as return value. `return` is only used for early returns. - `if` does not need parentheses around the condition, writing them is considered unidiomatic - `len()`, etc. are methods on structs, you can call them like this: `len(&amp;vs)` but generally you would use the method syntax: `vs.len()` - [`Vector2f`](http://www.rust-sfml.org/doc/rsfml/system/vector2/struct.Vector2f.html#method.add) already implements the [Add trait](http://doc.rust-lang.org/nightly/core/ops/trait.Add.html) and the [Mul trait](http://doc.rust-lang.org/nightly/core/ops/trait.Mul.html), is there a reason why you can't use that? This line: `ve = add(&amp;ve, &amp;scale(&amp;acce, dt));` would become: `ve = ve + acce * dt`.. Way cleaner ;) Your `Body` struct hints that you wanted to use structs and impls which would probably be cleaner than what you have now. What is the reason for not pursuing that design? You can read the [method syntax](https://doc.rust-lang.org/book/method-syntax.html) chapter in the book. Also you could check out the [next iteration of the Rust book](http://rust-lang.github.io/book/introduction.html), it's definitely not finished, but as you seem to have trouble with "Ownership &amp; Borrowing" and "Structs" you can read the chapters that are already rewritten. Just bear in mind that it is a work in progress.
I've begun writing a production renderer in Rust. Rather than doing the whole thing from scratch, instead I'm working on porting and binding a VFX-oriented software stack to Rust, and then I'll use that as the basis for the renderer. So far I've got a minimal Imath port going: https://github.com/anderslanglands/imath-rs And a binding to the Arnold API: https://github.com/anderslanglands/arnold-rs The former is obviously for basic linear algebra types and is the standard in the vfx industry, which the more complex libraries I will bind later depend on. The latter I will use for loading Arnold scene files so I can do easy correctness and performance comparisons against the best existing production renderer. Both of these libraries are heavily WIP. Once things stabilize and I flesh them out a little more I'll get them up on crates.io at which point I'll start respecting semantic versioning. This week I will write basic bindings for embree, because I'm not nearly smart enough to write a bleeding-edge ray accelerator: https://embree.github.io/ And in the following weeks I'll be binding OpenImageIO and OSL: https://github.com/OpenImageIO/oiio https://github.com/imageworks/OpenShadingLanguage At which point I'll be ready to start on the renderer proper. I'm currently looking forward to playing with Rayon for concurrency, and to seeing how Rust's ownership semantics come into play in the design stage. A renderer is a pretty complex set of interacting systems and needs to be highly parallel for performance, so it seems like a perfect fit for Rust. This is my first foray into actually writing Rust. I'd been working on a C++ version of this renderer for a few months but after getting to a fairly advanced point and hitting what appeared to be a memory corruption bug that neither ASAN nor Valgrind could track down I decided to start the whole thing over again in a language that's a little less keen to help me shoot myself in the foot. I'll probably blog about this along the way when I get some free time.
&gt; And regarding the documentation, I was talking about rust-sfml and other libraries Ah right, yes that I can understand ;) Once you get used to how rustdoc (the tool that generates the docs for all Rust projects) structures the docs and how [traits](https://doc.rust-lang.org/book/traits.html) work you will be able to extract more information out of it. But I agree, there is still a long way to go to improve community docs.
Depends on how you define mutable, if I read in the strings from a file, but then never change them, they aren't really mutable, but I still need a `String`.
Relevant part of the Cargo.toml https://gist.github.com/Thinkofname/dc8b38a24c4fdf8f097f As for performance having the camera in the same place for both the java client and rust one shows: Java: 430-540 fps Rust: 670-680. Chunks build a bit faster too 
[removed]
Greetings Rustaceans! So I am working my way through the Project Euler challenges, to get a hang of the syntax. Then I figured I wanted to bring threads into the mix. Here's what I've got so far: https://gist.github.com/anonymous/349e88c93a51de76899e (I realize the algorithm is really naïve - that's kind of the point of this exercize, as far as I am concerned right now) The problem is that the threads does not run in parallel, but starts one after the other. Any hints as to how to make them asynchronous?
Rustdoc generally doesn't show docs for trait implementations, you're supposed to look at the trait itself.
/me quickly checks to make sure my slash fic folder isn't in any of my crates... *whew*
Let me know! I've been holding off tweeting from @rustlang till afterwards :)
How is sharing a link "taking content"?
He probably wanted to mention how this account is probably a bot. Mirroring titles from phoronix: http://www.phoronix.com/scan.php?page=news_item&amp;px=Vulkano-Rust-Vulkan.
Some name lookups are relative to the current file, and some are absolute. I don't remember which is which, so `use` and `mod` and qualified names are frustrating for me.
I agree. Rust makes you regret choosing any other type than `usize`. I've tried using 32-bit numbers, and I've had to have `x as usize`/`len() as u32` in sooo many places, that I gave up. `.into()` is also verbose and doesn't always work. 
I see, thanks for explaining.
See my other comment - the dynamic libraries we produce always contain metadata, just like `rlib`s, but the latter are static, not dynamic.
I would be very much interested in such a post! I'm watching your library on GitHub and am planning to use it in a programming practical at my university in summer. Knowing about the design decisions and reading something about the internal structure would help a lot. So I'd like to encourage you to write a post anyway. I'm sure your English is surely good enough to explain `vulkano`. And I'm also sure someone from the community would gladly proof-read it. I would offer it myself, but as a German I have enough problems with English as well ;)
2 - Addition of an explicit lifetime causes an explosion of annotations everywhere. To me usefulness of it being explicit is not worth the disruption to the entire codebase it causes. 5 - `std` vs `::std` is weird and confusing. I strongly dislike how name lookup suddenly breaks when you split your main module into submodules. 
The principal difference is ownership: someone has to own the data `&amp;str` points to. You can't create a new `&amp;'static str` at run time other than by slicing a pre-defined constant. If some other structure owns the strings or they're all constants by all means use `HashMap&lt;&amp;str, f32&gt;` (in [this example](https://www.reddit.com/r/rust/comments/4c3q36/feedback_on_qwerty_to_dvorak_word_conversion/d1eyc4i) a `index: HashSet&lt;&amp;[u8]&gt;` relies on the keys' data being owned by `dict_contents`).
I'm just saying that I would love to read something about `vulkano`, preferably written by you :P Tutorials or introductions would be awesome, too.
True. A big important distinction though is that that's a slice taken from a stack array, so the string's length in bytes isn't part of its type anymore.
I'm going to try to get the [Rust Belt Rust](http://www.rust-belt-rust.com/) call for proposals open this week, start thinking about your talk!!!! &lt;3 EDIT: I did it!!!!!!! http://cfp.rust-belt-rust.com
Update: and now the [pub(restricted)](https://github.com/pnkfelix/rfcs/blob/pub-restricted/text/0000-pub-restricted.md) RFC has been accepted, which IMO will significantly change (for the better) how we think about structuring code.
Yes, mostly I just want to make sure that the proposed design would actually be capable of solving your specific problem, since the feature itself is useless if it doesn't have a broad range of applicability. Just playing around with it would suffice. :)
What about compiling GLSL to spir at compile-time without a separate build step? That does require a plugin and nightly but still seems like a very useful thing to have.
Feel free to hit me up on IRC if you have questions on this.
The structs look fine to me, there should not be a difference between the first two and the others. Just to be sure you can try and reorder the last two so that the `&amp;'a str` field is first in the struct definition in case that affects diesel codegen. Otherwise the problem may be with the schema definition so it would be helpful to post them here or in a gist. ~~**Edit:** It looks and I was wrong, `pretty=expanded` apparently does not show expanded plugins.~~ **Edit2:** Nevermind, I was trying to use the syntex based codegen. To help debugging this problem you may try and run `cargo rustc -- -Z unstable-options --pretty=expanded`which will show you the code produced by diesel codegen, maybe you can spot the problem there.
Thanks! That did sort it out.
&gt; A match is going to be the most efficient thing and in this case it's easy to do as all the matching -- including qd_map -- is static. Especially qd_map can be compiled to a computed jump. Can you elaborate this? I don't understand what you mean by match here, and `qd_map` is not static in my version. Unless you're talking about a different version.
Is there no GNU ABI version of Vulkan on Windows? When I try to compile, it complains about missing some Visual Studio stuff.
I asked about this in #rust-beginners and found a solution for my specific use-case*. But I'm still wondering why casting with `as` is not implemented as a trait? It could simplify some generic code if it was. \* Instead of `impl&lt;N&gt; Hsz for N where N: As&lt;f64&gt; {}` which does not work. I ended up depending on `num` to use `ToPrimitive` and `to_f64()`.
Let's get the hard-hitting questions out of the way: is the official pronunciation of Vulkano to [scream it like a special move in an anime](http://youtu.be/m6NZr7A72Zk)? This is how I pronounce it, and I'm afraid I'll have to fork the project if this isn't supported.
I wrote a Jamendo client today: https://github.com/jgillich/jamendo-rs At this point, it covers most of the public read-only API. OAuth will be added soon. Would love some feedback. :)
I am curious about the reasoning behind the usage of `Arc`. Is it that common to share Vulkan objects between different threads that always having the reference counting overhead is worth it for a simpler API?
Depends on the situation. In some cases, error messages are really cryptic (and it's even worst when you begin). I'm mainly thinking about closures.
This is a good first try, but I disagree with a lot of the things you said. I don't have time to write all my problems with the post ATM, but I'll try to come back and add them later. edit: My issues with the post There aren't as many as I thought there were earlier. I actually didn't realize it was possible to move only part of a struct - that's surprising behavior! It would have been more interesting if you went into detail about some of the other data structures as well. Anyway: ### 1 There's any easy and logical way to move an element out of a Vec: [swap_remove](https://doc.rust-lang.org/std/vec/struct.Vec.html#method.swap_remove). It actually removes the desired element from the Vec, which makes sense because the Vec would no longer own the element at the specified index. There's no need to deal with `mem::swap` and `mem::replace` yourself. ### 2 When you have the concept of a reference, you need to be really careful that any references you make are always valid. So I take issue with your assertion that moving by default is "an inherent limitation of Rust's memory model" because it's really just the way you should be writing code most of the time anyway. And it's really easy to make a struct copyable: just `#[derive(Copy,Clone)]` and you're good to go. ### 3 I'm not really sure why `join_into(a: &amp;mut String, b: &amp;String)` is awkward. To me it's actually really clear what's happening, which I appreciate. It also lets the borrow checker make sure that I'm only mutating a string that I hold a mutable reference to, which is helpful because it enforces Rust's safety guarantees. The only thing I'd change is make the type of b be `&amp;str` so it's more generic (though you do point this out in a footnote). ### 4 I'm not sure why you're portraying moving things out of a vector as such a difficult task. You listed 5 different safe and logically sound ways to move things out of a Vec.
I tried to reproduce with [this](https://gist.github.com/TimNN/c7e42eccc2826e679661) but that code compiles without a problem.
I managed to replicate it in the Diesel test suite.
I'm aware of that. My question is not about what can I do. It is, why an `As` trait is not already defined where `as` is implemented?
You can't just leave a hole in the Vec, because all the ops assume all the elements from 0 to len are initialized and valid. Otherwise it would have to be impl'd as basically an array of Options, which would be exceptionally inefficient. Also actually this is impossible, because you can get an `&amp;[T]` out of a `Vec&lt;T&gt;`, which guarantees all elements are initialized (I'm assuming you've done the more efficient thing and actually built a bitmask array on the side, rather than actually using Options). I suppose you could forbid slicing over "partially moved" parts of Vecs, but this would be even more flags and state to check on every operation. Vec isn't a magical part of the language, it's just some library code. The compiler can't build a bitmask for it to use or something. You're blaming the borrowchecker for not "getting" your code, but really it's the fact that Vec doesn't support your usecase (because supporting it would be awful for Vec's performance). If you just want to backshift all the elements, there's a function for that: `remove`. Having indexing desugar to `remove` would be, quite frankly, atrocious. 
Yep, this is what Drain was built for, but note that it massively constrains the Vec api while draining (the vec is completely inaccessible, all you can do is remove-successor) and actually clobbers the Vec's state while it exists (it tells the Vec it's empty so it doesn't drop anything until the holes are filled by the bckshift).
I've noticed a few slip-ups, but they are pretty minor. He writes very well for somebody who isn't a native speaker - I'm sure it wouldn't take much editing to clean up the few errors.
Thanks for the link. Sadly I'm not experienced enough to help, but it's good to know that there are discussions about it.
That's addressed to people who believe that unpublishing ought to be a right of the uploader; if you believe the uploader of the unpublished repo has no ownership of the contents of the repo, then you must conclude that unpublishing as an action has literally purpose other than to cause disruption. :P
Arch. Haven't updated KDE in a month or so; it's the stable version from Arch's repos.
This looks promising, I'll try it out. Thank you!
Geez, a *lot* of good changes this week. That symbol mangling PR in particular is an epic tale.
Excited about typechecking performance improvement. Thanks Marwes!
&gt; How does the compiler know how long `'a` lives? The compiler will infer from the function signature that `'a` is unbounded (no input lifetime to match with). So essentially, it is equivalent to `'static`. But using this is useful in the case where you make use of an unsafe functions to allocate on heap and returns a reference (not a raw pointer) to it. Then it gives _you_ a visual cue that it may not live for ever, but the compiler doesn't make any distinction between them.
Complete noob here, but playing with your example, and changing the `map` call to a `for` loop, i.e. let mut handles = vec![]; for k in 0 .. n_threads { let v = v.clone(); let t = thread::spawn(move || { println!("Thread {} spawning", k); let res = (0 .. 10) .map(|x| v_max * (k + 1 + x * n_threads)) .inspect(|x| { println!("Thread {} checking {}", k, x); thread::sleep(Duration::from_millis(50)); }) .find(|&amp;p| { divides(p, &amp;v) }); match res { Some(r) =&gt; println!("Result is {}", r), None =&gt; () } }); handles.push(t) } for h in handles { h.join().unwrap(); } made the threads all start at the same time. I don't know why it behaves differently. 
There are the various [`Cell`](https://doc.rust-lang.org/std/cell/index.html) types which allow some shared mutability. My guess is `Rc&lt;RefCell&lt;T&gt;&gt;` will give you close to the behavior you want.
Reading the amended text: &gt; So what do you do if you want to return a string from a function without giving it a `'static` lifetime? You use the owned sort of string type, in this case: `String`. Might be useful to change it to say something like: &gt; So what if you want it to return a dynamically generated string, that cannot be allocated known at compile time? You use the owned sort of string type, in this case: `String`. And give an example: fn get_number_string(number: i32) -&gt; String { format!("A number! {}", number) }
Worth also mentioning that there's an open RFC for rdylibs: https://github.com/rust-lang/rfcs/pull/1510
I feel like anything involving linkage usually ends up becoming a good story.
This looks very interesting, thank you!
It seems like if you write that kernel with optimized SSE you should be able to get pretty damn close since that's where presumably most of the time is spent. It makes sense to me to express it as a summing the products of 4x4 multiplied blocks. Unfortunately it looks like the data layout means one of them has to be transposed for things to line up, which would be an extra 8 instructions per 4x4 block just to move stuff around. Also the adds that happen to produce an element in the final 4x4 matrix can be done at the end. I.e. when when computing a 4x4 mul for each row/column you don't compute the full dot product, you just compute the component-wise multiplication and add that to a 4-wide accumulator for that element in the result (i.e. you have a temporary 4x4 matrix of 4-wide SIMD values). Then at the end you can just add up all those 4-wide SIMD values per matrix element and produce the final 4x4 matrix result. I'm not sure the compiler can ever do that, so you'd need some kind of intrinsics or a vector library to help.
Thank you. I just used your lib in my app and I was wondering if, while including the output of a template inside another, is there a performance hit if I return string from the inner template and include it in outer template using raw!()?
Others will.
Enums in Rust aren't enumerable like that. That they have tag numbers behind the scenes is just that: an implementation detail. The way I handle this is by having a step where loose integers are explicitly checked and converted into the enumeration. I usually do this through a macro that generates the necessary code for me; the [`conv`](https://crates.io/crates/conv) crate has a simple one. If you want `0` to not be a valid value... just don't use 0 for the tag value and let the conversion catch it.
The Index trait always returns a reference, changing this would break backwards compatibility, so that's not going to happen. 
One neat thing here is that there are k iterations in the kernel where k is large (hundreds), so you can sum into a result order that is convenient, and only shuffle elements around before you store that into C. This is in fact what BLIS dgemm kernel does, for example. I suspect there might be another mountaintop behind the first one, after the kernel is well optimized. Arranging for memory to arrive just in time to the algorithm is crucial. There's a few parameters that should be tweaked for the hardware (they are called K::nc(), ::kc(), ::mc() in the code); they need to be set so that the packed A &amp; B parts fit into processor caches.
&gt; Enums in Rust aren't enumerable like that. That they have tag numbers behind the scenes is just that: an implementation detail. IIRC if you set the tag sequence explicitly it's well-defined, and you can configure the enum's repr as well.
It's worth noting that in this specific case, there is little advantage to moving the `String` out of the vector over just passing two `&amp;str`s. The advantage is that the first string's original allocation can be (and is, with `+`) reused if the stuff to be appended fits in it*, but if it doesn't fit, the allocator will have to copy and reallocate no differently than if you did a new allocation yourself. Since in this case the length of the string is likely to double or so, the result is unlikely to fit in the original allocation, unless it happened to be a `String` that was previously larger and was then shrunken. Of course, the example is just an example and the suggested techniques work the same in scenarios where this doesn't apply. And it doesn't hurt to move either. But... just saying, avoiding copies is not always the be- and end-all. \* note that the capacity as seen by the allocator can be larger than the `capacity()`
Agreed. Makes me glad I took the job to review the PRs for TWiR. So many great changes...
You're misunderstanding the intent, but that's my fault, really. The idea was to run through the entire possible range of integer values and test to see if they exist in the enumeration. If so, add them to the hash map, otherwise continue on to the next integer value. generating the string hash map would have been the exact same except using format!("{:?}",opcode_var) instead. the theory is sound, but I figured there was a better solution. &gt; If you want 0 to not be a valid value... just don't use 0 for the tag value and let the conversion catch it. Unfortuantely not all languages are safe from such errors, any language that default initializes is prone to the problem. It was meant more as an explanation for why it was called 'None' rather than the more obvious 'Start'.
do you mean set every enumeration value explicitly? That seems unnecessarily tedious.
&gt; I was under the impression enum_primitive depended on a feature that never made it into rust 1.0, is that not the case? Yes and no, `enum_primitive` relies on `FromPrimitive` which was never stabilised as part of the stdlib, but is available [as an external crate](http://rust-num.github.io/num/num/index.html). The latter is what `enum_primitive` depends on. &gt; enum_derive is the piece I was missing though, I didn't come across it in my searching. Is there a good, solid listing of these extra dependencies I can troll through? I really haven't come across it for some reason, and it makes it double hard to know what's out there. Don't think so. I just searched for "enum" on crates.io, those seemed like promising hits from their description and going to check their docs confirmed they provided the kind of stuff you were looking for (spoiler alert: I've never used either)
You simply can't have the return type depend on values. What's wrong with `BTreeMap&lt;Val, Val&gt;`?
I would loose type safety as I need to restrict the key type to either i32 or f32 depending on the enum value passed and don't want to mix i32 and f32 values.
Well... you *can't*. From what I can tell, you're asking for dependent typing, which Rust doesn't have. I don't think there is any solution to this that does not involve *some* kind of dynamism: an enum around the map, enums *inside* the map, or the map inside a struct implementing a trait that you then box.
Where would one submit an issue to add documentation about an error? I encountered E0513: no type for local variable, and it would have been nice to have some hint about what I was doing wrong. For reference, I was basically trying to do ``` let a = 6usize; let b: [u8, a]; ``` which I am assuming errors because `a` is not constant.
This is an interesting post! Obviously, after you faced this issue once, you understand what it means and it is never again a problem. But it's interesting to hear the reactions of beginners who face the error messages for the first time, because they often point out shortcomings that more experienced Rustaceans just don't notice because it's all so obvious to us now.
Yeah, that should really be in its own repository.
The error message in this case is a little unclear, especially if you're new to Rust and coming from a C++ background where generics are defined by instantiation rather than constraints. Surely the compiler knows that T is a generic, and so could tell you that Debug *may* be undefined and you need to add bounds to T rather than implying that you need to add the trait to T (which I guess probably amounts to much the same thing but the wording is unclear for new users)? If that's not possible then perhaps an introductory paragraph in the generics section in the book, explaining how Rust works differently to C++ in this regard (by using bounds to constrain what you can pass rather than attempting instantiation and seeing if it works). Given that the C++ crowd is a target and at first glance Rust's generics *look* like C++ templates, it's not surprising that people get confused by this (I know I did first time). 
Iterating over tuples is often handy, you do it often in both Python (with dynamic typing) and D ("static foreach"). 
Currently, I'm implementing THUMB state instruction decoding and disassembly for [my GBA emulator](https://github.com/Evrey/GBArs). I'll push the next version on GitHub when this step is done. After that, I'll implement THUMB state execution, combined with yet even more refactoring, as both, THUMB and ARM state instructions, have many operations in common. Much room for simplification.
To quote [crates.io doc](http://doc.crates.io/crates-io.html): Take care when publishing a crate, because a publish is permanent. The version can never be overwritten, and the code cannot be deleted.
&gt; I can’t remember how I arrived to this solution since I made so many desperate attempts to fix it. That's a little sad that you don't remember, because I was really curious how you managed to fix that problem. But maybe you do remember if it was reading more docs, asking on IRC, or maybe just trial and error? In the error message, there was a note to run `rustc --explain E0277`. Have you noticed it? (I agree it's quite easy to miss). The explanation is also available [here](https://doc.rust-lang.org/error-index.html#E0277). The example there seems similar to your problem. Remember, unhelpful documentation and error messages are considered a bug in Rust! If you want, feel free to file an issue.
I remember some parts though. I realized in the docs that Box&lt;T&gt; can be printed so I started digging into the source code, because I didn't understand how it is possible to print a generic type such as Box&lt;T&gt; while it is not for mine. I looked at the Debug trait from Box&lt;T&gt; for some time. I was sure that the solution should be somewhere there. Then I made a lot of attempts without understanding the mechanics and finally it worked. This was not scientific at all. I read the --explain message, but it didn't click in what is missing. I was iterating around type T.
This is what I ended up doing extern crate rand; use rand::{thread_rng, Rng}; use std::env; fn main() { let pass_len: usize = match env::args().nth(1) { Some(n) =&gt; { match n.trim().parse() { Ok(n) =&gt; n, Err(_) =&gt; {8}, } }, None =&gt; {8}, }; let pass: String = thread_rng().gen_ascii_chars().take(pass_len).collect(); println!("{}", pass); } 
If you're iterating over it, you probably want a list.
The phrase "buffer of unicode bytes" is not very meaningful - I think you meant "buffer of UTF-8 encoded Unicode code points".
Those work. But when I tried this, #[macro_use] extern crate horrorshow; use horrorshow::prelude::*; fn test(tmpl1: Box&lt;Render&gt;) -&gt; Box&lt;Render&gt; { box_html! { html { body { : tmpl1 } } } } fn main() { } I got this error the trait `for&lt;'r, 'r&gt; core::ops::Fn&lt;(&amp;'r mut horrorshow::template::TemplateBuffer&lt;'r&gt;,)&gt;` is not implemented for the type `[closure@&lt;horrorshow macros&gt;:3:9: 4:45 tmpl1:Box&lt;horrorshow::render::Render + 'static&gt;]` 
&gt; And it's really easy to make a struct copyable: just `#[derive(Copy,Clone)]` and you're good to go. I feel the need to note that this require that all fields of the struct be `Copy` themselves, and that `String` and `Vec` are not for obvious reasons, so that actually many structs cannot be `Copy`.
Yeah, using XML writer for HTML5 indeed seems weird :) anyway, if you have any problems with xml-rs, feel free to create issues in its tracker, I'll try to respond as soon as possible :)
Note that C++03 allows constraining functions using SFINAE (e.g. `enable_if`), C++11 makes it easier with SFINAE for expressions and adds `std::enable_if`, C++14 allows constraining with variable templates and adds `std::enable_if_t`, and the Concept Technical Specification allows constraining using boolean predicates in `require`s clauses as well as with `concept`s. So I think it is fair to assume that C++ developers know what constraining a template in C++ is (otherwise it can be easily explained to them). The main differences are that in Rust: - all generic functions/methods are constrained, and - the implementations are checked against those constraints (and thus have to be properly constrained), while in C++ generics are: - unconstrained by default, and even when constrained, - the implementations are not checked against the constraints, which means that most generics in the wild are under-constrained and still fail with a horrible error message on instantiation. IMO the error one should avoid when explaining Rust generics to a C++ dev is to by mistake explain that _all generics_ in Rust are explicitly constrained and why. The problem is that this will make sense in a C++ devs head, but for a C++ dev `structs` are also generics... and the first data-structure a C++ dev wants to see is `Vec&lt;T&gt;`, but there in the source code and the documentation `T` is unconstrained, right there! Can it really be any `T`? But what if `T` is not e.g. default constructible, copy/move constructible/assignable, ... (concepts that a C++ dev brings from C++ but don't really apply in Rust). And in fact the following snippet fn main() { let v : Vec&lt;[i32]&gt; = Vec::new(); } fails with the following error, but there is no mention of the `Sized` constraint anywhere, not in the source code, not in the docs: &lt;anon&gt;:7:11: 7:21 error: the trait `core::marker::Sized` is not implemented for the type `[i32]` [E0277] &lt;anon&gt;:7 let v : Vec&lt;[i32]&gt; = Vec::new(); ^~~~~~~~~~ &lt;anon&gt;:7:11: 7:21 help: see the detailed explanation for E0277 &lt;anon&gt;:7:11: 7:21 note: `[i32]` does not have a constant size known at compile-time &lt;anon&gt;:7:11: 7:21 note: required by `collections::vec::Vec` &lt;anon&gt;:7:24: 7:32 error: the trait `core::marker::Sized` is not implemented for the type `[i32]` [E0277] &lt;anon&gt;:7 let v : Vec&lt;[i32]&gt; = Vec::new(); ^~~~~~~~ &lt;anon&gt;:7:24: 7:32 help: see the detailed explanation for E0277 &lt;anon&gt;:7:24: 7:32 note: `[i32]` does not have a constant size known at compile-time &lt;anon&gt;:7:24: 7:32 note: required by `collections::vec::Vec&lt;T&gt;::new` error: aborting due to 2 previous errors playpen: application terminated with error code 101 IMO this is the kind of stuff that is confusing. Saying that all generics are constrained but then generics on e.g. `struct`s aren't and still code fails to instantiate.
So in what charset *is* it encoded?
It seems to be in 'ISO-8859-2'
It's not exactly an oversight, it's been discussed a lot.
There, I fixed it.
What's wrong with Google's shaderc? It's C++, but it works. I've been interested in SPIR-V compilation for a while and this seems like a decent solution.
Well
`x..` is conceptually a range without an end, it does not end implicitly at the maximum capacity of the type of `x`. Try `0...std::i8::MAX`. Three dots make an inclusive range, but this was recently added and is not considered stable yet so you’ll need to be on the Rust nightly channel to use it.
Google's shaderc is exactly the same as glslang, which vulkano uses. The only purpose of writing a plugin would be to make the user's life easier, but it doesn't bring anything more in terms of features.
I’m curious to see when Redox will be self-hosting (compiling and developing Redox on Redox).
There are **4** kinds of ranges: - [`Range`](https://doc.rust-lang.org/std/ops/struct.Range.html): formed by `x..y` and meaning `[x, y)`. - [`RangeFrom`](https://doc.rust-lang.org/std/ops/struct.RangeFrom.html): formed by `x..` and meaning `[x, +∞)` - [`RangeFull`](https://doc.rust-lang.org/std/ops/struct.RangeFull.html): formed by `..` and meaning `(-∞, +∞)` or `[0, +∞)` depending on whether the type is signed or not. - [`RangeTo`](https://doc.rust-lang.org/std/ops/struct.RangeTo.html): formed by `..y` and meaning `(-∞, y)` or `[0, y)` depending on whether the type is signed or not. The 4 forms allow static dispatch, for example when you use `vec[..]` to transform the vector into a slice of its full content, there is no need to check the bounds: `..` is a `RangeFull` and the implementation of `Index&lt;RangeFull&gt;` for `Vec` just yields a slice over all elements of the vector. 2 of the 4 ranges implement the `Iterator` trait: `Range` and `RangeFrom` (the others do not necessarily have a starting point), the difference is that: - `Range` will iterate from `x` to `y`, checking at each and every step whether `y` was reached - `RangeFrom` will iterate from `x` onward, without any check the latter is more efficient (does not rely on the optimizer eliding the check) and supposes that *you* will interrupt the iteration before it overflows via some other mean.
I'm not talking about any specific language, but just generally what a type is. A tuple essentially an anonymous struct. Things like iteration just don't make sense for it, especially in statically typed languages.
Still, `multirust` was a way cooler name.
I do like the idea, but how would you handle traits with associated types? For example: impl&lt;T&gt; IntoIterator for Vec&lt;T&gt; type Item = T type IntoIter = IntoIter&lt;T&gt; fn into_iter(self) -&gt; IntoIter&lt;T&gt; impl&lt;'a, T&gt; IntoIterator for &amp;'a Vec&lt;T&gt; type Item = &amp;'a T type IntoIter = Iter&lt;'a, T&gt; fn into_iter(self) -&gt; Iter&lt;'a, T&gt; The problem with the docs is that it packs **a lot** of information, and making it more readable and user-friendly without sacrificing any of the information needs a lot thought, I think.
Did you compile with optimisations (`cargo build --release` or `rustc -O`)?
I can't since this is an online judge system. I have no idea what they set the compilation flag to. How big of an impact will -O do ?
Oh, I see, sorry :)
Very cool !
Well, I just tried it, and a debug build runs on input-7 in ~1.2 seconds, a release build in ~0.4s (I don't have a timer on hand; I'm about to go to bed, so those're rough guesstimates). If Rust is being beaten by a scripting language, it's *usually* because you're either using a debug build *or* you're testing some tightly optimised C component of said scripting language against a probably-not-optimised-at-all component of Rust.
:)
Hi everyone!! I'm honored to be a part of the team :) &lt;3 &lt;3 &lt;3
cc /u/i_am_jwilm who actually works there :). Want to submit this opening to https://rust.jobboard.io/?
The compiler spends way more time optimizing it
Does the submission time include the time it took to compile it?
I dont think it does
&gt; What exactly is different though ? There are multiple ways to turn Rust code into assembly. A regular build does a fairly straightforward transformation, but with optimizations turned on, the compiler does various things to improve the speed of the code. As just one example, code like this: fn foo(x: i32) -&gt; i32 { x + 1 } fn main() { let x = 5; let y = foo(x); } Without optimizations, the generated assembly code will have a `foo` function, and main will allocate an `x` value, call the function with `x`, and set it to a `y`. But with optimizations turned on, the compiler (and/or llvm, which the compiler uses) will 'inline' `foo`, which transforms the code into this: fn main() { let x = 5; let y = x + 1; } No more function call. So this is already faster, but equivalent. From there, it will notice that `y` is never used, and remove it. So now the code looks like this: fn main() { let x = 5; } It will now notice that `x` is never used, and remove it: fn main() { } And so none of this will even happen. An empty binary. This is just a taste of the kinds of things that can be done. In this case, we got an infinite speedup; we were doing a bunch of stuff, but now we're doing nothing! Does that all make sense? &gt; I think they should make some notes to warn new learners between the difference This is discussed in the book, right up front https://doc.rust-lang.org/book/getting-started.html#building-for-release
Your original code sequences the threads instead of parallelizing them because `map` creates an iterator adapater, `Map`, which is lazy. Notice the parameter to `map` is a function. That function gets invoked later when you actually consume the iterator. In contrast, the for loop does not take a function - it just has a body to execute. That makes it eager and thus spawn each thread immediately. So using `map` does not create any threads until you iterate over `handles`. However as you iterate `handles` you call `join`, which blocks the thread doing the iteration. That sequences the threads because the next thread will not be spawned until the next iteration, and that was just prevented with `join`.
Sure, I won't disagree with that. The signatures are useful for an experienced user who knows what they're looking for, but they're also worse than useless for trying to find something new/unfamiliar..
&gt; What exactly is different though ? The compiler spends time performing optimisations rather than just do a straightforward translation. Because Rust has lots of high-level constructs which *can* be optimised down to very little it becomes very important. &gt; Does this mean I can't use {:?} anymore ? Not at all, that's a completely different debug. Debug-printing is a property of the type not of the level of compilation.
Fair enough. I'm certainly grateful for all the work you've done, I'm just trying to think of ways to make it better. Out of curiosity, with "ways that various features interact", do you mean in terms of the features in Rust and how they depend on each other, or do you mean parts of rustdoc itself and how it processes information? (I realize those cases aren't entirely separate...)
Maybe there could be a button that expands out the docs for trait functions in the trait impl section at the bottom so you can ctrl+f
I would love that, I'm just not sure how it'd work if, say, the documentation gets generated to PDF instead of HTML.
Your code looks mostly optimal, and on my machine, using input 9: * Ruby: 8.0 seconds total time * Rust: 2.5 seconds total time I compiled with `rustc -O` I tried to squeeze a little more performance out, by: * locking and buffering stdin/stdout * re-using the allocation for the numbers vec and the line String The average runtime moved down a little more to 2 seconds
Then you wouldn't have that functionality, and it's no worse than it is now.
Do I spy a Dayan cube to the right of the table? 
The way I improved documentation for myself is that everytime I fail to find something, I go to #irc, ask nicely and some wonderful person answers in 10 seconds. Plus experience. Experience helps a lot.
I happened to run into this problem yesterday and solved move-outs with `Option`... let mut v:Vec&lt;Option&lt;Blah&gt;&gt; = ... ; let a = v[0].take(); 
Hey Erick! Thanks for sharing the opening! We've been hesitant to share this on Rust focused sites since the expected workload for the position isn't 100% Rust. There is a lot of Rust work right now, but we aren't making any promises that it will always be that way. We are a small shop, and everyone tends to wear several hats. The last thing we want to do is bait eager Rust developers with a promise of writing Rust all day, every day when that isn't necessarily true. With that huge disclaimer out of the way, do you think it's appropriate for such a position to be on the Rust job board, /u/erickt? For full disclosure, here's a bunch of our work either ongoing or upcoming: - Building OnePush, our notification delivery daemon (written in Rust!) - Adding support for other push providers (web push, windows, amzn, etc) - Replacing synchronous IO usage with Async as it becomes available or contributing to OSS projects to help make Async HTTP available - Optimizations - Releasing reusable parts as independent projects - Rails + Front-end work. - Push SDKs (android, iOS, windows, web push, etc.) - DevOps - Scaling our DB (currently PostgreSQL). - Blogging. Seriously. There's two or three Rust posts we want to write relating to OnePush. Can be hard to find time with our current team size. So, if you're interested in applying to this position, yes, there is Rust work. Ideally, we want someone who can double as a Rails dev, mobile SDK dev, or devops + DBA. If you are interested and have any questions, I'd be more than happy to try and answer them!
Maybe there could be a separate page for "Every method implemented by Vec or a Deref of Vec" with links back. A kind of index page.
Once rustc compiles on it, right?
Bah, if only it was in Toronto instead of Mountain View. Good luck to all who apply :)
Oh my god, thank you! What did I read the rust book for if I didn't even see this. Thank you very much! But what exactly does the {:?} do?
I totally agree with you in that traits are a plague where documentation is concerned, in multiple ways. Maybe if we could consolidate all the different issues we have with them in a single place, this could lead to a RFC to change the format of the generated Rust Docs? I see two information hiding issues: - the documentation for a trait's methods are only available on the trait's documentation, even though as you mention they would be very welcome on each type page - I have not found a way to supply *more* information for a given trait's method implementation, even though in some circumstances the author may wish to include additional guarantees, particular side-effects, etc... just because a type implements a trait does not mean that the method called is always called in a generic/polymorphic context I see a style issue: - as you noted, when a trait is implemented multiple times, its methods are cited over and over, for little benefits; your suggestions looks good, though a way to include associated items (types, constants, ...) is also necessary. I have a suggestion for associated types (using the syntax `K&lt;Associated = X&gt;`) but none for constants I see an overabundance of information issue: - as noted, the lack of integer generics or variadic generics can cause arrays and tuples to generate *lots* of boilerplate documentation. While these issues will solve themselves in due time, in the mean time maybe some attributes could be used to hint to rustdoc how to *fold* all those implementations together; at the very least for arrays I could imagine a `#[doc(fold = "0; 32")]` annotation or similar I have noted the issues in order of importance (to my eyes), hidden information being the chief issue since as mention it breaks CTRL+F.
This is placeholder used to format text for debugging purposes. http://rustbyexample.com/hello/print.html
Wouldn't it be possible to have shim lib, which has a stable C API to the outside and implements whatever the current Rust ABI is on the inside?
Oh. Well, thanks :) I just tried to add some things to my function and have this right now: fn init(path: &amp;str) -&gt; Result&lt;&amp;str, io::Error&gt; { match env::home_dir() { Some(ref p) =&gt; { println!("{}", p.display()); try!(fs::create_dir_all(p.into() + path)); }, None =&gt; println!("Impossible to get your home directory"), } Ok("Successfully initialized legion home directory") } How would I return an error in case of None? Error handling is pretty hard to grasp for me :/
Well, first of all you should avoid printing, and instead leave the caller to print by giving him the details instead. Then, as mentioned Rust is an expression language so you just have your full `match` expression evaluate to a value of type `Result&lt;&amp;str, io::Error&gt;` by: 1. pulling the `Ok` into the branch of `Some` 2. forging an error in the branch of `None` along the lines of: fn init(path: &amp;str) -&gt; Result&lt;&amp;str, io::Error&gt; { match env::home_dir() { Some(ref p) =&gt; { println!("{}", p.display()); try!(fs::create_dir_all(p.into() + path)); Ok("Successfully initialized legion home directory") }, None =&gt; Err(io::Error::new(io::ErrorKind::Error, "Impossible to get your home directory")), } } I would note that examples in the docs are *short and to the point* and about demonstrating the item documented and do not necessarily try to examplify good coding practices, so be wary of copying them wholesale. I would also note that returning a message when the function worked is not very interesting, so you should probably return `Result&lt;(), io::Error&gt;` where `()` is both the unit type and the single value representable by this type (it's a 0-length tuple). This can be done by changing the `Ok` expression to `Ok(())` and is frequently used when no useful value is to be returned but there might be errors. I am also not sure that `p.into() + path` will insert the separator if necessary, you might want to check that.
`as` is a compiler built-in and it came long before `From` and `Into`. If it was the other way around (`From` and `Into` came before `as`), maybe we would not have `as` for `i64 as f64` and it would have been implemented as `impl From&lt;i64&gt; for f64` instead. Note that `as` has other functions as well (e g `&amp;Struct as &amp;Trait`) which is also a compiler built-in and which cannot be expressed if `as` was just the `As` trait you describe.
&gt; Does this mean I can't use {:?} anymore ? You can use debug prints when compiling with optimisations (i e release mode) too. The `Debug` trait has nothing to do with doing a debug build.
Yes, the docs really are short and to the point. I tried to read through it all, but some things are...well, a bit complicated since Rust is very different from other systems programming languages. Thank you for your help. You and /u/jakko100 explained it a lot better than the docs :P
Well, I'm not quite good explainer yet, I'm learning still :) But glad I could help! 
Yeah, this might be helpful too.
&gt; But since he saw that the performance was kinda worse, he got turned off by that. We had this debate many times. Ultimately we decided on the same thing that C does: no optimization by default. For every potential user of Rust who leaves due to missing the part of the documentation that explains how to compile in release, there are even more potential Rust users who would leave due to not having proper stack traces by default or having long compile times.
Consider [MSDN docs for .NET `List&lt;T&gt;`](https://msdn.microsoft.com/en-us/library/6sh2ey19\(v=vs.110\).aspx) which show available extension methods as a separate table. In a similar vein it might be useful to list all (standard) trait impls for `Vec&lt;T&gt;` on the `Vec&lt;T&gt;` page.
How about something creates fresh rustdocs and pushes that to somewhere as well? That seems to be a very common release task too :-)
The weirdly ornamented collect() did the trick! I see what's going on now. Thanks!
These are just some small minigames for fun. It is like the `BSD-games` package, but for Redox.
Has Redox begun using the new support for naked functions yet? I would expect Redox to be the project to demonstrate the feature's utility before it could land in stable.
A lot in fact! It should be noted that we pretty much already had the code ready, we just had to comment it out. https://github.com/redox-os/redox/commit/9a204f19546e10a728d6fd28d7e8b01d3c94b4b1
I'm glad you liked it. We also have an `oxsay`: /----------------\ | Install Redox. | \----------~ ~---/ V &amp; &amp; _________&amp;_&amp; ~/ \ / c ^\ / | / \ \ \ | __ \__. |_!_| |_!_| 
&gt; Not all Rust features can cross the FFI boundary (e.g. generics), and even then in order to "load and properly call methods/trait impls, etc from other languages" you'd have to have a way to un-shim all of those APIs from behind the destination language's C FFI, which probably involves reimplementing swathes of the Rust compiler in a way specific to the destination language. Well, not in the general case, but to my understanding, both trait objects and _a previously known set_ of concrete types can very well travel (the appropriate functions for them can be called). On the other hand, this is probably better be solved by a shim-dylib that does just that amount of work by hand.
I love this! &gt; All you have to do is follow the Angular.js commit message conventions, Ah, this is useful, but not amazingly perfect. :) I really dislike this kind of thing. A tool that analyzed the source would be much better, though ridiculously harder. But beyond these minor quibbles, awesome! More of this kind of thing!
&gt; Ah, this is useful, but not amazingly perfect. :) I really dislike this kind of thing. A tool that analyzed the source would be much better, though ridiculously harder. Well, first step to try would be a tool that suggests you an angular commit message class by comparing the signature of `HEAD` with `HEAD^1`. (begrudgingly walks back into the shed and greets the new yak that mysteriously popped up)
Oh yeah, absolutely. I would much rather have this tool than not.
Truth. To be clear, "this kind of thing" is the commit conventions, not the tool :)
Like a language specific lib interposer, bridges perl&lt;=&gt;rustdylib and the interposer exposes a perl specific interface that knows about Rusty stuff. This interposer would load _all_ Rust libraries on behalf of the host language.
I'm wondering for some time already, is there are particular reason to hide docs for trait implementations, provided that the implementor did indeed write them? I cannot recall whether there ever was a time when these docs were not hidden. They look very useful, because for many traits (like your `Arc` example) it would benefit immensely to have at least some explanation of how exactly the implementation works.
It simply skips the stack pointer increment. For an in-depth explanation see [this](https://github.com/rust-lang/rfcs/pull/1201#issuecomment-198505381).
Neat! Presumably this allowed you to remove some assembly shims, yes? Can you link me to the commit that did so so that I can see how much code was removed? (Or if you haven't removed it yet, can you point me to the files that could potentially be removed?)
Woo, RustType in Redox! Great stuff. Is the stretched horizontal scale intentional?
I don't believe so, but I also don't remember why it was changed to be hidden. I would like to turn them back on, but haven't had the time to investigate.
Hey! Congrats, yo!
This is so cool. Sometime last week commenters were arguing about how to pronounce 'Redox' and in my mind I was like obviously it is 'red ox' and now I see real red ox:-)
I figured it out. I have it building a lib with a very thin binary on the top of it. I had added the macro_use to the lib, but not the bin.rs file. The thing is, I don't need the macro to be available outside of the lib crate. Is there a way to do this without forcing the binary to pull in the external crate since only the lib uses it?
Yeah, I followed that... I guess my question is more along the lines of [this one](https://github.com/rust-lang/rfcs/pull/1201#issuecomment-196242664) -- what happens if you do something that implicitly assumes that there _is_ a stack frame? e.g. is it possible to accidentally corrupt the stack, or does LLVM watch out for that due to the naked attribute.
I fear I don't know. It's odd that you'd need to declare the extern crate in the bin feature if you only need it in the lib, but I haven't really played with mixed bin/lib libraries.
YW, sorry it didn't work.
That's a horrible error but unfortunately, there's not much I can do about it. The problem is that `: tmpl1` moves `tmpl1` so the return type is actually `Box&lt;RenderOnce&gt;` (because the underlying closure only implements `FnOnce`). The solution is to use `: &amp;tmpl1`. 
Yeah, I wanted to suggest that as well. I think that generally speaking, adding more content to any system should entail also adding more structure. What that structure should be, I don't know but I can throw a few ideas in the air: 1. Separate out the examples somehow, to a separate column perhaps. 2. Separate out the lists if traits and impls. 3. I also don't like to see big modules/files when viewing source. Perhaps it makes sense to move the doc comments to a separate file. Make the examples external and just put references in the doc comments or something. Scrolling the text, be it the source code our the docs, means that you lose the connection between pieces of information and their physical location on screen which makes it harder to quickly retrieve them. That's at least my experience. 
You'd have to look back pretty far as (afaik) redox has been using a compiler with #[naked] patched in for quite a while.
&gt; For every potential user of Rust who leaves due to missing the part of the documentation that explains how to compile in release, there are even more potential Rust users who would leave due to not having proper stack traces by default or having long compile times. Why are you so sure about this?
On this note, something I've missed is a summary of methods implemented by a struct. Sometimes I know a method exists but don't know its name. I can't ctrl+f for it, so I have to scan the page until it jumps out at me. Compare this to something like [elixir's docs](http://elixir-lang.org/docs/stable/elixir/Map.html). They have a summary at the top of each type's page. Then on the left frame there are collapsible function listings. 
&gt; locking and buffering stdin/stdout How to do that? Actually, what does locking stdin/stdout mean?
You can still use `Debug` (and `{:?}`) on non-debug builds. Actually. By fiddling a bit with Cargo, you can enable debugging symbols on non-debug builds as well (equivalent of gcc's `-g`). Debug here means that extra assertions are inserted (like overflow checking), and the compiler doesn't do optimization passes, to compile faster.
The scale was 2:1, from the simple example. It is fixed in the latest master
Games require a number of operating system features to work correctly and efficiently: - Graphics - Input - Filesystem - Threading - Audio - Networking They are a good test for functionality, and fun!
It is always possible for LLVM to emit the wrong output, so the critical naked functions are usually inspected in the disassembly of the compiled kernel and/or programs, to be sure that this does not happen
I think you may want to try and find a different performance testing website. 
It is already possible to compile C programs for Redox. We have ported newlib, which has allowed us to port things like lua.
There will soon be 6 kinds of ranges in stable with the addition of these 2: * [`RangeInclusive`](https://doc.rust-lang.org/nightly/std/ops/enum.RangeInclusive.html) formed by `x...y` and meaning `[x, y]`. * [`RangeToInclusive`](https://doc.rust-lang.org/nightly/std/ops/struct.RangeToInclusive.html) formed by `...y` and meaning `(-∞, y]` or `[0, y]` depending on whether the type is signed or not.
Does the compiler explicitly state he is compiling in non-optimised mode or something? If not, that might be a way to give users a hint, a little verbosity for a debug setup doesn't sound too bad. 
That would be an extra line on every single compile step. I don't want to have to suffer through that verbosity to help market to users who are running tiny microbenchmarks without reading documentation thoroughly.
Cargo says "compiling in debug" or "compiling in release".
I tried something similar. I don't think you actually have to buffer stdin/stdout, since they are already buffered, but locking them and re-using the string and vec can be useful: use std::io; use std::io::prelude::*; fn main(){ let stdin = io::stdin(); let stdout = io::stdout(); let mut stdin = stdin.lock(); let mut stdout = stdout.lock(); let mut input = String::new(); let mut numbers = Vec::new(); stdin.read_line(&amp;mut input); let t_case: i64 = input.trim().parse::&lt;i64&gt;().unwrap(); for _ in 0..t_case { input.clear(); stdin.read_line(&amp;mut input); input.clear(); stdin.read_line(&amp;mut input); numbers.clear(); numbers.extend(input.split_whitespace().map(|s| s.parse::&lt;u32&gt;().unwrap())); numbers.sort_by(|a,b| b.cmp(a)); for i in &amp;numbers { write!(stdout, "{} ", i); } write!(stdout, "\n"); } stdout.flush(); } Here are the results I got on Input 9. I ran each of these a few times to make sure these numbers were reasonably stable and no cache funniness was going on; I didn't do anything to ensure results are statistically significant, but these times were reasonably stable. $ rustc --version rustc 1.9.0-nightly (98f0a9128 2016-03-23) $ ruby --version ruby 2.0.0p481 (2014-05-08 revision 45883) [universal.x86_64-darwin14] Program | Optimization | Time :--|:--|:-- ifarmlolis.rs| | 0m1.686s ifarmlolis.rs| -O | 0m0.606s annodomini.rs| | 0m3.408s annodomini.rs| -O| 0m0.394s ifarmlolis-friend.rb| | 0m1.838s As you can see, the original Rust version was similar in speed to the Ruby version, if compiled without optimization, but three times faster if compiled with. My "optimized" version was nearly twice as slow as the original Rust version when not compiled with optimization, but closer to twice as fast when compiled with. It gives close to a factor of 10x speedup between compiled with optimization, and without. When I tried my optimized version on the online judging system, it reported a running time of 4.208259 seconds on input 9. Based on that, and the other times reported here, I suspect they are compiling without optimizations. Actually, it's relatively easy to test that out. It doesn't show us the command line used to compile, but it does show you errors compiling. We can just add: #[cfg(debug_assertions)] fn test() { blah } and see if compilation fails. By default, if compiling with optimization, debug assertions are off. And indeed, compiling this on their scoring system fails. It sounds like the service ought to update their compilation flags. Rust compiled without optimizations can be fairly slow; it takes advantage of some fairly sophisticated abstractions which can involve many layers of function calls when compiled naively, though many of them can be optimized away by trivial inlining. However, that doesn't happen when running running without optimizations, giving you the results seen here.
Fantastic! It's a bit of a drive for me (I'm in West Virginia, about 2 hours away), but I'm definitely interested in checking it out.
So, unlike C, Rust doesn't do automatic line buffering in stdout? Also, in Rust, one doesn't do `setvbuf` but do buffering entirely in Rust?
So, the RFC was accepted, and an implementation has landed. It has to be unstable for at least one release before it's considered for stabilization. So a few weeks at minimum. After that, It Depends. There's still a lot of questions about the feature, so I expect it to be at least a few releases before it's going to happen.
Thanks. Is there a way to disable this line buffering? `StdoutRaw` is private, and I can't find a way to use `sys::stdio`. Perhaps it's only possible by calling the libc directly?
I think you could also use [`FromRawFd`](http://doc.rust-lang.org/std/os/unix/io/trait.FromRawFd.html) to construct a `File` from the raw file descriptor (0, 1, or 2). I don't think there is any more ergonomic or safe way to do so.
I think the traditional recommendation (other than a university course/textbook) would be http://wiki.osdev.org though it's often incomplete or obtuse. http://intermezzos.github.io/ is a rust-specific example but its also incomplete. There are assuredly many more, that's just what I rember right now.
Yeah. The Elixir docs aren't perfect, but they are such a breath of fresh air compared to the docs of many other languages. Beautifully designed and very professional. I think Rust docs should probably emphasize the types more, but yeah - we could learn a great deal from them.
But not if the function can also return an io error. This allows you to can system calls that result in an io error and send your own errors back. I was previously just sending strings 
Damn, I'm in the Austin area but sadly I'm a full time High Schooler, no chance for me to go. Have to keep it in mind for next year though!
I live in Austin and this will be the one week that I'll be out of town. Agghhh. Maybe I can make it work. Did i read correctly, Steve's course is from 9am-5pm?
What is needed is a graph based search engine, where you describe what you want and it tells you how to get there. Haskell as [Hoogle](https://www.haskell.org/hoogle/). Rust needs Roogle, Rustpile, Rustavista, Ring! For `extend` you know you want the Vec to mutable and it takes an element of the type already contained. That should narrow it down enough.
Yes I believe so. /u/steveklabnik, please correct me if I'm wrong. 
If that is the case, is it more of a learn-rust-for-beginners thing? I'm semi-well versed in rust, would I get much out of attending the whole thing? 
Another metric idea, measure the mean time to close coupled with the mean path of the dependency in the transitive graph. This will measure how much potential energy there is for a fix is something that isn't yours but is breaking your code.
Congrats, Carol!
Agreed that a ToC is very helpful, especially for long documentation pages. I find Javadoc's structure to be very convenient ([example](https://docs.oracle.com/javase/8/docs/api/java/util/ArrayList.html)): there's a summary table near the top of the page (method name, type signature and a one-line description) with the full documentation listed below. Additionally, the summary is in alphabetical order while the full listing is in implementation order, which enables dual indexing: for finding a particular method, you can quickly scan the table of methods, while the full documentation will likely have related methods listed nearby.
I agree the docs are noisy and as a rust beginner I've definitely had issues trying to find what I'm looking for. In my beginner use cases I'm mostly just trying to find what sorts of methods are available for my `Vec`, and in that scenario, whether the method is implemented for the struct or comes in via a trait doesn't matter. I would like an alphabetized section of "here's all the things you can call on your `vec`". The other thing that's kind of irritating to me, though I can understand the motivation for the design, is the "code as docs" theme. Like, I get that impl&lt;T&gt; Clone for Vec&lt;T&gt; where T: Clone fn clone(&amp;self) -&gt; Vec&lt;T&gt; fn clone_from(&amp;mut self, other: &amp;Vec&lt;T&gt;) is what it would look like if you were to view the source code, but to me all the repeated "impl" and "fn" and other rust syntax bits just clutter the page. On the other hand, I'm sure that it's a familiar nod to rust experts, who can see at a glance all the type and lifetime and trait annotations and get stuff out of it. But for me as a beginner, I usually have a thing, know its type, know roughly what I want to do with it, and now am scanning its documentation trying to find out whether a method exists for what I want, and what it's called. In this use case, the docs are very noisy and it's hard to pick out, e.g., "clone_from" in the example above, to consider whether that's the method I'm looking for.
Does anyone know if it would be possible to use Hoogle's type-searching algorithm on Rust? I'd imagine that traits would be analogous to type classes here...
The chemistry term is pronounced "ree-docks", short for "reduction-oxidation".
They definitely record each talk, though I don't know whether they make the videos available to the public or only to attendees. Even if the videos aren't public, I may very well end up giving this talk again sometime (maybe at Rust Belt Rust??), giving us a second chance for a recording. :)
The birds-of-a-feather sessions (which is just a fancy name for loosely-organized meetups in conference rooms) don't start until 7 PM, so if you aren't too far away you may able to make it to ours. :) (We don't have an exact time or day for the BoaF yet, because they're typically scheduled on a first-come basis on the giant paper calendar in the main hall.)
The description at http://conferences.oreilly.com/oscon/open-source-us/public/schedule/detail/49862 should hopefully give you an idea of whether or not you'd want to attend, but I'd assume that if you're semi-well versed in Rust then you may not be enthralled for the entire two-day session. :) I'll be trying to get in touch with the Rust community in Austin to see if they want to do some events outside of OSCON, however, so maybe you'll be able to make it to those?
Note that burntsushi's post on error handling has been incorporated into the book itself, which may have more recent updates than the original post: http://doc.rust-lang.org/stable/book/error-handling.html
Okay! Sounds good. I may not get a lot out of it....but I want to meet steeeeeeve (and other rustaceans...but he has helped me a lot on the IRC)
I said array, not vector. Arrays have a fixed size at compile time, so it's surprising at first that they don't let you do partial moves, like with structs. Some people might assume that arrays do have this ability, which would make it surprising that vectors don't. In fact, I'm pretty sure the borrow checker doesn't figure this out automatically because a) it would add a ton of code complexity (if it's even possible in every case, which it probably isn't) b) it would limit what we're able to do in the future with arrays (ie. we couldn't have the length of an array be a generic type parameter) and c) it would be really difficult to represent that indexing capability as a trait (maybe not even possible with the existing type system), and that would further complicate code generation. So it's a pretty bad idea. On the bright side, we could change our minds in the future, and it'd be backwards compatible. But I doubt that would happen.
If you use the functions built in to Vec then this isn't necessary (it would consume extra memory and make your code needlessly complex). The only time you should use Option is to represent the potential absence of a value, not to represent values that may or may not have been moved.
Looks like an opportunity to add a "did you mean to add a trait bound like this: [example function signature with required trait bound]" hint to the error message.
As an aside, I would recommend rewriting impl&lt;T: fmt::Debug&gt; Info for Buffer&lt;T&gt; as impl&lt;T&gt; Info for Buffer&lt;T&gt; where T: fmt::Debug since it tends to read easier and do a better job of highlighting the incidental nature of the constraint on T.
This is the way you should pronounce it.
Actually, we have not used a modified compiler.
The Redox red ox says "Install Redox" from oxsay in a Redox box.
CMake can generate rpm, deb, etc. packages for you and it's easy to integrate with Cargo based projects. I use CMake to build Rust modules for a C application. "Make install" works as expected and the next step is the native package generation.
Thank you. Now it is a bit more clear how everything works. By changing the variables to reference and using clone() in a couple of places, I was able to make it work in my actual app.
binary portability is long standing issue on GNU/Linux. C/C++ programs are suffering because glibc symbol versioning hell. You either give up and let the distribution handle it for you or dedicated enough to compile it on older distro version. I think rust can have better story on this glibc issue. If rust can have ability to pick older glibc symbol version when compiling, ex: realpath@GLIBC_2.2.5, rust will solving this problem much better than C/C++.
Just like + and -.
There's [this answer on SO](http://stackoverflow.com/a/31175323), but my major gripe with it is that you can't have both the CMake and the Cargo build files under the same `build/` directory.
It still can be removed (personally I hope that this will happen).
Well there are these two (unmaintained) projects. https://github.com/ajtulloch/roogle https://github.com/dbp/rustle
Where is the source of rustdoc actually hosted? Is this the place to look at: https://github.com/rust-lang/rust/tree/master/src/librustdoc?
No, the submission time doesn't include the compile time.
It's not specifically about this post, but I find the over-use of `match` rather than HoFs and macros a bit disheartening, especially for Result and Option which do have extensive manipulation facilities there. For instance, let update_result = self.update(); match update_result { Ok(_) =&gt; Ok(self.construct_id()), Err(err) =&gt; Err(err), } could also be written self.update().map(|_| self.construct_id()) likewise `parse_name` can be un-nested slightly: s.find(separator).and_then(|index| { match (T::from_str(&amp;s[..index]), T::from_str(&amp;s[index + 1..])) { (Ok(l), Ok(r)) =&gt; Some((l, r)), _ =&gt; None } }) Matches could be removed altogether if `Option` had `zip`. That can be obtained going through iterators, but it makes the code less readable (more obscure) rather than more, so that's a terrible idea: T::from_str(&amp;s[..index]).into_iter() .zip(T::from_str(&amp;s[index + 1..])) .next() 
I also thought about this but my graphic-engine knowledge is low for now. But i thought Rust could be a very competent language for a graphic-engine or game-engine too. In my opinion, first option is the best choice. You need to work on Metal wrapper, because you already have https://github.com/tomaka/vulkano as you said. Just be careful, the repository says: Warning: this library breaks every five minutes for the moment.. Also you would have to work on a unified shaders API and adapt it to both wrapper After that, you have a modern graphic library and cross-platform such as vulkano that cover all your requirements, except iOS and OSX. But in apple side, these Mac computers support Metal: MacBook (Early 2015) MacBook Air (Mid 2012 or newer) MacBook Pro (Mid 2012 or newer) Mac mini (Late 2012 or newer) iMac (Late 2012 or newer) Mac Pro (Late 2013 or newer) Source: https://support.apple.com/en-us/HT205073 I think is the right time to jump on these techs and you are not sacrificing too much market :) 
Sure, but that's a far cry from the LGPL kind of approach I'm going for.
well at least is made in rust, community will like for sure :) when you start with the work, i would really like to check on code if you upload to a repo
If Metal and Vulkan are almost 1-to-1 then it should be possible to make a wrapper. Perhaps someone will make an open source wrapper. I also wondered about this. Obviously, it would be non-trivial. I also wonder if eventually Apple will support Vulkan. They have been all about open standards in the past. I get the feeling that within Apple, developers also care about open standards and will push for that.
It would perhaps be more useful to get [gfx](https://github.com/gfx-rs/gfx) a Vulkan and Metal backend? My understanding is that it's designed to facilitate high performance support for those APIs too, even though it was first implemented for OpenGL and now [supports Direct3D 11](https://www.reddit.com/r/rust/comments/4bclmf/d3d11_backend_for_gfx_is_merged_we_are_officially/?ref=search_posts). (see [this comment](https://www.reddit.com/r/rust/comments/4bclmf/d3d11_backend_for_gfx_is_merged_we_are_officially/d18zvrb?context=1))
I'm certain they do. I think their main gripe with not supporting OpenGL 4.2-4.5 is actually the fact that Khronos was playing catch-up with Microsoft's DirectX. But here, they actually have a chance to fight Microsoft and they're passing it.
I've actually been considering this, but I'm not entirely sold on gfx yet. I want to make a game engine here, and adding this to gfx sounds entirely not trivial and might actually be a task as big as the whole project.
I didn't take a very good look at it, but it's kind of low level and very thorough. So my implementation would mean implementing a lot of functionality for Vulkan and Metal and then writing some kind of wrapper on top of gfx to make it more friendly for game development.
You're likely to want DX12 for Win10 and Xbone anyway. Vulkan on its own won't be realistic any time soon.
Check this file for an example:https://github.com/ihrwein/syslog-ng-rust-modules/blob/cmake/regex-parser/CMakeLists.txt The cmake/Modules/UseRust.cmake file can be also interesting for you. 
I would also like that. It's possible to document the `impl` block, but that's a rather blunt instrument in some cases.
Xbone and PS4 are currently not on the roadmap. This would seem like a great moment to use gfx, but it seems a bit complicated. It doesn't have a unified shading language and it only has an OpenGL and DirectX as backends.
I think you should talk with Gfx developers about those issues, to avoid duplicating functionality. For example, if Vulkan and Metal is similar enough, perhaps it's appropriate to have a library that abstracts over both, and have Gfx depend on such library. Or perhaps this doesn't make sense, and any library that abstract over Vulkan and Metal isn't usable by Gfx.
Of course, no need for credit either if that's a bother, I just like spreading the Word of HoFs. The more people are using the nice features from the stdlib (where they're suitable) the happier I am.
After [doing some investigation to determine whether they were compiling with optimizations](https://www.reddit.com/r/rust/comments/4cg5bu/why_is_this_code_snippet_slower_than_ruby/d1ie0z5), I contacted the site about this, and they replied: &gt; We have added the optimization flag "-O" while compiling Rust code with rustc in our Judge environment. And we'll soon upgrade the Rust compiler version from 1.4 to 1.7 I tried submitting one of my previous submissions again, and and you can see the results by comparing the [before](https://www.hackerearth.com/submission/3657414/) and [after](https://www.hackerearth.com/submission/3661147/) times. Since the solutions are hidden for those who haven't yet solved the problem, the results are: # Before Input | Time (sec) | Memory (KiB) --:|--:|--: Input #1 | 0.101566 | 64 Input #2 | 0.101507 | 64 Input #3 | 0.10117 | 64 Input #4 | 0.101398 | 64 Input #5 | 0.102128 | 64 Input #6 | 0.381284 | 8552 Input #7 | 2.921251 | 8552 Input #8 | 4.288444 | 8552 Input #9 | 4.211226 | 8552 # After Input | Time (sec) | Memory (KiB) --:|--:|--: Input #1 | 0.100813 | 64 Input #2 | 0.100858 | 64 Input #3 | 0.10077 | 64 Input #4 | 0.100859 | 64 Input #5 | 0.100763 | 64 Input #6 | 0.100685 | 64 Input #7 | 0.255479 | 8552 Input #8 | 0.370531 | 8552 Input #9 | 0.361709 | 8552 So, as you can see, for these larger inputs, that gives a greater than 10x improvement in speed. So now you should be able to properly compete with your Ruby, or C or C++, using friends.
&gt; Define an error type for your app that is an enum of all the possible error types that libraries you use can return, plus Ain't nobody got time for that. I just give the app-level Error type an Option&lt;Box&lt;Error&gt;&gt; accessible with the .cause() function.
At some point trait impls get so verbose that I've considered hiding them from the docs. [Should I?](https://bluss.github.io/rust-ndarray/master/ndarray/struct.ArrayBase.html). Fun fact: The majority of trait impls are already hidden, 210 trait impl clauses are used to implement left hand side scalar arithmetic operations, and those will not show up. [This is](https://bluss.github.io/rust-ndarray/master/ndarray/struct.ArrayBase.html#arithmetic-operations) the substitute for the autogenerated docs.
To me, it feels like rustdoc shouldn't be part of the compiler. But rather reside in an own repository. Is there a reason for this or did it just grow like that ;).
It uses the compiler in its operation.
See also https://github.com/japaric/rust-everywhere
I've been porting a library of mine for reading wavefiles from C to Rust -- [ledbettj/wavefile](https://github.com/ledbettj/wavefile) Would really appreciate any feedback or code review!
Yeah, two looong days.
thank you!!! &lt;3 &lt;3
With `cargo bench` only available in nightly, I haven't tested with a different rust version, but I'm getting the same ratio on my machine (Ubuntu 14.04 with an i7 with avx).
This issue is filed [here](https://github.com/rust-lang/rust/issues/24414).
It can but they don't recommend it, and you lose some functionality.
It's a trial of q&amp;a sort. You are the first to call me out on it in 2 weeks. ;-) If you like, we'll return to sort by new next week.
Awesome, Carol! Keep it up!
I'd love any feedback (API, documentation, etc.) if you have any.
You should take a look at the [quick-error](https://crates.io/crates/quick-error) crate. It makes it really simple to create these kind of nice error enums.
Cleaned up a small reusable piece of numerical code [1], using the excellent ndarray crate. It computes weighted means and variances online. Still working on the (currently) private larger project this is part of. [1] https://github.com/daniel-vainsencher/online_weighted_stats
As I understand it, Vulkan is accessible in Win32 apps if the installed video driver supports it, but not in ~~Metro~~ ~~Windows Store~~ UWP apps. (This understanding may be out of date; I'm not sure whether the issue was that it flat wouldn't work or that Microsoft wouldn't sign it for the Store.)
This is a good idea, but I can't see where it says that.
This is apparently due to a tricky interaction between trait objects and lifetime elision (and a bad error message). There's a lifetime associated with the trait object `T`, and since you didn't write one, it defaults to `'static` (the longest possible lifetime). Working code is given in this [issue](https://github.com/rust-lang/rust/issues/27953).
This sounds pretty good! The biggest issue I've seen was the problem with with writing multiple shaders. Not so long ago I've been experimenting with designing a language specifically for shaders by using a subset of Rust, but because Vulkan has been advancing very slowly and because of a lack of LLVM to SPIR-V translators, I've placed it on hold. I would be more than happy to help with this. I would be just as happy to help with a Vulkan/Metal backend, but considering how enormous both of these APIs are together, I'm a bit worried about the scope and how this would affect my plans with the game engine. Still, without any decent glue between these APIs, the game engine would lose cross-platform compatibility. Maybe the best approach would be to join and help until gfx reaches a point where it can support both of the APIs in terms of game rendering, then concentrate more on the actual game engine itself. As a sidenote, the unified shading language still sounds mighty great to me. With a few abstractions, it will mostly likely be able to run custom code for some platforms.
You have a point, but I'm a really, really big fan of standards and openness and working together. And this is why I'm willing to give up on some proprietary hacks that vendors place in order to have something nice and approachable by pretty much anyone, while still getting the best possible performance with what you have.
Aha! That sounds good
Yeah, I think &gt; near C performance is a bit misleading here. Rust is as fast as C, but forces you to be explicit about the times where it can't guarantee your safety via `unsafe`. Even then, safe Rust should be at least as fast as C *in practise* as although it happens to provide some safety via run-time checks for a few std types (C-in-practise would likely have to do something similar to these run-time checks in order to be safe anyway), Rust's type system also provides a lot more compile-time guarantees, removing the need for run-time checks that might be required when writing C. Anyway, I really liked this talk Eduard! It was a nice insight into `carboxyl` - I think there might be a lot of potential in building up an ecosystem around it.
There are definitely ways rustdoc can improve, and it can be used more after some time, but I'd like to offer one other suggestion: If you're spending more than 1-2 minutes trying to find a method, go mention it on the #rust IRC chat. There are many, many people idling there who would be more than willing to offer a helping link. It is hard to find things in rustdoc, but someone who's been working with it for a few months or more will gladly send you in the right direction. What rust lacks in maturity of document I on and tools it more than makes up for with community.
There's some stumbling over explaining Borrowing and Ownership. Being that this is the heart of complexity in Rust, I'm not too surprised. I'm beginning to think that Borrowing and ownership can't be discussed without Lifetimes. 
The problem here is that the `Sized` bound is always implicitly there if it's not mentioned that the function/struct/etc. doesn't require it. So in a sense, the documentation is lacking that it doesn't mention the `Sized` constraint. It's still true that all generics are constrained at compile time, but `Sized` is the only where it is done implicitly.
Yeah but using `step_by` you can do [`(3..(end+1)).step_by(3).any(|x| (x % 5) == 0)`](http://is.gd/SSywQK) [Here is a golfed `is_prime`](http://is.gd/HRV6DY).
&gt; but your replacement doesn't step by two Note that if it's the only issue... for i in 1..(end+1)/2 { let i = i*2+1; // ... } Or something close (I am wary of off by one errors) could probably work :)
This is [issue 6393](https://github.com/rust-lang/rust/issues/6393) - i e, there is a long term project to improve the borrow checker to handle these cases better. But right now, as a rule of thumb, the borrow checker borrows things for either a statement or a block.
What if all members of the struct implement `Send` or `Sync`? I thought in that case the struct was also `Send`/`Sync`.
Even though the reference is being cast to a raw pointer, it could still have references inside them that are being borrowed. Technically the raw pointer is `*mut Trait + 'a`, because lifetime elision means that `&amp;'a Trait` is equivalent to `&amp;'a Trait + 'a`. If you change the trait's function return type to `&amp;mut (Trait + 'static)` it works.
Yeah. Rust lets you choose whether accessing an array has bounds check or not, no overhead (should) be mandatory.
I come from the land of C++. Does Rust provide *any* elegant way to define classes which may inherit? If not, what is the preferred alternative paradigm to inheritance?
I skimmed through a bunch of code in lib.rs, and it looks really good in general! I saw a use of `if let Err(_)` which could be `.is_err()` instead, but the biggest concern I had was the abundance of numerical casts. For example, there are a bunch of getters that return `usize` when the values they're getting are not usize, and there are other places (`WaveFileIterator::bytes_per_sample`) where a non-usize value is being manipulated and stored in a usize field. Additionally, the casts from values returned by the Cursor API to usize suggest that the code should be using larger types to accommodate it, since the library risks incorrect results in environments where usize is smaller than u64.
What is it that you want to inherit? You may use traits for fine-grained control of what type gets what behavior. And if you want to inherit data, you're better off with composition anyway.
Thanks for taking a look, I appreciate it! I'll double check those casts. I did struggle with those initially where some function returned a u64 but another required a usize, for example, and I wasn't which way to cast. Edit: For example: /// The number of audio channels in the file. pub fn channels(&amp;self) -&gt; usize { self.info.channels as usize } Is it best practice to just return a `u16` here (since that's what it is in the file)? If I were writing this in C, I would probably just return an `unsigned int` here without specifying width. I think that was my thought process when returning `usize` -- this is the equivalent of `size_t`, right?
Then just set the start counter to 1 instead of 0 :&gt;.
Hey! Let's talk about magic! Box's dereference method actually moves instead of borrows. This is a compiler special-case that only works with Box.
`*foo` is usually desugared to `*foo.deref()`. That's why you need to write `&amp;*foo` to get the same result as `foo.deref()`. In your example it's even more comples, because `unwrapped` is of type `&amp;Box&lt;List&gt;`. So you actually need two dereference operators – one for `&amp;` and one for `Box`: let &amp;List{head, ref tail} = &amp;**unwrapped You can even get rid of both of those `&amp;`: let List{head, ref tail} = **unwrapped And speaking about the second example – `(*unwrapped).head`. Remember, in Rust `.` auto-derefs. So if your `unwrapped` had type `&amp;&amp;&amp;Arc&lt;Box&lt;&amp;&amp;List&gt;&gt;`, you could still use just a single dot to access `List` fields. There's also one small implementation detail worth talking about: `*foo` in expression is kind of magic. If `foo` has a type `Box&lt;List&gt;`, `*foo` doesn't always mean moving the `List` out of the box. The expression would have type `List`, but it would be actually an *lvalue* – a reference in disguise (if you know references in C++, then you get it). If you have an lvalue `*foo` expression of type `List` you can do two things with it: 1. Promote it to real reference: let x = &amp;*foo; Please not that it's **not equivalent** to: let tmp = *foo; let x = &amp;tmp; or even: let x = &amp;{*foo}; 2. Create a real value – by copy or move: let x = *foo; The magic u/stumpychubbins is talking about is that you can actually do this with `Box`. The other smart pointers (eg. `Arc`) can only do the former. But for `Box`, the compiler will use "normal deref" or "moving deref" depending on context. In your case, it's only a normal deref.
If all members are `Send` or `Sync`, the whole struct is (unless otherwise specified) also `Send` or `Sync` – however: `Send` or `Sync` constraints for structs/functions/etc. are always explicitly mentioned, e.g. https://doc.rust-lang.org/std/thread/fn.spawn.html: The `Send` constraint of the function type F is explicitly named.
&gt; Remember, in Rust `.` auto-derefs Oh sorry. I was thinking in C/C++. Thanks dude. I'm gonna take a few minutes to digest these. EDIT: I think I get it. I created a reference to a Box instead of a Box with `&amp;Some(ref unwrapped)`
&gt; This is a compiler special-case that only works with Box. But.. why? Is there documentation about this?
Yes, returning types with explicit sizes is almost always the right thing to do. Additionally, casting from (potentially) smaller types like usize to larger types like u64 is always safe (ie. won't lose precision), while casting from u64 to usize can chop off important bits depending on the platform.
Keep in mind this is my first real Rust project, but I'm putting in an effort to bring Rust in the workplace and this project seemed like a good start. Had a great time putting this together! [Docs are here](https://passivetotal.github.io/rust_api/doc/passivetotal/)
OK so what *is* passivetotal? Just imagine I've been living under a rock for the last months or so and please kindly explain. ;-)
Perhaps both `MapProduct` and `Val` can be generated by a macro? Perhaps called with the following syntax: define_structure! { Val: Int =&gt; i32, Real =&gt; f32 }
passivetotal is a platform for security analysts or anyone working in threat intelligence. They provide a set of tools to let you discover links between infrastructure and pivot to other related machines. For example, you can take a domain and make a call to `get_pdns` and get a list of IPs it has resolved to in the past, take those IPs and check `get_sslcert_history` and enumerate SSL certificates they have used, find more information on those certificates with `get_sslcert` and pivot from the fields there through more API calls. You'd be surprised at how much threat infrastructure you can discover through doing stuff like that. You can see the full set of API calls [here](https://api.passivetotal.org/api/docs/#api-_), though some are not implemented in the Rust API yet. Almost all of the GETs are. I could see it being useful for pentesters too. It helps anyone who needs to create some sort of graph of infrastructure and get historical DNS data, certificate data, etc. There's a subdomains enumerator as well. You can sign up for a free account, but I'm not sure how many API calls you can make per day. I know it's limited somehow. I just recently started working with them. I was working on the research team with the company that bought them, then recently had my focus shifted to primarily help them dev. Also, I already know most of the API and have worked on a Python client for it, so I figured this would be a great project to learn Rust and prove to engineering that a beginner can easily develop a useful project with Rust quickly, just like any other modern programming language. I wanted a project where I knew the problem in and out, so the only trouble would be learning syntax and patterns for Rust. I feel like I learned a ton from this (but honestly reading through the Iron source helped me out the most).
Thank you. You may want to add a link to your project README.
indeed, thanks! that is only `Size` is magic.
While lovely and useful, that doesn't solve the problem, it offers a workaround. There are a few issues with this: * It can't scale forever * It's more work for the (potentially large) proportion of people who don't spend their lives idling on IRC anyway. * Lovely people who answer questions for newbies should get more interesting questions to deal with than "I know there has to be a way to append the contents of an iterator onto a Vec, what is the method called?" I would prefer the problem were solved.
I want to write an application that can take input from a scanner and produce a pdf. Preferably any scanner..
you're looking for /r/playrust 
*cracks knuckles* *opens text editor* *starts writing proposal*
&gt; As a German, am I entitled to feel offended? Certainly! But if it's any consolation, my grandparents are German. :P Mostly by that part I meant that our attendees are unlikely to be confused, relative to if the same program were at, say, CMU or Case Western.
Awesome! Thanks guys. Is there one for Mac OS as well Or does that fall under Unix/linux. I'll look at these when I get home from work. I am on mobile now.
It does not, this is a community conference, not an official one. Not that that distinction is _super_ important, I know I'll be going to both. :) last year's Rustcamp team hasn't announced plans about our conference yet, as we're not as far along.
Thanks!
&gt;UnexpectedEOF &gt;UnexpectedEof why?
For OS X, you'll have to write a shim in Objective C wrapping [ImageKit](https://developer.apple.com/library/mac/documentation/GraphicsImaging/Reference/ImageKitReferenceCollection/).
google gave me [this](https://developer.apple.com/library/mac/documentation/Carbon/Conceptual/ImageCaptureServicesProgrammingGuide/01Introduction/01Introduction.html)
No it isn't, because the function passed to `filter` must take a reference. It could be written `.filter(|&amp;n|is_prime(n))` though.
&gt; lifetime elision means that `&amp;'a Trait` is equivalent to `&amp;'a Trait + 'a` For reference, this is documented only in [RFC 599](https://github.com/rust-lang/rfcs/blob/master/text/0599-default-object-bound.md).
I take it you're already in touch with http://www.meetup.com/Austin-Rust-Meetup/
If my grandfather was any indication, German and offended are basically the same thing. ;)
It seems strange to me to treat even numbers as special (especially as it's so easy to get it wrong for 2), rather than doing a full sieve of Eratosthenes which isn't much harder. If you really want to golf it, then you can shorten it to fn is_prime(x: u32) -&gt; bool { (2..x).all(|f| x%f != 0) }
`&amp;mut` references cannot be copied, the entire point is that only one can be active at a time. What happens when you pass one to a function is that it transparently gets reborrowed. You can make your code compile by making it `for s in &amp;mut *data`.
Must have forgotten that /s tag. Despite us Germans having no Humor (or so they say), I'm rather amused.
More of a personal project and learning experiment. I know I could just scan something but I'm trying to learn more about it
We should certainly update the website in this regard...
Oh I just recently aquired a New 3DS XL, and somehow forgot about FuryHunter's work.... hmmm.
The 3DS is pretty cool these days. There's an exploit that lets you run custom code right at boot, and other such shenanigans. And it also has fun games too.
Would the trait access enable what you want to do? the traits impls could be generated by a macro (if it's more convenient than two enums)
The features that are rolled up into inheritance in object oriented languages are provided through multiple mechanisms in Rust. If you want to define an interface shared by multiple types, use a trait. If you want to dynamically dispatch functions, use trait objects. If you want to reuse the structure of data, use composition (make the 'parent' a member field of the 'child').
But the website is updated already. &gt;MSVC builds of Rust additionally require the Microsoft Visual C++ build tools for Visual Studio 2013 or later. The easiest way to acquire the build tools is by installing Microsoft Visual C++ Build Tools 2015 which provides just the Visual C++ build tools. Alternately, you can install Visual Studio 2015 or Visual Studio 2013 and during install select the "C++ tools".
A handful of points and thoughts, mostly from Rust convention: - `get_` should typically be omitted from getters; this would leave `PTClient` having methods like `osint(query)`, `subdomains(query)`, `account()`, *&amp;c.* (Rust convention is `foo() -&gt; Foo` and `set_foo(Foo)`.) - It’d be really nice if you fixed the field names so they were in snake case rather than camel case. This is a problematic with using rustc_serialize, but it needs to be done for optimality. (Can Serde do such field mapping?) - Rust values brief (though still complete) names. Tacking “Response” on the end of all the response types is ugly, I’d rather have `Whois` than `WhoisResponse`. - `Result&lt;T, ResponseError&gt;` could be nice with a `type Result&lt;T&gt; = std::result::Result&lt;T, ResponseError&gt;`. Then the docs just show `Result&lt;T&gt;`, effectively hiding the consistent `ResponseError`. (Whether you call it just `Result` or not is an open question. `std::{fmt, io}` are two examples of things using typedefs named `Result`.) - Response structs and return types could do with being neatened, though again there’s the whole decoding convenience thing. Selecting `SSLCertHistoryResponse` as an example, it’d be nice to have `Client.ssl_certificate_history(query) -&gt; Result&lt;Vec&lt;SslCertificateHistory&gt;&gt;`. (Oh yeah, Rust goes for `Ssl` rather than `SSL`, siding with .NET conventions over Python conventions for once in the matter.) And wrapping every field in `Option` is sad for cases where a value will never be `null`. - Still too much string typing: expiration date, for example, is semantically a date, not a string, and an IP address is an IP address, not a string. You know the drill. More stuff about automatic encoders, *&amp;c. ad infinitum*.
I'm working on a Rom Hack Library for The Legend of Zelda: The Wind Waker (the GameCube one) and I simply used libcore, liballoc and libcollections to reexport an API that resembles the Standard Library like so: https://github.com/CryZe/libtww/blob/dd5ed13e09a3ddc81f56f0b07a223f25b0400c0e/src/lib.rs#L17..L51 And then I have lots of other stuff in there that's specific to Wind Waker. This works fairly well. Also something rather hacked together you could do is simply implementing all the symbols the ARM Linux Standard Library requires and use that. That's how I got the PowerPC Linux Standard Library running for Wii Homebrew Applications written in Rust.
Probably not. You should look at learning something like Python, C#, Java, etc first (maybe even C) and then come back to Rust once you're more confident in it.
I think it could be, but the learning materials that would make it so don't currently exist. So for a certain kind of person, it might be okay, but until there are specific resources for this, I doubt it will be generally. Basically, it depends on how willing you are to tough it out. And there are other languages where you don't have to do that.
In a sense, those *are* programming languages, they're just for programming the layout engine of some document rendering system. They're just not general purpose languages. Anyway, Rust is probably not a great first language. It's a bit tricky. Python, Java, and C are great suggestions, and I'd actually suggest learning a bit of each. Throw some Scheme and Haskell in the mix when you get comfortable, *then* you can decide what languages you want to learn for hobby work. If that's too much for you, start with Python or Java, since they have mature ecosystems (nice tools and stable libraries,) and fairly simple mental models. Rust isn't quite ready to fit that kind of description.
You'll be more productive at the start in a garbage collected language such as Python, Java or JavaScript. If you want raw speed and a sense for how your computer really works you should learn C. Rust is great for programs that need to be reliable and fast, but you do need to know a bit more to get started than some other languages. Also it does some things in "new" ways, so you will not learn things in a way that will help you quickly pick up other programming languages. 
&gt; In a sense, those are programming languages If you squint *really* hard and turn your head sideways, plain text is really just an incredibly simple programming language for writing text quines. :D
Definitely! That is what it is, a workaround.
Yes, though that's an incredibly specific domain.
This is a good one.
I knew it, those MIT license fans. /s
About time - and on April, 1st, too... ;-)
Why the fuck would they do this right at the end of the best month of the year!?
We simply has to do this, for the sake of Redox OS™.
Do you have a contract with NSA already? This will allow for better inter-continental-overview of user behaviors to improve the usability and stability of the Redox OS TM(R)(C)(OMG) System.
I would like to point out that April fools are a meme, using the original, broad definition of a [meme](https://en.wikipedia.org/wiki/Meme), and that memes are prohibited on this subreddit, according to the sidebar. Oh well, the sidebar also tells me to chill out.
I agree with what others said about Rust. A couple of people mentioned C as a first language, but I wouldn't recommend it. When you make mistakes in C, the error messages are likely not going to be as good, if you get any at all (with segfaults, uninitialized variables, buffer overruns, dangling pointers, etc.). I think it would be easier to deal with once you know how to program in another language. C may be fine with the right material, but I think Python, Ruby, Java, or C# would be better. I do think every programmer should learn C, just not as a first language.
Me too. I realised it's bullshit when reading first paragraph. (counting from zero of course)
Don't start with JavaScript. Just trying to figure out how closures capture variables, hoisting, etc. will make you tear your hair out. Then you'll write something like return { a: ... b: ... }; and you'll wonder why your code does not work at all. JavaScript is fine if you're a professional developer and you have to use it, then you can waste your time learning quirks and putting your code through jshint. Ruby might be better.
&gt; The source code of Redox OS(TM)(R)(C) is encrypted using the latest technologies (ROT26). Isn't this a little overkill? ROT13 is already pretty safe in it's nature. Also, I am afraid of what performance impacts it may have? 
By the way, someone stole your code and hosted it for free here: http://rot26.org/
It will be hard and slow to learn as first language but not impossibly hard. If you want to learn Rust and can accept that your productivity will initially be lower compared to say Python in exchange for learning low level concepts and good performance then go for it. In my opinion Rust is well suited to learning systems programming because the compiler is basically a strict teacher looking over your shoulder and preventing you from learning it the wrong way.
I don't agree that you are more productive in dynamically typed languages. Simply because finding bugs takes much more time than writing the code. Statically typed language can prevent you from making some very common mistakes, so you save time on debugging.
I'll second that, however I think C++ is much harder than Rust.
Every EULA should be this honest: &gt; violation [...] is defined, as anything that we believe is a violation. It doesn't have to be a part of any of the rules described above.
Seems to serve the book and newspaper industry pretty well for centuries!
I'm waiting for the IBM lawyers to contact them. http://dev.hasenj.org/post/3272592502/ibm-and-its-minions
Those damn software terrorist commies and their fancy technology..
I was kind of excited when I saw that... :)
I'd say that Python as used has a lot more weird "you have to know that" stuff in which the language doesn't assist you. Lua as well. They are simple in some regards, but that means a lot of the complex things are in the realm of tribal knowledge, which makes the languages hard in a different fashion. Any modern programming language in wide use is very hard to learn. The first step might differ in height, but it all becomes _very_ hard quick.
I found a cryptanalysis of ROT26 here: http://cs50.stackexchange.com/a/1295 Wouldn't it be better in the long run to switch to Enigma (the WW2 cipher), textbook RSA and the LoseLose hash function?
I forgot it was april fools day and got REALLY pissed for a minute
I believe the Rust language by itself is not bad, rather it's mostly the borrow checker. If a new guy wraps their head around the borrow checker, it'd probably be a great bonus when working with other languages with managed memory.
UnexpectedEOF is deprecated as of Rust 1.6.0.
So who do I call for support ;)? 
I feel like it would make sense for Rust to extend the use syntax for struct fields like JAI does: struct MyStruct { base: BaseClass, use base.{a, b}, pub use base.c as cool_field, pub use base.d as (tuple_0, tuple_1), }
Yes ROT13 is save for now, but mind the future! ROT26 is simply twice as save!
Just the kernel?
I had a rant prepared, where do I put it now!
 1) Use indices / IDs that never change, e g if you have all people stored in a ID -&gt; Person hashmap, then every post could reference the ID rather than the Person. The upside is that you don't need to clone things, the downside of this is that you have to go look up the Person in the hashmap every time you need to know the name. 2) As a variant of `Rc&lt;RefCell&lt;Person&gt;&gt;`, you can have `Rc&lt;Person&gt;` and have `struct Person { name: RefCell&lt;String&gt; }`. It will be somewhat less polluting (IMO) than `Rc&lt;RefCell&lt;Person&gt;&gt;`, as you can create a `Person.set_name` function and borrow the name inside.
There have been questions like this in the past and several people replied they had learned Rust as their first language without much problem. The most important part is motivation, if you are motivated enough you can learn anything.
I don't really like `Rc&lt;RefCell&lt;...&gt;&gt;` either. In general I always find a way to express relationships without using them. In your scenario, if we say Blog -&gt; Persons -&gt; Posts, then we can write it all with (-&gt; means "owns") http://is.gd/lz1jY2 Modifying person names just implies mutably borrowing the person from the blog. I am not sure it'll be that simple if you have multiple persons in a single post.
Now *where* do I get the license? I get that it's a joke, but I wanted to try this out, and can't
It's an old meme, but it checks out.
The EULA reads strangely like Trump :P
&gt; **Drawbacks** &gt; None, although the proposal might seem less convincing on other days of the year.
™®© was a nice touch.
No but actually I want this.
 ::std::mem::forget(rant); There, I fixed it.
I'm not a big fan of dynamically-typed languages, but a garbage-collector is still pretty useful when you start learning. Although I started with C and C++, so I guess that it's still possible to learn Rust if you put in the effort.
Or he can just use ES6, which is mich better. Automatic semicolon insertion still sucks, though. :-°
This is something I've wondered about. Is `std::fs::File` buffered? I'd dig through libstd and find out, but I won't be around a computer until later today. I've always just assumed that it is buffered, like C's `FILE*`.
It totally crushed my dreams until I figured it out...
&gt; Also, I am afraid of what performance impacts it may have? LLVM's pretty good at optimising this stuff out, so I wouldn't worry.
The JSON license[0] has this clausule: &gt; The Software shall be used for Good, not Evil. [0] http://www.json.org/license.html
Must've been bought by Oracle..
Assuming I understand correctly the relevant RFCs and bug reports, #3 will be fixed as soon as MIR becomes stable and the relevant dataflow analysis pass is added.
Depends. If you're working with a good IDE (XCode, VS, etc), the debugging tools are really mature and let you know what you did wrong, although I would still say that C is only a good first language if you have close help from a veteran to guide you through.
This largely boils down to non-lexical lifetimes. :/
&gt; I would argue that this is a good thing: overloading [] to produce new values is very surprising. But technically even a reference is a new value, and on the other hand this greatly limits the usability of Index as you can't e.g. really slice multi-dimensional structures without creating new structures. So there really is something to be said for the ability to return values from index.
Yes, I get that the implementation doesn't allow for a one-to-one translation. However, I would like to write functions that are general over both `&amp;[T]` and `Window&lt;T&gt;`, but I'm not sure how to go about doing it. I understand that `Index` has a certain semantic baggage, but is there another trait (perhaps `get(idx: usize)`?) that could be implemented for both? I'm already using Iterators, but I'm not so wild about that as it means that I have to iterate through values `0..n-1` just to get to value `n`.
your "TODO: ALL OF THIS OMG" s can be immortalised forever ;p
There was [a recent thread](https://www.reddit.com/r/rust/comments/4as7gx/why_make_the_index_trait_so_useless/) on the same theme. The TL;DR version is, if we want the `Index` trait to work as expected for slices, Vecs etc, where the returned reference to have the same lifetime as `self`, then we must define `Index::index` in a way that it always returns a reference with that lifetime.
Thanks---this is helpful to see what the current discussion is. I'll read through that and perhaps chime in there.
seriously though I want to be able to use emojis in idents. what sort of medieval language is this?
 if { self.find_thing_mut(name) .map(Thing::do_something) .is_none() } { self.init_thing(name.to_owned()); self.do_something(name) }
FUCK Damn you, Shenanigans Day! \**waves fist*\*
One that follows standards! Get emoji into this table and it'll work (with the non-ascii-identifier feature flag): http://unicode.org/reports/tr31/
Have you looked at the generated assembly to verify that the additional copies you blame on the `Entry` API aren't optimized away by LLVM?
You can use the `crossbeam` crate to allow a thread to access a reference to the outer stack, and use the slice's `split_at_mut(n/2)` to create 2 references that each have half of the array! Efficient, and no unsafe code ;) Here's an example https://gist.github.com/Connorcpu/3dc6233bd59522f0b6d650e90d781c63
I actually experienced the opposite effect when writing C. I was just looking at a piece of code, thinking about lifetimes and spotted a use after free in error path. 
Thank you very much, this is exactly what I was looking for. From the issue #24292 I gather that they will probably add some functionality like that back to the standard library some time in the future? I'd normally like to avoid third party code but given Cargo, I can't complain.
If I remember correctly, the only thing that will make libstd's JoinHandle return an Err is if the thread panics, and in the case of crossbeam, it would be unsafe to allow that panic to be caught, because it could have violated invariants of data on your own thread.
I write Python professionally. It's every day that I run into type safety problems.
I'd like to point out that "meme" is also a meme. So it is impossible to discuss spreading memes without also spreading the meme "meme" itself.
Yeah, it's a long standing problem, but it's currently being fixed by [RFC 1560](https://github.com/rust-lang/rfcs/pull/1560) and [PR 32213](https://github.com/rust-lang/rust/pull/32213). Expect this to work on nightly 1.10-1.12 or so.
Oh I didn't know. Thanks! 
Yeah, to be fair that data race was an error that resulted from the interaction between objects of 5 classes, so it's very possible to lose track. Use after frees tend to be more local.
I'd argue against that. C++ was the first language I learned, and I thought it was fine for that purpose. It was in a class setting, though, so that likely helped.
You are not alone. I actually have been thinking about that a lot and tried migrating to a trait-based approach more than once (it should be buried somewhere deep in the revision history). But as expected I always ran into some issue with an explosion of complex generics due to a lack of type erasure. Absolutely willing to try again, once we have abstract return types.
An ecosystem around it building on top of all the great multimedia libs we already have in Rust would be awesome. But I can't pull that off on my own. Any interested contributors?
Thank you for all the constructive feedback! As a takeaway I will do my best not to understate Rust's performance in the future. ;)
Me, too. I recently introduced a Result class into our code base and my team likes it :D
Good.. Good.. One step at a time...
I heard clang has better error messages
I'm the OP (not the submitter) - no I haven't. Is there an easy way to do this? (e.g. by using Cargo instead of manually running the compiler)
I was not expecting so many in-depth answers. So, first, thank you all for taking time to answer. You've given me a lot to think about. I'm gonna do some reading about C and Python (and more about Rust) and figure out where I want to start first.
Learning to do things in new ways doesn't scare me. If I can handle the difference between ser and estar in spanish and noun declensions in russian, I should be able to handle new concepts in a programming language.
REPL emoji tab completion in Julia was also an April 1st PR - https://github.com/JuliaLang/julia/pull/10709 (though that actually got merged and released)
Yeah. I use C# professionally too, and I used to love it: it is not a bad language at all. But once I used Rust, I started to REALLY miss Option&lt;T&gt; types. Now, I can kind of get around the problem by annotating which functions can return null and which ones can't (because, without option types, null is still the best way to indicate that *nothing* was returned when this differs semantically from *empty*) via Attributes and let the compiler tell me when I've perhaps messed up. It's still not the same though :(
What 'race type' conditions exist in java that don't exist in rust? I though Java provided a fair amount of safety in that area.
After much thought and discussion, we have decided to return to a free software model. For those who could not guess the license key, it is: APRIL FOOLS
I sometimes wonder if the whole dynamic programming language thing was a mistake. It does not have to be Rust. Even something like Go with its simple type system, feels like a massive improvement over Python. Don't get me wrong. I love Python. But having to check that the whole program works, after a small change vs Having the type system to cover your back so that you have some amount of confidence that the program will work, if it compiles, is a massive win. People say having to compile after a change reduces productivity. But isn't that argument missing the fact that with dynamic languages, one have to run the program and manually check the whole program works, after each change (unless you have written automated tests, of course). I think that is a whole lot difficult thing than starting a compile step and see if it compiles. So, while implementing a feature, with a language with a powerful type system, I usually does not have to run the program until I implement most of that feature. But with something like Python, I will have to run the whole program at each step, to see that it works until that point. I seriously cannot understand why someone will go with the latter. I understand that you can write automated tests. But I am not into TDD and stuff like that. I don't really like writing tests.... I tell this from experience implementing the the same program in Python, Haskell and Rust. May be I am just lazy, and like to just throw a compile command and wait for errors. But aren't we all?
I wouldn't go so far as to call the entirety of dynamic programming a mistake. For small/trivial programs (the original intended domain for PHP and Javascript), or quickly-changing experimental programs, or throwaway programs like shell scripts (which was the original intended domain for Perl, Python, and Ruby) it may be entirely acceptable to produce a program that's not maximally maintainable or robust against future changes. It's once you start scaling up that strong type systems start really pulling their weight.
&gt;For small/trivial programs (the original intended domain for PHP and Javascript), or quickly-changing experimental programs, or throwaway programs like shell scripts... Agree. But somewhere along the way, people started using them for bigger stuff, and when its problems became apparent, they went to the way of automated tests, TDD and stuff like that, instead of going back to statically typed languages and languages with powerful type systems...
Clang used to have better error messages, but GCC may have caught up now. But I was talking more about runtime errors. To elaborate: - In C, you can use an uninitialized variable, but the program may intermittently act unusual because the variable's value is undefined. In C#, you'd get a compile error about the variable not being initialized. - In C, you can access outside the bounds of an array, and if it is stack allocated, you could change other data on the stack. The result would be a program that acts weird never/sometimes/always. If the array is heap allocated, you'd more likely get a segmentation fault. Neither clearly tells a beginner that the index was out of bounds. In a higher-level language, you'd get an index-out-of-bounds exception. - In C, when you get a segfault, it doesn't print a stack trace unless you are using an IDE or debugger. Even when using a debugger, I have found C and C++ debuggers (at least gdb and lldb) to work a lot less of the time than Java or C# debuggers (like they won't give stack traces or won't step correctly). Java and C# debuggers have worked pretty much flawlessly for me. Then again, I haven't used gdb or lldb in the last two years. - In C, if you get a segfault, it may be because of a null pointer, a dangling pointer, an index out of bounds, or maybe another reason, and nothing tells you. You don't have to worry about dangling pointers in higher-level languages. - When I started with C, I found using libraries (with include paths, library paths, static and dynamic linking, 32 vs 64 bit DLLs, etc.) much more difficult than with Java.
Awesome, thank you. TIL you can use GAE custom runtimes to run Rust code on GAE.
Hmm. I imagine you could use `cargo rustc -- -Z emit-llvm` and somehow search through the result, but that's probably more tedious than using it with `--verbose` and copying + editing the `rustc` command it prints for the file you want to check.
I'm always a bit confused by this. Program Ruby and Python and rarely run into type level problems. At most sites, the type of the thing under inspection is clear. Most of the bugs I write are logic bugs. And there you are at it again: you either run the full program or the test suite.
&gt; Use after frees tend to be more local. Heh, in simple cases:)
I'm working on code that runs in the browser, but some parts, mainly a sort of preprocessing/compilation step, run in node. That's because there is significant code overlap between the various components. Performance in node has never been important, but starts getting in the way. This might be an opportunity to slowly convert some code to Rust without having to maintain two separate code bases that should function identically. Nice. There are really many interesting projects in Rust.
could this also be used for emscripten?
This is a project that me and a few others have been working on for a little while (most work was done in the past few weeks). Its nowhere near complete and still pretty buggy plus I'm still pretty new to rust so feedback is welcome.
&gt; I'm always a bit confused by this. Program Ruby and Python and rarely run into type level problems. I don't think this is true at all. Well, it's probably true if by 'type level problems' you mean situations that result in a `TypeError` exception, but not if you mean problems that would have been prevented by a statically typed language. At a previous job, I analysed all reported bugs in our Python code base over a few months, checking whether they would have happened in a language with Haskell-style type system (I did not know Rust at the time). About 60% would have been prevented by just using a language with a good static type system, without any attempts to really make the type system work for you. Assuming more deliberate use of the type system (e.g. aggressively using newtypes to distinguish between things that are representationally identical but semantically different), that number goes to over 80%. Now, I'm not claiming that this applies to all Python (or dynamically typed) programs, but I do think that people tend to massively underestimate the benefits a good static type system can have on correctness. EDIT: I just noticed that I probably misunderstood you. I thought you were saying that Python and Ruby programmers generally don't run into typing problems, but now I think you were just talking about your own experience. I obviously didn't mean to doubt the latter, I just disagree that this is true in general.
If the structs are defined in the Rust source code, you should serialize it with [Serde](https://github.com/serde-rs/serde). If you're okay with defining the data types in a separate file, consider [capnproto-rust](https://github.com/dwrensha/capnproto-rust). [Cap'n'proto](https://capnproto.org/) has clients for other languages too (that will be able to use the same `.capnp` file). Also it doesn't do only serialization but also RPC (see [this article](http://hoverbear.org/2015/03/09/learning-capn-proto-rpc/), [this](https://dwrensha.github.io/capnproto-rust/2016/01/11/async-rpc.html) and [this](https://capnproto.org/rpc.html)). I think Cap'n'proto meets your needs quite nicely. It doesn't need to encode/decode the messages because its on-wire representation is the same as memory (the only thing it needs to do is to when receiving is to validate the data). Also, it lets you to evolve the protocol over the time, by appending new fields to the message but maintaining backwards compatibility with old clients. I think it's awesome.
&gt; messages are defined as one or more C-like structs Do you have to do things in this way? It might be worth looking into something like [msgpack](https://github.com/3Hren/msgpack-rust) or [Cap'n Proto](https://github.com/dwrensha/capnproto-rust).
To continue on the Serde track, there are some binary serialization crates out there, that can be useful if you want a relatively compact format. [bincode](https://crates.io/crates/bincode) and [cbor](https://crates.io/crates/cbor) (although not updated in a while) are some examples. Edit: ...but that doesn't help when the protocol is specified. Could still serve as inspiration.
With clouds its comparable (in some cases better) fps-wise, without clouds it tends to be much better due the clouds being more complex in steven than vanilla. Examples [A](http://gfycat.com/HotGraciousAnglerfish) and [B (they wont clip through buildings with roofs)](http://i.imgur.com/z0OYCjM.png) Chunks always load in faster on steven though.
It seems there is nothing on crates.io, so I think you will need to do some FFI.
The problem is not much the `&amp;[u8] -&gt; RawByteMessage` translation, but more the 3 steps `&amp;[u8] -&gt; RawBytesStruct -&gt; LogicalStruct` to have the correctly typed struct. With many copies and verbosity, as `RawBytesStruct` can't use enum, will have null pointers, or will use `[u8; 4]` in place of `Ipv4Addr`. 
It's should be something like this, tho I'm not sure how to properly handle dynamicly sized structs. use libc; [repr(C)] struct DhcpMessage { op: libc::u_int8_t, htype: libc::u_int8_t, hlen: libc::u_int8_t ... } ... let msg: &amp;[u8] = ....; let dhcp_msg = msg.as_mut_ptr() as *mut DhcpMessage; 
Thanks, that looks good, wish it had a native unix client tho.
Really awesome work guys!
It runs on Linux via Mono just as natively as it does on Windows. It's MacOS that only just got proof-of-concept support and is in need of developers. There are unstable/experimental builds [available for download](https://truecraft.io/download).
I'm fine with it remaining only a client... I'm just saying that it'd be nice if Steven could connect to and work with a TrueCraft server (or a Minecraft 1.7 beta server).
Fantastic answer. I think you've nailed my issue and solved it very elegantly. It indeed enable zero-copy network handling and lazy evaluation of the logical type's fields. Simply great. Thanks a lot !
I do fulltime ruby and I definitely think type level problems are a big source of our bugs. I miss rust's enums and non-null types all the time.
Thanks I already came around and decided to try it, the 275MB download of mono was a biatch on my 3G tether. It looks like I'm missing some librarys so I wont get it working on 3G I don't know how popular it will get if performance isn't improved I already bought Mincraft (in alpha) and hae arbitary access to 1.5.x + versions.
&gt; I was also missing Rust's slices in C++ Fun fact: The equivalent of slices ([`std::basic_string_view`](http://en.cppreference.com/w/cpp/string/basic_string_view)) has been accepted for inclusion in C++17. 
Neat project! Is there a reason you're doing the openssl stuff yourself instead of using https://crates.io/crates/openssl-sys ? I am a little concerned with this pattern of "swap in uninitialized memory, do some processing, swap it back in, but it's a bit too early in the morning for me to make any more concrete statement than "hmmmmmmm"....
Sorry for stealing your karma. I stumbled onto your post while looking at your ghc work, and I found it very interesting. When I saw you hadn't submitted it already, I assumed you weren't a reddittor.
When I started the project the crate didn't have access to the parts of openssl I needed. By the looks of things thats changed so I might be able to drop that part now. (Edit: Doesn't look like its all there still) As for the "swap in uninitialized memory, do some processing, swap it back in" thats me not understanding on how to make the borrow checker happy and taking the lazy option. I'd love opinions on how to fix it.
It almost goes without saying, but if you get the chance I'd love to hear what you think about Rust and especially using Rust for games.
*your
Its pretty nice to use and I enjoy working with it. Pure rust libraries for doing game-dev things don't seem to be completely there yet (I used to use glutin but various things didn't work on some platforms and I wanted to release instead of tracking them all down, I do want to switch back though) but it is getting there. My only issues I really have is: * The borrow checker (mainly borrowing parts of self and then being able to use call functions on self until that borrow ends in some cases) this may be down to me misunderstanding something though, for the most part its fine though. * Compile times - This may be down to the [huge macro](https://github.com/Thinkofname/steven-rust/blob/master/src/world/block/mod.rs) I use for blocks but compile times have got really slow (2-5 minutes) which makes making small changes and checking the result really hard. * Debug build performance, somewhat unusable for games as you can generally compile a release build and join a server quicker than it takes for the debug build.
I wrote an [RFC](https://github.com/rust-lang/rfcs/pull/396).
Nonlexical SESE regions would be possible. The main pattern they would enable is code like this: fn f() { let mut x = 0; let y1 = &amp;mut x; use(y1); let y2 = &amp;mut x; use(y2); } However, once you have the machinery to consider regions that are sets of CFG nodes rather than scopes of expressions it isn't that much more difficult to have multiple exit points.
How so?
Well Sun and now Oracle certainly did litigate over Java a few times.
Would you mind explaining why c++ is the worst language to start with?
Looks to be the same &gt; openssl dgst -sha256 /usr/local/bin/cargo SHA256(/usr/local/bin/cargo)= d50f560eac885c19ea523a09d41d30b4fea035bb6a64eefca8f519e87dec827d 
This is something I've been struggling with too: In my project I need to support multiple use cases, but the majority of every use case is exactly the same. In C++ I'd make a base class with all the shared logic / data in it, maybe some virtual functions such that the shared logic can be changed. This shared logic may also register event handlers with some event manager, by making the event handler a virtual function, the specialized use case can change how that event is handled. Trait objects don't work here since there is no 'interface', the specialized use case is tightly coupled with the shared base. Composition doesn't work since then I cannot override event handlers already registered by the shared code. Neither does it allow the specialized use case to change the shared base behaviour by overriding a virtual function. This may be considered abuse of the object oriented model, but it works out really well in my C++ project :)
To be honest, you can run into similar problems in any language with reflection. Whenever I use dependency injection frameworks in Java, the only way to know if any change will work is to try to run the server.
Java prevents data races. But it also doesn't have any notion of `Sync` types and allows you to use every type as though it were `Sync`. e.g. you can modify an `ArrayList` from one thread while iterating over it from another.
That's a race condition though.
If /u/steveklabnik1 wants to use it for the rust book, he is hereby cordially invited.
You could have a base struct carrying the data (which is included in all 'child' types), and a trait using that data internally that may be implemented (and just gets one method to get the relevant base data plus a lot of methods on that data).
OK, this is a really weird issue: I diffed your strace log against mine (https://gist.github.com/anonymous/b85f36487ac305fa6bc56b19155a4c99) and the only major difference I found was the existence of those `brk()` syscalls. All other syscalls (minus e.g. memory offsets) are almost identical both in execution order and in parameters.
I'd just like to point out that there are plenty of bugs that a compiler *can't* check, so I still find it invaluable to write tests in Rust :)
How is this different from using Google container engine? 
&gt; Rust usually chooses explicitness. I've come to like explicitness a lot and some times even wish Rust were more explicit (like `fn foo() {}`returning `()` or the implicit `Sized` constraint on generics).
Is there a good reason why you don't use glium?
It's listed under the App Engine tab in Google's Cloud Console rather than Container Engine. I'm joking, but it's not too far off, as far as I can tell. This is a deployment to the Managed VM/Custom Runtime of App Engine, with the major difference as I see it being that this is not a cluster managed by Kubernetes. So the first issue is whether that matters to your application or not. App Engine can handle auto-scaling and throwing up more instances under load and de-scaling when load falls; I haven't used Container Engine, so I don't know how that compares. I imagine Container Engine is better when you're orchestrating multiple interacting container instances, while App Engine will be better for smaller applications or large applications where instance replication can handle requests. When you don't need Container Engine, the Custom Runtime should be simpler, and is definitely cheaper, since Container Engine charges at the same rate as Custom Runtimes (compute engine rates during Beta) plus additional costs for clusters over 5 nodes. So basically, for simple Request -&gt; Response applications that can run off of Google Datastore or Cloud SQL, the App Engine version should be simpler and cheaper, I think.
&gt; the things that a language considers sufficiently important to make a strong opinionated statement regarding them I think this is a false presupposition -- considering something important doesn't necessarily mean being opinionated about it. From my perspective, Rust considers concurrency so important that it refuses to box you in to a single concurrency approach. Instead, it leverages ownership and borrowing to guarantee *data race freedom regardless of concurrency paradigm*, a guarantee that holds up even as you freely mix approaches. This move is not unique to concurrency. As Rust clarified its core mechanisms, it moved as much as possible into the library space, *precisely to give users more expressiveness and control over important concerns*. As I've [argued elsewhere](http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html), you can in particular see data-race freedom as a rehabilitation of shared-state concurrency. Put differently, if you see straightline code like: ``` *x = true; if *x { ... } ``` where `x: &amp;mut bool` you can locally reason that the guard will be satisfied. Due to ownership, no other thread could have concurrent access -- and the same kind of reasoning is what prevents data races. **TL;DR: Rust's core value here is "concurrency without accidentally-shared state" aka "concurrency without data races".** (FWIW, I'm also not clear what you have in mind when it comes to "mutual incompatibilities"; as with anything in the library space, we try to standardize over time (through `std` or `rust-lang` crates) when it makes sense to have shared vocabulary.)
Interesting! What did you think they were? It's kind of hard for me to put myself in the shoes of a beginner.
Awesome! Do you have code to open, modify and save Minecraft worlds? The reason I am asking is my toy project: https://github.com/voronaam/javacraft It extracts bunch of static code analysis metrics from java bytecode and builds a map out of it. The idea is to engage java developers and expose them to the effects of code generation. Then they'd be able to smash some of the worst classes in a fun way. The map generation part is what holds me down now (the current code only outputs SQL that I manually execute against a sqlite-based map, which is ugly).
Just try to connect to my Minecraft server, I got a ```Disconnected protocol error: Wrong packet: SetInitialCompression(SetInitialCompression {threshold:256})```
Great post! But there's one tiny detail I want to nitpick about: &gt; When Rust says it values “memory safety”, it is saying that it will not crash with a segmentation fault. That's not exactly true. What it is saying is that either "it will try its best to not compile unsafe code" or "it _will_ crash your program if it does an unsafe operation at runtime" (e.g. panic on out-of-bounds, or segfault on stack overflow). Not crashing is the truly terrifying behavior.
Probably instead of thinking of them as types, you could imagine them to represent different ways of handling types. For instance, `&amp;T` may not mean "the type of a reference to a T", but "operate on this T using a reference." That is, one could imagine them being 'above' the type system in much the way that a program's logic is.
There's an answer for this on Stack Overflow: ["How do I pass disjoint slices from a vector to different threads?"](http://stackoverflow.com/a/33818407/42353).
Ah *blushes* I think I should read this subreddit more often then. *subscribed* :)
Nope this gets the world from the server which is in a different format over the network than it is saved on disk. Sorry
perhaps you mean`type F&lt;T&gt; = fn(T);`? The `T` needs to be part of the signature somewhere since you're parametrizing on it. If you want a type to be a pointer to a function with any type parameter you want something like `type F = ...`, not `type F&lt;T&gt; = ...`. The former means "give me a concrete type", the latter means "give me a type constructor, which when fed a `T` produces a concrete type." To have a generic `type F = ...` function pointer, you probably want something like `type F = for&lt;T&gt; fn()`. However, Rust doesn't support this, `for&lt;..&gt;` can only be used for lifetimes. This makes sense, because you're essentially asking for a pointer to something whose type is determined at runtime. Rust supports runtime types in the form of trait objects, but this is well out of scope of that and probably isn't something I'd expect Rust, or any statically compiled language with generics, to ever support. What are you trying to do? There's probably an easier way to do it.
Note that the signal on stack overflows recently was changed NOT to be a segmentation fault because it brought confusion :)
I don't think his point was to say that that's a useful way of thinking about the type system. Rather, it's a way that people might initially approach the type system without realizing that it's possibly hindering their understanding rather than helping it.
/r/playrust
That's how it works for me with C++ too. I first learned it from Stroustrup's The C++ Programming Language, where a lot of the features were accompanied by commentary on the design decisions.
Sure, here it is: [#32703](https://github.com/rust-lang/rust/issues/32703). Wouldn't the `default` be on the other `fn` on line 22? I can't remember what the keyword does specifically.
I'm finding the name of this project (and the original Go one) to be oddly disconcerting for some reason. What was the reason behind it?
Thanks for clearing up my misunderstanding.
Its based on Minecraft's player's name 'Steve' and also originally a joke about Go's unsearchable name so I decided to make the project's name equally unsearchable.
The point, as I understood it, isn't that it's helpful. The point is that it isn't, and some people have this (or a similiar) perspective. It's easy for an understanding roughly along the lines of what /u/RetraRoyale describes to appear when someone's learning C/C++. I certainly remember myself having it. I'd wager people without prior exposure to typed languages would be more prone to this.
I'm sorry for the question but, what is an ICE?
Internal compiler error
Exactly. I remember (at some point) knowing "references are types", writing some code, and then having the pieces click: "Oh, yeah. References ARE types. That means something." It's like how I got (marginally) better at billiards once I realized the balls are round. That is, that their roundness has consequences, rather than just being a vacuous fact about balls.
I think that I'm having trouble understanding the lifetime of a variable, but I'm not entirely sure how to solve it. I'm thinking it's the lack of an explicit lifetime on the variable "new_line", but the documentation only seems to cover lifetimes going into a function (for instance, assigning a lifetime so that parameters can be mutated and returned). I'm not sure how to accomplish this for an individual variable or if I'm even heading in the right direction on this tbh. For instance, I have: fn compare_files(f1: &amp;Path, f2: &amp;Path) -&gt; Result&lt;(), io::Error&gt; { let f = try!(File::open(f1)); let f = BufReader::new(f); let mut new_line: String; for line in f.lines().nth(0) { let line = match line { Ok(line) =&gt; line, Err(err) =&gt; panic!("failed to read line: {}", err), }; let split = line.split_whitespace(); let vec = split.collect::&lt;Vec&lt;&amp;str&gt;&gt;(); // new_line = some stuff in this for loop } //this next line and any lines below it (inlcluding another for loop) I get //use of possibly uninitialized variable: `new_line` println!("new_line = {:?}", new_line.trim_matches('\\')); } 
There are, see [Option::as_ref](http://doc.rust-lang.org/std/option/enum.Option.html#method.as_ref). foo.as_ref().unwrap()
That still returns an `Option`. The problem is not with `T`. It's with the function signature of `unwrap()` taking `self` instead of `&amp;self`.
As with `as_ref` you can also pattern match out references. match Some(1) { case Some(ref a) =&gt; println!("Ref captured"), case None =&gt; unreachable!() } EDIT: no case keyword needed, I have been doing a lot of scala code lately lol
I'm fine with the balance Rust strikes. Where things are implicit, the Rules about the implicit semantics usually fit in 1-3 sentences. And spooky action at a distance is mostly avoided. Yay for locality!
I know there are many meanings of "crash", but I would suggest that for Rust we don't use it for a safe, controlled abort such as a panic.
Good stuff! How does it compare to Hematite?
I tend to encode a *lot* of logic in the type system. :D
It's not generic enough. For example, you can't do Option&lt;String&gt;.as_ref().unwrap_or(&amp;str) That's why I needed to implement `unwrap_ref_or()`: https://gist.github.com/anonymous/5f9c186f8b2070e63710701e30036d78#file-t-rs-L18
To be more specific, an Internal Compiler Error is when `rustc` panics during compilation, as it normally shouldn't do that--ordinary compilation errors make it exit early as well, but via regular control flow. `rustc` is littered with detailed assertions to make it easier to find bugs or unexpected code paths. A stack overflow/infinite recursion bug, which is what's happening in OP's case, is also an ICE but it's not caught like panics are, unfortunately, so there's no easy way to tell exactly where it occurred without a good test case and a debug build of `rustc`.
So what's the takeaway here besides 'this is confusing'? When an lifetime error occurs when a lifetime was inferred, should the compiler describe the inferred lifetime, and suggest that a specific lifetime would help?
You could use fn foo(opt: &amp;Option&lt;String&gt;) -&gt; &amp;str { opt.as_ref().map(|s| &amp;s[..]).unwrap_or("") } or fn foo(opt: &amp;Option&lt;String&gt;) -&gt; &amp;str { opt.as_ref().map(|s| s.as_ref()).unwrap_or("") } 
Isn't that how the modern Minecraft clients do it as well,running a server in the background.
Somewhat* yes, but i'm only doing the client part so I never need to parse the world format. \* The server is special and shares a lot of memory with the client and packets are just passed instead of serialized.
Auto-deref is another place where Rust is actually less explicit than C++, but I like it that way. It's convenient to have type casting in situations where it won't hide a potential performance or correctness issue, and `Deref` is intended for practically free conversions between semantically similar types.
Tested again with fix, it works. Thanks!
Why not match it when you need it? Basically the same solution as you did, but not wrapped in an UnwrapRef?
I'm working on [Tetrahedrane](https://github.com/ca1ek/tetrahedrane), a 3d software renderer that works on Redox and all other platforms that support SDL2. [ Here is what I can do currently.](https://gfycat.com/BronzeThatInganue) It's horribly unoptimized. I need to make it use integer instead of floating point.
I actually like row polymorphism more than most FP features. I don't care much about laziness. Another language featuring row polymorphism is OCaml. But OCaml doesn't have something like Haskell's and Purescript's typeclasses (or Rust's traits) so it feels inadequate, and modular implicits weren't merged yet. Also OCaml doesn't have a Javascript-like syntax for (row-polymorphic) objects, that is, you write object method x = 1 method y = 2 end Instead of the more familiar { x: 1, y: 2 } Of Purescript (and Javascript). Purescript right now feature a very interesting set of features, and feels like a "Haskell done right" in many ways, without legacy cruft (except it's strict - Haskell wouldn't be Haskell if it weren't lazy). I agree with most differences of [this list](https://github.com/purescript/purescript/wiki/Differences-from-Haskell).
Consider using the [sodiumoxide](https://github.com/dnaq/sodiumoxide) binding to libsodium. There are some articles on the Internet on why something like libsodium (which is a better maintained fork of [NaCl](https://en.wikipedia.org/wiki/NaCl_(software\))) would be a good thing to use, for example [this](https://labs.opendns.com/2013/03/06/announcing-sodium-a-new-cryptographic-library/) and [this](https://blog.codecentric.de/en/2014/03/crypto-broken-apply-secure-crypto-developer/). edit: you should avoid using cryptographic primitives like SHA1 directly. If (for example) you want to store a password, libsodium [provides password hashing](https://download.libsodium.org/doc/password_hashing/index.html), as seen [here](https://dnaq.github.io/sodiumoxide/sodiumoxide/crypto/pwhash/index.html) on the sodiumoxide docs. In this case, it uses scrypt, salsa20 e sha256.
Structural typing (or row polymorphism I guess. Do they differ?) is a really cool feature, but is it really that important to you when it comes to scripting? Lua is fast and tiny. Is the polymorphism worth using a more complex language with a larger runtime?
Relevant thread: https://www.reddit.com/r/rust/comments/46s75m/rustcrypto_apparently_abandonned_alternatives/
That said, scoped threads were in the standard library in the past, right?
&gt; Much of my talk centers around the challenges of creating secure cryptographic implementations, most notably avoiding timing side channel attacks. To that end, I'm much more interested in libraries which wrap widely-used C and ASM code than ones written in pure Rust. Are there issues that make Rust less suitable than C for writing this sort of code? Or just no mature implementations yet?
Lua has no static typing, which is a productivity hazard for me. I prefer to catch errors at compile-time. If you added a compile-time static checking step to Lua it wouldn't make the language any slower, but would just catch errors at the build step. Purescript does that (and more), but to Javascript, and the Javascript code the Purescript compiler emits is very readable. If Purescript could also compile to Lua, I would consider using a Lua VM. Note that most static languages prevent the use of many idioms found in Javascript (and Lua too). Purescript can capture many such patterns using row polymorphism - it statically checks Javascript object manipulations, without adding any overhead. So it kinds of bridge the gap between the idioms of static and dynamic languages. For code that Purescript can't type check it offers a FFI interface to Javascript, that is analogous to Rust's `unsafe`, in that the programmer is responsible for not violating type soundness (but fortunately unsound programs aren't actually memory unsafe, thanks to the Javascript runtime) &gt; Structural typing (or row polymorphism I guess. Do they differ?) It appears that [row polymorphism isn't the same as structural subtyping](https://brianmckenna.org/blog/row_polymorphism_isnt_subtyping) but I don't know much about this. --- As an aside, I'm playing a game called Don't Starve Together, which offers modding with Lua. Various mods have made the game crash; often when a field of a `nil` value was accessed or other silly things like that. We know that a programming language can disallow such errors at compile time (like Rust does), but Lua doesn't offer tools to prevent an error like this.
There are many gotchas in writing constant time cryptographic code when using optimizing compilers in general. LLVM is quite smart and unless you disable optimizations will insert branches if it thinks it can improve performance. Code that on the surface does not appear to branch on secrets can end up doing so after going through LLVM's optimizer (this applies equally to clang or rustc). All that said, the C implementations of most cryptographic algorithms are more mature and often authored by cipher designers or other cryptographers. Both *ring* and sodiumoxide also ship assembly implementations that will offer much better performance than what's possible in pure C or Rust (Note: rust-crypto also ships some assembly cipher implementations, perhaps most notably of AES, leveraging Intel's native AES-NI hardware implementation of the cipher)
Thank you so much! I suppose I'm used to having my hand held by higher level languages in that regard. Quick follow up question if you don't mind. Before I had most of the logic in this function in a giant for loop. Essentially, the for loop held the println! and some other functionality after that. I then split the function into two for loops with the println! in the middle of them for debug purposes. It was after I split the for loop that I began seeing the error which made me start thinking it was a lifetime issue. Do you know why maybe the compiler wouldn't complain about the first scenario, but it did complain after I split the for loop up?
You want /r/playrust.
I'd have to see the original to answer for sure, but it's likely as simple as the compiler could prove if you used the variable then it was initialized and you introduced the ambiguity during the split.
Thank you for this talk by the way: [Rust: Unlocking Systems Programming](http://www.infoq.com/presentations/rust-thread-safety) Everything is so clearly laid out (Hopefully those not so used to Rust feel the same). I like to recommend this video to people.
To add to bascule's points: RSA is very broken if quantum computers improve enough, and there are many prominent cryptographers that believe that we are very close or that it is reasonable to assume that we have already reached this point. See Daniel Bernstein and Tanja Lange's recent talk at the 32c3: https://media.ccc.de/v/32c3-7210-pqchacks 
I am diving head first into Redox this week. Very excited to play around with it.
I'm still running around fixing clippy warnings in various projects! Last week, I learned something about lifetime elision, found a false positive in clippy and some problematic constellations (that make applying the suggestions harder than anticipated).
It's just the english word let, like you often see in mathematics.
You can also do that e.g. in Ruby using class level programming and it tends to be rather manageable. It all isn't _easy_,but so is type level programming.
&gt; I assume the origin is probably LISP or something older Yeah, first from mathematics, into Lisp then through ISWIM, ML, and OCaml (Rust was bootstrapped from OCaml).
Definitely agree. The talk I linked to is about "hacks" that one could employ until decent quantum-resistant standards and implementations emerge. Also, there is currently an ietf effort[1] to standardize a hash-based signature scheme (merkle signatures) which isn't based on thought-to-be-hard mathematical problems (The only requirement is a cryptographicly-secure hash function). This might not currently be of any use to op, but the math is there and standards are starting to catch up. I'm optimistic that we wont be waiting long to be able to use practical, quantum-secure crypto. 1: https://datatracker.ietf.org/doc/draft-irtf-cfrg-xmss-hash-based-signatures/
The really great thing is that it is so powerful yet so simple. So simple I can write them on my mobile phone from memory: * If there are multiple distinct input lifetimes and no output lifetime, they can be elided. * If there is exactly one lifetime for one input and output, this lifetime can be elided in both in- and output. That's it. No matter where the lifetimes are (in a `&amp;'_ ptr`, or `Generic&lt;'_&gt;`), the rules apply regardless.
&gt; isn't something I'd expect Rust, or any statically compiled language with generics, to ever support. That's a bit strong. (Glasgow) Haskell is a statically compiled language with generics and has had it for a long time. I believe OCaml has something equivalent via the module system as well. Any language which does polymorphism by uniform representation (and whose type system isn't already encumbered by other conflicting features) could support it without *too* much difficulty, it only needs to be implemented in the typechecker. Even languages like Rust could theoretically support it albeit with a bit more work.
That's true. I think of Python as good stepping stone, so beginner wouldn't be overwhelmed with all the information. But I believe it's beneficial to move to another language (Rust) soon.
I read it as "implement"
I'll be preparing some stuff for our next meetup in Cologne which is this Wednesday: http://rustaceans.cologne/2016/04/06/hack-and-learn.html
Cool, Thanks!
What does `case` do in Rust? I've never seen this keyword in Rust.
I am trying to write a new post for [os.phil-opp.com](http://os.phil-opp.com). It will create a very basic kernel malloc to unlock Box, Vec, and most collection types. The basic allocator just bumps a pointer and leaks all freed memory. This is bad, so I try to introduce a better allocator in the second half of the post. After that, we will explore CPU exceptions, interrupts, and keyboard input. I am very excited to work on it again! :)
The sixth sense?
This might be a good place to drop this: How about rust on these? They look sort of cheap, but more powerful than arduinos. http://www.ti.com/ww/en/launchpad/launchpads-msp430-msp-exp432p401r.html#tabs
I interpret /u/heinrich5991's sample as "implement `ops::Add` for `u32` as follows:"
Thanks for the response. Yeah, I didn't want to get into much detail on what I'm working on. Hmm, I see - I guess I'll just have to work around it then. I'll look at BigInt for inspiration. If you've got any other examples let me know!
Isn't there a `synchronized` block to handle exactly this case?
At an algorithmic level, you may want to look at boyers-moore. At a more technical level * you should be able to reserve your input and output buffers to the sizes of `replace_from` and `replace_with` * the labels in `fill_buffer` seems unnecessary, you can just `return` * the iterators play at the end of the function I feel unnecessarily complexify the code compared to just having a `candidate: Option&lt;_&gt;` and `if let`-ing on it * `self.index` seems unnecessary? Isn't it the sum of `flushed_index` and `self.buffer_in.len()`? * in the match case, you don't need to manually iterate the replacement, you can just `self.buffer_out.extend(self.replace_with);` * in fact I think you could do even better: your output buffer is either a subslice of the needle (from the front), a copy of the replacement slice or a single item, so you might be able to do away with the output buffer's allocation entirely or almost so. Lifetimes may get somewhat gnarly though.
This presentation makes me wonder why there is no `#[do_not_the_fck_optimise]` attribute yet.
Wisper words of wisdom: let it = "be";
I *think* the simple answer is: working with raw pointers isn't *supposed* to be convenient. They're dangerous, and you should be avoiding them wherever possible. After a while, I just got in the habit of re-borrowing raw pointers if I was doing more than one or two derefs in a block of code. That also helps to cut down the amount of `unsafe` you have in some cases.
* You don't need the loop label. You can simply `return` instead of `break 'consume`. * You can introduce a local variable `potential_match` that stores your 4-line match expression. This saves you a needless indentation level. * Instead of `match` for `Option&lt;T&gt;`, use `if let Some(x) = foo` (or `if foo.is_some()`/`if foo.is_none()`). That saves you another unnecessary indentation level. * You can replace your `if flush_index &gt; self.flushed_index` with `if flush_index &lt;= self.flushed_index { continue; }` And this is how your code looks like with these changes: fn fill_buffer(&amp;mut self) { while let Some(item) = self.iter.next() { self.index += 1; // buffer all incoming items self.buffer_in.push_back(item); // Prune existing partial match candidates that don't match the next item let removes: Vec&lt;_&gt; = self.candidates.iter().cloned() .filter(|start_index| { self.replace_from[self.index - *start_index] != item }).collect(); for r in removes { self.candidates.remove(&amp;r); } // Keep track of new partial match candidates if self.replace_from[0] == item { self.candidates.insert(self.index); } let potential_match = self.candidates.iter().cloned() .next() .into_iter() .find(|x| self.index - x + 1 == self.replace_from.len()); // if the length of the first match is the length of the replace sequence then it's a complete match if potential_match.is_none() { // We can flush the inbound buffer up to the first partial match // (or the full buffer if there are no partial matches) let flush_index = self.candidates.iter().next().map(|x| x - 1).unwrap_or(self.index); if flush_index &lt;= self.flushed_index { continue; } let mut flush: VecDeque&lt;_&gt; = self.buffer_in.drain(0 .. flush_index - self.flushed_index).collect(); self.buffer_out.append(&amp;mut flush); self.flushed_index = flush_index; return; } self.candidates.clear(); for &amp;x in self.replace_with.iter() { self.buffer_out.push_back(x); } self.buffer_in.clear(); self.flushed_index = self.index; return; } } 
It compiles and runs fine: http://is.gd/YHmhzB
It's not that it's *supposed* to be inconvenient. Just that auto-deref for references works using simply `.`, but this isn't a good idea for raw pointers because the point where dereferencing of raw pointers happens should always be explicitly visible in the source. Using an arrow operator `a-&gt;b` as just sugar for `(*a).b` like C *could* work, because that's still explicit, and more convenient, but since references already have auto-deref with `.`, it doesn't seem worthwhile to add an `-&gt;` operator *just* for raw pointers. (For the record I don't actually like autoderef, personally, and would have preferred to in fact have had an explicit `-&gt;` operator for references as well.)
Even if you could inhibit optimizations, you are still screwed because of [cache based side channel attacks](https://www.computer.org/csdl/trans/cc/preprint/06899633.pdf).
There is a list of ways you could use the word, ie senses, and GP is pointing out the sixth one.
I'm not familiar with exactly what Seastar offers, but I am not sure Rust needs a "framework" for this exactly. The individual pieces are already in place as various libraries: cheap, nonblocking message passing, promises, fast networking, etc.
Yes, I know. I was apparently making a horrible joke. 
Looks interesting, would be interested to see how to use it, including backups and restores and what not.
Currently working on a GBA emulator. All of the graphics modes are pretty much done so right now I'm working on fixing some subtle bugs with the instructions. Problem is it's happening somewhere in the BIOS in a loop so who knows when I'll find it. I have a GitHub repo up here: https://github.com/ExPixel/Pyrite2 (I've never run clippy but I'm sure it would have a field day with this one.)
Wow, I didn't know about Serkr. Some really impressive work there!
That example is off. "case" there is only the name of the binding, i.e. see http://is.gd/dbsvWL. This example: http://is.gd/MHwMVh actually shows the error that is being produced.
The idea of interacting directly with the hardware is appealing in some ways, and the numbers they have on their site are impressive. I haven't been able to build it on my machine though to test it out since scylladb appears to depend on antlr c++ which my distro doesn't package up and seems to be rather hard to track down and download.
There are several overlapping but distinct concepts of "system programming". To one extreme one want to control everything including the binary, memory and cache; to another extreme one just want to get the system (whatever it is) correctly and consistently working. There are lots of programming languages that cover some of the spectrum but not all, and the inconvenience of making usually wrong decisions does not seem to hurt the system programming itself.
I'm not really all that familiar with F#, but, if F# is using CLR strings, I think the answer is no. You could then have an `Option&lt;string&gt;` that was supposed to be `Some` but was actually `Some(evil null string)` instead of `Some("hello, world!")`. C# has something similar to an option type in `Nullable&lt;T&gt;`, but it provides practically no value for reference types (like strings) because they're already nullable.
Maybe ramp can be inspiring as well: https://github.com/Aatch/ramp
Most of the time F# maps null to None. See the third paragraph: https://msdn.microsoft.com/en-us/library/dd233197.aspx There are of course caveats in order to make F# interop, but it looks like they made null map to None in most cases.
I would really like this, especially when dealing with mutable raw pointers, since I don't think it's safe to do the same thing I do with `*const T -&gt; &amp;T` ``` unsafe { let x = &amp;*x; ... } ``` with &amp;mut (`*mut T -&gt; &amp;mut`)
Yeah, I used VBA and Iodine as my reference for how I would start making the emulator and they both use a table for looking up instructions, so I ended up doing the same. So I found [this table](http://imrannazar.com/ARM-Opcode-Map) converted it into some javascript arrays (which can be found in [this gist](https://gist.github.com/ExPixel/7ac31f79dd17573fe76f)) so I could generate some rust code and then use that for decoding. I'll probably add a way to find out where registers are being changed to make debugging easier though.
got [fraust-echo](https://github.com/bburdette/fraust-echo) working with alsa, which is good because I've had good luck with alsa on the pi 2. I don't have audio pass-through working on the pi yet, but I got audio output working. Also updated oscpad this week so it compiles again.
Actually that's the issue: since we don't manage to reproduce it, who's to say that there is no erratic behavior "sometimes" and it just does not get reported :/ We know we have bug in there, and it's anyone's guess where...
Oh my... I started by reading through VBA as well and I think it is the most horrible code I've seen in quite a while. Thus, I completely dropped it and started from scratch by reading the official ARM7TDMI reference manuals. They are pretty nice to read, way simpler than those massive and bloated x86 documents. [Here](https://github.com/Evrey/GBArs/blob/master/src/hardware/cpu/arminstruction/mod.rs) you can see the table I'm using to decode ARM state instructions and how I'm decoding those, although I'm about to refactor this thing, after seeing how much cleaner THUMB decoding turned out to be. I can't say what approach is better, your heavy specialisation or my generalisation of instruction groups, but at least we can both read our codes in parallel next to our reference documents. If you are interested in the PDFs I use... they are linked in the README.
I read through the reference manuals for the actual instructions as well. I began writing the emulator using a similar method but figured using a LUT and handling most of those if statements at compile time was going to give better performance. 
I think requiring `-&gt;` would be annoying in cases in which you have (for example) `&amp;Arc&lt;T&gt;` (common to avoid reference count bumping but still allow cloning the arc). Besides, you pretty much have to have auto*ref* for the left hand side of `.` in method calls, so autoderef can be viewed as being consistent.
It seems like virtually every cryto implementer is begging for support from compiler authors for slow predicatable code that doesn't unexpectedly change behavior. Here is is DJB's plea for help https://groups.google.com/forum/#!msg/boring-crypto/48qa1kWignU/o8GGp2K1DAAJ
Further, there are a few languages that use `let` for defining variables. So it has some precedence. I like it better than alternatives like `dim`, `var`, `val`, `def`, or `auto`.
I almost recruited you into my clan... Just kidding! What you're looking for is /r/playrust
This sub is for the Rust *programming language*. Try look at /r/playrust instead (not sure it accepts this kind of posts though).
Definitely not as fancy and impressive as most Rustacians here but I finally implemented json config files and multithreading to my [feretto](https://github.com/jkordish/feretto/tree/config_directory) application. Basically as an **Ops** guy I have been diving into Rust in hopes it will be my go to language for any tooling. Essentially the purpose of the application is to utilize [Consul](https://github.com/hashicorp/consul) to generate events from monitored log sources. My hope here is that I can possibly build in some form of self-healing for our micro-services as I can trigger events which the application monitors and then fires off commands. So through monitoring a log and it sees a specific string of something not working I could generate an event that another app sees so it knows to restart the service (one example).
Were going to do the final parts of allowing [embed_lang](https://github.com/Marwes/embed_lang)'s VM implement `Sync` but I might hold of a bit if I don't figure out a way to implement it without losing (to much) performance in single threaded programs . Plan B would otherwise be to improving the documentation, probably by writing a small tutorial, or start on a blog post about writing embed_lang (and its predecessor) in Rust from version 0.9-now.
Yes, of course, the issue as I understood it was whether Java allows data races in code - it does, at both compile time and run time. It's up to the programmer to prevent them, not the language/compiler. Even the higher level constructs like Futures allow them by following shared references in an improper way to access/mutate state.
As far as I know, it is sound as long as you don't break the aliasing rules. A `&amp;mut` must have unique access. So it's easy to introduce undefined behavior: unsafe { let x: *mut u32 = ... let y = &amp;mut *x; let z = &amp;*x; // breaks the aliasing rules }
Where the aliasing rules are completely undefined :D
Until you guys actually come up with an actual spec it must be "at most one mutable reference xor any number of shared references"...
I've seen it a few times and have absolutely no idea what it is, could someone explain?...the latest place is on the hello world Server in the hyper README.md res.send(b"Hello World!").unwrap(); the 'b' in the .send() method, what does it do? I'm not sure but I think I've seen other letters used as well (maybe?)
It makes it a 'byte sequence literal' (as opposed to a `str` literal, which would be guaranteed valid UTF-8).
That's definitely true. I'm just curious whether my approach will be "fast enough". If so, I'll stay with this IMHO cleaner decoding. And if not... perhaps I'll experiment with JITing. This will be especially useful for PSR modifying instructions.
It will make the string literal (`&amp;'static str`) represented as a raw byte-slice (`&amp;'static [u8]`). Basically, it does the same as `.as_bytes()` but without the method call, so it can be used in `const`/`static` bindings, and it looks nicer than `"Hello World!".as_bytes()`. 
Sorry, pretty new :D Thanks a bunch
What do you consider a world of pain? I'd estimate that a transaction takes about a second (based on three round trips between leader and followers @ 260ms each + 100ms round trip to client.) 1. Leader tells followers to add transaction to input log 1. Followers ack 1. Leader confirms log entry to followers, all nodes get to work 1. Followers return COMMIT/ROLLBACk to leader 1. Leader tells followers to add all results to log. (If transaction failed, leader can inform client here) 1. Nodes ack 1. Leader sends transaction confirmation to client, sends log entry confirmation to nodes
Did you use `cargo run` or `cargo run --release`?
Agreed. At least in that case, your application should be using a safe bindings crate, and the safe bindings crate should be using a `-sys` crate. It's that middle crate that's tough.
Nice :) I guess.. since I already have most (if not all) of that functionality already running in Atom by using several plugins. Are your package a meta for those? Or are you writing it by yourself?
What's the recommended way to add a signed integer to an unsigned integer? assert_eq!(20usize + -5isize, 15usize); Preferably with unsigned overflow semantics (so panic when the result overflows the unsigned left hand side). Right now I just do: assert_eq!(20usize.wrapping_add(-5isize as usize), 15usize); But that's just ugly and will not catch overflow.
If that is true, it's awesome ! :)
Thanks. Boyers-Moore is exactly what I should have found before I started!
If one can integrate the file io into mio I guess it'd be 80% of seastar. 
Thanks. Most of my experience with option types and the like, is from Haskell, where you try to avoid things like `is_none()` in favour of pattern matching. But in this case the code is actually simpler. 
if anyone is interested in collaborating on an async file IO library get in touch with me, I've had a few ideas and done a few experiments. dcb on IRC.
Looking forward to it, your posts are excellent. :)
From the README: &gt; Thanks [rust-clippy](https://github.com/Manishearth/rust-clippy). We do love the great project. This just made my day. :-) Oh, and if the authors are reading this: Is there something else you want a lint against? Feel free to file a clippy issue.
Try running racer on the command-line first - it will probably explain to you why it fails.
- RSA's security-per-bit is pretty bad, and getting worse due to advances in the index calculus.
racer has a long way to go, but the author has been active in working for a real fix -- a compiler (rustc) integration that can help with real type inference.
https://github.com/libpnet/libpnet, not yet drop-in, but there are people work on it! One thing most of the other commentors seem to ignore or not know is that *most* of what Seastar offers is a high-speed userspace TCP/IP stack. The other utilities it has are mostly orthogonal, and can be implemented separately pretty easily.
Got my html macro parser working, so eventually we can get react-style components w/ jsx (rsx?). Haven't made a git yet, but it works like this: html!(&lt;div&gt;{value}&lt;/div&gt;) Compiles to: Node { selector: "div", children: [Child::Text(value)] } // or Child::Node(Node), Child::Comp(&amp;Component) And then for components we can do (haven't coded props yet): struct Link { href: String } impl Component for Link { fn render(&amp;self) -&gt; Node { html!(&lt;a href={self.href}&gt;&lt;/a&gt;) } } Still haven't quite figured out how to do the state; I ported redux over for testing immutability (like this.setState({}) in react), but I'm a little stuck. Basically this is for virtual dom diffing when webassembly comes around and we can use rust natively in the browser. For now the macro can just be used for templating html server side.
I've been adding basic telemetry to rustup. I've had multiple false starts and gone down a few rabbit holes, but it's coming along slowly.
At least with the upcoming windows 10 updates you can get all the Linux packages to run it. Though I could see if you wanted full native support compared to running it through the compatibility layer
Yes, that's quiet a nice implementation. The only part that is not generic is Messaging. And yeah, I do admit that it won't be as easy in Rust as in Go. (I will probably fork it and redesign at some point...)
http://www.rustup.rs works great for me on Win10.
thanks
For those who don't understand the goodness here, you can now do `"abc".to_string()` and it'll be as performant as `"let s: String = "abc".into();`
Not that this ever likely mattered in any actual code, but at least now we can stop hand-wringing about mentioning its performance characteristics in the service of full disclosure. :P
[Ron Paul agrees](https://i.imgflip.com/11z2jc.jpg)
Yeah, and we can deprecate two clippy lints. :-)
&gt; It is such a massive resource hog, and it is getting worse. That's what happens when people get the bright idea to write desktop apps in a hybrid of node.js and Chromium. 
Where I look for more hashing performance instead of safety, I'd like to replace code like this: type Set = HashSet&lt;(i32, i32)&gt;; let mut s = Set::new(); With something similar to this (about 40 extra bytes of simple source code). Is something with this level of simplicity &amp; succinctness possible to archive with the future Rust standard library? use std::hash::FastHasher; type Set = HashSet&lt;(i32, i32), FastHasher&gt;; let mut s = Set::new(); (There is feature(hashmap_hasher), but I don't know if that allows to go down to about 40 bytes of extra source code, and I don't understand its usage well enough). Regarding the OP code, I suggest to take a look at the warnings given by the Rustc compiler. So I think Rust code like this is a bit better: type Pair = (i32, i32); fn iter_neighbors&lt;F: FnMut(Pair)&gt;(mut f: F, (i, j): Pair) { f((i - 1, j)); f((i + 1, j)); f((i, j - 1)); f((i, j + 1)); } 
Oh, right! I had a look at that before even starting Sanakirja. Now I'm confused, could the OP /u/sanxiyn clarify why a new crate was needed, instead of improving the existing one?
Awesome! I'd love some more details! How does this compare to [cockroach](https://github.com/cockroachdb/cockroach)? In particular does it support serializable snapshot isolation? What is the approach taken for multikey transactions, do you use hybrid logical clocks?
I'm not surprised that Euler integration veers off course so quickly, that's a known problem with it especially for small step sizes. I wouldn't worry about it.
http://devdocs.io/ https://github.com/rhysd/devdocs.vim Haven't tried it yet.
You can drop the `return`s (and semicolons) in `nthLoop` for more rustic code.
Unfortunately, the borrow checker can't reconcile sub-borrows with method calls on `&amp;mut self`, as it has to assert that the latter is unique for soundness reasons. We might get expanded semantics someday which relaxes this requirement, but there's not much movement on it for now. I find that the best approach here is to refactor `sub_proc` as a standalone function which takes `&amp;mut Foo` and the fields you need from `Bar`. This is enough to satisfy the borrow checker, as it's not requiring a whole unique reference to `Bar`. Alternatively, you could create a new struct which wraps borrows of the required fields of `Bar` and move the `sub_proc` method to that struct. This is more useful if you want to break `sub_proc` into several methods which access the same fields, as you don't have to pass the fields explicitly to every function.
Funny, I was thinking about a new "No need for `to_owned` anymore, `to_string` is fine now." lint. :P
Is it so? I didn't know!
You were not the first thinking that. I think mcarton has already added an issue.
Oh, this is cool. I use atom full-time already, and like it despite it's quirks. Very cool. 
Nice! Didn't know about this =)
I love the fact that emoji based error handling made it onto the list of RFCs. ;)
That's one more topic off the berate the newbie list. This is truely good news, even though as a newbie I just used sed to clear all my code of this, to the less readable .to_owned()
How about method chaining / fp currying? use std::collections::HashMap; #[derive(Debug, Clone)] struct Foo { a: isize, b: isize } impl Foo { fn sub_proc(&amp;mut self) -&gt; &amp;mut Self { self.b -= 40; self } fn add_proc(&amp;mut self) -&gt; &amp;mut Self { self.b += 20; self } fn compose(lhs: &amp;Self, rhs: &amp;Self) -&gt; Self { Foo { a: lhs.a + rhs.a, b: lhs.b + rhs.b } } } #[derive(Debug)] struct Bar { map: HashMap&lt;isize, Foo&gt; } impl Bar { fn new() -&gt; Self { Bar { map: HashMap::new() } } fn root_proc(&amp;mut self) { self.map .get_mut(&amp;1) .unwrap() .sub_proc() .add_proc(); // borrow only lasts for lifetime of chain, so we can reuse self.map .get_mut(&amp;2) .unwrap() .sub_proc() .add_proc(); // composing multiple items let lhs = self.map.get(&amp;2).cloned().unwrap(); let rhs = self.map.get(&amp;1).cloned().unwrap(); self.map.insert(3, Foo::compose(&amp;lhs, &amp;rhs)); // mapping over the hashmap for (_, x) in self.map.iter_mut() { x.add_proc(); } } } fn main() { let mut b = Bar::new(); b.map.insert(1, Foo { a: 2, b: 43 }); b.map.insert(2, Foo { a: 0, b: 0 }); b.root_proc(); println!("{:#?}", b); }
A "specialist" hash like impl Hasher for PairHash { fn finish(&amp;self) -&gt; u64 { self.hash } fn write(&amp;mut self, _: &amp;[u8]) { panic!("Cannot hash arbitrary bytes."); } fn write_i32(&amp;mut self, value: i32) { self.hash &lt;&lt;= 10; self.hash ^= value as u64; } } that calculates, in this case, `(i &lt;&lt; 10) ^ j` [on top /u/Aatch's code](https://www.reddit.com/r/rust/comments/4dd5yl/rust_vs_f_hashset_benchmark/d1qcl39) makes the code stupidly fast. This is fair IMO because the F# uses a similarly specialized hash function. implementation | time/s :-:|:-: original | 2.6 /u/Aatch | 1.5 `PairHash` | 1.4 /u/Aatch + `PairHash` | 0.28
Do you have any example problems that are of the type this particular programming competition is likely to give you? If so, I would recommend practicing with a few example problems, and if you find areas where you get stuck or are unable to find a good way to solve the problem, ask about it here, on StackOverflow, or on `#rust` (`irc.mozilla.org`). That way, you can get a feel for whether Rust, and your skill in it, are appropriate for the kind of competition you will be doing; and if you do run into issues, you can ask about it and improve your knowledge of Rust. What requirements does the competition have on choice of compilers, libraries available to use, etc? If it allows you to use any compiler you want, and the nightly compiler gives you some features that would help, then I would say go for it and use the nightly compiler. The main concern with gated features in nightly is that the feature may go away or change in the future, so it's not necessarily a good idea to use on production code unless you're committed to keeping up with changes and fixing breakage. But programming competitions don't really have that concern; the code is generally written to be used once and discarded, so there's no reason to worry about it possibly breaking in future releases.
I had trouble with Racer not understanding paths with `~` in them. Try switching to an absolute path.
Rust does not guarantee TCO, the last discussion I heard was a special syntax to opt in to it, but I don't know where it's gone from there.
Question: Shouldn't `to_string(1)` be less performant then `to_owned(1)`? Doesn't `to_string(1)` copy the &amp;str to a new heap allocation while `to_owned(1)` just calls `String::from_raw_parts(3)`?
The "uncreation" of s0 from /u/Aatch's code doesn't seem fair though, and even the explicit sizing of the set is debatable.
I'm not a native English speaker so I'm not sure to understand exactly your answer... My point is that, from what I understand of Rust rules and from my programming practice in applicative languages, I don't see a situation where one would not add a &amp; for immutable parameters (said otherwise, I would expect this to be the default practice). This way you avoid borrowck complaints that don't seem useful here as there's no side effet, and you still ensure that immutable data will be collected as the end of its creation scope. At first thought, the rule remains simple for the programmer: the scope of immutable variables is lexically scoped and the time where the data is freed is still known deterministically. EDIT: I ask this because I'm used to functional programming with almost-always pure datatypes (I try to stick to so-called *benign* side effects, if any). So I'm not interested in Rust as a systems programmer trying to get a safer language, but rather as a functional programmer used to safety and seeking greater performance and determinism of memory management (plus freedom from data races in concurrent settings where a few side effects are needed).
If you invoked String::from_raw_parts, dropping the resultant string would try to free the &amp;str... which is super incorrect (for instance, "hello" points to the binary's rodata). .to_owned(), .into(), String::from, and .to_string() are all identical now, and as efficient as possible.
Ah okay. So the only different between to_owned() and to_string() is one changes ownership?
There is no difference between `to_owned()` and `to_string()` on an `&amp;str`. Both of them create a new owned `String` that is a copy of the borrowed `&amp;str`. There used to be a performance difference, but even that is gone now; that's what this post is about.
[And #3](http://imgur.com/GVFbI3L)
Nope -- as far as &amp;str is concerned, to_string and to_owned do the exact same thing.
I added the `?Sized` to try to remove the `Sized` bound because the compiler was failing because `S` was not `Sized`. Adding the `?Sized` had no effect on the compiler error, and that's where I finally got stuck. (Been writing rust for about a week now)
One thing that should be noted is that because of Rust's design choice, people who write unsafe code can't rely on destructors performing safety-related work. For example a code like this looks safe: http://is.gd/M9acyI As long as the `Modifier` is alive, nobody else can access the `data` variable of `Container`, and the Modifier's destructor ensures that `data` is back to a correct state. However since one can just do `mem::forget(modifier)`, it is in reality unsafe. This can be fixed by adding a variable to the main object, like this: http://is.gd/FjcozY Then in every method of `Container`, check the value of `modifier_has_leaked`. 
Here is a [playground example](http://is.gd/17KeGX). Mutability is a property of the variable binding. That means that you can change the mutability of data, but only if you own it. Once I give a String to a function, it is free to mutate it, destroy it, or pass ownership on (say, through a return value). But if I only give it an immutable reference, it is restricted to making copies and reading the data within.
Of course, you can either go the [ndarray](https://crates.io/crates/ndarray) route and have both Matrix and MatrixView types (where the latter borrow into the former) or if you must have exactly one type use a `Cow` like approach (see [here](https://llogiq.github.io/2015/07/09/cow.html)).
Or as Gankro puts it: you need to [pre-poop your pants](http://cglab.ca/~abeinges/blah/everyone-poops/).
*You can't.* Aside from moving `t`, you can't return a borrow from a function that isn't derived from a borrow that was passed in. If you really can't change `S`... there's no way to work around that. You could redefine `Own::new` to take a borrow to `T`, and delete the `t` field entirely. Beyond that... don't use `S`? `/shrug`
I would instead compare against the byte versions; `Vec&lt;char&gt;` will be _incredibly_ wasteful of space. Like, 4x the size, 3/4ths of which is wasted. In other words, match raw_bf[self.index] { b'&gt;' =&gt; { self.increment(); } and so on
That's as far as it's gone.
The first error is that you are trying to use a moved value. That's because `add` and `connect` both consume the `Chain` instead of borrowing mutably. So I changed these fn add(mut self, to_add: String) fn connect(mut self, from: String, to: String) to these fn add(&amp;mut self, to_add: String) fn connect(&amp;mut self, from: String, to: String) Then you get an error about not being able to borrow an immutable `Chain` as mutable. So I changed this pub fn feed(input: String, chain: Chain) to this pub fn feed(input: String, mut chain: Chain) and your code compiled without further changes. However the signature of `feed` doesn't make a whole lot of sense. It requires ownership of a `Chain` and returns nothing. So you will get a `Chain`, mutate it, and then drop that `Chain` at the end of the function, losing all the work you just did. What you probably want is either pub fn feed(&amp;mut self, input: String) if you intend for the `Chain` to be continually mutable, or impl From&lt;String&gt; for Chain // or to be more generic impl &lt;T: AsRef&lt;str&gt;&gt; From&lt;T&gt; for Chain if you intend this to be more of a factory function to create a `Chain`.
Wow, both those are really cool, much appreciated! For any readers who are unaware like i was: - https://github.com/rsolomo/cargo-check - https://github.com/arcnmx/cargo-clippy Is there a reason you're not merging these upstream? **edit**: Looks like there's a handful of PRs that do a similar thing, so maybe `rust-lang/rust.vim` will have this functionality soon enough - might not need to Pr your changes :)
Wow. I just posted an answer at almost the same time as you, and we have almost the same reddit usernames.. with similar solutions / responses.
And don't touch any pointers that alias a reference.
This looks really great, thanks! Especially because of the Neovim usage! &lt;3
Depending on usage requirements, and what `S` actually is, building one pointing to the stored `T` on-the-fly when required might also work.
Thank you for the responses, in my project, S is CodedInputStream from rust-protobuf (I've updated the main post to reflect this).
I got sidetracked and built [something similar](https://github.com/AtheMathmo/rubefunge-93/blob/master/src/befunge.rs) last week - it's a befunge-93 interpreter. I put it together really quickly and wasn't going for efficiency but maybe you can get something useful from it (you'd be the only one!). It uses an instruction enum similar to what /u/thiez describes. I prefer this approach - the code just feels like it is segmented nicely.
Thanks! I looked up Befunge and it seems pretty convoluted.
Doesn't Rust still rely on drop flags for safety? That's essentially the C++ move constructor solution, just built into the language. And the RFC that was accepted (but still not yet implemented?) just removes them from the heap; it keeps them in a local context.
Very nice! The implementation seems to be by Trevor Perrin, who designed Noise and the Axolotl/double ratchet protocol as well, which is being used by Signal and Whatsapp. *Edit:* Whatsapp does use the Noise protocol, see their [whitepaper](https://www.whatsapp.com/security/WhatsApp-Security-Whitepaper.pdf).
Is it possible to disable a target in Cargo based on whether a feature is enabled? My use case is an application with multiple frontends (CLI, ncurses, and GTK). I want to be always build the core library, but have the `bin/myapp-BACKEND.rs` files only compiled if a certain feature is enabled for them. EDIT: Right now my solution is to let it always build the binaries, but restrict the actual implementation based on whether the feature was enabled: #[cfg(feature = "gtk3")] mod impl { ... } #[cfg(feature = "gtk3")] fn main() { impl::main(); } #[cfg(not(feature = "gtk3"))] fn main() { println!("Application was built without GTK support"); std::process::exit(1); }
 trait MathyStuff {} impl&lt;T&gt; MathyStuff for T where for&lt;'a, 'b&gt; &amp;'a T: Add&lt;&amp;'b T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Add&lt;T, Output=T&gt;, for&lt;'a&gt; T: Add&lt;&amp;'a T, Output=T&gt;, for&lt;'a, 'b&gt; &amp;'a T: Sub&lt;&amp;'b T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Sub&lt;T, Output=T&gt;, for&lt;'a&gt; T: Sub&lt;&amp;'a T, Output=T&gt;, for&lt;'a, 'b&gt; &amp;'a T: Mul&lt;&amp;'b T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Mul&lt;T, Output=T&gt;, for&lt;'a&gt; T: Mul&lt;&amp;'a T, Output=T&gt;, for&lt;'a, 'b&gt; &amp;'a T: Div&lt;&amp;'b T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Div&lt;T, Output=T&gt;, for&lt;'a&gt; T: Div&lt;&amp;'a T, Output=T&gt;, for&lt;'a, 'b&gt; &amp;'a T: Rem&lt;&amp;'b T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Rem&lt;T, Output=T&gt;, for&lt;'a&gt; T: Rem&lt;&amp;'a T, Output=T&gt;, for&lt;'a&gt; &amp;'a T: Neg&lt;Output=T&gt;, {} fn extended_gcd&lt;T: Integer&gt;(a: T, b: T) -&gt; GcdResult&lt;T&gt; where T: MathyStuff {[...]} 
Drop flags exist to run destructors on conditionally moved values. If you removed drop, you could remove drop flags. However without drops (and drop flags), if you conditionally moved values out in a branch, this would effectively cause a compiler mandated leak. This is because it wouldn't be possible for the programmer to answer the question "did I take a branch that moved this?" in a way the the compiler could understand. That is: let x = box 0; if cond { consume(x); } // x possibly moved, cannot call free(x) here This would in effect force all programmers to refactor their code to have static destruction semantics (all arms of a branch would have to consume the value): let x = box 0; if cond { consume(x); } else { free(x); // ok! x certainly not moved yet here } or wrap any value that is conditionally moved in an Option and None it out when it's moved. (which is basically what drop flags do today) let mut x = Some(box 0); if cond { consume(x.take().unwrap()); } x.take().map(|x| free(x)); You have a similar problem with assigning a value to a maybe-moved value, but again this would at worst cause a leak, which is "safe" in Rust.
I think we're just thinking about the same thing from a different angle. Perhaps the TL;DR is that you don't need destructors for safety, but once you have automatic destructors in the style of Rust you need at least some of them (or some analogous concept) for the automatic destructors to be safe. You couldn't have a random oracle decide whether to run each destructor / zero each drop flag. For the record, I was always a fan of static drop, because I favor explicitness in manual memory management.
Yeah, if you have dtors, you need some answer to conditional moves. This is exactly what the drop RFC decided on: * dynamic drop (flags evaluated when var goes out) * static drop (fix your damn code and drop it yerself!) * eager drop (*sigh* we'll just insert the static drop for you...) There is of course the hillarious 4th solution, though: * leak destructors of conditionally moved values silently Such a tragedy that pnkfelix never proposed this elegant solution!
&gt; This is fair IMO because the F# uses a similarly specialized hash function. FWIW, I tried different hash functions in the F# and it didn't affect performance. 
&gt; I tried different hash functions in the F# Specifically?
Perhaps my question was badly phrased, I meant to ask: does Rust guarantee that your value lives its entire scope (if not moved explicitly)? From your answer I'll take that as a "'yes', Rust does not implement eager drop and never will by design". I'm under the impression that Rust's unsafe doesn't mean you can go violate all of Rust's principles, but all it really means is that you are in charge of enforcing them (so you're still not allowed to go create 2 mut references to the same object, even though you could). In theory (if my assumptions are correct) this means that Rust may optimize based on that. Assuming it is UB to access the contents of a moved out value, the only way I see it still 'safe' (but not enforceable by compiler) to access the conditionally moved value by inverting the condition (you can do this in C, then it ends up in the news because someone messed up and now there's an RCE in getaddrinfo) let x = box 0; if cond { consume(x); } if !cond { drop(x); } But the compiler won't accept this, not even with unsafe around it, so idk what I'm talking about.
If you're interested in details, I recommend reading the rfc that settled this all: https://github.com/rust-lang/rfcs/blob/master/text/0320-nonzeroing-dynamic-drop.md It in turn links to eager/static drop rfcs. Edit: the point on unsafe is basically "which is more surprising and likely to screw up someone doing something touchy"
`i+4000*j` and `(i&lt;&lt;10)+j`. With the default boxed tuples I also compared them with the default `hash(i,j)` and there's no difference. With unboxed tuples, `hash(i,j)` allocates a tuple which defeats the purpose. 
It's worth noting that the `FnvHasher` version in Rust did something like let mut hash = 0xcbf29ce484222325; hash = (hash ^ ((i &gt;&gt; 0) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 8) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 16) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((i &gt;&gt; 24) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 0) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 8) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 16) as u8 as u64)).wrapping_mul(0x100000001b3); hash = (hash ^ ((j &gt;&gt; 24) as u8 as u64)).wrapping_mul(0x100000001b3); since your `FnvHasher` didn't implement an efficient `write_i32`. It's not the *value* of the hash that matters as much as the speed of performing the hash. The comparison in F# should be to an implementation of that. EDIT: ~~I just checked the source for F#'s `HashSet`; it turns out it's caching the hash code, so the speed of that is significantly less important. It should be simple to emulate this behaviour in Rust, though.~~ EDIT: Rust is doing that too. I think something's really suspicious.
Drop flags stop destructors running twice, which is definitely a memory safety problem, but Rust/the definition decides that it's not a memory safety problem to run destructors zero times.
Hm, I'm not sure I quite understand this point. For instance, I believe it would be safe to call the following function explicitly on every value that goes out of scope (at least, it better be, since it isn't `unsafe`!). fn drop_if_you_want&lt;T&gt;(x: T) { if random() { drop(x) } else { forget(x) } } Maybe you're thinking of a slightly different scenario? Also, it might be worth noting that drop flags aren't quite part of the destructor: they're also cleared by moves (e.g. moves into `forget`). 
I recently tried to create bindings for the [Newton](http://newtondynamics.com/) library. It compiles fine, but when it gets to the linking step it start spitting out errors on Windows. Here's the git repo if anyone want's to try their hand at decoding error messages: https://github.com/trolleyman/newton Example error messages: undefined reference to `vtable for __cxxabiv1::__si_class_type_info' undefined reference to `__gxx_personality_seh0' undefined reference to `vtable for __cxxabiv1::__class_type_info'
Cool, thanks! 
Thanks! 
I could have sworn I did it in a matrix library I was working on. I'll have to find out what I actually wrote.
&gt; On the other hand, it is basically manual memory management which is grim and one must ask why the compiler isn't doing this for us... No, there's an easy answer to that: Rust prefers to make costs explicit. If you choose to create stuff (Edit: on the heap), your code shows it. Rust doesn't *magically* speed up things (although LLVM sometimes does, but that's another discussion), it just gives you enough control to *manually* speed up things. That said, I wouldn't oppose an optimization that inserts "correct" sizes for collections behind the scenes (perhaps by statically analyzing the code for loop counters around insertions; this could work at least in simple cases). I'd still opt for the explicit version, because it's easier to reason about.
My understanding is that it was designed to be as convoluted as possible. The idea was to create a language which was incredibly difficult to compile - writing an interpreter for it isn't too bad though.
 trait Map { type Key; type Value; type Map&lt;K, V&gt;; ... } impl&lt;K, V&gt; Map for HashMap&lt;K, V&gt; { type Key = K; type Value = V; type Map&lt;K_, V_&gt; = HashMap&lt;K_, V_&gt;; ... } *Quxxy sighs dreamily.*
In what way does C++ rely on either destructors or "implicit deriving" to prevent use-after-free? What do you mean by implicit deriving? (Just wondering.)
You can do representation specialisation via specialisation of associated types, e.g. #![feature(specialization)] use std::mem; struct NaiveRepr&lt;T&gt;(*mut T, usize, usize); struct UnitRepr(usize); trait VecInternals { type Repr; } impl&lt;T&gt; VecInternals for T { default type Repr = NaiveRepr&lt;T&gt;; } impl VecInternals for () { type Repr = UnitRepr; } struct Vec&lt;T&gt; { repr: &lt;T as VecInternals&gt;::Repr, } fn main() { println!("u8 {}, () {}", mem::size_of::&lt;Vec&lt;u8&gt;&gt;(), mem::size_of::&lt;Vec&lt;()&gt;&gt;()); } Prints `u8 24, () 8`. (It may make sense in future to add sugar to make it less verbose, maybe.)
[`&lt;[T]&gt;::swap`](http://doc.rust-lang.org/beta/std/primitive.slice.html#method.swap) should help.
 impl&lt;usize, T&gt; Map for Vec&lt;T&gt; { type Key = usize; type Value = T; type Map&lt;K_, V_&gt; = Vec&lt;T&gt;; } ...or something like that.
That wouldn't be a particularly rustic implementation, it is more likely to be a restricted form of TCO (e.g. it's an error to have live values that need destruction after `return`) that makes programmer manually choose where and when they want resources to be cleaned up during TCO. This fits into Rust's form of manual memory management: do only the obvious thing automatically, and have the compiler tell the programmer when they need think about something non-obvious. Pushing destructors up to the caller would, in the general case, require a heap-allocated vector of values to destroy, which isn't something that makes sense to do implicitly (a user can do it manually, if that's the best way to handle clean-up in their case).
I think the relevant point is the parenthetical "(including the zeroing of drop flags as a 'destructor' here)", since sometimes not zeroing the drop flags would result in double drops.
That wouldn't work, because someone could do `Vec&lt;T&gt;::Map&lt;&amp;str, T&gt;` and it wouldn't do the right thing. Actually, it's not even that simple for the original `HashMap` idea because `HashMap` imposes constraints on the key type, which you'd have to somehow also impose on the `Map&lt;K, V&gt;` alias, except that implies the traits are only good for the *most* narrowly-defined possible container, which you probably wouldn't want and... \**brain fries*\*
That's not the impression I got from the parenthetical: which was the true destructor of a type is the following. fn real_drop(value: &amp;mut T) { if !value.dropped { value.dropped = true; Drop::drop(value) // user-provided } } And then "flipping a coin" would be wrapping all calls to that true destructor with `if random() { ... }`. I can't see a way for this to lead to multiple executions of a destructor. It's definitely problematic if instead `true_drop` was changed to something like the following (where modifying the flag and running the user destructor are separate), but that seems like an unexpected modification. if !value.dropped { if random() { value.dropped = true } if random() { Drop::drop(value) } }
The latter definition is more similar to what happens in my understanding, which could be wrong. When a value is moved, its drop flags are zeroed (this counts as a dtor according to my interpretation of the parenthetical, and so it may or may not happen). Then when the value is actually dropped, its dtor may run if the drop flags weren't zeroed, possibly causing a double free.
Maybe a little offtopic, but I was just thinking how cool it would be if we could create a small runtime for building applications based on Servo, like electron/atom-shell in the nodejs community. My issue with those existing solutions is the js core, which is without doubt no problem if one would write a runtime based on Servo. Anyway, good work like always guys!
No, the difference between them is that `to_owned` is an implementation of `std::borrow::ToOwned`, and `to_string` is an implementation of `std::string::ToString`. These have different semantics though it just happens that they are exactly the same for `&amp;str`.
 smaht_pointer I assumed this was a typo, then you did it a few more times. Is there a joke I'm not getting?
In my opinion (I have implemented both Befunge-93 and Funge-98) it is actually *less* convoluted than many other esolangs. [Fungeoids](https://esolangs.org/wiki/Fungeoid) are generally designed to be hard to compile ahead of time (though the recent evolution of JIT compilation made this "feature" somewhat less interesting), and not necessarily hard to write. Befunge-93 is particularly easy to write because it exposes the familiar stack concept with simple literals (unlike Brainf\*\*k :-), and unlike Funge-98 you are *forced* to read from and write to the "variables" (shared with a code space). It may be hard to read since complex Befunge-93 programs tend to be packed into the limited space (80x25 max) however.
It is a Boston dialect of C++
I'd advise against doing that. Now Matrices can never outlive the slice you initialized the matrix with, due to the new lifetime constraint on the struct. So you probably should use a View like struct and an owned struct instead.
Frontend is generated in Rust using Horrorshow template library. Is the font too small? What is your screen resolution?
The Borrow trait (and thus ToOwned) is supposed to mean that the Owned and Borrowed versions are equal and hash to the same value, I believe. 
And I [clippy'd](https://github.com/trevp/screech/pull/2) it for more rustic code. :-)
Thanks for the heads up. Should be due to a missing content type http header. I have added it now, so it should be fixed. Can you please try now..
Cheers! Glad to see `multipart` being used in a production environment! Any complaints or feedback?
But I'm speaking of fully immutable data. As long as I insert a `mut`, I see why ownership is important.
I totally agree with you but your example just highlight the fact that writing unsafe code isn't trivial. Why would you need a flag when you can check if the pointer is null or not? Why moving the pointer when you can access it through the container one? Finally as deferencing a pointer is unsafe, any additional method need to know the invariant of the type, and here clearly because of the way constructors works you can't assume that the pointer is never null. However if you really want to be sure to execute code after user provided one, you can by taking a lambda. It is maybe strange for someone coming from c++ but not for someone coming from a functional language where this would be the obvious design choice (and the only one). This is exactly how has been solved the scoped thread issue in `crossbeam`. 
I'm not the same person, but I can say that it's a bit dense, which makes it a bit harder to read. It's cool for headlines and short sentences, but I would recommend something more "regular" or "spacey" for body text. The white on black contrast is often hard enough. You could perhaps set `letter-spacing` for `p` to something like `0.04em` or `1px`, and the letters won't feel like they are trying to fuse. Just a recommendation. Cool project, otherwise :D
I think I have only used very basic functionality of the library. Upload a file and limit it to certain size. That worked perfectly, without any issues, the very first time. It was a very pleasant experience using the lib. But I just wonder if the common use cases could be made a bit more simpler. For example, a function to get all the text fields as a hashmap, and another to get all the file uploads...
I have no idea why everyone says that collection traits require higher kindedness. It seems to have become a kind of received wisdom that's being repeated without justification. I see a lot of examples like the one Quxxy posted, but I've never seen an explanation of what can't be expressed using a signature like `trait Map&lt;K, V&gt;` (and I've asked before). I think someone who was deeply familiar with lib_collections made the claim in the distant past, maybe without elaborating, and now everyone just repeats it. I expect the actual problem has to do with something like the drain or entry APIs. Paging /u/Gankro in the hopes that he knows what the higher-kindedness required actually is. ---- My own opinion is that most of the use of collection traits is to be able to provide blanket impls for `T: Sequence&lt;U&gt;` and so on. In order to do that, the collection traits will need to be arranged into a hierarchy, and to define that we would need both mutually exclusive trait bounds and higher-ranked type parameters in trait bounds, so that we could define them in part as: trait Map&lt;K, V&gt;: for&lt;T&gt; !Sequence&lt;T&gt; + for&lt;T&gt; !Set&lt;T&gt; trait Sequence&lt;T&gt;: for&lt;K, V&gt; !Map&lt;K, V&gt; + for&lt;T&gt; !Set&lt;T&gt; trait Set&lt;T&gt;: for&lt;T&gt; !Sequence&lt;T&gt; + for&lt;K, V&gt; !Map&lt;K, V&gt; --- [EDIT: Aha! The issue is indeed the `Drain` API.](https://apasel422.github.io/eclectic/eclectic/#a-note-on-trait-objects) There is actually a way to express this because of higher-ranked lifetimes. You could have every collection's signature include a lifetime parameter, and then when bounding you would have to do `for&lt;'a&gt; Sequence&lt;'a, T&gt;`. But this is quite gross and not worth it in my opinion. My solution would be to leave drain out of the trait API for now.
Increased letter spacing to 2px and changed white to gray! Hope it is better now..
This definition confuses me a lot. Surely you don't want an associated map type, the map type is the self type.
I support this idea. Basically replace js scripting with Rust, so the whole thing is not as slow as Atom. EDIT: Seems like it's planned: &gt; Modules can be written in JS or in Rust. I guess we should start rewriting jquery in Rust or something.
Indeed, but this was needed for an integration test to make sure several different types worked together correctly. No access to private fields. As a side note, I really wish that functions marked with `#[cfg(test)]` were available when a crate is under test. I.e., I wish a test version of the crate was built for use with tests. Then I could just grant access to the private member for tests only, with another function.
Fixed! Thank you!
How does this suggest a preference for an associated "Map" type as opposed to simply returning `Self&lt;Self::Key, U&gt;`? This function is also not actually the part of any standard library map's API, suggesting that it is not actually necessary to define map as a trait. &gt; It doesn't make a lot of sense to do `impl&lt;A, B, C, D&gt; Map&lt;A, B&gt; for HashMap&lt;C, D&gt;` You're right it doesn't, the impl would be `impl&lt;A, B&gt; Map&lt;A, B&gt; for HashMap&lt;A, B&gt;`. It also doesn't make sense to say that `HashMap&lt;A&gt;::Key = i32`; the definition of the trait can't preclude all poor implementations. The distinction between assoc type and type parameter for key and value is not significant to the point I am trying to make, which is that the "map" type is just the _Self_ type.
Incorporating the iterators in the traits requires HKT or at least some new rust feature. I'd guess that's the most major thing that's missing. Collection traits without iterators are a bit lacking. This kind of solution is very lacking http://apasel422.github.io/eclectic/eclectic/trait.Iter.html (boxed trait objects, suddenly iteration of that vector is more than 10 times slower). Yes really, being able to inline the iterator's internals and optimize the loop is pretty important.
The spacing would have been enough for me, personally, but the softer contrast is also nice. :) I know that some "eye conditions" can make white on black harder to read (sometimes because of double vision), which is why it's generally not recommended on displays. That doesn't mean that it shouldn't be used, just that it's good to be aware of its downsides. I suppose it's ok in this case, since the text is relatively short, so the effect can get some priority. Something like a blog or a forum would be another story.
How do you map `Self&lt;_, _&gt;` to the concrete type? Do you require that the concrete type have *only exactly* that many parameters, in that order? What about the hasher type? What if you have a map whose key is fixed? You'd need something like `impl&lt;K, V&gt; Map&lt;K, V&gt; for HashMap&lt;K, V&gt; where Self: type&lt;K, V&gt;`; some new syntax for getting across the idea that the concrete type is parameterisable and in what order the parameters are provided... ...or you could adapt the already existing `type` items and have one less construct in the language. And as for putting the types as parameters on the trait... *why?* Why repeat them when you can just make them associated? My point was that *given* the assertion that implementing the `Map` trait for a set of types *different* to the set of types being used by the implementing type doesn't make sense, why support it? It's broadly the same reason that `Iterator` uses an associated type instead of a parameter: what the iterator yields is a property of the iterator type, not the trait. And anyway, none of this is important given that this isn't a proposal and I just made the whole thing up off the top of my head in 10 seconds. I haven't even *seen* a complete proposal for HKTs.
Now it works! https://imgur.com/WdcOTyZ Nice works man. The font is not really readable IMHO (see screenshot, especially the title), but overall the app is quite nice. :) 
What color scheme and font is that?
There are a lot of projects that are currently technically possible using heap allocation and virtual calls, but would be possible without them given more work on the compiler. Heap allocation and virtual calls are impossibly slow compared to stack-allocated static calls, so the consensus seems to be to use less ergonomic alternatives that allow inlining etc. instead of using more ergonomic but slower designs.
I fixed that by removing the break that I put in for some reason when it hits a non-command char. You're right about the "stringiness" of the code, I should probably change that.
This has nothing to do with mutability - if ownership was tied to a scope as you are suggesting, you would not be able to transfer ownership. Transferring ownership is required for all of the following: - Returning types like String, Box etc. from functions. If the object is always cleaned up at the end of the scope you can't return it. - Efficient type conversions. Currently it's possible to convert boxed slices to Vecs, or Strings to OsStrings with essentially zero cost, because you can give up ownership of the original Box/String. Without ownership transfer you'd have to allocate new memory and copy the data. - Functions which extend objects' lifetimes. For example, the send method on a channel *must* take ownership of the object, it can't just borrow it. Likewise, a function that stores the object in a global variable must do the same. - Functions which consume an object. Think `drop`, `drain`, etc. If they didn't take ownership, then the object would still be usable after the call. As you can see, the "middle-ground" isn't really an option. However, immutable data does have *one* advantage when it comes to memory management: for truly immutable data (ie. no Cells or RefCells either) it's impossible to create cycles. That means if you consistently use reference counting, via Rc or Arc, you get automatic memory management without garbage collection. However, reference counting does come at a performance cost compared to a single-owernship model.
A `From` (or `Into`) impl wouldn't be enough to allow for `std::borrow::Cow`. This is something that many people forget: Traits have *semantics* – another example is I see a lot of types in Rust code which could trivially implement `Default`, but don't. Btw. I wonder if it would be a good idea to allow `#[derive(Default)]` if there is one argument-less constructor (or multiple ones where one is named `new`)?
Rust basically has two string types, `&amp;str` (an immutable borrowed slice) and `String` (an owned mutable string, akin to Java's `StringBuilder`). String literals are `&amp;str`. You now have three options: * use `.into()` to convert your literals to owned strings * change the type to `&amp;'static str` (only if all names are known at compile time) * change the type to `Cow&lt;'static, str&gt;` and use `.into()` – `std::borrow::Cow` is a type that can hold either a borrowed pointer or owned type.
I didn't actually realize that `to_owned` was implemented from a trait at all -- I thought it was just a regular method. But yeah, given there's a trait that represents converting from a borrowed version of a data type into an owned version, it would be wrong for `&amp;str` not to implement that. I don't know if `#[derive(Default)]` would do much; I find myself all the time having to get rid of my `#[derive(Debug)]` statements after including a type from another crate in my structure, where the crate author didn't implement `Debug`. This despite that the type is usually just a thin wrapper around an FFI pointer or something like that. I mean, if you don't want to expose implementation details, I feel like the least you can do is have an impl that shows `"&lt;struct Opaque&gt;"` or something when choosing not to `#[derive(Debug)]` -- but I think authors don't think to add the relevant derive statements over their types in the first place.
I'm in the same case. I reported this issue: https://github.com/servo/servo/issues/10030. It seems there is a problem with Ubuntu.
Interesting, I ended up using `.to_string()` which I assume is similar to using `.into()`, I'll have to look into the other techniques, especially the last one you listed. 
Submit to CotW?
 - Is it compiled in release mode? - Are you using webrender? - What webpages are you viewing? - What hash are you on? We had a whole host of IPC and constellation related panics over the last few weeks which have been fixed. There's a whole bunch of perf bugs which we're aware of and know how to fix but haven't gotten around to. This makes certain websites (e.g. twitter) very slow to load. I'm on Ubuntu on Intel integrated and with webrender on it works amazingly on master. But for the past few months it's been a string of crashes that we've since fixed.
Cool, please post it I am very interested.
I take a look at Servo about once every two weeks to a month, generally compiling in release mode and trying both with and without webrender (since it became available). I always do a git pull first. For webpages I tend to use sites such as CNN, bbc, slashdot, wikipedia, tweakers.net, reddit, twitter and github. Awesome to hear that these issues are being worked on, I really am excited about the tech in Servo. I'll try reporting any issues I see next time I try it out, if they're not in the tracker already. 
http://www.arewewebyet.org/topics/soap/ is correct, as far as I know. There aren't any libraries I'm aware of.
Well, you can also completely remove any mention of mutability from the function signature and still have the exact same function: http://is.gd/vsMMID One of the most important properties of borrowck is that it can operate solely on function signatures. This is necessary for ensuring that compile times don't spiral out of control. Your proposal would require it to inspect function bodies as well to determine if the moved parameters are mutated.
I suspect using gsoap to generate C code for your soap api and then calling those C functions from Rust via FFI would work pretty well.
Yeah, thinking a bit more about this, having more `Default` impls in `std` would be the better solution. I'm thinking of `Rc`, `Arc`, `Mutex`, etc. (where `T : Default`)
Someone named LorenVS has been [working on this](http://logs.glob.uno/?c=mozilla%23servo&amp;s=24+Mar+2016&amp;e=24+Mar+2016&amp;h=ripped+out+mozjs#c391661) recently, but I don't think they've released their code yet.
Self is a type variable just like any other, Rust just uses sugar to make it seem a bit magical. (Haskell for example doesn't.) The definition of Monad, which is the classic HKT use case, is: trait Monad { fn wrap&lt;T&gt;(T) -&gt; Self&lt;T&gt;; fn and&lt;T, U&gt;(Self&lt;T&gt;, Self&lt;U&gt;) -&gt; Self&lt;U&gt;; fn and_then&lt;F, T, U&gt;(Self&lt;T&gt;, F) -&gt; Self&lt;U&gt; where F: Fn(T) -&gt; Self&lt;U&gt;; } Edit: fix wrong sig on and_then
Fair enough. I don't object to people having made the claim, I object to the status it has obtained as received wisdom that people just repeat without understanding. The actual issue is an interesting problem that Rust has and not specific to collections - its about defining traits which include temporary views. I also suspect that higher kinded lifetimes are a much more constrained feature request than general higher kinded types, but I'm not certain of that.
I wrote [sodium-sys](https://github.com/rustyhorde/sodium-sys) which is similar to sodiumoxide in that it wraps libsodium. The exposed API is different, and the package will try to build libsodium if it can't find it, which is useful for building on windows. Also spent some effort on using the secure memory features exposed by libsodium (secmem::malloc and free).
Are you sure it's JS that is bottlenecking Atom? In my experience, JS is rarely the bottleneck. (Fun experiment to try: turn off the JITs in Firefox and see if you can notice the difference. I can't!)
Oh cool, thanks!
I think good sandboxing will require an architectural approach - I find the Chrome multiprocess model to be really ideal for most cases, since a parent process can spawn child processes with very finely grained sandboxes/ policies. If you are building for Linux you can try: http://plhk.ru/static/doc/seccomp/seccomp/index.html Seccomp gives decent control over system calls, letting you whitelist what calls can be made (and to a degree, their parameters). Most sandboxing relies heavily on the OS since it's going to be enforced by the kernel. So on Windows you could use AppContainer and Integrity Levels and tokens, whereas on Linux you could use users, groups, seccomp, or Linux Security Modules (or a dozen other sandboxing techniques that exist on Linux). Also, if you want to prevent 'design' issues, rust's type system can be a really powerful tool. For example, let's say you didn't want an unauthenticated user to have access to a service. You could have a linear type that takes users from Unauthenticated to Authenticated (two different structs), and only provided a 'do thing' member function to the Authenticated struct. Internally the structs may hold the same exact connection/ state, but you'll never be able to call 'do_thing' on the Unauthenticated struct because the function won't exist - you'll get a compile time error. edit: Here is an example of using a linear type to prevent an unauthenticated user from accidentally doing something privileged at compile time: http://is.gd/90hFTY
A lot of us run Ubuntu without those problems. I suspect the issues are something more specific--network or GPU or something.
&gt; Also, if you want to prevent 'design' issues, rust's type system can be a really powerful tool. Note that there is no real support from preventing the use of `unsafe` type, so while this protects from accidents in safe code, it still leaves the door open to accidents in unsafe code and of course, outright hacking.
The largest reason Servo performs so incredibly poorly for actual web browsing is that everything related to pulling stuff down over the network is extremely unoptimized. No prioritization, no speculation, no pipelining, no caching. Probably the only reason it has a network stack at all right now is because the W3C's test suite requires one.
Yeah, I'm certainly guilty of this. I know I've probably said something along the lines of "streaming iterators aren't possible in Rust," but of course they are, but they are typically limited in one way or another. Stated differently, if I have to use streaming iterators, then I'm typically embarrassed by whatever code it is that I ended up writing for it. (This actually may be a case where only higher kinded lifetimes would be needed?)
I'm a bit hazy on HKTs, but is there a reason this wouldn't work? trait Monad&lt;T, U&gt; { type SU; fn wrap(T) -&gt; Self; fn and(self, Self::SU) -&gt; Self::SU; fn and_then&lt;F&gt;(self, F) -&gt; Self::SU where F: Fn(T) -&gt; U; } then, for example, you could impl it as impl&lt;T, U&gt; Monad&lt;T, U&gt; for Option&lt;T&gt; { type SU = Option&lt;U&gt;; fn wrap(a: T) -&gt; Self { Option::Some(a) } fn and(self, b: Self::SU) -&gt; Self::SU { match self { Some(_) =&gt; b, None =&gt; None, } } fn and_then&lt;F&gt;(self, f: F) -&gt; Self::SU where F: Fn(T) -&gt; U { match self { Some(a) =&gt; Some(f(a)), None =&gt; None, } } } It's obviously not as nice, and there's no guarantee that the implementer makes SU the correct type, but assuming it's not abused, it should work correctly, no?
&gt; On the other hand, it is basically manual memory management which is grim and one must ask why the compiler isn't doing this for us... Because Rust doesn't have a garbage collector? If you want a GC'd language, use F#.
(Note that the signature of the function passed to and_then in my example was wrong (and yours is too, I assume because you copied). Instead I defined `map`!) &gt; there's no guarantee that the implementer makes SU the correct type, but assuming it's not abused, it should work correctly, no? The compiler can't assume that its not abused, so you can't take advantage of knowledge that your Monad's bind returns a Monad, which is what's very useful about the monadic abstraction.
Ah that makes sense. And yeah I just copied your trait and implemented as made sense.
Fair enough. And I fully support that. I don't see such traits as particularly great things to have in the standard lib anyway. I could probably get behind a semi-blessed support/de facto/nursery crate that provides them, similar to rustfmt or regex. Then getting HKT makes for a major version bump.
Ahhh gotcha, thanks!
Right, I looked at ATS, which I assume has HKTs, and it doesn't have any sort of regions (it uses dependently typed pointer arithmetic!).
I have the resize slowness problem even on a static html file stored on the disk. So I doubt it can be a network related problem. A GPU related problem seems more likely, but I don't know what I can do to help you diagnose the problem.
https://github.com/tomaka/vulkano
If you're just looking for a way to use GPGPU APIs (OpenCL/CUDA) with Rust, you do have a couple options. There's the [Rust OpenCL bindings](https://github.com/luqmana/rust-opencl), which should still work, though it looks like they didn't get very far with the high-level abstractions so you're probably stuck with low-level primitives most of the way. There's also the [ArrayFire bindings for Rust](https://github.com/arrayfire/arrayfire-rust), maintained by the ArrayFire team, which allows you to procedurally construct GPU programs in Rust. They do require that the ArrayFire binaries be installed. /u/tomaka17 is working on [Vulkan bindings](https://github.com/tomaka/vulkano) which ostensibly will support both compute shaders and OpenCL programs, but it's still very much a work-in-progress. As for compiling Rust to OpenCL/CUDA/SPIR-V, there hasn't been any efforts that I know of for some time. I mentioned one valiant effort in [another comment](https://www.reddit.com/r/rust/comments/4dnq0h/gpu_programming_using_rust/d1sq6si), but it hasn't been updated since early 2013; Rust and LLVM have both changed a lot since then.
I definitely want the features too. And I can see some useful operations that happen to span collections. I'm less convinced about something that says "this is a generic collection - it has a size and you can call `contains`". It seems less useful than combining orthogonal "ConstantAssociativeLookup" or "OrderedSequence" traits. My thinking is influenced by my time in Java/Scala and, to a lesser degree, C++. I can't think of a single instance where `Collection` was the appropriate abstraction choice. Even `List` and `Set` and `Map` interfaces are used "so you can substitute another implementation". But my experience says that substitutions rarely happen (and can have an unpredictable performance impact when you can't audit the implementation of whatever you're passing the collection to), usually those interfaces were chosen because they had sequence/uniqueness semantics, not because they actually needed to be a bag of items, and so often they were just iterated in their entirety that `Iterable` or `Iterator` would have been sufficient. In general my current thinking is the choice of data structures is important enough that hiding the real structure behind some abstraction is ultimately counterproductive even if it has some level of convenience. Maybe Rust's trait system is different enough that it'll work out. Maybe I'll change my mind again with more thought. That's just where I'm at now :)
I think I was trying it a week or two ago and had similar results. Github took a few minutes to show a blank screen with 3 icons or something. Duckduckgo wouldn't appear. I think I tried to find a minimum testcase but I messed up or something because I didn't get one.
can it handle different protocols? liek udp, tcp, ftp, etc?
You probably want https://crates.io/crates/syntex_syntax It's what Racer uses, which sounds like it maybe has similar goals to what you're trying to do?
There are two problems. One is the lifetime problem, that is: After starting the thread the main function could finish before the spawned thread in which case there wouldn't be any `Foo` object left in which case the spawned thread would operate on invalid memory. And then there is the problem of mutable aliasing: After `x.test()` in main you would still be able to mutate `x` and the spawned thread also has a mutable reference to `x`. This would break the rules Rust is trying to uphold to avoid memory unsafety and data races. So, it's a good thing that the compiler rejected your code. &gt; So, how can I achieve what I want to do? What *do* you want to do? Specifically, would you be fine with `test` blocking as long as possibly multiple threads are dealing with your `Foo` object? If so, you could try "scoped threads". This is not (yet) supported by the standard library but is available in external crates, including crossbeam. It would solve the lifetime problem. But then you would still need something like `Mutex` to solve the datarace problem. If you don't want the main thread to be blocked by `test`, you would have to solve the lifetime problem somehow differently. For example, using `Arc&lt;Mutex&lt;Foo&gt;&gt;`. The `Arc` solves the lifetime problem by keeping the wrapped object alive for as long as there is any `Arc` pointing to it (by thread-safe reference counting). And the `Mutex` wrapper still helps you avoid the data race problem by only allowing at most one usable mutable reference to exist. See [here](http://is.gd/gvF79H) for an `Arc`-based version that compiles. BTW: `Arc&lt;Mutex&lt;T&gt;&gt;` is similar to `Rc&lt;RefCell&lt;T&gt;&gt;`. Both allow sharing something and do "dynamic borrow checking". But `Rc&lt;RefCell&lt;T&gt;&gt;` is limited to a single thread whereas `Arc&lt;Mutex&lt;T&gt;&gt;` allows you to even share something between threads as well. 
I wish there were some single-source GPU library like [SYCL](https://www.khronos.org/sycl) for Rust (edit: actually, is collenchyma like SYCL?). But right now, the best language for GPU programming is GLSL. Now, since GPUs are standardizing in the SPIR-V binary format, perhaps rustc (the Rust compiler) could use the LLVM backend to emit SPIR-V code, and run on the GPU. This would take effort though.
As always, it depends on what you want. Does "characters" mean "number of Unicode scalar values?" Then yes. But you might mean "grapheme clusters".
Unfortunately, the story isn't quite so simple: text is *hard*. fn main() { let s1 = "a\u{308}"; let s2 = "ä"; println!("{} {}", s1, s2); println!("{} {}", s1.len(), s2.len()); println!("{} {}", s1.chars().count(), s2.chars().count()); } Prints ä ä 3 2 2 1 Looks the same... but different byte lengths *and* different `char` counts, since the first one is formed with [a combining character](https://en.wikipedia.org/wiki/Combining_character) (which [can be applied arbitrarily](http://eeemo.net/)). Additionally, Unicode includes [zero width/non-visible codepoints](https://en.wikipedia.org/wiki/Zero-width_space). A more accurate measure of the number of visible characters ("length of a string") would be counting graphemes, e.g. via the [unicode-segmentation](https://crates.io/crates/unicode-segmentation) crate. However... what are you really trying to measure? If it is, say, width of text in a terminal, even the number of visible characters [isn't right](https://en.wikipedia.org/wiki/Halfwidth_and_fullwidth_forms), e.g. the `!` in the following code is a fullwidth `!` which can take up two columns in a monospace font (doesn't seem to be visible here, but [it is on the playpen, for me](https://play.rust-lang.org/?gist=9a8248fd6d8fd044a85e523739d03642&amp;version=stable&amp;backtrace=2)): fn main() { let s1 = "！"; // .12. println!("{}", s1.len()); println!("{}", s1.chars().count()); } Prints `3` `1`. The right choice of "string length" depends on what you're doing and what you want from the length (for instance, `.len()` is correct if you're using it to preallocate buffers).
Last time I tested it, the fancy new combining emoji things *don't* count as single grapheme clusters. Plus, how many grapheme clusters a composite emoji appears to be depends on whether the renderer you're using *and* the font support them.
Thank you!! I'll definitely give it a try. I don't really care about performance as the request and response are supposed to be short. Do you know if there is a plan to extract these functionalities in a separate crate?
Soap has got to be a considered a legacy RPC framework at this point. There is very little reason not to target GRPC for anything new. I'd be really surprised if anyone goes through the trouble of creating a pure rust alternative.
Yes, using unsafe can bypass type safety, but that doesn't mean you can't handle that unsafety using the type system. I don't think it's true that the type system becomes tissue paper in the presence of any unsafe. If that were the case, rust would not be a very safe language, since it's all based on unsafe code somewhere along the line. What makes all of that unsafe code is the type system. &gt; But the OP is talking about using the language as a sandbox I don't read it this way - I read it as wanting to write sandboxed rust code, running rust in a limited environment. More generally, the question seems to be about design errors in rust - logical errors in the implementation of the program, not memory safety errors. What I'm saying is that these problems, as with the memory safety problems in rust, are all handled in the same place - the type system. Just as you can wrap around the unsafety of Vec by providing a typesafe interface, you can wrap around the unsafety of authenticating a user by providing information to the type system - as in the example I gave before. And I guess you're saying that it can't be enforced but that's only true, as you stated, in the case of 'unsafe' - in which case you know what you're getting into, just as with memory safety. edit: OK, I think maybe it seemed like I was saynig you could always use the type system to make ALL unsafe code safe. That is not what I'm saying. If I use enough unsafe in the right ways, as far as I am aware, rust's type system would not be enough to get around this unsafety.
Are there any guides you found useful in getting this working? Would really like to have this myself but haven't looked into neomake yet. 
"characters" aren't a thing in unicode
The example with Java is incorrect. Java is counting the number of bytes divided by two (number of UCS-2 code units), which becomes wrong once you enter the astral plane.
`rustc` can be used in this mode, which is more suitable than an actual parser for editor integration etc. `rustc -Z parse-only` (there are a bunch of them, see `rustc -Z help`)
Okay, anyways I've run into a bit of a problem. If I run "cargo rustc --bin -- -Z parse-only" it says "error: Unknown flag -Z". but if I do it with --lib --, then it works, but it doesn't check main.rs for syntax errors. Should I report this as a bug or is it fixable?
The user inputs the actual times of two dialogs from the video file of the movie. Say the time difference between these two dialogs is 30 minutes. Now the program goes to the subtitle file and look for the same two dialogs. And due to the rate difference, suppose that the difference between the same two dialogs, in subtitle is 40 minutes. That means time in subtitle is faster by 4/3 times than it is in the movie. So to fix it we have to multiply each time stamp in the subtitle by it's inverse, ie 3/4. After this, the program check if the time for the dialogs in the result, matches with the times provided by the user. It might still not be matching because the subtitles might be offset by a fixed amount. So now we just need to calculate the fixed offset by calculating the time differences between the user input time and the times we got in the first step, and again shift the subtitles by that amount, and we are done. The requires that the movie does not have any additional scenes that are not in the subtitles or the other way around...
Of course it's ok. it's your project and you have every right to do it the way you like it. :)
Yeah it is unfortunately.
This is not the crate name, but the binary name. You should know that a crate can only have one lib, but multiple binaries. Look at the crates.io documentation for the format, and you can use the [toml](https:/crates.io/crates/toml) crate to parse `Cargo.toml`
The point is that you rarely need to get the number of characters. * The number of UTF-8 bytes makes sense as a storage requirement. * The number of grapheme clusters makes sense as an approximation to the number of human-perceived characters. * The number of "codepoints" (Unicode scalar values) does not make sense except when it is embedded in the spec you have no control over (e.g. file parsing). `.chars().count()` or `.collect::&lt;Vec&lt;char&gt;&gt;()` is exactly for this kind of unfortunate cases. * The number of UTF-16 units equally does not make sense. The OP claims that it is widely accepted; it may be true, but it is also utterly useless.
You're thinking about `MAX_PATH` which is an ANSI WinAPI limit on path lengths[0], I'm talking about the *file name* limit on *NTFS itself*. [0] which incidentally also exists in POSIX though it tends to be much higher[1]: `PATH_MAX` in `limits.h`, which has [exactly the same issues as `MAX_PATH`](http://insanecoding.blogspot.be/2007/11/pathmax-simply-isnt.html) in that it only sets the limits of path-based APIs, it doesn't limit the paths themselves [1] 1024 on BSDs (including OSX), 4096 on Linux
Okay so I parse the toml file and get the name? Also I should be checking the field "name" or is there any other field I should be checking for too?
My apologies, now that I've read your comment a second time your meaning seems quite clear. A limit of 255 code units sounds like someone is using a byte to hold the filename length, [wikipedia](https://en.wikipedia.org/wiki/Comparison_of_file_systems#Limits) suggests this limit is relatively common (except that many other filesystems count bytes instead of characters).
There is no bin directory in most project's src folder though?
&gt; The right choice of "string length" depends on what you're doing and what you want from the length (for instance, .len() is correct if you're using it to preallocate buffers). That's certainly right, the only thing is the generic naming of `len` which will most likely evoke misinterpretations, so a name like `num_bytes` might have been better.
NTFS does not count characters either, it counts UTF-16 2-byte units.
I think one improvement would be to not have a `.len()`, and instead have `byte_len()` and `codepoint_len()` (and possibly `grapheme_len()`) to expose to the user that there is no single obvious length definition for text.
`src/main.rs` and `src/bin/*.rs` are the normal location of binaries, most projects either don't have a binary or only have one. Any other locations are specified in Cargo.toml, but I don't see that used often.
I think you're having the breaking issues because you're not wrapping around on int overflow
You may want to look at the [ocl](https://github.com/cogciprocate/ocl) crate which has nearly complete OpenCL API coverage and has both a high-level and low-level style interfaces.
The problem with the principle of least astonishment is that that it is heavily context-dependent. While it is seen as a guiding principle e.g. of Ruby (and Matz definitely popularised it), some miss a subtlety there: &gt; Everyone has an individual background. [...] The principle of least surprise means principle of least my surprise. -- Matz http://www.artima.com/intv/rubyP.html That fits the topic here quite neatly. "len()" not being the length in latin characters might come as a surprise to someone from the US, less for someone from Europe and definitely not for someone from Japan.
I'll give it a shot today! I was using YouCompleteMe because YCM uses `racerd` in place of `racer` which had large performance benefits. Namely that racerd doesn't have to re-parse everything on every completion invocation. Also, do you know if deoplete offers "automatic completion" where I don't even need to hit a keyboard shortcut to get autocomplete suggestions? I like that I can get suggestions as I type without doing anything special.
&gt; Number of graphemes? Scalar values? Grapheme clusters? What are these for? Well the number of USV is also the UTF32 storage, which is a storage information (Twitter's "140 characters" is a codepoints count for instance). The number of grapheme clusters seems significantly less useful technically but could be useful as part of human interfaces once in a while e.g. characters/words count in articles, or in fields you'd like to length-limit in more humane ways that "this doesn't fit our storage backend".
Well `codepoint_len()` already exists, it's `.chars().count()`. Following from that example, Rust could have provided a utf8 view and a grapheme clusters view, with their own respective lengths. So `bytes_len()` would be `utf8().count()`, and `grapheme_len()` would be `graphemes().count()`. Conveniently, that would have naturally extended into a utf16 view as well.
I would, but still learning Rust, though I'm proficient in Elm.
I don't think there's a way to dump that information right now, though it would definitely be info that a Rust IDE would love to have.
I'm not sure I understand what you're asking in your first sentence. Perhaps autocorrect messed it up?
I think type-level integers are going to be a game changer. I sincerely doubt that current libraries will be able to adapt without significant breaking changes, so most likely new libraries will emerge (probably by the same authors) with completely new APIs. As with any new language feature, it will be over used at first and it will take a while to learn how to use it effectively in particular on APIs. Another game changer will probably be the stabilization of compiler plugins, since that would allow implementing EDSL for math libraries in stable rust.
Specialization is another one. Many of the current libraries use generics all the way, so they are unable to use more efficient implementations for specific types.
I'm not convinced that your proposed solution would actually be more ergonomic than just having three different methods.
Neither am I. I was mostly thinking aloud. There are aspects of it I do like, particularly the consistency and extensibility.
The answer to this is "no". There was a PR to add this functionality, but it's been picked up and abandoned a couple of times. I haven't contributed to Cargo before, but I've grabbed that code and will try to get it cleaned up and PR'd again
Specializations are in Nightly correct? If so, how would I do this with them?
Iterators are not expansive to create. You have an XY Problem it sounds like.
Custom DSTs are also pretty much a pre-requisite to any decent numeric library.
&gt; Twitter's "140 characters" is a codepoints count for instance [Nope...](https://dev.twitter.com/overview/api/counting-characters#Definition_of_a_Character) [...although last I checked twitter doesn't do what they say they do anyway.](https://mail.python.org/pipermail/python-list/2013-August/653965.html)
Yes, so you would have to install nightly (I recommend multirust for this, unless you already have it), and enable the `specialization` feature in your crate. The problem I discovered just now is that your particular scenario may not yet be supported. [The tracking issue](https://github.com/rust-lang/rust/issues/31844) shows that only [#30652](https://github.com/rust-lang/rust/pull/30652) has landed, which enables method specialization, but not `default impl`. I have [tried to get it to work in the playpen](http://is.gd/fJndJn), with a method as well, but I can't seem figure it out. I think this is where my ability to help ends. :/ Maybe someone else knows. Edit: the playpen link was wrong. It should be correct now, but it's still not the solution.
This is actually a perfect example of where specialization is a workable answer, but the wrong one. What we actually want here is [mutual exclusion](https://github.com/rust-lang/rfcs/pull/1148). The compiler has a special case rule that `Copy: !Drop`, but that rule isn't a a first-class part of the trait system. If it _were_, the language would be able to infer that `String` cannot implement `Copy`, because `String` implements `Drop`. Then no specialization is needed. (Remember that specialization has a side effect in that other people can now provide specialized implementations for certain `Copy` types). There are also cases that specialization simply can't handle, like this: impl&lt;T: Copy&gt; FooBar for T { } impl&lt;T: Drop&gt; FooBar for T { } (There's actually an issue with this which is that `String` doesn't implement `Drop` itself, but if we were using the `Drop` bound in a more serious way like this a special case analogous to the inverse of OIBITs would make sense.)
Welcome someone participate to translate it. :)
nice!
awesome, guys.
This seems like the perfect intersection of two of my interests. I'm very busy over the next few days but I'll do the introductory chapter and send it you when I can.
Exist but are a horrible hack, I think everyone avoids them just because of that. I also presume they give ridiculous error messages and stuff like that.
Hey, only 12 issues. That's not too bad, especially since some of them aren't that big/easily exploitable. I'm sure there are more soundness issues, but still...
Thanks, I messed that up all by myself, I think... edited now for clarity. To expand on what I meant: "Are we web yet" [1] gives starting points to anyone interested in web development, is there any way for someone looking into rust specifically for numeric computations to get such an overview? [1] http://www.arewewebyet.org/
More so than "The Book"? If so looks like I need to learn yet another new language :P
It will be really nice to see rust settle into a mature state. The itch to use Rust in projects is clearly there. Everyday I see people wanting to use it for their next project if only there existed robust bindings for this or that.
How many others are out there?
no full featured book other than this yet.
That you're convincing me to learn Chinese to read this, but I joke. I look forward to it's translation and influence on other resources.
Tried installing it yesterday, but it appears broken on windows. It doesn't find rustc and cargo on the path, and I couldnät install multirust because a toolchain was already in place. Other than that, it looks nice. 
I mean "Rust is numeric" for some definition of numeric. I do not think that Rust is attractive yet for those doing numerical simulations, for example, because the libraries that one can idiomatically design right now in Rust are not as idiomatic/powerful like those in Python or C++. So while Rust has a lot of very attractive aspects, I don't think this is one of them. Why doesn't Rust have them? I think the main reason is that it is very very hard to write these EDSLs libraries (Eigen, Blaze, NT2) in Rust right now. Type level integers, specialization, plugins, DST, and maybe even HKTs will help, and these libraries will start to pop out. But I haven't seen even proof of concepts yet. Rust will be able to do better than C++ when we get there, but we are not there yet.
:)
You link to http://www.unicode.org/glossary/index.html#character which lists three different definitions for "character". So yeah it’s not "not a thing", but it’s still ambiguous and so not a useful term either.
That makes sense. Yikes!
Yeah, it would be great if this worked.
&gt; My perspective is that Float in num is restricted to f32 and f64, not implemented by any other types. What other types? Multi-precision floats?
 let result = if rhs &lt; 0 { lhs.checked_sub(-rhs as usize).unwrap() } else { lhs.checked_add(rhs as usize).unwrap() }; It doesn't get much simpler than that, and it's guaranteed to catch over/underflow even in release mode. (Implicit overflow checking is only enabled in debug builds.)
Sweet, thanks!
So, strictly speaking, it IS the best. 
&gt; type-level integers Are you referring to this? https://github.com/rust-lang/rfcs/issues/1038
There's at least two translations of The Book.
Thank you so much for writing this! I certainly agree TRPL has flaws, it's one of the reasons I'm working on a re-write myself. :) But beyond that, the more books the better, as far as I'm concerned.
There's two: https://github.com/ctjhoa/rust-learning/blob/master/zh_CN.md
I am trying to move out of a borrowed content in what (I think) is a safe way, but the borrow checker won't let me do it. Let's say I have enum Enum { Variant1(Inner), Variant2(Inner) } I want a function `swap(&amp;mut Enum)` that will turn `Variant1(x)` into `Variant2(x)` and vice versa, without cloning the inner `x`. So essentially: fn swap(variant: &amp;mut Enum) { match *variant { Variant1(x) =&gt; { *variant = Variant2(x); } Variant2(x) =&gt; { *variant = Variant1(x); } } } This doesn't work unless I clone both `variant` and `x`, but that seems wasteful. Is there a smarter way to do this? [Playpen link](https://play.rust-lang.org/?gist=e99ff6847e91b236c0f98170b84bbb08&amp;version=stable&amp;backtrace=0).
Sorry, `#![nobarries]` is waiting on more stabilisation of `Bushes&lt;T&gt;`. I think there's an issue filed already on `rust-lang/rfcs`.
I think you're mistaken, if you are aware of details of this feature please file an issue as I could not find `nobarries` in the repo.
http://is.gd/aAMiaU I created a trait that implements a 'variable' that can be get &amp; set through their string representations (this trait needs to be object safe). the `set` operation may fail because it essentially needs to parse the string, and because the trait needs to be object safe the choice for return type is a Result with boxed Error. I made a shared implementation for simple copy types (generic over the contained type). The problem is that `FromStr` uses an associated error type which needs to get converted to a generic Error. This is where the following error happens: ``` &lt;std macros&gt;:6:1: 6:32 error: the trait `std::error::Error` is not implemented for the type `&lt;T as core::str::FromStr&gt;::Err` [E0277] &lt;std macros&gt;:6 $ crate:: convert:: From:: from ( err ) ) } } ) ``` How to fix?
FWIW, the Python code should actually just have chars = string.ascii_letters + string.digits
I first tried this using the `ref` keyword but that wasn't accepted by the compiler: ``` if cond {ref a} else {ref b} = val; ``` An ugly solution is to just take plain mut references but it's ugly: ``` *if cond {&amp;mut a} else {&amp;mut b} = val; ```
Fair. You'll be happy to know your original code does actually run.
This is exactly what I needed, thanks!
Thanks! Unfortunately I can't take ownership as I only have mutable references to the objects I need to operate on :( I found this really ugly solution using `std::mem::swap`: [Ugly solution](https://play.rust-lang.org/?gist=b3e21cdd90df6d999d0e07ffc76541c8&amp;version=stable&amp;backtrace=0). Having to use an `if let` inside a match arm is annoying, but if I don't the compiler complains that my pattern is refutable (and I really don't think it is!)
Wouldn't specialization be mostly "transparent" to the user? (I say mostly, because I expect the versions not to be binary compatible)
Yes I guess it could be, but it's still a game changer.
Doesn't the `vmod_priv` structure need be `#[repr(C)]`?
&gt;serialize shouldn't be responsible for implementing Encodable -- it should be the crate where the type being used is defined. noise-rs also shouldn't be responsible for this impl - its dependencies right now are very small and nice. I don't think it's fair to say that noise-rs has a duty to add a dependency on every library that has a trait that a user might eventually want. &gt;there aren't many easy ways of handling this that don't cause diamond-esque conflicts. What's a diamond-esque conflict in this case? I'm not against generating an error when there are conflicting impls. I've really never seen this problem addressed better than with orphan instances, because forcing an impl to one place or another causes random PR churn, discussion, and dependency bloat (which can cause other problems), all just to centralize something that doesn't have any actual fundamental reason to be centralized. And since there's no real argument for whether the trait lib or the type lib should have the impl, we end up with impls in arbitrary places, with "sideways" dependencies being created in an arbitrary direction. I know I've ended up with circular dependencies more than once this way. Another argument in favor: sometimes an impl makes sense within your code, but only within a certain context you're working in, e.g. implementing "add" for vectors. Edit: replace "should" with "shouldn't".
Not sure it'll work for you in the end but I got it to compile: http://is.gd/NQxYrX I added a bound to the associated error type in the `impl` impl&lt;T: Value + Copy&gt; Var for ConVar&lt;T&gt; where T::Err: error::Error + 'static The reason I'm not sure it'll work is the example required the `'static` lifetime bound and I'm unclear on exactly why that's the case. You might end up having to specify some other lifetime bound. I'm envisioning one of these but, again, I'm not sure; none of them worked in the example. impl&lt;'a, T: Value + Copy&gt; Var for ConVar&lt;T&gt; where T::Err: error::Error + 'a impl&lt;T: Value + Copy&gt; Var for ConVar&lt;T&gt; where for&lt;'a&gt; T::Err: error::Error + 'a impl&lt;'a, T: Value + Copy&gt; Var for ConVar&lt;T&gt; where for&lt;'b: 'a&gt; T::Err: error::Error + 'b
~~I think overflow checks *are used* in release mode, although they can be automagically elided when it's provably safe to do so, e.g. with iterators~~ Scratch that, I thought about bounds checking, not integer arithmetic.
Link to article: http://blog.regehr.org/archives/1384 The slowdown is more reasonable than I thought, unless you're relying on auto-vectorization. For a lot of rust code it probably makes sense to enable `-Z force-overflow-checks=on` in release builds.
Nope http://is.gd/u6LCxS
&gt; No memes. Leave the image macros at home. This applies to comments, as well as top level posts. 
No, that is wrong. There are no overflow checks in release mode. Even if you could elide 50% of all the checks (which would be very optimistic!), it would still be crazy slow.
I don't understand where this `'static` bound is coming from. [FromStr](https://doc.rust-lang.org/std/str/trait.FromStr.html) doesn't talk about lifetimes anywhere. I tried adding the trait bound to the `Value` trait like so: ``` pub trait Value : str::FromStr&lt;Err = error::Error&gt; + string::ToString + Clone {} ``` But then I get `error: the trait `core::marker::Sized` is not implemented for the type `std::error::Error + 'static` [E0277]`
Well, the article up here seems to suggest that the checks are in fact not that slow
Yes. The point is if the implementors are always just f32, f64, we can stack all the functionality they have in common into the trait. It makes numeric code much more convenient. My example was that those types are both Display, are both Any, are both lots of things that are useful.
If you clone the crates.io index you could probably use libgit to count the number of entries at each commit EDIT: I created an example of doing this! https://gist.github.com/Connorcpu/c6deb393718949a77d96739b2a74c7a6
I'll just point you to the currently open RFC for it https://github.com/ubsan/rfcs/blob/custom-dst/text/0000-custom-dst.md
What about something like crate-local impls?
There is now a [`.save_all_limited()`](http://cybergeek94.github.io/multipart/doc/multipart/server/struct.Multipart.html#method.save_all_limited) in 0.7.0 (I released 0.6.0 before remembering I wanted to add these methods.) I might end up combining all the options for saving files into their own builder struct so there's not a bunch of different methods that do very similar things. 
Generating an error doesn't solve the problem; it is part of the problem. It means that impling a trait for a library type becomes a breaking change.
That looks very familiar. :) btw: Is there any reason why `generate_shares` takes a `&amp;Vec&lt;u8&gt;` instead of a `&amp;[u8]` and why `recover_secret` takes a `Vec&lt;String&gt;` by value?
Looking at the RFC (thanks /u/bbatha!), it is at least a step towards returning non-trivial things from Index trait implementations, that's already quite a lot.
The way you want to have it work cannot work (as-is) because what happens if the Vec reallocates its internal memory to make room for more? This would cause all &amp;str borrows in hashmap to end up as dangling pointers. But what are you trying to do with that hashmap? If you just want to efficiently look up if a name is already in the set, you could use a [`HashSet&lt;String&gt;`](https://doc.rust-lang.org/std/collections/hash_set/struct.HashSet.html). 
I don't know a lot about the needs of numerics libraries, but I can answer some of your questions. Custom DSTs are dynamically sized types which are not just one of the built-in DSTs or a wrapper around it. An RFC is [here](https://github.com/ubsan/rfcs/blob/custom-dst/text/0000-custom-dst.md). When people talk about type-level integers in Rust, they usually aren't talking about anything as serious as what you seem to be suggesting - theorem proving and so on. A realistic, more limited, implementation for Rust would be const parameters which are evaluated at compile time just like type parameters are. This is an achievable feature but it doesn't seem to be high priority right now.
I ran into the same problem in my project. The issue is that in general it's unsafe to both own data and store references to that data in the same struct. The reason is it's possible for the field with the owned data to change and invalidate references. That can happen when it just grows or when the struct is dropped. For example, it's possible for the field owning the data to be dropped before the field holding references to that data, and an implementation of `Drop` for whatever holds those references could rely on the references being valid, which would then be dangling. So in the name of safety that construct is disallowed. Knowing that, it's pretty easy to get what you want by storing them separately, creating the owned data and then creating references to it. Here's a working example: http://is.gd/3rkiJH
[removed]
~~Better yet you should drain the vec such that you don't incur the String clone overhead. Changing the signature for From to take ownership from the `Vec&lt;String&gt;` and then using `.into_iter()` should do the trick.~~ EDIT: Hmm this would significantly change how the data is operated on. This isn't quite what OP asked, but may be a better fit for what OP's trying to do.
Hmm? I don't call `clone` in my example at all - I only operate on references to existing `String`s. Taking ownership of the `Vec` would prevent taking references to its contents since those would be invalidated at the end of the method.
I misread your code, my apologies. You are correct. My edit wasn't clear on that.
But aren't they discussing a way to change the panics into traps or halts if you want (https://github.com/rust-lang/rfcs/pull/1513 )? If I compile Rust code with -O -Z force-overflow-checks=on, I don't see a large slowdown (something like 15% slower for heavily integer-numeric code). The real slowdown comes from not using -O (sometimes 10-20 times slower).
Sorry, I misread your statement. I thought you wanted `Float`to be implemented for more than the primitives.
Perhaps not the most interesting, but data! https://i.imgur.com/bmvsXJ7.png X axis is time, Y axis is the number of crates in crates.io (not the number of published versions)
You probably want to ask in /r/playrust.
Basically what's happening here is that `Box&lt;Trait&gt;` defaults to `Box&lt;Trait+'static&gt;`. This is basically elision, though not for functions.
This looks like it outputs crate.io upload timestamps, like a global commit rate but without anything else to compare it to, it doesn't measure the total activity in the rust community.
I keep wondering: why is `'static` the default (elision) lifetime for `Box&lt;Trait&gt;`? I've seen a few posts this week wondering about this...
Hi! Another post about [rusty-machine](https://github.com/AtheMathmo/rusty-machine) - this time with a slightly different flavour. I have just finished a first implementation of [Naive Bayes classifiers](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) in rusty-machine. I thought it would be fun to talk a little about the end-to-end process of designing, writing and testing this model in Rust. The post covers a lot of material - both specific to Rust and Machine learning in different places. I'm not really sure if this will be of interest to anyone reading but I know I'd like to learn more about people's processes. Feedback always welcome!
&gt; with a trap slightly cheaper than a panic It is not cheaper itself. The reason the former is faster is that it has no side-effect, namely that it does not unwind or call destructors, nor can it be catched. This means a lot in fact. From LLVMs point of view, SIGTRAPs are much easier to deal with, and easier to elide.
The overflow check is only done if `debug-assertions` is on, so just add that to _Cargo.toml_.
`Box` is owning, so an owned pointer to a thing that's internally a reference isn't so useful. `Box&lt;Trait+'a&gt;` is a box that will be restricted to a region, in which case you probably want `&amp;'a Trait + 'a`. IIRC the default for `&amp;Trait` is indeed `&amp;'a Trait + 'a`.
Is the compiler special-casing for `Box`?
From what I understood, this library is built on top of `regex` - it offloads all "simple" regex stuff to `regex`, and instead deals directly only with "extended" stuff like lookarounds. That is, see [this](https://github.com/google/pulldown-cmark/blob/d769fab30389def3bf96a82727ae19d9a5ca792e/fancy-regex/src/vm.rs#L234) and the block below it: `inner` is a `regex::Regex`, and the engine merely delegates the matching to it. 
One question, can this engine do [lazy matching](https://en.wikipedia.org/wiki/Regular_expression#Lazy_matching)?
&gt; Note that lazy matching is also a feature of regex-rs. Thanks! I was wondering about this.. but I couldn't find it ~~in the documentation~~ in a quick Google search, but it's [here](https://doc.rust-lang.org/regex/regex/index.html) on the documentation (marked as `(ungreedy)`).
That seems like an use case for including an `-&gt;` operator just for the sake of raw pointers...
Actually, a more interesting question is whether I can have two custom types, `MyBox` and `MyRef`, such as `MyBox&lt;Trait&gt;` stands for `MyBox&lt;Trait+'static&gt;`, but `MyRef&lt;Trait&gt;` implies `MyRef&lt;Trait+'a&gt;`. Perhaps this could be achieved with something like phantom types (I dunno).
Because `'static` places essentially no restrictions on use of the box. This way, if there are restrictions, they are clearly articulated in the type.
So where should we meet?
Yay! I'm so happy you were able to get this work on top of `regex`. Now we have a pure Rust solution to point to for folks that want "modern" regexes. :-) One thing you might consider for testing is to hook it up to Oniguruma and/or PCRE and compare the results of searching on a bunch of different regexes. You probably also want to make sure that `fancy-regex` passes the test suite in `regex`. Oh, and: static ref ONE_CHAR: Regex = Regex::new( "^(\\\\([!-/:-@\\[-`\\{-~aftnrv]|x[0-9a-fA-F]{2}|x\\{[0-9a-fA-F]{1,6}\\})\ |\\.|\\[(\\\\.|[^\\\\\\]])+\\]|\\\\[dDsSwW]|\\\\[pP](\\w|\\{\\w+\\}))" ).unwrap(); You do know about raw string literals, right? e.g., `"\\d" -&gt; r"\d"`. They might provide you with an increase in your quality of life. :-) The `x` flag may also improve your quality of life. :-)
Almost - `impl X for Y` is actually saying that Y does X. I have no idea why the Rust language got it backwards. (don't worry, [Haskell got it backwards too](https://github.com/purescript/purescript/wiki/Differences-from-Haskell#arrow-direction))
Bit late to change now :p
𝓓𝓮𝓪𝓻 𝓼𝓲𝓻 𝓪𝓷𝓭/𝓸𝓻 𝓶𝓪𝓭𝓪𝓶, &amp;nbsp; &amp;nbsp;&amp;nbsp;𝓘 𝔀𝓲𝓼𝓱 𝓽𝓸 𝓻𝓮𝓰𝓲𝓼𝓽𝓮𝓻 𝓪 𝓬𝓸𝓶𝓹𝓵𝓪𝓲𝓷𝓽 𝓪𝓫𝓸𝓾𝓽 𝓽𝓱𝓲𝓼 𝓼𝓸-𝓬𝓪𝓵𝓵𝓮𝓭 "𝓯𝓻𝓲𝓮𝓷𝓭𝓵𝔂" 𝓰𝓾𝓲𝓭𝓮. 𝓤𝓹𝓸𝓷 𝓻𝓮𝓪𝓭𝓲𝓷𝓰 𝓽𝓱𝓲𝓼 𝓭𝓸𝓬𝓾𝓶𝓮𝓷𝓽, 𝓘 𝔀𝓪𝓼 *𝓬𝓸𝓷𝓼𝓲𝓭𝓮𝓻𝓪𝓫𝓵𝔂 𝓫𝓮𝓯𝓾𝓭𝓭𝓵𝓮𝓭* 𝓾𝓹𝓸𝓷 𝓭𝓲𝓼𝓬𝓸𝓿𝓮𝓻𝓲𝓷𝓰 𝓷𝓸𝓽 𝓪 *𝓼𝓲𝓷𝓰𝓵𝓮* 𝓼𝓶𝓲𝓵𝓮𝔂 𝓯𝓪𝓬𝓮 𝓪𝓷𝔂𝔀𝓱𝓮𝓻𝓮 𝔀𝓲𝓽𝓱𝓲𝓷. &amp;nbsp; &amp;nbsp;&amp;nbsp;𝓗𝓸𝔀 𝔂𝓸𝓾 𝓬𝓪𝓷 𝓬𝓵𝓪𝓲𝓶 𝓽𝓱𝓲𝓼 𝓽𝓸 𝓫𝓮 "𝓯𝓻𝓲𝓮𝓷𝓭𝓵𝔂" 𝔀𝓲𝓽𝓱 𝓷𝓪𝓻𝔂 𝓪𝓷 𝓪𝓼𝓲𝓷𝓲𝓷𝓮 𝓮𝔁𝓹𝓻𝓮𝓼𝓼𝓲𝓸𝓷 𝓸𝓯 𝓼𝓲𝓶𝓹𝓵𝓲𝓼𝓽𝓲𝓬 𝓮𝓶𝓸𝓽𝓲𝓸𝓷𝓪𝓵 𝓬𝓸𝓷𝓿𝓮𝔂𝓪𝓷𝓬𝓮 𝓲𝓼 *𝓺𝓾𝓲𝓽𝓮* 𝓫𝓮𝔂𝓸𝓷𝓭 𝓶𝓮. &amp;nbsp; &amp;nbsp;&amp;nbsp;𝓨𝓸𝓾𝓻𝓼 𝓼𝓲𝓷𝓬𝓮𝓻𝓮𝓵𝔂, &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;[𝓔. 𝓙.](https://dl.dropboxusercontent.com/u/7049460/rust/e-j-battersmund.ogg) 𝓑𝓪𝓽𝓽𝓮𝓻𝓼𝓶𝓾𝓷𝓭, (𝓜𝓻𝓼.) &amp;nbsp; &amp;nbsp; But, seriously, this is all *really* good stuff to know. Is this integrated into the crate docs somehow? If it's just a loose file in the repo, I worry that people won't find it. 
Haha. Yeah, basically, I feel like the API documentation for `regex` is already too long. Some of the most important tips are already in the crate docs (e.g., "use lazy_static"). I'd be more open to putting them in the crate docs if there was a way to tuck them in a corner somewhere or something.
We need Monty-Python-complaint-letters-as-a-service now!
&gt; I like the idea of having this guide part of the repository rather than as a separate blog post Me too. It also has the advantage of only taking a few hours to write instead of a few weeks! (I try to avoid targeting too narrow of an audience in blog posts, which makes them much harder to write.)
He said that once about c# and sunk quite a lot of resources into that. 
&gt; how do I deterministically serialize that hashmap and the references? A canonical array and indices seemed like the right idea I agree. That's probably the best way. &gt; I considered just making a sorted copy of the list so I can binary-search the strings That's not a bad idea at all. &gt; it should be perfectly safe to do this kinda thing without extra overhead I'm not sure which extra overhead you're seeing. You seem to be doing the minimal amount of work already, but I don't have a good grasp on what you're really trying to do.
sure. we should invite everyone. either to there, or to a discussion somewhere else.
Rust and C# are quite different languages. That the latter failed to replace significant C++ usage doesn't mean anything for the former.
Still is, right? I thought they aren't doing vala that much anymore
&gt; For most cases, like aVec&lt;T&gt;, which is 3 64 bit words, for most machines it would be just as fast to copy, and you wouldn't have to deference. Note that Vec *is also a pointer*. If you get a reference to the Vec you get an additional indirection, but generally you'd get a *slice* instead which is two words, and points to the *backing store* for the vec, so there's no difference in the number of indirection between the `Vec&lt;T&gt;` and `&amp;[T]`, they're both a `*T` with metadata. Now if you need say an `&amp;mut Vec` (which I think would indeed require a Vec reference not a slice) then yes, you get an additional level of indirection. Though this is an indirection to a nearby stack frame which I really wouldn't expect to be expensive. &gt; but Rust takes advantage of null pointers to provide no sized Option&lt;T&gt;s. I have no idea what you're saying here. Do you mean that `Option` has no runtime overhead when `T` is a non-zero type?
Yep, but sometimes you need it to be a Vec, such as cases where you extend it. Of course then, having it be mutable, it would make more sense for it to use a pointer. But it would also be more convenient to be able to write functions taking `&amp;T`, which provides the most flexibility to the caller, while maintaining performance. Mutable references could technically be removed in some cases, by having them be assigned by return value discretely. Edit: With null pointer optimization, `Option&lt;Box&lt;T&gt;&gt;`, rather than being the size of `size_of bool` + `size_of pointer` it would just be `size_of pointer`, using a null pointer to represent `None`
&gt; Of course then, having it be mutable, it would make more sense for it to use a pointer. Yeah I edited my comment to add something about that in the meantime. You're passing a reference to a nearby stack frame, I'd expect it to be pretty cheap (technically that's what most methods do/get in the first place)
But then every function you'd call on the `Vec` would consume it, so unless you made `Vec` `Copy` (which is impossible since it is already `Drop`), you'd either only be able to use it once or need to return it back to the caller as part of the result.
Sorry, I didn't mean rust's `Copy`, I mean copying the struct to the the stack when borrowing it, and it being borrowed not dropping it. So like "Moving" a value, but not calling drop at end of the function, so the calling scope would retain ownership. I hope that makes sense.
That's what I thought, I don't see how it relates to the rest of the comment though so I wasn't sure I interpreted it right. &gt; Mutable references could technically be removed in some cases, by having them be assigned by return value discretely. Well sure but that would be an optimisation detail, and I'm really not sure it'd be a very valuable one.
Actually would say that on Windows, C# has been very successful replacing C++ for Windows specific applications. Besides games, applications older than C# and OS level coding, there is little new C++ being written. Even on UWA, most developers prefer a mix of C# with some C++/CX, than making full use of the C++ stack. I bet that if Microsoft exposed a WinRT API for those API only accessible to C++, there would exist even less use of C++/CX.
The optimizer can do this IIRC.
The null pointer optimization primarily reduces runtime size, not affecting runtime speed much. Your argument seems to be about runtime speed? Derefs around the same stack frame have a minimal cost.
There's a problem with that: Currently with a borrow, there is a *single canonical* representation of the value, whether on the stack or on the heap. With your proposal, there would exist a copy of that value, which would (hopefully) be canonical for the duration of the call, and cease to be canonical afterwards (at which point the values would be copied back to the original location. Now if the function for whatever reason fails (i.e. panics), who do you suspect would care to either invalidate the memory the function used and/or copy back the changed contents? Note that on panicking, there are no guarantees on what any memory held by a thread will contain.
&gt; I'd say c# is not nearly as futuristic as rust. Reified generics and value types are pretty futuristic. 
because I'll probably use it with [amethyst engine](https://github.com/amethyst/amethyst) that uses yaml for configuration. for amethyst yaml is justified because it is more complex. for baal toml would have been just fine.
I wondered about the same thing! – to pass references (talking about immutable only) to small structs as their contents instead of a pointer. But it's actually not so worthy as it may seem. Consider a big long function. The repeated indirect accesses within it would of course hurt performance. But since `&amp;T` means deep immutability in Rust (at least for types without inherent immutability, such as `Cell`s), the optimizer can and will cache the `T`'s fields in registers at the beginning of function. The overhead of caching would be marginal and the performance of the rest of function would be identical as in your proposed call convention. The short functions on the other hand, would most likely be optimized and inlined anyway, with no sign of pointers in generated assembly. So the wins for your proposed "ref-by-copy" would be only for functions that are both short and couldn't be inlined – either because they are virtual, or they are in different crate with LTO disabled (but isn't LTO on by default? I'm not sure here, let somebody more knowledgabe answer this). But not only this optimization is profitable in small number of cases, it's problematic too. A caller is free to cast `&amp;T` to `*const T` and eg. compare equality of addresses (or worse, assume the raw pointer will live long enough). Passing by value would of course break that possibility (creating references to temporaries is no-go, because it changes semantics). That could be solved in a few ways: 1. Let each function store a metadata whether it uses its argument "just as reference" or in fancy way. And let that metadata be used to choose calling convention. And I think that's a quite a good idea actually, because I feel that would make functions smaller and inlining threshold larger, so the code size would benefit from it. Unfortunately, it won't work for virtual calls, since we can't assume anything about the callee. **Edit: It seems to be implemented, actually! See u/gandro comment. (although only for private functions?)** 2. Always pass both the reference and a value. Seems crazy, but might actually be beneficial. 3. Change the semantics of `&amp;` so that casting to raw pointers can yield different addresses for the value from the same origin. Or just forbid it and introduce separate castable-to-raw reference and uncastable-to-raw reference. But that would have to wait till Rust 2.0. And it would be probably too inconvenient anyway.
Heh, yes I do know about them. I'm not sure I'm planning to use this regex, I'll probably go for by-character scanning for speed and precision (like actually caring which escaped literal is matching so I can unescape). This particular regex started life in my Python prototype, built out of smaller, more understandable pieces, then was cut-n-pasted into this big wad of line noise. For any regexes that stay, I'll be sure to use these tricks to make them more readable.
Sure, on the server, Java has taken a good chunk out of C++'s lunch, and at least on Windows, C# has made nice inroads (it's quite uncommon outside of Microsoft's turf, though). But if C++ is "displaced [..] almost everywhere", why is it [still in the top 10 at RedMonks](http://redmonk.com/sogrady/2016/02/19/language-rankings-1-16/) (even outperforming C# on github repos)?
How do you get the cursive type in Reddit Markdown?
So I have a struct: #[derive(Debug, Clone)] struct Polynomial { coefficients: Vec&lt;Coefficient&gt;, exponents: Vec&lt;Exponent&gt;, constant: f64, } and I am implementing a .get() method: impl Polynomial { fn get(&amp;self, index: &amp;usize) -&gt; (Coefficient, Exponent) { let c = self.coefficients.get(*index).unwrap(); let e = self.exponents.get(*index).unwrap(); (*c, *e) } } But compiling this gives the error: src/main.rs:70:7: 70:9 error: cannot move out of borrowed content [E0507] src/main.rs:70 (*c, *e) ^~ src/main.rs:70:7: 70:9 help: run `rustc --explain E0507` to see a detailed explanation src/main.rs:70:11: 70:13 error: cannot move out of borrowed content [E0507] src/main.rs:70 (*c, *e) I finally got it to compile like this: fn get(&amp;self, index: &amp;usize) -&gt; (&amp;Coefficient, &amp;Exponent) { let c = self.coefficients.get(*index).unwrap(); let e = self.exponents.get(*index).unwrap(); (c, e) } But I don't really understand what was wrong originally or why the fix works (specifically why returning &amp;Coefficient and &amp;Exponent makes any sense)? Can someone please explain? In addition, could someone please explain what returning a &amp;T actually means?
They're [part of Unicode](https://en.wikipedia.org/wiki/Mathematical_Alphanumeric_Symbols#Unicode_chart) (that is, they're not the ascii characters rendered in a different font).
You'd need to have a lifetime bound on the type parameter itself, so, `struct MyRef&lt;'a, T: 'a&gt; { ... }`.
C# should only be evaluated in the context of .NET and the stuff MS is pulling is really ahead of other "big players" out there. F# is an amazing FP language with first class/official support, stuff like async/await and RX came out on .NET first, Roslyn [allows you to create code analyzers for your libraries that can check if your library API is used incorrectly](https://msdn.microsoft.com/en-us/magazine/dn879356.aspx), they are also working on a OSS static compiler based on LLVM (LLILC). A lot of practical innovarions happen on .NET first (in the sense that they were maybe known inacademia but MS was the first implementation that could be used in wider industry). And any new addition to .NET will have a much easier time reusing existing work (of which there is a lot on .NET) than in the native land. If Microsoft play their cards right in a few yeara you won't be asking "why should I use .NET" you will be asking "why shouldn't I use .NET" and Rust will be an answer in cases where you need low level access (if Rust play it's cards right too :) ) because you want high level productivity by default and .NET has the potential to bexome a go to tool for that with the mix of performance/control/productivity it has over the alternatives.
Ok, everyone, you are invited to rust-num on freenode IRC network. Please pass on the invite.
Looks nifty! Will be following as it advances.